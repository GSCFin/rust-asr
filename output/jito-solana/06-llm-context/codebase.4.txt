This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
.buildkite/
  hooks/
    post-checkout
    post-command
    pre-command
  scripts/
    build-stable.sh
    build-stable.test.sh
    common.sh
    common.test.sh
    func-assert-eq.sh
    test-all.sh
    trigger-github-actions-windows-build.sh
  pipeline-upload.sh
  solana-private.sh
.config/
  nextest.toml
.github/
  ISSUE_TEMPLATE/
    0-community.md
    1-core-contributor.md
    2-feature-gate.yml
  scripts/
    add-team-to-ghsa.sh
    check-changelog.sh
    downstream-project-spl-common.sh
    downstream-project-spl-install-deps.sh
    install-all-deps.sh
    install-openssl.sh
    install-proto.sh
    purge-ubuntu-runner.sh
  workflows/
    add-team-to-ghsa.yml
    benchmark.yml
    cargo.yml
    changelog-label.yml
    client-targets.yml
    crate-check.yml
    dependabot-pr.yml
    docs.yml
    downstream-project-anchor.yml
    downstream-project-spl-nightly.yml
    downstream-project-spl.yml
    error-reporting.yml
    label-actions.yml
    publish-windows-tarball.yml
    rebase.yaml
    release.yml
    verify-packets.yml
  CODEOWNERS
  dependabot.yml
  label-actions.yml
  PULL_REQUEST_TEMPLATE.md
  RELEASE_TEMPLATE.md
account-decoder/
  src/
    lib.rs
    parse_account_data.rs
    parse_address_lookup_table.rs
    parse_bpf_loader.rs
    parse_config.rs
    parse_nonce.rs
    parse_stake.rs
    parse_sysvar.rs
    parse_token_extension.rs
    parse_token.rs
    parse_vote.rs
    validator_info.rs
  Cargo.toml
account-decoder-client-types/
  src/
    lib.rs
    token.rs
  Cargo.toml
accounts-cluster-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
accounts-db/
  benches/
    accounts_index.rs
    accounts.rs
    bench_accounts_file.rs
    bench_hashing.rs
    bench_lock_accounts.rs
    bench_serde.rs
    read_only_accounts_cache.rs
    utils.rs
  src/
    account_storage/
      stored_account_info.rs
    accounts_db/
      accounts_db_config.rs
      geyser_plugin_utils.rs
      stats.rs
      tests.rs
    accounts_index/
      account_map_entry.rs
      accounts_index_storage.rs
      bucket_map_holder.rs
      in_mem_accounts_index.rs
      iter.rs
      roots_tracker.rs
      secondary.rs
      stats.rs
    append_vec/
      meta.rs
      test_utils.rs
    rolling_bit_field/
      iterators.rs
    tiered_storage/
      byte_block.rs
      error.rs
      file.rs
      footer.rs
      hot.rs
      index.rs
      meta.rs
      mmap_utils.rs
      owners.rs
      readable.rs
      test_utils.rs
    account_info.rs
    account_locks.rs
    account_storage_reader.rs
    account_storage.rs
    accounts_cache.rs
    accounts_db.rs
    accounts_file.rs
    accounts_hash.rs
    accounts_index.rs
    accounts_update_notifier_interface.rs
    accounts.rs
    active_stats.rs
    ancestors.rs
    ancient_append_vecs.rs
    append_vec.rs
    blockhash_queue.rs
    contains.rs
    is_loadable.rs
    is_zero_lamport.rs
    lib.rs
    obsolete_accounts.rs
    partitioned_rewards.rs
    pubkey_bins.rs
    read_only_accounts_cache.rs
    rolling_bit_field.rs
    sorted_storages.rs
    stake_rewards.rs
    storable_accounts.rs
    tiered_storage.rs
    utils.rs
    waitable_condvar.rs
  store-histogram/
    src/
      main.rs
    Cargo.toml
  store-tool/
    src/
      main.rs
    Cargo.toml
  tests/
    read_only_accounts_cache.rs
  Cargo.toml
bam-banking-bench/
  src/
    main.rs
    mock_bam_server.rs
  .gitignore
  Cargo.toml
bam-local-cluster/
  examples/
    example_config.toml
  src/
    cluster_manager.rs
    config.rs
    lib.rs
    main.rs
  Cargo.toml
  README.md
banking-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
banking-stage-ingress-types/
  src/
    lib.rs
  Cargo.toml
banks-client/
  src/
    error.rs
    lib.rs
  Cargo.toml
banks-interface/
  src/
    lib.rs
  Cargo.toml
banks-server/
  src/
    banks_server.rs
    lib.rs
  Cargo.toml
bench-streamer/
  src/
    main.rs
  .gitignore
  Cargo.toml
bench-tps/
  src/
    bench.rs
    cli.rs
    keypairs.rs
    lib.rs
    log_transaction_service.rs
    main.rs
    perf_utils.rs
    rpc_with_retry_utils.rs
    send_batch.rs
  tests/
    fixtures/
      spl_instruction_padding.so
    bench_tps.rs
  .gitignore
  Cargo.toml
bench-vote/
  src/
    main.rs
  Cargo.toml
bloom/
  benches/
    bloom.rs
  src/
    bloom.rs
    lib.rs
  Cargo.toml
bucket_map/
  src/
    bucket_api.rs
    bucket_item.rs
    bucket_map.rs
    bucket_stats.rs
    bucket_storage.rs
    bucket.rs
    index_entry.rs
    lib.rs
    restart.rs
  tests/
    bucket_map.rs
  Cargo.toml
builtins/
  src/
    core_bpf_migration.rs
    lib.rs
    prototype.rs
  Cargo.toml
builtins-default-costs/
  src/
    lib.rs
  Cargo.toml
bundle/
  src/
    lib.rs
  Cargo.toml
cargo-registry/
  src/
    client.rs
    crate_handler.rs
    main.rs
    response_builder.rs
    sparse_index.rs
  Cargo.toml
ci/
  bench/
    common.sh
    part1.sh
    part2.sh
  common/
    limit-threads.sh
    shared-functions.sh
  coverage/
    common.sh
    part-1.sh
    part-2.sh
    part-3.sh
  docker/
    build.sh
    Dockerfile
    env.sh
    README.md
  downstream-projects/
    common.sh
    func-openbook-dex.sh
    func-spl.sh
    run-all.sh
    run-openbook-dex.sh
    run-spl.sh
  semver_bash/
    LICENSE
    README.md
    semver_test.sh
    semver.sh
  stable/
    common.sh
    run-all.sh
    run-local-cluster-partially.sh
    run-localnet.sh
    run-partition.sh
  xtask/
    src/
      commands/
        bump_version.rs
        hello.rs
      commands.rs
      common.rs
      main.rs
    Cargo.toml
  _
  .gitignore
  buildkite-pipeline.sh
  buildkite-secondary.yml
  buildkite-solana-private.sh
  channel_restriction.sh
  channel-info.sh
  check-channel-version.sh
  check-crates.sh
  check-install-all.sh
  crate-version.sh
  do-audit.sh
  docker-run-default-image.sh
  docker-run.sh
  env.sh
  format-url.sh
  hoover.sh
  intercept.sh
  localnet-sanity.sh
  nits.sh
  order-crates-for-publishing.py
  platform-tools-info.sh
  publish-crate.sh
  publish-installer.sh
  publish-metrics-dashboard.sh
  publish-tarball.sh
  run-local.sh
  run-sanity.sh
  rust-version.sh
  shellcheck.sh
  test-checks.sh
  test-coverage.sh
  test-dev-context-only-utils.sh
  test-downstream-builds.sh
  test-frozen-abi.sh
  test-miri.sh
  test-sanity.sh
  test-shuttle.sh
  test-stable.sh
  test-verify-packets-gossip.sh
  test.sh
  upload-benchmark.sh
  upload-ci-artifact.sh
  upload-github-release-asset.sh
clap-utils/
  src/
    compute_budget.rs
    compute_unit_price.rs
    fee_payer.rs
    input_parsers.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
clap-v3-utils/
  src/
    input_parsers/
      mod.rs
      signer.rs
    keygen/
      derivation_path.rs
      mnemonic.rs
      mod.rs
    compute_budget.rs
    fee_payer.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
cli/
  src/
    address_lookup_table.rs
    checks.rs
    clap_app.rs
    cli.rs
    cluster_query.rs
    compute_budget.rs
    feature.rs
    inflation.rs
    lib.rs
    main.rs
    memo.rs
    nonce.rs
    program_v4.rs
    program.rs
    spend_utils.rs
    stake.rs
    test_utils.rs
    validator_info.rs
    vote.rs
    wallet.rs
  tests/
    fixtures/
      alt_bn128.so
      build.sh
      noop_large.so
      noop.so
    address_lookup_table.rs
    cluster_query.rs
    nonce.rs
    program.rs
    request_airdrop.rs
    stake.rs
    transfer.rs
    validator_info.rs
    vote.rs
  .gitignore
  Cargo.toml
cli-config/
  src/
    config_input.rs
    config.rs
    lib.rs
  Cargo.toml
cli-output/
  src/
    cli_output.rs
    cli_version.rs
    display.rs
    lib.rs
  Cargo.toml
client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    connection_cache.rs
    lib.rs
    send_and_confirm_transactions_in_parallel.rs
    tpu_client.rs
    transaction_executor.rs
  .gitignore
  Cargo.toml
client-test/
  tests/
    client.rs
    send_and_confirm_transactions_in_parallel.rs
  .gitignore
  Cargo.toml
compute-budget/
  src/
    compute_budget_limits.rs
    compute_budget.rs
    lib.rs
  Cargo.toml
compute-budget-instruction/
  benches/
    process_compute_budget_instructions.rs
  src/
    builtin_programs_filter.rs
    compute_budget_instruction_details.rs
    compute_budget_program_id_filter.rs
    instructions_processor.rs
    lib.rs
  Cargo.toml
connection-cache/
  src/
    nonblocking/
      client_connection.rs
      mod.rs
    client_connection.rs
    connection_cache_stats.rs
    connection_cache.rs
    lib.rs
  Cargo.toml
core/
  benches/
    banking_stage.rs
    banking_trace.rs
    consensus.rs
    consumer.rs
    gen_keys.rs
    proto_to_packet.rs
    receive_and_buffer_utils.rs
    receive_and_buffer.rs
    scheduler.rs
    shredder.rs
    sigverify_stage.rs
  src/
    banking_stage/
      transaction_scheduler/
        bam_receive_and_buffer.rs
        bam_scheduler.rs
        bam_utils.rs
        batch_id_generator.rs
        greedy_scheduler.rs
        in_flight_tracker.rs
        mod.rs
        prio_graph_scheduler.rs
        receive_and_buffer.rs
        scheduler_common.rs
        scheduler_controller.rs
        scheduler_error.rs
        scheduler_metrics.rs
        scheduler.rs
        transaction_priority_id.rs
        transaction_state_container.rs
        transaction_state.rs
      committer.rs
      consume_worker.rs
      consumer.rs
      decision_maker.rs
      latest_validator_vote_packet.rs
      leader_slot_metrics.rs
      leader_slot_timing_metrics.rs
      progress_tracker.rs
      qos_service.rs
      read_write_account_set.rs
      scheduler_messages.rs
      tpu_to_pack.rs
      unified_scheduler.rs
      vote_packet_receiver.rs
      vote_storage.rs
      vote_worker.rs
    block_creation_loop/
      stats.rs
    bundle_stage/
      bundle_account_locker.rs
      bundle_consumer.rs
      bundle_packet_deserializer.rs
      bundle_stage_leader_metrics.rs
      bundle_storage.rs
    cluster_slots_service/
      cluster_slots.rs
      slot_supporters.rs
    consensus/
      fork_choice.rs
      heaviest_subtree_fork_choice.rs
      latest_validator_votes_for_frozen_banks.rs
      progress_map.rs
      tower_storage.rs
      tower_vote_state.rs
      tower1_14_11.rs
      tower1_7_14.rs
      tree_diff.rs
      vote_stake_tracker.rs
    forwarding_stage/
      packet_container.rs
    proxy/
      auth.rs
      block_engine_stage.rs
      fetch_stage_manager.rs
      mod.rs
      relayer_stage.rs
    repair/
      ancestor_hashes_service.rs
      cluster_slot_state_verifier.rs
      duplicate_repair_status.rs
      malicious_repair_handler.rs
      mod.rs
      outstanding_requests.rs
      packet_threshold.rs
      quic_endpoint.rs
      repair_generic_traversal.rs
      repair_handler.rs
      repair_response.rs
      repair_service.rs
      repair_weight.rs
      repair_weighted_traversal.rs
      request_response.rs
      result.rs
      serve_repair_service.rs
      serve_repair.rs
      standard_repair_handler.rs
    snapshot_packager_service/
      snapshot_gossip_manager.rs
    tip_manager/
      tip_distribution.rs
      tip_payment.rs
    admin_rpc_post_init.rs
    bam_connection.rs
    bam_dependencies.rs
    bam_manager.rs
    banking_simulation.rs
    banking_stage.rs
    banking_trace.rs
    block_creation_loop.rs
    bundle_sigverify_stage.rs
    bundle_stage.rs
    bundle.rs
    cluster_info_vote_listener.rs
    cluster_slots_service.rs
    commitment_service.rs
    completed_data_sets_service.rs
    consensus.rs
    cost_update_service.rs
    drop_bank_service.rs
    fetch_stage.rs
    forwarding_stage.rs
    gen_keys.rs
    lib.rs
    mock_alpenglow_consensus.rs
    next_leader.rs
    optimistic_confirmation_verifier.rs
    packet_bundle.rs
    replay_stage.rs
    resource_limits.rs
    result.rs
    sample_performance_service.rs
    scheduler_bindings_server.rs
    shred_fetch_stage.rs
    sigverify_stage.rs
    sigverify.rs
    snapshot_packager_service.rs
    staked_nodes_updater_service.rs
    stats_reporter_service.rs
    system_monitor_service.rs
    tip_manager.rs
    tpu_entry_notifier.rs
    tpu.rs
    tvu.rs
    unfrozen_gossip_verified_vote_hashes.rs
    validator.rs
    vortexor_receiver_adapter.rs
    vote_simulator.rs
    voting_service.rs
    warm_quic_cache_service.rs
    window_service.rs
  tests/
    bam_connection.rs
    fork-selection.rs
    scheduler_cost_adjustment.rs
    snapshots.rs
    unified_scheduler.rs
  .gitignore
  Cargo.toml
cost-model/
  benches/
    cost_model.rs
    cost_tracker.rs
  src/
    block_cost_limits.rs
    cost_model.rs
    cost_tracker_post_analysis.rs
    cost_tracker.rs
    lib.rs
    transaction_cost.rs
  Cargo.toml
curves/
  curve25519/
    src/
      curve_syscall_traits.rs
      edwards.rs
      errors.rs
      lib.rs
      ristretto.rs
      scalar.rs
    .gitignore
    Cargo.toml
dev/
  Dockerfile
dev-bins/
  .config/
    nextest.toml
  Cargo.toml
docker-solana/
  .gitignore
  build.sh
  Dockerfile
  README.md
docs/
  art/
    fork-generation.bob
    forks-pruned.bob
    forks-pruned2.bob
    forks.bob
    passive-staking-callflow.msc
    retransmit_stage.bob
    runtime.bob
    sdk-tools.bob
    spv-bank-hash.bob
    spv-block-merkle.bob
    tpu.bob
    transaction.bob
    tvu.bob
    validator-proposal.bob
    validator.bob
  components/
    Card.jsx
    HomeCtaLinks.jsx
  src/
    cli/
      examples/
        _category_.json
        choose-a-cluster.md
        delegate-stake.md
        deploy-a-program.md
        durable-nonce.md
        offline-signing.md
        sign-offchain-message.md
        test-validator.md
        transfer-tokens.md
      wallets/
        hardware/
          _category_.json
          index.md
          ledger.md
        _category_.json
        file-system.md
        index.md
        paper.md
      .usage.md.header
      index.md
      install.md
      intro.md
    clusters/
      available.md
      benchmark.md
      index.md
      metrics.md
      testnet.md
    consensus/
      commitments.md
      fork-generation.md
      leader-rotation.md
      managing-forks.md
      stake-delegation-and-rewards.md
      synchronization.md
      turbine-block-propagation.md
      vote-signing.md
    css/
      custom.css
    implemented-proposals/
      ed_overview/
        ed_validation_client_economics/
          ed_vce_overview.md
          ed_vce_state_validation_protocol_based_rewards.md
          ed_vce_state_validation_transaction_fees.md
          ed_vce_validation_stake_delegation.md
        ed_economic_sustainability.md
        ed_mvp.md
        ed_overview.md
        ed_references.md
        ed_storage_rent_economics.md
      abi-management.md
      bank-timestamp-correction.md
      commitment.md
      durable-tx-nonces.md
      epoch_accounts_hash.md
      index.md
      installer.md
      instruction_introspection.md
      leader-leader-transition.md
      leader-validator-transition.md
      persistent-account-storage.md
      readonly-accounts.md
      reliable-vote-transmission.md
      rent.md
      repair-service.md
      rpc-transaction-history.md
      snapshot-verification.md
      staking-rewards.md
      testing-programs.md
      tower-bft.md
      transaction-fees.md
      validator-timestamp-oracle.md
    operations/
      best-practices/
        _category_.json
        general.md
        monitoring.md
        security.md
      guides/
        _category_.json
        restart-cluster.md
        validator-failover.md
        validator-info.md
        validator-monitor.md
        validator-stake.md
        validator-start.md
        validator-troubleshoot.md
        vote-accounts.md
      _category_.json
      index.md
      prerequisites.md
      requirements.md
      setup-a-validator.md
      setup-an-rpc-node.md
      validator-or-rpc-node.md
    pages/
      styles.module.css
    proposals/
      accepted-design-proposals.md
      accounts-db-replication.md
      bankless-leader.md
      block-confirmation.md
      cluster-test-framework.md
      comprehensive-compute-fees.md
      embedding-move.md
      fee_transaction_priority.md
      handle-duplicate-block.md
      interchain-transaction-verification.md
      ledger-replication-to-implement.md
      log_data.md
      off-chain-message-signing.md
      optimistic_confirmation.md
      optimistic-confirmation-and-slashing.md
      optimistic-transaction-propagation-signal.md
      partitioned-inflationary-rewards-distribution.md
      program-instruction-macro.md
      return-data.md
      rip-curl.md
      simple-payment-and-state-verification.md
      slashing.md
      snapshot-verification.md
      tick-verification.md
      timely-vote-credits.md
      validator-proposal.md
      versioned-transactions.md
      vote-signing-to-implement.md
    runtime/
      zk-docs/
        ciphertext_ciphertext_equality.pdf
        ciphertext_commitment_equality.pdf
        ciphertext_validity.pdf
        percentage_with_cap.pdf
        pubkey_proof.pdf
        twisted_elgamal.pdf
        zero_proof.pdf
      programs.md
      sysvars.md
      zk-elgamal-proof.md
    theme/
      Footer/
        index.js
        styles.module.css
    validator/
      anatomy.md
      blockstore.md
      geyser.md
      gossip.md
      runtime.md
      tpu.md
      tvu.md
    architecture.md
    backwards-compatibility.md
    faq.md
    index.mdx
    proposals.md
    what-is-a-validator.md
    what-is-an-rpc-node.md
  static/
    img/
      favicon.ico
    katex/
      contrib/
        auto-render.js
        auto-render.min.js
        auto-render.mjs
        copy-tex.css
        copy-tex.js
        copy-tex.min.css
        copy-tex.min.js
        copy-tex.mjs
        mathtex-script-type.js
        mathtex-script-type.min.js
        mathtex-script-type.mjs
        mhchem.js
        mhchem.min.js
        mhchem.mjs
        render-a11y-string.js
        render-a11y-string.min.js
        render-a11y-string.mjs
      fonts/
        KaTeX_AMS-Regular.ttf
        KaTeX_AMS-Regular.woff
        KaTeX_AMS-Regular.woff2
        KaTeX_Caligraphic-Bold.ttf
        KaTeX_Caligraphic-Bold.woff
        KaTeX_Caligraphic-Bold.woff2
        KaTeX_Caligraphic-Regular.ttf
        KaTeX_Caligraphic-Regular.woff
        KaTeX_Caligraphic-Regular.woff2
        KaTeX_Fraktur-Bold.ttf
        KaTeX_Fraktur-Bold.woff
        KaTeX_Fraktur-Bold.woff2
        KaTeX_Fraktur-Regular.ttf
        KaTeX_Fraktur-Regular.woff
        KaTeX_Fraktur-Regular.woff2
        KaTeX_Main-Bold.ttf
        KaTeX_Main-Bold.woff
        KaTeX_Main-Bold.woff2
        KaTeX_Main-BoldItalic.ttf
        KaTeX_Main-BoldItalic.woff
        KaTeX_Main-BoldItalic.woff2
        KaTeX_Main-Italic.ttf
        KaTeX_Main-Italic.woff
        KaTeX_Main-Italic.woff2
        KaTeX_Main-Regular.ttf
        KaTeX_Main-Regular.woff
        KaTeX_Main-Regular.woff2
        KaTeX_Math-BoldItalic.ttf
        KaTeX_Math-BoldItalic.woff
        KaTeX_Math-BoldItalic.woff2
        KaTeX_Math-Italic.ttf
        KaTeX_Math-Italic.woff
        KaTeX_Math-Italic.woff2
        KaTeX_SansSerif-Bold.ttf
        KaTeX_SansSerif-Bold.woff
        KaTeX_SansSerif-Bold.woff2
        KaTeX_SansSerif-Italic.ttf
        KaTeX_SansSerif-Italic.woff
        KaTeX_SansSerif-Italic.woff2
        KaTeX_SansSerif-Regular.ttf
        KaTeX_SansSerif-Regular.woff
        KaTeX_SansSerif-Regular.woff2
        KaTeX_Script-Regular.ttf
        KaTeX_Script-Regular.woff
        KaTeX_Script-Regular.woff2
        KaTeX_Size1-Regular.ttf
        KaTeX_Size1-Regular.woff
        KaTeX_Size1-Regular.woff2
        KaTeX_Size2-Regular.ttf
        KaTeX_Size2-Regular.woff
        KaTeX_Size2-Regular.woff2
        KaTeX_Size3-Regular.ttf
        KaTeX_Size3-Regular.woff
        KaTeX_Size3-Regular.woff2
        KaTeX_Size4-Regular.ttf
        KaTeX_Size4-Regular.woff
        KaTeX_Size4-Regular.woff2
        KaTeX_Typewriter-Regular.ttf
        KaTeX_Typewriter-Regular.woff
        KaTeX_Typewriter-Regular.woff2
      katex.css
      katex.js
      katex.min.css
      katex.min.js
      katex.mjs
      README.md
    .nojekyll
  .eslintignore
  .eslintrc
  .gitignore
  .prettierignore
  .prettierrc.json
  babel.config.js
  build-cli-usage.sh
  build.sh
  convert-ascii-to-svg.sh
  deploy.sh
  docusaurus.config.js
  offline-cmd-md-links.sh
  package.json
  README.md
  set-solana-release-tag.sh
  sidebars.js
dos/
  src/
    cli.rs
    lib.rs
    main.rs
  Cargo.toml
download-utils/
  src/
    lib.rs
  Cargo.toml
entry/
  benches/
    entry_sigverify.rs
  src/
    entry.rs
    lib.rs
    poh.rs
    wincode.rs
  Cargo.toml
faucet/
  src/
    bin/
      faucet.rs
    faucet_mock.rs
    faucet.rs
    lib.rs
  tests/
    local-faucet.rs
  .gitignore
  Cargo.toml
feature-set/
  src/
    lib.rs
  Cargo.toml
fee/
  src/
    lib.rs
  Cargo.toml
fs/
  src/
    io_uring/
      dir_remover.rs
      file_creator.rs
      memory.rs
      mod.rs
      sequential_file_reader.rs
    buffered_reader.rs
    dirs.rs
    file_io.rs
    lib.rs
  Cargo.toml
genesis/
  src/
    address_generator.rs
    genesis_accounts.rs
    lib.rs
    main.rs
    stakes.rs
    unlocks.rs
  .gitignore
  Cargo.toml
  README.md
genesis-utils/
  src/
    lib.rs
    open.rs
  Cargo.toml
geyser-plugin-interface/
  src/
    geyser_plugin_interface.rs
    lib.rs
  Cargo.toml
  README.md
geyser-plugin-manager/
  src/
    accounts_update_notifier.rs
    block_metadata_notifier_interface.rs
    block_metadata_notifier.rs
    entry_notifier.rs
    geyser_plugin_manager.rs
    geyser_plugin_service.rs
    lib.rs
    slot_status_notifier.rs
    slot_status_observer.rs
    transaction_notifier.rs
  Cargo.toml
gossip/
  benches/
    crds_gossip_pull.rs
    crds_shards.rs
    crds.rs
    weighted_shuffle.rs
  src/
    cluster_info_metrics.rs
    cluster_info.rs
    contact_info.rs
    crds_data.rs
    crds_entry.rs
    crds_filter.rs
    crds_gossip_error.rs
    crds_gossip_pull.rs
    crds_gossip_push.rs
    crds_gossip.rs
    crds_shards.rs
    crds_value.rs
    crds.rs
    deprecated.rs
    duplicate_shred_handler.rs
    duplicate_shred_listener.rs
    duplicate_shred.rs
    epoch_slots.rs
    epoch_specs.rs
    gossip_error.rs
    gossip_service.rs
    legacy_contact_info.rs
    lib.rs
    node.rs
    ping_pong.rs
    protocol.rs
    push_active_set.rs
    received_cache.rs
    restart_crds_values.rs
    tlv.rs
    weighted_shuffle.rs
    wire_format_tests.rs
  tests/
    crds_gossip.rs
    gossip.rs
  .gitignore
  Cargo.toml
gossip-bin/
  src/
    main.rs
  Cargo.toml
install/
  src/
    bin/
      agave-install-init.rs
    build_env.rs
    command.rs
    config.rs
    defaults.rs
    lib.rs
    main.rs
    stop_process.rs
    update_manifest.rs
  .gitignore
  agave-install-init.sh
  build.rs
  Cargo.toml
  install-help.sh
io-uring/
  src/
    lib.rs
    ring.rs
    slab.rs
  Cargo.toml
jito-protos/
  src/
    lib.rs
  build.rs
  Cargo.toml
keygen/
  src/
    keygen.rs
  .gitignore
  Cargo.toml
lattice-hash/
  benches/
    bench_lt_hash.rs
  src/
    lib.rs
    lt_hash.rs
  Cargo.toml
ledger/
  benches/
    blockstore.rs
    make_shreds_from_entries.rs
    protobuf.rs
  proptest-regressions/
    blockstore_meta.txt
  src/
    blockstore/
      blockstore_purge.rs
      column.rs
      error.rs
    leader_schedule/
      identity_keyed.rs
      vote_keyed.rs
    shred/
      common.rs
      merkle_tree.rs
      merkle.rs
      payload.rs
      shred_code.rs
      shred_data.rs
      stats.rs
      traits.rs
      wire.rs
    ancestor_iterator.rs
    bank_forks_utils.rs
    bigtable_delete.rs
    bigtable_upload_service.rs
    bigtable_upload.rs
    bit_vec.rs
    block_error.rs
    blockstore_cleanup_service.rs
    blockstore_db.rs
    blockstore_meta.rs
    blockstore_metric_report_service.rs
    blockstore_metrics.rs
    blockstore_options.rs
    blockstore_processor.rs
    blockstore.rs
    entry_notifier_interface.rs
    entry_notifier_service.rs
    genesis_utils.rs
    leader_schedule_cache.rs
    leader_schedule_utils.rs
    leader_schedule.rs
    lib.rs
    next_slots_iterator.rs
    rooted_slot_iterator.rs
    shred.rs
    shredder.rs
    sigverify_shreds.rs
    slot_stats.rs
    staking_utils.rs
    token_balances.rs
    transaction_address_lookup_table_scanner.rs
    transaction_balances.rs
    use_snapshot_archives_at_startup.rs
    wire_format_tests.rs
  tests/
    blockstore.rs
    shred.rs
  .gitignore
  Cargo.toml
ledger-tool/
  src/
    args.rs
    bigtable.rs
    blockstore.rs
    error.rs
    ledger_path.rs
    ledger_utils.rs
    main.rs
    output.rs
    program.rs
  tests/
    basic.rs
  .gitignore
  Cargo.toml
local-cluster/
  src/
    cluster_tests.rs
    cluster.rs
    integration_tests.rs
    lib.rs
    local_cluster_snapshot_utils.rs
    local_cluster.rs
    validator_configs.rs
  tests/
    local_cluster.rs
  .gitignore
  Cargo.toml
logger/
  src/
    lib.rs
  Cargo.toml
measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  .gitignore
  Cargo.toml
merkle-tree/
  src/
    lib.rs
    merkle_tree.rs
  .gitignore
  Cargo.toml
metrics/
  benches/
    metrics.rs
  scripts/
    grafana-provisioning/
      dashboards/
        cluster-monitor.json
        dashboard.yml
    .gitignore
    adjust-dashboard-for-channel.py
    enable.sh
    grafana.ini
    influxdb.conf
    README.md
    start.sh
    status.sh
    stop.sh
    test.sh
  src/
    counter.rs
    datapoint.rs
    lib.rs
    metrics.rs
  .gitignore
  Cargo.toml
  README.md
multinode-demo/
  bench-tps.sh
  bootstrap-validator.sh
  common.sh
  delegate-stake.sh
  faucet.sh
  setup-from-mainnet-beta.sh
  setup-from-testnet.sh
  setup.sh
  validator-x.sh
  validator.sh
net/
  remote/
    cleanup.sh
    README.md
    remote-client.sh
    remote-deploy-update.sh
    remote-node-wait-init.sh
    remote-node.sh
    remote-sanity.sh
  scripts/
    azure-provider.sh
    colo_nodes
    colo-node-onacquire.sh
    colo-node-onfree.sh
    colo-provider.sh
    colo-utils.sh
    create-solana-user.sh
    disable-background-upgrades.sh
    ec2-provider.sh
    ec2-security-group-config.json
    gce-provider.sh
    gce-self-destruct.sh
    install-ag.sh
    install-at.sh
    install-certbot.sh
    install-docker.sh
    install-earlyoom.sh
    install-iftop.sh
    install-jq.sh
    install-libssl.sh
    install-perf.sh
    install-rsync.sh
    localtime.sh
    mount-additional-disk.sh
    network-config.sh
    remove-docker-interface.sh
    rsync-retry.sh
  .gitignore
  common.sh
  gce.sh
  init-metrics.sh
  net.sh
  scp.sh
  ssh.sh
net-utils/
  benches/
    token_bucket.rs
  src/
    ip_echo_client.rs
    ip_echo_server.rs
    lib.rs
    multihomed_sockets.rs
    socket_addr_space.rs
    sockets.rs
    token_bucket.rs
    tooling_for_tests.rs
  .gitignore
  Cargo.toml
notifier/
  src/
    lib.rs
  .gitignore
  Cargo.toml
perf/
  benches/
    dedup.rs
    discard.rs
    recycler.rs
    reset.rs
    shrink.rs
    sigverify.rs
  src/
    data_budget.rs
    deduper.rs
    discard.rs
    lib.rs
    packet.rs
    perf_libs.rs
    recycled_vec.rs
    recycler_cache.rs
    recycler.rs
    sigverify.rs
    test_tx.rs
    thread.rs
  build.rs
  Cargo.toml
platform-tools-sdk/
  cargo-build-sbf/
    src/
      main.rs
      post_processing.rs
      toolchain.rs
      utils.rs
    tests/
      crates/
        fail/
          src/
            lib.rs
          Cargo.toml
        noop/
          src/
            lib.rs
          Cargo.toml
        package-metadata/
          src/
            lib.rs
          Cargo.toml
        workspace-metadata/
          src/
            lib.rs
          Cargo.toml
      crates.rs
    .gitignore
    Cargo.toml
  cargo-test-sbf/
    src/
      main.rs
    Cargo.toml
  gen-headers/
    src/
      main.rs
    Cargo.toml
  sbf/
    c/
      inc/
        sol/
          inc/
            alt_bn128_compression.inc
            alt_bn128.inc
            assert.inc
            big_mod_exp.inc
            blake3.inc
            compute_units.inc
            cpi.inc
            keccak.inc
            last_restart_slot.inc
            log.inc
            poseidon.inc
            pubkey.inc
            return_data.inc
            secp256k1.inc
            sha.inc
          alt_bn128_compression.h
          alt_bn128.h
          assert.h
          big_mod_exp.h
          blake3.h
          compute_units.h
          constants.h
          cpi.h
          deserialize_deprecated.h
          deserialize.h
          entrypoint.h
          keccak.h
          last_restart_slot.h
          log.h
          poseidon.h
          pubkey.h
          return_data.h
          secp256k1.h
          sha.h
          string.h
          types.h
        sys/
          param.h
        deserialize_deprecated.h
        solana_sdk.h
        stdio.h
        stdlib.h
        string.h
        wchar.h
      README.md
      sbf.ld
      sbf.mk
    scripts/
      dump.sh
      install.sh
      objcopy.sh
      package.sh
      strip.sh
    .gitignore
    env.sh
poh/
  benches/
    poh_verify.rs
    poh.rs
    transaction_recorder.rs
  src/
    lib.rs
    poh_controller.rs
    poh_recorder.rs
    poh_service.rs
    record_channels.rs
    transaction_recorder.rs
  .gitignore
  Cargo.toml
poh-bench/
  src/
    main.rs
  Cargo.toml
poseidon/
  src/
    legacy.rs
    lib.rs
  Cargo.toml
precompiles/
  benches/
    ed25519_instructions.rs
    secp256k1_instructions.rs
    secp256r1_instructions.rs
  src/
    ed25519.rs
    lib.rs
    secp256k1.rs
    secp256r1.rs
  Cargo.toml
program-binaries/
  src/
    programs/
      core_bpf_address_lookup_table-3.0.0.so
      core_bpf_config-3.0.0.so
      core_bpf_feature_gate-0.0.1.so
      core_bpf_stake-1.0.1.so
      spl_associated_token_account-1.1.1.so
      spl_memo-1.0.0.so
      spl_memo-3.0.0.so
      spl_token_2022-8.0.0.so
      spl_token-3.5.0.so
      spl-jito_tip_distribution-0.1.10.so
      spl-jito_tip_distribution-0.1.4.so
      spl-jito_tip_distribution-0.1.7.so
      spl-jito_tip_payment-0.1.10.so
      spl-jito_tip_payment-0.1.4.so
      spl-jito_tip_payment-0.1.7.so
    lib.rs
  Cargo.toml
program-runtime/
  src/
    cpi.rs
    execution_budget.rs
    invoke_context.rs
    lib.rs
    loaded_programs.rs
    mem_pool.rs
    memory.rs
    serialization.rs
    stable_log.rs
    sysvar_cache.rs
  Cargo.toml
program-test/
  src/
    lib.rs
  tests/
    fixtures/
      noop_program.so
    bpf.rs
    builtins.rs
    compute_units.rs
    core_bpf.rs
    cpi.rs
    fuzz.rs
    genesis_accounts.rs
    lamports.rs
    panic.rs
    realloc.rs
    return_data.rs
    setup.rs
    spl.rs
    sysvar_last_restart_slot.rs
    sysvar.rs
    warp.rs
  Cargo.toml
programs/
  bpf_loader/
    benches/
      bpf_loader_upgradeable.rs
      serialization.rs
    src/
      lib.rs
    test_elfs/
      out/
        noop_aligned.so
        noop_unaligned.so
        sbpfv0_verifier_err.so
        sbpfv3_return_err.so
        sbpfv3_return_ok.so
      src/
        noop_aligned/
          noop_aligned.c
        noop_unaligned/
          noop_unaligned.c
      makefile
    Cargo.toml
  bpf-loader-tests/
    tests/
      common.rs
      extend_program_ix.rs
    Cargo.toml
    noop.so
  compute-budget/
    src/
      lib.rs
    Cargo.toml
  compute-budget-bench/
    benches/
      compute_budget.rs
    Cargo.toml
  ed25519-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  loader-v4/
    src/
      lib.rs
    Cargo.toml
  sbf/
    benches/
      bpf_loader.rs
    c/
      src/
        alloc/
          alloc.c
        alt_bn128/
          alt_bn128.c
        alt_bn128_compression/
          alt_bn128.c
        bench_alu/
          bench_alu.c
          test_bench_alu.c
        big_mod_exp/
          big_mod_exp.c
        deprecated_loader/
          deprecated_loader.c
        dup_accounts/
          dup_accounts.c
        error_handling/
          error_handling.c
        float/
          float.c
        invoke/
          invoke.c
        invoked/
          instruction.h
          invoked.c
        log_data/
          log_data.c
        move_funds/
          move_funds.c
        multiple_static/
          multiple_static.c
        noop/
          noop.c
        noop++/
          noop++.cc
        panic/
          panic.c
        poseidon/
          poseidon.c
        read_program/
          read_program.c
        relative_call/
          relative_call.c
        remaining_compute_units/
          remaining_compute_units.c
        return_data/
          return_data.c
        sanity/
          sanity.c
        sanity++/
          sanity++.cc
        sbf_to_sbf/
          entrypoint.c
          helper.c
          helper.h
        secp256k1_recover/
          secp256k1_recover.c
        ser/
          ser.c
        sha/
          sha.c
        stdlib/
          stdlib.c
        struct_pass/
          struct_pass.c
        struct_ret/
          struct_ret.c
        tuner/
          tuner.c
        tuner-variable-iterations/
          tuner-variable-iterations.c
      .gitignore
    rust/
      128bit/
        src/
          lib.rs
        Cargo.toml
      128bit_dep/
        src/
          lib.rs
        Cargo.toml
      account_mem/
        src/
          lib.rs
        Cargo.toml
      account_mem_deprecated/
        src/
          lib.rs
        Cargo.toml
      alloc/
        src/
          lib.rs
        Cargo.toml
      alt_bn128/
        src/
          lib.rs
        Cargo.toml
      alt_bn128_compression/
        src/
          lib.rs
        Cargo.toml
      big_mod_exp/
        src/
          lib.rs
        Cargo.toml
      call_args/
        src/
          lib.rs
        Cargo.toml
      call_depth/
        src/
          lib.rs
        Cargo.toml
      caller_access/
        src/
          lib.rs
        Cargo.toml
      curve25519/
        src/
          lib.rs
        Cargo.toml
      custom_heap/
        src/
          lib.rs
        Cargo.toml
      dep_crate/
        src/
          lib.rs
        Cargo.toml
      deprecated_loader/
        src/
          lib.rs
        Cargo.toml
      divide_by_zero/
        src/
          lib.rs
        Cargo.toml
      dup_accounts/
        src/
          lib.rs
        Cargo.toml
      error_handling/
        src/
          lib.rs
        Cargo.toml
      external_spend/
        src/
          lib.rs
        Cargo.toml
      get_minimum_delegation/
        src/
          lib.rs
        Cargo.toml
      inner_instruction_alignment_check/
        src/
          lib.rs
        Cargo.toml
      instruction_introspection/
        src/
          lib.rs
        Cargo.toml
      invoke/
        src/
          lib.rs
        Cargo.toml
      invoke_and_error/
        src/
          lib.rs
        Cargo.toml
      invoke_and_ok/
        src/
          lib.rs
        Cargo.toml
      invoke_and_return/
        src/
          lib.rs
        Cargo.toml
      invoke_dep/
        src/
          lib.rs
        Cargo.toml
      invoked/
        src/
          lib.rs
        Cargo.toml
      invoked_dep/
        src/
          lib.rs
        Cargo.toml
      iter/
        src/
          lib.rs
        Cargo.toml
      log_data/
        src/
          lib.rs
        Cargo.toml
      many_args/
        src/
          helper.rs
          lib.rs
        Cargo.toml
      many_args_dep/
        src/
          lib.rs
        Cargo.toml
      mem/
        src/
          lib.rs
        Cargo.toml
      mem_dep/
        src/
          lib.rs
        Cargo.toml
      membuiltins/
        src/
          lib.rs
        Cargo.toml
      noop/
        src/
          lib.rs
        Cargo.toml
      panic/
        src/
          lib.rs
        Cargo.toml
      param_passing/
        src/
          lib.rs
        Cargo.toml
      param_passing_dep/
        src/
          lib.rs
        Cargo.toml
      poseidon/
        src/
          lib.rs
        Cargo.toml
      r2_instruction_data_pointer/
        src/
          lib.rs
        Cargo.toml
      rand/
        src/
          lib.rs
        Cargo.toml
      realloc/
        src/
          lib.rs
        Cargo.toml
      realloc_dep/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke_dep/
        src/
          lib.rs
        Cargo.toml
      remaining_compute_units/
        src/
          lib.rs
        Cargo.toml
      ro_account_modify/
        src/
          lib.rs
        Cargo.toml
      ro_modify/
        src/
          lib.rs
        Cargo.toml
      sanity/
        src/
          lib.rs
        Cargo.toml
      secp256k1_recover/
        src/
          lib.rs
        Cargo.toml
      sha/
        src/
          lib.rs
        Cargo.toml
      sibling_inner_instructions/
        src/
          lib.rs
        Cargo.toml
      sibling_instructions/
        src/
          lib.rs
        Cargo.toml
      simulation/
        src/
          lib.rs
        Cargo.toml
      spoof1/
        src/
          lib.rs
        Cargo.toml
      spoof1_system/
        src/
          lib.rs
        Cargo.toml
      syscall-get-epoch-stake/
        src/
          lib.rs
        Cargo.toml
      sysvar/
        src/
          lib.rs
        Cargo.toml
      upgradeable/
        src/
          lib.rs
        Cargo.toml
      upgraded/
        src/
          lib.rs
        Cargo.toml
    tests/
      programs.rs
      simulation.rs
      syscall_get_epoch_stake.rs
      sysvar.rs
    .gitignore
    Cargo.toml
    Makefile
  system/
    benches/
      system.rs
    src/
      lib.rs
      system_instruction.rs
      system_processor.rs
    Cargo.toml
  vote/
    benches/
      process_vote.rs
      vote_instructions.rs
    src/
      vote_state/
        handler.rs
        mod.rs
      lib.rs
      vote_processor.rs
    Cargo.toml
  zk-elgamal-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
  zk-elgamal-proof-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  zk-token-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
pubsub-client/
  src/
    nonblocking/
      mod.rs
      pubsub_client.rs
    lib.rs
    pubsub_client.rs
  Cargo.toml
quic-client/
  src/
    nonblocking/
      mod.rs
      quic_client.rs
    lib.rs
    quic_client.rs
  tests/
    quic_client.rs
  Cargo.toml
rayon-threadlimit/
  src/
    lib.rs
  .gitignore
  Cargo.toml
rbpf-cli/
  src/
    main.rs
  Cargo.toml
remote-wallet/
  src/
    ledger_error.rs
    ledger.rs
    lib.rs
    locator.rs
    remote_keypair.rs
    remote_wallet.rs
  Cargo.toml
  README.md
reserved-account-keys/
  src/
    lib.rs
  Cargo.toml
rpc/
  src/
    rpc/
      account_resolver.rs
    cluster_tpu_info.rs
    filter.rs
    lib.rs
    max_slots.rs
    optimistically_confirmed_bank_tracker.rs
    parsed_token_accounts.rs
    rpc_cache.rs
    rpc_completed_slots_service.rs
    rpc_health.rs
    rpc_pubsub_service.rs
    rpc_pubsub.rs
    rpc_service.rs
    rpc_subscription_tracker.rs
    rpc_subscriptions.rs
    rpc.rs
    slot_status_notifier.rs
    transaction_notifier_interface.rs
    transaction_status_service.rs
  .gitignore
  Cargo.toml
rpc-client/
  src/
    nonblocking/
      mod.rs
      rpc_client.rs
    http_sender.rs
    lib.rs
    mock_sender.rs
    rpc_client.rs
    rpc_sender.rs
    spinner.rs
  Cargo.toml
rpc-client-api/
  src/
    bundles.rs
    client_error.rs
    custom_error.rs
    lib.rs
    response.rs
  Cargo.toml
rpc-client-nonce-utils/
  src/
    nonblocking/
      blockhash_query.rs
      mod.rs
    blockhash_query.rs
    lib.rs
  Cargo.toml
rpc-client-types/
  src/
    config.rs
    error_object.rs
    filter.rs
    lib.rs
    request.rs
    response.rs
  Cargo.toml
rpc-test/
  tests/
    nonblocking.rs
    rpc.rs
  .gitignore
  Cargo.toml
runtime/
  benches/
    accounts.rs
    prioritization_fee_cache.rs
    status_cache.rs
  src/
    accounts_background_service/
      pending_snapshot_packages.rs
      stats.rs
    bank/
      builtins/
        core_bpf_migration/
          error.rs
          mod.rs
          source_buffer.rs
          target_bpf_v2.rs
          target_builtin.rs
          target_core_bpf.rs
        mod.rs
      partitioned_epoch_rewards/
        calculation.rs
        distribution.rs
        epoch_rewards_hasher.rs
        mod.rs
        sysvar.rs
      accounts_lt_hash.rs
      address_lookup_table.rs
      bank_hash_details.rs
      check_transactions.rs
      fee_distribution.rs
      metrics.rs
      recent_blockhashes_account.rs
      serde_snapshot.rs
      sysvar_cache.rs
      tests.rs
    inflation_rewards/
      mod.rs
      points.rs
    serde_snapshot/
      obsolete_accounts.rs
      status_cache.rs
      storage.rs
      tests.rs
      types.rs
      utils.rs
    snapshot_package/
      compare.rs
    snapshot_utils/
      snapshot_storage_rebuilder.rs
    stakes/
      serde_stakes.rs
    account_saver.rs
    accounts_background_service.rs
    bank_client.rs
    bank_forks.rs
    bank_hash_cache.rs
    bank_utils.rs
    bank.rs
    commitment.rs
    dependency_tracker.rs
    epoch_stakes.rs
    genesis_utils.rs
    installed_scheduler_pool.rs
    lib.rs
    loader_utils.rs
    non_circulating_supply.rs
    prioritization_fee_cache.rs
    prioritization_fee.rs
    read_optimized_dashmap.rs
    rent_collector.rs
    runtime_config.rs
    serde_snapshot.rs
    snapshot_bank_utils.rs
    snapshot_controller.rs
    snapshot_minimizer.rs
    snapshot_package.rs
    snapshot_utils.rs
    stake_account.rs
    stake_history.rs
    stake_utils.rs
    stake_weighted_timestamp.rs
    stakes.rs
    static_ids.rs
    status_cache.rs
    transaction_batch.rs
    vote_sender_types.rs
  .gitignore
  Cargo.toml
runtime-transaction/
  benches/
    get_signature_details.rs
  src/
    runtime_transaction/
      sdk_transactions.rs
      transaction_view.rs
    instruction_data_len.rs
    instruction_meta.rs
    lib.rs
    runtime_transaction.rs
    signature_details.rs
    transaction_meta.rs
    transaction_with_meta.rs
  Cargo.toml
scheduler-bindings/
  src/
    lib.rs
  Cargo.toml
scheduling-utils/
  src/
    handshake/
      client.rs
      mod.rs
      server.rs
      shared.rs
      tests.rs
    error.rs
    lib.rs
    pubkeys_ptr.rs
    responses_region.rs
    thread_aware_account_locks.rs
    transaction_ptr.rs
  Cargo.toml
scripts/
  agave-build-lists.sh
  agave-install-deploy.sh
  agave-install-update-manifest-keypair.sh
  build-agave-xdp-ebpf.sh
  build-downstream-anchor-projects.sh
  cargo-clippy-nightly.sh
  cargo-clippy.sh
  cargo-for-all-lock-files.sh
  cargo-install-all.sh
  check-dev-context-only-utils.sh
  configure-metrics.sh
  confirm-cargo-version-numbers-before-bump.sh
  coverage.sh
  create-release-tarball.sh
  elf-hash-symbol.sh
  fd-monitor.sh
  generate-target-triple.sh
  iftop.sh
  increment-cargo-version.sh
  metrics-write-datapoint.sh
  net-shaper.sh
  net-stats.sh
  oom-monitor.sh
  oom-score-adj.sh
  patch-crates.sh
  patch-spl-crates-for-anchor.sh
  perf-plot.py
  perf-stats.py
  read-cargo-variable.sh
  reserve-cratesio-package-name.sh
  run.sh
  sed-i-all-rs-files-for-rust-analyzer.sh
  spl-token-cli-version.sh
  system-stats.sh
  ulimit-n.sh
  wallet-sanity.sh
sdk/
  README.md
send-transaction-service/
  src/
    lib.rs
    send_transaction_service_stats.rs
    send_transaction_service.rs
    test_utils.rs
    tpu_info.rs
    transaction_client.rs
  Cargo.toml
snapshots/
  src/
    archive_format.rs
    archive.rs
    error.rs
    hardened_unpack.rs
    kind.rs
    lib.rs
    paths.rs
    snapshot_archive_info.rs
    snapshot_config.rs
    snapshot_hash.rs
    snapshot_interval.rs
    snapshot_version.rs
    unarchive.rs
  Cargo.toml
stake-accounts/
  src/
    arg_parser.rs
    args.rs
    main.rs
    stake_accounts.rs
  Cargo.toml
storage-bigtable/
  build-proto/
    src/
      main.rs
    .gitignore
    build.sh
    Cargo.toml
    README.md
  proto/
    google.api.rs
    google.bigtable.v2.rs
    google.protobuf.rs
    google.rpc.rs
  src/
    access_token.rs
    bigtable.rs
    compression.rs
    lib.rs
    pki-goog-roots.pem
    root_ca_certificate.rs
  Cargo.toml
  init-bigtable.sh
  README.md
storage-proto/
  proto/
    confirmed_block.proto
    entries.proto
    transaction_by_addr.proto
  src/
    convert.rs
    lib.rs
  build.rs
  Cargo.toml
  README.md
streamer/
  examples/
    swqos.rs
  src/
    nonblocking/
      connection_rate_limiter.rs
      mod.rs
      qos.rs
      quic.rs
      recvmmsg.rs
      sendmmsg.rs
      simple_qos.rs
      stream_throttle.rs
      swqos.rs
      testing_utilities.rs
    evicting_sender.rs
    lib.rs
    msghdr.rs
    packet.rs
    quic.rs
    recvmmsg.rs
    sendmmsg.rs
    streamer.rs
  tests/
    recvmmsg.rs
  Cargo.toml
svm/
  doc/
    diagrams/
      context.svg
      context.tex
    spec.md
  src/
    account_loader.rs
    account_overrides.rs
    lib.rs
    message_processor.rs
    nonce_info.rs
    program_loader.rs
    rent_calculator.rs
    rollback_accounts.rs
    transaction_account_state_info.rs
    transaction_balances.rs
    transaction_commit_result.rs
    transaction_error_metrics.rs
    transaction_execution_result.rs
    transaction_processing_callback.rs
    transaction_processing_result.rs
    transaction_processor.rs
  tests/
    example-programs/
      clock-sysvar/
        src/
          lib.rs
        Cargo.toml
        clock_sysvar_program.so
      hello-solana/
        src/
          lib.rs
        Cargo.toml
        hello_solana_program.so
      simple-transfer/
        src/
          lib.rs
        Cargo.toml
        simple_transfer_program.so
      transfer-from-account/
        src/
          lib.rs
        Cargo.toml
        transfer_from_account_program.so
      write-to-account/
        src/
          lib.rs
        Cargo.toml
        write_to_account_program.so
    concurrent_tests.rs
    integration_test.rs
    mock_bank.rs
  Cargo.toml
svm-callback/
  src/
    lib.rs
  Cargo.toml
svm-feature-set/
  src/
    lib.rs
  Cargo.toml
svm-log-collector/
  src/
    lib.rs
  Cargo.toml
svm-measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  Cargo.toml
svm-rent-calculator/
  src/
    lib.rs
    rent_state.rs
    svm_rent_calculator.rs
  Cargo.toml
svm-test-harness/
  bin/
    test_exec_instr.rs
  src/
    fixture/
      account_state.rs
      error.rs
      feature_set.rs
      instr_context.rs
      instr_effects.rs
      mod.rs
    file.rs
    fuzz.rs
    instr.rs
    lib.rs
    program_cache.rs
    sysvar_cache.rs
  .gitignore
  build.rs
  Cargo.toml
  Makefile
svm-timings/
  src/
    lib.rs
  Cargo.toml
svm-transaction/
  src/
    svm_message/
      sanitized_message.rs
      sanitized_transaction.rs
    svm_transaction/
      sanitized_transaction.rs
    instruction.rs
    lib.rs
    message_address_table_lookup.rs
    svm_message.rs
    svm_transaction.rs
    tests.rs
  Cargo.toml
svm-type-overrides/
  src/
    lib.rs
  Cargo.toml
syscalls/
  gen-syscall-list/
    src/
      main.rs
    build.rs
    Cargo.toml
  src/
    cpi.rs
    lib.rs
    logging.rs
    mem_ops.rs
    sysvar.rs
  Cargo.toml
test-validator/
  src/
    lib.rs
  Cargo.toml
thread-manager/
  examples/
    common/
      mod.rs
    core_contention_basics.rs
    core_contention_contending_set.toml
    core_contention_dedicated_set.toml
    core_contention_sweep.rs
  src/
    lib.rs
    native_thread_runtime.rs
    policy.rs
    rayon_runtime.rs
    tokio_runtime.rs
  Cargo.toml
  README.md
tls-utils/
  src/
    config.rs
    crypto_provider.rs
    lib.rs
    quic_client_certificate.rs
    skip_client_verification.rs
    skip_server_verification.rs
    tls_certificates.rs
  Cargo.toml
  README
tokens/
  src/
    arg_parser.rs
    args.rs
    commands.rs
    db.rs
    lib.rs
    main.rs
    spl_token.rs
    stake.rs
    token_display.rs
  tests/
    commands.rs
  .gitignore
  Cargo.toml
  README.md
tps-client/
  src/
    bank_client.rs
    lib.rs
    rpc_client.rs
    tpu_client.rs
    utils.rs
  Cargo.toml
tpu-client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    lib.rs
    tpu_client.rs
  .gitignore
  Cargo.toml
tpu-client-next/
  src/
    node_address_service/
      leader_tpu_cache_service.rs
      recent_leader_slots.rs
      slot_event.rs
      slot_receiver.rs
      slot_update_service.rs
    quic_networking/
      error.rs
    client_builder.rs
    connection_worker.rs
    connection_workers_scheduler.rs
    leader_updater.rs
    lib.rs
    logging.rs
    metrics.rs
    node_address_service.rs
    quic_networking.rs
    send_transaction_stats.rs
    transaction_batch.rs
    websocket_node_address_service.rs
    workers_cache.rs
  tests/
    connection_workers_scheduler_test.rs
  Cargo.toml
transaction-context/
  src/
    instruction_accounts.rs
    instruction.rs
    lib.rs
    transaction_accounts.rs
    vm_slice.rs
  Cargo.toml
transaction-dos/
  src/
    main.rs
  .gitignore
  Cargo.toml
transaction-metrics-tracker/
  src/
    lib.rs
  Cargo.toml
transaction-status/
  benches/
    extract_memos.rs
  src/
    parse_token/
      extension/
        confidential_mint_burn.rs
        confidential_transfer_fee.rs
        confidential_transfer.rs
        cpi_guard.rs
        default_account_state.rs
        group_member_pointer.rs
        group_pointer.rs
        interest_bearing_mint.rs
        memo_transfer.rs
        metadata_pointer.rs
        mint_close_authority.rs
        mod.rs
        pausable.rs
        permanent_delegate.rs
        reallocate.rs
        scaled_ui_amount.rs
        token_group.rs
        token_metadata.rs
        transfer_fee.rs
        transfer_hook.rs
    extract_memos.rs
    lib.rs
    parse_accounts.rs
    parse_address_lookup_table.rs
    parse_associated_token.rs
    parse_bpf_loader.rs
    parse_instruction.rs
    parse_stake.rs
    parse_system.rs
    parse_token.rs
    parse_vote.rs
    token_balances.rs
  Cargo.toml
transaction-status-client-types/
  src/
    lib.rs
    option_serializer.rs
  Cargo.toml
transaction-view/
  benches/
    bytes.rs
    transaction_view.rs
  src/
    address_table_lookup_frame.rs
    bytes.rs
    instructions_frame.rs
    lib.rs
    message_header_frame.rs
    resolved_transaction_view.rs
    result.rs
    sanitize.rs
    signature_frame.rs
    static_account_keys_frame.rs
    transaction_data.rs
    transaction_frame.rs
    transaction_version.rs
    transaction_view.rs
  Cargo.toml
turbine/
  benches/
    cluster_info.rs
    cluster_nodes.rs
  src/
    broadcast_stage/
      broadcast_duplicates_run.rs
      broadcast_fake_shreds_run.rs
      broadcast_metrics.rs
      broadcast_utils.rs
      fail_entry_verification_broadcast_run.rs
      standard_broadcast_run.rs
    addr_cache.rs
    broadcast_stage.rs
    cluster_nodes.rs
    lib.rs
    quic_endpoint.rs
    retransmit_stage.rs
    sigverify_shreds.rs
    xdp.rs
  Cargo.toml
udp-client/
  src/
    nonblocking/
      mod.rs
      udp_client.rs
    lib.rs
    udp_client.rs
  Cargo.toml
unified-scheduler-logic/
  src/
    lib.rs
  Cargo.toml
unified-scheduler-pool/
  src/
    lib.rs
    sleepless_testing.rs
  Cargo.toml
validator/
  src/
    bin/
      solana-test-validator.rs
    cli/
      thread_args.rs
    commands/
      authorized_voter/
        mod.rs
      bam/
        mod.rs
      block_engine/
        mod.rs
      contact_info/
        mod.rs
      exit/
        mod.rs
      manage_block_production/
        mod.rs
      monitor/
        mod.rs
      plugin/
        mod.rs
      relayer/
        mod.rs
      repair_shred_from_peer/
        mod.rs
      repair_whitelist/
        mod.rs
      run/
        args/
          account_secondary_indexes.rs
          blockstore_options.rs
          json_rpc_config.rs
          pub_sub_config.rs
          rpc_bigtable_config.rs
          rpc_bootstrap_config.rs
          send_transaction_config.rs
        args.rs
        execute.rs
        mod.rs
      set_identity/
        mod.rs
      set_log_filter/
        mod.rs
      set_public_address/
        mod.rs
      shred/
        mod.rs
      staked_nodes_overrides/
        mod.rs
      wait_for_restart_window/
        mod.rs
      mod.rs
    admin_rpc_service.rs
    bootstrap.rs
    cli.rs
    dashboard.rs
    lib.rs
    main.rs
  tests/
    cli.rs
  .gitignore
  Cargo.toml
  solana-test-validator
verified-packet-receiver/
  src/
    lib.rs
    receiver.rs
  Cargo.toml
  Readme.md
version/
  src/
    legacy.rs
    lib.rs
  .gitignore
  build.rs
  Cargo.toml
vortexor/
  src/
    cli.rs
    lib.rs
    main.rs
    rpc_load_balancer.rs
    sender.rs
    stake_updater.rs
    vortexor.rs
  tests/
    vortexor.rs
  Cargo.toml
  README.md
vote/
  benches/
    vote_account.rs
  src/
    vote_state_view/
      field_frames.rs
      frame_v1_14_11.rs
      frame_v3.rs
      frame_v4.rs
      list_view.rs
    lib.rs
    vote_account.rs
    vote_parser.rs
    vote_state_view.rs
    vote_transaction.rs
  Cargo.toml
votor/
  src/
    consensus_pool/
      certificate_builder.rs
      parent_ready_tracker.rs
      slot_stake_counters.rs
      stats.rs
      vote_pool.rs
    consensus_pool_service/
      stats.rs
    event_handler/
      stats.rs
    timer_manager/
      stats.rs
      timers.rs
    commitment.rs
    common.rs
    consensus_metrics.rs
    consensus_pool_service.rs
    consensus_pool.rs
    event_handler.rs
    event.rs
    lib.rs
    root_utils.rs
    staked_validators_cache.rs
    timer_manager.rs
    vote_history_storage.rs
    vote_history.rs
    voting_service.rs
    voting_utils.rs
    votor.rs
  Cargo.toml
votor-messages/
  src/
    consensus_message.rs
    lib.rs
    vote.rs
  Cargo.toml
watchtower/
  src/
    main.rs
  .gitignore
  Cargo.toml
  README.md
web3.js/
  README.md
wen-restart/
  proto/
    wen_restart.proto
  src/
    heaviest_fork_aggregate.rs
    last_voted_fork_slots_aggregate.rs
    lib.rs
    wen_restart.rs
  build.rs
  Cargo.toml
xdp/
  src/
    device.rs
    lib.rs
    netlink.rs
    packet.rs
    program.rs
    route.rs
    socket.rs
    tx_loop.rs
    umem.rs
  Cargo.toml
xdp-ebpf/
  src/
    bin/
      agave-xdp-prog.rs
    lib.rs
  agave-xdp-prog
  Cargo.toml
  README
zk-keygen/
  README.md
zk-sdk/
  README.md
zk-token-sdk/
  src/
    encryption/
      auth_encryption.rs
      decode_u32_precomputation_for_G.bincode
      discrete_log.rs
      elgamal.rs
      grouped_elgamal.rs
      mod.rs
      pedersen.rs
    instruction/
      batched_grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      batched_range_proof/
        batched_range_proof_u128.rs
        batched_range_proof_u256.rs
        batched_range_proof_u64.rs
        mod.rs
      grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      transfer/
        encryption.rs
        mod.rs
        with_fee.rs
        without_fee.rs
      ciphertext_ciphertext_equality.rs
      ciphertext_commitment_equality.rs
      errors.rs
      fee_sigma.rs
      mod.rs
      pubkey_validity.rs
      range_proof.rs
      withdraw.rs
      zero_balance.rs
    range_proof/
      errors.rs
      generators.rs
      inner_product.rs
      mod.rs
      util.rs
    sigma_proofs/
      batched_grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      ciphertext_ciphertext_equality_proof.rs
      ciphertext_commitment_equality_proof.rs
      errors.rs
      fee_proof.rs
      mod.rs
      pubkey_proof.rs
      zero_balance_proof.rs
    zk_token_elgamal/
      pod/
        auth_encryption.rs
        elgamal.rs
        grouped_elgamal.rs
        instruction.rs
        mod.rs
        pedersen.rs
        range_proof.rs
        sigma_proofs.rs
      convert.rs
      decryption.rs
      mod.rs
      ops.rs
    errors.rs
    lib.rs
    macros.rs
    transcript.rs
    zk_token_proof_instruction.rs
    zk_token_proof_program.rs
    zk_token_proof_state.rs
  .gitignore
  Cargo.toml
  README.md
.codecov.yml
.dockerignore
.gitignore
.gitmodules
.mergify.yml
bootstrap
cargo
cargo-build-sbf
cargo-test-sbf
Cargo.toml
CHANGELOG.md
clippy.toml
CONTRIBUTING.md
deploy_programs
f
fetch-core-bpf.sh
fetch-perf-libs.sh
fetch-programs.sh
fetch-spl.sh
legal.md
LICENSE
privacy.md
README.md
RELEASE.md
rust-toolchain.toml
rustfmt.toml
s
SECURITY.md
start
start_multi
vercel.json

================================================================
Files
================================================================

================
File: programs/bpf_loader/benches/bpf_loader_upgradeable.rs
================
struct TestSetup {
⋮----
impl TestSetup {
fn new() -> Self {
⋮----
let instruction_accounts = vec![AccountMeta {
⋮----
let transaction_accounts = vec![
⋮----
fn prep_initialize_buffer(&mut self) {
self.instruction_accounts.push(AccountMeta {
⋮----
bincode::serialize(&UpgradeableLoaderInstruction::InitializeBuffer).unwrap();
⋮----
fn prep_write(&mut self) {
⋮----
.set_state(&UpgradeableLoaderState::Buffer {
authority_address: Some(self.authority_address),
⋮----
.unwrap();
⋮----
bytes: vec![64; PROGRAM_BUFFER_SIZE],
⋮----
fn prep_set_authority(&mut self, checked: bool) {
⋮----
.set_state(&UpgradeableLoaderState::ProgramData {
⋮----
upgrade_authority_address: Some(self.authority_address),
⋮----
self.transaction_accounts.push((
⋮----
self.instruction_data = bincode::serialize(&instruction).unwrap();
⋮----
fn prep_close(&mut self) {
⋮----
.push(self.transaction_accounts[1].clone());
self.instruction_data = bincode::serialize(&UpgradeableLoaderInstruction::Close).unwrap();
⋮----
fn run(&self) {
mock_process_instruction(
⋮----
self.transaction_accounts.clone(),
self.instruction_accounts.clone(),
Ok(()),
⋮----
fn bench_initialize_buffer(c: &mut Criterion) {
⋮----
test_setup.prep_initialize_buffer();
c.bench_function("initialize_buffer", |bencher| {
bencher.iter(|| test_setup.run())
⋮----
fn bench_write(c: &mut Criterion) {
⋮----
test_setup.prep_write();
c.bench_function("write", |bencher| {
bencher.iter(|| {
test_setup.run();
⋮----
fn bench_set_authority(c: &mut Criterion) {
⋮----
test_setup.prep_set_authority(false);
c.bench_function("set_authority", |bencher| {
⋮----
fn bench_close(c: &mut Criterion) {
⋮----
test_setup.prep_close();
c.bench_function("close", |bencher| {
⋮----
fn bench_set_authority_checked(c: &mut Criterion) {
⋮----
test_setup.prep_set_authority(true);
c.bench_function("set_authority_checked", |bencher| {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: programs/bpf_loader/benches/serialization.rs
================
fn create_inputs(owner: Pubkey, num_instruction_accounts: usize) -> TransactionContext<'static> {
⋮----
let transaction_accounts = vec![
⋮----
.into_iter()
.cycle()
.take(num_instruction_accounts)
.enumerate()
⋮----
instruction_accounts.push(InstructionAccount::new(
⋮----
let instruction_data = vec![1u8, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11];
⋮----
.configure_next_instruction_for_tests(0, instruction_accounts, instruction_data)
.unwrap();
transaction_context.push().unwrap();
⋮----
fn bench_serialize_unaligned(c: &mut Criterion) {
let transaction_context = create_inputs(bpf_loader_deprecated::id(), 7);
⋮----
.get_current_instruction_context()
⋮----
c.bench_function("serialize_unaligned", |b| {
b.iter(|| {
let _ = serialize_parameters(
⋮----
fn bench_serialize_unaligned_copy_account_data(c: &mut Criterion) {
⋮----
c.bench_function("serialize_unaligned_copy_account_data", |b| {
⋮----
fn bench_serialize_aligned(c: &mut Criterion) {
let transaction_context = create_inputs(bpf_loader::id(), 7);
⋮----
c.bench_function("serialize_aligned", |b| {
⋮----
fn bench_serialize_aligned_copy_account_data(c: &mut Criterion) {
⋮----
c.bench_function("serialize_aligned_copy_account_data", |b| {
⋮----
fn bench_serialize_unaligned_max_accounts(c: &mut Criterion) {
let transaction_context = create_inputs(bpf_loader_deprecated::id(), 255);
⋮----
c.bench_function("serialize_unaligned_max_accounts", |b| {
⋮----
fn bench_serialize_aligned_max_accounts(c: &mut Criterion) {
let transaction_context = create_inputs(bpf_loader::id(), 255);
⋮----
c.bench_function("serialize_aligned_max_accounts", |b| {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: programs/bpf_loader/src/lib.rs
================
use qualifier_attr::qualifiers;
⋮----
thread_local! {
⋮----
fn morph_into_deployment_environment_v1<'a>(
⋮----
let mut config = from.get_config().clone();
⋮----
// Once the tests are being build using a toolchain which supports the newer SBPF versions,
// the deployment of older versions will be disabled:
// config.enabled_sbpf_versions =
//     *config.enabled_sbpf_versions.end()..=*config.enabled_sbpf_versions.end();
⋮----
for (_key, (name, value)) in from.get_function_registry().iter() {
// Deployment of programs with sol_alloc_free is disabled. So do not register the syscall.
⋮----
result.register_function(unsafe { std::str::from_utf8_unchecked(name) }, value)?;
⋮----
Ok(result)
⋮----
pub fn load_program_from_bytes(
⋮----
let effective_slot = deployment_slot.saturating_add(DELAY_VISIBILITY_SLOT_OFFSET);
⋮----
// Safety: this is safe because the program is being reloaded in the cache.
⋮----
.map_err(|err| {
ic_logger_msg!(log_collector, "{}", err);
⋮----
Ok(loaded_program)
⋮----
/// Directly deploy a program using a provided invoke context.
/// This function should only be invoked from the runtime, since it does not
⋮----
/// This function should only be invoked from the runtime, since it does not
/// provide any account loads or checks.
⋮----
/// provide any account loads or checks.
pub fn deploy_program(
⋮----
pub fn deploy_program(
⋮----
morph_into_deployment_environment_v1(program_runtime_environment.clone()).map_err(|e| {
ic_logger_msg!(log_collector, "Failed to register syscalls: {}", e);
⋮----
register_syscalls_time.stop();
load_program_metrics.register_syscalls_us = register_syscalls_time.as_us();
// Verify using stricter deployment_program_runtime_environment
⋮----
load_elf_time.stop();
load_program_metrics.load_elf_us = load_elf_time.as_us();
⋮----
executable.verify::<RequisiteVerifier>().map_err(|err| {
⋮----
verify_code_time.stop();
load_program_metrics.verify_code_us = verify_code_time.as_us();
// Reload but with program_runtime_environment
let executor = load_program_from_bytes(
⋮----
if let Some(old_entry) = program_cache_for_tx_batch.find(program_id) {
executor.tx_usage_counter.store(
old_entry.tx_usage_counter.load(Ordering::Relaxed),
⋮----
load_program_metrics.program_id = program_id.to_string();
program_cache_for_tx_batch.store_modified_entry(*program_id, Arc::new(executor));
Ok(load_program_metrics)
⋮----
macro_rules! deploy_program {
⋮----
fn write_program_data(
⋮----
let instruction_context = transaction_context.get_current_instruction_context()?;
let mut program = instruction_context.try_borrow_instruction_account(0)?;
let data = program.get_data_mut()?;
let write_offset = program_data_offset.saturating_add(bytes.len());
if data.len() < write_offset {
ic_msg!(
⋮----
return Err(InstructionError::AccountDataTooSmall);
⋮----
data.get_mut(program_data_offset..write_offset)
.ok_or(InstructionError::AccountDataTooSmall)?
.copy_from_slice(bytes);
Ok(())
⋮----
/// Only used in macro, do not use directly!
pub fn calculate_heap_cost(heap_size: u32, heap_cost: u64) -> u64 {
⋮----
pub fn calculate_heap_cost(heap_size: u32, heap_cost: u64) -> u64 {
⋮----
rounded_heap_size.saturating_add(PAGE_SIZE_KB.saturating_mul(KIBIBYTE).saturating_sub(1));
⋮----
.checked_div(PAGE_SIZE_KB.saturating_mul(KIBIBYTE))
.expect("PAGE_SIZE_KB * KIBIBYTE > 0")
.saturating_sub(1)
.saturating_mul(heap_cost)
⋮----
/// Only used in macro, do not use directly!
#[cfg_attr(feature = "svm-internal", qualifiers(pub))]
fn create_vm<'a, 'b>(
⋮----
let stack_size = stack.len();
let heap_size = heap.len();
let memory_mapping = create_memory_mapping(
⋮----
.get_feature_set()
⋮----
invoke_context.get_feature_set().account_data_direct_mapping,
⋮----
invoke_context.set_syscall_context(SyscallContext {
⋮----
Ok(EbpfVm::new(
program.get_loader().clone(),
program.get_sbpf_version(),
⋮----
macro_rules! create_vm {
⋮----
fn create_memory_mapping<'a, 'b, C: ContextObject>(
⋮----
let config = executable.get_config();
let sbpf_version = executable.get_sbpf_version();
let regions: Vec<MemoryRegion> = vec![
⋮----
.into_iter()
.chain(additional_regions)
.collect();
Ok(MemoryMapping::new_with_access_violation_handler(
⋮----
transaction_context.access_violation_handler(
⋮----
declare_builtin_function!(
⋮----
mod migration_authority {
⋮----
pub(crate) fn process_instruction_inner<'a>(
⋮----
let log_collector = invoke_context.get_log_collector();
⋮----
let program_id = instruction_context.get_program_key()?;
let owner_id = instruction_context.get_program_owner()?;
// Program Management Instruction
⋮----
invoke_context.consume_checked(UPGRADEABLE_LOADER_COMPUTE_UNITS)?;
process_loader_upgradeable_instruction(invoke_context)
⋮----
invoke_context.consume_checked(DEFAULT_LOADER_COMPUTE_UNITS)?;
ic_logger_msg!(
⋮----
Err(InstructionError::UnsupportedProgramId)
⋮----
invoke_context.consume_checked(DEPRECATED_LOADER_COMPUTE_UNITS)?;
ic_logger_msg!(log_collector, "Deprecated loader is no longer supported");
⋮----
ic_logger_msg!(log_collector, "Invalid BPF loader id");
⋮----
.map(|_| 0)
.map_err(|error| Box::new(error) as Box<dyn std::error::Error>);
⋮----
// Program Invocation
⋮----
.find(program_id)
.ok_or_else(|| {
ic_logger_msg!(log_collector, "Program is not cached");
⋮----
get_or_create_executor_time.stop();
invoke_context.timings.get_or_create_executor_us += get_or_create_executor_time.as_us();
⋮----
ic_logger_msg!(log_collector, "Program is not deployed");
Err(Box::new(InstructionError::UnsupportedProgramId) as Box<dyn std::error::Error>)
⋮----
ProgramCacheEntryType::Loaded(executable) => execute(executable, invoke_context),
_ => Err(Box::new(InstructionError::UnsupportedProgramId) as Box<dyn std::error::Error>),
⋮----
fn process_loader_upgradeable_instruction(
⋮----
let instruction_data = instruction_context.get_instruction_data();
⋮----
match limited_deserialize(instruction_data, solana_packet::PACKET_DATA_SIZE as u64)? {
⋮----
instruction_context.check_number_of_instruction_accounts(2)?;
let mut buffer = instruction_context.try_borrow_instruction_account(0)?;
if UpgradeableLoaderState::Uninitialized != buffer.get_state()? {
ic_logger_msg!(log_collector, "Buffer account already initialized");
return Err(InstructionError::AccountAlreadyInitialized);
⋮----
let authority_key = Some(*instruction_context.get_key_of_instruction_account(1)?);
buffer.set_state(&UpgradeableLoaderState::Buffer {
⋮----
let buffer = instruction_context.try_borrow_instruction_account(0)?;
if let UpgradeableLoaderState::Buffer { authority_address } = buffer.get_state()? {
if authority_address.is_none() {
ic_logger_msg!(log_collector, "Buffer is immutable");
return Err(InstructionError::Immutable); // TODO better error code
⋮----
ic_logger_msg!(log_collector, "Incorrect buffer authority provided");
return Err(InstructionError::IncorrectAuthority);
⋮----
if !instruction_context.is_instruction_account_signer(1)? {
ic_logger_msg!(log_collector, "Buffer authority did not sign");
return Err(InstructionError::MissingRequiredSignature);
⋮----
ic_logger_msg!(log_collector, "Invalid Buffer account");
return Err(InstructionError::InvalidAccountData);
⋮----
drop(buffer);
write_program_data(
UpgradeableLoaderState::size_of_buffer_metadata().saturating_add(offset as usize),
⋮----
instruction_context.check_number_of_instruction_accounts(4)?;
let payer_key = *instruction_context.get_key_of_instruction_account(0)?;
let programdata_key = *instruction_context.get_key_of_instruction_account(1)?;
⋮----
instruction_context.check_number_of_instruction_accounts(8)?;
let authority_key = Some(*instruction_context.get_key_of_instruction_account(7)?);
// Verify Program account
let program = instruction_context.try_borrow_instruction_account(2)?;
if UpgradeableLoaderState::Uninitialized != program.get_state()? {
ic_logger_msg!(log_collector, "Program account already initialized");
⋮----
if program.get_data().len() < UpgradeableLoaderState::size_of_program() {
ic_logger_msg!(log_collector, "Program account too small");
⋮----
if program.get_lamports() < rent.minimum_balance(program.get_data().len()) {
ic_logger_msg!(log_collector, "Program account not rent-exempt");
return Err(InstructionError::ExecutableAccountNotRentExempt);
⋮----
let new_program_id = *program.get_key();
drop(program);
// Verify Buffer account
let buffer = instruction_context.try_borrow_instruction_account(3)?;
⋮----
ic_logger_msg!(log_collector, "Buffer and upgrade authority don't match");
⋮----
if !instruction_context.is_instruction_account_signer(7)? {
ic_logger_msg!(log_collector, "Upgrade authority did not sign");
⋮----
return Err(InstructionError::InvalidArgument);
⋮----
let buffer_key = *buffer.get_key();
⋮----
let buffer_data_len = buffer.get_data().len().saturating_sub(buffer_data_offset);
⋮----
if buffer.get_data().len() < UpgradeableLoaderState::size_of_buffer_metadata()
⋮----
ic_logger_msg!(log_collector, "Buffer account too small");
⋮----
ic_logger_msg!(log_collector, "Max data length is too large");
⋮----
Pubkey::find_program_address(&[new_program_id.as_ref()], program_id);
⋮----
ic_logger_msg!(log_collector, "ProgramData address is not derived");
⋮----
let mut buffer = instruction_context.try_borrow_instruction_account(3)?;
let mut payer = instruction_context.try_borrow_instruction_account(0)?;
payer.checked_add_lamports(buffer.get_lamports())?;
buffer.set_lamports(0)?;
⋮----
1.max(rent.minimum_balance(programdata_len)),
⋮----
.push(AccountMeta::new(buffer_key, false));
⋮----
let caller_program_id = instruction_context.get_program_key()?;
let signers = [[new_program_id.as_ref(), &[bump_seed]]]
.iter()
.map(|seeds| Pubkey::create_program_address(seeds, caller_program_id))
⋮----
.map_err(|e| e as u64)?;
invoke_context.native_invoke(instruction, signers.as_slice())?;
⋮----
deploy_program!(
⋮----
let mut programdata = instruction_context.try_borrow_instruction_account(1)?;
programdata.set_state(&UpgradeableLoaderState::ProgramData {
⋮----
.get_data_mut()?
.get_mut(
⋮----
..programdata_data_offset.saturating_add(buffer_data_len),
⋮----
.ok_or(InstructionError::AccountDataTooSmall)?;
⋮----
.get_data()
.get(buffer_data_offset..)
⋮----
dst_slice.copy_from_slice(src_slice);
buffer.set_data_length(UpgradeableLoaderState::size_of_buffer(0))?;
⋮----
let mut program = instruction_context.try_borrow_instruction_account(2)?;
program.set_state(&UpgradeableLoaderState::Program {
⋮----
program.set_executable(true)?;
⋮----
ic_logger_msg!(log_collector, "Deployed program {:?}", new_program_id);
⋮----
instruction_context.check_number_of_instruction_accounts(3)?;
let programdata_key = *instruction_context.get_key_of_instruction_account(0)?;
⋮----
instruction_context.check_number_of_instruction_accounts(7)?;
let authority_key = Some(*instruction_context.get_key_of_instruction_account(6)?);
let program = instruction_context.try_borrow_instruction_account(1)?;
if !program.is_writable() {
ic_logger_msg!(log_collector, "Program account not writeable");
⋮----
if program.get_owner() != program_id {
ic_logger_msg!(log_collector, "Program account not owned by loader");
return Err(InstructionError::IncorrectProgramId);
⋮----
} = program.get_state()?
⋮----
ic_logger_msg!(log_collector, "Program and ProgramData account mismatch");
⋮----
ic_logger_msg!(log_collector, "Invalid Program account");
⋮----
let buffer = instruction_context.try_borrow_instruction_account(2)?;
⋮----
if !instruction_context.is_instruction_account_signer(6)? {
⋮----
let buffer_lamports = buffer.get_lamports();
⋮----
let programdata = instruction_context.try_borrow_instruction_account(0)?;
⋮----
1.max(rent.minimum_balance(programdata.get_data().len()));
if programdata.get_data().len()
⋮----
ic_logger_msg!(log_collector, "ProgramData account not large enough");
⋮----
if programdata.get_lamports().saturating_add(buffer_lamports)
⋮----
return Err(InstructionError::InsufficientFunds);
⋮----
} = programdata.get_state()?
⋮----
ic_logger_msg!(log_collector, "Program was deployed in this block already");
⋮----
if upgrade_authority_address.is_none() {
ic_logger_msg!(log_collector, "Program not upgradeable");
return Err(InstructionError::Immutable);
⋮----
ic_logger_msg!(log_collector, "Incorrect upgrade authority provided");
⋮----
ic_logger_msg!(log_collector, "Invalid ProgramData account");
⋮----
let programdata_len = programdata.get_data().len();
drop(programdata);
⋮----
let mut programdata = instruction_context.try_borrow_instruction_account(0)?;
⋮----
.get_mut(programdata_data_offset.saturating_add(buffer_data_len)..)
⋮----
.fill(0);
let mut buffer = instruction_context.try_borrow_instruction_account(2)?;
let mut spill = instruction_context.try_borrow_instruction_account(3)?;
spill.checked_add_lamports(
⋮----
.get_lamports()
.saturating_add(buffer_lamports)
.saturating_sub(programdata_balance_required),
⋮----
programdata.set_lamports(programdata_balance_required)?;
⋮----
ic_logger_msg!(log_collector, "Upgraded program {:?}", new_program_id);
⋮----
let mut account = instruction_context.try_borrow_instruction_account(0)?;
let present_authority_key = instruction_context.get_key_of_instruction_account(1)?;
let new_authority = instruction_context.get_key_of_instruction_account(2).ok();
match account.get_state()? {
⋮----
if new_authority.is_none() {
ic_logger_msg!(log_collector, "Buffer authority is not optional");
⋮----
if authority_address != Some(*present_authority_key) {
⋮----
account.set_state(&UpgradeableLoaderState::Buffer {
authority_address: new_authority.cloned(),
⋮----
if upgrade_authority_address != Some(*present_authority_key) {
⋮----
account.set_state(&UpgradeableLoaderState::ProgramData {
⋮----
upgrade_authority_address: new_authority.cloned(),
⋮----
ic_logger_msg!(log_collector, "Account does not support authorities");
⋮----
ic_logger_msg!(log_collector, "New authority {:?}", new_authority);
⋮----
return Err(InstructionError::InvalidInstructionData);
⋮----
let new_authority_key = instruction_context.get_key_of_instruction_account(2)?;
⋮----
if !instruction_context.is_instruction_account_signer(2)? {
ic_logger_msg!(log_collector, "New authority did not sign");
⋮----
authority_address: Some(*new_authority_key),
⋮----
upgrade_authority_address: Some(*new_authority_key),
⋮----
ic_logger_msg!(log_collector, "New authority {:?}", new_authority_key);
⋮----
if instruction_context.get_index_of_instruction_account_in_transaction(0)?
== instruction_context.get_index_of_instruction_account_in_transaction(1)?
⋮----
let mut close_account = instruction_context.try_borrow_instruction_account(0)?;
let close_key = *close_account.get_key();
let close_account_state = close_account.get_state()?;
close_account.set_data_length(UpgradeableLoaderState::size_of_uninitialized())?;
⋮----
instruction_context.try_borrow_instruction_account(1)?;
recipient_account.checked_add_lamports(close_account.get_lamports())?;
close_account.set_lamports(0)?;
ic_logger_msg!(log_collector, "Closed Uninitialized {}", close_key);
⋮----
drop(close_account);
common_close_account(&authority_address, &instruction_context, &log_collector)?;
ic_logger_msg!(log_collector, "Closed Buffer {}", close_key);
⋮----
let program_account = instruction_context.try_borrow_instruction_account(3)?;
let program_key = *program_account.get_key();
if !program_account.is_writable() {
ic_logger_msg!(log_collector, "Program account is not writable");
⋮----
if program_account.get_owner() != program_id {
⋮----
let clock = invoke_context.get_sysvar_cache().get_clock()?;
⋮----
match program_account.get_state()? {
⋮----
drop(program_account);
common_close_account(
⋮----
.store_modified_entry(
⋮----
ic_logger_msg!(log_collector, "Closed Program {}", program_key);
⋮----
ic_logger_msg!(log_collector, "Account does not support closing");
⋮----
common_extend_program(invoke_context, additional_bytes, false)?;
⋮----
common_extend_program(invoke_context, additional_bytes, true)?;
⋮----
if !invoke_context.get_feature_set().enable_loader_v4 {
⋮----
let programdata_address = *instruction_context.get_key_of_instruction_account(0)?;
let program_address = *instruction_context.get_key_of_instruction_account(1)?;
⋮----
*instruction_context.get_key_of_instruction_account(2)?;
⋮----
.get_sysvar_cache()
.get_clock()
.map(|clock| clock.slot)?;
⋮----
if !programdata.is_writable() {
ic_logger_msg!(log_collector, "ProgramData account not writeable");
⋮----
}) = programdata.get_state()
⋮----
.len()
.saturating_sub(UpgradeableLoaderState::size_of_programdata_metadata()),
⋮----
let programdata_funds = programdata.get_lamports();
⋮----
!= upgrade_authority_address.unwrap_or(program_address)
⋮----
ic_logger_msg!(log_collector, "Incorrect migration authority provided");
⋮----
ic_logger_msg!(log_collector, "Migration authority did not sign");
⋮----
let mut program = instruction_context.try_borrow_instruction_account(1)?;
⋮----
program.set_data_from_slice(&[])?;
program.checked_add_lamports(programdata_funds)?;
program.set_owner(&loader_v4::id().to_bytes())?;
⋮----
programdata.set_lamports(0)?;
⋮----
invoke_context.native_invoke(
⋮----
&upgrade_authority_address.unwrap(),
⋮----
programdata.set_data_from_slice(&[])?;
⋮----
ic_logger_msg!(log_collector, "Migrated program {:?}", &program_address);
⋮----
fn common_extend_program(
⋮----
ic_logger_msg!(log_collector, "Additional bytes must be greater than 0");
⋮----
instruction_context.try_borrow_instruction_account(PROGRAM_DATA_ACCOUNT_INDEX)?;
let programdata_key = *programdata_account.get_key();
if program_id != programdata_account.get_owner() {
ic_logger_msg!(log_collector, "ProgramData owner is invalid");
return Err(InstructionError::InvalidAccountOwner);
⋮----
if !programdata_account.is_writable() {
ic_logger_msg!(log_collector, "ProgramData is not writable");
⋮----
instruction_context.try_borrow_instruction_account(PROGRAM_ACCOUNT_INDEX)?;
⋮----
let old_len = programdata_account.get_data().len();
let new_len = old_len.saturating_add(additional_bytes as usize);
⋮----
return Err(InstructionError::InvalidRealloc);
⋮----
} = programdata_account.get_state()?
⋮----
ic_logger_msg!(log_collector, "Program was extended in this block already");
⋮----
Some(*instruction_context.get_key_of_instruction_account(AUTHORITY_ACCOUNT_INDEX)?);
⋮----
if !instruction_context.is_instruction_account_signer(AUTHORITY_ACCOUNT_INDEX)? {
⋮----
ic_logger_msg!(log_collector, "ProgramData state is invalid");
⋮----
let balance = programdata_account.get_lamports();
let rent = invoke_context.get_sysvar_cache().get_rent()?;
let min_balance = rent.minimum_balance(new_len).max(1);
min_balance.saturating_sub(balance)
⋮----
drop(programdata_account);
⋮----
*instruction_context.get_key_of_instruction_account(optional_payer_account_index)?;
⋮----
programdata_account.set_data_length(new_len)?;
⋮----
programdata_account.set_state(&UpgradeableLoaderState::ProgramData {
⋮----
fn common_close_account(
⋮----
ic_logger_msg!(log_collector, "Account is immutable");
⋮----
if *authority_address != Some(*instruction_context.get_key_of_instruction_account(2)?) {
ic_logger_msg!(log_collector, "Incorrect authority provided");
⋮----
ic_logger_msg!(log_collector, "Authority did not sign");
⋮----
let mut recipient_account = instruction_context.try_borrow_instruction_account(1)?;
⋮----
close_account.set_state(&UpgradeableLoaderState::Uninitialized)?;
⋮----
fn execute<'a, 'b: 'a>(
⋮----
// We dropped the lifetime tracking in the Executor by setting it to 'static,
⋮----
let program_id = *instruction_context.get_program_key()?;
⋮----
instruction_context.get_program_owner()? == bpf_loader_deprecated::id();
⋮----
let use_jit = executable.get_compiled_program().is_some();
⋮----
let account_data_direct_mapping = invoke_context.get_feature_set().account_data_direct_mapping;
⋮----
serialize_time.stop();
⋮----
.map(|m| {
⋮----
.saturating_add(m.original_data_len as u64)
.saturating_add(if !is_loader_deprecated {
⋮----
let compute_meter_prev = invoke_context.get_remaining();
create_vm!(vm, executable, regions, accounts_metadata, invoke_context);
⋮----
ic_logger_msg!(log_collector, "Failed to create SBF VM: {}", e);
return Err(Box::new(InstructionError::ProgramEnvironmentSetupFailure));
⋮----
create_vm_time.stop();
vm.context_object_pointer.execute_time = Some(Measure::start("execute"));
⋮----
let (compute_units_consumed, result) = vm.execute_program(executable, !use_jit);
⋮----
MEMORY_POOL.with_borrow_mut(|memory_pool| {
memory_pool.put_stack(stack);
memory_pool.put_heap(heap);
debug_assert!(memory_pool.stack_len() <= MAX_INSTRUCTION_STACK_DEPTH);
debug_assert!(memory_pool.heap_len() <= MAX_INSTRUCTION_STACK_DEPTH);
⋮----
drop(vm);
invoke_context.insert_register_trace(register_trace);
if let Some(execute_time) = invoke_context.execute_time.as_mut() {
execute_time.stop();
invoke_context.timings.execute_us += execute_time.as_us();
⋮----
invoke_context.transaction_context.get_return_data();
if !return_data.is_empty() {
⋮----
let error: InstructionError = status.into();
Err(Box::new(error) as Box<dyn std::error::Error>)
⋮----
&& !matches!(error, EbpfError::SyscallError(_))
⋮----
invoke_context.consume(invoke_context.get_remaining());
⋮----
.map(|err| *err)
.unwrap_or_else(EbpfError::SyscallError);
⋮----
.enumerate()
.find(|(_, vm_addr_range)| vm_addr_range.contains(&vm_addr))
⋮----
transaction_context.get_current_instruction_context()?;
let account = instruction_context.try_borrow_instruction_account(
⋮----
if vm_addr.saturating_add(len) <= vm_addr_range.end {
⋮----
.saturating_add(len)
.saturating_sub(vm_addr_range.start)
⋮----
> account.get_data().len();
⋮----
if let Err(err) = account.can_data_be_changed() {
⋮----
debug_assert!(is_access_outside_of_data);
⋮----
if account.can_data_be_changed().is_err() {
⋮----
Err(if let EbpfError::SyscallError(err) = error {
⋮----
error.into()
⋮----
_ => Ok(()),
⋮----
fn deserialize_parameters(
⋮----
.get_current_instruction_context()?,
⋮----
&invoke_context.get_syscall_context()?.accounts_metadata,
⋮----
let execute_or_deserialize_result = execution_result.and_then(|_| {
deserialize_parameters(
⋮----
parameter_bytes.as_slice(),
⋮----
.map_err(|error| Box::new(error) as Box<dyn std::error::Error>)
⋮----
deserialize_time.stop();
invoke_context.timings.serialize_us += serialize_time.as_us();
invoke_context.timings.create_vm_us += create_vm_time.as_us();
invoke_context.timings.deserialize_us += deserialize_time.as_us();
⋮----
mod test_utils {
⋮----
fn check_loader_id(id: &Pubkey) -> bool {
⋮----
fn load_all_invoked_programs(invoke_context: &mut InvokeContext) {
⋮----
let program_runtime_environment = create_program_runtime_environment_v1(
invoke_context.get_feature_set(),
invoke_context.get_compute_budget(),
⋮----
let program_runtime_environment = Arc::new(program_runtime_environment.unwrap());
let num_accounts = invoke_context.transaction_context.get_number_of_accounts();
⋮----
.accounts()
.try_borrow(index)
.expect("Failed to get the account");
let owner = account.owner();
if check_loader_id(owner) {
⋮----
.get_key_of_account_at_index(index)
.expect("Failed to get account key");
if let Ok(loaded_program) = load_program_from_bytes(
⋮----
.data()
.get(programdata_data_offset.min(account.data().len())..)
.unwrap(),
⋮----
account.data().len(),
⋮----
program_runtime_environment.clone(),
⋮----
.store_modified_entry(*pubkey, Arc::new(loaded_program));
⋮----
mod tests {
⋮----
fn process_instruction(
⋮----
mock_process_instruction(
⋮----
fn load_program_account_from_elf(loader_id: &Pubkey, path: &str) -> AccountSharedData {
let mut file = File::open(path).expect("file open failed");
⋮----
file.read_to_end(&mut elf).unwrap();
⋮----
AccountSharedData::new(rent.minimum_balance(elf.len()), 0, loader_id);
program_account.set_data(elf);
program_account.set_executable(true);
⋮----
fn test_bpf_loader_invoke_main() {
⋮----
load_program_account_from_elf(&loader_id, "test_elfs/out/sbpfv3_return_ok.so");
⋮----
process_instruction(
⋮----
Err(InstructionError::UnsupportedProgramId),
⋮----
Some(0),
⋮----
vec![(program_id, program_account.clone())],
⋮----
Ok(()),
⋮----
vec![
⋮----
vec![parameter_meta.clone()],
⋮----
vec![parameter_meta.clone(), parameter_meta],
⋮----
vec![(program_id, program_account)],
⋮----
Err(InstructionError::ProgramFailedToComplete),
⋮----
invoke_context.mock_set_remaining(0);
⋮----
vec![(program_id, parameter_account.clone())],
⋮----
vec![(program_id, parameter_account)],
⋮----
fn test_bpf_loader_serialize_unaligned() {
⋮----
load_program_account_from_elf(&loader_id, "test_elfs/out/noop_unaligned.so");
⋮----
fn test_bpf_loader_serialize_aligned() {
⋮----
load_program_account_from_elf(&loader_id, "test_elfs/out/noop_aligned.so");
⋮----
fn test_bpf_loader_upgradeable_initialize_buffer() {
⋮----
bincode::serialize(&UpgradeableLoaderInstruction::InitializeBuffer).unwrap();
let instruction_accounts = vec![
⋮----
let accounts = process_instruction(
⋮----
instruction_accounts.clone(),
⋮----
let state: UpgradeableLoaderState = accounts.first().unwrap().state().unwrap();
assert_eq!(
⋮----
Err(InstructionError::AccountAlreadyInitialized),
⋮----
fn test_bpf_loader_upgradeable_write() {
⋮----
bytes: vec![42; 9],
⋮----
.unwrap();
⋮----
vec![(buffer_address, buffer_account.clone())],
⋮----
Err(InstructionError::InvalidAccountData),
⋮----
.set_state(&UpgradeableLoaderState::Buffer {
authority_address: Some(buffer_address),
⋮----
bytes: vec![42; 6],
⋮----
bytes: vec![42; 10],
⋮----
Err(InstructionError::AccountDataTooSmall),
⋮----
Err(InstructionError::MissingRequiredSignature),
⋮----
Err(InstructionError::IncorrectAuthority),
⋮----
Err(InstructionError::Immutable),
⋮----
fn truncate_data(account: &mut AccountSharedData, len: usize) {
let mut data = account.data().to_vec();
data.truncate(len);
account.set_data(data);
⋮----
fn test_bpf_loader_upgradeable_upgrade() {
let mut file = File::open("test_elfs/out/sbpfv3_return_ok.so").expect("file open failed");
⋮----
file.read_to_end(&mut elf_orig).unwrap();
let mut file = File::open("test_elfs/out/sbpfv3_return_err.so").expect("file open failed");
⋮----
file.read_to_end(&mut elf_new).unwrap();
assert_ne!(elf_orig.len(), elf_new.len());
⋮----
fn get_accounts(
⋮----
1.max(rent.minimum_balance(UpgradeableLoaderState::size_of_program()));
let min_programdata_balance = 1.max(rent.minimum_balance(
UpgradeableLoaderState::size_of_programdata(elf_orig.len().max(elf_new.len())),
⋮----
Pubkey::find_program_address(&[program_address.as_ref()], &loader_id);
⋮----
UpgradeableLoaderState::size_of_buffer(elf_new.len()),
⋮----
authority_address: Some(*buffer_authority),
⋮----
.data_as_mut_slice()
.get_mut(UpgradeableLoaderState::size_of_buffer_metadata()..)
.unwrap()
.copy_from_slice(elf_new);
⋮----
.set_state(&UpgradeableLoaderState::ProgramData {
⋮----
upgrade_authority_address: Some(*upgrade_authority_address),
⋮----
.set_state(&UpgradeableLoaderState::Program {
⋮----
let rent_account = create_account_for_test(&rent);
let clock_account = create_account_for_test(&Clock {
slot: SLOT.saturating_add(1),
⋮----
let transaction_accounts = vec![
⋮----
bincode::serialize(&UpgradeableLoaderInstruction::Upgrade).unwrap();
⋮----
let (transaction_accounts, instruction_accounts) = get_accounts(
⋮----
let accounts = process_instruction(transaction_accounts, instruction_accounts, Ok(()));
let min_programdata_balance = Rent::default().minimum_balance(
⋮----
assert_eq!(0, accounts.get(2).unwrap().lamports());
assert_eq!(1, accounts.get(3).unwrap().lamports());
⋮----
.first()
⋮----
.get(
⋮----
..UpgradeableLoaderState::size_of_programdata(elf_new.len()),
⋮----
assert_eq!(*elf_new.get(i).unwrap(), *byte);
⋮----
let (mut transaction_accounts, instruction_accounts) = get_accounts(
⋮----
.get_mut(0)
⋮----
let (mut transaction_accounts, mut instruction_accounts) = get_accounts(
⋮----
transaction_accounts.get_mut(6).unwrap().0 = invalid_upgrade_authority_address;
instruction_accounts.get_mut(6).unwrap().pubkey = invalid_upgrade_authority_address;
⋮----
let (transaction_accounts, mut instruction_accounts) = get_accounts(
⋮----
instruction_accounts.get_mut(6).unwrap().is_signer = false;
⋮----
*instruction_accounts.get_mut(3).unwrap() = instruction_accounts.get(2).unwrap().clone();
⋮----
Err(InstructionError::AccountBorrowFailed),
⋮----
*instruction_accounts.get_mut(3).unwrap() = instruction_accounts.first().unwrap().clone();
⋮----
*instruction_accounts.get_mut(1).unwrap() = instruction_accounts.get(2).unwrap().clone();
let instruction_data = bincode::serialize(&UpgradeableLoaderInstruction::Upgrade).unwrap();
⋮----
transaction_accounts.clone(),
⋮----
.get_mut(1)
⋮----
.set_owner(Pubkey::new_unique());
⋮----
Err(InstructionError::IncorrectProgramId),
⋮----
instruction_accounts.get_mut(1).unwrap().is_writable = false;
⋮----
Err(InstructionError::InvalidArgument),
⋮----
.set_state(&UpgradeableLoaderState::Uninitialized)
⋮----
transaction_accounts.get_mut(0).unwrap().0 = invalid_programdata_address;
instruction_accounts.get_mut(0).unwrap().pubkey = invalid_programdata_address;
⋮----
.get_mut(2)
⋮----
transaction_accounts.get_mut(2).unwrap().1 = AccountSharedData::new(
⋮----
elf_orig.len().max(elf_new.len()).saturating_add(1),
⋮----
authority_address: Some(upgrade_authority_address),
⋮----
truncate_data(&mut transaction_accounts.get_mut(2).unwrap().1, 5);
⋮----
fn test_bpf_loader_upgradeable_set_upgrade_authority() {
let instruction = bincode::serialize(&UpgradeableLoaderInstruction::SetAuthority).unwrap();
⋮----
&[program_address.as_ref()],
⋮----
upgrade_authority_address: Some(upgrade_authority_address),
⋮----
vec![programdata_meta.clone(), upgrade_authority_meta.clone()],
⋮----
vec![programdata_meta, upgrade_authority_meta],
⋮----
fn test_bpf_loader_upgradeable_set_upgrade_authority_checked() {
⋮----
bincode::serialize(&UpgradeableLoaderInstruction::SetAuthorityChecked).unwrap();
⋮----
vec![programdata_meta.clone(), new_upgrade_authority_meta.clone()],
Err(InstructionError::MissingAccount),
⋮----
fn test_bpf_loader_upgradeable_set_buffer_authority() {
⋮----
authority_address: Some(authority_address),
⋮----
let mut transaction_accounts = vec![
⋮----
vec![buffer_meta.clone(), authority_meta.clone()],
⋮----
vec![buffer_meta, authority_meta, new_authority_meta],
⋮----
fn test_bpf_loader_upgradeable_set_buffer_authority_checked() {
⋮----
vec![buffer_meta.clone(), new_authority_meta.clone()],
⋮----
fn test_bpf_loader_upgradeable_close() {
let instruction = bincode::serialize(&UpgradeableLoaderInstruction::Close).unwrap();
⋮----
upgrade_authority_address: Some(authority_address),
⋮----
assert_eq!(0, accounts.first().unwrap().lamports());
assert_eq!(2, accounts.get(1).unwrap().lamports());
⋮----
assert_eq!(state, UpgradeableLoaderState::Uninitialized);
⋮----
programdata_account = accounts.first().unwrap().clone();
program_account = accounts.get(3).unwrap().clone();
⋮----
Some(1),
⋮----
fn fuzz<F>(
⋮----
let mut mangled_bytes = bytes.to_vec();
⋮----
let offset = rng.random_range(offset.start..offset.end);
let value = rng.random_range(value.start..value.end);
*mangled_bytes.get_mut(offset).unwrap() = value;
work(&mut mangled_bytes);
⋮----
fn test_fuzz() {
⋮----
fuzz(
⋮----
0..elf.len(),
⋮----
program_account.set_data(bytes.to_vec());
⋮----
fn test_calculate_heap_cost() {
⋮----
assert_eq!(0, calculate_heap_cost(31 * 1024, heap_cost));
assert_eq!(0, calculate_heap_cost(32 * 1024, heap_cost));
assert_eq!(heap_cost, calculate_heap_cost(33 * 1024, heap_cost));
assert_eq!(heap_cost, calculate_heap_cost(64 * 1024, heap_cost));
⋮----
fn deploy_test_program(
⋮----
fn test_program_usage_count_on_upgrade() {
let transaction_accounts = vec![(
⋮----
with_mock_invoke_context!(invoke_context, transaction_context, transaction_accounts);
⋮----
.replenish(program_id, Arc::new(program));
⋮----
.set_slot_for_tests(2);
assert_matches!(
⋮----
.find(&program_id)
.expect("Didn't find upgraded program in the cache");
assert_eq!(updated_program.deployment_slot, 2);
⋮----
fn test_program_usage_count_on_non_upgrade() {
⋮----
.find(&program_id2)
⋮----
assert_eq!(program2.deployment_slot, 2);
assert_eq!(program2.tx_usage_counter.load(Ordering::Relaxed), 0);

================
File: programs/bpf_loader/test_elfs/src/noop_aligned/noop_aligned.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/bpf_loader/test_elfs/src/noop_unaligned/noop_unaligned.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/bpf_loader/test_elfs/makefile
================
SBF_SDK := ../../../platform-tools-sdk/sbf/c
include $(SBF_SDK)/sbf.mk

================
File: programs/bpf_loader/Cargo.toml
================
[package]
name = "solana-bpf-loader-program"
description = "Solana BPF loader"
documentation = "https://docs.rs/solana-bpf-loader-program"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_bpf_loader_program"

[features]
default = ["metrics"]
agave-unstable-api = []
metrics = ["solana-program-runtime/metrics"]
shuttle-test = [
    "solana-program-runtime/shuttle-test",
    "solana-sbpf/shuttle-test",
    "solana-svm-type-overrides/shuttle-test",
]
svm-internal = []

[dependencies]
agave-syscalls = { workspace = true }
bincode = { workspace = true }
qualifier_attr = { workspace = true }
solana-account = { workspace = true }
solana-bincode = { workspace = true }
solana-clock = { workspace = true }
solana-instruction = { workspace = true }
solana-loader-v3-interface = { workspace = true, features = ["serde"] }
solana-loader-v4-interface = { workspace = true, features = ["bincode"] }
solana-packet = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbpf = { workspace = true, features = ["jit"] }
solana-sdk-ids = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-measure = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction-context = { workspace = true, features = ["bincode"] }

[dev-dependencies]
assert_matches = { workspace = true }
criterion = { workspace = true }
rand = { workspace = true }
solana-bpf-loader-program = { path = ".", features = ["agave-unstable-api", "svm-internal"] }
solana-epoch-rewards = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-last-restart-slot = { workspace = true }
solana-program = { workspace = true }
solana-program-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-rent = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-svm-callback = { workspace = true }
solana-transaction-context = { workspace = true, features = ["dev-context-only-utils"] }
static_assertions = { workspace = true }
test-case = { workspace = true }

[[bench]]
name = "serialization"
harness = false

[[bench]]
name = "bpf_loader_upgradeable"
harness = false

================
File: programs/bpf-loader-tests/tests/common.rs
================
pub async fn setup_test_context() -> ProgramTestContext {
let program_test = ProgramTest::new("", id(), Some(solana_bpf_loader_program::Entrypoint::vm));
program_test.start_with_context().await
⋮----
pub async fn assert_ix_error(
⋮----
let mut signers = vec![fee_payer];
⋮----
signers.push(additional_payer);
⋮----
Some(&fee_payer.pubkey()),
⋮----
assert_eq!(
⋮----
pub async fn add_upgradeable_loader_account(
⋮----
let rent = context.banks_client.get_rent().await.unwrap();
⋮----
rent.minimum_balance(account_data_len),
⋮----
&id(),
⋮----
.set_state(account_state)
.expect("state failed to serialize into account data");
account_callback(&mut account);
context.set_account(account_address, &account);

================
File: programs/bpf-loader-tests/tests/extend_program_ix.rs
================
mod common;
⋮----
async fn test_extend_program() {
let mut context = setup_test_context().await;
let program_file = find_file("noop.so").expect("Failed to find the file");
let data = read_file(program_file);
⋮----
let (programdata_address, _) = Pubkey::find_program_address(&[program_address.as_ref()], &id());
add_upgradeable_loader_account(
⋮----
let program_data_len = data.len() + programdata_data_offset;
⋮----
upgrade_authority_address: Some(upgrade_authority.pubkey()),
⋮----
|account| account.data_as_mut_slice()[programdata_data_offset..].copy_from_slice(&data),
⋮----
&[extend_program_checked(
⋮----
&upgrade_authority.pubkey(),
Some(&payer.pubkey()),
⋮----
assert_matches!(client.process_transaction(transaction).await, Ok(()));
⋮----
.get_account(programdata_address)
⋮----
.unwrap()
.unwrap();
assert_eq!(
⋮----
async fn test_failed_extend_twice_in_same_slot() {
⋮----
.get_new_latest_blockhash(&recent_blockhash)
⋮----
assert_matches!(
⋮----
async fn test_failed_extend_upgrade_authority_did_not_sign() {
⋮----
&payer.pubkey(),
⋮----
let mut ix = extend_program_checked(
⋮----
async fn test_extend_program_not_upgradeable() {
⋮----
let payer_address = context.payer.pubkey();
assert_ix_error(
⋮----
extend_program_checked(&program_address, &payer_address, Some(&payer_address), 42),
⋮----
async fn test_extend_program_by_zero_bytes() {
⋮----
extend_program_checked(
⋮----
Some(&payer_address),
⋮----
Some(&upgrade_authority),
⋮----
async fn test_extend_program_past_max_size() {
⋮----
async fn test_extend_program_with_invalid_payer() {
⋮----
let rent = context.banks_client.get_rent().await.unwrap();
let upgrade_authority_address = context.payer.pubkey();
⋮----
upgrade_authority_address: Some(upgrade_authority_address),
⋮----
context.set_account(
&payer_with_sufficient_funds.pubkey(),
⋮----
&payer_with_insufficient_funds.pubkey(),
&AccountSharedData::new(rent.minimum_balance(0), 0, &system_program::id()),
⋮----
&payer_with_invalid_owner.pubkey(),
&AccountSharedData::new(rent.minimum_balance(0), 0, &id()),
⋮----
Some(&payer_with_insufficient_funds.pubkey()),
⋮----
Some(&payer_with_insufficient_funds),
⋮----
Some(&payer_with_invalid_owner.pubkey()),
⋮----
Some(&payer_with_invalid_owner),
⋮----
Some(&payer_with_sufficient_funds.pubkey()),
⋮----
.iter_mut()
.find(|meta| meta.pubkey == payer_with_sufficient_funds.pubkey())
.expect("expected to find payer account meta");
⋮----
async fn test_extend_program_without_payer() {
⋮----
extend_program_checked(&program_address, &upgrade_authority.pubkey(), None, 1024),
⋮----
.minimum_balance(ADDITIONAL_BYTES as usize)
.saturating_sub(rent.minimum_balance(0));
⋮----
async fn test_extend_program_with_invalid_system_program() {
⋮----
.find(|meta| meta.pubkey == crate::system_program::ID)
.expect("expected to find system program account meta");
⋮----
async fn test_extend_program_with_mismatch_program_data() {
⋮----
.find(|meta| meta.pubkey == programdata_address)
.expect("expected to find program data account meta");
⋮----
async fn test_extend_program_with_readonly_program_data() {
⋮----
async fn test_extend_program_with_invalid_program_data_state() {
⋮----
authority_address: Some(payer_address),
⋮----
async fn test_extend_program_with_invalid_program_data_owner() {
⋮----
upgrade_authority_address: Some(payer_address),
⋮----
|account| account.set_owner(invalid_owner),
⋮----
extend_program_checked(&program_address, &payer_address, Some(&payer_address), 1024),
⋮----
async fn test_extend_program_with_readonly_program() {
⋮----
.find(|meta| meta.pubkey == program_address)
.expect("expected to find program account meta");
⋮----
async fn test_extend_program_with_invalid_program_owner() {
⋮----
async fn test_extend_program_with_invalid_program_state() {

================
File: programs/bpf-loader-tests/Cargo.toml
================
# This package only exists to avoid circular dependencies during cargo publish:
# solana-bpf-loader-program -> solana-program-test -> solana-bpf-loader-program

[package]
name = "solana-bpf-loader-program-tests"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dev-dependencies]
assert_matches = { workspace = true }
bincode = { workspace = true }
solana-account = { workspace = true }
solana-bpf-loader-program = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-loader-v3-interface = { workspace = true }
solana-program-test = { workspace = true }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }

================
File: programs/compute-budget/src/lib.rs
================
use solana_program_runtime::declare_process_instruction;
⋮----
declare_process_instruction!(Entrypoint, DEFAULT_COMPUTE_UNITS, |_invoke_context| {

================
File: programs/compute-budget/Cargo.toml
================
[package]
name = "solana-compute-budget-program"
description = "Solana Compute Budget program"
documentation = "https://docs.rs/solana-compute-budget-program"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_compute_budget_program"

[features]
agave-unstable-api = []

[dependencies]
solana-program-runtime = { workspace = true }

================
File: programs/compute-budget-bench/benches/compute_budget.rs
================
fn bench_request_heap_frame(c: &mut Criterion) {
⋮----
vec![],
⋮----
c.bench_function("request_heap_limit", |bencher| {
bencher.iter(|| {
assert_eq!(
⋮----
fn bench_set_compute_unit_limit(c: &mut Criterion) {
⋮----
c.bench_function("set_compute_unit_limit", |bencher| {
⋮----
fn bench_set_compute_unit_price(c: &mut Criterion) {
⋮----
c.bench_function("set_compute_unit_price", |bencher| {
⋮----
fn bench_set_loaded_accounts_data_size_limit(c: &mut Criterion) {
⋮----
c.bench_function("set_loaded_accounts_data_size_limit", |bencher| {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: programs/compute-budget-bench/Cargo.toml
================
[package]
name = "solana-compute-budget-program-bench"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
criterion = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-instruction = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-compute-budget-program = { workspace = true }
solana-message = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-svm-transaction = { workspace = true }

[[bench]]
name = "compute_budget"
harness = false

================
File: programs/ed25519-tests/tests/process_transaction.rs
================
fn generate_keypair() -> ed25519_dalek::Keypair {
use rand::RngCore;
⋮----
rng.fill_bytes(&mut seed);
⋮----
ed25519_dalek::SecretKey::from_bytes(&seed[..ed25519_dalek::SECRET_KEY_LENGTH]).unwrap();
⋮----
async fn test_success() {
let mut context = ProgramTest::default().start_with_context().await;
⋮----
let privkey = generate_keypair();
⋮----
let signature = privkey.sign(message_arr).to_bytes();
let pubkey = privkey.public.to_bytes();
let instruction = new_ed25519_instruction_with_signature(message_arr, &signature, &pubkey);
⋮----
Some(&payer.pubkey()),
⋮----
assert_matches!(client.process_transaction(transaction).await, Ok(()));
⋮----
async fn test_failure() {
⋮----
let mut instruction = new_ed25519_instruction_with_signature(message_arr, &signature, &pubkey);
⋮----
assert_matches!(
⋮----
assert_eq!(3, PrecompileError::InvalidDataOffsets as u32);

================
File: programs/ed25519-tests/Cargo.toml
================
[package]
name = "solana-ed25519-program-tests"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dev-dependencies]
assert_matches = { workspace = true }
ed25519-dalek = { workspace = true }
rand = { workspace = true }
solana-ed25519-program = { workspace = true }
solana-instruction = { workspace = true }
solana-precompile-error = { workspace = true }
solana-program-test = { workspace = true }
solana-signer = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }

================
File: programs/loader-v4/src/lib.rs
================
pub fn get_state(data: &[u8]) -> Result<&LoaderV4State, InstructionError> {
⋮----
.get(0..LoaderV4State::program_data_offset())
.ok_or(InstructionError::AccountDataTooSmall)?
.try_into()
.unwrap();
Ok(std::mem::transmute::<
⋮----
fn get_state_mut(data: &mut [u8]) -> Result<&mut LoaderV4State, InstructionError> {
⋮----
.get_mut(0..LoaderV4State::program_data_offset())
⋮----
fn check_program_account(
⋮----
if !loader_v4::check_id(program.get_owner()) {
ic_logger_msg!(log_collector, "Program not owned by loader");
return Err(InstructionError::InvalidAccountOwner);
⋮----
let state = get_state(program.get_data())?;
if !program.is_writable() {
ic_logger_msg!(log_collector, "Program is not writeable");
return Err(InstructionError::InvalidArgument);
⋮----
if !instruction_context.is_instruction_account_signer(1)? {
ic_logger_msg!(log_collector, "Authority did not sign");
return Err(InstructionError::MissingRequiredSignature);
⋮----
ic_logger_msg!(log_collector, "Incorrect authority provided");
return Err(InstructionError::IncorrectAuthority);
⋮----
if matches!(state.status, LoaderV4Status::Finalized) {
ic_logger_msg!(log_collector, "Program is finalized");
return Err(InstructionError::Immutable);
⋮----
Ok(*state)
⋮----
fn process_instruction_write(
⋮----
let log_collector = invoke_context.get_log_collector();
⋮----
let instruction_context = transaction_context.get_current_instruction_context()?;
let mut program = instruction_context.try_borrow_instruction_account(0)?;
let authority_address = instruction_context.get_key_of_instruction_account(1)?;
let state = check_program_account(
⋮----
if !matches!(state.status, LoaderV4Status::Retracted) {
ic_logger_msg!(log_collector, "Program is not retracted");
⋮----
let destination_offset = (offset as usize).saturating_add(LoaderV4State::program_data_offset());
⋮----
.get_data_mut()?
.get_mut(destination_offset..destination_offset.saturating_add(bytes.len()))
.ok_or_else(|| {
ic_logger_msg!(log_collector, "Write out of bounds");
⋮----
.copy_from_slice(&bytes);
Ok(())
⋮----
fn process_instruction_copy(
⋮----
let source_program = instruction_context.try_borrow_instruction_account(2)?;
⋮----
let source_owner = &source_program.get_owner();
⋮----
(source_offset as usize).saturating_add(if loader_v4::check_id(source_owner) {
⋮----
ic_logger_msg!(log_collector, "Source is not a program");
⋮----
.get_data()
.get(source_offset..source_offset.saturating_add(length as usize))
⋮----
ic_logger_msg!(log_collector, "Read out of bounds");
⋮----
(destination_offset as usize).saturating_add(LoaderV4State::program_data_offset());
⋮----
.get_mut(destination_offset..destination_offset.saturating_add(length as usize))
⋮----
.copy_from_slice(data);
⋮----
fn process_instruction_set_program_length(
⋮----
let is_initialization = program.get_data().len() < LoaderV4State::program_data_offset();
⋮----
let rent = invoke_context.get_sysvar_cache().get_rent()?;
rent.minimum_balance(LoaderV4State::program_data_offset().saturating_add(new_size as usize))
.max(1)
⋮----
match program.get_lamports().cmp(&required_lamports) {
⋮----
ic_logger_msg!(
⋮----
return Err(InstructionError::InsufficientFunds);
⋮----
let recipient = instruction_context.try_borrow_instruction_account(2).ok();
⋮----
if !instruction_context.is_instruction_account_writable(2)? {
ic_logger_msg!(log_collector, "Recipient is not writeable");
⋮----
let lamports_to_receive = program.get_lamports().saturating_sub(required_lamports);
program.checked_sub_lamports(lamports_to_receive)?;
recipient.checked_add_lamports(lamports_to_receive)?;
⋮----
program.set_data_length(0)?;
⋮----
program.set_data_length(
LoaderV4State::program_data_offset().saturating_add(new_size as usize),
⋮----
program.set_executable(true)?;
let state = get_state_mut(program.get_data_mut()?)?;
⋮----
fn process_instruction_deploy(invoke_context: &mut InvokeContext) -> Result<(), InstructionError> {
⋮----
let current_slot = invoke_context.get_sysvar_cache().get_clock()?.slot;
if state.slot != 0 && state.slot.saturating_add(DEPLOYMENT_COOLDOWN_IN_SLOTS) > current_slot {
⋮----
ic_logger_msg!(log_collector, "Destination program is not retracted");
⋮----
.get(LoaderV4State::program_data_offset()..)
.ok_or(InstructionError::AccountDataTooSmall)?;
deploy_program!(
⋮----
fn process_instruction_retract(invoke_context: &mut InvokeContext) -> Result<(), InstructionError> {
⋮----
if state.slot.saturating_add(DEPLOYMENT_COOLDOWN_IN_SLOTS) > current_slot {
⋮----
if !matches!(state.status, LoaderV4Status::Deployed) {
ic_logger_msg!(log_collector, "Program is not deployed");
⋮----
.store_modified_entry(
*program.get_key(),
⋮----
fn process_instruction_transfer_authority(
⋮----
let new_authority_address = instruction_context.get_key_of_instruction_account(2)?;
⋮----
if !instruction_context.is_instruction_account_signer(2)? {
ic_logger_msg!(log_collector, "New authority did not sign");
⋮----
ic_logger_msg!(log_collector, "No change");
⋮----
fn process_instruction_finalize(
⋮----
let program = instruction_context.try_borrow_instruction_account(0)?;
⋮----
ic_logger_msg!(log_collector, "Program must be deployed to be finalized");
⋮----
drop(program);
let next_version = instruction_context.try_borrow_instruction_account(2)?;
if !loader_v4::check_id(next_version.get_owner()) {
ic_logger_msg!(log_collector, "Next version is not owned by loader");
⋮----
let state_of_next_version = get_state(next_version.get_data())?;
⋮----
ic_logger_msg!(log_collector, "Next version has a different authority");
⋮----
if matches!(state_of_next_version.status, LoaderV4Status::Finalized) {
ic_logger_msg!(log_collector, "Next version is finalized");
⋮----
let address_of_next_version = *next_version.get_key();
drop(next_version);
⋮----
declare_builtin_function!(
⋮----
fn process_instruction_inner<'a>(
⋮----
let instruction_data = instruction_context.get_instruction_data();
let program_id = instruction_context.get_program_key()?;
⋮----
invoke_context.consume_checked(DEFAULT_COMPUTE_UNITS)?;
match limited_deserialize(instruction_data, solana_packet::PACKET_DATA_SIZE as u64)? {
⋮----
process_instruction_write(invoke_context, offset, bytes)
⋮----
process_instruction_copy(invoke_context, destination_offset, source_offset, length)
⋮----
process_instruction_set_program_length(invoke_context, new_size)
⋮----
LoaderV4Instruction::Deploy => process_instruction_deploy(invoke_context),
LoaderV4Instruction::Retract => process_instruction_retract(invoke_context),
⋮----
process_instruction_transfer_authority(invoke_context)
⋮----
LoaderV4Instruction::Finalize => process_instruction_finalize(invoke_context),
⋮----
.map_err(|err| Box::new(err) as Box<dyn std::error::Error>)
⋮----
.find(program_id)
⋮----
ic_logger_msg!(log_collector, "Program is not cached");
⋮----
get_or_create_executor_time.stop();
invoke_context.timings.get_or_create_executor_us += get_or_create_executor_time.as_us();
⋮----
Err(Box::new(InstructionError::UnsupportedProgramId) as Box<dyn std::error::Error>)
⋮----
ProgramCacheEntryType::Loaded(executable) => execute(executable, invoke_context),
⋮----
.map(|_| 0)
⋮----
mod tests {
⋮----
fn process_instruction(
⋮----
.iter()
.map(
⋮----
mock_process_instruction(
⋮----
fn load_program_account_from_elf(
⋮----
.join(path)
.with_extension("so");
let mut file = File::open(path).expect("file open failed");
⋮----
file.read_to_end(&mut elf_bytes).unwrap();
⋮----
let account_size = LoaderV4State::program_data_offset().saturating_add(elf_bytes.len());
⋮----
rent.minimum_balance(account_size),
⋮----
let state = get_state_mut(program_account.data_as_mut_slice()).unwrap();
⋮----
program_account.data_as_mut_slice()[LoaderV4State::program_data_offset()..]
.copy_from_slice(&elf_bytes);
⋮----
fn clock(slot: Slot) -> AccountSharedData {
⋮----
create_account_shared_data_for_test(&clock)
⋮----
fn test_loader_instruction_general_errors(instruction: LoaderV4Instruction) {
let instruction = bincode::serialize(&instruction).unwrap();
⋮----
let transaction_accounts = vec![
⋮----
process_instruction(
⋮----
transaction_accounts.clone(),
⋮----
Err(InstructionError::MissingAccount),
⋮----
Err(InstructionError::InvalidAccountOwner),
⋮----
Err(InstructionError::InvalidArgument),
⋮----
Err(InstructionError::MissingRequiredSignature),
⋮----
Err(InstructionError::Immutable),
⋮----
Err(InstructionError::IncorrectAuthority),
⋮----
fn test_loader_instruction_write() {
⋮----
bytes: vec![8, 8, 8, 8],
⋮----
.unwrap(),
⋮----
Ok(()),
⋮----
.data()
.len()
.saturating_sub(LoaderV4State::program_data_offset())
.saturating_sub(3) as u32,
⋮----
Err(InstructionError::AccountDataTooSmall),
⋮----
test_loader_instruction_general_errors(LoaderV4Instruction::Write {
⋮----
fn test_loader_instruction_copy() {
⋮----
Err(InstructionError::AccountBorrowFailed),
⋮----
test_loader_instruction_general_errors(LoaderV4Instruction::Copy {
⋮----
fn test_loader_instruction_truncate() {
⋮----
let mut transaction_accounts = vec![
⋮----
let smaller_program_lamports = transaction_accounts[0].1.lamports();
let larger_program_lamports = transaction_accounts[4].1.lamports();
assert_ne!(smaller_program_lamports, larger_program_lamports);
let accounts = process_instruction(
⋮----
assert_eq!(
⋮----
assert_eq!(accounts[2].lamports(), transaction_accounts[2].1.lamports());
⋮----
.set_lamports(smaller_program_lamports);
⋮----
.set_lamports(larger_program_lamports);
⋮----
assert_eq!(accounts[2].lamports(), transaction_accounts[2].1.lamports(),);
⋮----
&bincode::serialize(&LoaderV4Instruction::SetProgramLength { new_size: 0 }).unwrap(),
⋮----
assert_eq!(accounts[0].data().len(), 0);
⋮----
&bincode::serialize(&LoaderV4Instruction::SetProgramLength { new_size: 8 }).unwrap(),
⋮----
.saturating_add(1) as u32,
⋮----
Err(InstructionError::InsufficientFunds),
⋮----
test_loader_instruction_general_errors(LoaderV4Instruction::SetProgramLength {
⋮----
fn test_loader_instruction_deploy() {
⋮----
&bincode::serialize(&LoaderV4Instruction::Deploy).unwrap(),
⋮----
transaction_accounts[0].1 = accounts[0].clone();
transaction_accounts[5].1 = clock(2000);
⋮----
assert_eq!(accounts[0].lamports(), transaction_accounts[0].1.lamports());
⋮----
transaction_accounts[5].1 = clock(3000);
⋮----
Err(InstructionError::InvalidAccountData),
⋮----
test_loader_instruction_general_errors(LoaderV4Instruction::Deploy);
⋮----
fn test_loader_instruction_retract() {
⋮----
&bincode::serialize(&LoaderV4Instruction::Retract).unwrap(),
⋮----
transaction_accounts[4].1 = clock(0);
⋮----
test_loader_instruction_general_errors(LoaderV4Instruction::Retract);
⋮----
fn test_loader_instruction_transfer_authority() {
⋮----
&bincode::serialize(&LoaderV4Instruction::TransferAuthority).unwrap(),
⋮----
test_loader_instruction_general_errors(LoaderV4Instruction::TransferAuthority);
⋮----
fn test_loader_instruction_finalize() {
⋮----
&bincode::serialize(&LoaderV4Instruction::Finalize).unwrap(),
⋮----
fn test_execute_program() {
⋮----
Some(0),
⋮----
Some(1),
⋮----
Err(InstructionError::UnsupportedProgramId),
⋮----
Some(2),
⋮----
Some(3),
⋮----
Some(4),

================
File: programs/loader-v4/Cargo.toml
================
[package]
name = "solana-loader-v4-program"
description = "Solana Loader v4"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_loader_v4_program"

[features]
agave-unstable-api = []
shuttle-test = [
    "solana-program-runtime/shuttle-test",
    "solana-sbpf/shuttle-test",
    "solana-svm-type-overrides/shuttle-test",
]
svm-internal = []

[dependencies]
log = { workspace = true }
solana-account = { workspace = true }
solana-bincode = { workspace = true }
solana-bpf-loader-program = { workspace = true, features = ["svm-internal"] }
solana-instruction = { workspace = true }
solana-loader-v3-interface = { workspace = true }
solana-loader-v4-interface = { workspace = true, features = ["serde"] }
solana-packet = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbpf = { workspace = true, features = ["jit"] }
solana-sdk-ids = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-measure = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-transaction-context = { workspace = true }

[dev-dependencies]
bincode = { workspace = true }
solana-clock = { workspace = true }
solana-sysvar = { workspace = true }

================
File: programs/sbf/benches/bpf_loader.rs
================
extern crate test;
⋮----
macro_rules! with_mock_invoke_context {
⋮----
fn bench_program_create_executable(bencher: &mut Bencher) {
let elf = load_program_from_file("bench_alu");
⋮----
let program_runtime_environment = create_program_runtime_environment_v1(
⋮----
let program_runtime_environment = Arc::new(program_runtime_environment.unwrap());
bencher.iter(|| {
let _ = Executable::<InvokeContext>::from_elf(&elf, program_runtime_environment.clone())
.unwrap();
⋮----
fn bench_program_alu(bencher: &mut Bencher) {
⋮----
let mut inner_iter = vec![];
⋮----
inner_iter.write_u64::<LittleEndian>(0).unwrap();
⋮----
with_mock_invoke_context!(invoke_context, bpf_loader::id(), 10000001);
let feature_set = invoke_context.get_feature_set();
⋮----
Executable::<InvokeContext>::from_elf(&elf, Arc::new(program_runtime_environment.unwrap()))
⋮----
executable.verify::<RequisiteVerifier>().unwrap();
executable.jit_compile().unwrap();
create_vm!(
⋮----
let (mut vm, _, _) = vm.unwrap();
println!("Interpreted:");
⋮----
.mock_set_remaining(i64::MAX as u64);
let (instructions, result) = vm.execute_program(&executable, true);
assert_eq!(SUCCESS, result.unwrap());
assert_eq!(ARMSTRONG_LIMIT, LittleEndian::read_u64(&inner_iter));
assert_eq!(
⋮----
vm.execute_program(&executable, true).1.unwrap();
⋮----
let summary = bencher.bench(|_bencher| Ok(())).unwrap().unwrap();
println!("  {:?} instructions", instructions);
println!("  {:?} ns/iter median", summary.median as u64);
assert!(0f64 != summary.median);
⋮----
println!("  {:?} MIPS", mips);
println!(
⋮----
println!("JIT to native:");
assert_eq!(SUCCESS, vm.execute_program(&executable, false).1.unwrap());
⋮----
vm.execute_program(&executable, false).1.unwrap();
⋮----
fn bench_program_execute_noop(bencher: &mut Bencher) {
⋮----
} = create_genesis_config(50);
⋮----
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
let mut bank_client = BankClient::new_shared(bank.clone());
⋮----
let mint_pubkey = mint_keypair.pubkey();
let (_bank, invoke_program_id) = load_program_of_loader_v4(
⋮----
let account_metas = vec![AccountMeta::new(mint_pubkey, true)];
⋮----
let message = Message::new(&[instruction], Some(&mint_pubkey));
⋮----
.send_and_confirm_message(&[&mint_keypair], message.clone())
⋮----
bank.clear_signatures();
⋮----
fn bench_create_vm(bencher: &mut Bencher) {
let elf = load_program_from_file("noop");
⋮----
invoke_context.mock_set_remaining(BUDGET);
⋮----
.get_feature_set()
⋮----
let account_data_direct_mapping = invoke_context.get_feature_set().account_data_direct_mapping;
⋮----
invoke_context.get_feature_set(),
⋮----
let (_serialized, regions, account_lengths, _instruction_data_offset) = serialize_parameters(
⋮----
.get_current_instruction_context()
.unwrap(),
⋮----
vm.unwrap();
⋮----
fn bench_instruction_count_tuner(_bencher: &mut Bencher) {
let elf = load_program_from_file("tuner");
⋮----
let (instructions, _result) = vm.execute_program(&executable, true);
measure.stop();
⋮----
fn clone_regions(regions: &[MemoryRegion]) -> Vec<MemoryRegion> {
⋮----
.iter()
.map(|region| {
⋮----
.collect()

================
File: programs/sbf/c/src/alloc/alloc.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/alt_bn128/alt_bn128.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/alt_bn128_compression/alt_bn128.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/bench_alu/bench_alu.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/bench_alu/test_bench_alu.c
================


================
File: programs/sbf/c/src/big_mod_exp/big_mod_exp.c
================
extern uint64_t entrypoint(const uint8_t *input) {
struct BigModExpParam{

================
File: programs/sbf/c/src/deprecated_loader/deprecated_loader.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/dup_accounts/dup_accounts.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/error_handling/error_handling.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/float/float.c
================
extern double log2(double arg);
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/invoke/invoke.c
================
uint64_t do_nested_invokes(uint64_t num_nested_invokes,
⋮----
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/invoked/instruction.h
================


================
File: programs/sbf/c/src/invoked/invoked.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/log_data/log_data.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/move_funds/move_funds.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/multiple_static/multiple_static.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/noop/noop.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/noop++/noop++.cc
================
#include <solana_sdk.h>
extern uint64_t entrypoint(const uint8_t *input) {
  SolAccountInfo ka[1];
  SolParameters params = (SolParameters) { .ka = ka };
  if (!sol_deserialize(input, &params, SOL_ARRAY_SIZE(ka))) {
    return ERROR_INVALID_ARGUMENT;
  }
  return SUCCESS;
}

================
File: programs/sbf/c/src/panic/panic.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/poseidon/poseidon.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/read_program/read_program.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/relative_call/relative_call.c
================
void __attribute__ ((noinline)) helper() {
⋮----
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/remaining_compute_units/remaining_compute_units.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/return_data/return_data.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/sanity/sanity.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/sanity++/sanity++.cc
================
#include <solana_sdk.h>
#define INVALID_INPUT 1
extern uint64_t entrypoint(const uint8_t *input) {
  SolAccountInfo ka[1];
  SolParameters params = (SolParameters) { .ka = ka };
  sol_log(__FILE__);
  if (!sol_deserialize(input, &params, SOL_ARRAY_SIZE(ka))) {
    return ERROR_INVALID_ARGUMENT;
  }
  sol_log_params(&params);
  return SUCCESS;
}

================
File: programs/sbf/c/src/sbf_to_sbf/entrypoint.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/sbf_to_sbf/helper.c
================
void helper_function(void) {

================
File: programs/sbf/c/src/sbf_to_sbf/helper.h
================
void helper_function(void);

================
File: programs/sbf/c/src/secp256k1_recover/secp256k1_recover.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/ser/ser.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/sha/sha.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/stdlib/stdlib.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/struct_pass/struct_pass.c
================
struct foo {const uint8_t *input;};
void foo(const uint8_t *input, struct foo foo) ;
extern uint64_t entrypoint(const uint8_t *input) {
⋮----
void foo(const uint8_t *input, struct foo foo) {

================
File: programs/sbf/c/src/struct_ret/struct_ret.c
================
struct test_struct { uint64_t x; uint64_t y; uint64_t z;};
static struct test_struct __attribute__ ((noinline)) test_function(void) {
⋮----
extern uint64_t entrypoint(const uint8_t* input) {

================
File: programs/sbf/c/src/tuner/tuner.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/src/tuner-variable-iterations/tuner-variable-iterations.c
================
extern uint64_t entrypoint(const uint8_t *input) {

================
File: programs/sbf/c/.gitignore
================
/out/

================
File: programs/sbf/rust/128bit/src/lib.rs
================
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
⋮----
let y = x.rotate_right(1);
assert_eq!(y, 170_141_183_460_469_231_731_687_303_715_884_105_728);
assert_eq!(
⋮----
assert_eq!(z, 340_282_366_920_938_463_463_374_607_431_768_211_454);
assert_eq!(u128::from(1u32.to_le()), 1);
assert_eq!(u128::from(1u32.to_be()), 0x0100_0000);
assert_eq!(solana_sbf_rust_128bit_dep::uadd(10, 20), 30u128);
assert_eq!(solana_sbf_rust_128bit_dep::usubtract(30, 20), 10u128);
assert_eq!(solana_sbf_rust_128bit_dep::umultiply(30, 20), 600u128);
assert_eq!(solana_sbf_rust_128bit_dep::udivide(20, 10), 2u128);
assert_eq!(solana_sbf_rust_128bit_dep::umodulo(20, 3), 2u128);
assert_eq!(solana_sbf_rust_128bit_dep::add(-10, -20), -30i128);
assert_eq!(solana_sbf_rust_128bit_dep::subtract(-30, -20), -10i128);
assert_eq!(solana_sbf_rust_128bit_dep::multiply(-30, -20), 600i128);
assert_eq!(solana_sbf_rust_128bit_dep::divide(-20, -10), 2i128);
assert_eq!(solana_sbf_rust_128bit_dep::modulo(-20, -3), -2i128);
⋮----
assert_eq!(u128::from(x) + u128::from(x), 36_893_488_147_419_103_230);
⋮----
assert_eq!(x.wrapping_shr(64) as u64, 1);
⋮----
assert_eq!(x, 0x0001_ffff_ffff_ffff_fffe);
⋮----
custom_heap_default!();
custom_panic_default!();
⋮----
mod test {
⋮----
fn test_entrypoint() {
assert_eq!(SUCCESS, entrypoint(std::ptr::null_mut()));

================
File: programs/sbf/rust/128bit/Cargo.toml
================
[package]
name = "solana-sbf-rust-128bit"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-program-entrypoint = { workspace = true }
solana-sbf-rust-128bit-dep = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/128bit_dep/src/lib.rs
================
pub fn uadd(x: u128, y: u128) -> u128 {
⋮----
pub fn usubtract(x: u128, y: u128) -> u128 {
⋮----
pub fn umultiply(x: u128, y: u128) -> u128 {
⋮----
pub fn udivide(n: u128, d: u128) -> u128 {
⋮----
pub fn umodulo(n: u128, d: u128) -> u128 {
⋮----
pub fn add(x: i128, y: i128) -> i128 {
⋮----
pub fn subtract(x: i128, y: i128) -> i128 {
⋮----
pub fn multiply(x: i128, y: i128) -> i128 {
⋮----
pub fn divide(n: i128, d: i128) -> i128 {
⋮----
pub fn modulo(n: i128, d: i128) -> i128 {

================
File: programs/sbf/rust/128bit_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-128bit-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[dependencies]

================
File: programs/sbf/rust/account_mem/src/lib.rs
================
pub fn process_instruction(
⋮----
let account = accounts.last().ok_or(ProgramError::NotEnoughAccountKeys)?;
let data_len = account.try_borrow_data()?.len().wrapping_add(10240);
let data_ptr = account.try_borrow_mut_data()?.as_mut_ptr();
⋮----
let data = data.as_mut_ptr().wrapping_sub(before);
⋮----
buf[i] = too_early(8)[i];
⋮----
sol_memcmp(too_early(8), &buf, 500);
⋮----
buf[i] = too_early(9)[i];
⋮----
sol_memcmp(&buf, too_early(9), 12);
⋮----
buf[i] = too_early(2)[i];
⋮----
sol_memset(too_early(2), 3, 3);
sol_memcpy(too_early(2), &buf, 3);
⋮----
sol_memcpy(&mut buf, too_early(3), 10);
⋮----
sol_memmove(buf.as_mut_ptr(), too_early(3).as_ptr(), 10);
⋮----
sol_memcpy(too_early(3), &buf, 10);
⋮----
sol_memmove(too_early(3).as_mut_ptr(), buf.as_ptr(), 10);
⋮----
sol_memmove(too_early(0).as_mut_ptr(), too_early(3).as_ptr(), 10);
⋮----
sol_memcmp(&buf, &data[data_len.saturating_sub(8)..], 16);
⋮----
sol_memcmp(&data[data_len.saturating_sub(7)..], &buf, 15);
⋮----
sol_memset(&mut data[data_len.saturating_sub(2)..], 0, 3);
⋮----
sol_memcpy(&mut buf, &data[data_len.saturating_sub(3)..], 10);
⋮----
sol_memmove(
buf.as_mut_ptr(),
data[data_len.saturating_sub(3)..].as_ptr(),
⋮----
sol_memcpy(&mut data[data_len.saturating_sub(3)..], &buf, 10);
⋮----
data[data_len.saturating_sub(3)..].as_mut_ptr(),
buf.as_ptr(),
⋮----
data[data_len..].as_mut_ptr(),
⋮----
Ok(())

================
File: programs/sbf/rust/account_mem/Cargo.toml
================
[package]
name = "solana-sbf-rust-account-mem"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-program-memory = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/account_mem_deprecated/src/lib.rs
================
pub fn process_instruction(
⋮----
let account = accounts.last().ok_or(ProgramError::NotEnoughAccountKeys)?;
let data_len = account.try_borrow_data()?.len();
let data_ptr = account.try_borrow_mut_data()?.as_mut_ptr();
⋮----
let data = data.as_mut_ptr().wrapping_sub(before);
unsafe { std::slice::from_raw_parts_mut(data, data_len.wrapping_add(100)) }
⋮----
buf[i] = too_early(8)[i];
⋮----
sol_memcmp(too_early(8), &buf, 90);
⋮----
buf[i] = too_early(9)[i];
⋮----
sol_memcmp(&buf, too_early(9), 12);
⋮----
sol_memset(too_early(2), 3, 3);
⋮----
sol_memcpy(&mut buf, too_early(3), 10);
⋮----
sol_memmove(buf.as_mut_ptr(), too_early(3).as_ptr(), 10);
⋮----
sol_memcpy(too_early(3), &buf, 10);
⋮----
sol_memmove(too_early(3).as_mut_ptr(), buf.as_ptr(), 10);
⋮----
sol_memmove(too_early(0).as_mut_ptr(), too_early(3).as_ptr(), 10);
⋮----
sol_memcmp(&buf, &data[data_len.saturating_sub(8)..], 16);
⋮----
sol_memcmp(&data[data_len.saturating_sub(7)..], &buf, 15);
⋮----
sol_memset(&mut data[data_len.saturating_sub(2)..], 0, 3);
⋮----
sol_memcpy(&mut buf, &data[data_len.saturating_sub(3)..], 10);
⋮----
sol_memmove(
buf.as_mut_ptr(),
data[data_len.saturating_sub(3)..].as_ptr(),
⋮----
sol_memcpy(&mut data[data_len.saturating_sub(3)..], &buf, 10);
⋮----
data[data_len.saturating_sub(3)..].as_mut_ptr(),
buf.as_ptr(),
⋮----
data[data_len..].as_mut_ptr(),
⋮----
Ok(())

================
File: programs/sbf/rust/account_mem_deprecated/Cargo.toml
================
[package]
name = "solana-sbf-rust-account-mem-deprecated"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program = { workspace = true }
solana-program-error = { workspace = true }
solana-program-memory = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/alloc/src/lib.rs
================
extern crate alloc;
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
⋮----
let layout = Layout::from_size_align(100, mem::align_of::<u8>()).unwrap();
⋮----
if ptr.is_null() {
msg!("Error: Alloc of 100 bytes failed");
⋮----
let layout = Layout::from_size_align(ITERS, mem::align_of::<u8>()).unwrap();
⋮----
msg!("Error: Alloc failed");
⋮----
*ptr.add(i) = i as u8;
⋮----
assert_eq!(*ptr.add(i), i as u8);
⋮----
sol_log_64(0x3, 0, 0, 0, u64::from(*ptr.add(42)));
assert_eq!(*ptr.add(42), 42);
⋮----
let ones = vec![1_usize; ITERS];
⋮----
for v in ones.iter() {
⋮----
sol_log_64(0x0, 0, 0, 0, sum as u64);
assert_eq!(sum, ITERS);
⋮----
v.push(i);
⋮----
sol_log_64(0x4, 0, 0, 0, v.len() as u64);
assert_eq!(v.len(), ITERS);
⋮----
custom_heap_default!();
custom_panic_default!();
⋮----
mod test {
⋮----
fn test_entrypoint() {
assert_eq!(SUCCESS, entrypoint(std::ptr::null_mut()));

================
File: programs/sbf/rust/alloc/Cargo.toml
================
[package]
name = "solana-sbf-rust-alloc"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/alt_bn128/src/lib.rs
================
fn alt_bn128_addition_be_test() {
let result = alt_bn128_g1_addition_be(&[
⋮----
assert!(result.is_ok());
assert_eq!(
⋮----
let result = alt_bn128_g1_addition_be(&[]);
⋮----
fn alt_bn128_addition_le_test() {
let result = alt_bn128_g1_addition_le(&[
⋮----
fn alt_bn128_multiplication_be_test() {
let result = alt_bn128_g1_multiplication_be(&[
⋮----
fn alt_bn128_multiplication_le_test() {
let result = alt_bn128_g1_multiplication_le(&[
⋮----
fn alt_bn128_pairing_be_test() {
let result = alt_bn128_pairing_be(&[
⋮----
fn alt_bn128_pairing_le_test() {
let result = alt_bn128_pairing_le(&[
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
msg!("alt_bn128");
alt_bn128_addition_be_test();
alt_bn128_addition_le_test();
alt_bn128_multiplication_be_test();
alt_bn128_multiplication_le_test();
alt_bn128_pairing_be_test();
alt_bn128_pairing_le_test();
⋮----
custom_heap_default!();
custom_panic_default!();

================
File: programs/sbf/rust/alt_bn128/Cargo.toml
================
[package]
name = "solana-sbf-rust-alt-bn128"
description = "Solana BPF test program written in Rust"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
array-bytes = { workspace = true }
solana-bn254 = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/alt_bn128_compression/src/lib.rs
================
fn alt_bn128_compression_g1_be() {
⋮----
points_g1.iter().for_each(|point| {
let g1_compressed_be = alt_bn128_g1_compress(point).unwrap();
let g1_decompressed_be = alt_bn128_g1_decompress(&g1_compressed_be).unwrap();
assert_eq!(*point, g1_decompressed_be);
⋮----
fn alt_bn128_compression_g1_le() {
⋮----
let g1_compressed_le = alt_bn128_g1_compress_le(point).unwrap();
let g1_decompressed_le = alt_bn128_g1_decompress_le(&g1_compressed_le).unwrap();
assert_eq!(*point, g1_decompressed_le);
⋮----
fn alt_bn128_compression_g2_be() {
⋮----
points_g2.iter().for_each(|point| {
let g2_compressed_be = alt_bn128_g2_compress(point).unwrap();
let g2_decompressed_be = alt_bn128_g2_decompress(&g2_compressed_be).unwrap();
assert_eq!(*point, g2_decompressed_be);
⋮----
fn alt_bn128_compression_g2_le() {
⋮----
let g2_compressed_le = alt_bn128_g2_compress_le(point).unwrap();
let g2_decompressed_le = alt_bn128_g2_decompress_le(&g2_compressed_le).unwrap();
assert_eq!(*point, g2_decompressed_le);
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
msg!("alt_bn128_compression");
alt_bn128_compression_g1_be();
alt_bn128_compression_g1_le();
alt_bn128_compression_g2_be();
alt_bn128_compression_g2_le();
⋮----
custom_heap_default!();
custom_panic_default!();

================
File: programs/sbf/rust/alt_bn128_compression/Cargo.toml
================
[package]
name = "solana-sbf-rust-alt-bn128-compression"
description = "Solana BPF test program written in Rust"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
array-bytes = { workspace = true }
solana-bn254 = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/big_mod_exp/src/lib.rs
================
fn big_mod_exp_test() {
⋮----
struct TestCase {
⋮----
let test_cases: Vec<TestCase> = serde_json::from_str(test_data).unwrap();
test_cases.iter().for_each(|test| {
⋮----
let result = big_mod_exp(base.as_slice(), exponent.as_slice(), modulus.as_slice());
assert_eq!(result, expected);
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
msg!("big_mod_exp");
big_mod_exp_test();
⋮----
custom_panic_default!();

================
File: programs/sbf/rust/big_mod_exp/Cargo.toml
================
[package]
name = "solana-sbf-rust-big-mod-exp"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
array-bytes = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-big-mod-exp = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/call_args/src/lib.rs
================
struct Test128 {
⋮----
struct InputData {
⋮----
struct OutputData {
⋮----
pub fn entry(_program_id: &Pubkey, _accounts: &[AccountInfo], data: &[u8]) -> ProgramResult {
⋮----
x.copy_from_slice(&buffer[3784..3800]);
⋮----
check_arr(x);
assert_eq!(x[10], 0x39);
assert_eq!(x[11], 0x37);
let y = check_arr_and_return(x);
⋮----
assert_eq!(y[10], 0x39);
assert_eq!(y[11], 0x37);
assert_eq!(y[15], 17);
let decoded: InputData = from_slice::<InputData>(data).unwrap();
⋮----
res_128: test_128_arg(decoded.test_128.a, decoded.test_128.b),
res_256: test_256_arg(decoded.test_128),
many_args_1: many_args(
⋮----
many_args_2: many_args_stack_space(
⋮----
let encoded = to_vec(&output).unwrap();
set_return_data(encoded.as_slice());
Ok(())
⋮----
extern "C" fn check_arr(x: [u8; 16]) {
for (idx, item) in x.iter().enumerate() {
⋮----
assert!(*item == 1u8);
⋮----
extern "C" fn check_arr_and_return(mut x: [u8; 16]) -> [u8; 16] {
⋮----
fn test_128_arg(x: u128, y: u128) -> u128 {
⋮----
fn test_256_arg(x: Test128) -> Test128 {
⋮----
extern "C" fn many_args(a: i64, b: i64, c: i64, d: i64, e: i64, f: i64, g: i64, h: i64) -> i64 {
⋮----
extern "C" fn many_args_stack_space(

================
File: programs/sbf/rust/call_args/Cargo.toml
================
[package]
name = "solana-sbf-rust-call-args"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
borsh = { workspace = true }
solana-account-info = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/call_depth/src/lib.rs
================
pub fn recurse(data: &mut [u8]) {
if data.len() <= 1 {
⋮----
recurse(&mut data[1..]);
sol_log_64(line!() as u64, 0, 0, 0, data[0] as u64);
⋮----
pub unsafe extern "C" fn entrypoint(input: *mut u8) -> u64 {
msg!("Call depth");
let depth = *input.add(16);
sol_log_64(line!() as u64, 0, 0, 0, depth as u64);
⋮----
data.push(i);
⋮----
recurse(&mut data);
⋮----
custom_heap_default!();
custom_panic_default!();

================
File: programs/sbf/rust/call_depth/Cargo.toml
================
[package]
name = "solana-sbf-rust-call-depth"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/caller_access/src/lib.rs
================
fn process_instruction(
⋮----
if instruction_data.len() == 32 {
let key = Pubkey::new_from_array(instruction_data.try_into().unwrap());
let ix = Instruction::new_with_bincode(key, &[2], vec![]);
let mut lamports = accounts[0].lamports();
⋮----
let mut data = accounts[0].try_borrow_mut_data()?;
⋮----
msg!("{:?} calling {:?}", program_id, key);
invoke(&ix, &[account])?;
⋮----
&accounts[1].key.to_bytes(),
vec![AccountMeta::new_readonly(*program_id, false)],
⋮----
msg!("{:?} calling {:?}", program_id, program_id);
invoke(&ix, accounts)?;
⋮----
_ => msg!("Should never get here"),
⋮----
Ok(())

================
File: programs/sbf/rust/caller_access/Cargo.toml
================
[package]
name = "solana-sbf-rust-caller-access"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/curve25519/src/lib.rs
================
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
⋮----
msg!("validate_edwards");
assert!(edwards::validate_edwards(&edwards_generator));
msg!("add_edwards");
assert_eq!(
⋮----
msg!("multiply_edwards");
⋮----
msg!("multiscalar_multiply_edwards");
⋮----
msg!("validate_ristretto");
assert!(ristretto::validate_ristretto(&ristretto_generator));
msg!("add_ristretto");
⋮----
msg!("multiply_ristretto");
⋮----
msg!("multiscalar_multiply_ristretto");
⋮----
custom_heap_default!();
custom_panic_default!();

================
File: programs/sbf/rust/curve25519/Cargo.toml
================
[package]
name = "solana-sbf-rust-curve25519"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-curve25519 = { workspace = true, features = ["agave-unstable-api"] }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/custom_heap/src/lib.rs
================
struct BumpAllocator;
⋮----
unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
if layout.size() == isize::MAX as usize - 0x42 {
⋮----
pos = pos.saturating_sub(layout.size());
pos &= !(layout.align().saturating_sub(1));
⋮----
return null_mut();
⋮----
unsafe fn dealloc(&self, _: *mut u8, _: Layout) {
⋮----
pub fn process_instruction(
⋮----
msg!("Custom heap");
⋮----
let layout = Layout::from_size_align(isize::MAX as usize - 0x42, align_of::<u8>()).unwrap();
let ptr = alloc(layout);
assert_eq!(ptr as u64, 0x42);
⋮----
Ok(())

================
File: programs/sbf/rust/custom_heap/Cargo.toml
================
[package]
name = "solana-sbf-rust-custom-heap"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[features]
default = ["custom-heap"]
custom-heap = []

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/dep_crate/src/lib.rs
================
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
⋮----
assert_eq!(1_000_000, LittleEndian::read_u32(&buf));
⋮----
assert_eq!(-5_000, LittleEndian::read_i16(&buf));
⋮----
mod test {
⋮----
fn test_entrypoint() {
assert_eq!(SUCCESS, entrypoint(std::ptr::null_mut()));

================
File: programs/sbf/rust/dep_crate/Cargo.toml
================
[package]
name = "solana-sbf-rust-dep-crate"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
byteorder = { workspace = true }
solana-program-entrypoint = { workspace = true }

================
File: programs/sbf/rust/deprecated_loader/src/lib.rs
================
struct SStruct {
⋮----
fn return_sstruct() -> SStruct {
⋮----
fn custom_panic(info: &core::panic::PanicInfo<'_>) {
// Full panic reporting
msg!(&format!("{info}"));
⋮----
fn process_instruction(
⋮----
msg!("Program identifier:");
program_id.log();
assert!(!bpf_loader::check_id(program_id));
// test_sol_alloc_free_no_longer_deployable calls this program with
// bpf_loader instead of bpf_loader_deprecated, so instruction_data isn't
match instruction_data.first() {
⋮----
let (bytes, _) = instruction_data[2..].split_at(std::mem::size_of::<usize>());
let new_len = usize::from_le_bytes(bytes.try_into().unwrap());
msg!("realloc to {}", new_len);
⋮----
account.resize(new_len)?;
assert_eq!(new_len, account.data_len());
⋮----
msg!("realloc extend from slice deprecated");
⋮----
let prev_len = account.data_len();
account.resize(prev_len + data.len())?;
account.data.borrow_mut()[prev_len..].copy_from_slice(data);
⋮----
msg!("DEPRECATED LOADER TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS");
⋮----
account.data.borrow().to_vec()
⋮----
let mut instruction_data = vec![TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_NESTED];
instruction_data.extend_from_slice(&expected);
invoke(
&create_instruction(
⋮----
.unwrap();
⋮----
msg!("DEPRECATED LOADER TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_NESTED");
⋮----
assert_eq!(*account.data.borrow(), &instruction_data[1..]);
⋮----
msg!("DEPRECATED LOADER TEST_CPI_ACCOUNT_UPDATE_CALLEE_GROWS");
⋮----
let mut instruction_data = instruction_data.to_vec();
let mut expected = account.data.borrow().to_vec();
expected.extend_from_slice(&instruction_data[1..]);
⋮----
instruction_data.to_vec(),
⋮----
assert_eq!(&*account.data.borrow(), &expected);
⋮----
msg!(
⋮----
let new_len = usize::from_le_bytes(instruction_data[2..10].try_into().unwrap());
⋮----
let expected = account.data.borrow()[..new_len].to_vec();
let mut instruction_data = vec![REALLOC, 0];
instruction_data.extend_from_slice(&new_len.to_le_bytes());
⋮----
assert_eq!(
⋮----
msg!("DEPRECATED LOADER TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_CALLEE_SHRINKS");
⋮----
.borrow_mut()
.as_mut_ptr()
.add(prev_len + data.len()) = SENTINEL;
⋮----
expected.extend_from_slice(&instruction_data[10..]);
⋮----
vec![TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_CALLEE_SHRINKS_NESTED];
⋮----
assert_eq!(*account.data.borrow(), &prev_data[..new_len]);
⋮----
msg!("DEPRECATED LOADER TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_CALLEE_SHRINKS_NESTED");
⋮----
let new_len = usize::from_le_bytes(instruction_data[1..9].try_into().unwrap());
account.resize(new_len).unwrap();
⋮----
msg!("Account keys and instruction input data:");
sol_log_params(accounts, instruction_data);
⋮----
let result_str = std::str::from_utf8(&sparkle_heart).unwrap();
assert_eq!(4, result_str.len());
assert_eq!("💖", result_str);
msg!(result_str);
⋮----
let s = return_sstruct();
assert_eq!(s.x + s.y + s.z, 6);
⋮----
panic!();
⋮----
Ok(())
⋮----
pub fn create_instruction(
⋮----
.iter()
.map(|(key, is_writable, is_signer)| {
⋮----
.collect();
⋮----
mod test {
⋮----
fn test_return_sstruct() {
assert_eq!(SStruct { x: 1, y: 2, z: 3 }, return_sstruct());

================
File: programs/sbf/rust/deprecated_loader/Cargo.toml
================
[package]
name = "solana-sbf-rust-deprecated-loader"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbf-rust-invoke-dep = { workspace = true }
solana-sbf-rust-realloc-dep = { workspace = true }
solana-sdk-ids = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/divide_by_zero/src/lib.rs
================
fn process_instruction(
⋮----
asm!(
⋮----
Ok(())

================
File: programs/sbf/rust/divide_by_zero/Cargo.toml
================
[package]
name = "solana-sbf-rust-divide-by-zero"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/dup_accounts/src/lib.rs
================
fn process_instruction(
⋮----
msg!("modify account (2) data");
accounts[2].data.borrow_mut()[0] = 1;
⋮----
msg!("modify account (3) data");
accounts[3].data.borrow_mut()[0] = 2;
⋮----
msg!("modify account (2,3) data");
accounts[2].data.borrow_mut()[0] += 1;
accounts[3].data.borrow_mut()[0] += 2;
⋮----
msg!("modify account (1,2) lamports");
**accounts[1].lamports.borrow_mut() -= 1;
**accounts[2].lamports.borrow_mut() += 1;
⋮----
msg!("modify account (1,3) lamports");
**accounts[1].lamports.borrow_mut() -= 2;
**accounts[3].lamports.borrow_mut() += 2;
⋮----
msg!("modify account (1,2,3) lamports");
**accounts[1].lamports.borrow_mut() -= 3;
⋮----
msg!("check account (0,1,2,3) privs");
assert!(accounts[0].is_signer);
assert!(!accounts[1].is_signer);
assert!(accounts[2].is_signer);
assert!(accounts[3].is_signer);
assert!(accounts[0].is_writable);
assert!(accounts[1].is_writable);
assert!(accounts[2].is_writable);
assert!(accounts[3].is_writable);
if accounts.len() > 4 {
⋮----
vec![
⋮----
invoke(&instruction, accounts)?;
⋮----
assert_eq!(accounts[2].try_borrow_mut_data()?[0], 3);
assert_eq!(accounts[3].try_borrow_mut_data()?[0], 3);
⋮----
msg!("Unrecognized command");
return Err(ProgramError::InvalidArgument);
⋮----
Ok(())

================
File: programs/sbf/rust/dup_accounts/Cargo.toml
================
[package]
name = "solana-sbf-rust-dup-accounts"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/error_handling/src/lib.rs
================
pub enum MyError {
⋮----
fn from(e: MyError) -> Self {
⋮----
impl ToStr for MyError {
fn to_str(&self) -> &'static str {
⋮----
fn process_instruction(
⋮----
msg!("return success");
Ok(())
⋮----
msg!("return a builtin");
Err(ProgramError::InvalidAccountData)
⋮----
msg!("return default enum start value");
Err(MyError::DefaultEnumStart.into())
⋮----
msg!("return custom error");
Err(MyError::TheAnswer.into())
⋮----
let data = accounts[0].try_borrow_mut_data()?;
let data2 = accounts[0].try_borrow_mut_data()?;
assert_eq!(*data, *data2);
⋮----
msg!("return pubkey error");
Err(PubkeyError::MaxSeedLengthExceeded.into())
⋮----
msg!("Unsupported");
Err(ProgramError::InvalidInstructionData)

================
File: programs/sbf/rust/error_handling/Cargo.toml
================
[package]
name = "solana-sbf-rust-error-handling"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
num-derive = { workspace = true }
num-traits = { workspace = true }
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
thiserror = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/external_spend/src/lib.rs
================
fn process_instruction(
⋮----
**accounts[0].lamports.borrow_mut() -= 1;
Ok(())

================
File: programs/sbf/rust/external_spend/Cargo.toml
================
[package]
name = "solana-sbf-rust-external-spend"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/get_minimum_delegation/src/lib.rs
================
fn process_instruction(
⋮----
msg!(
⋮----
Ok(())

================
File: programs/sbf/rust/get_minimum_delegation/Cargo.toml
================
[package]
name = "solana-sbf-rust-get-minimum-delegation"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-stake-interface = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/inner_instruction_alignment_check/src/lib.rs
================
fn process_instruction(
⋮----
accounts: vec![AccountMeta {
⋮----
data: instruction_data.to_owned(),
⋮----
let _ = invoke(&instruction, infos);
⋮----
Ok(())
⋮----
custom_heap_default!();
⋮----
fn custom_panic(info: &core::panic::PanicInfo<'_>) {
msg!(&format!("{info}"));

================
File: programs/sbf/rust/inner_instruction_alignment_check/Cargo.toml
================
[package]
name = "solana-sbf-rust-inner_instruction_alignment_check"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/instruction_introspection/src/lib.rs
================
fn process_instruction(
⋮----
if instruction_data.is_empty() {
return Err(ProgramError::InvalidAccountData);
⋮----
let instructions_account = accounts.last().ok_or(ProgramError::NotEnoughAccountKeys)?;
assert_eq!(*instructions_account.key, instructions::id());
let data_len = instructions_account.try_borrow_data()?.len();
⋮----
assert_eq!(current_instruction, my_index);
msg!(&format!("id: {}", instruction.program_id));
msg!(&format!("data[0]: {}", instruction.data[0]));
msg!(&format!("index: {current_instruction}"));
if instruction_data.len() == 2 {
invoke(
⋮----
vec![AccountMeta::new_readonly(instructions::id(), false)],
⋮----
Ok(())

================
File: programs/sbf/rust/instruction_introspection/Cargo.toml
================
[package]
name = "solana-sbf-rust-instruction-introspection"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-instructions-sysvar = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/invoke/src/lib.rs
================
use solana_program_memory::sol_memcmp;
⋮----
fn do_nested_invokes(num_nested_invokes: u64, accounts: &[AccountInfo]) -> ProgramResult {
assert!(accounts[ARGUMENT_INDEX].is_signer);
let pre_argument_lamports = accounts[ARGUMENT_INDEX].lamports();
let pre_invoke_argument_lamports = accounts[INVOKED_ARGUMENT_INDEX].lamports();
⋮----
let mut lamports = (*accounts[ARGUMENT_INDEX].lamports).borrow_mut();
**lamports = (*lamports).saturating_sub(5);
let mut lamports = (*accounts[INVOKED_ARGUMENT_INDEX].lamports).borrow_mut();
**lamports = (*lamports).saturating_add(5);
⋮----
msg!("First invoke");
let instruction = create_instruction(
⋮----
vec![NESTED_INVOKE, num_nested_invokes as u8],
⋮----
invoke(&instruction, accounts)?;
msg!("2nd invoke from first program");
⋮----
assert_eq!(
⋮----
Ok(())
⋮----
fn process_instruction<'a>(
⋮----
msg!("invoke Rust program");
⋮----
msg!("Call system program create account");
⋮----
let from_lamports = accounts[FROM_INDEX].lamports();
let to_lamports = accounts[DERIVED_KEY1_INDEX].lamports();
assert_eq!(accounts[DERIVED_KEY1_INDEX].data_len(), 0);
assert!(solana_system_interface::program::check_id(
⋮----
invoke_signed(
⋮----
assert_eq!(program_id, accounts[DERIVED_KEY1_INDEX].owner);
⋮----
let mut data = accounts[DERIVED_KEY1_INDEX].try_borrow_mut_data()?;
assert_eq!(data[MAX_PERMITTED_DATA_INCREASE.saturating_sub(1)], 0);
data[MAX_PERMITTED_DATA_INCREASE.saturating_sub(1)] = 0x0f;
assert_eq!(data[MAX_PERMITTED_DATA_INCREASE.saturating_sub(1)], 0x0f);
⋮----
msg!("Call system program transfer");
⋮----
msg!("Test data translation");
⋮----
let mut data = accounts[ARGUMENT_INDEX].try_borrow_mut_data()?;
⋮----
vec![VERIFY_TRANSLATIONS, 1, 2, 3, 4, 5],
⋮----
msg!("Test no instruction data");
⋮----
vec![],
⋮----
msg!("Test refcell usage");
⋮----
vec![RETURN_OK, 1, 2, 3, 4, 5],
⋮----
let _ref_mut = accounts[writable].try_borrow_mut_lamports()?;
⋮----
let _ref_mut = accounts[writable].try_borrow_mut_data()?;
⋮----
let _ref_mut = accounts[writable].try_borrow_lamports()?;
⋮----
let _ref_mut = accounts[writable].try_borrow_data()?;
⋮----
let _ref_mut = accounts[readable].try_borrow_mut_lamports()?;
⋮----
let _ref_mut = accounts[readable].try_borrow_mut_data()?;
⋮----
let _ref_mut = accounts[readable].try_borrow_lamports()?;
⋮----
let _ref_mut = accounts[readable].try_borrow_data()?;
⋮----
msg!("Test create_program_address");
⋮----
msg!("Test try_find_program_address");
⋮----
Pubkey::try_find_program_address(&[b"You pass butter"], program_id).unwrap();
assert_eq!(&address, accounts[DERIVED_KEY1_INDEX].key);
assert_eq!(bump_seed, bump_seed1);
⋮----
msg!("Test derived signers");
⋮----
assert!(!accounts[DERIVED_KEY1_INDEX].is_signer);
assert!(!accounts[DERIVED_KEY2_INDEX].is_signer);
assert!(!accounts[DERIVED_KEY3_INDEX].is_signer);
let invoked_instruction = create_instruction(
⋮----
vec![DERIVED_SIGNERS, bump_seed2, bump_seed3],
⋮----
msg!("Test readonly with writable account");
⋮----
vec![VERIFY_WRITER],
⋮----
invoke(&invoked_instruction, accounts)?;
⋮----
msg!("Test nested invoke");
⋮----
do_nested_invokes(4, accounts)?;
⋮----
msg!("Test privilege deescalation");
⋮----
assert!(accounts[INVOKED_ARGUMENT_INDEX].is_signer);
assert!(accounts[INVOKED_ARGUMENT_INDEX].is_writable);
⋮----
vec![VERIFY_PRIVILEGE_DEESCALATION],
⋮----
msg!("Verify data values are retained and updated");
⋮----
let data = accounts[ARGUMENT_INDEX].try_borrow_data()?;
⋮----
assert_eq!(data[i as usize], i);
⋮----
let data = accounts[INVOKED_ARGUMENT_INDEX].try_borrow_data()?;
⋮----
msg!("Verify data write before cpi call with deescalated writable");
⋮----
assert_eq!(data[i as usize], 42);
⋮----
msg!("Create account and init data");
⋮----
let to_lamports = accounts[DERIVED_KEY2_INDEX].lamports();
⋮----
vec![CREATE_AND_INIT, bump_seed2],
⋮----
let data = accounts[DERIVED_KEY2_INDEX].try_borrow_mut_data()?;
assert_eq!(data[0], 0x0e);
⋮----
assert_eq!(data[i], i as u8);
⋮----
msg!("Test return data via invoked");
⋮----
set_return_data(b"x");
⋮----
vec![SET_RETURN_DATA],
⋮----
let _ = invoke(&instruction, accounts);
⋮----
msg!("Test accounts re-ordering");
⋮----
vec![RETURN_OK],
⋮----
let mut reordered_accounts = accounts.to_vec();
let account_info = reordered_accounts.remove(FROM_INDEX);
reordered_accounts.push(accounts[0].clone());
reordered_accounts.push(account_info);
invoke(&instruction, &reordered_accounts)?;
⋮----
msg!("Test privilege escalation signer");
let mut invoked_instruction = create_instruction(
⋮----
vec![VERIFY_PRIVILEGE_ESCALATION],
⋮----
msg!("Test privilege escalation writable");
⋮----
msg!("Test program not owned by loader");
⋮----
msg!("Test program not executable");
⋮----
msg!("Empty accounts slice");
⋮----
invoke(&instruction, &[])?;
⋮----
msg!("Test program max seeds");
let instruction = create_instruction(*accounts[INVOKED_PROGRAM_INDEX].key, &[], vec![]);
⋮----
msg!("Test program max signers");
⋮----
msg!("Test resize violation");
⋮----
let ptr = accounts[FROM_INDEX].data.borrow().as_ptr() as u64 as *mut _;
let len = accounts[FROM_INDEX].data_len();
⋮----
let mut lamports = accounts[FROM_INDEX].lamports();
⋮----
let mut lamports = accounts[DERIVED_KEY1_INDEX].lamports();
⋮----
let ptr = accounts[SYSTEM_PROGRAM_INDEX].data.borrow().as_ptr() as u64 as *mut _;
let len = accounts[SYSTEM_PROGRAM_INDEX].data_len();
⋮----
let mut lamports = accounts[SYSTEM_PROGRAM_INDEX].lamports();
⋮----
&[system_info.clone(), from_info.clone(), derived_info.clone()],
⋮----
msg!("Test max instruction data len exceeded");
let data_len = MAX_CPI_INSTRUCTION_DATA_LEN.saturating_add(1) as usize;
⋮----
create_instruction(*accounts[INVOKED_PROGRAM_INDEX].key, &[], vec![0; data_len]);
invoke_signed(&instruction, &[], &[])?;
⋮----
msg!("Test max instruction accounts exceeded");
⋮----
let account_metas_len = (MAX_CPI_INSTRUCTION_ACCOUNTS as usize).saturating_add(1);
let account_metas = vec![(&default_key, false, false); account_metas_len];
⋮----
create_instruction(*accounts[INVOKED_PROGRAM_INDEX].key, &account_metas, vec![]);
⋮----
msg!("Test max account infos ok before SIMD-0339 and before increase cpi info");
⋮----
let account_infos = vec![accounts[0].clone(); account_infos_len];
invoke_signed(&instruction, &account_infos, &[])?;
⋮----
msg!("Test max account infos exceeded before SIMD-0339 and before increase cpi info");
⋮----
msg!("Test max account infos ok before SIMD-0339");
⋮----
msg!("Test max account infos exceeded before SIMD-0339");
⋮----
let account_infos_len = MAX_CPI_ACCOUNT_INFOS.saturating_add(1);
⋮----
msg!("Test max account infos ok");
⋮----
msg!("Test max account infos exceeded");
⋮----
msg!("Test return error");
⋮----
vec![RETURN_ERROR],
⋮----
msg!("Test privilege deescalation escalation signer");
⋮----
vec![VERIFY_PRIVILEGE_DEESCALATION_ESCALATION_SIGNER],
⋮----
msg!("Test privilege deescalation escalation writable");
⋮----
vec![VERIFY_PRIVILEGE_DEESCALATION_ESCALATION_WRITABLE],
⋮----
msg!("Test writable deescalation writable");
⋮----
buffer.copy_from_slice(&accounts[INVOKED_ARGUMENT_INDEX].data.borrow()[..NUM_BYTES]);
⋮----
vec![WRITE_ACCOUNT, NUM_BYTES as u8],
⋮----
let _ = do_nested_invokes(5, accounts);
⋮----
let _ = do_nested_invokes(8, accounts);
⋮----
let _ = do_nested_invokes(9, accounts);
⋮----
msg!("Test calling precompiled program from cpi");
⋮----
Instruction::new_with_bytes(*accounts[ED25519_PROGRAM_INDEX].key, &[], vec![]);
⋮----
let mut lamports = (*accounts[0].lamports).borrow_mut();
**lamports = (*lamports).saturating_add(1);
⋮----
set_return_data(&[1u8; 1028]);
⋮----
msg!("Test duplicate privilege escalation signer");
⋮----
msg!("Test duplicate privilege escalation writable");
⋮----
msg!("TEST_FORBID_WRITE_AFTER_OWNERSHIP_CHANGE_IN_CALLEE");
invoke(
&create_instruction(
⋮----
vec![
⋮----
.unwrap();
⋮----
let byte_offset = usize::from_le_bytes(instruction_data[1..9].try_into().unwrap());
unsafe { *account.data.borrow_mut().get_unchecked_mut(byte_offset) = 42 };
⋮----
msg!("TEST_FORBID_WRITE_AFTER_OWNERSHIP_CHANGE_IN_CALLEE_NESTED");
⋮----
account.data.borrow_mut().fill(0);
account.assign(&system_program::id());
⋮----
msg!("TEST_FORBID_WRITE_AFTER_OWNERSHIP_CHANGE_IN_CALLER");
⋮----
account.assign(invoked_program_id);
⋮----
msg!("TEST_FORBID_LEN_UPDATE_AFTER_OWNERSHIP_CHANGE_MOVING_DATA_POINTER");
⋮----
account.resize(0).unwrap();
account.assign(realloc_program_id);
⋮----
account.data.borrow_mut().as_mut_ptr() as *mut RcBox<RefCell<&mut [u8]>>;
⋮----
account.data.borrow_mut().as_mut_ptr().add(rc_box_size),
⋮----
unsafe { account.data.borrow_mut().as_mut_ptr().offset(-8) as *mut u64 };
⋮----
overwrite_account_data(
⋮----
let mut instruction_data = vec![REALLOC, 0];
instruction_data.extend_from_slice(&rc_box_size.to_le_bytes());
assert_eq!(account.data_len(), 0);
⋮----
instruction_data.to_vec(),
⋮----
assert_eq!(account.data_len(), rc_box_size);
⋮----
overwrite_account_data(account, Rc::new(RefCell::new(&mut [])));
⋮----
msg!("TEST_FORBID_LEN_UPDATE_AFTER_OWNERSHIP_CHANGE");
⋮----
target_account.resize(0).unwrap();
target_account.assign(realloc_program_id);
⋮----
target_account.data.borrow_mut().as_mut_ptr() as *mut RcBox<RefCell<&mut [u8]>>;
⋮----
account.data.borrow_mut().as_mut_ptr(),
⋮----
msg!("TEST_ALLOW_WRITE_AFTER_OWNERSHIP_CHANGE_TO_CALLER");
⋮----
vec![ASSIGN_ACCOUNT_TO_CALLER],
⋮----
.borrow_mut()
.get_unchecked_mut(instruction_data[1] as usize) = 42
⋮----
msg!("TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS");
⋮----
let prev_len = account.data_len();
account.resize(prev_len + data.len())?;
account.data.borrow_mut()[prev_len..].copy_from_slice(data);
account.data.borrow().to_vec()
⋮----
let mut instruction_data = vec![TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_NESTED];
instruction_data.extend_from_slice(&expected);
⋮----
msg!("TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_NESTED");
⋮----
assert_eq!(*account.data.borrow(), &instruction_data[1..]);
⋮----
msg!("TEST_CPI_ACCOUNT_UPDATE_CALLEE_GROWS");
⋮----
let mut instruction_data = instruction_data.to_vec();
let mut expected = account.data.borrow().to_vec();
expected.extend_from_slice(&instruction_data[1..]);
⋮----
assert_eq!(&*account.data.borrow(), &expected);
⋮----
msg!("TEST_CPI_ACCOUNT_UPDATE_CALLEE_SHRINKS_SMALLER_THAN_ORIGINAL_LEN");
⋮----
let new_len = usize::from_le_bytes(instruction_data[2..10].try_into().unwrap());
⋮----
let expected = account.data.borrow()[..new_len].to_vec();
⋮----
instruction_data.extend_from_slice(&new_len.to_le_bytes());
⋮----
msg!("TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_CALLEE_SHRINKS");
⋮----
.as_mut_ptr()
.add(prev_len + data.len()) = SENTINEL;
⋮----
expected.extend_from_slice(&instruction_data[10..]);
⋮----
vec![TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_CALLEE_SHRINKS_NESTED];
⋮----
assert_eq!(*account.data.borrow(), &prev_data[..new_len]);
⋮----
msg!("TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS_CALLEE_SHRINKS_NESTED");
⋮----
let new_len = usize::from_le_bytes(instruction_data[1..9].try_into().unwrap());
account.resize(new_len).unwrap();
⋮----
msg!("TEST_CPI_INVALID_KEY_POINTER");
⋮----
fn overwrite_account_key(account: &AccountInfo, key: *const Pubkey) {
⋮----
overwrite_account_key(&accounts[ARGUMENT_INDEX], key as *const Pubkey);
⋮----
msg!("TEST_CPI_INVALID_LAMPORTS_POINTER");
⋮----
let mut lamports = account.lamports();
⋮----
.replace(unsafe { mem::transmute::<&'_ mut u64, &'a mut u64>(&mut lamports) });
⋮----
msg!("TEST_CPI_INVALID_OWNER_POINTER");
⋮----
fn overwrite_account_owner(account: &AccountInfo, owner: *const Pubkey) {
⋮----
overwrite_account_owner(account, owner as *const Pubkey);
⋮----
msg!("TEST_CPI_INVALID_DATA_POINTER");
⋮----
slice::from_raw_parts_mut(account.data.borrow_mut()[1..].as_mut_ptr(), 0)
⋮----
account.data.replace(data);
⋮----
msg!("TEST_READ_ACCOUNT");
⋮----
let byte_index = usize::from_le_bytes(instruction_data[2..10].try_into().unwrap());
⋮----
*(account.data.borrow().get_unchecked(byte_index) as *const u8).cast::<u64>()
⋮----
assert_eq!(data, 0);
⋮----
msg!("TEST_WRITE_ACCOUNT");
⋮----
target_account.data.borrow_mut()[byte_index] = instruction_data[10];
⋮----
msg!("TEST_CALLEE_ACCOUNT_UPDATES");
if instruction_data.len() < 3 + 3 * std::mem::size_of::<usize>() {
return Ok(());
⋮----
let resize = usize::from_le_bytes(instruction_data[3..11].try_into().unwrap());
⋮----
usize::from_le_bytes(instruction_data[11..19].try_into().unwrap());
⋮----
usize::from_le_bytes(instruction_data[19..27].try_into().unwrap());
⋮----
let prev = accounts[ARGUMENT_INDEX].try_borrow_data().unwrap().as_ptr();
let data = accounts[ARGUMENT_INDEX].try_borrow_data().unwrap().to_vec();
let old = accounts[ARGUMENT_INDEX].data.replace(data.leak());
let post = accounts[ARGUMENT_INDEX].try_borrow_data().unwrap().as_ptr();
⋮----
panic!("failed to clone the data");
⋮----
Some(old)
⋮----
account.resize(resize).unwrap();
⋮----
account.data.borrow_mut()[pre_write_offset] ^= 0xe5;
⋮----
if !invoke_struction.is_empty() {
⋮----
invoke_struction.to_vec(),
⋮----
account.data.borrow_mut()[post_write_offset] ^= 0xe5;
⋮----
msg!("TEST_STACK_HEAP_ZEROED");
⋮----
let heap_len = usize::from_le_bytes(instruction_data[1..9].try_into().unwrap());
⋮----
let pos = usize::from_le_bytes(heap[0..8].try_into().unwrap())
.saturating_sub(MM_HEAP_START as usize);
assert!(heap[8..pos] == ZEROS[8..pos], "heap not zeroed");
heap[8..pos].fill(42);
⋮----
assert!(stack == &ZEROS[..STACK_FRAME_SIZE], "stack not zeroed");
stack.fill(42);
⋮----
assert_eq!(sol_memcmp(stack, &ZEROS, ZEROED_BYTES_LENGTH), 0);
stack[..ZEROED_BYTES_LENGTH].fill(42);
⋮----
msg!("TEST_ACCOUNT_INFO_IN_ACCOUNT");
let account_offset = usize::from_le_bytes(instruction_data[1..9].try_into().unwrap());
let mut instruction_data = vec![TEST_WRITE_ACCOUNT, 1];
instruction_data.extend_from_slice(&1u64.to_le_bytes());
instruction_data.push(1);
let data = accounts[ARGUMENT_INDEX].data.borrow().as_ptr();
let len = accounts.len();
⋮----
std::slice::from_raw_parts_mut(data.add(account_offset) as *mut AccountInfo, len)
⋮----
accounts.as_ptr(),
account_info_in_account.as_mut_ptr(),
⋮----
msg!("TEST_ACCOUNT_INFO_LAMPORTS_RC_IN_ACCOUNT");
let mut account0 = accounts[0].clone();
let account1 = accounts[1].clone();
let account2 = accounts[2].clone();
⋮----
let dst = account1.data.borrow_mut().as_mut_ptr();
⋮----
msg!("TEST_ACCOUNT_INFO_DATA_RC_IN_ACCOUNT");
⋮----
msg!("Test minimum cost of a CPI invocation with 1 account meta and 1 account info");
let account_infos: Vec<AccountInfo<'_>> = vec![accounts[NOOP_PROGRAM_INDEX].clone()];
⋮----
vec![(accounts[NOOP_PROGRAM_INDEX].key, false, false)];
⋮----
create_instruction(*accounts[NOOP_PROGRAM_INDEX].key, &account_metas, vec![]);
let before_cpi = sol_remaining_compute_units();
⋮----
let after_cpi = sol_remaining_compute_units();
⋮----
//need to use upper bound here, as different versions of sbpf add/remove speciliazed intructions hence leading to different CU usage
⋮----
panic!("CU used more than baseline");
⋮----
msg!(
⋮----
let mut selected_indices: Vec<usize> = vec![NOOP_PROGRAM_INDEX];
while selected_indices.len() < 64 {
selected_indices.push(selected_indices[0]);
⋮----
.iter()
.map(|&i| accounts[i].clone())
.collect();
⋮----
account_metas.push((accounts[NOOP_PROGRAM_INDEX].key, false, false));
while account_metas.len() < 255 {
⋮----
panic!("CU usedmore than baseline");
⋮----
_ => panic!("unexpected program data"),
⋮----
struct RcBox<T> {
⋮----
unsafe fn overwrite_account_data(account: &AccountInfo, data: Rc<RefCell<&mut [u8]>>) {

================
File: programs/sbf/rust/invoke/Cargo.toml
================
[package]
name = "solana-sbf-rust-invoke"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-program-memory = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbf-rust-invoke-dep = { workspace = true }
solana-sbf-rust-invoked-dep = { workspace = true }
solana-sbf-rust-realloc-dep = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-system-interface = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/invoke_and_error/src/lib.rs
================
fn process_instruction(
⋮----
.iter()
.map(|acc| AccountMeta {
⋮----
.collect(),
data: instruction_data.to_owned(),
⋮----
let _ = invoke(&instruction, infos);
Err(42.into())

================
File: programs/sbf/rust/invoke_and_error/Cargo.toml
================
[package]
name = "solana-sbf-rust-invoke-and-error"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/invoke_and_ok/src/lib.rs
================
fn process_instruction(
⋮----
.iter()
.map(|acc| AccountMeta {
⋮----
.collect(),
data: instruction_data.to_owned(),
⋮----
let _ = invoke(&instruction, infos);
Ok(())

================
File: programs/sbf/rust/invoke_and_ok/Cargo.toml
================
[package]
name = "solana-sbf-rust-invoke-and-ok"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/invoke_and_return/src/lib.rs
================
fn process_instruction(
⋮----
.iter()
.map(|acc| AccountMeta {
⋮----
.collect(),
data: instruction_data.to_owned(),
⋮----
invoke(&instruction, &accounts[1..])

================
File: programs/sbf/rust/invoke_and_return/Cargo.toml
================
[package]
name = "solana-sbf-rust-invoke-and-return"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/invoke_dep/src/lib.rs
================


================
File: programs/sbf/rust/invoke_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-invoke-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["lib"]

================
File: programs/sbf/rust/invoked/src/lib.rs
================
fn process_instruction(
⋮----
msg!("Invoked program");
if instruction_data.is_empty() {
return Ok(());
⋮----
assert_eq!(get_return_data(), None);
⋮----
msg!("verify data translations");
⋮----
assert_eq!(&instruction_data[1..], &[1, 2, 3, 4, 5]);
assert_eq!(accounts.len(), 4);
assert_eq!(accounts[ARGUMENT_INDEX].lamports(), 42);
assert_eq!(accounts[ARGUMENT_INDEX].data_len(), 100);
assert!(accounts[ARGUMENT_INDEX].is_signer);
assert!(accounts[ARGUMENT_INDEX].is_writable);
assert!(!accounts[ARGUMENT_INDEX].executable);
⋮----
let data = accounts[ARGUMENT_INDEX].try_borrow_data()?;
⋮----
assert_eq!(data[i as usize], i);
⋮----
assert_eq!(
⋮----
assert_eq!(accounts[INVOKED_ARGUMENT_INDEX].lamports(), 20);
assert_eq!(accounts[INVOKED_ARGUMENT_INDEX].data_len(), 10);
assert!(accounts[INVOKED_ARGUMENT_INDEX].is_signer);
assert!(accounts[INVOKED_ARGUMENT_INDEX].is_writable);
assert!(!accounts[INVOKED_ARGUMENT_INDEX].executable);
assert_eq!(accounts[INVOKED_PROGRAM_INDEX].key, program_id);
assert_eq!(accounts[INVOKED_PROGRAM_INDEX].owner, &loader_v4::id());
assert!(!accounts[INVOKED_PROGRAM_INDEX].is_signer);
assert!(!accounts[INVOKED_PROGRAM_INDEX].is_writable);
assert!(accounts[INVOKED_PROGRAM_INDEX].executable);
⋮----
let data = accounts[INVOKED_PROGRAM_INDEX].try_borrow_data()?;
assert!(accounts[INVOKED_PROGRAM_DUP_INDEX]
⋮----
sol_log_64(data[0] as u64, 0, 0, 0, 0);
⋮----
msg!("Ok");
⋮----
msg!("return error");
return Err(ProgramError::Custom(42));
⋮----
msg!("verify derived signers");
⋮----
assert!(accounts[DERIVED_KEY1_INDEX].is_signer);
assert!(!accounts[DERIVED_KEY2_INDEX].is_signer);
assert!(!accounts[DERIVED_KEY3_INDEX].is_signer);
⋮----
let invoked_instruction = create_instruction(
⋮----
vec![VERIFY_NESTED_SIGNERS],
⋮----
invoke_signed(
⋮----
&[accounts[DERIVED_KEY2_INDEX].key.as_ref(), &[bump_seed3]],
⋮----
msg!("verify nested derived signers");
⋮----
assert!(!accounts[DERIVED_KEY1_INDEX].is_signer);
assert!(accounts[DERIVED_KEY2_INDEX].is_signer);
assert!(accounts[DERIVED_KEY3_INDEX].is_signer);
⋮----
msg!("verify writable");
⋮----
assert!(!accounts[ARGUMENT_INDEX].is_writable);
⋮----
msg!("Verify privilege escalation");
⋮----
msg!("verify privilege deescalation");
⋮----
assert!(!accounts[INVOKED_ARGUMENT_INDEX].is_signer);
assert!(!accounts[INVOKED_ARGUMENT_INDEX].is_writable);
⋮----
msg!("verify privilege deescalation escalation signer");
⋮----
vec![VERIFY_PRIVILEGE_ESCALATION],
⋮----
invoke(&invoked_instruction, accounts)?;
⋮----
msg!("verify privilege deescalation escalation writable");
⋮----
msg!("nested invoke");
⋮----
assert!(instruction_data.len() > 1);
**accounts[INVOKED_ARGUMENT_INDEX].lamports.borrow_mut() -= 1;
**accounts[ARGUMENT_INDEX].lamports.borrow_mut() += 1;
⋮----
msg!("Invoke again");
⋮----
vec![NESTED_INVOKE, remaining_invokes - 1],
⋮----
msg!("Last invoked");
⋮----
let mut data = accounts[INVOKED_ARGUMENT_INDEX].try_borrow_mut_data()?;
⋮----
msg!("write account");
⋮----
accounts[ARGUMENT_INDEX].data.borrow_mut()[i as usize] = instruction_data[1];
⋮----
msg!("Create and init data");
⋮----
let from_lamports = accounts[FROM_INDEX].lamports();
let to_lamports = accounts[DERIVED_KEY2_INDEX].lamports();
assert_eq!(accounts[DERIVED_KEY2_INDEX].data_len(), 0);
assert!(solana_system_interface::program::check_id(
⋮----
assert_eq!(accounts[FROM_INDEX].lamports(), from_lamports - 1);
assert_eq!(accounts[DERIVED_KEY2_INDEX].lamports(), to_lamports + 1);
assert_eq!(program_id, accounts[DERIVED_KEY2_INDEX].owner);
⋮----
let mut data = accounts[DERIVED_KEY2_INDEX].try_borrow_mut_data()?;
assert_eq!(data[0], 0);
⋮----
assert_eq!(data[0], 0x0e);
assert_eq!(data[MAX_PERMITTED_DATA_INCREASE - 1], 0);
⋮----
assert_eq!(data[MAX_PERMITTED_DATA_INCREASE - 1], 0x0f);
⋮----
msg!("Set return data");
set_return_data(b"Set by invoked");
⋮----
msg!("Assigning account to caller");
⋮----
account.assign(caller_program_id);
⋮----
_ => panic!(),
⋮----
Ok(())

================
File: programs/sbf/rust/invoked/Cargo.toml
================
[package]
name = "solana-sbf-rust-invoked"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbf-rust-invoked-dep = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-system-interface = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/invoked_dep/src/lib.rs
================
pub fn create_instruction(
⋮----
.iter()
.map(|(key, is_writable, is_signer)| {
⋮----
.collect();

================
File: programs/sbf/rust/invoked_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-invoked-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[dependencies]
solana-instruction = { workspace = true }
solana-pubkey = { workspace = true }

[lib]
crate-type = ["lib"]

================
File: programs/sbf/rust/iter/src/lib.rs
================
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
⋮----
for v in ones.iter() {
⋮----
sol_log_64(0xff, 0, 0, 0, sum);
assert_eq!(sum, ITERS as u64);
⋮----
custom_heap_default!();
custom_panic_default!();
⋮----
mod test {
⋮----
fn test_entrypoint() {
assert_eq!(SUCCESS, entrypoint(std::ptr::null_mut()));

================
File: programs/sbf/rust/iter/Cargo.toml
================
[package]
name = "solana-sbf-rust-iter"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/log_data/src/lib.rs
================
fn process_instruction(
⋮----
let fields: Vec<&[u8]> = instruction_data.split(|e| *e == 0).collect();
set_return_data(&[0x08, 0x01, 0x44]);
sol_log_data(&fields);
Ok(())

================
File: programs/sbf/rust/log_data/Cargo.toml
================
[package]
name = "solana-sbf-rust-log-data"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/many_args/src/helper.rs
================
pub fn many_args(
⋮----
sol_log("same package");
sol_log_64(arg1, arg2, arg3, arg4, arg5);
sol_log_64(arg6, arg7, arg8, arg9, 0);

================
File: programs/sbf/rust/many_args/src/lib.rs
================
mod helper;
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
msg!("Call same package");
assert_eq!(crate::helper::many_args(1, 2, 3, 4, 5, 6, 7, 8, 9), 45);
msg!("Call another package");
assert_eq!(
⋮----
custom_heap_default!();
custom_panic_default!();
⋮----
mod test {
⋮----
fn test_entrypoint() {
assert_eq!(SUCCESS, entrypoint(std::ptr::null_mut()));

================
File: programs/sbf/rust/many_args/Cargo.toml
================
[package]
name = "solana-sbf-rust-many-args"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-sbf-rust-many-args-dep = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/many_args_dep/src/lib.rs
================
pub fn many_args(
⋮----
msg!("Another package - many_args");
sol_log_64(arg1, arg2, arg3, arg4, arg5);
sol_log_64(arg6, arg7, arg8, arg9, 0);
⋮----
pub struct Ret {
⋮----
pub fn many_args_sret(
⋮----
msg!("Another package - many_args_sret");
⋮----
mod test {
⋮----
fn test_many_args() {
assert_eq!(45, many_args(1, 2, 3, 4, 5, 6, 7, 8, 9));
⋮----
fn test_sret() {
assert_eq!(

================
File: programs/sbf/rust/many_args_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-many-args-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[dependencies]
solana-msg = { workspace = true }
solana-program = { workspace = true }

================
File: programs/sbf/rust/mem/src/lib.rs
================
pub fn process_instruction(
⋮----
struct MemOpSyscalls();
impl MemOps for MemOpSyscalls {
unsafe fn memcpy(&self, dst: &mut [u8], src: &[u8], n: usize) {
sol_memcpy(dst, src, n)
⋮----
unsafe fn memmove(&self, dst: *mut u8, src: *mut u8, n: usize) {
sol_memmove(dst, src, n)
⋮----
unsafe fn memset(&self, s: &mut [u8], c: u8, n: usize) {
sol_memset(s, c, n)
⋮----
unsafe fn memcmp(&self, s1: &[u8], s2: &[u8], n: usize) -> i32 {
sol_memcmp(s1, s2, n)
⋮----
run_mem_tests(MemOpSyscalls::default());
Ok(())

================
File: programs/sbf/rust/mem/Cargo.toml
================
[package]
name = "solana-sbf-rust-mem"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-program-memory = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbf-rust-mem-dep = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/mem_dep/src/lib.rs
================
pub trait MemOps {
⋮----
pub fn run_mem_tests<T: MemOps>(mem_ops: T) {
⋮----
mem_ops.memcpy(dst, src, 1);
assert_eq!(&src[..1], dst);
⋮----
mem_ops.memcpy(dst, src, 3);
assert_eq!(&src[..3], dst);
⋮----
mem_ops.memcpy(dst, src, 8);
assert_eq!(&src[..8], dst);
⋮----
mem_ops.memcpy(dst, src, 9);
assert_eq!(&src[..9], dst);
⋮----
mem_ops.memcpy(dst, src, 16);
assert_eq!(&src[..16], dst);
⋮----
mem_ops.memcpy(dst, src, 18);
assert_eq!(&src[..18], dst);
⋮----
mem_ops.memcpy(dst, &src[1..], 17);
assert_eq!(&src[1..], &dst[..17]);
⋮----
mem_ops.memcpy(&mut dst[1..], &src[1..], 17);
assert_eq!(&src[1..], &dst[1..]);
⋮----
mem_ops.memmove(&mut buf[0] as *mut u8, &mut buf[1] as *mut u8, 1);
assert_eq!(buf[0], buf[1]);
⋮----
mem_ops.memmove(&mut buf[1] as *mut u8, &mut buf[0] as *mut u8, 1);
⋮----
mem_ops.memmove(&mut buf[0] as *mut u8, &mut buf[3] as *mut u8, 3);
assert_eq!(buf[..3], buf[3..]);
⋮----
mem_ops.memmove(&mut buf[3] as *mut u8, &mut buf[0] as *mut u8, 3);
⋮----
mem_ops.memmove(&mut buf[0] as *mut u8, &mut buf[8] as *mut u8, 8);
assert_eq!(buf[..8], buf[8..]);
⋮----
mem_ops.memmove(&mut buf[8] as *mut u8, &mut buf[0] as *mut u8, 8);
⋮----
mem_ops.memmove(&mut buf[0] as *mut u8, &mut buf[9] as *mut u8, 9);
assert_eq!(buf[..9], buf[9..]);
⋮----
mem_ops.memmove(&mut buf[1] as *mut u8, &mut buf[0] as *mut u8, 9);
assert_eq!(&mut [0_u8, 0, 1, 2, 3, 4, 5, 6, 7, 8], buf);
⋮----
mem_ops.memmove(&mut buf[9] as *mut u8, &mut buf[0] as *mut u8, 9);
⋮----
mem_ops.memmove(&mut buf[0] as *mut u8, &mut buf[16] as *mut u8, 16);
assert_eq!(buf[..16], buf[16..]);
⋮----
mem_ops.memmove(&mut buf[16] as *mut u8, &mut buf[0] as *mut u8, 16);
⋮----
mem_ops.memmove(&mut buf[0] as *mut u8, &mut buf[18] as *mut u8, 18);
assert_eq!(buf[..18], buf[18..]);
⋮----
mem_ops.memmove(&mut buf[18] as *mut u8, &mut buf[0] as *mut u8, 18);
⋮----
mem_ops.memmove(&mut buf[1] as *mut u8, &mut buf[18] as *mut u8, 17);
assert_eq!(buf[1..17], buf[18..34]);
⋮----
mem_ops.memmove(&mut buf[19] as *mut u8, &mut buf[1] as *mut u8, 17);
assert_eq!(buf[..17], buf[19..]);
⋮----
mem_ops.memmove(&mut buf[0] as *mut u8, &mut buf[3] as *mut u8, 5);
assert_eq!(buf, &mut [1, 1, 1, 1, 1, 1, 1, 1, 0]);
⋮----
mem_ops.memset(&mut buf[0..], 1, 1);
assert_eq!(exp[..1], buf[..1]);
mem_ops.memset(&mut buf[0..], 1, 3);
assert_eq!(exp[..3], buf[..3]);
mem_ops.memset(&mut buf[0..], 1, 8);
assert_eq!(exp[..8], buf[..8]);
mem_ops.memset(&mut buf[0..], 1, 9);
assert_eq!(exp[..9], buf[..9]);
mem_ops.memset(&mut buf[0..], 1, 16);
assert_eq!(exp[..16], buf[..16]);
mem_ops.memset(&mut buf[0..], 1, 18);
assert_eq!(exp[..18], buf[..18]);
mem_ops.memset(&mut buf[1..], 1, 17);
assert_eq!(exp[1..18], buf[1..18]);
⋮----
assert_eq!(-1, mem_ops.memcmp(&[0_u8], &[1_u8], 1));
assert_eq!(-1, mem_ops.memcmp(&[0_u8, 0, 0], &[0_u8, 0, 1], 3));
assert_eq!(
⋮----
assert_eq!(0, mem_ops.memcmp(&[0_u8; 8], &[0_u8; 8], 8));
assert_eq!(-1, mem_ops.memcmp(&[0_u8; 8], &[1_u8; 8], 8));
assert_eq!(-1, mem_ops.memcmp(&[0_u8; 16], &[1_u8; 16], 16));
assert_eq!(-1, mem_ops.memcmp(&[0_u8; 18], &[1_u8; 18], 18));
⋮----
assert_eq!(-1, mem_ops.memcmp(&one[1..], &two[0..], 17));
assert_eq!(-1, mem_ops.memcmp(&one[1..], &two[1..], 17));

================
File: programs/sbf/rust/mem_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-mem-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["lib"]

[dependencies]

================
File: programs/sbf/rust/membuiltins/src/lib.rs
================
extern crate compiler_builtins;
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
⋮----
struct MemOpSyscalls();
impl MemOps for MemOpSyscalls {
unsafe fn memcpy(&self, dst: &mut [u8], src: &[u8], n: usize) {
⋮----
compiler_builtins::mem::memcpy(dst.as_mut_ptr(), src.as_ptr(), n);
⋮----
unsafe fn memmove(&self, dst: *mut u8, src: *mut u8, n: usize) {
⋮----
unsafe fn memset(&self, s: &mut [u8], c: u8, n: usize) {
⋮----
compiler_builtins::mem::memset(s.as_mut_ptr(), c as i32, n);
⋮----
unsafe fn memcmp(&self, s1: &[u8], s2: &[u8], n: usize) -> i32 {
unsafe { compiler_builtins::mem::memcmp(s1.as_ptr(), s2.as_ptr(), n) }
⋮----
run_mem_tests(mem_ops);
⋮----
custom_heap_default!();
custom_panic_default!();

================
File: programs/sbf/rust/membuiltins/Cargo.toml
================
[package]
name = "solana-sbf-rust-membuiltins"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-program-entrypoint = { workspace = true }
solana-sbf-rust-mem-dep = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/noop/src/lib.rs
================
fn process_instruction(
⋮----
Ok(())

================
File: programs/sbf/rust/noop/Cargo.toml
================
[package]
name = "solana-sbf-rust-noop"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/panic/src/lib.rs
================
fn custom_panic(info: &core::panic::PanicInfo<'_>) {
⋮----
fn process_instruction(
⋮----
assert_eq!(1, 2);
Ok(())

================
File: programs/sbf/rust/panic/Cargo.toml
================
[package]
name = "solana-sbf-rust-panic"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[features]
default = ["custom-panic"]
custom-panic = []

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/param_passing/src/lib.rs
================
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
⋮----
sol_log_64(0, 0, 0, 0, test_dep.thirty as u64);
assert!(test_dep.thirty == 30);
⋮----
custom_heap_default!();
custom_panic_default!();
⋮----
mod test {
⋮----
fn test_entrypoint() {
assert_eq!(SUCCESS, entrypoint(std::ptr::null_mut()));

================
File: programs/sbf/rust/param_passing/Cargo.toml
================
[package]
name = "solana-sbf-rust-param-passing"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-sbf-rust-param-passing-dep = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/param_passing_dep/src/lib.rs
================
pub struct Data<'a> {
⋮----
pub struct TestDep {
⋮----
impl TestDep {
pub fn new(data: &Data<'_>, _one: u64, _two: u64, _three: u64, _four: u64, five: u64) -> Self {
⋮----
mod test {
⋮----
fn test_dep() {
⋮----
assert_eq!(TestDep { thirty: 30 }, TestDep::new(&data, 1, 2, 3, 4, 5));

================
File: programs/sbf/rust/param_passing_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-param-passing-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[dependencies]

================
File: programs/sbf/rust/poseidon/src/lib.rs
================
fn test_poseidon_input_ones_twos() -> Result<(), PoseidonSyscallError> {
⋮----
let hash = hashv(
⋮----
assert_eq!(
⋮----
Ok(())
⋮----
fn test_poseidon_input_one() -> Result<(), PoseidonSyscallError> {
⋮----
let hash = hashv(Parameters::Bn254X5, Endianness::BigEndian, &[&input])?;
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
msg!("poseidon_hash");
if let Err(e) = test_poseidon_input_ones_twos() {
return e.into();
⋮----
if let Err(e) = test_poseidon_input_one() {
⋮----
custom_heap_default!();
custom_panic_default!();

================
File: programs/sbf/rust/poseidon/Cargo.toml
================
[package]
name = "solana-sbf-rust-poseidon"
description = "Solana SBF test program written in Rust"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
array-bytes = { workspace = true }
solana-msg = { workspace = true }
solana-poseidon = { workspace = true, features = ["agave-unstable-api"] }
solana-program-entrypoint = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/r2_instruction_data_pointer/src/lib.rs
================
pub unsafe extern "C" fn entrypoint(_input: *mut u8, instruction_data_addr: *const u8) -> u64 {

================
File: programs/sbf/rust/r2_instruction_data_pointer/Cargo.toml
================
[package]
name = "solana-sbf-rust-r2-instruction-data-pointer"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-cpi = { workspace = true }
solana-program-entrypoint = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/rand/src/lib.rs
================
fn process_instruction(
⋮----
msg!("rand");
Ok(())

================
File: programs/sbf/rust/rand/Cargo.toml
================
[package]
name = "solana-sbf-rust-rand"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
getrandom = { workspace = true, features = ["custom"] }
rand = { workspace = true }
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/realloc/src/lib.rs
================
fn process_instruction(
⋮----
let (bytes, _) = instruction_data[2..].split_at(std::mem::size_of::<usize>());
let new_len = usize::from_le_bytes(bytes.try_into().unwrap());
msg!("realloc to {}", new_len);
account.resize(new_len)?;
assert_eq!(new_len, account.data_len());
⋮----
let pre_len = account.data_len();
⋮----
let new_len = pre_len.saturating_add(usize::from_le_bytes(bytes.try_into().unwrap()));
msg!("realloc extend by {}", new_len);
⋮----
msg!("undo realloc");
account.resize(pre_len)?;
assert_eq!(pre_len, account.data_len());
⋮----
let (bytes, _) = instruction_data[4..].split_at(std::mem::size_of::<usize>());
⋮----
account.try_borrow_mut_data()?[pre_len..].fill(fill);
⋮----
assert!(pre_len < new_len);
⋮----
let (bytes, _) = instruction_data[1..].split_at(new_len);
let value = u64::from_le_bytes(bytes.try_into().unwrap());
msg!(
⋮----
.try_borrow_mut_data()
.unwrap()
.as_mut_ptr()
⋮----
assert_eq!(
⋮----
msg!("realloc and assign");
account.resize(MAX_PERMITTED_DATA_INCREASE)?;
assert_eq!(MAX_PERMITTED_DATA_INCREASE, account.data_len());
account.assign(&system_program::id());
assert_eq!(*account.owner, system_program::id());
⋮----
msg!("realloc and assign to self via system program");
⋮----
let new_len = pre_len.saturating_add(MAX_PERMITTED_DATA_INCREASE);
⋮----
let mut data = account.data.borrow_mut();
let data_ptr = data.as_mut_ptr();
*(data_ptr.offset(-8) as *mut u64) = new_len as u64;
⋮----
invoke(
⋮----
assert_eq!(account.owner, program_id);
⋮----
msg!("assign to self via system program and realloc");
⋮----
account.resize(pre_len.saturating_add(MAX_PERMITTED_DATA_INCREASE))?;
⋮----
msg!("dealloc and assign to caller");
account.resize(0)?;
assert_eq!(account.data_len(), 0);
account.assign(accounts[1].key);
assert_eq!(account.owner, accounts[1].key);
⋮----
msg!("check");
assert_eq!(100, account.data_len());
let data = account.try_borrow_mut_data()?;
for x in data[0..5].iter() {
assert_eq!(0, *x);
⋮----
for x in data[5..].iter() {
assert_eq!(2, *x);
⋮----
account.resize(10)?;
⋮----
let mut data = account.try_borrow_mut_data()?;
⋮----
assert_eq!(0, data[i]);
⋮----
data.fill(1);
⋮----
assert_eq!(1, data[i]);
⋮----
account.resize(5)?;
⋮----
let data = account.try_borrow_data()?;
⋮----
msg!("realloc extend from slice");
⋮----
let prev_len = account.data_len();
account.resize(prev_len.saturating_add(data.len()))?;
account.data.borrow_mut()[prev_len..].copy_from_slice(data);
⋮----
_ => panic!(),
⋮----
Ok(())

================
File: programs/sbf/rust/realloc/Cargo.toml
================
[package]
name = "solana-sbf-rust-realloc"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbf-rust-realloc-dep = { workspace = true }
solana-system-interface = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/realloc_dep/src/lib.rs
================
pub fn realloc(program_id: &Pubkey, address: &Pubkey, size: usize, bump: &mut u8) -> Instruction {
let mut instruction_data = vec![REALLOC, *bump];
instruction_data.extend_from_slice(&size.to_le_bytes());
*bump = bump.saturating_add(1);
⋮----
vec![AccountMeta::new(*address, false)],
⋮----
pub fn realloc_extend(
⋮----
let mut instruction_data = vec![REALLOC_EXTEND, *bump];
⋮----
pub fn realloc_extend_and_fill(
⋮----
let mut instruction_data = vec![
⋮----
pub fn realloc_extend_and_undo(
⋮----
let mut instruction_data = vec![REALLOC_EXTEND_AND_UNDO, *bump];
⋮----
pub fn extend_and_write_u64(program_id: &Pubkey, address: &Pubkey, value: u64) -> Instruction {
let mut instruction_data = vec![EXTEND_AND_WRITE_U64];
instruction_data.extend_from_slice(&value.to_le_bytes());

================
File: programs/sbf/rust/realloc_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-realloc-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[dependencies]
solana-instruction = { workspace = true }
solana-pubkey = { workspace = true }

[lib]
crate-type = ["lib"]

================
File: programs/sbf/rust/realloc_invoke/src/lib.rs
================
fn process_instruction(
⋮----
let pre_len = account.data_len();
⋮----
msg!("invoke realloc to zero of ro account");
let mut instruction = realloc(invoke_program_id, account.key, 0, &mut bump);
⋮----
invoke(&instruction, accounts)?;
⋮----
msg!("invoke realloc to zero");
invoke(
&realloc(invoke_program_id, account.key, 0, &mut bump),
⋮----
assert_eq!(0, account.data_len());
⋮----
msg!("invoke realloc max + 1");
⋮----
&realloc(
⋮----
MAX_PERMITTED_DATA_INCREASE.saturating_add(1),
⋮----
msg!("invoke realloc max");
⋮----
&realloc_extend(
⋮----
assert_eq!(
⋮----
instruction_data[2..].split_at(std::mem::size_of::<usize>());
let new_len = usize::from_le_bytes(bytes.try_into().unwrap());
msg!("invoke realloc to {} byte(s)", new_len);
⋮----
let mut instruction_data = vec![INVOKE_REALLOC_TO, 1];
instruction_data.extend_from_slice(&new_len.to_le_bytes());
⋮----
vec![
⋮----
invoke(&realloc_to_ix, accounts)?;
assert_eq!(new_len, account.data_len());
let (bytes, _) = remaining_data.split_at(std::mem::size_of::<usize>());
let extend_len = usize::from_le_bytes(bytes.try_into().unwrap());
msg!("realloc extend {} byte(s)", extend_len);
account.resize(new_len.saturating_add(extend_len))?;
assert_eq!(new_len.saturating_add(extend_len), account.data_len());
⋮----
let cpi_len = usize::from_le_bytes(bytes.try_into().unwrap());
msg!("realloc to {} byte(s)", cpi_len);
account.resize(cpi_len)?;
msg!("invoke");
⋮----
instruction_data.extend_from_slice(&cpi_len.to_le_bytes());
⋮----
assert_eq!(cpi_len, account.data_len());
⋮----
let final_len = usize::from_le_bytes(bytes.try_into().unwrap());
msg!("realloc to {} byte(s)", final_len.saturating_add(1));
⋮----
.try_borrow_mut_data()?
.as_mut_ptr()
.add(final_len.saturating_add(1)) = 0xAA;
⋮----
msg!("realloc to {} byte(s)", final_len);
⋮----
*(account.try_borrow_mut_data()?.as_mut_ptr().offset(-8) as *mut u64) =
⋮----
msg!("invoke realloc max twice");
⋮----
let new_len = pre_len.saturating_add(MAX_PERMITTED_DATA_INCREASE);
⋮----
account.resize(new_len.saturating_add(MAX_PERMITTED_DATA_INCREASE))?;
⋮----
msg!("invoke realloc and assign");
⋮----
vec![AccountMeta::new(*account.key, false)],
⋮----
assert_eq!(*account.owner, system_program::id());
⋮----
msg!("invoke realloc and assign to self via system program");
⋮----
msg!("invoke assign to self and realloc via system program");
⋮----
msg!("realloc invoke check size");
account.resize(100)?;
assert_eq!(100, account.data_len());
account.try_borrow_mut_data()?[pre_len..].fill(2);
⋮----
let (bytes, _) = instruction_data[2..].split_at(std::mem::size_of::<usize>());
⋮----
msg!("realloc to {}", new_len);
account.resize(new_len)?;
⋮----
account.try_borrow_mut_data()?[pre_len..].fill(instruction_data[1]);
⋮----
msg!("realloc invoke recursive");
⋮----
let mut new_instruction_data = vec![];
new_instruction_data.extend_from_slice(&[INVOKE_REALLOC_TO, 2]);
new_instruction_data.extend_from_slice(&final_len.to_le_bytes());
⋮----
assert_eq!(final_len, account.data_len());
let data = account.try_borrow_mut_data()?;
⋮----
assert_eq!(data[i], instruction_data[1]);
⋮----
assert_eq!(data[i], new_instruction_data[1]);
⋮----
msg!("Create new account, realloc, and check");
⋮----
assert_eq!(pre_len, accounts[1].data_len());
accounts[1].resize(pre_len.saturating_add(1))?;
assert_eq!(pre_len.saturating_add(1), accounts[1].data_len());
assert_eq!(accounts[1].owner, program_id);
⋮----
assert_eq!(final_len, accounts[1].data_len());
⋮----
msg!("realloc zerod");
⋮----
let pre_len = usize::from_le_bytes(bytes.try_into().unwrap());
let new_len = pre_len.saturating_mul(2);
assert_eq!(pre_len, 100);
⋮----
assert_eq!(account.owner, program_id);
assert_eq!(account.data_len(), 0);
⋮----
assert_eq!(account.data_len(), new_len);
⋮----
assert_eq!(data[i], 0);
⋮----
msg!("invoke realloc max invoke max");
⋮----
account.resize(MAX_PERMITTED_DATA_INCREASE)?;
assert_eq!(MAX_PERMITTED_DATA_INCREASE, account.data_len());
account.assign(invoke_program_id);
assert_eq!(account.owner, invoke_program_id);
⋮----
msg!("invoke invoke max twice");
⋮----
account.assign(accounts[2].key);
assert_eq!(account.owner, accounts[2].key);
⋮----
panic!("last invoke should fail");
⋮----
_ => panic!(),
⋮----
Ok(())

================
File: programs/sbf/rust/realloc_invoke/Cargo.toml
================
[package]
name = "solana-sbf-rust-realloc-invoke"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbf-rust-realloc-dep = { workspace = true }
solana-sbf-rust-realloc-invoke-dep = { workspace = true }
solana-system-interface = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/realloc_invoke_dep/src/lib.rs
================


================
File: programs/sbf/rust/realloc_invoke_dep/Cargo.toml
================
[package]
name = "solana-sbf-rust-realloc-invoke-dep"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["lib"]

================
File: programs/sbf/rust/remaining_compute_units/src/lib.rs
================
pub fn process_instruction(
⋮----
if i.is_multiple_of(500) {
let remaining = sol_remaining_compute_units();
msg!("remaining compute units: {:?}", remaining);
⋮----
i = i.saturating_add(1);
⋮----
msg!("i: {:?}", i);
Ok(())

================
File: programs/sbf/rust/remaining_compute_units/Cargo.toml
================
[package]
name = "solana-sbf-rust-remaining-compute-units"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/ro_account_modify/src/lib.rs
================
fn process_instruction(
⋮----
assert!(!accounts[ARGUMENT_INDEX].is_writable);
⋮----
msg!("modify ro account");
assert_eq!(0, accounts[ARGUMENT_INDEX].try_borrow_data()?[0]);
accounts[ARGUMENT_INDEX].try_borrow_mut_data()?[0] = 1;
⋮----
msg!("invoke and modify ro account");
⋮----
accounts: vec![AccountMeta::new_readonly(
⋮----
data: vec![INSTRUCTION_MODIFY],
⋮----
invoke(&instruction, accounts)?;
⋮----
msg!("modify and invoke ro account");
⋮----
data: vec![INSTRUCTION_VERIFY_MODIFIED],
⋮----
msg!("verify modified");
assert_eq!(1, accounts[ARGUMENT_INDEX].try_borrow_data()?[0])
⋮----
_ => panic!("Unknown instruction"),
⋮----
Ok(())

================
File: programs/sbf/rust/ro_account_modify/Cargo.toml
================
[package]
name = "solana-sbf-rust-ro-account_modify"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/ro_modify/src/lib.rs
================
struct SolInstruction {
⋮----
struct SolAccountMeta {
⋮----
struct SolAccountInfo {
⋮----
fn check_preconditions(
⋮----
for (in_info, static_info) in in_infos.iter().zip(static_infos) {
check!(in_info.key.as_ref().as_ptr() as u64, static_info.key_addr);
check!(
⋮----
check!(in_info.data_len() as u64, static_info.data_len);
⋮----
Ok(())
⋮----
fn process_instruction(
⋮----
check_preconditions(accounts, READONLY_ACCOUNTS)?;
⋮----
accounts_addr: metas.as_ptr() as u64,
accounts_len: metas.len(),
data_addr: system_instruction.data.as_ptr() as u64,
data_len: system_instruction.data.len(),
⋮----
check!(42, unsafe { read_val(ptr) });
⋮----
let new_accounts = &mut [READONLY_ACCOUNTS[0].clone(), READONLY_ACCOUNTS[1].clone()];
⋮----
let mut new_account = accounts[1].clone();
⋮----
invoke(&instruction, &[accounts[0].clone(), new_account])?;
⋮----
_ => check!(0, 1),
⋮----
macro_rules! check {
⋮----
pub unsafe fn read_val<T: Copy>(ptr: *mut T) -> T {

================
File: programs/sbf/rust/ro_modify/Cargo.toml
================
[package]
name = "solana-sbf-rust-ro-modify"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-system-interface = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/sanity/src/lib.rs
================
struct SStruct {
⋮----
enum TestEnum {
⋮----
enum Test64BitEnum {
⋮----
fn return_sstruct() -> SStruct {
⋮----
pub fn process_instruction(
⋮----
msg!("Program identifier:");
program_id.log();
assert!(!bpf_loader::check_id(program_id));
msg!("Account keys and instruction input data:");
sol_log_params(accounts, instruction_data);
⋮----
let result_str = std::str::from_utf8(&sparkle_heart).unwrap();
assert_eq!(4, result_str.len());
assert_eq!("💖", result_str);
msg!(result_str);
⋮----
let s = return_sstruct();
assert_eq!(s.x + s.y + s.z, 6);
⋮----
panic!();
⋮----
let zero = accounts[0].try_borrow_mut_data()?.len() as f64;
⋮----
let num = num.powf(0.333f64);
assert!(1.9986f64 < num && num < 2.0f64);
⋮----
assert_eq!(std::mem::size_of::<TestEnum>(), 4);
assert_eq!(std::mem::size_of::<Test64BitEnum>(), 8);
⋮----
check_type_assumptions();
sol_log_compute_units();
Ok(())
⋮----
mod test {
⋮----
fn test_return_sstruct() {
assert_eq!(SStruct { x: 1, y: 2, z: 3 }, return_sstruct());

================
File: programs/sbf/rust/sanity/Cargo.toml
================
[package]
name = "solana-sbf-rust-sanity"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/secp256k1_recover/src/lib.rs
================
fn test_secp256k1_recover() {
⋮----
let public_key = secp256k1_recover(&hash[..], recovery_id, &signature[..]).unwrap();
assert_eq!(public_key.to_bytes(), expected);
⋮----
fn test_secp256k1_recover_malleability() {
⋮----
use sha3::Digest;
⋮----
hasher.update(message);
solana_hash::Hash::new_from_array(hasher.finalize().into())
⋮----
hasher.hash(message);
hasher.result()
⋮----
let signature = libsecp256k1::Signature::parse_standard_slice(&signature_bytes).unwrap();
⋮----
let alt_recovery_id = libsecp256k1::RecoveryId::parse(recovery_id ^ 1).unwrap();
let alt_signature_bytes = alt_signature.serialize();
let alt_recovery_id = alt_recovery_id.serialize();
⋮----
secp256k1_recover(message_hash.as_bytes(), recovery_id, &signature_bytes[..]).unwrap();
assert_eq!(recovered_pubkey.to_bytes(), pubkey_bytes);
let alt_recovered_pubkey = secp256k1_recover(
message_hash.as_bytes(),
⋮----
.unwrap();
assert_eq!(alt_recovered_pubkey.to_bytes(), pubkey_bytes);
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
msg!("secp256k1_recover");
test_secp256k1_recover();
test_secp256k1_recover_malleability();
⋮----
custom_heap_default!();
custom_panic_default!();

================
File: programs/sbf/rust/secp256k1_recover/Cargo.toml
================
[package]
name = "solana-sbf-rust-secp256k1-recover"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
libsecp256k1 = { workspace = true }
sha3 = { workspace = true }
solana-hash = { workspace = true }
solana-keccak-hasher = { workspace = true, features = ["sha3"] }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-secp256k1-recover = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/sha/src/lib.rs
================
fn test_sha256_hasher() {
use solana_sha256_hasher::hashv;
let vals = &["Gaggablaghblagh!".as_ref(), "flurbos".as_ref()];
⋮----
use sha2::Digest;
⋮----
hasher.update(val);
⋮----
solana_hash::Hash::new_from_array(hasher.finalize().into())
⋮----
hasher.hashv(vals);
hasher.result()
⋮----
assert_eq!(hashv(vals), hash);
⋮----
fn test_keccak256_hasher() {
use solana_keccak_hasher::hashv;
⋮----
use sha3::Digest;
⋮----
fn test_blake3_hasher() {
use solana_blake3_hasher::hashv;
⋮----
let hash = blake3::hash(&[v0, v1].concat());
assert_eq!(hashv(vals).as_bytes(), hash.as_bytes());
⋮----
pub extern "C" fn entrypoint(_input: *mut u8) -> u64 {
msg!("sha");
test_sha256_hasher();
test_keccak256_hasher();
test_blake3_hasher();
⋮----
custom_heap_default!();
custom_panic_default!();
⋮----
mod test {
⋮----
fn test_sha() {

================
File: programs/sbf/rust/sha/Cargo.toml
================
[package]
name = "solana-sbf-rust-sha"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
blake3 = { workspace = true }
sha2 = { workspace = true }
sha3 = { workspace = true }
solana-blake3-hasher = { workspace = true, features = ["blake3"] }
solana-hash = { workspace = true }
solana-keccak-hasher = { workspace = true, features = ["sha3"] }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-sha256-hasher = { workspace = true, features = ["sha2"] }

[lints]
workspace = true

================
File: programs/sbf/rust/sibling_inner_instructions/src/lib.rs
================
fn process_instruction(
⋮----
msg!("sibling inner");
⋮----
vec![AccountMeta::new_readonly(*accounts[1].key, false)],
⋮----
vec![
⋮----
assert_eq!(TRANSACTION_LEVEL_STACK_HEIGHT + 1, get_stack_height());
assert_eq!(
⋮----
assert_eq!(get_processed_sibling_instruction(3), None);
Ok(())

================
File: programs/sbf/rust/sibling_inner_instructions/Cargo.toml
================
[package]
name = "solana-sbf-rust-sibling-inner-instructions"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true, features = ["syscalls"] }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/sibling_instructions/src/lib.rs
================
fn process_instruction(
⋮----
msg!("sibling");
⋮----
vec![AccountMeta::new_readonly(*accounts[1].key, false)],
⋮----
vec![
⋮----
invoke(&instruction3, accounts)?;
invoke(&instruction2, accounts)?;
invoke(&instruction1, accounts)?;
invoke(&instruction0, accounts)?;
⋮----
assert_eq!(TRANSACTION_LEVEL_STACK_HEIGHT, get_stack_height());
assert_eq!(
⋮----
assert_eq!(get_processed_sibling_instruction(2), None);
Ok(())

================
File: programs/sbf/rust/sibling_instructions/Cargo.toml
================
[package]
name = "solana-sbf-rust-sibling-instructions"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/simulation/src/lib.rs
================
declare_id!("Sim1jD5C35odT8mzctm8BWnjic8xW5xgeb5MbcbErTo");
⋮----
pub fn process_instruction(
⋮----
let account_info_iter = &mut accounts.iter();
let slot_history_account_info = next_account_info(account_info_iter)?;
let clock_account_info = next_account_info(account_info_iter)?;
let data = slot_history_account_info.data.borrow();
let slot: u64 = u64::from_le_bytes(data[data.len() - 8..].try_into().unwrap());
let clock_from_cache = Clock::get().unwrap();
let clock_from_account = Clock::from_account_info(clock_account_info).unwrap();
msg!("next_slot from slot history is {:?} ", slot);
msg!("clock from cache is in slot {:?} ", clock_from_cache.slot);
msg!(
⋮----
msg!("On-chain");
⋮----
panic!("Simulation");
⋮----
Ok(())

================
File: programs/sbf/rust/simulation/Cargo.toml
================
[package]
name = "solana-sbf-rust-simulation"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-clock = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/spoof1/src/lib.rs
================
fn process_instruction(
⋮----
tmp_native_owner.copy_from_slice(accounts[0].owner.as_ref());
accounts[0].assign(fake_system.owner);
⋮----
let mut new_system = system.clone();
new_system.data = fake_system.data.clone();
let account_metas = vec![
⋮----
msg!("swapped owner and data");
invoke(&ix, &[target.clone(), me.clone(), new_system])?;
accounts[0].assign(&Pubkey::from(tmp_native_owner));
Ok(())

================
File: programs/sbf/rust/spoof1/Cargo.toml
================
[package]
name = "solana-sbf-rust-spoof1"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-instruction = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-system-interface = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/spoof1_system/src/lib.rs
================
fn process_instruction(
⋮----
let to_balance = to.lamports();
**to.lamports.borrow_mut() = to_balance + from.lamports();
**from.lamports.borrow_mut() = 0u64;
Ok(())

================
File: programs/sbf/rust/spoof1_system/Cargo.toml
================
[package]
name = "solana-sbf-rust-spoof1-system"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/syscall-get-epoch-stake/src/lib.rs
================
pub fn process_instruction(
⋮----
let total_stake = get_epoch_total_stake();
assert_ne!(total_stake, 0);
msg!("Total Stake: {}", total_stake);
⋮----
let vote_stake = get_epoch_stake_for_vote_account(vote_address);
assert_ne!(vote_stake, 0);
msg!("Vote Stake for account {}: {}", i, vote_stake);
⋮----
check_vote_account_stake(0);
check_vote_account_stake(1);
set_return_data(&total_stake.to_le_bytes());
Ok(())

================
File: programs/sbf/rust/syscall-get-epoch-stake/Cargo.toml
================
[package]
name = "solana-sbf-syscall-get-epoch-stake"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/sysvar/src/lib.rs
================
use solana_sysvar::recent_blockhashes::RecentBlockhashes;
⋮----
fn sol_get_sysvar_handler<T>(dst: &mut [u8], offset: u64, length: u64) -> Result<(), ProgramError>
⋮----
solana_program_entrypoint::SUCCESS => Ok(()),
e => Err(e.into()),
⋮----
fn sol_get_sysvar<T>() -> Result<T, ProgramError>
⋮----
let mut data = vec![0; len];
⋮----
bincode::deserialize(&data).map_err(|_| ProgramError::InvalidArgument)
⋮----
Err(ProgramError::UnsupportedSysvar)
⋮----
pub fn process_instruction(
⋮----
match instruction_data.first() {
⋮----
msg!("Clock identifier:");
sysvar::clock::id().log();
let clock = Clock::from_account_info(&accounts[2]).unwrap();
assert_ne!(clock, Clock::default());
⋮----
assert_eq!(clock, got_clock);
⋮----
assert_eq!(clock, sgs_clock);
⋮----
msg!("EpochRewards identifier:");
sysvar::epoch_rewards::id().log();
let epoch_rewards = EpochRewards::from_account_info(&accounts[10]).unwrap();
⋮----
assert_eq!(epoch_rewards, got_epoch_rewards);
⋮----
assert_eq!(epoch_rewards, sgs_epoch_rewards);
⋮----
msg!("EpochSchedule identifier:");
sysvar::epoch_schedule::id().log();
let epoch_schedule = EpochSchedule::from_account_info(&accounts[3]).unwrap();
assert_eq!(epoch_schedule, EpochSchedule::default());
⋮----
assert_eq!(epoch_schedule, got_epoch_schedule);
⋮----
assert_eq!(epoch_schedule, sgs_epoch_schedule);
⋮----
msg!("RecentBlockhashes identifier:");
sysvar::recent_blockhashes::id().log();
⋮----
RecentBlockhashes::from_account_info(&accounts[5]).unwrap();
assert_ne!(recent_blockhashes, RecentBlockhashes::default());
⋮----
msg!("Rent identifier:");
sysvar::rent::id().log();
let rent = Rent::from_account_info(&accounts[6]).unwrap();
⋮----
assert_eq!(rent, got_rent);
⋮----
assert_eq!(rent, sgs_rent);
⋮----
msg!("SlotHistory identifier:");
sysvar::slot_history::id().log();
assert_eq!(
⋮----
Ok(())
⋮----
msg!("Instructions identifier:");
instructions::id().log();
assert_eq!(*accounts[4].owner, sysvar::id());
⋮----
assert_eq!(0, index);
⋮----
msg!("StakeHistory identifier:");
sysvar::stake_history::id().log();
let _ = StakeHistory::from_account_info(&accounts[9]).unwrap();
let stake_history_sysvar = StakeHistorySysvar(1);
assert!(stake_history_sysvar.get_entry(0).is_some());
⋮----
msg!("SlotHashes identifier:");
sysvar::slot_hashes::id().log();
⋮----
assert!(pod_slot_hashes.get( &0)?.is_some());
⋮----
accounts[2].data.borrow_mut().as_mut_ptr(),
⋮----
_ => Err(ProgramError::InvalidInstructionData),

================
File: programs/sbf/rust/sysvar/Cargo.toml
================
[package]
name = "solana-sbf-rust-sysvar"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["cdylib"]

[dependencies]
bincode = { workspace = true }
solana-account-info = { workspace = true }
solana-define-syscall = { workspace = true }
solana-instruction = { workspace = true }
solana-instructions-sysvar = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-stake-interface = { workspace = true, features = ["sysvar"] }
solana-sysvar = { workspace = true, features = ["bincode"] }

[lints]
workspace = true

================
File: programs/sbf/rust/upgradeable/src/lib.rs
================
fn process_instruction(
⋮----
msg!("Upgradeable program");
assert_eq!(accounts.len(), 1);
assert_eq!(*accounts[0].key, clock::id());
Err(42.into())

================
File: programs/sbf/rust/upgradeable/Cargo.toml
================
[package]
name = "solana-sbf-rust-upgradeable"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
name = "solana_sbf_rust_upgradeable"
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/rust/upgraded/src/lib.rs
================
fn process_instruction(
⋮----
msg!("Upgraded program");
assert_eq!(accounts.len(), 1);
assert_eq!(*accounts[0].key, clock::id());
Err(43.into())

================
File: programs/sbf/rust/upgraded/Cargo.toml
================
[package]
name = "solana-sbf-rust-upgraded"
version = { workspace = true }
description = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
name = "solana_sbf_rust_upgraded"
crate-type = ["cdylib"]

[dependencies]
solana-account-info = { workspace = true }
solana-msg = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: programs/sbf/tests/programs.rs
================
fn process_transaction_and_record_inner(
⋮----
let commit_result = load_execute_and_commit_transaction(bank, tx);
⋮----
} = commit_result.unwrap();
let inner_instructions = inner_instructions.expect("cpi recording should be enabled");
let log_messages = log_messages.expect("log recording should be enabled");
⋮----
fn load_execute_and_commit_transaction(bank: &Bank, tx: Transaction) -> TransactionCommitResult {
let txs = vec![tx];
let tx_batch = bank.prepare_batch_for_tests(txs);
⋮----
.load_execute_and_commit_transactions(
⋮----
commit_results.pop().unwrap()
⋮----
fn bank_with_feature_activated(
⋮----
let slot = parent.slot().saturating_add(1);
⋮----
bank.activate_feature(feature_id);
⋮----
.write()
.unwrap()
.insert(bank)
.clone_without_scheduler()
⋮----
fn bank_with_feature_deactivated(
⋮----
bank.deactivate_feature(feature_id);
⋮----
fn test_program_sbf_sanity() {
⋮----
programs.extend_from_slice(&[
⋮----
let current_dir = env::current_dir().unwrap();
⋮----
File::create(current_dir.join("target").join("sanity_programs.txt")).unwrap();
for program in programs.iter() {
writeln!(file, "{}", program.0.trim_start_matches("solana_sbf_rust_"))
.expect("Failed to write to file");
⋮----
println!("Test program: {:?}", program.0);
⋮----
let account_metas = vec![
⋮----
let accounts = vec![
⋮----
sysvar_cache.fill_missing_entries(|pubkey, callbackback| {
⋮----
let rent_data = bincode::serialize(&rent).unwrap();
callbackback(&rent_data);
⋮----
instruction: instruction.into(),
⋮----
.unwrap();
⋮----
Some(err) => Err(err),
None => Ok(()),
⋮----
assert!(result.is_ok(), "{result:?}");
⋮----
assert!(result.is_err(), "{result:?}");
⋮----
fn test_program_sbf_loader_deprecated() {
⋮----
programs.extend_from_slice(&[("deprecated_loader")]);
⋮----
programs.extend_from_slice(&[("solana_sbf_rust_deprecated_loader")]);
⋮----
println!("Test program: {:?}", program);
⋮----
} = create_genesis_config(50);
⋮----
.remove(&agave_feature_set::disable_deploy_of_alloc_free_syscall::id())
⋮----
let program_id = create_program(&bank, &bpf_loader_deprecated::id(), program);
⋮----
.advance_slot(1, bank_forks.as_ref(), &Pubkey::default())
.expect("Failed to advance the slot");
let account_metas = vec![AccountMeta::new(mint_keypair.pubkey(), true)];
⋮----
let result = bank_client.send_and_confirm_instruction(&mint_keypair, instruction);
assert!(result.is_ok());
⋮----
fn test_sol_alloc_free_no_longer_deployable_with_upgradeable_loader() {
⋮----
let mut bank_client = BankClient::new_shared(bank.clone());
⋮----
let (_bank, _program_id) = load_program_of_loader_v4(
⋮----
fn test_program_sbf_duplicate_accounts() {
⋮----
programs.extend_from_slice(&[("dup_accounts")]);
⋮----
programs.extend_from_slice(&[("solana_sbf_rust_dup_accounts")]);
⋮----
let (bank, program_id) = load_program_of_loader_v4(
⋮----
bank.store_account(&payee_pubkey, &payee_account);
⋮----
bank.store_account(&pubkey, &account);
let instruction = Instruction::new_with_bytes(program_id, &[1], account_metas.clone());
⋮----
let data = bank_client.get_account_data(&pubkey).unwrap().unwrap();
⋮----
assert_eq!(data[0], 1);
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[2], account_metas.clone());
⋮----
assert_eq!(data[0], 2);
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[3], account_metas.clone());
⋮----
assert_eq!(data[0], 3);
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[4], account_metas.clone());
⋮----
let lamports = bank_client.get_balance(&pubkey).unwrap();
⋮----
assert_eq!(lamports, 11);
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[5], account_metas.clone());
⋮----
assert_eq!(lamports, 12);
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[6], account_metas.clone());
⋮----
assert_eq!(lamports, 13);
⋮----
let pubkey = keypair.pubkey();
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[7], account_metas.clone());
let message = Message::new(&[instruction], Some(&mint_keypair.pubkey()));
let result = bank_client.send_and_confirm_message(&[&mint_keypair, &keypair], message);
⋮----
fn test_program_sbf_error_handling() {
⋮----
programs.extend_from_slice(&[("error_handling")]);
⋮----
programs.extend_from_slice(&[("solana_sbf_rust_error_handling")]);
⋮----
let (_bank, program_id) = load_program_of_loader_v4(
⋮----
assert_eq!(
⋮----
let result = result.unwrap_err().unwrap();
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[8], account_metas.clone());
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[9], account_metas.clone());
⋮----
fn test_return_data_and_log_data_syscall() {
⋮----
programs.extend_from_slice(&[("log_data")]);
⋮----
programs.extend_from_slice(&[("solana_sbf_rust_log_data")]);
⋮----
bank.freeze();
⋮----
let blockhash = bank.last_blockhash();
⋮----
let result = bank.simulate_transaction(&sanitized_tx, false);
assert!(result.result.is_ok());
assert_eq!(result.logs[1], "Program data: AQID BAUG");
⋮----
fn test_program_sbf_invoke_sanity() {
⋮----
enum Languages {
⋮----
programs.push((Languages::C, "invoke", "invoked", "noop"));
⋮----
programs.push((
⋮----
let (_bank, invoke_program_id) = load_program_of_loader_v4(
⋮----
let (_bank, invoked_program_id) = load_program_of_loader_v4(
⋮----
let (bank, noop_program_id) = load_program_of_loader_v4(
⋮----
bank.store_account(&argument_keypair.pubkey(), &account);
⋮----
bank.store_account(&invoked_argument_keypair.pubkey(), &account);
⋮----
bank.store_account(&from_keypair.pubkey(), &account);
⋮----
bank.store_account(&unexecutable_program_keypair.pubkey(), &account);
⋮----
bank.store_account(&noop_program_keypair.pubkey(), &account);
⋮----
Pubkey::find_program_address(&[derived_key2.as_ref()], &invoked_program_id);
let mint_pubkey = mint_keypair.pubkey();
⋮----
let signers = vec![
⋮----
let mut instructions = vec![Instruction::new_with_bytes(
⋮----
instructions.extend_from_slice(additional_instructions);
let message = Message::new(&instructions, Some(&mint_pubkey));
let tx = Transaction::new(&signers, message.clone(), bank.last_blockhash());
⋮----
process_transaction_and_record_inner(bank, tx);
⋮----
.first()
.map(|instructions| {
⋮----
.iter()
.filter_map(|ix| {
⋮----
.get(ix.instruction.program_id_index as usize)
⋮----
.cloned()
.collect()
⋮----
.unwrap_or_default();
⋮----
.get(1)
⋮----
println!("Running success test #{:?}", test);
⋮----
do_invoke(test, additional_instructions, bank);
assert_eq!(result, Ok(()));
assert_eq!(invoked_programs.len(), expected_invoked_programs.len());
assert_eq!(invoked_programs, expected_invoked_programs);
assert_eq!(no_invoked_programs.len(), 0);
⋮----
do_invoke_success(
⋮----
&[Instruction::new_with_bytes(noop_program_id, &[], vec![])],
⋮----
Languages::C => vec![
⋮----
Languages::Rust => vec![
⋮----
.as_ref(),
⋮----
let bank = bank_with_feature_activated(
⋮----
assert!(bank
⋮----
&[invoked_program_id.clone(); 16],
⋮----
let bank = bank_with_feature_deactivated(
⋮----
assert!(!bank
⋮----
println!("Running failure test #{:?}", test);
⋮----
let (result, log_messages, executed_units, invoked_programs, _) = do_invoke(
⋮----
assert_eq!(result, Err(expected_error));
⋮----
assert_eq!(executed_units, compute_unit_limit as u64);
⋮----
assert!(executed_units < compute_unit_limit as u64);
⋮----
assert_eq!(log_messages.len(), expected_log_messages.len());
⋮----
.into_iter()
.zip(log_messages)
.for_each(|(expected_log_message, log_message)| {
⋮----
assert_eq!(log_message, expected_log_message);
⋮----
do_invoke_failure_test_local_with_compute_check(
⋮----
do_invoke_failure_test_local(
⋮----
&[argument_keypair.pubkey()],
⋮----
&[unexecutable_program_keypair.pubkey()],
⋮----
Some(vec![
⋮----
invoked_program_id.clone(),
⋮----
assert_eq!(43, bank.get_balance(&derived_key1));
let account = bank.get_account(&derived_key1).unwrap();
assert_eq!(&invoke_program_id, account.owner());
⋮----
assert_eq!(i as u8, account.data()[i]);
⋮----
bank.store_account(&derived_key1, &AccountSharedData::default());
⋮----
account_metas.clone(),
⋮----
let message = Message::new(&[instruction], Some(&mint_pubkey));
⋮----
message.clone(),
bank.last_blockhash(),
⋮----
process_transaction_and_record_inner(&bank, tx);
⋮----
.map(|ix| &message.account_keys[ix.instruction.program_id_index as usize])
⋮----
.collect();
assert_eq!(invoked_programs, vec![]);
⋮----
fn test_program_sbf_program_id_spoofing() {
⋮----
let (_bank, malicious_swap_pubkey) = load_program_of_loader_v4(
⋮----
let (bank, malicious_system_pubkey) = load_program_of_loader_v4(
⋮----
bank.store_account(&from_pubkey, &account);
⋮----
bank.store_account(&to_pubkey, &account);
⋮----
Instruction::new_with_bytes(malicious_swap_pubkey, &[], account_metas.clone());
⋮----
assert_eq!(10, bank.get_balance(&from_pubkey));
assert_eq!(0, bank.get_balance(&to_pubkey));
⋮----
fn test_program_sbf_caller_has_access_to_cpi_program() {
⋮----
let (_bank, caller_pubkey) = load_program_of_loader_v4(
⋮----
let (_bank, caller2_pubkey) = load_program_of_loader_v4(
⋮----
let instruction = Instruction::new_with_bytes(caller_pubkey, &[1], account_metas.clone());
⋮----
fn test_program_sbf_ro_modify() {
⋮----
let (bank, program_pubkey) = load_program_of_loader_v4(
⋮----
bank.store_account(&test_keypair.pubkey(), &account);
⋮----
let instruction = Instruction::new_with_bytes(program_pubkey, &[1], account_metas.clone());
⋮----
let result = bank_client.send_and_confirm_message(&[&mint_keypair, &test_keypair], message);
⋮----
let instruction = Instruction::new_with_bytes(program_pubkey, &[3], account_metas.clone());
⋮----
let instruction = Instruction::new_with_bytes(program_pubkey, &[4], account_metas.clone());
⋮----
fn test_program_sbf_call_depth() {
⋮----
.contains_key(&feature_set::raise_cpi_nesting_limit_to_8::id()),
⋮----
.contains_key(&feature_set::increase_cpi_account_info_limit::id()),
⋮----
Instruction::new_with_bincode(program_id, &(budget.max_call_depth - 1), vec![]);
⋮----
let instruction = Instruction::new_with_bincode(program_id, &budget.max_call_depth, vec![]);
⋮----
assert!(result.is_err());
⋮----
fn test_program_sbf_compute_budget() {
⋮----
Instruction::new_with_bincode(program_id, &0, vec![]),
⋮----
Some(&mint_keypair.pubkey()),
⋮----
let result = bank_client.send_and_confirm_message(&[&mint_keypair], message);
⋮----
fn assert_instruction_count() {
⋮----
println!("\n  {:36} expected actual  diff", "SBF program");
for (program_name, expected_consumption) in programs.iter() {
⋮----
let mut transaction_accounts = vec![
⋮----
let instruction_accounts = vec![AccountMeta {
⋮----
.set_data_from_slice(&load_program_from_file(program_name));
transaction_accounts[0].1.set_executable(true);
⋮----
print!("  {:36} {:8}", program_name, *expected_consumption);
mock_process_instruction(
⋮----
Some(0),
⋮----
Ok(()),
⋮----
*prev_compute_meter.borrow_mut() = invoke_context.get_remaining();
⋮----
.borrow()
.saturating_sub(invoke_context.get_remaining());
⋮----
println!(
⋮----
assert!(consumption <= *expected_consumption);
⋮----
fn test_program_sbf_instruction_introspection() {
⋮----
} = create_genesis_config(50_000);
⋮----
let instruction0 = Instruction::new_with_bytes(program_id, &[0u8, 0u8], account_metas.clone());
let instruction1 = Instruction::new_with_bytes(program_id, &[0u8, 1u8], account_metas.clone());
⋮----
let account_metas = vec![AccountMeta::new(sysvar::instructions::id(), false)];
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[0], vec![]);
⋮----
assert_eq!(result.unwrap_err().unwrap(), expected_error,);
assert!(bank.get_account(&sysvar::instructions::id()).is_none());
⋮----
fn test_program_sbf_r2_instruction_data_pointer(num_accounts: usize, input_data_len: usize) {
⋮----
bank.store_account(
⋮----
account_metas.push(AccountMeta::new(pubkey, false));
⋮----
account_metas.push(AccountMeta::new_readonly(pubkey, false));
⋮----
let input_data: Vec<u8> = (0..input_data_len).map(|i| (i % 256) as u8).collect();
⋮----
let return_data = result.return_data.unwrap().data;
assert_eq!(input_data, return_data);
⋮----
fn get_stable_genesis_config() -> GenesisConfigInfo {
⋮----
Pubkey::from_str("GLh546CXmtZdvpEzL8sxzqhhUf7KPvmGaRpFHB5W1sjV").unwrap();
⋮----
let stake_pubkey = Pubkey::from_str("HGq9JF77xFXRgWRJy8VQuhdbdugrT856RvQDzr1KJo6E").unwrap();
let mut genesis_config = create_genesis_config_with_leader_ex(
⋮----
&mint_keypair.pubkey(),
⋮----
&voting_keypair.pubkey(),
⋮----
bootstrap_validator_stake_lamports(),
⋮----
vec![],
⋮----
genesis_config.creation_time = Duration::ZERO.as_secs() as UnixTimestamp;
⋮----
fn test_program_sbf_invoke_stable_genesis_and_bank() {
⋮----
} = get_stable_genesis_config();
⋮----
let bank_client = BankClient::new_shared(bank.clone());
⋮----
let program_id = program_keypair.pubkey();
⋮----
Instruction::new_with_bytes(program_id, &[0], vec![AccountMeta::new(clock::id(), false)]);
let result = bank_client.send_and_confirm_instruction(&mint_keypair, instruction.clone());
⋮----
indirect_program_keypair.pubkey(),
⋮----
vec![
⋮----
load_upgradeable_buffer(
⋮----
&buffer_keypair.pubkey(),
&authority_keypair.pubkey(),
⋮----
bank_client.send_and_confirm_instruction(&mint_keypair, invoke_instruction.clone());
⋮----
&[redeployment_instruction.clone(), invoke_instruction],
⋮----
let (result, _, _, _) = process_transaction_and_record_inner(&bank, tx);
⋮----
&[program_keypair.pubkey().as_ref()],
⋮----
Some(&authority_keypair.pubkey()),
Some(&program_id),
⋮----
Instruction::new_with_bytes(program_id, &[1], vec![AccountMeta::new(clock::id(), false)]);
⋮----
&[undeployment_instruction.clone(), invoke_instruction],
⋮----
.expect("Failed to generate hash");
println!("Stable test produced bank hash: {}", bank.hash());
println!("Expected hash: {}", expected_hash);
⋮----
fn test_program_sbf_invoke_in_same_tx_as_deployment() {
⋮----
let (program_keypair, deployment_instructions) = instructions_to_load_program_of_loader_v4(
⋮----
let (bank, indirect_program_id) = load_program_of_loader_v4(
⋮----
.enumerate()
⋮----
let mut instructions = deployment_instructions.clone();
instructions.push(invoke_instruction);
⋮----
Message::new(&instructions, Some(&mint_keypair.pubkey())),
⋮----
let result = load_execute_and_commit_transaction(&bank, tx);
⋮----
if let TransactionError::InstructionError(instr_no, ty) = result.unwrap_err() {
assert!(instr_no <= 40);
assert_eq!(ty, InstructionError::UnsupportedProgramId);
⋮----
panic!("Invalid error type");
⋮----
fn test_program_sbf_invoke_in_same_tx_as_redeployment() {
⋮----
instructions_to_load_program_of_loader_v4(
⋮----
loader_v4_instruction::retract(&program_id, &authority_keypair.pubkey());
⋮----
deployment_instructions.split_off(deployment_instructions.len() - 3);
⋮----
let signers = std::iter::once(signers[0]).chain(std::iter::repeat(signers[1]));
for (instruction, signers) in deployment_instructions.into_iter().zip(signers) {
⋮----
.send_and_confirm_message(signers, message)
⋮----
undeployment_instruction.clone(),
redeployment_instructions[0].clone(),
redeployment_instructions[1].clone(),
redeployment_instructions[2].clone(),
⋮----
assert_eq!(result.unwrap_err(), expected_error,);
⋮----
fn test_program_sbf_invoke_in_same_tx_as_undeployment() {
⋮----
fn test_program_sbf_disguised_as_sbf_loader() {
⋮----
programs.extend_from_slice(&[("noop")]);
⋮----
programs.extend_from_slice(&[("solana_sbf_rust_noop")]);
⋮----
bank.deactivate_feature(&agave_feature_set::remove_bpf_loader_incorrect_program_id::id());
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
let account_metas = vec![AccountMeta::new_readonly(program_id, false)];
⋮----
fn test_program_reads_from_program_account() {
⋮----
let data = bank_client.get_account_data(&program_id).unwrap().unwrap();
⋮----
.send_and_confirm_instruction(&mint_keypair, instruction)
⋮----
fn test_program_sbf_c_dup() {
⋮----
let account = AccountSharedData::new_data(42, &[1_u8, 2, 3], &system_program::id()).unwrap();
bank.store_account(&account_address, &account);
⋮----
fn test_program_sbf_upgrade() {
⋮----
&new_authority_keypair.pubkey(),
⋮----
let message = Message::new(&[authority_instruction], Some(&mint_keypair.pubkey()));
⋮----
.send_and_confirm_message(
⋮----
deployment_instructions.insert(
deployment_instructions.len() - 3,
loader_v4_instruction::retract(&program_id, &new_authority_keypair.pubkey()),
⋮----
.advance_slot(1, &bank_forks, &Pubkey::default())
⋮----
fn test_program_sbf_upgrade_via_cpi() {
⋮----
let (_bank, invoke_and_return) = load_program_of_loader_v4(
⋮----
.insert(0, AccountMeta::new(loader_v4::id(), false));
⋮----
let mut upgrade_instruction = deployment_instructions.pop().unwrap();
⋮----
let message = Message::new(&[upgrade_instruction], Some(&mint_keypair.pubkey()));
⋮----
.send_and_confirm_message(&[&mint_keypair, &new_authority_keypair], message)
⋮----
fn test_program_sbf_ro_account_modify() {
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[0], account_metas.clone());
⋮----
fn test_program_sbf_realloc() {
⋮----
} = create_genesis_config(1_000_000_000_000);
⋮----
feature_set.deactivate(&feature_set::stricter_abi_and_runtime_constraints::id());
feature_set.deactivate(&feature_set::account_data_direct_mapping::id());
⋮----
let mut instruction = realloc(&program_id, &pubkey, 0, &mut bump);
⋮----
realloc(&program_id, &pubkey, 0, &mut bump),
⋮----
Some(&mint_pubkey),
⋮----
assert_eq!(0, data.len());
⋮----
realloc_extend_and_undo(
⋮----
realloc_extend_and_fill(
⋮----
assert_eq!((i + 1) * MAX_PERMITTED_DATA_INCREASE, data.len());
⋮----
for i in 0..data.len() {
assert_eq!(data[i], 1);
⋮----
realloc(&program_id, &pubkey, 6, &mut bump),
⋮----
assert_eq!(6, data.len());
⋮----
extend_and_write_u64(&program_id, &pubkey, 0x1122334455667788),
⋮----
assert_eq!(8, data.len());
assert_eq!(0x1122334455667788, unsafe { *data.as_ptr().cast::<u64>() });
⋮----
vec![AccountMeta::new(pubkey, false)],
⋮----
let account = bank.get_account(&pubkey).unwrap();
assert_eq!(&solana_system_interface::program::id(), account.owner());
⋮----
assert_eq!(MAX_PERMITTED_DATA_INCREASE, data.len());
⋮----
assert_eq!(&program_id, account.owner());
⋮----
assert_eq!(2 * MAX_PERMITTED_DATA_INCREASE, data.len());
⋮----
vec![AccountMeta::new(pubkey, true)],
⋮----
fn test_program_sbf_realloc_invoke() {
⋮----
let (_bank, realloc_program_id) = load_program_of_loader_v4(
⋮----
let (bank, realloc_invoke_program_id) = load_program_of_loader_v4(
⋮----
let pubkey = keypair.pubkey().clone();
⋮----
let invoke_pubkey = invoke_keypair.pubkey().clone();
⋮----
assert_eq!(account.lamports(), START_BALANCE);
⋮----
realloc(&realloc_program_id, &pubkey, 0, &mut bump),
⋮----
assert_eq!(&realloc_program_id, account.owner());
⋮----
bank.store_account(&invoke_pubkey, &invoke_account);
⋮----
.get_account_data(&invoke_pubkey)
⋮----
assert_eq!(100, data.len());
⋮----
assert_eq!(data[i], 0);
⋮----
for i in 5..data.len() {
assert_eq!(data[i], 2);
⋮----
let new_pubkey = new_keypair.pubkey().clone();
let mut instruction_data = vec![];
instruction_data.extend_from_slice(&[INVOKE_CREATE_ACCOUNT_REALLOC_CHECK, 1]);
instruction_data.extend_from_slice(&100_usize.to_le_bytes());
⋮----
let data = bank_client.get_account_data(&new_pubkey).unwrap().unwrap();
assert_eq!(200, data.len());
let account = bank.get_account(&new_pubkey).unwrap();
assert_eq!(&realloc_invoke_program_id, account.owner());
⋮----
invoke_account.set_data_from_slice(&vec![1; pre_len]);
⋮----
instruction_data.extend_from_slice(&[INVOKE_DEALLOC_AND_ASSIGN, 1]);
instruction_data.extend_from_slice(&pre_len.to_le_bytes());
⋮----
assert_eq!(new_len, data.len());
⋮----
instruction_data.extend_from_slice(&[INVOKE_REALLOC_TO_THEN_LOCAL_REALLOC_EXTEND, 1]);
instruction_data.extend_from_slice(&cpi_extend_bytes.to_le_bytes());
instruction_data.extend_from_slice(&local_extend_bytes.to_le_bytes());
let result = bank_client.send_and_confirm_message(
⋮----
assert!(
⋮----
invoke_account.set_data(vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9]);
⋮----
instruction_data.extend_from_slice(&[INVOKE_REALLOC_SHRINK_THEN_CPI_THEN_REALLOC_EXTEND, 1]);
instruction_data.extend_from_slice(&5_u64.to_le_bytes());
instruction_data.extend_from_slice(&10_u64.to_le_bytes());
⋮----
assert_eq!(data, &[0, 1, 2, 3, 4, 0, 0, 0, 0, 0]);
⋮----
&[realloc(&realloc_program_id, &pubkey, 0, &mut bump)],
⋮----
instruction_data.extend_from_slice(&[INVOKE_REALLOC_RECURSIVE, 1]);
⋮----
fn test_program_sbf_processed_inner_instruction() {
⋮----
let (_bank, sibling_program_id) = load_program_of_loader_v4(
⋮----
let (_bank, sibling_inner_program_id) = load_program_of_loader_v4(
⋮----
let (_bank, noop_program_id) = load_program_of_loader_v4(
⋮----
let (_bank, invoke_and_return_program_id) = load_program_of_loader_v4(
⋮----
assert!(bank_client
⋮----
fn test_program_fees() {
⋮----
} = create_genesis_config(500_000_000);
⋮----
compute_fee_bins: vec![
⋮----
bank.set_fee_structure(&fee_structure);
⋮----
let feature_set = bank.feature_set.clone();
⋮----
let pre_balance = bank_client.get_balance(&mint_keypair.pubkey()).unwrap();
⋮----
&[Instruction::new_with_bytes(program_id, &[], vec![])],
⋮----
process_compute_budget_instructions(
⋮----
.unwrap_or_default(),
⋮----
bank.feature_set.as_ref().into(),
⋮----
.send_and_confirm_message(&[&mint_keypair], message)
⋮----
let post_balance = bank_client.get_balance(&mint_keypair.pubkey()).unwrap();
assert_eq!(pre_balance - post_balance, expected_normal_fee);
⋮----
Instruction::new_with_bytes(program_id, &[], vec![]),
⋮----
assert!(expected_normal_fee < expected_prioritized_fee);
⋮----
assert_eq!(pre_balance - post_balance, expected_prioritized_fee);
⋮----
fn test_program_sbf_inner_instruction_alignment_checks() {
⋮----
let noop = create_program(&bank, &bpf_loader_deprecated::id(), "solana_sbf_rust_noop");
let inner_instruction_alignment_check = create_program(
⋮----
fn test_cpi_account_ownership_writability() {
⋮----
} = create_genesis_config(100_123_456_789);
⋮----
let (bank, realloc_program_id) = load_program_of_loader_v4(
⋮----
bank.register_unique_recent_blockhash_for_test();
⋮----
bank.store_account(&account_keypair.pubkey(), &account);
let mut instruction_data = vec![instruction_id];
instruction_data.extend_from_slice(byte_index.to_le_bytes().as_ref());
⋮----
let instruction_data = vec![
⋮----
let mut account_metas = account_metas.clone();
account_metas.push(AccountMeta::new(account2_keypair.pubkey(), false));
for target_account in [1, account_metas.len() as u8 - 1] {
⋮----
bank.store_account(&account2_keypair.pubkey(), &account);
⋮----
let tx = Transaction::new(&[&mint_keypair], message.clone(), bank.last_blockhash());
let (result, _, logs, _) = process_transaction_and_record_inner(&bank, tx);
⋮----
result.unwrap();
let account = bank.get_account(&account_keypair.pubkey()).unwrap();
assert_eq!(account.data(), vec![0; 40]);
⋮----
let instruction_data = vec![TEST_ALLOW_WRITE_AFTER_OWNERSHIP_CHANGE_TO_CALLER, 1, 42, 42];
⋮----
fn test_cpi_account_data_updates() {
⋮----
[false, true].into_iter().flat_map(move |z| {
⋮----
.flat_map(move |y| [false, true].into_iter().map(move |x| (x, y, z)))
⋮----
let deprecated_program_id = create_program(
⋮----
account.set_data(b"foo".to_vec());
⋮----
let mut instruction_data = vec![TEST_CPI_ACCOUNT_UPDATE_CALLER_GROWS];
instruction_data.extend_from_slice(b"bar");
⋮----
assert_eq!(account.data(), b"foobar");
⋮----
let mut instruction_data = vec![TEST_CPI_ACCOUNT_UPDATE_CALLEE_GROWS];
⋮----
assert_eq!(account.data(), b"foo");
⋮----
account.set_data(b"foobar".to_vec());
⋮----
let mut instruction_data = vec![
⋮----
instruction_data.extend_from_slice(4usize.to_le_bytes().as_ref());
⋮----
assert_eq!(account.data(), b"foob");
⋮----
instruction_data.extend_from_slice(7usize.to_le_bytes().as_ref());
instruction_data.extend_from_slice(b"bazbad");
⋮----
assert_eq!(account.data(), b"foobazb");
⋮----
instruction_data.extend_from_slice(1usize.to_le_bytes().as_ref());
⋮----
assert_eq!(account.data(), b"f");
⋮----
fn test_cpi_invalid_account_info_pointers() {
⋮----
let mut account_metas = vec![
⋮----
let (new_bank, invoke_program_id) = load_program_of_loader_v4(
⋮----
account_metas.push(AccountMeta::new_readonly(invoke_program_id, false));
program_ids.push(invoke_program_id);
⋮----
let (new_bank, c_invoke_program_id) = load_program_of_loader_v4(
⋮----
account_metas.push(AccountMeta::new_readonly(c_invoke_program_id, false));
program_ids.push(c_invoke_program_id);
⋮----
fn test_deplete_cost_meter_with_access_violation() {
⋮----
let (bank, invoke_program_id) = load_program_of_loader_v4(
⋮----
bank_forks.as_ref(),
⋮----
let mut instruction_data = vec![TEST_WRITE_ACCOUNT, 2];
instruction_data.extend_from_slice(3usize.to_le_bytes().as_ref());
instruction_data.push(42);
⋮----
Instruction::new_with_bytes(invoke_program_id, &instruction_data, account_metas.clone());
⋮----
let tx = Transaction::new(&[&mint_keypair], message, bank.last_blockhash());
let result = load_execute_and_commit_transaction(&bank, tx).unwrap();
⋮----
assert_eq!(result.executed_units, u64::from(compute_unit_limit));
⋮----
fn test_program_sbf_deplete_cost_meter_with_divide_by_zero() {
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[], vec![]);
⋮----
fn test_deny_access_beyond_current_length() {
⋮----
bank.store_account(&readonly_account_keypair.pubkey(), &account);
bank.store_account(&writable_account_keypair.pubkey(), &account);
⋮----
let mut instruction_data = vec![TEST_READ_ACCOUNT, instruction_account_index];
⋮----
fn test_deny_executable_write() {
⋮----
fn test_update_callee_account() {
⋮----
let data: Vec<u8> = (0..10240).map(|n| n as u8).collect();
account.set_data(data);
⋮----
let mut instruction_data = vec![TEST_CALLEE_ACCOUNT_UPDATES, 0, 0];
instruction_data.extend_from_slice(20480usize.to_le_bytes().as_ref());
instruction_data.extend_from_slice(0usize.to_le_bytes().as_ref());
instruction_data.extend_from_slice(16384usize.to_le_bytes().as_ref());
instruction_data.extend_from_slice(&[TEST_CALLEE_ACCOUNT_UPDATES, 0, 0]);
⋮----
.get_account_data(&account_keypair.pubkey())
⋮----
assert_eq!(data.len(), 20480);
data.iter().enumerate().for_each(|(i, v)| {
⋮----
assert_eq!(*v, expected, "offset:{i} {v:#x} != {expected:#x}");
⋮----
let mut instruction_data = vec![TEST_CALLEE_ACCOUNT_UPDATES, 1, 0];
⋮----
instruction_data.extend_from_slice(19480usize.to_le_bytes().as_ref());
⋮----
instruction_data.extend_from_slice(8129usize.to_le_bytes().as_ref());
⋮----
assert_eq!(data.len(), 19480);
⋮----
instruction_data.extend_from_slice(16385usize.to_le_bytes().as_ref());
⋮----
instruction_data.extend_from_slice(&[TEST_CALLEE_ACCOUNT_UPDATES, 1, 0]);
⋮----
let mut instruction_data = vec![TEST_CALLEE_ACCOUNT_UPDATES, 1, 1];
⋮----
instruction_data.extend_from_slice(8190usize.to_le_bytes().as_ref());
⋮----
instruction_data.extend_from_slice(8191usize.to_le_bytes().as_ref());
⋮----
assert_eq!(data.len(), 10240);
⋮----
fn test_account_info_in_account() {
⋮----
programs.push("invoke");
⋮----
programs.push("solana_sbf_rust_invoke");
⋮----
let mut instruction_data = vec![TEST_ACCOUNT_INFO_IN_ACCOUNT];
instruction_data.extend_from_slice(32usize.to_le_bytes().as_ref());
⋮----
fn test_account_info_rc_in_account() {
⋮----
let instruction_data = vec![TEST_ACCOUNT_INFO_LAMPORTS_RC, 0, 0, 0];
⋮----
assert!(result.is_ok(), "{logs:?}");
⋮----
let instruction_data = vec![TEST_ACCOUNT_INFO_DATA_RC, 0, 0, 0];
⋮----
fn test_clone_account_data() {
⋮----
let (bank, invoke_program_id2) = load_program_of_loader_v4(
⋮----
let error = format!(
⋮----
assert!(logs.iter().any(|log| log.contains(&error)), "{logs:?}");
⋮----
fn test_stack_heap_zeroed() {
⋮----
let mut instruction_data = vec![TEST_STACK_HEAP_ZEROED];
instruction_data.extend_from_slice(&heap_len.to_le_bytes());
⋮----
fn test_function_call_args() {
⋮----
struct Test128 {
⋮----
struct InputData {
⋮----
struct OutputData {
⋮----
let instruction_data = to_vec(&input_data).unwrap();
⋮----
fn verify_many_args(input: &InputData) -> i64 {
⋮----
.overflowing_add(input.arg2)
⋮----
.overflowing_sub(input.arg3)
⋮----
.overflowing_add(input.arg4)
⋮----
.overflowing_sub(input.arg5)
⋮----
.overflowing_sub(input.arg7)
⋮----
.overflowing_add(input.arg8)
⋮----
.as_ref()
⋮----
let decoded: OutputData = from_slice::<OutputData>(return_data).unwrap();
⋮----
assert_eq!(decoded.many_args_1, verify_many_args(&input_data));
assert_eq!(decoded.many_args_2, verify_many_args(&input_data));
⋮----
fn test_mem_syscalls_overlap_account_begin_or_end() {
⋮----
let (bank, loader_v4_program_id) = load_program_of_loader_v4(
⋮----
Instruction::new_with_bytes(program_id, &[instr], account_metas.clone());
⋮----
let last_line = logs.last().unwrap();
⋮----
assert!(last_line.contains(" failed: Access violation"), "{logs:?}");
⋮----
Instruction::new_with_bytes(program_id, &[instr, 0], account_metas.clone());

================
File: programs/sbf/tests/simulation.rs
================
fn test_no_panic_banks_client() {
⋮----
} = create_genesis_config(50);
⋮----
let mut bank_client = BankClient::new_shared(bank.clone());
⋮----
let (bank, program_id) = load_program_of_loader_v4(
⋮----
bank_forks.as_ref(),
⋮----
bank.freeze();
⋮----
vec![
⋮----
let blockhash = bank.last_blockhash();
let message = Message::new(&[instruction], Some(&mint_keypair.pubkey()));
⋮----
let result = bank.simulate_transaction(&sanitized_tx, false);
assert!(result.result.is_ok());
⋮----
fn test_no_panic_rpc_client() {
⋮----
.add_program("solana_sbf_rust_simulation", program_id)
.start();
let rpc_client = test_validator.get_rpc_client();
let blockhash = rpc_client.get_latest_blockhash().unwrap();
⋮----
accounts: vec![
⋮----
data: vec![],
⋮----
Some(&payer.pubkey()),
⋮----
match rpc_client.send_and_confirm_transaction(&transaction) {
⋮----
if !format!("{e:?}").contains("Program is not deployed") {
panic!("Unexpected error: {e:?}");
⋮----
panic!("Timeout waiting for program to become deployable");
⋮----
sleep(Duration::from_millis(100));

================
File: programs/sbf/tests/syscall_get_epoch_stake.rs
================
fn test_syscall_get_epoch_stake() {
⋮----
let stakes = vec![100_000_000, 500_000_000];
let voting_keypairs = vec![
⋮----
let total_stake: u64 = stakes.iter().sum();
⋮----
} = create_genesis_config_with_vote_accounts(1_000_000_000, &voting_keypairs, stakes.clone());
⋮----
.iter()
.map(|keypair| {
let node_id = keypair.node_keypair.pubkey();
let authorized_voter = keypair.vote_keypair.pubkey();
let vote_account = VoteAccount::try_from(create_v4_account_with_authorized(
⋮----
.unwrap();
⋮----
bank.set_epoch_stakes_for_test(0, epoch_stakes_epoch_0);
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
let (bank, program_id) = load_program_of_loader_v4(
⋮----
bank.freeze();
⋮----
vec![
⋮----
let blockhash = bank.last_blockhash();
let message = Message::new(&[instruction], Some(&mint_keypair.pubkey()));
⋮----
let result = bank.simulate_transaction(&sanitized_tx, false);
assert!(result.result.is_ok());
let return_data_le_bytes: [u8; 8] = result.return_data.unwrap().data[0..8].try_into().unwrap();
⋮----
assert_eq!(total_stake_from_return_data, total_stake);

================
File: programs/sbf/tests/sysvar.rs
================
fn test_sysvar_syscalls() {
⋮----
} = create_genesis_config(50);
genesis_config.accounts.remove(&disable_fees_sysvar::id());
⋮----
bank.set_sysvar_for_tests(&epoch_rewards);
⋮----
stake_history.add(
⋮----
bank.set_sysvar_for_tests(&stake_history);
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
let (bank, program_id) = load_program_of_loader_v4(
⋮----
bank_forks.as_ref(),
⋮----
bank.store_account(
⋮----
bank.freeze();
let blockhash = bank.last_blockhash();
⋮----
vec![
⋮----
let message = Message::new(&[instruction], Some(&mint_keypair.pubkey()));
⋮----
let result = bank.simulate_transaction(&sanitized_tx, false);
assert!(result.result.is_ok());
⋮----
assert!(result.result.is_err());

================
File: programs/sbf/.gitignore
================
/target/
/farf/

================
File: programs/sbf/Cargo.toml
================
[package]
name = "solana-sbf-programs"
documentation = "https://docs.rs/solana"
readme = "README.md"
publish = false
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[workspace]
members = [
    "rust/128bit",
    "rust/128bit_dep",
    "rust/account_mem",
    "rust/account_mem_deprecated",
    "rust/alloc",
    "rust/alt_bn128",
    "rust/alt_bn128_compression",
    "rust/big_mod_exp",
    "rust/call_args",
    "rust/call_depth",
    "rust/caller_access",
    "rust/curve25519",
    "rust/custom_heap",
    "rust/dep_crate",
    "rust/deprecated_loader",
    "rust/divide_by_zero",
    "rust/dup_accounts",
    "rust/error_handling",
    "rust/external_spend",
    "rust/get_minimum_delegation",
    "rust/inner_instruction_alignment_check",
    "rust/instruction_introspection",
    "rust/invoke",
    "rust/invoke_and_error",
    "rust/invoke_and_ok",
    "rust/invoke_and_return",
    "rust/invoked",
    "rust/iter",
    "rust/log_data",
    "rust/many_args",
    "rust/many_args_dep",
    "rust/mem",
    "rust/mem_dep",
    "rust/membuiltins",
    "rust/noop",
    "rust/panic",
    "rust/param_passing",
    "rust/param_passing_dep",
    "rust/poseidon",
    "rust/r2_instruction_data_pointer",
    "rust/rand",
    "rust/realloc",
    "rust/realloc_invoke",
    "rust/remaining_compute_units",
    "rust/ro_account_modify",
    "rust/ro_modify",
    "rust/sanity",
    "rust/secp256k1_recover",
    "rust/sha",
    "rust/sibling_inner_instructions",
    "rust/sibling_instructions",
    "rust/simulation",
    "rust/spoof1",
    "rust/spoof1_system",
    "rust/syscall-get-epoch-stake",
    "rust/sysvar",
    "rust/upgradeable",
    "rust/upgraded",
]
[workspace.package]
version = "4.0.0-alpha.0"
description = "Solana SBF test program written in Rust"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
repository = "https://github.com/anza-xyz/agave"
homepage = "https://anza.xyz"
license = "Apache-2.0"
edition = "2021"

[workspace.lints.rust.unexpected_cfgs]
level = "warn"
check-cfg = [
    'cfg(target_os, values("solana"))',
    'cfg(feature, values("custom-panic", "custom-heap"))',
    'cfg(target_feature, values("dynamic-frames"))',
]

[workspace.dependencies]
agave-feature-set = { path = "../../feature-set", version = "=4.0.0-alpha.0" }
agave-logger = { path = "../../logger", version = "=4.0.0-alpha.0" }
agave-reserved-account-keys = { path = "../../reserved-account-keys", version = "=4.0.0-alpha.0" }
agave-syscalls = { path = "../../syscalls", version = "=4.0.0-alpha.0" }
agave-validator = { path = "../../validator", version = "=4.0.0-alpha.0" }
array-bytes = "=1.4.1"
bincode = { version = "1.1.4", default-features = false }
blake3 = "1.0.0"
borsh = "1.5.1"
byteorder = "1.3.2"
elf = "0.0.10"
getrandom = "0.2.10"
itertools = "0.13.0"
libsecp256k1 = { version = "0.7.0", default-features = false }
log = "0.4.11"
miow = "0.3.6"
net2 = "0.2.37"
num-derive = "0.4.2"
num-traits = "0.2"
rand = "0.8"
serde = { version = "1.0.112", features = ["derive"] }
serde_json = "1.0.56"
sha2 = "0.10.8"
sha3 = "0.10.8"
solana-account-decoder = { path = "../../account-decoder", version = "=4.0.0-alpha.0" }
solana-account-info = "=3.0.0"
solana-accounts-db = { path = "../../accounts-db", version = "=4.0.0-alpha.0" }
solana-big-mod-exp = "=3.0.0"
solana-blake3-hasher = { version = "=3.1.0", features = ["blake3"] }
solana-bn254 = "=3.1.2"
solana-bpf-loader-program = { path = "../bpf_loader", version = "=4.0.0-alpha.0" }
solana-cli-output = { path = "../../cli-output", version = "=4.0.0-alpha.0" }
solana-clock = { version = "=3.0.0", features = ["serde", "sysvar"] }
solana-compute-budget = { path = "../../compute-budget", version = "=4.0.0-alpha.0" }
solana-compute-budget-instruction = { path = "../../compute-budget-instruction", version = "=4.0.0-alpha.0" }
solana-cpi = "=3.0.0"
solana-curve25519 = { path = "../../curves/curve25519", version = "=4.0.0-alpha.0" }
solana-define-syscall = "=3.0.0"
solana-fee = { path = "../../fee", version = "=4.0.0-alpha.0" }
solana-hash = { version = "=4.0.1", features = ["bytemuck", "serde", "std"] }
solana-instruction = "=3.0.0"
solana-instructions-sysvar = "=3.0.0"
solana-keccak-hasher = { version = "=3.1.0", features = ["sha3"] }
solana-ledger = { path = "../../ledger", version = "=4.0.0-alpha.0" }
solana-measure = { path = "../../measure", version = "=4.0.0-alpha.0" }
solana-msg = "=3.0.0"
solana-poseidon = { path = "../../poseidon/", version = "=4.0.0-alpha.0" }
solana-program = "=3.0.0"
solana-program-entrypoint = "=3.1.0"
solana-program-error = "=3.0.0"
solana-program-memory = "=3.0.0"
solana-program-runtime = { path = "../../program-runtime", version = "=4.0.0-alpha.0" }
solana-pubkey = { version = "=3.0.0", default-features = false }
solana-runtime = { path = "../../runtime", version = "=4.0.0-alpha.0" }
solana-runtime-transaction = { path = "../../runtime-transaction", version = "=4.0.0-alpha.0" }
solana-sbf-rust-128bit-dep = { path = "rust/128bit_dep", version = "=4.0.0-alpha.0" }
solana-sbf-rust-invoke-dep = { path = "rust/invoke_dep", version = "=4.0.0-alpha.0" }
solana-sbf-rust-invoked-dep = { path = "rust/invoked_dep", version = "=4.0.0-alpha.0" }
solana-sbf-rust-many-args-dep = { path = "rust/many_args_dep", version = "=4.0.0-alpha.0" }
solana-sbf-rust-mem-dep = { path = "rust/mem_dep", version = "=4.0.0-alpha.0" }
solana-sbf-rust-param-passing-dep = { path = "rust/param_passing_dep", version = "=4.0.0-alpha.0" }
solana-sbf-rust-r2-instruction-data-pointer = { path = "rust/r2_instruction_data_pointer", version = "=4.0.0-alpha.0" }
solana-sbf-rust-realloc-dep = { path = "rust/realloc_dep", version = "=4.0.0-alpha.0" }
solana-sbf-rust-realloc-invoke-dep = { path = "rust/realloc_invoke_dep", version = "=4.0.0-alpha.0" }
solana-sbpf = "=0.13.1"
solana-sdk-ids = "=3.0.0"
solana-secp256k1-recover = "=3.0.0"
solana-sha256-hasher = { version = "=3.1.0", features = ["sha2"] }
solana-stake-interface = { version = "=2.0.1", features = ["bincode"] }
solana-svm = { path = "../../svm", version = "=4.0.0-alpha.0" }
solana-svm-callback = { path = "../../svm-callback", version = "=4.0.0-alpha.0" }
solana-svm-feature-set = { path = "../../svm-feature-set", version = "=4.0.0-alpha.0" }
solana-svm-log-collector = { path = "../../svm-log-collector", version = "=4.0.0-alpha.0" }
solana-svm-test-harness = { path = "../../svm-test-harness", version = "=4.0.0-alpha.0" }
solana-svm-timings = { path = "../../svm-timings", version = "=4.0.0-alpha.0" }
solana-svm-transaction = { path = "../../svm-transaction", version = "=4.0.0-alpha.0" }
solana-svm-type-overrides = { path = "../../svm-type-overrides", version = "=4.0.0-alpha.0" }
solana-system-interface = { version = "=2.0", features = ["bincode"] }
solana-sysvar = "=3.0.0"
solana-transaction-context = { path = "../../transaction-context", version = "=4.0.0-alpha.0" }
solana-transaction-status = { path = "../../transaction-status", version = "=4.0.0-alpha.0" }
solana-vote = { path = "../../vote", version = "=4.0.0-alpha.0" }
solana-vote-program = { path = "../../programs/vote", version = "=4.0.0-alpha.0" }
test-case = "3.3.1"
thiserror = "1.0"

[features]
sbf_c = []
sbf_rust = []
sbf_sanity_list = []
dummy-for-ci-check = ["sbf_c", "sbf_rust", "sbf_sanity_list"]
# This was needed for ci
frozen-abi = []

[dev-dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
agave-reserved-account-keys = { workspace = true }
agave-syscalls = { workspace = true }
agave-validator = { workspace = true }
bincode = { workspace = true }
borsh = { workspace = true }
byteorder = { workspace = true }
elf = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
miow = { workspace = true }
net2 = { workspace = true }
solana-account = "3.2.0"
solana-account-decoder = { workspace = true }
solana-account-info = "3.0.0"
solana-accounts-db = { workspace = true }
solana-bpf-loader-program = { workspace = true }
solana-cli-output = { workspace = true }
solana-client-traits = "3.0.0"
solana-clock = "3.0.0"
solana-cluster-type = "3.0.0"
solana-compute-budget = { workspace = true }
solana-compute-budget-instruction = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-compute-budget-interface = "3.0.0"
solana-fee = { workspace = true }
solana-fee-calculator = "3.0.0"
solana-fee-structure = "3.0.0"
solana-genesis-config = "3.0.0"
solana-hash = "3.1.0"
solana-instruction = "3.0.0"
solana-keypair = "3.0.0"
solana-ledger = { workspace = true }
solana-loader-v3-interface = "6.1.0"
solana-loader-v4-interface = "3.1.0"
solana-measure = { workspace = true }
solana-message = "3.0.0"
solana-program = { workspace = true }
solana-program-entrypoint = "3.1.0"
solana-program-runtime = { workspace = true }
solana-pubkey = "3.0.0"
solana-rent = "3.0.0"
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime-transaction = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-sbf-rust-invoke-dep = { workspace = true }
solana-sbf-rust-realloc-dep = { workspace = true }
solana-sbf-rust-realloc-invoke-dep = { workspace = true }
solana-sbpf = { workspace = true, features = ["jit"] }
solana-sdk-ids = "3.0.0"
solana-signer = "3.0.0"
solana-stake-interface = "2.0.1"
solana-svm = { workspace = true }
solana-svm-callback = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-test-harness = { workspace = true }
solana-svm-timings = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-system-interface = "2.0"
solana-sysvar = "3.0.0"
solana-transaction = "3.0.2"
solana-transaction-context = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-transaction-error = "3.0.0"
solana-transaction-status = { workspace = true }
solana-vote = { workspace = true }
solana-vote-program = { workspace = true }
test-case = { workspace = true }

[profile.release]
# The test programs are build in release mode
# Minimize their file size so that they fit into the account size limit
strip = true

[[bench]]
name = "bpf_loader"

[patch.crates-io]
# We include the following crates as our dependencies from crates.io:
#
#  * spl-associated-token-account-interface
#  * spl-instruction-padding
#  * spl-memo-interface
#  * spl-pod
#  * spl-token
#  * spl-token-2022-interface
#  * spl-token-metadata-interface
#
# They are included indirectly, for example, `account-decoder` depends on
#
#     solana-sdk = { workspace = true }
#
# and that is specified as
#
#     spl-token = "=3.5.0"
#
# in `../../Cargo.toml`.
#
# `spl-token`, in turn, depends on `solana-program`, which we explicitly specify
# above as a local path dependency:
#
#     solana-program = { path = "../../sdk/program", version = "=1.16.0" }
#
# Unfortunately, Cargo will try to resolve the `spl-token` `solana-program`
# dependency only using what is available on crates.io.  Crates.io normally
# contains a previous version of these crates, and we end up with two versions
# of `solana-program` and `solana-zk-token-sdk` and all of their dependencies in
# our build tree.
#
# If you are developing downstream using non-crates-io solana-program (local or
# forked repo, or from github rev, eg), duplicate the following patch statements
# in your Cargo.toml. If you still hit duplicate-type errors with the patch
# statements in place, run `cargo update -p solana-program` and/or `cargo update
# -p solana-zk-token-sdk` to remove extraneous versions from your Cargo.lock
# file.
#
# There is a similar override in `../../Cargo.toml`.  Please keep both comments
# and the overrides in sync.
solana-curve25519 = { path = "../../curves/curve25519" }

================
File: programs/sbf/Makefile
================
SBF_SDK_PATH := ../../platform-tools-sdk/sbf
SRC_DIR := c/src
OUT_DIR := target/deploy
TOOLCHAIN := 1.89.0-sbpf-solana-v1.52

test-v2:
	VER=v2 $(MAKE) test-version

test-v1:
	VER=v1 $(MAKE) test-version

test-v0: all rust-v0 test-all

clean-all:
	rm -rf target/deploy target/sbpf*

test-all:
	SBF_OUT_DIR=$(OUT_DIR) cargo test --features="sbf_rust,sbf_c" $(TEST_ARGS)

test-rust:
	SBF_OUT_DIR=$(OUT_DIR) cargo test --features="sbf_rust" $(TEST_ARGS)

test-version:
	SBPF_CPU=$(VER) $(MAKE) all ; \
	$(MAKE) rust-new ; \
	$(MAKE) test-all

rust-v0:
	cargo +$(TOOLCHAIN) build --release --target sbpf-solana-solana --workspace ; \
	cp -r target/sbpf-solana-solana/release/* target/deploy

rust-new:
	RUSTFLAGS="-C instrument-coverage=no" cargo +$(TOOLCHAIN) build --release --target sbpf$(VER)-solana-solana --workspace ; \
	cp -r target/sbpf$(VER)-solana-solana/release/* target/deploy

.PHONY: test-v0

include $(SBF_SDK_PATH)/c/sbf.mk

================
File: programs/system/benches/system.rs
================
struct TestSetup {
⋮----
impl TestSetup {
fn new() -> Self {
⋮----
Pubkey::create_with_seed(&base_address, SEED, &owner_address).unwrap();
let transaction_accounts = vec![
⋮----
fn prep_create_account(&mut self) {
self.instruction_accounts = vec![
⋮----
.unwrap();
⋮----
fn prep_create_account_with_seed(&mut self) {
⋮----
seed: SEED.to_string(),
⋮----
fn prep_allocate(&mut self) {
self.instruction_accounts = vec![AccountMeta {
⋮----
bincode::serialize(&SystemInstruction::Allocate { space: 2 }).unwrap();
⋮----
fn prep_allocate_with_seed(&mut self) {
⋮----
fn prep_assign(&mut self) {
⋮----
fn prep_assign_with_seed(&mut self) {
⋮----
fn prep_transfer(&mut self) {
⋮----
bincode::serialize(&SystemInstruction::Transfer { lamports: 1 }).unwrap();
⋮----
fn prep_transfer_with_seed(&mut self) {
self.transaction_accounts[1].1.set_lamports(ACCOUNT_BALANCE);
⋮----
from_seed: SEED.to_string(),
⋮----
fn prep_initialize_nonce_account(&mut self) {
⋮----
self.transaction_accounts = vec![
⋮----
bincode::serialize(&SystemInstruction::InitializeNonceAccount(nonce_address)).unwrap();
⋮----
fn prep_authorize_nonce_account(&mut self) {
⋮----
self.transaction_accounts = vec![(nonce_address, nonce_account)];
⋮----
bincode::serialize(&SystemInstruction::AuthorizeNonceAccount(nonce_address)).unwrap();
⋮----
fn prep_advance_nonce_account(&mut self) {
⋮----
bincode::serialize(&SystemInstruction::AdvanceNonceAccount).unwrap();
⋮----
fn prep_upgrade_nonce_account(&mut self) {
⋮----
bincode::serialize(&SystemInstruction::UpgradeNonceAccount).unwrap();
⋮----
fn prep_withdraw_nonce_account(&mut self) {
⋮----
bincode::serialize(&SystemInstruction::WithdrawNonceAccount(1)).unwrap();
⋮----
fn run(&self) {
mock_process_instruction(
⋮----
self.transaction_accounts.clone(),
self.instruction_accounts.clone(),
Ok(()),
⋮----
fn bench_create_account(c: &mut Criterion) {
⋮----
test_setup.prep_create_account();
c.bench_function("create_account", |bencher| {
bencher.iter(|| test_setup.run())
⋮----
fn bench_create_account_with_seed(c: &mut Criterion) {
⋮----
test_setup.prep_create_account_with_seed();
c.bench_function("create_account_with_seed", |bencher| {
⋮----
fn bench_allocate(c: &mut Criterion) {
⋮----
test_setup.prep_allocate();
c.bench_function("allocate", |bencher| bencher.iter(|| test_setup.run()));
⋮----
fn bench_allocate_with_seed(c: &mut Criterion) {
⋮----
test_setup.prep_allocate_with_seed();
c.bench_function("allocate_with_seed", |bencher| {
⋮----
fn bench_assign(c: &mut Criterion) {
⋮----
test_setup.prep_assign();
c.bench_function("assign", |bencher| bencher.iter(|| test_setup.run()));
⋮----
fn bench_assign_with_seed(c: &mut Criterion) {
⋮----
test_setup.prep_assign_with_seed();
c.bench_function("assign_with_seed", |bencher| {
⋮----
fn bench_transfer(c: &mut Criterion) {
⋮----
test_setup.prep_transfer();
c.bench_function("transfer", |bencher| bencher.iter(|| test_setup.run()));
⋮----
fn bench_transfer_with_seed(c: &mut Criterion) {
⋮----
test_setup.prep_transfer_with_seed();
c.bench_function("transfer_with_seed", |bencher| {
⋮----
fn bench_initialize_nonce_account(c: &mut Criterion) {
⋮----
test_setup.prep_initialize_nonce_account();
c.bench_function("initialize_nonce_account", |bencher| {
⋮----
fn bench_authorize_nonce_account(c: &mut Criterion) {
⋮----
test_setup.prep_authorize_nonce_account();
c.bench_function("authorize_nonce_account", |bencher| {
⋮----
fn bench_advance_nonce_account(c: &mut Criterion) {
⋮----
test_setup.prep_advance_nonce_account();
c.bench_function("advance_nonce_account", |bencher| {
⋮----
fn bench_upgrade_nonce_account(c: &mut Criterion) {
⋮----
test_setup.prep_upgrade_nonce_account();
c.bench_function("upgrade_nonce_account", |bencher| {
⋮----
fn bench_withdraw_nonce_account(c: &mut Criterion) {
⋮----
test_setup.prep_withdraw_nonce_account();
c.bench_function("withdraw_nonce_account", |bencher| {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: programs/system/src/lib.rs
================
pub mod system_instruction;
pub mod system_processor;
use solana_sdk_ids::system_program;

================
File: programs/system/src/system_instruction.rs
================
fn checked_add(a: u64, b: u64) -> Result<u64, InstructionError> {
a.checked_add(b).ok_or(InstructionError::InsufficientFunds)
⋮----
pub fn advance_nonce_account(
⋮----
if !account.is_writable() {
ic_msg!(
⋮----
return Err(InstructionError::InvalidArgument);
⋮----
let state: Versions = account.get_state()?;
match state.state() {
⋮----
if !signers.contains(&data.authority) {
⋮----
return Err(InstructionError::MissingRequiredSignature);
⋮----
return Err(SystemError::NonceBlockhashNotExpired.into());
⋮----
account.set_state(&Versions::new(State::Initialized(new_data)))
⋮----
Err(InstructionError::InvalidAccountData)
⋮----
pub(crate) fn withdraw_nonce_account(
⋮----
let mut from = instruction_context.try_borrow_instruction_account(from_account_index)?;
if !from.is_writable() {
⋮----
let state: Versions = from.get_state()?;
let signer = match state.state() {
⋮----
if lamports > from.get_lamports() {
⋮----
return Err(InstructionError::InsufficientFunds);
⋮----
*from.get_key()
⋮----
if lamports == from.get_lamports() {
⋮----
from.set_state(&Versions::new(State::Uninitialized))?;
⋮----
let min_balance = rent.minimum_balance(from.get_data().len());
let amount = checked_add(lamports, min_balance)?;
if amount > from.get_lamports() {
⋮----
if !signers.contains(&signer) {
⋮----
from.checked_sub_lamports(lamports)?;
drop(from);
let mut to = instruction_context.try_borrow_instruction_account(to_account_index)?;
to.checked_add_lamports(lamports)?;
Ok(())
⋮----
pub(crate) fn initialize_nonce_account(
⋮----
match account.get_state::<Versions>()?.state() {
⋮----
let min_balance = rent.minimum_balance(account.get_data().len());
if account.get_lamports() < min_balance {
⋮----
account.set_state(&Versions::new(state))
⋮----
pub(crate) fn authorize_nonce_account(
⋮----
.authorize(signers, *nonce_authority)
⋮----
Ok(versions) => account.set_state(&versions),
⋮----
Err(InstructionError::MissingRequiredSignature)
⋮----
mod test {
⋮----
macro_rules! push_instruction_context {
⋮----
macro_rules! prepare_mockup {
⋮----
macro_rules! set_invoke_context_blockhash {
⋮----
fn default_is_uninitialized() {
assert_eq!(State::default(), State::Uninitialized)
⋮----
fn expected_behavior() {
prepare_mockup!(
⋮----
push_instruction_context!(invoke_context, instruction_context, instruction_accounts);
⋮----
.try_borrow_instruction_account(NONCE_ACCOUNT_INDEX)
.unwrap();
⋮----
authority: *nonce_account.get_key(),
⋮----
signers.insert(*nonce_account.get_key());
let versions = nonce_account.get_state::<Versions>().unwrap();
assert_eq!(versions.state(), &State::Uninitialized);
set_invoke_context_blockhash!(invoke_context, 95);
let authorized = *nonce_account.get_key();
initialize_nonce_account(&mut nonce_account, &authorized, &rent, &invoke_context).unwrap();
⋮----
assert_eq!(versions.state(), &State::Initialized(data.clone()));
set_invoke_context_blockhash!(invoke_context, 63);
advance_nonce_account(&mut nonce_account, &signers, &invoke_context).unwrap();
⋮----
set_invoke_context_blockhash!(invoke_context, 31);
⋮----
assert_eq!(versions.state(), &State::Initialized(data));
set_invoke_context_blockhash!(invoke_context, 0);
⋮----
.try_borrow_instruction_account(WITHDRAW_TO_ACCOUNT_INDEX)
⋮----
let withdraw_lamports = nonce_account.get_lamports();
let expect_nonce_lamports = nonce_account.get_lamports() - withdraw_lamports;
let expect_to_lamports = to_account.get_lamports() + withdraw_lamports;
drop(nonce_account);
drop(to_account);
withdraw_nonce_account(
⋮----
assert_eq!(nonce_account.get_lamports(), expect_nonce_lamports);
assert_eq!(to_account.get_lamports(), expect_to_lamports);
⋮----
fn nonce_inx_initialized_account_not_signer_fail() {
⋮----
let authority = *nonce_account.get_key();
initialize_nonce_account(&mut nonce_account, &authority, &rent, &invoke_context).unwrap();
⋮----
let result = advance_nonce_account(&mut nonce_account, &signers, &invoke_context);
assert_eq!(result, Err(InstructionError::MissingRequiredSignature));
⋮----
fn nonce_inx_too_early_fail() {
⋮----
assert_eq!(result, Err(SystemError::NonceBlockhashNotExpired.into()));
⋮----
fn nonce_inx_uninitialized_account_fail() {
⋮----
assert_eq!(result, Err(InstructionError::InvalidAccountData));
⋮----
fn nonce_inx_independent_nonce_authority_ok() {
⋮----
.try_borrow_instruction_account(NONCE_ACCOUNT_INDEX + 1)
⋮----
let authorized = *nonce_authority.get_key();
⋮----
signers.insert(authorized);
⋮----
assert_eq!(result, Ok(()));
⋮----
fn nonce_inx_no_nonce_authority_sig_fail() {
⋮----
fn withdraw_inx_unintialized_acc_ok() {
⋮----
let expect_from_lamports = nonce_account.get_lamports() - withdraw_lamports;
⋮----
assert_eq!(nonce_account.get_lamports(), expect_from_lamports);
⋮----
fn withdraw_inx_unintialized_acc_unsigned_fail() {
⋮----
let result = withdraw_nonce_account(
⋮----
fn withdraw_inx_unintialized_acc_insuff_funds_fail() {
⋮----
let withdraw_lamports = nonce_account.get_lamports() + 1;
⋮----
assert_eq!(result, Err(InstructionError::InsufficientFunds));
⋮----
fn withdraw_inx_uninitialized_acc_two_withdraws_ok() {
⋮----
let withdraw_lamports = nonce_account.get_lamports() / 2;
let from_expect_lamports = nonce_account.get_lamports() - withdraw_lamports;
let to_expect_lamports = to_account.get_lamports() + withdraw_lamports;
⋮----
assert_eq!(nonce_account.get_lamports(), from_expect_lamports);
assert_eq!(to_account.get_lamports(), to_expect_lamports);
⋮----
fn withdraw_inx_initialized_acc_two_withdraws_ok() {
⋮----
fn withdraw_inx_initialized_acc_nonce_too_early_fail() {
⋮----
fn withdraw_inx_initialized_acc_insuff_funds_fail() {
⋮----
fn withdraw_inx_initialized_acc_insuff_rent_fail() {
⋮----
fn withdraw_inx_overflow() {
⋮----
fn initialize_inx_ok() {
⋮----
initialize_nonce_account(&mut nonce_account, &authorized, &rent, &invoke_context);
⋮----
fn initialize_inx_initialized_account_fail() {
⋮----
fn initialize_inx_uninitialized_acc_insuff_funds_fail() {
⋮----
nonce_account.checked_sub_lamports(42 * 2).unwrap();
⋮----
fn authorize_inx_ok() {
⋮----
authorize_nonce_account(&mut nonce_account, &authority, &signers, &invoke_context).unwrap();
⋮----
fn authorize_inx_uninitialized_state_fail() {
⋮----
let result = authorize_nonce_account(
⋮----
fn authorize_inx_bad_authority_fail() {
⋮----
authorize_nonce_account(&mut nonce_account, &authorized, &signers, &invoke_context);
⋮----
fn verify_nonce_ok() {
⋮----
signers.insert(nonce_account.get_key());
let versions: Versions = nonce_account.get_state().unwrap();
⋮----
initialize_nonce_account(&mut nonce_account, &authorized, &rent, &invoke_context)
⋮----
transaction_context.pop().unwrap();
let post_accounts = transaction_context.deconstruct_without_keys().unwrap();
assert_matches!(
⋮----
fn verify_nonce_bad_acc_state_fail() {
⋮----
push_instruction_context!(invoke_context, _instruction_context, instruction_accounts);
⋮----
assert_eq!(
⋮----
fn verify_nonce_bad_query_hash_fail() {
⋮----
initialize_nonce_account(
⋮----
set_invoke_context_blockhash!(invoke_context, 1);

================
File: programs/system/src/system_processor.rs
================
struct Address {
⋮----
impl Address {
fn is_signer(&self, signers: &HashSet<Pubkey>) -> bool {
⋮----
signers.contains(&base)
⋮----
signers.contains(&self.address)
⋮----
fn create(
⋮----
Pubkey::create_with_seed(base, seed, owner).map_err(|e| e as u64)?;
⋮----
ic_msg!(
⋮----
return Err(SystemError::AddressWithSeedMismatch.into());
⋮----
Some(*base)
⋮----
Ok(Self {
⋮----
fn allocate(
⋮----
if !address.is_signer(signers) {
⋮----
return Err(InstructionError::MissingRequiredSignature);
⋮----
if !account.get_data().is_empty() || !system_program::check_id(account.get_owner()) {
⋮----
return Err(SystemError::AccountAlreadyInUse.into());
⋮----
return Err(SystemError::InvalidAccountDataLength.into());
⋮----
account.set_data_length(space as usize)?;
Ok(())
⋮----
fn assign(
⋮----
if account.get_owner() == owner {
return Ok(());
⋮----
ic_msg!(invoke_context, "Assign: account {:?} must sign", address);
⋮----
account.set_owner(&owner.to_bytes())
⋮----
fn allocate_and_assign(
⋮----
allocate(to, to_address, space, signers, invoke_context)?;
assign(to, to_address, owner, signers, invoke_context)
⋮----
fn create_account(
⋮----
let mut to = instruction_context.try_borrow_instruction_account(to_account_index)?;
if to.get_lamports() > 0 {
⋮----
allocate_and_assign(&mut to, to_address, space, owner, signers, invoke_context)?;
⋮----
transfer(
⋮----
fn transfer_verified(
⋮----
let mut from = instruction_context.try_borrow_instruction_account(from_account_index)?;
if !from.get_data().is_empty() {
ic_msg!(invoke_context, "Transfer: `from` must not carry data");
return Err(InstructionError::InvalidArgument);
⋮----
if lamports > from.get_lamports() {
⋮----
return Err(SystemError::ResultWithNegativeLamports.into());
⋮----
from.checked_sub_lamports(lamports)?;
drop(from);
⋮----
to.checked_add_lamports(lamports)?;
⋮----
fn transfer(
⋮----
if !instruction_context.is_instruction_account_signer(from_account_index)? {
⋮----
transfer_verified(
⋮----
fn transfer_with_seed(
⋮----
if !instruction_context.is_instruction_account_signer(from_base_account_index)? {
⋮----
instruction_context.get_key_of_instruction_account(from_base_account_index)?,
⋮----
.map_err(|e| e as u64)?;
let from_key = instruction_context.get_key_of_instruction_account(from_account_index)?;
⋮----
declare_process_instruction!(Entrypoint, DEFAULT_COMPUTE_UNITS, |invoke_context| {
⋮----
mod tests {
⋮----
fn from(address: Pubkey) -> Self {
⋮----
fn process_instruction(
⋮----
mock_process_instruction(
⋮----
fn create_default_account() -> AccountSharedData {
⋮----
fn create_recent_blockhashes_account_for_test<'a, I>(
⋮----
let recent_blockhash_iter = sorted_iter.take(MAX_ENTRIES);
let recent_blockhashes: RecentBlockhashes = recent_blockhash_iter.collect();
to_account(&recent_blockhashes, &mut account);
⋮----
fn create_default_recent_blockhashes_account() -> AccountSharedData {
⋮----
create_recent_blockhashes_account_for_test(vec![
⋮----
fn create_default_rent_account() -> AccountSharedData {
⋮----
fn test_create_account() {
⋮----
let accounts = process_instruction(
⋮----
.unwrap(),
vec![(from, from_account), (to, to_account)],
vec![
⋮----
Ok(()),
⋮----
assert_eq!(accounts[0].lamports(), 50);
assert_eq!(accounts[1].lamports(), 50);
assert_eq!(accounts[1].owner(), &new_owner);
assert_eq!(accounts[1].data(), &[0, 0]);
⋮----
fn test_create_account_with_seed() {
⋮----
let to = Pubkey::create_with_seed(&from, seed, &new_owner).unwrap();
⋮----
seed: seed.to_string(),
⋮----
fn test_create_account_with_seed_separate_base_account() {
⋮----
let to = Pubkey::create_with_seed(&base, seed, &new_owner).unwrap();
⋮----
vec![(from, from_account), (to, to_account), (base, base_account)],
⋮----
fn test_address_create_with_seed_mismatch() {
with_mock_invoke_context!(invoke_context, transaction_context, Vec::new());
⋮----
assert_eq!(
⋮----
fn test_create_account_with_seed_missing_sig() {
⋮----
Err(InstructionError::MissingRequiredSignature),
⋮----
assert_eq!(accounts[0].lamports(), 100);
assert_eq!(accounts[1], AccountSharedData::default());
⋮----
fn test_create_with_zero_lamports() {
⋮----
assert_eq!(accounts[1].lamports(), 0);
assert_eq!(*accounts[1].owner(), new_owner);
⋮----
fn test_create_negative_lamports() {
⋮----
process_instruction(
⋮----
Err(SystemError::ResultWithNegativeLamports.into()),
⋮----
fn test_request_more_than_allowed_data_length() {
⋮----
let instruction_accounts = vec![
⋮----
vec![(from, from_account.clone()), (to, to_account.clone())],
instruction_accounts.clone(),
Err(SystemError::InvalidAccountDataLength.into()),
⋮----
assert_eq!(accounts[1].data().len() as u64, MAX_PERMITTED_DATA_LENGTH);
⋮----
fn test_create_already_in_use() {
⋮----
let unchanged_account = owned_account.clone();
⋮----
vec![(from, from_account.clone()), (owned_key, owned_account)],
⋮----
Err(SystemError::AccountAlreadyInUse.into()),
⋮----
assert_eq!(accounts[1], unchanged_account);
⋮----
vec![(from, from_account), (owned_key, owned_account)],
⋮----
fn test_create_unsigned() {
⋮----
fn test_create_sysvar_invalid_id_with_feature() {
⋮----
fn test_create_data_populated() {
⋮----
data: vec![0, 1, 2, 3],
⋮----
vec![(from, from_account), (populated_key, populated_account)],
⋮----
fn test_create_from_account_is_nonce_fail() {
⋮----
.unwrap();
⋮----
vec![(nonce, nonce_account), (new, new_account)],
⋮----
Err(InstructionError::InvalidArgument),
⋮----
fn test_assign() {
⋮----
vec![(pubkey, account.clone())],
vec![AccountMeta {
⋮----
&bincode::serialize(&SystemInstruction::Assign { owner: new_owner }).unwrap(),
⋮----
vec![(pubkey, account)],
⋮----
fn test_process_bogus_instruction() {
⋮----
let data = serialize(&instruction).unwrap();
⋮----
Err(InstructionError::MissingAccount),
⋮----
vec![(from, from_account)],
⋮----
fn test_transfer_lamports() {
⋮----
let transaction_accounts = vec![(from, from_account), (to, to_account)];
⋮----
&bincode::serialize(&SystemInstruction::Transfer { lamports: 50 }).unwrap(),
transaction_accounts.clone(),
⋮----
assert_eq!(accounts[1].lamports(), 51);
⋮----
&bincode::serialize(&SystemInstruction::Transfer { lamports: 101 }).unwrap(),
⋮----
assert_eq!(accounts[1].lamports(), 1);
⋮----
&bincode::serialize(&SystemInstruction::Transfer { lamports: 0 }).unwrap(),
⋮----
fn test_transfer_with_seed() {
⋮----
let from_seed = "42".to_string();
⋮----
let from = Pubkey::create_with_seed(&base, from_seed.as_str(), &from_owner).unwrap();
⋮----
vec![(from, from_account), (base, base_account), (to, to_account)];
⋮----
from_seed: from_seed.clone(),
⋮----
assert_eq!(accounts[2].lamports(), 51);
⋮----
assert_eq!(accounts[2].lamports(), 1);
⋮----
fn test_transfer_lamports_from_nonce_account_fail() {
⋮----
fn process_nonce_instruction(
⋮----
.iter()
.map(|meta| {
⋮----
create_default_recent_blockhashes_account()
⋮----
.collect();
⋮----
fn test_process_nonce_ix_no_acc_data_fail() {
⋮----
process_nonce_instruction(
⋮----
Err(InstructionError::InvalidAccountData),
⋮----
fn test_process_nonce_ix_no_keyed_accs_fail() {
⋮----
&serialize(&SystemInstruction::AdvanceNonceAccount).unwrap(),
⋮----
fn test_process_nonce_ix_only_nonce_acc_fail() {
⋮----
vec![(pubkey, create_default_account())],
⋮----
fn test_process_nonce_ix_ok() {
⋮----
let nonce_account = nonce_account::create_account(1_000_000).into_inner();
⋮----
&serialize(&SystemInstruction::InitializeNonceAccount(nonce_address)).unwrap(),
⋮----
let blockhash = hash(&serialize(&0).unwrap());
⋮----
let new_recent_blockhashes_account = create_recent_blockhashes_account_for_test(vec![
⋮----
invoke_context.environment_config.blockhash = hash(&serialize(&0).unwrap());
⋮----
fn test_process_withdraw_ix_no_acc_data_fail() {
⋮----
fn test_process_withdraw_ix_no_keyed_accs_fail() {
⋮----
&serialize(&SystemInstruction::WithdrawNonceAccount(42)).unwrap(),
⋮----
fn test_process_withdraw_ix_only_nonce_acc_fail() {
⋮----
vec![(nonce_address, create_default_account())],
⋮----
fn test_process_withdraw_ix_ok() {
⋮----
fn test_process_initialize_ix_no_keyed_accs_fail() {
⋮----
&serialize(&SystemInstruction::InitializeNonceAccount(Pubkey::default())).unwrap(),
⋮----
fn test_process_initialize_ix_only_nonce_acc_fail() {
⋮----
vec![(nonce_address, nonce_account)],
⋮----
fn test_process_initialize_ix_ok() {
⋮----
fn test_process_authorize_ix_ok() {
⋮----
&serialize(&SystemInstruction::AuthorizeNonceAccount(nonce_address)).unwrap(),
vec![(nonce_address, accounts[0].clone())],
⋮----
fn test_process_authorize_bad_account_data_fail() {
⋮----
fn test_nonce_initialize_with_empty_recent_blockhashes_fail() {
⋮----
let new_recent_blockhashes_account = create_recent_blockhashes_account_for_test(vec![]);
⋮----
Err(SystemError::NonceNoRecentBlockhashes.into()),
⋮----
fn test_nonce_advance_with_empty_recent_blockhashes_fail() {
⋮----
fn test_nonce_account_upgrade_check_owner() {
⋮----
&serialize(&SystemInstruction::UpgradeNonceAccount).unwrap(),
vec![(nonce_address, nonce_account.clone())],
⋮----
Err(InstructionError::InvalidAccountOwner),
⋮----
assert_eq!(accounts.len(), 1);
assert_eq!(accounts[0], nonce_account);
⋮----
fn new_nonce_account(versions: NonceVersions) -> AccountSharedData {
⋮----
fn test_nonce_account_upgrade() {
⋮----
let nonce_account = new_nonce_account(versions);
⋮----
let versions = NonceVersions::Legacy(Box::new(NonceState::Initialized(data.clone())));
⋮----
let mut accounts = process_instruction(
⋮----
let nonce_account = accounts.remove(0);
let durable_nonce = DurableNonce::from_blockhash(durable_nonce.as_hash());
assert_ne!(data.durable_nonce, durable_nonce);
⋮----
fn test_assign_native_loader_and_transfer() {
⋮----
assert_eq!(accounts[0].owner(), &solana_sdk_ids::native_loader::id());
⋮----
assert_eq!(accounts[1].owner(), &solana_sdk_ids::native_loader::id());
assert_eq!(accounts[1].lamports(), 150);

================
File: programs/system/Cargo.toml
================
[package]
name = "solana-system-program"
description = "Solana System program"
documentation = "https://docs.rs/solana-system-program"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_system_program"

[features]
agave-unstable-api = []

[dependencies]
bincode = { workspace = true }
log = { workspace = true }
serde = { workspace = true }
solana-account = { workspace = true }
solana-bincode = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-instruction = { workspace = true }
solana-nonce = { workspace = true, features = ["serde"] }
solana-nonce-account = { workspace = true }
solana-packet = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true, features = ["sha2"] }
solana-sdk-ids = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-system-interface = { workspace = true, features = ["serde"] }
solana-sysvar = { workspace = true }
solana-transaction-context = { workspace = true, features = ["bincode"] }

[dev-dependencies]
agave-feature-set = { workspace = true }
assert_matches = { workspace = true }
criterion = { workspace = true }
solana-compute-budget = { workspace = true }
solana-hash = { workspace = true }
solana-nonce-account = { workspace = true }
solana-rent = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-svm-callback = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-system-program = { path = ".", features = ["agave-unstable-api"] }
solana-transaction-context = { workspace = true, features = ["dev-context-only-utils"] }

[[bench]]
name = "system"
harness = false

================
File: programs/vote/benches/process_vote.rs
================
extern crate test;
⋮----
fn create_accounts() -> (
⋮----
slot_hashes.add(i as Slot, Hash::new_unique());
⋮----
vote_state.process_next_vote_slot(next_vote_slot, 0, 0);
⋮----
let mut vote_account_data: Vec<u8> = vec![0; VoteStateV3::size_of()];
⋮----
VoteStateV3::serialize(&versioned, &mut vote_account_data).unwrap();
⋮----
let transaction_accounts = vec![
⋮----
.map(|index_in_callee| AccountMeta {
pubkey: transaction_accounts[1usize.saturating_add(index_in_callee)].0,
⋮----
fn bench_process_deprecated_vote_instruction(
⋮----
deprecated_feature_set.deactivate(&deprecate_legacy_vote_ixs::id());
bencher.iter(|| {
mock_process_instruction_with_feature_set(
⋮----
transaction_accounts.clone(),
instruction_account_metas.clone(),
Ok(()),
⋮----
&deprecated_feature_set.runtime_features(),
⋮----
fn bench_process_vote_instruction(
⋮----
mock_process_instruction(
⋮----
fn bench_process_vote(bencher: &mut Bencher) {
⋮----
create_accounts();
⋮----
.saturating_add(num_vote_slots)
.saturating_sub(1);
⋮----
.iter()
.find(|(slot, _hash)| *slot == last_vote_slot)
.unwrap()
⋮----
(num_initial_votes..=last_vote_slot).collect(),
⋮----
let instruction_data = bincode::serialize(&VoteInstruction::Vote(vote)).unwrap();
bench_process_deprecated_vote_instruction(
⋮----
fn bench_process_vote_state_update(bencher: &mut Bencher) {
⋮----
((num_initial_votes.saturating_add(1)..=last_vote_slot).zip((1u32..=31).rev())).collect();
⋮----
vote_state_update.root = Some(num_initial_votes);
⋮----
bincode::serialize(&VoteInstruction::UpdateVoteState(vote_state_update)).unwrap();
⋮----
fn bench_process_tower_sync(bencher: &mut Bencher) {
⋮----
tower_sync.root = Some(num_initial_votes);
⋮----
let instruction_data = bincode::serialize(&VoteInstruction::TowerSync(tower_sync)).unwrap();
bench_process_vote_instruction(

================
File: programs/vote/benches/vote_instructions.rs
================
fn create_default_rent_account() -> AccountSharedData {
⋮----
fn create_default_clock_account() -> AccountSharedData {
⋮----
fn create_accounts() -> (
⋮----
slot_hashes.add(i as Slot, Hash::new_unique());
⋮----
vote_state.process_next_vote_slot(next_vote_slot, 0, 0);
⋮----
let mut vote_account_data: Vec<u8> = vec![0; VoteStateV3::size_of()];
⋮----
VoteStateV3::serialize(&versioned, &mut vote_account_data).unwrap();
⋮----
let transaction_accounts = vec![
⋮----
let instruction_account_metas = vec![
⋮----
fn create_test_account() -> (Pubkey, AccountSharedData) {
⋮----
let balance = rent.minimum_balance(VoteStateV3::size_of());
⋮----
create_v4_account_with_authorized(
⋮----
fn create_test_account_with_authorized() -> (Pubkey, Pubkey, Pubkey, AccountSharedData) {
⋮----
fn process_instruction(
⋮----
mock_process_instruction(
&id(),
⋮----
fn process_deprecated_instruction(
⋮----
deprecated_feature_set.deactivate(&deprecate_legacy_vote_ixs::id());
mock_process_instruction_with_feature_set(
⋮----
&deprecated_feature_set.runtime_features(),
⋮----
struct BenchAuthorize {
⋮----
impl BenchAuthorize {
fn new() -> Self {
let (vote_pubkey, vote_account) = create_test_account();
⋮----
let instruction_data = serialize(&VoteInstruction::Authorize(
⋮----
.unwrap();
⋮----
let instruction_accounts = vec![
⋮----
fn run(&self) {
let _accounts = process_instruction(
⋮----
self.transaction_accounts.clone(),
self.instruction_accounts.clone(),
Ok(()),
⋮----
struct BenchInitializeAccount {
⋮----
impl BenchInitializeAccount {
⋮----
let vote_account = AccountSharedData::new(100, VoteStateV3::size_of(), &id());
⋮----
let instruction_data = serialize(&VoteInstruction::InitializeAccount(VoteInit {
⋮----
pub fn run(&self) {
⋮----
struct BenchVote {
⋮----
impl BenchVote {
pub fn new() -> Self {
⋮----
create_accounts();
⋮----
.saturating_add(num_vote_slots)
.saturating_sub(1);
⋮----
.iter()
.find(|(slot, _hash)| *slot == last_vote_slot)
.unwrap()
⋮----
(num_initial_votes..=last_vote_slot).collect(),
⋮----
assert_eq!(vote.slots.len(), 4);
let instruction_data = serialize(&VoteInstruction::Vote(vote)).unwrap();
⋮----
let _accounts = process_deprecated_instruction(
⋮----
struct BenchWithdraw {
⋮----
impl BenchWithdraw {
⋮----
struct BenchUpdateValidatorIdentity {
⋮----
impl BenchUpdateValidatorIdentity {
⋮----
create_test_account_with_authorized();
⋮----
let instruction_data = serialize(&VoteInstruction::UpdateValidatorIdentity).unwrap();
⋮----
struct BenchUpdateCommission {
⋮----
impl BenchUpdateCommission {
⋮----
let instruction_data = serialize(&VoteInstruction::UpdateCommission(u8::MAX)).unwrap();
⋮----
struct BenchVoteSwitch {
⋮----
impl BenchVoteSwitch {
⋮----
serialize(&VoteInstruction::VoteSwitch(vote, Hash::default())).unwrap();
⋮----
struct BenchAuthorizeChecked {
⋮----
impl BenchAuthorizeChecked {
⋮----
serialize(&VoteInstruction::AuthorizeChecked(VoteAuthorize::Voter)).unwrap();
⋮----
struct BenchUpdateVoteState {
⋮----
impl BenchUpdateVoteState {
fn new(switch: bool) -> Self {
⋮----
((num_initial_votes.saturating_add(1)..=last_vote_slot).zip((1u32..=31).rev()))
.collect();
⋮----
vote_state_update.root = Some(num_initial_votes);
⋮----
serialize(&VoteInstruction::UpdateVoteStateSwitch(
⋮----
serialize(&VoteInstruction::UpdateVoteState(vote_state_update)).unwrap()
⋮----
struct BenchAuthorizeWithSeed {
⋮----
impl BenchAuthorizeWithSeed {
⋮----
Pubkey::create_with_seed(&voter_base_key, voter_seed.as_str(), &voter_owner).unwrap();
⋮----
withdrawer_seed.as_str(),
⋮----
let vote_account = create_v4_account_with_authorized(
⋮----
let instruction_data = serialize(&VoteInstruction::AuthorizeWithSeed(
⋮----
struct BenchAuthorizeCheckedWithSeed {
⋮----
impl BenchAuthorizeCheckedWithSeed {
⋮----
current_authority_seed.as_str(),
⋮----
let instruction_data = serialize(&VoteInstruction::AuthorizeCheckedWithSeed(
⋮----
struct BenchCompactUpdateVoteState {
⋮----
impl BenchCompactUpdateVoteState {
⋮----
let vote = Vote::new(vec![1], Hash::default());
let vote_state_update = VoteStateUpdate::from(vec![(1, 1)]);
let slot_hashes = SlotHashes::new(&[(*vote.slots.last().unwrap(), vote.hash)]);
⋮----
serialize(&VoteInstruction::CompactUpdateVoteStateSwitch(
⋮----
serialize(&VoteInstruction::CompactUpdateVoteState(vote_state_update)).unwrap()
⋮----
struct BenchTowerSync {
⋮----
impl BenchTowerSync {
⋮----
let tower_sync = TowerSync::from(vec![(1, 1)]);
⋮----
serialize(&VoteInstruction::TowerSyncSwitch(
⋮----
serialize(&VoteInstruction::TowerSync(tower_sync)).unwrap()
⋮----
fn bench_initialize_account(c: &mut Criterion) {
⋮----
c.bench_function("vote_instruction_initialize_account", |bencher| {
bencher.iter(|| test_setup.run())
⋮----
fn bench_authorize(c: &mut Criterion) {
⋮----
c.bench_function("vote_instruction_authorize", |bencher| {
⋮----
fn bench_vote(c: &mut Criterion) {
⋮----
c.bench_function("vote_instruction_vote", |bencher| {
⋮----
fn bench_withdraw(c: &mut Criterion) {
⋮----
c.bench_function("vote_instruction_withdraw", |bencher| {
⋮----
fn bench_update_validator_identity(c: &mut Criterion) {
⋮----
c.bench_function("vote_update_validator_identity", |bencher| {
⋮----
fn bench_update_commission(c: &mut Criterion) {
⋮----
c.bench_function("vote_update_commission", |bencher| {
⋮----
fn bench_vote_switch(c: &mut Criterion) {
⋮----
c.bench_function("vote_vote_switch", |bencher| {
⋮----
fn bench_authorize_checked(c: &mut Criterion) {
⋮----
c.bench_function("vote_authorize_checked", |bencher| {
⋮----
fn bench_update_vote_state(c: &mut Criterion) {
⋮----
c.bench_function("vote_update_vote_state", |bencher| {
⋮----
fn bench_update_vote_state_switch(c: &mut Criterion) {
⋮----
c.bench_function("vote_update_vote_state_switch", |bencher| {
⋮----
fn bench_authorize_with_seed(c: &mut Criterion) {
⋮----
c.bench_function("vote_authorize_with_seed", |bencher| {
⋮----
fn bench_authorize_checked_with_seed(c: &mut Criterion) {
⋮----
c.bench_function("vote_authorize_checked_with_seed", |bencher| {
⋮----
fn bench_compact_update_vote_state(c: &mut Criterion) {
⋮----
c.bench_function("vote_compact_update_vote_state", |bencher| {
⋮----
fn bench_compact_update_vote_state_switch(c: &mut Criterion) {
⋮----
c.bench_function("vote_compact_update_vote_state_switch", |bencher| {
⋮----
fn bench_tower_sync(c: &mut Criterion) {
⋮----
c.bench_function("vote_tower_sync", |bencher| {
⋮----
fn bench_tower_sync_switch(c: &mut Criterion) {
⋮----
c.bench_function("vote_tower_sync_switch", |bencher| {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: programs/vote/src/vote_state/handler.rs
================
pub trait VoteStateHandle {
⋮----
fn credits_for_vote_at_index(&self, index: usize) -> u64 {
⋮----
.votes()
.get(index)
.map_or(0, |landed_vote| landed_vote.latency);
⋮----
match latency.checked_sub(VOTE_CREDITS_GRACE_SLOTS) {
⋮----
match VOTE_CREDITS_MAXIMUM_PER_SLOT.checked_sub(diff) {
⋮----
fn increment_credits(&mut self, epoch: Epoch, credits: u64) {
if self.epoch_credits().is_empty() {
self.epoch_credits_mut().push((epoch, 0, 0));
} else if epoch != self.epoch_credits().last().unwrap().0 {
let (_, credits, prev_credits) = *self.epoch_credits().last().unwrap();
⋮----
self.epoch_credits_mut().push((epoch, credits, credits));
⋮----
self.epoch_credits_mut().last_mut().unwrap().0 = epoch;
⋮----
if self.epoch_credits().len() > MAX_EPOCH_CREDITS_HISTORY {
self.epoch_credits_mut().remove(0);
⋮----
self.epoch_credits_mut().last_mut().unwrap().1 = self
.epoch_credits()
.last()
.unwrap()
⋮----
.saturating_add(credits);
⋮----
fn process_timestamp(&mut self, slot: Slot, timestamp: UnixTimestamp) -> Result<(), VoteError> {
let last_timestamp = self.last_timestamp();
⋮----
return Err(VoteError::TimestampTooOld);
⋮----
self.set_last_timestamp(BlockTimestamp { slot, timestamp });
Ok(())
⋮----
fn pop_expired_votes(&mut self, next_vote_slot: Slot) {
while let Some(vote) = self.last_lockout() {
if !vote.is_locked_out_at_slot(next_vote_slot) {
self.votes_mut().pop_back();
⋮----
fn double_lockouts(&mut self) {
let stack_depth = self.votes().len();
for (i, v) in self.votes_mut().iter_mut().enumerate() {
⋮----
> i.checked_add(v.confirmation_count() as usize).expect(
⋮----
v.lockout.increase_confirmation_count(1);
⋮----
fn process_next_vote_slot(&mut self, next_vote_slot: Slot, epoch: Epoch, current_slot: Slot) {
⋮----
.last_voted_slot()
.is_some_and(|last_voted_slot| next_vote_slot <= last_voted_slot)
⋮----
self.pop_expired_votes(next_vote_slot);
⋮----
latency: compute_vote_latency(next_vote_slot, current_slot),
⋮----
if self.votes().len() == MAX_LOCKOUT_HISTORY {
let credits = self.credits_for_vote_at_index(0);
let landed_vote = self.votes_mut().pop_front().unwrap();
self.set_root_slot(Some(landed_vote.slot()));
self.increment_credits(epoch, credits);
⋮----
self.votes_mut().push_back(landed_vote);
self.double_lockouts();
⋮----
fn credits(&self) -> u64 {
⋮----
self.epoch_credits().last().unwrap().1
⋮----
fn nth_recent_lockout(&self, position: usize) -> Option<&Lockout> {
if position < self.votes().len() {
⋮----
.len()
.checked_sub(position)
.and_then(|pos| pos.checked_sub(1))?;
self.votes().get(pos).map(|vote| &vote.lockout)
⋮----
impl VoteStateHandle for VoteStateV3 {
fn authorized_withdrawer(&self) -> &Pubkey {
⋮----
fn set_authorized_withdrawer(&mut self, authorized_withdrawer: Pubkey) {
⋮----
fn authorized_voters(&self) -> &AuthorizedVoters {
⋮----
fn set_new_authorized_voter<F>(
⋮----
let epoch_authorized_voter = self.get_and_update_authorized_voter(current_epoch)?;
verify(epoch_authorized_voter)?;
if self.authorized_voters.contains(target_epoch) {
return Err(VoteError::TooSoonToReauthorize.into());
⋮----
.ok_or(InstructionError::InvalidAccountData)?;
⋮----
self.prior_voters.last().map(|range| range.2).unwrap_or(0);
⋮----
return Err(InstructionError::InvalidAccountData);
⋮----
self.prior_voters.append((
⋮----
.insert(target_epoch, *authorized_pubkey);
⋮----
fn get_and_update_authorized_voter(
⋮----
.get_and_cache_authorized_voter_for_epoch(current_epoch)
⋮----
.purge_authorized_voters(current_epoch);
Ok(pubkey)
⋮----
fn commission(&self) -> u8 {
⋮----
fn set_commission(&mut self, commission: u8) {
⋮----
fn node_pubkey(&self) -> &Pubkey {
⋮----
fn set_node_pubkey(&mut self, node_pubkey: Pubkey) {
⋮----
fn set_block_revenue_collector(&mut self, _collector: Pubkey) {
⋮----
fn votes(&self) -> &VecDeque<LandedVote> {
⋮----
fn votes_mut(&mut self) -> &mut VecDeque<LandedVote> {
⋮----
fn set_votes(&mut self, votes: VecDeque<LandedVote>) {
⋮----
fn contains_slot(&self, candidate_slot: Slot) -> bool {
⋮----
.binary_search_by(|vote| vote.slot().cmp(&candidate_slot))
.is_ok()
⋮----
fn last_lockout(&self) -> Option<&Lockout> {
self.votes.back().map(|vote| &vote.lockout)
⋮----
fn last_voted_slot(&self) -> Option<Slot> {
self.last_lockout().map(|v| v.slot())
⋮----
fn root_slot(&self) -> Option<Slot> {
⋮----
fn set_root_slot(&mut self, root_slot: Option<Slot>) {
⋮----
fn current_epoch(&self) -> Epoch {
if self.epoch_credits.is_empty() {
⋮----
self.epoch_credits.last().unwrap().0
⋮----
fn epoch_credits(&self) -> &Vec<(Epoch, u64, u64)> {
⋮----
fn epoch_credits_mut(&mut self) -> &mut Vec<(Epoch, u64, u64)> {
⋮----
fn last_timestamp(&self) -> &BlockTimestamp {
⋮----
fn set_last_timestamp(&mut self, timestamp: BlockTimestamp) {
⋮----
fn set_vote_account_state(
⋮----
if (vote_account.get_data().len() < VoteStateV3::size_of())
&& (!vote_account.is_rent_exempt_at_data_length(VoteStateV3::size_of())
⋮----
.set_data_length(VoteStateV3::size_of())
.is_err())
⋮----
return vote_account.set_state(&VoteStateVersions::V1_14_11(Box::new(
⋮----
vote_account.set_state(&VoteStateVersions::V3(Box::new(self)))
⋮----
impl VoteStateHandle for VoteStateV4 {
⋮----
.purge_authorized_voters(current_epoch.saturating_sub(1));
⋮----
fn set_block_revenue_collector(&mut self, collector: Pubkey) {
⋮----
if (vote_account.get_data().len() < VoteStateV4::size_of())
&& (!vote_account.is_rent_exempt_at_data_length(VoteStateV4::size_of())
⋮----
.set_data_length(VoteStateV4::size_of())
⋮----
return Err(InstructionError::AccountNotRentExempt);
⋮----
vote_account.set_state(&VoteStateVersions::V4(Box::new(self)))
⋮----
pub(crate) fn create_new_vote_state_v4(
⋮----
pub(crate) fn create_new_vote_state_v4_for_tests(
⋮----
pub enum VoteStateTargetVersion {
⋮----
enum TargetVoteState {
⋮----
pub struct VoteStateHandler {
⋮----
impl VoteStateHandle for VoteStateHandler {
⋮----
TargetVoteState::V3(v3) => v3.authorized_withdrawer(),
TargetVoteState::V4(v4) => v4.authorized_withdrawer(),
⋮----
TargetVoteState::V3(v3) => v3.set_authorized_withdrawer(authorized_withdrawer),
TargetVoteState::V4(v4) => v4.set_authorized_withdrawer(authorized_withdrawer),
⋮----
TargetVoteState::V3(v3) => v3.authorized_voters(),
TargetVoteState::V4(v4) => v4.authorized_voters(),
⋮----
v3.set_new_authorized_voter(authorized_pubkey, current_epoch, target_epoch, verify)
⋮----
v4.set_new_authorized_voter(authorized_pubkey, current_epoch, target_epoch, verify)
⋮----
TargetVoteState::V3(v3) => v3.get_and_update_authorized_voter(current_epoch),
TargetVoteState::V4(v4) => v4.get_and_update_authorized_voter(current_epoch),
⋮----
TargetVoteState::V3(v3) => v3.commission(),
TargetVoteState::V4(v4) => v4.commission(),
⋮----
TargetVoteState::V3(v3) => v3.set_commission(commission),
TargetVoteState::V4(v4) => v4.set_commission(commission),
⋮----
TargetVoteState::V3(v3) => v3.node_pubkey(),
TargetVoteState::V4(v4) => v4.node_pubkey(),
⋮----
TargetVoteState::V3(v3) => v3.set_node_pubkey(node_pubkey),
TargetVoteState::V4(v4) => v4.set_node_pubkey(node_pubkey),
⋮----
TargetVoteState::V3(v3) => v3.set_block_revenue_collector(collector),
TargetVoteState::V4(v4) => v4.set_block_revenue_collector(collector),
⋮----
TargetVoteState::V3(v3) => v3.votes(),
TargetVoteState::V4(v4) => v4.votes(),
⋮----
TargetVoteState::V3(v3) => v3.votes_mut(),
TargetVoteState::V4(v4) => v4.votes_mut(),
⋮----
TargetVoteState::V3(v3) => v3.set_votes(votes),
TargetVoteState::V4(v4) => v4.set_votes(votes),
⋮----
TargetVoteState::V3(v3) => v3.contains_slot(candidate_slot),
TargetVoteState::V4(v4) => v4.contains_slot(candidate_slot),
⋮----
TargetVoteState::V3(v3) => v3.last_lockout(),
TargetVoteState::V4(v4) => v4.last_lockout(),
⋮----
TargetVoteState::V3(v3) => v3.last_voted_slot(),
TargetVoteState::V4(v4) => v4.last_voted_slot(),
⋮----
TargetVoteState::V3(v3) => v3.root_slot(),
TargetVoteState::V4(v4) => v4.root_slot(),
⋮----
TargetVoteState::V3(v3) => v3.set_root_slot(root_slot),
TargetVoteState::V4(v4) => v4.set_root_slot(root_slot),
⋮----
TargetVoteState::V3(v3) => v3.current_epoch(),
TargetVoteState::V4(v4) => v4.current_epoch(),
⋮----
TargetVoteState::V3(v3) => v3.epoch_credits(),
TargetVoteState::V4(v4) => v4.epoch_credits(),
⋮----
TargetVoteState::V3(v3) => v3.epoch_credits_mut(),
TargetVoteState::V4(v4) => v4.epoch_credits_mut(),
⋮----
TargetVoteState::V3(v3) => v3.last_timestamp(),
TargetVoteState::V4(v4) => v4.last_timestamp(),
⋮----
TargetVoteState::V3(v3) => v3.set_last_timestamp(timestamp),
TargetVoteState::V4(v4) => v4.set_last_timestamp(timestamp),
⋮----
TargetVoteState::V3(v3) => v3.set_vote_account_state(vote_account),
TargetVoteState::V4(v4) => v4.set_vote_account_state(vote_account),
⋮----
impl VoteStateHandler {
pub fn init_vote_account_state(
⋮----
VoteStateV3::new(vote_init, clock).set_vote_account_state(vote_account)
⋮----
let vote_state = create_new_vote_state_v4(vote_account.get_key(), vote_init, clock);
vote_state.set_vote_account_state(vote_account)
⋮----
pub fn deinitialize_vote_account_state(
⋮----
VoteStateV3::default().set_vote_account_state(vote_account)
⋮----
vote_account.get_data_mut()?.fill(0);
⋮----
pub fn check_vote_account_length(
⋮----
let length = vote_account.get_data().len();
⋮----
Err(InstructionError::InvalidAccountData)
⋮----
pub(crate) fn new_v3(vote_state: VoteStateV3) -> Self {
⋮----
pub(crate) fn new_v4(vote_state: VoteStateV4) -> Self {
⋮----
pub fn default_v3() -> Self {
⋮----
pub fn default_v4() -> Self {
⋮----
pub fn as_ref_v3(&self) -> &VoteStateV3 {
⋮----
_ => panic!("not a v3"),
⋮----
pub fn as_ref_v4(&self) -> &VoteStateV4 {
⋮----
_ => panic!("not a v4"),
⋮----
pub fn serialize(self) -> Vec<u8> {
⋮----
let mut data = vec![0; VoteStateV3::size_of()];
⋮----
bincode::serialize_into(&mut data[..], &versioned).unwrap();
⋮----
let mut data = vec![0; VoteStateV4::size_of()];
⋮----
pub(crate) fn compute_vote_latency(voted_for_slot: Slot, current_slot: Slot) -> u8 {
std::cmp::min(current_slot.saturating_sub(voted_for_slot), u8::MAX as u64) as u8
⋮----
pub(crate) fn try_convert_to_vote_state_v4(
⋮----
Err(InstructionError::UninitializedAccount)
⋮----
VoteStateVersions::V1_14_11(state) => Ok(VoteStateV4 {
⋮----
inflation_rewards_commission_bps: u16::from(state.commission).saturating_mul(100),
⋮----
votes: landed_votes_from_lockouts(state.votes),
⋮----
authorized_voters: state.authorized_voters.clone(),
⋮----
VoteStateVersions::V3(state) => Ok(VoteStateV4 {
⋮----
VoteStateVersions::V4(state) => Ok(*state),
⋮----
fn landed_votes_from_lockouts(lockouts: VecDeque<Lockout>) -> VecDeque<LandedVote> {
lockouts.into_iter().map(|lockout| lockout.into()).collect()
⋮----
mod tests {
⋮----
fn mock_transaction_context(
⋮----
vec![(id(), program_account), (vote_pubkey, vote_account)],
⋮----
.configure_next_instruction_for_tests(
⋮----
vec![InstructionAccount::new(1, false, true)],
vec![],
⋮----
.unwrap();
⋮----
fn get_max_sized_vote_state_v3() -> VoteStateV3 {
⋮----
authorized_voters.insert(i, Pubkey::new_unique());
⋮----
votes: VecDeque::from(vec![LandedVote::default(); MAX_LOCKOUT_HISTORY]),
root_slot: Some(u64::MAX),
epoch_credits: vec![(0, 0, 0); MAX_EPOCH_CREDITS_HISTORY],
⋮----
fn get_max_sized_vote_state_v4() -> VoteStateV4 {
⋮----
bls_pubkey_compressed: Some([255; BLS_PUBLIC_KEY_COMPRESSED_SIZE]),
⋮----
fn set_new_authorized_voter_and_assert<T: VoteStateHandle>(
⋮----
// Set a new authorized voter
⋮----
.set_new_authorized_voter(&new_voter, 0, epoch_offset, |_| Ok(()))
⋮----
assert_eq!(
⋮----
// Trying to set authorized voter for same epoch again should fail
⋮----
// Setting the same authorized voter again should succeed
⋮----
.set_new_authorized_voter(&new_voter, 2, 2 + epoch_offset, |_| Ok(()))
⋮----
// Set a third and fourth authorized voter
⋮----
.set_new_authorized_voter(&new_voter2, 3, 3 + epoch_offset, |_| Ok(()))
⋮----
.set_new_authorized_voter(&new_voter3, 6, 6 + epoch_offset, |_| Ok(()))
⋮----
// Check can set back to original voter
⋮----
.set_new_authorized_voter(&original_voter, 9, 9 + epoch_offset, |_| Ok(()))
⋮----
// Run with these voters for a while, check the ranges of authorized
// voters is correct
⋮----
fn test_set_new_authorized_voter() {
⋮----
// Start with v3. We'll also check `prior_voters`.
⋮----
assert!(vote_state.prior_voters.last().is_none());
set_new_authorized_voter_and_assert(
⋮----
Some(|vote_state: &VoteStateV3| vote_state.prior_voters.last().unwrap()),
⋮----
let mut vote_state = create_new_vote_state_v4(&vote_pubkey, &vote_init, &clock);
set_new_authorized_voter_and_assert(&mut vote_state, original_voter, epoch_offset, None);
⋮----
fn assert_authorized_voter_is_locked_within_epoch<T: VoteStateHandle>(
⋮----
fn test_authorized_voter_is_locked_within_epoch() {
⋮----
assert_authorized_voter_is_locked_within_epoch(&mut vote_state, &original_voter);
⋮----
fn test_get_and_update_authorized_voter_v3() {
⋮----
assert_eq!(vote_state.authorized_voters().len(), 1);
⋮----
assert!(vote_state
⋮----
.set_new_authorized_voter(&new_authorized_voter, 5, 7, |_| Ok(()))
⋮----
fn test_get_and_update_authorized_voter_v4() {
⋮----
let mut vote_state = create_new_vote_state_v4(
⋮----
assert_eq!(vote_state.authorized_voters().len(), 2);
⋮----
.set_new_authorized_voter(&new_authorized_voter, 7, 9, |_| Ok(()))
⋮----
fn test_vote_state_max_size<T: Clone + VoteStateHandle>(
⋮----
let mut max_sized_data = vec![0; max_size];
let (start_leader_schedule_epoch, _) = vote_state.authorized_voters().last().unwrap();
⋮----
.set_new_authorized_voter(
⋮----
|_| Ok(()),
⋮----
verify_serialize(vote_state.clone(), &mut max_sized_data);
⋮----
fn test_vote_state_epoch_credits<T: VoteStateHandle>(mut vote_state: T) {
assert_eq!(vote_state.credits(), 0);
assert_eq!(vote_state.epoch_credits().clone(), vec![]);
let mut expected = vec![];
⋮----
vote_state.increment_credits(epoch, 1);
⋮----
expected.push((epoch, credits, credits - epoch));
⋮----
while expected.len() > MAX_EPOCH_CREDITS_HISTORY {
expected.remove(0);
⋮----
assert_eq!(vote_state.credits(), credits);
assert_eq!(vote_state.epoch_credits().clone(), expected);
⋮----
fn test_vote_state_epoch0_no_credits<T: VoteStateHandle>(mut vote_state: T) {
assert_eq!(vote_state.epoch_credits().len(), 0);
vote_state.increment_credits(1, 1);
assert_eq!(vote_state.epoch_credits().len(), 1);
vote_state.increment_credits(2, 1);
assert_eq!(vote_state.epoch_credits().len(), 2);
⋮----
fn test_vote_state_increment_credits<T: VoteStateHandle>(mut vote_state: T) {
⋮----
vote_state.increment_credits(i, 1);
⋮----
assert!(vote_state.epoch_credits().len() <= MAX_EPOCH_CREDITS_HISTORY);
⋮----
fn test_vote_process_timestamp<T: VoteStateHandle>(mut vote_state: T) {
⋮----
vote_state.set_last_timestamp(BlockTimestamp { slot, timestamp });
⋮----
assert_eq!(vote_state.process_timestamp(slot, timestamp), Ok(()));
⋮----
assert_eq!(vote_state.process_timestamp(slot + 1, timestamp), Ok(()));
⋮----
vote_state.set_last_timestamp(BlockTimestamp::default());
assert_eq!(vote_state.process_timestamp(0, timestamp), Ok(()));
⋮----
enum ExpectedVoteStateVersion {
⋮----
fn init_vote_account_state_v3_and_assert(
⋮----
let transaction_context = mock_transaction_context(vote_pubkey, vote_account, rent);
let instruction_context = transaction_context.get_next_instruction_context().unwrap();
⋮----
.try_borrow_instruction_account(0)
⋮----
let vote_state_versions = vote_account.get_state::<VoteStateVersions>().unwrap();
⋮----
assert!(matches!(
⋮----
assert!(!vote_state_versions.is_uninitialized());
⋮----
assert_eq!(v1_14_11.node_pubkey, vote_init.node_pubkey);
⋮----
assert_eq!(v1_14_11.commission, vote_init.commission);
⋮----
panic!("should be v1_14_11");
⋮----
assert!(matches!(vote_state_versions, VoteStateVersions::V3(_)));
⋮----
assert_eq!(v3.node_pubkey, vote_init.node_pubkey);
⋮----
assert_eq!(v3.authorized_withdrawer, vote_init.authorized_withdrawer);
assert_eq!(v3.commission, vote_init.commission);
⋮----
panic!("should be v3");
⋮----
fn init_vote_account_state_v4_and_assert(
⋮----
assert_eq!(result, expected_result);
if result.is_ok() {
⋮----
assert!(matches!(vote_state_versions, VoteStateVersions::V4(_)));
⋮----
assert_eq!(v4.node_pubkey, vote_init.node_pubkey);
⋮----
assert_eq!(v4.authorized_withdrawer, vote_init.authorized_withdrawer);
⋮----
assert_eq!(v4.inflation_rewards_collector, vote_pubkey);
assert_eq!(v4.block_revenue_collector, vote_init.node_pubkey);
⋮----
assert_eq!(v4.pending_delegator_rewards, 0);
assert_eq!(v4.bls_pubkey_compressed, None);
assert!(v4.votes.is_empty());
assert_eq!(v4.root_slot, None);
assert!(v4.epoch_credits.is_empty());
assert_eq!(v4.last_timestamp, BlockTimestamp::default());
⋮----
panic!("should be v4");
⋮----
fn deinit_vote_account_state_v3_and_assert(
⋮----
let transaction_context = mock_transaction_context(vote_pubkey, vote_account, rent.clone());
⋮----
assert!(vote_state_versions.is_uninitialized());
⋮----
fn deinit_vote_account_state_v4_and_assert(
⋮----
let account_data = vote_account.get_data();
assert!(account_data.iter().all(|&b| b == 0),);
⋮----
assert!(matches!(vote_state_versions, VoteStateVersions::V0_23_5(_)));
⋮----
fn test_init_vote_account_state_v3() {
⋮----
let lamports = rent.minimum_balance(v1_14_11_size);
let vote_account = AccountSharedData::new(lamports, v1_14_11_size, &id());
init_vote_account_state_v3_and_assert(
⋮----
rent.clone(),
⋮----
let lamports = rent.minimum_balance(v3_size);
⋮----
let vote_account = AccountSharedData::new(lamports, v3_size, &id());
⋮----
fn test_init_vote_account_state_v4() {
⋮----
init_vote_account_state_v4_and_assert(
⋮----
Err(InstructionError::AccountNotRentExempt),
⋮----
let lamports = rent.minimum_balance(v4_size);
⋮----
Ok(()),
⋮----
let vote_account = AccountSharedData::new(lamports, v4_size, &id());
⋮----
fn test_deinitialize_vote_account_state_v3() {
⋮----
deinit_vote_account_state_v3_and_assert(
⋮----
fn test_deinitialize_vote_account_state_v4() {
⋮----
deinit_vote_account_state_v4_and_assert(vote_pubkey, vote_account, rent.clone());
⋮----
deinit_vote_account_state_v4_and_assert(vote_pubkey, vote_account, rent);
⋮----
fn test_v4_commission_basis_points() {
⋮----
handler.set_commission(input);
assert_eq!(handler.commission(), input);
let vote_state_v4 = handler.as_ref_v4();
assert_eq!(vote_state_v4.inflation_rewards_commission_bps, expected);
⋮----
fn test_v4_conversion_from_all_versions() {
⋮----
let votes = vec![Lockout::new(100)];
let votes_deque: VecDeque<Lockout> = votes.iter().cloned().collect();
let epoch_credits = vec![(5, 200, 100)];
let root_slot = Some(50);
⋮----
assert_eq!(v4_state.inflation_rewards_collector, vote_pubkey);
assert_eq!(v4_state.block_revenue_collector, node_pubkey);
⋮----
assert_eq!(v4_state.pending_delegator_rewards, 0);
assert_eq!(v4_state.bls_pubkey_compressed, None);
⋮----
votes: votes_deque.clone(),
epoch_credits: epoch_credits.clone(),
⋮----
let versioned = VoteStateVersions::V1_14_11(Box::new(vote_state_v2.clone()));
let vote_state_v4 = try_convert_to_vote_state_v4(versioned, &vote_pubkey).unwrap();
assert_eq!(vote_state_v4.node_pubkey, node_pubkey);
assert_eq!(vote_state_v4.authorized_withdrawer, authorized_withdrawer);
⋮----
assert_eq!(vote_state_v4.epoch_credits, epoch_credits);
assert_eq!(vote_state_v4.root_slot, root_slot);
assert_eq!(vote_state_v4.votes.len(), vote_state_v2.votes.len());
for (v4_vote, v2_vote) in vote_state_v4.votes.iter().zip(vote_state_v2.votes.iter()) {
assert_eq!(v4_vote.lockout, *v2_vote);
⋮----
verify_v4_defaults(&vote_state_v4, (commission as u16) * 100);
⋮----
vote_state_v3.votes.push_back(LandedVote {
⋮----
vote_state_v3.epoch_credits = epoch_credits.clone();
⋮----
let versioned = VoteStateVersions::V3(Box::new(vote_state_v3.clone()));
⋮----
assert_eq!(vote_state_v4.last_timestamp, vote_state_v3.last_timestamp);
assert_eq!(vote_state_v4.votes, vote_state_v3.votes);
⋮----
bls_pubkey_compressed: Some([42; BLS_PUBLIC_KEY_COMPRESSED_SIZE]),
⋮----
initial_vote_state_v4.votes.push_back(LandedVote {
⋮----
let versioned = VoteStateVersions::V4(Box::new(initial_vote_state_v4.clone()));
⋮----
assert_eq!(vote_state_v4, initial_vote_state_v4);
⋮----
fn test_v3_v4_size_equality() {
⋮----
assert_eq!(v3_size, v4_size, "V3 and V4 should have the same size");
⋮----
fn test_v4_resize_behavior(
⋮----
rent.minimum_balance(VoteStateV4::size_of())
⋮----
rent.minimum_balance(account_size)
⋮----
let mut vote_account = AccountSharedData::new(lamports, account_size, &id());
vote_account.set_data_from_slice(&vec![0; account_size]);

================
File: programs/vote/src/vote_state/mod.rs
================
pub mod handler;
⋮----
pub(crate) mod handler;
⋮----
enum PreserveBehaviorInHandlerHelper {
⋮----
impl PreserveBehaviorInHandlerHelper {
fn new(target_version: VoteStateTargetVersion, check_initialized: bool) -> Self {
⋮----
fn get_vote_state_handler_checked(
⋮----
let vote_state = VoteStateV3::deserialize(vote_account.get_data())?;
if check_initialized && vote_state.is_uninitialized() {
return Err(InstructionError::UninitializedAccount);
⋮----
Ok(VoteStateHandler::new_v3(vote_state))
⋮----
let versioned = VoteStateVersions::deserialize(vote_account.get_data())?;
if versioned.is_uninitialized() {
⋮----
handler::try_convert_to_vote_state_v4(versioned, vote_account.get_key())?;
Ok(VoteStateHandler::new_v4(vote_state))
⋮----
fn check_and_filter_proposed_vote_state(
⋮----
if proposed_lockouts.is_empty() {
return Err(VoteError::EmptySlots);
⋮----
.back()
.expect("must be nonempty, checked above")
.slot();
if let Some(last_vote_slot) = vote_state.votes().back().map(|lockout| lockout.slot()) {
⋮----
return Err(VoteError::VoteTooOld);
⋮----
if slot_hashes.is_empty() {
return Err(VoteError::SlotsMismatch);
⋮----
let earliest_slot_hash_in_history = slot_hashes.last().unwrap().0;
⋮----
*proposed_root = vote_state.root_slot();
for vote in vote_state.votes().iter().rev() {
if vote.slot() <= root {
*proposed_root = Some(vote.slot());
⋮----
let mut slot_hashes_index = slot_hashes.len();
let mut proposed_lockouts_indices_to_filter = vec![];
while proposed_lockouts_index < proposed_lockouts.len() && slot_hashes_index > 0 {
⋮----
proposed_lockouts[proposed_lockouts_index].slot()
⋮----
if root_to_check.is_none()
⋮----
<= proposed_lockouts[proposed_lockouts_index.checked_sub(1).expect(
⋮----
.slot()
⋮----
return Err(VoteError::SlotsNotOrdered);
⋮----
.checked_sub(1)
.expect("`slot_hashes_index` is positive when computing `ancestor_slot`")]
⋮----
match proposed_vote_slot.cmp(&ancestor_slot) {
⋮----
if slot_hashes_index == slot_hashes.len() {
⋮----
return Err(VoteError::AssertionFailed);
⋮----
if !vote_state.contains_slot(proposed_vote_slot) && root_to_check.is_none() {
proposed_lockouts_indices_to_filter.push(proposed_lockouts_index);
⋮----
assert_eq!(new_proposed_root, proposed_vote_slot);
⋮----
proposed_lockouts_index = proposed_lockouts_index.checked_add(1).expect(
⋮----
if root_to_check.is_some() {
return Err(VoteError::RootOnDifferentFork);
⋮----
slot_hashes_index = slot_hashes_index.checked_sub(1).expect(
⋮----
if proposed_lockouts_index != proposed_lockouts.len() {
⋮----
assert_eq!(last_proposed_slot, slot_hashes[slot_hashes_index].0);
⋮----
warn!(
⋮----
inc_new_counter_info!("dropped-vote-hash", 1);
return Err(VoteError::SlotHashMismatch);
⋮----
proposed_lockouts.retain(|_lockout| {
let should_retain = if filter_votes_index == proposed_lockouts_indices_to_filter.len() {
⋮----
filter_votes_index = filter_votes_index.checked_add(1).unwrap();
⋮----
Ok(())
⋮----
fn check_slots_are_valid<T: VoteStateHandle>(
⋮----
let mut j = slot_hashes.len();
while i < vote_slots.len() && j > 0 {
⋮----
.last_voted_slot()
.is_some_and(|last_voted_slot| vote_slots[i] <= last_voted_slot)
⋮----
.checked_add(1)
.expect("`i` is bounded by `MAX_LOCKOUT_HISTORY` when finding larger slots");
⋮----
if vote_slots[i] != slot_hashes[j.checked_sub(1).expect("`j` is positive")].0 {
⋮----
.expect("`j` is positive when finding newer slots");
⋮----
.expect("`i` is bounded by `MAX_LOCKOUT_HISTORY` when hash is found");
⋮----
.expect("`j` is positive when hash is found");
⋮----
if j == slot_hashes.len() {
debug!(
⋮----
if i != vote_slots.len() {
info!(
⋮----
pub fn process_new_vote_state(
⋮----
assert!(!new_state.is_empty());
if new_state.len() > MAX_LOCKOUT_HISTORY {
return Err(VoteError::TooManyVotes);
⋮----
match (new_root, vote_state.root_slot()) {
⋮----
return Err(VoteError::RootRollBack);
⋮----
if vote.confirmation_count() == 0 {
return Err(VoteError::ZeroConfirmations);
} else if vote.confirmation_count() > MAX_LOCKOUT_HISTORY as u32 {
return Err(VoteError::ConfirmationTooLarge);
⋮----
if vote.slot() <= new_root
⋮----
return Err(VoteError::SlotSmallerThanRoot);
⋮----
if previous_vote.slot() >= vote.slot() {
⋮----
} else if previous_vote.confirmation_count() <= vote.confirmation_count() {
return Err(VoteError::ConfirmationsNotOrdered);
} else if vote.slot() > previous_vote.lockout.last_locked_out_slot() {
return Err(VoteError::NewVoteStateLockoutMismatch);
⋮----
previous_vote = Some(vote);
⋮----
for current_vote in vote_state.votes() {
if current_vote.slot() <= new_root {
⋮----
.checked_add(vote_state.credits_for_vote_at_index(current_vote_state_index))
.expect("`earned_credits` does not overflow");
current_vote_state_index = current_vote_state_index.checked_add(1).expect(
⋮----
while current_vote_state_index < vote_state.votes().len()
&& new_vote_state_index < new_state.len()
⋮----
let current_vote = &vote_state.votes()[current_vote_state_index];
⋮----
match current_vote.slot().cmp(&new_vote.slot()) {
⋮----
if current_vote.lockout.last_locked_out_slot() >= new_vote.slot() {
return Err(VoteError::LockoutConflict);
⋮----
if new_vote.confirmation_count() < current_vote.confirmation_count() {
return Err(VoteError::ConfirmationRollBack);
⋮----
new_vote.latency = vote_state.votes()[current_vote_state_index].latency;
⋮----
new_vote_state_index = new_vote_state_index.checked_add(1).expect(
⋮----
for new_vote in new_state.iter_mut() {
⋮----
new_vote.latency = handler::compute_vote_latency(new_vote.slot(), current_slot);
⋮----
if vote_state.root_slot() != new_root {
vote_state.increment_credits(epoch, earned_credits);
⋮----
let last_slot = new_state.back().unwrap().slot();
vote_state.process_timestamp(last_slot, timestamp)?;
⋮----
vote_state.set_root_slot(new_root);
vote_state.set_votes(new_state);
⋮----
pub fn process_vote_unfiltered<T: VoteStateHandle>(
⋮----
check_slots_are_valid(vote_state, vote_slots, &vote.hash, slot_hashes)?;
⋮----
.iter()
.for_each(|s| vote_state.process_next_vote_slot(*s, epoch, current_slot));
⋮----
pub fn process_vote(
⋮----
if vote.slots.is_empty() {
⋮----
let earliest_slot_in_history = slot_hashes.last().map(|(slot, _hash)| *slot).unwrap_or(0);
⋮----
.filter(|slot| **slot >= earliest_slot_in_history)
.cloned()
⋮----
if vote_slots.is_empty() {
return Err(VoteError::VotesTooOldAllFiltered);
⋮----
process_vote_unfiltered(
⋮----
pub fn process_vote_unchecked<T: VoteStateHandle>(
⋮----
let slot_hashes: Vec<_> = vote.slots.iter().rev().map(|x| (*x, vote.hash)).collect();
⋮----
vote_state.current_epoch(),
⋮----
pub fn process_slot_votes_unchecked<T: VoteStateHandle>(vote_state: &mut T, slots: &[Slot]) {
⋮----
process_slot_vote_unchecked(vote_state, *slot);
⋮----
pub fn process_slot_vote_unchecked<T: VoteStateHandle>(vote_state: &mut T, slot: Slot) {
let _ = process_vote_unchecked(vote_state, Vote::new(vec![slot], Hash::default()));
⋮----
pub fn authorize<S: std::hash::BuildHasher>(
⋮----
let mut vote_state = get_vote_state_handler_checked(
⋮----
verify_authorized_signer(vote_state.authorized_withdrawer(), signers).is_ok();
vote_state.set_new_authorized_voter(
⋮----
.ok_or(InstructionError::InvalidAccountData)?,
⋮----
verify_authorized_signer(&epoch_authorized_voter, signers)
⋮----
verify_authorized_signer(vote_state.authorized_withdrawer(), signers)?;
vote_state.set_authorized_withdrawer(*authorized);
⋮----
vote_state.set_vote_account_state(vote_account)
⋮----
pub fn update_validator_identity<S: std::hash::BuildHasher>(
⋮----
verify_authorized_signer(node_pubkey, signers)?;
vote_state.set_node_pubkey(*node_pubkey);
vote_state.set_block_revenue_collector(*node_pubkey);
⋮----
pub fn update_commission<S: std::hash::BuildHasher>(
⋮----
let vote_state_result = get_vote_state_handler_checked(
⋮----
commission > decoded_vote_state.commission()
⋮----
if enforce_commission_update_rule && !is_commission_update_allowed(clock.slot, epoch_schedule) {
return Err(VoteError::CommissionUpdateTooLate.into());
⋮----
vote_state.set_commission(commission);
⋮----
pub fn is_commission_update_allowed(slot: Slot, epoch_schedule: &EpochSchedule) -> bool {
⋮----
.saturating_sub(epoch_schedule.first_normal_slot)
.checked_rem(epoch_schedule.slots_per_epoch)
⋮----
relative_slot.saturating_mul(2) <= epoch_schedule.slots_per_epoch
⋮----
fn verify_authorized_signer<S: std::hash::BuildHasher>(
⋮----
if signers.contains(authorized) {
⋮----
Err(InstructionError::MissingRequiredSignature)
⋮----
pub fn withdraw<S: std::hash::BuildHasher>(
⋮----
instruction_context.try_borrow_instruction_account(vote_account_index)?;
let vote_state = get_vote_state_handler_checked(
⋮----
.get_lamports()
.checked_sub(lamports)
.ok_or(InstructionError::InsufficientFunds)?;
⋮----
.epoch_credits()
.last()
.map(|(last_epoch_with_credits, _, _)| {
⋮----
current_epoch.saturating_sub(*last_epoch_with_credits) < 2
⋮----
.unwrap_or(false);
⋮----
return Err(VoteError::ActiveVoteAccountClose.into());
⋮----
let min_rent_exempt_balance = rent_sysvar.minimum_balance(vote_account.get_data().len());
⋮----
return Err(InstructionError::InsufficientFunds);
⋮----
vote_account.checked_sub_lamports(lamports)?;
drop(vote_account);
let mut to_account = instruction_context.try_borrow_instruction_account(to_account_index)?;
to_account.checked_add_lamports(lamports)?;
⋮----
pub fn initialize_account<S: std::hash::BuildHasher>(
⋮----
if !versioned.is_uninitialized() {
return Err(InstructionError::AccountAlreadyInitialized);
⋮----
verify_authorized_signer(&vote_init.node_pubkey, signers)?;
⋮----
pub fn process_vote_with_account<S: std::hash::BuildHasher>(
⋮----
let authorized_voter = vote_state.get_and_update_authorized_voter(clock.epoch)?;
verify_authorized_signer(&authorized_voter, signers)?;
process_vote(&mut vote_state, vote, slot_hashes, clock.epoch, clock.slot)?;
⋮----
.max()
.ok_or(VoteError::EmptySlots)
.and_then(|slot| vote_state.process_timestamp(*slot, timestamp))?;
⋮----
pub fn process_vote_state_update<S: std::hash::BuildHasher>(
⋮----
do_process_vote_state_update(
⋮----
pub fn do_process_vote_state_update(
⋮----
check_and_filter_proposed_vote_state(
⋮----
process_new_vote_state(
⋮----
.map(|lockout| LandedVote::from(*lockout))
.collect(),
⋮----
pub fn process_tower_sync<S: std::hash::BuildHasher>(
⋮----
do_process_tower_sync(
⋮----
fn do_process_tower_sync(
⋮----
pub fn create_account_with_authorized(
⋮----
let mut vote_account = AccountSharedData::new(lamports, VoteStateV3::size_of(), &id());
⋮----
vote_account.data_as_mut_slice(),
⋮----
.unwrap();
⋮----
pub fn create_v4_account_with_authorized(
⋮----
let mut vote_account = AccountSharedData::new(lamports, VoteStateV4::size_of(), &id());
⋮----
mod tests {
⋮----
fn vote_state_new_for_test(
⋮----
fn test_vote_state_upgrade_from_1_14_11(target_version: VoteStateTargetVersion) {
⋮----
let mut vote_state = vote_state_new_for_test(&vote_pubkey, target_version);
vote_state.increment_credits(0, 100);
assert_eq!(
⋮----
vote_state.increment_credits(1, 200);
⋮----
vote_state.increment_credits(2, 300);
⋮----
vec![
⋮----
.into_iter()
.for_each(|v| vote_state.process_next_vote_slot(v, 4, 0));
⋮----
VoteState1_14_11::from(vote_state.as_ref_v3().clone())
⋮----
node_pubkey: *vote_state.node_pubkey(),
authorized_withdrawer: *vote_state.authorized_withdrawer(),
commission: vote_state.commission(),
⋮----
.votes()
⋮----
.map(|landed_vote| (*landed_vote).into())
⋮----
root_slot: vote_state.root_slot(),
authorized_voters: vote_state.authorized_voters().clone(),
epoch_credits: vote_state.epoch_credits().clone(),
last_timestamp: vote_state.last_timestamp().clone(),
⋮----
let version1_14_11_serialized_len = version1_14_11_serialized.len();
⋮----
let lamports = rent.minimum_balance(version1_14_11_serialized_len);
⋮----
AccountSharedData::new(lamports, version1_14_11_serialized_len, &id());
vote_account.set_data_from_slice(&version1_14_11_serialized);
⋮----
vec![(id(), processor_account), (vote_pubkey, vote_account)],
rent.clone(),
⋮----
.configure_next_instruction_for_tests(
⋮----
vec![InstructionAccount::new(1, false, true)],
vec![],
⋮----
let instruction_context = transaction_context.get_next_instruction_context().unwrap();
⋮----
.try_borrow_instruction_account(0)
⋮----
let vote_state_version = borrowed_account.get_state::<VoteStateVersions>().unwrap();
assert_matches!(vote_state_version, VoteStateVersions::V1_14_11(_));
let converted_vote_state = get_vote_state_handler_checked(
⋮----
assert!(vote_state == converted_vote_state);
⋮----
assert_matches!(vote_state_version, VoteStateVersions::V3(_));
⋮----
assert_matches!(vote_state_version, VoteStateVersions::V4(_));
⋮----
assert_eq!(vote_state, converted_vote_state);
⋮----
fn test_vote_lockout(target_version: VoteStateTargetVersion) {
let mut vote_state = vote_state_new_for_test(&solana_pubkey::new_rand(), target_version);
⋮----
process_slot_vote_unchecked(&mut vote_state, (INITIAL_LOCKOUT * i) as u64);
⋮----
assert_eq!(vote_state.votes().len(), MAX_LOCKOUT_HISTORY);
assert_eq!(vote_state.root_slot(), Some(0));
check_lockouts(&vote_state);
let top_vote = vote_state.votes().front().unwrap().slot();
let slot = vote_state.last_lockout().unwrap().last_locked_out_slot();
process_slot_vote_unchecked(&mut vote_state, slot);
assert_eq!(Some(top_vote), vote_state.root_slot());
⋮----
.front()
.unwrap()
⋮----
.last_locked_out_slot();
⋮----
assert_eq!(vote_state.votes().len(), 2);
⋮----
fn test_update_commission(target_version: VoteStateTargetVersion) {
⋮----
let node_pubkey = *vote_state.node_pubkey();
let withdrawer_pubkey = *vote_state.authorized_withdrawer();
vote_state.set_commission(10);
let serialized = vote_state.serialize();
let serialized_len = serialized.len();
⋮----
let lamports = rent.minimum_balance(serialized_len);
let mut vote_account = AccountSharedData::new(lamports, serialized_len, &id());
vote_account.set_data_from_slice(&serialized);
⋮----
vec![(id(), processor_account), (node_pubkey, vote_account)],
⋮----
let signers: HashSet<Pubkey> = vec![withdrawer_pubkey].into_iter().collect();
⋮----
assert_matches!(
⋮----
fn test_vote_double_lockout_after_expiration(target_version: VoteStateTargetVersion) {
⋮----
process_slot_vote_unchecked(&mut vote_state, i as u64);
⋮----
process_slot_vote_unchecked(&mut vote_state, (2 + INITIAL_LOCKOUT + 1) as u64);
⋮----
process_slot_vote_unchecked(&mut vote_state, (2 + INITIAL_LOCKOUT + 2) as u64);
⋮----
process_slot_vote_unchecked(&mut vote_state, (2 + INITIAL_LOCKOUT + 3) as u64);
⋮----
fn test_expire_multiple_votes(target_version: VoteStateTargetVersion) {
⋮----
assert_eq!(vote_state.votes()[0].confirmation_count(), 3);
⋮----
vote_state.votes()[1].slot() + vote_state.votes()[1].lockout.lockout() + 1;
process_slot_vote_unchecked(&mut vote_state, expire_slot);
⋮----
assert_eq!(vote_state.votes()[0].slot(), 0);
assert_eq!(vote_state.votes()[1].slot(), expire_slot);
process_slot_vote_unchecked(&mut vote_state, expire_slot + 1);
⋮----
assert_eq!(vote_state.votes()[1].confirmation_count(), 2);
assert_eq!(vote_state.votes()[2].confirmation_count(), 1);
⋮----
fn test_vote_credits(target_version: VoteStateTargetVersion) {
⋮----
assert_eq!(vote_state.credits(), 0);
process_slot_vote_unchecked(&mut vote_state, MAX_LOCKOUT_HISTORY as u64 + 1);
assert_eq!(vote_state.credits(), 1);
process_slot_vote_unchecked(&mut vote_state, MAX_LOCKOUT_HISTORY as u64 + 2);
assert_eq!(vote_state.credits(), 2);
process_slot_vote_unchecked(&mut vote_state, MAX_LOCKOUT_HISTORY as u64 + 3);
assert_eq!(vote_state.credits(), 3);
⋮----
fn test_duplicate_vote(target_version: VoteStateTargetVersion) {
⋮----
process_slot_vote_unchecked(&mut vote_state, 0);
process_slot_vote_unchecked(&mut vote_state, 1);
⋮----
assert_eq!(vote_state.nth_recent_lockout(0).unwrap().slot(), 1);
assert_eq!(vote_state.nth_recent_lockout(1).unwrap().slot(), 0);
assert!(vote_state.nth_recent_lockout(2).is_none());
⋮----
fn test_nth_recent_lockout(target_version: VoteStateTargetVersion) {
⋮----
assert!(vote_state.nth_recent_lockout(MAX_LOCKOUT_HISTORY).is_none());
⋮----
fn check_lockouts(vote_state: &VoteStateHandler) {
let votes = vote_state.votes();
for (i, vote) in votes.iter().enumerate() {
⋮----
.len()
.checked_sub(i)
.expect("`i` is less than `vote_state.votes().len()`");
⋮----
fn recent_votes(vote_state: &VoteStateHandler) -> Vec<Vote> {
⋮----
let start = votes.len().saturating_sub(MAX_RECENT_VOTES);
(start..votes.len())
.map(|i| Vote::new(vec![votes.get(i).unwrap().slot()], Hash::default()))
.collect()
⋮----
fn test_process_missed_votes(target_version: VoteStateTargetVersion) {
let mut vote_state_a = vote_state_new_for_test(&solana_pubkey::new_rand(), target_version);
let mut vote_state_b = vote_state_new_for_test(&solana_pubkey::new_rand(), target_version);
(0..5).for_each(|i| process_slot_vote_unchecked(&mut vote_state_a, i as u64));
assert_ne!(recent_votes(&vote_state_a), recent_votes(&vote_state_b));
let slots = (0u64..MAX_RECENT_VOTES as u64).collect();
⋮----
assert_eq!(recent_votes(&vote_state_a), recent_votes(&vote_state_b));
⋮----
fn test_process_vote_skips_old_vote(mut vote_state: VoteStateHandler) {
let vote = Vote::new(vec![0], Hash::default());
let slot_hashes: Vec<_> = vec![(0, vote.hash)];
⋮----
let recent = recent_votes(&vote_state);
⋮----
assert_eq!(recent, recent_votes(&vote_state));
⋮----
fn test_check_slots_are_valid_vote_empty_slot_hashes(vote_state: VoteStateHandler) {
⋮----
fn test_check_slots_are_valid_new_vote(vote_state: VoteStateHandler) {
⋮----
let slot_hashes: Vec<_> = vec![(*vote.slots.last().unwrap(), vote.hash)];
⋮----
fn test_check_slots_are_valid_bad_hash(vote_state: VoteStateHandler) {
⋮----
let slot_hashes: Vec<_> = vec![(*vote.slots.last().unwrap(), hash(vote.hash.as_ref()))];
⋮----
fn test_check_slots_are_valid_bad_slot(vote_state: VoteStateHandler) {
let vote = Vote::new(vec![1], Hash::default());
⋮----
fn test_check_slots_are_valid_duplicate_vote(mut vote_state: VoteStateHandler) {
⋮----
fn test_check_slots_are_valid_next_vote(mut vote_state: VoteStateHandler) {
⋮----
let vote = Vote::new(vec![0, 1], Hash::default());
let slot_hashes: Vec<_> = vec![(1, vote.hash), (0, vote.hash)];
⋮----
fn test_check_slots_are_valid_next_vote_only(mut vote_state: VoteStateHandler) {
⋮----
fn test_process_vote_empty_slots(mut vote_state: VoteStateHandler) {
let vote = Vote::new(vec![], Hash::default());
⋮----
pub fn process_new_vote_state_from_lockouts(
⋮----
new_state.into_iter().map(LandedVote::from).collect(),
⋮----
fn test_vote_state_update_increment_credits(mut vote_state: VoteStateHandler) {
let test_vote_groups: Vec<Vec<Slot>> = vec![
⋮----
let mut vote_state_after_vote = vote_state.clone();
process_vote_unchecked(
⋮----
slots: vote_group.clone(),
⋮----
fn test_timely_credits(target_version: VoteStateTargetVersion) {
let test_vote_groups: Vec<(Vec<Slot>, Slot, u32)> = vec![
⋮----
for i in 0..test_vote_groups.len() {
let mut vote_state_1 = new_vote_state();
let mut vote_state_2 = new_vote_state();
test_vote_groups.iter().take(i + 1).for_each(|vote_group| {
⋮----
slots: vote_group.0.clone(),
⋮----
vote.slots.iter().rev().map(|x| (*x, vote.hash)).collect();
⋮----
assert_eq!(vote_state_1.credits(), vote_group.2 as u64);
assert_eq!(vote_state_2.credits(), vote_group.2 as u64);
⋮----
fn test_retroactive_voting_timely_credits(mut vote_state: VoteStateHandler) {
⋮----
let test_vote_state_updates: Vec<(Vec<(Slot, u32)>, Slot, Option<Slot>, u32)> = vec![
⋮----
.for_each(|proposed_vote_state| {
⋮----
.map(|(slot, confirmation_count)| LandedVote {
⋮----
assert_eq!(vote_state.credits(), proposed_vote_state.3 as u64);
⋮----
fn test_process_new_vote_too_many_votes(mut vote_state1: VoteStateHandler) {
⋮----
.map(|slot| {
⋮----
.collect();
let current_epoch = vote_state1.current_epoch();
⋮----
fn test_process_new_vote_state_root_rollback(mut vote_state1: VoteStateHandler) {
⋮----
process_slot_vote_unchecked(&mut vote_state1, i as Slot);
⋮----
assert_eq!(vote_state1.root_slot().unwrap(), 1);
let mut vote_state2 = vote_state1.clone();
process_slot_vote_unchecked(&mut vote_state2, MAX_LOCKOUT_HISTORY as Slot + 3);
let lesser_root = Some(0);
let current_epoch = vote_state2.current_epoch();
⋮----
fn test_process_new_vote_state_zero_confirmations(mut vote_state1: VoteStateHandler) {
⋮----
let bad_votes: VecDeque<Lockout> = vec![
⋮----
fn test_process_new_vote_state_confirmations_too_large(initial_vote_state: VoteStateHandler) {
let mut vote_state1 = initial_vote_state.clone();
⋮----
let good_votes: VecDeque<Lockout> = vec![Lockout::new_with_confirmation_count(
⋮----
process_new_vote_state_from_lockouts(
⋮----
let bad_votes: VecDeque<Lockout> = vec![Lockout::new_with_confirmation_count(
⋮----
fn test_process_new_vote_state_slot_smaller_than_root(mut vote_state1: VoteStateHandler) {
⋮----
fn test_process_new_vote_state_slots_not_ordered(mut vote_state1: VoteStateHandler) {
⋮----
fn test_process_new_vote_state_confirmations_not_ordered(mut vote_state1: VoteStateHandler) {
⋮----
fn test_process_new_vote_state_new_vote_state_lockout_mismatch(
⋮----
fn test_process_new_vote_state_confirmation_rollback(mut vote_state1: VoteStateHandler) {
⋮----
let votes: VecDeque<Lockout> = vec![
⋮----
process_new_vote_state_from_lockouts(&mut vote_state1, votes, None, None, current_epoch)
⋮----
fn test_process_new_vote_state_root_progress(mut vote_state1: VoteStateHandler) {
⋮----
process_slot_vote_unchecked(&mut vote_state1, i as u64);
⋮----
assert!(vote_state1.root_slot().is_none());
⋮----
process_slot_vote_unchecked(&mut vote_state2, new_vote as Slot);
assert_ne!(vote_state1.root_slot(), vote_state2.root_slot());
⋮----
vote_state2.votes().clone(),
vote_state2.root_slot(),
⋮----
vote_state2.current_epoch(),
⋮----
assert_eq!(vote_state1, vote_state2);
⋮----
fn test_process_new_vote_state_same_slot_but_not_common_ancestor(
⋮----
process_slot_votes_unchecked(&mut vote_state1, &[1, 2, 5]);
⋮----
process_slot_votes_unchecked(&mut vote_state2, &[1, 2, 3, 5, 7]);
⋮----
fn test_process_new_vote_state_lockout_violation(initial_vote_state: VoteStateHandler) {
⋮----
process_slot_votes_unchecked(&mut vote_state1, &[1, 2, 4, 5]);
⋮----
fn test_process_new_vote_state_lockout_violation2(initial_vote_state: VoteStateHandler) {
⋮----
process_slot_votes_unchecked(&mut vote_state1, &[1, 2, 5, 6, 7]);
⋮----
process_slot_votes_unchecked(&mut vote_state2, &[1, 2, 3, 5, 6, 8]);
⋮----
fn test_process_new_vote_state_expired_ancestor_not_removed(mut vote_state1: VoteStateHandler) {
process_slot_votes_unchecked(&mut vote_state1, &[1, 2, 3, 9]);
⋮----
process_slot_vote_unchecked(&mut vote_state2, 10);
assert_eq!(vote_state2.votes()[0].slot(), 1);
assert_eq!(vote_state2.votes()[0].lockout.last_locked_out_slot(), 9);
⋮----
assert_eq!(vote_state1, vote_state2,);
⋮----
fn test_process_new_vote_current_state_contains_bigger_slots(
⋮----
process_slot_votes_unchecked(&mut vote_state1, &[6, 7, 8]);
⋮----
let root = Some(1);
⋮----
let good_votes: VecDeque<LandedVote> = vec![
⋮----
good_votes.clone(),
⋮----
assert_eq!(*vote_state1.votes(), good_votes);
⋮----
fn test_filter_old_votes(mut vote_state: VoteStateHandler) {
⋮----
let vote = Vote::new(vec![old_vote_slot], Hash::default());
let slot_hashes = vec![(3, Hash::new_unique()), (2, Hash::new_unique())];
⋮----
.find(|(slot, _hash)| *slot == vote_slot)
⋮----
let vote = Vote::new(vec![old_vote_slot, vote_slot], vote_slot_hash);
process_vote(&mut vote_state, &vote, &slot_hashes, 0, 0).unwrap();
⋮----
fn build_slot_hashes(slots: Vec<Slot>) -> Vec<(Slot, Hash)> {
⋮----
.rev()
.map(|x| (*x, Hash::new_unique()))
⋮----
fn build_vote_state(
⋮----
if !vote_slots.is_empty() {
⋮----
.find(|(slot, _hash)| slot == vote_slots.last().unwrap())
⋮----
process_vote_unfiltered(&mut vote_state, &vote.slots, &vote, slot_hashes, 0, 0)
⋮----
fn test_check_and_filter_proposed_vote_state_empty(target_version: VoteStateTargetVersion) {
let empty_slot_hashes = build_slot_hashes(vec![]);
let empty_vote_state = build_vote_state(target_version, vec![], &empty_slot_hashes);
let mut tower_sync = TowerSync::from(vec![]);
⋮----
let mut tower_sync = TowerSync::from(vec![(0, 1)]);
⋮----
fn test_check_and_filter_proposed_vote_state_too_old(target_version: VoteStateTargetVersion) {
let slot_hashes = build_slot_hashes(vec![1, 2, 3, 4]);
⋮----
let vote_state = build_vote_state(target_version, vec![1, 2, 3, latest_vote], &slot_hashes);
let mut tower_sync = TowerSync::from(vec![(latest_vote, 1)]);
⋮----
let slot_hashes = build_slot_hashes(vec![earliest_slot_in_history]);
let mut tower_sync = TowerSync::from(vec![(earliest_slot_in_history - 1, 1)]);
⋮----
fn run_test_check_and_filter_proposed_vote_state_older_than_history_root(
⋮----
assert!(proposed_root < earliest_slot_in_history);
⋮----
.max(earliest_slot_in_history);
let mut slot_hashes = build_slot_hashes(
(current_vote_state_slots.first().copied().unwrap_or(0)..=latest_slot_in_history)
⋮----
build_vote_state(target_version, current_vote_state_slots, &slot_hashes);
vote_state.set_root_slot(current_vote_state_root);
slot_hashes.retain(|slot| slot.0 >= earliest_slot_in_history);
assert!(!proposed_slots_and_lockouts.is_empty());
⋮----
.find(|(slot, _hash)| *slot == proposed_slots_and_lockouts.last().unwrap().0)
⋮----
tower_sync.root = Some(proposed_root);
⋮----
assert_eq!(tower_sync.root, expected_root);
assert!(
⋮----
assert_eq!(vote_state.root_slot(), expected_root);
⋮----
fn test_check_and_filter_proposed_vote_state_older_than_history_root(
⋮----
let current_vote_state_slots: Vec<Slot> = vec![1, 2, 3, 4];
⋮----
let proposed_slots_and_lockouts = vec![(5, 1)];
⋮----
let expected_root = Some(4);
let expected_vote_state = vec![Lockout::new_with_confirmation_count(5, 1)];
run_test_check_and_filter_proposed_vote_state_older_than_history_root(
⋮----
let current_vote_state_root = Some(0);
⋮----
let proposed_slots_and_lockouts = vec![(4, 2), (5, 1)];
⋮----
let expected_root = Some(3);
let expected_vote_state = vec![
⋮----
let current_vote_state_slots: Vec<Slot> = vec![1, 2, 4];
⋮----
let expected_root = Some(2);
⋮----
let current_vote_state_slots: Vec<Slot> = vec![3, 4];
⋮----
let proposed_slots_and_lockouts = vec![(3, 3), (4, 2), (5, 1)];
⋮----
let current_vote_state_slots: Vec<Slot> = vec![];
⋮----
fn test_check_and_filter_proposed_vote_state_slots_not_ordered(
⋮----
let vote_state = build_vote_state(target_version, vec![1], &slot_hashes);
⋮----
let mut tower_sync = TowerSync::from(vec![(2, 2), (1, 3), (vote_slot, 1)]);
⋮----
let mut tower_sync = TowerSync::from(vec![(2, 2), (2, 2), (vote_slot, 1)]);
⋮----
fn test_check_and_filter_proposed_vote_state_older_than_history_slots_filtered(
⋮----
let mut vote_state = build_vote_state(target_version, vec![1, 2, 3, 4], &slot_hashes);
⋮----
let slot_hashes = build_slot_hashes(vec![earliest_slot_in_history, 12, 13, 14]);
⋮----
let mut tower_sync = TowerSync::from(vec![
⋮----
assert!(do_process_tower_sync(&mut vote_state, &slot_hashes, 0, 0, tower_sync,).is_ok());
⋮----
fn test_check_and_filter_proposed_vote_state_older_than_history_slots_not_filtered(
⋮----
let slot_hashes = build_slot_hashes(vec![4]);
let mut vote_state = build_vote_state(target_version, vec![4], &slot_hashes);
⋮----
TowerSync::from(vec![(existing_older_than_history_slot, 3), (vote_slot, 2)]);
⋮----
assert_eq!(tower_sync.lockouts.len(), 2);
⋮----
fn test_check_and_filter_proposed_vote_state_older_than_history_slots_filtered_and_not_filtered(
⋮----
let slot_hashes = build_slot_hashes(vec![6]);
let mut vote_state = build_vote_state(target_version, vec![6], &slot_hashes);
⋮----
assert_eq!(tower_sync.lockouts.len(), 3);
⋮----
fn test_check_and_filter_proposed_vote_state_slot_not_on_fork(
⋮----
let slot_hashes = build_slot_hashes(vec![2, 4, 6, 8]);
let vote_state = build_vote_state(target_version, vec![2, 4, 6], &slot_hashes);
⋮----
let vote_slot = vote_state.votes().back().unwrap().slot() + 2;
⋮----
let mut tower_sync = TowerSync::from(vec![(missing_vote_slot, 2), (vote_slot, 3)]);
⋮----
fn test_check_and_filter_proposed_vote_state_root_on_different_fork(
⋮----
let vote_state = build_vote_state(target_version, vec![6], &slot_hashes);
⋮----
assert_eq!(vote_slot, slot_hashes.first().unwrap().0);
⋮----
let mut tower_sync = TowerSync::from(vec![(vote_slot, 1)]);
⋮----
tower_sync.root = Some(new_root);
⋮----
fn test_check_and_filter_proposed_vote_state_slot_newer_than_slot_history(
⋮----
let slot_hashes = build_slot_hashes(vec![2, 4, 6, 8, 10]);
⋮----
let missing_vote_slot = slot_hashes.first().unwrap().0 + 1;
⋮----
let mut tower_sync = TowerSync::from(vec![(8, 2), (missing_vote_slot, 3)]);
⋮----
fn test_check_and_filter_proposed_vote_state_slot_all_slot_hashes_in_update_ok(
⋮----
let mut vote_state = build_vote_state(target_version, vec![2, 4, 6], &slot_hashes);
⋮----
let mut tower_sync = TowerSync::from(vec![(2, 4), (4, 3), (6, 2), (vote_slot, 1)]);
⋮----
fn test_check_and_filter_proposed_vote_state_slot_some_slot_hashes_in_update_ok(
⋮----
let mut tower_sync = TowerSync::from(vec![(4, 2), (vote_slot, 1)]);
⋮----
fn test_check_and_filter_proposed_vote_state_slot_hash_mismatch(
⋮----
fn test_epoch_half_check(slot: Slot, expected_allowed: bool) {
⋮----
fn test_warmup_epoch_half_check_with_warmup() {
⋮----
assert!(is_commission_update_allowed(0, &epoch_schedule));
assert!(is_commission_update_allowed(
⋮----
fn test_epoch_half_check_with_warmup(slot: Slot, expected_allowed: bool) {
⋮----
fn test_create_v4_account_with_authorized() {
⋮----
let vote_account = create_v4_account_with_authorized(
⋮----
Some(bls_pubkey_compressed),
⋮----
assert_eq!(vote_account.lamports(), lamports);
assert_eq!(vote_account.owner(), &id());
assert_eq!(vote_account.data().len(), VoteStateV4::size_of());
let vote_state_v4 = VoteStateV4::deserialize(vote_account.data(), &node_pubkey).unwrap();
assert_eq!(vote_state_v4.node_pubkey, node_pubkey);
⋮----
assert_eq!(vote_state_v4.authorized_withdrawer, authorized_withdrawer);
⋮----
fn test_update_validator_identity_syncs_block_revenue_collector() {
⋮----
vote_state_new_for_test(&solana_pubkey::new_rand(), VoteStateTargetVersion::V4);
⋮----
let signers: HashSet<Pubkey> = vec![withdrawer_pubkey, new_node_pubkey]
⋮----
update_validator_identity(
⋮----
VoteStateV4::deserialize(borrowed_account.get_data(), &new_node_pubkey).unwrap();
assert_eq!(vote_state.node_pubkey, new_node_pubkey);
assert_eq!(vote_state.block_revenue_collector, new_node_pubkey);

================
File: programs/vote/src/lib.rs
================
pub mod vote_processor;
pub mod vote_state;
⋮----
extern crate solana_metrics;
⋮----
extern crate solana_frozen_abi_macro;

================
File: programs/vote/src/vote_processor.rs
================
fn process_authorize_with_seed_instruction(
⋮----
if instruction_context.is_instruction_account_signer(2)? {
let base_pubkey = instruction_context.get_key_of_instruction_account(2)?;
expected_authority_keys.insert(
⋮----
.map_err(|e| e as u64)?,
⋮----
declare_process_instruction!(Entrypoint, DEFAULT_COMPUTE_UNITS, |invoke_context| {
⋮----
mod tests {
⋮----
fn vote_state_size_of(vote_state_v4_enabled: bool) -> usize {
⋮----
fn deserialize_vote_state_for_test(
⋮----
VoteStateHandler::new_v4(VoteStateV4::deserialize(account_data, vote_pubkey).unwrap())
⋮----
VoteStateHandler::new_v3(VoteStateV3::deserialize(account_data).unwrap())
⋮----
struct VoteAccountTestFixtureWithAuthorities {
⋮----
fn create_default_account() -> AccountSharedData {
⋮----
fn process_instruction(
⋮----
mock_process_instruction_with_feature_set(
&id(),
⋮----
fn process_instruction_as_one_arg(
⋮----
.iter()
.map(|meta| meta.pubkey)
.collect();
pubkeys.insert(sysvar::clock::id());
pubkeys.insert(sysvar::epoch_schedule::id());
pubkeys.insert(sysvar::rent::id());
pubkeys.insert(sysvar::slot_hashes::id());
⋮----
.map(|pubkey| {
⋮----
} else if *pubkey == invalid_vote_state_pubkey() {
⋮----
owner: invalid_vote_state_pubkey(),
⋮----
owner: id(),
⋮----
process_instruction(
⋮----
instruction.accounts.clone(),
⋮----
fn invalid_vote_state_pubkey() -> Pubkey {
Pubkey::from_str("BadVote111111111111111111111111111111111111").unwrap()
⋮----
fn create_default_rent_account() -> AccountSharedData {
⋮----
fn create_default_clock_account() -> AccountSharedData {
⋮----
fn create_test_account(vote_state_v4_enabled: bool) -> (Pubkey, AccountSharedData) {
⋮----
let balance = rent.minimum_balance(VoteStateV4::size_of());
⋮----
let balance = rent.minimum_balance(vote_state_size_of(vote_state_v4_enabled));
⋮----
fn create_test_account_with_authorized(
⋮----
let account = create_test_account_with_provided_authorized(
⋮----
fn create_test_account_with_provided_authorized(
⋮----
fn create_test_account_with_authorized_from_seed(
⋮----
Pubkey::create_with_seed(&voter_base_key, voter_seed.as_str(), &voter_owner).unwrap();
⋮----
withdrawer_seed.as_str(),
⋮----
.unwrap();
⋮----
fn create_test_account_with_epoch_credits(
⋮----
let space = vote_state_size_of(vote_state_v4_enabled);
let lamports = Rent::default().minimum_balance(space);
⋮----
let epoch_credits = vote_state.epoch_credits_mut();
epoch_credits.clear();
⋮----
for (epoch, credits) in credits_to_append.iter().enumerate() {
current_epoch_credits = current_epoch_credits.saturating_add(*credits);
epoch_credits.push((
u64::try_from(epoch).unwrap(),
⋮----
let mut account = AccountSharedData::new(lamports, space, &id());
account.set_data_from_slice(&vote_state.serialize());
⋮----
fn create_serialized_votes() -> (Vote, Vec<(Vec<u8>, bool)>) {
let vote = Vote::new(vec![1], Hash::default());
let vote_state_update = VoteStateUpdate::from(vec![(1, 1)]);
let tower_sync = TowerSync::from(vec![(1, 1)]);
⋮----
vote.clone(),
vec![
⋮----
fn test_vote_process_instruction_decode_bail(vote_state_v4_enabled: bool) {
⋮----
Err(InstructionError::MissingAccount),
⋮----
fn test_initialize_vote_account(vote_state_v4_enabled: bool) {
⋮----
AccountSharedData::new(100, vote_state_size_of(vote_state_v4_enabled), &id());
⋮----
let instruction_data = serialize(&VoteInstruction::InitializeAccount(VoteInit {
⋮----
let mut instruction_accounts = vec![
⋮----
let accounts = process_instruction(
⋮----
instruction_accounts.clone(),
Ok(()),
⋮----
Err(InstructionError::AccountAlreadyInitialized),
⋮----
Err(InstructionError::InvalidAccountData),
⋮----
Err(InstructionError::MissingRequiredSignature),
⋮----
fn test_vote_update_validator_identity(vote_state_v4_enabled: bool) {
⋮----
create_test_account_with_authorized(vote_state_v4_enabled);
⋮----
let instruction_data = serialize(&VoteInstruction::UpdateValidatorIdentity).unwrap();
let transaction_accounts = vec![
⋮----
transaction_accounts.clone(),
⋮----
let vote_state = deserialize_vote_state_for_test(
⋮----
accounts[0].data(),
⋮----
assert_ne!(*vote_state.node_pubkey(), node_pubkey);
⋮----
assert_eq!(*vote_state.node_pubkey(), node_pubkey);
⋮----
assert_eq!(vote_state.as_ref_v4().block_revenue_collector, node_pubkey,);
⋮----
fn test_vote_update_commission(vote_state_v4_enabled: bool) {
⋮----
let instruction_data = serialize(&VoteInstruction::UpdateCommission(42)).unwrap();
⋮----
&serialize(&VoteInstruction::UpdateCommission(200)).unwrap(),
⋮----
assert_eq!(vote_state.commission(), 200);
⋮----
assert_eq!(vote_state.commission(), 42);
⋮----
assert_eq!(vote_state.commission(), 0);
⋮----
fn test_vote_signature(vote_state_v4_enabled: bool) {
let (vote_pubkey, vote_account) = create_test_account(vote_state_v4_enabled);
let (vote, instruction_datas) = create_serialized_votes();
let slot_hashes = SlotHashes::new(&[(*vote.slots.last().unwrap(), vote.hash)]);
⋮----
let mut transaction_accounts = vec![
⋮----
Err(InstructionError::InvalidInstructionData)
⋮----
Err(err)
⋮----
error(InstructionError::MissingRequiredSignature),
⋮----
Ok(())
⋮----
let expected_lockout = Lockout::new(*vote.slots.last().unwrap());
assert_eq!(vote_state.votes().len(), 1);
assert_eq!(vote_state.votes()[0].lockout, expected_lockout);
assert_eq!(vote_state.credits(), 0);
⋮----
*vote.slots.last().unwrap(),
⋮----
error(VoteError::SlotHashMismatch.into()),
⋮----
error(VoteError::SlotsMismatch.into()),
⋮----
transaction_accounts[1] = (sysvar::slot_hashes::id(), slot_hashes_account.clone());
⋮----
error(InstructionError::UninitializedAccount),
⋮----
fn test_authorize_voter(vote_state_v4_enabled: bool) {
⋮----
let instruction_data = serialize(&VoteInstruction::Authorize(
⋮----
transaction_accounts[0] = (vote_pubkey, accounts[0].clone());
⋮----
Err(VoteError::TooSoonToReauthorize.into()),
⋮----
instruction_accounts.push(AccountMeta {
⋮----
instruction_accounts.pop();
⋮----
transaction_accounts.push((sysvar::slot_hashes::id(), slot_hashes_account));
instruction_accounts.insert(
⋮----
let mut authorized_instruction_accounts = instruction_accounts.clone();
authorized_instruction_accounts.push(AccountMeta {
⋮----
Err(if is_tower_sync {
⋮----
authorized_instruction_accounts.clone(),
⋮----
fn test_authorize_withdrawer(vote_state_v4_enabled: bool) {
⋮----
transaction_accounts.push((authorized_voter_pubkey, AccountSharedData::default()));
⋮----
fn test_vote_withdraw(vote_state_v4_enabled: bool) {
⋮----
let lamports = vote_account.lamports();
⋮----
&serialize(&VoteInstruction::Authorize(
⋮----
.unwrap(),
⋮----
&serialize(&VoteInstruction::Withdraw(lamports)).unwrap(),
⋮----
assert_eq!(accounts[0].lamports(), 0);
assert_eq!(accounts[3].lamports(), lamports);
let post_state: VoteStateVersions = accounts[0].state().unwrap();
assert!(post_state.is_uninitialized());
⋮----
&serialize(&VoteInstruction::Withdraw(lamports + 1)).unwrap(),
⋮----
Err(InstructionError::InsufficientFunds),
⋮----
&serialize(&VoteInstruction::Withdraw(withdraw_lamports)).unwrap(),
⋮----
assert_eq!(accounts[0].lamports(), lamports - withdraw_lamports);
assert_eq!(accounts[3].lamports(), withdraw_lamports);
⋮----
fn test_vote_state_withdraw(vote_state_v4_enabled: bool) {
⋮----
create_test_account_with_epoch_credits(vote_state_v4_enabled, &[2, 1]);
⋮----
create_test_account_with_epoch_credits(vote_state_v4_enabled, &[2, 1, 3]);
⋮----
.minimum_balance(vote_account_with_epoch_credits_1.data().len())
.max(1);
let lamports = vote_account_with_epoch_credits_1.lamports();
⋮----
&serialize(&VoteInstruction::Withdraw(lamports - minimum_balance + 1)).unwrap(),
⋮----
Err(VoteError::ActiveVoteAccountClose.into()),
⋮----
fn perform_authorize_with_seed_test(
⋮----
&serialize(&VoteInstruction::AuthorizeWithSeed(
⋮----
current_authority_derived_key_seed: current_authority_seed.clone(),
⋮----
fn perform_authorize_checked_with_seed_test(
⋮----
&serialize(&VoteInstruction::AuthorizeCheckedWithSeed(
⋮----
fn test_voter_base_key_can_authorize_new_voter(vote_state_v4_enabled: bool) {
⋮----
} = create_test_account_with_authorized_from_seed(vote_state_v4_enabled);
⋮----
perform_authorize_with_seed_test(
⋮----
fn test_withdrawer_base_key_can_authorize_new_voter(vote_state_v4_enabled: bool) {
⋮----
fn test_voter_base_key_can_not_authorize_new_withdrawer(vote_state_v4_enabled: bool) {
⋮----
let instruction_accounts = vec![
⋮----
fn test_withdrawer_base_key_can_authorize_new_withdrawer(vote_state_v4_enabled: bool) {
⋮----
fn test_voter_base_key_can_authorize_new_voter_checked(vote_state_v4_enabled: bool) {
⋮----
perform_authorize_checked_with_seed_test(
⋮----
fn test_withdrawer_base_key_can_authorize_new_voter_checked(vote_state_v4_enabled: bool) {
⋮----
fn test_voter_base_key_can_not_authorize_new_withdrawer_checked(vote_state_v4_enabled: bool) {
⋮----
fn test_withdrawer_base_key_can_authorize_new_withdrawer_checked(vote_state_v4_enabled: bool) {
⋮----
fn test_spoofed_vote(vote_state_v4_enabled: bool) {
process_instruction_as_one_arg(
⋮----
&vote(
&invalid_vote_state_pubkey(),
⋮----
Err(InstructionError::InvalidAccountOwner),
⋮----
&update_vote_state(
⋮----
&compact_update_vote_state(
⋮----
&tower_sync(
⋮----
fn test_create_account_vote_state_1_14_11(vote_state_v4_enabled: bool) {
⋮----
let instructions = create_account_with_config(
⋮----
let space = usize::from_le_bytes(instructions[0].data[12..20].try_into().unwrap());
assert_eq!(space, vote_state::VoteState1_14_11::size_of());
let empty_vote_account = AccountSharedData::new(101, space, &id());
⋮----
instructions[1].accounts.clone(),
⋮----
fn test_create_account_vote_state_current(vote_state_v4_enabled: bool) {
⋮----
space: vote_state_size_of(vote_state_v4_enabled) as u64,
⋮----
assert_eq!(space, vote_state_size_of(vote_state_v4_enabled));
⋮----
fn test_vote_process_instruction(vote_state_v4_enabled: bool) {
⋮----
Err(InstructionError::InvalidInstructionData),
⋮----
&vote_switch(
⋮----
&authorize(
⋮----
&update_vote_state_switch(
⋮----
&compact_update_vote_state_switch(
⋮----
&tower_sync(&Pubkey::default(), &Pubkey::default(), TowerSync::default()),
⋮----
&tower_sync_switch(
⋮----
&update_validator_identity(
⋮----
&update_commission(&Pubkey::new_unique(), &Pubkey::new_unique(), 0),
⋮----
&withdraw(
⋮----
fn test_vote_authorize_checked(vote_state_v4_enabled: bool) {
⋮----
let mut instruction = authorize_checked(
⋮----
instruction.accounts = instruction.accounts[0..2].to_vec();
⋮----
let vote_account = create_test_account_with_provided_authorized(
⋮----
let authorized_account = create_default_account();
let new_authorized_account = create_default_account();
⋮----
&serialize(&VoteInstruction::AuthorizeChecked(VoteAuthorize::Voter)).unwrap(),
⋮----
&serialize(&VoteInstruction::AuthorizeChecked(

================
File: programs/vote/Cargo.toml
================
[package]
name = "solana-vote-program"
description = "Solana Vote program"
documentation = "https://docs.rs/solana-vote-program"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_vote_program"

[features]
default = ["metrics"]
agave-unstable-api = []
dev-context-only-utils = []
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-program-runtime/frozen-abi",
    "solana-vote-interface/frozen-abi",
]
metrics = ["dep:solana-metrics"]

[dependencies]
agave-feature-set = { workspace = true }
bincode = { workspace = true }
log = { workspace = true }
num-derive = { workspace = true }
num-traits = { workspace = true }
serde = { workspace = true }
solana-account = { workspace = true }
solana-bincode = { workspace = true }
solana-clock = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-metrics = { workspace = true, optional = true }
solana-packet = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true, features = ["curve25519"] }
solana-rent = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signer = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-transaction = { workspace = true, features = ["bincode"] }
solana-transaction-context = { workspace = true, features = ["bincode"] }
solana-vote-interface = { workspace = true, features = ["bincode"] }
thiserror = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
assert_matches = { workspace = true }
criterion = { workspace = true }
solana-account = { workspace = true }
solana-clock = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-instruction = { workspace = true }
solana-program-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-rent = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-vote-program = { path = ".", features = ["agave-unstable-api"] }
test-case = { workspace = true }

[[bench]]
name = "vote_instructions"
harness = false

[lints]
workspace = true

================
File: programs/zk-elgamal-proof/benches/verify_proofs.rs
================
fn bench_pubkey_validity(c: &mut Criterion) {
⋮----
let proof_data = PubkeyValidityProofData::new(&keypair).unwrap();
c.bench_function("pubkey_validity", |b| {
b.iter(|| {
proof_data.verify_proof().unwrap();
⋮----
fn bench_zero_ciphertext(c: &mut Criterion) {
⋮----
let ciphertext = keypair.pubkey().encrypt(0_u64);
let proof_data = ZeroCiphertextProofData::new(&keypair, &ciphertext).unwrap();
c.bench_function("zero_ciphertext", |b| {
⋮----
fn bench_grouped_ciphertext_2_handles_validity(c: &mut Criterion) {
⋮----
let destination_pubkey = destination_keypair.pubkey();
⋮----
let auditor_pubkey = auditor_keypair.pubkey();
⋮----
.unwrap();
c.bench_function("grouped_ciphertext_2_handles_validity", |b| {
⋮----
fn bench_grouped_ciphertext_3_handles_validity(c: &mut Criterion) {
⋮----
let source_pubkey = source_keypair.pubkey();
⋮----
c.bench_function("grouped_ciphertext_3_handles_validity", |b| {
⋮----
fn bench_ciphertext_commitment_equality(c: &mut Criterion) {
⋮----
let ciphertext = keypair.pubkey().encrypt(amount);
⋮----
c.bench_function("ciphertext_commitment_equality", |b| {
⋮----
fn bench_ciphertext_ciphertext_equality(c: &mut Criterion) {
⋮----
let source_ciphertext = source_keypair.pubkey().encrypt(amount);
⋮----
.pubkey()
.encrypt_with(amount, &destination_opening);
⋮----
destination_keypair.pubkey(),
⋮----
c.bench_function("ciphertext_ciphertext_equality", |b| {
⋮----
fn bench_batched_grouped_ciphertext_2_handles_validity(c: &mut Criterion) {
⋮----
c.bench_function("batched_grouped_ciphertext_validity", |b| {
⋮----
fn bench_batched_grouped_ciphertext_3_handles_validity(c: &mut Criterion) {
⋮----
c.bench_function("batched_grouped_ciphertext_3_handles_validity", |b| {
⋮----
fn bench_percentage_with_cap(c: &mut Criterion) {
⋮----
c.bench_function("percentage_with_cap", |b| {
⋮----
fn bench_batched_range_proof_u64(c: &mut Criterion) {
⋮----
vec![
⋮----
vec![8, 8, 8, 8, 8, 8, 8, 8],
⋮----
c.bench_function("batched_range_proof_u64", |b| {
⋮----
fn bench_batched_range_proof_u128(c: &mut Criterion) {
⋮----
vec![16, 16, 16, 16, 16, 16, 16, 16],
⋮----
c.bench_function("batched_range_proof_u128", |b| {
⋮----
fn bench_batched_range_proof_u256(c: &mut Criterion) {
⋮----
vec![32, 32, 32, 32, 32, 32, 32, 32],
⋮----
c.bench_function("batched_range_proof_u256", |b| {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: programs/zk-elgamal-proof/src/lib.rs
================
fn process_verify_proof<T, U>(invoke_context: &mut InvokeContext) -> Result<(), InstructionError>
⋮----
let instruction_context = transaction_context.get_current_instruction_context()?;
let instruction_data = instruction_context.get_instruction_data();
⋮----
let context_data = if instruction_data.len() == INSTRUCTION_DATA_LENGTH_WITH_PROOF_ACCOUNT {
⋮----
instruction_context.try_borrow_instruction_account(accessed_accounts)?;
accessed_accounts = accessed_accounts.checked_add(1).unwrap();
⋮----
.try_into()
.map_err(|_| InstructionError::InvalidInstructionData)?,
⋮----
.map_err(|_| InstructionError::InvalidInstructionData)?;
⋮----
.checked_add(std::mem::size_of::<T>())
.ok_or(InstructionError::InvalidInstructionData)?;
⋮----
.get_data()
.get(proof_data_start..proof_data_end)
.ok_or(InstructionError::InvalidAccountData)?;
let proof_data = bytemuck::try_from_bytes::<T>(proof_data_raw).map_err(|_| {
ic_msg!(invoke_context, "invalid proof data");
⋮----
proof_data.verify_proof().map_err(|err| {
ic_msg!(invoke_context, "proof verification failed: {:?}", err);
⋮----
*proof_data.context_data()
⋮----
ProofInstruction::proof_data::<T, U>(instruction_data).ok_or_else(|| {
⋮----
ic_msg!(invoke_context, "proof_verification failed: {:?}", err);
⋮----
if instruction_context.get_number_of_instruction_accounts()
⋮----
.checked_add(2)
.ok_or(InstructionError::ArithmeticOverflow)?
⋮----
.try_borrow_instruction_account(accessed_accounts.checked_add(1).unwrap())?
.get_key();
⋮----
if *proof_context_account.get_owner() != id() {
return Err(InstructionError::InvalidAccountOwner);
⋮----
ProofContextStateMeta::try_from_bytes(proof_context_account.get_data())?;
if proof_context_state_meta.proof_type != ProofType::Uninitialized.into() {
return Err(InstructionError::AccountAlreadyInitialized);
⋮----
if proof_context_account.get_data().len() != context_state_data.len() {
return Err(InstructionError::InvalidAccountData);
⋮----
proof_context_account.set_data_from_slice(&context_state_data)?;
⋮----
Ok(())
⋮----
fn process_close_proof_context(invoke_context: &mut InvokeContext) -> Result<(), InstructionError> {
⋮----
if !instruction_context.is_instruction_account_signer(2)? {
return Err(InstructionError::MissingRequiredSignature);
⋮----
*instruction_context.get_key_of_instruction_account(2)?
⋮----
let proof_context_account_pubkey = *instruction_context.get_key_of_instruction_account(0)?;
let destination_account_pubkey = *instruction_context.get_key_of_instruction_account(1)?;
⋮----
return Err(InstructionError::InvalidInstructionData);
⋮----
let mut proof_context_account = instruction_context.try_borrow_instruction_account(0)?;
⋮----
if proof_context_state_meta.proof_type == ProofType::Uninitialized.into() {
return Err(InstructionError::UninitializedAccount);
⋮----
let mut destination_account = instruction_context.try_borrow_instruction_account(1)?;
destination_account.checked_add_lamports(proof_context_account.get_lamports())?;
proof_context_account.set_lamports(0)?;
proof_context_account.set_data_length(0)?;
proof_context_account.set_owner(system_program::id().as_ref())?;
⋮----
declare_process_instruction!(Entrypoint, 0, |invoke_context| {

================
File: programs/zk-elgamal-proof/Cargo.toml
================
[package]
name = "solana-zk-elgamal-proof-program"
description = "Solana Zk ElGamal Proof Program"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
bytemuck = { workspace = true }
num-derive = { workspace = true }
num-traits = { workspace = true }
solana-instruction = { workspace = true, features = ["std"] }
solana-program-runtime = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-zk-sdk = { workspace = true }

[dev-dependencies]
criterion = { workspace = true }
curve25519-dalek = { workspace = true }

[[bench]]
name = "verify_proofs"
harness = false

================
File: programs/zk-elgamal-proof-tests/tests/process_transaction.rs
================
async fn test_zero_balance() {
⋮----
let zero_ciphertext = elgamal_keypair.pubkey().encrypt(0_u64);
⋮----
ZeroCiphertextProofData::new(&elgamal_keypair, &zero_ciphertext).unwrap();
let incorrect_pubkey = elgamal_keypair.pubkey();
⋮----
ZeroCiphertextProofData::new(&incorrect_keypair, &zero_ciphertext).unwrap();
test_verify_proof_without_context(
⋮----
test_verify_proof_from_account_with_context(
⋮----
test_verify_proof_with_context(
⋮----
test_close_context_state(
⋮----
async fn test_ciphertext_ciphertext_equality() {
⋮----
let source_ciphertext = source_keypair.pubkey().encrypt(amount);
⋮----
.pubkey()
.encrypt_with(amount, &destination_opening);
⋮----
destination_keypair.pubkey(),
⋮----
.unwrap();
let incorrect_pubkey = source_keypair.pubkey();
⋮----
async fn test_pubkey_validity() {
⋮----
let success_proof_data = PubkeyValidityProofData::new(&elgamal_keypair).unwrap();
⋮----
let fail_proof_data = PubkeyValidityProofData::new(&incorrect_keypair).unwrap();
⋮----
async fn test_batched_range_proof_u64() {
⋮----
vec![&commitment_1, &commitment_2],
vec![amount_1, amount_2],
vec![32, 32],
vec![&opening_1, &opening_2],
⋮----
vec![&opening_1, &incorrect_opening],
⋮----
async fn test_batched_range_proof_u128() {
⋮----
vec![64, 64],
⋮----
async fn test_batched_range_proof_u256() {
⋮----
vec![&commitment_1, &commitment_2, &commitment_3, &commitment_4],
vec![amount_1, amount_2, amount_3, amount_4],
vec![64, 64, 64, 64],
vec![&opening_1, &opening_2, &opening_3, &opening_4],
⋮----
vec![&opening_1, &opening_2, &opening_3, &incorrect_opening],
⋮----
async fn test_ciphertext_commitment_equality() {
⋮----
let ciphertext = keypair.pubkey().encrypt(amount);
⋮----
let incorrect_pubkey = keypair.pubkey();
⋮----
async fn test_grouped_ciphertext_2_handles_validity() {
⋮----
let destination_pubkey = destination_keypair.pubkey();
⋮----
let auditor_pubkey = auditor_keypair.pubkey();
⋮----
async fn test_batched_grouped_ciphertext_2_handles_validity() {
⋮----
async fn test_grouped_ciphertext_3_handles_validity() {
⋮----
let source_pubkey = source_keypair.pubkey();
⋮----
async fn test_batched_grouped_ciphertext_3_handles_validity() {
⋮----
async fn test_verify_proof_without_context<T, U>(
⋮----
program_test.set_compute_max_units(500_000);
⋮----
program_test.add_account(
⋮----
data: bytes_of(success_proof_data).to_vec(),
⋮----
data: bytes_of(fail_proof_data).to_vec(),
⋮----
let mut context = program_test.start_with_context().await;
⋮----
let instructions = vec![proof_instruction.encode_verify_proof(None, success_proof_data)];
⋮----
&instructions.with_max_compute_unit_limit(),
Some(&payer.pubkey()),
⋮----
client.get_latest_blockhash().await.unwrap(),
⋮----
client.process_transaction(transaction).await.unwrap();
let instructions = vec![proof_instruction.encode_verify_proof(None, fail_proof_data)];
⋮----
.process_transaction(transaction)
⋮----
.unwrap_err()
⋮----
assert_eq!(
⋮----
vec![wrong_instruction_type.encode_verify_proof(None, success_proof_data)];
⋮----
&instruction.with_max_compute_unit_limit(),
⋮----
vec![proof_instruction.encode_verify_proof_from_account(None, &success_proof_account, 0)];
⋮----
vec![proof_instruction.encode_verify_proof_from_account(None, &fail_proof_account, 0)];
⋮----
async fn test_verify_proof_with_context<T, U>(
⋮----
let rent = context.banks_client.get_rent().await.unwrap();
⋮----
context_state_account: &context_state_account.pubkey(),
context_state_authority: &context_state_authority.pubkey(),
⋮----
let instructions = vec![
⋮----
vec![instruction_type.encode_verify_proof(Some(context_state_info), success_proof_data)];
⋮----
context_state_account: &context_state_account_and_authority.pubkey(),
context_state_authority: &context_state_account_and_authority.pubkey(),
⋮----
async fn test_verify_proof_from_account_with_context<T, U>(
⋮----
let instructions = vec![instruction_type.encode_verify_proof_from_account(
⋮----
async fn test_close_context_state<T, U>(
⋮----
let instruction = close_context_state(
⋮----
context_state_authority: &incorrect_authority.pubkey(),
⋮----
&destination_account.pubkey(),
⋮----
&vec![instruction].with_max_compute_unit_limit(),
⋮----
&vec![instruction.clone()].with_max_compute_unit_limit(),
⋮----
trait WithMaxComputeUnitLimit {
⋮----
impl WithMaxComputeUnitLimit for Vec<solana_instruction::Instruction> {
fn with_max_compute_unit_limit(mut self) -> Self {
self.push(

================
File: programs/zk-elgamal-proof-tests/Cargo.toml
================
[package]
name = "solana-zk-elgamal-proof-program-tests"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dev-dependencies]
bytemuck = { workspace = true }
solana-account = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-program-test = { workspace = true }
solana-pubkey = { workspace = true }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
solana-zk-sdk = { workspace = true }

================
File: programs/zk-token-proof/benches/verify_proofs.rs
================
fn bench_pubkey_validity(c: &mut Criterion) {
⋮----
let proof_data = PubkeyValidityData::new(&keypair).unwrap();
c.bench_function("pubkey_validity", |b| {
b.iter(|| {
proof_data.verify_proof().unwrap();
⋮----
fn bench_range_proof_u64(c: &mut Criterion) {
⋮----
let proof_data = RangeProofU64Data::new(&commitment, amount, &opening).unwrap();
c.bench_function("range_proof_u64", |b| {
⋮----
fn bench_withdraw(c: &mut Criterion) {
⋮----
let current_ciphertext = keypair.pubkey().encrypt(current_balance);
⋮----
.unwrap();
c.bench_function("withdraw", |b| {
⋮----
fn bench_zero_balance(c: &mut Criterion) {
⋮----
let ciphertext = keypair.pubkey().encrypt(0_u64);
let proof_data = ZeroBalanceProofData::new(&keypair, &ciphertext).unwrap();
c.bench_function("zero_balance", |b| {
⋮----
fn bench_grouped_ciphertext_2_handles_validity(c: &mut Criterion) {
⋮----
let destination_pubkey = destination_keypair.pubkey();
⋮----
let auditor_pubkey = auditor_keypair.pubkey();
⋮----
c.bench_function("grouped_ciphertext_2_handles_validity", |b| {
⋮----
fn bench_grouped_ciphertext_3_handles_validity(c: &mut Criterion) {
⋮----
let source_pubkey = source_keypair.pubkey();
⋮----
c.bench_function("grouped_ciphertext_3_handles_validity", |b| {
⋮----
fn bench_ciphertext_commitment_equality(c: &mut Criterion) {
⋮----
let ciphertext = keypair.pubkey().encrypt(amount);
⋮----
c.bench_function("ciphertext_commitment_equality", |b| {
⋮----
fn bench_ciphertext_ciphertext_equality(c: &mut Criterion) {
⋮----
let source_ciphertext = source_keypair.pubkey().encrypt(amount);
⋮----
.pubkey()
.encrypt_with(amount, &destination_opening);
⋮----
destination_keypair.pubkey(),
⋮----
c.bench_function("ciphertext_ciphertext_equality", |b| {
⋮----
fn bench_batched_grouped_ciphertext_2_handles_validity(c: &mut Criterion) {
⋮----
c.bench_function("batched_grouped_ciphertext_validity", |b| {
⋮----
fn bench_batched_grouped_ciphertext_3_handles_validity(c: &mut Criterion) {
⋮----
c.bench_function("batched_grouped_ciphertext_3_handles_validity", |b| {
⋮----
fn bench_fee_sigma(c: &mut Criterion) {
⋮----
c.bench_function("fee_sigma", |b| {
⋮----
fn bench_batched_range_proof_u64(c: &mut Criterion) {
⋮----
vec![
⋮----
vec![8, 8, 8, 8, 8, 8, 8, 8],
⋮----
c.bench_function("batched_range_proof_u64", |b| {
⋮----
fn bench_batched_range_proof_u128(c: &mut Criterion) {
⋮----
vec![16, 16, 16, 16, 16, 16, 16, 16],
⋮----
c.bench_function("batched_range_proof_u128", |b| {
⋮----
fn bench_batched_range_proof_u256(c: &mut Criterion) {
⋮----
vec![32, 32, 32, 32, 32, 32, 32, 32],
⋮----
c.bench_function("batched_range_proof_u256", |b| {
⋮----
fn bench_transfer(c: &mut Criterion) {
⋮----
let spendable_ciphertext = source_keypair.pubkey().encrypt(spendable_balance);
⋮----
c.bench_function("transfer", |b| {
⋮----
fn bench_transfer_with_fee(c: &mut Criterion) {
⋮----
let withdraw_withheld_authority_pubkey = withdraw_withheld_authority_keypair.pubkey();
⋮----
c.bench_function("transfer_with_fee", |b| {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: programs/zk-token-proof/src/lib.rs
================
fn process_verify_proof<T, U>(invoke_context: &mut InvokeContext) -> Result<(), InstructionError>
⋮----
let instruction_context = transaction_context.get_current_instruction_context()?;
let instruction_data = instruction_context.get_instruction_data();
⋮----
let context_data = if instruction_data.len() == INSTRUCTION_DATA_LENGTH_WITH_PROOF_ACCOUNT {
⋮----
return Err(InstructionError::InvalidInstructionData);
⋮----
instruction_context.try_borrow_instruction_account(accessed_accounts)?;
accessed_accounts = accessed_accounts.checked_add(1).unwrap();
⋮----
.try_into()
.map_err(|_| InstructionError::InvalidInstructionData)?,
⋮----
.map_err(|_| InstructionError::InvalidInstructionData)?;
⋮----
.checked_add(std::mem::size_of::<T>())
.ok_or(InstructionError::InvalidInstructionData)?;
⋮----
.get_data()
.get(proof_data_start..proof_data_end)
.ok_or(InstructionError::InvalidAccountData)?;
let proof_data = bytemuck::try_from_bytes::<T>(proof_data_raw).map_err(|_| {
ic_msg!(invoke_context, "invalid proof data");
⋮----
proof_data.verify_proof().map_err(|err| {
ic_msg!(invoke_context, "proof verification failed: {:?}", err);
⋮----
*proof_data.context_data()
⋮----
ProofInstruction::proof_data::<T, U>(instruction_data).ok_or_else(|| {
⋮----
ic_msg!(invoke_context, "proof_verification failed: {:?}", err);
⋮----
if instruction_context.get_number_of_instruction_accounts() > accessed_accounts {
⋮----
.get_key_of_instruction_account(accessed_accounts.checked_add(1).unwrap())?;
⋮----
if *proof_context_account.get_owner() != id() {
return Err(InstructionError::InvalidAccountOwner);
⋮----
ProofContextStateMeta::try_from_bytes(proof_context_account.get_data())?;
if proof_context_state_meta.proof_type != ProofType::Uninitialized.into() {
return Err(InstructionError::AccountAlreadyInitialized);
⋮----
if proof_context_account.get_data().len() != context_state_data.len() {
return Err(InstructionError::InvalidAccountData);
⋮----
proof_context_account.set_data_from_slice(&context_state_data)?;
⋮----
Ok(())
⋮----
fn process_close_proof_context(invoke_context: &mut InvokeContext) -> Result<(), InstructionError> {
⋮----
if !instruction_context.is_instruction_account_signer(2)? {
return Err(InstructionError::MissingRequiredSignature);
⋮----
*instruction_context.get_key_of_instruction_account(2)?
⋮----
let proof_context_account_pubkey = *instruction_context.get_key_of_instruction_account(0)?;
let destination_account_pubkey = *instruction_context.get_key_of_instruction_account(1)?;
⋮----
let mut proof_context_account = instruction_context.try_borrow_instruction_account(0)?;
⋮----
let mut destination_account = instruction_context.try_borrow_instruction_account(1)?;
destination_account.checked_add_lamports(proof_context_account.get_lamports())?;
proof_context_account.set_lamports(0)?;
proof_context_account.set_data_length(0)?;
proof_context_account.set_owner(system_program::id().as_ref())?;
⋮----
declare_process_instruction!(Entrypoint, 0, |invoke_context| {

================
File: programs/zk-token-proof/Cargo.toml
================
[package]
name = "solana-zk-token-proof-program"
description = "Solana Zk Token Proof Program"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
bytemuck = { workspace = true }
num-derive = { workspace = true }
num-traits = { workspace = true }
solana-instruction = { workspace = true }
solana-program-runtime = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-zk-token-sdk = { workspace = true }

[dev-dependencies]
criterion = { workspace = true }
curve25519-dalek = { workspace = true }

[[bench]]
name = "verify_proofs"
harness = false

================
File: pubsub-client/src/nonblocking/mod.rs
================
pub mod pubsub_client;

================
File: pubsub-client/src/nonblocking/pubsub_client.rs
================
pub type PubsubClientResult<T = ()> = Result<T, PubsubClientError>;
⋮----
pub enum PubsubClientError {
⋮----
type UnsubscribeFn = Box<dyn FnOnce() -> BoxFuture<'static, ()> + Send>;
type SubscribeResponseMsg =
⋮----
type SubscribeRequestMsg = (String, Value, oneshot::Sender<SubscribeResponseMsg>);
type SubscribeResult<'a, T> = PubsubClientResult<(BoxStream<'a, T>, UnsubscribeFn)>;
type RequestMsg = (
⋮----
/// A client for subscribing to messages from the RPC server.
///
⋮----
///
/// See the [module documentation][self].
⋮----
/// See the [module documentation][self].
#[derive(Debug)]
pub struct PubsubClient {
⋮----
async fn connect_with_retry<R: IntoClientRequest>(
⋮----
let client_request = request.into_client_request().map_err(Box::new)?;
⋮----
let result = connect_async(client_request.clone())
⋮----
.map(|(socket, _)| socket);
⋮----
if response.status() == StatusCode::TOO_MANY_REQUESTS && connection_retries > 0 {
⋮----
if let Some(retry_after) = response.headers().get(header::RETRY_AFTER) {
if let Ok(retry_after) = retry_after.to_str() {
⋮----
debug!(
⋮----
sleep(duration).await;
⋮----
return result.map_err(Box::new);
⋮----
impl PubsubClient {
pub async fn new<R: IntoClientRequest>(request: R) -> PubsubClientResult<Self> {
⋮----
let ws = connect_with_retry(client_request)
⋮----
.map_err(PubsubClientError::ConnectionError)?;
⋮----
Ok(Self {
⋮----
pub async fn shutdown(self) -> PubsubClientResult {
let _ = self.shutdown_sender.send(());
self.ws.await.unwrap() // WS future should not be cancelled or panicked
⋮----
async fn subscribe<'a, T>(&self, operation: &str, params: Value) -> SubscribeResult<'a, T>
⋮----
.send((operation.to_string(), params, response_sender))
.map_err(|err| PubsubClientError::ConnectionClosed(err.to_string()))?;
⋮----
.map_err(|err| PubsubClientError::ConnectionClosed(err.to_string()))??;
Ok((
⋮----
.filter_map(|value| ready(serde_json::from_value::<T>(value).ok()))
.boxed(),
⋮----
pub async fn account_subscribe(
⋮----
let params = json!([pubkey.to_string(), config]);
self.subscribe("account", params).await
⋮----
/// Subscribe to block events.
    ///
⋮----
///
    /// Receives messages of type [`RpcBlockUpdate`] when a block is confirmed or finalized.
⋮----
/// Receives messages of type [`RpcBlockUpdate`] when a block is confirmed or finalized.
    ///
⋮----
///
    /// This method is disabled by default. It can be enabled by passing
⋮----
/// This method is disabled by default. It can be enabled by passing
    /// `--rpc-pubsub-enable-block-subscription` to `agave-validator`.
⋮----
/// `--rpc-pubsub-enable-block-subscription` to `agave-validator`.
    ///
⋮----
///
    /// # RPC Reference
⋮----
/// # RPC Reference
    ///
⋮----
///
    /// This method corresponds directly to the [`blockSubscribe`] RPC method.
⋮----
/// This method corresponds directly to the [`blockSubscribe`] RPC method.
    ///
⋮----
///
    /// [`blockSubscribe`]: https://solana.com/docs/rpc/websocket#blocksubscribe
⋮----
/// [`blockSubscribe`]: https://solana.com/docs/rpc/websocket#blocksubscribe
    pub async fn block_subscribe(
⋮----
pub async fn block_subscribe(
⋮----
self.subscribe("block", json!([filter, config])).await
⋮----
pub async fn logs_subscribe(
⋮----
self.subscribe("logs", json!([filter, config])).await
⋮----
/// Subscribe to program account events.
    ///
⋮----
///
    /// Receives messages of type [`RpcKeyedAccount`] when an account owned
⋮----
/// Receives messages of type [`RpcKeyedAccount`] when an account owned
    /// by the given program changes.
⋮----
/// by the given program changes.
    ///
⋮----
///
    /// This method corresponds directly to the [`programSubscribe`] RPC method.
⋮----
/// This method corresponds directly to the [`programSubscribe`] RPC method.
    ///
⋮----
///
    /// [`programSubscribe`]: https://solana.com/docs/rpc/websocket#programsubscribe
⋮----
/// [`programSubscribe`]: https://solana.com/docs/rpc/websocket#programsubscribe
    pub async fn program_subscribe(
⋮----
pub async fn program_subscribe(
⋮----
self.subscribe("program", params).await
⋮----
pub async fn vote_subscribe(&self) -> SubscribeResult<'_, RpcVote> {
self.subscribe("vote", json!([])).await
⋮----
/// Subscribe to root events.
    ///
⋮----
///
    /// Receives messages of type [`Slot`] when a new [root] is set by the
⋮----
/// Receives messages of type [`Slot`] when a new [root] is set by the
    /// validator.
⋮----
/// validator.
    ///
⋮----
///
    /// [root]: https://solana.com/docs/terminology#root
⋮----
/// [root]: https://solana.com/docs/terminology#root
    ///
⋮----
///
    /// This method corresponds directly to the [`rootSubscribe`] RPC method.
⋮----
/// This method corresponds directly to the [`rootSubscribe`] RPC method.
    ///
⋮----
///
    /// [`rootSubscribe`]: https://solana.com/docs/rpc/websocket#rootsubscribe
⋮----
/// [`rootSubscribe`]: https://solana.com/docs/rpc/websocket#rootsubscribe
    pub async fn root_subscribe(&self) -> SubscribeResult<'_, Slot> {
⋮----
pub async fn root_subscribe(&self) -> SubscribeResult<'_, Slot> {
self.subscribe("root", json!([])).await
⋮----
pub async fn signature_subscribe(
⋮----
let params = json!([signature.to_string(), config]);
self.subscribe("signature", params).await
⋮----
/// Subscribe to slot events.
    ///
⋮----
///
    /// Receives messages of type [`SlotInfo`] when a slot is processed.
⋮----
/// Receives messages of type [`SlotInfo`] when a slot is processed.
    ///
⋮----
///
    /// This method corresponds directly to the [`slotSubscribe`] RPC method.
⋮----
/// This method corresponds directly to the [`slotSubscribe`] RPC method.
    ///
⋮----
///
    /// [`slotSubscribe`]: https://solana.com/docs/rpc/websocket#slotsubscribe
⋮----
/// [`slotSubscribe`]: https://solana.com/docs/rpc/websocket#slotsubscribe
    pub async fn slot_subscribe(&self) -> SubscribeResult<'_, SlotInfo> {
⋮----
pub async fn slot_subscribe(&self) -> SubscribeResult<'_, SlotInfo> {
self.subscribe("slot", json!([])).await
⋮----
pub async fn slot_updates_subscribe(&self) -> SubscribeResult<'_, SlotUpdate> {
self.subscribe("slotsUpdates", json!([])).await
⋮----
async fn run_ws(
⋮----
// Send `Message::Ping` each 10s if no any other communication
⋮----
// Read message for subscribe
⋮----
Ok(())
⋮----
mod tests {

================
File: pubsub-client/src/lib.rs
================
pub mod nonblocking;
pub mod pubsub_client;

================
File: pubsub-client/src/pubsub_client.rs
================
pub use crate::nonblocking::pubsub_client::PubsubClientError;
⋮----
pub struct PubsubClientSubscription<T>
⋮----
impl<T> Drop for PubsubClientSubscription<T>
⋮----
fn drop(&mut self) {
self.send_unsubscribe()
.unwrap_or_else(|_| warn!("unable to unsubscribe from websocket"));
⋮----
.write()
.unwrap()
.close(None)
.unwrap_or_else(|_| warn!("unable to close websocket"));
⋮----
fn send_subscribe(
⋮----
.send(Message::Text(body.into()))
.map_err(Box::new)?;
let message = writable_socket.write().unwrap().read().map_err(Box::new)?;
⋮----
fn extract_subscription_id(message: Message) -> Result<u64, PubsubClientError> {
let message_text = &message.into_text().map_err(Box::new)?;
⋮----
if let Some(Number(x)) = json_msg.get("result") {
if let Some(x) = x.as_u64() {
return Ok(x);
⋮----
Err(PubsubClientError::UnexpectedSubscriptionResponse(format!(
⋮----
/// Send an unsubscribe message to the server.
    ///
⋮----
///
    /// Note that this will block as long as the internal subscription receiver
⋮----
/// Note that this will block as long as the internal subscription receiver
    /// is waiting on messages from the server, and this can take an unbounded
⋮----
/// is waiting on messages from the server, and this can take an unbounded
    /// amount of time if the server does not send any messages.
⋮----
/// amount of time if the server does not send any messages.
    ///
⋮----
///
    /// If a pubsub client needs to shutdown reliably it should use
⋮----
/// If a pubsub client needs to shutdown reliably it should use
    /// the async client in [`crate::nonblocking::pubsub_client`].
⋮----
/// the async client in [`crate::nonblocking::pubsub_client`].
    pub fn send_unsubscribe(&self) -> Result<(), PubsubClientError> {
⋮----
pub fn send_unsubscribe(&self) -> Result<(), PubsubClientError> {
let method = format!("{}Unsubscribe", self.operation);
⋮----
.send(Message::Text(
json!({
⋮----
.to_string()
.into(),
⋮----
.map_err(Box::new)
.map_err(|err| err.into())
⋮----
fn read_message(
⋮----
if message.is_ping() {
return Ok(None);
⋮----
if let Some(Object(params)) = json_msg.get("params") {
if let Some(result) = params.get("result") {
if let Ok(x) = serde_json::from_value::<T>(result.clone()) {
return Ok(Some(x));
⋮----
Err(PubsubClientError::UnexpectedMessageError(format!(
⋮----
/// Shutdown the internel message receiver and wait for its thread to exit.
    ///
⋮----
///
    /// Note that this will block as long as the subscription receiver is
⋮----
/// Note that this will block as long as the subscription receiver is
    /// waiting on messages from the server, and this can take an unbounded
⋮----
/// waiting on messages from the server, and this can take an unbounded
    /// amount of time if the server does not send any messages.
⋮----
/// the async client in [`crate::nonblocking::pubsub_client`].
    pub fn shutdown(&mut self) -> std::thread::Result<()> {
⋮----
pub fn shutdown(&mut self) -> std::thread::Result<()> {
if self.t_cleanup.is_some() {
info!("websocket thread - shutting down");
self.exit.store(true, Ordering::Relaxed);
let x = self.t_cleanup.take().unwrap().join();
info!("websocket thread - shut down.");
⋮----
warn!("websocket thread - already shut down.");
Ok(())
⋮----
pub type PubsubLogsClientSubscription = PubsubClientSubscription<RpcResponse<RpcLogsResponse>>;
pub type LogsSubscription = (
⋮----
pub type PubsubSlotClientSubscription = PubsubClientSubscription<SlotInfo>;
pub type SlotsSubscription = (PubsubSlotClientSubscription, Receiver<SlotInfo>);
pub type PubsubSignatureClientSubscription =
⋮----
pub type SignatureSubscription = (
⋮----
pub type PubsubBlockClientSubscription = PubsubClientSubscription<RpcResponse<RpcBlockUpdate>>;
pub type BlockSubscription = (
⋮----
pub type PubsubProgramClientSubscription = PubsubClientSubscription<RpcResponse<RpcKeyedAccount>>;
pub type ProgramSubscription = (
⋮----
pub type PubsubAccountClientSubscription = PubsubClientSubscription<RpcResponse<UiAccount>>;
pub type AccountSubscription = (
⋮----
pub type PubsubVoteClientSubscription = PubsubClientSubscription<RpcVote>;
pub type VoteSubscription = (PubsubVoteClientSubscription, Receiver<RpcVote>);
pub type PubsubRootClientSubscription = PubsubClientSubscription<Slot>;
pub type RootSubscription = (PubsubRootClientSubscription, Receiver<Slot>);
/// A client for subscribing to messages from the RPC server.
///
⋮----
///
/// See the [module documentation][self].
⋮----
/// See the [module documentation][self].
pub struct PubsubClient {}
⋮----
pub struct PubsubClient {}
fn connect_with_retry<R: IntoClientRequest>(
⋮----
let client_request = request.into_client_request().map_err(Box::new)?;
⋮----
let result = connect(client_request.clone()).map(|(socket, _)| socket);
⋮----
if response.status() == StatusCode::TOO_MANY_REQUESTS && connection_retries > 0 {
⋮----
if let Some(retry_after) = response.headers().get(header::RETRY_AFTER) {
if let Ok(retry_after) = retry_after.to_str() {
⋮----
debug!(
⋮----
sleep(duration);
⋮----
return result.map_err(Box::new);
⋮----
impl PubsubClient {
/// Subscribe to account events.
    ///
⋮----
///
    /// Receives messages of type [`UiAccount`] when an account's lamports or data changes.
⋮----
/// Receives messages of type [`UiAccount`] when an account's lamports or data changes.
    pub fn account_subscribe<R: IntoClientRequest>(
⋮----
pub fn account_subscribe<R: IntoClientRequest>(
⋮----
let socket = connect_with_retry(client_request)?;
let (sender, receiver) = unbounded();
⋮----
let socket_clone = socket.clone();
⋮----
let exit_clone = exit.clone();
let body = json!({
⋮----
.to_string();
⋮----
t_cleanup: Some(t_cleanup),
⋮----
Ok((result, receiver))
⋮----
pub fn block_subscribe<R: IntoClientRequest>(
⋮----
pub fn logs_subscribe<R: IntoClientRequest>(
⋮----
pub fn program_subscribe<R: IntoClientRequest>(
⋮----
pub fn vote_subscribe<R: IntoClientRequest>(
⋮----
pub fn root_subscribe<R: IntoClientRequest>(
⋮----
pub fn signature_subscribe<R: IntoClientRequest>(
⋮----
pub fn slot_subscribe<R: IntoClientRequest>(
⋮----
pub fn slot_updates_subscribe<R: IntoClientRequest>(
⋮----
Ok(PubsubClientSubscription {
⋮----
fn cleanup_with_sender<T>(
⋮----
let handler = move |message| match sender.send(message) {
⋮----
info!("receive error: {err:?}");
⋮----
fn cleanup_with_handler<T, F>(
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
Ok(Some(message)) => handler(message),
⋮----
info!("websocket - exited receive loop");
⋮----
mod tests {

================
File: pubsub-client/Cargo.toml
================
[package]
name = "solana-pubsub-client"
description = "Solana Pubsub Client"
documentation = "https://docs.rs/solana-pubsub-client"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
crossbeam-channel = { workspace = true }
futures-util = { workspace = true }
http = { workspace = true }
log = { workspace = true }
semver = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account-decoder-client-types = { workspace = true }
solana-clock = { workspace = true }
solana-pubkey = { workspace = true }
solana-rpc-client-types = { workspace = true }
solana-signature = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tokio-stream = { workspace = true }
tokio-tungstenite = { workspace = true, features = ["rustls-tls-webpki-roots"] }
tungstenite = { workspace = true, features = ["rustls-tls-webpki-roots"] }
url = { workspace = true }

[dev-dependencies]
anyhow = { workspace = true }
solana-commitment-config = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }

================
File: quic-client/src/nonblocking/mod.rs
================
pub mod quic_client;

================
File: quic-client/src/nonblocking/quic_client.rs
================
pub struct QuicLazyInitializedEndpoint {
⋮----
pub enum QuicError {
⋮----
fn from(quic_error: QuicError) -> Self {
Self::Custom(format!("{quic_error:?}"))
⋮----
impl QuicLazyInitializedEndpoint {
pub fn new(
⋮----
fn create_endpoint(&self) -> Endpoint {
⋮----
endpoint.clone()
⋮----
.expect("QuicLazyInitializedEndpoint::create_endpoint bind_in_range")
⋮----
info!("Local endpoint is : {client_socket:?}");
⋮----
let mut crypto = tls_client_config_builder()
.with_client_auth_cert(
vec![self.client_certificate.certificate.clone()],
self.client_certificate.key.clone_key(),
⋮----
.expect("Failed to set QUIC client certificates");
⋮----
crypto.alpn_protocols = vec![ALPN_TPU_PROTOCOL_ID.to_vec()];
let mut config = ClientConfig::new(Arc::new(QuicClientConfig::try_from(crypto).unwrap()));
⋮----
let timeout = IdleTimeout::try_from(QUIC_MAX_TIMEOUT).unwrap();
transport_config.max_idle_timeout(Some(timeout));
transport_config.keep_alive_interval(Some(QUIC_KEEP_ALIVE));
transport_config.send_fairness(QUIC_SEND_FAIRNESS);
config.transport_config(Arc::new(transport_config));
endpoint.set_default_client_config(config);
⋮----
async fn get_endpoint(&self) -> Arc<Endpoint> {
⋮----
.get_or_init(|| async { Arc::new(self.create_endpoint()) })
⋮----
.clone()
⋮----
impl Default for QuicLazyInitializedEndpoint {
fn default() -> Self {
let (cert, priv_key) = new_dummy_x509_certificate(&Keypair::new());
⋮----
struct QuicNewConnection {
⋮----
impl QuicNewConnection {
async fn make_connection(
⋮----
let endpoint = endpoint.get_endpoint().await;
let server_name = socket_addr_to_quic_server_name(addr);
let connecting = endpoint.connect(addr, &server_name)?;
stats.total_connections.fetch_add(1, Ordering::Relaxed);
if let Ok(connecting_result) = timeout(QUIC_CONNECTION_HANDSHAKE_TIMEOUT, connecting).await
⋮----
if connecting_result.is_err() {
stats.connection_errors.fetch_add(1, Ordering::Relaxed);
⋮----
make_connection_measure.stop();
⋮----
.fetch_add(make_connection_measure.as_ms(), Ordering::Relaxed);
⋮----
Ok(Self {
⋮----
Err(ConnectionError::TimedOut.into())
⋮----
fn create_endpoint(config: EndpointConfig, client_socket: UdpSocket) -> Endpoint {
⋮----
.expect("QuicNewConnection::create_endpoint quinn::Endpoint::new")
⋮----
async fn make_connection_0rtt(
⋮----
let connecting = self.endpoint.connect(addr, &server_name)?;
⋮----
let connection = match connecting.into_0rtt() {
⋮----
if let Ok(zero_rtt) = timeout(QUIC_CONNECTION_HANDSHAKE_TIMEOUT, zero_rtt).await {
⋮----
stats.zero_rtt_accepts.fetch_add(1, Ordering::Relaxed);
⋮----
stats.zero_rtt_rejects.fetch_add(1, Ordering::Relaxed);
⋮----
return Err(ConnectionError::TimedOut.into());
⋮----
timeout(QUIC_CONNECTION_HANDSHAKE_TIMEOUT, connecting).await
⋮----
Ok(self.connection.clone())
⋮----
pub struct QuicClient {
⋮----
impl QuicClient {
pub async fn close(&self) {
let mut conn_guard = self.connection.lock().await;
if let Some(conn) = conn_guard.take() {
debug!(
⋮----
conn.connection.close(
CONNECTION_CLOSE_CODE_APPLICATION_CLOSE.into(),
⋮----
pub fn new(endpoint: Arc<QuicLazyInitializedEndpoint>, addr: SocketAddr) -> Self {
⋮----
async fn _send_buffer_using_conn(
⋮----
let mut send_stream = connection.open_uni().await?;
send_stream.write_all(data).await?;
Ok(())
⋮----
async fn _send_buffer(
⋮----
let maybe_conn = conn_guard.as_mut();
⋮----
if conn.connection.stable_id() == last_connection_id {
let conn = conn.make_connection_0rtt(self.addr, stats).await;
⋮----
info!(
⋮----
return Err(err);
⋮----
stats.connection_reuse.fetch_add(1, Ordering::Relaxed);
conn.connection.clone()
⋮----
self.endpoint.clone(),
⋮----
*conn_guard = Some(conn.clone());
⋮----
let new_stats = connection.stats();
⋮----
.update_stat(
⋮----
.update_stat(&self.stats.data_blocked, new_stats.frame_tx.data_blocked);
⋮----
.update_stat(&self.stats.acks, new_stats.frame_tx.acks);
if data.is_empty() {
return Ok(connection);
⋮----
last_connection_id = connection.stable_id();
measure_prepare_connection.stop();
⋮----
measure_send_packet.stop();
stats.successful_packets.fetch_add(1, Ordering::Relaxed);
⋮----
.fetch_add(measure_send_packet.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(measure_prepare_connection.as_us(), Ordering::Relaxed);
trace!(
⋮----
last_error = Some(err);
⋮----
Err(last_error.expect("QuicClient::_send_buffer last_error.expect"))
⋮----
pub async fn send_buffer<T>(
⋮----
self._send_buffer(data.as_ref(), stats, connection_stats)
⋮----
.map_err(Into::<ClientErrorKind>::into)?;
⋮----
pub async fn send_batch<T>(
⋮----
if buffers.is_empty() {
return Ok(());
⋮----
._send_buffer(buffers[0].as_ref(), stats, connection_stats)
⋮----
for data in buffers[1..buffers.len()].iter() {
Self::_send_buffer_using_conn(data.as_ref(), &connection).await?;
⋮----
pub fn server_addr(&self) -> &SocketAddr {
⋮----
pub fn stats(&self) -> Arc<ClientStats> {
self.stats.clone()
⋮----
pub struct QuicClientConnection {
⋮----
impl QuicClientConnection {
pub fn base_stats(&self) -> Arc<ClientStats> {
self.client.stats()
⋮----
pub fn connection_stats(&self) -> Arc<ConnectionCacheStats> {
self.connection_stats.clone()
⋮----
pub fn new_with_client(
⋮----
impl ClientConnection for QuicClientConnection {
fn server_addr(&self) -> &SocketAddr {
self.client.server_addr()
⋮----
async fn send_data_batch(&self, buffers: &[Vec<u8>]) -> TransportResult<()> {
⋮----
let len = buffers.len();
⋮----
.send_batch(buffers, &stats, self.connection_stats.clone())
⋮----
.add_client_stats(&stats, len, res.is_ok());
⋮----
async fn send_data(&self, data: &[u8]) -> TransportResult<()> {
⋮----
let num_packets = if data.is_empty() { 0 } else { 1 };
⋮----
.send_buffer(data, &stats, self.connection_stats.clone())
.map_ok(|v| {
⋮----
.add_client_stats(&stats, num_packets, true);
⋮----
.map_err(|e| {
warn!(
⋮----
datapoint_warn!("send-wire-async", ("failure", 1, i64),);
⋮----
.add_client_stats(&stats, num_packets, false);
e.into()

================
File: quic-client/src/lib.rs
================
pub mod nonblocking;
pub mod quic_client;
⋮----
extern crate solana_metrics;
⋮----
pub struct QuicPool {
⋮----
impl ConnectionPool for QuicPool {
type BaseClientConnection = Quic;
type NewConnectionConfig = QuicConfig;
fn add_connection(&mut self, config: &Self::NewConnectionConfig, addr: &SocketAddr) -> usize {
let connection = self.create_pool_entry(config, addr);
let idx = self.connections.len();
self.connections.push(connection);
⋮----
fn num_connections(&self) -> usize {
self.connections.len()
⋮----
fn get(&self, index: usize) -> Result<Arc<Self::BaseClientConnection>, ConnectionPoolError> {
⋮----
.get(index)
.cloned()
.ok_or(ConnectionPoolError::IndexOutOfRange)
⋮----
fn create_pool_entry(
⋮----
Arc::new(Quic(Arc::new(QuicClient::new(
self.endpoint.clone(),
⋮----
impl Drop for QuicPool {
fn drop(&mut self) {
debug!(
⋮----
for connection in self.connections.drain(..) {
close_quic_connection(connection.0.clone());
⋮----
pub struct QuicConfig {
⋮----
impl Clone for QuicConfig {
fn clone(&self) -> Self {
let cert_guard = self.client_certificate.read().unwrap();
⋮----
client_certificate: RwLock::new(cert_guard.clone()),
maybe_staked_nodes: self.maybe_staked_nodes.clone(),
⋮----
client_endpoint: self.client_endpoint.clone(),
⋮----
impl NewConnectionConfig for QuicConfig {
fn new() -> Result<Self, ClientError> {
let (cert, priv_key) = new_dummy_x509_certificate(&Keypair::new());
Ok(Self {
⋮----
impl QuicConfig {
fn create_endpoint(&self) -> QuicLazyInitializedEndpoint {
⋮----
QuicLazyInitializedEndpoint::new(cert_guard.clone(), self.client_endpoint.as_ref().cloned())
⋮----
pub fn update_client_certificate(&mut self, keypair: &Keypair, _ipaddr: IpAddr) {
let (cert, priv_key) = new_dummy_x509_certificate(keypair);
let mut cert_guard = self.client_certificate.write().unwrap();
⋮----
pub fn update_keypair(&self, keypair: &Keypair) {
⋮----
pub fn set_staked_nodes(
⋮----
self.maybe_staked_nodes = Some(staked_nodes.clone());
self.maybe_client_pubkey = Some(*client_pubkey);
⋮----
pub fn update_client_endpoint(&mut self, client_socket: UdpSocket) {
let runtime = get_runtime();
let _guard = runtime.enter();
⋮----
self.client_endpoint = Some(
⋮----
.expect("QuicNewConnection::create_endpoint quinn::Endpoint::new"),
⋮----
pub struct Quic(Arc<QuicClient>);
impl BaseClientConnection for Quic {
type BlockingClientConnection = BlockingQuicClientConnection;
type NonblockingClientConnection = NonblockingQuicClientConnection;
fn new_blocking_connection(
⋮----
self.0.clone(),
⋮----
fn new_nonblocking_connection(
⋮----
pub struct QuicConnectionManager {
⋮----
impl ConnectionManager for QuicConnectionManager {
type ConnectionPool = QuicPool;
⋮----
fn new_connection_pool(&self) -> Self::ConnectionPool {
⋮----
endpoint: Arc::new(self.connection_config.create_endpoint()),
⋮----
fn new_connection_config(&self) -> QuicConfig {
self.connection_config.clone()
⋮----
fn update_key(&self, key: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
self.connection_config.update_keypair(key);
Ok(())
⋮----
impl QuicConnectionManager {
pub fn new_with_connection_config(connection_config: QuicConfig) -> Self {
⋮----
pub type QuicConnectionCache = ConnectionCache<QuicPool, QuicConnectionManager, QuicConfig>;
pub fn new_quic_connection_cache(
⋮----
config.update_client_certificate(keypair, ipaddr);
config.set_staked_nodes(staked_nodes, &keypair.pubkey());

================
File: quic-client/src/quic_client.rs
================
struct AsyncTaskSemaphore {
⋮----
impl AsyncTaskSemaphore {
pub fn new(permits: u64) -> Self {
⋮----
pub fn acquire(&self) -> MutexGuard<'_, u64> {
let mut count = self.counter.lock().unwrap();
⋮----
count = self.cond_var.wait(count).unwrap();
⋮----
/// Acquire the lock and decrement the usage count
    pub fn release(&self) {
⋮----
pub fn release(&self) {
⋮----
self.cond_var.notify_one();
⋮----
.thread_name("solQuicClientRt")
.enable_all()
.build()
.unwrap()
⋮----
pub fn get_runtime() -> &'static Runtime {
⋮----
async fn send_data_async(
⋮----
let result = timeout(SEND_DATA_TIMEOUT, connection.send_data(&buffer)).await;
ASYNC_TASK_SEMAPHORE.release();
handle_send_result(result, connection)
⋮----
async fn send_data_batch_async(
⋮----
let result = timeout(
u32::try_from(buffers.len())
.map(|size| SEND_DATA_TIMEOUT.saturating_mul(size))
.unwrap_or(Duration::MAX),
connection.send_data_batch(&buffers),
⋮----
fn handle_send_result(
⋮----
client_stats.send_timeout.fetch_add(1, Ordering::Relaxed);
let stats = connection.connection_stats();
stats.add_client_stats(&client_stats, 0, false);
info!("Timedout sending data {:?}", connection.server_addr());
Err(TransportError::Custom("Timedout sending data".to_string()))
⋮----
pub struct QuicClientConnection {
⋮----
impl QuicClientConnection {
pub fn new(
⋮----
pub fn new_with_client(
⋮----
impl ClientConnection for QuicClientConnection {
fn server_addr(&self) -> &SocketAddr {
self.inner.server_addr()
⋮----
fn send_data_batch(&self, buffers: &[Vec<u8>]) -> TransportResult<()> {
RUNTIME.block_on(self.inner.send_data_batch(buffers))?;
Ok(())
⋮----
fn send_data_async(&self, data: Arc<Vec<u8>>) -> TransportResult<()> {
let _lock = ASYNC_TASK_SEMAPHORE.acquire();
let inner = self.inner.clone();
let _handle = RUNTIME.spawn(send_data_async(inner, data));
⋮----
fn send_data_batch_async(&self, buffers: Vec<Vec<u8>>) -> TransportResult<()> {
⋮----
let _handle = RUNTIME.spawn(send_data_batch_async(inner, buffers));
⋮----
fn send_data(&self, buffer: &[u8]) -> TransportResult<()> {
RUNTIME.block_on(self.inner.send_data(buffer))?;
⋮----
pub(crate) fn close_quic_connection(connection: Arc<QuicClient>) {
trace!("Closing QUIC connection to {}", connection.server_addr());
RUNTIME.block_on(connection.close());

================
File: quic-client/tests/quic_client.rs
================
mod tests {
⋮----
fn check_packets(
⋮----
let mut all_packets = vec![];
⋮----
while now.elapsed().as_secs() < 10 {
if let Ok(packets) = receiver.recv_timeout(Duration::from_secs(1)) {
total_packets = total_packets.saturating_add(packets.len());
all_packets.push(packets)
⋮----
assert_eq!(p.meta().size, num_bytes);
⋮----
assert!(total_packets > 0);
⋮----
fn server_args() -> (UdpSocket, CancellationToken, Keypair) {
let port_range = localhost_port_range_for_tests();
⋮----
bind_to(IpAddr::V4(Ipv4Addr::LOCALHOST), port_range.0).expect("should bind"),
⋮----
fn test_quic_client_multiple_writes() {
⋮----
let (sender, receiver) = unbounded();
⋮----
let (s, cancel, keypair) = server_args();
⋮----
vec![s.try_clone().unwrap()],
⋮----
cancel.clone(),
⋮----
.unwrap();
let addr = s.local_addr().unwrap().ip();
let port = s.local_addr().unwrap().port();
⋮----
let packets = vec![vec![0u8; PACKET_DATA_SIZE]; num_expected_packets];
assert!(client.send_data_batch_async(packets).is_ok());
check_packets(receiver, num_bytes, num_expected_packets);
cancel.cancel();
t.join().unwrap();
⋮----
async fn nonblocking_check_packets(
⋮----
if let Ok(packets) = receiver.try_recv() {
⋮----
sleep(Duration::from_secs(1)).await;
⋮----
async fn test_nonblocking_quic_client_multiple_writes() {
⋮----
let _ = client.send_data(&packet).await;
⋮----
nonblocking_check_packets(receiver, num_bytes, num_expected_packets).await;
⋮----
t.await.unwrap();
⋮----
fn test_quic_bi_direction() {
⋮----
let (request_recv_socket, request_recv_cancel, keypair) = server_args();
⋮----
[request_recv_socket.try_clone().unwrap()],
⋮----
staked_nodes.clone(),
⋮----
request_recv_cancel.clone(),
⋮----
drop(request_recv_endpoints);
let (response_recv_socket, response_recv_cancel, keypair2) = server_args();
let (sender2, receiver2) = unbounded();
let addr = response_recv_socket.local_addr().unwrap().ip();
let port = response_recv_socket.local_addr().unwrap().port();
⋮----
response_recv_cancel.clone(),
⋮----
let addr = request_recv_socket.local_addr().unwrap().ip();
let port = request_recv_socket.local_addr().unwrap().port();
⋮----
let (cert, priv_key) = new_dummy_x509_certificate(&Keypair::new());
⋮----
.pop()
.expect("at least one endpoint");
drop(response_recv_endpoints);
⋮----
QuicLazyInitializedEndpoint::new(client_certificate, Some(response_recv_endpoint));
⋮----
assert!(request_sender.send_data_batch_async(packets).is_ok());
⋮----
info!("Received requests!");
⋮----
assert!(response_sender.send_data_batch_async(packets).is_ok());
check_packets(receiver2, num_bytes, num_expected_packets);
info!("Received responses!");
drop(request_sender);
drop(response_sender);
request_recv_cancel.cancel();
request_recv_thread.join().unwrap();
info!("Request receiver exited!");
response_recv_cancel.cancel();
response_recv_thread.join().unwrap();
info!("Response receiver exited!");
⋮----
async fn test_connection_close() {
⋮----
.send_buffer(&packet, &client_stats, connection_cache_stats.clone())
⋮----
client.close().await;

================
File: quic-client/Cargo.toml
================
[package]
name = "solana-quic-client"
description = "Solana Quic Client"
documentation = "https://docs.rs/solana-quic-client"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
async-lock = { workspace = true }
async-trait = { workspace = true }
futures = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
quinn = { workspace = true }
quinn-proto = { workspace = true }
rustls = { workspace = true }
solana-connection-cache = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-metrics = { workspace = true }
solana-net-utils = { workspace = true }
solana-pubkey = { workspace = true, default-features = false }
solana-quic-definitions = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-tls-utils = { workspace = true }
solana-transaction-error = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[dev-dependencies]
agave-logger = { workspace = true }
crossbeam-channel = { workspace = true }
solana-net-utils = { workspace = true, features = ["dev-context-only-utils"] }
solana-packet = { workspace = true }
solana-perf = { workspace = true }
solana-quic-client = { path = ".", features = ["agave-unstable-api"] }
solana-streamer = { workspace = true, features = ["dev-context-only-utils"] }
tokio-util = { workspace = true }

================
File: rayon-threadlimit/src/lib.rs
================
.ok()
.and_then(|num_threads| {
warn!(
⋮----
num_threads.parse().ok()
⋮----
.unwrap_or_else(|| num_cpus::get() / 2)
.max(1)
⋮----
pub fn get_thread_count() -> usize {
⋮----
pub fn get_max_thread_count() -> usize {
⋮----
get_thread_count().saturating_mul(2)

================
File: rayon-threadlimit/.gitignore
================
/target/
/farf/

================
File: rayon-threadlimit/Cargo.toml
================
[package]
name = "solana-rayon-threadlimit"
description = "solana-rayon-threadlimit"
documentation = "https://docs.rs/solana-rayon-threadlimit"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
log = { workspace = true }
num_cpus = { workspace = true }

[dev-dependencies]
solana-rayon-threadlimit = { path = ".", features = ["agave-unstable-api"] }

================
File: rbpf-cli/src/main.rs
================
fn main() {
println!(

================
File: rbpf-cli/Cargo.toml
================
[package]
name = "rbpf-cli"
description = "Decommissioned CLI to test and analyze SBF programs"
keywords = ["SBF", "interpreter", "JIT"]
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]

================
File: README.md
================
<p align="center">
  <a href="https://anza.xyz">
    <img alt="Anza" src="https://i.postimg.cc/VkKTnMM9/agave-logo-talc-1.png" width="250" />
  </a>
</p>

[![Build status](https://badge.buildkite.com/3a7c88c0f777e1a0fddacc190823565271ae4c251ef78d83a8.svg)](https://buildkite.com/jito/jito-solana)

# About

This repository contains Jito's fork of the Solana validator.

We recommend checking out our [Gitbook](https://jito-foundation.gitbook.io/mev/jito-solana/building-the-software) for
more detailed instructions on building and running Jito-Solana.

---

## **1. Install rustc, cargo and rustfmt.**

```bash
$ curl https://sh.rustup.rs -sSf | sh
$ source $HOME/.cargo/env
$ rustup component add rustfmt
```

The `rust-toolchain.toml` file pins a specific rust version and ensures that
cargo commands run with that version. Note that cargo will automatically install
the correct version if it is not already installed.

On Linux systems you may need to install libssl-dev, pkg-config, zlib1g-dev, protobuf etc.

On Ubuntu:

```bash
$ sudo apt-get update
$ sudo apt-get install libssl-dev libudev-dev pkg-config zlib1g-dev llvm clang cmake make libprotobuf-dev protobuf-compiler libclang-dev
```

On Fedora:

```bash
$ sudo dnf install openssl-devel systemd-devel pkg-config zlib-devel llvm clang cmake make protobuf-devel protobuf-compiler perl-core libclang-dev
```

## **2. Download the source code.**

```bash
$ git clone https://github.com/jito-foundation/jito-solana.git
$ cd jito-solana
```

## **3. Build.**

```bash
$ ./cargo build
```

> [!NOTE]
> Note that this builds a debug version that is **not suitable for running a testnet or mainnet validator**. Please read [`docs/src/cli/install.md`](docs/src/cli/install.md#build-from-source) for instructions to build a release version for test and production uses.

# Testing

**Run the test suite:**

```bash
$ ./cargo test
```

### Starting a local testnet

Start your own testnet locally, instructions are in the [online docs](https://docs.anza.xyz/clusters/benchmark).

### Accessing the remote development cluster

* `devnet` - stable public cluster for development accessible via
devnet.solana.com. Runs 24/7. Learn more about the [public clusters](https://docs.anza.xyz/clusters)

# Benchmarking

First, install the nightly build of rustc. `cargo bench` requires the use of the
unstable features only available in the nightly build.

```bash
$ rustup install nightly
```

Run the benchmarks:

```bash
$ cargo +nightly bench
```

# Release Process

The release process for this project is described [here](RELEASE.md).

# Code coverage

To generate code coverage statistics:

```bash
$ scripts/coverage.sh
$ open target/cov/lcov-local/index.html
```

Why coverage? While most see coverage as a code quality metric, we see it primarily as a developer
productivity metric. When a developer makes a change to the codebase, presumably it's a *solution* to
some problem. Our unit-test suite is how we encode the set of *problems* the codebase solves. Running
the test suite should indicate that your change didn't *infringe* on anyone else's solutions. Adding a
test *protects* your solution from future changes. Say you don't understand why a line of code exists,
try deleting it and running the unit-tests. The nearest test failure should tell you what problem
was solved by that code. If no test fails, go ahead and submit a Pull Request that asks, "what
problem is solved by this code?" On the other hand, if a test does fail and you can think of a
better way to solve the same problem, a Pull Request with your solution would most certainly be
welcome! Likewise, if rewriting a test can better communicate what code it's protecting, please
send us that patch!

================
File: RELEASE.md
================
# Agave Release process

## Branches and Tags

```
========================= master branch (edge channel) =======================>
         \                      \                     \
          \___v0.7.0 tag         \                     \
           \                      \         v0.9.0 tag__\
            \          v0.8.0 tag__\                     \
 v0.7.1 tag__\                      \                 v0.9 branch (beta channel)
              \___v0.7.2 tag         \___v0.8.1 tag
               \                      \
                \                      \
           v0.7 branch         v0.8 branch (stable channel)

```

### master branch

All new development occurs on the `master` branch.

Bug fixes that affect a `vX.Y` branch are first made on `master`. This is to
allow a fix some soak time on `master` before it is applied to one or more
stabilization branches.

Merging to `master` first also helps ensure that fixes applied to one release
are present for future releases.  (Sometimes the joy of landing a critical
release blocker in a branch causes you to forget to propagate back to
`master`!)"

Once the bug fix lands on `master` it is cherry-picked into the `vX.Y` branch
and potentially the `vX.Y-1` branch. The exception to this rule is when a bug
fix for `vX.Y` doesn't apply to `master` or `vX.Y-1`.

Immediately after a new stabilization branch is forged, the `Cargo.toml` minor
version (*Y*) in the `master` branch is incremented by the release engineer.
Incrementing the major version of the `master` branch is outside the scope of
this document.

### v*X.Y* stabilization branches

These are stabilization branches. They are created from the `master` branch approximately
every 13 weeks.

### v*X.Y.Z* release tag

The release tags are created as desired by the owner of the given stabilization
branch, and cause that *X.Y.Z* release to be shipped to https://crates.io

Immediately after a new v*X.Y.Z* branch tag has been created, the `Cargo.toml`
patch version number (*Z*) of the stabilization branch is incremented by the
release engineer.

## Channels

Channels are used by end-users (humans and bots) to consume the branches
described in the previous section, so they may automatically update to the most
recent version matching their desired stability.

There are three release channels that map to branches as follows:

* edge - tracks the `master` branch, least stable.
* beta - tracks the largest (and latest) `vX.Y` stabilization branch, more stable.
* stable - tracks the second largest `vX.Y` stabilization branch, most stable.

## Steps to Create a Branch

### Major release branch

1. If the new branch will be the first branch of a new major release check that
   all eligible deprecated symbols have been removed. Our policy is to deprecate
   for at least one full minor version before removal.

### Create the new branch

1. Check out the latest commit on `master` branch:
    ```
    git fetch --all
    git checkout upstream/master
    ```
1. Determine the new branch name. The name should be "v" + the first 2 version fields
   from Cargo.toml. For example, a Cargo.toml with version = "0.9.0" implies
   the next branch name is "v0.9".
1. Create the new branch and push this branch to the `agave` repository:
    ```
    git checkout -b <branchname>
    git push -u origin <branchname>
    ```

Alternatively use the Github UI.

### Update master branch to the next release minor version

1. After the new branch has been created and pushed, update the Cargo.toml files on **master** to the next semantic
   version (e.g. 0.9.0 -> 0.10.0) with:
     ```
     $ scripts/increment-cargo-version.sh minor
     ```
1. Push all the changed Cargo.toml and Cargo.lock files to the `master` branch with something like:
    ```
    git co -b version_update
    git ls-files -m | xargs git add
    git commit -m 'Bump version to X.Y+1.0'
    git push -u origin version_update
    ```
1. Confirm that your freshly cut release branch is shown as `BETA_CHANNEL` and the previous release branch
   as `STABLE_CHANNEL`:
    ```
    ci/channel-info.sh
    ```

### Update the Changelog

Create a PR that makes the following updates to [CHANGELOG.md](https://github.com/anza-xyz/agave/blob/master/CHANGELOG.md) in master:
* Advance the channel links with the newly created branch becoming beta.
* Add a new section `X.Y.0-Unreleased` for the new master version.
* Remove the `Unreleased` annotation for the section that has now become beta.

### Miscellaneous Clean up

1. Pin the spl-token-cli version in the newly promoted stable branch by setting `splTokenCliVersion` in scripts/spl-token-cli-version.sh to the latest release that depends on the stable branch (usually this will be the latest spl-token-cli release).
1. Update [CHANGELOG.md](https://github.com/jito-foundation/jito-solana/blob/master/CHANGELOG.md) to remove the channel links on the new branch. Additionally, remove any wording about the new branch being unreleased.
1. Update [CODEOWNERS](https://github.com/jito-foundation/jito-solana/blob/master/.github/CODEOWNERS) to `* @anza-xyz/backport-reviewers` on the new branch.
1. Update [mergify.yml](https://github.com/jito-foundation/jito-solana/blob/master/.mergify.yml) to add backport actions for the new branch and remove actions for the obsolete branch.
1. Adjust the [Github backport labels](https://github.com/anza-xyz/agave/labels) to add the new branch label and remove the label for the obsolete branch.
1. Announce on Discord #development that the release branch exists so people know to use the new backport labels.

## Steps to Create a Release

### Create the Release Tag on GitHub

1. Go to [GitHub Releases](https://github.com/jito-foundation/jito-solana/releases) for tagging a release.
1. Click "Draft new release". The release tag must exactly match the `version`
   field in `/Cargo.toml` prefixed by `v`.
    1. If the Cargo.toml version field is **0.12.3**, then the release tag must be **v0.12.3**
1. Make sure the Target Branch field matches the branch you want to make a release on.
    1. If you want to release v0.12.0, the target branch must be v0.12
1. Fill the release notes.
    1. If this is the first release on the branch (e.g. v0.13.**0**), paste in [this
       template](https://raw.githubusercontent.com/jito-foundation/jito-solana/master/.github/RELEASE_TEMPLATE.md).
       Engineering Lead can provide summary contents for release notes if needed.
    1. If this is a patch release, review all the commits since the previous release on this branch and add details as
       needed.
1. Click "Save Draft", then confirm the release notes look good and the tag name and branch are correct.
1. Ensure all desired commits (usually backports) are landed on the branch by now.
1. Ensure the release is marked **"This is a pre-release"**. This flag will need to be removed manually after confirming
   the Linux binary artifacts appear at a later step.
1. Go back into edit the release and click "Publish release" while being marked as a pre-release.
1. Confirm there is new git tag with intended version number at the intended revision after running `git fetch` locally.

### Update release branch with the next patch version

[This action](https://github.com/jito-foundation/jito-solana/blob/master/.github/workflows/increment-cargo-version-on-release.yml)
ensures that publishing a release will trigger the creation of a PR to update the Cargo.toml files on **release branch**
to the next semantic version (e.g. 0.9.0 -> 0.9.1). Ensure that the created PR makes it through CI and gets submitted.

Note: As of 2024-03-26 the above action is failing so version bumps are done manually. The version bump script is
incorrectly updating hashbrown and proc-macro2 versions which should be reverted.

### Prepare for the next release

1. Go to [GitHub Releases](https://github.com/jito-foundation/jito-solana/releases) and create a new draft release
   for `X.Y.Z+1` with empty release notes. This allows people to incrementally add new release notes until it's time for
   the next release
    1. Also, point the branch field to the same branch and mark the release as **"This is a pre-release"**.

### Verify release automation success

Go to [Agave Releases](https://github.com/jito-foundation/jito-solana/releases) and click on the latest release that you
just published.
Verify that all of the build artifacts are present (15 assets), then uncheck **"This is a pre-release"** for the
release.

Build artifacts can take up to 60 minutes after creating the tag before
appearing. To check for progress:

* The `agave-secondary` Buildkite pipeline handles creating the Linux and macOS release artifacts and updated crates.
  Look for a job under the tag name of the release: https://buildkite.com/jito-foundation/jito-solana-secondary.
* The Windows release artifacts are produced by GitHub Actions. Look for a job under the tag name of the
  release: https://github.com/jito-foundation/jito-solana/actions.

[Crates.io agave-validator](https://crates.io/crates/agave-validator) should have an updated agave-validator version.
This can take 2-3 hours, and sometimes fails in the `agave-secondary` job.
If this happens and the error is non-fatal, click "Retry" on the "publish crate" job

### Update software on testnet.solana.com

See the documentation at https://github.com/solana-labs/cluster-ops/. devnet.solana.com and mainnet-beta.solana.com run
stable releases that have been tested on testnet. Do not update devnet or mainnet-beta with a beta release.

================
File: remote-wallet/src/ledger_error.rs
================
pub enum LedgerError {

================
File: remote-wallet/src/ledger.rs
================
static CHECK_MARK: Emoji = Emoji("✅ ", "");
⋮----
/// Ledger vendor ID
const LEDGER_VID: u16 = 0x2c97;
/// Ledger product IDs
const LEDGER_NANO_S_PIDS: [u16; 33] = [
⋮----
mod commands {
⋮----
enum ConfigurationVersion {
⋮----
pub enum PubkeyDisplayMode {
⋮----
pub struct LedgerSettings {
⋮----
/// Ledger Wallet device
pub struct LedgerWallet {
⋮----
pub struct LedgerWallet {
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
write!(f, "HidDevice")
⋮----
impl LedgerWallet {
pub fn new(device: hidapi::HidDevice) -> Self {
⋮----
// Transport Protocol:
//		* Communication Channel Id		(2 bytes big endian )
//		* Command Tag				(1 byte)
//		* Packet Sequence ID			(2 bytes big endian)
//		* Payload				(Optional)
//
// Payload
//		* APDU Total Length			(2 bytes big endian)
//		* APDU_CLA				(1 byte)
//		* APDU_INS				(1 byte)
//		* APDU_P1				(1 byte)
//		* APDU_P2				(1 byte)
//		* APDU_LENGTH 	        (1 byte (2 bytes DEPRECATED))
//		* APDU_Payload				(Variable)
⋮----
fn write(
⋮----
let data_len = data.len();
⋮----
let size = min(64 - header, data_len - offset);
⋮----
chunk[0..5].copy_from_slice(&[
⋮----
let data_len = data.len() + 6;
chunk[5..13].copy_from_slice(&[
⋮----
(data.len() >> 8) as u8,
data.len() as u8,
⋮----
let data_len = data.len() + 5;
chunk[5..12].copy_from_slice(&[
⋮----
chunk[header..header + size].copy_from_slice(&data[offset..offset + size]);
⋮----
trace!("Ledger write {:?}", &hid_chunk[..]);
let n = self.device.write(&hid_chunk[..])?;
⋮----
return Err(RemoteWalletError::Protocol("Write data size mismatch"));
⋮----
return Err(RemoteWalletError::Protocol(
⋮----
Ok(())
⋮----
//		* APDU_LENGTH				(1 byte)
⋮----
fn read(&self) -> Result<Vec<u8>, RemoteWalletError> {
⋮----
// terminate the loop if `sequence_number` reaches its max_value and report error
⋮----
let chunk_size = self.device.read(&mut chunk)?;
trace!("Ledger read {:?}", &chunk[..]);
⋮----
return Err(RemoteWalletError::Protocol("Unexpected chunk header"));
⋮----
// Read message size and status word.
⋮----
message.extend_from_slice(&chunk[offset..chunk_size]);
message.truncate(message_size);
if message.len() == message_size {
⋮----
if message.len() < 2 {
return Err(RemoteWalletError::Protocol("No status word"));
⋮----
((message[message.len() - 2] as usize) << 8) | (message[message.len() - 1] as usize);
trace!("Read status {status:x}");
⋮----
let new_len = message.len() - 2;
message.truncate(new_len);
Ok(message)
⋮----
fn _send_apdu(
⋮----
self.write(command, p1, p2, data, outdated_app)?;
if p1 == P1_CONFIRM && is_last_part(p2) {
println!(
⋮----
let result = self.read()?;
println!("{CHECK_MARK}Approved");
Ok(result)
⋮----
self.read()
⋮----
fn send_apdu(
⋮----
self._send_apdu(command, p1, p2, data, self.outdated_app())
⋮----
fn get_firmware_version(&self) -> Result<FirmwareVersion, RemoteWalletError> {
self.get_configuration_vector().map(|config| match config {
⋮----
FirmwareVersion::new(config[2].into(), config[3].into(), config[4].into())
⋮----
FirmwareVersion::new(config[1].into(), config[2].into(), config[3].into())
⋮----
pub fn get_settings(&self) -> Result<LedgerSettings, RemoteWalletError> {
⋮----
fn get_configuration_vector(&self) -> Result<ConfigurationVersion, RemoteWalletError> {
if let Ok(config) = self._send_apdu(commands::GET_APP_CONFIGURATION, 0, 0, &[], false) {
if config.len() != 5 {
return Err(RemoteWalletError::Protocol("Version packet size mismatch"));
⋮----
Ok(ConfigurationVersion::Current(config))
⋮----
self._send_apdu(commands::DEPRECATED_GET_APP_CONFIGURATION, 0, 0, &[], true)?;
if config.len() != 4 {
⋮----
Ok(ConfigurationVersion::Deprecated(config))
⋮----
fn outdated_app(&self) -> bool {
⋮----
fn parse_status(status: usize) -> Result<(), RemoteWalletError> {
⋮----
Err(err.into())
⋮----
Err(RemoteWalletError::Protocol("Unknown error"))
⋮----
fn name(&self) -> &str {
⋮----
fn read_device(
⋮----
.manufacturer_string()
.and_then(|s| Manufacturer::try_from(s).ok())
.unwrap_or_default();
⋮----
.product_string()
.unwrap_or("Unknown")
.to_lowercase()
.replace(' ', "-");
let serial = dev_info.serial_number().unwrap_or("Unknown").to_string();
let host_device_path = dev_info.path().to_string_lossy().to_string();
let version = self.get_firmware_version()?;
⋮----
let pubkey_result = self.get_pubkey(&DerivationPath::default(), false);
⋮----
Err(err) => (Pubkey::default(), Some(err)),
⋮----
Ok(RemoteWalletInfo {
⋮----
fn get_pubkey(
⋮----
let derivation_path = extend_and_serialize(derivation_path);
let key = self.send_apdu(
if self.outdated_app() {
⋮----
Pubkey::try_from(key).map_err(|_| RemoteWalletError::Protocol("Key packet size mismatch"))
⋮----
fn sign_message(
⋮----
if !data.is_empty() && data[0] == 0xff {
return self.sign_offchain_message(derivation_path, data);
⋮----
let mut payload = if self.outdated_app() {
extend_and_serialize(derivation_path)
⋮----
extend_and_serialize_multiple(&[derivation_path])
⋮----
if data.len() > u16::MAX as usize {
return Err(RemoteWalletError::InvalidInput(
"Message to sign is too long".to_string(),
⋮----
let max_size = MAX_CHUNK_SIZE - payload.len();
let empty = vec![];
let (data, remaining_data) = if data.len() > max_size {
data.split_at(max_size)
⋮----
(data, empty.as_ref())
⋮----
for byte in (data.len() as u16).to_be_bytes().iter() {
payload.push(*byte);
⋮----
payload.extend_from_slice(data);
trace!("Serialized payload length {:?}", payload.len());
let p2 = if remaining_data.is_empty() {
⋮----
let mut result = self.send_apdu(
⋮----
if !remaining_data.is_empty() {
⋮----
.chunks(MAX_CHUNK_SIZE)
.map(|data| {
⋮----
(data.len() as u16).to_be_bytes().to_vec()
⋮----
vec![]
⋮----
.collect();
chunks.last_mut().unwrap().0 &= !P2_MORE;
⋮----
result = self.send_apdu(
⋮----
.map_err(|_| RemoteWalletError::Protocol("Signature packet size mismatch"))
⋮----
fn sign_offchain_message(
⋮----
if message.len()
⋮----
"Off-chain message to sign is too long".to_string(),
⋮----
let mut data = extend_and_serialize_multiple(&[derivation_path]);
data.extend_from_slice(message);
⋮----
let mut payload = data.as_slice();
while payload.len() > MAX_CHUNK_SIZE {
⋮----
self.send_apdu(commands::SIGN_OFFCHAIN_MESSAGE, p1, p2 | P2_MORE, chunk)?;
⋮----
let result = self.send_apdu(commands::SIGN_OFFCHAIN_MESSAGE, p1, p2, payload)?;
⋮----
pub fn is_valid_ledger(vendor_id: u16, product_id: u16) -> bool {
⋮----
vendor_id == LEDGER_VID && product_ids.iter().any(|pids| pids.contains(&product_id))
⋮----
fn extend_and_serialize(derivation_path: &DerivationPath) -> Vec<u8> {
let byte = if derivation_path.change().is_some() {
⋮----
} else if derivation_path.account().is_some() {
⋮----
let mut concat_derivation = vec![byte];
for index in derivation_path.path() {
concat_derivation.extend_from_slice(&index.to_bits().to_be_bytes());
⋮----
fn extend_and_serialize_multiple(derivation_paths: &[&DerivationPath]) -> Vec<u8> {
let mut concat_derivation = vec![derivation_paths.len() as u8];
⋮----
concat_derivation.append(&mut extend_and_serialize(derivation_path));
⋮----
pub fn get_ledger_from_info(
⋮----
let devices = wallet_manager.list_devices();
⋮----
.iter()
.filter(|&device_info| device_info.matches(&info));
⋮----
.clone()
.all(|device_info| device_info.error.is_some())
⋮----
let first_device = matches.next();
⋮----
return Err(device.error.clone().unwrap());
⋮----
.filter(|&device_info| device_info.error.is_none())
.map(|device_info| {
let query_item = format!("{} ({})", device_info.get_pretty_path(), device_info.model,);
(device_info.host_device_path.clone(), query_item)
⋮----
if matches.is_empty() {
return Err(RemoteWalletError::NoDeviceFound);
⋮----
matches.sort_by(|a, b| a.1.cmp(&b.1));
let (host_device_paths, items): (Vec<String>, Vec<String>) = matches.into_iter().unzip();
let wallet_host_device_path = if host_device_paths.len() > 1 {
⋮----
.with_prompt(format!(
⋮----
.default(0)
.items(&items[..])
.interact()
.unwrap();
⋮----
wallet_manager.get_ledger(wallet_host_device_path)
⋮----
fn is_last_part(p2: u8) -> bool {
⋮----
mod tests {
⋮----
fn test_is_last_part() {
assert!(is_last_part(0b00));
assert!(is_last_part(0b01));
assert!(is_last_part(0b101));
assert!(is_last_part(0b1001));
assert!(is_last_part(0b1101));
assert!(!is_last_part(0b10));
assert!(!is_last_part(0b11));
assert!(!is_last_part(0b110));
assert!(!is_last_part(0b111));
assert!(!is_last_part(0b1010));
⋮----
assert!(is_last_part(p2));
⋮----
assert!(!is_last_part(p2));
assert!(is_last_part(p2 & !P2_MORE));
⋮----
fn test_parse_status() {
LedgerWallet::parse_status(APDU_SUCCESS_CODE).expect("unexpected result");
if let RemoteWalletError::LedgerError(err) = LedgerWallet::parse_status(0x6985).unwrap_err()
⋮----
assert_eq!(err, LedgerError::UserCancel);
⋮----
if let RemoteWalletError::Protocol(err) = LedgerWallet::parse_status(0x6fff).unwrap_err() {
assert_eq!(err, "Unknown error");

================
File: remote-wallet/src/lib.rs
================
pub mod ledger;
pub mod ledger_error;
pub mod locator;
pub mod remote_keypair;
pub mod remote_wallet;

================
File: remote-wallet/src/locator.rs
================
pub enum Manufacturer {
⋮----
pub struct ManufacturerError;
⋮----
fn from(_: Infallible) -> Self {
⋮----
impl FromStr for Manufacturer {
type Err = ManufacturerError;
fn from_str(s: &str) -> Result<Self, Self::Err> {
let s = s.to_ascii_lowercase();
match s.as_str() {
MANUFACTURER_LEDGER => Ok(Self::Ledger),
_ => Err(ManufacturerError),
⋮----
type Error = ManufacturerError;
fn try_from(s: &str) -> Result<Self, Self::Error> {
⋮----
fn as_ref(&self) -> &str {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
let s: &str = self.as_ref();
write!(f, "{s}")
⋮----
pub enum LocatorError {
⋮----
pub struct Locator {
⋮----
let maybe_path = self.pubkey.map(|p| p.to_string());
let path = maybe_path.as_deref().unwrap_or("/");
⋮----
.try_scheme(Some("usb"))
.unwrap()
.try_authority(Some(self.manufacturer.as_ref()))
⋮----
.try_path(path)
.unwrap();
let uri = builder.build().unwrap();
write!(f, "{uri}")
⋮----
impl Locator {
pub fn new_from_path<P: AsRef<str>>(path: P) -> Result<Self, LocatorError> {
let path = path.as_ref();
⋮----
pub fn new_from_uri(uri: &URIReference<'_>) -> Result<Self, LocatorError> {
let scheme = uri.scheme().map(|s| s.as_str().to_ascii_lowercase());
let host = uri.host().map(|h| h.to_string());
⋮----
let path = uri.path().segments().first().and_then(|s| {
if !s.is_empty() {
Some(s.as_str())
⋮----
Self::new_from_parts(host.as_str(), path)
⋮----
(Some(_scheme), Some(_host)) => Err(LocatorError::UnimplementedScheme),
(None, Some(_host)) => Err(LocatorError::UnimplementedScheme),
(_, None) => Err(LocatorError::ManufacturerError(ManufacturerError)),
⋮----
pub fn new_from_parts<V, VE, P, PE>(
⋮----
let manufacturer = manufacturer.try_into().map_err(|e| e.into())?;
⋮----
Some(pubkey.try_into().map_err(|e| e.into())?)
⋮----
Ok(Self {
⋮----
mod tests {
⋮----
fn test_manufacturer() {
assert_eq!(MANUFACTURER_LEDGER.try_into(), Ok(Manufacturer::Ledger));
assert!(
⋮----
assert_eq!(Manufacturer::Ledger.as_ref(), MANUFACTURER_LEDGER);
⋮----
fn test_locator_new_from_parts() {
⋮----
let pubkey_str = pubkey.to_string();
⋮----
assert_matches!(
⋮----
pubkey: Some(pubkey),
⋮----
fn test_locator_new_from_uri() {
⋮----
// usb://ledger/{PUBKEY}?key=0/0
⋮----
.try_authority(Some(Manufacturer::Ledger.as_ref()))
⋮----
.try_path(pubkey_str.as_str())
⋮----
.try_query(Some("key=0/0"))
⋮----
assert_eq!(Locator::new_from_uri(&uri), Ok(expect));
// usb://ledger/{PUBKEY}
⋮----
// usb://ledger
⋮----
.try_path("")
⋮----
// usb://ledger/
⋮----
.try_path("/")
⋮----
// bad-scheme://ledger
⋮----
.try_scheme(Some("bad-scheme"))
⋮----
assert_eq!(
⋮----
// usb://bad-manufacturer
⋮----
.try_authority(Some("bad-manufacturer"))
⋮----
// usb://ledger/bad-pubkey
⋮----
.try_path("bad-pubkey")
⋮----
fn test_locator_new_from_path() {
⋮----
let path = format!("usb://ledger/{pubkey}?key=0/0");
Locator::new_from_path(path).unwrap();
// usb://ledger/{PUBKEY}?key=0'/0'
let path = format!("usb://ledger/{pubkey}?key=0'/0'");
⋮----
assert_eq!(Locator::new_from_path(path), Ok(expect));
let path = format!("usb://ledger/{pubkey}");

================
File: remote-wallet/src/remote_keypair.rs
================
pub struct RemoteKeypair {
⋮----
impl RemoteKeypair {
pub fn new(
⋮----
RemoteWalletType::Ledger(wallet) => wallet.get_pubkey(&derivation_path, confirm_key)?,
⋮----
Ok(Self {
⋮----
impl Signer for RemoteKeypair {
fn try_pubkey(&self) -> Result<Pubkey, SignerError> {
Ok(self.pubkey)
⋮----
fn try_sign_message(&self, message: &[u8]) -> Result<Signature, SignerError> {
⋮----
.sign_message(&self.derivation_path, message)
.map_err(|e| e.into()),
⋮----
fn is_interactive(&self) -> bool {
⋮----
pub fn generate_remote_keypair(
⋮----
let ledger = get_ledger_from_info(remote_wallet_info, keypair_name, wallet_manager)?;
let path = format!("{}{}", ledger.pretty_path, derivation_path.get_query());
Ok(RemoteKeypair::new(
⋮----
Err(RemoteWalletError::DeviceTypeMismatch)

================
File: remote-wallet/src/remote_wallet.rs
================
pub enum RemoteWalletError {
⋮----
fn from(err: hidapi::HidError) -> RemoteWalletError {
RemoteWalletError::Hid(err.to_string())
⋮----
fn from(err: RemoteWalletError) -> SignerError {
⋮----
RemoteWalletError::DeviceTypeMismatch => SignerError::Connection(err.to_string()),
RemoteWalletError::InvalidDevice => SignerError::Connection(err.to_string()),
⋮----
RemoteWalletError::LedgerError(e) => SignerError::Protocol(e.to_string()),
⋮----
RemoteWalletError::Protocol(e) => SignerError::Protocol(e.to_string()),
⋮----
SignerError::UserCancel("remote wallet operation rejected by the user".to_string())
⋮----
_ => SignerError::Custom(err.to_string()),
⋮----
/// Collection of connected RemoteWallets
pub struct RemoteWalletManager {
⋮----
pub struct RemoteWalletManager {
⋮----
impl RemoteWalletManager {
/// Create a new instance.
    #[cfg(feature = "hidapi")]
pub fn new(usb: Arc<Mutex<hidapi::HidApi>>) -> Rc<Self> {
⋮----
/// Repopulate device list
    /// Note: this method iterates over and updates all devices
⋮----
/// Note: this method iterates over and updates all devices
    #[cfg(feature = "hidapi")]
pub fn update_devices(&self) -> Result<usize, RemoteWalletError> {
let mut usb = self.usb.lock();
usb.refresh_devices()?;
let devices = usb.device_list();
let num_prev_devices = self.devices.read().len();
let mut detected_devices = vec![];
let mut errors = vec![];
for device_info in devices.filter(|&device_info| {
⋮----
is_valid_hid_device(device_info.usage_page(), device_info.interface_number());
⋮----
let is_valid_hid_device = true; // libusb backend does not provide DeviceInfo::usage_page()
⋮----
&& is_valid_ledger(device_info.vendor_id(), device_info.product_id())
⋮----
match usb.open_path(device_info.path()) {
⋮----
let result = ledger.read_device(device_info);
⋮----
ledger.pretty_path = info.get_pretty_path();
let path = device_info.path().to_str().unwrap().to_string();
trace!("Found device: {info:?}");
detected_devices.push(Device {
⋮----
error!("Error connecting to ledger device to read info: {err}");
errors.push(err)
⋮----
Err(err) => error!("Error connecting to ledger device to read info: {err}"),
⋮----
let num_curr_devices = detected_devices.len();
*self.devices.write() = detected_devices;
if num_curr_devices == 0 && !errors.is_empty() {
return Err(errors[0].clone());
⋮----
Ok(num_curr_devices - num_prev_devices)
⋮----
Err(RemoteWalletError::Hid(
"hidapi crate compilation disabled in solana-remote-wallet.".to_string(),
⋮----
/// List connected and acknowledged wallets
    pub fn list_devices(&self) -> Vec<RemoteWalletInfo> {
⋮----
pub fn list_devices(&self) -> Vec<RemoteWalletInfo> {
self.devices.read().iter().map(|d| d.info.clone()).collect()
⋮----
/// Get a particular wallet
    #[allow(unreachable_patterns)]
pub fn get_ledger(
⋮----
.read()
.iter()
.find(|device| device.info.host_device_path == host_device_path)
.ok_or(RemoteWalletError::PubkeyNotFound)
.and_then(|device| match &device.wallet_type {
RemoteWalletType::Ledger(ledger) => Ok(ledger.clone()),
_ => Err(RemoteWalletError::DeviceTypeMismatch),
⋮----
/// Get wallet info.
    pub fn get_wallet_info(&self, pubkey: &Pubkey) -> Option<RemoteWalletInfo> {
⋮----
pub fn get_wallet_info(&self, pubkey: &Pubkey) -> Option<RemoteWalletInfo> {
⋮----
.find(|d| &d.info.pubkey == pubkey)
.map(|d| d.info.clone())
⋮----
/// Update devices in maximum `max_polling_duration` if it doesn't succeed
    pub fn try_connect_polling(&self, max_polling_duration: &Duration) -> bool {
⋮----
pub fn try_connect_polling(&self, max_polling_duration: &Duration) -> bool {
⋮----
while start_time.elapsed() <= *max_polling_duration {
if let Ok(num_devices) = self.update_devices() {
⋮----
trace!("{num_devices} Remote Wallet{plural} found");
⋮----
pub trait RemoteWallet<T> {
fn name(&self) -> &str {
⋮----
fn read_device(&mut self, dev_info: &T) -> Result<RemoteWalletInfo, RemoteWalletError> {
unimplemented!();
⋮----
fn get_pubkey(
⋮----
fn sign_message(
⋮----
fn sign_offchain_message(
⋮----
pub struct Device {
⋮----
pub enum RemoteWalletType {
⋮----
pub struct RemoteWalletInfo {
⋮----
impl RemoteWalletInfo {
pub fn parse_locator(locator: Locator) -> Self {
⋮----
pubkey: locator.pubkey.unwrap_or_default(),
⋮----
pub fn get_pretty_path(&self) -> String {
format!("usb://{}/{:?}", self.manufacturer, self.pubkey,)
⋮----
pub(crate) fn matches(&self, other: &Self) -> bool {
⋮----
pub fn is_valid_hid_device(usage_page: u16, interface_number: i32) -> bool {
⋮----
pub fn initialize_wallet_manager() -> Result<Rc<RemoteWalletManager>, RemoteWalletError> {
⋮----
Ok(RemoteWalletManager::new(hidapi))
⋮----
pub fn maybe_wallet_manager() -> Result<Option<Rc<RemoteWalletManager>>, RemoteWalletError> {
let wallet_manager = initialize_wallet_manager()?;
let device_count = wallet_manager.update_devices()?;
⋮----
Ok(Some(wallet_manager))
⋮----
drop(wallet_manager);
Ok(None)
⋮----
mod tests {
⋮----
fn test_parse_locator() {
⋮----
pubkey: Some(pubkey),
⋮----
assert!(wallet_info.matches(&RemoteWalletInfo {
⋮----
fn test_remote_wallet_info_matches() {
⋮----
model: "Nano S".to_string(),
serial: "0001".to_string(),
host_device_path: "/host/device/path".to_string(),
⋮----
assert!(!info.matches(&test_info));
⋮----
assert!(info.matches(&test_info));
test_info.model = "Other".to_string();
⋮----
test_info.model = "Nano S".to_string();
⋮----
test_info.host_device_path = "/host/device/path".to_string();
⋮----
fn test_get_pretty_path() {
⋮----
let pubkey_str = pubkey.to_string();
⋮----
model: "nano-s".to_string(),
⋮----
serial: "".to_string(),
⋮----
assert_eq!(

================
File: remote-wallet/Cargo.toml
================
[package]
name = "solana-remote-wallet"
documentation = "https://docs.rs/solana-remote-wallet"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = ["linux-static-hidraw"]
agave-unstable-api = []
linux-shared-hidraw = ["hidapi/linux-shared-hidraw"]
linux-shared-libusb = ["hidapi/linux-shared-libusb"]
linux-static-hidraw = ["hidapi/linux-static-hidraw"]
linux-static-libusb = ["hidapi/linux-static-libusb"]

[dependencies]
console = { workspace = true }
dialoguer = { workspace = true }
hidapi = { workspace = true, optional = true }
log = { workspace = true }
num-derive = { workspace = true }
num-traits = { workspace = true }
parking_lot = { workspace = true }
qstring = { workspace = true }
semver = { workspace = true }
solana-derivation-path = { workspace = true }
solana-offchain-message = { workspace = true }
solana-pubkey = { workspace = true, features = ["std"] }
solana-signature = { workspace = true, features = ["std"] }
solana-signer = { workspace = true }
thiserror = { workspace = true }
uriparse = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }

================
File: remote-wallet/README.md
================
Solana Remote Wallet
===

Library for interacting with "remote" wallets, meaning any wallet where the private key bytes are not directly available,
such as Ledger devices.

## Ledger udev-rules

In order to use a Ledger device on Linux machines, users must apply certain udev rules. These are available at the
[udev-rules repository](https://github.com/LedgerHQ/udev-rules) maintained by the Ledger team.

================
File: reserved-account-keys/src/lib.rs
================
fn example() -> Self {
⋮----
pub struct ReservedAccountKeys {
⋮----
impl Default for ReservedAccountKeys {
fn default() -> Self {
⋮----
impl ReservedAccountKeys {
pub fn new(reserved_accounts: &[ReservedAccount]) -> Self {
⋮----
.iter()
.filter(|reserved| reserved.feature_id.is_none())
.map(|reserved| reserved.key)
.collect(),
⋮----
.filter_map(|ReservedAccount { key, feature_id }| {
feature_id.as_ref().map(|feature_id| (*key, *feature_id))
⋮----
pub fn new_all_activated() -> Self {
⋮----
active: Self::all_keys_iter().copied().collect(),
⋮----
pub fn is_reserved(&self, key: &Pubkey) -> bool {
self.active.contains(key)
⋮----
pub fn update_active_set(&mut self, feature_set: &FeatureSet) {
self.inactive.retain(|reserved_key, feature_id| {
if feature_set.is_active(feature_id) {
self.active.insert(*reserved_key);
⋮----
pub fn all_keys_iter() -> impl Iterator<Item = &'static Pubkey> {
⋮----
.map(|reserved_key| &reserved_key.key)
⋮----
pub fn empty_key_set() -> HashSet<Pubkey> {
⋮----
pub struct ReservedAccount {
⋮----
impl ReservedAccount {
pub fn new_pending(key: Pubkey, feature_id: Pubkey) -> Self {
⋮----
feature_id: Some(feature_id),
⋮----
pub fn new_active(key: Pubkey) -> Self {
⋮----
vec![
⋮----
mod tests {
⋮----
fn test_is_reserved() {
⋮----
assert!(
⋮----
fn test_update_active_set() {
⋮----
let reserved_accounts = vec![
⋮----
assert!(reserved_account_keys.is_reserved(&active_reserved_key));
assert!(!reserved_account_keys.is_reserved(&pending_reserved_keys[0]));
assert!(!reserved_account_keys.is_reserved(&pending_reserved_keys[1]));
let previous_reserved_account_keys = reserved_account_keys.clone();
⋮----
reserved_account_keys.update_active_set(&feature_set);
assert_eq!(reserved_account_keys, previous_reserved_account_keys);
feature_set.active_mut().insert(feature_ids[0], 0);
⋮----
assert!(reserved_account_keys.is_reserved(&pending_reserved_keys[0]));
⋮----
feature_set.active_mut().insert(feature_ids[1], 0);
⋮----
assert!(reserved_account_keys.is_reserved(&pending_reserved_keys[1]));

================
File: reserved-account-keys/Cargo.toml
================
[package]
name = "agave-reserved-account-keys"
description = "Reserved Solana account keys"
documentation = "https://docs.rs/agave-reserved-account-keys"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]
all-features = true
rustdoc-args = ["--cfg=docsrs"]

[features]
agave-unstable-api = []
frozen-abi = ["dep:solana-frozen-abi", "dep:solana-frozen-abi-macro"]

[dependencies]
agave-feature-set = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-pubkey = { workspace = true, default-features = false }
solana-sdk-ids = { workspace = true }

[dev-dependencies]
solana-message = { workspace = true }
solana-sysvar = { workspace = true }

[lints]
workspace = true

================
File: rpc/src/rpc/account_resolver.rs
================
pub(crate) fn get_account_from_overwrites_or_bank(
⋮----
.and_then(|accounts| accounts.get(pubkey).cloned())
.or_else(|| bank.get_account(pubkey))

================
File: rpc/src/cluster_tpu_info.rs
================
pub struct ClusterTpuInfo {
⋮----
impl ClusterTpuInfo {
pub fn new(cluster_info: Arc<ClusterInfo>, poh_recorder: Arc<RwLock<PohRecorder>>) -> Self {
⋮----
impl TpuInfo for ClusterTpuInfo {
fn refresh_recent_peers(&mut self) {
⋮----
.tpu_peers()
.into_iter()
.chain(once(self.cluster_info.my_contact_info()))
.filter_map(|node| Some((*node.pubkey(), node.tpu(Protocol::QUIC)?)))
.collect();
⋮----
fn get_leader_tpus(&self, max_count: u64) -> Vec<&SocketAddr> {
let recorder = self.poh_recorder.read().unwrap();
⋮----
.filter_map(|i| recorder.leader_after_n_slots(i * NUM_CONSECUTIVE_LEADER_SLOTS))
⋮----
drop(recorder);
let mut unique_leaders = vec![];
for leader in leaders.iter() {
if let Some(addr) = self.recent_peers.get(leader) {
if !unique_leaders.contains(&addr) {
unique_leaders.push(addr);
⋮----
fn get_not_unique_leader_tpus(&self, max_count: u64) -> Vec<&SocketAddr> {
⋮----
.iter()
.filter_map(|leader_pubkey| self.recent_peers.get(leader_pubkey))
.collect()
⋮----
mod test {
⋮----
fn test_refresh_recent_peers() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
let mut expected_validator_pubkeys = vec![
⋮----
expected_validator_pubkeys.sort();
let validator_keypairs = vec![
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config_with_vote_accounts(
⋮----
vec![10_000; 3],
⋮----
bank.last_blockhash(),
bank.clone(),
Some((2, 2)),
bank.ticks_per_slot(),
⋮----
&validator_vote_keypairs0.node_keypair.pubkey(),
timestamp(),
⋮----
&validator_vote_keypairs1.node_keypair.pubkey(),
⋮----
&validator_vote_keypairs2.node_keypair.pubkey(),
⋮----
cluster_info.insert_info(validator1_contact_info);
cluster_info.insert_info(validator2_contact_info);
⋮----
leader_info.refresh_recent_peers();
⋮----
leader_info.recent_peers.keys().copied().collect::<Vec<_>>();
refreshed_recent_peers.sort();
assert_eq!(refreshed_recent_peers, expected_validator_pubkeys);
⋮----
fn test_get_leader_tpus() {
⋮----
ContactInfo::new_localhost(&node_keypair.pubkey(), timestamp()),
⋮----
validator_vote_keypairs0.node_keypair.pubkey(),
⋮----
validator_vote_keypairs1.node_keypair.pubkey(),
⋮----
validator_vote_keypairs2.node_keypair.pubkey(),
⋮----
.cloned()
⋮----
recent_peers: recent_peers.clone(),
⋮----
let slot = bank.slot();
⋮----
solana_ledger::leader_schedule_utils::slot_leader_at(slot, &bank).unwrap();
assert_eq!(
⋮----
.unwrap();
let mut expected_leader_sockets = vec![
⋮----
expected_leader_sockets.dedup();
assert_eq!(leader_info.get_leader_tpus(2), expected_leader_sockets);
⋮----
let expected_leader_sockets = vec![
⋮----
let mut unique_expected_leader_sockets = expected_leader_sockets.clone();
unique_expected_leader_sockets.dedup();
⋮----
assert!(leader_info.get_leader_tpus(x).len() <= recent_peers.len());
assert_eq!(leader_info.get_not_unique_leader_tpus(x).len(), x as usize);

================
File: rpc/src/filter.rs
================
pub fn filter_allows(filter: &RpcFilterType, account: &AccountSharedData) -> bool {
⋮----
RpcFilterType::DataSize(size) => account.data().len() as u64 == *size,
RpcFilterType::Memcmp(compare) => compare.bytes_match(account.data()),
RpcFilterType::TokenAccountState => Account::valid_account_data(account.data()),

================
File: rpc/src/lib.rs
================
mod cluster_tpu_info;
pub mod filter;
pub mod max_slots;
pub mod optimistically_confirmed_bank_tracker;
pub mod parsed_token_accounts;
pub mod rpc;
mod rpc_cache;
pub mod rpc_completed_slots_service;
pub mod rpc_health;
pub mod rpc_pubsub;
pub mod rpc_pubsub_service;
pub mod rpc_service;
pub mod rpc_subscription_tracker;
pub mod rpc_subscriptions;
pub mod slot_status_notifier;
pub mod transaction_notifier_interface;
pub mod transaction_status_service;
⋮----
extern crate log;
⋮----
extern crate serde_json;
⋮----
extern crate solana_metrics;

================
File: rpc/src/max_slots.rs
================
use std::sync::atomic::AtomicU64;
⋮----
pub struct MaxSlots {

================
File: rpc/src/optimistically_confirmed_bank_tracker.rs
================
pub struct OptimisticallyConfirmedBank {
⋮----
impl OptimisticallyConfirmedBank {
pub fn locked_from_bank_forks_root(bank_forks: &Arc<RwLock<BankForks>>) -> Arc<RwLock<Self>> {
⋮----
bank: bank_forks.read().unwrap().root_bank(),
⋮----
pub enum BankNotification {
⋮----
pub enum SlotNotification {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
⋮----
write!(f, "OptimisticallyConfirmed({slot:?})")
⋮----
BankNotification::Frozen(bank) => write!(f, "Frozen({})", bank.slot()),
BankNotification::NewRootBank(bank) => write!(f, "Root({})", bank.slot()),
BankNotification::NewRootedChain(chain) => write!(f, "RootedChain({chain:?})"),
⋮----
pub type BankNotificationWithDependencyWork = (
⋮----
pub type BankNotificationReceiver = Receiver<BankNotificationWithDependencyWork>;
pub type BankNotificationSender = Sender<BankNotificationWithDependencyWork>;
⋮----
pub struct BankNotificationSenderConfig {
⋮----
pub type SlotNotificationReceiver = Receiver<SlotNotification>;
pub type SlotNotificationSender = Sender<SlotNotification>;
pub struct OptimisticallyConfirmedBankTracker {
⋮----
impl OptimisticallyConfirmedBankTracker {
pub fn new(
⋮----
.name("solOpConfBnkTrk".to_string())
.spawn(move || loop {
if exit.load(Ordering::Relaxed) {
⋮----
.unwrap();
⋮----
fn recv_notification(
⋮----
let notification = receiver.recv_timeout(Duration::from_secs(1))?;
⋮----
Ok(())
⋮----
fn notify_slot_status(
⋮----
for sender in slot_notification_subscribers.read().unwrap().iter() {
match sender.send(notification.clone()) {
⋮----
info!("Failed to send notification {notification:?}, error: {err:?}");
⋮----
fn notify_or_defer(
⋮----
if bank.is_frozen() {
if bank.slot() > *last_notified_confirmed_slot {
debug!(
⋮----
subscriptions.notify_gossip_subscribers(bank.slot());
*last_notified_confirmed_slot = bank.slot();
⋮----
SlotNotification::OptimisticallyConfirmed(bank.slot()),
⋮----
prioritization_fee_cache.finalize_priority_fee(bank.slot(), bank.bank_id());
⋮----
} else if bank.slot() > bank_forks.read().unwrap().root() {
pending_optimistically_confirmed_banks.insert(bank.slot());
debug!("notify_or_defer defer notifying for slot {:?}", bank.slot());
⋮----
fn notify_or_defer_confirmed_banks(
⋮----
for confirmed_bank in bank.parents_inclusive().iter().rev() {
if confirmed_bank.slot() > slot_threshold {
⋮----
fn notify_new_root_slots(
⋮----
if slot_notification_subscribers.is_none() {
⋮----
roots.sort_unstable();
assert!(roots.len() >= 2);
for i in 1..roots.len() {
⋮----
debug!("Doing SlotNotification::Root for root {root}, parent: {parent}");
⋮----
pub fn process_notification(
⋮----
debug!("received bank notification: {notification:?} event: {dependency_work:?}");
if let Some(tracker) = dependency_tracker.as_ref() {
⋮----
tracker.wait_for_dependency(dependency_work);
⋮----
let bank = bank_forks.read().unwrap().get(slot);
⋮----
optimistically_confirmed_bank.write().unwrap();
if bank.slot() > w_optimistically_confirmed_bank.bank.slot() && bank.is_frozen()
⋮----
w_optimistically_confirmed_bank.bank = bank.clone();
⋮----
drop(w_optimistically_confirmed_bank);
} else if slot > bank_forks.read().unwrap().root() {
pending_optimistically_confirmed_banks.insert(slot);
⋮----
inc_new_counter_info!("dropped-already-rooted-optimistic-bank-notification", 1);
⋮----
subscriptions.notify_slot_update(SlotUpdate::OptimisticConfirmation {
⋮----
timestamp: timestamp(),
⋮----
let frozen_slot = bank.slot();
if let Some(parent) = bank.parent() {
⋮----
.transaction_count()
.saturating_sub(parent.transaction_count());
subscriptions.notify_slot_update(SlotUpdate::Frozen {
⋮----
num_transaction_entries: bank.transaction_entries_count(),
⋮----
num_failed_transactions: bank.transaction_error_count(),
max_transactions_per_entry: bank.transactions_per_entry_max(),
⋮----
SlotNotification::Frozen((bank.slot(), bank.parent_slot())),
⋮----
if pending_optimistically_confirmed_banks.remove(&bank.slot()) {
⋮----
bank.clone(),
⋮----
if frozen_slot > w_optimistically_confirmed_bank.bank.slot() {
⋮----
let root_slot = bank.slot();
⋮----
if root_slot > w_optimistically_confirmed_bank.bank.slot() {
⋮----
pending_optimistically_confirmed_banks.retain(|&s| s > root_slot);
⋮----
pub fn close(self) -> thread::Result<()> {
self.join()
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
mod tests {
⋮----
fn get_root_notifications(receiver: &Receiver<SlotNotification>) -> Vec<SlotNotification> {
⋮----
while let Ok(notification) = receiver.recv_timeout(Duration::from_millis(100)) {
notifications.push(notification);
⋮----
fn test_process_notification() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(100);
⋮----
let bank0 = bank_forks.read().unwrap().get(0).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank1);
let bank1 = bank_forks.read().unwrap().get(1).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank2);
let bank2 = bank_forks.read().unwrap().get(2).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank3);
⋮----
bank_forks.clone(),
⋮----
optimistically_confirmed_bank.clone(),
⋮----
assert_eq!(optimistically_confirmed_bank.read().unwrap().bank.slot(), 0);
⋮----
assert_eq!(optimistically_confirmed_bank.read().unwrap().bank.slot(), 2);
assert_eq!(highest_confirmed_slot, 2);
⋮----
assert_eq!(pending_optimistically_confirmed_banks.len(), 1);
assert!(pending_optimistically_confirmed_banks.contains(&3));
assert_eq!(highest_confirmed_slot, 3);
let bank3 = bank_forks.read().unwrap().get(3).unwrap();
bank3.freeze();
⋮----
assert_eq!(optimistically_confirmed_bank.read().unwrap().bank.slot(), 3);
⋮----
assert_eq!(pending_optimistically_confirmed_banks.len(), 0);
⋮----
bank_forks.write().unwrap().insert(bank4);
⋮----
assert!(pending_optimistically_confirmed_banks.contains(&4));
assert_eq!(highest_confirmed_slot, 4);
let bank4 = bank_forks.read().unwrap().get(4).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank5);
let bank5 = bank_forks.read().unwrap().get(5).unwrap();
⋮----
let (sender, receiver) = unbounded();
bank_notification_senders.push(sender);
let subscribers = Some(Arc::new(RwLock::new(bank_notification_senders)));
let parent_roots = bank5.ancestors.keys();
⋮----
assert_eq!(optimistically_confirmed_bank.read().unwrap().bank.slot(), 5);
⋮----
assert!(!pending_optimistically_confirmed_banks.contains(&4));
⋮----
assert_eq!(newest_root_slot, 0);
⋮----
assert_eq!(newest_root_slot, 5);
let notifications = get_root_notifications(&receiver);
assert_eq!(notifications.len(), 5);
⋮----
bank_forks.write().unwrap().insert(bank6);
⋮----
bank_forks.write().unwrap().insert(bank7);
bank_forks.write().unwrap().set_root(7, None, None);
⋮----
assert!(!pending_optimistically_confirmed_banks.contains(&6));
⋮----
let bank7 = bank_forks.read().unwrap().get(7).unwrap();
let parent_roots = bank7.ancestors.keys();
⋮----
assert_eq!(optimistically_confirmed_bank.read().unwrap().bank.slot(), 7);
⋮----
assert_eq!(newest_root_slot, 7);
⋮----
assert_eq!(notifications.len(), 1);
⋮----
fn test_event_synchronization() {
⋮----
let tracker_clone = dependency_tracker.clone();
⋮----
Some(work_id_1),
⋮----
&Some(tracker_clone.clone()),
⋮----
assert_eq!(highest_confirmed_slot, 1);
⋮----
bank1.freeze();
⋮----
Some(work_id_2),
⋮----
&Some(tracker_clone),
⋮----
assert_eq!(optimistically_confirmed_bank.read().unwrap().bank.slot(), 1);
⋮----
dependency_tracker.mark_this_and_all_previous_work_processed(work_id_1);
dependency_tracker.mark_this_and_all_previous_work_processed(work_id_2);
handle.join().unwrap();

================
File: rpc/src/parsed_token_accounts.rs
================
pub fn get_parsed_token_account(
⋮----
let additional_data = get_token_account_mint(account.data())
.and_then(|mint_pubkey| {
⋮----
.and_then(|mint_account| get_additional_mint_data(bank, mint_account.data()).ok())
.map(|data| AccountAdditionalDataV3 {
spl_token_additional_data: Some(data),
⋮----
encode_ui_account(
⋮----
pub fn get_parsed_token_accounts<I>(
⋮----
keyed_accounts.filter_map(move |(pubkey, account)| {
let additional_data = get_token_account_mint(account.data()).and_then(|mint_pubkey| {
mint_data.get(&mint_pubkey).cloned().or_else(|| {
let (_, data) = get_mint_owner_and_additional_data(&bank, &mint_pubkey).ok()?;
⋮----
mint_data.insert(mint_pubkey, data);
Some(data)
⋮----
let maybe_encoded_account = encode_ui_account(
⋮----
Some(RpcKeyedAccount {
pubkey: pubkey.to_string(),
⋮----
pub(crate) fn get_mint_owner_and_additional_data(
⋮----
Ok((
⋮----
let mint_account = bank.get_account(mint).ok_or_else(|| {
Error::invalid_params("Invalid param: could not find mint".to_string())
⋮----
let mint_data = get_additional_mint_data(bank, mint_account.data())?;
Ok((*mint_account.owner(), mint_data))
⋮----
fn get_additional_mint_data(bank: &Bank, data: &[u8]) -> Result<SplTokenAdditionalDataV2> {
⋮----
.map_err(|_| {
Error::invalid_params("Invalid param: Token mint could not be unpacked".to_string())
⋮----
.map(|mint| {
⋮----
.map(|x| (*x, bank.clock().unix_timestamp))
.ok();

================
File: rpc/src/rpc_cache.rs
================
pub struct LargestAccountsCache {
⋮----
struct LargestAccountsCacheValue {
⋮----
impl LargestAccountsCache {
pub(crate) fn new(duration: u64) -> Self {
⋮----
pub(crate) fn get_largest_accounts(
⋮----
self.cache.get(filter).and_then(|value| {
if let Ok(elapsed) = value.cached_time.elapsed() {
⋮----
return Some((value.slot, value.accounts.clone()));
⋮----
pub(crate) fn set_largest_accounts(
⋮----
self.cache.insert(
filter.clone(),
⋮----
accounts: accounts.to_owned(),
⋮----
pub mod test {
⋮----
fn test_old_entries_expire() {
⋮----
let filter = Some(RpcLargestAccountsFilter::Circulating);
⋮----
cache.set_largest_accounts(&filter, 1000, &accounts);
⋮----
assert_eq!(cache.get_largest_accounts(&filter), None);

================
File: rpc/src/rpc_completed_slots_service.rs
================
pub struct RpcCompletedSlotsService;
impl RpcCompletedSlotsService {
pub fn spawn(
⋮----
.name("solRpcComplSlot".to_string())
.spawn(move || loop {
if exit.load(Ordering::Relaxed) {
⋮----
.recv_timeout(Duration::from_millis(COMPLETE_SLOT_REPORT_SLEEP_MS))
⋮----
info!("RpcCompletedSlotService channel disconnected, exiting.");
⋮----
rpc_subscriptions.notify_slot_update(SlotUpdate::Completed {
⋮----
timestamp: timestamp(),
⋮----
slot_status_notifier.read().unwrap().notify_completed(slot);
⋮----
.unwrap()

================
File: rpc/src/rpc_health.rs
================
pub enum RpcHealthStatus {
⋮----
pub struct RpcHealth {
⋮----
impl RpcHealth {
pub fn new(
⋮----
pub fn check(&self) -> RpcHealthStatus {
⋮----
if let Some(stub_health_status) = *self.stub_health_status.read().unwrap() {
⋮----
if self.override_health_check.load(Ordering::Relaxed) {
⋮----
.read()
.unwrap()
⋮----
.slot();
let mut optimistic_slot_infos = match self.blockstore.get_latest_optimistic_slots(1) {
⋮----
warn!("health check: blockstore error: {err}");
⋮----
optimistic_slot_infos.pop()
⋮----
warn!("health check: blockstore does not contain any optimistically confirmed slots");
⋮----
.saturating_sub(self.health_check_slot_distance)
⋮----
.saturating_sub(my_latest_optimistically_confirmed_slot);
warn!(
⋮----
pub(crate) fn stub(
⋮----
pub(crate) fn stub_set_health_status(&self, stub_health_status: Option<RpcHealthStatus>) {
*self.stub_health_status.write().unwrap() = stub_health_status;
⋮----
pub mod tests {
⋮----
fn test_get_health() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(100);
⋮----
let bank0 = bank_forks.read().unwrap().root_bank();
assert!(bank0.slot() == 0);
⋮----
optimistically_confirmed_bank.clone(),
blockstore.clone(),
⋮----
override_health_check.clone(),
⋮----
assert_eq!(health.check(), RpcHealthStatus::Ok);
override_health_check.store(false, Ordering::Relaxed);
assert_eq!(health.check(), RpcHealthStatus::Unknown);
⋮----
.insert_optimistic_slot(15, &Hash::default(), UnixTimestamp::default())
.unwrap();
assert_eq!(health.check(), RpcHealthStatus::Behind { num_slots: 15 });
⋮----
optimistically_confirmed_bank.write().unwrap().bank = bank4.clone();
assert_eq!(health.check(), RpcHealthStatus::Behind { num_slots: 11 });
⋮----
optimistically_confirmed_bank.write().unwrap().bank = bank5.clone();
⋮----
optimistically_confirmed_bank.write().unwrap().bank = bank15.clone();
⋮----
optimistically_confirmed_bank.write().unwrap().bank = bank16.clone();

================
File: rpc/src/rpc_pubsub_service.rs
================
pub struct PubSubConfig {
⋮----
impl Default for PubSubConfig {
fn default() -> Self {
⋮----
notification_threads: NonZeroUsize::new(get_thread_count()),
⋮----
impl PubSubConfig {
pub const fn default_for_tests() -> Self {
⋮----
pub struct PubSubService {
⋮----
impl PubSubService {
pub fn new(
⋮----
let subscription_control = subscriptions.control().clone();
⋮----
.name("solRpcPubSub".to_string())
.spawn(move || {
info!("PubSubService has started");
⋮----
.thread_name("solRpcPubSubRt")
.worker_threads(pubsub_config.worker_threads)
.enable_all()
.build()
.expect("runtime creation failed");
if let Err(err) = runtime.block_on(listen(
⋮----
error!("PubSubService has stopped due to error: {err}");
⋮----
info!("PubSubService has stopped");
⋮----
.expect("thread spawn failed");
⋮----
pub fn close(self) -> thread::Result<()> {
self.join()
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
struct SentNotificationStats {
⋮----
impl SentNotificationStats {
fn maybe_report(&self) {
if self.last_report.should_update(METRICS_REPORT_INTERVAL_MS) {
datapoint_info!(
⋮----
struct BroadcastHandler {
⋮----
fn increment_sent_notification_stats(
⋮----
stats.num_account.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_logs.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_program.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_signature.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_slot.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_slots_updates.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_root.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_vote.fetch_add(1, Ordering::Relaxed);
⋮----
stats.num_block.fetch_add(1, Ordering::Relaxed);
⋮----
stats.total_creation_to_queue_time_us.fetch_add(
notification.created_at.elapsed().as_micros() as u64,
⋮----
stats.maybe_report();
⋮----
impl BroadcastHandler {
fn new(current_subscriptions: Arc<DashMap<SubscriptionId, SubscriptionToken>>) -> Self {
⋮----
fn handle(&self, notification: RpcNotification) -> Result<Option<Arc<String>>, Error> {
⋮----
.entry(notification.subscription_id)
⋮----
increment_sent_notification_stats(
entry.get().params(),
⋮----
entry.remove();
⋮----
.upgrade()
.ok_or(Error::NotificationIsGone)
.map(Some)
⋮----
Ok(None)
⋮----
pub struct TestBroadcastReceiver {
⋮----
impl TestBroadcastReceiver {
pub fn recv(&mut self) -> String {
match self.recv_timeout(std::time::Duration::from_secs(10)) {
Err(err) => panic!("broadcast receiver error: {err}"),
⋮----
pub fn recv_timeout(&mut self, timeout: std::time::Duration) -> Result<String, String> {
⋮----
match self.inner.try_recv() {
⋮----
debug!(
⋮----
if let Some(json) = self.handler.handle(notification).expect("handler failed") {
return Ok(json.to_string());
⋮----
if started.elapsed() > timeout {
return Err("TestBroadcastReceiver: no data, timeout reached".into());
⋮----
sleep(std::time::Duration::from_millis(50));
⋮----
Err(e) => return Err(e.to_string()),
⋮----
pub fn test_connection(
⋮----
subscriptions.control().clone(),
⋮----
inner: subscriptions.control().broadcast_receiver(),
⋮----
enum Error {
⋮----
async fn handle_connection(
⋮----
let mut server = Server::new(socket.compat());
let request = server.receive_request().await?;
⋮----
key: request.key(),
⋮----
server.send_response(&accept).await?;
let mut builder = server.into_builder();
builder.set_max_message_size(4_096);
builder.set_max_frame_size(4_096);
let (mut sender, mut receiver) = builder.finish();
let mut broadcast_receiver = subscription_control.broadcast_receiver();
⋮----
json_rpc_handler.extend_with(rpc_impl.to_delegate());
⋮----
let receive_future = receiver.receive_data(&mut data);
pin!(receive_future);
⋮----
select! {
⋮----
if let Some(response) = json_rpc_handler.handle_request(data_str).await {
sender.send_text(&response).await?;
⋮----
data.clear();
⋮----
Ok(())
⋮----
async fn listen(
⋮----
info!("rpc_pubsub listening on {listen_address:?}");
⋮----
error!(
⋮----
return Err(e);
⋮----
mod tests {
⋮----
fn test_pubsub_new() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let thread = pubsub_service.thread_hdl.thread();
assert_eq!(thread.name().unwrap(), "solRpcPubSub");

================
File: rpc/src/rpc_pubsub.rs
================
pub trait RpcSolPubSub {
⋮----
pub use internal::RpcSolPubSubInternal;
mod internal {
⋮----
pub trait RpcSolPubSubInternal {
⋮----
pub struct RpcSolPubSubImpl {
⋮----
impl RpcSolPubSubImpl {
pub fn new(
⋮----
fn subscribe(&self, params: SubscriptionParams) -> Result<SubscriptionId> {
⋮----
.subscribe(params)
.map_err(|_| Error {
⋮----
.into(),
⋮----
let id = token.id();
self.current_subscriptions.insert(id, token);
Ok(id)
⋮----
fn unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
if self.current_subscriptions.remove(&id).is_some() {
Ok(true)
⋮----
Err(Error {
⋮----
message: "Invalid subscription id.".into(),
⋮----
pub fn block_until_processed(&self, rpc_subscriptions: &Arc<RpcSubscriptions>) {
⋮----
rpc.slot_subscribe().unwrap();
rpc_subscriptions.notify_slot(1, 0, 0);
receiver.recv();
⋮----
fn param<T: FromStr>(param_str: &str, thing: &str) -> Result<T> {
param_str.parse::<T>().map_err(|_e| Error {
⋮----
message: format!("Invalid Request: Invalid {thing} provided"),
⋮----
impl RpcSolPubSubInternal for RpcSolPubSubImpl {
fn account_subscribe(
⋮----
} = config.unwrap_or_default();
⋮----
commitment: commitment.unwrap_or_default(),
⋮----
encoding: encoding.unwrap_or(UiAccountEncoding::Binary),
⋮----
self.subscribe(SubscriptionParams::Account(params))
⋮----
fn account_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
self.unsubscribe(id)
⋮----
fn program_subscribe(
⋮----
let config = config.unwrap_or_default();
let mut filters = config.filters.unwrap_or_default();
if let Err(error) = verify_filters(&filters) {
return Err(Error {
⋮----
message: error.to_string(),
⋮----
optimize_filters(&mut filters);
⋮----
.unwrap_or(UiAccountEncoding::Binary),
⋮----
commitment: config.account_config.commitment.unwrap_or_default(),
with_context: config.with_context.unwrap_or_default(),
⋮----
self.subscribe(SubscriptionParams::Program(params))
⋮----
fn program_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn logs_subscribe(
⋮----
if keys.len() != 1 {
⋮----
message: "Invalid Request: Only 1 address supported".into(),
⋮----
commitment: config.and_then(|c| c.commitment).unwrap_or_default(),
⋮----
self.subscribe(SubscriptionParams::Logs(params))
⋮----
fn logs_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn signature_subscribe(
⋮----
commitment: config.commitment.unwrap_or_default(),
enable_received_notification: config.enable_received_notification.unwrap_or_default(),
⋮----
self.subscribe(SubscriptionParams::Signature(params))
⋮----
fn signature_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn slot_subscribe(&self) -> Result<SubscriptionId> {
self.subscribe(SubscriptionParams::Slot)
⋮----
fn slot_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn slots_updates_subscribe(&self) -> Result<SubscriptionId> {
self.subscribe(SubscriptionParams::SlotsUpdates)
⋮----
fn slots_updates_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn block_subscribe(
⋮----
return Err(Error::new(jsonrpc_core::ErrorCode::MethodNotFound));
⋮----
let commitment = config.commitment.unwrap_or_default();
check_is_at_least_confirmed(commitment)?;
⋮----
encoding: config.encoding.unwrap_or(UiTransactionEncoding::Base64),
⋮----
transaction_details: config.transaction_details.unwrap_or_default(),
show_rewards: config.show_rewards.unwrap_or_default(),
⋮----
self.subscribe(SubscriptionParams::Block(params))
⋮----
fn block_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn vote_subscribe(&self) -> Result<SubscriptionId> {
⋮----
self.subscribe(SubscriptionParams::Vote)
⋮----
fn vote_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn root_subscribe(&self) -> Result<SubscriptionId> {
self.subscribe(SubscriptionParams::Root)
⋮----
fn root_unsubscribe(&self, id: SubscriptionId) -> Result<bool> {
⋮----
fn get_version(&self) -> Result<RpcVersionInfo> {
⋮----
Ok(RpcVersionInfo {
solana_core: version.to_string(),
feature_set: Some(version.feature_set),
⋮----
mod tests {
⋮----
mod transaction {
⋮----
fn process_transaction_and_notify(
⋮----
.read()
.unwrap()
.get(current_slot)
⋮----
.process_transaction(tx)?;
⋮----
subscriptions.notify_subscribers(commitment_slots);
Ok(())
⋮----
fn test_signature_subscribe() {
⋮----
} = create_genesis_config(10_000);
⋮----
let bob_pubkey = bob.pubkey();
⋮----
let blockhash = bank.last_blockhash();
⋮----
bank_forks.clone(),
⋮----
rpc.signature_subscribe(
tx.signatures[0].to_string(),
Some(RpcSignatureSubscribeConfig {
commitment: Some(CommitmentConfig::finalized()),
⋮----
.unwrap();
process_transaction_and_notify(&bank_forks, &tx, &rpc_subscriptions, 0).unwrap();
let response = receiver.recv();
⋮----
let expected = json!({
⋮----
assert_eq!(
⋮----
enable_received_notification: Some(true),
⋮----
rpc_subscriptions.notify_signatures_received((received_slot, vec![tx.signatures[0]]));
⋮----
commitment: Some(CommitmentConfig::confirmed()),
⋮----
fn test_signature_unsubscribe() {
⋮----
io.extend_with(rpc.to_delegate());
⋮----
let req = format!(
⋮----
let _res = io.handle_request_sync(&req);
⋮----
let res = io.handle_request_sync(req);
⋮----
let expected: Response = serde_json::from_str(expected).unwrap();
let result: Response = serde_json::from_str(&res.unwrap()).unwrap();
assert_eq!(result, expected);
// Test bad parameter
⋮----
fn test_account_subscribe() {
⋮----
} = create_genesis_config(10_000_000_000);
⋮----
activate_all_features(&mut genesis_config);
⋮----
let bank0 = bank_forks.read().unwrap().get(0).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank1);
⋮----
rpc.account_subscribe(
vote_account.pubkey().to_string(),
Some(RpcAccountInfoConfig {
commitment: Some(CommitmentConfig::processed()),
encoding: Some(encoding),
⋮----
rpc.block_until_processed(&rpc_subscriptions);
⋮----
let bank = bank_forks.read().unwrap().working_bank();
let rent = &bank.rent_collector().rent;
⋮----
rent.minimum_balance(0),
rent.minimum_balance(VoteStateV4::size_of()),
⋮----
let tx = system_transaction::transfer(&alice, &from.pubkey(), balance, blockhash);
process_transaction_and_notify(&bank_forks, &tx, &rpc_subscriptions, 1).unwrap();
let mut ixs = vec![system_instruction::create_account(
⋮----
ixs.append(&mut vote_instruction::create_account_with_config(
&from.pubkey(),
&vote_account.pubkey(),
⋮----
node_pubkey: validator.pubkey(),
authorized_voter: voter.pubkey(),
⋮----
let message = Message::new(&ixs, Some(&from.pubkey()));
⋮----
// Test signature confirmation notification #1
⋮----
.get(1)
⋮----
.get_account(&vote_account.pubkey())
⋮----
let expected_data = account.data();
⋮----
fn test_account_subscribe_with_encoding() {
⋮----
nonce_account.pubkey().to_string(),
⋮----
encoding: Some(UiAccountEncoding::JsonParsed),
⋮----
&alice.pubkey(),
&nonce_account.pubkey(),
⋮----
let message = Message::new(&ixs, Some(&alice.pubkey()));
⋮----
.get_account(&nonce_account.pubkey())
⋮----
let expected_data = parse_account_data_v3(
⋮----
fn test_account_unsubscribe() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
fn test_account_commitment_not_fulfilled() {
⋮----
bob.pubkey().to_string(),
⋮----
let tx = system_transaction::transfer(&alice, &bob.pubkey(), 100, blockhash);
⋮----
.process_transaction(&tx)
⋮----
rpc_subscriptions.notify_subscribers(CommitmentSlots::default());
// allow 200ms for notification thread to wake
⋮----
let _panic = receiver.recv();
⋮----
fn test_account_commitment() {
⋮----
fn test_slot_subscribe() {
⋮----
rpc_subscriptions.notify_slot(0, 0, 0);
⋮----
let expected_res_str = serde_json::to_string(&expected_res).unwrap();
let expected = format!(
⋮----
assert_eq!(expected, response);
⋮----
fn test_slot_unsubscribe() {
⋮----
let sub_id = rpc.slot_subscribe().unwrap();
⋮----
assert!(rpc.slot_unsubscribe(42.into()).is_err());
assert!(rpc.slot_unsubscribe(sub_id).is_ok());
⋮----
fn test_vote_subscribe() {
⋮----
(0..10).map(|_| ValidatorVoteKeypairs::new_rand()).collect();
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config_with_vote_accounts(
⋮----
vec![100; validator_voting_keypairs.len()],
⋮----
// Setup Subscriptions
⋮----
// Setup RPC
⋮----
rpc.vote_subscribe().unwrap();
⋮----
slots: vec![1, 2],
⋮----
subscriptions.notify_vote(
⋮----
fn test_vote_unsubscribe() {
⋮----
let sub_id = rpc.vote_subscribe().unwrap();
assert!(rpc.vote_unsubscribe(42.into()).is_err());
assert!(rpc.vote_unsubscribe(sub_id).is_ok());
⋮----
fn test_get_version() {
⋮----
let version = rpc.get_version().unwrap();
⋮----
assert_eq!(version.to_string(), expected_version.to_string());
assert_eq!(version.feature_set.unwrap(), expected_version.feature_set);

================
File: rpc/src/rpc_service.rs
================
enum SnapshotKind {
⋮----
struct TimeoutStream<S> {
⋮----
fn new(inner: S, timeout: Duration) -> Self {
⋮----
impl<S> Stream for TimeoutStream<S>
⋮----
type Item = std::io::Result<Bytes>;
fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
⋮----
return Poll::Ready(Some(Err(std::io::Error::new(
⋮----
Pin::new(&mut self.inner).poll_next(cx)
⋮----
pub struct JsonRpcService {
⋮----
pub request_processor: JsonRpcRequestProcessor, // Used only by test_rpc_new()...
⋮----
struct RpcRequestMiddleware {
⋮----
impl RpcRequestMiddleware {
pub fn new(
⋮----
.unwrap(),
⋮----
fn redirect(location: &str) -> hyper::Response<hyper::Body> {
⋮----
.status(hyper::StatusCode::SEE_OTHER)
.header(hyper::header::LOCATION, location)
.body(hyper::Body::from(String::from(location)))
.unwrap()
⋮----
fn not_found() -> hyper::Response<hyper::Body> {
⋮----
.status(hyper::StatusCode::NOT_FOUND)
.body(hyper::Body::empty())
⋮----
fn internal_server_error() -> hyper::Response<hyper::Body> {
⋮----
.status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
⋮----
fn strip_leading_slash(path: &str) -> Option<&str> {
path.strip_prefix('/')
⋮----
fn is_file_get_path(&self, path: &str) -> bool {
⋮----
if self.snapshot_config.is_none() {
⋮----
self.full_snapshot_archive_path_regex.is_match(path)
|| self.incremental_snapshot_archive_path_regex.is_match(path)
⋮----
async fn open_no_follow(path: impl AsRef<Path>) -> std::io::Result<tokio::fs::File> {
⋮----
.read(true)
.write(false)
.create(false)
.custom_flags(libc::O_NOFOLLOW)
.open(path)
⋮----
// TODO: Is there any way to achieve the same on Windows?
⋮----
fn find_snapshot_file<P>(&self, stem: P) -> (PathBuf, SnapshotKind)
⋮----
.is_match(Path::new("").join(&stem).to_str().unwrap());
⋮----
.as_ref()
⋮----
let local_path = root.join(&stem);
let path = if local_path.exists() {
⋮----
// remote snapshot archive path
snapshot_paths::build_snapshot_archives_remote_dir(root).join(stem)
⋮----
fn process_file_get(&self, path: &str) -> RequestMiddlewareAction {
⋮----
let stem = Self::strip_leading_slash(path).expect("path already verified");
⋮----
inc_new_counter_info!("rpc-get_genesis", 1);
(self.ledger_path.join(stem), None)
⋮----
inc_new_counter_info!("rpc-get_snapshot", 1);
let (path, snapshot_type) = self.find_snapshot_file(stem);
(path, Some(snapshot_type))
⋮----
.map(|m| m.len())
.unwrap_or(0)
.to_string();
info!("get {path} -> {filename:?} ({file_length} bytes)");
if cfg!(not(test)) {
assert!(
⋮----
let snapshot_timeout = self.snapshot_config.as_ref().and_then(|config| {
snapshot_type.map(|st| {
⋮----
.get()
.saturating_mul(solana_clock::DEFAULT_MS_PER_SLOT),
⋮----
Err(err) => Ok(if err.kind() == std::io::ErrorKind::NotFound {
⋮----
FramedRead::new(file, BytesCodec::new()).map_ok(|b| b.freeze());
⋮----
Ok(hyper::Response::builder()
.header(hyper::header::CONTENT_LENGTH, file_length)
.body(body)
.unwrap())
⋮----
fn health_check(&self) -> &'static str {
let response = match self.health.check() {
⋮----
info!("health check: {response}");
⋮----
impl RequestMiddleware for RpcRequestMiddleware {
fn on_request(&self, request: hyper::Request<hyper::Body>) -> RequestMiddlewareAction {
trace!("request uri: {}", request.uri());
⋮----
if request.uri().path() == FULL_SNAPSHOT_REQUEST_PATH
|| request.uri().path() == INCREMENTAL_SNAPSHOT_REQUEST_PATH
⋮----
if request.uri().path() == FULL_SNAPSHOT_REQUEST_PATH {
Some(full_snapshot_archive_info.snapshot_archive_info().clone())
⋮----
full_snapshot_archive_info.slot(),
⋮----
.map(|incremental_snapshot_archive_info| {
⋮----
.snapshot_archive_info()
.clone()
⋮----
RpcRequestMiddleware::redirect(&format!(
⋮----
.into();
⋮----
if let Some(path) = match_supply_path(request.uri().path()) {
process_rest(&self.bank_forks, path)
} else if self.is_file_get_path(request.uri().path()) {
self.process_file_get(request.uri().path())
} else if request.uri().path() == "/health" {
⋮----
.status(hyper::StatusCode::OK)
.body(hyper::Body::from(self.health_check()))
⋮----
.into()
⋮----
request.into()
⋮----
fn match_supply_path(path: &str) -> Option<&str> {
⋮----
"/v0/circulating-supply" | "/v0/total-supply" => Some(path),
⋮----
pub enum SupplyCalcError {
⋮----
async fn calculate_circulating_supply_async(bank: &Arc<Bank>) -> Result<u64, SupplyCalcError> {
let total_supply = bank.capitalization();
⋮----
tokio::task::spawn_blocking(move || calculate_non_circulating_supply(&bank))
⋮----
.expect("Failed to spawn blocking task")
.map_err(|e| SupplyCalcError::Scan(e.to_string()))?;
Ok(total_supply.saturating_sub(non_circulating_supply.lamports))
⋮----
async fn handle_rest(bank_forks: &Arc<RwLock<BankForks>>, path: &str) -> Option<String> {
⋮----
let bank = bank_forks.read().unwrap().root_bank();
let supply_result = calculate_circulating_supply_async(&bank).await;
⋮----
Ok(supply) => Some(build_balance_message(supply, false, false)),
⋮----
Some(build_balance_message(total_supply, false, false))
⋮----
fn process_rest(bank_forks: &Arc<RwLock<BankForks>>, path: &str) -> RequestMiddlewareAction {
let bank_forks = bank_forks.clone();
let path = path.to_string();
⋮----
let result = handle_rest(&bank_forks, path.as_str()).await;
⋮----
Some(s) => Ok(hyper::Response::builder()
⋮----
.body(hyper::Body::from(s))
.unwrap()),
None => Ok(RpcRequestMiddleware::not_found()),
⋮----
pub struct JsonRpcServiceConfig<'a> {
⋮----
pub struct RpcTpuClientArgs<'a>(
⋮----
impl JsonRpcService {
pub fn new_with_config(config: JsonRpcServiceConfig) -> Result<Self, String> {
let runtime = service_runtime(
⋮----
.map(|recorder| ClusterTpuInfo::new(config.cluster_info.clone(), recorder));
⋮----
config.cluster_info.clone(),
config.send_transaction_service_config.tpu_peers.clone(),
⋮----
Some(identity_keypair),
⋮----
config.rpc_config.clone(),
⋮----
config.bank_forks.clone(),
config.block_commitment_cache.clone(),
config.blockstore.clone(),
⋮----
config.ledger_path.as_path(),
⋮----
Ok(json_rpc_service)
⋮----
fn new<
⋮----
info!("rpc bound to {rpc_addr:?}");
info!("rpc configuration: {config:?}");
⋮----
instance_name: bigtable_instance_name.clone(),
app_profile_id: bigtable_app_profile_id.clone(),
⋮----
.block_on(solana_storage_bigtable::LedgerStorage::new_with_config(
⋮----
.map(|bigtable_ledger_storage| {
info!("BigTable ledger storage initialized");
⋮----
Some(Arc::new(BigTableUploadService::new_with_config(
runtime.clone(),
bigtable_ledger_storage.clone(),
blockstore.clone(),
block_commitment_cache.clone(),
max_complete_transaction_status_slot.clone(),
⋮----
exit_bigtable_ledger_upload_service.clone(),
⋮----
Some(bigtable_ledger_storage),
⋮----
.unwrap_or_else(|err| {
error!("Failed to initialize BigTable ledger storage: {err:?}");
⋮----
.unwrap_or(MAX_REQUEST_BODY_SIZE);
⋮----
snapshot_config.clone(),
bank_forks.clone(),
⋮----
validator_exit.clone(),
health.clone(),
cluster_info.clone(),
⋮----
client.clone(),
⋮----
let test_request_processor = request_processor.clone();
let ledger_path = ledger_path.to_path_buf();
let (close_handle_sender, close_handle_receiver) = unbounded();
⋮----
.name("solJsonRpcSvc".to_string())
.spawn(move || {
renice_this_thread(rpc_niceness_adj).unwrap();
⋮----
io.extend_with(rpc_minimal::MinimalImpl.to_delegate());
⋮----
io.extend_with(rpc_bank::BankDataImpl.to_delegate());
io.extend_with(rpc_accounts::AccountsDataImpl.to_delegate());
io.extend_with(rpc_accounts_scan::AccountsScanImpl.to_delegate());
io.extend_with(rpc_full::FullImpl.to_delegate());
⋮----
let xbigtable = req.headers().get("x-bigtable");
if xbigtable.is_some_and(|v| v == "disabled") {
request_processor.clone_without_bigtable()
⋮----
request_processor.clone()
⋮----
.event_loop_executor(runtime.handle().clone())
.threads(1)
.cors(DomainsValidation::AllowOnly(vec![
⋮----
.cors_max_age(86400)
.request_middleware(request_middleware)
.max_request_body_size(max_request_body_size)
.start_http(&rpc_addr);
⋮----
warn!(
⋮----
close_handle_sender.send(Err(e.to_string())).unwrap();
⋮----
let server = server.unwrap();
close_handle_sender.send(Ok(server.close_handle())).unwrap();
server.wait();
exit_bigtable_ledger_upload_service.store(true, Ordering::Relaxed);
⋮----
.unwrap();
let close_handle = close_handle_receiver.recv().unwrap()?;
let close_handle_ = close_handle.clone();
⋮----
.write()
⋮----
.register_exit(Box::new(move || {
close_handle_.close();
⋮----
Ok(Self {
⋮----
close_handle: Some(close_handle),
⋮----
pub fn exit(&mut self) {
if let Some(c) = self.close_handle.take() {
c.close()
⋮----
pub fn join(mut self) -> thread::Result<()> {
self.exit();
self.thread_hdl.join()
⋮----
pub fn get_client_key_updater(&self) -> Arc<dyn NotifyKeyUpdate + Send + Sync> {
self.client_updater.clone()
⋮----
pub fn service_runtime(
⋮----
// The jsonrpc_http_server crate supports two execution models:
//
// - By default, it spawns a number of threads - configured with .threads(N) - and runs a
//   single-threaded futures executor in each thread.
// - Alternatively when configured with .event_loop_executor(executor) and .threads(1),
//   it executes all the tasks on the given executor, not spawning any extra internal threads.
⋮----
// We use the latter configuration, using a multi threaded tokio runtime as the executor. We
// do this so we can configure the number of worker threads, the number of blocking threads
// and then use tokio::task::spawn_blocking() to avoid blocking the worker threads on CPU
// bound operations like getMultipleAccounts. This results in reduced latency, since fast
// rpc calls (the majority) are not blocked by slow CPU bound ones.
⋮----
// NB: `rpc_blocking_threads` shouldn't be set too high (defaults to num_cpus / 2). Too many
let rpc_threads = 1.max(rpc_threads);
let rpc_blocking_threads = 1.max(rpc_blocking_threads);
⋮----
.worker_threads(rpc_threads)
.max_blocking_threads(rpc_blocking_threads)
.on_thread_start(move || renice_this_thread(rpc_niceness_adj).unwrap())
.thread_name("solRpcEl")
.enable_all()
.build()
.expect("Runtime"),
⋮----
mod tests {
⋮----
fn test_rpc_new() {
⋮----
} = create_genesis_config(10_000);
⋮----
let validator_exit = create_validator_exit(exit.clone());
⋮----
let cluster_info = Arc::new(new_test_cluster_info());
⋮----
solana_net_utils::find_available_port_in_range(ip_addr, port_range).unwrap(),
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
Some(runtime.handle().clone()),
⋮----
send_transaction_service_config.tpu_peers.clone(),
⋮----
.expect("assume successful JsonRpcService start");
let thread = rpc_service.thread_hdl.thread();
assert_eq!(thread.name().unwrap(), "solJsonRpcSvc");
assert_eq!(
⋮----
rpc_service.exit();
rpc_service.join().unwrap();
⋮----
fn create_bank_forks() -> Arc<RwLock<BankForks>> {
⋮----
fn test_process_rest_api() {
let bank_forks = create_bank_forks();
let runtime = tokio::runtime::Runtime::new().unwrap();
runtime.block_on(async {
⋮----
let circulating_supply = handle_rest(&bank_forks, "/v0/circulating-supply").await;
assert!(circulating_supply.is_some());
let total_supply = handle_rest(&bank_forks, "/v0/total-supply").await;
assert!(total_supply.is_some());
⋮----
fn test_strip_prefix() {
assert_eq!(RpcRequestMiddleware::strip_leading_slash("/"), Some(""));
assert_eq!(RpcRequestMiddleware::strip_leading_slash("
⋮----
RpcRequestMiddleware::strip_leading_slash("//abc"),

================
File: rpc/src/rpc_subscription_tracker.rs
================
pub struct SubscriptionId(u64);
⋮----
fn from(value: u64) -> Self {
SubscriptionId(value)
⋮----
fn from(value: SubscriptionId) -> Self {
⋮----
pub enum SubscriptionParams {
⋮----
impl SubscriptionParams {
fn method(&self) -> &'static str {
⋮----
fn commitment(&self) -> Option<CommitmentConfig> {
⋮----
SubscriptionParams::Account(params) => Some(params.commitment),
SubscriptionParams::Logs(params) => Some(params.commitment),
SubscriptionParams::Program(params) => Some(params.commitment),
SubscriptionParams::Signature(params) => Some(params.commitment),
SubscriptionParams::Block(params) => Some(params.commitment),
⋮----
fn is_commitment_watcher(&self) -> bool {
⋮----
!commitment.is_confirmed()
⋮----
fn is_gossip_watcher(&self) -> bool {
⋮----
commitment.is_confirmed()
⋮----
fn is_node_progress_watcher(&self) -> bool {
matches!(
⋮----
pub struct AccountSubscriptionParams {
⋮----
pub struct BlockSubscriptionParams {
⋮----
pub enum BlockSubscriptionKind {
⋮----
pub struct LogsSubscriptionParams {
⋮----
pub enum LogsSubscriptionKind {
⋮----
pub struct ProgramSubscriptionParams {
⋮----
pub struct SignatureSubscriptionParams {
⋮----
pub struct SubscriptionControl(Arc<SubscriptionControlInner>);
pub struct WeakSubscriptionTokenRef(Weak<SubscriptionTokenInner>, SubscriptionId);
struct SubscriptionControlInner {
⋮----
impl SubscriptionControl {
pub fn new(
⋮----
Self(Arc::new(SubscriptionControlInner {
⋮----
pub fn broadcast_receiver(&self) -> broadcast::Receiver<RpcNotification> {
self.0.broadcast_sender.subscribe()
⋮----
pub fn subscribe(&self, params: SubscriptionParams) -> Result<SubscriptionToken, Error> {
debug!(
⋮----
let count = self.0.subscriptions.len();
⋮----
let token = SubscriptionToken(
⋮----
self.0.counter.create_token(),
⋮----
let weak_ref = WeakSubscriptionTokenRef(Arc::downgrade(&token.0), token.0.id);
⋮----
match self.0.subscriptions.entry(params) {
DashEntry::Occupied(mut entry) => match entry.get().0.upgrade() {
Some(token_ref) => Ok(SubscriptionToken(token_ref, self.0.counter.create_token())),
// This means the last Arc for this Weak pointer entered the drop just before us,
// but could not remove the entry since we are holding the write lock.
// See `Drop` implementation for `SubscriptionTokenInner` for further info.
⋮----
create_token_and_weak_ref(entry.get().1, entry.key().clone());
entry.insert(weak_ref);
Ok(token)
⋮----
inc_new_counter_info!("rpc-subscription-refused-limit-reached", 1);
return Err(Error::TooManySubscriptions);
⋮----
let id = SubscriptionId::from(self.0.next_id.fetch_add(1, Ordering::AcqRel));
let (token, weak_ref) = create_token_and_weak_ref(id, entry.key().clone());
⋮----
.send(NotificationEntry::Subscribed(token.0.params.clone(), id).into());
⋮----
datapoint_info!(
⋮----
pub fn total(&self) -> usize {
self.0.subscriptions.len()
⋮----
pub fn assert_subscribed(&self, params: &SubscriptionParams) {
assert!(self.0.subscriptions.contains_key(params));
⋮----
pub fn assert_unsubscribed(&self, params: &SubscriptionParams) {
assert!(!self.0.subscriptions.contains_key(params));
⋮----
pub fn account_subscribed(&self, pubkey: &Pubkey) -> bool {
self.0.subscriptions.iter().any(|item| {
if let SubscriptionParams::Account(params) = item.key() {
⋮----
pub fn logs_subscribed(&self, pubkey: Option<&Pubkey>) -> bool {
⋮----
if let SubscriptionParams::Logs(params) = item.key() {
⋮----
LogsSubscriptionKind::Single(pubkey) => Some(pubkey),
⋮----
pub fn signature_subscribed(&self, signature: &Signature) -> bool {
⋮----
if let SubscriptionParams::Signature(params) = item.key() {
⋮----
pub struct SubscriptionInfo {
⋮----
impl SubscriptionInfo {
pub fn id(&self) -> SubscriptionId {
⋮----
pub fn method(&self) -> &'static str {
⋮----
pub fn params(&self) -> &SubscriptionParams {
⋮----
pub fn commitment(&self) -> Option<CommitmentConfig> {
⋮----
pub enum Error {
⋮----
struct LogsSubscriptionsIndex {
⋮----
impl LogsSubscriptionsIndex {
fn add(&mut self, params: &LogsSubscriptionParams) {
⋮----
*self.single_count.entry(key).or_default() += 1;
⋮----
self.update_config();
⋮----
fn remove(&mut self, params: &LogsSubscriptionParams) {
⋮----
LogsSubscriptionKind::Single(key) => match self.single_count.entry(key) {
⋮----
*entry.get_mut() -= 1;
if *entry.get() == 0 {
entry.remove();
⋮----
Entry::Vacant(_) => error!("missing entry in single_count"),
⋮----
fn update_config(&self) {
let mentioned_addresses = self.single_count.keys().copied().collect();
⋮----
.read()
.unwrap()
.root_bank()
⋮----
.write()
.unwrap() = config;
⋮----
pub struct SubscriptionsTracker {
⋮----
// Accounts, logs, programs, signatures (not gossip)
⋮----
// Accounts, logs, programs, signatures (gossip)
⋮----
// Slots, slots updates, roots, votes.
⋮----
impl SubscriptionsTracker {
pub fn new(bank_forks: Arc<RwLock<BankForks>>) -> Self {
⋮----
pub fn subscribe(
⋮----
last_notified_slot: RwLock::new(last_notified_slot()),
⋮----
commitment: params.commitment(),
method: params.method(),
params: params.clone(),
⋮----
self.logs_subscriptions_index.add(params);
⋮----
.entry(params.signature)
.or_default()
.insert(id, Arc::clone(&info));
⋮----
if info.params.is_commitment_watcher() {
self.commitment_watchers.insert(id, Arc::clone(&info));
⋮----
if info.params.is_gossip_watcher() {
self.gossip_watchers.insert(id, Arc::clone(&info));
⋮----
if info.params.is_node_progress_watcher() {
⋮----
.insert(info.params.clone(), Arc::clone(&info));
⋮----
pub fn unsubscribe(&mut self, params: SubscriptionParams, id: SubscriptionId) {
⋮----
self.logs_subscriptions_index.remove(params);
⋮----
if let Entry::Occupied(mut entry) = self.by_signature.entry(params.signature) {
if entry.get_mut().remove(&id).is_none() {
warn!("Subscriptions inconsistency (missing entry in by_signature)");
⋮----
if entry.get_mut().is_empty() {
⋮----
if params.is_commitment_watcher() {
if self.commitment_watchers.remove(&id).is_none() {
warn!("Subscriptions inconsistency (missing entry in commitment_watchers)");
⋮----
if params.is_gossip_watcher() {
if self.gossip_watchers.remove(&id).is_none() {
warn!("Subscriptions inconsistency (missing entry in gossip_watchers)");
⋮----
if params.is_node_progress_watcher() {
if self.node_progress_watchers.remove(&params).is_none() {
warn!("Subscriptions inconsistency (missing entry in node_progress_watchers)");
⋮----
pub fn by_signature(
⋮----
pub fn commitment_watchers(&self) -> &HashMap<SubscriptionId, Arc<SubscriptionInfo>> {
⋮----
pub fn gossip_watchers(&self) -> &HashMap<SubscriptionId, Arc<SubscriptionInfo>> {
⋮----
pub fn node_progress_watchers(&self) -> &HashMap<SubscriptionParams, Arc<SubscriptionInfo>> {
⋮----
struct SubscriptionTokenInner {
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
f.debug_struct("SubscriptionTokenInner")
.field("id", &self.id)
.finish()
⋮----
impl Drop for SubscriptionTokenInner {
⋮----
fn drop(&mut self) {
match self.control.subscriptions.entry(self.params.clone()) {
⋮----
warn!("Subscriptions inconsistency (missing entry in by_params)");
⋮----
DashEntry::Occupied(entry) if entry.get().0.strong_count() == 0 => {
⋮----
.send(NotificationEntry::Unsubscribed(self.params.clone(), self.id).into());
⋮----
pub struct SubscriptionToken(Arc<SubscriptionTokenInner>, CounterToken);
impl SubscriptionToken {
⋮----
mod tests {
⋮----
struct ControlWrapper {
⋮----
impl ControlWrapper {
fn new() -> Self {
⋮----
fn assert_subscribed(&self, expected_params: &SubscriptionParams, expected_id: u64) {
if let NotificationEntry::Subscribed(params, id) = self.receiver.recv().unwrap().entry {
assert_eq!(&params, expected_params);
assert_eq!(id, SubscriptionId::from(expected_id));
⋮----
panic!("unexpected notification");
⋮----
self.assert_silence();
⋮----
fn assert_unsubscribed(&self, expected_params: &SubscriptionParams, expected_id: u64) {
if let NotificationEntry::Unsubscribed(params, id) = self.receiver.recv().unwrap().entry
⋮----
fn assert_silence(&self) {
assert!(self.receiver.try_recv().is_err());
⋮----
fn notify_subscribe() {
⋮----
let token1 = control.control.subscribe(SubscriptionParams::Slot).unwrap();
control.assert_subscribed(&SubscriptionParams::Slot, 0);
drop(token1);
control.assert_unsubscribed(&SubscriptionParams::Slot, 0);
⋮----
fn notify_subscribe_multiple() {
⋮----
let token2 = token1.clone();
⋮----
let token3 = control.control.subscribe(SubscriptionParams::Slot).unwrap();
drop(token3);
control.assert_silence();
drop(token2);
⋮----
fn notify_subscribe_two_subscriptions() {
⋮----
let token_slot1 = control.control.subscribe(SubscriptionParams::Slot).unwrap();
⋮----
let token_signature1 = control.control.subscribe(signature_params.clone()).unwrap();
control.assert_subscribed(&signature_params, 1);
let token_slot2 = control.control.subscribe(SubscriptionParams::Slot).unwrap();
let token_signature2 = control.control.subscribe(signature_params.clone()).unwrap();
drop(token_slot1);
⋮----
drop(token_slot2);
⋮----
drop(token_signature2);
⋮----
drop(token_signature1);
control.assert_unsubscribed(&signature_params, 1);
let token_slot3 = control.control.subscribe(SubscriptionParams::Slot).unwrap();
control.assert_subscribed(&SubscriptionParams::Slot, 2);
drop(token_slot3);
control.assert_unsubscribed(&SubscriptionParams::Slot, 2);
⋮----
fn subscription_info() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
tracker.subscribe(SubscriptionParams::Slot, 0.into(), || 0);
⋮----
.get(&SubscriptionParams::Slot)
.unwrap();
assert_eq!(info.commitment, None);
assert_eq!(info.params, SubscriptionParams::Slot);
assert_eq!(info.method, SubscriptionParams::Slot.method());
assert_eq!(info.id, SubscriptionId::from(0));
assert_eq!(*info.last_notified_slot.read().unwrap(), 0);
⋮----
tracker.subscribe(account_params.clone(), 1.into(), || 42);
⋮----
.get(&SubscriptionId::from(1))
⋮----
assert_eq!(info.commitment, Some(CommitmentConfig::finalized()));
assert_eq!(info.params, account_params);
assert_eq!(info.method, account_params.method());
assert_eq!(info.id, SubscriptionId::from(1));
assert_eq!(*info.last_notified_slot.read().unwrap(), 42);
⋮----
fn subscription_indexes() {
fn counts(tracker: &SubscriptionsTracker) -> (usize, usize, usize, usize) {
⋮----
tracker.by_signature.len(),
tracker.commitment_watchers.len(),
tracker.gossip_watchers.len(),
tracker.node_progress_watchers.len(),
⋮----
assert_eq!(counts(&tracker), (0, 0, 0, 1));
tracker.unsubscribe(SubscriptionParams::Slot, 0.into());
assert_eq!(counts(&tracker), (0, 0, 0, 0));
⋮----
tracker.subscribe(account_params.clone(), 1.into(), || 0);
assert_eq!(counts(&tracker), (0, 1, 0, 0));
tracker.unsubscribe(account_params, 1.into());
⋮----
tracker.subscribe(account_params2.clone(), 2.into(), || 0);
assert_eq!(counts(&tracker), (0, 0, 1, 0));
tracker.unsubscribe(account_params2, 2.into());
⋮----
tracker.subscribe(signature_params.clone(), 3.into(), || 0);
assert_eq!(counts(&tracker), (1, 1, 0, 0));
tracker.unsubscribe(signature_params, 3.into());

================
File: rpc/src/rpc_subscriptions.rs
================
mod transaction {
⋮----
fn get_transaction_logs(
⋮----
LogsSubscriptionKind::Single(pubkey) => Some(pubkey),
⋮----
let mut logs = bank.get_transaction_logs(pubkey);
if matches!(params.kind, LogsSubscriptionKind::All) {
⋮----
logs.retain(|log| !log.is_vote);
⋮----
pub struct TimestampedNotificationEntry {
⋮----
fn from(entry: NotificationEntry) -> Self {
⋮----
pub enum NotificationEntry {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
⋮----
NotificationEntry::Root(root) => write!(f, "Root({root})"),
NotificationEntry::Vote(vote) => write!(f, "Vote({vote:?})"),
NotificationEntry::Slot(slot_info) => write!(f, "Slot({slot_info:?})"),
⋮----
write!(f, "SlotUpdate({slot_update:?})")
⋮----
write!(f, "Bank({{slot: {:?}}})", commitment_slots.slot)
⋮----
write!(f, "SignaturesReceived({slot_signatures:?})")
⋮----
NotificationEntry::Gossip(slot) => write!(f, "Gossip({slot:?})"),
⋮----
write!(f, "Subscribed({params:?}, {id:?})")
⋮----
write!(f, "Unsubscribed({params:?}, {id:?})")
⋮----
fn check_commitment_and_notify<P, S, B, F, X, I>(
⋮----
let bank = bank_forks.read().unwrap().get(slot);
⋮----
let results = bank_method(&bank, params);
let mut w_last_notified_slot = subscription.last_notified_slot.write().unwrap();
⋮----
filter_results(results, params, *w_last_notified_slot, bank);
⋮----
notifier.notify(
⋮----
pub struct RpcNotification {
⋮----
struct RpcNotificationResponse<T> {
⋮----
fn from(notification: RpcNotificationResponse<T>) -> Self {
⋮----
struct RpcNotificationContext {
⋮----
struct RecentItems {
⋮----
impl RecentItems {
fn new(max_len: usize, max_total_bytes: usize) -> Self {
⋮----
fn push(&mut self, item: Arc<String>) {
⋮----
.checked_add(item.len())
.expect("total bytes overflow");
self.queue.push_back(item);
while self.total_bytes > self.max_total_bytes || self.queue.len() > self.max_len {
let item = self.queue.pop_front().expect("can't be empty");
⋮----
.checked_sub(item.len())
.expect("total bytes underflow");
⋮----
let last_metrics_ago = now.duration_since(self.last_metrics_submission);
⋮----
datapoint_info!(
⋮----
trace!(
⋮----
struct RpcNotifier {
⋮----
thread_local! {
⋮----
struct NotificationParams<T> {
⋮----
struct Notification<T> {
⋮----
impl RpcNotifier {
fn notify<T>(&self, value: T, subscription: &SubscriptionInfo, is_final: bool)
⋮----
let buf_arc = RPC_NOTIFIER_BUF.with(|buf| {
let mut buf = buf.borrow_mut();
buf.clear();
⋮----
jsonrpc: Some(jsonrpc_core::Version::V2),
method: subscription.method(),
⋮----
subscription: subscription.id(),
⋮----
.expect("serialization never fails");
let buf_str = str::from_utf8(&buf).expect("json is always utf-8");
⋮----
subscription_id: subscription.id(),
⋮----
// There is an unlikely case where this can fail: if the last subscription is closed
// just as the notifier generates a notification for it.
let _ = self.sender.send(notification);
inc_new_counter_info!("rpc-pubsub-messages", 1);
inc_new_counter_info!("rpc-pubsub-bytes", buf_arc.len());
self.recent_items.lock().unwrap().push(buf_arc);
⋮----
fn filter_block_result_txs(
⋮----
.into_iter()
.filter(|tx| tx.account_keys().iter().any(|key| key == &pk))
.collect(),
⋮----
if block.transactions.is_empty() {
⋮----
return Ok(None);
⋮----
.encode_with_options(
⋮----
.map_err(|err| match err {
⋮----
// If last_modified_slot < last_notified_slot, then the last notif was for a fork.
// That's the risk clients take when subscribing to non-finalized commitments.
Ok(Some(RpcBlockUpdate {
⋮----
block: Some(block),
⋮----
fn filter_account_result(
⋮----
let (account, last_modified_slot) = result.unwrap_or_default();
let account = (last_modified_slot != last_notified_slot).then(|| {
if is_known_spl_token_id(account.owner())
⋮----
get_parsed_token_account(&bank, &params.pubkey, account, None)
⋮----
encode_ui_account(&params.pubkey, &account, params.encoding, None, None)
⋮----
fn filter_signature_result(
⋮----
result.map(|result| {
⋮----
err: result.err().map(Into::into),
⋮----
fn filter_program_results(
⋮----
let accounts_is_empty = accounts.is_empty();
⋮----
let filters = params.filters.clone();
let keyed_accounts = accounts.into_iter().filter(move |(_, account)| {
⋮----
.iter()
.all(|filter_type| filter_allows(filter_type, account))
⋮----
let accounts = if is_known_spl_token_id(&params.pubkey)
⋮----
let accounts = get_parsed_token_accounts(bank, keyed_accounts);
⋮----
let accounts = keyed_accounts.map(move |(pubkey, account)| RpcKeyedAccount {
pubkey: pubkey.to_string(),
account: encode_ui_account(&pubkey, &account, encoding, None, None),
⋮----
fn filter_logs_results(
⋮----
let responses = logs.into_iter().flatten().map(|log| RpcLogsResponse {
signature: log.signature.to_string(),
err: log.result.err().map(Into::into),
⋮----
fn initial_last_notified_slot(
⋮----
let slot = if params.commitment.is_finalized() {
⋮----
.read()
.unwrap()
.highest_super_majority_root()
} else if params.commitment.is_confirmed() {
optimistically_confirmed_bank.read().unwrap().bank.slot()
⋮----
block_commitment_cache.read().unwrap().slot()
⋮----
let bank = bank_forks.read().unwrap().get(slot)?;
Some(bank.get_account_modified_slot(&params.pubkey)?.1)
⋮----
struct PubsubNotificationStats {
⋮----
impl PubsubNotificationStats {
fn maybe_submit(&mut self) {
⋮----
let elapsed = self.since.as_ref().map(Instant::elapsed);
if elapsed.unwrap_or(Duration::MAX) < SUBMIT_CADENCE {
⋮----
since: Some(Instant::now()),
⋮----
pub struct RpcSubscriptions {
⋮----
impl Drop for RpcSubscriptions {
fn drop(&mut self) {
self.shutdown().unwrap_or_else(|err| {
warn!("RPC Notification - shutdown error: {err:?}");
⋮----
impl RpcSubscriptions {
pub fn new(
⋮----
pub fn new_for_tests(
⋮----
let ledger_path = get_tmp_ledger_path!();
let blockstore = Blockstore::open(&ledger_path).unwrap();
⋮----
pub fn new_for_tests_with_blockstore(
⋮----
Some(rpc_notifier_ready.clone()),
⋮----
if rpc_notifier_ready.load(Ordering::Relaxed) {
⋮----
} else if (Instant::now() - start_time).as_millis() > 5000 {
panic!("RPC notifier thread setup took too long");
⋮----
pub fn new_with_config(
⋮----
let subscriptions = SubscriptionsTracker::new(bank_forks.clone());
⋮----
sender: broadcast_sender.clone(),
⋮----
let t_cleanup = config.notification_threads.map(|notification_threads| {
let exit = exit.clone();
⋮----
.name("solRpcNotifier".to_string())
.spawn(move || {
⋮----
.num_threads(notification_threads.get())
.thread_name(|i| format!("solRpcNotify{i:02}"))
.build()
.unwrap();
pool.install(|| {
⋮----
rpc_notifier_ready.fetch_or(true, Ordering::Relaxed);
⋮----
notification_sender.clone(),
⋮----
notification_sender: config.notification_threads.map(|_| notification_sender),
⋮----
pub fn default_with_bank_forks(
⋮----
pub fn control(&self) -> &SubscriptionControl {
⋮----
pub fn notify_subscribers(&self, commitment_slots: CommitmentSlots) {
self.enqueue_notification(NotificationEntry::Bank(commitment_slots));
⋮----
pub fn notify_gossip_subscribers(&self, slot: Slot) {
self.enqueue_notification(NotificationEntry::Gossip(slot));
⋮----
pub fn notify_slot_update(&self, slot_update: SlotUpdate) {
self.enqueue_notification(NotificationEntry::SlotUpdate(slot_update));
⋮----
pub fn notify_slot(&self, slot: Slot, parent: Slot, root: Slot) {
self.enqueue_notification(NotificationEntry::Slot(SlotInfo { slot, parent, root }));
self.enqueue_notification(NotificationEntry::SlotUpdate(SlotUpdate::CreatedBank {
⋮----
timestamp: timestamp(),
⋮----
pub fn notify_signatures_received(&self, slot_signatures: (Slot, Vec<Signature>)) {
self.enqueue_notification(NotificationEntry::SignaturesReceived(slot_signatures));
⋮----
pub fn notify_vote(&self, vote_pubkey: Pubkey, vote: VoteTransaction, signature: Signature) {
self.enqueue_notification(NotificationEntry::Vote((vote_pubkey, vote, signature)));
⋮----
pub fn notify_roots(&self, mut rooted_slots: Vec<Slot>) {
rooted_slots.sort_unstable();
rooted_slots.into_iter().for_each(|root| {
self.enqueue_notification(NotificationEntry::SlotUpdate(SlotUpdate::Root {
⋮----
self.enqueue_notification(NotificationEntry::Root(root));
⋮----
fn enqueue_notification(&self, notification_entry: NotificationEntry) {
⋮----
match notification_sender.send(notification_entry.into()) {
⋮----
warn!("Dropped RPC Notification - receiver disconnected : {notification:?}");
⋮----
fn process_notifications(
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
match notification_receiver.recv_timeout(Duration::from_millis(RECEIVE_DELAY_MILLIS)) {
⋮----
subscriptions.subscribe(params.clone(), id, || {
initial_last_notified_slot(
⋮----
.unwrap_or(0)
⋮----
subscriptions.unsubscribe(params, id);
⋮----
.node_progress_watchers()
.get(&SubscriptionParams::Slot)
⋮----
debug!("slot notify: {slot_info:?}");
inc_new_counter_info!("rpc-subscription-notify-slot", 1);
notifier.notify(slot_info, sub, false);
⋮----
.get(&SubscriptionParams::SlotsUpdates)
⋮----
inc_new_counter_info!("rpc-subscription-notify-slots-updates", 1);
notifier.notify(slot_update, sub, false);
⋮----
.get(&SubscriptionParams::Vote)
⋮----
vote_pubkey: vote_pubkey.to_string(),
slots: vote_info.slots(),
hash: bs58::encode(vote_info.hash()).into_string(),
timestamp: vote_info.timestamp(),
signature: signature.to_string(),
⋮----
debug!("vote notify: {vote_info:?}");
inc_new_counter_info!("rpc-subscription-notify-vote", 1);
notifier.notify(&rpc_vote, sub, false);
⋮----
.get(&SubscriptionParams::Root)
⋮----
debug!("root notify: {root:?}");
inc_new_counter_info!("rpc-subscription-notify-root", 1);
notifier.notify(root, sub, false);
⋮----
max_complete_transaction_status_slot.clone(),
subscriptions.commitment_watchers(),
⋮----
subscriptions.gossip_watchers(),
⋮----
if let Some(subs) = subscriptions.by_signature().get(slot_signature)
⋮----
for subscription in subs.values() {
⋮----
subscription.params()
⋮----
error!("invalid params type in visit_by_signature");
⋮----
queued_at.elapsed().as_micros() as u64;
⋮----
warn!("RPC Notification thread - sender disconnected");
⋮----
stats.maybe_submit();
⋮----
fn notify_watchers(
⋮----
let subscriptions = subscriptions.into_par_iter();
subscriptions.for_each(|(_id, subscription)| {
let slot = if let Some(commitment) = subscription.commitment() {
if commitment.is_finalized() {
Some(commitment_slots.highest_super_majority_root)
} else if commitment.is_confirmed() {
Some(commitment_slots.highest_confirmed_slot)
⋮----
Some(commitment_slots.slot)
⋮----
error!("missing commitment in notify_watchers");
⋮----
match subscription.params() {
⋮----
num_accounts_found.fetch_add(1, Ordering::Relaxed);
⋮----
let notified = check_commitment_and_notify(
⋮----
|bank, params| bank.get_account_modified_slot(&params.pubkey),
⋮----
num_accounts_notified.fetch_add(1, Ordering::Relaxed);
⋮----
num_blocks_found.fetch_add(1, Ordering::Relaxed);
⋮----
// We're calling it unnotified in this context
⋮----
subscription.last_notified_slot.write().unwrap();
⋮----
(*w_last_unnotified_slot..slot).collect();
let ancestors = bank.proper_ancestors_set();
slots_to_notify.retain(|slot| ancestors.contains(slot));
slots_to_notify.push(slot);
⋮----
if s > max_complete_transaction_status_slot.load(Ordering::SeqCst) {
⋮----
.get_complete_block(s, false)
.map_err(|e| {
error!("get_complete_block error: {e}");
⋮----
.and_then(|block| filter_block_result_txs(block, s, params));
⋮----
num_blocks_notified.fetch_add(1, Ordering::Relaxed);
⋮----
err: Some(err),
⋮----
num_logs_found.fetch_add(1, Ordering::Relaxed);
⋮----
num_logs_notified.fetch_add(1, Ordering::Relaxed);
⋮----
num_programs_found.fetch_add(1, Ordering::Relaxed);
⋮----
bank.get_program_accounts_modified_since_parent(&params.pubkey)
⋮----
num_programs_notified.fetch_add(1, Ordering::Relaxed);
⋮----
num_signatures_found.fetch_add(1, Ordering::Relaxed);
⋮----
bank.get_signature_status_processed_since_parent(&params.signature)
⋮----
num_signatures_notified.fetch_add(1, Ordering::Relaxed);
⋮----
_ => error!("wrong subscription type in alps map"),
⋮----
total_time.stop();
let total_notified = num_accounts_notified.load(Ordering::Relaxed)
+ num_logs_notified.load(Ordering::Relaxed)
+ num_programs_notified.load(Ordering::Relaxed)
+ num_signatures_notified.load(Ordering::Relaxed);
let total_ms = total_time.as_ms();
⋮----
debug!(
⋮----
fn shutdown(&mut self) -> std::thread::Result<()> {
if self.t_cleanup.is_some() {
info!("RPC Notification thread - shutting down");
self.exit.store(true, Ordering::Relaxed);
let x = self.t_cleanup.take().unwrap().join();
info!("RPC Notification thread - shut down.");
⋮----
warn!("RPC Notification thread - already shut down.");
Ok(())
⋮----
fn total(&self) -> usize {
self.control.total()
⋮----
pub(crate) mod tests {
⋮----
struct AccountResult {
⋮----
fn make_account_result(
⋮----
json!({
⋮----
fn test_check_account_subscribe() {
⋮----
} = create_genesis_config(100);
⋮----
let blockhash = bank.last_blockhash();
⋮----
let bank0 = bank_forks.read().unwrap().get(0).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank1);
⋮----
bank_forks.clone(),
⋮----
let expected0 = make_account_result(
⋮----
system_instruction::transfer(&alice.pubkey(), &mint_keypair.pubkey(), 1);
let message = Message::new(&[instruction], Some(&mint_keypair.pubkey()));
⋮----
let expected1 = make_account_result(
⋮----
let expected2 = make_account_result(
⋮----
let subscribe_cases = vec![
⋮----
.account_subscribe(
pubkey.to_string(),
Some(RpcAccountInfoConfig {
commitment: Some(CommitmentConfig::processed()),
⋮----
.assert_subscribed(&SubscriptionParams::Account(AccountSubscriptionParams {
⋮----
rpc.block_until_processed(&subscriptions);
⋮----
.get(1)
⋮----
.process_transaction(&tx)
⋮----
subscriptions.notify_subscribers(commitment_slots);
let response = receiver.recv();
assert_eq!(
⋮----
rpc.account_unsubscribe(sub_id).unwrap();
⋮----
.assert_unsubscribed(&SubscriptionParams::Account(AccountSubscriptionParams {
⋮----
fn test_check_confirmed_block_subscribe() {
⋮----
} = create_genesis_config(10_000);
⋮----
let rent_exempt_amount = bank.get_minimum_balance_for_rent_exemption(0);
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
blockstore.clone(),
⋮----
commitment: Some(CommitmentConfig::confirmed()),
encoding: Some(UiTransactionEncoding::Json),
transaction_details: Some(TransactionDetails::Signatures),
⋮----
commitment: config.commitment.unwrap(),
encoding: config.encoding.unwrap(),
transaction_details: config.transaction_details.unwrap(),
show_rewards: config.show_rewards.unwrap_or_default(),
⋮----
let sub_id = rpc.block_subscribe(filter, Some(config)).unwrap();
⋮----
.assert_subscribed(&SubscriptionParams::Block(params.clone()));
let bank = bank_forks.read().unwrap().working_bank();
⋮----
let max_complete_transaction_status_slot = Arc::new(AtomicU64::new(blockstore.max_root()));
bank.transfer(rent_exempt_amount, &mint_keypair, &keypair2.pubkey())
⋮----
populate_blockstore_for_tests(
create_test_transaction_entries(
vec![&mint_keypair, &keypair1, &keypair2, &keypair3],
bank.clone(),
⋮----
subscriptions.notify_gossip_subscribers(slot);
let actual_resp = receiver.recv();
let actual_resp = serde_json::from_str::<serde_json::Value>(&actual_resp).unwrap();
⋮----
ConfirmedBlock::from(blockstore.get_complete_block(slot, false).unwrap());
⋮----
let expected_resp = json!({
⋮----
assert_eq!(expected_resp, actual_resp);
// should not trigger since commitment NOT set to finalized
subscriptions.notify_subscribers(CommitmentSlots {
⋮----
let should_err = receiver.recv_timeout(Duration::from_millis(300));
assert!(should_err.is_err());
rpc.slot_unsubscribe(sub_id).unwrap();
⋮----
.assert_unsubscribed(&SubscriptionParams::Block(params));
⋮----
fn test_check_confirmed_block_subscribe_with_mentions() {
⋮----
RpcBlockSubscribeFilter::MentionsAccountOrProgram(keypair1.pubkey().to_string());
⋮----
kind: BlockSubscriptionKind::MentionsAccountOrProgram(keypair1.pubkey()),
⋮----
// make sure it filtered out the other keypairs
⋮----
confirmed_block.transactions.retain(|tx_with_meta| {
⋮----
.account_keys()
⋮----
.any(|key| key == &keypair1.pubkey())
⋮----
fn test_check_finalized_block_subscribe() {
⋮----
commitment: Some(CommitmentConfig::finalized()),
⋮----
// should not trigger since commitment set to finalized
⋮----
fn test_check_program_subscribe() {
⋮----
.get(0)
⋮----
.program_subscribe(
stake::program::id().to_string(),
Some(RpcProgramAccountsConfig {
⋮----
.assert_subscribed(&SubscriptionParams::Program(ProgramSubscriptionParams {
⋮----
subscriptions.notify_subscribers(CommitmentSlots::default());
⋮----
let expected = json!({
⋮----
rpc.program_unsubscribe(sub_id).unwrap();
⋮----
.assert_unsubscribed(&SubscriptionParams::Program(ProgramSubscriptionParams {
⋮----
fn test_check_program_subscribe_for_missing_optimistically_confirmed_slot() {
// Testing if we can get the pubsub notification if a slot does not
// receive OptimisticallyConfirmed but its descendant slot get the confirmed
// notification.
⋮----
let bank1 = bank_forks.read().unwrap().get(1).unwrap();
// add account for alice and process the transaction at bank1
⋮----
bank1.process_transaction(&tx).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank2);
// add account for bob and process the transaction at bank2
⋮----
let bank2 = bank_forks.read().unwrap().get(2).unwrap();
bank2.process_transaction(&tx).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank3);
// add account for joe and process the transaction at bank3
⋮----
let bank3 = bank_forks.read().unwrap().get(3).unwrap();
bank3.process_transaction(&tx).unwrap();
// now add programSubscribe at the "confirmed" commitment level
⋮----
optimistically_confirmed_bank.clone(),
⋮----
// Optimistically notifying slot 3 without notifying slot 1 and 2, bank3 is unfrozen, we expect
// to see transaction for alice and bob to be notified in order.
⋮----
None, /* no dependency work */
⋮----
&None, // no dependency tracker
⋮----
// a closure to reduce code duplications in building expected responses:
⋮----
let expected = build_expected_resp(1, 1, &alice.pubkey().to_string(), 0);
⋮----
let expected = build_expected_resp(2, 2, &bob.pubkey().to_string(), 0);
⋮----
bank3.freeze();
⋮----
let expected = build_expected_resp(3, 3, &joe.pubkey().to_string(), 0);
⋮----
fn test_check_program_subscribe_for_missing_optimistically_confirmed_slot_with_no_banks_no_notifications(
⋮----
// notification with a bank in the BankForks. We are not expecting to receive any notifications -- should panic.
⋮----
rpc.program_subscribe(
⋮----
// Optimistically notifying slot 3 without notifying slot 1 and 2, bank3 is not in the bankforks, we do not
// expect to see any RPC notifications.
⋮----
// The following should panic
let _response = receiver.recv();
⋮----
fn test_check_program_subscribe_for_missing_optimistically_confirmed_slot_with_no_banks() {
⋮----
// notification. It differs from the test_check_program_subscribe_for_missing_optimistically_confirmed_slot
// test in that when the descendant get confirmed, the descendant does not have a bank yet.
⋮----
// Optimistically notifying slot 3 without notifying slot 1 and 2, bank3 is not in the bankforks, we expect
// to see transaction for alice and bob to be notified only when bank3 is added to the fork and
// frozen. The notifications should be in the increasing order of the slot.
⋮----
fn test_check_signature_subscribe() {
⋮----
system_transaction::transfer(&mint_keypair, &alice.pubkey(), 1, blockhash);
⋮----
system_transaction::transfer(&mint_keypair, &alice.pubkey(), 2, blockhash);
⋮----
system_transaction::transfer(&mint_keypair, &alice.pubkey(), 3, blockhash);
⋮----
.process_transaction(&past_bank_tx)
⋮----
bank_forks.read().unwrap().get(0).unwrap(),
⋮----
bank_forks.write().unwrap().insert(next_bank);
⋮----
.process_transaction(&processed_tx)
⋮----
let bank1 = bank_forks.read().unwrap().get(1).unwrap().clone();
⋮----
cache0.increase_confirmation_stake(1, 10);
⋮----
block_commitment.entry(0).or_insert(cache0);
block_commitment.entry(1).or_insert(cache1);
⋮----
slot: bank1.slot(),
⋮----
.signature_subscribe(
past_bank_tx.signatures[0].to_string(),
Some(RpcSignatureSubscribeConfig {
⋮----
enable_received_notification: Some(false),
⋮----
processed_tx.signatures[0].to_string(),
⋮----
unprocessed_tx.signatures[0].to_string(),
⋮----
// Add a subscription that gets `received` notifications
⋮----
enable_received_notification: Some(true),
⋮----
assert!(subscriptions
⋮----
.notify_signatures_received((received_slot, vec![unprocessed_tx.signatures[0]]));
⋮----
struct Notification {
⋮----
let json = json!({
⋮----
serde_json::to_string(&json).unwrap()
⋮----
// Expect to receive a notification from bank 1 because this subscription is
// looking for 0 confirmations and so checks the current bank
let expected = expected_notification(
⋮----
id: past_bank_sub_id1.into(),
⋮----
let response = past_bank_receiver1.recv();
assert_eq!(expected, response);
// Expect to receive a notification from bank 0 because this subscription is
// looking for 1 confirmation and so checks the past bank
⋮----
id: past_bank_sub_id2.into(),
⋮----
let response = past_bank_receiver2.recv();
⋮----
id: processed_sub_id.into(),
⋮----
let response = processed_receiver.recv();
⋮----
// Expect a "received" notification
⋮----
id: processed_sub_id3.into(),
⋮----
let response = processed_receiver3.recv();
⋮----
// Subscription should be automatically removed after notification
assert!(!subscriptions
⋮----
// Unprocessed signature subscription should not be removed
⋮----
fn test_check_slot_subscribe() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let sub_id = rpc.slot_subscribe().unwrap();
⋮----
.assert_subscribed(&SubscriptionParams::Slot);
subscriptions.notify_slot(0, 0, 0);
⋮----
let expected_res_str = serde_json::to_string(&expected_res).unwrap();
let expected = format!(
⋮----
.assert_unsubscribed(&SubscriptionParams::Slot);
⋮----
fn test_check_root_subscribe() {
⋮----
let sub_id = rpc.root_subscribe().unwrap();
⋮----
.assert_subscribed(&SubscriptionParams::Root);
subscriptions.notify_roots(vec![2, 1, 3]);
⋮----
serde_json::to_string(&serde_json::to_value(expected_root).unwrap()).unwrap();
⋮----
rpc.root_unsubscribe(sub_id).unwrap();
⋮----
.assert_unsubscribed(&SubscriptionParams::Root);
⋮----
fn test_gossip_separate_account_notifications() {
⋮----
let bank1 = Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 1);
⋮----
alice.pubkey().to_string(),
⋮----
assert!(subscriptions.control.account_subscribed(&alice.pubkey()));
rpc0.block_until_processed(&subscriptions);
⋮----
// Add the transaction to the 1st bank and then freeze the bank
let bank1 = bank_forks.write().unwrap().get(1).unwrap();
⋮----
bank1.freeze();
// Add the same transaction to the unfrozen 2nd bank
⋮----
.get(2)
⋮----
// First, notify the unfrozen bank first to queue pending notification
⋮----
// Now, notify the frozen bank and ensure its notifications are processed
⋮----
let response = receiver0.recv();
⋮----
rpc0.account_unsubscribe(sub_id0).unwrap();
⋮----
rpc1.block_until_processed(&subscriptions);
⋮----
bank2.freeze();
⋮----
let response = receiver1.recv();
⋮----
rpc1.account_unsubscribe(sub_id1).unwrap();
assert!(!subscriptions.control.account_subscribed(&alice.pubkey()));
⋮----
fn make_logs_result(signature: &str, subscription_id: u64) -> serde_json::Value {
⋮----
fn test_logs_subscribe() {
⋮----
.logs_subscribe(RpcTransactionLogsFilter::All, Some(sub_config.clone()))
⋮----
assert!(subscriptions.control.logs_subscribed(None));
⋮----
.logs_subscribe(
RpcTransactionLogsFilter::Mentions(vec![alice.pubkey().to_string()]),
Some(sub_config),
⋮----
assert!(subscriptions.control.logs_subscribed(Some(&alice.pubkey())));
rpc_alice.block_until_processed(&subscriptions);
⋮----
assert!(bank_forks
⋮----
subscriptions.notify_subscribers(CommitmentSlots::new_from_slot(0));
⋮----
make_logs_result(&tx.signatures[0].to_string(), u64::from(sub_id_for_all));
let response_all = receiver_all.recv();
⋮----
make_logs_result(&tx.signatures[0].to_string(), u64::from(sub_id_for_alice));
let response_alice = receiver_alice.recv();
⋮----
rpc_all.logs_unsubscribe(sub_id_for_all).unwrap();
assert!(!subscriptions.control.logs_subscribed(None));
rpc_alice.logs_unsubscribe(sub_id_for_alice).unwrap();
assert!(!subscriptions.control.logs_subscribed(Some(&alice.pubkey())));
⋮----
fn test_total_subscriptions() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(100);
⋮----
.account_subscribe(Pubkey::default().to_string(), None)
⋮----
assert_eq!(subscriptions.total(), 1);
⋮----
.program_subscribe(Pubkey::default().to_string(), None)
⋮----
assert_eq!(subscriptions.total(), 2);
⋮----
.logs_subscribe(RpcTransactionLogsFilter::All, None)
⋮----
assert_eq!(subscriptions.total(), 3);
⋮----
.signature_subscribe(Signature::default().to_string(), None)
⋮----
assert_eq!(subscriptions.total(), 4);
⋮----
let sub_id5 = rpc5.slot_subscribe().unwrap();
assert_eq!(subscriptions.total(), 5);
⋮----
let sub_id6 = rpc6.vote_subscribe().unwrap();
assert_eq!(subscriptions.total(), 6);
⋮----
let sub_id7 = rpc7.root_subscribe().unwrap();
assert_eq!(subscriptions.total(), 7);
// Add duplicate account subscription, but it shouldn't increment the count.
⋮----
rpc8.account_unsubscribe(sub_id8).unwrap();
⋮----
rpc2.program_unsubscribe(sub_id2).unwrap();
⋮----
rpc3.logs_unsubscribe(sub_id3).unwrap();
⋮----
rpc4.signature_unsubscribe(sub_id4).unwrap();
⋮----
rpc5.slot_unsubscribe(sub_id5).unwrap();
⋮----
rpc6.vote_unsubscribe(sub_id6).unwrap();
⋮----
rpc7.root_unsubscribe(sub_id7).unwrap();
assert_eq!(subscriptions.total(), 0);

================
File: rpc/src/rpc.rs
================
use solana_runtime::installed_scheduler_pool::BankWithScheduler;
⋮----
mod transaction {
⋮----
pub mod account_resolver;
type RpcCustomResult<T> = std::result::Result<T, RpcCustomError>;
⋮----
fn new_response<T>(bank: &Bank, value: T) -> RpcResponse<T> {
⋮----
context: RpcResponseContext::new(bank.slot()),
⋮----
fn is_finalized(
⋮----
slot <= block_commitment_cache.highest_super_majority_root()
&& (blockstore.is_root(slot) || bank.status_cache_ancestors().contains(&slot))
⋮----
pub struct JsonRpcConfig {
⋮----
impl Default for JsonRpcConfig {
fn default() -> Self {
⋮----
impl JsonRpcConfig {
pub fn default_for_test() -> Self {
⋮----
pub struct RpcBigtableConfig {
⋮----
impl Default for RpcBigtableConfig {
⋮----
let bigtable_instance_name = solana_storage_bigtable::DEFAULT_INSTANCE_NAME.to_string();
let bigtable_app_profile_id = solana_storage_bigtable::DEFAULT_APP_PROFILE_ID.to_string();
⋮----
pub struct JsonRpcRequestProcessor {
⋮----
impl Metadata for JsonRpcRequestProcessor {}
impl JsonRpcRequestProcessor {
pub fn clone_without_bigtable(&self) -> JsonRpcRequestProcessor {
⋮----
..self.clone()
⋮----
fn get_bank_with_config(&self, config: RpcContextConfig) -> Result<Arc<Bank>> {
⋮----
let bank = self.bank(commitment);
⋮----
if bank.slot() < min_context_slot {
return Err(RpcCustomError::MinContextSlotNotReached {
context_slot: bank.slot(),
⋮----
.into());
⋮----
Ok(bank)
⋮----
fn check_if_transaction_history_enabled(&self) -> Result<()> {
⋮----
return Err(RpcCustomError::TransactionHistoryNotAvailable.into());
⋮----
Ok(())
⋮----
async fn calculate_non_circulating_supply(
⋮----
.spawn_blocking(move || calculate_non_circulating_supply(&bank))
⋮----
.expect("Failed to spawn blocking task")
⋮----
pub async fn get_filtered_indexed_accounts(
⋮----
let index_key = index_key.to_owned();
let program_id = program_id.to_owned();
⋮----
.spawn_blocking(move || {
bank.get_filtered_indexed_accounts(
⋮----
account.owner().eq(&program_id)
⋮----
.iter()
.all(|filter_type| filter_allows(filter_type, account))
⋮----
bank.byte_limit_for_scans(),
⋮----
fn bank_from_slot(&self, slot: Slot) -> Option<Arc<Bank>> {
debug!("bank_from_slot: {slot}");
let r_bank_forks = self.bank_forks.read().unwrap();
r_bank_forks.get(slot)
⋮----
fn bank(&self, commitment: Option<CommitmentConfig>) -> Arc<Bank> {
debug!("RPC commitment_config: {commitment:?}");
let commitment = commitment.unwrap_or_default();
if commitment.is_confirmed() {
⋮----
.read()
.unwrap()
⋮----
.clone();
debug!("RPC using optimistically confirmed slot: {:?}", bank.slot());
⋮----
.slot_with_commitment(commitment.commitment);
⋮----
debug!("RPC using the heaviest slot: {slot:?}");
⋮----
debug!("RPC using block: {slot:?}");
⋮----
CommitmentLevel::Confirmed => unreachable!(),
⋮----
r_bank_forks.get(slot).unwrap_or_else(|| {
warn!(
⋮----
r_bank_forks.root_bank()
⋮----
fn genesis_creation_time(&self) -> UnixTimestamp {
self.bank(None).genesis_creation_time()
⋮----
pub fn new(
⋮----
let (transaction_sender, transaction_receiver) = unbounded();
⋮----
pub fn new_from_bank<Client: ClientWithCreator>(
⋮----
use crate::rpc_service::service_runtime;
let genesis_hash = bank.hash();
⋮----
let bank = bank_forks.read().unwrap().root_bank();
let blockstore = Arc::new(Blockstore::open(&get_tmp_ledger_path!()).unwrap());
⋮----
&keypair.pubkey(),
⋮----
let runtime = service_runtime(rpc_threads, rpc_blocking_threads, rpc_niceness_adj);
⋮----
Some(runtime.handle().clone()),
cluster_info.clone(),
⋮----
exit.clone(),
⋮----
let slot = bank.slot();
⋮----
validator_exit: create_validator_exit(exit.clone()),
⋮----
pub async fn get_account_info(
⋮----
} = config.unwrap_or_default();
let bank = self.get_bank_with_config(RpcContextConfig {
⋮----
let encoding = encoding.unwrap_or(UiAccountEncoding::Binary);
⋮----
.spawn_blocking({
⋮----
move || get_encoded_account(&bank, &pubkey, encoding, data_slice, None)
⋮----
.expect("rpc: get_encoded_account panicked")?;
Ok(new_response(&bank, response))
⋮----
pub async fn get_multiple_accounts(
⋮----
let encoding = encoding.unwrap_or(UiAccountEncoding::Base64);
let mut accounts = Vec::with_capacity(pubkeys.len());
⋮----
accounts.push(
⋮----
get_encoded_account(&bank, &pubkey, encoding, data_slice, None)
⋮----
.expect("rpc: get_encoded_account panicked")?,
⋮----
Ok(new_response(&bank, accounts))
⋮----
pub fn get_minimum_balance_for_rent_exemption(
⋮----
self.bank(commitment)
.get_minimum_balance_for_rent_exemption(data_len)
⋮----
pub async fn get_program_accounts(
⋮----
optimize_filters(&mut filters);
⋮----
if let Some(owner) = get_spl_token_owner_filter(&program_id, &filters)? {
self.get_filtered_spl_token_accounts_by_owner(
⋮----
} else if let Some(mint) = get_spl_token_mint_filter(&program_id, &filters)? {
self.get_filtered_spl_token_accounts_by_mint(
⋮----
self.get_filtered_program_accounts(
⋮----
let accounts = if is_known_spl_token_id(&program_id)
⋮----
get_parsed_token_accounts(Arc::clone(&bank), keyed_accounts.into_iter()).collect()
⋮----
.into_iter()
.map(|(pubkey, account)| {
Ok(RpcKeyedAccount {
pubkey: pubkey.to_string(),
account: encode_account(&account, &pubkey, encoding, data_slice_config)?,
⋮----
Ok(match with_context {
true => OptionalContext::Context(new_response(&bank, accounts)),
⋮----
fn filter_map_rewards<'a, F>(
⋮----
.filter(|reward| addresses.contains(&reward.pubkey))
.map(move |reward| (reward.pubkey.clone(), (reward, slot)))
⋮----
fn filter_rewards<F>(
⋮----
.flatten()
.filter(move |reward| reward.reward_type.is_some_and(reward_type_filter))
⋮----
pub async fn get_inflation_reward(
⋮----
let config = config.unwrap_or_default();
let epoch_schedule = self.get_epoch_schedule();
let first_available_block = self.get_first_available_block().await;
⋮----
.get_epoch(self.get_slot(context_config)?)
.saturating_sub(1),
⋮----
// Rewards for this epoch are found in the first confirmed block of the next epoch
let first_slot_in_epoch = epoch_schedule.get_first_slot_in_epoch(epoch.saturating_add(1));
⋮----
if self.bigtable_ledger_storage.is_some() {
return Err(RpcCustomError::LongTermStorageSlotSkipped {
⋮----
return Err(RpcCustomError::BlockCleanedUp {
⋮----
.get_blocks_with_limit(first_slot_in_epoch, 1, Some(context_config))
⋮----
.first()
.ok_or(RpcCustomError::BlockNotAvailable {
⋮----
// Determine if partitioned epoch rewards were enabled for the desired
// epoch
let bank = self.get_bank_with_config(context_config)?;
// Get first block in the epoch
⋮----
.get_block(
⋮----
Some(RpcBlockConfig::rewards_with_commitment(config.commitment).into()),
⋮----
return Err(RpcCustomError::BlockNotAvailable {
⋮----
// If there is a gap in blockstore or long-term historical storage that
// includes the epoch boundary, the `get_blocks_with_limit()` call above
// will return the slot of the block at the end of that gap, not a
// legitimate epoch-boundary block. Therefore, verify that the parent of
// `epoch_boundary_block` occurred before the `first_slot_in_epoch`. If
// it didn't, return an error; it will be impossible to locate
⋮----
return Err(RpcCustomError::SlotNotEpochBoundary {
⋮----
let epoch_has_partitioned_rewards = epoch_boundary_block.num_reward_partitions.is_some();
⋮----
addresses.iter().map(|pubkey| pubkey.to_string()).collect();
⋮----
.collect()
⋮----
let num_partitions = epoch_boundary_block.num_reward_partitions.expect(
⋮----
.expect("num_partitions should never exceed usize::MAX");
⋮----
.expect("UiConfirmedBlock::previous_blockhash should be properly formed"),
⋮----
for address in addresses.iter() {
let address_string = address.to_string();
if !reward_map.contains_key(&address_string) {
let partition_index = hasher.clone().hash_address_to_partition(address);
⋮----
.entry(partition_index)
.and_modify(|list| list.push(address_string.clone()))
.or_insert(vec![address_string]);
⋮----
.get_blocks_with_limit(
⋮----
Some(context_config),
⋮----
for (partition_index, addresses) in partition_index_addresses.iter() {
let slot = *block_list.get(*partition_index).ok_or_else(|| {
⋮----
.map(|block_height| {
⋮----
.saturating_add(num_partitions as u64)
.saturating_add(1)
⋮----
.expect(
⋮----
slot: bank.slot(),
current_block_height: bank.block_height(),
⋮----
return Err(RpcCustomError::BlockNotAvailable { slot }.into());
⋮----
reward_map.extend(index_reward_map);
⋮----
.map(|address| {
if let Some((reward, slot)) = reward_map.get(&address.to_string()) {
return Some(RpcInflationReward {
⋮----
amount: reward.lamports.unsigned_abs(),
⋮----
.collect();
Ok(rewards)
⋮----
pub fn get_inflation_governor(
⋮----
self.bank(commitment).inflation().into()
⋮----
pub fn get_inflation_rate(&self) -> RpcInflationRate {
let bank = self.bank(None);
let epoch = bank.epoch();
let inflation = bank.inflation();
let slot_in_year = bank.slot_in_year_for_inflation();
⋮----
total: inflation.total(slot_in_year),
validator: inflation.validator(slot_in_year),
foundation: inflation.foundation(slot_in_year),
⋮----
pub fn get_epoch_schedule(&self) -> EpochSchedule {
let bank = self.bank(Some(CommitmentConfig::finalized()));
bank.epoch_schedule().clone()
⋮----
pub fn get_balance(
⋮----
let bank = self.get_bank_with_config(config)?;
Ok(new_response(&bank, bank.get_balance(pubkey)))
⋮----
pub fn confirm_transaction(
⋮----
let status = bank.get_signature_status(signature);
⋮----
Some(status) => Ok(new_response(&bank, status.is_ok())),
None => Ok(new_response(&bank, false)),
⋮----
fn get_block_commitment(&self, block: Slot) -> RpcBlockCommitment<BlockCommitmentArray> {
let r_block_commitment = self.block_commitment_cache.read().unwrap();
⋮----
.get_block_commitment(block)
.map(|block_commitment| block_commitment.commitment),
total_stake: r_block_commitment.total_stake(),
⋮----
fn get_slot(&self, config: RpcContextConfig) -> Result<Slot> {
⋮----
Ok(bank.slot())
⋮----
fn get_block_height(&self, config: RpcContextConfig) -> Result<u64> {
⋮----
Ok(bank.block_height())
⋮----
fn get_max_retransmit_slot(&self) -> Slot {
self.max_slots.retransmit.load(Ordering::Relaxed)
⋮----
fn get_max_shred_insert_slot(&self) -> Slot {
self.max_slots.shred_insert.load(Ordering::Relaxed)
⋮----
fn get_slot_leader(&self, config: RpcContextConfig) -> Result<String> {
⋮----
Ok(bank.collector_id().to_string())
⋮----
fn get_slot_leaders(
⋮----
bank.epoch_schedule().get_epoch_and_slot_index(start_slot);
⋮----
while slot_leaders.len() < limit {
⋮----
self.leader_schedule_cache.get_epoch_leader_schedule(epoch)
⋮----
slot_leaders.extend(
⋮----
.get_slot_leaders()
⋮----
.skip(slot_index as usize)
.take(limit.saturating_sub(slot_leaders.len())),
⋮----
return Err(Error::invalid_params(format!(
⋮----
Ok(slot_leaders)
⋮----
fn minimum_ledger_slot(&self) -> Result<Slot> {
match self.blockstore.slot_meta_iterator(0) {
Ok(mut metas) => match metas.next() {
Some((slot, _meta)) => Ok(slot),
None => Err(Error::invalid_request()),
⋮----
warn!("slot_meta_iterator failed: {err:?}");
Err(Error::invalid_request())
⋮----
fn get_transaction_count(&self, config: RpcContextConfig) -> Result<u64> {
⋮----
Ok(bank.transaction_count())
⋮----
fn get_cached_largest_accounts(
⋮----
let largest_accounts_cache = self.largest_accounts_cache.read().unwrap();
largest_accounts_cache.get_largest_accounts(filter)
⋮----
fn set_cached_largest_accounts(
⋮----
let mut largest_accounts_cache = self.largest_accounts_cache.write().unwrap();
largest_accounts_cache.set_largest_accounts(filter, slot, accounts)
⋮----
async fn get_largest_accounts(
⋮----
let bank = self.bank(config.commitment);
let sort_results = config.sort_results.unwrap_or(true);
if let Some((slot, accounts)) = self.get_cached_largest_accounts(&config.filter) {
Ok(RpcResponse {
⋮----
let (addresses, address_filter) = if let Some(filter) = config.clone().filter {
⋮----
.calculate_non_circulating_supply(&bank)
⋮----
.map_err(|e| RpcCustomError::ScanError {
message: e.to_string(),
⋮----
let addresses = non_circulating_supply.accounts.into_iter().collect();
⋮----
bank.get_largest_accounts(
⋮----
.map(|(address, lamports)| RpcAccountBalance {
address: address.to_string(),
⋮----
self.set_cached_largest_accounts(&config.filter, bank.slot(), &accounts);
⋮----
async fn get_supply(
⋮----
self.calculate_non_circulating_supply(&bank)
⋮----
let total_supply = bank.capitalization();
⋮----
vec![]
⋮----
.map(|pubkey| pubkey.to_string())
⋮----
Ok(new_response(
⋮----
fn get_vote_accounts(
⋮----
Some(verify_pubkey(vote_pubkey)?)
⋮----
let vote_accounts = bank.vote_accounts();
⋮----
.epoch_vote_accounts(bank.get_epoch_and_slot_index(bank.slot()).0)
.ok_or_else(Error::invalid_request)?;
⋮----
.unwrap_or(DELINQUENT_VALIDATOR_SLOT_DISTANCE);
⋮----
.filter_map(|(vote_pubkey, (activated_stake, account))| {
⋮----
let vote_state_view = account.vote_state_view();
let last_vote = vote_state_view.last_voted_slot().unwrap_or(0);
let num_epoch_credits = vote_state_view.num_epoch_credits();
⋮----
.epoch_credits_iter()
.skip(
⋮----
.saturating_sub(MAX_RPC_VOTE_ACCOUNT_INFO_EPOCH_CREDITS_HISTORY),
⋮----
.map(Into::into)
⋮----
Some(RpcVoteAccountInfo {
vote_pubkey: vote_pubkey.to_string(),
node_pubkey: vote_state_view.node_pubkey().to_string(),
⋮----
commission: vote_state_view.commission(),
root_slot: vote_state_view.root_slot().unwrap_or(0),
⋮----
epoch_vote_account: epoch_vote_accounts.contains_key(vote_pubkey),
⋮----
.partition(|vote_account_info| {
if bank.slot() >= delinquent_validator_slot_distance {
vote_account_info.last_vote > bank.slot() - delinquent_validator_slot_distance
⋮----
let keep_unstaked_delinquents = config.keep_unstaked_delinquents.unwrap_or_default();
⋮----
.filter(|vote_account_info| vote_account_info.activated_stake > 0)
⋮----
Ok(RpcVoteAccountStatus {
⋮----
fn check_blockstore_root<T>(
⋮----
debug!(
⋮----
if slot >= self.blockstore.max_root() {
⋮----
if self.blockstore.is_skipped(slot) {
return Err(RpcCustomError::SlotSkipped { slot }.into());
⋮----
fn check_slot_cleaned_up<T>(
⋮----
.get_first_available_block()
.unwrap_or_default();
⋮----
.into();
⋮----
return Err(err);
⋮----
fn check_bigtable_result<T>(
⋮----
return Err(RpcCustomError::LongTermStorageSlotSkipped { slot: *slot }.into());
⋮----
fn check_blockstore_writes_complete(&self, slot: Slot) -> Result<()> {
⋮----
.load(Ordering::SeqCst)
⋮----
Err(RpcCustomError::BlockStatusNotAvailableYet { slot }.into())
⋮----
pub async fn get_block(
⋮----
self.check_if_transaction_history_enabled()?;
⋮----
.map(|config| config.convert_to_current())
⋮----
let encoding = config.encoding.unwrap_or(UiTransactionEncoding::Json);
⋮----
transaction_details: config.transaction_details.unwrap_or_default(),
show_rewards: config.rewards.unwrap_or(true),
⋮----
let commitment = config.commitment.unwrap_or_default();
check_is_at_least_confirmed(commitment)?;
⋮----
.highest_super_majority_root()
⋮----
self.check_blockstore_writes_complete(slot)?;
⋮----
move || blockstore.get_rooted_block(slot, true)
⋮----
.expect("Failed to spawn blocking task");
self.check_blockstore_root(&result, slot)?;
⋮----
.encode_with_options(encoding, encoding_options)
.map_err(RpcCustomError::from)
⋮----
.expect("Failed to spawn blocking task")?;
⋮----
encoded_block.block_time = Some(self.genesis_creation_time());
encoded_block.block_height = Some(0);
⋮----
if result.is_err() {
⋮----
let bigtable_result = bigtable_ledger_storage.get_confirmed_block(slot).await;
self.check_bigtable_result(&bigtable_result)?;
⋮----
bigtable_result.ok().map(encode_block).into();
return encoded_block_future.await.transpose();
⋮----
self.check_slot_cleaned_up(&result, slot)?;
⋮----
.ok()
.map(ConfirmedBlock::from)
.map(encode_block)
⋮----
} else if commitment.is_confirmed() {
let confirmed_bank = self.bank(Some(CommitmentConfig::confirmed()));
if confirmed_bank.status_cache_ancestors().contains(&slot) {
⋮----
move || blockstore.get_complete_block(slot, true)
⋮----
.map(|mut confirmed_block| async move {
if confirmed_block.block_time.is_none()
|| confirmed_block.block_height.is_none()
⋮----
if let Some(bank) = r_bank_forks.get(slot) {
if confirmed_block.block_time.is_none() {
confirmed_block.block_time = Some(bank.clock().unix_timestamp);
⋮----
if confirmed_block.block_height.is_none() {
confirmed_block.block_height = Some(bank.block_height());
⋮----
Ok(encoded_block)
⋮----
Err(RpcCustomError::BlockNotAvailable { slot }.into())
⋮----
pub async fn get_blocks(
⋮----
.highest_super_majority_root();
let min_context_slot = config.min_context_slot.unwrap_or_default();
if commitment.is_finalized() && highest_super_majority_root < min_context_slot {
⋮----
let end_slot = min(
end_slot.unwrap_or_else(|| start_slot.saturating_add(MAX_GET_CONFIRMED_BLOCKS_RANGE)),
if commitment.is_finalized() {
⋮----
self.get_bank_with_config(config)?.slot()
⋮----
return Ok(vec![]);
⋮----
.get_confirmed_blocks(start_slot, (end_slot - start_slot) as usize + 1)
⋮----
.map(|mut bigtable_blocks| {
bigtable_blocks.retain(|&slot| slot <= end_slot);
⋮----
.map_err(|_| {
⋮----
.to_string(),
⋮----
.rooted_slot_iterator(max(start_slot, lowest_blockstore_slot))
.map_err(|_| Error::internal_error())?
.filter(|&slot| slot <= end_slot && slot <= highest_super_majority_root)
⋮----
.last()
.cloned()
.unwrap_or_else(|| start_slot.saturating_sub(1));
⋮----
let confirmed_bank = self.get_bank_with_config(config)?;
⋮----
.status_cache_ancestors()
⋮----
.filter(|&slot| slot <= end_slot && slot > last_element)
⋮----
blocks.append(&mut confirmed_blocks);
⋮----
Ok(blocks)
⋮----
pub async fn get_blocks_with_limit(
⋮----
return Ok(bigtable_ledger_storage
.get_confirmed_blocks(start_slot, limit)
⋮----
.unwrap_or_default());
⋮----
.take(limit)
.filter(|&slot| slot <= highest_super_majority_root)
⋮----
if blocks.len() < limit {
⋮----
.filter(|&slot| slot > last_element)
⋮----
blocks.truncate(limit);
⋮----
pub async fn get_block_time(&self, slot: Slot) -> Result<Option<UnixTimestamp>> {
⋮----
return Ok(Some(self.genesis_creation_time()));
⋮----
let result = self.blockstore.get_rooted_block_time(slot);
⋮----
return Ok(bigtable_result
⋮----
.and_then(|confirmed_block| confirmed_block.block_time));
⋮----
Ok(result.ok())
⋮----
Ok(Some(bank.clock().unix_timestamp))
⋮----
pub fn get_signature_confirmation_status(
⋮----
Ok(self
.get_transaction_status(signature, &bank)
.map(|transaction_status| {
⋮----
.unwrap_or(MAX_LOCKOUT_HISTORY + 1);
⋮----
pub fn get_signature_status(
⋮----
Ok(bank
.get_signature_status_slot(&signature)
.map(|(_, status)| status))
⋮----
pub async fn get_signature_statuses(
⋮----
.map(|x| x.search_transaction_history)
.unwrap_or(false);
⋮----
let bank = self.bank(Some(CommitmentConfig::processed()));
let mut statuses: Vec<Option<TransactionStatus>> = vec![];
⋮----
let status = if let Some(status) = self.get_transaction_status(signature, &bank) {
Some(status)
⋮----
.get_rooted_transaction_status(signature)
⋮----
.filter(|(slot, _status_meta)| {
⋮----
.map(|(slot, status_meta)| {
let err = status_meta.status.clone().err();
⋮----
confirmation_status: Some(TransactionConfirmationStatus::Finalized),
⋮----
.get_signature_status(&signature)
⋮----
.map(Some)
.unwrap_or(None)
⋮----
statuses.push(status);
⋮----
Ok(new_response(&bank, statuses))
⋮----
fn get_transaction_status(
⋮----
let (slot, status) = bank.get_signature_status_slot(&signature)?;
let optimistically_confirmed_bank = self.bank(Some(CommitmentConfig::confirmed()));
⋮----
optimistically_confirmed_bank.get_signature_status_slot(&signature);
let r_block_commitment_cache = self.block_commitment_cache.read().unwrap();
let confirmations = if r_block_commitment_cache.root() >= slot
&& is_finalized(&r_block_commitment_cache, bank, &self.blockstore, slot)
⋮----
.get_confirmation_count(slot)
.or(Some(0))
⋮----
let err = status.clone().err();
Some(TransactionStatus {
⋮----
confirmation_status: if confirmations.is_none() {
Some(TransactionConfirmationStatus::Finalized)
} else if optimistically_confirmed.is_some() {
Some(TransactionConfirmationStatus::Confirmed)
⋮----
Some(TransactionConfirmationStatus::Processed)
⋮----
pub async fn get_transaction(
⋮----
let highest_confirmed_slot = confirmed_bank.slot();
blockstore.get_complete_transaction(signature, highest_confirmed_slot)
⋮----
blockstore.get_rooted_transaction(signature)
⋮----
Ok(confirmed_tx_with_meta.encode(encoding, max_supported_transaction_version).map_err(RpcCustomError::from)?)
⋮----
match confirmed_transaction.unwrap_or(None) {
⋮----
if commitment.is_confirmed()
⋮----
.contains(&confirmed_transaction.slot)
⋮----
if confirmed_transaction.block_time.is_none() {
⋮----
.get(confirmed_transaction.slot)
.map(|bank| bank.clock().unix_timestamp);
⋮----
return Ok(Some(encode_transaction(confirmed_transaction)?));
⋮----
.get_confirmed_transaction(&signature)
⋮----
.map(encode_transaction)
.transpose();
⋮----
Ok(None)
⋮----
pub async fn get_signatures_for_address(
⋮----
let highest_slot = if commitment.is_confirmed() {
⋮----
confirmed_bank.slot()
⋮----
.get_confirmed_signatures_for_address2(address, highest_slot, before, until, limit)
.map_err(|err| Error::invalid_params(format!("{err}")))?;
⋮----
.map(|x| {
let mut item: RpcConfirmedTransactionStatusWithSignature = x.into();
⋮----
item.confirmation_status = Some(TransactionConfirmationStatus::Finalized);
⋮----
item.confirmation_status = Some(TransactionConfirmationStatus::Confirmed);
if item.block_time.is_none() {
⋮----
.get(item.slot)
⋮----
if results.len() < limit {
⋮----
if !results.is_empty() {
limit -= results.len();
bigtable_before = results.last().map(|x| x.signature);
⋮----
if found_before && bigtable_before.is_some() {
⋮----
.get_signature_status(&bigtable_before.unwrap())
⋮----
warn!("Failed to query Bigtable: {err:?}");
return Err(RpcCustomError::LongTermStorageUnreachable.into());
⋮----
.get_confirmed_signatures_for_address(
⋮----
bigtable_before.as_ref(),
until.as_ref(),
⋮----
results.iter().map(|result| result.signature).collect();
⋮----
if before != Some(bigtable_result.signature)
&& !results_set.contains(&bigtable_result.signature)
⋮----
results.push(bigtable_result);
⋮----
Ok(map_results(results))
⋮----
pub async fn get_first_available_block(&self) -> Slot {
⋮----
.unwrap_or(slot);
⋮----
pub fn get_token_account_balance(
⋮----
let account = bank.get_account(pubkey).ok_or_else(|| {
Error::invalid_params("Invalid param: could not find account".to_string())
⋮----
if !is_known_spl_token_id(account.owner()) {
return Err(Error::invalid_params(
"Invalid param: not a Token account".to_string(),
⋮----
let token_account = StateWithExtensions::<TokenAccount>::unpack(account.data())
.map_err(|_| Error::invalid_params("Invalid param: not a Token account".to_string()))?;
let mint = &Pubkey::from_str(&token_account.base.mint.to_string())
.expect("Token account mint should be convertible to Pubkey");
let (_, data) = get_mint_owner_and_additional_data(&bank, mint)?;
let balance = token_amount_to_ui_amount_v3(token_account.base.amount, &data);
Ok(new_response(&bank, balance))
⋮----
pub fn get_token_supply(
⋮----
let mint_account = bank.get_account(mint).ok_or_else(|| {
⋮----
if !is_known_spl_token_id(mint_account.owner()) {
⋮----
"Invalid param: not a Token mint".to_string(),
⋮----
let mint = StateWithExtensions::<Mint>::unpack(mint_account.data()).map_err(|_| {
Error::invalid_params("Invalid param: mint could not be unpacked".to_string())
⋮----
.map(|x| (*x, bank.clock().unix_timestamp))
.ok();
⋮----
let supply = token_amount_to_ui_amount_v3(
⋮----
Ok(new_response(&bank, supply))
⋮----
pub async fn get_token_largest_accounts(
⋮----
let (mint_owner, data) = get_mint_owner_and_additional_data(&bank, &mint)?;
if !is_known_spl_token_id(&mint_owner) {
⋮----
.get_filtered_spl_token_accounts_by_mint(
⋮----
vec![],
⋮----
let amount = StateWithExtensions::<TokenAccount>::unpack(account.data())
.map(|account| account.base.amount)
.unwrap_or(0);
⋮----
if token_balances.len() >= NUM_LARGEST_ACCOUNTS {
⋮----
.peek()
.expect("BinaryHeap::peek should succeed when len > 0");
⋮----
token_balances.pop();
⋮----
token_balances.push(Reverse(new_entry));
⋮----
.into_sorted_vec()
⋮----
.map(|Reverse((amount, address))| {
Ok(RpcTokenAccountBalance {
⋮----
amount: token_amount_to_ui_amount_v3(amount, &data),
⋮----
Ok(new_response(&bank, token_balances))
⋮----
pub async fn get_token_accounts_by_owner(
⋮----
let (token_program_id, mint) = get_token_program_id_and_mint(&bank, token_account_filter)?;
let mut filters = vec![];
⋮----
filters.push(RpcFilterType::Memcmp(Memcmp::new_raw_bytes(
⋮----
mint.to_bytes().into(),
⋮----
.get_filtered_spl_token_accounts_by_owner(
⋮----
get_parsed_token_accounts(bank.clone(), keyed_accounts.into_iter()).collect()
⋮----
pub async fn get_token_accounts_by_delegate(
⋮----
let mut filters = vec![
⋮----
filters.push(RpcFilterType::TokenAccountState);
⋮----
async fn get_filtered_program_accounts(
⋮----
.contains(&AccountIndex::ProgramId)
⋮----
if !self.config.account_indexes.include_key(&program_id) {
return Err(RpcCustomError::KeyExcludedFromSecondaryIndex {
index_key: program_id.to_string(),
⋮----
self.get_filtered_indexed_accounts(
⋮----
bank.get_filtered_program_accounts(
⋮----
async fn get_filtered_spl_token_accounts_by_owner(
⋮----
owner_key.to_bytes().into(),
⋮----
.contains(&AccountIndex::SplTokenOwner)
⋮----
if !self.config.account_indexes.include_key(&owner_key) {
⋮----
index_key: owner_key.to_string(),
⋮----
self.get_filtered_program_accounts(bank, program_id, filters, sort_results)
⋮----
async fn get_filtered_spl_token_accounts_by_mint(
⋮----
mint_key.to_bytes().into(),
⋮----
.contains(&AccountIndex::SplTokenMint)
⋮----
if !self.config.account_indexes.include_key(&mint_key) {
⋮----
index_key: mint_key.to_string(),
⋮----
fn get_latest_blockhash(&self, config: RpcContextConfig) -> Result<RpcResponse<RpcBlockhash>> {
⋮----
let blockhash = bank.last_blockhash();
⋮----
.get_blockhash_last_valid_block_height(&blockhash)
.expect("bank blockhash queue should contain blockhash");
⋮----
blockhash: blockhash.to_string(),
⋮----
fn is_blockhash_valid(
⋮----
let is_valid = bank.is_blockhash_valid(blockhash);
Ok(new_response(&bank, is_valid))
⋮----
fn get_stake_minimum_delegation(&self, config: RpcContextConfig) -> Result<RpcResponse<u64>> {
⋮----
.is_active(&agave_feature_set::stake_raise_minimum_delegation_to_1_sol::id()),
⋮----
Ok(new_response(&bank, stake_minimum_delegation))
⋮----
fn get_recent_prioritization_fees(
⋮----
.get_prioritization_fees(&pubkeys)
⋮----
.map(|(slot, prioritization_fee)| RpcPrioritizationFee {
⋮----
.collect())
⋮----
pub(crate) fn optimize_filters(filters: &mut [RpcFilterType]) {
filters.iter_mut().for_each(|filter_type| {
⋮----
if let Err(err) = compare.convert_to_raw_bytes() {
warn!("Invalid filter: bytes could not be decoded, {err}");
⋮----
pub(crate) fn verify_filters(filters: &[RpcFilterType]) -> Result<()> {
if filters.len() > MAX_GET_PROGRAM_ACCOUNT_FILTERS {
⋮----
verify_filter(filter)?;
⋮----
fn verify_filter(input: &RpcFilterType) -> Result<()> {
⋮----
.verify()
.map_err(|e| Error::invalid_params(format!("Invalid param: {e:?}")))
⋮----
pub fn verify_pubkey(input: &str) -> Result<Pubkey> {
⋮----
.parse()
⋮----
fn verify_hash(input: &str) -> Result<Hash> {
⋮----
fn verify_signature(input: &str) -> Result<Signature> {
⋮----
fn verify_token_account_filter(
⋮----
let mint = verify_pubkey(&mint_str)?;
Ok(TokenAccountsFilter::Mint(mint))
⋮----
let program_id = verify_pubkey(&program_id_str)?;
Ok(TokenAccountsFilter::ProgramId(program_id))
⋮----
fn verify_and_parse_signatures_for_address_params(
⋮----
let address = verify_pubkey(&address)?;
⋮----
.map(|ref before| verify_signature(before))
.transpose()?;
let until = until.map(|ref until| verify_signature(until)).transpose()?;
let limit = limit.unwrap_or(MAX_GET_CONFIRMED_SIGNATURES_FOR_ADDRESS2_LIMIT);
⋮----
Ok((address, before, until, limit))
⋮----
pub(crate) fn check_is_at_least_confirmed(commitment: CommitmentConfig) -> Result<()> {
if !commitment.is_at_least_confirmed() {
⋮----
fn get_encoded_account(
⋮----
let response = if is_known_spl_token_id(account.owner())
⋮----
get_parsed_token_account(bank, pubkey, account, overwrite_accounts)
⋮----
encode_account(&account, pubkey, encoding, data_slice)?
⋮----
Ok(Some(response))
⋮----
None => Ok(None),
⋮----
fn encode_account<T: ReadableAccount>(
⋮----
.map(|s| min(s.length, account.data().len().saturating_sub(s.offset)))
.unwrap_or(account.data().len())
⋮----
let message = format!(
⋮----
Err(error::Error {
⋮----
Ok(encode_ui_account(
⋮----
fn get_spl_token_owner_filter(
⋮----
if !is_known_spl_token_id(program_id) {
return Ok(None);
⋮----
RpcFilterType::DataSize(size) => data_size_filter = Some(*size),
⋮----
let offset = memcmp.offset();
if let Some(bytes) = memcmp.raw_bytes_as_ref() {
⋮----
memcmp_filter = Some(bytes);
⋮----
if bytes.len() == PUBKEY_BYTES {
owner_key = Pubkey::try_from(bytes).ok();
⋮----
if data_size_filter == Some(account_packed_len as u64)
|| memcmp_filter == Some(&[ACCOUNTTYPE_ACCOUNT])
⋮----
Ok(owner_key)
⋮----
debug!("spl_token program filters do not match by-owner index requisites");
⋮----
fn get_spl_token_mint_filter(
⋮----
mint = Pubkey::try_from(bytes).ok();
⋮----
Ok(mint)
⋮----
debug!("spl_token program filters do not match by-mint index requisites");
⋮----
fn get_token_program_id_and_mint(
⋮----
let (mint_owner, _) = get_mint_owner_and_additional_data(bank, &mint)?;
⋮----
Ok((mint_owner, Some(mint)))
⋮----
if is_known_spl_token_id(&program_id) {
Ok((program_id, None))
⋮----
Err(Error::invalid_params(
"Invalid param: unrecognized Token program id".to_string(),
⋮----
fn _send_transaction(
⋮----
.send(transaction_info)
.unwrap_or_else(|err| warn!("Failed to enqueue transaction: {err}"));
Ok(signature.to_string())
⋮----
pub mod rpc_minimal {
⋮----
pub trait Minimal {
⋮----
pub struct MinimalImpl;
impl Minimal for MinimalImpl {
type Metadata = JsonRpcRequestProcessor;
fn get_balance(
⋮----
debug!("get_balance rpc request received: {pubkey_str:?}");
let pubkey = verify_pubkey(&pubkey_str)?;
meta.get_balance(&pubkey, config.unwrap_or_default())
⋮----
fn get_epoch_info(
⋮----
debug!("get_epoch_info rpc request received");
let bank = meta.get_bank_with_config(config.unwrap_or_default())?;
Ok(bank.get_epoch_info())
⋮----
fn get_genesis_hash(&self, meta: Self::Metadata) -> Result<String> {
debug!("get_genesis_hash rpc request received");
Ok(meta.genesis_hash.to_string())
⋮----
fn get_health(&self, meta: Self::Metadata) -> Result<String> {
match meta.health.check() {
RpcHealthStatus::Ok => Ok("ok".to_string()),
RpcHealthStatus::Unknown => Err(RpcCustomError::NodeUnhealthy {
⋮----
.into()),
RpcHealthStatus::Behind { num_slots } => Err(RpcCustomError::NodeUnhealthy {
num_slots_behind: Some(num_slots),
⋮----
fn get_identity(&self, meta: Self::Metadata) -> Result<RpcIdentity> {
debug!("get_identity rpc request received");
Ok(RpcIdentity {
identity: meta.cluster_info.id().to_string(),
⋮----
fn get_slot(&self, meta: Self::Metadata, config: Option<RpcContextConfig>) -> Result<Slot> {
debug!("get_slot rpc request received");
meta.get_slot(config.unwrap_or_default())
⋮----
fn get_block_height(
⋮----
debug!("get_block_height rpc request received");
meta.get_block_height(config.unwrap_or_default())
⋮----
fn get_highest_snapshot_slot(&self, meta: Self::Metadata) -> Result<RpcSnapshotSlotInfo> {
debug!("get_highest_snapshot_slot rpc request received");
if meta.snapshot_config.is_none() {
return Err(RpcCustomError::NoSnapshot.into());
⋮----
.map(|snapshot_config| {
⋮----
.unwrap();
⋮----
.ok_or(RpcCustomError::NoSnapshot)?;
⋮----
Ok(RpcSnapshotSlotInfo {
⋮----
fn get_transaction_count(
⋮----
debug!("get_transaction_count rpc request received");
meta.get_transaction_count(config.unwrap_or_default())
⋮----
fn get_version(&self, _: Self::Metadata) -> Result<RpcVersionInfo> {
debug!("get_version rpc request received");
⋮----
Ok(RpcVersionInfo {
solana_core: version.to_string(),
feature_set: Some(version.feature_set),
⋮----
debug!("get_vote_accounts rpc request received");
meta.get_vote_accounts(config)
⋮----
fn get_leader_schedule(
⋮----
let (slot, maybe_config) = options.map(|options| options.unzip()).unwrap_or_default();
let config = maybe_config.or(config).unwrap_or_default();
⋮----
let _ = verify_pubkey(identity)?;
⋮----
let bank = meta.bank(config.commitment);
let slot = slot.unwrap_or_else(|| bank.slot());
let epoch = bank.epoch_schedule().get_epoch(slot);
debug!("get_leader_schedule rpc request received: {slot:?}");
Ok(meta
⋮----
.get_epoch_leader_schedule(epoch)
.map(|leader_schedule| {
⋮----
leader_schedule.get_slot_leaders().iter().enumerate(),
⋮----
schedule_by_identity.retain(|k, _| *k == identity);
⋮----
pub mod rpc_bank {
⋮----
pub trait BankData {
⋮----
pub struct BankDataImpl;
impl BankData for BankDataImpl {
⋮----
fn get_minimum_balance_for_rent_exemption(
⋮----
debug!("get_minimum_balance_for_rent_exemption rpc request received: {data_len:?}");
⋮----
return Err(Error::invalid_request());
⋮----
Ok(meta.get_minimum_balance_for_rent_exemption(data_len, commitment))
⋮----
fn get_inflation_governor(
⋮----
debug!("get_inflation_governor rpc request received");
Ok(meta.get_inflation_governor(commitment))
⋮----
fn get_inflation_rate(&self, meta: Self::Metadata) -> Result<RpcInflationRate> {
debug!("get_inflation_rate rpc request received");
Ok(meta.get_inflation_rate())
⋮----
fn get_epoch_schedule(&self, meta: Self::Metadata) -> Result<EpochSchedule> {
debug!("get_epoch_schedule rpc request received");
Ok(meta.get_epoch_schedule())
⋮----
fn get_slot_leader(
⋮----
debug!("get_slot_leader rpc request received");
meta.get_slot_leader(config.unwrap_or_default())
⋮----
debug!("get_slot_leaders rpc request received (start: {start_slot} limit: {limit})");
⋮----
.get_slot_leaders(None, start_slot, limit)?
⋮----
.map(|identity| identity.to_string())
⋮----
fn get_block_production(
⋮----
debug!("get_block_production rpc request received");
⋮----
Some(verify_pubkey(identity)?)
⋮----
bank.epoch_schedule().get_first_slot_in_epoch(bank.epoch()),
bank.slot(),
⋮----
let last_slot = range.last_slot.unwrap_or_else(|| bank.slot());
⋮----
let slot_history = bank.get_slot_history();
if first_slot < slot_history.oldest() {
⋮----
if last_slot > slot_history.newest() {
⋮----
let slot_leaders = meta.get_slot_leaders(
⋮----
last_slot.saturating_sub(first_slot) as usize + 1,
⋮----
let entry = block_production.entry(identity).or_default();
if slot_history.check(slot) == solana_slot_history::Check::Found {
⋮----
.map(|(k, v)| (k.to_string(), v))
.collect(),
⋮----
pub mod rpc_accounts {
⋮----
pub trait AccountsData {
⋮----
pub struct AccountsDataImpl;
impl AccountsData for AccountsDataImpl {
⋮----
fn get_account_info(
⋮----
debug!("get_account_info rpc request received: {pubkey_str:?}");
⋮----
meta.get_account_info(pubkey, config).await
⋮----
.boxed()
⋮----
fn get_multiple_accounts(
⋮----
.unwrap_or(MAX_MULTIPLE_ACCOUNTS);
if pubkey_strs.len() > max_multiple_accounts {
⋮----
.map(|pubkey_str| verify_pubkey(&pubkey_str))
⋮----
meta.get_multiple_accounts(pubkeys, config).await
⋮----
fn get_block_commitment(
⋮----
debug!("get_block_commitment rpc request received");
Ok(meta.get_block_commitment(block))
⋮----
fn get_token_account_balance(
⋮----
debug!("get_token_account_balance rpc request received: {pubkey_str:?}");
⋮----
meta.get_token_account_balance(&pubkey, commitment)
⋮----
fn get_token_supply(
⋮----
debug!("get_token_supply rpc request received: {mint_str:?}");
⋮----
meta.get_token_supply(&mint, commitment)
⋮----
pub mod rpc_accounts_scan {
⋮----
pub trait AccountsScan {
⋮----
pub struct AccountsScanImpl;
impl AccountsScan for AccountsScanImpl {
⋮----
fn get_program_accounts(
⋮----
debug!("get_program_accounts rpc request received: {program_id_str:?}");
⋮----
Some(config.account_config),
config.filters.unwrap_or_default(),
config.with_context.unwrap_or_default(),
config.sort_results.unwrap_or(true),
⋮----
(None, vec![], false, true)
⋮----
verify_filters(&filters)?;
meta.get_program_accounts(program_id, config, filters, with_context, sort_results)
⋮----
fn get_largest_accounts(
⋮----
debug!("get_largest_accounts rpc request received");
async move { Ok(meta.get_largest_accounts(config).await?) }.boxed()
⋮----
fn get_supply(
⋮----
debug!("get_supply rpc request received");
async move { Ok(meta.get_supply(config).await?) }.boxed()
⋮----
fn get_token_largest_accounts(
⋮----
debug!("get_token_largest_accounts rpc request received: {mint_str:?}");
⋮----
meta.get_token_largest_accounts(mint, commitment).await
⋮----
fn get_token_accounts_by_owner(
⋮----
debug!("get_token_accounts_by_owner rpc request received: {owner_str:?}");
⋮----
let owner = verify_pubkey(&owner_str)?;
let token_account_filter = verify_token_account_filter(token_account_filter)?;
meta.get_token_accounts_by_owner(owner, token_account_filter, config, true)
⋮----
fn get_token_accounts_by_delegate(
⋮----
debug!("get_token_accounts_by_delegate rpc request received: {delegate_str:?}");
⋮----
let delegate = verify_pubkey(&delegate_str)?;
⋮----
meta.get_token_accounts_by_delegate(delegate, token_account_filter, config, true)
⋮----
pub mod rpc_full {
⋮----
pub trait Full {
⋮----
pub struct FullImpl;
impl Full for FullImpl {
⋮----
fn get_recent_performance_samples(
⋮----
debug!("get_recent_performance_samples request received");
let limit = limit.unwrap_or(PERFORMANCE_SAMPLES_LIMIT);
⋮----
.get_recent_perf_samples(limit)
.map_err(|err| {
warn!("get_recent_performance_samples failed: {err:?}");
⋮----
.map(|(slot, sample)| rpc_perf_sample_from_perf_sample(slot, sample))
⋮----
fn get_cluster_nodes(&self, meta: Self::Metadata) -> Result<Vec<RpcContactInfo>> {
debug!("get_cluster_nodes rpc request received");
⋮----
let socket_addr_space = cluster_info.socket_addr_space();
let my_shred_version = cluster_info.my_shred_version();
Ok(cluster_info
.all_peers()
⋮----
.filter_map(|(contact_info, _)| {
if my_shred_version == contact_info.shred_version()
⋮----
.gossip()
.map(|addr| socket_addr_space.check(&addr))
.unwrap_or_default()
⋮----
cluster_info.get_node_version(contact_info.pubkey())
⋮----
(Some(version.to_string()), Some(version.feature_set))
⋮----
Some(RpcContactInfo {
pubkey: contact_info.pubkey().to_string(),
gossip: contact_info.gossip(),
⋮----
.tvu(Protocol::UDP)
.filter(|addr| socket_addr_space.check(addr)),
⋮----
.tpu(Protocol::UDP)
⋮----
.tpu(Protocol::QUIC)
⋮----
.tpu_forwards(Protocol::UDP)
⋮----
.tpu_forwards(Protocol::QUIC)
⋮----
.tpu_vote(Protocol::UDP)
⋮----
.serve_repair(Protocol::UDP)
⋮----
.rpc()
⋮----
.rpc_pubsub()
⋮----
shred_version: Some(my_shred_version),
⋮----
fn get_signature_statuses(
⋮----
if signature_strs.len() > MAX_GET_SIGNATURE_STATUSES_QUERY_ITEMS {
return Box::pin(future::err(Error::invalid_params(format!(
⋮----
let mut signatures: Vec<Signature> = vec![];
⋮----
match verify_signature(&signature_str) {
⋮----
signatures.push(signature);
⋮----
Box::pin(async move { meta.get_signature_statuses(signatures, config).await })
⋮----
fn get_max_retransmit_slot(&self, meta: Self::Metadata) -> Result<Slot> {
debug!("get_max_retransmit_slot rpc request received");
Ok(meta.get_max_retransmit_slot())
⋮----
fn get_max_shred_insert_slot(&self, meta: Self::Metadata) -> Result<Slot> {
debug!("get_max_shred_insert_slot rpc request received");
Ok(meta.get_max_shred_insert_slot())
⋮----
fn request_airdrop(
⋮----
debug!("request_airdrop rpc request received");
trace!(
⋮----
let faucet_addr = meta.config.faucet_addr.ok_or_else(Error::invalid_request)?;
⋮----
verify_hash(&blockhash)?
⋮----
bank.confirmed_last_blockhash()
⋮----
request_airdrop_transaction(&faucet_addr, &pubkey, lamports, blockhash).map_err(
⋮----
info!("request_airdrop_transaction failed: {err:?}");
⋮----
let wire_transaction = serialize(&transaction).map_err(|err| {
info!("request_airdrop: serialize error: {err:?}");
⋮----
let message_hash = transaction.message().hash();
let signature = if !transaction.signatures.is_empty() {
⋮----
return Err(RpcCustomError::TransactionSignatureVerificationFailure.into());
⋮----
_send_transaction(
⋮----
fn send_transaction(
⋮----
debug!("send_transaction rpc request received");
⋮----
let tx_encoding = encoding.unwrap_or(UiTransactionEncoding::Base58);
let binary_encoding = tx_encoding.into_binary_encoding().ok_or_else(|| {
Error::invalid_params(format!(
⋮----
Some(CommitmentConfig::processed())
⋮----
preflight_commitment.map(|commitment| CommitmentConfig { commitment })
⋮----
let preflight_bank = &*meta.get_bank_with_config(RpcContextConfig {
⋮----
let transaction = sanitize_transaction(
⋮----
preflight_bank.get_reserved_account_keys(),
⋮----
.is_active(&agave_feature_set::static_instruction_limit::id()),
⋮----
let blockhash = *transaction.message().recent_blockhash();
let message_hash = *transaction.message_hash();
let signature = *transaction.signature();
⋮----
.get_durable_nonce()
.map(|&pubkey| (pubkey, blockhash));
if durable_nonce_info.is_some() || (skip_preflight && last_valid_block_height == 0) {
last_valid_block_height = preflight_bank.block_height() + MAX_PROCESSING_AGE as u64;
⋮----
let verification_error = transaction.verify().err();
if verification_error.is_none() && !meta.config.skip_preflight_health_check {
⋮----
inc_new_counter_info!("rpc-send-tx_health-unknown", 1);
return Err(RpcCustomError::NodeUnhealthy {
⋮----
inc_new_counter_info!("rpc-send-tx_health-behind", 1);
⋮----
preflight_bank.simulate_transaction(&transaction, false)
⋮----
inc_new_counter_info!("rpc-send-tx_err-blockhash-not-found", 1);
⋮----
inc_new_counter_info!("rpc-send-tx_err-other", 1);
⋮----
return Err(RpcCustomError::SendTransactionPreflightFailure {
message: format!("Transaction simulation failed: {err}"),
⋮----
err: Some(err.into()),
logs: Some(logs),
⋮----
units_consumed: Some(units_consumed),
loaded_accounts_data_size: Some(loaded_accounts_data_size),
return_data: return_data.map(|return_data| return_data.into()),
⋮----
fn simulate_transaction(
⋮----
debug!("simulate_transaction rpc request received");
⋮----
let bank = &*meta.get_bank_with_config(RpcContextConfig {
⋮----
let recent_blockhash = bank.last_blockhash();
⋮----
.set_recent_blockhash(recent_blockhash);
⋮----
.get_blockhash_last_valid_block_height(&recent_blockhash)
⋮----
blockhash.replace(RpcBlockhash {
blockhash: recent_blockhash.to_string(),
⋮----
bank.get_reserved_account_keys(),
⋮----
transaction.verify().err()
⋮----
bank.simulate_transaction(&transaction, enable_cpi_recording)
⋮----
let account_keys = transaction.message().account_keys();
let number_of_accounts = account_keys.len();
⋮----
.unwrap_or(UiAccountEncoding::Base64);
⋮----
return Err(Error::invalid_params("base58 encoding not supported"));
⋮----
if config_accounts.addresses.len() > number_of_accounts {
⋮----
Some(vec![None; config_accounts.addresses.len()])
⋮----
post_simulation_accounts_map.insert(pubkey, data);
⋮----
Some(
⋮----
.map(|address_str| {
let pubkey = verify_pubkey(address_str)?;
get_encoded_account(
⋮----
Some(&post_simulation_accounts_map),
⋮----
let inner_instructions = inner_instructions.map(|info| {
map_inner_instructions(info)
.map(|converted| parse_ui_inner_instructions(converted, &account_keys))
⋮----
err: result.err().map(Into::into),
⋮----
pre_token_balances: pre_token_balances.map(|balances| {
balances.into_iter().map(|balance| solana_ledger::transaction_balances::svm_token_info_to_token_balance(balance).into()).collect()
⋮----
post_token_balances: post_token_balances.map(|balances| {
⋮----
loaded_addresses: Some(UiLoadedAddresses::from(&transaction.get_loaded_addresses())),
⋮----
fn simulate_bundle(
⋮----
if rpc_bundle_request.encoded_transactions.len() > MAX_BUNDLES_SIMULATED {
⋮----
debug!("simulate_bundle rpc request received");
let config = config.unwrap_or_else(|| RpcSimulateBundleConfig {
pre_execution_accounts_configs: vec![
⋮----
post_execution_accounts_configs: vec![
⋮----
if !(pre_execution_accounts_configs.len()
== rpc_bundle_request.encoded_transactions.len()
&& post_execution_accounts_configs.len()
== rpc_bundle_request.encoded_transactions.len())
⋮----
for config in pre_execution_accounts_configs.iter() {
⋮----
.as_ref()
.and_then(|c| c.encoding)
.unwrap_or(UiAccountEncoding::Base64)
⋮----
for config in post_execution_accounts_configs.iter() {
⋮----
let tx_encoding = transaction_encoding.unwrap_or(UiTransactionEncoding::Base64);
⋮----
.map(|encoded_tx| {
⋮----
.map(|de| de.1)
⋮----
for tx in unsanitized_txs.iter() {
if !packet_hashes.insert(tx.message.hash()) {
return Err(Error::invalid_params("duplicate transactions"));
⋮----
let bank = match simulation_bank.unwrap_or_default() {
SimulationSlotConfig::Commitment(commitment) => Ok(meta.bank(Some(commitment))),
SimulationSlotConfig::Slot(slot) => meta.bank_from_slot(slot).ok_or_else(|| {
Error::invalid_params(format!("bank not found for the provided slot: {slot}"))
⋮----
SimulationSlotConfig::Tip => Ok(meta.bank_forks.read().unwrap().working_bank()),
⋮----
let max_accounts = bank.get_transaction_account_lock_limit();
if pre_execution_accounts_configs.iter().any(|config| {
⋮----
config.addresses.len() > max_accounts
⋮----
if post_execution_accounts_configs.iter().any(|config| {
⋮----
unsanitized_txs.iter_mut().for_each(|tx| {
tx.message.set_recent_blockhash(recent_blockhash);
⋮----
.map(|unsanitized_tx| {
sanitize_transaction(
⋮----
bank.as_ref(),
⋮----
if let Err(e) = tx.verify() {
⋮----
account_configs_to_accounts(&pre_execution_accounts_configs)?;
⋮----
account_configs_to_accounts(&post_execution_accounts_configs)?;
let results = bank.simulate_transactions_unchecked_with_pre_accounts(
⋮----
Some(1_000),
⋮----
.zip(results.iter())
.find(|(_tx, (_pre_accounts, result, _post_accounts))| result.result.is_err())
⋮----
*tx.signature(),
result.result.as_ref().err().unwrap().to_string(),
⋮----
tx_signature: Some(tx.signature().to_string()),
⋮----
.zip(results.into_iter())
.map(|(_tx, (pre_accounts, result, post_accounts))| {
Ok(RpcSimulateBundleTransactionResult {
err: result.result.err(),
logs: Some(result.logs),
pre_execution_accounts: Some(
⋮----
.map(|(address, data)| {
encode_account(
⋮----
post_execution_accounts: Some(
⋮----
units_consumed: Some(result.units_consumed),
return_data: result.return_data.map(|return_data| return_data.into()),
⋮----
Ok(new_response(&bank, result))
⋮----
fn minimum_ledger_slot(&self, meta: Self::Metadata) -> Result<Slot> {
debug!("minimum_ledger_slot rpc request received");
meta.minimum_ledger_slot()
⋮----
fn get_block(
⋮----
debug!("get_block rpc request received: {slot:?}");
Box::pin(async move { meta.get_block(slot, config).await })
⋮----
fn get_blocks(
⋮----
wrapper.map(|wrapper| wrapper.unzip()).unwrap_or_default();
debug!("get_blocks rpc request received: {start_slot}-{end_slot:?}");
⋮----
meta.get_blocks(start_slot, end_slot, config.or(maybe_config))
⋮----
fn get_blocks_with_limit(
⋮----
debug!("get_blocks_with_limit rpc request received: {start_slot}-{limit}",);
Box::pin(async move { meta.get_blocks_with_limit(start_slot, limit, config).await })
⋮----
fn get_block_time(
⋮----
Box::pin(async move { meta.get_block_time(slot).await })
⋮----
fn get_transaction(
⋮----
debug!("get_transaction rpc request received: {signature_str:?}");
let signature = verify_signature(&signature_str);
⋮----
Box::pin(async move { meta.get_transaction(signature.unwrap(), config).await })
⋮----
fn get_signatures_for_address(
⋮----
verify_and_parse_signatures_for_address_params(address, before, until, limit);
⋮----
meta.get_signatures_for_address(
⋮----
fn get_first_available_block(&self, meta: Self::Metadata) -> BoxFuture<Result<Slot>> {
debug!("get_first_available_block rpc request received");
Box::pin(async move { Ok(meta.get_first_available_block().await) })
⋮----
fn get_inflation_reward(
⋮----
let mut addresses: Vec<Pubkey> = vec![];
⋮----
match verify_pubkey(&address_str) {
⋮----
addresses.push(pubkey);
⋮----
Box::pin(async move { meta.get_inflation_reward(addresses, config).await })
⋮----
fn get_latest_blockhash(
⋮----
debug!("get_latest_blockhash rpc request received");
meta.get_latest_blockhash(config.unwrap_or_default())
⋮----
Hash::from_str(&blockhash).map_err(|e| Error::invalid_params(format!("{e:?}")))?;
meta.is_blockhash_valid(&blockhash, config.unwrap_or_default())
⋮----
fn get_fee_for_message(
⋮----
debug!("get_fee_for_message rpc request received");
⋮----
let bank = &*meta.get_bank_with_config(config.unwrap_or_default())?;
⋮----
Error::invalid_params(format!("invalid transaction message: {err}"))
⋮----
.map_err(|err| Error::invalid_params(format!("invalid transaction message: {err}")))?;
let fee = bank.get_fee_for_message(&sanitized_message);
Ok(new_response(bank, fee))
⋮----
fn get_stake_minimum_delegation(
⋮----
debug!("get_stake_minimum_delegation rpc request received");
meta.get_stake_minimum_delegation(config.unwrap_or_default())
⋮----
let pubkey_strs = pubkey_strs.unwrap_or_default();
⋮----
if pubkey_strs.len() > MAX_TX_ACCOUNT_LOCKS {
⋮----
meta.get_recent_prioritization_fees(pubkeys)
⋮----
fn rpc_perf_sample_from_perf_sample(slot: u64, sample: PerfSample) -> RpcPerfSample {
⋮----
num_non_vote_transactions: Some(num_non_vote_transactions),
⋮----
fn decode_and_deserialize<T>(
⋮----
inc_new_counter_info!("rpc-base58_encoded_tx", 1);
if encoded.len() > MAX_BASE58_SIZE {
⋮----
.into_vec()
.map_err(|e| Error::invalid_params(format!("invalid base58 encoding: {e:?}")))?
⋮----
inc_new_counter_info!("rpc-base64_encoded_tx", 1);
if encoded.len() > MAX_BASE64_SIZE {
⋮----
.decode(encoded)
.map_err(|e| Error::invalid_params(format!("invalid base64 encoding: {e:?}")))?
⋮----
if wire_output.len() > PACKET_DATA_SIZE {
⋮----
.with_limit(PACKET_DATA_SIZE as u64)
.with_fixint_encoding()
.allow_trailing_bytes()
.deserialize_from(&wire_output[..])
⋮----
.map(|output| (wire_output, output))
⋮----
fn sanitize_transaction(
⋮----
.map_err(|err| Error::invalid_params(format!("invalid transaction: {err}")))
⋮----
pub fn account_configs_to_accounts(
⋮----
accounts.push(Pubkey::from_str(address).map_err(|_| {
⋮----
execution_accounts.push(accounts);
⋮----
Ok(execution_accounts)
⋮----
pub fn create_validator_exit(exit: Arc<AtomicBool>) -> Arc<RwLock<Exit>> {
⋮----
validator_exit.register_exit(Box::new(move || exit.store(true, Ordering::Relaxed)));
⋮----
pub fn create_test_transaction_entries(
⋮----
let blockhash = bank.confirmed_last_blockhash();
let rent_exempt_amount = bank.get_minimum_balance_for_rent_exemption(0);
⋮----
&keypair1.pubkey(),
⋮----
signatures.push(success_tx.signatures[0]);
let entry_1 = solana_entry::entry::next_entry(&blockhash, 1, vec![success_tx]);
⋮----
&keypair3.pubkey(),
⋮----
signatures.push(ix_error_tx.signatures[0]);
let entry_2 = solana_entry::entry::next_entry(&entry_1.hash, 1, vec![ix_error_tx]);
(vec![entry_1, entry_2], signatures)
⋮----
pub fn populate_blockstore_for_tests(
⋮----
let parent_slot = bank.parent_slot();
⋮----
blockstore.insert_shreds(shreds, None, false).unwrap();
blockstore.set_roots(std::iter::once(&slot)).unwrap();
let (transaction_status_sender, transaction_status_receiver) = unbounded();
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
tss_exit.clone(),
⋮----
assert_eq!(
⋮----
transaction_status_service.quiesce_and_join_for_tests(tss_exit);
⋮----
pub mod tests {
⋮----
pub(crate) fn new_test_cluster_info() -> ClusterInfo {
⋮----
fn create_test_request(method: &str, params: Option<serde_json::Value>) -> serde_json::Value {
json!({
⋮----
fn parse_success_result<T: DeserializeOwned>(response: Response) -> T {
⋮----
Output::Success(success) => serde_json::from_value(success.result).unwrap(),
⋮----
panic!("Expected success but received: {failure:?}");
⋮----
panic!("Expected single response");
⋮----
fn parse_failure_response(response: Response) -> (i64, String) {
⋮----
panic!("Expected failure but received: {success:?}");
⋮----
Output::Failure(failure) => (failure.error.code.code(), failure.error.message),
⋮----
fn expected_loaded_accounts_data_size(bank: &Bank, tx: &Transaction) -> u32 {
⋮----
for key in tx.message.account_keys.iter() {
if let Some(account) = bank.get_account(key) {
assert!(
⋮----
(account.data().len() + TRANSACTION_ACCOUNT_BASE_SIZE) as u32;
⋮----
fn test_builtin_processor(
⋮----
let log_collector = invoke_context.get_log_collector();
invoke_context.consume_checked(TestBuiltinEntrypoint::COMPUTE_UNITS)?;
⋮----
let instruction_context = transaction_context.get_current_instruction_context()?;
let instruction_data = instruction_context.get_instruction_data();
⋮----
let (l_bytes, s_bytes) = instruction_data.split_at(8);
let lamports = u64::from_le_bytes(l_bytes.try_into().unwrap());
let space = u64::from_le_bytes(s_bytes.try_into().unwrap());
⋮----
ic_logger_msg!(log_collector, "I am logging from a builtin program!");
ic_logger_msg!(log_collector, "I am about to CPI to System!");
let from_pubkey = *instruction_context.get_key_of_instruction_account(0)?;
let to_pubkey = *instruction_context.get_key_of_instruction_account(1)?;
let owner_pubkey = *instruction_context.get_key_of_instruction_account(2)?;
invoke_context.native_invoke(
⋮----
ic_logger_msg!(log_collector, "All done!");
Ok(0)
⋮----
declare_builtin_function!(
⋮----
impl TestBuiltinEntrypoint {
⋮----
fn cache_entry() -> ProgramCacheEntry {
ProgramCacheEntry::new_builtin(0, Self::NAME.len(), Self::vm)
⋮----
fn instruction(
⋮----
let mut data = vec![0; 16];
data[0..8].copy_from_slice(&lamports.to_le_bytes());
data[8..16].copy_from_slice(&space.to_le_bytes());
⋮----
vec![
⋮----
struct RpcHandler {
⋮----
impl RpcHandler {
fn start() -> Self {
⋮----
fn start_with_config(config: JsonRpcConfig) -> Self {
⋮----
new_bank_forks_with_config(BankTestConfig {
⋮----
account_indexes: Some(config.account_indexes.clone()),
⋮----
let ledger_path = get_tmp_ledger_path!();
let blockstore = Arc::new(Blockstore::open(&ledger_path).unwrap());
let bank = bank_forks.read().unwrap().working_bank();
let leader_pubkey = *bank.collector_id();
⋮----
let validator_exit = create_validator_exit(exit);
let cluster_info = Arc::new(new_test_cluster_info());
let identity = cluster_info.id();
cluster_info.insert_info(ContactInfo::new_with_socketaddr(
⋮----
&socketaddr!(Ipv4Addr::LOCALHOST, 1234),
⋮----
bank_forks.clone(),
block_commitment_cache.clone(),
blockstore.clone(),
⋮----
RpcHealth::stub(optimistically_confirmed_bank.clone(), blockstore.clone()),
⋮----
max_slots.clone(),
⋮----
max_complete_transaction_status_slot.clone(),
⋮----
service_runtime(rpc_threads, rpc_blocking_threads, rpc_niceness_adj),
⋮----
io.extend_with(rpc_minimal::MinimalImpl.to_delegate());
io.extend_with(rpc_bank::BankDataImpl.to_delegate());
io.extend_with(rpc_accounts::AccountsDataImpl.to_delegate());
io.extend_with(rpc_accounts_scan::AccountsScanImpl.to_delegate());
io.extend_with(rpc_full::FullImpl.to_delegate());
⋮----
fn handle_request_sync(&self, req: serde_json::Value) -> Response {
⋮----
.handle_request_sync(&req.to_string(), self.meta.clone())
.expect("no response");
serde_json::from_str(response).expect("failed to deserialize response")
⋮----
fn overwrite_working_bank_entries(&self, entries: Vec<Entry>) {
populate_blockstore_for_tests(
⋮----
self.working_bank(),
self.blockstore.clone(),
self.max_complete_transaction_status_slot.clone(),
⋮----
fn create_test_transactions_and_populate_blockstore(&self) -> Vec<Signature> {
⋮----
let bank = self.working_bank();
⋮----
bank.transfer(
⋮----
&keypair2.pubkey(),
⋮----
let (entries, signatures) = create_test_transaction_entries(
vec![&self.mint_keypair, &keypair1, &keypair2, &keypair3],
⋮----
self.overwrite_working_bank_entries(entries);
⋮----
fn create_test_versioned_transactions_and_populate_blockstore(
⋮----
address_table_key.unwrap_or_else(|| self.store_address_lookup_table());
⋮----
let recent_blockhash = bank.confirmed_last_blockhash();
⋮----
account_keys: vec![self.mint_keypair.pubkey()],
instructions: vec![],
⋮----
address_table_lookups: vec![MessageAddressTableLookup {
⋮----
VersionedTransaction::try_new(legacy_message, &[&self.mint_keypair]).unwrap();
signatures.push(legacy_tx.signatures[0]);
⋮----
VersionedTransaction::try_new(version_0_message, &[&self.mint_keypair]).unwrap();
signatures.push(version_0_tx.signatures[0]);
let entry1 = next_versioned_entry(&recent_blockhash, 1, vec![legacy_tx]);
let entry2 = next_versioned_entry(&entry1.hash, 1, vec![version_0_tx]);
let entries = vec![entry1, entry2];
⋮----
fn store_address_lookup_table(&self) -> Pubkey {
⋮----
addresses: Cow::Owned(vec![Pubkey::new_unique()]),
⋮----
let address_table_data = address_table_state.serialize_for_tests().unwrap();
⋮----
bank.get_minimum_balance_for_rent_exemption(address_table_data.len());
⋮----
bank.store_account(&address_table_pubkey, &address_table_account);
⋮----
fn add_roots_to_blockstore(&self, mut roots: Vec<Slot>) {
roots.retain(|&slot| slot > 0);
if roots.is_empty() {
⋮----
let mut parent_bank = self.bank_forks.read().unwrap().working_bank();
for (i, root) in roots.iter().enumerate() {
⋮----
Bank::new_from_parent(parent_bank.clone(), parent_bank.collector_id(), *root);
⋮----
.write()
⋮----
.insert(new_bank)
.clone_without_scheduler();
⋮----
fill_blockstore_slot_with_ticks(
⋮----
self.blockstore.set_roots(roots.iter()).unwrap();
⋮----
parent_bank.clone(),
parent_bank.collector_id(),
roots.iter().max().unwrap() + 1,
⋮----
self.bank_forks.write().unwrap().insert(new_bank);
for root in roots.iter() {
⋮----
.set_root(*root, None, Some(0));
⋮----
.get(*root)
⋮----
.clock()
⋮----
self.blockstore.set_block_time(*root, block_time).unwrap();
⋮----
fn advance_bank_to_confirmed_slot(&self, slot: Slot) -> Arc<Bank> {
let parent_bank = self.working_bank();
⋮----
.insert(Bank::new_from_parent(parent_bank, &Pubkey::default(), slot))
⋮----
CommitmentSlots::new_from_slot(self.bank_forks.read().unwrap().highest_slot()),
⋮----
*self.block_commitment_cache.write().unwrap() = new_block_commitment;
⋮----
fn store_vote_account(&self, vote_pubkey: &Pubkey, vote_state: VoteStateV4) {
⋮----
let balance = bank.get_minimum_balance_for_rent_exemption(space);
⋮----
vote_account.set_state(&versioned).unwrap();
bank.store_account(vote_pubkey, &vote_account);
⋮----
fn update_prioritization_fee_cache(&self, transactions: Vec<Transaction>) {
⋮----
.map(RuntimeTransaction::from_transaction_for_tests)
⋮----
prioritization_fee_cache.update(&bank, transactions.iter());
⋮----
fn get_prioritization_fee_cache(&self) -> &PrioritizationFeeCache {
⋮----
fn working_bank(&self) -> Arc<Bank> {
self.bank_forks.read().unwrap().working_bank()
⋮----
fn leader_pubkey(&self) -> Pubkey {
*self.working_bank().collector_id()
⋮----
fn test_rpc_request_processor_new() {
⋮----
let genesis = create_genesis_config(100);
⋮----
let bank = meta.bank_forks.read().unwrap().root_bank();
bank.transfer(20, &genesis.mint_keypair, &bob_pubkey)
⋮----
fn test_rpc_get_balance() {
let genesis = create_genesis_config(20);
let mint_pubkey = genesis.mint_keypair.pubkey();
⋮----
let req = format!(
⋮----
let res = io.handle_request_sync(&req, meta);
let expected = json!({
⋮----
let result = serde_json::from_str::<Value>(&res.expect("actual response"))
.expect("actual response deserialization");
assert_eq!(result, expected);
⋮----
fn test_rpc_get_balance_via_client() {
⋮----
async fn use_client(client: rpc_minimal::gen_client::Client, mint_pubkey: Pubkey) -> u64 {
⋮----
.get_balance(mint_pubkey.to_string(), None)
⋮----
let client = use_client(client, mint_pubkey);
⋮----
assert_eq!(response, 20);
⋮----
fn test_rpc_get_cluster_nodes() {
⋮----
let request = create_test_request("getClusterNodes", None);
let result: Value = parse_success_result(rpc.handle_request_sync(request));
let expected = json!([{
⋮----
fn test_rpc_get_recent_performance_samples() {
⋮----
.write_perf_sample(
⋮----
.expect("write to blockstore");
let request = create_test_request("getRecentPerformanceSamples", None);
⋮----
fn test_rpc_get_recent_performance_samples_invalid_limit() {
⋮----
let request = create_test_request("getRecentPerformanceSamples", Some(json!([10_000])));
let response = parse_failure_response(rpc.handle_request_sync(request));
⋮----
ErrorCode::InvalidParams.code(),
⋮----
assert_eq!(response, expected);
⋮----
fn test_rpc_get_slot_leader() {
⋮----
let request = create_test_request("getSlotLeader", None);
let result: String = parse_success_result(rpc.handle_request_sync(request));
let expected = rpc.leader_pubkey().to_string();
⋮----
fn test_rpc_get_tx_count() {
⋮----
let genesis = create_genesis_config(10);
⋮----
bank.transfer(1, &genesis.mint_keypair, &bob_pubkey)
⋮----
bank.transfer(2, &genesis.mint_keypair, &bob_pubkey)
⋮----
bank.transfer(3, &genesis.mint_keypair, &bob_pubkey)
⋮----
bank.transfer(4, &genesis.mint_keypair, &bob_pubkey)
⋮----
let res = io.handle_request_sync(req, meta);
⋮----
serde_json::from_str(expected).expect("expected response deserialization");
let result: Response = serde_json::from_str(&res.expect("actual response"))
⋮----
fn test_rpc_minimum_ledger_slot() {
⋮----
rpc.create_test_transactions_and_populate_blockstore();
let request = create_test_request("minimumLedgerSlot", None);
let result: Slot = parse_success_result(rpc.handle_request_sync(request));
assert_eq!(0, result);
⋮----
fn test_get_supply() {
⋮----
let request = create_test_request("getSupply", None);
⋮----
parse_success_result(rpc.handle_request_sync(request));
result.value.non_circulating_accounts.sort();
⋮----
let mut non_circulating_accounts: Vec<String> = non_circulating_accounts()
⋮----
non_circulating_accounts.sort();
let total_capitalization = rpc.working_bank().capitalization();
⋮----
fn test_get_supply_exclude_account_list() {
⋮----
let request = create_test_request(
⋮----
Some(json!([{"excludeNonCirculatingAccountsList": true}])),
⋮----
let result: RpcResponse<RpcSupply> = parse_success_result(rpc.handle_request_sync(request));
⋮----
non_circulating_accounts: vec![],
⋮----
assert_eq!(result.value, expected);
⋮----
fn test_get_largest_accounts() {
⋮----
let non_circulating_key = &non_circulating_accounts()[0];
let bank = rpc.working_bank();
bank.process_transaction(&system_transaction::transfer(
⋮----
bank.confirmed_last_blockhash(),
⋮----
.expect("process transaction");
let request = create_test_request("getLargestAccounts", None);
⋮----
assert_eq!(largest_accounts_result.value.len(), 20);
⋮----
Some(json!([rpc.mint_keypair.pubkey().to_string()])),
⋮----
assert!(largest_accounts_result.value.contains(&RpcAccountBalance {
⋮----
create_test_request("getBalance", Some(json!([non_circulating_key.to_string()])));
⋮----
Some(json!([{"filter":"circulating"}])),
⋮----
assert!(!largest_accounts_result.value.contains(&RpcAccountBalance {
⋮----
Some(json!([{"filter":"nonCirculating"}])),
⋮----
assert_eq!(largest_accounts_result.value.len(), 1);
⋮----
fn test_rpc_get_minimum_balance_for_rent_exemption() {
⋮----
create_test_request("getMinimumBalanceForRentExemption", Some(json!([data_len])));
let result: u64 = parse_success_result(rpc.handle_request_sync(request));
⋮----
.working_bank()
.get_minimum_balance_for_rent_exemption(data_len);
⋮----
fn test_rpc_get_inflation() {
⋮----
let request = create_test_request("getInflationGovernor", None);
let result: RpcInflationGovernor = parse_success_result(rpc.handle_request_sync(request));
let expected: RpcInflationGovernor = bank.inflation().into();
⋮----
let request = create_test_request("getInflationRate", None);
let result: RpcInflationRate = parse_success_result(rpc.handle_request_sync(request));
⋮----
fn test_rpc_get_epoch_schedule() {
⋮----
let request = create_test_request("getEpochSchedule", None);
let result: EpochSchedule = parse_success_result(rpc.handle_request_sync(request));
let expected = bank.epoch_schedule();
assert_eq!(expected, &result);
⋮----
fn test_rpc_get_leader_schedule() {
⋮----
Some(json!([0u64])),
Some(json!([null, {"identity": rpc.leader_pubkey().to_string()}])),
Some(json!([{"identity": rpc.leader_pubkey().to_string()}])),
⋮----
let request = create_test_request("getLeaderSchedule", params);
⋮----
let expected = Some(HashMap::from_iter(std::iter::once((
rpc.leader_pubkey().to_string(),
⋮----
let request = create_test_request("getLeaderSchedule", Some(json!([42424242])));
⋮----
Some(json!([{"identity": Pubkey::new_unique().to_string() }])),
⋮----
let expected = Some(HashMap::default());
⋮----
fn test_rpc_get_slot_leaders() {
⋮----
let query_limit = 2 * bank.epoch_schedule().slots_per_epoch;
⋮----
create_test_request("getSlotLeaders", Some(json!([query_start, query_limit])));
let result: Vec<String> = parse_success_result(rpc.handle_request_sync(request));
assert_eq!(result.len(), query_limit as usize);
⋮----
let query_start = 2 * bank.epoch_schedule().slots_per_epoch;
⋮----
fn test_rpc_get_account_info() {
⋮----
let address = pubkey.to_string();
let data = vec![1, 2, 3, 4, 5];
let account = AccountSharedData::create(42, data.clone(), Pubkey::default(), false, 0);
bank.store_account(&pubkey, &account);
⋮----
Some(json!([address, {"encoding": "base64"}])),
⋮----
let expected = json!([BASE64_STANDARD.encode(&data), "base64"]);
assert_eq!(result["value"]["data"], expected);
assert_eq!(result["value"]["space"], 5);
⋮----
Some(json!([address, {"encoding": "base64", "dataSlice": {"length": 2, "offset": 1}}])),
⋮----
let expected = json!([BASE64_STANDARD.encode(&data[1..3]), "base64"]);
⋮----
Some(json!([address, {"encoding": "binary", "dataSlice": {"length": 2, "offset": 1}}])),
⋮----
let expected = bs58::encode(&data[1..3]).into_string();
⋮----
json!([address, {"encoding": "jsonParsed", "dataSlice": {"length": 2, "offset": 1}}]),
⋮----
fn test_encode_account_does_not_throw_when_slice_larger_than_account() {
let data = vec![42; 5];
⋮----
let result = encode_account(
⋮----
Some(UiDataSliceConfig {
length: account.data().len() + 1,
⋮----
assert!(result.is_ok());
⋮----
fn test_encode_account_throws_when_data_too_large_to_base58_encode() {
let data = vec![42; MAX_BASE58_BYTES + 1];
⋮----
let _ = encode_account(&account, &pubkey, UiAccountEncoding::Base58, None).unwrap();
⋮----
fn test_encode_account_does_not_throw_despite_data_too_large_to_base58_encode_because_dataslice_makes_it_fit(
⋮----
fn test_encode_account_does_not_throw_despite_dataslice_being_too_large_to_base58_encode_because_account_is_small_enough_to_fit(
⋮----
let data = vec![42; MAX_BASE58_BYTES];
⋮----
fn test_encode_account_does_not_throw_despite_account_and_dataslice_being_too_large_to_base58_encode_because_their_intersection_fits(
⋮----
fn test_rpc_get_multiple_accounts() {
⋮----
Some(json!([[
⋮----
let result: RpcResponse<Value> = parse_success_result(rpc.handle_request_sync(request));
let expected = json!([
⋮----
Some(json!([
⋮----
fn test_rpc_get_program_accounts() {
⋮----
bank.store_account(&new_program_account_key, &new_program_account);
⋮----
Some(json!([new_program_id.to_string()])),
⋮----
let result: Vec<RpcKeyedAccount> = parse_success_result(rpc.handle_request_sync(request));
let expected_value = vec![RpcKeyedAccount {
⋮----
assert_eq!(result, expected_value);
⋮----
.map(|_| {
⋮----
assert_eq!(result.len(), 2);
⋮----
assert_eq!(result.len(), 0);
⋮----
assert_eq!(result.len(), 1);
⋮----
fn test_rpc_simulate_bundle_happy_path() {
⋮----
let lamports = bank.get_minimum_balance_for_rent_exemption(data_len);
⋮----
bank.store_account(&leader_pubkey, &leader_account_data);
bank.freeze();
⋮----
let encoded_mev_tx = general_purpose::STANDARD.encode(serialize(&mev_tx).unwrap());
let encoded_tip_tx = general_purpose::STANDARD.encode(serialize(&tip_tx).unwrap());
let b64_data = general_purpose::STANDARD.encode(leader_account_data.data());
⋮----
let expected_response = json!({
⋮----
let request = format!(
⋮----
.handle_request_sync(&request, meta.clone())
.expect("response");
⋮----
.expect("expected_response deserialization");
⋮----
.expect("actual_response deserialization");
assert_eq!(expected_response, actual_response);
⋮----
fn test_rpc_simulate_transaction() {
⋮----
let tx_serialized_encoded = bs58::encode(serialize(&tx).unwrap()).into_string();
⋮----
let tx_badsig_serialized_encoded = bs58::encode(serialize(&tx).unwrap()).into_string();
⋮----
let tx_invalid_recent_blockhash = bs58::encode(serialize(&tx).unwrap()).into_string();
⋮----
let loaded_accounts_data_size = expected_loaded_accounts_data_size(&bank, &tx);
⋮----
let res = io.handle_request_sync(&req, meta.clone());
⋮----
serde_json::from_value(expected).expect("expected response deserialization");
⋮----
let latest_blockhash = bank.confirmed_last_blockhash();
⋮----
.get_blockhash_last_valid_block_height(&latest_blockhash)
.expect("blockhash exists");
⋮----
fn test_rpc_simulate_transaction_with_parsing_token_accounts() {
⋮----
bank.get_minimum_balance_for_rent_exemption(spl_token_interface::state::Mint::LEN);
let mint_pubkey = Pubkey::from_str("mint111111111111111111111111111111111111111").unwrap();
⋮----
mint_data.into(),
⋮----
bank.store_account(&mint_pubkey, &account);
⋮----
bank.get_minimum_balance_for_rent_exemption(spl_token_interface::state::Account::LEN);
⋮----
let owner_pubkey = Pubkey::from_str("owner11111111111111111111111111111111111111").unwrap();
⋮----
token_account_data.into(),
⋮----
bank.store_account(&token_account_pubkey, &account);
⋮----
fn test_rpc_simulate_transaction_with_inner_instructions() {
⋮----
let from_pubkey = from.pubkey();
⋮----
let to_pubkey = to.pubkey();
⋮----
let lamports = bank.rent_collector().rent.minimum_balance(space);
⋮----
Some(&from_pubkey),
⋮----
base64::prelude::BASE64_STANDARD.encode(serialize(&tx).unwrap());
⋮----
fn test_rpc_simulate_transaction_panic_on_unfrozen_bank() {
⋮----
assert!(!bank.is_frozen());
⋮----
// should panic because `bank` is not frozen
let _ = io.handle_request_sync(&req, meta);
⋮----
fn test_rpc_get_signature_statuses() {
⋮----
let confirmed_block_signatures = rpc.create_test_transactions_and_populate_blockstore();
⋮----
let expected_res: transaction::Result<()> = Ok(());
let json: Value = serde_json::from_str(&res.unwrap()).unwrap();
⋮----
serde_json::from_value(json["result"]["value"][0].clone())
⋮----
let result = result.as_ref().unwrap();
assert_eq!(expected_res, result.status);
assert_eq!(None, result.confirmations);
⋮----
bank.get_minimum_balance_for_rent_exemption(0) + 10,
⋮----
assert!(result.is_none());
⋮----
let expected_res: transaction::Result<()> = Err(TransactionError::InstructionError(
⋮----
assert_eq!(expected_res, result.as_ref().unwrap().status);
⋮----
fn test_rpc_fail_request_airdrop() {
⋮----
// Expect internal error because no faucet is available
⋮----
fn test_rpc_send_bad_tx() {
⋮----
assert_eq!(error["code"], ErrorCode::InvalidParams.code());
⋮----
fn test_rpc_send_transaction_preflight() {
⋮----
let validator_exit = create_validator_exit(exit.clone());
⋮----
let (bank_forks, mint_keypair, ..) = new_bank_forks();
⋮----
let health = RpcHealth::stub(optimistically_confirmed_bank.clone(), blockstore.clone());
health.stub_set_health_status(Some(RpcHealthStatus::Ok));
bank_forks.write().unwrap().get(0).unwrap().freeze();
⋮----
health.clone(),
⋮----
runtime.clone(),
⋮----
// sendTransaction will fail due to insanity
⋮----
let recent_blockhash = bank_forks.read().unwrap().root_bank().last_blockhash();
bad_transaction.sign(&[&mint_keypair], recent_blockhash);
⋮----
// sendTransaction will fail due to poor node health
health.stub_set_health_status(Some(RpcHealthStatus::Behind { num_slots: 42 }));
⋮----
// sendTransaction will fail due to invalid signature
⋮----
// sendTransaction will fail due to sanitization failure
bad_transaction.signatures.clear();
⋮----
fn test_rpc_verify_filter() {
⋮----
0,                                                                                      // offset
MemcmpEncodedBytes::Base58("13LeFbG6m2EP1fqCj9k66fcXsoTHMMtgr7c78AivUrYD".to_string()),
⋮----
assert_eq!(verify_filter(&filter), Ok(()));
⋮----
MemcmpEncodedBytes::Base58("III".to_string()),
⋮----
assert!(verify_filter(&filter).is_err());
⋮----
fn test_rpc_verify_pubkey() {
⋮----
assert_eq!(verify_pubkey(&pubkey.to_string()).unwrap(), pubkey);
⋮----
fn test_rpc_verify_signature() {
⋮----
hash(&[0]),
⋮----
fn new_bank_forks() -> (Arc<RwLock<BankForks>>, Keypair, Arc<Keypair>) {
new_bank_forks_with_config(BankTestConfig::default())
⋮----
fn new_bank_forks_with_config(
⋮----
} = create_genesis_config(TEST_MINT_LAMPORTS);
⋮----
bank.add_builtin(
⋮----
fn test_rpc_get_identity() {
⋮----
let request = create_test_request("getIdentity", None);
⋮----
let expected: Value = json!({ "identity": rpc.identity.to_string() });
⋮----
fn test_rpc_get_max_slots() {
⋮----
rpc.max_slots.retransmit.store(42, Ordering::Relaxed);
rpc.max_slots.shred_insert.store(43, Ordering::Relaxed);
let request = create_test_request("getMaxRetransmitSlot", None);
⋮----
assert_eq!(result, 42);
let request = create_test_request("getMaxShredInsertSlot", None);
⋮----
assert_eq!(result, 43);
⋮----
fn test_rpc_get_version() {
⋮----
let request = create_test_request("getVersion", None);
⋮----
fn test_rpc_processor_get_block_commitment() {
⋮----
let bank_forks = new_bank_forks().0;
⋮----
.entry(0)
.or_insert_with(|| commitment_slot0.clone());
⋮----
.entry(1)
.or_insert_with(|| commitment_slot1.clone());
⋮----
CommitmentSlots::new_from_slot(bank_forks.read().unwrap().highest_slot()),
⋮----
RpcHealth::stub(optimistically_confirmed_bank.clone(), blockstore),
⋮----
fn test_rpc_get_block_commitment() {
⋮----
block_0_commitment.increase_confirmation_stake(2, 9);
⋮----
&mut *rpc.block_commitment_cache.write().unwrap(),
⋮----
HashMap::from_iter(std::iter::once((0, block_0_commitment.clone()))),
⋮----
let request = create_test_request("getBlockCommitment", Some(json!([0u64])));
let result: RpcBlockCommitment<_> = parse_success_result(rpc.handle_request_sync(request));
⋮----
commitment: Some(block_0_commitment.commitment),
⋮----
let request = create_test_request("getBlockCommitment", Some(json!([1u64])));
⋮----
fn test_get_block_with_versioned_tx() {
⋮----
bank.set_sysvar_for_tests(&SlotHashes::default());
rpc.create_test_versioned_transactions_and_populate_blockstore(None);
⋮----
let confirmed_block = result.unwrap();
assert_eq!(confirmed_block.transactions.len(), 2);
⋮----
let request = create_test_request("getBlock", Some(json!([0u64,])));
⋮----
fn test_get_block() {
⋮----
let request = create_test_request("getBlock", Some(json!([0u64])));
⋮----
assert_eq!(confirmed_block.rewards, vec![]);
⋮----
} in confirmed_block.transactions.into_iter()
⋮----
if transaction.signatures[0] == confirmed_block_signatures[0].to_string() {
let meta = meta.unwrap();
assert_eq!(meta.status, Ok(()));
assert_eq!(meta.err, None);
} else if transaction.signatures[0] == confirmed_block_signatures[1].to_string() {
⋮----
assert_eq!(meta, None);
⋮----
let request = create_test_request("getBlock", Some(json!([0u64, "binary"])));
⋮----
deserialize(&bs58::decode(&transaction).into_vec().unwrap()).unwrap();
⋮----
fn test_get_block_config() {
⋮----
assert!(confirmed_block.transactions.is_none());
assert!(confirmed_block.rewards.is_none());
for (i, signature) in confirmed_block.signatures.unwrap()[..2].iter().enumerate() {
assert_eq!(*signature, confirmed_block_signatures[i].to_string());
⋮----
assert!(confirmed_block.signatures.is_none());
assert_eq!(confirmed_block.rewards.unwrap(), vec![]);
⋮----
fn test_get_block_production() {
⋮----
rpc.add_roots_to_blockstore(vec![0, 1, 3, 4, 8]);
⋮----
.set_highest_super_majority_root(8);
let request = create_test_request("getBlockProduction", Some(json!([])));
⋮----
Some(json!([{
⋮----
fn test_get_blocks() {
⋮----
let _ = rpc.create_test_transactions_and_populate_blockstore();
⋮----
let request = create_test_request("getBlocks", Some(json!([0u64])));
let result: Vec<Slot> = parse_success_result(rpc.handle_request_sync(request));
assert_eq!(result, vec![0, 1, 3, 4, 8]);
let request = create_test_request("getBlocks", Some(json!([2u64])));
⋮----
assert_eq!(result, vec![3, 4, 8]);
let request = create_test_request("getBlocks", Some(json!([0u64, 4u64])));
⋮----
assert_eq!(result, vec![0, 1, 3, 4]);
let request = create_test_request("getBlocks", Some(json!([0u64, 7u64])));
⋮----
let request = create_test_request("getBlocks", Some(json!([9u64, 11u64])));
⋮----
assert_eq!(result, Vec::<Slot>::new());
⋮----
.set_highest_super_majority_root(u64::MAX);
⋮----
Some(json!([0u64, MAX_GET_CONFIRMED_BLOCKS_RANGE])),
⋮----
Some(json!([0u64, MAX_GET_CONFIRMED_BLOCKS_RANGE + 1])),
⋮----
fn test_get_blocks_with_limit() {
⋮----
let request = create_test_request("getBlocksWithLimit", Some(json!([0u64, 500_001u64])));
⋮----
let request = create_test_request("getBlocksWithLimit", Some(json!([0u64, 0u64])));
⋮----
let request = create_test_request("getBlocksWithLimit", Some(json!([2u64, 2u64])));
⋮----
assert_eq!(result, vec![3, 4]);
let request = create_test_request("getBlocksWithLimit", Some(json!([2u64, 3u64])));
⋮----
let request = create_test_request("getBlocksWithLimit", Some(json!([2u64, 500_000u64])));
⋮----
let request = create_test_request("getBlocksWithLimit", Some(json!([9u64, 500_000u64])));
⋮----
fn test_get_block_time() {
⋮----
rpc.add_roots_to_blockstore(vec![1, 2, 3, 4, 5, 6, 7]);
⋮----
.get(0)
⋮----
.unix_timestamp_from_genesis();
⋮----
.set_highest_super_majority_root(7);
let slot_duration = slot_duration_from_slots_per_year(rpc.working_bank().slots_per_year());
let request = create_test_request("getBlockTime", Some(json!([2u64])));
let result: Option<UnixTimestamp> = parse_success_result(rpc.handle_request_sync(request));
let expected = Some(base_timestamp);
⋮----
let request = create_test_request("getBlockTime", Some(json!([7u64])));
⋮----
let expected = Some(base_timestamp + (7 * slot_duration).as_secs() as i64);
⋮----
let request = create_test_request("getBlockTime", Some(json!([12345u64])));
⋮----
fn test_get_vote_accounts() {
⋮----
let mut bank = rpc.working_bank();
⋮----
assert_eq!(bank.vote_accounts().len(), 1);
⋮----
&alice_vote_keypair.pubkey(),
⋮----
node_pubkey: mint_keypair.pubkey(),
authorized_voter: alice_vote_keypair.pubkey(),
authorized_withdrawer: alice_vote_keypair.pubkey(),
⋮----
&bank.get_sysvar_cache_for_tests().get_clock().unwrap(),
⋮----
rpc.store_vote_account(&alice_vote_keypair.pubkey(), alice_vote_state);
assert_eq!(bank.vote_accounts().len(), 2);
⋮----
let res = io.handle_request_sync(req, meta.clone());
let result: Value = serde_json::from_str(&res.expect("actual response"))
⋮----
serde_json::from_value(result["result"].clone()).unwrap();
assert!(vote_account_status.current.is_empty());
assert_eq!(vote_account_status.delinquent.len(), 1);
⋮----
assert_ne!(vote_account_info.activated_stake, 0);
⋮----
&leader_vote_keypair.pubkey(),
⋮----
TowerSync::new_from_slot(bank.slot(), bank.hash()),
⋮----
bank = rpc.advance_bank_to_confirmed_slot(bank.slot() + 1);
⋮----
Some(&rpc.mint_keypair.pubkey()),
⋮----
bank.last_blockhash(),
⋮----
bank.process_transaction(&transaction)
⋮----
advance_bank();
⋮----
assert!(vote_account_status.delinquent.is_empty());
assert_eq!(vote_account_status.current.len(), 2);
⋮----
.find(|x| x.vote_pubkey == leader_vote_keypair.pubkey().to_string())
⋮----
assert_ne!(leader_info.activated_stake, 0);
⋮----
assert_eq!(vote_account_status.current.len(), 1);
assert_eq!(vote_account_status.delinquent.len(), 0);
⋮----
assert!(!vote_account_status
⋮----
rpc.advance_bank_to_confirmed_slot(bank.slot() + TEST_SLOTS_PER_EPOCH);
⋮----
fn test_is_finalized() {
⋮----
blockstore.set_roots([0, 1].iter()).unwrap();
⋮----
cache0.increase_rooted_stake(50);
⋮----
cache1.increase_rooted_stake(40);
⋮----
cache2.increase_rooted_stake(20);
⋮----
block_commitment.entry(1).or_insert(cache0);
block_commitment.entry(2).or_insert(cache1);
block_commitment.entry(3).or_insert(cache2);
⋮----
assert!(is_finalized(&block_commitment_cache, &bank, &blockstore, 0));
assert!(is_finalized(&block_commitment_cache, &bank, &blockstore, 1));
assert!(!is_finalized(
⋮----
fn test_token_rpcs() {
⋮----
let mut account_data = vec![0; account_size];
⋮----
account_state.pack_base();
account_state.init_account_type().unwrap();
⋮----
let memo_transfer = account_state.init_extension::<MemoTransfer>(true).unwrap();
memo_transfer.require_incoming_transfer_memos = true.into();
⋮----
data: account_data.to_vec(),
⋮----
bank.store_account(&token_account_pubkey, &token_account);
⋮----
let mut mint_data = vec![0; mint_size];
⋮----
StateWithExtensionsMut::<Mint>::unpack_uninitialized(&mut mint_data).unwrap();
⋮----
mint_state.pack_base();
mint_state.init_account_type().unwrap();
⋮----
OptionalNonZeroPubkey::try_from(Some(owner)).unwrap();
⋮----
data: mint_data.to_vec(),
⋮----
bank.store_account(&Pubkey::from_str(&mint.to_string()).unwrap(), &mint_account);
⋮----
bank.store_account(&other_token_account_pubkey, &token_account);
let mut account_data = vec![0; TokenAccount::get_packed_len()];
⋮----
TokenAccount::pack(token_account, &mut account_data).unwrap();
⋮----
bank.store_account(&token_with_different_mint_pubkey, &token_account);
⋮----
let mut mint_data = vec![0; Mint::get_packed_len()];
⋮----
Mint::pack(mint_state, &mut mint_data).unwrap();
⋮----
serde_json::from_value(result["result"]["value"].clone()).unwrap();
⋮----
assert!((balance.ui_amount.unwrap() - 4.2).abs() < error);
assert_eq!(balance.amount, 420.to_string());
assert_eq!(balance.decimals, 2);
assert_eq!(balance.ui_amount_string, "4.2".to_string());
⋮----
assert!(result.get("error").is_some());
⋮----
assert!((supply.ui_amount.unwrap() - 5.0).abs() < error);
assert_eq!(supply.amount, 500.to_string());
assert_eq!(supply.decimals, 2);
assert_eq!(supply.ui_amount_string, "5".to_string());
⋮----
assert_eq!(accounts.len(), 3);
⋮----
assert_eq!(accounts.len(), 2);
⋮----
assert_eq!(accounts.len(), 4);
⋮----
assert!(accounts.is_empty());
⋮----
bank.store_account(
&Pubkey::from_str(&new_mint.to_string()).unwrap(),
⋮----
bank.store_account(&token_with_smaller_balance, &token_account);
⋮----
fn test_token_parsing(
⋮----
let mut extensions = vec![ExtensionType::MintCloseAuthority];
if interest_bearing_config.is_some() {
extensions.push(ExtensionType::InterestBearingConfig);
⋮----
if scaled_ui_amount_config.is_some() {
extensions.push(ExtensionType::ScaledUiAmount);
⋮----
let mint_size = ExtensionType::try_calculate_account_len::<Mint>(&extensions).unwrap();
⋮----
if let Some(interest_bearing_config) = interest_bearing_config.as_mut() {
⋮----
bank.clock().unix_timestamp.saturating_sub(1_000_000).into();
interest_bearing_config.last_update_timestamp = bank.clock().unix_timestamp.into();
⋮----
if let Some(scaled_ui_amount_config) = scaled_ui_amount_config.as_ref() {
⋮----
.map(|v| (v, bank.clock().unix_timestamp)),
⋮----
let token_ui_amount = token_amount_to_ui_amount_v3(amount, &additional_data);
let delegated_ui_amount = token_amount_to_ui_amount_v3(delegated_amount, &additional_data);
⋮----
token_amount_to_ui_amount_v3(rent_exempt_amount, &additional_data);
let mut expected_value = json!({
⋮----
expected_value["parsed"]["info"]["extensions"] = json!([
⋮----
assert_eq!(result["result"]["value"]["data"], expected_value,);
⋮----
fn test_get_spl_token_owner_filter() {
⋮----
assert!(get_spl_token_owner_filter(
⋮----
let mut first_half_bytes = owner.to_bytes().to_vec();
first_half_bytes.resize(16, 0);
⋮----
fn test_get_spl_token_mint_filter() {
⋮----
assert!(get_spl_token_mint_filter(
⋮----
fn test_rpc_single_gossip() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(100);
⋮----
let bank0 = bank_forks.read().unwrap().get(0).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank1);
let bank1 = bank_forks.read().unwrap().get(1).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank2);
let bank2 = bank_forks.read().unwrap().get(2).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank3);
⋮----
optimistically_confirmed_bank.clone(),
⋮----
let slot: Slot = serde_json::from_value(json["result"].clone()).unwrap();
assert_eq!(slot, 0);
⋮----
assert_eq!(slot, 2);
⋮----
let bank3 = bank_forks.read().unwrap().get(3).unwrap();
⋮----
assert_eq!(slot, 3);
⋮----
fn test_worst_case_encoded_tx_goldens() {
let ff_tx = vec![0xffu8; PACKET_DATA_SIZE];
let tx58 = bs58::encode(&ff_tx).into_string();
assert_eq!(tx58.len(), MAX_BASE58_SIZE);
let tx64 = BASE64_STANDARD.encode(&ff_tx);
assert_eq!(tx64.len(), MAX_BASE64_SIZE);
⋮----
fn test_decode_and_deserialize_too_large_payloads_fail() {
⋮----
let tx_ser = vec![0xffu8; too_big];
let tx58 = bs58::encode(&tx_ser).into_string();
let tx58_len = tx58.len();
⋮----
let tx64 = BASE64_STANDARD.encode(&tx_ser);
let tx64_len = tx64.len();
⋮----
let tx_ser = vec![0x00u8; too_big];
⋮----
let tx_ser = vec![0xffu8; PACKET_DATA_SIZE - 2];
let mut tx64 = BASE64_STANDARD.encode(&tx_ser);
⋮----
tx64.push('!');
⋮----
let mut tx58 = bs58::encode(&tx_ser).into_string();
⋮----
tx58.push('!');
⋮----
fn test_sanitize_unsanitary() {
⋮----
.to_string();
⋮----
fn test_sanitize_unsupported_transaction_version() {
⋮----
signatures: vec![Signature::default()],
⋮----
account_keys: vec![Pubkey::new_unique()],
⋮----
fn test_rpc_get_stake_minimum_delegation() {
⋮----
let request = create_test_request("getStakeMinimumDelegation", None);
let response: RpcResponse<u64> = parse_success_result(rpc.handle_request_sync(request));
⋮----
fn test_get_fee_for_message() {
⋮----
assert_eq!(response.value, TEST_SIGNATURE_FEE);
⋮----
Some(json!([BASE64_STANDARD.encode(serialize(&v0_msg).unwrap())])),
⋮----
fn test_rpc_get_recent_prioritization_fees() {
fn wait_for_cache_blocks(cache: &PrioritizationFeeCache, num_blocks: usize) {
while cache.available_block_count() < num_blocks {
⋮----
fn assert_fee_vec_eq(
⋮----
expected.sort_by(|a, b| a.slot.partial_cmp(&b.slot).unwrap());
actual.sort_by(|a, b| a.slot.partial_cmp(&b.slot).unwrap());
assert_eq!(expected, actual);
⋮----
let slot0 = rpc.working_bank().slot();
let bank0_id = rpc.working_bank().bank_id();
⋮----
let transactions = vec![
⋮----
rpc.update_prioritization_fee_cache(transactions);
let cache = rpc.get_prioritization_fee_cache();
cache.finalize_priority_fee(slot0, bank0_id);
wait_for_cache_blocks(cache, 1);
let request = create_test_request("getRecentPrioritizationFees", None);
⋮----
assert_fee_vec_eq(
⋮----
&mut vec![RpcPrioritizationFee {
⋮----
Some(json!([[account1.to_string()]])),
⋮----
Some(json!([[account2.to_string()]])),
⋮----
rpc.advance_bank_to_confirmed_slot(1);
let slot1 = rpc.working_bank().slot();
let bank1_id = rpc.working_bank().bank_id();
⋮----
cache.finalize_priority_fee(slot1, bank1_id);
wait_for_cache_blocks(cache, 2);
⋮----
&mut vec![

================
File: rpc/src/slot_status_notifier.rs
================
pub trait SlotStatusNotifierInterface {
⋮----
pub type SlotStatusNotifier = Arc<RwLock<dyn SlotStatusNotifierInterface + Sync + Send>>;

================
File: rpc/src/transaction_notifier_interface.rs
================
pub trait TransactionNotifier {
⋮----
pub type TransactionNotifierArc = Arc<dyn TransactionNotifier + Sync + Send>;

================
File: rpc/src/transaction_status_service.rs
================
enum Error {
⋮----
type Result<T> = std::result::Result<T, Error>;
⋮----
pub struct TransactionStatusService {
⋮----
impl TransactionStatusService {
⋮----
pub fn new(
⋮----
.name("solTxStatusWrtr".to_string())
.spawn({
let transaction_status_receiver = transaction_status_receiver.clone();
⋮----
info!("{} has started", Self::SERVICE_NAME);
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
.recv_timeout(Duration::from_secs(1))
⋮----
info!("{} is stopping because: {err}", Self::SERVICE_NAME);
⋮----
transaction_notifier.clone(),
⋮----
depenency_tracker.clone(),
⋮----
error!("{} is stopping because: {err}", Self::SERVICE_NAME);
exit.store(true, Ordering::Relaxed);
⋮----
info!("{} has stopped", Self::SERVICE_NAME);
⋮----
.unwrap();
⋮----
fn write_transaction_status_batch(
⋮----
Some(blockstore.get_write_batch()?)
⋮----
) in izip!(
⋮----
let fee = fee_details.total_fee();
let inner_instructions = inner_instructions.map(|inner_instructions| {
map_inner_instructions(inner_instructions).collect()
⋮----
let pre_token_balances = Some(pre_token_balances);
let post_token_balances = Some(post_token_balances);
let rewards = Some(vec![]);
let loaded_addresses = transaction.get_loaded_addresses();
⋮----
compute_units_consumed: Some(executed_units),
⋮----
if let Some(transaction_notifier) = transaction_notifier.as_ref() {
let is_vote = transaction.is_simple_vote_transaction();
let message_hash = transaction.message_hash();
let signature = transaction.signature();
let transaction = transaction.to_versioned_transaction();
transaction_notifier.notify_transaction(
⋮----
if !(enable_extended_tx_metadata_storage || transaction_notifier.is_some()) {
transaction_status_meta.log_messages.take();
transaction_status_meta.inner_instructions.take();
transaction_status_meta.return_data.take();
⋮----
if let Some(batch) = status_and_memos_batch.as_mut() {
if let Some(memos) = extract_and_fmt_memos(transaction.message()) {
blockstore.add_transaction_memos_to_batch(
transaction.signature(),
⋮----
let message = transaction.message();
⋮----
.account_keys()
.iter()
.enumerate()
.map(|(index, key)| (key, message.is_writable(index)));
blockstore.add_transaction_status_to_batch(
⋮----
*transaction.signature(),
⋮----
blockstore.write_batch(batch)?;
⋮----
if let Some(dependency_tracker) = dependency_tracker.as_ref() {
⋮----
dependency_tracker.mark_this_and_all_previous_work_processed(work_id);
⋮----
if !bank.is_frozen() {
return Err(Error::NonFrozenBank(bank.slot()));
⋮----
max_complete_transaction_status_slot.fetch_max(bank.slot(), Ordering::SeqCst);
⋮----
Ok(())
⋮----
fn write_block_meta(bank: &Bank, blockstore: &Blockstore) -> Result<()> {
let slot = bank.slot();
blockstore.set_block_time(slot, bank.clock().unix_timestamp)?;
blockstore.set_block_height(slot, bank.block_height())?;
let rewards = bank.get_rewards_and_num_partitions();
if rewards.should_record() {
⋮----
.into_iter()
.map(|(pubkey, reward_info)| Reward {
pubkey: pubkey.to_string(),
⋮----
reward_type: Some(reward_info.reward_type),
⋮----
.collect();
⋮----
blockstore.write_rewards(slot, blockstore_rewards)?;
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
pub fn quiesce_and_join_for_tests(self, exit: Arc<AtomicBool>) {
⋮----
if self.transaction_status_receiver.is_empty() {
⋮----
assert!(
⋮----
self.join().unwrap();
⋮----
pub(crate) mod tests {
⋮----
struct TestNotifierKey {
⋮----
struct TestNotification {
⋮----
struct TestTransactionNotifier {
⋮----
impl TestTransactionNotifier {
pub fn new() -> Self {
⋮----
impl TransactionNotifier for TestTransactionNotifier {
fn notify_transaction(
⋮----
self.notifications.insert(
⋮----
_meta: transaction_status_meta.clone(),
transaction: transaction.clone(),
⋮----
fn build_test_transaction_legacy() -> Transaction {
⋮----
let pubkey1 = keypair1.pubkey();
⋮----
fn test_notify_transaction() {
let genesis_config = create_genesis_config(2).genesis_config;
⋮----
let (transaction_status_sender, transaction_status_receiver) = unbounded();
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger");
⋮----
let transaction = build_test_transaction_legacy();
⋮----
let expected_transaction = transaction.clone();
let mut nonce_account = nonce_account::create_account(1).into_inner();
⋮----
.set_state(&nonce::versions::Versions::new(
⋮----
let commit_result = Ok(CommittedTransaction {
status: Ok(()),
⋮----
pre_balances: vec![vec![123456]],
post_balances: vec![vec![234567]],
⋮----
let owner = Pubkey::new_unique().to_string();
let token_program_id = Pubkey::new_unique().to_string();
⋮----
mint: Pubkey::new_unique().to_string(),
ui_token_amount: token_amount_to_ui_amount_v3(
⋮----
owner: owner.clone(),
program_id: token_program_id.clone(),
⋮----
pre_token_balances: vec![vec![pre_token_balance]],
post_token_balances: vec![vec![post_token_balance]],
⋮----
let message_hash = *transaction.message_hash();
let transaction_index: usize = bank.transaction_count().try_into().unwrap();
⋮----
transactions: vec![transaction],
commit_results: vec![commit_result],
⋮----
costs: vec![Some(123)],
transaction_indexes: vec![transaction_index],
⋮----
Some(test_notifier.clone()),
⋮----
exit.clone(),
⋮----
.send(TransactionStatusMessage::Batch((
⋮----
transaction_status_service.quiesce_and_join_for_tests(exit);
assert_eq!(test_notifier.notifications.len(), 1);
⋮----
assert!(test_notifier.notifications.contains_key(&key));
let result = test_notifier.notifications.get(&key).unwrap();
assert_eq!(
⋮----
fn test_batch_transaction_status_and_memos() {
⋮----
let transaction1 = build_test_transaction_legacy();
⋮----
let transaction2 = build_test_transaction_legacy();
⋮----
let expected_transaction1 = transaction1.clone();
let expected_transaction2 = transaction2.clone();
⋮----
pre_balances: vec![vec![123456], vec![234567]],
post_balances: vec![vec![234567], vec![345678]],
⋮----
pre_token_balances: vec![vec![], vec![]],
post_token_balances: vec![vec![], vec![]],
⋮----
let transaction_index1: usize = bank.transaction_count().try_into().unwrap();
⋮----
transactions: vec![transaction1, transaction2],
commit_results: vec![commit_result.clone(), commit_result],
balances: balances.clone(),
⋮----
costs: vec![Some(123), Some(456)],
transaction_indexes: vec![transaction_index1, transaction_index2],
⋮----
Some(dependency_tracker.clone()),
⋮----
Some(work_id),
⋮----
assert_eq!(test_notifier.notifications.len(), 2);
⋮----
message_hash: *expected_transaction1.message_hash(),
⋮----
message_hash: *expected_transaction2.message_hash(),
⋮----
assert!(test_notifier.notifications.contains_key(&key1));
assert!(test_notifier.notifications.contains_key(&key2));
let result1 = test_notifier.notifications.get(&key1).unwrap();
let result2 = test_notifier.notifications.get(&key2).unwrap();

================
File: rpc/.gitignore
================
/target/
/farf/

================
File: rpc/Cargo.toml
================
[package]
name = "solana-rpc"
description = "Solana RPC"
documentation = "https://docs.rs/solana-rpc"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_rpc"

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "solana-ledger/dev-context-only-utils",
    "solana-rpc/dev-context-only-utils",
]

[dependencies]
agave-feature-set = { workspace = true }
agave-snapshots = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
bs58 = { workspace = true }
crossbeam-channel = { workspace = true }
dashmap = { workspace = true }
itertools = { workspace = true }
jsonrpc-core = { workspace = true }
jsonrpc-core-client = { workspace = true }
jsonrpc-derive = { workspace = true }
jsonrpc-http-server = { workspace = true }
jsonrpc-pubsub = { workspace = true }
libc = { workspace = true }
log = { workspace = true }
rayon = { workspace = true }
regex = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
soketto = { workspace = true }
solana-account = { workspace = true }
solana-account-decoder = { workspace = true }
solana-accounts-db = { workspace = true }
solana-cli-output = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-entry = { workspace = true }
solana-epoch-info = { workspace = true }
solana-epoch-rewards-hasher = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-faucet = { workspace = true }
solana-genesis-config = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-native-token = { workspace = true }
solana-perf = { workspace = true }
solana-poh = { workspace = true }
solana-poh-config = { workspace = true }
solana-program-pack = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true }
solana-quic-definitions = { workspace = true }
solana-rayon-threadlimit = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-runtime = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-send-transaction-service = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-slot-history = { workspace = true }
solana-storage-bigtable = { workspace = true }
solana-svm = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }
solana-sysvar = { workspace = true }
solana-time-utils = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status = { workspace = true }
solana-validator-exit = { workspace = true }
solana-version = { workspace = true }
solana-vote = { workspace = true }
solana-vote-program = { workspace = true }
spl-generic-token = { workspace = true }
spl-token-2022-interface = { workspace = true }
spl-token-interface = { workspace = true }
stream-cancel = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tokio-util = { workspace = true, features = ["codec", "compat"] }

[dev-dependencies]
agave-reserved-account-keys = { workspace = true }
serial_test = { workspace = true }
solana-address-lookup-table-interface = { workspace = true }
solana-cluster-type = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-fee-structure = { workspace = true }
solana-instruction = { workspace = true }
solana-net-utils = { workspace = true }
solana-nonce = { workspace = true }
solana-nonce-account = { workspace = true }
solana-program-option = { workspace = true }
solana-program-runtime = { workspace = true }
solana-rent = { workspace = true }
solana-rpc = { path = ".", features = ["dev-context-only-utils"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime-transaction = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-sdk-ids = { workspace = true }
solana-send-transaction-service = { workspace = true, features = ["dev-context-only-utils"] }
solana-sha256-hasher = { workspace = true }
solana-stake-interface = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-vote-interface = { workspace = true }
spl-pod = { workspace = true }
symlink = { workspace = true }
test-case = { workspace = true }

================
File: rpc-client/src/nonblocking/mod.rs
================
pub mod rpc_client;

================
File: rpc-client/src/nonblocking/rpc_client.rs
================
pub use crate::mock_sender::Mocks;
⋮----
pub struct RpcClient {
⋮----
impl RpcClient {
/// Create an `RpcClient` from an [`RpcSender`] and an [`RpcClientConfig`].
    ///
⋮----
///
    /// This is the basic constructor, allowing construction with any type of
⋮----
/// This is the basic constructor, allowing construction with any type of
    /// `RpcSender`. Most applications should use one of the other constructors,
⋮----
/// `RpcSender`. Most applications should use one of the other constructors,
    /// such as [`RpcClient::new`], [`RpcClient::new_with_commitment`] or
⋮----
/// such as [`RpcClient::new`], [`RpcClient::new_with_commitment`] or
    /// [`RpcClient::new_with_timeout`].
⋮----
/// [`RpcClient::new_with_timeout`].
    pub fn new_sender<T: RpcSender + Send + Sync + 'static>(
⋮----
pub fn new_sender<T: RpcSender + Send + Sync + 'static>(
⋮----
pub fn new(url: String) -> Self {
⋮----
pub fn new_with_commitment(url: String, commitment_config: CommitmentConfig) -> Self {
⋮----
pub fn new_with_timeout(url: String, timeout: Duration) -> Self {
⋮----
pub fn new_with_timeout_and_commitment(
⋮----
pub fn new_with_timeouts_and_commitment(
⋮----
confirm_transaction_initial_timeout: Some(confirm_transaction_initial_timeout),
⋮----
pub fn new_mock(url: String) -> Self {
⋮----
pub fn new_mock_with_mocks(url: String, mocks: Mocks) -> Self {
⋮----
pub fn new_mock_with_mocks_map<U: ToString>(url: U, mocks: MocksMap) -> Self {
⋮----
pub fn new_socket(addr: SocketAddr) -> Self {
Self::new(get_rpc_request_str(addr, false))
⋮----
pub fn new_socket_with_commitment(
⋮----
Self::new_with_commitment(get_rpc_request_str(addr, false), commitment_config)
⋮----
pub fn new_socket_with_timeout(addr: SocketAddr, timeout: Duration) -> Self {
let url = get_rpc_request_str(addr, false);
⋮----
pub fn url(&self) -> String {
self.sender.url()
⋮----
pub fn commitment(&self) -> CommitmentConfig {
⋮----
pub async fn send_and_confirm_transaction(
⋮----
.send_transaction_and_get_latest_blockhash(transaction, None)
⋮----
match self.get_signature_status(&signature).await? {
Some(Ok(_)) => return Ok(signature),
Some(Err(e)) => return Err(e.into()),
⋮----
.is_blockhash_valid(&latest_blockhash, CommitmentConfig::processed())
⋮----
// Block hash is not found by some reason
⋮----
} else if cfg!(not(test))
⋮----
sleep(Duration::from_millis(500)).await;
⋮----
Err(RpcError::ForUser(
⋮----
.to_string(),
⋮----
.into())
⋮----
pub async fn send_and_confirm_transaction_with_spinner(
⋮----
self.send_and_confirm_transaction_with_spinner_and_commitment(
⋮----
self.commitment(),
⋮----
pub async fn send_and_confirm_transaction_with_spinner_and_commitment(
⋮----
self.send_and_confirm_transaction_with_spinner_and_config(
⋮----
preflight_commitment: Some(commitment.commitment),
⋮----
async fn send_transaction_and_get_latest_blockhash(
⋮----
let (latest_blockhash, signature) = join!(
⋮----
Ok((latest_blockhash?, signature?))
⋮----
pub async fn send_and_confirm_transaction_with_spinner_and_config(
⋮----
.send_transaction_and_get_latest_blockhash(transaction, Some(config))
⋮----
self.confirm_transaction_with_spinner(&signature, &latest_blockhash, commitment)
⋮----
Ok(signature)
⋮----
pub async fn send_transaction(
⋮----
self.send_transaction_with_config(
⋮----
preflight_commitment: Some(self.commitment().commitment),
⋮----
pub async fn send_transaction_with_config(
⋮----
let encoding = config.encoding.unwrap_or(UiTransactionEncoding::Base64);
⋮----
commitment: config.preflight_commitment.unwrap_or_default(),
⋮----
encoding: Some(encoding),
preflight_commitment: Some(preflight_commitment.commitment),
⋮----
let serialized_encoded = serialize_and_encode(transaction, encoding)?;
⋮----
.send(
⋮----
json!([serialized_encoded, config]),
⋮----
}) = err.kind()
⋮----
debug!("{code} {message}");
⋮----
for (i, log) in logs.iter().enumerate() {
debug!("{:>3}: {}", i + 1, log);
⋮----
debug!("");
⋮----
return Err(err);
⋮----
.map_err(|err| Into::<ClientError>::into(RpcError::ParseError(err.to_string())))?;
// A mismatching RPC response signature indicates an issue with the RPC node, and
// should not be passed along to confirmation methods. The transaction may or may
// not have been submitted to the cluster, so callers should verify the success of
// the correct transaction signature independently.
if signature != *transaction.get_signature() {
Err(RpcError::RpcRequestError(format!(
⋮----
Ok(*transaction.get_signature())
⋮----
/// Check the confirmation status of a transaction.
    ///
⋮----
///
    /// Returns `true` if the given transaction succeeded and has been committed
⋮----
/// Returns `true` if the given transaction succeeded and has been committed
    /// with the configured [commitment level][cl], which can be retrieved with
⋮----
/// with the configured [commitment level][cl], which can be retrieved with
    /// the [`commitment`](RpcClient::commitment) method.
⋮----
/// the [`commitment`](RpcClient::commitment) method.
    ///
⋮----
///
    /// [cl]: https://solana.com/docs/rpc#configuring-state-commitment
⋮----
/// [cl]: https://solana.com/docs/rpc#configuring-state-commitment
    ///
⋮----
///
    /// Note that this method does not wait for a transaction to be confirmed
⋮----
/// Note that this method does not wait for a transaction to be confirmed
    /// &mdash; it only checks whether a transaction has been confirmed. To
⋮----
/// &mdash; it only checks whether a transaction has been confirmed. To
    /// submit a transaction and wait for it to confirm, use
⋮----
/// submit a transaction and wait for it to confirm, use
    /// [`send_and_confirm_transaction`][RpcClient::send_and_confirm_transaction].
⋮----
/// [`send_and_confirm_transaction`][RpcClient::send_and_confirm_transaction].
    ///
⋮----
///
    /// _This method returns `false` if the transaction failed, even if it has
⋮----
/// _This method returns `false` if the transaction failed, even if it has
    /// been confirmed._
⋮----
/// been confirmed._
    ///
⋮----
///
    /// # RPC Reference
⋮----
/// # RPC Reference
    ///
⋮----
///
    /// This method is built on the [`getSignatureStatuses`] RPC method.
⋮----
/// This method is built on the [`getSignatureStatuses`] RPC method.
    ///
⋮----
///
    /// [`getSignatureStatuses`]: https://solana.com/docs/rpc/http/getsignaturestatuses
⋮----
/// [`getSignatureStatuses`]: https://solana.com/docs/rpc/http/getsignaturestatuses
    ///
⋮----
///
    /// # Examples
⋮----
/// # Examples
    ///
⋮----
///
    /// ```
⋮----
/// ```
    /// # use solana_rpc_client_api::client_error::Error;
⋮----
/// # use solana_rpc_client_api::client_error::Error;
    /// # use solana_rpc_client::nonblocking::rpc_client::RpcClient;
⋮----
/// # use solana_rpc_client::nonblocking::rpc_client::RpcClient;
    /// # use solana_keypair::Keypair;
⋮----
/// # use solana_keypair::Keypair;
    /// # use solana_system_transaction as system_transaction;
⋮----
/// # use solana_system_transaction as system_transaction;
    /// # use solana_signature::Signature;
⋮----
/// # use solana_signature::Signature;
    /// # use solana_signer::Signer;
⋮----
/// # use solana_signer::Signer;
    /// # futures::executor::block_on(async {
⋮----
/// # futures::executor::block_on(async {
    /// #     let rpc_client = RpcClient::new_mock("succeeds".to_string());
⋮----
/// #     let rpc_client = RpcClient::new_mock("succeeds".to_string());
    pub async fn confirm_transaction(&self, signature: &Signature) -> ClientResult<bool> {
⋮----
pub async fn confirm_transaction(&self, signature: &Signature) -> ClientResult<bool> {
Ok(self
.confirm_transaction_with_commitment(signature, self.commitment())
⋮----
pub async fn confirm_transaction_with_commitment(
⋮----
let Response { context, value } = self.get_signature_statuses(&[*signature]).await?;
Ok(Response {
⋮----
.as_ref()
.filter(|result| result.satisfies_commitment(commitment_config))
.map(|result| result.status.is_ok())
.unwrap_or_default(),
⋮----
pub async fn confirm_transaction_with_spinner(
⋮----
let desired_confirmations = if commitment.is_finalized() {
⋮----
progress_bar.set_message(format!(
⋮----
.unwrap_or_default();
⋮----
.get_signature_status_with_commitment(signature, CommitmentConfig::processed())
⋮----
if status.is_none() {
⋮----
.is_blockhash_valid(recent_blockhash, CommitmentConfig::processed())
⋮----
if blockhash_not_found && now.elapsed() >= confirm_transaction_initial_timeout {
⋮----
if cfg!(not(test)) {
⋮----
return Err(err.into());
⋮----
return Err(RpcError::ForUser(
⋮----
.into());
⋮----
.get_signature_status_with_commitment(signature, commitment)
⋮----
.is_some()
⋮----
progress_bar.set_message("Transaction confirmed");
progress_bar.finish_and_clear();
return Ok(());
⋮----
.get_num_blocks_since_signature_confirmation(signature)
⋮----
.unwrap_or(confirmations);
if now.elapsed().as_secs() >= MAX_HASH_AGE_IN_SECONDS as u64 {
⋮----
pub async fn simulate_transaction(
⋮----
self.simulate_transaction_with_config(
⋮----
commitment: Some(self.commitment()),
⋮----
pub async fn simulate_transaction_with_config(
⋮----
let commitment = config.commitment.unwrap_or_default();
⋮----
commitment: Some(commitment),
⋮----
self.send(
⋮----
pub async fn simulate_bundle(
⋮----
self.simulate_bundle_with_config(
⋮----
simulation_bank: Some(SimulationSlotConfig::Commitment(self.commitment())),
pre_execution_accounts_configs: vec![None; bundle.len()],
post_execution_accounts_configs: vec![None; bundle.len()],
⋮----
pub async fn simulate_bundle_with_config(
⋮----
.unwrap_or(UiTransactionEncoding::Base64);
let simulation_bank = Some(config.simulation_bank.unwrap_or_default());
⋮----
.iter()
.map(|tx| serialize_and_encode(tx, transaction_encoding))
⋮----
transaction_encoding: Some(transaction_encoding),
⋮----
json!([rpc_bundle_request, config]),
⋮----
pub async fn get_highest_snapshot_slot(&self) -> ClientResult<RpcSnapshotSlotInfo> {
self.send(RpcRequest::GetHighestSnapshotSlot, Value::Null)
⋮----
pub async fn get_signature_status(
⋮----
self.get_signature_status_with_commitment(signature, self.commitment())
⋮----
pub async fn get_signature_statuses(
⋮----
let signatures: Vec<_> = signatures.iter().map(|s| s.to_string()).collect();
self.send(RpcRequest::GetSignatureStatuses, json!([signatures]))
⋮----
pub async fn get_signature_statuses_with_history(
⋮----
json!([signatures, {
⋮----
pub async fn get_signature_status_with_commitment(
⋮----
json!([[signature.to_string()]]),
⋮----
Ok(result.value[0]
.clone()
⋮----
.map(|status_meta| status_meta.status))
⋮----
pub async fn get_signature_status_with_commitment_and_history(
⋮----
json!([[signature.to_string()], {
⋮----
pub async fn get_slot(&self) -> ClientResult<Slot> {
self.get_slot_with_commitment(self.commitment()).await
⋮----
pub async fn get_slot_with_commitment(
⋮----
self.send(RpcRequest::GetSlot, json!([commitment_config]))
⋮----
pub async fn get_block_height(&self) -> ClientResult<u64> {
self.get_block_height_with_commitment(self.commitment())
⋮----
pub async fn get_block_height_with_commitment(
⋮----
self.send(RpcRequest::GetBlockHeight, json!([commitment_config]))
⋮----
pub async fn get_slot_leaders(
⋮----
self.send(RpcRequest::GetSlotLeaders, json!([start_slot, limit]))
⋮----
.and_then(|slot_leaders: Vec<String>| {
⋮----
.map(|slot_leader| {
Pubkey::from_str(slot_leader).map_err(|err| {
ClientErrorKind::Custom(format!("pubkey deserialization failed: {err}"))
.into()
⋮----
.collect()
⋮----
pub async fn get_block_production(&self) -> RpcResult<RpcBlockProduction> {
self.send(RpcRequest::GetBlockProduction, Value::Null).await
⋮----
pub async fn get_block_production_with_config(
⋮----
self.send(RpcRequest::GetBlockProduction, json!([config]))
⋮----
pub async fn supply(&self) -> RpcResult<RpcSupply> {
self.supply_with_commitment(self.commitment()).await
⋮----
pub async fn supply_with_commitment(
⋮----
self.send(RpcRequest::GetSupply, json!([commitment_config]))
⋮----
pub async fn get_largest_accounts_with_config(
⋮----
self.send(RpcRequest::GetLargestAccounts, json!([config]))
⋮----
pub async fn get_vote_accounts(&self) -> ClientResult<RpcVoteAccountStatus> {
self.get_vote_accounts_with_commitment(self.commitment())
⋮----
pub async fn get_vote_accounts_with_commitment(
⋮----
self.get_vote_accounts_with_config(RpcGetVoteAccountsConfig {
commitment: Some(commitment_config),
⋮----
pub async fn get_vote_accounts_with_config(
⋮----
self.send(RpcRequest::GetVoteAccounts, json!([config]))
⋮----
pub async fn wait_for_max_stake(
⋮----
self.wait_for_max_stake_below_threshold_with_timeout_helper(
⋮----
pub async fn wait_for_max_stake_below_threshold_with_timeout(
⋮----
Some(timeout),
⋮----
async fn wait_for_max_stake_below_threshold_with_timeout_helper(
⋮----
let vote_accounts = self.get_vote_accounts_with_commitment(commitment).await?;
⋮----
.chain(vote_accounts.delinquent.iter())
.map(|vote_account| {
⋮----
if start.elapsed() > timeout {
return Err(ClientErrorKind::Custom(
"timed out waiting for max stake to drop".to_string(),
⋮----
info!(
⋮----
sleep(Duration::from_secs(5)).await;
⋮----
Ok(())
⋮----
pub async fn get_cluster_nodes(&self) -> ClientResult<Vec<RpcContactInfo>> {
self.send(RpcRequest::GetClusterNodes, Value::Null).await
⋮----
pub async fn get_block(&self, slot: Slot) -> ClientResult<EncodedConfirmedBlock> {
self.get_block_with_encoding(slot, UiTransactionEncoding::Json)
⋮----
pub async fn get_block_with_encoding(
⋮----
self.send(RpcRequest::GetBlock, json!([slot, encoding]))
⋮----
pub async fn get_block_with_config(
⋮----
self.send(RpcRequest::GetBlock, json!([slot, config])).await
⋮----
pub async fn get_blocks(
⋮----
self.send(RpcRequest::GetBlocks, json!([start_slot, end_slot]))
⋮----
pub async fn get_blocks_with_commitment(
⋮----
let json = if end_slot.is_some() {
json!([start_slot, end_slot, commitment_config])
⋮----
json!([start_slot, commitment_config])
⋮----
self.send(RpcRequest::GetBlocks, json).await
⋮----
pub async fn get_blocks_with_limit(
⋮----
self.send(RpcRequest::GetBlocksWithLimit, json!([start_slot, limit]))
⋮----
pub async fn get_blocks_with_limit_and_commitment(
⋮----
json!([start_slot, limit, commitment_config]),
⋮----
pub async fn get_signatures_for_address(
⋮----
self.get_signatures_for_address_with_config(
⋮----
pub async fn get_signatures_for_address_with_config(
⋮----
before: config.before.map(|signature| signature.to_string()),
until: config.until.map(|signature| signature.to_string()),
⋮----
json!([address.to_string(), config]),
⋮----
Ok(result)
⋮----
pub async fn get_transaction(
⋮----
json!([signature.to_string(), encoding]),
⋮----
pub async fn get_transaction_with_config(
⋮----
json!([signature.to_string(), config]),
⋮----
pub async fn get_block_time(&self, slot: Slot) -> ClientResult<UnixTimestamp> {
⋮----
let response = self.send(request, json!([slot])).await;
⋮----
.map(|result_json: Value| {
if result_json.is_null() {
return Err(RpcError::ForUser(format!("Block Not Found: slot={slot}")).into());
⋮----
.map_err(|err| ClientError::new_with_request(err.into(), request))?;
trace!("Response block timestamp {slot:?} {result:?}");
⋮----
.map_err(|err| err.into_with_request(request))?
⋮----
pub async fn get_epoch_info(&self) -> ClientResult<EpochInfo> {
self.get_epoch_info_with_commitment(self.commitment()).await
⋮----
pub async fn get_epoch_info_with_commitment(
⋮----
self.send(RpcRequest::GetEpochInfo, json!([commitment_config]))
⋮----
pub async fn get_leader_schedule(
⋮----
self.get_leader_schedule_with_commitment(slot, self.commitment())
⋮----
pub async fn get_leader_schedule_with_commitment(
⋮----
self.get_leader_schedule_with_config(
⋮----
pub async fn get_leader_schedule_with_config(
⋮----
self.send(RpcRequest::GetLeaderSchedule, json!([slot, config]))
⋮----
pub async fn get_epoch_schedule(&self) -> ClientResult<EpochSchedule> {
self.send(RpcRequest::GetEpochSchedule, Value::Null).await
⋮----
pub async fn get_recent_performance_samples(
⋮----
self.send(RpcRequest::GetRecentPerformanceSamples, json!([limit]))
⋮----
pub async fn get_recent_prioritization_fees(
⋮----
.map(|address| address.to_string())
.collect();
self.send(RpcRequest::GetRecentPrioritizationFees, json!([addresses]))
⋮----
pub async fn get_identity(&self) -> ClientResult<Pubkey> {
let rpc_identity: RpcIdentity = self.send(RpcRequest::GetIdentity, Value::Null).await?;
rpc_identity.identity.parse::<Pubkey>().map_err(|_| {
⋮----
RpcError::ParseError("Pubkey".to_string()).into(),
⋮----
pub async fn get_inflation_governor(&self) -> ClientResult<RpcInflationGovernor> {
self.send(RpcRequest::GetInflationGovernor, Value::Null)
⋮----
pub async fn get_inflation_rate(&self) -> ClientResult<RpcInflationRate> {
self.send(RpcRequest::GetInflationRate, Value::Null).await
⋮----
pub async fn get_inflation_reward(
⋮----
json!([
⋮----
pub async fn get_version(&self) -> ClientResult<RpcVersionInfo> {
self.send(RpcRequest::GetVersion, Value::Null).await
⋮----
pub async fn minimum_ledger_slot(&self) -> ClientResult<Slot> {
self.send(RpcRequest::MinimumLedgerSlot, Value::Null).await
⋮----
pub async fn get_account(&self, pubkey: &Pubkey) -> ClientResult<Account> {
self.get_account_with_commitment(pubkey, self.commitment())
⋮----
.ok_or_else(|| RpcError::ForUser(format!("AccountNotFound: pubkey={pubkey}")).into())
⋮----
pub async fn get_account_with_commitment(
⋮----
encoding: Some(UiAccountEncoding::Base64Zstd),
⋮----
self.get_ui_account_with_config(pubkey, config)
⋮----
.map(|response| Response {
⋮----
value: response.value.map(|ui_account| {
ui_account.decode().expect(
⋮----
pub async fn get_account_with_config(
⋮----
json!([pubkey.to_string(), config]),
⋮----
return Err(
RpcError::ForUser(format!("AccountNotFound: pubkey={pubkey}")).into(),
⋮----
trace!("Response account {pubkey:?} {rpc_account:?}");
let account = rpc_account.and_then(|rpc_account| rpc_account.decode());
⋮----
.map_err(|err| {
Into::<ClientError>::into(RpcError::ForUser(format!(
⋮----
pub async fn get_ui_account_with_config(
⋮----
trace!("Response account {pubkey:?} {ui_account:?}");
⋮----
pub async fn get_max_retransmit_slot(&self) -> ClientResult<Slot> {
self.send(RpcRequest::GetMaxRetransmitSlot, Value::Null)
⋮----
pub async fn get_max_shred_insert_slot(&self) -> ClientResult<Slot> {
self.send(RpcRequest::GetMaxShredInsertSlot, Value::Null)
⋮----
pub async fn get_multiple_accounts(
⋮----
.get_multiple_accounts_with_commitment(pubkeys, self.commitment())
⋮----
pub async fn get_multiple_accounts_with_commitment(
⋮----
self.get_multiple_ui_accounts_with_config(
⋮----
.into_iter()
.map(|ui_account| {
ui_account.map(|ui_account| {
⋮----
.collect(),
⋮----
pub async fn get_multiple_accounts_with_config(
⋮----
commitment: config.commitment.or_else(|| Some(self.commitment())),
⋮----
let pubkeys: Vec<_> = pubkeys.iter().map(|pubkey| pubkey.to_string()).collect();
⋮----
.send(RpcRequest::GetMultipleAccounts, json!([pubkeys, config]))
⋮----
.map(|rpc_account| rpc_account.and_then(|a| a.decode()))
⋮----
pub async fn get_multiple_ui_accounts_with_config(
⋮----
pub async fn get_account_data(&self, pubkey: &Pubkey) -> ClientResult<Vec<u8>> {
Ok(self.get_account(pubkey).await?.data)
⋮----
pub async fn get_minimum_balance_for_rent_exemption(
⋮----
.send(request, json!([data_len]))
⋮----
.map_err(|err| err.into_with_request(request))?;
⋮----
trace!("Response minimum balance {data_len:?} {minimum_balance:?}");
Ok(minimum_balance)
⋮----
pub async fn get_balance(&self, pubkey: &Pubkey) -> ClientResult<u64> {
⋮----
.get_balance_with_commitment(pubkey, self.commitment())
⋮----
pub async fn get_balance_with_commitment(
⋮----
json!([pubkey.to_string(), commitment_config]),
⋮----
pub async fn get_program_accounts(
⋮----
self.get_program_ui_accounts_with_config(
⋮----
.map(|response| {
⋮----
.map(|(pubkey, ui_account)| {
⋮----
pub async fn get_program_accounts_with_config(
⋮----
.unwrap_or_else(|| self.commitment());
config.account_config.commitment = Some(commitment);
⋮----
.parse_value();
parse_keyed_accounts(accounts, RpcRequest::GetProgramAccounts)
⋮----
pub async fn get_program_ui_accounts_with_config(
⋮----
pubkey_ui_account_client_result_from_keyed_accounts(
⋮----
pub async fn get_stake_minimum_delegation(&self) -> ClientResult<u64> {
self.get_stake_minimum_delegation_with_commitment(self.commitment())
⋮----
pub async fn get_stake_minimum_delegation_with_commitment(
⋮----
json!([commitment_config]),
⋮----
pub async fn get_transaction_count(&self) -> ClientResult<u64> {
self.get_transaction_count_with_commitment(self.commitment())
⋮----
pub async fn get_transaction_count_with_commitment(
⋮----
self.send(RpcRequest::GetTransactionCount, json!([commitment_config]))
⋮----
pub async fn get_first_available_block(&self) -> ClientResult<Slot> {
self.send(RpcRequest::GetFirstAvailableBlock, Value::Null)
⋮----
pub async fn get_genesis_hash(&self) -> ClientResult<Hash> {
let hash_str: String = self.send(RpcRequest::GetGenesisHash, Value::Null).await?;
let hash = hash_str.parse().map_err(|_| {
⋮----
RpcError::ParseError("Hash".to_string()).into(),
⋮----
Ok(hash)
⋮----
pub async fn get_health(&self) -> ClientResult<()> {
⋮----
.map(|_| ())
⋮----
pub async fn get_token_account(&self, pubkey: &Pubkey) -> ClientResult<Option<UiTokenAccount>> {
⋮----
.get_token_account_with_commitment(pubkey, self.commitment())
⋮----
pub async fn get_token_account_with_commitment(
⋮----
encoding: Some(UiAccountEncoding::JsonParsed),
⋮----
return Ok(Response {
⋮----
value: Some(token_account),
⋮----
Err(Into::<ClientError>::into(RpcError::ForUser(format!(
⋮----
pub async fn get_token_account_balance(&self, pubkey: &Pubkey) -> ClientResult<UiTokenAmount> {
⋮----
.get_token_account_balance_with_commitment(pubkey, self.commitment())
⋮----
pub async fn get_token_account_balance_with_commitment(
⋮----
pub async fn get_token_accounts_by_delegate(
⋮----
.get_token_accounts_by_delegate_with_commitment(
⋮----
pub async fn get_token_accounts_by_delegate_with_commitment(
⋮----
TokenAccountsFilter::Mint(mint) => RpcTokenAccountsFilter::Mint(mint.to_string()),
⋮----
RpcTokenAccountsFilter::ProgramId(program_id.to_string())
⋮----
json!([delegate.to_string(), token_account_filter, config]),
⋮----
pub async fn get_token_accounts_by_owner(
⋮----
.get_token_accounts_by_owner_with_commitment(
⋮----
pub async fn get_token_accounts_by_owner_with_commitment(
⋮----
json!([owner.to_string(), token_account_filter, config]),
⋮----
pub async fn get_token_largest_accounts(
⋮----
.get_token_largest_accounts_with_commitment(mint, self.commitment())
⋮----
pub async fn get_token_largest_accounts_with_commitment(
⋮----
json!([mint.to_string(), commitment_config]),
⋮----
pub async fn get_token_supply(&self, mint: &Pubkey) -> ClientResult<UiTokenAmount> {
⋮----
.get_token_supply_with_commitment(mint, self.commitment())
⋮----
pub async fn get_token_supply_with_commitment(
⋮----
pub async fn request_airdrop(&self, pubkey: &Pubkey, lamports: u64) -> ClientResult<Signature> {
self.request_airdrop_with_config(
⋮----
pub async fn request_airdrop_with_blockhash(
⋮----
recent_blockhash: Some(recent_blockhash.to_string()),
⋮----
pub async fn request_airdrop_with_config(
⋮----
json!([pubkey.to_string(), lamports, config]),
⋮----
.and_then(|signature: String| {
Signature::from_str(&signature).map_err(|err| {
ClientErrorKind::Custom(format!("signature deserialization failed: {err}")).into()
⋮----
.map_err(|_| {
⋮----
pub(crate) async fn poll_balance_with_timeout_and_commitment(
⋮----
.get_balance_with_commitment(pubkey, commitment_config)
⋮----
return Ok(bal.value);
⋮----
sleep(*polling_frequency).await;
if now.elapsed() > *timeout {
return Err(e);
⋮----
pub async fn poll_get_balance_with_commitment(
⋮----
self.poll_balance_with_timeout_and_commitment(
⋮----
pub async fn wait_for_balance_with_commitment(
⋮----
.poll_get_balance_with_commitment(pubkey, commitment_config)
⋮----
if expected_balance.is_none() || (balance_result.is_err() && run == LAST) {
⋮----
trace!(
⋮----
return Ok(balance_result);
⋮----
pub async fn poll_for_signature(&self, signature: &Signature) -> ClientResult<()> {
self.poll_for_signature_with_commitment(signature, self.commitment())
⋮----
pub async fn poll_for_signature_with_commitment(
⋮----
.get_signature_status_with_commitment(signature, commitment_config)
⋮----
if now.elapsed().as_secs() > 15 {
return Err(RpcError::ForUser(format!(
⋮----
sleep(Duration::from_millis(250)).await;
⋮----
pub async fn poll_for_signature_confirmation(
⋮----
debug!("check_confirmations request failed: {err:?}");
⋮----
if now.elapsed().as_secs() > 20 {
⋮----
return Ok(confirmed_blocks);
⋮----
Ok(confirmed_blocks)
⋮----
pub async fn get_num_blocks_since_signature_confirmation(
⋮----
.ok_or_else(|| {
⋮----
ClientErrorKind::Custom("signature not found".to_string()),
⋮----
.unwrap_or(MAX_LOCKOUT_HISTORY + 1);
Ok(confirmations)
⋮----
pub async fn get_latest_blockhash(&self) -> ClientResult<Hash> {
⋮----
.get_latest_blockhash_with_commitment(self.commitment())
⋮----
Ok(blockhash)
⋮----
pub async fn get_latest_blockhash_with_commitment(
⋮----
.send::<Response<RpcBlockhash>>(RpcRequest::GetLatestBlockhash, json!([commitment]))
⋮----
let blockhash = blockhash.parse().map_err(|_| {
⋮----
Ok((blockhash, last_valid_block_height))
⋮----
pub async fn is_blockhash_valid(
⋮----
json!([blockhash.to_string(), commitment,]),
⋮----
pub async fn get_fee_for_message(
⋮----
let serialized = message.serialize();
let serialized_encoded = BASE64_STANDARD.encode(serialized);
⋮----
json!([serialized_encoded, self.commitment()]),
⋮----
.ok_or_else(|| ClientErrorKind::Custom("Invalid blockhash".to_string()).into())
⋮----
pub async fn get_new_latest_blockhash(&self, blockhash: &Hash) -> ClientResult<Hash> {
⋮----
while start.elapsed().as_secs() < 5 {
if let Ok(new_blockhash) = self.get_latest_blockhash().await {
⋮----
return Ok(new_blockhash);
⋮----
debug!("Got same blockhash ({blockhash:?}), will retry...");
sleep(Duration::from_millis(DEFAULT_MS_PER_SLOT / 2)).await;
⋮----
Err(RpcError::ForUser(format!(
⋮----
pub async fn send<T>(&self, request: RpcRequest, params: Value) -> ClientResult<T>
⋮----
assert!(params.is_array() || params.is_null());
⋮----
.send(request, params)
⋮----
.map_err(|err| ClientError::new_with_request(err.into(), request))
⋮----
pub fn get_transport_stats(&self) -> RpcTransportStats {
self.sender.get_transport_stats()
⋮----
fn serialize_and_encode<T>(input: &T, encoding: UiTransactionEncoding) -> ClientResult<String>
⋮----
let serialized = serialize(input)
.map_err(|e| ClientErrorKind::Custom(format!("Serialization failed: {e}")))?;
⋮----
UiTransactionEncoding::Base58 => bs58::encode(serialized).into_string(),
UiTransactionEncoding::Base64 => BASE64_STANDARD.encode(serialized),
⋮----
return Err(ClientErrorKind::Custom(format!(
⋮----
Ok(encoded)
⋮----
pub(crate) fn get_rpc_request_str(rpc_addr: SocketAddr, tls: bool) -> String {
⋮----
format!("https://{rpc_addr}")
⋮----
format!("http://{rpc_addr}")
⋮----
pub(crate) fn parse_keyed_accounts(
⋮----
let mut pubkey_accounts: Vec<(Pubkey, Account)> = Vec::with_capacity(accounts.len());
for RpcKeyedAccount { pubkey, account } in accounts.into_iter() {
let pubkey = pubkey.parse().map_err(|_| {
⋮----
pubkey_accounts.push((
⋮----
account.decode().ok_or_else(|| {
⋮----
RpcError::ParseError("Account from rpc".to_string()).into(),
⋮----
Ok(pubkey_accounts)
⋮----
fn pubkey_ui_account_client_result_from_keyed_accounts(
⋮----
let mut pubkey_ui_accounts: Vec<(Pubkey, UiAccount)> = Vec::with_capacity(accounts.len());
for RpcKeyedAccount { account, pubkey } in accounts.iter() {
⋮----
pubkey_ui_accounts.push((pubkey, account.clone()));
⋮----
Ok(pubkey_ui_accounts)
⋮----
pub fn create_rpc_client_mocks() -> crate::mock_sender::Mocks {
⋮----
let pubkey = Pubkey::from_str("BgvYtJEfmZYdVKiptmMjxGzv8iQoo4MWjsP3QsTkhhxa").unwrap();
mock_encoded_account(&pubkey)
⋮----
.unwrap();
mocks.insert(get_account_request, get_account_response);

================
File: rpc-client/src/http_sender.rs
================
pub struct HttpSender {
⋮----
impl HttpSender {
pub fn new<U: ToString>(url: U) -> Self {
⋮----
pub fn new_with_timeout<U: ToString>(url: U, timeout: Duration) -> Self {
⋮----
.default_headers(Self::default_headers())
.timeout(timeout)
.pool_idle_timeout(timeout)
.build()
.expect("build rpc client"),
⋮----
pub fn new_with_client<U: ToString>(url: U, client: reqwest::Client) -> Self {
⋮----
client: Arc::new(reqwest_middleware::ClientBuilder::new(client).build()),
url: url.to_string(),
⋮----
pub fn new_with_client_with_middleware<U: ToString>(
⋮----
pub fn default_headers() -> header::HeaderMap {
⋮----
default_headers.append(
⋮----
format!("rust/{}", solana_version::Version::default()).as_str(),
⋮----
.unwrap(),
⋮----
struct StatsUpdater<'a> {
⋮----
fn new(stats: &'a RwLock<RpcTransportStats>) -> Self {
⋮----
fn add_rate_limited_time(&mut self, duration: Duration) {
⋮----
impl Drop for StatsUpdater<'_> {
fn drop(&mut self) {
let mut stats = self.stats.write().unwrap();
⋮----
stats.elapsed_time += Instant::now().duration_since(self.request_start_time);
⋮----
impl RpcSender for HttpSender {
fn get_transport_stats(&self) -> RpcTransportStats {
self.stats.read().unwrap().clone()
⋮----
async fn send(
⋮----
let request_id = self.request_id.fetch_add(1, Ordering::Relaxed);
let request_json = request.build_request_json(request_id, params).to_string();
⋮----
let client = self.client.clone();
let request_json = request_json.clone();
⋮----
.post(&self.url)
.header(CONTENT_TYPE, "application/json")
.body(request_json)
.send()
⋮----
if !response.status().is_success() {
if response.status() == StatusCode::TOO_MANY_REQUESTS
⋮----
if let Some(retry_after) = response.headers().get(RETRY_AFTER) {
if let Ok(retry_after) = retry_after.to_str() {
⋮----
debug!(
⋮----
sleep(duration).await;
stats_updater.add_rate_limited_time(duration);
⋮----
return Err(response.error_for_status().unwrap_err().into());
⋮----
if json["error"].is_object() {
return match serde_json::from_value::<RpcErrorObject>(json["error"].clone()) {
⋮----
match serde_json::from_value::<RpcSimulateTransactionResult>(json["error"]["data"].clone()) {
⋮----
debug!("Failed to deserialize RpcSimulateTransactionResult: {err:?}");
⋮----
match serde_json::from_value::<custom_error::NodeUnhealthyErrorData>(json["error"]["data"].clone()) {
⋮----
Err(RpcError::RpcResponseError {
⋮----
.into())
⋮----
Err(err) => Err(RpcError::RpcRequestError(format!(
⋮----
.into()),
⋮----
return Ok(json["result"].take());
⋮----
fn url(&self) -> String {
self.url.clone()
⋮----
mod tests {
⋮----
async fn http_sender_on_tokio_multi_thread() {
let http_sender = HttpSender::new("http://localhost:1234".to_string());
⋮----
.send(RpcRequest::GetVersion, serde_json::Value::Null)
⋮----
async fn http_sender_on_tokio_current_thread() {

================
File: rpc-client/src/lib.rs
================
pub mod http_sender;
pub mod mock_sender;
pub mod nonblocking;
pub mod rpc_client;
pub mod rpc_sender;
pub mod spinner;
⋮----
pub mod mock_sender_for_cli {

================
File: rpc-client/src/mock_sender.rs
================
pub type Mocks = HashMap<RpcRequest, Value>;
⋮----
fn from(mocks: Mocks) -> Self {
⋮----
map.insert(key, [value].into());
⋮----
MocksMap(map)
⋮----
pub struct MocksMap(pub HashMap<RpcRequest, VecDeque<Value>>);
⋮----
fn from_iter<T: IntoIterator<Item = (RpcRequest, Value)>>(iter: T) -> Self {
⋮----
map.insert(request, value);
⋮----
impl MocksMap {
pub fn insert(&mut self, request: RpcRequest, value: Value) {
let queue = self.0.entry(request).or_default();
queue.push_back(value)
⋮----
pub fn pop_front_with_request(&mut self, request: &RpcRequest) -> Option<Value> {
self.0.get_mut(request).and_then(|queue| queue.pop_front())
⋮----
pub struct MockSender {
⋮----
impl MockSender {
pub fn new<U: ToString>(url: U) -> Self {
⋮----
pub fn new_with_mocks<U: ToString>(url: U, mocks: Mocks) -> Self {
⋮----
url: url.to_string(),
⋮----
pub fn new_with_mocks_map<U: ToString>(url: U, mocks: MocksMap) -> Self {
⋮----
impl RpcSender for MockSender {
fn get_transport_stats(&self) -> RpcTransportStats {
⋮----
async fn send(
⋮----
if let Some(value) = self.mocks.write().unwrap().pop_front_with_request(&request) {
return Ok(value);
⋮----
return Ok(Value::Null);
⋮----
let method = &request.build_request_json(42, params.clone())["method"];
let val = match method.as_str().unwrap() {
⋮----
transaction_count: Some(123),
⋮----
Err(TransactionError::AccountInUse)
⋮----
Err(TransactionError::InstructionError(
⋮----
Ok(())
⋮----
let err = status.clone().err();
Some(TransactionStatus {
⋮----
confirmation_status: Some(TransactionConfirmationStatus::Finalized),
⋮----
let statuses: Vec<Option<TransactionStatus>> = params.as_array().unwrap()[0]
.as_array()
.unwrap()
.iter()
.map(|_| status.clone())
.collect();
⋮----
version: Some(TransactionVersion::LEGACY),
⋮----
signatures: vec!["3AsdoALgZFuq2oUVWrDYhg2pNeaLJKPLf8hU2mQ6U8qJxeJ6hsrPVpMn9ma39DtfYCrDQSvngWRP8NnTpEhezJpE".to_string()],
⋮----
account_keys: vec![
⋮----
recent_blockhash: "D37n3BSG71oUWcWjbZ37jZP7UfsxG2QMKeuALJ1PYvM6".to_string(),
instructions: vec![UiCompiledInstruction {
⋮----
meta: Some(UiTransactionStatusMeta {
⋮----
status: Ok(()),
⋮----
pre_balances: vec![499999999999999950, 50, 1],
post_balances: vec![499999999999999950, 50, 1],
⋮----
block_time: Some(1628633791),
⋮----
"getTransactionCount" => json![1234],
"getSlot" => json![0],
"getMaxShredInsertSlot" => json![0],
"requestAirdrop" => Value::String(Signature::from([8; 64]).to_string()),
"getHighestSnapshotSlot" => json!(RpcSnapshotSlotInfo {
⋮----
"getSlotLeaders" => json!([PUBKEY]),
⋮----
if params.is_null() {
json!(Response {
⋮----
serde_json::from_value(params).unwrap();
let config = config[0].clone();
⋮----
by_identity.insert(config.identity.unwrap(), (1, 123));
let config_range = config.range.unwrap_or_default();
⋮----
"getStakeMinimumDelegation" => json!(Response {
⋮----
"getSupply" => json!(Response {
⋮----
address: PUBKEY.to_string(),
⋮----
json!(RpcVoteAccountStatus {
⋮----
Signature::from([8; 64]).to_string()
⋮----
let tx_str = params.as_array().unwrap()[0].as_str().unwrap().to_string();
let data = BASE64_STANDARD.decode(tx_str).unwrap();
let tx: Transaction = bincode::deserialize(&data).unwrap();
tx.signatures[0].to_string()
⋮----
"getMinimumBalanceForRentExemption" => json![20],
⋮----
json!(RpcVersionInfo {
⋮----
blockhash: PUBKEY.to_string(),
⋮----
value: json!(Some(0)),
⋮----
"getClusterNodes" => serde_json::to_value(vec![RpcContactInfo {
⋮----
previous_blockhash: "mfcyqEXB3DnHXki6KjjmZck6YjmZLvpAByy2fj4nh6B".to_string(),
blockhash: "3Eq21vXNB5s86c62bVuUfTeaMif1N2kUqRPBmGRJhyTA".to_string(),
⋮----
transactions: vec![EncodedTransactionWithStatusMeta {
⋮----
block_height: Some(428),
⋮----
"getBlocks" => serde_json::to_value(vec![1, 2, 3])?,
"getBlocksWithLimit" => serde_json::to_value(vec![1, 2, 3])?,
⋮----
serde_json::to_value(vec![RpcConfirmedTransactionStatusWithSignature {
⋮----
"getRecentPerformanceSamples" => serde_json::to_value(vec![RpcPerfSample {
⋮----
"getRecentPrioritizationFees" => serde_json::to_value(vec![RpcPrioritizationFee {
⋮----
identity: PUBKEY.to_string(),
⋮----
"getInflationReward" => serde_json::to_value(vec![
⋮----
"minimumLedgerSlot" => json![123],
"getMaxRetransmitSlot" => json![123],
⋮----
value: vec![Value::Null, Value::Null]
⋮----
let pubkey = Pubkey::from_str(PUBKEY).unwrap();
serde_json::to_value(vec![
⋮----
Ok(val)
⋮----
fn url(&self) -> String {
format!("MockSender: {}", self.url)
⋮----
pub(crate) fn mock_encoded_account(pubkey: &Pubkey) -> UiAccount {
⋮----
data: UiAccountData::Binary("".to_string(), UiAccountEncoding::Base64),
owner: pubkey.to_string(),
⋮----
space: Some(0),
⋮----
mod tests {
⋮----
fn test_mock_encoded_account() {
⋮----
data: vec![],
⋮----
let expected = encode_ui_account(&pubkey, &account, UiAccountEncoding::Base64, None, None);
assert_eq!(expected, mock_encoded_account(&pubkey));

================
File: rpc-client/src/rpc_client.rs
================
pub use crate::mock_sender::Mocks;
⋮----
pub struct RpcClientConfig {
⋮----
impl RpcClientConfig {
pub fn with_commitment(commitment_config: CommitmentConfig) -> Self {
⋮----
pub trait SerializableMessage {
⋮----
impl SerializableMessage for LegacyMessage {
fn serialize(&self) -> Vec<u8> {
self.serialize()
⋮----
impl SerializableMessage for v0::Message {
⋮----
pub trait SerializableTransaction: Serialize {
⋮----
impl SerializableTransaction for Transaction {
fn get_signature(&self) -> &Signature {
⋮----
fn get_recent_blockhash(&self) -> &Hash {
⋮----
fn uses_durable_nonce(&self) -> bool {
uses_durable_nonce(self).is_some()
⋮----
impl SerializableTransaction for VersionedTransaction {
⋮----
self.message.recent_blockhash()
⋮----
self.uses_durable_nonce()
⋮----
pub struct GetConfirmedSignaturesForAddress2Config {
⋮----
pub struct RpcClient {
⋮----
impl Drop for RpcClient {
fn drop(&mut self) {
self.runtime.take().expect("runtime").shutdown_background();
⋮----
impl RpcClient {
pub fn new_sender<T: RpcSender + Send + Sync + 'static>(
⋮----
runtime: Some(
⋮----
.thread_name("solRpcClient")
.enable_io()
.enable_time()
.build()
.unwrap(),
⋮----
/// Create an HTTP `RpcClient`.
    ///
⋮----
///
    /// The URL is an HTTP URL, usually for port 8899, as in
⋮----
/// The URL is an HTTP URL, usually for port 8899, as in
    /// "http://localhost:8899".
⋮----
/// "http://localhost:8899".
    ///
⋮----
///
    /// The client has a default timeout of 30 seconds, and a default [commitment
⋮----
/// The client has a default timeout of 30 seconds, and a default [commitment
    /// level][cl] of [`Finalized`].
⋮----
/// level][cl] of [`Finalized`].
    ///
⋮----
///
    /// [cl]: https://solana.com/docs/rpc#configuring-state-commitment
⋮----
/// [cl]: https://solana.com/docs/rpc#configuring-state-commitment
    /// [`Finalized`]: solana_commitment_config::CommitmentLevel::Finalized
⋮----
/// [`Finalized`]: solana_commitment_config::CommitmentLevel::Finalized
    ///
⋮----
///
    /// # Examples
⋮----
/// # Examples
    ///
⋮----
///
    /// ```
⋮----
/// ```
    /// # use solana_rpc_client::rpc_client::RpcClient;
⋮----
/// # use solana_rpc_client::rpc_client::RpcClient;
    /// let url = "http://localhost:8899".to_string();
⋮----
/// let url = "http://localhost:8899".to_string();
    /// let client = RpcClient::new(url);
⋮----
/// let client = RpcClient::new(url);
    /// ```
⋮----
/// ```
    pub fn new<U: ToString>(url: U) -> Self {
⋮----
pub fn new<U: ToString>(url: U) -> Self {
⋮----
/// Create an HTTP `RpcClient` with specified [commitment level][cl].
    ///
/// [cl]: https://solana.com/docs/rpc#configuring-state-commitment
    ///
⋮----
///
    /// The client has a default timeout of 30 seconds, and a user-specified
⋮----
/// The client has a default timeout of 30 seconds, and a user-specified
    /// [`CommitmentLevel`] via [`CommitmentConfig`].
⋮----
/// [`CommitmentLevel`] via [`CommitmentConfig`].
    ///
⋮----
///
    /// [`CommitmentLevel`]: solana_commitment_config::CommitmentLevel
⋮----
/// [`CommitmentLevel`]: solana_commitment_config::CommitmentLevel
    ///
⋮----
/// ```
    /// # use solana_commitment_config::CommitmentConfig;
⋮----
/// # use solana_commitment_config::CommitmentConfig;
    /// # use solana_rpc_client::rpc_client::RpcClient;
/// let url = "http://localhost:8899".to_string();
    /// let commitment_config = CommitmentConfig::processed();
⋮----
/// let commitment_config = CommitmentConfig::processed();
    /// let client = RpcClient::new_with_commitment(url, commitment_config);
⋮----
/// let client = RpcClient::new_with_commitment(url, commitment_config);
    /// ```
⋮----
/// ```
    pub fn new_with_commitment<U: ToString>(url: U, commitment_config: CommitmentConfig) -> Self {
⋮----
pub fn new_with_commitment<U: ToString>(url: U, commitment_config: CommitmentConfig) -> Self {
⋮----
/// Create an HTTP `RpcClient` with specified timeout.
    ///
⋮----
///
    /// The client has and a default [commitment level][cl] of
⋮----
/// The client has and a default [commitment level][cl] of
    /// [`Finalized`].
⋮----
/// [`Finalized`].
    ///
⋮----
/// ```
    /// # use std::time::Duration;
⋮----
/// # use std::time::Duration;
    /// # use solana_rpc_client::rpc_client::RpcClient;
⋮----
/// # use solana_rpc_client::rpc_client::RpcClient;
    /// let url = "http://localhost::8899".to_string();
⋮----
/// let url = "http://localhost::8899".to_string();
    /// let timeout = Duration::from_secs(1);
⋮----
/// let timeout = Duration::from_secs(1);
    /// let client = RpcClient::new_with_timeout(url, timeout);
⋮----
/// let client = RpcClient::new_with_timeout(url, timeout);
    /// ```
⋮----
/// ```
    pub fn new_with_timeout<U: ToString>(url: U, timeout: Duration) -> Self {
⋮----
pub fn new_with_timeout<U: ToString>(url: U, timeout: Duration) -> Self {
⋮----
/// Create an HTTP `RpcClient` with specified timeout and [commitment level][cl].
    ///
⋮----
/// # use solana_rpc_client::rpc_client::RpcClient;
    /// # use solana_commitment_config::CommitmentConfig;
⋮----
/// # use solana_commitment_config::CommitmentConfig;
    /// let url = "http://localhost::8899".to_string();
/// let timeout = Duration::from_secs(1);
    /// let commitment_config = CommitmentConfig::processed();
⋮----
/// let commitment_config = CommitmentConfig::processed();
    /// let client = RpcClient::new_with_timeout_and_commitment(
⋮----
/// let client = RpcClient::new_with_timeout_and_commitment(
    ///     url,
⋮----
///     url,
    ///     timeout,
⋮----
///     timeout,
    ///     commitment_config,
⋮----
///     commitment_config,
    /// );
⋮----
/// );
    /// ```
⋮----
/// ```
    pub fn new_with_timeout_and_commitment<U: ToString>(
⋮----
pub fn new_with_timeout_and_commitment<U: ToString>(
⋮----
///
    /// The `confirm_transaction_initial_timeout` argument specifies the amount of
⋮----
/// The `confirm_transaction_initial_timeout` argument specifies the amount of
    /// time to allow for the server to initially process a transaction, when
⋮----
/// time to allow for the server to initially process a transaction, when
    /// confirming a transaction via one of the `_with_spinner` methods, like
⋮----
/// confirming a transaction via one of the `_with_spinner` methods, like
    /// [`RpcClient::send_and_confirm_transaction_with_spinner`]. In
⋮----
/// [`RpcClient::send_and_confirm_transaction_with_spinner`]. In
    /// other words, setting `confirm_transaction_initial_timeout` to > 0 allows
⋮----
/// other words, setting `confirm_transaction_initial_timeout` to > 0 allows
    /// `RpcClient` to wait for confirmation of a transaction that the server
⋮----
/// `RpcClient` to wait for confirmation of a transaction that the server
    /// has not "seen" yet.
⋮----
/// has not "seen" yet.
    ///
⋮----
/// let commitment_config = CommitmentConfig::processed();
    /// let confirm_transaction_initial_timeout = Duration::from_secs(10);
⋮----
/// let confirm_transaction_initial_timeout = Duration::from_secs(10);
    /// let client = RpcClient::new_with_timeouts_and_commitment(
⋮----
/// let client = RpcClient::new_with_timeouts_and_commitment(
    ///     url,
⋮----
///     commitment_config,
    ///     confirm_transaction_initial_timeout,
⋮----
///     confirm_transaction_initial_timeout,
    /// );
/// ```
    pub fn new_with_timeouts_and_commitment<U: ToString>(
⋮----
pub fn new_with_timeouts_and_commitment<U: ToString>(
⋮----
confirm_transaction_initial_timeout: Some(confirm_transaction_initial_timeout),
⋮----
/// Create a mock `RpcClient`.
    ///
⋮----
///
    /// A mock `RpcClient` contains an implementation of [`RpcSender`] that does
⋮----
/// A mock `RpcClient` contains an implementation of [`RpcSender`] that does
    /// not use the network, and instead returns synthetic responses, for use in
⋮----
/// not use the network, and instead returns synthetic responses, for use in
    /// tests.
⋮----
/// tests.
    ///
⋮----
///
    /// It is primarily for internal use, with limited customizability, and
⋮----
/// It is primarily for internal use, with limited customizability, and
    /// behaviors determined by internal Solana test cases. New users should
⋮----
/// behaviors determined by internal Solana test cases. New users should
    /// consider implementing `RpcSender` themselves and constructing
⋮----
/// consider implementing `RpcSender` themselves and constructing
    /// `RpcClient` with [`RpcClient::new_sender`] to get mock behavior.
⋮----
/// `RpcClient` with [`RpcClient::new_sender`] to get mock behavior.
    ///
⋮----
///
    /// Unless directed otherwise, a mock `RpcClient` will generally return a
⋮----
/// Unless directed otherwise, a mock `RpcClient` will generally return a
    /// reasonable default response to any request, at least for [`RpcRequest`]
⋮----
/// reasonable default response to any request, at least for [`RpcRequest`]
    /// values for which responses have been implemented.
⋮----
/// values for which responses have been implemented.
    ///
⋮----
///
    /// This mock can be customized by changing the `url` argument, which is not
⋮----
/// This mock can be customized by changing the `url` argument, which is not
    /// actually a URL, but a simple string directive that changes the mock
⋮----
/// actually a URL, but a simple string directive that changes the mock
    /// behavior in specific scenarios:
⋮----
/// behavior in specific scenarios:
    ///
⋮----
///
    /// - It is customary to set the `url` to "succeeds" for mocks that should
⋮----
/// - It is customary to set the `url` to "succeeds" for mocks that should
    ///   return successfully, though this value is not actually interpreted.
⋮----
///   return successfully, though this value is not actually interpreted.
    ///
⋮----
///
    /// - If `url` is "fails" then any call to `send` will return `Ok(Value::Null)`.
⋮----
/// - If `url` is "fails" then any call to `send` will return `Ok(Value::Null)`.
    ///
⋮----
///
    /// - Other possible values of `url` are specific to different `RpcRequest`
⋮----
/// - Other possible values of `url` are specific to different `RpcRequest`
    ///   values. Read the implementation of (non-public) `MockSender` for
⋮----
///   values. Read the implementation of (non-public) `MockSender` for
    ///   details.
⋮----
///   details.
    ///
⋮----
///
    /// The [`RpcClient::new_mock_with_mocks`] function offers further
⋮----
/// The [`RpcClient::new_mock_with_mocks`] function offers further
    /// customization options.
⋮----
/// customization options.
    ///
⋮----
/// # use solana_rpc_client::rpc_client::RpcClient;
    /// // Create an `RpcClient` that always succeeds
⋮----
/// // Create an `RpcClient` that always succeeds
    /// let url = "succeeds".to_string();
⋮----
/// let url = "succeeds".to_string();
    /// let successful_client = RpcClient::new_mock(url);
⋮----
/// let successful_client = RpcClient::new_mock(url);
    /// ```
⋮----
/// ```
    ///
⋮----
/// # use solana_rpc_client::rpc_client::RpcClient;
    /// // Create an `RpcClient` that always fails
⋮----
/// // Create an `RpcClient` that always fails
    /// let url = "fails".to_string();
⋮----
/// let url = "fails".to_string();
    /// let successful_client = RpcClient::new_mock(url);
/// ```
    pub fn new_mock<U: ToString>(url: U) -> Self {
⋮----
pub fn new_mock<U: ToString>(url: U) -> Self {
⋮----
///
    /// This mock can be customized in two ways:
⋮----
/// This mock can be customized in two ways:
    ///
⋮----
///
    /// 1) By changing the `url` argument, which is not actually a URL, but a
⋮----
/// 1) By changing the `url` argument, which is not actually a URL, but a
    ///    simple string directive that changes the mock behavior in specific
⋮----
///    simple string directive that changes the mock behavior in specific
    ///    scenarios.
⋮----
///    scenarios.
    ///
⋮----
///
    ///    It is customary to set the `url` to "succeeds" for mocks that should
⋮----
///    It is customary to set the `url` to "succeeds" for mocks that should
    ///    return successfully, though this value is not actually interpreted.
⋮----
///    return successfully, though this value is not actually interpreted.
    ///
⋮----
///
    ///    If `url` is "fails" then any call to `send` will return `Ok(Value::Null)`.
⋮----
///    If `url` is "fails" then any call to `send` will return `Ok(Value::Null)`.
    ///
⋮----
///
    ///    Other possible values of `url` are specific to different `RpcRequest`
⋮----
///    Other possible values of `url` are specific to different `RpcRequest`
    ///    values. Read the implementation of `MockSender` (which is non-public)
⋮----
///    values. Read the implementation of `MockSender` (which is non-public)
    ///    for details.
⋮----
///    for details.
    ///
⋮----
///
    /// 2) Custom responses can be configured by providing [`Mocks`]. This type
⋮----
/// 2) Custom responses can be configured by providing [`Mocks`]. This type
    ///    is a [`HashMap`] from [`RpcRequest`] to a JSON [`Value`] response,
⋮----
///    is a [`HashMap`] from [`RpcRequest`] to a JSON [`Value`] response,
    ///    Any entries in this map override the default behavior for the given
⋮----
///    Any entries in this map override the default behavior for the given
    ///    request.
⋮----
///    request.
    ///
⋮----
///
    /// [`HashMap`]: std::collections::HashMap
⋮----
/// [`HashMap`]: std::collections::HashMap
    ///
⋮----
/// ```
    /// # use solana_rpc_client_api::{
⋮----
/// # use solana_rpc_client_api::{
    /// #     request::RpcRequest,
⋮----
/// #     request::RpcRequest,
    /// #     response::{Response, RpcResponseContext},
⋮----
/// #     response::{Response, RpcResponseContext},
    /// # };
⋮----
/// # };
    /// # use solana_rpc_client::rpc_client::RpcClient;
⋮----
/// # use solana_rpc_client::rpc_client::RpcClient;
    /// # use std::collections::HashMap;
⋮----
/// # use std::collections::HashMap;
    /// # use serde_json::json;
⋮----
/// # use serde_json::json;
    /// // Create a mock with a custom response to the `GetBalance` request
⋮----
/// // Create a mock with a custom response to the `GetBalance` request
    /// let account_balance = 50;
⋮----
/// let account_balance = 50;
    /// let account_balance_response = json!(Response {
⋮----
/// let account_balance_response = json!(Response {
    ///     context: RpcResponseContext { slot: 1, api_version: None },
⋮----
///     context: RpcResponseContext { slot: 1, api_version: None },
    ///     value: json!(account_balance),
⋮----
///     value: json!(account_balance),
    /// });
⋮----
/// });
    ///
⋮----
///
    /// let mut mocks = HashMap::new();
⋮----
/// let mut mocks = HashMap::new();
    /// mocks.insert(RpcRequest::GetBalance, account_balance_response);
⋮----
/// mocks.insert(RpcRequest::GetBalance, account_balance_response);
    /// let url = "succeeds".to_string();
⋮----
/// let url = "succeeds".to_string();
    /// let client = RpcClient::new_mock_with_mocks(url, mocks);
⋮----
/// let client = RpcClient::new_mock_with_mocks(url, mocks);
    /// ```
⋮----
/// ```
    pub fn new_mock_with_mocks<U: ToString>(url: U, mocks: Mocks) -> Self {
⋮----
pub fn new_mock_with_mocks<U: ToString>(url: U, mocks: Mocks) -> Self {
⋮----
///
    /// 2) Custom responses can be configured by providing [`MocksMap`]. This type
⋮----
/// 2) Custom responses can be configured by providing [`MocksMap`]. This type
    ///    is a [`HashMap`] from [`RpcRequest`] to a [`Vec`] of JSON [`Value`] responses,
⋮----
///    is a [`HashMap`] from [`RpcRequest`] to a [`Vec`] of JSON [`Value`] responses,
    ///    Any entries in this map override the default behavior for the given
⋮----
///
    /// The [`RpcClient::new_mock_with_mocks_map`] function offers further
⋮----
/// The [`RpcClient::new_mock_with_mocks_map`] function offers further
    /// customization options.
///
    ///
⋮----
/// # };
    /// # use solana_rpc_client::{rpc_client::RpcClient, mock_sender::MocksMap};
⋮----
/// # use solana_rpc_client::{rpc_client::RpcClient, mock_sender::MocksMap};
    /// # use serde_json::json;
/// // Create a mock with a custom response to the `GetBalance` request
    /// let account_balance_x = 50;
⋮----
/// let account_balance_x = 50;
    /// let account_balance_y = 100;
⋮----
/// let account_balance_y = 100;
    /// let account_balance_z = 150;
⋮----
/// let account_balance_z = 150;
    /// let account_balance_req_responses = vec![
⋮----
/// let account_balance_req_responses = vec![
    ///     (
⋮----
///     (
    ///         RpcRequest::GetBalance,
⋮----
///         RpcRequest::GetBalance,
    ///         json!(Response {
⋮----
///         json!(Response {
    ///             context: RpcResponseContext {
⋮----
///             context: RpcResponseContext {
    ///                 slot: 1,
⋮----
///                 slot: 1,
    ///                 api_version: None,
⋮----
///                 api_version: None,
    ///             },
⋮----
///             },
    ///             value: json!(account_balance_x),
⋮----
///             value: json!(account_balance_x),
    ///         })
⋮----
///         })
    ///     ),
⋮----
///     ),
    ///     (
⋮----
///             },
    ///             value: json!(account_balance_y),
⋮----
///             value: json!(account_balance_y),
    ///         })
///     ),
    /// ];
⋮----
/// ];
    ///
⋮----
///
    /// let mut mocks = MocksMap::from_iter(account_balance_req_responses);
⋮----
/// let mut mocks = MocksMap::from_iter(account_balance_req_responses);
    /// mocks.insert(
⋮----
/// mocks.insert(
    ///     RpcRequest::GetBalance,
⋮----
///     RpcRequest::GetBalance,
    ///     json!(Response {
⋮----
///     json!(Response {
    ///         context: RpcResponseContext {
⋮----
///         context: RpcResponseContext {
    ///             slot: 1,
⋮----
///             slot: 1,
    ///             api_version: None,
⋮----
///             api_version: None,
    ///         },
⋮----
///         },
    ///         value: json!(account_balance_z),
⋮----
///         value: json!(account_balance_z),
    ///     }),
⋮----
///     }),
    /// );
⋮----
/// );
    /// let url = "succeeds".to_string();
⋮----
/// let url = "succeeds".to_string();
    /// let client = RpcClient::new_mock_with_mocks_map(url, mocks);
⋮----
/// let client = RpcClient::new_mock_with_mocks_map(url, mocks);
    /// ```
⋮----
/// ```
    pub fn new_mock_with_mocks_map<U: ToString>(url: U, mocks: MocksMap) -> Self {
⋮----
pub fn new_mock_with_mocks_map<U: ToString>(url: U, mocks: MocksMap) -> Self {
⋮----
/// Create an HTTP `RpcClient` from a [`SocketAddr`].
    ///
⋮----
/// ```
    /// # use std::net::{Ipv4Addr, SocketAddr};
⋮----
/// # use std::net::{Ipv4Addr, SocketAddr};
    /// # use solana_rpc_client::rpc_client::RpcClient;
⋮----
/// # use solana_rpc_client::rpc_client::RpcClient;
    /// let addr = SocketAddr::from((Ipv4Addr::LOCALHOST, 8899));
⋮----
/// let addr = SocketAddr::from((Ipv4Addr::LOCALHOST, 8899));
    /// let client = RpcClient::new_socket(addr);
⋮----
/// let client = RpcClient::new_socket(addr);
    /// ```
⋮----
/// ```
    pub fn new_socket(addr: SocketAddr) -> Self {
⋮----
pub fn new_socket(addr: SocketAddr) -> Self {
Self::new(get_rpc_request_str(addr, false))
⋮----
/// Create an HTTP `RpcClient` from a [`SocketAddr`] with specified [commitment level][cl].
    ///
⋮----
/// # use solana_commitment_config::CommitmentConfig;
    /// let addr = SocketAddr::from((Ipv4Addr::LOCALHOST, 8899));
⋮----
/// let addr = SocketAddr::from((Ipv4Addr::LOCALHOST, 8899));
    /// let commitment_config = CommitmentConfig::processed();
⋮----
/// let commitment_config = CommitmentConfig::processed();
    /// let client = RpcClient::new_socket_with_commitment(
⋮----
/// let client = RpcClient::new_socket_with_commitment(
    ///     addr,
⋮----
///     addr,
    ///     commitment_config
⋮----
///     commitment_config
    /// );
/// ```
    pub fn new_socket_with_commitment(
⋮----
pub fn new_socket_with_commitment(
⋮----
Self::new_with_commitment(get_rpc_request_str(addr, false), commitment_config)
⋮----
/// Create an HTTP `RpcClient` from a [`SocketAddr`] with specified timeout.
    ///
⋮----
///
    /// The client has a default [commitment level][cl] of [`Finalized`].
⋮----
/// The client has a default [commitment level][cl] of [`Finalized`].
    ///
⋮----
/// # use std::net::{Ipv4Addr, SocketAddr};
    /// # use std::time::Duration;
⋮----
/// let addr = SocketAddr::from((Ipv4Addr::LOCALHOST, 8899));
    /// let timeout = Duration::from_secs(1);
⋮----
/// let timeout = Duration::from_secs(1);
    /// let client = RpcClient::new_socket_with_timeout(addr, timeout);
⋮----
/// let client = RpcClient::new_socket_with_timeout(addr, timeout);
    /// ```
⋮----
/// ```
    pub fn new_socket_with_timeout(addr: SocketAddr, timeout: Duration) -> Self {
⋮----
pub fn new_socket_with_timeout(addr: SocketAddr, timeout: Duration) -> Self {
let url = get_rpc_request_str(addr, false);
⋮----
/// Get the configured url of the client's sender
    pub fn url(&self) -> String {
⋮----
pub fn url(&self) -> String {
(self.rpc_client.as_ref()).url()
⋮----
pub fn commitment(&self) -> CommitmentConfig {
(self.rpc_client.as_ref()).commitment()
⋮----
pub fn send_and_confirm_transaction(
⋮----
self.invoke((self.rpc_client.as_ref()).send_and_confirm_transaction(transaction))
⋮----
pub fn send_and_confirm_transaction_with_spinner(
⋮----
self.invoke(
(self.rpc_client.as_ref()).send_and_confirm_transaction_with_spinner(transaction),
⋮----
pub fn send_and_confirm_transaction_with_spinner_and_commitment(
⋮----
(self.rpc_client.as_ref())
.send_and_confirm_transaction_with_spinner_and_commitment(transaction, commitment),
⋮----
pub fn send_and_confirm_transaction_with_spinner_and_config(
⋮----
(self.rpc_client.as_ref()).send_and_confirm_transaction_with_spinner_and_config(
⋮----
pub fn send_transaction(
⋮----
self.invoke((self.rpc_client.as_ref()).send_transaction(transaction))
⋮----
pub fn send_transaction_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).send_transaction_with_config(transaction, config))
⋮----
pub fn send<T>(&self, request: RpcRequest, params: Value) -> ClientResult<T>
⋮----
self.invoke((self.rpc_client.as_ref()).send(request, params))
⋮----
pub fn confirm_transaction(&self, signature: &Signature) -> ClientResult<bool> {
self.invoke((self.rpc_client.as_ref()).confirm_transaction(signature))
⋮----
pub fn confirm_transaction_with_commitment(
⋮----
.confirm_transaction_with_commitment(signature, commitment_config),
⋮----
pub fn confirm_transaction_with_spinner(
⋮----
self.invoke((self.rpc_client.as_ref()).confirm_transaction_with_spinner(
⋮----
pub fn simulate_transaction(
⋮----
self.invoke((self.rpc_client.as_ref()).simulate_transaction(transaction))
⋮----
pub fn simulate_transaction_with_config(
⋮----
(self.rpc_client.as_ref()).simulate_transaction_with_config(transaction, config),
⋮----
pub fn simulate_bundle(
⋮----
self.invoke((self.rpc_client.as_ref()).simulate_bundle(bundle))
⋮----
pub fn simulate_bundle_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).simulate_bundle_with_config(bundle, config))
⋮----
pub fn get_highest_snapshot_slot(&self) -> ClientResult<RpcSnapshotSlotInfo> {
self.invoke((self.rpc_client.as_ref()).get_highest_snapshot_slot())
⋮----
pub fn get_signature_status(
⋮----
self.invoke((self.rpc_client.as_ref()).get_signature_status(signature))
⋮----
pub fn get_signature_statuses(
⋮----
self.invoke((self.rpc_client.as_ref()).get_signature_statuses(signatures))
⋮----
pub fn get_signature_statuses_with_history(
⋮----
self.invoke((self.rpc_client.as_ref()).get_signature_statuses_with_history(signatures))
⋮----
pub fn get_signature_status_with_commitment(
⋮----
.get_signature_status_with_commitment(signature, commitment_config),
⋮----
pub fn get_signature_status_with_commitment_and_history(
⋮----
(self.rpc_client.as_ref()).get_signature_status_with_commitment_and_history(
⋮----
pub fn get_slot(&self) -> ClientResult<Slot> {
self.invoke((self.rpc_client.as_ref()).get_slot())
⋮----
pub fn get_slot_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).get_slot_with_commitment(commitment_config))
⋮----
pub fn get_block_height(&self) -> ClientResult<u64> {
self.invoke((self.rpc_client.as_ref()).get_block_height())
⋮----
pub fn get_block_height_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).get_block_height_with_commitment(commitment_config))
⋮----
pub fn get_slot_leaders(&self, start_slot: Slot, limit: u64) -> ClientResult<Vec<Pubkey>> {
self.invoke((self.rpc_client.as_ref()).get_slot_leaders(start_slot, limit))
⋮----
pub fn get_block_production(&self) -> RpcResult<RpcBlockProduction> {
self.invoke((self.rpc_client.as_ref()).get_block_production())
⋮----
pub fn get_block_production_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_block_production_with_config(config))
⋮----
pub fn supply(&self) -> RpcResult<RpcSupply> {
self.invoke((self.rpc_client.as_ref()).supply())
⋮----
pub fn supply_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).supply_with_commitment(commitment_config))
⋮----
pub fn get_largest_accounts_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_largest_accounts_with_config(config))
⋮----
pub fn get_vote_accounts(&self) -> ClientResult<RpcVoteAccountStatus> {
self.invoke((self.rpc_client.as_ref()).get_vote_accounts())
⋮----
pub fn get_vote_accounts_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).get_vote_accounts_with_commitment(commitment_config))
⋮----
pub fn get_vote_accounts_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_vote_accounts_with_config(config))
⋮----
pub fn wait_for_max_stake(
⋮----
self.invoke((self.rpc_client.as_ref()).wait_for_max_stake(commitment, max_stake_percent))
⋮----
pub fn wait_for_max_stake_below_threshold_with_timeout(
⋮----
(self.rpc_client.as_ref()).wait_for_max_stake_below_threshold_with_timeout(
⋮----
pub fn get_cluster_nodes(&self) -> ClientResult<Vec<RpcContactInfo>> {
self.invoke((self.rpc_client.as_ref()).get_cluster_nodes())
⋮----
pub fn get_block(&self, slot: Slot) -> ClientResult<EncodedConfirmedBlock> {
self.invoke((self.rpc_client.as_ref()).get_block(slot))
⋮----
pub fn get_block_with_encoding(
⋮----
self.invoke((self.rpc_client.as_ref()).get_block_with_encoding(slot, encoding))
⋮----
pub fn get_block_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_block_with_config(slot, config))
⋮----
pub fn get_blocks(&self, start_slot: Slot, end_slot: Option<Slot>) -> ClientResult<Vec<Slot>> {
self.invoke((self.rpc_client.as_ref()).get_blocks(start_slot, end_slot))
⋮----
pub fn get_blocks_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).get_blocks_with_commitment(
⋮----
pub fn get_blocks_with_limit(&self, start_slot: Slot, limit: usize) -> ClientResult<Vec<Slot>> {
self.invoke((self.rpc_client.as_ref()).get_blocks_with_limit(start_slot, limit))
⋮----
pub fn get_blocks_with_limit_and_commitment(
⋮----
(self.rpc_client.as_ref()).get_blocks_with_limit_and_commitment(
⋮----
pub fn get_signatures_for_address(
⋮----
self.invoke((self.rpc_client.as_ref()).get_signatures_for_address(address))
⋮----
pub fn get_signatures_for_address_with_config(
⋮----
(self.rpc_client.as_ref()).get_signatures_for_address_with_config(address, config),
⋮----
pub fn get_transaction(
⋮----
self.invoke((self.rpc_client.as_ref()).get_transaction(signature, encoding))
⋮----
pub fn get_transaction_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_transaction_with_config(signature, config))
⋮----
pub fn get_block_time(&self, slot: Slot) -> ClientResult<UnixTimestamp> {
self.invoke((self.rpc_client.as_ref()).get_block_time(slot))
⋮----
pub fn get_epoch_info(&self) -> ClientResult<EpochInfo> {
self.invoke((self.rpc_client.as_ref()).get_epoch_info())
⋮----
pub fn get_epoch_info_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).get_epoch_info_with_commitment(commitment_config))
⋮----
pub fn get_leader_schedule(
⋮----
self.invoke((self.rpc_client.as_ref()).get_leader_schedule(slot))
⋮----
pub fn get_leader_schedule_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_leader_schedule_with_commitment(slot, commitment_config),
⋮----
pub fn get_leader_schedule_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_leader_schedule_with_config(slot, config))
⋮----
pub fn get_epoch_schedule(&self) -> ClientResult<EpochSchedule> {
self.invoke((self.rpc_client.as_ref()).get_epoch_schedule())
⋮----
pub fn get_recent_performance_samples(
⋮----
self.invoke((self.rpc_client.as_ref()).get_recent_performance_samples(limit))
⋮----
pub fn get_recent_prioritization_fees(
⋮----
self.invoke((self.rpc_client.as_ref()).get_recent_prioritization_fees(addresses))
⋮----
pub fn get_identity(&self) -> ClientResult<Pubkey> {
self.invoke((self.rpc_client.as_ref()).get_identity())
⋮----
pub fn get_inflation_governor(&self) -> ClientResult<RpcInflationGovernor> {
self.invoke((self.rpc_client.as_ref()).get_inflation_governor())
⋮----
pub fn get_inflation_rate(&self) -> ClientResult<RpcInflationRate> {
self.invoke((self.rpc_client.as_ref()).get_inflation_rate())
⋮----
pub fn get_inflation_reward(
⋮----
self.invoke((self.rpc_client.as_ref()).get_inflation_reward(addresses, epoch))
⋮----
pub fn get_version(&self) -> ClientResult<RpcVersionInfo> {
self.invoke((self.rpc_client.as_ref()).get_version())
⋮----
pub fn minimum_ledger_slot(&self) -> ClientResult<Slot> {
self.invoke((self.rpc_client.as_ref()).minimum_ledger_slot())
⋮----
pub fn get_account(&self, pubkey: &Pubkey) -> ClientResult<Account> {
self.invoke((self.rpc_client.as_ref()).get_account(pubkey))
⋮----
pub fn get_account_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_account_with_commitment(pubkey, commitment_config),
⋮----
pub fn get_account_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_account_with_config(pubkey, config))
⋮----
pub fn get_ui_account_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_ui_account_with_config(pubkey, config))
⋮----
pub fn get_max_retransmit_slot(&self) -> ClientResult<Slot> {
self.invoke((self.rpc_client.as_ref()).get_max_retransmit_slot())
⋮----
pub fn get_max_shred_insert_slot(&self) -> ClientResult<Slot> {
self.invoke((self.rpc_client.as_ref()).get_max_shred_insert_slot())
⋮----
pub fn get_multiple_accounts(&self, pubkeys: &[Pubkey]) -> ClientResult<Vec<Option<Account>>> {
self.invoke((self.rpc_client.as_ref()).get_multiple_accounts(pubkeys))
⋮----
pub fn get_multiple_accounts_with_commitment(
⋮----
.get_multiple_accounts_with_commitment(pubkeys, commitment_config),
⋮----
pub fn get_multiple_accounts_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_multiple_accounts_with_config(pubkeys, config))
⋮----
pub fn get_multiple_ui_accounts_with_config(
⋮----
(self.rpc_client.as_ref()).get_multiple_ui_accounts_with_config(pubkeys, config),
⋮----
pub fn get_account_data(&self, pubkey: &Pubkey) -> ClientResult<Vec<u8>> {
self.invoke((self.rpc_client.as_ref()).get_account_data(pubkey))
⋮----
pub fn get_minimum_balance_for_rent_exemption(&self, data_len: usize) -> ClientResult<u64> {
self.invoke((self.rpc_client.as_ref()).get_minimum_balance_for_rent_exemption(data_len))
⋮----
pub fn get_balance(&self, pubkey: &Pubkey) -> ClientResult<u64> {
self.invoke((self.rpc_client.as_ref()).get_balance(pubkey))
⋮----
pub fn get_balance_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_balance_with_commitment(pubkey, commitment_config),
⋮----
pub fn get_program_accounts(&self, pubkey: &Pubkey) -> ClientResult<Vec<(Pubkey, Account)>> {
self.invoke((self.rpc_client.as_ref()).get_program_accounts(pubkey))
⋮----
pub fn get_program_accounts_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_program_accounts_with_config(pubkey, config))
⋮----
pub fn get_program_ui_accounts_with_config(
⋮----
self.invoke((self.rpc_client.as_ref()).get_program_ui_accounts_with_config(pubkey, config))
⋮----
pub fn get_stake_minimum_delegation(&self) -> ClientResult<u64> {
self.invoke((self.rpc_client.as_ref()).get_stake_minimum_delegation())
⋮----
pub fn get_stake_minimum_delegation_with_commitment(
⋮----
.get_stake_minimum_delegation_with_commitment(commitment_config),
⋮----
pub fn get_transaction_count(&self) -> ClientResult<u64> {
self.invoke((self.rpc_client.as_ref()).get_transaction_count())
⋮----
pub fn get_transaction_count_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_transaction_count_with_commitment(commitment_config),
⋮----
pub fn get_first_available_block(&self) -> ClientResult<Slot> {
self.invoke((self.rpc_client.as_ref()).get_first_available_block())
⋮----
pub fn get_genesis_hash(&self) -> ClientResult<Hash> {
self.invoke((self.rpc_client.as_ref()).get_genesis_hash())
⋮----
pub fn get_health(&self) -> ClientResult<()> {
self.invoke((self.rpc_client.as_ref()).get_health())
⋮----
pub fn get_token_account(&self, pubkey: &Pubkey) -> ClientResult<Option<UiTokenAccount>> {
self.invoke((self.rpc_client.as_ref()).get_token_account(pubkey))
⋮----
pub fn get_token_account_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_token_account_with_commitment(pubkey, commitment_config),
⋮----
pub fn get_token_account_balance(&self, pubkey: &Pubkey) -> ClientResult<UiTokenAmount> {
self.invoke((self.rpc_client.as_ref()).get_token_account_balance(pubkey))
⋮----
pub fn get_token_account_balance_with_commitment(
⋮----
.get_token_account_balance_with_commitment(pubkey, commitment_config),
⋮----
pub fn get_token_accounts_by_delegate(
⋮----
.get_token_accounts_by_delegate(delegate, token_account_filter),
⋮----
pub fn get_token_accounts_by_delegate_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_token_accounts_by_delegate_with_commitment(
⋮----
pub fn get_token_accounts_by_owner(
⋮----
(self.rpc_client.as_ref()).get_token_accounts_by_owner(owner, token_account_filter),
⋮----
pub fn get_token_accounts_by_owner_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_token_accounts_by_owner_with_commitment(
⋮----
pub fn get_token_largest_accounts(
⋮----
self.invoke((self.rpc_client.as_ref()).get_token_largest_accounts(mint))
⋮----
pub fn get_token_largest_accounts_with_commitment(
⋮----
.get_token_largest_accounts_with_commitment(mint, commitment_config),
⋮----
pub fn get_token_supply(&self, mint: &Pubkey) -> ClientResult<UiTokenAmount> {
self.invoke((self.rpc_client.as_ref()).get_token_supply(mint))
⋮----
pub fn get_token_supply_with_commitment(
⋮----
(self.rpc_client.as_ref()).get_token_supply_with_commitment(mint, commitment_config),
⋮----
pub fn request_airdrop(&self, pubkey: &Pubkey, lamports: u64) -> ClientResult<Signature> {
self.invoke((self.rpc_client.as_ref()).request_airdrop(pubkey, lamports))
⋮----
pub fn request_airdrop_with_blockhash(
⋮----
self.invoke((self.rpc_client.as_ref()).request_airdrop_with_blockhash(
⋮----
pub fn request_airdrop_with_config(
⋮----
(self.rpc_client.as_ref()).request_airdrop_with_config(pubkey, lamports, config),
⋮----
pub fn poll_get_balance_with_commitment(
⋮----
(self.rpc_client.as_ref()).poll_get_balance_with_commitment(pubkey, commitment_config),
⋮----
pub fn wait_for_balance_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).wait_for_balance_with_commitment(
⋮----
.ok()
⋮----
pub fn poll_for_signature(&self, signature: &Signature) -> ClientResult<()> {
self.invoke((self.rpc_client.as_ref()).poll_for_signature(signature))
⋮----
pub fn poll_for_signature_with_commitment(
⋮----
.poll_for_signature_with_commitment(signature, commitment_config),
⋮----
pub fn poll_for_signature_confirmation(
⋮----
.poll_for_signature_confirmation(signature, min_confirmed_blocks),
⋮----
pub fn get_num_blocks_since_signature_confirmation(
⋮----
(self.rpc_client.as_ref()).get_num_blocks_since_signature_confirmation(signature),
⋮----
pub fn get_latest_blockhash(&self) -> ClientResult<Hash> {
self.invoke((self.rpc_client.as_ref()).get_latest_blockhash())
⋮----
pub fn get_latest_blockhash_with_commitment(
⋮----
self.invoke((self.rpc_client.as_ref()).get_latest_blockhash_with_commitment(commitment))
⋮----
pub fn is_blockhash_valid(
⋮----
self.invoke((self.rpc_client.as_ref()).is_blockhash_valid(blockhash, commitment))
⋮----
pub fn get_fee_for_message(&self, message: &impl SerializableMessage) -> ClientResult<u64> {
self.invoke((self.rpc_client.as_ref()).get_fee_for_message(message))
⋮----
pub fn get_new_latest_blockhash(&self, blockhash: &Hash) -> ClientResult<Hash> {
self.invoke((self.rpc_client.as_ref()).get_new_latest_blockhash(blockhash))
⋮----
pub fn get_transport_stats(&self) -> RpcTransportStats {
(self.rpc_client.as_ref()).get_transport_stats()
⋮----
pub fn get_feature_activation_slot(&self, feature_id: &Pubkey) -> ClientResult<Option<Slot>> {
self.get_account_with_commitment(feature_id, self.commitment())
.and_then(|maybe_feature_account| {
⋮----
.map(|feature_account| {
bincode::deserialize(feature_account.data()).map_err(|_| {
⋮----
"Failed to deserialize feature account".to_string(),
⋮----
.transpose()
⋮----
.map(|maybe_feature: Option<Feature>| {
maybe_feature.and_then(|feature| feature.activated_at)
⋮----
fn invoke<T, F: std::future::Future<Output = ClientResult<T>>>(&self, f: F) -> ClientResult<T> {
tokio::task::block_in_place(move || self.runtime.as_ref().expect("runtime").block_on(f))
⋮----
pub fn get_inner_client(&self) -> &Arc<nonblocking::rpc_client::RpcClient> {
⋮----
pub fn runtime(&self) -> &tokio::runtime::Runtime {
self.runtime.as_ref().expect("runtime")
⋮----
pub fn create_rpc_client_mocks() -> crate::mock_sender::Mocks {
⋮----
let pubkey = Pubkey::from_str("BgvYtJEfmZYdVKiptmMjxGzv8iQoo4MWjsP3QsTkhhxa").unwrap();
mock_encoded_account(&pubkey)
⋮----
.unwrap();
mocks.insert(get_account_request, get_account_response);
⋮----
mod tests {
⋮----
fn test_send() {
_test_send();
⋮----
async fn test_send_async_current_thread() {
⋮----
async fn test_send_async_multi_thread() {
⋮----
fn _test_send() {
let (sender, receiver) = unbounded();
⋮----
let rpc_addr = "0.0.0.0:0".parse().unwrap();
⋮----
io.add_method("getBalance", |_params: Params| {
⋮----
io.add_method("getLatestBlockhash", |params: Params| {
⋮----
"deadbeefXjn8o3yroDHxUtKsZZgoy4GPkPPXfouKNHhx".to_string(),
⋮----
.threads(1)
.cors(DomainsValidation::AllowOnly(vec![
⋮----
.start_http(&rpc_addr)
.expect("Unable to start RPC server");
sender.send(*server.address()).unwrap();
server.wait();
⋮----
let rpc_addr = receiver.recv().unwrap();
⋮----
.send(
⋮----
json!(["deadbeefXjn8o3yroDHxUtKsZZgoy4GPkPPXfouKNHhx"]),
⋮----
assert_eq!(balance, 50);
⋮----
.send(RpcRequest::GetLatestBlockhash, Value::Null)
⋮----
assert_eq!(blockhash, "deadbeefXjn8o3yroDHxUtKsZZgoy4GPkPPXfouKNHhx");
⋮----
rpc_client.send(RpcRequest::GetLatestBlockhash, json!(["parameter"]));
assert!(blockhash.is_err());
⋮----
fn test_send_transaction() {
let rpc_client = RpcClient::new_mock("succeeds".to_string());
⋮----
let signature = rpc_client.send_transaction(&tx);
assert_eq!(signature.unwrap(), tx.signatures[0]);
let rpc_client = RpcClient::new_mock("fails".to_string());
⋮----
assert!(signature.is_err());
let rpc_client = RpcClient::new_mock("malicious".to_string());
⋮----
fn test_custom_request() {
⋮----
let slot = rpc_client.get_slot().unwrap();
assert_eq!(slot, 0);
⋮----
assert_eq!(slot, custom_slot);
⋮----
fn test_get_signature_status() {
⋮----
let status = rpc_client.get_signature_status(&signature).unwrap();
assert_eq!(status, Some(Ok(())));
let rpc_client = RpcClient::new_mock("sig_not_found".to_string());
⋮----
assert_eq!(status, None);
let rpc_client = RpcClient::new_mock("account_in_use".to_string());
⋮----
assert_eq!(status, Some(Err(TransactionError::AccountInUse)));
⋮----
fn test_send_and_confirm_transaction() {
⋮----
let result = rpc_client.send_and_confirm_transaction(&tx);
result.unwrap();
⋮----
assert!(result.is_err());
let rpc_client = RpcClient::new_mock("instruction_error".to_string());
⋮----
assert_matches!(
⋮----
if let ErrorKind::Io(err) = result.unwrap_err().kind() {
assert_eq!(err.kind(), io::ErrorKind::Other);
⋮----
fn test_rpc_client_thread() {
⋮----
fn get_block_production_with_config_no_error() -> ClientResult<()> {
⋮----
identity: Some(Keypair::new().pubkey().to_string()),
⋮----
let prod = rpc_client.get_block_production_with_config(config)?.value;
assert!(!prod.by_identity.is_empty());
Ok(())
⋮----
fn test_get_latest_blockhash() {
⋮----
let expected_blockhash: Hash = PUBKEY.parse().unwrap();
let blockhash = rpc_client.get_latest_blockhash().expect("blockhash ok");
assert_eq!(blockhash, expected_blockhash);
⋮----
let is_err = rpc_client.get_latest_blockhash().is_err();
assert!(is_err);
⋮----
fn test_get_stake_minimum_delegation() {
⋮----
let actual_minimum_delegation = rpc_client.get_stake_minimum_delegation().unwrap();
assert_eq!(expected_minimum_delegation, actual_minimum_delegation);
⋮----
.get_stake_minimum_delegation_with_commitment(CommitmentConfig::confirmed())
⋮----
fn test_get_program_accounts_with_config() {
⋮----
data: vec![],
⋮----
pubkey: pubkey.to_string(),
account: encode_ui_account(&pubkey, &account, UiAccountEncoding::Base64, None, None),
⋮----
let expected_result = vec![(pubkey, account.clone())];
⋮----
serde_json::to_value(OptionalContext::NoContext(vec![keyed_account.clone()]))
⋮----
.into_iter()
.collect();
let rpc_client = RpcClient::new_mock_with_mocks("mock_client".to_string(), mocks);
⋮----
.get_program_accounts_with_config(
⋮----
encoding: Some(UiAccountEncoding::Base64),
⋮----
assert_eq!(expected_result, result);
⋮----
value: vec![keyed_account.clone()],
⋮----
with_context: Some(true),
⋮----
let expected_result = vec![
⋮----
mocks.insert(
⋮----
value: vec![
⋮----
let rpc_client = RpcClient::new_mock_with_mocks_map("mock_client".to_string(), mocks);
⋮----
assert_eq!(result1.len(), 1);
⋮----
assert_eq!(result2.len(), 1);
⋮----
assert_eq!(result_3.len(), 3);
⋮----
assert_eq!(result_4.len(), 0);
result1.extend(result2);
result1.extend(result_3);
assert_eq!(expected_result, result1);
⋮----
fn test_get_program_ui_accounts_with_config() {
⋮----
data: UiAccountData::Binary("".to_string(), UiAccountEncoding::Base64),
owner: program_id.to_string(),
⋮----
space: Some(0),
⋮----
account: account.clone(),
⋮----
// Test: without context
⋮----
.get_program_ui_accounts_with_config(
⋮----
fn test_get_fee_for_message_sends_properly_serialized_v0_transaction<M>(message: M)
⋮----
let serialized_message = message.serialize();
let serialized_message_base64 = BASE64_STANDARD.encode(serialized_message);
⋮----
io.add_method("getFeeForMessage", move |params: Params| match params {
⋮----
let first_element = p.first().unwrap();
⋮----
assert_eq!(actual_serialized_message, &serialized_message_base64);
return future::ok(json!(Response {
⋮----
panic!("Expected an array of params to be forwarded to `getFeeForMessage");
⋮----
let fee: u64 = rpc_client.get_fee_for_message(&message).unwrap();
assert_eq!(fee, 42);

================
File: rpc-client/src/rpc_sender.rs
================
pub struct RpcTransportStats {
⋮----
pub trait RpcSender {

================
File: rpc-client/src/spinner.rs
================
pub fn new_progress_bar() -> ProgressBar {
⋮----
progress_bar.set_style(
⋮----
.template("{spinner:.green} {wide_msg}")
.expect("ProgressStyle::template direct input to be correct"),
⋮----
progress_bar.enable_steady_tick(Duration::from_millis(100));
⋮----
pub struct SendTransactionProgress {
⋮----
impl SendTransactionProgress {
pub fn set_message_for_confirmed_transactions(&self, progress_bar: &ProgressBar, status: &str) {
progress_bar.set_message(format!(

================
File: rpc-client/Cargo.toml
================
[package]
name = "solana-rpc-client"
description = "Solana RPC Client"
documentation = "https://docs.rs/solana-rpc-client"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = ["spinner"]
agave-unstable-api = []
# Support rpc-client methods that feature a spinner progress bar for
# command-line interfaces
spinner = ["dep:indicatif"]

[dependencies]
async-trait = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
bs58 = { workspace = true }
futures = { workspace = true }
indicatif = { workspace = true, optional = true }
log = { workspace = true }
reqwest = { workspace = true, features = ["blocking", "brotli", "deflate", "gzip", "rustls-tls", "json"] }
reqwest-middleware = { workspace = true }
semver = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account = { workspace = true }
solana-account-decoder = { workspace = true }
solana-account-decoder-client-types = { workspace = true, features = ["zstd"] }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-epoch-info = { workspace = true, features = ["serde"] }
solana-epoch-schedule = { workspace = true, features = ["serde"] }
solana-feature-gate-interface = { workspace = true, features = ["serde"] }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-message = { workspace = true }
solana-pubkey = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-signature = { workspace = true }
solana-transaction = { workspace = true, features = ["bincode"] }
solana-transaction-error = { workspace = true }
solana-transaction-status-client-types = { workspace = true }
solana-version = { workspace = true }
solana-vote-interface = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[dev-dependencies]
assert_matches = { workspace = true }
crossbeam-channel = { workspace = true }
futures = { workspace = true }
jsonrpc-core = { workspace = true }
jsonrpc-http-server = { workspace = true }
solana-account-decoder = { workspace = true }
solana-keypair = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-signer = { workspace = true }
solana-system-transaction = { workspace = true }
static_assertions = { workspace = true }
test-case = { workspace = true }

================
File: rpc-client-api/src/bundles.rs
================
pub enum RpcBundleSimulationSummary {
⋮----
pub enum RpcBundleExecutionError {
⋮----
pub struct RpcSimulateBundleResult {
⋮----
pub struct RpcSimulateBundleTransactionResult {
⋮----
pub struct RpcSimulateBundleConfig {
⋮----
pub enum SimulationSlotConfig {
⋮----
impl Default for SimulationSlotConfig {
fn default() -> Self {
⋮----
pub struct RpcBundleRequest {

================
File: rpc-client-api/src/client_error.rs
================
pub enum ErrorKind {
⋮----
impl ErrorKind {
pub fn get_transaction_error(&self) -> Option<TransactionError> {
⋮----
}) => Some(tx_err.clone().into()),
Self::TransactionError(tx_err) => Some(tx_err.clone()),
⋮----
fn from(err: TransportError) -> Self {
⋮----
fn from(client_error_kind: ErrorKind) -> Self {
⋮----
ErrorKind::Reqwest(err) => Self::Custom(format!("{err:?}")),
ErrorKind::RpcError(err) => Self::Custom(format!("{err:?}")),
ErrorKind::SerdeJson(err) => Self::Custom(format!("{err:?}")),
ErrorKind::SigningError(err) => Self::Custom(format!("{err:?}")),
ErrorKind::Custom(err) => Self::Custom(format!("{err:?}")),
ErrorKind::Middleware(err) => Self::Custom(format!("{err:?}")),
⋮----
pub struct Error {
⋮----
impl Error {
pub fn new_with_request(kind: ErrorKind, request: request::RpcRequest) -> Self {
⋮----
request: Some(request),
⋮----
pub fn into_with_request(self, request: request::RpcRequest) -> Self {
⋮----
pub fn request(&self) -> Option<&request::RpcRequest> {
self.request.as_ref()
⋮----
pub fn kind(&self) -> &ErrorKind {
self.kind.as_ref()
⋮----
self.kind.get_transaction_error()
⋮----
fn from(kind: ErrorKind) -> Self {
⋮----
kind: Box::new(err.into()),
⋮----
fn from(client_error: Error) -> Self {
(*client_error.kind).into()
⋮----
fn from(err: std::io::Error) -> Self {
⋮----
fn from(err: reqwest::Error) -> Self {
⋮----
fn from(err: reqwest_middleware::Error) -> Self {
⋮----
reqwest_middleware::Error::Reqwest(err) => err.into(),
⋮----
fn from(err: request::RpcError) -> Self {
⋮----
fn from(err: serde_json::error::Error) -> Self {
⋮----
fn from(err: SignerError) -> Self {
⋮----
fn from(err: TransactionError) -> Self {
⋮----
pub type Result<T> = std::result::Result<T, Error>;

================
File: rpc-client-api/src/custom_error.rs
================
pub enum RpcCustomError {
⋮----
pub struct NodeUnhealthyErrorData {
⋮----
pub struct MinContextSlotNotReachedErrorData {
⋮----
pub struct EpochRewardsPeriodActiveErrorData {
⋮----
fn from(err: EncodeError) -> Self {
⋮----
fn from(e: RpcCustomError) -> Self {
⋮----
message: format!(
⋮----
data: Some(serde_json::json!(result)),
⋮----
message: "Transaction signature verification failure".to_string(),
⋮----
message: format!("Block not available for slot {slot}"),
⋮----
format!("Node is behind by {num_slots_behind} slots")
⋮----
"Node is unhealthy".to_string()
⋮----
data: Some(serde_json::json!(NodeUnhealthyErrorData {
⋮----
message: format!("Transaction precompile verification failure {e:?}"),
⋮----
message: "No snapshot".to_string(),
⋮----
message: format!("Slot {slot} was skipped, or missing in long-term storage"),
⋮----
message: "Transaction history is not available from this node".to_string(),
⋮----
message: "Transaction signature length mismatch".to_string(),
⋮----
message: format!("Block status not yet available for slot {slot}"),
⋮----
message: "Minimum context slot has not been reached".to_string(),
data: Some(serde_json::json!(MinContextSlotNotReachedErrorData {
⋮----
message: format!("Epoch rewards period still active at slot {slot}"),
data: Some(serde_json::json!(EpochRewardsPeriodActiveErrorData {
⋮----
data: Some(serde_json::json!({
⋮----
message: "Failed to query long-term storage; please try again".to_string(),
⋮----
mod tests {
⋮----
fn test_deseriailze_epoch_rewards_period_active_error_data(serialized_data: Value) {
⋮----
.get("currentBlockHeight")
.map(|v| v.as_u64().unwrap())
.unwrap();
⋮----
.get("rewardsCompleteBlockHeight")
⋮----
let expected_slot: Option<u64> = serialized_data.get("slot").map(|v| v.as_u64().unwrap());
⋮----
serde_json::from_value(serialized_data).expect("Failed to deserialize test fixture");
assert_eq!(

================
File: rpc-client-api/src/lib.rs
================
pub mod bundles;
pub mod client_error;
pub mod custom_error;
pub mod response;

================
File: rpc-client-api/src/response.rs
================
use crate::client_error;
⋮----
pub type RpcResult<T> = client_error::Result<Response<T>>;

================
File: rpc-client-api/Cargo.toml
================
[package]
name = "solana-rpc-client-api"
description = "Solana Client Common Utilities"
documentation = "https://docs.rs/solana-rpc-client-api"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
anyhow = { workspace = true }
jsonrpc-core = { workspace = true }
reqwest = { workspace = true, default-features = false, features = ["rustls-tls"] }
reqwest-middleware = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account-decoder-client-types = { workspace = true }
# solana-bundle = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-rpc-client-types = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-svm = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status-client-types = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
test-case = { workspace = true }

================
File: rpc-client-nonce-utils/src/nonblocking/blockhash_query.rs
================
pub enum Source {
⋮----
impl Source {
pub async fn get_blockhash(
⋮----
.get_latest_blockhash_with_commitment(commitment)
⋮----
Ok(blockhash)
⋮----
.and_then(|ref a| nonblocking::data_from_account(a))?;
Ok(data.blockhash())
⋮----
pub async fn is_blockhash_valid(
⋮----
Ok(match self {
Self::Cluster => rpc_client.is_blockhash_valid(blockhash, commitment).await?,
⋮----
pub enum BlockhashQuery {
⋮----
impl BlockhashQuery {
pub fn new(blockhash: Option<Hash>, sign_only: bool, nonce_account: Option<Pubkey>) -> Self {
⋮----
.map(Source::NonceAccount)
.unwrap_or(Source::Cluster);
⋮----
_ => panic!("Cannot resolve blockhash"),
⋮----
pub fn new_from_matches(matches: &ArgMatches<'_>) -> Self {
let blockhash = value_of(matches, BLOCKHASH_ARG.name);
let sign_only = matches.is_present(SIGN_ONLY_ARG.name);
let nonce_account = pubkey_of(matches, NONCE_ARG.name);
⋮----
BlockhashQuery::Static(hash) => Ok(*hash),
⋮----
.is_blockhash_valid(rpc_client, hash, commitment)
⋮----
return Err(format!("Hash has expired {hash:?}").into());
⋮----
Ok(*hash)
⋮----
BlockhashQuery::Rpc(source) => source.get_blockhash(rpc_client, commitment).await,
⋮----
impl Default for BlockhashQuery {
fn default() -> Self {
⋮----
mod tests {
⋮----
use clap::App;
⋮----
fn test_blockhash_query_new_ok() {
let blockhash = hash(&[1u8]);
⋮----
assert_eq!(
⋮----
fn test_blockhash_query_new_no_nonce_fail() {
⋮----
fn test_blockhash_query_new_nonce_fail() {
⋮----
BlockhashQuery::new(None, true, Some(nonce_pubkey));
⋮----
fn test_blockhash_query_new_from_matches_ok() {
⋮----
.nonce_args(false)
.offline_args();
⋮----
let blockhash_string = blockhash.to_string();
let matches = test_commands.clone().get_matches_from(vec![
⋮----
.clone()
.get_matches_from(vec!["blockhash_query_test"]);
⋮----
let nonce_string = nonce_pubkey.to_string();
⋮----
fn test_blockhash_query_new_from_matches_without_nonce_fail() {
⋮----
.arg(blockhash_arg())
.arg(sign_only_arg().requires(""));
let matches = test_commands.get_matches_from(vec!["blockhash_query_test", "--sign-only"]);
⋮----
fn test_blockhash_query_new_from_matches_with_nonce_fail() {
⋮----
let matches = test_commands.get_matches_from(vec![
⋮----
async fn test_blockhash_query_get_blockhash() {
let test_blockhash = hash(&[0u8]);
let rpc_blockhash = hash(&[1u8]);
let get_latest_blockhash_response = json!(Response {
⋮----
let is_blockhash_valid_response = json!(Response {
⋮----
mocks.insert(
⋮----
get_latest_blockhash_response.clone(),
⋮----
let rpc_client = RpcClient::new_mock_with_mocks("".to_string(), mocks);
⋮----
is_blockhash_valid_response.clone(),
⋮----
let rpc_client = RpcClient::new_mock("fails".to_string());
assert!(BlockhashQuery::default()
⋮----
let nonce_blockhash = *durable_nonce.as_hash();
⋮----
.unwrap();
⋮----
let rpc_nonce_account = encode_ui_account(
⋮----
let get_account_response = json!(Response {
⋮----
mocks.insert(RpcRequest::GetAccountInfo, get_account_response.clone());
⋮----
mocks.insert(RpcRequest::GetAccountInfo, get_account_response);
⋮----
assert!(BlockhashQuery::Rpc(Source::NonceAccount(nonce_pubkey))

================
File: rpc-client-nonce-utils/src/nonblocking/mod.rs
================
pub mod blockhash_query;
⋮----
pub enum Error {
⋮----
pub async fn get_account(rpc_client: &RpcClient, nonce_pubkey: &Pubkey) -> Result<Account, Error> {
get_account_with_commitment(rpc_client, nonce_pubkey, CommitmentConfig::default()).await
⋮----
pub async fn get_account_with_commitment(
⋮----
.get_account_with_commitment(nonce_pubkey, commitment)
⋮----
.map_err(|e| Error::Client(format!("{e}")))
.and_then(|result| {
⋮----
.ok_or_else(|| Error::Client(format!("AccountNotFound: pubkey={nonce_pubkey}")))
⋮----
.and_then(|a| account_identity_ok(&a).map(|()| a))
⋮----
pub fn account_identity_ok<T: ReadableAccount>(account: &T) -> Result<(), Error> {
if account.owner() != &solana_sdk_ids::system_program::ID {
Err(Error::InvalidAccountOwner)
} else if account.data().is_empty() {
Err(Error::UnexpectedDataSize)
⋮----
Ok(())
⋮----
pub fn state_from_account<T: ReadableAccount + StateMut<Versions>>(
⋮----
account_identity_ok(account)?;
let versions = StateMut::<Versions>::state(account).map_err(|_| Error::InvalidAccountData)?;
Ok(State::from(versions))
⋮----
pub fn data_from_account<T: ReadableAccount + StateMut<Versions>>(
⋮----
state_from_account(account).and_then(|ref s| data_from_state(s).cloned())
⋮----
pub fn data_from_state(state: &State) -> Result<&Data, Error> {
⋮----
State::Uninitialized => Err(Error::InvalidStateForOperation),
State::Initialized(data) => Ok(data),

================
File: rpc-client-nonce-utils/src/blockhash_query.rs
================
pub enum Source {
⋮----
impl Source {
pub fn get_blockhash(
⋮----
let (blockhash, _) = rpc_client.get_latest_blockhash_with_commitment(commitment)?;
Ok(blockhash)
⋮----
.and_then(|ref a| crate::data_from_account(a))?;
Ok(data.blockhash())
⋮----
pub fn is_blockhash_valid(
⋮----
Ok(match self {
Self::Cluster => rpc_client.is_blockhash_valid(blockhash, commitment)?,
⋮----
pub enum BlockhashQuery {
⋮----
impl BlockhashQuery {
pub fn new(blockhash: Option<Hash>, sign_only: bool, nonce_account: Option<Pubkey>) -> Self {
⋮----
.map(Source::NonceAccount)
.unwrap_or(Source::Cluster);
⋮----
_ => panic!("Cannot resolve blockhash"),
⋮----
pub fn new_from_matches(matches: &ArgMatches<'_>) -> Self {
let blockhash = value_of(matches, BLOCKHASH_ARG.name);
let sign_only = matches.is_present(SIGN_ONLY_ARG.name);
let nonce_account = pubkey_of(matches, NONCE_ARG.name);
⋮----
BlockhashQuery::None(hash) => Ok(*hash),
⋮----
if !source.is_blockhash_valid(rpc_client, hash, commitment)? {
return Err(format!("Hash has expired {hash:?}").into());
⋮----
Ok(*hash)
⋮----
BlockhashQuery::All(source) => source.get_blockhash(rpc_client, commitment),
⋮----
impl Default for BlockhashQuery {
fn default() -> Self {
⋮----
mod tests {
⋮----
use clap::App;
⋮----
fn test_blockhash_query_new_ok() {
let blockhash = hash(&[1u8]);
⋮----
assert_eq!(
⋮----
fn test_blockhash_query_new_no_nonce_fail() {
⋮----
fn test_blockhash_query_new_nonce_fail() {
⋮----
BlockhashQuery::new(None, true, Some(nonce_pubkey));
⋮----
fn test_blockhash_query_new_from_matches_ok() {
⋮----
.nonce_args(false)
.offline_args();
⋮----
let blockhash_string = blockhash.to_string();
let matches = test_commands.clone().get_matches_from(vec![
⋮----
.clone()
.get_matches_from(vec!["blockhash_query_test"]);
⋮----
let nonce_string = nonce_pubkey.to_string();
⋮----
fn test_blockhash_query_new_from_matches_without_nonce_fail() {
⋮----
.arg(blockhash_arg())
.arg(sign_only_arg().requires(""));
let matches = test_commands.get_matches_from(vec!["blockhash_query_test", "--sign-only"]);
⋮----
fn test_blockhash_query_new_from_matches_with_nonce_fail() {
⋮----
let matches = test_commands.get_matches_from(vec![
⋮----
fn test_blockhash_query_get_blockhash() {
let test_blockhash = hash(&[0u8]);
let rpc_blockhash = hash(&[1u8]);
let get_latest_blockhash_response = json!(Response {
⋮----
let is_blockhash_valid_response = json!(Response {
⋮----
mocks.insert(
⋮----
get_latest_blockhash_response.clone(),
⋮----
let rpc_client = RpcClient::new_mock_with_mocks("".to_string(), mocks);
⋮----
is_blockhash_valid_response.clone(),
⋮----
let rpc_client = RpcClient::new_mock("fails".to_string());
assert!(BlockhashQuery::default()
⋮----
let nonce_blockhash = *durable_nonce.as_hash();
⋮----
.unwrap();
⋮----
let rpc_nonce_account = encode_ui_account(
⋮----
let get_account_response = json!(Response {
⋮----
mocks.insert(RpcRequest::GetAccountInfo, get_account_response.clone());
⋮----
mocks.insert(RpcRequest::GetAccountInfo, get_account_response);
⋮----
assert!(BlockhashQuery::All(Source::NonceAccount(nonce_pubkey))

================
File: rpc-client-nonce-utils/src/lib.rs
================
pub mod blockhash_query;
pub mod nonblocking;
⋮----
pub fn get_account(rpc_client: &RpcClient, nonce_pubkey: &Pubkey) -> Result<Account, Error> {
get_account_with_commitment(rpc_client, nonce_pubkey, CommitmentConfig::default())
⋮----
pub fn get_account_with_commitment(
⋮----
.get_account_with_commitment(nonce_pubkey, commitment)
.map_err(|e| Error::Client(format!("{e}")))
.and_then(|result| {
⋮----
.ok_or_else(|| Error::Client(format!("AccountNotFound: pubkey={nonce_pubkey}")))
⋮----
.and_then(|a| account_identity_ok(&a).map(|()| a))

================
File: rpc-client-nonce-utils/Cargo.toml
================
[package]
name = "solana-rpc-client-nonce-utils"
description = "Solana RPC Client Nonce Utilities"
documentation = "https://docs.rs/solana-rpc-client-nonce-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = []
agave-unstable-api = []
clap = ["dep:clap", "dep:solana-clap-utils"]

[dependencies]
clap = { version = "2.33.0", optional = true }
solana-account = { workspace = true, features = ["bincode"] }
solana-clap-utils = { workspace = true, optional = true }
solana-commitment-config = { workspace = true }
solana-hash = { workspace = true }
solana-message = { workspace = true }
solana-nonce = { workspace = true, features = ["serde"] }
solana-pubkey = { workspace = true }
solana-rpc-client = { workspace = true }
solana-sdk-ids = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
anyhow = { workspace = true }
futures = { workspace = true }
serde_json = { workspace = true }
solana-account-decoder = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-keypair = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true }
tokio = { workspace = true, features = ["full"] }

================
File: rpc-client-types/src/config.rs
================
pub struct RpcSignatureStatusConfig {
⋮----
pub struct RpcSendTransactionConfig {
⋮----
pub struct RpcSimulateTransactionAccountsConfig {
⋮----
pub struct RpcSimulateTransactionConfig {
⋮----
pub struct RpcRequestAirdropConfig {
⋮----
pub struct RpcLeaderScheduleConfig {
⋮----
pub struct RpcBlockProductionConfigRange {
⋮----
pub struct RpcBlockProductionConfig {
⋮----
pub struct RpcGetVoteAccountsConfig {
⋮----
pub enum RpcLeaderScheduleConfigWrapper {
⋮----
impl RpcLeaderScheduleConfigWrapper {
pub fn unzip(&self) -> (Option<Slot>, Option<RpcLeaderScheduleConfig>) {
⋮----
RpcLeaderScheduleConfigWrapper::ConfigOnly(config) => (None, config.clone()),
⋮----
pub enum RpcLargestAccountsFilter {
⋮----
pub struct RpcLargestAccountsConfig {
⋮----
pub struct RpcSupplyConfig {
⋮----
pub struct RpcEpochConfig {
⋮----
pub enum RpcAccountIndex {
⋮----
pub struct RpcAccountInfoConfig {
⋮----
pub struct RpcProgramAccountsConfig {
⋮----
pub enum RpcTransactionLogsFilter {
⋮----
pub struct RpcTransactionLogsConfig {
⋮----
pub enum RpcTokenAccountsFilter {
⋮----
pub struct RpcSignatureSubscribeConfig {
⋮----
pub enum RpcBlockSubscribeFilter {
⋮----
pub struct RpcBlockSubscribeConfig {
⋮----
pub struct RpcSignaturesForAddressConfig {
⋮----
pub enum RpcEncodingConfigWrapper<T> {
⋮----
pub fn convert_to_current(&self) -> T {
⋮----
RpcEncodingConfigWrapper::Current(config) => config.unwrap_or_default(),
⋮----
pub fn convert<U: EncodingConfig + From<T>>(&self) -> RpcEncodingConfigWrapper<U> {
⋮----
RpcEncodingConfigWrapper::Current(config.map(|config| config.into()))
⋮----
pub trait EncodingConfig {
⋮----
pub struct RpcBlockConfig {
⋮----
impl EncodingConfig for RpcBlockConfig {
fn new_with_encoding(encoding: &Option<UiTransactionEncoding>) -> Self {
⋮----
impl RpcBlockConfig {
pub fn rewards_only() -> Self {
⋮----
transaction_details: Some(TransactionDetails::None),
⋮----
pub fn rewards_with_commitment(commitment: Option<CommitmentConfig>) -> Self {
⋮----
fn from(config: RpcBlockConfig) -> Self {
RpcEncodingConfigWrapper::Current(Some(config))
⋮----
pub struct RpcTransactionConfig {
⋮----
impl EncodingConfig for RpcTransactionConfig {
⋮----
pub enum RpcBlocksConfigWrapper {
⋮----
impl RpcBlocksConfigWrapper {
pub fn unzip(&self) -> (Option<Slot>, Option<RpcContextConfig>) {
⋮----
pub struct RpcContextConfig {

================
File: rpc-client-types/src/error_object.rs
================
pub struct RpcErrorObject {

================
File: rpc-client-types/src/filter.rs
================
pub enum RpcFilterType {
⋮----
impl RpcFilterType {
pub fn verify(&self) -> Result<(), RpcFilterError> {
⋮----
RpcFilterType::DataSize(_) => Ok(()),
⋮----
if bytes.len() > MAX_DATA_BASE58_SIZE {
return Err(RpcFilterError::DataTooLarge);
⋮----
let bytes = bs58::decode(&bytes).into_vec()?;
if bytes.len() > MAX_DATA_SIZE {
Err(RpcFilterError::DataTooLarge)
⋮----
Ok(())
⋮----
if bytes.len() > MAX_DATA_BASE64_SIZE {
⋮----
let bytes = BASE64_STANDARD.decode(bytes)?;
⋮----
RpcFilterType::TokenAccountState => Ok(()),
⋮----
pub enum RpcFilterError {
⋮----
pub enum MemcmpEncodedBytes {
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
enum DataType {
⋮----
enum RpcMemcmpEncoding {
⋮----
struct RpcMemcmpInner {
⋮----
DataType::Encoded(bytes) => match data.encoding.unwrap_or(RpcMemcmpEncoding::Base58) {
⋮----
Ok(memcmp_encoded_bytes)
⋮----
pub struct Memcmp {
/// Data offset to begin match
    offset: usize,
/// Bytes, encoded with specified encoding
    #[serde(flatten)]
⋮----
impl Memcmp {
pub fn new(offset: usize, encoded_bytes: MemcmpEncodedBytes) -> Self {
⋮----
pub fn new_raw_bytes(offset: usize, bytes: Vec<u8>) -> Self {
⋮----
pub fn new_base58_encoded(offset: usize, bytes: &[u8]) -> Self {
⋮----
bytes: MemcmpEncodedBytes::Base58(bs58::encode(bytes).into_string()),
⋮----
pub fn offset(&self) -> usize {
⋮----
pub fn bytes(&self) -> Option<Cow<'_, Vec<u8>>> {
⋮----
Base58(bytes) => bs58::decode(bytes).into_vec().ok().map(Cow::Owned),
Base64(bytes) => BASE64_STANDARD.decode(bytes).ok().map(Cow::Owned),
Bytes(bytes) => Some(Cow::Borrowed(bytes)),
⋮----
pub fn convert_to_raw_bytes(&mut self) -> Result<(), RpcFilterError> {
⋮----
let bytes = bs58::decode(bytes).into_vec()?;
self.bytes = Bytes(bytes);
⋮----
_ => Ok(()),
⋮----
pub fn bytes_match(&self, data: &[u8]) -> bool {
match self.bytes() {
⋮----
if self.offset > data.len() {
⋮----
if data[self.offset..].len() < bytes.len() {
⋮----
data[self.offset..self.offset + bytes.len()] == bytes[..]
⋮----
pub fn raw_bytes_as_ref(&self) -> Option<&[u8]> {
⋮----
Some(bytes)
⋮----
mod tests {
⋮----
fn test_worst_case_encoded_tx_goldens() {
let ff_data = vec![0xffu8; MAX_DATA_SIZE];
let data58 = bs58::encode(&ff_data).into_string();
assert_eq!(data58.len(), MAX_DATA_BASE58_SIZE);
let data64 = BASE64_STANDARD.encode(&ff_data);
assert_eq!(data64.len(), MAX_DATA_BASE64_SIZE);
⋮----
fn test_bytes_match() {
let data = vec![1, 2, 3, 4, 5];
assert!(Memcmp {
⋮----
assert!(!Memcmp {
⋮----
fn test_verify_memcmp() {
⋮----
assert_eq!(base58_bytes.len(), 128);
assert_eq!(
⋮----
assert_eq!(base58_bytes.len(), 129);
⋮----
formatcp!(r#"{{"bytes":"{BASE58_STR}","offset":{OFFSET}}}"#);
⋮----
formatcp!(r#"{{"bytes":"{BASE58_STR}","offset":{OFFSET},"encoding":"binary"}}"#);
⋮----
formatcp!(r#"{{"bytes":"{BASE58_STR}","offset":{OFFSET},"encoding":"base58"}}"#);
⋮----
formatcp!(r#"{{"bytes":"{BASE64_STR}","offset":{OFFSET},"encoding":"base64"}}"#);
⋮----
formatcp!(r#"{{"bytes":[0, 1, 2, 3],"offset":{OFFSET},"encoding":"base64"}}"#);
⋮----
formatcp!(r#"{{"bytes":[0, 1, 2, 3],"offset":{OFFSET},"encoding":null}}"#);
⋮----
formatcp!(r#"{{"bytes":[0, 1, 2, 3],"offset":{OFFSET},"encoding":"bytes"}}"#);
⋮----
formatcp!(r#"{{"bytes":"{BASE58_STR}","offset":{OFFSET},"encoding":"bytes"}}"#);
⋮----
fn test_filter_deserialize() {
let default: Memcmp = serde_json::from_str(DEFAULT_ENCODING_FILTER).unwrap();
⋮----
assert!(binary.is_err());
let base58_filter: Memcmp = serde_json::from_str(BASE58_FILTER).unwrap();
⋮----
let base64_filter: Memcmp = serde_json::from_str(BASE64_FILTER).unwrap();
⋮----
let bytes_filter: Memcmp = serde_json::from_str(BYTES_FILTER).unwrap();
⋮----
let bytes_filter: Memcmp = serde_json::from_str(BYTES_FILTER_WITH_ENCODING).unwrap();
⋮----
let base64_filter: Memcmp = serde_json::from_str(MISMATCHED_BASE64_FILTER).unwrap();
⋮----
serde_json::from_str(MISMATCHED_BYTES_FILTER_WITH_ENCODING).unwrap();
⋮----
fn test_filter_serialize() {
⋮----
bytes: MemcmpEncodedBytes::Base58(BASE58_STR.to_string()),
⋮----
let serialized_json = json!(base58);
⋮----
bytes: MemcmpEncodedBytes::Base64(BASE64_STR.to_string()),
⋮----
let serialized_json = json!(base64);
⋮----
bytes: MemcmpEncodedBytes::Bytes(BYTES.to_vec()),
⋮----
let serialized_json = json!(bytes);

================
File: rpc-client-types/src/lib.rs
================
pub mod config;
pub mod error_object;
pub mod filter;
pub mod request;
pub mod response;

================
File: rpc-client-types/src/request.rs
================
pub use solana_address::Address;
⋮----
pub enum RpcRequest {
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
⋮----
write!(f, "{method}")
⋮----
impl RpcRequest {
pub fn build_request_json(self, id: u64, params: Value) -> Value {
⋮----
json!({
⋮----
pub enum RpcResponseErrorData {
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
⋮----
if logs.is_empty() {
Ok(())
⋮----
writeln!(f, "{} log messages:", logs.len())?;
⋮----
writeln!(f, "  {log}")?;
⋮----
_ => Ok(()),
⋮----
pub enum RpcError {
⋮----
pub enum TokenAccountsFilter {
⋮----
mod tests {
⋮----
fn test_build_request_json() {
⋮----
let addr = json!("deadbeefXjn8o3yroDHxUtKsZZgoy4GPkPPXfouKNHhx");
let request = test_request.build_request_json(1, json!([addr]));
assert_eq!(request["method"], "getAccountInfo");
assert_eq!(request["params"], json!([addr]));
⋮----
assert_eq!(request["method"], "getBalance");
⋮----
let request = test_request.build_request_json(1, Value::Null);
assert_eq!(request["method"], "getEpochInfo");
⋮----
assert_eq!(request["method"], "getLatestBlockhash");
⋮----
assert_eq!(request["method"], "getSlot");
⋮----
assert_eq!(request["method"], "getTransactionCount");
⋮----
assert_eq!(request["method"], "requestAirdrop");
⋮----
assert_eq!(request["method"], "sendTransaction");
⋮----
assert_eq!(request["method"], "getTokenLargestAccounts");
⋮----
fn test_build_request_json_config_options() {
⋮----
let request = test_request.build_request_json(1, json!([commitment_config]));
assert_eq!(request["params"], json!([commitment_config.clone()]));
⋮----
let request = test_request.build_request_json(1, json!([addr, commitment_config]));
assert_eq!(request["params"], json!([addr, commitment_config]));
⋮----
let token_account_filter = RpcTokenAccountsFilter::Mint(mint.to_string());
⋮----
.build_request_json(1, json!([addr, token_account_filter, commitment_config]));
assert_eq!(

================
File: rpc-client-types/src/response.rs
================
pub enum OptionalContext<T> {
⋮----
pub fn parse_value(self) -> T {
⋮----
pub struct RpcResponseContext {
⋮----
pub struct RpcApiVersion(semver::Version);
⋮----
type Target = semver::Version;
fn deref(&self) -> &Self::Target {
⋮----
impl Default for RpcApiVersion {
fn default() -> Self {
Self(solana_version::Version::default().as_semver_version())
⋮----
impl Serialize for RpcApiVersion {
fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>
⋮----
serializer.serialize_str(&self.to_string())
⋮----
fn deserialize<D>(deserializer: D) -> std::result::Result<Self, D::Error>
⋮----
Ok(RpcApiVersion(
semver::Version::from_str(&s).map_err(serde::de::Error::custom)?,
⋮----
impl RpcResponseContext {
pub fn new(slot: Slot) -> Self {
⋮----
api_version: Some(RpcApiVersion::default()),
⋮----
pub struct Response<T> {
⋮----
pub struct RpcBlockCommitment<T> {
⋮----
pub struct RpcBlockhashFeeCalculator {
⋮----
pub struct RpcBlockhash {
⋮----
pub struct RpcFeeCalculator {
⋮----
pub struct RpcFeeRateGovernor {
⋮----
pub struct RpcInflationGovernor {
⋮----
fn from(inflation: Inflation) -> Self {
⋮----
pub struct RpcInflationRate {
⋮----
pub struct RpcKeyedAccount {
⋮----
pub struct SlotInfo {
⋮----
pub struct SlotTransactionStats {
⋮----
pub enum SlotUpdate {
⋮----
impl SlotUpdate {
pub fn slot(&self) -> Slot {
⋮----
pub enum RpcSignatureResult {
⋮----
pub struct RpcLogsResponse {
⋮----
pub struct ProcessedSignatureResult {
⋮----
pub enum ReceivedSignatureResult {
⋮----
pub struct RpcContactInfo {
⋮----
pub type RpcLeaderSchedule = HashMap<String, Vec<usize>>;
⋮----
pub struct RpcBlockProductionRange {
⋮----
pub struct RpcBlockProduction {
⋮----
pub struct RpcVersionInfo {
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
write!(f, "{}", self.solana_core)
⋮----
if let Some(version) = self.solana_core.split_whitespace().next() {
write!(f, "{version}")
⋮----
pub struct RpcIdentity {
⋮----
pub struct RpcVote {
⋮----
pub struct RpcVoteAccountStatus {
⋮----
pub struct RpcVoteAccountInfo {
⋮----
pub struct RpcSignatureConfirmation {
⋮----
pub struct RpcSimulateTransactionResult {
⋮----
pub struct RpcStorageTurn {
⋮----
pub struct RpcAccountBalance {
⋮----
pub struct RpcSupply {
⋮----
pub enum StakeActivationState {
⋮----
pub struct RpcTokenAccountBalance {
⋮----
pub struct RpcConfirmedTransactionStatusWithSignature {
⋮----
pub struct RpcPerfSample {
⋮----
pub struct RpcInflationReward {
⋮----
pub enum RpcBlockUpdateError {
⋮----
pub struct RpcBlockUpdate {
⋮----
fn from(value: ConfirmedTransactionStatusWithSignature) -> Self {
⋮----
signature: signature.to_string(),
⋮----
err: err.map(Into::into),
⋮----
pub struct RpcSnapshotSlotInfo {
⋮----
pub struct RpcPrioritizationFee {
⋮----
pub mod tests {
⋮----
fn rpc_perf_sample_deserialize_old() {
⋮----
let input = json!({
⋮----
.to_string();
⋮----
serde_json::from_str(&input).expect("Can parse RpcPerfSample from string as JSON");
⋮----
assert_eq!(actual, expected);
⋮----
fn rpc_perf_sample_serializes_num_non_vote_transactions() {
⋮----
let num_non_vote_transactions = Some(757);
⋮----
serde_json::to_value(input).expect("Can convert RpcPerfSample into a JSON value");
let expected = json!({

================
File: rpc-client-types/Cargo.toml
================
[package]
name = "solana-rpc-client-types"
description = "Solana RPC Client Types"
documentation = "https://docs.rs/solana-rpc-client-types"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
base64 = { workspace = true }
bs58 = { workspace = true, features = ["std"] }
semver = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account = { workspace = true }
solana-account-decoder-client-types = { workspace = true }
solana-address = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true, features = ["serde"] }
solana-fee-calculator = { workspace = true, features = ["serde"] }
solana-inflation = { workspace = true }
solana-reward-info = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status-client-types = { workspace = true }
solana-version = { workspace = true }
spl-generic-token = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
const_format = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }

================
File: rpc-test/tests/nonblocking.rs
================
async fn test_tpu_send_transaction() {
let (test_validator, mint_keypair) = TestValidatorGenesis::default().start_async().await;
let rpc_client = Arc::new(test_validator.get_async_rpc_client());
⋮----
rpc_client.clone(),
&test_validator.rpc_pubsub_url(),
⋮----
.unwrap();
let recent_blockhash = rpc_client.get_latest_blockhash().await.unwrap();
⋮----
assert!(tpu_client.send_transaction(&tx).await);
⋮----
let signatures = vec![tx.signatures[0]];
⋮----
assert!(now.elapsed() < timeout);
⋮----
.get_signature_statuses(&signatures)
⋮----
if !statuses.value.is_empty() {
⋮----
tpu_client.shutdown().await;
⋮----
async fn test_tpu_cache_slot_updates() {
let (test_validator, _) = TestValidatorGenesis::default().start_async().await;
⋮----
exit.clone(),
⋮----
let start_slot = leader_tpu_service.estimated_current_slot();
⋮----
let current_slot = leader_tpu_service.estimated_current_slot();
⋮----
sleep(sleep_time).await;
⋮----
exit.store(true, Ordering::Relaxed);
leader_tpu_service.join().await;

================
File: rpc-test/tests/rpc.rs
================
macro_rules! json_req {
⋮----
fn post_rpc(request: Value, rpc_url: &str) -> Value {
⋮----
.post(rpc_url)
.header(CONTENT_TYPE, "application/json")
.body(request.to_string())
.send()
.unwrap();
serde_json::from_str(&response.text().unwrap()).unwrap()
⋮----
fn test_rpc_send_tx() {
⋮----
TestValidator::with_no_fees(alice.pubkey(), None, SocketAddrSpace::Unspecified);
let rpc_url = test_validator.rpc_url();
⋮----
let req = json_req!("getLatestBlockhash", json!([]));
let json = post_rpc(req, &rpc_url);
⋮----
.as_str()
.unwrap()
.parse()
⋮----
info!("blockhash: {blockhash:?}");
⋮----
Rent::default().minimum_balance(0),
⋮----
let serialized_encoded_tx = bs58::encode(serialize(&tx).unwrap()).into_string();
let req = json_req!("sendTransaction", json!([serialized_encoded_tx]));
let json: Value = post_rpc(req, &rpc_url);
⋮----
let request = json_req!("getSignatureStatuses", [[signature]]);
⋮----
let json = post_rpc(request.clone(), &rpc_url);
⋮----
serde_json::from_value(json["result"]["value"][0].clone()).unwrap();
if let Some(result) = result.as_ref() {
if result.err.is_none() {
⋮----
sleep(Duration::from_millis(500));
⋮----
assert!(confirmed_tx);
⋮----
encoding: Some(UiAccountEncoding::Base64),
⋮----
let req = json_req!(
⋮----
info!("{:?}", json["result"]["value"]);
⋮----
fn test_simulation_replaced_blockhash() -> ClientResult<()> {
⋮----
let validator = TestValidator::with_no_fees(alice.pubkey(), None, SocketAddrSpace::Unspecified);
let rpc_client = RpcClient::new(validator.rpc_url());
⋮----
let res = rpc_client.simulate_transaction_with_config(
&system_transaction::transfer(&alice, &bob.pubkey(), lamports, Hash::default()),
⋮----
assert!(
⋮----
let blockhash = res.value.replacement_blockhash.unwrap();
assert_ne!(
⋮----
Ok(())
⋮----
fn test_rpc_invalid_requests() {
⋮----
let req = json_req!("getBalance", json!(["invalid9999"]));
⋮----
let the_error = json["error"]["message"].as_str().unwrap();
assert_eq!(the_error, "Invalid param: Invalid");
let req = json_req!("getAccountInfo", json!(["invalid9999"]));
⋮----
let req = json_req!("getAccountInfo", json!([bob_pubkey.to_string()]));
⋮----
assert!(the_value.is_null());
⋮----
fn test_rpc_slot_updates() {
⋮----
let rt = Runtime::new().unwrap();
let rpc_pubsub_url = test_validator.rpc_pubsub_url();
rt.spawn(async move {
let pubsub_client = PubsubClient::new(&rpc_pubsub_url).await.unwrap();
⋮----
pubsub_client.slot_updates_subscribe().await.unwrap();
while let Some(slot_update) = slot_notifications.next().await {
update_sender.send(slot_update).unwrap();
⋮----
slot_unsubscribe().await;
⋮----
.recv_timeout(Duration::from_secs(2))
⋮----
let verify_slot = first_update.slot() + 2;
let expected_updates = vec![
⋮----
let mut expected_updates = expected_updates.into_iter().peekable();
⋮----
while expected_updates.peek().is_some() || !slot_update_completed {
assert!(test_start.elapsed() < Duration::from_secs(30));
⋮----
if update.slot() == verify_slot {
⋮----
assert_eq!(Some(update_name), expected_updates.next());
⋮----
fn test_rpc_subscriptions() {
⋮----
TestValidator::with_no_fees_udp(alice.pubkey(), None, SocketAddrSpace::Unspecified);
let transactions_socket = bind_to_localhost_unique().unwrap();
transactions_socket.connect(test_validator.tpu()).unwrap();
let rpc_client = RpcClient::new(test_validator.rpc_url());
let recent_blockhash = rpc_client.get_latest_blockhash().unwrap();
let transfer_amount = Rent::default().minimum_balance(0);
⋮----
.map(|_| {
⋮----
.collect();
⋮----
transactions.iter().map(|tx| tx.signatures[0]).collect();
⋮----
.iter()
.map(|tx| tx.message.account_keys[1])
⋮----
let signature_set_clone = signature_set.clone();
let account_set_clone = account_set.clone();
⋮----
let signature_subscription_ready_clone = signature_subscription_ready.clone();
let account_subscription_ready_clone = account_subscription_ready.clone();
⋮----
let pubsub_client = Arc::new(PubsubClient::new(&rpc_pubsub_url).await.unwrap());
⋮----
let status_sender = status_sender.clone();
let signature_subscription_ready_clone = signature_subscription_ready_clone.clone();
⋮----
.signature_subscribe(
⋮----
Some(RpcSignatureSubscribeConfig {
commitment: Some(CommitmentConfig::confirmed()),
⋮----
signature_subscription_ready_clone.fetch_add(1, Ordering::SeqCst);
let response = sig_notifications.next().await.unwrap();
status_sender.send((signature, response)).unwrap();
sig_unsubscribe().await;
⋮----
let account_sender = account_sender.clone();
let account_subscription_ready_clone = account_subscription_ready_clone.clone();
⋮----
.account_subscribe(
⋮----
Some(RpcAccountInfoConfig {
⋮----
account_subscription_ready_clone.fetch_add(1, Ordering::SeqCst);
let response = account_notifications.next().await.unwrap();
account_sender.send((pubkey, response)).unwrap();
account_unsubscribe().await;
⋮----
while (signature_subscription_ready.load(Ordering::SeqCst) != transactions.len()
|| account_subscription_ready.load(Ordering::SeqCst) != transactions.len())
&& now.elapsed() < Duration::from_secs(15)
⋮----
sleep(Duration::from_millis(100))
⋮----
let num = signature_subscription_ready.load(Ordering::SeqCst);
if num != transactions.len() {
error!(
⋮----
let num = account_subscription_ready.load(Ordering::SeqCst);
⋮----
.get_balance_with_commitment(&alice.pubkey(), CommitmentConfig::processed())
⋮----
assert!(mint_balance >= transactions.len() as u64);
transactions.iter().for_each(|tx| {
⋮----
.send(&bincode::serialize(&tx).unwrap())
⋮----
let expected_mint_balance = mint_balance - (transfer_amount * transactions.len() as u64);
while mint_balance != expected_mint_balance && now.elapsed() < Duration::from_secs(15) {
⋮----
sleep(Duration::from_millis(100));
⋮----
error!("mint-check timeout. mint_balance {mint_balance:?}");
⋮----
while !signature_set.is_empty() {
let timeout = deadline.saturating_duration_since(Instant::now());
match status_receiver.recv_timeout(timeout) {
⋮----
assert!(result.err.is_none());
assert!(signature_set.remove(&sig));
⋮----
panic!("Unexpected result");
⋮----
panic!(
⋮----
while !account_set.is_empty() {
⋮----
match account_receiver.recv_timeout(timeout) {
⋮----
assert_eq!(result.value.lamports, Rent::default().minimum_balance(0));
assert!(account_set.remove(&pubkey));
⋮----
fn run_tpu_send_transaction(tpu_use_quic: bool) {
⋮----
let mint_pubkey = mint_keypair.pubkey();
⋮----
test_validator.rpc_url(),
⋮----
rpc_client.clone(),
&test_validator.rpc_pubsub_url(),
⋮----
.send_transaction(&tx),
⋮----
assert!(success);
⋮----
let signatures = vec![tx.signatures[0]];
⋮----
assert!(now.elapsed() < timeout);
let statuses = rpc_client.get_signature_statuses(&signatures).unwrap();
if !statuses.value.is_empty() {
⋮----
fn test_tpu_send_transaction() {
run_tpu_send_transaction( false)
⋮----
fn test_tpu_send_transaction_with_quic() {
run_tpu_send_transaction( true)
⋮----
fn deserialize_rpc_error() -> ClientResult<()> {
⋮----
let blockhash = rpc_client.get_latest_blockhash()?;
let mut tx = system_transaction::transfer(&alice, &bob.pubkey(), lamports, blockhash);
tx.signatures.clear();
let err = rpc_client.send_transaction(&tx);
let err = err.unwrap_err();
match err.kind() {
⋮----
panic!()
⋮----
ClientErrorKind::RpcError(RpcError::RpcResponseError { .. }) => Ok(()),
⋮----
fn test_simulate_bundle() {
⋮----
mint_keypair.pubkey(),
⋮----
test_too_many_bundles(&rpc_client, &mint_keypair);
test_wrong_number_pre_accounts(&rpc_client, &mint_keypair);
test_wrong_number_post_accounts(&rpc_client, &mint_keypair);
test_invalid_transaction_encoding(&rpc_client, &mint_keypair);
test_wrong_pre_account_encoding(&rpc_client, &mint_keypair);
test_wrong_post_account_encoding(&rpc_client, &mint_keypair);
test_replace_recent_blockhash_with_sig_verify(&rpc_client, &mint_keypair);
test_bad_signature(&rpc_client, &mint_keypair);
test_bad_pubkey_pre_accounts(&rpc_client, &mint_keypair);
test_bad_pubkey_post_accounts(&rpc_client, &mint_keypair);
test_single_tx_ok(&rpc_client, &mint_keypair);
test_chained_transfers_ok(&rpc_client, &mint_keypair);
test_single_bad_tx(&rpc_client, &mint_keypair);
test_last_tx_fails(&rpc_client, &mint_keypair);
test_duplicate_transactions(&rpc_client, &mint_keypair);
test_program_execution_error(&rpc_client, &mint_keypair);
⋮----
fn test_too_many_bundles(rpc_client: &RpcClient, mint_keypair: &Keypair) {
let latest_blockhash = rpc_client.get_latest_blockhash().unwrap();
⋮----
.get_minimum_balance_for_rent_exemption(0)
⋮----
.simulate_bundle_with_config(&transactions, RpcSimulateBundleConfig::default())
.unwrap_err();
⋮----
}) = simulate_result.kind()
⋮----
panic!("unexpected error");
⋮----
assert_eq!(message, "bundle size too large, max 20 transactions");
assert_eq!(*code, -32602);
assert_matches!(data, &RpcResponseErrorData::Empty);
⋮----
fn test_wrong_number_pre_accounts(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
let transactions = vec![system_transaction::transfer(
⋮----
.simulate_bundle_with_config(
⋮----
pre_execution_accounts_configs: vec![None; transactions.len().saturating_add(1)],
post_execution_accounts_configs: vec![None; transactions.len()],
transaction_encoding: Some(UiTransactionEncoding::Base64),
simulation_bank: Some(SimulationSlotConfig::Tip),
⋮----
assert_eq!(
⋮----
fn test_wrong_number_post_accounts(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
pre_execution_accounts_configs: vec![None; transactions.len()],
post_execution_accounts_configs: vec![None; transactions.len().saturating_add(1)],
⋮----
fn test_invalid_transaction_encoding(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
transaction_encoding: Some(UiTransactionEncoding::Base58),
⋮----
fn test_wrong_pre_account_encoding(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
pre_execution_accounts_configs: vec![Some(RpcSimulateTransactionAccountsConfig {
⋮----
fn test_wrong_post_account_encoding(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
post_execution_accounts_configs: vec![Some(RpcSimulateTransactionAccountsConfig {
⋮----
fn test_duplicate_transactions(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
let transactions = vec![
⋮----
assert_eq!(message, "duplicate transactions");
⋮----
fn test_replace_recent_blockhash_with_sig_verify(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
fn test_bad_signature(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
let mut transactions = vec![system_transaction::transfer(
⋮----
transactions.get_mut(0).unwrap().signatures[0] = Signature::default();
⋮----
fn test_bad_pubkey_pre_accounts(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
fn test_bad_pubkey_post_accounts(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
fn test_single_tx_ok(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
let mint_balance = rpc_client.get_balance(&mint_keypair.pubkey()).unwrap();
⋮----
assert_eq!(simulate_result.transaction_results.len(), 1);
let result = simulate_result.transaction_results.first().unwrap();
assert_eq!(result.err, None);
let pre_execution_accounts = result.pre_execution_accounts.as_ref().unwrap();
assert_eq!(pre_execution_accounts.len(), 2);
assert_eq!(pre_execution_accounts[0].lamports, mint_balance);
assert_eq!(pre_execution_accounts[1].lamports, 0);
let post_execution_accounts = result.post_execution_accounts.as_ref().unwrap();
assert_eq!(post_execution_accounts.len(), 2);
⋮----
assert_eq!(post_execution_accounts[1].lamports, rent);
⋮----
fn test_chained_transfers_ok(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
pre_execution_accounts_configs: vec![
⋮----
post_execution_accounts_configs: vec![
⋮----
assert_eq!(simulate_result.transaction_results.len(), 2);
⋮----
assert_eq!(pre_execution_accounts.len(), 1);
⋮----
assert_eq!(post_execution_accounts[1].lamports, rent.saturating_mul(2));
let result = simulate_result.transaction_results.get(1).unwrap();
⋮----
assert_eq!(pre_execution_accounts[1].lamports, rent.saturating_mul(2));
⋮----
assert_eq!(post_execution_accounts.len(), 3);
⋮----
assert_eq!(post_execution_accounts[2].lamports, rent);
⋮----
fn test_single_bad_tx(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
&mint_keypair.pubkey(),
rent.saturating_mul(2),
⋮----
let transactions = vec![account_not_found_tx.clone()];
⋮----
assert_eq!(result.err, Some(TransactionError::AccountNotFound));
⋮----
fn test_last_tx_fails(rpc_client: &RpcClient, mint_keypair: &Keypair) {
⋮----
let bad_tx_signature = *transactions.get(1).unwrap().signatures.first().unwrap();
⋮----
fn test_program_execution_error(rpc_client: &RpcClient, mint_keypair: &Keypair) {

================
File: rpc-test/.gitignore
================
/target/
/farf/

================
File: rpc-test/Cargo.toml
================
[package]
name = "solana-rpc-test"
description = "Solana RPC Test"
documentation = "https://docs.rs/solana-rpc-test"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
bincode = { workspace = true }
bs58 = { workspace = true }
crossbeam-channel = { workspace = true }
futures-util = { workspace = true }
log = { workspace = true }
reqwest = { workspace = true, features = ["blocking", "brotli", "deflate", "gzip", "rustls-tls", "json"] }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account-decoder = { workspace = true }
solana-client = { workspace = true }
solana-net-utils = { workspace = true }
solana-pubsub-client = { workspace = true }
solana-rpc = { workspace = true }
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-test-validator = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction-status = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[dev-dependencies]
agave-logger = { workspace = true }
assert_matches = { workspace = true }
solana-client = { workspace = true, features = ["dev-context-only-utils"] }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-connection-cache = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-system-transaction = { workspace = true }
solana-transaction = { workspace = true }

================
File: runtime/benches/accounts.rs
================
extern crate test;
⋮----
fn deposit_many(bank: &Bank, pubkeys: &mut Vec<Pubkey>, num: usize) -> Result<(), LamportsError> {
⋮----
AccountSharedData::new((t + 1) as u64, 0, AccountSharedData::default().owner());
pubkeys.push(pubkey);
assert!(bank.get_account(&pubkey).is_none());
⋮----
assert_eq!(bank.get_account(&pubkey).unwrap(), account);
⋮----
Ok(())
⋮----
fn bench_accounts_create(bencher: &mut Bencher) {
let (genesis_config, _) = create_genesis_config(10_000);
let bank0 = Bank::new_with_paths_for_benches(&genesis_config, vec![PathBuf::from("bench_a0")]);
bencher.iter(|| {
let mut pubkeys: Vec<Pubkey> = vec![];
deposit_many(&bank0, &mut pubkeys, 1000).unwrap();
⋮----
fn bench_accounts_squash(bencher: &mut Bencher) {
let (genesis_config, _) = create_genesis_config(100_000);
⋮----
vec![PathBuf::from("bench_a1")],
⋮----
deposit_many(&prev_bank, &mut pubkeys, 250_000).unwrap();
prev_bank.freeze();
⋮----
prev_bank.clone(),
⋮----
test_utils::deposit(&next_bank, &pubkeys[0], 1).unwrap();
next_bank.squash();

================
File: runtime/benches/prioritization_fee_cache.rs
================
extern crate test;
⋮----
fn build_sanitized_transaction(
⋮----
Some(signer_account),
⋮----
fn bench_process_transactions_single_slot(bencher: &mut Bencher) {
⋮----
.map(|n| {
⋮----
build_sanitized_transaction(
⋮----
.collect();
bencher.iter(|| {
prioritization_fee_cache.update(&bank, transactions.iter());
⋮----
fn process_transactions_multiple_slots(banks: &[Arc<Bank>], num_slots: usize, num_threads: usize) {
⋮----
.num_threads(num_threads)
.build()
.unwrap();
⋮----
pool.install(|| {
⋮----
let index = rng().random_range(0..num_slots);
prioritization_fee_cache.update(&banks[index], transactions.iter());
⋮----
fn bench_process_transactions_multiple_slots(bencher: &mut Bencher) {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let bank = bank_forks.read().unwrap().working_bank();
⋮----
.map(|n| Arc::new(Bank::new_from_parent(bank.clone(), &collector, n as u64)))
⋮----
process_transactions_multiple_slots(&banks, NUM_SLOTS, NUM_THREADS);

================
File: runtime/benches/status_cache.rs
================
extern crate test;
⋮----
fn bench_status_cache_serialize(bencher: &mut Bencher) {
⋮----
status_cache.add_root(0);
status_cache.clear();
⋮----
id = hash(id.as_ref());
let mut sigbytes = Vec::from(id.as_ref());
⋮----
sigbytes.extend(id.as_ref());
let sig = Signature::try_from(sigbytes).unwrap();
status_cache.insert(&blockhash, sig, 0, Ok(()));
⋮----
assert!(status_cache.roots().contains(&0));
bencher.iter(|| {
let _ = serialize(&status_cache.root_slot_deltas()).unwrap();
⋮----
fn bench_status_cache_serialize_max(bencher: &mut Bencher) {
⋮----
fill_status_cache(&mut status_cache, max_cache_entries, 100_000);
⋮----
fn bench_status_cache_root_slot_deltas(bencher: &mut Bencher) {
⋮----
let slots: Vec<_> = (42..).take(MAX_CACHE_ENTRIES).collect();
⋮----
status_cache.insert(&Hash::new_unique(), Hash::new_unique(), *slot, Ok(()));
⋮----
status_cache.add_root(*slot);
⋮----
bencher.iter(|| test::black_box(status_cache.root_slot_deltas()));
⋮----
fn fill_status_cache(status_cache: &mut BankStatusCache, max_cache_entries: u64, num_txs: usize) {
⋮----
fill_status_cache_slot(status_cache, &blockhash, slot, num_txs);
⋮----
fn fill_status_cache_slot(
⋮----
status_cache.insert(blockhash, tx_hash, slot, Ok(()));
⋮----
fn bench_status_cache_check_and_insert(bencher: &mut Bencher) {
⋮----
fill_status_cache(&mut status_cache, max_cache_entries - 1, 100_000);
⋮----
fill_status_cache_slot(&mut status_cache, &blockhash, max_cache_entries, 100_000);
⋮----
rng.fill(&mut sigbytes);
tx_hashes.push(Signature::from(sigbytes));
⋮----
.get_status(*tx_hash, &blockhash, &ancestors)
.is_none()
⋮----
status_cache.insert(&blockhash, *tx_hash, slot, Ok(()));
⋮----
fn bench_status_cache_add_roots(bencher: &mut Bencher) {
⋮----
status_cache.add_root(root);

================
File: runtime/src/accounts_background_service/pending_snapshot_packages.rs
================
pub struct PendingSnapshotPackages {
⋮----
impl PendingSnapshotPackages {
pub fn push(&mut self, snapshot_package: SnapshotPackage) {
⋮----
if let Some(pending_snapshot_package) = pending_package.as_ref() {
assert!(
⋮----
assert_eq!(
⋮----
info!(
⋮----
*pending_package = Some(snapshot_package);
⋮----
pub fn pop(&mut self) -> Option<SnapshotPackage> {
let pending_full = self.full.take();
let pending_incremental = self.incremental.take();
⋮----
panic!(
⋮----
self.incremental = Some(pending_incremental);
⋮----
assert!(pending_full.snapshot_kind.is_full_snapshot());
Some(pending_full)
⋮----
assert!(pending_incremental.snapshot_kind.is_incremental_snapshot());
Some(pending_incremental)
⋮----
mod tests {
⋮----
fn new(snapshot_kind: SnapshotKind, slot: Slot) -> SnapshotPackage {
⋮----
fn new_full(slot: Slot) -> SnapshotPackage {
new(SnapshotKind::Archive(SnapshotArchiveKind::Full), slot)
⋮----
fn new_incr(slot: Slot, base: Slot) -> SnapshotPackage {
new(
⋮----
fn test_default() {
⋮----
assert!(pending_snapshot_packages.full.is_none());
assert!(pending_snapshot_packages.incremental.is_none());
⋮----
fn test_push() {
⋮----
pending_snapshot_packages.push(new_full(slot));
assert_eq!(pending_snapshot_packages.full.as_ref().unwrap().slot, slot);
⋮----
pending_snapshot_packages.push(new_incr(slot, full_slot));
⋮----
fn test_push_older_full() {
⋮----
full: Some(new_full(slot)),
⋮----
pending_snapshot_packages.push(new_full(slot - 1));
⋮----
fn test_push_older_incremental() {
⋮----
incremental: Some(new_incr(slot, base)),
⋮----
pending_snapshot_packages.push(new_incr(slot - 1, base));
⋮----
fn test_pop() {
⋮----
assert!(pending_snapshot_packages.pop().is_none());
⋮----
pending_snapshot_packages.full = Some(new_full(slot));
⋮----
let snapshot_package = pending_snapshot_packages.pop().unwrap();
assert!(snapshot_package.snapshot_kind.is_full_snapshot());
assert_eq!(snapshot_package.slot, slot);
⋮----
pending_snapshot_packages.incremental = Some(new_incr(slot, base));
⋮----
assert!(snapshot_package.snapshot_kind.is_incremental_snapshot());
⋮----
pending_snapshot_packages.full = Some(new_full(full_slot));
pending_snapshot_packages.incremental = Some(new_incr(incr_slot, full_slot));
⋮----
assert_eq!(snapshot_package.slot, full_slot);
⋮----
assert_eq!(snapshot_package.slot, incr_slot);
⋮----
fn test_pop_invalid_pending_full() {
⋮----
full: Some(new_incr(110, 100)),
⋮----
pending_snapshot_packages.pop();
⋮----
fn test_pop_invalid_pending_incremental() {
⋮----
incremental: Some(new_full(100)),

================
File: runtime/src/accounts_background_service/stats.rs
================
pub(super) struct StatsManager {
⋮----
impl StatsManager {
⋮----
pub(super) fn new() -> Self {
⋮----
pub(super) fn record_and_maybe_submit(&mut self, runtime: Duration) {
self.stats.record(runtime);
self.maybe_submit();
⋮----
fn maybe_submit(&mut self) {
⋮----
datapoint_info!(
⋮----
struct Stats {
⋮----
impl Stats {
fn record(&mut self, runtime: Duration) {
⋮----
self.min_runtime = self.min_runtime.min(runtime);
self.max_runtime = self.max_runtime.max(runtime);
⋮----
fn mean_runtime(&self) -> Duration {
debug_assert!(self.num_iterations > 0);
debug_assert!(self.num_iterations <= u32::MAX as usize);
⋮----
impl Default for Stats {
fn default() -> Self {
⋮----
mod tests {
⋮----
fn test_stats_record() {
⋮----
stats.record(runtime1);
assert_eq!(stats.num_iterations, 1);
assert_eq!(stats.cumulative_runtime, runtime1);
assert_eq!(stats.min_runtime, runtime1);
assert_eq!(stats.max_runtime, runtime1);
⋮----
stats.record(runtime2);
assert_eq!(stats.num_iterations, 2);
assert_eq!(stats.cumulative_runtime, runtime1 + runtime2);
⋮----
assert_eq!(stats.max_runtime, runtime2);
⋮----
stats.record(runtime3);
assert_eq!(stats.num_iterations, 3);
assert_eq!(stats.cumulative_runtime, runtime1 + runtime2 + runtime3);
assert_eq!(stats.min_runtime, runtime3);
⋮----
fn test_stats_mean_runtime() {
⋮----
stats.record(Duration::from_secs(1));
stats.record(Duration::from_secs(3));
stats.record(Duration::from_secs(5));
stats.record(Duration::from_secs(7));
assert_eq!(stats.mean_runtime().as_secs(), (1 + 3 + 5 + 7) / 4);
⋮----
fn test_stats_mean_runtime_panic_zero_iterations() {
⋮----
let _ = stats.mean_runtime();
⋮----
fn test_stats_mean_runtime_panic_too_many_iterations() {

================
File: runtime/src/bank/builtins/core_bpf_migration/error.rs
================
pub enum CoreBpfMigrationError {

================
File: runtime/src/bank/builtins/core_bpf_migration/mod.rs
================
pub(crate) mod error;
mod source_buffer;
mod target_bpf_v2;
mod target_builtin;
mod target_core_bpf;
⋮----
fn checked_add<T: CheckedAdd>(a: T, b: T) -> Result<T, CoreBpfMigrationError> {
a.checked_add(&b)
.ok_or(CoreBpfMigrationError::ArithmeticOverflow)
⋮----
fn checked_sub<T: CheckedSub>(a: T, b: T) -> Result<T, CoreBpfMigrationError> {
a.checked_sub(&b)
⋮----
impl Bank {
fn new_target_program_account(
⋮----
self.get_minimum_balance_for_rent_exemption(UpgradeableLoaderState::size_of_program());
⋮----
account.set_executable(true);
Ok(account)
⋮----
fn new_target_program_data_account(
⋮----
} = bincode::deserialize(&source.buffer_account.data()[..buffer_metadata_size])?
⋮----
return Err(CoreBpfMigrationError::UpgradeAuthorityMismatch(
⋮----
let elf = &source.buffer_account.data()[buffer_metadata_size..];
⋮----
let space = programdata_metadata_size + elf.len();
let lamports = self.get_minimum_balance_for_rent_exemption(space);
⋮----
account.data_as_mut_slice()[programdata_metadata_size..].copy_from_slice(elf);
⋮----
Err(CoreBpfMigrationError::InvalidBufferAccount(
⋮----
fn directly_invoke_loader_v3_deploy(
⋮----
let data_len = programdata.len();
⋮----
.get_environments_for_epoch(self.epoch);
⋮----
.compute_budget()
.unwrap_or(ComputeBudget::new_with_defaults(
⋮----
sysvar_cache.fill_missing_entries(|pubkey, set_sysvar| {
if let Some(account) = self.get_account(pubkey) {
set_sysvar(account.data());
⋮----
vec![],
self.rent_collector.rent.clone(),
⋮----
struct MockCallback {}
impl InvokeContextCallback for MockCallback {}
let feature_set = self.feature_set.runtime_features();
⋮----
compute_budget.to_budget(),
compute_budget.to_cost(),
⋮----
dummy_invoke_context.get_log_collector(),
⋮----
program_runtime_environments.program_runtime_v1.clone(),
⋮----
UpgradeableLoaderState::size_of_program().saturating_add(data_len),
⋮----
load_program_metrics.submit_datapoint(&mut dummy_invoke_context.timings);
⋮----
.write()
.unwrap()
.merge(
⋮----
&program_cache_for_tx_batch.drain_modified_entries(),
⋮----
Ok(())
⋮----
pub(crate) fn migrate_builtin_to_core_bpf(
⋮----
datapoint_info!(config.datapoint_name, ("slot", self.slot, i64));
⋮----
self.new_target_program_account(&target.program_data_address)?;
⋮----
self.new_target_program_data_account(&source, config.upgrade_authority_address)?;
let old_data_size = checked_add(
target.program_account.data().len(),
source.buffer_account.data().len(),
⋮----
let new_data_size = checked_add(
new_target_program_account.data().len(),
new_target_program_data_account.data().len(),
⋮----
self.directly_invoke_loader_v3_deploy(
⋮----
new_target_program_data_account.data(),
⋮----
let lamports_to_burn = checked_add(
target.program_account.lamports(),
source.buffer_account.lamports(),
⋮----
.and_then(|v| checked_add(v, target.program_data_account_lamports))?;
let lamports_to_fund = checked_add(
new_target_program_account.lamports(),
new_target_program_data_account.lamports(),
⋮----
self.update_captalization(lamports_to_burn, lamports_to_fund)?;
self.store_account(&target.program_address, &new_target_program_account);
self.store_account(
⋮----
self.store_account(&source.buffer_address, &AccountSharedData::default());
⋮----
.remove(&target.program_address);
self.calculate_and_update_accounts_data_size_delta_off_chain(old_data_size, new_data_size);
⋮----
pub(crate) fn upgrade_core_bpf_program(
⋮----
datapoint_info!(datapoint_name, ("slot", self.slot, i64));
⋮----
// Attempt serialization first before modifying the bank.
⋮----
self.new_target_program_data_account(&source, target.upgrade_authority_address)?;
// Gather old and new account data sizes, for updating the bank's
⋮----
target.program_data_account.data().len(),
⋮----
let new_data_size = new_target_program_data_account.data().len();
⋮----
target.program_data_account.lamports(),
⋮----
let lamports_to_fund = new_target_program_data_account.lamports();
⋮----
pub(crate) fn upgrade_loader_v2_program_with_loader_v3_program(
⋮----
// Loader v2 programs do not have an upgrade authority, so pass `None` when
// creating the new program data account.
⋮----
self.new_target_program_data_account(&source, None)?;
⋮----
fn update_captalization(
⋮----
match lamports_to_burn.cmp(&lamports_to_fund) {
⋮----
.fetch_sub(checked_sub(lamports_to_burn, lamports_to_fund)?, Relaxed);
⋮----
.fetch_add(checked_sub(lamports_to_fund, lamports_to_burn)?, Relaxed);
⋮----
pub(crate) mod tests {
⋮----
fn test_elf() -> Vec<u8> {
⋮----
.read_to_end(&mut elf)
.unwrap();
⋮----
pub(crate) struct TestContext {
⋮----
impl TestContext {
pub(crate) fn new(
⋮----
let elf = test_elf();
⋮----
let space = buffer_metadata_size + elf.len();
let lamports = bank.get_minimum_balance_for_rent_exemption(space);
⋮----
account.data_as_mut_slice()[buffer_metadata_size..].copy_from_slice(&elf);
⋮----
bank.store_account_and_update_capitalization(
⋮----
pub(crate) fn calculate_post_migration_capitalization_and_accounts_data_size_delta_off_chain(
⋮----
.get_account(&self.target_program_address)
.unwrap_or_default();
let source_buffer_account = bank.get_account(&self.source_buffer_address).unwrap();
⋮----
UpgradeableLoaderState::size_of_programdata_metadata() + self.elf.len();
let expected_post_migration_capitalization = bank.capitalization()
- builtin_account.lamports()
- source_buffer_account.lamports()
+ bank.get_minimum_balance_for_rent_exemption(resulting_program_data_len)
+ bank.get_minimum_balance_for_rent_exemption(resulting_programdata_data_len);
⋮----
bank.accounts_data_size_delta_off_chain.load(Relaxed)
⋮----
- builtin_account.data().len() as i64
- source_buffer_account.data().len() as i64;
⋮----
fn calculate_post_upgrade_capitalization_and_accounts_data_size_delta_off_chain(
⋮----
.get_account(&get_program_data_address(&self.target_program_address))
⋮----
- program_data_account.lamports()
⋮----
- program_data_account.data().len() as i64
⋮----
pub(crate) fn run_program_checks(&self, bank: &Bank, migration_or_upgrade_slot: Slot) {
assert!(bank.get_account(&self.source_buffer_address).is_none());
let program_account = bank.get_account(&self.target_program_address).unwrap();
let program_data_address = get_program_data_address(&self.target_program_address);
assert_eq!(program_account.owner(), &bpf_loader_upgradeable::id());
assert!(program_account.executable());
let program_account_state: UpgradeableLoaderState = program_account.state().unwrap();
assert_eq!(
⋮----
let program_data_account = bank.get_account(&program_data_address).unwrap();
assert_eq!(program_data_account.owner(), &bpf_loader_upgradeable::id());
⋮----
bincode::deserialize(&program_data_account.data()[..programdata_metadata_size])
⋮----
assert!(!bank
⋮----
.read()
⋮----
let entries = program_cache.get_flattened_entries(true, true);
⋮----
.iter()
.find(|(program_id, _)| program_id == &self.target_program_address)
.map(|(_, entry)| entry)
⋮----
assert_eq!(target_entry.deployment_slot, migration_or_upgrade_slot);
assert_eq!(target_entry.effective_slot, migration_or_upgrade_slot + 1);
assert_matches!(target_entry.program, ProgramCacheEntryType::Loaded(..));
⋮----
fn test_migrate_builtin(upgrade_authority_address: Option<Pubkey>) {
let mut bank = create_simple_test_bank(0);
⋮----
AccountSharedData::new_data(1, &builtin_name, &native_loader::id()).unwrap();
bank.store_account_and_update_capitalization(&builtin_id, &account);
bank.add_builtin(
⋮----
builtin_name.as_str(),
⋮----
builtin_name.len(),
⋮----
assert_eq!(&bank.get_account(&builtin_id).unwrap(), &builtin_account);
⋮----
.calculate_post_migration_capitalization_and_accounts_data_size_delta_off_chain(&bank);
⋮----
let migration_slot = bank.slot();
bank.migrate_builtin_to_core_bpf(&builtin_id, &core_bpf_migration_config)
⋮----
test_context.run_program_checks(&bank, migration_slot);
⋮----
fn test_migrate_stateless_builtin(upgrade_authority_address: Option<Pubkey>) {
⋮----
assert!(bank.get_account(&builtin_id).is_none());
⋮----
let data = test_elf();
let end_offset = data.iter().rposition(|&x| x != 0).map_or(0, |i| i + 1);
⋮----
verified_build_hash: Some(expected_hash),
⋮----
fn test_migrate_fail_authority_mismatch() {
⋮----
let upgrade_authority_address = Some(Pubkey::new_unique());
⋮----
upgrade_authority_address: Some(Pubkey::new_unique()),
⋮----
assert_matches!(
⋮----
fn test_migrate_fail_verified_build_mismatch() {
⋮----
verified_build_hash: Some(Hash::default()),
⋮----
fn test_migrate_none_authority_with_some_buffer_authority() {
⋮----
bank.store_account_and_update_capitalization(&source_buffer_address, &account);
⋮----
let program_data_address = get_program_data_address(&builtin_id);
⋮----
program_data_account.state().unwrap();
⋮----
fn set_up_test_core_bpf_program(
⋮----
let programdata_address = get_program_data_address(program_address);
⋮----
let space = data.len();
⋮----
account.data_as_mut_slice().copy_from_slice(&data);
bank.store_account_and_update_capitalization(program_address, &account);
⋮----
account.data_as_mut_slice()[programdata_metadata_size..].copy_from_slice(&elf);
bank.store_account_and_update_capitalization(&programdata_address, &account);
⋮----
fn test_upgrade_core_bpf_program(upgrade_authority_address: Option<Pubkey>) {
⋮----
set_up_test_core_bpf_program(
⋮----
.calculate_post_upgrade_capitalization_and_accounts_data_size_delta_off_chain(&bank);
let upgrade_slot = bank.slot();
bank.upgrade_core_bpf_program(
⋮----
test_context.run_program_checks(&bank, upgrade_slot);
assert_eq!(bank.capitalization(), expected_post_upgrade_capitalization);
⋮----
fn test_upgrade_fail_authority_mismatch() {
⋮----
set_up_test_core_bpf_program(&mut bank, &program_address, upgrade_authority_address);
⋮----
Some(Pubkey::new_unique()),
⋮----
fn test_upgrade_none_authority_with_some_buffer_authority() {
⋮----
set_up_test_core_bpf_program(&mut bank, &program_address, None);
⋮----
let program_data_address = get_program_data_address(&program_address);
⋮----
mod cpi_mockup {
⋮----
declare_process_instruction!(Entrypoint, 0, |invoke_context| {
⋮----
enum TestPrototype<'a> {
⋮----
fn deconstruct(&'a self) -> (&'a Pubkey, &'a CoreBpfMigrationConfig) {
⋮----
prototype.core_bpf_migration_config.as_ref().unwrap(),
⋮----
fn activate_feature_and_run_checks(
⋮----
let (bank, bank_forks) = root_bank.wrap_with_bank_forks_for_tests();
⋮----
assert!(!bank.feature_set.is_active(feature_id));
assert!(bank.get_account(source_buffer_address).is_some());
⋮----
goto_end_of_slot(bank.clone());
⋮----
assert!(bank.feature_set.is_active(feature_id));
⋮----
let next_slot = bank.slot() + 1;
⋮----
bank.process_transaction(&Transaction::new(
&vec![&mint_keypair],
⋮----
Some(&mint_keypair.pubkey()),
⋮----
bank.last_blockhash(),
⋮----
vec![AccountMeta::new_readonly(*program_id, false)],
⋮----
fn test_migrate_builtin_e2e(prototype: TestPrototype) {
⋮----
create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
root_bank.add_builtin(
⋮----
ProgramCacheEntry::new_builtin(0, cpi_program_name.len(), cpi_mockup::Entrypoint::vm),
⋮----
let (builtin_id, config) = prototype.deconstruct();
⋮----
feature_set.deactivate(feature_id);
⋮----
activate_feature_and_run_checks(
⋮----
fn test_migrate_builtin_e2e_failure() {
let (genesis_config, _mint_keypair) = create_genesis_config(0);
⋮----
let (builtin_id, config) = test_prototype.deconstruct();
⋮----
feature_set.inactive_mut().insert(*feature_id);
⋮----
assert!(bank
⋮----
fn test_migrate_builtin_e2e_init_after_failed_migration() {
⋮----
feature_set.active_mut().insert(*feature_id, 0);
⋮----
activated_at: Some(0),
⋮----
.remove_programs(
⋮----
.clone()
.into_iter(),
⋮----
.clear();
bank.compute_and_apply_features_after_snapshot_restore();
⋮----
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
fn test_migrate_builtin_e2e_init_after_successful_migration() {
let (mut genesis_config, _mint_keypair) = create_genesis_config(0);
⋮----
let program_data_size = program_data_metadata_size + elf.len();
let builtin_program_data_address = get_program_data_address(builtin_id);
⋮----
builtin_program_data_account.data_as_mut_slice()[program_data_metadata_size..]
.copy_from_slice(&elf);
⋮----
.insert(*builtin_id, builtin_program_account.into());
genesis_config.accounts.insert(
⋮----
builtin_program_data_account.into(),
⋮----
let fetched_builtin_program_account = bank.get_account(builtin_id).unwrap();
⋮----
bank.get_account(&builtin_program_data_address).unwrap();
⋮----
check_builtin_is_bpf(&bank);
⋮----
fn test_upgrade_loader_v2_program_with_loader_v3_program() {
⋮----
let space = elf.len();
⋮----
account.data_as_mut_slice().copy_from_slice(&elf);
⋮----
bank.upgrade_loader_v2_program_with_loader_v3_program(
⋮----
let migrated_program_account = bank.get_account(&bpf_loader_v2_program_address).unwrap();
⋮----
fn test_upgrade_loader_v2_program_with_loader_v3_program_fail_invalid_buffer() {
⋮----
fn mock_bpf_loader_v2_program(bank: &Bank) -> AccountSharedData {
⋮----
fn test_replace_spl_token_with_p_token_e2e() {
⋮----
let program_account = mock_bpf_loader_v2_program(&root_bank);
root_bank.store_account_and_update_capitalization(&program_id, &program_account);
⋮----
feature_set.deactivate(&feature_id);
⋮----
fn test_replace_spl_token_with_p_token_e2e_failure() {
⋮----
root_bank.store_account_and_update_capitalization(program_id, &program_account);
⋮----
fn test_startup_from_snapshot_after_replace_spl_token_with_p_token() {
⋮----
bank.fill_bank_with_ticks_for_tests();
bank.squash();
bank.force_flush_accounts_cache();
let (_tmp_dir, accounts_dir) = create_tmp_accounts_dir_for_tests();
let bank_snapshots_dir = tempfile::TempDir::new().unwrap();
let snapshot_archives_dir = tempfile::TempDir::new().unwrap();
⋮----
let full_snapshot_archive_info = bank_to_full_snapshot_archive(
bank_snapshots_dir.path(),
⋮----
snapshot_archives_dir.path(),
⋮----
let roundtrip_bank = bank_from_snapshot_archives(
⋮----
.load_program(&bpf_loader_v2_program_address, false, upgrade_slot)
⋮----
program_cache.assign_program(
⋮----
drop(program_cache);
test_context.run_program_checks(&roundtrip_bank, upgrade_slot);
assert_eq!(bank, roundtrip_bank);
⋮----
fn test_replace_spl_token_with_p_token_and_funded_program_data_account_e2e() {
⋮----
root_bank.store_account_and_update_capitalization(
&get_program_data_address(&program_id),
⋮----
fn test_replace_spl_token_with_p_token_and_existing_program_data_account_failure() {
⋮----
assert!(!bank.feature_set.is_active(&feature_id));
assert!(bank.get_account(&source_buffer_address).is_some());
⋮----
assert!(bank.feature_set.is_active(&feature_id));
let program_account = bank.get_account(&program_id).unwrap();
assert_eq!(program_account.owner(), &bpf_loader::id());
⋮----
.get_account(&get_program_data_address(&program_id))
⋮----
assert_eq!(program_data_account.data().len(), 0);
assert_eq!(program_data_account.lamports(), 1_000_000_000);

================
File: runtime/src/bank/builtins/core_bpf_migration/source_buffer.rs
================
pub(crate) struct SourceBuffer {
⋮----
impl SourceBuffer {
pub(crate) fn new_checked(
⋮----
.get_account_with_fixed_root(buffer_address)
.ok_or(CoreBpfMigrationError::AccountNotFound(*buffer_address))?;
if buffer_account.owner() != &bpf_loader_upgradeable::id() {
return Err(CoreBpfMigrationError::IncorrectOwner(*buffer_address));
⋮----
if buffer_account.data().len() >= buffer_metadata_size {
⋮----
bincode::deserialize(&buffer_account.data()[..buffer_metadata_size])?
⋮----
return Ok(Self {
⋮----
Err(CoreBpfMigrationError::InvalidBufferAccount(*buffer_address))
⋮----
pub(crate) fn new_checked_with_verified_build_hash(
⋮----
let data = buffer.buffer_account.data();
⋮----
let end_offset = data.iter().rposition(|&x| x != 0).map_or(offset, |i| i + 1);
⋮----
return Err(CoreBpfMigrationError::BuildHashMismatch(
⋮----
Ok(buffer)
⋮----
mod tests {
⋮----
fn store_account(bank: &Bank, address: &Pubkey, data: &[u8], owner: &Pubkey) {
let space = data.len();
let lamports = bank.get_minimum_balance_for_rent_exemption(space);
⋮----
account.data_as_mut_slice().copy_from_slice(data);
bank.store_account_and_update_capitalization(address, &account);
⋮----
fn test_source_buffer() {
let bank = create_simple_test_bank(0);
⋮----
assert_matches!(
⋮----
store_account(
⋮----
.unwrap(),
⋮----
let elf = vec![4u8; 200];
⋮----
let data_len = buffer_metadata_size + elf.len();
let mut data = vec![0u8; data_len];
⋮----
.unwrap();
data[buffer_metadata_size..].copy_from_slice(&elf);
store_account(&bank, &buffer_address, &data, &bpf_loader_upgradeable::id());
let source_buffer = SourceBuffer::new_checked(&bank, &buffer_address).unwrap();
assert_eq!(source_buffer.buffer_address, buffer_address);
assert_eq!(
⋮----
test_success(Some(Pubkey::new_unique()));
test_success(None);

================
File: runtime/src/bank/builtins/core_bpf_migration/target_bpf_v2.rs
================
pub(crate) struct TargetBpfV2 {
⋮----
impl TargetBpfV2 {
pub(crate) fn new_checked(
⋮----
.get_account_with_fixed_root(program_address)
.ok_or(CoreBpfMigrationError::AccountNotFound(*program_address))?;
if program_account.owner() != &bpf_loader::id() {
return Err(CoreBpfMigrationError::IncorrectOwner(*program_address));
⋮----
if !program_account.executable() {
return Err(CoreBpfMigrationError::ProgramAccountNotExecutable(
⋮----
let program_data_address = get_program_data_address(program_address);
⋮----
if let Some(account) = bank.get_account_with_fixed_root(&program_data_address) {
if account.owner() != &system_program::id() {
return Err(CoreBpfMigrationError::ProgramHasDataAccount(
⋮----
account.lamports()
⋮----
Ok(Self {
⋮----
mod tests {
⋮----
fn store_account(bank: &Bank, address: &Pubkey, data: &[u8], owner: &Pubkey, executable: bool) {
let space = data.len();
let lamports = bank.get_minimum_balance_for_rent_exemption(space);
⋮----
account.set_executable(executable);
account.data_as_mut_slice().copy_from_slice(data);
bank.store_account_and_update_capitalization(address, &account);
⋮----
fn test_target_bpf_v2() {
let bank = create_simple_test_bank(0);
⋮----
let elf = vec![4u8; 200];
assert_matches!(
⋮----
store_account(
⋮----
store_account(&bank, &program_address, &elf, &bpf_loader::id(), true);
let target_bpf_v2 = TargetBpfV2::new_checked(&bank, &program_address).unwrap();
assert_eq!(target_bpf_v2.program_address, program_address);
assert_eq!(target_bpf_v2.program_account.data(), elf.as_slice());

================
File: runtime/src/bank/builtins/core_bpf_migration/target_builtin.rs
================
pub(crate) struct TargetBuiltin {
⋮----
impl TargetBuiltin {
pub(crate) fn new_checked(
⋮----
.get_account_with_fixed_root(program_address)
.ok_or(CoreBpfMigrationError::AccountNotFound(*program_address))?;
if program_account.owner() != &NATIVE_LOADER_ID {
return Err(CoreBpfMigrationError::IncorrectOwner(*program_address));
⋮----
if bank.get_account_with_fixed_root(program_address).is_some() {
return Err(CoreBpfMigrationError::AccountExists(*program_address));
⋮----
let program_data_address = get_program_data_address(program_address);
⋮----
if let Some(account) = bank.get_account_with_fixed_root(&program_data_address) {
if account.owner() != &SYSTEM_PROGRAM_ID {
return Err(CoreBpfMigrationError::ProgramHasDataAccount(
⋮----
account.lamports()
⋮----
Ok(Self {
⋮----
mod tests {
⋮----
fn store_account<T: serde::Serialize>(
⋮----
let data = bincode::serialize(data).unwrap();
let data_len = data.len();
let lamports = bank.get_minimum_balance_for_rent_exemption(data_len);
⋮----
bank.store_account_and_update_capitalization(address, &account);
⋮----
fn test_target_program_builtin(program_address: Pubkey, activation_feature: Option<Pubkey>) {
⋮----
let mut bank = create_simple_test_bank(0);
⋮----
bank.store_account(
⋮----
bank.get_minimum_balance_for_rent_exemption(feature::Feature::size_of()),
⋮----
bank.compute_and_apply_new_feature_activations();
⋮----
let program_account = bank.get_account_with_fixed_root(&program_address).unwrap();
let program_data_address = get_program_data_address(&program_address);
⋮----
TargetBuiltin::new_checked(&bank, &program_address, &migration_target).unwrap();
assert_eq!(target_builtin.program_address, program_address);
assert_eq!(target_builtin.program_account, program_account);
assert_eq!(target_builtin.program_data_address, program_data_address);
store_account(
⋮----
assert_matches!(
⋮----
&program_account.data(),
program_account.executable(),
program_account.owner(),
⋮----
upgrade_authority_address: Some(Pubkey::new_unique()),
⋮----
bank.store_account_and_update_capitalization(
⋮----
fn test_target_program_stateless_builtin(program_address: Pubkey) {
⋮----
let bank = create_simple_test_bank(0);

================
File: runtime/src/bank/builtins/core_bpf_migration/target_core_bpf.rs
================
pub(crate) struct TargetCoreBpf {
⋮----
impl TargetCoreBpf {
pub(crate) fn new_checked(
⋮----
let program_data_address = get_program_data_address(program_address);
⋮----
.get_account_with_fixed_root(program_address)
.ok_or(CoreBpfMigrationError::AccountNotFound(*program_address))?;
if program_account.owner() != &bpf_loader_upgradeable::id() {
return Err(CoreBpfMigrationError::IncorrectOwner(*program_address));
⋮----
if !program_account.executable() {
return Err(CoreBpfMigrationError::ProgramAccountNotExecutable(
⋮----
return Err(CoreBpfMigrationError::InvalidProgramAccount(
⋮----
.get_account_with_fixed_root(&program_data_address)
.ok_or(CoreBpfMigrationError::ProgramHasNoDataAccount(
⋮----
if program_data_account.owner() != &bpf_loader_upgradeable::id() {
return Err(CoreBpfMigrationError::IncorrectOwner(program_data_address));
⋮----
if program_data_account.data().len() >= programdata_metadata_size {
⋮----
} = bincode::deserialize(&program_data_account.data()[..programdata_metadata_size])?
⋮----
return Ok(Self {
⋮----
Err(CoreBpfMigrationError::InvalidProgramDataAccount(
⋮----
mod tests {
⋮----
fn store_account(bank: &Bank, address: &Pubkey, data: &[u8], owner: &Pubkey, executable: bool) {
let space = data.len();
let lamports = bank.get_minimum_balance_for_rent_exemption(space);
⋮----
account.set_executable(executable);
account.data_as_mut_slice().copy_from_slice(data);
bank.store_account_and_update_capitalization(address, &account);
⋮----
fn test_target_core_bpf() {
let bank = create_simple_test_bank(0);
⋮----
let program_data_address = get_program_data_address(&program_address);
assert_matches!(
⋮----
store_account(
⋮----
.unwrap(),
⋮----
upgrade_authority_address: Some(Pubkey::new_unique()),
⋮----
let elf = vec![4u8; 200];
⋮----
let data_len = programdata_metadata_size + elf.len();
let mut data = vec![0u8; data_len];
⋮----
.unwrap();
data[programdata_metadata_size..].copy_from_slice(&elf);
⋮----
let target_core_bpf = TargetCoreBpf::new_checked(&bank, &program_address).unwrap();
assert_eq!(target_core_bpf.program_address, program_address);
assert_eq!(target_core_bpf.program_data_address, program_data_address);
assert_eq!(
⋮----
test_success(Some(Pubkey::new_unique()));
test_success(None);

================
File: runtime/src/bank/builtins/mod.rs
================
pub(crate) mod core_bpf_migration;

================
File: runtime/src/bank/partitioned_epoch_rewards/calculation.rs
================
struct DelegationRewards {
⋮----
struct RewardsAccumulator {
⋮----
impl RewardsAccumulator {
fn add_reward(&mut self, vote_pubkey: Pubkey, vote_reward: VoteReward, stakers_reward: u64) {
⋮----
.entry(vote_pubkey)
.and_modify(|dst_vote_reward| {
⋮----
.saturating_add(vote_reward.vote_rewards)
⋮----
.or_insert(vote_reward);
self.num_stake_rewards = self.num_stake_rewards.saturating_add(1);
⋮----
.saturating_add(stakers_reward);
⋮----
fn accumulate_into_larger(self, rhs: Self) -> Self {
let (mut dst, src) = if self.vote_rewards.len() >= rhs.vote_rewards.len() {
⋮----
.and_modify(|dst_vote_reward: &mut VoteReward| {
⋮----
dst.num_stake_rewards = dst.num_stake_rewards.saturating_add(src.num_stake_rewards);
⋮----
.saturating_add(src.total_stake_rewards_lamports);
⋮----
impl Bank {
⋮----
pub(in crate::bank) fn begin_partitioned_rewards(
⋮----
self.distribute_vote_rewards(parent_epoch, rewards_calculation, rewards_metrics);
let slot = self.slot();
⋮----
self.block_height() + REWARD_CALCULATION_NUM_BLOCKS;
⋮----
let num_partitions = self.get_reward_distribution_num_blocks(&stake_rewards);
self.set_epoch_reward_status_calculation(distribution_starting_block_height, stake_rewards);
self.create_epoch_rewards_sysvar(
⋮----
datapoint_info!(
⋮----
pub(in crate::bank) fn calculate_rewards(
⋮----
self.epoch_rewards_calculation_cache.lock().unwrap();
⋮----
.entry(self.parent_hash)
.or_insert_with(|| {
let stake_delegations = self.filter_stake_delegations(stake_delegations);
Arc::new(self.calculate_rewards_for_partitioning(
⋮----
.clone();
drop(epoch_rewards_calculation_cache);
⋮----
pub(in crate::bank) fn distribute_vote_rewards(
⋮----
self.store_vote_accounts_partitioned(vote_account_rewards, rewards_metrics);
self.update_vote_rewards(vote_account_rewards);
⋮----
assert!(point_value.rewards >= total_vote_rewards + total_stake_rewards_lamports);
info!(
⋮----
let stakes = self.stakes_cache.stakes();
⋮----
stakes.stake_delegations().len(),
stakes.vote_accounts().len(),
⋮----
self.capitalization.fetch_add(total_vote_rewards, Relaxed);
⋮----
self.stakes_cache.stakes().history().get(prev_epoch)
⋮----
fn store_vote_accounts_partitioned(
⋮----
let (_, measure_us) = measure_us!({
⋮----
.fetch_add(measure_us, Relaxed);
⋮----
pub(super) fn calculate_rewards_for_partitioning<'a>(
⋮----
let capitalization = self.capitalization();
⋮----
} = self.calculate_previous_epoch_inflation_rewards(capitalization, prev_epoch);
⋮----
.calculate_validator_rewards(
⋮----
.unwrap_or_default();
⋮----
/// Calculate epoch reward and return vote and stake rewards.
    fn calculate_validator_rewards<'a>(
⋮----
fn calculate_validator_rewards<'a>(
⋮----
self.calculate_reward_points_partitioned(
⋮----
.map(|point_value| {
⋮----
.calculate_stake_vote_rewards(
⋮----
point_value.clone(),
⋮----
pub(in crate::bank) fn filter_stake_delegations<'a>(
⋮----
.is_active(&feature_set::stake_minimum_delegation_for_rewards::id())
⋮----
.is_active(&agave_feature_set::stake_raise_minimum_delegation_to_1_sol::id()),
⋮----
.max(LAMPORTS_PER_SOL);
Some(min_stake_delegation)
⋮----
fn get_epoch_params_for_recalculation<'a>(
⋮----
let stake_history = stakes.history().clone();
let stake_delegations = stakes.stake_delegations_vec();
⋮----
let leader_schedule_epoch = self.epoch_schedule().get_leader_schedule_epoch(self.slot());
⋮----
.epoch_stakes(leader_schedule_epoch)
.expect(
⋮----
.stakes()
.vote_accounts();
⋮----
fn redeem_delegation_rewards(
⋮----
let reward_calc_tracer = reward_calc_tracer.as_ref().map(|outer| {
⋮----
outer(&RewardCalculationEvent::Staking(stake_pubkey, inner_event))
⋮----
let vote_pubkey = stake_account.delegation().voter_pubkey;
let Some(vote_account) = cached_vote_accounts.get(&vote_pubkey) else {
debug!("could not find vote account {vote_pubkey} in cache");
⋮----
let vote_state = vote_account.vote_state_view();
let stake_state = stake_account.stake_state();
match redeem_rewards(
⋮----
let commission = vote_state.commission();
⋮----
let vote_account = vote_account.into();
⋮----
Some(DelegationRewards {
⋮----
debug!("redeem_rewards() failed for {stake_pubkey}: {e:?}");
⋮----
fn calculate_stake_vote_rewards<'a>(
⋮----
let new_warmup_cooldown_rate_epoch = self.new_warmup_cooldown_rate_epoch();
⋮----
// For N stake delegations, where N is >1,000,000, we produce:
// * N stake rewards,
// * M vote rewards, where M is a number of stake nodes. Currently, way
//   smaller number than 1,000,000. And we can expect it to always be
//   significantly smaller than number of delegations.
//
// Producing the stake reward with rayon triggers a lot of
// (re)allocations. To avoid that, we allocate it at the start and
// pass `stake_rewards.spare_capacity_mut()` as one of iterators.
let mut stake_rewards = PartitionedStakeRewards::with_capacity(stake_delegations.len());
let rewards_accumulator: RewardsAccumulator = thread_pool.install(|| {
⋮----
.par_iter()
.zip_eq(stake_rewards.spare_capacity_mut())
.with_min_len(500)
.filter_map(|(maybe_stake_delegation, stake_reward_ref)| {
⋮----
maybe_stake_delegation.and_then(|(stake_pubkey, stake_account)| {
self.redeem_delegation_rewards(
⋮----
reward_calc_tracer.as_ref(),
⋮----
Some(stake_reward),
Some((stakers_reward, vote_pubkey, vote_reward)),
⋮----
// It's important that for every stake delegation, we write
stake_reward_ref.write(stake_reward);
⋮----
.fold(
⋮----
rewards_accumulator.add_reward(vote_pubkey, vote_reward, stake_reward);
⋮----
.reduce(
⋮----
rewards_accumulator_a.accumulate_into_larger(rewards_accumulator_b)
⋮----
stake_rewards.assume_init(num_stake_rewards);
⋮----
measure_redeem_rewards.stop();
metrics.redeem_rewards_us = measure_redeem_rewards.as_us();
⋮----
fn calculate_reward_points_partitioned<'a>(
⋮----
let (points, measure_us) = measure_us!(thread_pool.install(|| {
⋮----
metrics.calculate_points_us.fetch_add(measure_us, Relaxed);
(points > 0).then_some(PointValue { rewards, points })
⋮----
/// If rewards are still active, recalculates partitioned stake rewards and
    /// updates Bank::epoch_reward_status. This method assumes that vote rewards
⋮----
/// updates Bank::epoch_reward_status. This method assumes that vote rewards
    /// have already been calculated and delivered, and *only* recalculates
⋮----
/// have already been calculated and delivered, and *only* recalculates
    /// stake rewards
⋮----
/// stake rewards
    pub(in crate::bank) fn recalculate_partitioned_rewards_if_active<F, TP>(
⋮----
pub(in crate::bank) fn recalculate_partitioned_rewards_if_active<F, TP>(
⋮----
let epoch_rewards_sysvar = self.get_epoch_rewards_sysvar();
⋮----
let thread_pool = thread_pool_builder();
⋮----
self.recalculate_stake_rewards(&epoch_rewards_sysvar, thread_pool.borrow());
self.set_epoch_reward_status_distribution(
⋮----
/// Returns a vector of partitioned stake rewards. StakeRewards are
    /// recalculated from an active EpochRewards sysvar, vote accounts from
⋮----
/// recalculated from an active EpochRewards sysvar, vote accounts from
    /// EpochStakes, and stake accounts from StakesCache.
⋮----
/// EpochStakes, and stake accounts from StakesCache.
    fn recalculate_stake_rewards(
⋮----
fn recalculate_stake_rewards(
⋮----
assert!(epoch_rewards_sysvar.active);
// If rewards are active, the rewarded epoch is always the immediately
// preceding epoch.
let rewarded_epoch = self.epoch().saturating_sub(1);
⋮----
} = self.get_epoch_params_for_recalculation(&stakes);
// On recalculation, only the `StakeRewardCalculation::stake_rewards`
// field is relevant. It is assumed that vote-account rewards have
// already been calculated and delivered, while
// `StakeRewardCalculation::total_rewards` only reflects rewards that
// have not yet been distributed.
let (_, StakeRewardCalculation { stake_rewards, .. }) = self.calculate_stake_vote_rewards(
⋮----
null_tracer(),
&mut RewardsMetrics::default(), // This is required, but not reporting anything at the moment
⋮----
drop(stakes);
let partition_indices = hash_rewards_into_partitions(
⋮----
mod tests {
⋮----
fn test_store_vote_accounts_partitioned() {
let (genesis_config, _mint_keypair) = create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
.map(|_| (Pubkey::new_unique(), VoteReward::new_random()))
⋮----
.iter()
.for_each(|(vote_key, vote_reward_info)| {
⋮----
commission: Some(vote_reward_info.commission),
⋮----
vote_rewards_account.accounts_with_rewards.push((
⋮----
vote_reward_info.vote_account.clone(),
⋮----
bank.store_vote_accounts_partitioned(&vote_rewards_account, &metrics);
assert_eq!(
⋮----
// load accounts to make sure they were stored correctly
⋮----
.load_slow_with_fixed_root(&bank.ancestors, vote_key)
.unwrap();
assert!(accounts_equal(
⋮----
fn test_store_vote_accounts_partitioned_empty() {
⋮----
bank.store_vote_accounts_partitioned(&vote_rewards, &metrics);
assert_eq!(expected, vote_rewards.accounts_with_rewards.len());
assert_eq!(0, total_vote_rewards);
⋮----
/// Test rewards computation and partitioned rewards distribution at the epoch boundary
    fn test_rewards_computation() {
⋮----
fn test_rewards_computation() {
⋮----
// Delegations with sufficient stake to get rewards (2 SOL).
⋮----
// Delegations with insufficient stake (0.5 SOL).
⋮----
.map(|_| 2_000_000_000)
.chain((0..delegations_without_rewards).map(|_| 500_000_000))
⋮----
let bank = create_reward_bank_with_specific_stakes(
⋮----
// Calculate rewards
let thread_pool = ThreadPoolBuilder::new().num_threads(1).build().unwrap();
⋮----
let stakes = bank.stakes_cache.stakes();
⋮----
} = bank.get_epoch_params_for_recalculation(&stakes);
let calculated_rewards = bank.calculate_validator_rewards(
⋮----
let vote_rewards = &calculated_rewards.as_ref().unwrap().vote_rewards_accounts;
⋮----
.as_ref()
.unwrap()
⋮----
.map(|(_, reward_info, _)| reward_info.lamports)
⋮----
// assert that total rewards matches the sum of vote rewards and stake rewards
⋮----
// assert that number of stake rewards matches
⋮----
fn test_rewards_point_calculation() {
⋮----
create_default_reward_bank(expected_num_delegations, SLOTS_PER_EPOCH).0;
⋮----
let stakes: RwLockReadGuard<Stakes<StakeAccount<Delegation>>> = bank.stakes_cache.stakes();
⋮----
let point_value = bank.calculate_reward_points_partitioned(
⋮----
assert!(point_value.is_some());
assert_eq!(point_value.as_ref().unwrap().rewards, expected_rewards);
assert_eq!(point_value.as_ref().unwrap().points, 8400000000000);
⋮----
fn test_rewards_point_calculation_empty() {
⋮----
// bank with no rewards to distribute
let (genesis_config, _mint_keypair) = create_genesis_config(LAMPORTS_PER_SOL);
⋮----
assert!(point_value.is_none());
⋮----
fn test_calculate_stake_vote_rewards() {
⋮----
} = create_default_reward_bank(expected_num_delegations, SLOTS_PER_EPOCH).0;
let vote_pubkey = voters.first().unwrap();
let stake_pubkey = *stakers.first().unwrap();
⋮----
.load_slow_with_fixed_root(&bank.ancestors, &stake_pubkey)
⋮----
rewards: 100000, // lamports to split
points: 1000,    // over these points
⋮----
let reward_calc_tracer = Some(tracer);
let rewarded_epoch = bank.epoch();
⋮----
let (vote_rewards_accounts, stake_reward_calculation) = bank.calculate_stake_vote_rewards(
⋮----
.load_slow_with_fixed_root(&bank.ancestors, vote_pubkey)
⋮----
let vote_state = VoteStateV4::deserialize(vote_account.data(), vote_pubkey).unwrap();
⋮----
assert_eq!(vote_rewards_accounts.accounts_with_rewards.len(), 1);
⋮----
assert_eq!(account.lamports(), vote_account.lamports());
assert!(accounts_equal(account, &vote_account));
⋮----
assert_eq!(vote_pubkey_from_result, vote_pubkey);
assert_eq!(stake_reward_calculation.stake_rewards.num_rewards(), 1);
⋮----
let stake_state: StakeStateV2 = stake_account.state().unwrap();
let mut stake = stake_state.stake().unwrap();
stake.credits_observed = vote_state.credits();
⋮----
fn compare_stake_rewards(
⋮----
for (i, partition) in received_stake_rewards.iter().enumerate() {
⋮----
assert_eq!(partition, expected_partition);
⋮----
fn test_recalculate_stake_rewards() {
⋮----
// Distribute 4 rewards over 2 blocks
let (RewardBank { bank, .. }, _) = create_reward_bank(
⋮----
} = bank.calculate_rewards_for_partitioning(
⋮----
let epoch_rewards_sysvar = bank.get_epoch_rewards_sysvar();
⋮----
bank.recalculate_stake_rewards(&epoch_rewards_sysvar, &thread_pool);
⋮----
build_partitioned_stake_rewards(&recalculated_rewards, &recalculated_partition_indices);
let expected_partition_indices = hash_rewards_into_partitions(
⋮----
build_partitioned_stake_rewards(&expected_stake_rewards, &expected_partition_indices);
⋮----
compare_stake_rewards(&expected_stake_rewards_partitioned, &recalculated_rewards);
// Advance to first distribution block, ie. child block of the epoch
// boundary; slot is advanced 2 to demonstrate that distribution works
// on block-height, not slot
let new_slot = bank.slot() + 2;
⋮----
// Note that recalculated rewards are **NOT** the same as expected
// rewards, which were calculated before any distribution. This is
// because "Recalculated rewards" doesn't include already distributed
⋮----
assert_eq!(recalculated_rewards[0].num_rewards(), 0);
let starting_index = (bank.block_height() + 1
⋮----
compare_stake_rewards(
⋮----
let new_slot = bank.slot() + 1;
⋮----
assert!(!epoch_rewards_sysvar.active);
⋮----
fn test_recalculate_stake_rewards_distribution_complete() {
⋮----
assert_eq!(expected_stake_rewards.len(), recalculated_rewards.len());
compare_stake_rewards(&expected_stake_rewards, &recalculated_rewards);
⋮----
fn test_recalculate_partitioned_rewards() {
⋮----
let mut stakes = vec![2_000_000_000; expected_num_delegations];
stakes.push(40_000_000_000);
let (RewardBank { bank, .. }, _) = create_reward_bank_with_specific_stakes(
⋮----
let expected_starting_block_height = bank.block_height() + 1;
⋮----
bank.recalculate_partitioned_rewards_if_active(|| &thread_pool);
⋮----
panic!("{:?} not active", bank.epoch_reward_status);
⋮----
build_partitioned_stake_rewards(recalculated_rewards, partition_indices);
⋮----
let sysvar = bank.get_epoch_rewards_sysvar();
assert_eq!(point_value.rewards, sysvar.total_rewards);
⋮----
assert_eq!(bank.epoch_reward_status, EpochRewardStatus::Inactive);
⋮----
fn test_initialize_after_snapshot_restore() {
⋮----
let stakes = vec![
⋮----
bank.epoch_reward_status.clone()
⋮----
panic!("{:?} not active calculation", bank.epoch_reward_status);
⋮----
bank.initialize_after_snapshot_restore(|| &thread_pool);
⋮----
panic!("{:?} not active distribution", bank.epoch_reward_status);
⋮----
fn test_reward_accumulator() {
⋮----
accumulator1.add_reward(
⋮----
vote_account: vote_account_a.clone(),
⋮----
vote_account: vote_account_b.clone(),
⋮----
accumulator2.add_reward(
⋮----
assert_eq!(accumulator1.num_stake_rewards, 2);
assert_eq!(accumulator1.total_stake_rewards_lamports, 100);
let vote_reward_a_1 = accumulator1.vote_rewards.get(&vote_pubkey_a).unwrap();
assert_eq!(vote_reward_a_1.commission, 10);
assert_eq!(vote_reward_a_1.vote_rewards, 50);
let vote_reward_b_1 = accumulator1.vote_rewards.get(&vote_pubkey_b).unwrap();
assert_eq!(vote_reward_b_1.commission, 10);
assert_eq!(vote_reward_b_1.vote_rewards, 50);
let vote_reward_b_2 = accumulator2.vote_rewards.get(&vote_pubkey_b).unwrap();
assert_eq!(vote_reward_b_2.commission, 10);
assert_eq!(vote_reward_b_2.vote_rewards, 30);
let vote_reward_c_2 = accumulator2.vote_rewards.get(&vote_pubkey_c).unwrap();
assert_eq!(vote_reward_c_2.commission, 10);
assert_eq!(vote_reward_c_2.vote_rewards, 50);
let accumulator = accumulator1.accumulate_into_larger(accumulator2);
assert_eq!(accumulator.num_stake_rewards, 4);
assert_eq!(accumulator.total_stake_rewards_lamports, 180);
let vote_reward_a = accumulator.vote_rewards.get(&vote_pubkey_a).unwrap();
assert_eq!(vote_reward_a.commission, 10);
assert_eq!(vote_reward_a.vote_rewards, 50);
let vote_reward_b = accumulator.vote_rewards.get(&vote_pubkey_b).unwrap();
assert_eq!(vote_reward_b.commission, 10);
assert_eq!(vote_reward_b.vote_rewards, 80);
let vote_reward_c = accumulator.vote_rewards.get(&vote_pubkey_c).unwrap();
assert_eq!(vote_reward_c.commission, 10);
assert_eq!(vote_reward_c.vote_rewards, 50);
⋮----
fn test_epoch_rewards_cache_multiple_forks() {
⋮----
create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
genesis_config.accounts.insert(
⋮----
.into(),
⋮----
.insert(stake_pubkey, stake_account.into());
⋮----
let slots_per_epoch = bank.epoch_schedule().slots_per_epoch;
⋮----
let cache = bank.epoch_rewards_calculation_cache.lock().unwrap();
assert!(
⋮----
let cache = bank_fork1.epoch_rewards_calculation_cache.lock().unwrap();
⋮----
let cache = bank_fork2.epoch_rewards_calculation_cache.lock().unwrap();
⋮----
fn add_voters_and_populate(
⋮----
create_staked_node_accounts(stake_lamports);
bank.store_account_and_update_capitalization(&vote_pubkey, &vote_account);
bank.store_account_and_update_capitalization(&stake_pubkey, &stake_account);
voters.insert(vote_pubkey);
stakers.insert(stake_pubkey);
⋮----
populate_vote_accounts_with_votes(bank, voters.iter().copied(), commission);
⋮----
fn assert_cached_rewards(
⋮----
assert_eq!(cache.len(), expected_cache_len);
let partitioned = cache.get(&bank.parent_hash()).unwrap().as_ref();
⋮----
.map(|(pubkey, _reward, _acc)| *pubkey)
.collect();
⋮----
.filter_map(|reward| reward.as_ref())
.map(|reward| reward.stake_pubkey)
⋮----
assert_eq!(expected_voters, &voters);
assert_eq!(expected_stakers, &stakers);
assert_eq!(*total_vote_rewards_lamports, expected_vote_rewards);
assert_eq!(*total_stake_rewards_lamports, expected_stake_rewards);
assert_eq!(point_value.rewards, expected_rewards);
assert_eq!(point_value.points, expected_points);
⋮----
assert_eq!(bank.capitalization(), parent_cap + expected_vote_rewards);
⋮----
fn test_epoch_boundary() {
⋮----
let stakes: Vec<_> = (0..delegations).map(|_| stake_lamports).collect();
⋮----
) = create_reward_bank_with_specific_stakes(
⋮----
let mut voters: HashSet<_> = voters.into_iter().collect();
let mut stakers: HashSet<_> = stakers.into_iter().collect();
let epoch_rewards_sysvar_balance = bank1.get_balance(&solana_sysvar::epoch_rewards::id());
assert_eq!(epoch_rewards_sysvar_balance, 1);
assert_cached_rewards(
⋮----
add_voters_and_populate(&bank1, &mut voters, &mut stakers, 5, 5_000_000_000, 10);
let parent_capitalization = bank1.capitalization();
⋮----
Some(parent_capitalization),
⋮----
add_voters_and_populate(&bank2, &mut voters, &mut stakers, 10, 8_000_000_000, 10);
let parent_capitalization = bank2.capitalization();

================
File: runtime/src/bank/partitioned_epoch_rewards/distribution.rs
================
enum DistributionError {
⋮----
struct DistributionResults {
⋮----
impl Bank {
pub(in crate::bank) fn distribute_partitioned_epoch_rewards(&mut self) {
⋮----
let height = self.block_height();
⋮----
let epoch_rewards_sysvar = self.get_epoch_rewards_sysvar();
let (partition_indices, partition_us) = measure_us!({
⋮----
self.set_epoch_reward_status_distribution(
⋮----
datapoint_info!(
⋮----
unreachable!(
⋮----
distribution_starting_block_height + partition_rewards.partition_indices.len() as u64;
assert!(
⋮----
self.distribute_epoch_rewards_in_partition(partition_rewards, partition_index);
⋮----
if height.saturating_add(1) >= distribution_end_exclusive {
⋮----
assert!(matches!(
⋮----
self.set_epoch_rewards_sysvar_to_inactive();
⋮----
fn distribute_epoch_rewards_in_partition(
⋮----
let pre_capitalization = self.capitalization();
⋮----
) = measure_us!(self.store_stake_accounts_in_partition(partition_rewards, partition_index));
self.capitalization.fetch_add(lamports_distributed, Relaxed);
self.update_epoch_rewards_sysvar(lamports_distributed + lamports_burned);
self.update_reward_history_in_partition(&updated_stake_rewards);
⋮----
post_capitalization: self.capitalization(),
total_stake_accounts_count: partition_rewards.all_stake_rewards.num_rewards(),
total_num_partitions: partition_rewards.partition_indices.len(),
⋮----
store_stake_accounts_count: updated_stake_rewards.len(),
⋮----
report_partitioned_reward_metrics(self, metrics);
⋮----
fn update_reward_history_in_partition(&self, stake_rewards: &[StakeReward]) -> usize {
let mut rewards = self.rewards.write().unwrap();
rewards.reserve(stake_rewards.len());
let initial_len = rewards.len();
⋮----
.iter()
.filter(|x| x.get_stake_reward() > 0)
.for_each(|x| rewards.push((x.stake_pubkey, x.stake_reward_info)));
rewards.len().saturating_sub(initial_len)
⋮----
fn build_updated_stake_reward(
⋮----
.get(&partitioned_stake_reward.stake_pubkey)
.ok_or(DistributionError::AccountNotFound)?
.clone();
let (mut account, stake_state): (AccountSharedData, StakeStateV2) = stake_account.into();
⋮----
.checked_add_lamports(partitioned_stake_reward.stake_reward)
.map_err(|_| DistributionError::ArithmeticOverflow)?;
assert_eq!(
⋮----
.set_state(&StakeStateV2::Stake(
⋮----
.map_err(|_| DistributionError::UnableToSetState)?;
Ok(StakeReward {
⋮----
lamports: i64::try_from(partitioned_stake_reward.stake_reward).unwrap(),
post_balance: account.lamports(),
commission: Some(partitioned_stake_reward.commission),
⋮----
fn store_stake_accounts_in_partition(
⋮----
.get(partition_index as usize)
.unwrap_or_else(|| {
panic!(
⋮----
let mut updated_stake_rewards = Vec::with_capacity(indices.len());
let stakes_cache = self.stakes_cache.stakes();
let stakes_cache_accounts = stakes_cache.stake_delegations();
⋮----
.get(*index)
⋮----
.as_ref()
⋮----
panic!("partition reward {index} is empty");
⋮----
updated_stake_rewards.push(stake_reward);
⋮----
error!(
⋮----
drop(stakes_cache);
self.store_accounts((self.slot(), &updated_stake_rewards[..]));
⋮----
mod tests {
⋮----
fn test_distribute_partitioned_epoch_rewards() {
let (genesis_config, _mint_keypair) = create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
.map(|_| Some(PartitionedStakeReward::new_random()))
⋮----
hash_rewards_into_partitions(&stake_rewards, &Hash::new_from_array([1; 32]), 2);
bank.set_epoch_reward_status_distribution(
bank.block_height() + REWARD_CALCULATION_NUM_BLOCKS,
⋮----
bank.distribute_partitioned_epoch_rewards();
⋮----
fn test_distribute_partitioned_epoch_rewards_too_many_partitions() {
⋮----
let partition_indices = hash_rewards_into_partitions(
⋮----
bank.epoch_schedule().slots_per_epoch as usize + 1,
⋮----
bank.block_height(),
⋮----
fn test_distribute_partitioned_epoch_rewards_empty() {
⋮----
vec![],
⋮----
fn populate_starting_stake_accounts_from_stake_rewards(bank: &Bank, rewards: &[StakeReward]) {
⋮----
for stake_reward in rewards.iter() {
let lamports = stake_reward.stake_account.lamports()
⋮----
bank.store_account(&stake_reward.stake_pubkey, &validator_stake_account);
⋮----
fn test_distribute_partitioned_epoch_rewards_bank_capital_and_sysvar_balance() {
⋮----
create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
bank.create_epoch_rewards_sysvar(
⋮----
let pre_epoch_rewards_account = bank.get_account(&sysvar::epoch_rewards::id()).unwrap();
⋮----
bank.get_minimum_balance_for_rent_exemption(pre_epoch_rewards_account.data().len());
assert_eq!(pre_epoch_rewards_account.lamports(), expected_balance);
⋮----
.map(|_| StakeReward::new_random())
⋮----
.map(|stake_reward| stake_reward.stake_reward_info.lamports)
⋮----
populate_starting_stake_accounts_from_stake_rewards(&bank, &stake_rewards);
let all_rewards = convert_rewards(stake_rewards);
⋮----
distribution_starting_block_height: bank.block_height() + REWARD_CALCULATION_NUM_BLOCKS,
⋮----
partition_indices: vec![(0..expected_num as usize).collect::<Vec<_>>()],
⋮----
let pre_cap = bank.capitalization();
bank.distribute_epoch_rewards_in_partition(&partitioned_rewards, 0);
let post_cap = bank.capitalization();
let post_epoch_rewards_account = bank.get_account(&sysvar::epoch_rewards::id()).unwrap();
assert_eq!(post_epoch_rewards_account.lamports(), expected_balance);
⋮----
from_account(&post_epoch_rewards_account).unwrap();
assert_eq!(epoch_rewards.total_rewards, total_rewards);
assert_eq!(epoch_rewards.distributed_rewards, rewards_to_distribute,);
assert_eq!(pre_cap + rewards_to_distribute, post_cap);
⋮----
fn test_epoch_credit_rewards_and_history_update() {
⋮----
stake_rewards.push(StakeReward::new_random());
let stake_rewards = convert_rewards(stake_rewards);
⋮----
hash_rewards_into_partitions(&stake_rewards, &Hash::new_from_array([1; 32]), 100);
let num_partitions = partition_indices.len();
⋮----
let pre_update_history_len = bank.rewards.read().unwrap().len();
⋮----
} = bank.store_stake_accounts_in_partition(&partitioned_rewards, i as u64);
⋮----
bank.update_reward_history_in_partition(&updated_stake_rewards);
assert_eq!(updated_stake_rewards.len(), num_history_updates);
⋮----
let post_update_history_len = bank.rewards.read().unwrap().len();
assert_eq!(total_rewards, expected_rewards);
assert_eq!(total_num_updates, expected_num);
⋮----
fn test_update_reward_history_in_partition() {
⋮----
let i_zero = rng.random_range(0..expected_num);
⋮----
let num_in_history = bank.update_reward_history_in_partition(&stake_rewards);
⋮----
stake_rewards.remove(i_zero);
⋮----
.read()
.unwrap()
⋮----
.zip(stake_rewards.iter())
.for_each(|((k, reward_info), expected_stake_reward)| {
⋮----
assert_eq!(num_in_history, expected_num);
⋮----
fn test_build_updated_stake_reward() {
⋮----
let stakes_cache = bank.stakes_cache.stakes();
⋮----
.unwrap();
bank.store_account(&overflowing_account, &stake_account);
⋮----
bank.store_account(&successful_account, &stake_account);
⋮----
commission: Some(commission),
⋮----
fn test_update_reward_history_in_partition_empty() {
⋮----
let stake_rewards = vec![];
⋮----
assert_eq!(num_in_history, 0);
⋮----
fn test_store_stake_accounts_in_partition() {
⋮----
let converted_rewards = convert_rewards(stake_rewards);
⋮----
.enumerated_rewards_iter()
.map(|(_, stake_reward)| stake_reward.stake_reward)
⋮----
} = bank.store_stake_accounts_in_partition(&partitioned_rewards, 0);
assert_eq!(expected_total, lamports_distributed);
⋮----
fn test_store_stake_accounts_in_partition_empty() {
⋮----
partition_indices: vec![vec![]],

================
File: runtime/src/bank/partitioned_epoch_rewards/epoch_rewards_hasher.rs
================
pub(in crate::bank::partitioned_epoch_rewards) fn hash_rewards_into_partitions(
⋮----
let mut indices = vec![vec![]; num_partitions];
for (i, reward) in stake_rewards.enumerated_rewards_iter() {
⋮----
.clone()
.hash_address_to_partition(&reward.stake_pubkey);
indices[partition_index].push(i);
⋮----
mod tests {
⋮----
fn test_hash_rewards_into_partitions() {
⋮----
.map(|_| Some(PartitionedStakeReward::new_random()))
⋮----
let partition_indices = hash_rewards_into_partitions(&stake_rewards, &Hash::default(), 5);
let total_num_after_hash_partition: usize = partition_indices.iter().map(|x| x.len()).sum();
assert_eq!(expected_num, total_num_after_hash_partition);
⋮----
fn test_hash_rewards_into_partitions_empty() {
⋮----
hash_rewards_into_partitions(&stake_rewards, &Hash::default(), num_partitions);
assert_eq!(partition_indices.len(), num_partitions);
for indices in partition_indices.iter().take(num_partitions) {
assert!(indices.is_empty());
⋮----
fn test_get_stake_rewards_partition_range_panic() {
⋮----
create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
hash_rewards_into_partitions(&stake_rewards, &Hash::new_from_array([1; 32]), 10);
bank.set_epoch_reward_status_distribution(
bank.block_height() + REWARD_CALCULATION_NUM_BLOCKS,
⋮----
partition_indices.clone(),

================
File: runtime/src/bank/partitioned_epoch_rewards/mod.rs
================
mod calculation;
mod distribution;
mod epoch_rewards_hasher;
mod sysvar;
⋮----
pub(crate) struct PartitionedStakeReward {
⋮----
pub(crate) struct PartitionedStakeRewards {
⋮----
impl PartitionedStakeRewards {
pub(crate) fn with_capacity(capacity: usize) -> Self {
⋮----
pub(crate) fn num_rewards(&self) -> usize {
⋮----
pub(crate) fn total_len(&self) -> usize {
self.rewards.len()
⋮----
pub(crate) fn get(&self, index: usize) -> Option<&Option<PartitionedStakeReward>> {
self.rewards.get(index)
⋮----
pub(crate) fn enumerated_rewards_iter(
⋮----
.iter()
.enumerate()
.filter_map(|(index, reward)| reward.as_ref().map(|reward| (index, reward)))
⋮----
fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<Option<PartitionedStakeReward>>] {
self.rewards.spare_capacity_mut()
⋮----
unsafe fn assume_init(&mut self, num_stake_rewards: usize) {
⋮----
self.rewards.set_len(self.rewards.capacity());
⋮----
fn from_iter<T: IntoIterator<Item = Option<PartitionedStakeReward>>>(iter: T) -> Self {
⋮----
let rewards = Vec::from_iter(iter.into_iter().inspect(|reward| {
if reward.is_some() {
len_some = len_some.saturating_add(1);
⋮----
pub(crate) struct StartBlockHeightAndRewards {
⋮----
pub(crate) struct StartBlockHeightAndPartitionedRewards {
⋮----
pub(crate) enum EpochRewardStatus {
⋮----
pub(crate) enum EpochRewardPhase {
⋮----
pub(super) struct VoteRewardsAccounts {
⋮----
pub(super) struct VoteRewardsAccountsStorable<'a> {
⋮----
fn account<Ret>(
⋮----
callback((pubkey, account).into())
⋮----
fn is_zero_lamport(&self, index: usize) -> bool {
⋮----
.lamports()
⋮----
fn data_len(&self, index: usize) -> usize {
⋮----
.data()
.len()
⋮----
fn pubkey(&self, index: usize) -> &Pubkey {
⋮----
fn slot(&self, _index: usize) -> Slot {
self.target_slot()
⋮----
fn target_slot(&self) -> Slot {
⋮----
fn len(&self) -> usize {
self.vote_rewards_accounts.accounts_with_rewards.len()
⋮----
/// result of calculating the stake rewards at end of epoch
pub(super) struct StakeRewardCalculation {
⋮----
pub(super) struct StakeRewardCalculation {
/// each individual stake account to reward
    stake_rewards: Arc<PartitionedStakeRewards>,
/// total lamports across all `stake_rewards`
    total_stake_rewards_lamports: u64,
⋮----
struct CalculateValidatorRewardsResult {
⋮----
impl Default for CalculateValidatorRewardsResult {
fn default() -> Self {
⋮----
pub(super) struct FilteredStakeDelegations<'a> {
⋮----
pub(super) fn len(&self) -> usize {
self.stake_delegations.len()
⋮----
pub(super) fn par_iter(
⋮----
.par_iter()
// We yield `None` items instead of filtering them out to
// keep the number of elements predictable. It's better to
.map(|(pubkey, stake_account)| {
⋮----
if stake_account.delegation().stake < min_stake_delegation =>
⋮----
Some((*pubkey, *stake_account))
⋮----
pub(super) struct EpochRewardCalculateParamInfo<'a> {
⋮----
pub(super) struct PartitionedRewardsCalculation {
⋮----
pub(crate) type StakeRewards = Vec<StakeReward>;
⋮----
pub struct KeyedRewardsAndNumPartitions {
⋮----
impl KeyedRewardsAndNumPartitions {
pub fn should_record(&self) -> bool {
!self.keyed_rewards.is_empty() || self.num_partitions.is_some()
⋮----
impl Bank {
pub fn get_rewards_and_num_partitions(&self) -> KeyedRewardsAndNumPartitions {
let keyed_rewards = self.rewards.read().unwrap().clone();
let epoch_rewards_sysvar = self.get_epoch_rewards_sysvar();
let epoch_schedule = self.epoch_schedule();
let parent_epoch = epoch_schedule.get_epoch(self.parent_slot());
let is_first_block_in_epoch = self.epoch() > parent_epoch;
⋮----
.then_some(epoch_rewards_sysvar.num_partitions);
⋮----
pub(crate) fn set_epoch_reward_status_calculation(
⋮----
pub(crate) fn set_epoch_reward_status_distribution(
⋮----
pub(super) fn partitioned_epoch_rewards_config(&self) -> &PartitionedEpochRewardsConfig {
⋮----
pub(super) fn partitioned_rewards_stake_account_stores_per_block(&self) -> u64 {
self.partitioned_epoch_rewards_config()
⋮----
pub(super) fn get_reward_distribution_num_blocks(
⋮----
let total_stake_accounts = rewards.num_rewards();
if self.epoch_schedule.warmup && self.epoch < self.first_normal_epoch() {
⋮----
.div_ceil(self.partitioned_rewards_stake_account_stores_per_block() as usize)
⋮----
num_chunks.clamp(
⋮----
(self.epoch_schedule.slots_per_epoch / MAX_FACTOR_OF_REWARD_BLOCKS_IN_EPOCH).max(1),
⋮----
pub fn force_reward_interval_end_for_tests(&mut self) {
⋮----
mod tests {
⋮----
impl PartitionedStakeReward {
fn maybe_from(stake_reward: &StakeReward) -> Option<Self> {
⋮----
stake_reward.stake_account.state()
⋮----
Some(Self {
⋮----
commission: stake_reward.stake_reward_info.commission.unwrap(),
⋮----
pub fn new_random() -> Self {
Self::maybe_from(&StakeReward::new_random()).unwrap()
⋮----
pub fn build_partitioned_stake_rewards(
⋮----
.map(|partition_index| {
⋮----
.map(|&index| stake_rewards.get(index).unwrap().clone())
⋮----
pub fn convert_rewards(
⋮----
.into_iter()
.map(|stake_reward| Some(PartitionedStakeReward::maybe_from(&stake_reward).unwrap()))
.collect()
⋮----
enum RewardInterval {
⋮----
fn get_reward_interval(&self) -> RewardInterval {
if matches!(self.epoch_reward_status, EpochRewardStatus::Active(_)) {
⋮----
fn is_calculated(&self) -> bool {
matches!(
⋮----
fn is_partitioned(&self) -> bool {
⋮----
fn get_epoch_rewards_from_cache(
⋮----
.lock()
.unwrap()
.get(parent_hash)
.cloned()
⋮----
fn get_epoch_rewards_cache_len(&self) -> usize {
self.epoch_rewards_calculation_cache.lock().unwrap().len()
⋮----
pub(super) struct RewardBank {
⋮----
pub(super) fn create_default_reward_bank(
⋮----
create_reward_bank(
⋮----
pub(super) fn create_reward_bank(
⋮----
create_reward_bank_with_specific_stakes(
vec![2_000_000_000; expected_num_delegations],
⋮----
pub(super) fn create_reward_bank_with_specific_stakes(
⋮----
let validator_keypairs = (0..stakes.len())
.map(|_| ValidatorVoteKeypairs::new_rand())
⋮----
} = create_genesis_config_with_vote_accounts(1_000_000_000, &validator_keypairs, stakes);
⋮----
let mut accounts_db_config: AccountsDbConfig = ACCOUNTS_DB_CONFIG_FOR_TESTING.clone();
⋮----
Some(Pubkey::new_unique()),
⋮----
populate_vote_accounts_with_votes(
⋮----
validator_keypairs.iter().map(|k| k.vote_keypair.pubkey()),
⋮----
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
.map(|k| k.vote_keypair.pubkey())
.collect(),
⋮----
.map(|k| k.stake_keypair.pubkey())
⋮----
pub(super) fn populate_vote_accounts_with_votes(
⋮----
.get_account(&vote_pubkey)
.unwrap_or_else(|| panic!("missing vote account {vote_pubkey:?}"));
⋮----
Some(VoteStateV4::deserialize(vote_account.data(), &vote_pubkey).unwrap());
if let Some(state) = vote_state.as_mut() {
state.set_commission(commission);
⋮----
let versioned = VoteStateVersions::V4(Box::new(vote_state.take().unwrap()));
vote_account.set_state(&versioned).unwrap();
⋮----
vote_state = Some(*v);
⋮----
_ => panic!("Has to be of type V4"),
⋮----
bank.store_account_and_update_capitalization(&vote_pubkey, &vote_account);
⋮----
fn test_force_reward_interval_end() {
let (genesis_config, _mint_keypair) = create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
.map(|_| Some(PartitionedStakeReward::new_random()))
⋮----
let partition_indices = vec![(0..expected_num).collect()];
bank.set_epoch_reward_status_distribution(
bank.block_height() + REWARD_CALCULATION_NUM_BLOCKS,
⋮----
assert!(bank.get_reward_interval() == RewardInterval::InsideInterval);
bank.force_reward_interval_end_for_tests();
assert!(bank.get_reward_interval() == RewardInterval::OutsideInterval);
⋮----
fn test_get_reward_distribution_num_blocks_cap() {
⋮----
create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
bank.partitioned_rewards_stake_account_stores_per_block();
assert_eq!(stake_account_stores_per_block, 10);
⋮----
assert_eq!(
⋮----
check_num_reward_distribution_blocks(test_record.0, test_record.1);
⋮----
fn test_get_reward_distribution_num_blocks_normal() {
⋮----
assert_eq!(bank.get_reward_distribution_num_blocks(&stake_rewards), 2);
⋮----
fn test_get_reward_distribution_num_blocks_warmup() {
⋮----
assert_eq!(bank.get_reward_distribution_num_blocks(&rewards), 1);
⋮----
fn test_get_reward_distribution_num_blocks_none() {
⋮----
.map(|i| {
⋮----
Some(PartitionedStakeReward::new_random())
⋮----
assert_eq!(rewards.rewards.len(), rewards_all);
assert_eq!(rewards.num_rewards(), expected_rewards_some);
⋮----
fn test_rewards_computation_and_partitioned_distribution_one_block() {
⋮----
) = create_default_reward_bank(100, starting_slot - 1);
⋮----
let pre_cap = previous_bank.capitalization();
⋮----
bank_forks.as_ref(),
previous_bank.clone(),
⋮----
let post_cap = curr_bank.capitalization();
⋮----
assert_matches!(
⋮----
assert!(curr_bank.is_calculated());
assert!(curr_bank
⋮----
assert_eq!(post_cap, pre_cap);
let _ = bank_forks.write().unwrap().set_root(slot, None, None);
assert_eq!(curr_bank.get_epoch_rewards_cache_len(), 0);
⋮----
.get_account(&solana_sysvar::epoch_rewards::id())
.unwrap();
⋮----
solana_account::from_account(&account).unwrap();
assert_eq!(post_cap, pre_cap + epoch_rewards.distributed_rewards);
⋮----
curr_bank.get_balance(&solana_sysvar::epoch_rewards::id());
assert!(epoch_rewards_lamports > 0);
⋮----
fn test_rewards_computation_and_partitioned_distribution_two_blocks() {
⋮----
) = create_reward_bank(100, 50, starting_slot - 1);
⋮----
.unwrap_or_default();
⋮----
solana_account::from_account(&pre_sysvar_account).unwrap_or_default();
⋮----
assert_eq!(curr_bank.get_epoch_rewards_cache_len(), 1);
starting_hash = Some(curr_bank.parent_hash);
⋮----
assert!(curr_bank.is_partitioned());
⋮----
let _ = bank_forks.write().unwrap().set_root(slot - 1, None, None);
⋮----
fn test_rewards_period_system_transfer() {
⋮----
let validator_keypairs = vec![&validator_vote_keypairs];
⋮----
} = create_genesis_config_with_vote_accounts(
⋮----
vec![1_000_000_000; 1],
⋮----
let vote_key = validator_keypairs[0].vote_keypair.pubkey();
⋮----
.find(|(address, _)| **address == vote_key)
.map(|(_, account)| account)
⋮----
.clone();
⋮----
let new_stake_address = new_stake_signer.pubkey();
⋮----
&vote_account.into(),
⋮----
.extend(vec![(new_stake_address, new_stake_account)]);
⋮----
let num_slots_in_epoch = previous_bank.get_slots_in_epoch(previous_bank.epoch());
assert_eq!(num_slots_in_epoch, 32);
⋮----
let tower_sync = TowerSync::new_from_slot(slot - 1, previous_bank.hash());
⋮----
previous_bank.last_blockhash(),
⋮----
bank.process_transaction(&vote).unwrap();
⋮----
bank.last_blockhash(),
⋮----
let system_result = bank.process_transaction(&system_tx);
assert!(system_result.is_ok());
bank.register_unique_recent_blockhash_for_test();
⋮----
fn test_get_rewards_and_partitions() {
⋮----
create_reward_bank(num_rewards, stake_account_stores_per_block, starting_slot);
⋮----
} = epoch_boundary_bank.get_rewards_and_num_partitions();
for (_pubkey, reward) in keyed_rewards.iter() {
assert_eq!(reward.reward_type, RewardType::Voting);
⋮----
assert_eq!(keyed_rewards.len(), num_rewards);
⋮----
} = partition0_bank.get_rewards_and_num_partitions();
⋮----
assert_eq!(reward.reward_type, RewardType::Staking);
⋮----
total_staking_rewards += keyed_rewards.len();
assert_eq!(num_partitions, None);
⋮----
} = partition1_bank.get_rewards_and_num_partitions();
⋮----
assert_eq!(total_staking_rewards, num_rewards);
⋮----
fn test_rewards_and_partitions_should_record() {
⋮----
commission: Some(5),
⋮----
keyed_rewards: vec![],
⋮----
assert!(!rewards_and_partitions.should_record());
⋮----
keyed_rewards: vec![(Pubkey::new_unique(), reward)],
⋮----
assert!(rewards_and_partitions.should_record());
⋮----
num_partitions: Some(42),

================
File: runtime/src/bank/partitioned_epoch_rewards/sysvar.rs
================
impl Bank {
fn log_epoch_rewards_sysvar(&self, prefix: &str) {
if let Some(account) = self.get_account(&sysvar::epoch_rewards::id()) {
⋮----
from_account(&account).unwrap();
info!("{prefix} epoch_rewards sysvar: {epoch_rewards:?}");
⋮----
info!("{prefix} epoch_rewards sysvar: none");
⋮----
pub(in crate::bank) fn create_epoch_rewards_sysvar(
⋮----
assert!(point_value.rewards >= distributed_rewards);
let parent_blockhash = self.last_blockhash();
⋮----
self.update_sysvar_account(&sysvar::epoch_rewards::id(), |account| {
create_account(
⋮----
self.inherit_specially_retained_account_fields(account),
⋮----
self.log_epoch_rewards_sysvar("create");
⋮----
pub(in crate::bank::partitioned_epoch_rewards) fn update_epoch_rewards_sysvar(
⋮----
let mut epoch_rewards = self.get_epoch_rewards_sysvar();
assert!(epoch_rewards.active);
epoch_rewards.distribute(distributed);
⋮----
self.log_epoch_rewards_sysvar("update");
⋮----
pub(in crate::bank::partitioned_epoch_rewards) fn set_epoch_rewards_sysvar_to_inactive(&self) {
⋮----
assert!(epoch_rewards.total_rewards >= epoch_rewards.distributed_rewards);
⋮----
self.log_epoch_rewards_sysvar("set_inactive");
⋮----
pub(in crate::bank::partitioned_epoch_rewards) fn get_epoch_rewards_sysvar(
⋮----
from_account(
⋮----
.get_account(&sysvar::epoch_rewards::id())
.unwrap_or_default(),
⋮----
.unwrap_or_default()
⋮----
mod tests {
⋮----
fn test_epoch_rewards_sysvar() {
⋮----
create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
parent_blockhash: bank.last_blockhash(),
⋮----
let epoch_rewards = bank.get_epoch_rewards_sysvar();
assert_eq!(
⋮----
bank.create_epoch_rewards_sysvar(10, 42, num_partitions, &point_value);
let account = bank.get_account(&sysvar::epoch_rewards::id()).unwrap();
let expected_balance = bank.get_minimum_balance_for_rent_exemption(account.data().len());
assert_eq!(account.lamports(), expected_balance);
let epoch_rewards: sysvar::epoch_rewards::EpochRewards = from_account(&account).unwrap();
assert_eq!(epoch_rewards, expected_epoch_rewards);
let parent_blockhash = bank.last_blockhash();
let parent_slot = bank.slot();
⋮----
bank.update_epoch_rewards_sysvar(10);

================
File: runtime/src/bank/accounts_lt_hash.rs
================
impl Bank {
pub fn update_accounts_lt_hash(&self) {
let delta_lt_hash = self.calculate_delta_lt_hash();
let mut accounts_lt_hash = self.accounts_lt_hash.lock().unwrap();
accounts_lt_hash.0.mix_in(&delta_lt_hash);
⋮----
fn calculate_delta_lt_hash(&self) -> LtHash {
⋮----
let slot = self.slot();
// If we don't find the account in the cache, we need to go load it.
// We want the version of the account *before* it was written in this slot.
// Bank::ancestors *includes* this slot, so we need to remove it before loading.
⋮----
let mut ancestors = self.ancestors.clone();
ancestors.remove(&self.slot());
⋮----
// Slot 0 is special when calculating the accounts lt hash.
// Primordial accounts (those in genesis) that are modified by transaction processing
// in slot 0 will have Alive entries in the accounts lt hash cache.
// When calculating the accounts lt hash, if an account was initially alive, we mix
// *out* its previous lt hash value.  In slot 0, we haven't stored any previous lt hash
// values (since it is in the first slot), yet we'd still mix out these accounts!
// This produces the incorrect accounts lt hash.
// From the perspective of the accounts lt hash, in slot 0 we cannot have any accounts
// as previously alive.  So to work around this issue, we clear the cache.
// And since `strictly_ancestors` is empty, loading the previous version of the account
// from accounts db will return `None` (aka Dead), which is the correct behavior.
assert!(strictly_ancestors.is_empty());
self.cache_for_accounts_lt_hash.clear();
⋮----
// Get all the accounts stored in this slot.
// Since this bank is in the middle of being frozen, it hasn't been rooted.
// That means the accounts should all be in the write cache, and loading will be fast.
let (accounts_curr, time_loading_accounts_curr) = meas_dur!({
⋮----
let num_accounts_total = accounts_curr.len();
⋮----
struct Stats {
⋮----
impl AddAssign for Stats {
fn add_assign(&mut self, other: Self) {
⋮----
// Work on chunks of 128 pubkeys, which is 4 KiB.
// And 4 KiB is likely the smallest a real page size will be.
// And a single page is likely the smallest size a disk read will actually read.
// This can be tuned larger, but likely not smaller.
⋮----
.par_iter()
.fold_chunks(
⋮----
// load the initial state of the account
let (initial_state_of_account, measure_load) = meas_dur!({
⋮----
// If the initial state of the account is not in the accounts
// lt hash cache, or is explicitly unknown, then it is likely
// this account was stored *outside* of transaction processing
// (e.g. creating a new bank).
// Do not populate the read cache, as this account likely will
// not be accessed again soon.
⋮----
// mix out the previous version of the account
⋮----
// nothing to do here
⋮----
meas_dur!(accounts_equal(curr_account, &prev_account));
⋮----
// this account didn't actually change, so skip it for lt hashing
⋮----
meas_dur!(AccountsDb::lt_hash_account(&prev_account, pubkey));
⋮----
meas_dur!(accum.0.mix_out(&prev_lt_hash.0));
⋮----
// mix in the new version of the account
⋮----
meas_dur!(AccountsDb::lt_hash_account(curr_account, pubkey));
let (_, measure_mixing) = meas_dur!(accum.0.mix_in(&curr_lt_hash.0));
⋮----
.reduce(
⋮----
accum.0.mix_in(&elem.0);
⋮----
.install(do_calculate_delta_lt_hash);
let total_time = measure_total.end_as_duration();
⋮----
num_accounts_total.saturating_sub(stats.num_accounts_unmodified);
datapoint_info!(
⋮----
pub fn inspect_account_for_accounts_lt_hash(
⋮----
meas_dur!(self.cache_for_accounts_lt_hash.contains_key(address));
⋮----
let freeze_guard = self.freeze_lock();
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
let (_, insert_time) = meas_dur!({
⋮----
drop(freeze_guard);
⋮----
.fetch_add(insert_time.as_nanos() as u64, Ordering::Relaxed);
⋮----
.fetch_add(lookup_time.as_nanos() as u64, Ordering::Relaxed);
⋮----
pub struct Stats {
⋮----
pub enum InitialStateOfAccount {
⋮----
pub enum CacheValue {
⋮----
mod tests {
⋮----
enum Features {
⋮----
fn genesis_config_with(features: Features) -> (GenesisConfig, Keypair) {
⋮----
fn test_update_accounts_lt_hash() {
⋮----
bank.get_minimum_balance_for_rent_exemption(0),
⋮----
bank.register_unique_recent_blockhash_for_test();
bank.transfer(amount, &mint_keypair, &keypair1.pubkey())
.unwrap();
bank.transfer(amount, &mint_keypair, &keypair2.pubkey())
⋮----
bank.transfer(amount, &mint_keypair, &keypair5.pubkey())
⋮----
bank.freeze();
let prev_accounts_lt_hash = bank.accounts_lt_hash.lock().unwrap().clone();
let prev_mint = bank.get_account_with_fixed_root(&mint_keypair.pubkey());
let prev_account1 = bank.get_account_with_fixed_root(&keypair1.pubkey());
let prev_account2 = bank.get_account_with_fixed_root(&keypair2.pubkey());
let prev_account3 = bank.get_account_with_fixed_root(&keypair3.pubkey());
let prev_account4 = bank.get_account_with_fixed_root(&keypair4.pubkey());
let prev_account5 = bank.get_account_with_fixed_root(&keypair5.pubkey());
assert!(prev_mint.is_some());
assert!(prev_account1.is_some());
assert!(prev_account2.is_some());
assert!(prev_account3.is_none());
assert!(prev_account4.is_none());
assert!(prev_account5.is_some());
⋮----
Pubkey::from_str("SysvarS1otHashes111111111111111111111111111").unwrap(),
Pubkey::from_str("SysvarC1ock11111111111111111111111111111111").unwrap(),
Pubkey::from_str("SysvarRecentB1ockHashes11111111111111111111").unwrap(),
Pubkey::from_str("SysvarS1otHistory11111111111111111111111111").unwrap(),
⋮----
.iter()
.map(|address| bank.get_account_with_fixed_root(address))
.collect();
⋮----
let slot = bank.slot() + 1;
⋮----
bank.transfer(amount, &keypair2, &keypair1.pubkey())
⋮----
bank.transfer(amount, &mint_keypair, &keypair4.pubkey())
⋮----
bank.transfer(amount, &keypair4, &keypair3.pubkey())
⋮----
bank.store_account(&keypair5.pubkey(), prev_account5.as_ref().unwrap());
⋮----
let actual_delta_lt_hash = bank.calculate_delta_lt_hash();
let post_accounts_lt_hash = bank.accounts_lt_hash.lock().unwrap().clone();
let post_mint = bank.get_account_with_fixed_root(&mint_keypair.pubkey());
let post_account1 = bank.get_account_with_fixed_root(&keypair1.pubkey());
let post_account2 = bank.get_account_with_fixed_root(&keypair2.pubkey());
let post_account3 = bank.get_account_with_fixed_root(&keypair3.pubkey());
let post_account4 = bank.get_account_with_fixed_root(&keypair4.pubkey());
let post_account5 = bank.get_account_with_fixed_root(&keypair5.pubkey());
assert!(post_mint.is_some());
assert!(post_account1.is_some());
assert!(post_account2.is_none());
assert!(post_account3.is_some());
assert!(post_account4.is_none());
assert!(post_account5.is_some());
⋮----
let mut expected_accounts_lt_hash = prev_accounts_lt_hash.clone();
⋮----
expected_delta_lt_hash.mix_out(&prev_lt_hash.0);
expected_accounts_lt_hash.0.mix_out(&prev_lt_hash.0);
⋮----
let post = post.unwrap_or_default();
⋮----
expected_delta_lt_hash.mix_in(&post_lt_hash.0);
expected_accounts_lt_hash.0.mix_in(&post_lt_hash.0);
⋮----
updater(&mint_keypair.pubkey(), prev_mint, post_mint);
updater(&keypair1.pubkey(), prev_account1, post_account1);
updater(&keypair2.pubkey(), prev_account2, post_account2);
updater(&keypair3.pubkey(), prev_account3, post_account3);
updater(&keypair4.pubkey(), prev_account4, post_account4);
updater(&keypair5.pubkey(), prev_account5, post_account5);
for (i, sysvar) in sysvars.iter().enumerate() {
updater(
⋮----
prev_sysvar_accounts[i].clone(),
post_sysvar_accounts[i].clone(),
⋮----
let expected = expected_delta_lt_hash.checksum();
let actual = actual_delta_lt_hash.checksum();
assert_eq!(
⋮----
let expected = expected_accounts_lt_hash.0.checksum();
let actual = post_accounts_lt_hash.0.checksum();
⋮----
fn test_slot0_accounts_lt_hash(features: Features) {
let (genesis_config, mint_keypair) = genesis_config_with(features);
⋮----
assert_eq!(bank.slot(), 0);
bank.transfer(LAMPORTS_PER_SOL, &mint_keypair, &Pubkey::new_unique())
⋮----
let actual_accounts_lt_hash = bank.accounts_lt_hash.lock().unwrap().clone();
⋮----
.calculate_accounts_lt_hash_at_startup_from_index(&bank.ancestors, bank.slot());
assert_eq!(actual_accounts_lt_hash, calculated_accounts_lt_hash);
⋮----
fn test_inspect_account_for_accounts_lt_hash(features: Features) {
let (genesis_config, _mint_keypair) = genesis_config_with(features);
⋮----
assert_eq!(bank.cache_for_accounts_lt_hash.len(), 0);
bank.inspect_account_for_accounts_lt_hash(
⋮----
bank.inspect_account_for_accounts_lt_hash(&address, &AccountState::Dead, true);
assert_eq!(bank.cache_for_accounts_lt_hash.len(), 1);
assert!(bank.cache_for_accounts_lt_hash.contains_key(&address));
⋮----
bank.inspect_account_for_accounts_lt_hash(&address, &AccountState::Alive(&account), true);
assert_eq!(bank.cache_for_accounts_lt_hash.len(), 2);
⋮----
.get(&address)
.unwrap()
.value()
⋮----
assert_eq!(*cached_account, account);
⋮----
panic!("wrong initial state for account");
⋮----
let updated_lamports = account.lamports() + 1;
account.set_lamports(updated_lamports);
⋮----
assert_eq!(cached_account.lamports(), initial_lamports);
⋮----
assert_eq!(bank.cache_for_accounts_lt_hash.len(), 3);
⋮----
_ => panic!("wrong initial state for account"),
⋮----
let num_cache_entries_prev = bank.cache_for_accounts_lt_hash.len();
⋮----
let num_cache_entries_curr = bank.cache_for_accounts_lt_hash.len();
assert_eq!(num_cache_entries_curr, num_cache_entries_prev);
assert!(!bank.cache_for_accounts_lt_hash.contains_key(&address));
⋮----
fn test_calculate_accounts_lt_hash_at_startup_from_index(features: Features) {
⋮----
bank.transfer(amount, &mint_keypair, &pubkey::new_rand())
⋮----
let expected_accounts_lt_hash = bank.accounts_lt_hash.lock().unwrap().clone();
bank.squash();
bank.force_flush_accounts_cache();
⋮----
assert_eq!(expected_accounts_lt_hash, calculated_accounts_lt_hash);
⋮----
fn test_verify_accounts_lt_hash_at_startup(
⋮----
let (mut genesis_config, mint_keypair) = genesis_config_with(features);
⋮----
bank.transfer(amount, &mint_keypair, &duplicate_pubkey)
⋮----
bank.fill_bank_with_ticks_for_tests();
⋮----
let accounts: Vec<_> = iter::repeat_with(Keypair::new).take(num_accounts).collect();
⋮----
bank.transfer(amount, &mint_keypair, &account.pubkey())
⋮----
assert_ne!(bank.get_balance(&account.pubkey()), 0);
⋮----
bank.transfer(
bank.get_balance(&accounts[i].pubkey()),
⋮----
assert_eq!(bank.get_balance(&accounts[i].pubkey()), 0);
⋮----
let bank_snapshots_dir = TempDir::new().unwrap();
let snapshot_archives_dir = TempDir::new().unwrap();
⋮----
Some(snapshot_config.snapshot_version),
⋮----
index: Some(accounts_index_config),
⋮----
assert!(roundtrip_bank.is_frozen());
assert_eq!(roundtrip_bank, *bank);
⋮----
fn test_accounts_lt_hash_cache_values_from_bank_new(features: Features) {
⋮----
.map(|entry| (*entry.key(), entry.value().clone()))
⋮----
actual_cache.sort_unstable_by(|a, b| a.0.cmp(&b.0));
assert_eq!(expected_cache, actual_cache.as_slice());
⋮----
fn test_snapshots(features: Features) {

================
File: runtime/src/bank/address_lookup_table.rs
================
fn into_address_loader_error(err: AddressLookupError) -> AddressLoaderError {
⋮----
impl AddressLoader for &Bank {
fn load_addresses(
⋮----
self.load_addresses_from_ref(
⋮----
.iter()
.map(SVMMessageAddressTableLookup::from),
⋮----
.map(|(loaded_addresses, _deactivation_slot)| loaded_addresses)
⋮----
impl Bank {
pub fn load_addresses_from_ref<'a>(
⋮----
.sysvar_cache()
.get_slot_hashes()
.map_err(|_| AddressLoaderError::SlotHashesSysvarNotFound)?;
⋮----
deactivation_slot = deactivation_slot.min(
⋮----
.load_lookup_table_addresses_into(
⋮----
.map_err(into_address_loader_error)?,
⋮----
Ok((loaded_addresses, deactivation_slot))

================
File: runtime/src/bank/bank_hash_details.rs
================
pub struct BankHashDetails {
⋮----
impl BankHashDetails {
pub fn new(bank_hash_details: Vec<SlotDetails>) -> Self {
⋮----
version: solana_version::version!().to_string(),
account_data_encoding: "base64".to_string(),
⋮----
pub fn filename(&self) -> Result<String, String> {
if self.bank_hash_details.is_empty() {
return Err("BankHashDetails does not contains details for any banks".to_string());
⋮----
let details = self.bank_hash_details.first().unwrap();
⋮----
let filename = if self.bank_hash_details.len() == 1 {
format!("{first_slot}-{first_hash}.json")
⋮----
let details = self.bank_hash_details.last().unwrap();
⋮----
format!("{first_slot}-{first_hash}_{last_slot}-{last_hash}.json")
⋮----
Ok(filename)
⋮----
pub struct TransactionDetails {
⋮----
pub struct TransactionCommitDetails {
⋮----
fn from(committed_tx: CommittedTransaction) -> Self {
⋮----
pub struct SlotDetails {
⋮----
pub struct BankHashComponents {
⋮----
impl SlotDetails {
pub fn new_from_bank(bank: &Bank, include_bank_hash_components: bool) -> Result<Self, String> {
let slot = bank.slot();
if !bank.is_frozen() {
return Err(format!(
⋮----
let accounts = bank.get_accounts_for_bank_hash_details();
Some(BankHashComponents {
parent_bank_hash: bank.parent_hash().to_string(),
signature_count: bank.signature_count(),
last_blockhash: bank.last_blockhash().to_string(),
⋮----
.lock()
.unwrap()
⋮----
.checksum()
.to_string(),
⋮----
Ok(Self {
⋮----
bank_hash: bank.hash().to_string(),
⋮----
pub struct AccountsDetails {
⋮----
struct SerdeAccount {
⋮----
fn from(pubkey_account: &(Pubkey, AccountSharedData)) -> Self {
⋮----
pubkey: pubkey.to_string(),
owner: account.owner().to_string(),
lamports: account.lamports(),
executable: account.executable(),
data: BASE64_STANDARD.encode(account.data()),
⋮----
type Error = String;
fn try_from(temp_account: SerdeAccount) -> Result<Self, Self::Error> {
let pubkey = Pubkey::from_str(&temp_account.pubkey).map_err(|err| err.to_string())?;
⋮----
.decode(temp_account.data)
.map_err(|err| err.to_string())?,
owner: Pubkey::from_str(&temp_account.owner).map_err(|err| err.to_string())?,
⋮----
Ok((pubkey, account))
⋮----
impl Serialize for AccountsDetails {
fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
⋮----
let mut seq = serializer.serialize_seq(Some(self.accounts.len()))?;
for account in self.accounts.iter() {
⋮----
seq.serialize_element(&temp_account)?;
⋮----
seq.end()
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
type PubkeyAccount = (Pubkey, AccountSharedData);
⋮----
.into_iter()
.map(PubkeyAccount::try_from)
.collect();
let pubkey_accounts = pubkey_accounts.map_err(de::Error::custom)?;
Ok(AccountsDetails {
⋮----
pub fn write_bank_hash_details_file(bank: &Bank) -> std::result::Result<(), String> {
⋮----
let details = BankHashDetails::new(vec![slot_details]);
⋮----
.get_base_working_path()
.join("bank_hash_details");
let path = parent_dir.join(details.filename()?);
if !path.exists() {
info!("writing bank hash details file: {}", path.display());
⋮----
.map_err(|err| format!("Unable to create file at {}: {err}", path.display()))?;
⋮----
.map_err(|err| format!("Unable to write file at {}: {err}", path.display()))?;
⋮----
Ok(())
⋮----
pub mod tests {
⋮----
fn build_details(num_slots: usize) -> BankHashDetails {
⋮----
.map(|slot| {
⋮----
data: vec![0, 9, 1, 8, 2, 7, 3, 6, 4, 5],
⋮----
accounts: vec![(account_pubkey, account)],
⋮----
bank_hash: format!("bank{slot}"),
bank_hash_components: Some(BankHashComponents {
parent_bank_hash: "parent_bank_hash".into(),
⋮----
last_blockhash: "last_blockhash".into(),
accounts_lt_hash_checksum: "accounts_lt_hash_checksum".into(),
⋮----
transactions: vec![],
⋮----
fn test_serde_bank_hash_details() {
⋮----
let bank_hash_details = build_details(num_slots);
let serialized_bytes = serde_json::to_vec(&bank_hash_details).unwrap();
⋮----
serde_json::from_slice(&serialized_bytes).unwrap();
assert_eq!(bank_hash_details, deserialized_bank_hash_details);

================
File: runtime/src/bank/check_transactions.rs
================
impl Bank {
pub fn check_transactions_with_forwarding_delay(
⋮----
self.check_transactions(
⋮----
.saturating_sub(max_tx_fwd_delay)
.saturating_sub(forward_transactions_to_leader_at_slot_offset as usize),
⋮----
pub fn check_transactions<Tx: TransactionWithMeta>(
⋮----
self.check_transactions_with_processed_slots(
⋮----
pub fn check_transactions_with_processed_slots<Tx: TransactionWithMeta>(
⋮----
let lock_results = self.check_age_and_compute_budget_limits(
⋮----
self.check_status_cache(
⋮----
fn check_age_and_compute_budget_limits<Tx: TransactionWithMeta>(
⋮----
let hash_queue = self.blockhash_queue.read().unwrap();
let last_blockhash = hash_queue.last_hash();
⋮----
.get_lamports_per_signature(&last_blockhash)
.unwrap();
⋮----
let raise_cpi_limit = feature_set.is_active(&raise_cpi_nesting_limit_to_8::id());
⋮----
.iter()
.zip(lock_results)
.map(|(tx, lock_res)| match lock_res {
⋮----
.borrow()
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(feature_set)
.map(|limit| {
⋮----
let fee_details = calculate_fee_details(
tx.borrow(),
⋮----
compute_budget.get_compute_budget_and_limits(
⋮----
limit.get_compute_budget_and_limits(
⋮----
.inspect_err(|_err| {
⋮----
self.check_transaction_age(
⋮----
Err(e) => Err(e.clone()),
⋮----
.collect()
⋮----
fn checked_transactions_details_with_test_override(
⋮----
fn check_transaction_age(
⋮----
let recent_blockhash = tx.recent_blockhash();
if let Some(hash_info) = hash_queue.get_hash_info_if_valid(recent_blockhash, max_age) {
Ok(Self::checked_transactions_details_with_test_override(
⋮----
hash_info.lamports_per_signature(),
⋮----
.check_load_and_advance_message_nonce_account(
⋮----
Some(nonce),
⋮----
Err(TransactionError::BlockhashNotFound)
⋮----
pub(super) fn check_load_and_advance_message_nonce_account(
⋮----
let nonce_is_advanceable = message.recent_blockhash() != next_durable_nonce.as_hash();
⋮----
self.load_message_nonce_account(message)?;
let previous_lamports_per_signature = nonce_data.get_lamports_per_signature();
⋮----
.set_state(&NonceVersions::new(next_nonce_state))
.ok()?;
Some((
⋮----
pub(super) fn load_message_nonce_account(
⋮----
.is_active(&agave_feature_set::require_static_nonce_account::id());
let nonce_address = message.get_durable_nonce(require_static_nonce_account)?;
let nonce_account = self.get_account_with_fixed_root(nonce_address)?;
⋮----
nonce_account::verify_nonce_account(&nonce_account, message.recent_blockhash())?;
⋮----
.get_ix_signers(NONCED_TX_MARKER_IX_INDEX as usize)
.any(|signer| signer == &nonce_data.authority);
⋮----
Some((*nonce_address, nonce_account, nonce_data))
⋮----
fn check_status_cache<Tx: TransactionWithMeta>(
⋮----
let mut check_results = Vec::with_capacity(sanitized_txs.len());
⋮----
Some(Vec::with_capacity(sanitized_txs.len()))
⋮----
let rcache = self.status_cache.read().unwrap();
for (sanitized_tx_ref, lock_result) in sanitized_txs.iter().zip(lock_results) {
let sanitized_tx = sanitized_tx_ref.borrow();
let (result, processed_slot) = if lock_result.is_ok() {
if let Some(slot) = self.get_processed_slot(sanitized_tx, &rcache) {
⋮----
(Err(TransactionError::AlreadyProcessed), Some(slot))
⋮----
check_results.push(result);
if let Some(processed_slots) = processed_slots.as_mut() {
processed_slots.push(processed_slot)
⋮----
fn get_processed_slot(
⋮----
let key = sanitized_tx.message_hash();
let transaction_blockhash = sanitized_tx.recent_blockhash();
⋮----
.get_status(key, transaction_blockhash, &self.ancestors)
.map(|status| status.0)
⋮----
mod tests {
⋮----
fn test_check_and_load_message_nonce_account_ok() {
⋮----
let (bank, _mint_keypair, custodian_keypair, nonce_keypair, _) = setup_nonce_with_bank(
⋮----
let custodian_pubkey = custodian_keypair.pubkey();
let nonce_pubkey = nonce_keypair.pubkey();
let nonce_hash = get_nonce_blockhash(&bank, &nonce_pubkey).unwrap();
let message = new_sanitized_message(Message::new_with_blockhash(
⋮----
Some(&custodian_pubkey),
⋮----
let mut nonce_account = bank.get_account(&nonce_pubkey).unwrap();
let nonce_data = get_nonce_data_from_account(&nonce_account).unwrap();
⋮----
.set_state(&NonceVersions::new(NonceState::new_initialized(
⋮----
bank.store_account(&nonce_pubkey, &nonce_account);
let nonce_account = bank.get_account(&nonce_pubkey).unwrap();
let (_, next_lamports_per_signature) = bank.last_blockhash_and_lamports_per_signature();
⋮----
.try_advance_nonce(bank.next_durable_nonce(), next_lamports_per_signature)
⋮----
assert_eq!(
⋮----
fn test_check_and_load_message_nonce_account_not_nonce_fail() {
⋮----
let (_, lamports_per_signature) = bank.last_blockhash_and_lamports_per_signature();
assert!(bank
⋮----
fn test_check_and_load_message_nonce_account_missing_ix_pubkey_fail() {
⋮----
message.instructions[0].accounts.clear();
⋮----
fn test_check_and_load_message_nonce_account_nonce_acc_does_not_exist_fail() {
⋮----
let missing_pubkey = missing_keypair.pubkey();
⋮----
fn test_check_and_load_message_nonce_account_bad_tx_hash_fail() {
⋮----
fn test_check_and_load_message_nonce_account_nonce_is_alt(require_static_nonce_account: bool) {
⋮----
let (bank, _mint_keypair, _custodian_keypair, nonce_keypair, _) = setup_nonce_with_bank(
⋮----
Some(nonce_authority),
⋮----
writable: vec![nonce_pubkey],
readonly: vec![],
⋮----
account_keys: vec![nonce_authority, system_program::id()],
⋮----
instructions: vec![CompiledInstruction::new(
⋮----
address_table_lookups: vec![MessageAddressTableLookup {
⋮----
.unwrap(),

================
File: runtime/src/bank/fee_distribution.rs
================
enum DepositFeeError {
⋮----
pub struct FeeDistribution {
⋮----
impl FeeDistribution {
pub fn get_deposit(&self) -> u64 {
⋮----
impl Bank {
pub(super) fn distribute_transaction_fee_details(&self) {
let fee_details = self.collector_fee_details.read().unwrap();
if fee_details.total_transaction_fee() == 0 {
⋮----
self.calculate_reward_and_burn_fee_details(&fee_details);
let total_burn = self.deposit_or_burn_fee(deposit).saturating_add(burn);
self.capitalization.fetch_sub(total_burn, Relaxed);
⋮----
pub fn calculate_reward_for_transaction(
⋮----
self.last_blockhash_and_lamports_per_signature();
⋮----
self.fee_structure().lamports_per_signature,
⋮----
FeeFeatures::from(self.feature_set.as_ref()),
⋮----
} = self.calculate_reward_and_burn_fee_details(&CollectorFeeDetails::from(fee_details));
⋮----
pub fn calculate_reward_and_burn_fee_details(
⋮----
let burn = fee_details.transaction_fee * self.burn_percent() / 100;
⋮----
.saturating_add(fee_details.transaction_fee.saturating_sub(burn));
⋮----
const fn burn_percent(&self) -> u64 {
⋮----
fn deposit_or_burn_fee(&self, deposit: u64) -> u64 {
⋮----
match self.deposit_fees(&self.collector_id, deposit) {
⋮----
self.rewards.write().unwrap().push((
⋮----
debug!(
⋮----
datapoint_warn!(
⋮----
fn deposit_fees(&self, pubkey: &Pubkey, fees: u64) -> Result<u64, DepositFeeError> {
⋮----
.get_account_with_fixed_root_no_cache(pubkey)
.unwrap_or_default();
if !system_program::check_id(account.owner()) {
return Err(DepositFeeError::InvalidAccountOwner);
⋮----
let recipient_pre_rent_state = get_account_rent_state(
&self.rent_collector().rent,
account.lamports(),
account.data().len(),
⋮----
let distribution = account.checked_add_lamports(fees);
if distribution.is_err() {
return Err(DepositFeeError::LamportOverflow);
⋮----
let recipient_post_rent_state = get_account_rent_state(
⋮----
transition_allowed(&recipient_pre_rent_state, &recipient_post_rent_state);
⋮----
return Err(DepositFeeError::InvalidRentPayingAccount);
⋮----
self.store_account(pubkey, &account);
Ok(account.lamports())
⋮----
pub mod tests {
⋮----
fn test_deposit_or_burn_zero_fee() {
let genesis = create_genesis_config(0);
⋮----
assert_eq!(bank.deposit_or_burn_fee(0), 0);
⋮----
fn test_deposit_or_burn_fee() {
⋮----
enum Scenario {
⋮----
struct TestCase {
⋮----
impl TestCase {
fn new(scenario: Scenario) -> Self {
⋮----
let mut genesis = create_genesis_config(0);
⋮----
let min_rent_exempt_balance = rent.minimum_balance(0);
⋮----
bank.store_account(bank.collector_id(), &account);
assert!(initial_balance + deposit < min_rent_exempt_balance);
⋮----
let initial_collector_id_balance = bank.get_balance(bank.collector_id());
burn += bank.deposit_or_burn_fee(deposit);
let new_collector_id_balance = bank.get_balance(bank.collector_id());
⋮----
assert_eq!(initial_collector_id_balance, new_collector_id_balance);
assert_eq!(initial_burn + deposit, burn);
let locked_rewards = bank.rewards.read().unwrap();
assert!(
⋮----
assert_eq!(
⋮----
assert_eq!(initial_burn, burn);
⋮----
fn test_deposit_fees() {
⋮----
let genesis = create_genesis_config(initial_balance);
⋮----
let pubkey = genesis.mint_keypair.pubkey();
⋮----
fn test_deposit_fees_with_overflow() {
⋮----
fn test_deposit_fees_invalid_account_owner() {
⋮----
let genesis = create_genesis_config_with_leader(0, &pubkey::new_rand(), initial_balance);
⋮----
let pubkey = genesis.voting_keypair.pubkey();
⋮----
fn test_deposit_fees_invalid_rent_paying() {
⋮----
let min_rent_exempt_balance = genesis_config.rent.minimum_balance(0);
⋮----
assert!(initial_balance + deposit_amount < min_rent_exempt_balance);
⋮----
fn test_distribute_transaction_fee_details_normal() {
⋮----
let expected_burn = transaction_fee * bank.burn_percent() / 100;
⋮----
let initial_capitalization = bank.capitalization();
⋮----
bank.distribute_transaction_fee_details();
⋮----
fn test_distribute_transaction_fee_details_zero() {
⋮----
assert_eq!(initial_capitalization, bank.capitalization());
⋮----
fn test_distribute_transaction_fee_details_overflow_failure() {

================
File: runtime/src/bank/metrics.rs
================
pub(crate) struct NewEpochTimings {
⋮----
pub(crate) struct RewardsMetrics {
⋮----
pub(crate) struct NewBankTimings {
⋮----
pub(crate) fn report_new_epoch_metrics(
⋮----
datapoint_info!(
⋮----
pub(crate) fn report_new_bank_metrics(
⋮----
pub(crate) struct RewardsStoreMetrics {
⋮----
pub(crate) fn report_partitioned_reward_metrics(bank: &Bank, timings: RewardsStoreMetrics) {
⋮----
pub(crate) fn report_loaded_programs_stats(stats: &ProgramCacheStats, slot: Slot) {
let hits = stats.hits.load(Ordering::Relaxed);
let misses = stats.misses.load(Ordering::Relaxed);
let evictions: u64 = stats.evictions.values().sum();
let reloads = stats.reloads.load(Ordering::Relaxed);
let insertions = stats.insertions.load(Ordering::Relaxed);
let lost_insertions = stats.lost_insertions.load(Ordering::Relaxed);
let replacements = stats.replacements.load(Ordering::Relaxed);
let one_hit_wonders = stats.one_hit_wonders.load(Ordering::Relaxed);
let prunes_orphan = stats.prunes_orphan.load(Ordering::Relaxed);
let prunes_environment = stats.prunes_environment.load(Ordering::Relaxed);
let empty_entries = stats.empty_entries.load(Ordering::Relaxed);
let water_level = stats.water_level.load(Ordering::Relaxed);
⋮----
stats.log();

================
File: runtime/src/bank/recent_blockhashes_account.rs
================
fn update_account<'a, I>(account: &mut AccountSharedData, recent_blockhash_iter: I) -> Option<()>
⋮----
let recent_blockhash_iter = sorted_iter.take(MAX_ENTRIES);
⋮----
let recent_blockhashes: RecentBlockhashes = recent_blockhash_iter.collect();
to_account(&recent_blockhashes, account)
⋮----
pub(in crate::bank) fn create_account_with_data_and_fields<'a, I>(
⋮----
update_account(&mut account, recent_blockhash_iter).unwrap();
⋮----
mod tests {
⋮----
fn create_account_with_data_for_test<'a, I>(recent_blockhash_iter: I) -> AccountSharedData
⋮----
create_account_with_data_and_fields(recent_blockhash_iter, DUMMY_INHERITABLE_ACCOUNT_FIELDS)
⋮----
fn test_create_account_empty() {
let account = create_account_with_data_for_test(vec![]);
let recent_blockhashes = from_account::<RecentBlockhashes, _>(&account).unwrap();
assert_eq!(recent_blockhashes, RecentBlockhashes::default());
⋮----
fn test_create_account_full() {
⋮----
let account = create_account_with_data_for_test(vec![
⋮----
assert_eq!(recent_blockhashes.len(), MAX_ENTRIES);
⋮----
fn test_create_account_truncate() {
⋮----
fn test_create_account_unsorted() {
⋮----
.map(|i| {
⋮----
.collect();
unsorted_blocks.shuffle(&mut rng());
let account = create_account_with_data_for_test(
⋮----
.iter()
.map(|(i, hash)| IterItem(*i, hash, def_lamports_per_signature)),
⋮----
.map(|(i, hash)| IterItem(*i, hash, def_lamports_per_signature))
⋮----
unsorted_recent_blockhashes.sort();
unsorted_recent_blockhashes.reverse();
⋮----
.into_iter()
.map(|IterItem(_, b, f)| Entry::new(b, f)))
⋮----
assert_eq!(*recent_blockhashes, expected_recent_blockhashes);

================
File: runtime/src/bank/serde_snapshot.rs
================
mod tests {
⋮----
fn copy_append_vecs<P: AsRef<Path>>(
⋮----
let storage_entries = accounts_db.get_storages(RangeFull).0;
let storage: AccountStorageMap = AccountStorageMap::with_capacity(storage_entries.len());
⋮----
for storage_entry in storage_entries.into_iter() {
let storage_path = storage_entry.path();
let file_name = AccountsFile::file_name(storage_entry.slot(), storage_entry.id());
let output_path = output_dir.as_ref().join(file_name);
⋮----
storage_entry.accounts.len(),
⋮----
storage_entry.slot(),
storage_entry.id(),
⋮----
next_append_vec_id = next_append_vec_id.max(new_storage_entry.id());
storage.insert(new_storage_entry.slot(), Arc::new(new_storage_entry));
⋮----
Ok(StorageAndNextAccountsFileId {
⋮----
fn test_serialize_bank_snapshot(storage_access: StorageAccess) {
let (mut genesis_config, _) = create_genesis_config(500);
⋮----
let deposit_amount = bank0.get_minimum_balance_for_rent_exemption(0);
let bank1 = Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 1);
⋮----
bank_test_utils::deposit(&bank1, &key1, deposit_amount).unwrap();
⋮----
bank_test_utils::deposit(&bank2, &key2, deposit_amount).unwrap();
assert_eq!(bank2.get_balance(&key2), deposit_amount);
⋮----
bank_test_utils::deposit(&bank2, &key3, 0).unwrap();
⋮----
bank2.squash();
bank2.force_flush_accounts_cache();
let expected_accounts_lt_hash = bank2.accounts_lt_hash.lock().unwrap().clone();
⋮----
let mut bank_fields = bank2.get_fields_to_serialize();
⋮----
let accounts_lt_hash = Some(bank_fields.accounts_lt_hash.clone().into());
⋮----
bank2.get_bank_hash_stats(),
&get_storages_to_serialize(&bank2.get_snapshot_storages(None)),
⋮----
accounts_db.write_version.load(Ordering::Acquire),
⋮----
.unwrap();
⋮----
drop(writer);
let (_accounts_dir, dbank_paths) = get_temp_accounts_paths(4).unwrap();
let copied_accounts = TempDir::new().unwrap();
⋮----
copy_append_vecs(accounts_db, copied_accounts.path(), storage_access).unwrap();
let cursor = Cursor::new(buf.as_slice());
⋮----
assert_eq!(dbank.get_balance(&key1), 0);
assert_eq!(dbank.get_balance(&key2), deposit_amount);
assert_eq!(dbank.get_balance(&key3), 0);
assert_eq!(
⋮----
assert_eq!(dbank.get_bank_hash_stats(), bank2.get_bank_hash_stats());
assert_eq!(dbank, bank2);
⋮----
fn add_root_and_flush_write_cache(bank: &Bank) {
bank.rc.accounts.add_root(bank.slot());
bank.flush_accounts_cache_slot_for_tests()
⋮----
fn test_extra_fields_eof(storage_access: StorageAccess) {
⋮----
let (genesis_config, _) = create_genesis_config(500);
⋮----
bank0.squash();
let mut bank = Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 1);
bank.freeze();
add_root_and_flush_write_cache(&bank0);
⋮----
bank.epoch_stakes.insert(
⋮----
assert_eq!(bank.epoch_stakes.len(), 3);
let snapshot_storages = bank.get_snapshot_storages(None);
let mut buf = vec![];
⋮----
&get_storages_to_serialize(&snapshot_storages),
⋮----
let mut reader = std::io::BufReader::new(&buf[rdr.position() as usize..]);
⋮----
let storage_and_next_append_vec_id = copy_append_vecs(
⋮----
copied_accounts.path(),
⋮----
assert_eq!(bank.epoch_stakes, dbank.epoch_stakes);
⋮----
fn test_extra_fields_full_snapshot_archive() {
⋮----
activate_all_features(&mut genesis_config);
⋮----
while !bank.is_complete() {
bank.fill_bank_with_ticks_for_tests();
⋮----
let (_tmp_dir, accounts_dir) = create_tmp_accounts_dir_for_tests();
let bank_snapshots_dir = TempDir::new().unwrap();
let full_snapshot_archives_dir = TempDir::new().unwrap();
let incremental_snapshot_archives_dir = TempDir::new().unwrap();
⋮----
full_snapshot_archives_dir.path(),
incremental_snapshot_archives_dir.path(),
⋮----
bank_snapshots_dir.path(),
⋮----
mod test_bank_serialize {
⋮----
pub struct BankAbiTestWrapper {
⋮----
pub fn wrapper<S>(_bank: &PhantomData<Bank>, serializer: S) -> Result<S::Ok, S::Error>
⋮----
let snapshot_storages = AccountsDb::example().get_storages(0..1).0;
assert!(!snapshot_storages.is_empty());
⋮----
let mut bank_fields = bank.get_fields_to_serialize();
⋮----
obsolete_incremental_snapshot_persistence: Some(
⋮----
obsolete_epoch_accounts_hash: Some(Hash::new_unique()),
⋮----
accounts_lt_hash: Some(AccountsLtHash(LtHash::identity()).into()),

================
File: runtime/src/bank/sysvar_cache.rs
================
use super::Bank;
⋮----
mod tests {
⋮----
fn test_sysvar_cache_initialization() {
let (genesis_config, _mint_keypair) = create_genesis_config(100_000);
⋮----
let bank0_sysvar_cache = bank0.transaction_processor.sysvar_cache();
let bank0_cached_clock = bank0_sysvar_cache.get_clock();
let bank0_cached_epoch_schedule = bank0_sysvar_cache.get_epoch_schedule();
let bank0_cached_rent = bank0_sysvar_cache.get_rent();
assert!(bank0_cached_clock.is_ok());
assert!(bank0_cached_epoch_schedule.is_ok());
assert!(bank0_cached_rent.is_ok());
assert!(bank0_sysvar_cache.get_slot_hashes().is_err());
assert!(bank0_sysvar_cache.get_epoch_rewards().is_err());
let bank1_slot = bank0.slot() + 1;
⋮----
bank0.clone(),
⋮----
let bank1_sysvar_cache = bank1.transaction_processor.sysvar_cache();
let bank1_cached_clock = bank1_sysvar_cache.get_clock();
let bank1_cached_epoch_schedule = bank1_sysvar_cache.get_epoch_schedule();
let bank1_cached_rent = bank1_sysvar_cache.get_rent();
assert!(bank1_cached_clock.is_ok());
assert!(bank1_cached_epoch_schedule.is_ok());
assert!(bank1_cached_rent.is_ok());
assert!(bank1_sysvar_cache.get_slot_hashes().is_ok());
assert!(bank1_sysvar_cache.get_epoch_rewards().is_err());
assert_ne!(bank0_cached_clock, bank1_cached_clock);
assert_eq!(bank0_cached_epoch_schedule, bank1_cached_epoch_schedule);
assert_eq!(bank0_cached_rent, bank1_cached_rent);
let bank2_slot = bank1.slot() + 1;
let bank2 = Bank::new_from_parent(bank1.clone(), &Pubkey::default(), bank2_slot);
let bank2_sysvar_cache = bank2.transaction_processor.sysvar_cache();
let bank2_cached_clock = bank2_sysvar_cache.get_clock();
let bank2_cached_epoch_schedule = bank2_sysvar_cache.get_epoch_schedule();
let bank2_cached_rent = bank2_sysvar_cache.get_rent();
assert!(bank2_cached_clock.is_ok());
assert!(bank2_cached_epoch_schedule.is_ok());
assert!(bank2_cached_rent.is_ok());
assert!(bank2_sysvar_cache.get_slot_hashes().is_ok());
assert!(bank2_sysvar_cache.get_epoch_rewards().is_err());
assert_ne!(bank1_cached_clock, bank2_cached_clock);
assert_eq!(bank1_cached_epoch_schedule, bank2_cached_epoch_schedule);
assert_eq!(bank1_cached_rent, bank2_cached_rent);
assert_ne!(
⋮----
fn test_reset_and_fill_sysvar_cache() {
⋮----
let bank1_cached_fees = bank1_sysvar_cache.get_fees();
⋮----
let bank1_cached_slot_hashes = bank1_sysvar_cache.get_slot_hashes();
let bank1_cached_epoch_rewards = bank1_sysvar_cache.get_epoch_rewards();
⋮----
assert!(bank1_cached_slot_hashes.is_ok());
assert!(bank1_cached_epoch_rewards.is_err());
drop(bank1_sysvar_cache);
bank1.transaction_processor.reset_sysvar_cache();
⋮----
assert!(bank1_sysvar_cache.get_clock().is_err());
assert!(bank1_sysvar_cache.get_epoch_schedule().is_err());
assert!(bank1_sysvar_cache.get_rent().is_err());
assert!(bank1_sysvar_cache.get_slot_hashes().is_err());
⋮----
parent_blockhash: bank1.parent().unwrap().last_blockhash(),
⋮----
bank1.create_epoch_rewards_sysvar(
⋮----
.fill_missing_sysvar_cache_entries(&bank1);
⋮----
assert_eq!(bank1_sysvar_cache.get_clock(), bank1_cached_clock);
assert_eq!(
⋮----
assert_eq!(bank1_sysvar_cache.get_fees(), bank1_cached_fees);
assert_eq!(bank1_sysvar_cache.get_rent(), bank1_cached_rent);

================
File: runtime/src/bank/tests.rs
================
impl VoteReward {
pub fn new_random() -> Self {
⋮----
let validator_stake_lamports = rng.random_range(1..200);
⋮----
let commission: u8 = rng.random_range(1..20);
⋮----
&validator_voting_keypair.pubkey(),
⋮----
vote_rewards: rng.random_range(1..200),
⋮----
fn create_genesis_config_no_tx_fee_no_rent(lamports: u64) -> (GenesisConfig, Keypair) {
⋮----
fn create_genesis_config_no_tx_fee(lamports: u64) -> (GenesisConfig, Keypair) {
⋮----
pub(in crate::bank) fn create_genesis_config(lamports: u64) -> (GenesisConfig, Keypair) {
⋮----
pub(in crate::bank) fn new_sanitized_message(message: Message) -> SanitizedMessage {
⋮----
.unwrap()
⋮----
fn test_race_register_tick_freeze() {
⋮----
let (mut genesis_config, _) = create_genesis_config(50);
⋮----
let hash = hash(p.as_ref());
⋮----
let bank0_ = bank0.clone();
⋮----
.name("freeze".to_string())
.spawn(move || loop {
if bank0_.is_complete() {
assert_eq!(bank0_.last_blockhash(), hash);
⋮----
.unwrap();
⋮----
.name("register_tick".to_string())
.spawn(move || {
bank0_.register_tick_for_test(&hash);
⋮----
register_tick_thread.join().unwrap();
freeze_thread.join().unwrap();
⋮----
fn new_executed_processing_result(
⋮----
Ok(ProcessedTransaction::Executed(Box::new(
⋮----
impl Bank {
fn clean_accounts_for_tests(&self) {
self.rc.accounts.accounts_db.clean_accounts_for_tests()
⋮----
fn test_bank_unix_timestamp_from_genesis() {
let (genesis_config, _mint_keypair) = create_genesis_config(1);
⋮----
assert_eq!(
⋮----
let slots_per_sec = (genesis_config.poh_config.target_tick_duration.as_secs_f32()
⋮----
.recip();
⋮----
bank = Arc::new(new_from_parent(bank));
⋮----
assert!(bank.unix_timestamp_from_genesis() - genesis_config.creation_time >= 1);
⋮----
fn test_bank_new() {
⋮----
let dummy_leader_stake_lamports = bootstrap_validator_stake_lamports();
⋮----
} = create_genesis_config_with_leader(
⋮----
assert_eq!(bank.get_balance(&mint_keypair.pubkey()), mint_lamports);
⋮----
let rent_account = bank.get_account(&sysvar::rent::id()).unwrap();
let rent = from_account::<sysvar::rent::Rent, _>(&rent_account).unwrap();
assert_eq!(rent.burn_percent, 5);
assert_eq!(rent.exemption_threshold, 1.2);
assert_eq!(rent.lamports_per_byte_year, 5);
⋮----
pub(crate) fn create_simple_test_bank(lamports: u64) -> Bank {
let (genesis_config, _mint_keypair) = create_genesis_config(lamports);
⋮----
fn create_simple_test_arc_bank(lamports: u64) -> (Arc<Bank>, Arc<RwLock<BankForks>>) {
let bank = create_simple_test_bank(lamports);
bank.wrap_with_bank_forks_for_tests()
⋮----
fn test_bank_block_height() {
let (bank0, _bank_forks) = create_simple_test_arc_bank(1);
assert_eq!(bank0.block_height(), 0);
let bank1 = Arc::new(new_from_parent(bank0));
assert_eq!(bank1.block_height(), 1);
⋮----
fn test_bank_update_epoch_stakes() {
⋮----
fn epoch_stake_keys(&self) -> Vec<Epoch> {
let mut keys: Vec<Epoch> = self.epoch_stakes.keys().copied().collect();
keys.sort_unstable();
⋮----
fn epoch_stake_key_info(&self) -> (Epoch, Epoch, usize) {
⋮----
(*keys.first().unwrap(), *keys.last().unwrap(), keys.len())
⋮----
let mut bank = create_simple_test_bank(100_000);
let initial_epochs = bank.epoch_stake_keys();
assert_eq!(initial_epochs, vec![0, 1]);
⋮----
bank.update_epoch_stakes(*existing_epoch);
assert_eq!(bank.epoch_stake_keys(), initial_epochs);
⋮----
for epoch in (initial_epochs.len() as Epoch)..(MAX_LEADER_SCHEDULE_STAKES - 1) {
bank.update_epoch_stakes(dbg!(epoch));
assert_eq!(bank.epoch_stakes.len() as Epoch, epoch + 1);
⋮----
bank.update_epoch_stakes(MAX_LEADER_SCHEDULE_STAKES - 1);
⋮----
bank.update_epoch_stakes(MAX_LEADER_SCHEDULE_STAKES);
⋮----
fn bank0_sysvar_delta() -> u64 {
⋮----
fn bank1_sysvar_delta() -> u64 {
⋮----
fn bank2_sysvar_delta() -> u64 {
⋮----
fn test_bank_capitalization() {
⋮----
.map(|_| {
⋮----
.collect(),
⋮----
bank0.freeze();
⋮----
fn assert_capitalization_diff(
⋮----
let old = bank.capitalization();
updater();
let new = bank.capitalization();
if asserter(old, new) {
add_root_and_flush_write_cache(bank);
⋮----
declare_process_instruction!(MockBuiltin, 1, |_invoke_context| {
⋮----
fn test_store_account_and_update_capitalization_missing() {
let bank = create_simple_test_bank(0);
⋮----
assert_capitalization_diff(
⋮----
|| bank.store_account_and_update_capitalization(&pubkey, &account),
⋮----
assert_eq!(old + some_lamports, new);
⋮----
assert_eq!(account, bank.get_account(&pubkey).unwrap());
⋮----
fn test_store_account_and_update_capitalization_increased() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(old_lamports);
⋮----
let pubkey = mint_keypair.pubkey();
⋮----
assert_eq!(old + 100, new);
⋮----
fn test_store_account_and_update_capitalization_decreased() {
⋮----
assert_eq!(old - 300, new);
⋮----
fn test_store_account_and_update_capitalization_unchanged() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(lamports);
⋮----
assert_eq!(old, new);
⋮----
pub(in crate::bank) fn new_from_parent_next_epoch(
⋮----
let mut slot = parent.slot();
let mut epoch = parent.epoch();
⋮----
slot += parent.epoch_schedule().get_slots_in_epoch(epoch);
epoch = parent.epoch_schedule().get_epoch(slot);
⋮----
fn test_bank_update_vote_stake_rewards() {
let thread_pool = ThreadPoolBuilder::new().num_threads(1).build().unwrap();
check_bank_update_vote_stake_rewards(|bank: &Bank| {
bank._load_vote_and_stake_accounts(&thread_pool, null_tracer())
⋮----
fn _load_vote_and_stake_accounts(
⋮----
let stakes = self.stakes_cache.stakes();
let stake_delegations = stakes.stake_delegations_vec();
let stake_delegations = self.filter_stake_delegations(stake_delegations);
fn merge(mut acc: HashSet<Pubkey>, other: HashSet<Pubkey>) -> HashSet<Pubkey> {
if acc.len() < other.len() {
return merge(other, acc);
⋮----
acc.extend(other);
⋮----
let voter_pubkeys = thread_pool.install(|| {
⋮----
.par_iter()
.filter_map(|stake_delegation| stake_delegation)
.fold(
⋮----
voter_pubkeys.insert(stake_account.delegation().voter_pubkey);
⋮----
.reduce(HashSet::default, merge)
⋮----
let cached_vote_accounts = stakes.vote_accounts();
⋮----
if let Some(vote_account) = cached_vote_accounts.get(vote_pubkey) {
return Some(vote_account.clone());
⋮----
let account = self.get_account_with_fixed_root(vote_pubkey)?;
if account.owner() == &solana_vote_program
&& VoteStateV4::deserialize(account.data(), vote_pubkey).is_ok()
⋮----
vote_accounts_cache_miss_count.fetch_add(1, Relaxed);
⋮----
VoteAccount::try_from(account).ok()
⋮----
let Some(vote_account) = get_vote_account(&vote_pubkey) else {
invalid_vote_keys.insert(vote_pubkey, InvalidCacheEntryReason::Missing);
⋮----
if vote_account.owner() != &solana_vote_program {
invalid_vote_keys.insert(vote_pubkey, InvalidCacheEntryReason::WrongOwner);
⋮----
Some((vote_pubkey, stake_delegations))
⋮----
let stake_delegations_map: DashMap<Pubkey, StakeDelegations> = thread_pool.install(|| {
⋮----
.into_par_iter()
.filter_map(make_vote_delegations_entry)
.collect()
⋮----
let delegation = stake_account.delegation();
⋮----
stake_delegations_map.get_mut(&delegation.voter_pubkey)
⋮----
if let Some(reward_calc_tracer) = reward_calc_tracer.as_ref() {
⋮----
reward_calc_tracer(&event);
⋮----
let stake_delegation = (*stake_pubkey, stake_account.clone());
vote_delegations.push(stake_delegation);
⋮----
thread_pool.install(|| {
⋮----
.for_each(push_stake_delegation);
⋮----
type StakeDelegations = Vec<(Pubkey, StakeAccount<Delegation>)>;
type StakeDelegationsMap = DashMap<Pubkey, StakeDelegations>;
⋮----
fn check_bank_update_vote_stake_rewards<F>(load_vote_and_stake_accounts: F)
⋮----
bank0.store_account_and_update_capitalization(&stake_id, &stake_account);
let mut vote_state = Some(VoteStateV4::deserialize(vote_account.data(), &vote_id).unwrap());
⋮----
if let Some(v) = vote_state.as_mut() {
⋮----
let versioned = VoteStateVersions::V4(Box::new(vote_state.take().unwrap()));
vote_account.set_state(&versioned).unwrap();
bank0.store_account_and_update_capitalization(&vote_id, &vote_account);
⋮----
vote_state = Some(*v);
⋮----
_ => panic!("Has to be of type V4"),
⋮----
assert!(bank0.rewards.read().unwrap().is_empty());
load_vote_and_stake_accounts(&bank0);
⋮----
bank0.clone(),
⋮----
bank0.get_slots_in_epoch(bank0.epoch()) + 1,
⋮----
assert_ne!(bank1.capitalization(), bank0.capitalization());
⋮----
bank1.freeze();
⋮----
bank1.clone(),
⋮----
bank1.slot() + 1,
⋮----
assert_ne!(bank2.capitalization(), bank0.capitalization());
let paid_rewards = bank2.capitalization()
- bank0.capitalization()
- bank1_sysvar_delta()
- bank2_sysvar_delta();
⋮----
} = bank2.calculate_previous_epoch_inflation_rewards(bank0.capitalization(), bank0.epoch());
assert!(
⋮----
assert!((validator_rewards as f64 - paid_rewards as f64).abs() < 1.0);
⋮----
bank2.freeze();
add_root_and_flush_write_cache(&bank0);
add_root_and_flush_write_cache(&bank1);
add_root_and_flush_write_cache(&bank2);
⋮----
fn do_test_bank_update_rewards_determinism() -> u64 {
⋮----
bank.store_account_and_update_capitalization(&stake_id1, &stake_account1);
bank.store_account_and_update_capitalization(&stake_id2, &stake_account2);
⋮----
bank.store_account_and_update_capitalization(&vote_id, &vote_account);
⋮----
bank.clone(),
⋮----
bank.get_slots_in_epoch(bank.epoch()) + 1,
⋮----
assert_ne!(bank1.capitalization(), bank.capitalization());
⋮----
add_root_and_flush_write_cache(&bank);
⋮----
let rewards = bank1.rewards.read().unwrap();
⋮----
.iter()
.find(|(_address, reward)| reward.reward_type == RewardType::Voting)
⋮----
let bank2 = Bank::new_from_parent(bank1.clone(), &Pubkey::default(), bank1.slot() + 1);
let rewards = bank2.rewards.read().unwrap();
⋮----
.find(|(_address, reward)| reward.reward_type == RewardType::Staking)
⋮----
bank1.capitalization()
⋮----
fn test_bank_update_rewards_determinism() {
⋮----
let expected_capitalization = do_test_bank_update_rewards_determinism();
⋮----
let actual_capitalization = do_test_bank_update_rewards_determinism();
assert_eq!(actual_capitalization, expected_capitalization);
⋮----
impl VerifyAccountsHashConfig {
fn default_for_test() -> Self {
⋮----
fn test_purge_empty_accounts() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee(LAMPORTS_PER_SOL);
let amount = genesis_config.rent.minimum_balance(0);
⋮----
Bank::new_for_tests(&genesis_config).wrap_with_bank_forks_for_tests();
⋮----
let blockhash = bank.last_blockhash();
⋮----
bank.process_transaction(&tx).unwrap();
bank.freeze();
bank.squash();
bank = new_from_parent_with_fork_next_slot(bank, bank_forks.as_ref());
⋮----
bank.force_flush_accounts_cache();
let hash = bank.calculate_accounts_lt_hash_for_tests();
bank.clean_accounts_for_tests();
assert_eq!(bank.calculate_accounts_lt_hash_for_tests(), hash);
let bank0 = new_from_parent_with_fork_next_slot(bank.clone(), bank_forks.as_ref());
⋮----
let tx = system_transaction::transfer(&mint_keypair, &keypair.pubkey(), amount, blockhash);
bank0.process_transaction(&tx).unwrap();
let bank1 = new_from_parent_with_fork_next_slot(bank0.clone(), bank_forks.as_ref());
⋮----
bank1.process_transaction(&tx).unwrap();
⋮----
assert_eq!(bank1.get_account(&keypair.pubkey()), None);
info!("bank0 purge");
let hash = bank0.calculate_accounts_lt_hash_for_tests();
bank0.clean_accounts_for_tests();
assert_eq!(bank0.calculate_accounts_lt_hash_for_tests(), hash);
⋮----
info!("bank1 purge");
bank1.clean_accounts_for_tests();
⋮----
assert!(bank0.verify_accounts(VerifyAccountsHashConfig::default_for_test(), None));
⋮----
bank0.squash();
⋮----
bank1.squash();
⋮----
assert!(bank1.verify_accounts(VerifyAccountsHashConfig::default_for_test(), None));
assert_eq!(bank0.get_account(&keypair.pubkey()), None);
⋮----
fn test_two_payments_to_one_party() {
let (genesis_config, mint_keypair) = create_genesis_config(LAMPORTS_PER_SOL);
⋮----
assert_eq!(bank.last_blockhash(), genesis_config.hash());
bank.transfer(amount, &mint_keypair, &pubkey).unwrap();
assert_eq!(bank.get_balance(&pubkey), amount);
bank.transfer(amount * 2, &mint_keypair, &pubkey).unwrap();
assert_eq!(bank.get_balance(&pubkey), amount * 3);
assert_eq!(bank.transaction_count(), 2);
assert_eq!(bank.non_vote_transaction_count_since_restart(), 2);
⋮----
fn test_one_source_two_tx_one_batch() {
⋮----
let t1 = system_transaction::transfer(&mint_keypair, &key1, amount, genesis_config.hash());
let t2 = system_transaction::transfer(&mint_keypair, &key2, amount, genesis_config.hash());
let txs = [t1.clone(), t2.clone()];
let res = bank.process_transactions(txs.iter());
assert_eq!(res.len(), 2);
assert_eq!(res[0], Ok(()));
assert_eq!(res[1], Err(TransactionError::AccountInUse));
⋮----
assert_eq!(bank.get_balance(&key1), amount);
assert_eq!(bank.get_balance(&key2), 0);
assert_eq!(bank.get_signature_status(&t1.signatures[0]), Some(Ok(())));
assert_eq!(bank.get_signature_status(&t2.signatures[0]), None);
⋮----
fn test_one_tx_two_out_atomic_fail() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee_no_rent(amount);
⋮----
&mint_keypair.pubkey(),
⋮----
let message = Message::new(&instructions, Some(&mint_keypair.pubkey()));
let tx = Transaction::new(&[&mint_keypair], message, genesis_config.hash());
⋮----
assert_eq!(bank.get_balance(&mint_keypair.pubkey()), amount);
assert_eq!(bank.get_balance(&key1), 0);
⋮----
fn test_one_tx_two_out_atomic_pass() {
⋮----
assert_eq!(bank.get_balance(&key2), amount);
⋮----
fn test_detect_failed_duplicate_transactions() {
let (mut genesis_config, mint_keypair) = create_genesis_config(10_000);
⋮----
system_transaction::transfer(&mint_keypair, &dest.pubkey(), 10_000, genesis_config.hash());
⋮----
assert!(!bank.has_signature(&signature));
⋮----
assert_eq!(bank.get_balance(&dest.pubkey()), 0);
assert_eq!(bank.get_balance(&mint_keypair.pubkey()), 5000);
⋮----
fn test_account_not_found() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(0);
⋮----
assert_eq!(bank.transaction_count(), 0);
assert_eq!(bank.non_vote_transaction_count_since_restart(), 0);
⋮----
fn test_insufficient_funds() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee(mint_amount);
⋮----
assert_eq!(bank.transaction_count(), 1);
assert_eq!(bank.non_vote_transaction_count_since_restart(), 1);
⋮----
let mint_pubkey = mint_keypair.pubkey();
assert_eq!(bank.get_balance(&mint_pubkey), mint_amount - amount);
⋮----
fn test_executed_transaction_count_post_bank_transaction_count_fix() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(mint_amount);
⋮----
assert_eq!(bank.executed_transaction_count(), 2);
assert_eq!(bank.transaction_error_count(), 1);
⋮----
bank_forks.as_ref(),
⋮----
assert_eq!(bank2.transaction_count(), 3);
assert_eq!(bank2.executed_transaction_count(), 1);
assert_eq!(bank2.transaction_error_count(), 1);
⋮----
fn test_transfer_to_newb() {
⋮----
fn test_transfer_to_sysvar() {
⋮----
assert_eq!(bank.get_balance(&normal_pubkey), 0);
assert_eq!(bank.get_balance(&sysvar_pubkey), 1_169_280);
bank.transfer(amount, &mint_keypair, &normal_pubkey)
⋮----
bank.transfer(amount, &mint_keypair, &sysvar_pubkey)
.unwrap_err();
assert_eq!(bank.get_balance(&normal_pubkey), amount);
⋮----
let bank = new_from_parent_with_fork_next_slot(bank, bank_forks.as_ref());
⋮----
fn test_bank_withdraw() {
let bank = create_simple_test_bank(100);
⋮----
test_utils::deposit(&bank, &key, 3).unwrap();
assert_eq!(bank.get_balance(&key), 3);
⋮----
assert_eq!(bank.withdraw(&key, 2), Ok(()));
assert_eq!(bank.get_balance(&key), 1);
⋮----
fn test_bank_withdraw_from_nonce_account() {
let (mut genesis_config, _mint_keypair) = create_genesis_config(100_000);
⋮----
let min_balance = bank.get_minimum_balance_for_rent_exemption(nonce::state::State::size());
⋮----
bank.store_account(&nonce.pubkey(), &nonce_account);
assert_eq!(bank.get_balance(&nonce.pubkey()), min_balance + 42);
⋮----
bank.withdraw(&nonce.pubkey(), 42).unwrap();
assert_eq!(bank.get_balance(&nonce.pubkey()), min_balance);
⋮----
fn test_bank_tx_fee() {
⋮----
} = create_genesis_config_with_leader(mint, &leader, 3);
⋮----
.create_fee_calculator()
⋮----
genesis_config.fee_rate_governor.burn(expected_fee_paid);
⋮----
let capitalization = bank.capitalization();
⋮----
bank.last_blockhash(),
⋮----
let initial_balance = bank.get_balance(&leader);
assert_eq!(bank.process_transaction(&tx), Ok(()));
assert_eq!(bank.get_balance(&key), arbitrary_transfer_amount);
⋮----
assert_eq!(bank.get_balance(&leader), initial_balance);
goto_end_of_slot(bank.clone());
assert_eq!(bank.signature_count(), 1);
⋮----
let bank = Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank, &leader, 1);
let mut tx = system_transaction::transfer(&mint_keypair, &key, 1, bank.last_blockhash());
⋮----
bank.process_transaction(&tx)
.expect_err("instruction error");
⋮----
fn test_bank_tx_compute_unit_fee() {
⋮----
let expected_fee_paid = calculate_test_fee(
&new_sanitized_message(Message::new(&[], Some(&Pubkey::new_unique()))),
⋮----
bank.fee_structure(),
⋮----
fn test_bank_blockhash_fee_structure() {
⋮----
} = create_genesis_config_with_leader(1_000_000, &leader, 3);
⋮----
let cheap_blockhash = bank.last_blockhash();
let cheap_lamports_per_signature = bank.get_lamports_per_signature();
assert_eq!(cheap_lamports_per_signature, 0);
⋮----
let expensive_blockhash = bank.last_blockhash();
let expensive_lamports_per_signature = bank.get_lamports_per_signature();
assert!(cheap_lamports_per_signature < expensive_lamports_per_signature);
let bank = Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank, &leader, 2);
⋮----
let initial_mint_balance = bank.get_balance(&mint_keypair.pubkey());
⋮----
let cheap_fee = calculate_test_fee(
⋮----
let expensive_fee = calculate_test_fee(
⋮----
fn test_bank_blockhash_compute_unit_fee_structure() {
⋮----
} = create_genesis_config_with_leader(1_000_000_000, &leader, 3);
⋮----
fn test_debits_before_credits() {
⋮----
create_genesis_config_no_tx_fee_no_rent(2 * LAMPORTS_PER_SOL);
⋮----
genesis_config.hash(),
⋮----
&keypair.pubkey(),
⋮----
let results = bank.process_transactions(txs.iter());
assert!(results[0].is_err());
⋮----
fn test_readonly_accounts(relax_intrabatch_account_locks: bool) {
⋮----
} = create_genesis_config_with_leader(500, &solana_pubkey::new_rand(), 0);
⋮----
bank.deactivate_feature(&feature_set::relax_intrabatch_account_locks::id());
⋮----
let next_slot = bank.slot() + 1;
⋮----
let (bank, _bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
&authorized_voter.pubkey(),
⋮----
bank.store_account(&vote_pubkey0, &vote_account0);
bank.store_account(&vote_pubkey1, &vote_account1);
bank.store_account(&vote_pubkey2, &vote_account2);
bank.transfer(10, &mint_keypair, &payer0.pubkey()).unwrap();
bank.transfer(10, &mint_keypair, &payer1.pubkey()).unwrap();
bank.transfer(1, &mint_keypair, &authorized_voter.pubkey())
⋮----
let ix0 = vote_instruction::tower_sync(&vote_pubkey0, &authorized_voter.pubkey(), vote.clone());
⋮----
Some(&payer0.pubkey()),
⋮----
let ix1 = vote_instruction::tower_sync(&vote_pubkey1, &authorized_voter.pubkey(), vote.clone());
⋮----
Some(&payer1.pubkey()),
⋮----
assert_eq!(results[0], Ok(()));
assert_eq!(results[1], Ok(()));
let ix0 = vote_instruction::tower_sync(&vote_pubkey2, &authorized_voter.pubkey(), vote);
⋮----
fn test_interleaving_locks() {
⋮----
&alice.pubkey(),
⋮----
let pay_alice = vec![tx1];
let lock_result = bank.prepare_batch_for_tests(pay_alice);
⋮----
.load_execute_and_commit_transactions(
⋮----
assert!(commit_results[0].is_ok());
⋮----
drop(lock_result);
assert!(bank
⋮----
fn test_load_and_execute_commit_transactions_fees_only() {
⋮----
genesis_config.epoch_schedule.get_first_slot_in_epoch(1),
⋮----
bank.store_account(
⋮----
genesis_config.rent.minimum_balance(0) - 1,
⋮----
let nonce_balance = genesis_config.rent.minimum_balance(nonce_size);
⋮----
&nonce::versions::Versions::new(nonce::state::State::Initialized(nonce_data.clone())),
⋮----
bank.store_account(&nonce_pubkey, &nonce_account);
⋮----
Instruction::new_with_bincode(missing_program_id, &0, vec![]),
⋮----
Some(&rent_paying_fee_payer),
&nonce_data.blockhash(),
⋮----
let batch = bank.prepare_batch_for_tests(vec![transaction]);
⋮----
fn test_load_and_execute_commit_transactions_failure() {
⋮----
let starting_balance = 2 * genesis_config.rent.minimum_balance(0) + 10_000;
⋮----
let transfer_amount = genesis_config.rent.minimum_balance(0);
⋮----
Instruction::new_with_bincode(system_program::id(), &(), vec![]),
⋮----
Some(&fee_payer),
&bank.last_blockhash(),
⋮----
fn test_load_and_execute_commit_transactions_success() {
⋮----
fn test_readonly_relaxed_locks() {
let (genesis_config, _) = create_genesis_config(3);
⋮----
account_keys: vec![key0.pubkey(), key3],
⋮----
instructions: vec![],
⋮----
let tx = Transaction::new(&[&key0], message, genesis_config.hash());
let txs = vec![tx];
let batch0 = bank.prepare_batch_for_tests(txs);
assert!(batch0.lock_results()[0].is_ok());
⋮----
account_keys: vec![key1.pubkey(), key3],
⋮----
let tx = Transaction::new(&[&key1], message, genesis_config.hash());
⋮----
let batch1 = bank.prepare_batch_for_tests(txs);
assert!(batch1.lock_results()[0].is_err());
⋮----
account_keys: vec![key2.pubkey(), key3],
⋮----
let tx = Transaction::new(&[&key2], message, genesis_config.hash());
⋮----
let batch2 = bank.prepare_batch_for_tests(txs);
assert!(batch2.lock_results()[0].is_ok());
⋮----
fn test_bank_invalid_account_index() {
let (genesis_config, mint_keypair) = create_genesis_config(1);
⋮----
system_transaction::transfer(&mint_keypair, &keypair.pubkey(), 1, genesis_config.hash());
let mut tx_invalid_program_index = tx.clone();
⋮----
fn test_bank_pay_to_self() {
⋮----
bank.transfer(amount, &mint_keypair, &key1.pubkey())
⋮----
assert_eq!(bank.get_balance(&key1.pubkey()), amount);
let tx = system_transaction::transfer(&key1, &key1.pubkey(), amount, genesis_config.hash());
let _res = bank.process_transaction(&tx);
⋮----
bank.get_signature_status(&tx.signatures[0])
⋮----
fn new_from_parent(parent: Arc<Bank>) -> Bank {
let slot = parent.slot() + 1;
⋮----
fn new_from_parent_with_fork_next_slot(parent: Arc<Bank>, fork: &RwLock<BankForks>) -> Arc<Bank> {
⋮----
fn test_bank_parents() {
let (genesis_config, _) = create_genesis_config(1);
⋮----
let bank = new_from_parent(parent.clone());
assert!(Arc::ptr_eq(&bank.parents()[0], &parent));
⋮----
fn test_tx_already_processed() {
⋮----
&key1.pubkey(),
genesis_config.rent.minimum_balance(0),
⋮----
fn test_bank_parent_already_processed() {
⋮----
system_transaction::transfer(&mint_keypair, &key1.pubkey(), amount, genesis_config.hash());
assert_eq!(parent.process_transaction(&tx), Ok(()));
let bank = new_from_parent_with_fork_next_slot(parent, bank_forks.as_ref());
⋮----
fn test_bank_parent_account_spend() {
⋮----
let bank = new_from_parent_with_fork_next_slot(parent.clone(), bank_forks.as_ref());
let tx = system_transaction::transfer(&key1, &key2.pubkey(), amount, genesis_config.hash());
⋮----
assert_eq!(parent.get_signature_status(&tx.signatures[0]), None);
⋮----
fn test_bank_hash_internal_state() {
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee_no_rent(LAMPORTS_PER_SOL);
⋮----
let initial_state = bank0.hash_internal_state();
assert_eq!(bank1.hash_internal_state(), initial_state);
⋮----
bank0.transfer(amount, &mint_keypair, &pubkey).unwrap();
⋮----
assert_ne!(bank0.hash_internal_state(), initial_state);
bank1.transfer(amount, &mint_keypair, &pubkey).unwrap();
⋮----
assert_eq!(bank0.hash_internal_state(), bank1.hash_internal_state());
let bank2 = new_from_parent_with_fork_next_slot(bank1, &bank_forks1);
assert_ne!(bank0.hash_internal_state(), bank2.hash_internal_state());
⋮----
bank2.transfer(amount, &mint_keypair, &pubkey2).unwrap();
bank2.squash();
bank2.force_flush_accounts_cache();
assert!(bank2.verify_accounts(VerifyAccountsHashConfig::default_for_test(), None));
⋮----
fn test_bank_hash_internal_state_verify() {
⋮----
create_genesis_config_no_tx_fee_no_rent(LAMPORTS_PER_SOL);
⋮----
let bank0_state = bank0.hash_internal_state();
⋮----
assert_ne!(bank0_state, bank2.hash_internal_state());
assert_ne!(bank0_state, bank0.hash_internal_state());
⋮----
assert_eq!(bank0_state, bank0.hash_internal_state());
⋮----
bank3.freeze();
add_root_and_flush_write_cache(&bank3);
assert!(bank3.verify_accounts(VerifyAccountsHashConfig::default_for_test(), None));
⋮----
fn test_verify_hash_unfrozen() {
let bank = create_simple_test_bank(2_000);
assert!(bank.verify_hash());
⋮----
fn test_verify_snapshot_bank() {
⋮----
bank.transfer(
⋮----
assert!(bank.verify_snapshot_bank(false, false, bank.slot(), None));
bank.increment_signature_count(1);
assert!(!bank.verify_snapshot_bank(false, false, bank.slot(), None));
⋮----
fn test_bank_hash_same_transactions_different_fork() {
⋮----
bank2.transfer(amount, &mint_keypair, &pubkey).unwrap();
⋮----
let bank0_hash = bank0.hash();
let bank1_hash = bank1.hash();
let bank2_hash = bank2.hash();
assert_ne!(bank0_hash, bank1_hash);
assert_ne!(bank0_hash, bank2_hash);
assert_ne!(bank1_hash, bank2_hash);
⋮----
fn test_hash_internal_state_genesis() {
let bank0 = Bank::new_for_tests(&create_genesis_config(10).0);
let bank1 = Bank::new_for_tests(&create_genesis_config(20).0);
assert_ne!(bank0.hash_internal_state(), bank1.hash_internal_state());
⋮----
fn test_hash_internal_state_order() {
⋮----
bank0.transfer(amount, &mint_keypair, &key0).unwrap();
bank0.transfer(amount * 2, &mint_keypair, &key1).unwrap();
bank1.transfer(amount * 2, &mint_keypair, &key1).unwrap();
bank1.transfer(amount, &mint_keypair, &key0).unwrap();
⋮----
fn test_hash_internal_state_error() {
⋮----
bank.transfer(amount, &mint_keypair, &key0).unwrap();
let orig = bank.hash_internal_state();
⋮----
assert_ne!(orig, bank.hash_internal_state());
⋮----
assert!(bank.transfer(amount, &empty_keypair, &key0).is_err());
assert_eq!(orig, bank.hash_internal_state());
⋮----
fn test_bank_hash_internal_state_squash() {
⋮----
let bank0 = Arc::new(Bank::new_for_tests(&create_genesis_config(10).0));
let hash0 = bank0.hash_internal_state();
⋮----
assert_ne!(hash0, bank1.hash_internal_state());
⋮----
assert!(bank1.parents().is_empty());
⋮----
fn test_bank_squash() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee(2 * LAMPORTS_PER_SOL);
⋮----
trace!("parent process tx ");
assert_eq!(parent.process_transaction(&tx_transfer_mint_to_1), Ok(()));
trace!("done parent process tx ");
assert_eq!(parent.transaction_count(), 1);
assert_eq!(parent.non_vote_transaction_count_since_restart(), 1);
⋮----
trace!("new from parent");
⋮----
trace!("done new from parent");
⋮----
assert_eq!(bank.transaction_count(), parent.transaction_count());
⋮----
system_transaction::transfer(&key1, &key2.pubkey(), amount, genesis_config.hash());
assert_eq!(bank.process_transaction(&tx_transfer_1_to_2), Ok(()));
⋮----
assert_eq!(bank.get_balance(&key1.pubkey()), 0);
assert_eq!(bank.get_account(&key1.pubkey()), None);
assert_eq!(bank.get_balance(&key2.pubkey()), amount);
trace!("start");
⋮----
trace!("SQUASH");
⋮----
fn test_bank_get_account_in_parent_after_squash() {
⋮----
.transfer(amount, &mint_keypair, &key1.pubkey())
⋮----
assert_eq!(parent.get_balance(&key1.pubkey()), amount);
⋮----
fn test_bank_get_account_in_parent_after_squash2() {
⋮----
assert_eq!(bank0.get_balance(&key1.pubkey()), amount);
⋮----
.transfer(3 * amount, &mint_keypair, &key1.pubkey())
⋮----
.transfer(2 * amount, &mint_keypair, &key1.pubkey())
⋮----
assert_eq!(bank0.get_balance(&key1.pubkey()), 4 * amount);
assert_eq!(bank3.get_balance(&key1.pubkey()), 4 * amount);
assert_eq!(bank2.get_balance(&key1.pubkey()), 3 * amount);
bank3.squash();
assert_eq!(bank1.get_balance(&key1.pubkey()), 4 * amount);
⋮----
bank3.clone(),
⋮----
.transfer(4 * amount, &mint_keypair, &key1.pubkey())
⋮----
assert_eq!(bank4.get_balance(&key1.pubkey()), 8 * amount);
⋮----
bank4.squash();
⋮----
bank4.clone(),
⋮----
bank5.squash();
⋮----
Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank5, &Pubkey::default(), 6);
bank6.squash();
assert_eq!(bank3.get_balance(&key1.pubkey()), 8 * amount);
assert_eq!(bank2.get_balance(&key1.pubkey()), 8 * amount);
⋮----
fn test_bank_get_account_modified_since_parent_with_fixed_root() {
⋮----
let result = bank1.get_account_modified_since_parent_with_fixed_root(&pubkey);
assert!(result.is_some());
let (account, slot) = result.unwrap();
assert_eq!(account.lamports(), amount);
assert_eq!(slot, 0);
⋮----
assert!(bank2
⋮----
bank2.transfer(2 * amount, &mint_keypair, &pubkey).unwrap();
⋮----
let result = bank2.get_account_modified_since_parent_with_fixed_root(&pubkey);
⋮----
assert_eq!(account.lamports(), 3 * amount);
assert_eq!(slot, 1);
⋮----
Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank2, &Pubkey::default(), 3);
⋮----
fn test_bank_update_sysvar_account() {
⋮----
use sysvar::clock::Clock;
⋮----
let (mut genesis_config, _mint_keypair) = create_genesis_config(500);
⋮----
for feature_id in FeatureSet::default().inactive() {
activate_feature(&mut genesis_config, *feature_id);
⋮----
bank1.update_sysvar_account(&dummy_clock_id, |optional_account| {
assert!(optional_account.is_none());
let mut account = create_account(
⋮----
bank1.inherit_specially_retained_account_fields(optional_account),
⋮----
account.set_rent_epoch(dummy_rent_epoch);
⋮----
let current_account = bank1.get_account(&dummy_clock_id).unwrap();
⋮----
assert_eq!(dummy_rent_epoch, current_account.rent_epoch());
⋮----
assert!(optional_account.is_some());
create_account(
⋮----
let bank2 = Arc::new(Bank::new_from_parent(bank1.clone(), &Pubkey::default(), 1));
⋮----
bank2.update_sysvar_account(&dummy_clock_id, |optional_account| {
let slot = from_account::<Clock, _>(optional_account.as_ref().unwrap())
⋮----
bank2.inherit_specially_retained_account_fields(optional_account),
⋮----
let current_account = bank2.get_account(&dummy_clock_id).unwrap();
⋮----
fn test_bank_epoch_vote_accounts() {
⋮----
create_genesis_config_with_leader(5, &leader_pubkey, leader_lamports).genesis_config;
⋮----
.epoch_vote_accounts(0)
.map(|accounts| {
⋮----
.filter_map(|(pubkey, (stake, account))| {
if account.node_pubkey() == &leader_pubkey {
Some((*pubkey, *stake))
⋮----
assert_eq!(leader_vote_stake.len(), 1);
let (leader_vote_account, leader_stake) = leader_vote_stake.pop().unwrap();
assert!(leader_stake > 0);
⋮----
let vote_accounts = parent.epoch_vote_accounts(epoch);
assert!(vote_accounts.is_some());
⋮----
parent.clone(),
⋮----
assert!(child.epoch_vote_accounts(epoch).is_some());
⋮----
fn test_zero_signatures() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(500);
⋮----
let mut transfer_instruction = system_instruction::transfer(&mint_keypair.pubkey(), &key, 0);
⋮----
let tx = Transaction::new(&Vec::<&Keypair>::new(), message, bank.last_blockhash());
⋮----
assert_eq!(bank.get_balance(&key), 0);
⋮----
fn test_bank_get_slots_in_epoch() {
let (genesis_config, _) = create_genesis_config(500);
⋮----
assert_eq!(bank.get_slots_in_epoch(0), MINIMUM_SLOTS_PER_EPOCH);
assert_eq!(bank.get_slots_in_epoch(2), (MINIMUM_SLOTS_PER_EPOCH * 4));
⋮----
fn test_is_delta_true() {
⋮----
assert_eq!(bank.process_transaction(&tx_transfer_mint_to_1), Ok(()));
assert!(bank.is_delta.load(Relaxed));
let bank1 = new_from_parent(bank.clone());
let hash1 = bank1.hash_internal_state();
assert!(!bank1.is_delta.load(Relaxed));
assert_ne!(hash1, bank.hash());
bank1.register_default_tick_for_test();
⋮----
assert_eq!(bank1.hash_internal_state(), hash1);
⋮----
fn test_is_empty() {
⋮----
assert!(bank0.is_empty());
⋮----
assert_eq!(bank0.process_transaction(&tx_transfer_mint_to_1), Ok(()));
assert!(!bank0.is_empty());
⋮----
fn test_bank_inherit_tx_count() {
⋮----
assert_eq!(bank0.transaction_count(), 0);
assert_eq!(bank0.non_vote_transaction_count_since_restart(), 0);
assert_eq!(bank2.transaction_count(), 0);
assert_eq!(bank2.non_vote_transaction_count_since_restart(), 0);
assert_eq!(bank1.transaction_count(), 1);
assert_eq!(bank1.non_vote_transaction_count_since_restart(), 1);
⋮----
assert_eq!(bank6.transaction_count(), 1);
assert_eq!(bank6.non_vote_transaction_count_since_restart(), 1);
⋮----
fn test_bank_inherit_fee_rate_governor() {
⋮----
let bank1 = Arc::new(new_from_parent(bank0.clone()));
⋮----
fn test_bank_vote_accounts() {
⋮----
} = create_genesis_config_with_leader(500, &solana_pubkey::new_rand(), 1);
⋮----
let vote_accounts = bank.vote_accounts();
assert_eq!(vote_accounts.len(), 1);
⋮----
&vote_keypair.pubkey(),
⋮----
node_pubkey: mint_keypair.pubkey(),
authorized_voter: vote_keypair.pubkey(),
authorized_withdrawer: vote_keypair.pubkey(),
⋮----
bank.process_transaction(&transaction).unwrap();
⋮----
assert_eq!(vote_accounts.len(), 2);
assert!(vote_accounts.get(&vote_keypair.pubkey()).is_some());
assert!(bank.withdraw(&vote_keypair.pubkey(), 10).is_ok());
⋮----
fn test_bank_cloned_stake_delegations() {
⋮----
.into_iter()
⋮----
genesis_config.add_account(pubkey, account);
⋮----
let stake_delegations = bank.stakes_cache.stakes().stake_delegations().clone();
assert_eq!(stake_delegations.len(), 1);
⋮----
let rent = &bank.rent_collector().rent;
let vote_rent_exempt_reserve = rent.minimum_balance(VoteStateV4::size_of());
let stake_rent_exempt_reserve = rent.minimum_balance(StakeStateV2::size_of());
⋮----
.is_active(&agave_feature_set::stake_raise_minimum_delegation_to_1_sol::id()),
⋮----
instructions.extend(stake_instruction::create_account_and_delegate_stake(
⋮----
&stake_keypair.pubkey(),
⋮----
&Authorized::auto(&stake_keypair.pubkey()),
⋮----
assert_eq!(stake_delegations.len(), 2);
assert!(stake_delegations.get(&stake_keypair.pubkey()).is_some());
⋮----
fn test_is_delta_with_no_committables() {
let (genesis_config, mint_keypair) = create_genesis_config(8000);
⋮----
bank.is_delta.store(false, Relaxed);
⋮----
system_transaction::transfer(&keypair1, &keypair2.pubkey(), 1, bank.last_blockhash());
⋮----
assert!(!bank.is_delta.load(Relaxed));
⋮----
fn test_bank_get_program_accounts() {
⋮----
let genesis_accounts: Vec<_> = parent.get_all_accounts(false).unwrap();
⋮----
let bank0 = Arc::new(new_from_parent(parent));
⋮----
bank0.store_account(&pubkey0, &account0);
⋮----
let bank2 = Arc::new(new_from_parent(bank1.clone()));
⋮----
bank2.store_account(&pubkey1, &account1);
⋮----
bank2.store_account(&pubkey2, &account2);
let bank3 = Arc::new(new_from_parent(bank2));
⋮----
fn test_get_filtered_indexed_accounts_limit_exceeded() {
let (genesis_config, _mint_keypair) = create_genesis_config(500);
⋮----
account_indexes.indexes.insert(AccountIndex::ProgramId);
⋮----
account_indexes: Some(account_indexes),
⋮----
bank.store_account(&address, &account);
⋮----
fn test_get_filtered_indexed_accounts() {
⋮----
.get_filtered_indexed_accounts(
⋮----
assert_eq!(indexed_accounts.len(), 1);
assert_eq!(indexed_accounts[0], (address, account));
⋮----
let bank = Arc::new(new_from_parent(bank));
bank.store_account(&address, &new_account);
⋮----
assert_eq!(indexed_accounts[0], (address, new_account.clone()));
⋮----
|account| account.owner() == &program_id,
⋮----
assert!(indexed_accounts.is_empty());
⋮----
|account| account.owner() == &another_program_id,
⋮----
assert_eq!(indexed_accounts[0], (address, new_account));
⋮----
fn test_status_cache_ancestors() {
⋮----
let (parent, _bank_forks) = create_simple_test_arc_bank(500);
let bank1 = Arc::new(new_from_parent(parent));
⋮----
let bank = new_from_parent(bank);
⋮----
fn test_add_builtin() {
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee_no_rent(500);
⋮----
fn mock_vote_program_id() -> Pubkey {
⋮----
declare_process_instruction!(MockBuiltin, 1, |invoke_context| {
⋮----
assert!(bank.get_account(&mock_vote_program_id()).is_none());
bank.add_mockup_builtin(mock_vote_program_id(), MockBuiltin::vm);
assert!(bank.get_account(&mock_vote_program_id()).is_some());
⋮----
&mock_account.pubkey(),
⋮----
node_pubkey: mock_validator_identity.pubkey(),
⋮----
instructions[1].program_id = mock_vote_program_id();
⋮----
fn test_add_duplicate_static_program() {
⋮----
let slot = bank.slot().saturating_add(1);
⋮----
bank.add_mockup_builtin(solana_vote_program::id(), MockBuiltin::vm);
⋮----
.write()
⋮----
.insert(bank)
.clone_without_scheduler();
let vote_loader_account = bank.get_account(&solana_vote_program::id()).unwrap();
let new_vote_loader_account = bank.get_account(&solana_vote_program::id()).unwrap();
assert_eq!(vote_loader_account.data(), new_vote_loader_account.data());
⋮----
fn test_add_instruction_processor_for_existing_unrelated_accounts() {
⋮----
let mut bank = create_simple_test_bank(500);
⋮----
let stakes = bank.stakes_cache.stakes();
assert!(stakes.vote_accounts().as_ref().is_empty());
⋮----
assert!(bank.stakes_cache.stakes().stake_delegations().is_empty());
⋮----
.fetch_add(vote_account.lamports() + stake_account.lamports(), Relaxed);
bank.store_account(&vote_id, &vote_account);
bank.store_account(&stake_id, &stake_account);
⋮----
assert!(!stakes.vote_accounts().as_ref().is_empty());
⋮----
assert!(!bank.stakes_cache.stakes().stake_delegations().is_empty());
⋮----
bank.add_builtin(
⋮----
let old_hash = bank.calculate_accounts_lt_hash_for_tests();
bank.add_mockup_builtin(vote_id, MockBuiltin::vm);
bank.add_mockup_builtin(stake_id, MockBuiltin::vm);
⋮----
let new_hash = bank.calculate_accounts_lt_hash_for_tests();
assert_eq!(old_hash, new_hash);
⋮----
fn test_recent_blockhashes_sysvar() {
let (mut bank, _bank_forks) = create_simple_test_arc_bank(500);
⋮----
let bhq_account = bank.get_account(&sysvar::recent_blockhashes::id()).unwrap();
⋮----
from_account::<sysvar::recent_blockhashes::RecentBlockhashes, _>(&bhq_account).unwrap();
assert_eq!(recent_blockhashes.len(), i);
let most_recent_hash = recent_blockhashes.iter().next().unwrap().blockhash;
assert!(bank.is_hash_valid_for_age(&most_recent_hash, 0));
⋮----
fn test_blockhash_queue_sysvar_consistency() {
let (bank, _bank_forks) = create_simple_test_arc_bank(100_000);
⋮----
let bank_last_blockhash = bank.last_blockhash();
assert_eq!(sysvar_recent_blockhash, bank_last_blockhash);
⋮----
fn test_hash_internal_state_unchanged() {
⋮----
fn test_hash_internal_state_unchanged_with_ticks() {
⋮----
let bank1 = new_from_parent(bank);
⋮----
fn test_banks_leak() {
fn add_lotsa_stake_accounts(genesis_config: &mut GenesisConfig) {
⋮----
(0..LOTSA).for_each(|_| {
⋮----
genesis_config.add_account(
⋮----
create_lockup_stake_account(
⋮----
let (mut genesis_config, _) = create_genesis_config(100_000_000_000_000);
add_lotsa_stake_accounts(&mut genesis_config);
⋮----
error!(
⋮----
let pages_consumed = std::fs::read_to_string(format!("/proc/{pid}/statm"))
⋮----
.split_whitespace()
.next()
⋮----
error!("{num_banks} banks, sleeping for 5 sec");
⋮----
pub(in crate::bank) fn get_nonce_blockhash(bank: &Bank, nonce_pubkey: &Pubkey) -> Option<Hash> {
let account = bank.get_account(nonce_pubkey)?;
let nonce_data = get_nonce_data_from_account(&account)?;
Some(nonce_data.blockhash())
⋮----
pub(in crate::bank) fn get_nonce_data_from_account(
⋮----
let nonce_versions = StateMut::<nonce::versions::Versions>::state(account).ok()?;
if let nonce::state::State::Initialized(nonce_data) = nonce_versions.state() {
Some(nonce_data.clone())
⋮----
fn nonce_setup(
⋮----
let mut setup_ixs = vec![system_instruction::transfer(
⋮----
let nonce_authority = nonce_authority.unwrap_or_else(|| nonce_keypair.pubkey());
setup_ixs.extend_from_slice(&system_instruction::create_nonce_account(
&custodian_keypair.pubkey(),
&nonce_keypair.pubkey(),
⋮----
let message = Message::new(&setup_ixs, Some(&mint_keypair.pubkey()));
⋮----
bank.process_transaction(&setup_tx)?;
Ok((custodian_keypair, nonce_keypair))
⋮----
type NonceSetup = (Arc<Bank>, Keypair, Keypair, Keypair, Arc<RwLock<BankForks>>);
pub(in crate::bank) fn setup_nonce_with_bank<F>(
⋮----
let (mut genesis_config, mint_keypair) = create_genesis_config(supply_lamports);
⋮----
genesis_cfg_fn(&mut genesis_config);
⋮----
let (mut bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
let (custodian_keypair, nonce_keypair) = nonce_setup(
⋮----
Ok((
⋮----
pub(in crate::bank) fn next_durable_nonce(&self) -> DurableNonce {
let hash_queue = self.blockhash_queue.read().unwrap();
let last_blockhash = hash_queue.last_hash();
⋮----
fn test_assign_from_nonce_account_fail() {
let (bank, _bank_forks) = create_simple_test_arc_bank(100_000_000);
⋮----
let ix = system_instruction::assign(&nonce.pubkey(), &Pubkey::from([9u8; 32]));
let message = Message::new(&[ix], Some(&nonce.pubkey()));
⋮----
let expect = Err(TransactionError::InstructionError(
⋮----
assert_eq!(bank.process_transaction(&tx), expect);
⋮----
fn test_nonce_must_be_advanceable() {
let mut bank = create_simple_test_bank(100_000_000);
⋮----
let nonce_authority = nonce_keypair.pubkey();
let durable_nonce = DurableNonce::from_blockhash(&bank.last_blockhash());
⋮----
bank.store_account(&nonce_keypair.pubkey(), &nonce_account);
let ix = system_instruction::advance_nonce_account(&nonce_keypair.pubkey(), &nonce_authority);
let message = Message::new(&[ix], Some(&nonce_keypair.pubkey()));
let tx = Transaction::new(&[&nonce_keypair], message, *durable_nonce.as_hash());
⋮----
fn test_nonce_transaction() {
⋮----
setup_nonce_with_bank(
⋮----
let alice_pubkey = alice_keypair.pubkey();
let custodian_pubkey = custodian_keypair.pubkey();
let nonce_pubkey = nonce_keypair.pubkey();
assert_eq!(bank.get_balance(&custodian_pubkey), 4_750_000);
assert_eq!(bank.get_balance(&nonce_pubkey), 250_000);
let nonce_hash = get_nonce_blockhash(&bank, &nonce_pubkey).unwrap();
⋮----
Some(&custodian_pubkey),
⋮----
assert_eq!(bank.process_transaction(&nonce_tx), Ok(()));
⋮----
recent_message.recent_blockhash = bank.last_blockhash();
⋮----
.get_fee_for_message(&new_sanitized_message(recent_message))
⋮----
assert_eq!(bank.get_balance(&custodian_pubkey), expected_balance);
⋮----
assert_eq!(bank.get_balance(&alice_pubkey), 100_000);
let new_nonce = get_nonce_blockhash(&bank, &nonce_pubkey).unwrap();
assert_ne!(nonce_hash, new_nonce);
⋮----
let mut recent_message = nonce_tx.message.clone();
⋮----
assert_ne!(
⋮----
fn test_nonce_transaction_with_tx_wide_caps() {
⋮----
setup_nonce_with_bank(10_000_000, |_| {}, 5_000_000, 250_000, None, feature_set).unwrap();
⋮----
fn test_nonce_authority() {
⋮----
let bad_nonce_authority = bad_nonce_authority_keypair.pubkey();
let custodian_account = bank.get_account(&custodian_pubkey).unwrap();
debug!("alice: {alice_pubkey}");
debug!("custodian: {custodian_pubkey}");
debug!("nonce: {nonce_pubkey}");
debug!("nonce account: {:?}", bank.get_account(&nonce_pubkey));
debug!("cust: {custodian_account:?}");
⋮----
debug!("{nonce_tx:?}");
let initial_custodian_balance = custodian_account.lamports();
⋮----
fn test_nonce_payer() {
⋮----
debug!("cust: {:?}", bank.get_account(&custodian_pubkey));
⋮----
Some(&nonce_pubkey),
⋮----
fn test_nonce_payer_tx_wide_cap() {
⋮----
250_000 + FeeStructure::default().compute_fee_bins.last().unwrap().fee;
⋮----
fn test_nonce_fee_calculator_updates() {
let (mut genesis_config, mint_keypair) = create_genesis_config(1_000_000);
⋮----
nonce_setup(&bank, &mint_keypair, 500_000, 100_000, None).unwrap();
⋮----
.get_account(&nonce_pubkey)
.and_then(|acc| {
⋮----
match nonce_versions.ok()?.state() {
⋮----
Some((data.blockhash(), data.fee_calculator))
⋮----
bank.process_transaction(&nonce_tx).unwrap();
⋮----
assert_ne!(stored_nonce_hash, nonce_hash);
assert_ne!(stored_fee_calculator, fee_calculator);
⋮----
fn test_nonce_fee_calculator_updates_tx_wide_cap() {
⋮----
fn test_check_ro_durable_nonce_fails() {
⋮----
let account_metas = vec![
⋮----
bank = new_from_parent_with_fork_next_slot(bank, bank_forks.as_ref())
⋮----
let (_, lamports_per_signature) = bank.last_blockhash_and_lamports_per_signature();
⋮----
fn test_collect_balances() {
⋮----
bank0.store_account(&keypair.pubkey(), &keypair_account);
⋮----
bank0.store_account(&program_id, &program_account);
let instructions = vec![CompiledInstruction::new(1, &(), vec![0])];
⋮----
vec![program_id],
⋮----
let txs = vec![tx0, tx1];
let batch = bank0.prepare_batch_for_tests(txs.clone());
let balances = bank0.collect_balances(&batch);
assert_eq!(balances.len(), 2);
assert_eq!(balances[0], vec![8, 11, 1]);
assert_eq!(balances[1], vec![8, 0, 1]);
let txs: Vec<_> = txs.into_iter().rev().collect();
let batch = bank0.prepare_batch_for_tests(txs);
⋮----
assert_eq!(balances[0], vec![8, 0, 1]);
assert_eq!(balances[1], vec![8, 11, 1]);
⋮----
fn test_pre_post_transaction_balances() {
let (mut genesis_config, _mint_keypair) = create_genesis_config(500_000);
⋮----
let bank0 = new_from_parent_with_fork_next_slot(parent, bank_forks.as_ref());
⋮----
bank0.store_account(&keypair0.pubkey(), &keypair0_account);
bank0.store_account(&keypair1.pubkey(), &keypair1_account);
⋮----
let blockhash = bank0.last_blockhash();
⋮----
let txs = vec![tx0, tx1, tx2];
let lock_result = bank0.prepare_batch_for_tests(txs);
let (commit_results, balance_collector) = bank0.load_execute_and_commit_transactions(
⋮----
let (native_pre, native_post, _, _) = balance_collector.unwrap().into_vecs();
⋮----
assert_eq!(transaction_balances_set.pre_balances.len(), 3);
assert_eq!(transaction_balances_set.post_balances.len(), 3);
assert!(commit_results[0].was_executed_successfully());
⋮----
assert_matches!(commit_results[1], Err(TransactionError::AccountNotFound));
assert_eq!(transaction_balances_set.pre_balances[1], vec![0, 0, 1]);
assert_eq!(transaction_balances_set.post_balances[1], vec![0, 0, 1]);
⋮----
fn test_transaction_with_duplicate_accounts_in_instruction() {
⋮----
bank.store_account(&from_pubkey, &from_account);
bank.store_account(&to_pubkey, &to_account);
⋮----
Some(&mint_keypair.pubkey()),
⋮----
let result = bank.process_transaction(&tx);
assert_eq!(result, Ok(()));
assert_eq!(bank.get_balance(&from_pubkey), 80 * LAMPORTS_PER_SOL);
assert_eq!(bank.get_balance(&to_pubkey), 20 * LAMPORTS_PER_SOL);
⋮----
fn test_transaction_with_program_ids_passed_to_programs() {
⋮----
fn test_account_ids_after_program_ids() {
⋮----
tx.message.account_keys.push(solana_pubkey::new_rand());
⋮----
let account = bank.get_account(&solana_vote_program::id()).unwrap();
info!("account: {account:?}");
assert!(account.executable());
⋮----
fn test_incinerator() {
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee_no_rent(1_000_000_000_000);
⋮----
let pre_capitalization = bank.capitalization();
let burn_amount = bank.get_minimum_balance_for_rent_exemption(0) - 1;
assert_eq!(bank.get_balance(&incinerator::id()), 0);
bank.transfer(burn_amount, &mint_keypair, &incinerator::id())
⋮----
assert_eq!(bank.get_balance(&incinerator::id()), burn_amount);
⋮----
assert_eq!(bank.capitalization(), pre_capitalization - burn_amount);
⋮----
fn test_duplicate_account_key() {
⋮----
tx.message.account_keys.push(from_pubkey);
⋮----
assert_eq!(result, Err(TransactionError::AccountLoadedTwice));
⋮----
fn test_process_transaction_with_too_many_account_locks() {
⋮----
let transaction_account_lock_limit = bank.get_transaction_account_lock_limit();
while tx.message.account_keys.len() <= transaction_account_lock_limit {
⋮----
assert_eq!(result, Err(TransactionError::TooManyAccountLocks));
⋮----
fn test_program_id_as_payer() {
⋮----
info!(
⋮----
assert_eq!(tx.message.account_keys.len(), 4);
tx.message.account_keys.clear();
tx.message.account_keys.push(solana_vote_program::id());
tx.message.account_keys.push(mint_keypair.pubkey());
⋮----
tx.message.account_keys.push(to_pubkey);
⋮----
tx.message.instructions[0].accounts.clear();
tx.message.instructions[0].accounts.push(2);
tx.message.instructions[0].accounts.push(3);
⋮----
assert_eq!(result, Err(TransactionError::SanitizeFailure));
⋮----
fn test_ref_account_key_after_program_id() {
⋮----
assert_eq!(tx.message.account_keys.len(), 5);
tx.message.instructions[0].accounts.remove(0);
tx.message.instructions[0].accounts.push(4);
⋮----
fn test_fuzz_instructions() {
⋮----
let bank = create_simple_test_bank(1_000_000_000);
⋮----
.enumerate()
.map(|i| {
⋮----
let name = format!("program{i:?}");
⋮----
name.as_str(),
⋮----
(key, name.as_bytes().to_vec())
⋮----
.collect();
⋮----
let balance = if rng().random_ratio(9, 10) {
let lamports = if rng().random_ratio(1, 5) {
rng().random_range(0..10)
⋮----
rng().random_range(20..100)
⋮----
let space = rng().random_range(0..10);
⋮----
bank.store_account(&key, &account);
⋮----
let num_keys = if rng().random_ratio(1, 5) {
rng().random_range(0..max_keys)
⋮----
rng().random_range(1..4)
⋮----
let num_instructions = rng().random_range(0..max_keys - num_keys);
let mut account_keys: Vec<_> = if rng().random_ratio(1, 5) {
⋮----
let idx = rng().random_range(0..keys.len());
⋮----
idx = rng().random_range(0..keys.len());
if !inserted.contains(&idx) {
⋮----
inserted.insert(idx);
⋮----
let num_accounts_to_pass = rng().random_range(0..num_keys);
⋮----
.map(|_| rng().random_range(0..num_keys))
⋮----
let program_index: u8 = rng().random_range(0..num_keys);
if rng().random_ratio(4, 5) {
let programs_index = rng().random_range(0..program_keys.len());
⋮----
vec![]
⋮----
let account_keys_len = std::cmp::max(account_keys.len(), 2);
let num_signatures = if rng().random_ratio(1, 5) {
rng().random_range(0..account_keys_len + 10)
⋮----
rng().random_range(1..account_keys_len)
⋮----
let num_required_signatures = if rng().random_ratio(1, 5) {
rng().random_range(0..account_keys_len + 10) as u8
⋮----
rng().random_range(1..std::cmp::max(2, num_signatures)) as u8
⋮----
let num_readonly_signed_accounts = if rng().random_ratio(1, 5) {
rng().random_range(0..account_keys_len) as u8
⋮----
rng().random_range(0..max)
⋮----
if rng().random_ratio(1, 5) || (num_required_signatures as usize) >= account_keys_len {
⋮----
rng().random_range(0..account_keys_len - num_required_signatures as usize) as u8
⋮----
recent_blockhash: bank.last_blockhash(),
⋮----
signatures: vec![Signature::default(); num_signatures],
⋮----
assert_eq!(bank.get_balance(key), *balance);
⋮----
let account = bank.get_account(key).unwrap();
⋮----
assert_eq!(account.data(), name);
⋮----
info!("result: {result:?}");
let result_key = format!("{result:?}");
*results.entry(result_key).or_insert(0) += 1;
⋮----
info!("results: {results:?}");
⋮----
fn test_bank_hash_consistency() {
⋮----
vec![],
⋮----
Some(Pubkey::from([42; 32])),
⋮----
Some(feature_set),
⋮----
goto_end_of_slot(Arc::clone(&bank));
⋮----
assert_eq!(bank.epoch(), 0);
⋮----
assert_eq!(bank.epoch(), 1);
⋮----
assert_eq!(bank.epoch(), 2);
⋮----
fn test_same_program_id_uses_unique_executable_accounts() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(50000);
⋮----
program2_account.set_executable(true);
bank.store_account(&program2_pubkey, &program2_account);
let instruction = Instruction::new_with_bincode(program2_pubkey, &10, vec![]);
⋮----
&[instruction.clone(), instruction],
⋮----
assert!(bank.process_transaction(&tx).is_ok());
assert_eq!(6, bank.get_account(&program1_pubkey).unwrap().data().len());
assert_eq!(1, bank.get_account(&program2_pubkey).unwrap().data().len());
⋮----
fn test_clean_nonrooted() {
⋮----
let (genesis_config, _mint_keypair) = create_genesis_config(1_000_000_000);
⋮----
info!("pubkey0: {pubkey0}");
info!("pubkey1: {pubkey1}");
⋮----
goto_end_of_slot(bank0.clone());
⋮----
bank0.force_flush_accounts_cache();
⋮----
let bank1 = Arc::new(Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 1));
test_utils::deposit(&bank1, &pubkey0, some_lamports).unwrap();
goto_end_of_slot(bank1.clone());
⋮----
bank1.flush_accounts_cache_slot_for_tests();
bank1.print_accounts_stats();
⋮----
test_utils::deposit(&bank2, &pubkey1, some_lamports).unwrap();
bank2.store_account(&pubkey0, &account_zero);
goto_end_of_slot(bank2.clone());
⋮----
bank2.print_accounts_stats();
drop(bank1);
bank2.clean_accounts_for_tests();
⋮----
test_utils::deposit(&bank3, &pubkey1, some_lamports + 1).unwrap();
goto_end_of_slot(bank3.clone());
⋮----
bank3.force_flush_accounts_cache();
bank3.clean_accounts_for_tests();
bank3.rc.accounts.accounts_db.assert_ref_count(&pubkey0, 2);
assert!(bank3
⋮----
bank3.print_accounts_stats();
⋮----
fn test_shrink_candidate_slots_cached() {
⋮----
test_utils::deposit(&bank1, &pubkey1, some_lamports).unwrap();
test_utils::deposit(&bank1, &pubkey2, some_lamports).unwrap();
⋮----
bank1.force_flush_accounts_cache();
let bank2 = Arc::new(new_from_parent(bank1));
⋮----
bank2.store_account(&pubkey0, &account0);
⋮----
assert_eq!(bank2.shrink_candidate_slots(), 2);
⋮----
.map(|slot| {
⋮----
.alive_account_count_in_slot(slot)
⋮----
assert_eq!(bank2.shrink_candidate_slots(), 0);
assert_eq!(alive_counts, vec![12, 1, 6]);
⋮----
fn test_add_builtin_no_overwrite() {
⋮----
let (parent_bank, _bank_forks) = create_simple_test_arc_bank(100_000);
⋮----
assert_eq!(bank.get_account_modified_slot(&program_id), None);
⋮----
.add_mockup_builtin(program_id, MockBuiltin::vm);
assert_eq!(bank.get_account_modified_slot(&program_id).unwrap().1, slot);
let mut bank = Arc::new(new_from_parent(bank));
⋮----
fn test_add_builtin_loader_no_overwrite() {
⋮----
assert_eq!(bank.get_account_modified_slot(&loader_id), None);
⋮----
.add_mockup_builtin(loader_id, MockBuiltin::vm);
assert_eq!(bank.get_account_modified_slot(&loader_id).unwrap().1, slot);
⋮----
fn test_add_builtin_account() {
⋮----
activate_all_features(&mut genesis_config);
⋮----
add_root_and_flush_write_cache(&bank.parent().unwrap());
⋮----
|| bank.add_builtin_account("mock_program", &program_id),
⋮----
assert_eq!(old + 1, new);
⋮----
|| bank.add_builtin_account("mock_program v2", &program_id),
⋮----
fn add_root_and_flush_write_cache(bank: &Bank) {
bank.rc.accounts.add_root(bank.slot());
bank.flush_accounts_cache_slot_for_tests()
⋮----
fn test_add_builtin_account_inherited_cap_while_replacing() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(100_000);
⋮----
bank.add_builtin_account("mock_program", &program_id);
⋮----
bank.withdraw(&mint_keypair.pubkey(), 10).unwrap();
⋮----
test_utils::deposit(&bank, &program_id, 10).unwrap();
⋮----
bank.add_builtin_account("mock_program v2", &program_id);
⋮----
fn test_add_builtin_account_squatted_while_not_replacing() {
⋮----
fn test_add_builtin_account_after_frozen() {
⋮----
let program_id = Pubkey::from_str("CiXgo2KHKSDmDnV1F6B69eWFgNAPiSBjjYvfB4cvRNre").unwrap();
⋮----
fn test_add_precompiled_account() {
⋮----
|| bank.add_precompiled_account(&program_id),
⋮----
fn test_add_precompiled_account_inherited_cap_while_replacing() {
⋮----
bank.add_precompiled_account(&program_id);
⋮----
fn test_add_precompiled_account_squatted_while_not_replacing() {
⋮----
fn test_add_precompiled_account_after_frozen() {
⋮----
fn test_reconfigure_token2_native_mint() {
⋮----
create_genesis_config_with_leader(5, &solana_pubkey::new_rand(), 0).genesis_config;
⋮----
assert_eq!(bank.get_balance(&token::native_mint::id()), 1000000000);
let native_mint_account = bank.get_account(&token::native_mint::id()).unwrap();
assert_eq!(native_mint_account.data().len(), 82);
assert_eq!(native_mint_account.owner(), &token::id());
⋮----
fn test_bank_load_program() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee(1_000_000_000);
⋮----
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
let mut file = File::open("../programs/bpf_loader/test_elfs/out/noop_aligned.so").unwrap();
⋮----
file.read_to_end(&mut elf).unwrap();
⋮----
program_account.set_executable(true);
program_account.set_rent_epoch(1);
⋮----
programdata_data_offset + elf.len(),
⋮----
.set_state(&UpgradeableLoaderState::ProgramData {
⋮----
programdata_account.data_as_mut_slice()[programdata_data_offset..].copy_from_slice(&elf);
programdata_account.set_rent_epoch(1);
bank.store_account_and_update_capitalization(&program_key, &program_account);
bank.store_account_and_update_capitalization(&programdata_key, &programdata_account);
⋮----
let invocation_message = Message::new(&[instruction], Some(&mint_keypair.pubkey()));
let binding = mint_keypair.insecure_clone();
⋮----
invocation_message.clone(),
⋮----
assert!(bank.process_transaction(&transaction).is_ok());
⋮----
.read()
⋮----
let [program] = program_cache.get_slot_versions_for_tests(&program_key) else {
panic!();
⋮----
assert_matches!(program.program, ProgramCacheEntryType::Loaded(_));
⋮----
fn test_bpf_loader_upgradeable_deploy_with_max_len(formalize_loaded_transaction_data_size: bool) {
⋮----
bank.deactivate_feature(&feature_set::formalize_loaded_transaction_data_size::id());
⋮----
let mut bank_client = BankClient::new_shared(bank.clone());
⋮----
&[program_keypair.pubkey().as_ref()],
⋮----
let instruction = Instruction::new_with_bytes(program_keypair.pubkey(), &[], Vec::new());
⋮----
let slot_versions = program_cache.get_slot_versions_for_tests(&program_keypair.pubkey());
assert!(slot_versions.is_empty());
⋮----
.advance_slot(1, bank_forks.as_ref(), &mint_keypair.pubkey())
⋮----
.expect("file open failed");
⋮----
let program_len = elf.len();
⋮----
bank.get_minimum_balance_for_rent_exemption(UpgradeableLoaderState::size_of_program());
let min_buffer_balance = bank.get_minimum_balance_for_rent_exemption(
⋮----
let min_programdata_balance = bank.get_minimum_balance_for_rent_exemption(
⋮----
UpgradeableLoaderState::size_of_buffer(elf.len()),
⋮----
.set_state(&UpgradeableLoaderState::Buffer {
authority_address: Some(upgrade_authority_keypair.pubkey()),
⋮----
.data_as_mut_slice()
.get_mut(UpgradeableLoaderState::size_of_buffer_metadata()..)
⋮----
.copy_from_slice(&elf);
⋮----
UpgradeableLoaderState::size_of_programdata(elf.len()),
⋮----
bank.store_account(&program_keypair.pubkey(), &program_account);
⋮----
assert_eq!(slot_versions.len(), 1);
assert_eq!(slot_versions[0].deployment_slot, bank.slot());
assert_eq!(slot_versions[0].effective_slot, bank.slot());
assert!(matches!(
⋮----
bank.store_account(&buffer_address, &buffer_account);
⋮----
let message = Message::new(&[instruction], Some(&mint_keypair.pubkey()));
let transaction = Transaction::new(&[&binding], message, bank.last_blockhash());
⋮----
let slot_versions = program_cache.get_slot_versions_for_tests(&buffer_address);
⋮----
let fee_calculator = genesis_config.fee_rate_governor.create_fee_calculator();
⋮----
.saturating_add(min_programdata_balance)
.saturating_sub(min_buffer_balance.saturating_add(deploy_fees));
⋮----
&payer_keypair.pubkey(),
⋮----
payer_base_balance.saturating_add(min_payer_balance),
⋮----
bank.store_account(&program_keypair.pubkey(), &AccountSharedData::default());
bank.store_account(&programdata_address, &AccountSharedData::default());
⋮----
&program_keypair.pubkey(),
⋮----
&upgrade_authority_keypair.pubkey(),
⋮----
elf.len(),
⋮----
.unwrap(),
Some(&payer_keypair.pubkey()),
⋮----
assert!(bank_client
⋮----
assert_eq!(bank.get_balance(&buffer_address), 0);
assert_eq!(None, bank.get_account(&buffer_address));
let post_program_account = bank.get_account(&program_keypair.pubkey()).unwrap();
assert_eq!(post_program_account.lamports(), min_program_balance);
assert_eq!(post_program_account.owner(), &bpf_loader_upgradeable::id());
⋮----
let state: UpgradeableLoaderState = post_program_account.state().unwrap();
⋮----
let post_programdata_account = bank.get_account(&programdata_address).unwrap();
assert_eq!(post_programdata_account.lamports(), min_programdata_balance);
⋮----
let state: UpgradeableLoaderState = post_programdata_account.state().unwrap();
⋮----
.data()
.get(UpgradeableLoaderState::size_of_programdata_metadata()..)
⋮----
assert_eq!(*elf.get(i).unwrap(), *byte);
⋮----
let transaction = Transaction::new(&[&binding], invocation_message, bank.last_blockhash());
⋮----
assert_eq!(slot_versions[0].deployment_slot, bank.slot() - 1);
⋮----
bank.clear_signatures();
⋮----
max_data_len: elf.len(),
⋮----
vec![
⋮----
bank.store_account(&programdata_address, &programdata_account);
⋮----
bank.store_account(&buffer_address, &AccountSharedData::default());
⋮----
min_program_balance.saturating_sub(1),
⋮----
*instructions.get_mut(0).unwrap() = system_instruction::create_account(
⋮----
(UpgradeableLoaderState::size_of_program() as u64).saturating_add(1),
⋮----
(UpgradeableLoaderState::size_of_program() as u64).saturating_sub(1),
⋮----
deploy_fees.saturating_add(min_program_balance),
⋮----
elf.len().saturating_sub(1),
⋮----
let mut modified_buffer_account = buffer_account.clone();
modified_buffer_account.set_lamports(u64::MAX / 2);
bank.store_account(&buffer_address, &modified_buffer_account);
⋮----
fn truncate_data(account: &mut AccountSharedData, len: usize) {
let mut data = account.data().to_vec();
data.truncate(len);
account.set_data(data);
⋮----
truncate_data(
⋮----
truncate_data(&mut modified_buffer_account, 5);
⋮----
authority_address: Some(buffer_address),
⋮----
fn test_compute_active_feature_set() {
let (bank0, _bank_forks) = create_simple_test_arc_bank(100_000);
⋮----
feature_set.inactive_mut().insert(test_feature);
bank.feature_set = Arc::new(feature_set.clone());
let (feature_set, new_activations) = bank.compute_active_feature_set(true);
assert!(new_activations.is_empty());
assert!(!feature_set.is_active(&test_feature));
test_utils::deposit(&bank, &test_feature, 42).unwrap();
⋮----
assert_eq!(feature.activated_at, None);
bank.store_account(&test_feature, &feature::create_account(&feature, 42));
let feature = feature::from_account(&bank.get_account(&test_feature).expect("get_account"))
.expect("from_account");
⋮----
let (feature_set, new_activations) = bank.compute_active_feature_set(false);
⋮----
let (_feature_set, new_activations) = bank.compute_active_feature_set(true);
assert_eq!(new_activations.len(), 1);
assert!(new_activations.contains(&test_feature));
bank.compute_and_apply_new_feature_activations();
⋮----
assert_eq!(feature.activated_at, Some(1));
⋮----
assert!(feature_set.is_active(&test_feature));
⋮----
fn test_reserved_account_keys() {
⋮----
fn test_block_limits() {
⋮----
assert!(!bank
⋮----
bank.compute_and_apply_features_after_snapshot_restore();
⋮----
let (mut genesis_config, _keypair) = create_genesis_config(100_000);
⋮----
activate_feature(
⋮----
fn test_program_replacement() {
let mut bank = create_simple_test_bank(0);
⋮----
bank.store_account_and_update_capitalization(
⋮----
assert_eq!(bank.get_balance(&old_address), 100);
⋮----
bank.store_account_and_update_capitalization(&new_address, &new_program_account);
assert_eq!(bank.get_balance(&new_address), 123);
let original_capitalization = bank.capitalization();
bank.replace_program_account(&old_address, &new_address, "bank-apply_program_replacement");
assert_eq!(bank.get_balance(&new_address), 0);
assert_eq!(bank.get_account(&old_address), Some(new_program_account));
assert_eq!(bank.capitalization(), original_capitalization - 100);
⋮----
fn min_rent_exempt_balance_for_sysvars(bank: &Bank, sysvar_ids: &[Pubkey]) -> u64 {
⋮----
.map(|sysvar_id| {
trace!("min_rent_excempt_balance_for_sysvars: {sysvar_id}");
bank.get_minimum_balance_for_rent_exemption(
bank.get_account(sysvar_id).unwrap().data().len(),
⋮----
.sum()
⋮----
fn test_adjust_sysvar_balance_for_rent() {
⋮----
assert_eq!(smaller_sample_sysvar.lamports(), 1);
bank.adjust_sysvar_balance_for_rent(&mut smaller_sample_sysvar);
⋮----
smaller_sample_sysvar.data().len() + 1,
⋮----
bank.adjust_sysvar_balance_for_rent(&mut bigger_sample_sysvar);
assert!(smaller_sample_sysvar.lamports() < bigger_sample_sysvar.lamports());
let excess_lamports = smaller_sample_sysvar.lamports() + 999;
smaller_sample_sysvar.set_lamports(excess_lamports);
⋮----
assert_eq!(smaller_sample_sysvar.lamports(), excess_lamports);
⋮----
fn test_update_clock_timestamp() {
⋮----
} = create_genesis_config_with_leader(5, &leader_pubkey, 3);
⋮----
bank = new_from_parent(Arc::new(bank));
⋮----
bank.update_clock(None);
⋮----
update_vote_account_timestamp(
⋮----
slot: bank.slot(),
timestamp: bank.unix_timestamp_from_genesis() - 1,
⋮----
&voting_keypair.pubkey(),
⋮----
timestamp: bank.unix_timestamp_from_genesis(),
⋮----
timestamp: bank.unix_timestamp_from_genesis() + 1,
⋮----
fn poh_estimate_offset(bank: &Bank) -> Duration {
let mut epoch_start_slot = bank.epoch_schedule.get_first_slot_in_epoch(bank.epoch());
if epoch_start_slot == bank.slot() {
⋮----
.get_first_slot_in_epoch(bank.epoch() - 1);
⋮----
bank.slot().saturating_sub(epoch_start_slot) as u32
⋮----
fn test_timestamp_slow() {
fn max_allowable_delta_since_epoch(bank: &Bank, max_allowable_drift: u32) -> i64 {
let poh_estimate_offset = poh_estimate_offset(bank);
(poh_estimate_offset.as_secs()
+ (poh_estimate_offset * max_allowable_drift / 100).as_secs()) as i64
⋮----
let recent_timestamp: UnixTimestamp = bank.unix_timestamp_from_genesis();
⋮----
((slot_duration * MAX_ALLOWABLE_DRIFT_PERCENTAGE_SLOW_V2 * 32) / 100).as_secs() as i64 + 1;
⋮----
assert_eq!(bank.clock().epoch_start_timestamp, recent_timestamp);
⋮----
fn test_timestamp_fast() {
⋮----
- (poh_estimate_offset * max_allowable_drift / 100).as_secs()) as i64
⋮----
fn test_program_is_native_loader(formalize_loaded_transaction_data_size: bool) {
⋮----
bank.activate_feature(&feature_set::formalize_loaded_transaction_data_size::id());
⋮----
let err = bank.process_transaction(&tx).unwrap_err();
⋮----
assert_eq!(err, TransactionError::ProgramAccountNotFound);
⋮----
fn test_invoke_non_program_account_owned_by_a_builtin(
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(10000000);
⋮----
&created_account_keypair.pubkey(),
⋮----
assert_eq!(bank.process_transaction(&tx), Err(expected_error),);
⋮----
fn test_debug_bank() {
let (genesis_config, _mint_keypair) = create_genesis_config(50000);
⋮----
let debug = format!("{bank:#?}");
assert!(!debug.is_empty());
⋮----
enum AcceptableScanResults {
⋮----
fn test_store_scan_consistency<F>(
⋮----
// Set up initial bank
⋮----
create_genesis_config_with_leader(10, &solana_pubkey::new_rand(), 374_999_998_287_840)
⋮----
bank0.set_callback(drop_callback);
// Set up pubkeys to write to
⋮----
.take(total_pubkeys)
⋮----
// Write accounts to the store
⋮----
bank0.store_account(key, &starting_account);
⋮----
// Set aside a subset of accounts to modify
⋮----
.take(total_pubkeys_to_modify)
⋮----
// Thread that runs scan and constantly checks for
// consistency
let pubkeys_to_modify_ = pubkeys_to_modify.clone();
// Channel over which the bank to scan is sent
⋮----
) = bounded(1);
⋮----
) = unbounded();
⋮----
let exit = exit.clone();
let num_banks_scanned = num_banks_scanned.clone();
⋮----
.name("scan".to_string())
⋮----
info!("starting scan iteration");
if exit.load(Relaxed) {
info!("scan exiting");
⋮----
bank_to_scan_receiver.recv_timeout(Duration::from_millis(10))
⋮----
info!("scanning program accounts for slot {}", bank_to_scan.slot());
⋮----
bank_to_scan.get_program_accounts(&program_id, &ScanConfig::default());
let _ = scan_finished_sender.send(bank_to_scan.bank_id());
num_banks_scanned.fetch_add(1, Relaxed);
match (&acceptable_scan_results, accounts_result.is_err()) {
⋮----
assert!(accounts_result.is_ok())
⋮----
// Should never see empty accounts because no slot ever deleted
// any of the original accounts, and the scan should reflect the
// account state at some frozen slot `X` (no partial updates).
⋮----
assert!(!accounts.is_empty());
⋮----
let account_balance = account.lamports();
if pubkeys_to_modify_.contains(&pubkey) {
target_accounts_found.insert(pubkey);
⋮----
assert_eq!(account_balance, expected_lamports);
⋮----
// All pubkeys in the specified set should have the same balance
expected_lamports = Some(account_balance);
⋮----
// Should've found all the accounts, i.e. no partial cleans should
assert_eq!(target_accounts_found.len(), total_pubkeys_to_modify);
⋮----
.name("update".to_string())
⋮----
update_f(
⋮----
if num_banks_scanned.load(Relaxed) > min_expected_number_of_scans {
⋮----
exit.store(true, Relaxed);
scan_thread.join().unwrap();
update_thread.join().unwrap();
assert!(remaining_loops > 0, "test timed out");
⋮----
fn test_store_scan_consistency_unrooted() {
let (pruned_banks_sender, pruned_banks_receiver) = unbounded();
⋮----
test_store_scan_consistency(
⋮----
let mut current_minor_fork_bank = current_major_fork_bank.clone();
⋮----
let lamports = current_minor_fork_bank.slot() + starting_lamports + 1;
⋮----
.chunks(pubkeys_to_modify.len() / num_new_banks)
⋮----
let slot = current_minor_fork_bank.slot() + 2;
⋮----
current_minor_fork_bank.store_account(key, &account);
⋮----
current_minor_fork_bank.freeze();
⋮----
current_minor_fork_bank.slot() - 1,
⋮----
let lamports = current_major_fork_bank.slot() + starting_lamports + 1;
⋮----
for key in pubkeys_to_modify.iter() {
current_major_fork_bank.store_account(key, &account);
⋮----
if bank_to_scan_sender.send(current_minor_fork_bank).is_err() {
⋮----
current_major_fork_bank.freeze();
current_major_fork_bank.squash();
current_major_fork_bank.force_flush_accounts_cache();
current_major_fork_bank.clean_accounts_for_tests();
pruned_banks_request_handler.handle_request(&current_major_fork_bank);
⋮----
Some(Box::new(SendDroppedBankCallback::new(pruned_banks_sender))),
⋮----
fn test_store_scan_consistency_root() {
⋮----
let mut current_bank = bank0.clone();
⋮----
let lamports_this_round = current_bank.slot() + starting_lamports + 1;
⋮----
current_bank.store_account(key, &account);
⋮----
current_bank.freeze();
if bank_to_scan_sender.send(prev_bank).is_err() {
⋮----
current_bank.squash();
if current_bank.slot() % 2 == 0 {
current_bank.force_flush_accounts_cache();
current_bank.clean_accounts();
⋮----
prev_bank = current_bank.clone();
let slot = current_bank.slot() + 1;
⋮----
pruned_banks_request_handler.handle_request(&current_bank);
⋮----
fn setup_banks_on_fork_to_remove(
⋮----
assert!(pubkeys_to_modify.len() > 1);
⋮----
assert!(num_banks_on_fork >= 2);
assert!(step_size >= 2);
let pubkeys_to_modify: Vec<Pubkey> = pubkeys_to_modify.iter().cloned().collect();
let pubkeys_to_modify_per_slot = (pubkeys_to_modify.len() / step_size).max(1);
for _ in (0..num_banks_on_fork).step_by(step_size) {
⋮----
let slot = bank_at_fork_tip.slot() + 1;
⋮----
lamports_this_round = bank_at_fork_tip.bank_id() + starting_lamports + 1;
⋮----
let key = pubkeys_to_modify[pubkey_index_to_modify % pubkeys_to_modify.len()];
bank_at_fork_tip.store_account(&key, &account);
⋮----
bank_at_fork_tip.freeze();
slots_on_fork.push((bank_at_fork_tip.slot(), bank_at_fork_tip.bank_id()));
⋮----
let ancestors: Vec<(Slot, usize)> = slots_on_fork.iter().map(|(s, _)| (*s, 0)).collect();
⋮----
fn test_remove_unrooted_before_scan() {
⋮----
let (bank_at_fork_tip, slots_on_fork, ancestors) = setup_banks_on_fork_to_remove(
⋮----
pubkeys_to_modify.clone(),
⋮----
for k in pubkeys_to_modify.iter() {
assert!(bank_at_fork_tip.load_slow(&ancestors, k).is_some());
⋮----
bank_at_fork_tip.remove_unrooted_slots(&slots_on_fork);
⋮----
assert!(bank_at_fork_tip.load_slow(&ancestors, k).is_none());
⋮----
if bank_to_scan_sender.send(bank_at_fork_tip.clone()).is_err() {
⋮----
let finished_scan_bank_id = scan_finished_receiver.recv();
if finished_scan_bank_id.is_err() {
⋮----
assert_eq!(finished_scan_bank_id.unwrap(), bank_at_fork_tip.bank_id());
⋮----
fn test_remove_unrooted_scan_then_recreate_same_slot_before_scan() {
⋮----
let mut prev_bank = bank0.clone();
⋮----
info!("setting up banks elapsed: {}", start.elapsed().as_millis());
if prev_bank.slot() != 0 {
⋮----
if bank_to_scan_sender.send(prev_bank.clone()).is_err() {
⋮----
assert_eq!(finished_scan_bank_id.unwrap(), prev_bank.bank_id());
⋮----
fn test_remove_unrooted_scan_interleaved_with_remove_unrooted_slots() {
⋮----
let slot_to_remove = *slots_on_fork.last().unwrap();
bank_at_fork_tip.remove_unrooted_slots(&[slot_to_remove]);
⋮----
bank_at_fork_tip.remove_unrooted_slots(&[(slot, bank_id)]);
⋮----
fn test_get_inflation_start_slot_devnet_testnet() {
⋮----
} = create_genesis_config_with_leader(42, &solana_pubkey::new_rand(), 42);
⋮----
.remove(&feature_set::pico_inflation::id())
⋮----
.remove(&feature_set::full_inflation::devnet_and_testnet::id())
⋮----
for pair in feature_set::FULL_INFLATION_FEATURE_PAIRS.iter() {
genesis_config.accounts.remove(&pair.vote_id).unwrap();
genesis_config.accounts.remove(&pair.enable_id).unwrap();
⋮----
let mut bank = new_from_parent(Arc::new(bank));
⋮----
assert_eq!(bank.get_inflation_start_slot(), 0);
assert_eq!(bank.slot(), 2);
⋮----
activated_at: Some(1),
⋮----
bank.feature_set = Arc::new(bank.compute_active_feature_set(true).0);
assert_eq!(bank.get_inflation_start_slot(), 1);
⋮----
assert_eq!(bank.slot(), 3);
⋮----
activated_at: Some(2),
⋮----
assert_eq!(bank.get_inflation_start_slot(), 2);
⋮----
activated_at: Some(3),
⋮----
fn test_get_inflation_start_slot_mainnet() {
⋮----
assert_eq!(bank.slot(), 4);
⋮----
activated_at: Some(bank.slot()),
⋮----
fn test_get_inflation_num_slots_with_activations() {
⋮----
assert_eq!(bank.get_inflation_num_slots(), 0);
⋮----
assert_eq!(bank.get_inflation_num_slots(), 2 * slots_per_epoch);
let pico_inflation_activation_slot = bank.slot();
⋮----
activated_at: Some(pico_inflation_activation_slot),
⋮----
assert_eq!(bank.get_inflation_num_slots(), slots_per_epoch);
⋮----
let full_inflation_activation_slot = bank.slot();
⋮----
activated_at: Some(full_inflation_activation_slot),
⋮----
fn test_get_inflation_num_slots_already_activated() {
⋮----
fn test_stake_vote_account_validity() {
⋮----
check_stake_vote_account_validity(
⋮----
|bank: &Bank| bank._load_vote_and_stake_accounts(&thread_pool, null_tracer()),
⋮----
fn test_epoch_schedule_from_genesis_config() {
⋮----
let validator_keypairs = vec![&validator_vote_keypairs0, &validator_vote_keypairs1];
⋮----
} = create_genesis_config_with_vote_accounts(
⋮----
vec![LAMPORTS_PER_SOL; 2],
⋮----
assert_eq!(bank.epoch_schedule(), &genesis_config.epoch_schedule);
⋮----
fn check_stake_vote_account_validity<F>(check_owner_change: bool, load_vote_and_stake_accounts: F)
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config_with_vote_accounts(
⋮----
let vote_and_stake_accounts = load_vote_and_stake_accounts(&bank);
assert_eq!(vote_and_stake_accounts.len(), 2);
⋮----
.get_account(&validator_vote_keypairs0.vote_keypair.pubkey())
.unwrap_or_default();
let original_lamports = vote_account.lamports();
vote_account.set_lamports(0);
⋮----
&validator_vote_keypairs0.vote_keypair.pubkey(),
⋮----
vote_account.set_lamports(original_lamports);
vote_account.set_owner(bogus_vote_program);
⋮----
assert_eq!(bank.vote_accounts().len(), 1);
⋮----
.get_account(&validator_vote_keypairs1.stake_keypair.pubkey())
⋮----
stake_account.set_owner(bogus_stake_program);
⋮----
&validator_vote_keypairs1.stake_keypair.pubkey(),
⋮----
fn test_vote_epoch_panic() {
⋮----
bootstrap_validator_stake_lamports(),
⋮----
let vote_keypair = keypair_from_seed(&[1u8; 32]).unwrap();
⋮----
setup_ixs.extend(vote_instruction::create_account_with_config(
⋮----
authorized_withdrawer: mint_keypair.pubkey(),
⋮----
setup_ixs.push(vote_instruction::withdraw(
⋮----
setup_ixs.push(system_instruction::transfer(
⋮----
let result = bank.process_transaction(&Transaction::new(
⋮----
Message::new(&setup_ixs, Some(&mint_keypair.pubkey())),
⋮----
assert!(result.is_ok());
⋮----
fn test_tx_log_order(relax_intrabatch_account_locks: bool) {
⋮----
*bank.transaction_log_collector_config.write().unwrap() = TransactionLogCollectorConfig {
⋮----
bank.transfer(100, &mint_keypair, &sender0.pubkey())
⋮----
bank.transfer(100, &mint_keypair, &sender1.pubkey())
⋮----
let batch = bank.prepare_batch_for_tests(txs);
⋮----
assert_eq!(commit_results.len(), 3);
⋮----
assert!(commit_results[0]
⋮----
assert!(commit_results[1].is_ok());
assert!(commit_results[1]
⋮----
assert!(commit_results[2].is_ok());
⋮----
assert!(commit_results[2].is_err());
⋮----
let stored_logs = &bank.transaction_log_collector.read().unwrap().logs;
⋮----
.find(|transaction_log_info| transaction_log_info.signature == success_sig)
⋮----
assert!(success_log_info.result.is_ok());
let success_log = success_log_info.log_messages.clone().pop().unwrap();
assert!(success_log.contains(&"success".to_string()));
⋮----
.find(|transaction_log_info| transaction_log_info.signature == failure_sig)
⋮----
assert!(failure_log_info.result.is_err());
let failure_log = failure_log_info.log_messages.clone().pop().unwrap();
assert!(failure_log.contains(&"failed".to_string()));
⋮----
fn test_tx_return_data() {
⋮----
Some(0),
Some(MAX_RETURN_DATA / 2),
Some(MAX_RETURN_DATA - 1),
⋮----
usize::to_le_bytes(index).to_vec()
⋮----
let txs = vec![Transaction::new_signed_with_payer(
⋮----
let return_data = commit_results[0].as_ref().unwrap().return_data.clone();
⋮----
let return_data = return_data.unwrap();
assert_eq!(return_data.program_id, mock_program_id);
let mut expected_data = vec![0u8; index + 1];
⋮----
assert_eq!(return_data.data, expected_data);
⋮----
assert!(return_data.is_none());
⋮----
fn test_get_largest_accounts() {
⋮----
create_genesis_config_with_leader(42, &solana_pubkey::new_rand(), 42);
⋮----
let pubkeys: Vec<_> = (0..5).map(|_| Pubkey::new_unique()).collect();
let pubkeys_hashset: HashSet<_> = pubkeys.iter().cloned().collect();
⋮----
.cloned()
.zip(vec![
⋮----
bank.store_account(&pubkeys_balances[0].0, &account0);
⋮----
bank.store_account(&pubkeys_balances[1].0, &account1);
⋮----
bank.store_account(&pubkeys_balances[2].0, &account2);
⋮----
bank.store_account(&pubkeys_balances[3].0, &account3);
⋮----
bank.store_account(&pubkeys_balances[4].0, &account4);
let exclude4: HashSet<_> = pubkeys[4..].iter().cloned().collect();
let mut sorted_accounts = pubkeys_balances.clone();
sorted_accounts.sort_by(|a, b| a.1.cmp(&b.1).reverse());
⋮----
.get_largest_accounts(10, &pubkeys_hashset, AccountAddressFilter::Include, false)
⋮----
assert_eq!(results.len(), sorted_accounts.len());
for pubkey_balance in sorted_accounts.iter() {
assert!(results.contains(pubkey_balance));
⋮----
let mut sorted_results = results.clone();
sorted_results.sort_by(|a, b| a.1.cmp(&b.1).reverse());
assert_eq!(sorted_results, results);
let expected_accounts = sorted_accounts[1..].to_vec();
⋮----
.get_largest_accounts(10, &exclude4, AccountAddressFilter::Exclude, false)
⋮----
assert_eq!(results.len(), 10);
for pubkey_balance in expected_accounts.iter() {
⋮----
let expected_accounts = sorted_accounts[0..4].to_vec();
⋮----
.get_largest_accounts(4, &pubkeys_hashset, AccountAddressFilter::Include, false)
⋮----
assert_eq!(results.len(), expected_accounts.len());
⋮----
let expected_accounts = expected_accounts[1..4].to_vec();
⋮----
.get_largest_accounts(3, &exclude4, AccountAddressFilter::Exclude, false)
⋮----
fn test_transfer_sysvar() {
⋮----
let orig_lamports = bank.get_account(&sysvar::clock::id()).unwrap().lamports();
⋮----
let accounts = vec![
⋮----
let message = Message::new(&[ix], Some(&mint_keypair.pubkey()));
⋮----
fn test_clean_dropped_unrooted_frozen_banks() {
⋮----
do_test_clean_dropped_unrooted_banks(FreezeBank1::Yes);
⋮----
fn test_clean_dropped_unrooted_unfrozen_banks() {
⋮----
do_test_clean_dropped_unrooted_banks(FreezeBank1::No);
⋮----
enum FreezeBank1 {
⋮----
fn do_test_clean_dropped_unrooted_banks(freeze_bank1: FreezeBank1) {
⋮----
.transfer(amount, &mint_keypair, &key2.pubkey())
⋮----
Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank0.clone(), &collector, slot);
⋮----
bank1.store_account(&key4.pubkey(), &AccountSharedData::new(0, 0, &owner));
bank1.store_account(&key5.pubkey(), &AccountSharedData::new(0, 0, &owner));
⋮----
let bank2 = Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank0, &collector, slot);
⋮----
.transfer(amount * 2, &mint_keypair, &key2.pubkey())
⋮----
.transfer(amount, &mint_keypair, &key3.pubkey())
⋮----
bank2.store_account(&key5.pubkey(), &AccountSharedData::new(0, 0, &owner));
⋮----
bank_forks.write().unwrap().remove(1);
⋮----
.assert_ref_count(&key1.pubkey(), expected_ref_count_for_cleaned_up_keys);
bank2.rc.accounts.accounts_db.assert_ref_count(
&key3.pubkey(),
⋮----
.assert_ref_count(&key4.pubkey(), expected_ref_count_for_cleaned_up_keys);
⋮----
&key5.pubkey(),
⋮----
fn test_compute_budget_program_noop() {
⋮----
Instruction::new_with_bincode(program_id, &0, vec![]),
⋮----
let tx = Transaction::new(&[&mint_keypair], message, bank.last_blockhash());
⋮----
fn test_compute_request_instruction() {
⋮----
fn test_failed_compute_request_instruction() {
⋮----
bank.transfer(10, &mint_keypair, &payer0_keypair.pubkey())
⋮----
bank.transfer(10, &mint_keypair, &payer1_keypair.pubkey())
⋮----
Some(&payer0_keypair.pubkey()),
⋮----
Some(&payer1_keypair.pubkey()),
⋮----
Transaction::new(&[&payer0_keypair], message0, bank.last_blockhash()),
Transaction::new(&[&payer1_keypair], message1, bank.last_blockhash()),
⋮----
assert_eq!(bank.signature_count(), 3);
⋮----
fn test_verify_and_hash_transaction_sig_len() {
⋮----
let from_pubkey = from_keypair.pubkey();
let to_pubkey = to_keypair.pubkey();
enum TestCase {
⋮----
Some(&from_pubkey),
⋮----
assert_eq!(tx.message.header.num_required_signatures, 1);
⋮----
let signature = to_keypair.sign_message(&tx.message.serialize());
tx.signatures.push(signature);
⋮----
tx.signatures.remove(0);
⋮----
let tx = make_transaction(TestCase::RemoveSignature);
assert_matches!(
⋮----
let tx = make_transaction(TestCase::AddSignature);
⋮----
fn test_verify_transactions_packet_data_size() {
⋮----
let pubkey = keypair.pubkey();
⋮----
.take(size)
⋮----
let message = Message::new(&ixs[..], Some(&pubkey));
⋮----
let tx = make_transaction(5);
assert!(bincode::serialized_size(&tx).unwrap() <= PACKET_DATA_SIZE as u64);
⋮----
let tx = make_transaction(25);
assert!(bincode::serialized_size(&tx).unwrap() > PACKET_DATA_SIZE as u64);
⋮----
let tx = make_transaction(size);
⋮----
fn test_verify_transactions_instruction_limit(simd_0160_enabled: bool) {
⋮----
bank.deactivate_feature(&feature_set::static_instruction_limit::id());
⋮----
accounts: vec![0],
data: vec![],
⋮----
.take(ix_count)
⋮----
vec![pubkey, Pubkey::new_unique()],
⋮----
fn test_check_reserved_keys() {
⋮----
assert_eq!(bank.check_reserved_keys(&transaction), Ok(()));
⋮----
.insert(transaction.account_keys()[1]);
⋮----
fn test_call_precomiled_program() {
⋮----
} = create_genesis_config_with_leader(42, &Pubkey::new_unique(), 42);
⋮----
use rand::RngCore;
⋮----
rng.fill_bytes(&mut ret);
⋮----
&pubkey.serialize()[1..].try_into().unwrap(),
⋮----
solana_secp256k1_program::sign_message(&secp_privkey.serialize(), &message_arr[..])
⋮----
rng.fill_bytes(&mut seed);
⋮----
let signature = privkey.sign(message_arr).to_bytes();
let pubkey = privkey.public.to_bytes();
⋮----
fn calculate_test_fee(
⋮----
process_compute_budget_instructions(
message.program_instructions_iter(),
⋮----
.unwrap_or_default(),
⋮----
fn test_calculate_fee() {
let message = new_sanitized_message(Message::new(&[], Some(&Pubkey::new_unique())));
⋮----
let message = new_sanitized_message(Message::new(&[ix0, ix1], Some(&key0)));
⋮----
fn test_calculate_fee_compute_units() {
⋮----
let max_fee = fee_structure.compute_fee_bins.last().unwrap().fee;
⋮----
let message = new_sanitized_message(Message::new(&[ix0, ix1], Some(&Pubkey::new_unique())));
⋮----
let message = new_sanitized_message(Message::new(
⋮----
Instruction::new_with_bincode(Pubkey::new_unique(), &0_u8, vec![]),
⋮----
Some(&Pubkey::new_unique()),
⋮----
let fee = calculate_test_fee(&message, 1, &fee_structure);
⋮----
fn test_calculate_prioritization_fee() {
⋮----
let fee = calculate_test_fee(
⋮----
fn test_calculate_fee_secp256k1() {
⋮----
accounts: vec![],
⋮----
data: vec![1],
⋮----
ix0.clone(),
secp_instruction1.clone(),
secp_instruction2.clone(),
⋮----
Some(&key0),
⋮----
assert_eq!(calculate_test_fee(&message, 1, &fee_structure,), 2);
secp_instruction1.data = vec![0];
secp_instruction2.data = vec![10];
⋮----
assert_eq!(calculate_test_fee(&message, 1, &fee_structure,), 11);
⋮----
fn test_an_empty_instruction_without_program(formalize_loaded_transaction_data_size: bool) {
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee_no_rent(1);
⋮----
let mut ix = system_instruction::transfer(&mint_keypair.pubkey(), &destination, 0);
⋮----
fn test_transaction_log_collector_get_logs_for_address() {
⋮----
mentioned_address_map.insert(address, vec![0]);
⋮----
fn test_accounts_data_size_with_good_transaction() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(1_000 * LAMPORTS_PER_SOL);
⋮----
.minimum_balance(ACCOUNT_SIZE.try_into().unwrap()),
⋮----
let accounts_data_size_before = bank.load_accounts_data_size();
let accounts_data_size_delta_before = bank.load_accounts_data_size_delta();
let accounts_data_size_delta_on_chain_before = bank.load_accounts_data_size_delta_on_chain();
let result = bank.process_transaction(&transaction);
let accounts_data_size_after = bank.load_accounts_data_size();
let accounts_data_size_delta_after = bank.load_accounts_data_size_delta();
let accounts_data_size_delta_on_chain_after = bank.load_accounts_data_size_delta_on_chain();
⋮----
fn test_accounts_data_size_with_bad_transaction() {
⋮----
let (genesis_config, _mint_keypair) = create_genesis_config(1_000 * LAMPORTS_PER_SOL);
⋮----
assert!(result.is_err());
assert_eq!(accounts_data_size_after, accounts_data_size_before,);
⋮----
enum MockTransferInstruction {
⋮----
declare_process_instruction!(MockTransferBuiltin, 1, |invoke_context| {
⋮----
fn create_mock_transfer(
⋮----
Some(&payer.pubkey()),
⋮----
fn test_invalid_rent_state_changes_existing_accounts() {
⋮----
} = create_genesis_config_with_leader(100 * LAMPORTS_PER_SOL, &Pubkey::new_unique(), 42);
⋮----
let rent_exempt_minimum = genesis_config.rent.minimum_balance(account_data_size);
⋮----
genesis_config.accounts.insert(
rent_paying_account.pubkey(),
⋮----
rent_exempt_account.pubkey(),
⋮----
let recent_blockhash = bank.last_blockhash();
⋮----
let account = bank.get_account(pubkey).unwrap();
Rent::default().is_exempt(account.lamports(), account.data().len())
⋮----
let tx = create_mock_transfer(
⋮----
assert!(!check_account_is_rent_exempt(&rent_paying_account.pubkey()));
⋮----
assert!(bank.get_account(&rent_paying_account.pubkey()).is_none());
⋮----
&rent_paying_account.pubkey(),
⋮----
let result = bank.transfer(1, &mint_keypair, &rent_paying_account.pubkey());
⋮----
assert!(check_account_is_rent_exempt(&rent_paying_account.pubkey()));
⋮----
assert!(check_account_is_rent_exempt(&rent_exempt_account.pubkey()));
let result = bank.transfer(1, &mint_keypair, &rent_exempt_account.pubkey());
⋮----
assert!(bank.get_account(&rent_exempt_account.pubkey()).is_none());
⋮----
fn test_invalid_rent_state_changes_new_accounts() {
⋮----
fn test_drained_created_account() {
⋮----
&created_keypair.pubkey(),
⋮----
assert!(bank.get_account(&created_keypair.pubkey()).is_none());
⋮----
fn test_rent_state_changes_sysvars() {
⋮----
fn test_invalid_rent_state_changes_fee_payer() {
⋮----
let rent_exempt_minimum = genesis_config.rent.minimum_balance(0);
⋮----
rent_paying_fee_payer.pubkey(),
⋮----
&rent_exempt_fee_payer.pubkey(),
⋮----
let dummy_message = new_sanitized_message(Message::new_with_blockhash(
⋮----
Some(&rent_exempt_fee_payer.pubkey()),
⋮----
let fee = bank.get_fee_for_message(&dummy_message).unwrap();
⋮----
Some(&rent_paying_fee_payer.pubkey()),
⋮----
assert!(!check_account_is_rent_exempt(
⋮----
let fee_payer_balance = bank.get_balance(&rent_paying_fee_payer.pubkey());
⋮----
&sender.pubkey(),
⋮----
&rent_paying_fee_payer.pubkey(),
⋮----
bank.get_balance(&rent_paying_fee_payer.pubkey()) - fee,
⋮----
assert_eq!(0, bank.get_balance(&rent_paying_fee_payer.pubkey()));
⋮----
assert!(check_account_is_rent_exempt(
⋮----
bank.transfer(fee, &mint_keypair, &rent_exempt_fee_payer.pubkey())
⋮----
let fee_payer_balance = bank.get_balance(&rent_exempt_fee_payer.pubkey());
assert_eq!(fee_payer_balance, rent_exempt_minimum + fee);
⋮----
bank.transfer(fee + 1, &mint_keypair, &rent_exempt_fee_payer.pubkey())
⋮----
assert!(bank.get_balance(&rent_exempt_fee_payer.pubkey()) > rent_exempt_minimum + fee);
⋮----
bank.get_balance(&rent_exempt_fee_payer.pubkey()) - fee,
⋮----
assert_eq!(0, bank.get_balance(&rent_exempt_fee_payer.pubkey()));
⋮----
assert!(bank.get_balance(&rent_exempt_fee_payer.pubkey()) < rent_exempt_minimum + fee);
⋮----
fn test_rent_state_incinerator() {
⋮----
bank.transfer(amount, &mint_keypair, &solana_sdk_ids::incinerator::id())
⋮----
fn test_update_accounts_data_size() {
⋮----
let initial_data_size = bank.load_accounts_data_size() as i64;
⋮----
.store(data_size, Release);
bank.update_accounts_data_size_delta_on_chain(
(initial_data_size + data_size + 1).saturating_neg(),
⋮----
assert_eq!(bank.load_accounts_data_size(), 0);
⋮----
let mut bank = create_simple_test_bank(100);
⋮----
.store((data_size_remaining + 1) as i64, Release);
assert_eq!(bank.load_accounts_data_size(), u64::MAX);
⋮----
let initial = bank.load_accounts_data_size() as i64;
let delta1 = rng.random_range(-500..500);
bank.update_accounts_data_size_delta_on_chain(delta1);
let delta2 = rng.random_range(-500..500);
bank.update_accounts_data_size_delta_off_chain(delta2);
⋮----
enum MockReallocInstruction {
⋮----
declare_process_instruction!(MockReallocBuiltin, 1, |invoke_context| {
⋮----
fn create_mock_realloc_tx(
⋮----
fn test_resize_and_rent() {
⋮----
} = create_genesis_config_with_leader(1_000_000_000, &Pubkey::new_unique(), 42);
⋮----
let rent_exempt_minimum_small = genesis_config.rent.minimum_balance(account_data_size_small);
⋮----
let rent_exempt_minimum_large = genesis_config.rent.minimum_balance(account_data_size_large);
⋮----
&funding_keypair.pubkey(),
⋮----
rent_paying_account.set_rent_epoch(1);
bank.store_account(&rent_paying_pubkey, &rent_paying_account);
let tx = create_mock_realloc_tx(
⋮----
.position(|key| key == &rent_paying_pubkey)
.unwrap() as u8;
⋮----
assert_eq!(bank.process_transaction(&tx).unwrap_err(), expected_err);
⋮----
.position(|key| key == &created_keypair.pubkey())
⋮----
fn test_accounts_data_size_and_resize_transactions() {
⋮----
rng.random_range(1..MAX_PERMITTED_DATA_LENGTH as usize - MAX_PERMITTED_DATA_INCREASE);
⋮----
bank.store_account(&account_pubkey, &account_data);
⋮----
let account_grow_size = rng.random_range(1..MAX_PERMITTED_DATA_INCREASE);
let transaction = create_mock_realloc_tx(
⋮----
rng.random_range(MAX_PERMITTED_DATA_LENGTH / 2..MAX_PERMITTED_DATA_LENGTH) as usize;
⋮----
let account_shrink_size = rng.random_range(1..account_size);
⋮----
fn test_accounts_data_size_with_default_bank() {
⋮----
fn test_accounts_data_size_from_genesis() {
⋮----
let slot = bank.slot() + 1;
⋮----
let data_size = rand::rng().random_range(3333..4444);
⋮----
genesis_config.rent.minimum_balance(data_size),
⋮----
bank.fill_bank_with_ticks_for_tests();
⋮----
fn test_cap_accounts_data_allocations_per_transaction() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
let mut keypairs = vec![mint_keypair.insecure_clone()];
⋮----
bank.rent_collector()
⋮----
.minimum_balance(MAX_PERMITTED_DATA_LENGTH as usize),
⋮----
keypairs.push(keypair);
instructions.push(instruction);
⋮----
let signers: Vec<_> = keypairs.iter().collect();
let transaction = Transaction::new(&signers, message, bank.last_blockhash());
⋮----
assert_eq!(accounts_data_size_before, accounts_data_size_after);
⋮----
fn test_calculate_fee_with_congestion_multiplier() {
⋮----
fn test_calculate_fee_with_request_heap_frame_flag() {
⋮----
fn test_is_in_slot_hashes_history() {
use solana_slot_hashes::MAX_ENTRIES;
⋮----
assert!(!bank0.is_in_slot_hashes_history(&0));
assert!(!bank0.is_in_slot_hashes_history(&1));
⋮----
let new_bank = Arc::new(new_from_parent(last_bank));
assert!(new_bank.is_in_slot_hashes_history(&0));
⋮----
assert!(!new_bank.is_in_slot_hashes_history(&0));
⋮----
fn test_feature_activation_loaded_programs_cache_preparation_phase(
⋮----
feature_set.deactivate(&feature_set::disable_sbpf_v0_execution::id());
feature_set.deactivate(&feature_set::reenable_sbpf_v0_execution::id());
⋮----
feature_set.deactivate(&feature_set::formalize_loaded_transaction_data_size::id());
⋮----
let (root_bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
let program_data = include_bytes!("../../../programs/bpf_loader/test_elfs/out/noop_aligned.so");
⋮----
lamports: Rent::default().minimum_balance(program_data.len()).min(1),
data: program_data.to_vec(),
⋮----
root_bank.store_account(&program_keypair.pubkey(), &program_account);
⋮----
let signers = vec![&binding];
goto_end_of_slot(root_bank.clone());
let bank = new_from_parent_with_fork_next_slot(root_bank, bank_forks.as_ref());
let transaction = Transaction::new(&signers, message.clone(), bank.last_blockhash());
let result_without_feature_enabled = bank.process_transaction(&transaction);
assert_eq!(result_without_feature_enabled, Ok(()));
⋮----
std::cmp::max(genesis_config.rent.minimum_balance(Feature::size_of()), 1);
⋮----
.get_environments_for_epoch(0)
⋮----
.get_environments_for_epoch(1)
⋮----
assert!(Arc::ptr_eq(
⋮----
assert_eq!(slot_versions.len(), 2);
⋮----
let result_with_feature_enabled = bank.process_transaction(&transaction);
⋮----
fn test_feature_activation_loaded_programs_epoch_transition() {
⋮----
let (mut genesis_config, mint_keypair) = create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
.remove(&feature_set::disable_fees_sysvar::id());
⋮----
.remove(&feature_set::reenable_sbpf_v0_execution::id());
⋮----
.reroot(bank.epoch());
assert!(upcoming_environments.is_some());
⋮----
program_cache.prune(bank.slot(), upcoming_environments);
program_cache.sort_and_unload(percentage::Percentage::from(0));
⋮----
fn test_verify_accounts() {
⋮----
system_transaction::transfer(&key2, &key3.pubkey(), amount, blockhash),
⋮----
bank.get_fee_for_message(transaction.message()).unwrap()
⋮----
bank.transfer(amount + fee, &mint, &key1.pubkey()).unwrap();
bank.transfer(amount + fee, &mint, &key2.pubkey()).unwrap();
bank.transfer(amount + fee, &key2, &key3.pubkey()).unwrap();
assert_eq!(bank.get_balance(&key2.pubkey()), 0);
⋮----
do_transfers(&bank);
⋮----
assert!(bank.verify_accounts(VerifyAccountsHashConfig::default_for_test(), None));
⋮----
fn test_squash_timing_add_assign() {
⋮----
assert!(t0 == expected);
⋮----
fn test_system_instruction_allocate() {
⋮----
let amount = genesis_config.rent.minimum_balance(data_len);
⋮----
let alice_with_seed = Pubkey::create_with_seed(&alice_pubkey, seed, &owner).unwrap();
⋮----
.transfer_and_confirm(amount, &mint_keypair, &alice_pubkey)
⋮----
Some(&alice_pubkey),
⋮----
fn with_create_zero_lamport<F>(callback: F)
⋮----
let bob_pubkey = bob_keypair.pubkey();
⋮----
let (genesis_config, mint_keypair) = create_genesis_config_no_tx_fee_no_rent(mint_lamports);
⋮----
let bank_client = BankClient::new_shared(bank.clone());
⋮----
.transfer_and_confirm(mint_lamports, &mint_keypair, &alice_pubkey)
⋮----
let bank = new_from_parent_next_epoch(bank, &bank_forks, 2);
⋮----
let bank = Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank, &collector, slot);
⋮----
bank.store_account(&bob_pubkey, &account);
⋮----
.transfer_and_confirm(
⋮----
callback(&bank);
let lamports = genesis_config.rent.minimum_balance(len2);
⋮----
let message = Message::new(&[ix], Some(&alice_pubkey));
let r = bank_client.send_and_confirm_message(&[&alice_keypair, &bob_keypair], message);
assert!(r.is_ok());
⋮----
fn test_create_zero_lamport_with_clean() {
with_create_zero_lamport(|bank| {
⋮----
assert_eq!(6, bank.get_snapshot_storages(None).len());
bank.clean_accounts();
assert_eq!(5, bank.get_snapshot_storages(None).len());
⋮----
fn test_create_zero_lamport_without_clean() {
with_create_zero_lamport(|_| {
⋮----
fn test_system_instruction_assign_with_seed() {
⋮----
fn test_system_instruction_unsigned_transaction() {
let (genesis_config, alice_keypair) = create_genesis_config_no_tx_fee(LAMPORTS_PER_SOL);
⋮----
let mallory_pubkey = mallory_keypair.pubkey();
⋮----
.transfer_and_confirm(amount, &alice_keypair, &mallory_pubkey)
⋮----
assert_eq!(bank_client.get_balance(&mallory_pubkey).unwrap(), amount);
⋮----
fn test_calc_vote_accounts_to_store_empty() {
⋮----
assert!(result.accounts_with_rewards.is_empty());
⋮----
fn test_calc_vote_accounts_to_store_overflow() {
⋮----
vote_account.set_lamports(u64::MAX);
vote_account_rewards.insert(
⋮----
fn test_calc_vote_accounts_to_store_normal() {
⋮----
vote_account.set_lamports(1);
⋮----
vote_account: vote_account.clone(),
⋮----
assert_eq!(result.accounts_with_rewards.len(), 1);
⋮----
_ = vote_account.checked_add_lamports(vote_rewards);
assert!(accounts_equal(account, &vote_account));
⋮----
assert_eq!(*pubkey_result, pubkey);
⋮----
fn test_register_hard_fork() {
fn get_hard_forks(bank: &Bank) -> Vec<Slot> {
bank.hard_forks().iter().map(|(slot, _)| *slot).collect()
⋮----
let (genesis_config, _mint_keypair) = create_genesis_config(10);
⋮----
let bank7 = Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 7);
bank7.register_hard_fork(6);
bank7.register_hard_fork(7);
bank7.register_hard_fork(8);
assert_eq!(get_hard_forks(&bank7), vec![7, 8]);
⋮----
bank9.freeze();
bank9.register_hard_fork(9);
bank9.register_hard_fork(10);
assert_eq!(get_hard_forks(&bank9), vec![7, 8, 10]);
⋮----
fn test_last_restart_slot() {
fn last_restart_slot_dirty(bank: &Bank) -> bool {
⋮----
.get_pubkeys_for_slot(bank.slot());
let dirty_accounts: HashSet<_> = dirty_accounts.into_iter().collect();
dirty_accounts.contains(&sysvar::last_restart_slot::id())
⋮----
fn get_last_restart_slot(bank: &Bank) -> Option<Slot> {
bank.get_account(&sysvar::last_restart_slot::id())
.and_then(|account| {
let lrs: Option<LastRestartSlot> = from_account(&account);
⋮----
.map(|account| account.last_restart_slot)
⋮----
} = create_genesis_config_with_leader(mint_lamports, &leader_pubkey, validator_stake_lamports);
⋮----
.remove(&feature_set::last_restart_slot_sysvar::id())
⋮----
bank0.register_hard_fork(6);
assert!(!last_restart_slot_dirty(&bank0));
assert_eq!(get_last_restart_slot(&bank0), None);
⋮----
assert!(!last_restart_slot_dirty(&bank1));
assert_eq!(get_last_restart_slot(&bank1), None);
⋮----
.activate_feature(&feature_set::last_restart_slot_sysvar::id());
let bank2 = Arc::new(Bank::new_from_parent(bank1.clone(), &Pubkey::default(), 2));
assert!(last_restart_slot_dirty(&bank2));
assert_eq!(get_last_restart_slot(&bank2), Some(0));
⋮----
assert!(last_restart_slot_dirty(&bank3));
assert_eq!(get_last_restart_slot(&bank3), Some(0));
⋮----
assert!(!last_restart_slot_dirty(&bank4));
assert_eq!(get_last_restart_slot(&bank4), Some(0));
⋮----
assert!(!last_restart_slot_dirty(&bank5));
assert_eq!(get_last_restart_slot(&bank5), Some(0));
⋮----
assert!(last_restart_slot_dirty(&bank6));
assert_eq!(get_last_restart_slot(&bank6), Some(6));
bank6.register_hard_fork(10);
⋮----
assert!(!last_restart_slot_dirty(&bank7));
assert_eq!(get_last_restart_slot(&bank7), Some(6));
⋮----
fn test_failed_simulation_compute_units() {
⋮----
bank.get_account(&program_id).unwrap().data().len() as u32;
declare_process_instruction!(MockBuiltin, MOCK_BUILTIN_UNITS, |invoke_context| {
⋮----
&[Instruction::new_with_bincode(program_id, &0, vec![])],
⋮----
let transaction = Transaction::new(&[&mint_keypair], message, bank.last_blockhash());
⋮----
let simulation = bank.simulate_transaction(&sanitized, false);
assert_eq!(expected_consumed_units, simulation.units_consumed);
⋮----
fn test_failed_simulation_load_error() {
⋮----
let mint_balance = bank.get_account(&mint_keypair.pubkey()).unwrap().lamports();
⋮----
fn test_filter_program_errors_and_collect_fee_details() {
⋮----
} = create_genesis_config_with_leader(initial_payer_balance, &Pubkey::new_unique(), 3);
⋮----
let results = vec![
⋮----
bank.filter_program_errors_and_collect_fee_details(&results);
⋮----
fn test_priority_fee_total() {
let (genesis_config, _mint_keypair) = create_genesis_config(1_000_000);
⋮----
assert_eq!(bank.priority_fee_total(), 0);
⋮----
assert_eq!(bank.priority_fee_total(), priority_fee);
⋮----
assert_eq!(bank.priority_fee_total(), new_priority_fee);
⋮----
let mut fee_details = bank.collector_fee_details.write().unwrap();
fee_details.accumulate(&FeeDetails::new(2000, 3000));
⋮----
assert_eq!(bank.priority_fee_total(), new_priority_fee + 3000);
⋮----
fn test_deploy_last_epoch_slot() {
⋮----
let slots_in_epoch = bank.epoch_schedule().get_slots_in_epoch(0);
⋮----
eprintln!("now at slot {} epoch {}", bank.slot(), bank.epoch());
⋮----
let min_program_balance = bank.get_minimum_balance_for_rent_exemption(
LoaderV4State::program_data_offset().saturating_add(elf.len()),
⋮----
.get_mut(0..LoaderV4State::program_data_offset())
⋮----
program_state.authority_address_or_next_version = upgrade_authority_keypair.pubkey();
⋮----
.get_mut(LoaderV4State::program_data_offset()..)
⋮----
let min_payer_balance = min_program_balance.saturating_add(deploy_fees);
⋮----
let transaction = Transaction::new(signers, message.clone(), bank.last_blockhash());
let ret = bank.process_transaction(&transaction);
assert!(ret.is_ok(), "ret: {ret:?}");
⋮----
assert_eq!(result_with_feature_enabled, Ok(()));
⋮----
fn test_loader_v3_to_v4_migration(formalize_loaded_transaction_data_size: bool) {
⋮----
.set_state(&UpgradeableLoaderState::Program {
⋮----
.set_state(&UpgradeableLoaderState::Uninitialized)
⋮----
.get_mut(UpgradeableLoaderState::size_of_programdata_metadata()..)
⋮----
Transaction::new(signers, message.clone(), bank.last_blockhash());
⋮----
bank.get_minimum_balance_for_rent_exemption(upgradeable_programdata_account.data().len());
upgradeable_programdata_account.set_lamports(min_programdata_balance);
⋮----
upgrade_authority_address: Some(upgrade_authority_keypair.pubkey()),
⋮----
&upgradeable_programdata_account.clone(),
⋮----
bank.store_account(&payer_keypair.pubkey(), &payer_account);
let case_redeployment_cooldown = vec![
⋮----
let error = bank.process_transaction(&transaction).unwrap_err();
⋮----
let case_too_few_accounts = vec![
⋮----
let case_readonly_programdata = vec![
⋮----
let case_incorrect_authority = vec![
⋮----
let case_missing_signature = vec![
⋮----
let case_readonly_program = vec![
⋮----
let case_program_has_wrong_owner = vec![
⋮----
let case_incorrect_programdata_address = vec![
⋮----
let case_invalid_program_account = vec![
⋮----
let case_missing_loader_v4 = vec![
⋮----
message.clone(),
⋮----
assert_eq!(error, TransactionError::InstructionError(0, expected_error));
⋮----
finalized_migration_transaction.clone(),
Err(TransactionError::InstructionError(
⋮----
Ok(()),
⋮----
bank.get_minimum_balance_for_rent_exemption(programdata_account.data().len());
programdata_account.set_lamports(min_programdata_balance);
⋮----
.saturating_add(LAMPORTS_PER_SOL)
.saturating_add(fee_calculator.lamports_per_signature);
⋮----
assert!(result.is_ok(), "result: {result:?}");
⋮----
let execution_result = bank.process_transaction(&transaction);
assert_eq!(execution_result, expected_execution_result);
⋮----
fn test_blockhash_last_valid_block_height() {
⋮----
let last_blockhash = bank.last_blockhash();
⋮----
assert_eq!(i, bank.block_height);
⋮----
.get_blockhash_last_valid_block_height(&last_blockhash)
⋮----
assert!(bank.is_blockhash_valid(&last_blockhash));
⋮----
assert_eq!(bank.block_height, i as u64);
⋮----
assert_eq!(last_valid_block_height, MAX_PROCESSING_AGE as u64);
assert!(!bank.is_blockhash_valid(&last_blockhash));
⋮----
fn test_bank_epoch_stakes() {
⋮----
let stakes = (1..num_of_nodes.checked_add(1).expect("Shouldn't be big")).collect::<Vec<_>>();
⋮----
.map(|_| ValidatorVoteKeypairs::new_rand())
⋮----
let total_stake = stakes.iter().sum();
⋮----
create_genesis_config_with_vote_accounts(1_000_000_000, &voting_keypairs, stakes.clone());
⋮----
bank0.get_slots_in_epoch(0) + 1,
⋮----
let initial_epochs = bank0.epoch_stake_keys();
⋮----
assert_eq!(bank0.epoch(), 0);
assert_eq!(bank0.epoch_total_stake(0), Some(total_stake));
assert_eq!(bank0.epoch_node_id_to_stake(0, &Pubkey::new_unique()), None);
for (i, keypair) in voting_keypairs.iter().enumerate() {
⋮----
assert_eq!(bank0.epoch().saturating_add(1), 1);
assert_eq!(bank0.epoch_total_stake(1), Some(total_stake));
assert_eq!(bank0.epoch_node_id_to_stake(1, &Pubkey::new_unique()), None);
⋮----
assert_eq!(bank0.get_current_epoch_total_stake(), total_stake);
⋮----
assert_eq!(bank1.epoch(), 1);
assert_eq!(bank1.epoch_total_stake(1), Some(total_stake));
assert_eq!(bank1.epoch_node_id_to_stake(1, &Pubkey::new_unique()), None);
⋮----
assert_eq!(bank1.epoch().saturating_add(1), 2);
assert_eq!(bank1.epoch_total_stake(2), Some(total_stake));
assert_eq!(bank1.epoch_node_id_to_stake(2, &Pubkey::new_unique()), None);
⋮----
assert_eq!(bank1.get_current_epoch_total_stake(), total_stake);
⋮----
.map(|keypair| {
let node_id = keypair.node_keypair.pubkey();
let authorized_voter = keypair.vote_keypair.pubkey();
let vote_account = VoteAccount::try_from(create_v4_account_with_authorized(
⋮----
bank1.set_epoch_stakes_for_test(1, make_new_epoch_stakes(stake_coefficient_epoch_1));
bank1.set_epoch_stakes_for_test(2, make_new_epoch_stakes(stake_coefficient_epoch_2));
⋮----
for keypair in voting_keypairs.iter() {
⋮----
fn test_rehash_accounts_unmodified() {
⋮----
bank.store_account_and_update_capitalization(&pubkey, &account);
⋮----
let prev_bank_hash = bank.hash();
bank.rehash();
let post_bank_hash = bank.hash();
assert_eq!(post_bank_hash, prev_bank_hash);
⋮----
fn test_should_use_vote_keyed_leader_schedule() {
⋮----
let feature_activation_epoch = bank.epoch_schedule().get_epoch(feature_activation_slot);
assert!(feature_activation_epoch <= bank_epoch);
feature_set.activate(
⋮----
let test_bank = create_test_bank(0, Some(0));
⋮----
let slot_in_prev_epoch = epoch_schedule.get_first_slot_in_epoch(1);
let test_bank = create_test_bank(2, Some(slot_in_prev_epoch));
⋮----
let current_epoch_slot = epoch_schedule.get_last_slot_in_epoch(1);
let test_bank = create_test_bank(1, Some(current_epoch_slot));
⋮----
let test_bank = create_test_bank(1, None);
let max_cached_leader_schedule = epoch_schedule.get_leader_schedule_epoch(test_bank.slot());
⋮----
assert_eq!(test_bank.should_use_vote_keyed_leader_schedule(epoch), None);
⋮----
fn test_apply_builtin_program_feature_transitions_for_new_epoch() {
let (genesis_config, _mint_keypair) = create_genesis_config(100_000);
⋮----
bank.compute_and_apply_genesis_features();
for precompile in get_precompiles() {
bank.store_account(&precompile.program_id, &AccountSharedData::default());
⋮----
bank.add_precompiled_account_with_owner(
⋮----
bank.add_precompiled_account(&precompile.program_id);
⋮----
fn test_startup_from_snapshot_after_precompile_transition() {
⋮----
fn test_parent_block_id() {
⋮----
let parent_block_id = Some(Hash::new_unique());
parent_bank.set_block_id(parent_block_id);
⋮----
assert_eq!(parent_block_id, child_bank.parent_block_id());
⋮----
fn test_simulate_transactions_unchecked_with_pre_accounts() {}

================
File: runtime/src/inflation_rewards/mod.rs
================
pub mod points;
⋮----
struct CalculatedStakeRewards {
⋮----
pub fn redeem_rewards(
⋮----
if let Some(inflation_point_calc_tracer) = inflation_point_calc_tracer.as_ref() {
inflation_point_calc_tracer(
&InflationPointCalculationEvent::EffectiveStakeAtRewardedEpoch(stake.stake(
⋮----
inflation_point_calc_tracer(&InflationPointCalculationEvent::RentExemptReserve(
⋮----
inflation_point_calc_tracer(&InflationPointCalculationEvent::Commission(
vote_state.commission(),
⋮----
if let Some((stakers_reward, voters_reward)) = redeem_stake_rewards(
⋮----
Ok((stakers_reward, voters_reward, stake))
⋮----
Err(StakeError::NoCreditsToRedeem.into())
⋮----
Err(InstructionError::InvalidAccountData)
⋮----
fn redeem_stake_rewards(
⋮----
inflation_point_calc_tracer(&InflationPointCalculationEvent::CreditsObserved(
⋮----
calculate_stake_rewards(
⋮----
inflation_point_calc_tracer.as_ref(),
⋮----
.map(|calculated_stake_rewards| {
⋮----
Some(calculated_stake_rewards.new_credits_observed),
⋮----
fn calculate_stake_rewards(
⋮----
} = calculate_stake_points_and_credits(
⋮----
inflation_point_calc_tracer(&SkippedReason::DisabledInflation.into());
⋮----
inflation_point_calc_tracer(&SkippedReason::JustActivated.into());
⋮----
return Some(CalculatedStakeRewards {
⋮----
inflation_point_calc_tracer(&SkippedReason::ZeroPoints.into());
⋮----
inflation_point_calc_tracer(&SkippedReason::ZeroPointValue.into());
⋮----
.checked_mul(u128::from(point_value.rewards))
.expect("Rewards intermediate calculation should fit within u128")
.checked_div(point_value.points)
.unwrap();
let rewards = u64::try_from(rewards).expect("Rewards should fit within u64");
⋮----
inflation_point_calc_tracer(&SkippedReason::ZeroReward.into());
⋮----
commission_split(vote_state.commission(), rewards);
⋮----
inflation_point_calc_tracer(&InflationPointCalculationEvent::SplitRewards(
⋮----
(*point_value).clone(),
⋮----
inflation_point_calc_tracer(&SkippedReason::TooEarlyUnfairSplit.into());
⋮----
Some(CalculatedStakeRewards {
⋮----
fn commission_split(commission: u8, on: u64) -> (u64, u64, bool) {
match commission.min(100) {
⋮----
.checked_mul(u128::from(split))
.expect("multiplication of a u64 and u8 should not overflow")
⋮----
.checked_mul(u128::from(
⋮----
.checked_sub(split)
.expect("commission cannot be greater than 100"),
⋮----
mod tests {
⋮----
fn new_stake(
⋮----
credits_observed: vote_state.credits(),
⋮----
fn test_stake_state_redeem_rewards() {
⋮----
let mut stake = new_stake(stake_lamports, &Pubkey::default(), &vote_state, u64::MAX);
assert_eq!(
⋮----
vote_state.increment_credits(0, 1);
⋮----
assert_eq!(stake.credits_observed, 2);
⋮----
fn test_stake_state_calculate_rewards() {
⋮----
let mut stake = new_stake(1, &Pubkey::default(), &vote_state, u64::MAX);
⋮----
vote_state.increment_credits(1, 1);
⋮----
vote_state.increment_credits(2, 1);
⋮----
fn calculate_rewards_tests(stake: u64, rewards: u64, credits: u64) {
⋮----
let stake = new_stake(stake, &Pubkey::default(), &vote_state, u64::MAX);
vote_state.increment_credits(0, credits);
⋮----
&VoteStateView::from(vote_state.clone()),
⋮----
null_tracer(),
⋮----
fn test_stake_state_calculate_points_with_typical_values() {
⋮----
let stake = new_stake(
⋮----
fn test_commission_split() {
⋮----
assert_eq!(commission_split(commission, 1), (0, 1, false));
⋮----
assert_eq!(commission_split(commission, 1), (1, 0, false));
⋮----
assert_eq!(commission_split(commission, 10), (9, 0, true));
⋮----
assert_eq!(commission_split(commission, 10), (0, 9, true));
⋮----
let (voter_portion, staker_portion, was_split) = commission_split(commission, 10);
assert_eq!((voter_portion, staker_portion, was_split), (5, 5, true));

================
File: runtime/src/inflation_rewards/points.rs
================
pub struct PointValue {
⋮----
pub(crate) struct CalculatedStakePoints {
⋮----
pub enum InflationPointCalculationEvent {
⋮----
pub(crate) fn null_tracer() -> Option<impl Fn(&InflationPointCalculationEvent)> {
⋮----
pub enum SkippedReason {
⋮----
fn from(reason: SkippedReason) -> Self {
⋮----
pub fn calculate_points(
⋮----
Ok(calculate_stake_points(
⋮----
null_tracer(),
⋮----
Err(InstructionError::InvalidAccountData)
⋮----
fn calculate_stake_points(
⋮----
calculate_stake_points_and_credits(
⋮----
pub(crate) fn calculate_stake_points_and_credits(
⋮----
let credits_in_vote = new_vote_state.credits();
match credits_in_vote.cmp(&credits_in_stake) {
⋮----
if let Some(inflation_point_calc_tracer) = inflation_point_calc_tracer.as_ref() {
inflation_point_calc_tracer(&SkippedReason::ZeroCreditsAndReturnRewound.into());
⋮----
inflation_point_calc_tracer(&SkippedReason::ZeroCreditsAndReturnCurrent.into());
⋮----
for epoch_credits_item in new_vote_state.epoch_credits_iter() {
let (epoch, final_epoch_credits, initial_epoch_credits) = epoch_credits_item.into();
let stake_amount = u128::from(stake.delegation.stake(
⋮----
new_credits_observed = new_credits_observed.max(final_epoch_credits);
⋮----
inflation_point_calc_tracer(&InflationPointCalculationEvent::CalculatedPoints(
⋮----
mod tests {
⋮----
fn new_stake(
⋮----
credits_observed: vote_state.credits(),
⋮----
fn test_stake_state_calculate_points_with_typical_values() {
⋮----
let stake = new_stake(
⋮----
vote_state.increment_credits(0, 1);
⋮----
assert_eq!(

================
File: runtime/src/serde_snapshot/obsolete_accounts.rs
================
struct SerdeObsoleteAccounts {
⋮----
impl SerdeObsoleteAccounts {
fn new_from_storage_entry_at_slot(storage: &AccountStorageEntry, snapshot_slot: Slot) -> Self {
⋮----
.obsolete_accounts_for_snapshots(snapshot_slot)
⋮----
.into_iter()
.map(|item| (item.offset, item.data_len, item.slot))
.collect();
⋮----
id: storage.id() as SerializedAccountsFileId,
bytes: storage.get_obsolete_bytes(Some(snapshot_slot)) as u64,
⋮----
pub(crate) struct SerdeObsoleteAccountsMap {
⋮----
impl SerdeObsoleteAccountsMap {
pub(crate) fn new_from_storages(
⋮----
let map = DashMap::with_capacity(snapshot_storages.len());
snapshot_storages.par_iter().for_each(|storage| {
map.insert(
storage.slot(),
⋮----
pub(crate) fn remove(&self, slot: &Slot) -> Option<(ObsoleteAccounts, AccountsFileId, usize)> {
self.map.remove(slot).map(|(_, entry)| {
⋮----
.map(|(offset, data_len, slot)| ObsoleteAccountItem {
⋮----
mod test {
⋮----
fn test_serialize_and_deserialize_obsolete_accounts(
⋮----
.map(|j| ObsoleteAccountItem {
⋮----
.collect(),
⋮----
obsolete_accounts.insert(slot, obsolete_accounts_list);
⋮----
.iter()
.map(|entry| {
⋮----
.value()
⋮----
id: *entry.key() as SerializedAccountsFileId,
⋮----
(*entry.key(), serde_obsolete_accounts)
⋮----
serialize_into(&mut writer, &obsolete_accounts_map).unwrap();
drop(writer);
let cursor = Cursor::new(buf.as_slice());
⋮----
deserialize_from(&mut reader).unwrap();
assert_eq!(
⋮----
deserialized_obsolete_accounts.remove(&slot).unwrap();
assert_eq!(obsolete_accounts, deserialized_obsolete_accounts.0);

================
File: runtime/src/serde_snapshot/status_cache.rs
================
use shuttle::sync::Mutex;
⋮----
use std::sync::Mutex;
⋮----
type SerdeBankSlotDelta = SerdeSlotDelta<Result<(), SerdeTransactionError>>;
type SerdeSlotDelta<T> = (Slot, bool, SerdeStatus<T>);
type SerdeStatus<T> = ahash::HashMap<Hash, (usize, Vec<(KeySlice, T)>)>;
pub fn serialize_status_cache(
⋮----
.iter()
.map(|slot_delta| {
let status_map = slot_delta.2.lock().unwrap();
⋮----
.map(|(key, value)| {
⋮----
.map(|(key_slice, result)| {
⋮----
result.clone().map_err(SerdeTransactionError::from),
⋮----
Ok(())
⋮----
pub fn deserialize_status_cache(
⋮----
.with_limit(snapshot_utils::MAX_SNAPSHOT_DATA_FILE_SIZE)
.with_fixint_encoding()
.allow_trailing_bytes()
.deserialize_from(stream)?;
⋮----
(*key_slice, result.clone().map_err(TransactionError::from))
⋮----
Ok(slot_deltas)
⋮----
enum SerdeTransactionError {
⋮----
fn from(err: TransactionError) -> Self {
⋮----
TransactionError::InstructionError(i, inner) => Self::InstructionError(i, inner.into()),
⋮----
fn from(err: SerdeTransactionError) -> Self {
⋮----
Self::InstructionError(i, inner.into())
⋮----
enum SerdeInstructionError {
⋮----
fn from(err: SerdeInstructionError) -> Self {
⋮----
fn from(err: InstructionError) -> Self {

================
File: runtime/src/serde_snapshot/storage.rs
================
pub(crate) type SerializedAccountsFileId = usize;
⋮----
pub struct SerializableAccountStorageEntry {
⋮----
impl SerializableAccountStorageEntry {
pub fn new(
⋮----
id: accounts.id() as SerializedAccountsFileId,
accounts_current_len: accounts.accounts.len()
- accounts.get_obsolete_bytes(Some(snapshot_slot)),
⋮----
pub(crate) trait SerializableStorage {
⋮----
impl SerializableStorage for SerializableAccountStorageEntry {
fn id(&self) -> SerializedAccountsFileId {
⋮----
fn current_len(&self) -> usize {

================
File: runtime/src/serde_snapshot/tests.rs
================
mod serde_snapshot_tests {
⋮----
fn linear_ancestors(end_slot: u64) -> Ancestors {
let mut ancestors: Ancestors = vec![(0, 0)].into_iter().collect();
⋮----
ancestors.insert(i, (i - 1) as usize);
⋮----
fn context_accountsdb_from_stream<R>(
⋮----
let accounts_db_fields = deserialize_accounts_db_fields(stream)?;
⋮----
reconstruct_accountsdb_from_fields(
⋮----
.map(|(accounts_db, _)| accounts_db)
⋮----
fn accountsdb_from_stream<R>(
⋮----
fn accountsdb_to_stream<W>(
⋮----
let write_version = accounts_db.write_version.load(Ordering::Acquire);
serialize_into(
⋮----
fn copy_append_vecs(
⋮----
let storage_entries = accounts_db.get_storages(RangeFull).0;
let storage: AccountStorageMap = AccountStorageMap::with_capacity(storage_entries.len());
⋮----
for storage_entry in storage_entries.into_iter() {
let file_name = AccountsFile::file_name(storage_entry.slot(), storage_entry.id());
let output_path = output_dir.as_ref().join(file_name);
let mut reader = AccountStorageReader::new(&storage_entry, None).unwrap();
⋮----
AccountsFile::new_from_file(output_path, reader.len(), storage_access)?;
⋮----
storage_entry.slot(),
storage_entry.id(),
⋮----
next_append_vec_id = next_append_vec_id.max(new_storage_entry.id());
storage.insert(new_storage_entry.slot(), Arc::new(new_storage_entry));
⋮----
Ok(StorageAndNextAccountsFileId {
⋮----
fn reconstruct_accounts_db_via_serialization(
⋮----
let mut writer = Cursor::new(vec![]);
let snapshot_storages = accounts.get_storages(..=slot).0;
accountsdb_to_stream(
⋮----
&get_storages_to_serialize(&snapshot_storages),
⋮----
.unwrap();
let buf = writer.into_inner();
⋮----
let copied_accounts = TempDir::new().unwrap();
⋮----
copy_append_vecs(accounts, copied_accounts.path(), storage_access).unwrap();
let mut accounts_db = accountsdb_from_stream(
⋮----
.as_mut()
.unwrap()
.push(copied_accounts);
⋮----
fn check_accounts_local(accounts: &Accounts, pubkeys: &[Pubkey], num: usize) {
⋮----
let idx = rng().random_range(0..num - 1);
let ancestors = vec![(0, 0)].into_iter().collect();
let account = accounts.load_without_fixed_root(&ancestors, &pubkeys[idx]);
let account1 = Some((
AccountSharedData::new((idx + 1) as u64, 0, AccountSharedData::default().owner()),
⋮----
assert_eq!(account, account1);
⋮----
fn test_accounts_serialize(storage_access: StorageAccess) {
⋮----
let (_accounts_dir, paths) = get_temp_accounts_paths(4).unwrap();
⋮----
.take(100)
.collect();
for (i, pubkey) in pubkeys.iter().enumerate() {
⋮----
accounts.store_accounts_seq((slot, [(pubkey, &account)].as_slice()), None);
⋮----
check_accounts_local(&accounts, &pubkeys, 100);
accounts.accounts_db.add_root_and_flush_write_cache(slot);
⋮----
.calculate_accounts_lt_hash_at_startup_from_index(&Ancestors::default(), slot);
⋮----
&get_storages_to_serialize(&accounts.accounts_db.get_storages(..=slot).0),
⋮----
let storage_and_next_append_vec_id = copy_append_vecs(
⋮----
copied_accounts.path(),
⋮----
let (_accounts_dir, daccounts_paths) = get_temp_accounts_paths(2).unwrap();
⋮----
accountsdb_from_stream(
⋮----
.unwrap(),
⋮----
check_accounts_local(&daccounts, &pubkeys, 100);
⋮----
assert_eq!(accounts_hash, daccounts_hash);
⋮----
fn test_remove_unrooted_slot_snapshot(storage_access: StorageAccess) {
⋮----
db.store_for_tests((unrooted_slot, [(&key, &account0)].as_slice()));
db.remove_unrooted_slots(&[(unrooted_slot, unrooted_bank_id)]);
⋮----
db.store_for_tests((new_root, [(&key2, &account0)].as_slice()));
db.add_root_and_flush_write_cache(new_root);
let db = reconstruct_accounts_db_via_serialization(
⋮----
db.assert_load_account(new_root, key2, 1);
let unrooted_slot_ancestors = vec![(unrooted_slot, 1)].into_iter().collect();
assert!(db
⋮----
fn test_accounts_db_serialize1(
⋮----
let mut pubkeys: Vec<Pubkey> = vec![];
accounts.create_account(&mut pubkeys, 0, 100, 0, 0);
⋮----
accounts.add_root_and_flush_write_cache(0);
accounts.check_storage(0, 100, 100);
accounts.clean_accounts_for_tests();
accounts.check_accounts(&pubkeys, 0, 100, 1);
⋮----
accounts.modify_accounts(&pubkeys, 0, 100, 2);
⋮----
accounts.check_accounts(&pubkeys, 0, 100, 2);
let mut pubkeys1: Vec<Pubkey> = vec![];
⋮----
accounts.modify_accounts(&pubkeys, latest_slot, 10, 3);
let account = AccountSharedData::new(0, 0, AccountSharedData::default().owner());
accounts.store_for_tests((latest_slot, [(&pubkeys[30], &account)].as_slice()));
accounts.create_account(&mut pubkeys1, latest_slot, 10, 0, 0);
accounts.add_root_and_flush_write_cache(latest_slot);
accounts.check_storage(1, 21, 21);
⋮----
let mut pubkeys2: Vec<Pubkey> = vec![];
accounts.modify_accounts(&pubkeys, latest_slot, 20, 4);
⋮----
accounts.store_for_tests((latest_slot, [(&pubkeys[31], &account)].as_slice()));
accounts.create_account(&mut pubkeys2, latest_slot, 10, 0, 0);
⋮----
accounts.check_storage(2, 31, 31);
let ancestors = linear_ancestors(latest_slot);
⋮----
accounts.check_storage(0, 78, 100);
accounts.check_storage(1, 11, 21);
⋮----
let daccounts = reconstruct_accounts_db_via_serialization(
⋮----
assert_eq!(
⋮----
daccounts.print_count_and_status("daccounts");
daccounts.check_accounts(&pubkeys[35..], 0, 65, 37);
daccounts.check_accounts(&pubkeys1, 1, 10, 1);
⋮----
daccounts.check_storage(0, 78, 78);
daccounts.check_storage(1, 11, 11);
⋮----
daccounts.check_storage(0, 78, 100);
daccounts.check_storage(1, 11, 21);
⋮----
daccounts.check_storage(0, 100, 100);
daccounts.check_storage(1, 21, 21);
⋮----
daccounts.check_storage(2, 31, 31);
⋮----
fn test_accounts_db_serialize_zero_and_free(storage_access: StorageAccess) {
⋮----
let owner = *AccountSharedData::default().owner();
⋮----
accounts.store_for_tests((current_slot, [(&pubkey, &account)].as_slice()));
accounts.add_root(current_slot);
⋮----
accounts.store_for_tests((current_slot, [(&pubkey, &zero_lamport_account)].as_slice()));
accounts.store_for_tests((current_slot, [(&pubkey2, &account2)].as_slice()));
accounts.add_root_and_flush_write_cache(current_slot);
accounts.assert_load_account(current_slot, pubkey, zero_lamport);
accounts.print_accounts_stats("accounts");
⋮----
accounts.print_accounts_stats("accounts_post_purge");
let accounts = reconstruct_accounts_db_via_serialization(
⋮----
accounts.print_accounts_stats("reconstructed");
⋮----
fn with_chained_zero_lamport_accounts<F>(f: F)
⋮----
accounts.store_for_tests((current_slot, [(&purged_pubkey1, &account2)].as_slice()));
⋮----
accounts.store_for_tests((
⋮----
[(&purged_pubkey1, &zero_lamport_account)].as_slice(),
⋮----
accounts.store_for_tests((current_slot, [(&purged_pubkey2, &account3)].as_slice()));
⋮----
[(&purged_pubkey2, &zero_lamport_account)].as_slice(),
⋮----
accounts.store_for_tests((current_slot, [(&dummy_pubkey, &dummy_account)].as_slice()));
⋮----
accounts.print_accounts_stats("pre_f");
let accounts = f(accounts, current_slot);
accounts.print_accounts_stats("post_f");
accounts.assert_load_account(current_slot, pubkey, some_lamport);
accounts.assert_load_account(current_slot, purged_pubkey1, 0);
accounts.assert_load_account(current_slot, purged_pubkey2, 0);
accounts.assert_load_account(current_slot, dummy_pubkey, dummy_lamport);
⋮----
accounts.calculate_capitalization_at_startup_from_index(&Ancestors::default(), 4);
⋮----
assert_eq!(calculated_capitalization, expected_capitalization);
⋮----
fn test_accounts_purge_chained_purge_before_snapshot_restore(storage_access: StorageAccess) {
⋮----
with_chained_zero_lamport_accounts(|accounts, current_slot| {
accounts.set_latest_full_snapshot_slot(0);
⋮----
reconstruct_accounts_db_via_serialization(
⋮----
fn test_accounts_purge_chained_purge_after_snapshot_restore(storage_access: StorageAccess) {
⋮----
accounts.print_accounts_stats("after_reconstruct");
⋮----
fn test_accounts_purge_long_chained_after_snapshot_restore(storage_access: StorageAccess) {
⋮----
accounts.print_count_and_status("before reconstruct");
⋮----
accounts.print_count_and_status("before purge zero");
⋮----
accounts.print_count_and_status("after purge zero");
accounts.assert_load_account(current_slot, pubkey, old_lamport);
⋮----
fn test_accounts_clean_after_snapshot_restore_then_old_revives(storage_access: StorageAccess) {
⋮----
accounts.store_for_tests((current_slot, [(&pubkey1, &account)].as_slice()));
accounts.store_for_tests((current_slot, [(&pubkey2, &account)].as_slice()));
⋮----
assert_eq!(0, accounts.alive_account_count_in_slot(current_slot));
accounts.add_root_and_flush_write_cache(current_slot - 1);
accounts.assert_ref_count(&pubkey1, 1);
accounts.store_for_tests((current_slot, [(&pubkey1, &account2)].as_slice()));
⋮----
assert_eq!(1, accounts.alive_account_count_in_slot(current_slot));
accounts.assert_ref_count(&pubkey1, 2);
⋮----
accounts.store_for_tests((current_slot, [(&pubkey1, &account3)].as_slice()));
⋮----
accounts.assert_ref_count(&pubkey1, 3);
⋮----
accounts.store_for_tests((current_slot, [(&pubkey1, &zero_lamport_account)].as_slice()));
⋮----
.write()
⋮----
.remove(&current_slot);
⋮----
.insert(current_slot);
⋮----
accounts.assert_load_account(current_slot, pubkey1, zero_lamport);
accounts.assert_load_account(current_slot, pubkey2, old_lamport);
⋮----
((current_slot - 1)..=current_slot).for_each(|slot| accounts.flush_root_write_cache(slot));
⋮----
info!("pubkey: {pubkey1}");
accounts.print_accounts_stats("pre_clean");
⋮----
accounts.flush_root_write_cache(current_slot);
accounts.set_latest_full_snapshot_slot(current_slot);
⋮----
accounts.assert_not_load_account(current_slot, pubkey1);
⋮----
fn test_shrink_stale_slots_processed(storage_access: StorageAccess) {
⋮----
.map(|_| solana_pubkey::new_rand())
⋮----
accounts.store_for_tests((current_slot, [(pubkey, &account)].as_slice()));
⋮----
accounts.shrink_all_slots(*startup, &EpochSchedule::default(), None);
⋮----
.calculate_capitalization_at_startup_from_index(&no_ancestors, current_slot);
⋮----
.calculate_accounts_lt_hash_at_startup_from_index(&no_ancestors, current_slot);
⋮----
assert_eq!(accounts_lt_hash_pre, accounts_lt_hash_post);
accounts.shrink_all_slots(*startup, &epoch_schedule, None);
⋮----
fn test_remap_append_vec_file(
⋮----
let tmp = tempfile::tempdir().unwrap();
let old_path = tmp.path().join(format!("123.{old_id}"));
let expected_remapped_path = tmp.path().join(format!("123.{expected_remapped_id}"));
File::create(&old_path).unwrap();
become_ungovernable(tmp.path());
⋮----
remap_append_vec_file(123, old_id, &old_path, &next_append_vec_id, &num_collisions)
⋮----
assert_eq!(remapped_id as usize, expected_remapped_id);
assert_eq!(&remapped_path, &expected_remapped_path);
assert_eq!(num_collisions.load(Ordering::Relaxed), expected_collisions);
⋮----
fn test_remap_append_vec_file_error() {
⋮----
let original_path = tmp.path().join("123.456");
⋮----
remap_append_vec_file(

================
File: runtime/src/serde_snapshot/types.rs
================
pub struct SerdeAccountsLtHash(
⋮----
fn from(accounts_lt_hash: SerdeAccountsLtHash) -> Self {
Self(LtHash(accounts_lt_hash.0))
⋮----
fn from(accounts_lt_hash: AccountsLtHash) -> Self {
Self(accounts_lt_hash.0 .0)

================
File: runtime/src/serde_snapshot/utils.rs
================
use solana_frozen_abi::abi_example::TransparentAsHelper;
⋮----
pub fn serialize_iter_as_seq<I>(iter: I) -> impl Serialize
⋮----
struct SerializableSequencedIterator<I> {
⋮----
impl<I> TransparentAsHelper for SerializableSequencedIterator<I> {}
impl<I> Serialize for SerializableSequencedIterator<I>
⋮----
fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
⋮----
let iter = self.iter.borrow_mut().take().unwrap().into_iter();
let mut seq = serializer.serialize_seq(Some(iter.len()))?;
⋮----
seq.serialize_element(&item)?;
⋮----
seq.end()
⋮----
iter: std::cell::RefCell::new(Some(iter)),
⋮----
pub fn serialize_iter_as_tuple<I>(iter: I) -> impl Serialize
⋮----
let mut tup = serializer.serialize_tuple(iter.len())?;
⋮----
tup.serialize_element(&item)?;
⋮----
tup.end()
⋮----
pub fn serialize_iter_as_map<K, V, I>(iter: I) -> impl Serialize
⋮----
struct SerializableMappedIterator<I> {
⋮----
impl<I> TransparentAsHelper for SerializableMappedIterator<I> {}
impl<K, V, I> Serialize for SerializableMappedIterator<I>
⋮----
serializer.collect_map(self.iter.borrow_mut().take().unwrap())

================
File: runtime/src/snapshot_package/compare.rs
================
pub fn cmp_snapshot_packages_by_priority(a: &SnapshotPackage, b: &SnapshotPackage) -> Ordering {
cmp_snapshot_kinds_by_priority(&a.snapshot_kind, &b.snapshot_kind).then(a.slot.cmp(&b.slot))
⋮----
pub fn cmp_snapshot_kinds_by_priority(a: &SnapshotKind, b: &SnapshotKind) -> Ordering {
⋮----
cmp_snapshot_archive_kinds_by_priority(snapshot_archive_kind_a, snapshot_archive_kind_b)
⋮----
pub fn cmp_snapshot_archive_kinds_by_priority(
⋮----
base_slot_a.cmp(base_slot_b)
⋮----
pub fn are_snapshot_packages_the_same_kind(a: &SnapshotPackage, b: &SnapshotPackage) -> bool {
are_snapshot_kinds_the_same_kind(&a.snapshot_kind, &b.snapshot_kind)
⋮----
pub fn are_snapshot_kinds_the_same_kind(a: &SnapshotKind, b: &SnapshotKind) -> bool {
⋮----
are_snapshot_archive_kinds_the_same_kind(archive_a, archive_b)
⋮----
pub fn are_snapshot_archive_kinds_the_same_kind(
⋮----
mod tests {
⋮----
fn new(snapshot_kind: SnapshotKind, slot: Slot) -> SnapshotPackage {
⋮----
fn test_cmp_snapshot_packages_by_priority() {
⋮----
new(SnapshotKind::Archive(SnapshotArchiveKind::Full), 11),
new(SnapshotKind::Archive(SnapshotArchiveKind::Full), 22),
⋮----
new(SnapshotKind::Archive(SnapshotArchiveKind::Full), 33),
⋮----
new(
⋮----
cmp_snapshot_packages_by_priority(&snapshot_package_a, &snapshot_package_b);
assert_eq!(expected_result, actual_result);
⋮----
fn test_cmp_snapshot_kinds_by_priority() {
⋮----
let actual_result = cmp_snapshot_kinds_by_priority(&snapshot_kind_a, &snapshot_kind_b);
⋮----
fn test_cmp_snapshot_archive_kinds_by_priority() {
⋮----
let actual_result = cmp_snapshot_archive_kinds_by_priority(
⋮----
fn test_are_snapshot_packages_the_same_kind() {
⋮----
are_snapshot_packages_the_same_kind(&snapshot_package_a, &snapshot_package_b);
⋮----
fn test_are_snapshot_kinds_the_same_kind() {
⋮----
are_snapshot_kinds_the_same_kind(&snapshot_kind_a, &snapshot_kind_b);

================
File: runtime/src/snapshot_utils/snapshot_storage_rebuilder.rs
================
pub(crate) struct SnapshotStorageRebuilder {
⋮----
impl SnapshotStorageRebuilder {
pub(crate) fn rebuild_storage(
⋮----
let snapshot_storage_lengths = snapshot_storage_lengths_from_fields(accounts_db_fields);
⋮----
Ok(account_storage_map)
⋮----
fn new(
⋮----
let storage = AccountStorageMap::with_capacity(snapshot_storage_lengths.len());
⋮----
.iter()
.map(|(slot, storage_lengths)| {
(*slot, Mutex::new(Vec::with_capacity(storage_lengths.len())))
⋮----
.collect();
⋮----
fn spawn_rebuilder_threads(
⋮----
let thread_pool = rebuilder.build_thread_pool();
⋮----
thread_pool.install(|| rebuilder.process_buffered_files(append_vec_files))?;
⋮----
let (exit_sender, exit_receiver) = unbounded();
⋮----
Self::spawn_receiver_thread(&thread_pool, exit_sender.clone(), rebuilder.clone());
⋮----
drop(exit_sender);
rebuilder.wait_for_completion(exit_receiver)?;
Ok(Arc::try_unwrap(rebuilder).unwrap().storage)
⋮----
fn process_buffered_files(&self, append_vec_files: Vec<PathBuf>) -> Result<(), SnapshotError> {
⋮----
.into_par_iter()
.map(|path| self.process_append_vec_file(path))
⋮----
fn spawn_receiver_thread(
⋮----
thread_pool.spawn(move || {
for path in rebuilder.file_receiver.iter() {
match rebuilder.process_append_vec_file(path) {
⋮----
.send(Err(err))
.expect("sender should be connected");
⋮----
.send(Ok(()))
⋮----
fn process_append_vec_file(&self, path: PathBuf) -> Result<(), SnapshotError> {
let filename = path.file_name().unwrap().to_str().unwrap().to_owned();
if let Ok((slot, append_vec_id)) = get_slot_and_append_vec_id(&filename) {
⋮----
.fetch_max((append_vec_id + 1) as AccountsFileId, Ordering::Relaxed);
⋮----
let slot_storage_count = self.insert_storage_file(&slot, path);
if slot_storage_count == self.snapshot_storage_lengths.get(&slot).unwrap().len() {
self.process_complete_slot(slot)?;
self.processed_slot_count.fetch_add(1, Ordering::AcqRel);
⋮----
Ok(())
⋮----
fn insert_storage_file(&self, slot: &Slot, path: PathBuf) -> usize {
let slot_paths = self.storage_paths.get(slot).unwrap();
let mut lock = slot_paths.lock().unwrap();
lock.push(path);
lock.len()
⋮----
fn process_complete_slot(&self, slot: Slot) -> Result<(), SnapshotError> {
let slot_storage_paths = self.storage_paths.get(&slot).unwrap();
let lock = slot_storage_paths.lock().unwrap();
⋮----
.map(|path| {
let filename = path.file_name().unwrap().to_str().unwrap();
let (_, old_append_vec_id) = get_slot_and_append_vec_id(filename)?;
⋮----
.get(&slot)
.unwrap()
.get(&old_append_vec_id)
.unwrap();
⋮----
SnapshotFrom::Archive => remap_and_reconstruct_single_storage(
⋮----
path.as_path(),
⋮----
SnapshotFrom::Dir => reconstruct_single_storage(
⋮----
.as_ref()
.and_then(|accounts| accounts.remove(&slot)),
⋮----
Ok(storage_entry)
⋮----
if slot_stores.len() != 1 {
return Err(SnapshotError::RebuildStorages(format!(
⋮----
let storage = slot_stores.into_iter().next().unwrap();
self.storage.insert(slot, storage);
⋮----
fn wait_for_completion(
⋮----
let num_slots = self.snapshot_storage_lengths.len();
⋮----
select! {
⋮----
fn build_thread_pool(&self) -> ThreadPool {
⋮----
.thread_name(|i| format!("solRbuildSnap{i:02}"))
.num_threads(self.num_threads)
.build()
.expect("new rayon threadpool")
⋮----
pub(crate) fn get_slot_and_append_vec_id(filename: &str) -> Result<(Slot, usize), SnapshotError> {
let mut parts = filename.splitn(2, '.');
let slot = parts.next().and_then(|s| Slot::from_str(s).ok());
let id = parts.next().and_then(|s| usize::from_str(s).ok());
slot.zip(id)
.ok_or_else(|| SnapshotError::InvalidAppendVecPath(PathBuf::from(filename)))
⋮----
mod tests {
⋮----
fn test_get_slot_and_append_vec_id() {
⋮----
get_slot_and_append_vec_id(&AccountsFile::file_name(expected_slot, expected_id))
⋮----
assert_eq!(expected_slot, slot);
assert_eq!(expected_id as usize, id);

================
File: runtime/src/stakes/serde_stakes.rs
================
pub enum SerdeStakesToStakeFormat {
⋮----
impl SerdeStakesToStakeFormat {
pub fn vote_accounts(&self) -> &VoteAccounts {
⋮----
Self::Stake(stakes) => stakes.vote_accounts(),
Self::Account(stakes) => stakes.vote_accounts(),
⋮----
pub fn staked_nodes(&self) -> Arc<HashMap<Pubkey, u64>> {
⋮----
Self::Stake(stakes) => stakes.staked_nodes(),
Self::Account(stakes) => stakes.staked_nodes(),
⋮----
fn eq(&self, other: &Self) -> bool {
⋮----
stakes == &Stakes::<Stake>::from(other.clone())
⋮----
other == &Stakes::<Stake>::from(stakes.clone())
⋮----
fn from(stakes: Stakes<StakeAccount>) -> Self {
⋮----
impl Serialize for SerdeStakesToStakeFormat {
fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
⋮----
Self::Stake(stakes) => stakes.serialize(serializer),
Self::Account(stakes) => serialize_stake_accounts_to_stake_format(stakes, serializer),
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
Ok(Self::Stake(stakes))
⋮----
pub(crate) fn serialize_stake_accounts_to_delegation_format<S: Serializer>(
⋮----
SerdeStakeAccountsToDelegationFormat::from(stakes.clone()).serialize(serializer)
⋮----
fn serialize_stake_accounts_to_stake_format<S: Serializer>(
⋮----
SerdeStakeAccountsToStakeFormat::from(stakes.clone()).serialize(serializer)
⋮----
stake_delegations: SerdeStakeAccountMapToDelegationFormat(stake_delegations),
⋮----
stake_delegations: SerdeStakeAccountMapToStakeFormat(stake_delegations),
⋮----
struct SerdeStakeAccountsToDelegationFormat {
⋮----
struct SerdeStakeAccountsToStakeFormat {
⋮----
struct SerdeStakeAccountMapToDelegationFormat(ImHashMap<Pubkey, StakeAccount>);
impl Serialize for SerdeStakeAccountMapToDelegationFormat {
⋮----
let mut s = serializer.serialize_map(Some(self.0.len()))?;
for (pubkey, stake_account) in self.0.iter() {
s.serialize_entry(pubkey, stake_account.delegation())?;
⋮----
s.end()
⋮----
struct SerdeStakeAccountMapToStakeFormat(ImHashMap<Pubkey, StakeAccount>);
impl Serialize for SerdeStakeAccountMapToStakeFormat {
⋮----
s.serialize_entry(pubkey, stake_account.stake())?;
⋮----
mod tests {
⋮----
fn test_serde_stakes_to_stake_format() {
⋮----
stake_delegations.insert(
⋮----
.unwrap(),
⋮----
let wrapped_stakes = SerdeStakesToStakeFormat::Account(stake_account_stakes.clone());
let serialized_stakes = bincode::serialize(&wrapped_stakes).unwrap();
let stake_stakes = bincode::deserialize::<Stakes<Stake>>(&serialized_stakes).unwrap();
⋮----
assert_eq!(expected_stake_stakes, stake_stakes);
⋮----
fn test_serde_stakes_to_delegation_format() {
⋮----
struct SerializableDummy {
⋮----
struct DeserializableDummy {
⋮----
fn from(dummy: SerializableDummy) -> Self {
⋮----
stakes: dummy.stakes.into(),
⋮----
unused: rng.random(),
epoch: rng.random(),
⋮----
for _ in 0..rng.random_range(5usize..10) {
⋮----
let commission = rng.random_range(0..101);
⋮----
rng.random_range(0..1_000_000),
⋮----
stakes_cache.check_and_store(&vote_pubkey, &vote_account, None);
for _ in 0..rng.random_range(10usize..20) {
⋮----
let rent = Rent::with_slots_per_epoch(rng.random());
⋮----
stakes_cache.check_and_store(&stake_pubkey, &stake_account, None);
⋮----
let stakes: Stakes<StakeAccount> = stakes_cache.stakes().clone();
assert!(stakes.vote_accounts.as_ref().len() >= 5);
assert!(stakes.stake_delegations.len() >= 50);
⋮----
stakes: stakes.clone(),
⋮----
assert!(dummy.stakes.vote_accounts().as_ref().len() >= 5);
let data = bincode::serialize(&dummy).unwrap();
let other: DeserializableDummy = bincode::deserialize(&data).unwrap();
assert_eq!(other, dummy.into());
⋮----
assert_eq!(other.stakes, stakes)

================
File: runtime/src/account_saver.rs
================
fn max_number_of_accounts_to_collect(
⋮----
.iter()
.zip(txs)
.filter_map(|(processing_result, tx)| {
⋮----
.processed_transaction()
.map(|processed_tx| (processed_tx, tx))
⋮----
.map(|(processed_tx, tx)| match processed_tx {
⋮----
Ok(_) => tx.num_write_locks() as usize,
Err(_) => executed_tx.loaded_transaction.rollback_accounts.count(),
⋮----
ProcessedTransaction::FeesOnly(fees_only_tx) => fees_only_tx.rollback_accounts.count(),
⋮----
.sum()
⋮----
pub fn collect_accounts_to_store<'a, T: SVMMessage>(
⋮----
let collect_capacity = max_number_of_accounts_to_collect(txs, processing_results);
⋮----
.is_some()
.then(|| Vec::with_capacity(collect_capacity));
for (index, (processing_result, transaction)) in processing_results.iter().zip(txs).enumerate()
⋮----
let Some(processed_tx) = processing_result.processed_transaction() else {
// Don't store any accounts if tx wasn't executed
⋮----
let transaction_ref = txs_refs.as_ref().map(|txs_refs| txs_refs[index].borrow());
⋮----
if executed_tx.execution_details.status.is_ok() {
collect_accounts_for_successful_tx(
⋮----
collect_accounts_for_failed_tx(
⋮----
fn collect_accounts_for_successful_tx<'a, T: SVMMessage>(
⋮----
for (i, (address, account)) in (0..transaction.account_keys().len()).zip(transaction_accounts) {
if !transaction.is_writable(i) {
⋮----
// Accounts that are invoked and also not passed as an instruction
// account to a program don't need to be stored because it's assumed
if transaction.is_invoked(i) && !transaction.is_instruction_account(i) {
⋮----
collected_accounts.push((address, account));
⋮----
.push(transaction_ref.expect("transaction ref must exist if collecting"));
⋮----
fn collect_accounts_for_failed_tx<'a>(
⋮----
mod tests {
⋮----
fn new_sanitized_tx<T: Signers>(
⋮----
fn new_executed_processing_result(
⋮----
Ok(ProcessedTransaction::Executed(Box::new(
⋮----
fn test_collect_accounts_to_store() {
⋮----
let instructions = vec![CompiledInstruction::new(2, &(), vec![0, 1])];
⋮----
vec![keypair0.pubkey(), pubkey, native_loader::id()],
⋮----
let transaction_accounts0 = vec![
⋮----
let tx0 = new_sanitized_tx(&[&keypair0], message, Hash::default());
⋮----
vec![keypair1.pubkey(), pubkey, native_loader::id()],
⋮----
let transaction_accounts1 = vec![
⋮----
let tx1 = new_sanitized_tx(&[&keypair1], message, Hash::default());
⋮----
program_indices: vec![],
⋮----
let txs = vec![tx0.clone(), tx1.clone()];
let processing_results = vec![
⋮----
let max_collected_accounts = max_number_of_accounts_to_collect(&txs, &processing_results);
assert_eq!(max_collected_accounts, 2);
⋮----
let transaction_refs = collect_transactions.then(|| txs.iter().collect::<Vec<_>>());
⋮----
collect_accounts_to_store(&txs, &transaction_refs, &processing_results);
assert_eq!(collected_accounts.len(), 2);
assert!(collected_accounts
⋮----
let transactions = transactions.unwrap();
assert_eq!(transactions.len(), 2);
assert!(transactions.iter().any(|txn| (*txn).eq(&tx0)));
assert!(transactions.iter().any(|txn| (*txn).eq(&tx1)));
⋮----
assert!(transactions.is_none());
⋮----
fn test_collect_accounts_for_failed_tx_rollback_fee_payer_only() {
let from = keypair_from_seed(&[1; 32]).unwrap();
let from_address = from.pubkey();
⋮----
let instructions = vec![system_instruction::transfer(&from_address, &to_address, 42)];
let message = Message::new(&instructions, Some(&from_address));
⋮----
let transaction_accounts = vec![
⋮----
let tx = new_sanitized_tx(&[&from], message, blockhash);
⋮----
fee_payer: (from_address, from_account_pre.clone()),
⋮----
let txs = vec![tx];
let processing_results = vec![new_executed_processing_result(
⋮----
assert_eq!(max_collected_accounts, 1);
⋮----
assert_eq!(collected_accounts.len(), 1);
assert_eq!(
⋮----
assert_eq!(transactions.len(), collected_accounts.len());
⋮----
fn test_collect_accounts_for_failed_tx_rollback_separate_nonce_and_fee_payer() {
⋮----
let nonce_authority = keypair_from_seed(&[0; 32]).unwrap();
⋮----
nonce_authority.pubkey(),
⋮----
AccountSharedData::new_data(43, &nonce_state, &system_program::id()).unwrap();
⋮----
let instructions = vec![
⋮----
let tx = new_sanitized_tx(&[&nonce_authority, &from], message, blockhash);
⋮----
AccountSharedData::new_data(42, &nonce_state, &system_program::id()).unwrap();
⋮----
nonce: (nonce_address, nonce_account_pre.clone()),
⋮----
.find(|(pubkey, _account)| *pubkey == &nonce_address)
.map(|(_pubkey, account)| *account)
.cloned()
.unwrap();
⋮----
assert!(nonce_account::verify_nonce_account(
⋮----
fn test_collect_accounts_for_failed_tx_rollback_same_nonce_and_fee_payer() {
⋮----
let nonce_address = nonce_authority.pubkey();
⋮----
let message = Message::new(&instructions, Some(&nonce_address));
⋮----
fn test_collect_accounts_for_failed_fees_only_tx() {
⋮----
let processing_results = vec![Ok(ProcessedTransaction::FeesOnly(Box::new(

================
File: runtime/src/accounts_background_service.rs
================
mod pending_snapshot_packages;
mod stats;
pub use pending_snapshot_packages::PendingSnapshotPackages;
⋮----
use qualifier_attr::qualifiers;
⋮----
pub type SnapshotRequestSender = Sender<SnapshotRequest>;
pub type SnapshotRequestReceiver = Receiver<SnapshotRequest>;
pub type DroppedSlotsSender = Sender<(Slot, BankId)>;
pub type DroppedSlotsReceiver = Receiver<(Slot, BankId)>;
⋮----
struct PrunedBankQueueLenReporter {
⋮----
impl PrunedBankQueueLenReporter {
fn report(&self, q_len: usize) {
⋮----
let last_report_time = self.last_report_time.load(Ordering::Acquire);
⋮----
&& now.saturating_sub(last_report_time) > BANK_DROP_SIGNAL_CHANNEL_REPORT_INTERVAL
⋮----
datapoint_warn!("excessive_pruned_bank_channel_len", ("len", q_len, i64));
self.last_report_time.store(now, Ordering::Release);
⋮----
pub struct SendDroppedBankCallback {
⋮----
impl DropCallback for SendDroppedBankCallback {
fn callback(&self, bank: &Bank) {
BANK_DROP_QUEUE_REPORTER.report(self.sender.len());
if let Err(SendError(_)) = self.sender.send((bank.slot(), bank.bank_id())) {
info!("bank DropCallback signal queue disconnected.");
⋮----
fn clone_box(&self) -> Box<dyn DropCallback + Send + Sync> {
Box::new(self.clone())
⋮----
impl Debug for SendDroppedBankCallback {
fn fmt(&self, f: &mut Formatter) -> fmt::Result {
write!(f, "SendDroppedBankCallback({self:p})")
⋮----
impl SendDroppedBankCallback {
pub fn new(sender: DroppedSlotsSender) -> Self {
⋮----
pub struct SnapshotRequest {
⋮----
impl Debug for SnapshotRequest {
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
f.debug_struct("SnapshotRequest")
.field("request kind", &self.request_kind)
.field("bank slot", &self.snapshot_root_bank.slot())
.field("block height", &self.snapshot_root_bank.block_height())
.finish_non_exhaustive()
⋮----
/// What kind of request is this?
#[derive(Debug, Copy, Clone, Eq, PartialEq)]
pub enum SnapshotRequestKind {
⋮----
pub struct SnapshotRequestHandler {
⋮----
impl SnapshotRequestHandler {
// Returns the latest requested snapshot slot and storages
⋮----
pub fn handle_snapshot_requests(
⋮----
self.get_next_snapshot_request()?;
datapoint_info!(
⋮----
let snapshot_kind = new_snapshot_kind(&snapshot_request)?;
Some(self.handle_snapshot_request(non_snapshot_time_us, snapshot_request, snapshot_kind))
⋮----
/// Get the next snapshot request to handle
    ///
⋮----
///
    /// Look through the snapshot request channel to find the highest priority one to handle next.
⋮----
/// Look through the snapshot request channel to find the highest priority one to handle next.
    /// If there are no snapshot requests in the channel, return None.  Otherwise return the
⋮----
/// If there are no snapshot requests in the channel, return None.  Otherwise return the
    /// highest priority one.  Unhandled snapshot requests with slots GREATER-THAN the handled one
⋮----
/// highest priority one.  Unhandled snapshot requests with slots GREATER-THAN the handled one
    /// will be re-enqueued.  The remaining will be dropped.
⋮----
/// will be re-enqueued.  The remaining will be dropped.
    ///
⋮----
///
    /// Also return the number of snapshot requests initially in the channel, and the number of
⋮----
/// Also return the number of snapshot requests initially in the channel, and the number of
    /// ones re-enqueued.
⋮----
/// ones re-enqueued.
    fn get_next_snapshot_request(
⋮----
fn get_next_snapshot_request(
⋮----
/*num outstanding snapshot requests*/ usize,
/*num re-enqueued snapshot requests*/ usize,
⋮----
let mut requests: Vec<_> = self.snapshot_request_receiver.try_iter().collect();
let requests_len = requests.len();
debug!("outstanding snapshot requests ({requests_len}): {requests:?}");
⋮----
// SAFETY: We know the len is 1, so `pop` will return `Some`
let snapshot_request = requests.pop().unwrap();
Some((snapshot_request, 1, 0))
⋮----
requests.select_nth_unstable_by(requests_len - 1, cmp_requests_by_priority);
// SAFETY: We know the len is > 1, so `pop` will return `Some`
⋮----
let handled_request_slot = snapshot_request.snapshot_root_bank.slot();
// re-enqueue any remaining requests for slots GREATER-THAN the one that will be handled
⋮----
.into_iter()
.filter(|snapshot_request| {
snapshot_request.snapshot_root_bank.slot() > handled_request_slot
⋮----
.map(|snapshot_request| {
⋮----
.request_sender()
.try_send(snapshot_request)
.expect("re-enqueue snapshot request");
⋮----
.count();
Some((snapshot_request, requests_len, num_re_enqueued_requests))
⋮----
fn handle_snapshot_request(
⋮----
info!("handling snapshot request: {snapshot_request:?}, {snapshot_kind:?}");
⋮----
if snapshot_kind.is_full_snapshot() {
// The latest full snapshot slot is what accounts-db uses to properly handle
// zero lamport accounts.  We are handling a full snapshot request here, and
// since taking a snapshot is not allowed to fail, we can update accounts-db now.
⋮----
.set_latest_full_snapshot_slot(snapshot_root_bank.slot());
⋮----
// Forced cache flushing MUST flush all roots <= snapshot_root_bank.slot().
// That's because `snapshot_root_bank.slot()` must be root at this point,
snapshot_root_bank.force_flush_accounts_cache();
assert!(
⋮----
flush_accounts_cache_time.stop();
⋮----
snapshot_root_bank.clean_accounts();
clean_time.stop();
let (_, shrink_ancient_time_us) = measure_us!(snapshot_root_bank.shrink_ancient_slots());
⋮----
snapshot_root_bank.shrink_candidate_slots();
shrink_time.stop();
⋮----
snapshot_root_bank.get_snapshot_storages(None),
⋮----
.lock()
.unwrap()
.push(snapshot_package);
snapshot_time.stop();
info!(
⋮----
total_time.stop();
⋮----
Ok(snapshot_root_bank.slot())
⋮----
fn peek_next_snapshot_request_slot(&self) -> Option<Slot> {
let (next_request, _, _) = self.get_next_snapshot_request()?;
let next_slot = next_request.snapshot_root_bank.slot();
⋮----
.try_send(next_request)
⋮----
Some(next_slot)
⋮----
pub struct PrunedBanksRequestHandler {
⋮----
impl PrunedBanksRequestHandler {
⋮----
fn handle_request(&self, bank: &Bank) -> usize {
let mut banks_to_purge: Vec<_> = self.pruned_banks_receiver.try_iter().collect();
banks_to_purge.sort_by_key(|(slot, _id)| *slot);
let num_banks_to_purge = banks_to_purge.len();
let grouped_banks_to_purge: Vec<_> = banks_to_purge.chunk_by(|a, b| a.0 == b.0).collect();
⋮----
num_banks_to_purge.saturating_sub(grouped_banks_to_purge.len());
⋮----
let accounts_db = bank.rc.accounts.accounts_db.as_ref();
accounts_db.thread_pool_background.install(|| {
grouped_banks_to_purge.into_par_iter().for_each(|group| {
group.iter().for_each(|(slot, bank_id)| {
accounts_db.purge_slot(*slot, *bank_id, true);
⋮----
fn remove_dead_slots(
⋮----
*removed_slots_count += self.handle_request(bank);
remove_slots_time.stop();
*total_remove_slots_time += remove_slots_time.as_us();
⋮----
pub struct AbsRequestHandlers {
⋮----
impl AbsRequestHandlers {
⋮----
.handle_snapshot_requests(non_snapshot_time_us)
⋮----
pub struct AccountsBackgroundService {
⋮----
impl AccountsBackgroundService {
pub fn new(
⋮----
.name("solAcctsBgSvc".to_string())
.spawn({
let is_running = is_running.clone();
let stop = stop.clone();
⋮----
info!("AccountsBackgroundService has started");
⋮----
if exit.load(Ordering::Relaxed) || stop.load(Ordering::Relaxed) {
⋮----
let bank = bank_forks.read().unwrap().root_bank();
⋮----
.remove_dead_slots(
⋮----
.map(|last_snapshot_end_time: Instant| {
last_snapshot_end_time.elapsed().as_micros()
⋮----
.unwrap_or_default();
⋮----
request_handlers.handle_snapshot_requests(non_snapshot_time);
⋮----
last_snapshot_end_time = Some(Instant::now());
⋮----
error!(
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
.peek_next_snapshot_request_slot();
⋮----
next_snapshot_request_slot.unwrap_or(Slot::MAX),
bank.slot(),
⋮----
.saturating_sub(1);
let duration_since_previous_clean = previous_clean_time.elapsed();
⋮----
.flush_accounts_cache(force_flush, Some(max_clean_slot_inclusive));
⋮----
bank.rc.accounts.accounts_db.clean_accounts(
Some(max_clean_slot_inclusive),
⋮----
bank.epoch_schedule(),
⋮----
let duration_since_previous_shrink = previous_shrink_time.elapsed();
⋮----
bank.shrink_ancient_slots();
⋮----
bank.shrink_candidate_slots();
⋮----
stats.record_and_maybe_submit(start_time.elapsed());
sleep(Duration::from_millis(INTERVAL_MS));
⋮----
info!("AccountsBackgroundService has stopped");
is_running.store(false, Ordering::Relaxed);
⋮----
.unwrap();
⋮----
pub fn setup_bank_drop_callback(bank_forks: Arc<RwLock<BankForks>>) -> DroppedSlotsReceiver {
assert_eq!(bank_forks.read().unwrap().banks().len(), 1);
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
⋮----
.enable_bank_drop_callback();
root_bank.set_callback(Some(Box::new(SendDroppedBankCallback::new(
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_background.join()
⋮----
pub fn status(&self) -> &AbsStatus {
⋮----
pub struct AbsStatus {
⋮----
impl AbsStatus {
pub fn is_running(&self) -> bool {
self.is_running.load(Ordering::Relaxed)
⋮----
pub fn stop(&self) {
self.stop.store(true, Ordering::Relaxed)
⋮----
pub fn new_for_tests() -> Self {
⋮----
fn new_snapshot_kind(snapshot_request: &SnapshotRequest) -> Option<SnapshotKind> {
⋮----
SnapshotRequestKind::FullSnapshot => Some(SnapshotKind::Archive(SnapshotArchiveKind::Full)),
⋮----
.latest_full_snapshot_slot()
⋮----
Some(SnapshotKind::Archive(SnapshotArchiveKind::Incremental(
⋮----
warn!(
⋮----
fn cmp_requests_by_priority(a: &SnapshotRequest, b: &SnapshotRequest) -> cmp::Ordering {
let slot_a = a.snapshot_root_bank.slot();
let slot_b = b.snapshot_root_bank.slot();
cmp_snapshot_request_kinds_by_priority(&a.request_kind, &b.request_kind)
.then(slot_a.cmp(&slot_b))
⋮----
fn cmp_snapshot_request_kinds_by_priority(
⋮----
mod test {
⋮----
fn test_accounts_background_service_remove_dead_slots() {
let genesis = create_genesis_config(10);
⋮----
let (pruned_banks_sender, pruned_banks_receiver) = unbounded();
⋮----
bank0.store_account(
⋮----
assert!(bank0.get_account(&account_key).is_some());
pruned_banks_sender.send((0, 0)).unwrap();
assert!(!bank0.rc.accounts.scan_slot(0, |_| Some(())).is_empty());
pruned_banks_request_handler.remove_dead_slots(&bank0, &mut 0, &mut 0);
assert!(bank0.rc.accounts.scan_slot(0, |_| Some(())).is_empty());
⋮----
fn test_get_next_snapshot_request() {
⋮----
NonZeroU64::new(FULL_SNAPSHOT_INTERVAL).unwrap(),
⋮----
NonZeroU64::new(INCREMENTAL_SNAPSHOT_INTERVAL).unwrap(),
⋮----
snapshot_request_sender.clone(),
⋮----
snapshot_request_sender.send(snapshot_request).unwrap();
⋮----
let mut genesis_config_info = create_genesis_config(10);
⋮----
let bank0 = bank.clone();
fn latest_full_snapshot_slot(bank: &Bank) -> Option<Slot> {
bank.rc.accounts.accounts_db.latest_full_snapshot_slot()
⋮----
fn set_latest_full_snapshot_slot(bank: &Bank, slot: Slot) {
⋮----
.set_latest_full_snapshot_slot(slot);
⋮----
let slot = bank.slot() + 1;
⋮----
bank.clone(),
⋮----
if bank.block_height().is_multiple_of(FULL_SNAPSHOT_INTERVAL) {
send_snapshot_request(Arc::clone(&bank), SnapshotRequestKind::FullSnapshot);
⋮----
.block_height()
.is_multiple_of(INCREMENTAL_SNAPSHOT_INTERVAL)
⋮----
send_snapshot_request(
⋮----
make_banks(303);
assert_eq!(latest_full_snapshot_slot(&bank0), None);
⋮----
.get_next_snapshot_request()
⋮----
assert_eq!(
⋮----
assert_eq!(snapshot_request.snapshot_root_bank.slot(), 240);
set_latest_full_snapshot_slot(&bank0, 240);
assert_eq!(latest_full_snapshot_slot(&bank0), Some(240));
⋮----
assert_eq!(snapshot_request.snapshot_root_bank.slot(), 300);
⋮----
assert!(snapshot_request_handler
⋮----
fn test_pruned_banks_request_handler_handle_request() {
⋮----
let genesis_config_info = create_genesis_config(10);
⋮----
bank.rc.accounts.accounts_db.enable_bank_drop_callback();
bank.set_callback(Some(Box::new(SendDroppedBankCallback::new(
⋮----
fork0_bank0.clone(),
⋮----
fork0_bank0.slot() + 1,
⋮----
fork0_bank1.clone(),
⋮----
fork0_bank1.slot() + 1,
⋮----
fork1_bank1.clone(),
⋮----
fork1_bank1.slot() + 1,
⋮----
fork0_bank2.clone(),
⋮----
fork0_bank2.slot() + 1,
⋮----
fork0_bank3.squash();
drop(fork3_bank3);
drop(fork1_bank2);
drop(fork0_bank2);
drop(fork1_bank1);
drop(fork2_bank1);
drop(fork0_bank1);
drop(fork0_bank0);
let num_banks_purged = pruned_banks_request_handler.handle_request(&fork0_bank3);
assert_eq!(num_banks_purged, 7);
⋮----
fn test_cmp_snapshot_request_kinds_by_priority() {
⋮----
let actual_result = cmp_snapshot_request_kinds_by_priority(
⋮----
assert_eq!(expected_result, actual_result);

================
File: runtime/src/bank_client.rs
================
mod transaction {
⋮----
pub struct BankClient {
⋮----
impl Client for BankClient {
fn tpu_addr(&self) -> String {
"Local BankClient".to_string()
⋮----
impl AsyncClient for BankClient {
fn async_send_versioned_transaction(
⋮----
let signature = transaction.signatures.first().cloned().unwrap_or_default();
let transaction_sender = self.transaction_sender.clone();
transaction_sender.send(transaction).unwrap();
Ok(signature)
⋮----
impl SyncClient for BankClient {
fn send_and_confirm_message<T: Signers + ?Sized>(
⋮----
let blockhash = self.bank.last_blockhash();
⋮----
self.bank.process_transaction(&transaction)?;
Ok(transaction.signatures.first().cloned().unwrap_or_default())
⋮----
fn send_and_confirm_instruction(
⋮----
let message = Message::new(&[instruction], Some(&keypair.pubkey()));
self.send_and_confirm_message(&[keypair], message)
⋮----
fn transfer_and_confirm(
⋮----
system_instruction::transfer(&keypair.pubkey(), pubkey, lamports);
self.send_and_confirm_instruction(keypair, transfer_instruction)
⋮----
fn get_account_data(&self, pubkey: &Pubkey) -> Result<Option<Vec<u8>>> {
Ok(self
⋮----
.get_account(pubkey)
.map(|account| Account::from(account).data))
⋮----
fn get_account(&self, pubkey: &Pubkey) -> Result<Option<Account>> {
Ok(self.bank.get_account(pubkey).map(Account::from))
⋮----
fn get_account_with_commitment(
⋮----
fn get_balance(&self, pubkey: &Pubkey) -> Result<u64> {
Ok(self.bank.get_balance(pubkey))
⋮----
fn get_balance_with_commitment(
⋮----
fn get_minimum_balance_for_rent_exemption(&self, data_len: usize) -> Result<u64> {
Ok(self.bank.get_minimum_balance_for_rent_exemption(data_len))
⋮----
fn get_signature_status(
⋮----
Ok(self.bank.get_signature_status(signature))
⋮----
fn get_signature_status_with_commitment(
⋮----
fn get_slot(&self) -> Result<u64> {
Ok(self.bank.slot())
⋮----
fn get_slot_with_commitment(&self, _commitment_config: CommitmentConfig) -> Result<u64> {
⋮----
fn get_transaction_count(&self) -> Result<u64> {
Ok(self.bank.transaction_count())
⋮----
fn get_transaction_count_with_commitment(
⋮----
fn poll_for_signature_confirmation(
⋮----
assert_eq!(
⋮----
if self.bank.get_signature_status(signature).is_some() {
⋮----
if now.elapsed().as_secs() > 15 {
return Err(TransportError::IoError(io::Error::other(format!(
⋮----
sleep(Duration::from_millis(250));
⋮----
Ok(confirmed_blocks)
⋮----
fn poll_for_signature(&self, signature: &Signature) -> Result<()> {
⋮----
let response = self.bank.get_signature_status(signature);
⋮----
if res.is_ok() {
⋮----
Ok(())
⋮----
fn get_epoch_info(&self) -> Result<EpochInfo> {
Ok(self.bank.get_epoch_info())
⋮----
fn get_latest_blockhash(&self) -> Result<Hash> {
Ok(self.bank.last_blockhash())
⋮----
fn get_latest_blockhash_with_commitment(
⋮----
.get_blockhash_last_valid_block_height(&blockhash)
.expect("bank blockhash queue should contain blockhash");
Ok((blockhash, last_valid_block_height))
⋮----
fn is_blockhash_valid(
⋮----
Ok(self.bank.is_blockhash_valid(blockhash))
⋮----
fn get_fee_for_message(&self, message: &Message) -> Result<u64> {
⋮----
message.clone(),
self.bank.get_reserved_account_keys(),
⋮----
.ok()
.and_then(|sanitized_message| self.bank.get_fee_for_message(&sanitized_message))
.ok_or_else(|| TransportError::IoError(io::Error::other("Unable calculate fee")))
⋮----
impl BankClient {
fn run(bank: &Bank, transaction_receiver: Receiver<VersionedTransaction>) {
while let Ok(tx) = transaction_receiver.recv() {
let mut transactions = vec![tx];
while let Ok(tx) = transaction_receiver.try_recv() {
transactions.push(tx);
⋮----
let _ = bank.try_process_entry_transactions(transactions);
⋮----
pub fn new_shared(bank: Arc<Bank>) -> Self {
let (transaction_sender, transaction_receiver) = unbounded();
let thread_bank = bank.clone();
⋮----
.name("solBankClient".to_string())
.spawn(move || Self::run(&thread_bank, transaction_receiver))
.unwrap();
⋮----
pub fn new(bank: Bank) -> Self {
⋮----
pub fn set_sysvar_for_tests<T: SysvarSerialize>(&self, sysvar: &T) {
self.bank.set_sysvar_for_tests(sysvar);
⋮----
pub fn advance_slot(
⋮----
self.bank.clone(),
⋮----
self.bank.slot().checked_add(by)?,
⋮----
.write()
.unwrap()
.insert(new_bank)
.clone_without_scheduler();
self.set_sysvar_for_tests(&clock::Clock {
slot: self.bank.slot(),
⋮----
Some(self.bank.clone())
⋮----
mod tests {
⋮----
fn test_bank_client_new_with_keypairs() {
let (genesis_config, john_doe_keypair) = create_genesis_config(LAMPORTS_PER_SOL);
let john_pubkey = john_doe_keypair.pubkey();
⋮----
let jane_pubkey = jane_doe_keypair.pubkey();
let doe_keypairs = vec![&john_doe_keypair, &jane_doe_keypair];
⋮----
let amount = genesis_config.rent.minimum_balance(0);
⋮----
.push(AccountMeta::new(jane_pubkey, true));
let message = Message::new(&[transfer_instruction], Some(&john_pubkey));
⋮----
.send_and_confirm_message(&doe_keypairs, message)
⋮----
assert_eq!(bank_client.get_balance(&bob_pubkey).unwrap(), amount);

================
File: runtime/src/bank_forks.rs
================
pub type AtomicSlot = AtomicU64;
⋮----
pub struct ReadOnlyAtomicSlot {
⋮----
impl ReadOnlyAtomicSlot {
pub fn get(&self) -> Slot {
self.slot.load(Ordering::Acquire)
⋮----
pub struct SharableBanks {
⋮----
impl SharableBanks {
pub fn root(&self) -> Arc<Bank> {
self.root_bank.load_full()
⋮----
pub fn working(&self) -> Arc<Bank> {
self.working_bank.load_full()
⋮----
pub fn load(&self) -> BankPair {
⋮----
root_bank: self.root(),
working_bank: self.working(),
⋮----
pub struct BankPair {
⋮----
struct SetRootMetrics {
⋮----
struct SetRootTimings {
⋮----
pub struct BankForks {
⋮----
type Output = Arc<Bank>;
fn index(&self, bank_slot: Slot) -> &Self::Output {
⋮----
impl BankForks {
pub fn new_rw_arc(root_bank: Bank) -> Arc<RwLock<Self>> {
⋮----
let root_slot = root_bank.slot();
⋮----
banks.insert(
⋮----
BankWithScheduler::new_without_scheduler(root_bank.clone()),
⋮----
let parents = root_bank.parents();
⋮----
.insert(
parent.slot(),
BankWithScheduler::new_without_scheduler(parent.clone()),
⋮----
.is_some()
⋮----
descendants.entry(root_slot).or_default();
for parent in root_bank.proper_ancestors() {
descendants.entry(parent).or_default().insert(root_slot);
⋮----
root_bank: Arc::new(ArcSwap::from(root_bank.clone())),
working_bank: Arc::new(ArcSwap::from(root_bank.clone())),
⋮----
dumped_slot_subscribers: vec![],
⋮----
root_bank.set_fork_graph_in_program_cache(Arc::downgrade(&bank_forks));
⋮----
pub fn banks(&self) -> &HashMap<Slot, BankWithScheduler> {
⋮----
pub fn get_vote_only_mode_signal(&self) -> Arc<AtomicBool> {
self.in_vote_only_mode.clone()
⋮----
pub fn len(&self) -> usize {
self.banks.len()
⋮----
pub fn is_empty(&self) -> bool {
self.banks.is_empty()
⋮----
pub fn ancestors(&self) -> HashMap<Slot, HashSet<Slot>> {
let root = self.root();
⋮----
.iter()
.map(|(slot, bank)| {
let ancestors = bank.proper_ancestors().filter(|k| *k >= root);
(*slot, ancestors.collect())
⋮----
.collect()
⋮----
pub fn descendants(&self) -> HashMap<Slot, HashSet<Slot>> {
self.descendants.clone()
⋮----
pub fn frozen_banks(&self) -> impl Iterator<Item = (Slot, Arc<Bank>)> + '_ {
⋮----
.filter(|(_, b)| b.is_frozen())
.map(|(&k, b)| (k, b.clone_without_scheduler()))
⋮----
pub fn active_bank_slots(&self) -> Vec<Slot> {
⋮----
.filter(|(_, v)| !v.is_frozen())
.map(|(k, _v)| *k)
⋮----
pub fn get_with_scheduler(&self, bank_slot: Slot) -> Option<BankWithScheduler> {
self.banks.get(&bank_slot).map(|b| b.clone_with_scheduler())
⋮----
pub fn get(&self, bank_slot: Slot) -> Option<Arc<Bank>> {
self.get_with_scheduler(bank_slot)
.map(|b| b.clone_without_scheduler())
⋮----
pub fn get_with_checked_hash(
⋮----
let maybe_bank = self.get(bank_slot);
⋮----
assert_eq!(bank.hash(), expected_hash);
⋮----
pub fn bank_hash(&self, slot: Slot) -> Option<Hash> {
self.get(slot).map(|bank| bank.hash())
⋮----
pub fn sharable_banks(&self) -> SharableBanks {
self.sharable_banks.clone()
⋮----
pub fn root_bank(&self) -> Arc<Bank> {
self.sharable_banks.root()
⋮----
pub fn install_scheduler_pool(&mut self, pool: InstalledSchedulerPoolArc) {
info!("Installed new scheduler_pool into bank_forks: {pool:?}");
assert!(
⋮----
pub fn insert(&mut self, bank: Bank) -> BankWithScheduler {
self.insert_with_scheduling_mode(SchedulingMode::BlockVerification, bank)
⋮----
pub fn insert_with_scheduling_mode(
⋮----
if self.root.load(Ordering::Relaxed) < self.highest_slot_at_startup {
bank.set_check_program_modification_slot(true);
⋮----
let prev = self.banks.insert(bank.slot(), bank.clone_with_scheduler());
assert!(prev.is_none());
let slot = bank.slot();
self.descendants.entry(slot).or_default();
for parent in bank.proper_ancestors() {
self.descendants.entry(parent).or_default().insert(slot);
⋮----
self.working_slot = self.find_highest_slot();
self.sharable_banks.working_bank.store(self.working_bank());
⋮----
fn install_scheduler_into_bank(
⋮----
let context = SchedulingContext::new_with_mode(mode, bank.clone());
let scheduler = scheduler_pool.take_scheduler(context);
let bank_with_scheduler = BankWithScheduler::new(bank, Some(scheduler));
if matches!(mode, SchedulingMode::BlockVerification) {
scheduler_pool.register_timeout_listener(bank_with_scheduler.create_timeout_listener());
⋮----
pub fn insert_from_ledger(&mut self, bank: Bank) -> BankWithScheduler {
self.highest_slot_at_startup = std::cmp::max(self.highest_slot_at_startup, bank.slot());
self.insert(bank)
⋮----
pub fn remove(&mut self, slot: Slot) -> Option<BankWithScheduler> {
let bank = self.banks.remove(&slot)?;
⋮----
let Entry::Occupied(mut entry) = self.descendants.entry(parent) else {
panic!("this should not happen!");
⋮----
entry.get_mut().remove(&slot);
if entry.get().is_empty() && !self.banks.contains_key(&parent) {
entry.remove_entry();
⋮----
let Entry::Occupied(entry) = self.descendants.entry(slot) else {
⋮----
if entry.get().is_empty() {
⋮----
Some(bank)
⋮----
pub fn highest_slot(&self) -> Slot {
⋮----
fn find_highest_slot(&self) -> Slot {
self.banks.values().map(|bank| bank.slot()).max().unwrap()
⋮----
pub fn working_bank(&self) -> Arc<Bank> {
self.banks[&self.highest_slot()].clone_without_scheduler()
⋮----
pub fn working_bank_with_scheduler(&self) -> BankWithScheduler {
self.banks[&self.highest_slot()].clone_with_scheduler()
⋮----
pub fn register_dumped_slot_subscriber(&mut self, notifier: DumpedSlotSubscription) {
self.dumped_slot_subscribers.push(notifier);
⋮----
pub fn dump_slots<'a, I>(&mut self, slots: I) -> (Vec<(Slot, BankId)>, Vec<BankWithScheduler>)
⋮----
let mut lock = subscriber.lock().unwrap();
⋮----
.map(|slot| {
⋮----
.remove(*slot)
.expect("BankForks should not have been purged yet");
⋮----
.map_err(|err| {
warn!("Unable to write bank hash details file: {err}");
⋮----
.ok();
((*slot, bank.bank_id()), bank)
⋮----
.unzip()
⋮----
fn do_set_root_return_metrics(
⋮----
let old_epoch = self.sharable_banks.root().epoch();
⋮----
.get(root)
.expect("root bank didn't exist in bank_forks");
self.root.store(root, Ordering::Release);
self.sharable_banks.root_bank.store(Arc::clone(root_bank));
let new_epoch = root_bank.epoch();
⋮----
info!(
⋮----
root_bank.clear_epoch_rewards_cache();
⋮----
.parents()
.last()
.map(|bank| bank.transaction_count())
.unwrap_or(0);
let mut banks = vec![root_bank];
⋮----
banks.extend(parents.iter());
let total_parent_banks = banks.len();
⋮----
snapshot_controller.handle_new_roots(root, &banks)
⋮----
squash_timing += root_bank.squash();
⋮----
let new_tx_count = root_bank.transaction_count();
let accounts_data_len = root_bank.load_accounts_data_size() as i64;
⋮----
self.prune_non_rooted(root, highest_super_majority_root);
prune_time.stop();
let dropped_banks_len = removed_banks.len();
⋮----
drop(parents);
drop_parent_banks_time.stop();
⋮----
prune_non_rooted_ms: prune_time.as_ms() as i64,
drop_parent_banks_ms: drop_parent_banks_time.as_ms() as i64,
⋮----
pub fn prune_program_cache(&self, root: Slot) {
if let Some(root_bank) = self.banks.get(&root) {
root_bank.prune_program_cache(root, root_bank.epoch());
⋮----
pub fn set_root(
⋮----
self.do_set_root_return_metrics(root, snapshot_controller, highest_super_majority_root);
datapoint_info!(
⋮----
pub fn root(&self) -> Slot {
self.root.load(Ordering::Relaxed)
⋮----
pub fn get_atomic_root(&self) -> ReadOnlyAtomicSlot {
⋮----
slot: self.root.clone(),
⋮----
fn prune_non_rooted(
⋮----
let highest_super_majority_root = highest_super_majority_root.unwrap_or(root);
⋮----
.keys()
.copied()
.filter(|slot| {
⋮----
|| self.descendants[&root].contains(slot)
⋮----
&& self.descendants[slot].contains(&root));
⋮----
.collect();
prune_slots_time.stop();
⋮----
.into_iter()
.filter_map(|slot| self.remove(slot))
⋮----
prune_remove_time.stop();
⋮----
prune_slots_time.as_ms(),
prune_remove_time.as_ms(),
⋮----
impl ForkGraph for BankForks {
fn relationship(&self, a: Slot, b: Slot) -> BlockRelation {
let known_slot_range = self.root()..=self.highest_slot();
if known_slot_range.contains(&a) && known_slot_range.contains(&b) {
⋮----
.then_some(BlockRelation::Equal)
.or_else(|| {
self.banks.get(&b).and_then(|bank| {
⋮----
.contains_key(&a)
.then_some(BlockRelation::Ancestor)
⋮----
self.descendants.get(&b).and_then(|slots| {
slots.contains(&a).then_some(BlockRelation::Descendant)
⋮----
.unwrap_or(BlockRelation::Unrelated)
⋮----
impl Drop for BankForks {
fn drop(&mut self) {
info!("BankForks::drop(): started...");
self.banks.clear();
if let Some(scheduler_pool) = self.scheduler_pool.take() {
scheduler_pool.uninstalled_from_bank_forks();
⋮----
info!("BankForks::drop(): ...finished");
⋮----
mod tests {
⋮----
fn test_bank_forks_new_rw_arc_memory_leak() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
assert_eq!(Arc::strong_count(&bank_forks), 1);
⋮----
fn test_bank_forks_new() {
⋮----
let mut bank_forks = bank_forks.write().unwrap();
let child_bank = Bank::new_from_parent(bank_forks[0].clone(), &Pubkey::default(), 1);
child_bank.register_default_tick_for_test();
bank_forks.insert(child_bank);
assert_eq!(bank_forks[1u64].tick_height(), 1);
assert_eq!(bank_forks.working_bank().tick_height(), 1);
⋮----
fn test_bank_forks_descendants() {
⋮----
let bank0 = bank_forks[0].clone();
let bank = Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 1);
bank_forks.insert(bank);
⋮----
let descendants = bank_forks.descendants();
let children: HashSet<u64> = [1u64, 2u64].iter().copied().collect();
assert_eq!(children, *descendants.get(&0).unwrap());
assert!(descendants[&1].is_empty());
assert!(descendants[&2].is_empty());
⋮----
fn test_bank_forks_ancestors() {
⋮----
let ancestors = bank_forks.ancestors();
assert!(ancestors[&0].is_empty());
let parents: Vec<u64> = ancestors[&1].iter().cloned().collect();
assert_eq!(parents, vec![0]);
let parents: Vec<u64> = ancestors[&2].iter().cloned().collect();
⋮----
fn test_bank_forks_frozen_banks() {
⋮----
.frozen_banks()
.map(|(slot, _bank)| slot)
⋮----
assert!(frozen_slots.contains(&0));
assert!(!frozen_slots.contains(&1));
⋮----
fn test_bank_forks_active_banks() {
⋮----
assert_eq!(bank_forks.active_bank_slots(), vec![1]);
⋮----
fn test_bank_forks_different_set_root() {
⋮----
} = create_genesis_config_with_leader(10_000, &leader_keypair.pubkey(), 1_000);
⋮----
let mut bank_forks0 = bank_forks0.write().unwrap();
bank_forks0.set_root(0, None, None);
⋮----
let mut bank_forks1 = bank_forks1.write().unwrap();
⋮----
Bank::new_from_parent(bank_forks0[slot - 1].clone(), &Pubkey::default(), slot);
⋮----
Bank::new_from_parent(bank_forks1[slot - 1].clone(), &Pubkey::default(), slot);
⋮----
let recent_timestamp: UnixTimestamp = child.unix_timestamp_from_genesis();
update_vote_account_timestamp(
⋮----
slot: child.slot(),
⋮----
&voting_keypair.pubkey(),
⋮----
bank_forks0.insert(child1);
bank_forks0.set_root(slot, None, None);
bank_forks1.insert(child2);
⋮----
let child1 = &bank_forks0.working_bank();
let child2 = &bank_forks1.working_bank();
child1.freeze();
child2.freeze();
info!("child0.ancestors: {:?}", child1.ancestors);
info!("child1.ancestors: {:?}", child2.ancestors);
assert_eq!(child1.hash(), child2.hash());
⋮----
fn make_hash_map(data: Vec<(Slot, Vec<Slot>)>) -> HashMap<Slot, HashSet<Slot>> {
data.into_iter()
.map(|(k, v)| (k, v.into_iter().collect()))
⋮----
fn extend_bank_forks(bank_forks: Arc<RwLock<BankForks>>, parent_child_pairs: &[(Slot, Slot)]) {
for (parent, child) in parent_child_pairs.iter() {
let parent: Arc<Bank> = bank_forks.read().unwrap().banks[parent].clone();
bank_forks.write().unwrap().insert(Bank::new_from_parent(
⋮----
fn test_bank_forks_with_set_root() {
⋮----
let parent_child_pairs = vec![(0, 1), (1, 2), (0, 3), (3, 4)];
extend_bank_forks(bank_forks.clone(), &parent_child_pairs);
assert_eq!(
⋮----
bank_forks.write().unwrap().set_root(
⋮----
bank_forks.read().unwrap().get(2).unwrap().squash();
⋮----
let parent_child_pairs = vec![(2, 5), (5, 6)];
⋮----
fn test_bank_forks_with_highest_super_majority_root() {
⋮----
assert_eq!(bank.slot(), 0);
⋮----
Some(1),
⋮----
fn test_fork_graph() {
⋮----
let parent_child_pairs = vec![
⋮----
assert_matches!(bank_forks.relationship(0, 3), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(0, 10), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(0, 12), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(1, 3), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(2, 10), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(2, 12), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(4, 10), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(4, 12), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(6, 10), BlockRelation::Unrelated);
assert_matches!(bank_forks.relationship(5, 12), BlockRelation::Unrelated);
assert_matches!(bank_forks.relationship(6, 12), BlockRelation::Ancestor);
assert_matches!(bank_forks.relationship(6, 2), BlockRelation::Descendant);
assert_matches!(bank_forks.relationship(10, 2), BlockRelation::Descendant);
assert_matches!(bank_forks.relationship(8, 3), BlockRelation::Descendant);
assert_matches!(bank_forks.relationship(6, 3), BlockRelation::Unrelated);
assert_matches!(bank_forks.relationship(12, 2), BlockRelation::Descendant);
assert_matches!(bank_forks.relationship(12, 1), BlockRelation::Unrelated);
assert_matches!(bank_forks.relationship(1, 2), BlockRelation::Unrelated);
assert_matches!(bank_forks.relationship(1, 13), BlockRelation::Unknown);
assert_matches!(bank_forks.relationship(13, 2), BlockRelation::Unknown);
bank_forks.set_root(
⋮----
assert_matches!(bank_forks.relationship(1, 2), BlockRelation::Unknown);
assert_matches!(bank_forks.relationship(2, 0), BlockRelation::Unknown);

================
File: runtime/src/bank_hash_cache.rs
================
pub type DumpedSlotSubscription = Arc<Mutex<bool>>;
pub struct BankHashCache {
⋮----
impl BankHashCache {
pub fn new(bank_forks: Arc<RwLock<BankForks>>) -> Self {
let sharable_banks = bank_forks.read().unwrap().sharable_banks();
⋮----
.write()
.unwrap()
.register_dumped_slot_subscriber(dumped_slot_subscription.clone());
⋮----
pub fn dumped_slot_subscription(&self) -> DumpedSlotSubscription {
self.dumped_slot_subscription.clone()
⋮----
pub fn hash(&mut self, slot: Slot, slots_dumped: &mut MutexGuard<bool>) -> Option<Hash> {
⋮----
self.hashes.clear();
⋮----
if let Some(hash) = self.hashes.get(&slot) {
return Some(*hash);
⋮----
let Some(hash) = self.bank_forks.read().unwrap().bank_hash(slot) else {
⋮----
let prev_hash = self.hashes.insert(slot, hash);
debug_assert!(
⋮----
Some(hash)
⋮----
pub fn root(&mut self) -> Slot {
self.get_root_bank_and_prune_cache().slot()
⋮----
pub fn get_root_bank_and_prune_cache(&mut self) -> Arc<Bank> {
let root_bank = self.sharable_banks.root();
if root_bank.slot() != self.last_root {
self.last_root = root_bank.slot();
self.hashes = self.hashes.split_off(&self.last_root);

================
File: runtime/src/bank_utils.rs
================
pub fn setup_bank_and_vote_pubkeys_for_tests(
⋮----
.map(|_| ValidatorVoteKeypairs::new_rand())
.collect();
⋮----
.iter()
.map(|k| k.vote_keypair.pubkey())
⋮----
vec![stake; validator_voting_keypairs.len()],
⋮----
pub fn find_and_send_votes(
⋮----
.zip(commit_results.iter())
.for_each(|(tx, commit_result)| {
if tx.is_simple_vote_transaction() && commit_result.was_executed_successfully() {
⋮----
if parsed_vote.1.last_voted_slot().is_some() {
let _ = vote_sender.send(parsed_vote);

================
File: runtime/src/bank.rs
================
struct VerifyAccountsHashConfig {
⋮----
mod accounts_lt_hash;
mod address_lookup_table;
pub mod bank_hash_details;
pub mod builtins;
mod check_transactions;
mod fee_distribution;
mod metrics;
pub(crate) mod partitioned_epoch_rewards;
mod recent_blockhashes_account;
mod serde_snapshot;
mod sysvar_cache;
pub(crate) mod tests;
⋮----
pub type BankStatusCache = StatusCache<Result<()>>;
⋮----
pub type BankSlotDelta = SlotDelta<Result<()>>;
⋮----
pub struct SquashTiming {
⋮----
impl AddAssign for SquashTiming {
fn add_assign(&mut self, rhs: Self) {
⋮----
pub struct CollectorFeeDetails {
⋮----
impl CollectorFeeDetails {
pub(crate) fn accumulate(&mut self, fee_details: &FeeDetails) {
⋮----
.saturating_add(fee_details.transaction_fee());
⋮----
.saturating_add(fee_details.prioritization_fee());
⋮----
pub fn total_transaction_fee(&self) -> u64 {
self.transaction_fee.saturating_add(self.priority_fee)
⋮----
pub fn total_priority_fee(&self) -> u64 {
⋮----
fn from(fee_details: FeeDetails) -> Self {
⋮----
transaction_fee: fee_details.transaction_fee(),
priority_fee: fee_details.prioritization_fee(),
⋮----
pub struct BankRc {
⋮----
impl BankRc {
pub(crate) fn new(accounts: Accounts) -> Self {
⋮----
pub struct LoadAndExecuteTransactionsOutput {
⋮----
pub struct BundleTransactionSimulationResult {
⋮----
pub struct AccountData {
⋮----
pub struct TransactionSimulationResult {
⋮----
impl TransactionSimulationResult {
pub fn new_error(err: TransactionError) -> Self {
⋮----
logs: vec![],
⋮----
post_simulation_accounts: vec![],
⋮----
result: Err(err),
⋮----
pub struct TransactionBalancesSet {
⋮----
impl TransactionBalancesSet {
pub fn new(pre_balances: TransactionBalances, post_balances: TransactionBalances) -> Self {
assert_eq!(pre_balances.len(), post_balances.len());
⋮----
pub type TransactionBalances = Vec<Vec<u64>>;
pub type PreCommitResult<'a> = Result<Option<RwLockReadGuard<'a, Hash>>>;
⋮----
pub enum TransactionLogCollectorFilter {
⋮----
pub struct TransactionLogCollectorConfig {
⋮----
pub struct TransactionLogInfo {
⋮----
pub struct TransactionLogCollector {
⋮----
impl TransactionLogCollector {
pub fn get_logs_for_address(
⋮----
None => Some(self.logs.clone()),
Some(address) => self.mentioned_address_map.get(address).map(|log_indices| {
⋮----
.iter()
.filter_map(|i| self.logs.get(*i).cloned())
.collect()
⋮----
pub struct BankFieldsToDeserialize {
⋮----
pub struct BankFieldsToSerialize {
⋮----
impl PartialEq for Bank {
fn eq(&self, other: &Self) -> bool {
⋮----
*blockhash_queue.read().unwrap() == *other.blockhash_queue.read().unwrap()
⋮----
&& *hash.read().unwrap() == *other.hash.read().unwrap()
⋮----
&& *hard_forks.read().unwrap() == *other.hard_forks.read().unwrap()
&& transaction_count.load(Relaxed) == other.transaction_count.load(Relaxed)
&& tick_height.load(Relaxed) == other.tick_height.load(Relaxed)
&& signature_count.load(Relaxed) == other.signature_count.load(Relaxed)
&& capitalization.load(Relaxed) == other.capitalization.load(Relaxed)
⋮----
&& collector_fees.load(Relaxed) == other.collector_fees.load(Relaxed)
⋮----
&& *inflation.read().unwrap() == *other.inflation.read().unwrap()
&& *stakes_cache.stakes() == *other.stakes_cache.stakes()
⋮----
&& is_delta.load(Relaxed) == other.is_delta.load(Relaxed)
⋮----
*hash_overrides.lock().unwrap() == *other.hash_overrides.lock().unwrap())
&& *accounts_lt_hash.lock().unwrap() == *other.accounts_lt_hash.lock().unwrap()
&& *block_id.read().unwrap() == *other.block_id.read().unwrap()
⋮----
impl BankFieldsToSerialize {
pub fn default_for_tests() -> Self {
⋮----
accounts_lt_hash: AccountsLtHash(LtHash([0x7E57; LtHash::NUM_ELEMENTS])),
⋮----
pub enum RewardCalculationEvent<'a, 'b> {
⋮----
pub trait RewardCalcTracer: Fn(&RewardCalculationEvent) + Send + Sync {}
impl<T: Fn(&RewardCalculationEvent) + Send + Sync> RewardCalcTracer for T {}
fn null_tracer() -> Option<impl RewardCalcTracer> {
⋮----
pub trait DropCallback: fmt::Debug {
⋮----
pub struct OptionalDropCallback(Option<Box<dyn DropCallback + Send + Sync>>);
⋮----
pub struct HashOverrides {
⋮----
impl HashOverrides {
fn get_hash_override(&self, slot: Slot) -> Option<&HashOverride> {
self.hashes.get(&slot)
⋮----
fn get_blockhash_override(&self, slot: Slot) -> Option<&Hash> {
self.get_hash_override(slot)
.map(|hash_override| &hash_override.blockhash)
⋮----
fn get_bank_hash_override(&self, slot: Slot) -> Option<&Hash> {
⋮----
.map(|hash_override| &hash_override.bank_hash)
⋮----
pub fn add_override(&mut self, slot: Slot, blockhash: Hash, bank_hash: Hash) {
⋮----
.insert(
⋮----
.is_none();
assert!(is_new);
⋮----
struct HashOverride {
⋮----
pub struct Bank {
⋮----
struct VoteReward {
⋮----
type VoteRewards = HashMap<Pubkey, VoteReward, PubkeyHasherBuilder>;
⋮----
pub struct NewBankOptions {
⋮----
pub struct BankTestConfig {
⋮----
impl Default for BankTestConfig {
fn default() -> Self {
use solana_accounts_db::accounts_db::ACCOUNTS_DB_CONFIG_FOR_TESTING;
⋮----
struct PrevEpochInflationRewards {
⋮----
pub struct ProcessedTransactionCounts {
⋮----
pub struct BankHashStats {
⋮----
impl BankHashStats {
pub fn update<T: ReadableAccount>(&mut self, account: &T) {
if account.lamports() == 0 {
⋮----
.wrapping_add(account.data().len() as u64);
if account.executable() {
⋮----
self.num_lamports_stored = self.num_lamports_stored.wrapping_add(account.lamports());
⋮----
pub fn accumulate(&mut self, other: &BankHashStats) {
⋮----
self.total_data_len = self.total_data_len.wrapping_add(other.total_data_len);
⋮----
.wrapping_add(other.num_lamports_stored);
⋮----
pub struct AtomicBankHashStats {
⋮----
impl AtomicBankHashStats {
pub fn new(stat: &BankHashStats) -> Self {
⋮----
pub fn accumulate(&self, other: &BankHashStats) {
⋮----
.fetch_add(other.num_updated_accounts, Relaxed);
⋮----
.fetch_add(other.num_removed_accounts, Relaxed);
self.total_data_len.fetch_add(other.total_data_len, Relaxed);
⋮----
.fetch_add(other.num_lamports_stored, Relaxed);
⋮----
.fetch_add(other.num_executable_accounts, Relaxed);
⋮----
pub fn load(&self) -> BankHashStats {
⋮----
num_updated_accounts: self.num_updated_accounts.load(Relaxed),
num_removed_accounts: self.num_removed_accounts.load(Relaxed),
num_lamports_stored: self.num_lamports_stored.load(Relaxed),
total_data_len: self.total_data_len.load(Relaxed),
num_executable_accounts: self.num_executable_accounts.load(Relaxed),
⋮----
struct NewEpochBundle {
⋮----
impl Bank {
fn default_with_accounts(accounts: Accounts) -> Self {
⋮----
drop_callback: RwLock::new(OptionalDropCallback(None)),
⋮----
accounts_lt_hash: Mutex::new(AccountsLtHash(LtHash::identity())),
⋮----
bank.accounts_data_size_initial = bank.calculate_accounts_data_size().unwrap();
⋮----
pub fn new_from_genesis(
⋮----
bank.ancestors = Ancestors::from(vec![bank.slot()]);
⋮----
.set_execution_cost(compute_budget.to_cost());
⋮----
bank.cluster_type = Some(genesis_config.cluster_type);
⋮----
bank.feature_set = Arc::new(feature_set.unwrap_or_default());
⋮----
bank.process_genesis_config(genesis_config);
⋮----
bank.process_genesis_config(genesis_config, collector_id_for_tests, genesis_hash);
bank.compute_and_apply_genesis_features();
⋮----
let stakes = bank.stakes_cache.stakes().clone();
⋮----
for epoch in 0..=bank.get_leader_schedule_epoch(bank.slot) {
⋮----
.insert(epoch, VersionedEpochStakes::new(stakes.clone(), epoch));
⋮----
bank.update_stake_history(None);
⋮----
bank.update_clock(None);
bank.update_rent();
bank.update_epoch_schedule();
bank.update_recent_blockhashes();
bank.update_last_restart_slot();
⋮----
.fill_missing_sysvar_cache_entries(&bank);
⋮----
pub fn new_from_parent(parent: Arc<Bank>, collector_id: &Pubkey, slot: Slot) -> Self {
⋮----
null_tracer(),
⋮----
pub fn new_from_parent_with_options(
⋮----
Self::_new_from_parent(parent, collector_id, slot, null_tracer(), new_bank_options)
⋮----
pub fn new_from_parent_with_tracer(
⋮----
Some(reward_calc_tracer),
⋮----
fn get_rent_collector_from(rent_collector: &RentCollector, epoch: Epoch) -> RentCollector {
rent_collector.clone_with_epoch(epoch)
⋮----
fn _new_from_parent(
⋮----
parent.freeze();
assert_ne!(slot, parent.slot());
let epoch_schedule = parent.epoch_schedule().clone();
let epoch = epoch_schedule.get_epoch(slot);
let (rc, bank_rc_creation_time_us) = measure_us!({
⋮----
let (status_cache, status_cache_time_us) = measure_us!(Arc::clone(&parent.status_cache));
let (fee_rate_governor, fee_components_time_us) = measure_us!(
⋮----
let bank_id = rc.bank_id_generator.fetch_add(1, Relaxed) + 1;
⋮----
measure_us!(RwLock::new(parent.blockhash_queue.read().unwrap().clone()));
⋮----
measure_us!(StakesCache::new(parent.stakes_cache.stakes().clone()));
let (epoch_stakes, epoch_stakes_time_us) = measure_us!(parent.epoch_stakes.clone());
let (transaction_processor, builtin_program_ids_time_us) = measure_us!(
⋮----
measure_us!(parent.transaction_debug_keys.clone());
⋮----
measure_us!(parent.transaction_log_collector_config.clone());
let (feature_set, feature_set_time_us) = measure_us!(parent.feature_set.clone());
let accounts_data_size_initial = parent.load_accounts_data_size();
⋮----
.checked_add(1)
.expect("max tick height addition overflowed")
.checked_mul(parent.ticks_per_slot)
.expect("max tick height multiplication overflowed"),
⋮----
.expect("block height addition overflowed"),
⋮----
capitalization: AtomicU64::new(parent.capitalization()),
⋮----
inflation: parent.inflation.clone(),
transaction_count: AtomicU64::new(parent.transaction_count()),
⋮----
parent.non_vote_transaction_count_since_restart(),
⋮----
parent_hash: parent.hash(),
parent_slot: parent.slot(),
⋮----
tick_height: AtomicU64::new(parent.tick_height.load(Relaxed)),
⋮----
hard_forks: parent.hard_forks.clone(),
rewards: RwLock::new(vec![]),
⋮----
reserved_account_keys: parent.reserved_account_keys.clone(),
drop_callback: RwLock::new(OptionalDropCallback(
⋮----
.read()
.unwrap()
⋮----
.as_ref()
.map(|drop_callback| drop_callback.clone_box()),
⋮----
cost_tracker: RwLock::new(parent.read_cost_tracker().unwrap().new_from_parent_limits()),
⋮----
epoch_reward_status: parent.epoch_reward_status.clone(),
⋮----
fee_structure: parent.fee_structure.clone(),
⋮----
hash_overrides: parent.hash_overrides.clone(),
accounts_lt_hash: Mutex::new(parent.accounts_lt_hash.lock().unwrap().clone()),
⋮----
epoch_rewards_calculation_cache: parent.epoch_rewards_calculation_cache.clone(),
⋮----
let (_, ancestors_time_us) = measure_us!({
⋮----
let (_, update_epoch_time_us) = measure_us!({
⋮----
measure_us!(new.prepare_program_cache_for_upcoming_feature_set());
let (_, update_sysvars_time_us) = measure_us!({
⋮----
let (_, fill_sysvar_cache_time_us) = measure_us!(new
⋮----
measure_us!({
⋮----
time.stop();
report_new_bank_metrics(
⋮----
parent.slot(),
⋮----
total_elapsed_time_us: time.as_us(),
⋮----
report_loaded_programs_stats(
⋮----
.write()
⋮----
.reset();
⋮----
pub fn set_fork_graph_in_program_cache(&self, fork_graph: Weak<RwLock<BankForks>>) {
⋮----
.set_fork_graph(fork_graph);
⋮----
fn prepare_program_cache_for_upcoming_feature_set(&self) {
let (_epoch, slot_index) = self.epoch_schedule.get_epoch_and_slot_index(self.slot);
let slots_in_epoch = self.epoch_schedule.get_slots_in_epoch(self.epoch);
let (upcoming_feature_set, _newly_activated) = self.compute_active_feature_set(true);
⋮----
.min(slots_in_epoch)
.checked_div(2)
.unwrap();
⋮----
epoch_boundary_preparation.upcoming_environments.as_ref()
⋮----
let upcoming_environments = upcoming_environments.clone();
⋮----
epoch_boundary_preparation.programs_to_recompile.pop()
⋮----
drop(epoch_boundary_preparation);
drop(program_cache);
if let Some(recompiled) = load_program_with_pubkey(
⋮----
recompiled.tx_usage_counter.fetch_add(
⋮----
.load(Ordering::Relaxed),
⋮----
program_cache.assign_program(&upcoming_environments, key, recompiled);
⋮----
} else if slot_index.saturating_add(slots_in_recompilation_phase) >= slots_in_epoch {
let new_environments = self.create_program_runtime_environments(&upcoming_feature_set);
let mut upcoming_environments = self.transaction_processor.environments.clone();
⋮----
epoch_boundary_preparation.upcoming_epoch = self.epoch.saturating_add(1);
epoch_boundary_preparation.upcoming_environments = Some(upcoming_environments);
⋮----
.get_flattened_entries(changed_program_runtime_v1, changed_program_runtime_v2);
⋮----
.sort_by_cached_key(|(_id, program)| program.decayed_usage_counter(self.slot));
⋮----
pub fn prune_program_cache(&self, new_root_slot: Slot, new_root_epoch: Epoch) {
⋮----
.reroot(new_root_epoch);
⋮----
.prune(new_root_slot, upcoming_environments);
⋮----
pub fn prune_program_cache_by_deployment_slot(&self, deployment_slot: Slot) {
⋮----
.prune_by_deployment_slot(deployment_slot);
⋮----
pub fn new_warmup_cooldown_rate_epoch(&self) -> Option<Epoch> {
⋮----
.new_warmup_cooldown_rate_epoch(&self.epoch_schedule)
⋮----
fn compute_new_epoch_caches_and_rewards(
⋮----
let stakes = self.stakes_cache.stakes();
let stake_delegations = stakes.stake_delegations_vec();
⋮----
measure_us!(stakes.calculate_activated_stake(
⋮----
let (rewards_calculation, update_rewards_with_thread_pool_time_us) = measure_us!(self
⋮----
fn process_new_epoch(
⋮----
let epoch = self.epoch();
let slot = self.slot();
let (thread_pool, thread_pool_time_us) = measure_us!(ThreadPoolBuilder::new()
⋮----
let (_, apply_feature_activations_time_us) = measure_us!(
⋮----
} = self.compute_new_epoch_caches_and_rewards(
⋮----
.activate_epoch(epoch, stake_history, vote_accounts);
let leader_schedule_epoch = self.epoch_schedule.get_leader_schedule_epoch(slot);
⋮----
measure_us!(self.update_epoch_stakes(leader_schedule_epoch));
self.begin_partitioned_rewards(
⋮----
report_new_epoch_metrics(
⋮----
let new_environments = self.create_program_runtime_environments(&self.feature_set);
⋮----
.set_environments(new_environments);
⋮----
pub fn byte_limit_for_scans(&self) -> Option<usize> {
⋮----
pub fn proper_ancestors_set(&self) -> HashSet<Slot> {
HashSet::from_iter(self.proper_ancestors())
⋮----
pub(crate) fn proper_ancestors(&self) -> impl Iterator<Item = Slot> + '_ {
⋮----
.keys()
.into_iter()
.filter(move |slot| *slot != self.slot)
⋮----
pub fn set_callback(&self, callback: Option<Box<dyn DropCallback + Send + Sync>>) {
*self.drop_callback.write().unwrap() = OptionalDropCallback(callback);
⋮----
pub fn vote_only_bank(&self) -> bool {
⋮----
/// Like `new_from_parent` but additionally:
    /// * Doesn't assume that the parent is anywhere near `slot`, parent could be millions of slots
⋮----
/// * Doesn't assume that the parent is anywhere near `slot`, parent could be millions of slots
    pub fn warp_from_parent(parent: Arc<Bank>, collector_id: &Pubkey, slot: Slot) -> Self {
⋮----
pub fn warp_from_parent(parent: Arc<Bank>, collector_id: &Pubkey, slot: Slot) -> Self {
⋮----
let parent_timestamp = parent.clock().unix_timestamp;
⋮----
new.update_epoch_stakes(new.epoch_schedule().get_epoch(slot));
new.tick_height.store(new.max_tick_height(), Relaxed);
let mut clock = new.clock();
⋮----
new.update_sysvar_account(&sysvar::clock::id(), |account| {
create_account(
⋮----
new.inherit_specially_retained_account_fields(account),
⋮----
.fill_missing_sysvar_cache_entries(&new);
new.freeze();
⋮----
pub(crate) fn new_from_snapshot(
⋮----
let (stakes, stakes_time) = measure_time!(Stakes::new(&fields.stakes, |pubkey| {
⋮----
info!("Loading Stakes took: {stakes_time}");
let stakes_accounts_load_duration = now.elapsed();
⋮----
cluster_type: Some(genesis_config.cluster_type),
⋮----
assert_eq!(
⋮----
assert_eq!(bank.ticks_per_slot, genesis_config.ticks_per_slot);
⋮----
assert_eq!(bank.max_tick_height, (bank.slot + 1) * bank.ticks_per_slot);
⋮----
assert_eq!(bank.epoch_schedule, genesis_config.epoch_schedule);
assert_eq!(bank.epoch, bank.epoch_schedule.get_epoch(bank.slot));
bank.initialize_after_snapshot_restore(|| {
⋮----
.thread_name(|i| format!("solBnkClcRwds{i:02}"))
.build()
.expect("new rayon threadpool")
⋮----
datapoint_info!(
⋮----
pub(crate) fn get_fields_to_serialize(&self) -> BankFieldsToSerialize {
⋮----
blockhash_queue: self.blockhash_queue.read().unwrap().clone(),
⋮----
hash: *self.hash.read().unwrap(),
⋮----
hard_forks: self.hard_forks.read().unwrap().clone(),
transaction_count: self.transaction_count.load(Relaxed),
tick_height: self.tick_height.load(Relaxed),
signature_count: self.signature_count.load(Relaxed),
capitalization: self.capitalization.load(Relaxed),
⋮----
collector_fees: self.collector_fees.load(Relaxed),
fee_rate_governor: self.fee_rate_governor.clone(),
rent_collector: self.rent_collector.clone(),
epoch_schedule: self.epoch_schedule.clone(),
inflation: *self.inflation.read().unwrap(),
stakes: self.stakes_cache.stakes().clone(),
is_delta: self.is_delta.load(Relaxed),
accounts_data_len: self.load_accounts_data_size(),
versioned_epoch_stakes: self.epoch_stakes.clone(),
accounts_lt_hash: self.accounts_lt_hash.lock().unwrap().clone(),
⋮----
pub fn collector_id(&self) -> &Pubkey {
⋮----
pub fn genesis_creation_time(&self) -> UnixTimestamp {
⋮----
pub fn slot(&self) -> Slot {
⋮----
pub fn bank_id(&self) -> BankId {
⋮----
pub fn epoch(&self) -> Epoch {
⋮----
pub fn first_normal_epoch(&self) -> Epoch {
self.epoch_schedule().first_normal_epoch
⋮----
pub fn freeze_lock(&self) -> RwLockReadGuard<'_, Hash> {
self.hash.read().unwrap()
⋮----
pub fn hash(&self) -> Hash {
*self.hash.read().unwrap()
⋮----
pub fn is_frozen(&self) -> bool {
*self.hash.read().unwrap() != Hash::default()
⋮----
pub fn freeze_started(&self) -> bool {
self.freeze_started.load(Relaxed)
⋮----
pub fn status_cache_ancestors(&self) -> Vec<u64> {
let mut roots = self.status_cache.read().unwrap().roots().clone();
let min = roots.iter().min().cloned().unwrap_or(0);
for ancestor in self.ancestors.keys() {
⋮----
roots.insert(ancestor);
⋮----
let mut ancestors: Vec<_> = roots.into_iter().collect();
⋮----
ancestors.sort();
⋮----
/// computed unix_timestamp at this slot height
    pub fn unix_timestamp_from_genesis(&self) -> i64 {
⋮----
pub fn unix_timestamp_from_genesis(&self) -> i64 {
self.genesis_creation_time.saturating_add(
⋮----
.saturating_mul(self.ns_per_slot)
.saturating_div(1_000_000_000) as i64,
⋮----
fn update_sysvar_account<F>(&self, pubkey: &Pubkey, updater: F)
⋮----
let old_account = self.get_account_with_fixed_root(pubkey);
let mut new_account = updater(&old_account);
// When new sysvar comes into existence (with RENT_UNADJUSTED_INITIAL_BALANCE lamports),
// this code ensures that the sysvar's balance is adjusted to be rent-exempt.
self.adjust_sysvar_balance_for_rent(&mut new_account);
self.store_account_and_update_capitalization(pubkey, &new_account);
⋮----
fn inherit_specially_retained_account_fields(
⋮----
.map(|a| a.lamports())
.unwrap_or(RENT_UNADJUSTED_INITIAL_BALANCE),
⋮----
.map(|a| a.rent_epoch())
.unwrap_or(INITIAL_RENT_EPOCH),
⋮----
pub fn clock(&self) -> sysvar::clock::Clock {
from_account(&self.get_account(&sysvar::clock::id()).unwrap_or_default())
.unwrap_or_default()
⋮----
fn update_clock(&self, parent_epoch: Option<Epoch>) {
let mut unix_timestamp = self.clock().unix_timestamp;
⋮----
self.epoch()
⋮----
let first_slot_in_epoch = self.epoch_schedule().get_first_slot_in_epoch(epoch);
Some((first_slot_in_epoch, self.clock().epoch_start_timestamp))
⋮----
let ancestor_timestamp = self.clock().unix_timestamp;
⋮----
self.get_timestamp_estimate(max_allowable_drift, epoch_start_timestamp)
⋮----
if parent_epoch.is_some() && parent_epoch.unwrap() != self.epoch() {
⋮----
self.clock().epoch_start_timestamp
⋮----
unix_timestamp = self.unix_timestamp_from_genesis();
epoch_start_timestamp = self.unix_timestamp_from_genesis();
⋮----
epoch: self.epoch_schedule().get_epoch(self.slot),
leader_schedule_epoch: self.epoch_schedule().get_leader_schedule_epoch(self.slot),
⋮----
self.update_sysvar_account(&sysvar::clock::id(), |account| {
⋮----
self.inherit_specially_retained_account_fields(account),
⋮----
pub fn update_last_restart_slot(&self) {
⋮----
.is_active(&feature_set::last_restart_slot_sysvar::id());
⋮----
.get_account(&sysvar::last_restart_slot::id())
.and_then(|account| {
let lrs: Option<LastRestartSlot> = from_account(&account);
⋮----
.map(|account| account.last_restart_slot);
⋮----
let hard_forks_r = self.hard_forks.read().unwrap();
⋮----
.rev()
.find(|(hard_fork, _)| *hard_fork <= slot)
.map(|(slot, _)| *slot)
.unwrap_or(0)
⋮----
if current_last_restart_slot != Some(last_restart_slot) {
self.update_sysvar_account(&sysvar::last_restart_slot::id(), |account| {
⋮----
pub fn set_sysvar_for_tests<T>(&self, sysvar: &T)
⋮----
self.update_sysvar_account(&T::id(), |account| {
⋮----
self.transaction_processor.reset_sysvar_cache();
⋮----
.fill_missing_sysvar_cache_entries(self);
⋮----
fn update_slot_history(&self) {
self.update_sysvar_account(&sysvar::slot_history::id(), |account| {
⋮----
.map(|account| from_account::<SlotHistory, _>(account).unwrap())
.unwrap_or_default();
slot_history.add(self.slot());
⋮----
fn update_slot_hashes(&self) {
self.update_sysvar_account(&sysvar::slot_hashes::id(), |account| {
⋮----
.map(|account| from_account::<SlotHashes, _>(account).unwrap())
⋮----
slot_hashes.add(self.parent_slot, self.parent_hash);
⋮----
pub fn get_slot_history(&self) -> SlotHistory {
from_account(&self.get_account(&sysvar::slot_history::id()).unwrap()).unwrap()
⋮----
fn update_epoch_stakes(&mut self, leader_schedule_epoch: Epoch) {
if !self.epoch_stakes.contains_key(&leader_schedule_epoch) {
self.epoch_stakes.retain(|&epoch, _| {
epoch >= leader_schedule_epoch.saturating_sub(MAX_LEADER_SCHEDULE_STAKES - 1)
⋮----
let stakes = self.stakes_cache.stakes().clone();
⋮----
info!(
⋮----
.stakes()
.vote_accounts()
.delegated_stakes()
.map(|(pubkey, stake)| (*pubkey, stake))
.collect();
trace!("new epoch stakes, stakes: {vote_stakes:#?}");
⋮----
.insert(leader_schedule_epoch, new_epoch_stakes);
⋮----
pub fn set_epoch_stakes_for_test(&mut self, epoch: Epoch, stakes: VersionedEpochStakes) {
self.epoch_stakes.insert(epoch, stakes);
⋮----
fn update_rent(&self) {
self.update_sysvar_account(&sysvar::rent::id(), |account| {
⋮----
fn update_epoch_schedule(&self) {
self.update_sysvar_account(&sysvar::epoch_schedule::id(), |account| {
⋮----
self.epoch_schedule(),
⋮----
fn update_stake_history(&self, epoch: Option<Epoch>) {
if epoch == Some(self.epoch()) {
⋮----
self.update_sysvar_account(&stake_history::id(), |account| {
⋮----
self.stakes_cache.stakes().history(),
⋮----
pub fn epoch_duration_in_years(&self, prev_epoch: Epoch) -> f64 {
self.epoch_schedule().get_slots_in_epoch(prev_epoch) as f64 / self.slots_per_year
⋮----
fn get_inflation_start_slot(&self) -> Slot {
⋮----
.full_inflation_features_enabled()
⋮----
.filter_map(|id| self.feature_set.activated_slot(id))
⋮----
slots.sort_unstable();
slots.first().cloned().unwrap_or_else(|| {
⋮----
.activated_slot(&feature_set::pico_inflation::id())
⋮----
fn get_inflation_num_slots(&self) -> u64 {
let inflation_activation_slot = self.get_inflation_start_slot();
let inflation_start_slot = self.epoch_schedule().get_first_slot_in_epoch(
self.epoch_schedule()
.get_epoch(inflation_activation_slot)
.saturating_sub(1),
⋮----
self.epoch_schedule().get_first_slot_in_epoch(self.epoch()) - inflation_start_slot
⋮----
pub fn slot_in_year_for_inflation(&self) -> f64 {
let num_slots = self.get_inflation_num_slots();
⋮----
fn calculate_previous_epoch_inflation_rewards(
⋮----
let slot_in_year = self.slot_in_year_for_inflation();
⋮----
let inflation = self.inflation.read().unwrap();
⋮----
(*inflation).validator(slot_in_year),
(*inflation).foundation(slot_in_year),
⋮----
let prev_epoch_duration_in_years = self.epoch_duration_in_years(prev_epoch);
⋮----
fn calc_vote_accounts_to_store(vote_account_rewards: VoteRewards) -> VoteRewardsAccounts {
let len = vote_account_rewards.len();
⋮----
vote_account_rewards.into_iter().for_each(
⋮----
if let Err(err) = vote_account.checked_add_lamports(vote_rewards) {
debug!("reward redemption failed for {vote_pubkey}: {err:?}");
⋮----
result.accounts_with_rewards.push((
⋮----
post_balance: vote_account.lamports(),
commission: Some(commission),
⋮----
fn update_vote_rewards(&self, vote_rewards: &VoteRewardsAccounts) {
let mut rewards = self.rewards.write().unwrap();
rewards.reserve(vote_rewards.accounts_with_rewards.len());
⋮----
.for_each(|(vote_pubkey, vote_reward, _)| {
rewards.push((*vote_pubkey, *vote_reward));
⋮----
fn update_recent_blockhashes_locked(&self, locked_blockhash_queue: &BlockhashQueue) {
⋮----
self.update_sysvar_account(&sysvar::recent_blockhashes::id(), |account| {
let recent_blockhash_iter = locked_blockhash_queue.get_recent_blockhashes();
⋮----
pub fn update_recent_blockhashes(&self) {
let blockhash_queue = self.blockhash_queue.read().unwrap();
self.update_recent_blockhashes_locked(&blockhash_queue);
⋮----
fn get_timestamp_estimate(
⋮----
let slots_per_epoch = self.epoch_schedule().slots_per_epoch;
let vote_accounts = self.vote_accounts();
let recent_timestamps = vote_accounts.iter().filter_map(|(pubkey, (_, account))| {
let vote_state = account.vote_state_view();
let last_timestamp = vote_state.last_timestamp();
let slot_delta = self.slot().checked_sub(last_timestamp.slot)?;
⋮----
.then_some((*pubkey, (last_timestamp.slot, last_timestamp.timestamp)))
⋮----
let epoch = self.epoch_schedule().get_epoch(self.slot());
let stakes = self.epoch_vote_accounts(epoch)?;
let stake_weighted_timestamp = calculate_stake_weighted_timestamp(
⋮----
self.slot(),
⋮----
.is_active(&feature_set::warp_timestamp_again::id()),
⋮----
get_timestamp_estimate_time.stop();
⋮----
pub fn rehash(&self) {
let mut hash = self.hash.write().unwrap();
let new = self.hash_internal_state();
⋮----
warn!("Updating bank hash to {new}");
⋮----
pub fn freeze(&self) {
⋮----
self.distribute_transaction_fee_details();
self.update_slot_history();
self.run_incinerator();
self.freeze_started.store(true, Relaxed);
self.update_accounts_lt_hash();
*hash = self.hash_internal_state();
self.rc.accounts.accounts_db.mark_slot_frozen(self.slot());
⋮----
pub fn unfreeze_for_ledger_tool(&self) {
self.freeze_started.store(false, Relaxed);
⋮----
pub fn epoch_schedule(&self) -> &EpochSchedule {
⋮----
pub fn squash(&self) -> SquashTiming {
self.freeze();
let mut roots = vec![self.slot()];
roots.append(&mut self.parents().iter().map(|p| p.slot()).collect());
⋮----
for slot in roots.iter().rev() {
let add_root_timing = self.rc.accounts.add_root(*slot);
⋮----
squash_accounts_time.stop();
*self.rc.parent.write().unwrap() = None;
⋮----
.for_each(|slot| self.status_cache.write().unwrap().add_root(*slot));
squash_cache_time.stop();
⋮----
squash_accounts_ms: squash_accounts_time.as_ms(),
⋮----
squash_cache_ms: squash_cache_time.as_ms(),
⋮----
pub fn parent(&self) -> Option<Arc<Bank>> {
self.rc.parent.read().unwrap().clone()
⋮----
pub fn parent_slot(&self) -> Slot {
⋮----
pub fn parent_hash(&self) -> Hash {
⋮----
fn process_genesis_config(
⋮----
self.fee_rate_governor = genesis_config.fee_rate_governor.clone();
for (pubkey, account) in genesis_config.accounts.iter() {
assert!(
⋮----
let account_shared_data = create_account_shared_data(account);
self.store_account(pubkey, &account_shared_data);
self.capitalization.fetch_add(account.lamports(), Relaxed);
self.accounts_data_size_initial += account.data().len() as u64;
⋮----
for (pubkey, account) in genesis_config.rewards_pools.iter() {
⋮----
let collector_id = self.stakes_cache.stakes().highest_staked_node().copied();
⋮----
let collector_id = collector_id.or(collector_id_for_tests);
⋮----
collector_id.expect("genesis processing failed because no staked nodes exist");
⋮----
let genesis_hash = genesis_config.hash();
⋮----
let genesis_hash = genesis_hash.unwrap_or(genesis_config.hash());
self.blockhash_queue.write().unwrap().genesis_hash(
⋮----
self.hashes_per_tick = genesis_config.hashes_per_tick();
self.ticks_per_slot = genesis_config.ticks_per_slot();
self.ns_per_slot = genesis_config.ns_per_slot();
⋮----
self.slots_per_year = genesis_config.slots_per_year();
self.epoch_schedule = genesis_config.epoch_schedule.clone();
⋮----
self.epoch_schedule().clone(),
⋮----
genesis_config.rent.clone(),
⋮----
fn burn_and_purge_account(&self, program_id: &Pubkey, mut account: AccountSharedData) {
let old_data_size = account.data().len();
self.capitalization.fetch_sub(account.lamports(), Relaxed);
account.set_lamports(0);
account.data_as_mut_slice().fill(0);
self.store_account(program_id, &account);
self.calculate_and_update_accounts_data_size_delta_off_chain(old_data_size, 0);
⋮----
pub fn add_precompiled_account(&self, program_id: &Pubkey) {
self.add_precompiled_account_with_owner(program_id, native_loader::id())
⋮----
fn add_precompiled_account_with_owner(&self, program_id: &Pubkey, owner: Pubkey) {
if let Some(account) = self.get_account_with_fixed_root(program_id) {
⋮----
self.burn_and_purge_account(program_id, account);
⋮----
let (lamports, rent_epoch) = self.inherit_specially_retained_account_fields(&None);
⋮----
data: vec![],
⋮----
self.store_account_and_update_capitalization(program_id, &account);
⋮----
pub fn set_rent_burn_percentage(&mut self, burn_percent: u8) {
⋮----
pub fn set_hashes_per_tick(&mut self, hashes_per_tick: Option<u64>) {
⋮----
pub fn last_blockhash(&self) -> Hash {
self.blockhash_queue.read().unwrap().last_hash()
⋮----
pub fn last_blockhash_and_lamports_per_signature(&self) -> (Hash, u64) {
⋮----
let last_hash = blockhash_queue.last_hash();
⋮----
.get_lamports_per_signature(&last_hash)
⋮----
pub fn is_blockhash_valid(&self, hash: &Hash) -> bool {
⋮----
blockhash_queue.is_hash_valid_for_age(hash, MAX_PROCESSING_AGE)
⋮----
pub fn get_minimum_balance_for_rent_exemption(&self, data_len: usize) -> u64 {
self.rent_collector.rent.minimum_balance(data_len).max(1)
⋮----
pub fn get_lamports_per_signature(&self) -> u64 {
⋮----
pub fn get_lamports_per_signature_for_blockhash(&self, hash: &Hash) -> Option<u64> {
⋮----
blockhash_queue.get_lamports_per_signature(hash)
⋮----
pub fn get_fee_for_message(&self, message: &SanitizedMessage) -> Option<u64> {
⋮----
blockhash_queue.get_lamports_per_signature(message.recent_blockhash())
⋮----
.or_else(|| {
self.load_message_nonce_account(message).map(
⋮----
nonce_data.get_lamports_per_signature()
⋮----
Some(self.get_fee_for_message_with_lamports_per_signature(message, lamports_per_signature))
⋮----
pub fn get_fee_for_message_with_lamports_per_signature(
⋮----
process_compute_budget_instructions(
message.program_instructions_iter(),
⋮----
.unwrap_or_default(),
⋮----
self.fee_structure().lamports_per_signature,
⋮----
FeeFeatures::from(self.feature_set.as_ref()),
⋮----
pub fn get_blockhash_last_valid_block_height(&self, blockhash: &Hash) -> Option<Slot> {
⋮----
.get_hash_age(blockhash)
.map(|age| self.block_height + MAX_PROCESSING_AGE as u64 - age)
⋮----
pub fn confirmed_last_blockhash(&self) -> Hash {
⋮----
let parents = self.parents();
if parents.is_empty() {
self.last_blockhash()
⋮----
let index = NUM_BLOCKHASH_CONFIRMATIONS.min(parents.len() - 1);
parents[index].last_blockhash()
⋮----
pub fn clear_signatures(&self) {
self.status_cache.write().unwrap().clear();
⋮----
pub fn clear_slot_signatures(&self, slot: Slot) {
self.status_cache.write().unwrap().clear_slot_entries(slot);
⋮----
fn update_transaction_statuses(
⋮----
let mut status_cache = self.status_cache.write().unwrap();
assert_eq!(sanitized_txs.len(), processing_results.len());
for (tx, processing_result) in sanitized_txs.iter().zip(processing_results) {
⋮----
status_cache.insert(
tx.recent_blockhash(),
tx.message_hash(),
⋮----
processed_tx.status(),
⋮----
tx.signature(),
⋮----
fn register_recent_blockhash(&self, blockhash: &Hash, scheduler: &InstalledSchedulerRwLock) {
⋮----
let mut w_blockhash_queue = self.blockhash_queue.write().unwrap();
⋮----
.lock()
⋮----
.get_blockhash_override(self.slot())
.copied()
.inspect(|blockhash_override| {
⋮----
let blockhash = blockhash_override.as_ref().unwrap_or(blockhash);
w_blockhash_queue.register_hash(blockhash, self.fee_rate_governor.lamports_per_signature);
self.update_recent_blockhashes_locked(&w_blockhash_queue);
⋮----
pub fn register_unique_recent_blockhash_for_test(&self) {
self.register_recent_blockhash(
⋮----
pub fn register_recent_blockhash_for_test(
⋮----
w_blockhash_queue.register_hash(blockhash, lamports_per_signature);
⋮----
.register_hash(blockhash, self.fee_rate_governor.lamports_per_signature);
⋮----
pub fn register_tick(&self, hash: &Hash, scheduler: &InstalledSchedulerRwLock) {
⋮----
if self.is_block_boundary(self.tick_height.load(Relaxed) + 1) {
self.register_recent_blockhash(hash, scheduler);
⋮----
self.tick_height.fetch_add(1, Relaxed);
⋮----
pub fn register_tick_for_test(&self, hash: &Hash) {
self.register_tick(hash, &BankWithScheduler::no_scheduler_available())
⋮----
pub fn register_default_tick_for_test(&self) {
self.register_tick_for_test(&Hash::default())
⋮----
pub fn is_complete(&self) -> bool {
self.tick_height() == self.max_tick_height()
⋮----
pub fn is_block_boundary(&self, tick_height: u64) -> bool {
⋮----
pub fn get_transaction_account_lock_limit(&self) -> usize {
⋮----
.is_active(&feature_set::increase_tx_account_lock_limit::id())
⋮----
pub fn prepare_entry_batch(
⋮----
.is_active(&agave_feature_set::static_instruction_limit::id());
⋮----
.map(|tx| {
⋮----
self.get_reserved_account_keys(),
⋮----
Ok(TransactionBatch::new(
self.try_lock_accounts(&sanitized_txs, false),
⋮----
pub fn try_lock_accounts(
⋮----
self.try_lock_accounts_with_results(
⋮----
txs.iter().map(|_| Ok(())),
⋮----
.is_active(&feature_set::relax_intrabatch_account_locks::id()),
⋮----
pub fn try_lock_accounts_with_results(
⋮----
let tx_account_lock_limit = self.get_transaction_account_lock_limit();
let mut batch_message_hashes = AHashSet::with_capacity(txs.len());
⋮----
.enumerate()
.map(|(i, tx_result)| match tx_result {
⋮----
if batch_message_hashes.insert(txs[i].message_hash()) {
Ok(())
⋮----
Err(TransactionError::AlreadyProcessed)
⋮----
Ok(()) => Ok(()),
Err(e) => Err(e),
⋮----
self.rc.accounts.lock_accounts(
txs.iter(),
⋮----
pub fn prepare_sanitized_batch<'a, 'b, Tx: TransactionWithMeta>(
⋮----
self.prepare_sanitized_batch_with_results(txs, txs.iter().map(|_| Ok(())), false)
⋮----
pub fn prepare_sanitized_batch_relax_intrabatch_account_locks<
⋮----
transactions.iter().map(|_| Ok(())),
⋮----
pub fn prepare_sanitized_batch_with_results<'a, 'b, Tx: TransactionWithMeta>(
⋮----
pub fn prepare_unlocked_batch_from_single_tx<'a, Tx: SVMMessage>(
⋮----
let lock_result = validate_account_locks(transaction.account_keys(), tx_account_lock_limit);
⋮----
vec![lock_result],
⋮----
batch.set_needs_unlock(false);
⋮----
/// Prepare a transaction batch from a single transaction after locking accounts
    pub fn prepare_locked_batch_from_single_tx<'a, Tx: TransactionWithMeta>(
⋮----
pub fn prepare_locked_batch_from_single_tx<'a, Tx: TransactionWithMeta>(
⋮----
self.prepare_sanitized_batch(slice::from_ref(transaction))
⋮----
pub fn resanitize_transaction_minimally(
⋮----
if self.epoch() != sanitized_epoch {
self.check_reserved_keys(transaction)?;
⋮----
if self.slot() > alt_invalidation_slot {
⋮----
self.load_addresses_from_ref(transaction.message_address_table_lookups())?;
⋮----
pub fn simulate_transaction(
⋮----
assert!(self.is_frozen(), "simulation bank must be frozen");
self.simulate_transaction_unchecked(transaction, enable_cpi_recording)
⋮----
pub fn simulate_transaction_unchecked(
⋮----
let account_keys = transaction.account_keys();
let number_of_accounts = account_keys.len();
let account_overrides = self.get_account_overrides_for_simulation(&account_keys);
let batch = self.prepare_unlocked_batch_from_single_tx(transaction);
⋮----
} = self.load_and_execute_transactions(
⋮----
account_overrides: Some(&account_overrides),
⋮----
debug!("simulate_transaction: {timings:?}");
⋮----
.pop()
.unwrap_or(Err(TransactionError::InvalidProgramForExecution));
⋮----
let executed_units = processed_tx.executed_units();
let loaded_accounts_data_size = processed_tx.loaded_accounts_data_size();
⋮----
.take(number_of_accounts)
⋮----
Some(executed_tx.loaded_transaction.fee_details.total_fee()),
⋮----
vec![],
Err(fees_only_tx.load_error),
Some(fees_only_tx.fee_details.total_fee()),
⋮----
Err(error) => (vec![], Err(error), None, None, None, None, 0, 0),
⋮----
let logs = logs.unwrap_or_default();
⋮----
balance_collector.into_vecs();
⋮----
native_pre.pop(),
native_post.pop(),
token_pre.pop(),
token_post.pop(),
⋮----
pub fn simulate_transactions_unchecked_with_pre_accounts<Tx: TransactionWithMeta>(
⋮----
if transactions.is_empty() {
return vec![];
⋮----
account_overrides.merge(self.get_account_overrides_for_simulation(&account_keys));
for account in transaction.account_keys().iter() {
if !account_overrides.accounts().contains_key(account) {
⋮----
self.get_account_shared_data(account)
⋮----
account_overrides.set_account(account, Some(account_shared_data));
⋮----
izip!(transactions, pre_accounts, post_accounts)
⋮----
if let Some(account) = account_overrides.get(pubkey) {
accounts_pre_loaded.push((*pubkey, account.clone()));
⋮----
self.get_account_shared_data(pubkey)
⋮----
accounts_pre_loaded.push((*pubkey, account_shared_data));
⋮----
accounts_pre_loaded.push((*pubkey, AccountSharedData::default()));
⋮----
let number_of_accounts = transaction.account_keys().len();
⋮----
for (pubkey, account) in executed_tx.loaded_transaction.accounts.iter()
⋮----
account_overrides.set_account(pubkey, Some(account.clone()));
⋮----
.set_account(&fee_payer.0, Some(fee_payer.1.clone()));
⋮----
account_overrides.set_account(&nonce.0, Some(nonce.1.clone()));
⋮----
let execution_result = result.clone();
⋮----
accounts_post_loaded.push((*pubkey, account.clone()));
⋮----
accounts_post_loaded.push((*pubkey, account_shared_data));
⋮----
accounts_post_loaded.push((*pubkey, AccountSharedData::default()));
⋮----
simulation_results.push((
⋮----
if execution_result.is_err() {
⋮----
pub fn get_account_overrides_for_simulation(
⋮----
if account_keys.iter().any(|pubkey| *pubkey == slot_history_id) {
let current_account = self.get_account_with_fixed_root(&slot_history_id);
⋮----
if slot_history.check(self.slot()) == Check::Found {
let ancestors = Ancestors::from(self.proper_ancestors().collect::<Vec<_>>());
⋮----
self.load_slow_with_fixed_root(&ancestors, &slot_history_id)
⋮----
account_overrides.set_slot_history(Some(account));
⋮----
pub fn unlock_accounts<'a, Tx: SVMMessage + 'a>(
⋮----
self.rc.accounts.unlock_accounts(txs_and_results);
⋮----
pub fn remove_unrooted_slots(&self, slots: &[(Slot, BankId)]) {
self.rc.accounts.accounts_db.remove_unrooted_slots(slots)
⋮----
pub fn get_hash_age(&self, hash: &Hash) -> Option<u64> {
self.blockhash_queue.read().unwrap().get_hash_age(hash)
⋮----
pub fn is_hash_valid_for_age(&self, hash: &Hash, max_age: usize) -> bool {
⋮----
.is_hash_valid_for_age(hash, max_age)
⋮----
pub fn collect_balances(
⋮----
let mut balances: TransactionBalances = vec![];
for transaction in batch.sanitized_transactions() {
let mut transaction_balances: Vec<u64> = vec![];
for account_key in transaction.account_keys().iter() {
transaction_balances.push(self.get_balance(account_key));
⋮----
balances.push(transaction_balances);
⋮----
pub fn load_and_execute_transactions(
⋮----
let sanitized_txs = batch.sanitized_transactions();
let (check_results, check_us) = measure_us!(self.check_transactions(
⋮----
timings.saturating_add_in_place(ExecuteTimingType::CheckUs, check_us);
⋮----
self.last_blockhash_and_lamports_per_signature();
⋮----
self.epoch_schedule().get_epoch(self.slot.saturating_add(
⋮----
epoch_total_stake: self.get_current_epoch_total_stake(),
feature_set: self.feature_set.runtime_features(),
⋮----
.clone(),
⋮----
.get_environments_for_epoch(effective_epoch_of_deployments),
rent: self.rent_collector.rent.clone(),
⋮----
.load_and_execute_sanitized_transactions(
⋮----
error_counters.accumulate(&sanitized_output.error_metrics);
timings.accumulate(&sanitized_output.execute_timings);
⋮----
measure_us!(self.collect_logs(sanitized_txs, &sanitized_output.processing_results));
timings.saturating_add_in_place(ExecuteTimingType::CollectLogsUs, collect_logs_us);
⋮----
.zip(sanitized_txs)
⋮----
for key in tx.account_keys().iter() {
if debug_keys.contains(key) {
let result = processing_result.flattened_result();
info!("slot: {} result: {:?} tx: {:?}", self.slot, result, tx);
⋮----
if processing_result.was_processed() {
⋮----
tx.signature_details().num_transaction_signatures();
⋮----
if !tx.is_simple_vote_transaction() {
⋮----
match processing_result.flattened_result() {
⋮----
debug!("tx error: {err:?} {tx:?}");
⋮----
fn collect_logs(
⋮----
self.transaction_log_collector_config.read().unwrap();
⋮----
.zip(transactions)
.filter_map(|(processing_result, transaction)| {
let processed_tx = processing_result.processed_transaction()?;
let execution_details = processed_tx.execution_details()?;
⋮----
if !collected_logs.is_empty() {
let mut transaction_log_collector = self.transaction_log_collector.write().unwrap();
⋮----
let transaction_log_index = transaction_log_collector.logs.len();
transaction_log_collector.logs.push(log);
for key in filtered_mentioned_addresses.into_iter() {
⋮----
.entry(key)
.or_default()
.push(transaction_log_index);
⋮----
fn collect_transaction_logs(
⋮----
let log_messages = execution_details.log_messages.as_ref()?;
⋮----
.is_empty()
⋮----
for key in transaction.account_keys().iter() {
⋮----
.contains(key)
⋮----
filtered_mentioned_addresses.push(*key);
⋮----
let is_vote = transaction.is_simple_vote_transaction();
⋮----
!is_vote || !filtered_mentioned_addresses.is_empty()
⋮----
!filtered_mentioned_addresses.is_empty()
⋮----
Some((
⋮----
signature: *transaction.signature(),
result: execution_details.status.clone(),
⋮----
log_messages: log_messages.clone(),
⋮----
pub fn load_accounts_data_size(&self) -> u64 {
⋮----
.saturating_add_signed(self.load_accounts_data_size_delta())
⋮----
pub fn load_accounts_data_size_delta(&self) -> i64 {
let delta_on_chain = self.load_accounts_data_size_delta_on_chain();
let delta_off_chain = self.load_accounts_data_size_delta_off_chain();
delta_on_chain.saturating_add(delta_off_chain)
⋮----
pub fn load_accounts_data_size_delta_on_chain(&self) -> i64 {
self.accounts_data_size_delta_on_chain.load(Acquire)
⋮----
pub fn load_accounts_data_size_delta_off_chain(&self) -> i64 {
self.accounts_data_size_delta_off_chain.load(Acquire)
⋮----
fn update_accounts_data_size_delta_on_chain(&self, amount: i64) {
⋮----
.fetch_update(AcqRel, Acquire, |accounts_data_size_delta_on_chain| {
Some(accounts_data_size_delta_on_chain.saturating_add(amount))
⋮----
fn update_accounts_data_size_delta_off_chain(&self, amount: i64) {
⋮----
.fetch_update(AcqRel, Acquire, |accounts_data_size_delta_off_chain| {
Some(accounts_data_size_delta_off_chain.saturating_add(amount))
⋮----
fn calculate_and_update_accounts_data_size_delta_off_chain(
⋮----
let data_size_delta = calculate_data_size_delta(old_data_size, new_data_size);
self.update_accounts_data_size_delta_off_chain(data_size_delta);
⋮----
fn filter_program_errors_and_collect_fee_details(
⋮----
processing_results.iter().for_each(|processing_result| {
⋮----
accumulated_fee_details.accumulate(&processed_tx.fee_details());
⋮----
.accumulate(&accumulated_fee_details);
⋮----
fn update_bank_hash_stats<'a>(&self, accounts: &impl StorableAccounts<'a>) {
⋮----
(0..accounts.len()).for_each(|i| {
accounts.account(i, |account| {
stats.update(&account);
⋮----
self.bank_hash_stats.accumulate(&stats);
⋮----
pub fn commit_transactions(
⋮----
self.increment_transaction_count(processed_transactions_count);
self.increment_non_vote_transaction_count_since_restart(
⋮----
self.increment_signature_count(signature_count);
⋮----
processed_transactions_count.saturating_sub(processed_with_successful_result_count);
⋮----
.fetch_add(processed_with_failure_result_count, Relaxed);
⋮----
self.is_delta.store(true, Relaxed);
self.transaction_entries_count.fetch_add(1, Relaxed);
⋮----
.fetch_max(processed_transactions_count, Relaxed);
⋮----
let ((), store_accounts_us) = measure_us!({
⋮----
measure_us!(self.update_stakes_cache(sanitized_txs, &processing_results));
let ((), update_executors_us) = measure_us!({
⋮----
.filter_map(|processing_result| processing_result.processed_transaction())
.filter_map(|processed_tx| processed_tx.execution_details())
.filter_map(|details| {
⋮----
.is_ok()
.then_some(details.accounts_data_len_delta)
⋮----
.sum();
self.update_accounts_data_size_delta_on_chain(accounts_data_len_delta);
⋮----
measure_us!(self.update_transaction_statuses(sanitized_txs, &processing_results));
self.filter_program_errors_and_collect_fee_details(&processing_results);
timings.saturating_add_in_place(ExecuteTimingType::StoreUs, store_accounts_us);
timings.saturating_add_in_place(
⋮----
timings.saturating_add_in_place(ExecuteTimingType::UpdateExecutorsUs, update_executors_us);
⋮----
fn create_commit_results(
⋮----
.map(|processing_result| {
⋮----
let executed_units = processing_result.executed_units();
let loaded_accounts_data_size = processing_result.loaded_accounts_data_size();
⋮----
let successful = executed_tx.was_successful();
⋮----
loaded_accounts[0].1.lamports()
⋮----
rollback_accounts.fee_payer().1.lamports()
⋮----
Ok(CommittedTransaction {
⋮----
loaded_accounts_count: loaded_accounts.len(),
⋮----
ProcessedTransaction::FeesOnly(fees_only_tx) => Ok(CommittedTransaction {
status: Err(fees_only_tx.load_error),
⋮----
loaded_accounts_count: fees_only_tx.rollback_accounts.count(),
⋮----
.fee_payer()
⋮----
.lamports(),
⋮----
fn run_incinerator(&self) {
⋮----
self.get_account_modified_since_parent_with_fixed_root(&incinerator::id())
⋮----
self.store_account(&incinerator::id(), &AccountSharedData::default());
⋮----
pub(crate) fn get_accounts_for_bank_hash_details(&self) -> Vec<(Pubkey, AccountSharedData)> {
⋮----
.get_pubkey_account_for_slot(self.slot());
accounts.sort_unstable_by(|a, b| a.0.cmp(&b.0));
⋮----
pub fn cluster_type(&self) -> ClusterType {
self.cluster_type.unwrap()
⋮----
pub fn load_execute_and_commit_transactions(
⋮----
self.do_load_execute_and_commit_transactions_with_pre_commit_callback(
⋮----
pub fn load_execute_and_commit_transactions_with_pre_commit_callback<'a>(
⋮----
Some(pre_commit_callback),
⋮----
fn do_load_execute_and_commit_transactions_with_pre_commit_callback<'a>(
⋮----
pre_commit_callback(timings, &processing_results)?
⋮----
let commit_results = self.commit_transactions(
batch.sanitized_transactions(),
⋮----
drop(freeze_lock);
Ok((commit_results, balance_collector))
⋮----
pub fn process_transaction(&self, tx: &Transaction) -> Result<()> {
self.try_process_transactions(std::iter::once(tx))?[0].clone()
⋮----
pub fn process_transaction_with_metadata(
⋮----
let txs = vec![tx.into()];
let batch = self.prepare_entry_batch(txs)?;
let (mut commit_results, ..) = self.load_execute_and_commit_transactions(
⋮----
Some(1000 * 1000),
⋮----
commit_results.remove(0)
⋮----
pub fn try_process_transactions<'a>(
⋮----
.map(|tx| VersionedTransaction::from(tx.clone()))
⋮----
self.try_process_entry_transactions(txs)
⋮----
pub fn try_process_entry_transactions(
⋮----
Ok(self.process_transaction_batch(&batch))
⋮----
fn process_transaction_batch(
⋮----
self.load_execute_and_commit_transactions(
⋮----
.map(|commit_result| commit_result.and_then(|committed_tx| committed_tx.status))
⋮----
pub fn transfer(&self, n: u64, keypair: &Keypair, to: &Pubkey) -> Result<Signature> {
let blockhash = self.last_blockhash();
⋮----
self.process_transaction(&tx).map(|_| signature)
⋮----
pub fn read_balance(account: &AccountSharedData) -> u64 {
account.lamports()
⋮----
pub fn get_balance(&self, pubkey: &Pubkey) -> u64 {
self.get_account(pubkey)
.map(|x| Self::read_balance(&x))
⋮----
pub fn parents(&self) -> Vec<Arc<Bank>> {
let mut parents = vec![];
let mut bank = self.parent();
⋮----
parents.push(parent.clone());
bank = parent.parent();
⋮----
pub fn parents_inclusive(self: Arc<Self>) -> Vec<Arc<Bank>> {
let mut parents = self.parents();
parents.insert(0, self);
⋮----
pub fn store_account(&self, pubkey: &Pubkey, account: &AccountSharedData) {
self.store_accounts((self.slot(), &[(pubkey, account)][..]))
⋮----
pub fn store_accounts<'a>(&self, accounts: impl StorableAccounts<'a>) {
assert!(!self.freeze_started());
⋮----
let new_warmup_cooldown_rate_epoch = self.new_warmup_cooldown_rate_epoch();
⋮----
self.stakes_cache.check_and_store(
account.pubkey(),
⋮----
self.update_bank_hash_stats(&accounts);
self.rc.accounts.store_accounts_par(accounts, None);
m.stop();
⋮----
.fetch_add(m.as_us(), Relaxed);
⋮----
pub fn force_flush_accounts_cache(&self) {
⋮----
.flush_accounts_cache(true, Some(self.slot()))
⋮----
pub fn flush_accounts_cache_if_needed(&self) {
⋮----
.flush_accounts_cache(false, Some(self.slot()))
⋮----
fn store_account_and_update_capitalization(
⋮----
self.get_account_with_fixed_root_no_cache(pubkey)
⋮----
match new_account.lamports().cmp(&old_account.lamports()) {
⋮----
let diff = new_account.lamports() - old_account.lamports();
trace!("store_account_and_update_capitalization: increased: {pubkey} {diff}");
self.capitalization.fetch_add(diff, Relaxed);
⋮----
let diff = old_account.lamports() - new_account.lamports();
trace!("store_account_and_update_capitalization: decreased: {pubkey} {diff}");
self.capitalization.fetch_sub(diff, Relaxed);
⋮----
old_account.data().len()
⋮----
trace!(
⋮----
.fetch_add(new_account.lamports(), Relaxed);
⋮----
self.store_account(pubkey, new_account);
self.calculate_and_update_accounts_data_size_delta_off_chain(
⋮----
new_account.data().len(),
⋮----
pub fn accounts(&self) -> Arc<Accounts> {
self.rc.accounts.clone()
⋮----
fn apply_simd_0306_cost_tracker_changes(&mut self) {
let mut cost_tracker = self.write_cost_tracker().unwrap();
let block_cost_limit = cost_tracker.get_block_limit();
let vote_cost_limit = cost_tracker.get_vote_limit();
let account_cost_limit = block_cost_limit.saturating_mul(40).saturating_div(100);
cost_tracker.set_limits(account_cost_limit, block_cost_limit, vote_cost_limit);
⋮----
fn apply_simd_0339_invoke_cost_changes(&mut self) {
⋮----
.is_active(&raise_cpi_nesting_limit_to_8::id());
⋮----
.is_active(&increase_cpi_account_info_limit::id());
⋮----
.compute_budget()
⋮----
.unwrap_or(&ComputeBudget::new_with_defaults(
⋮----
.to_cost();
⋮----
.set_execution_cost(compute_budget);
⋮----
fn apply_activated_features(&mut self) {
⋮----
reserved_keys.update_active_set(&self.feature_set);
⋮----
self.add_active_builtin_programs();
⋮----
.is_active(&feature_set::raise_block_limits_to_100m::id())
⋮----
let block_cost_limit = simd_0286_block_limits();
⋮----
let account_cost_limit = cost_tracker.get_account_limit();
⋮----
.is_active(&feature_set::raise_account_cu_limit::id())
⋮----
self.apply_simd_0306_cost_tracker_changes();
⋮----
.is_active(&feature_set::increase_cpi_account_info_limit::id())
⋮----
self.apply_simd_0339_invoke_cost_changes();
⋮----
let environments = self.create_program_runtime_environments(&self.feature_set);
⋮----
fn create_program_runtime_environments(
⋮----
let simd_0268_active = feature_set.is_active(&raise_cpi_nesting_limit_to_8::id());
let simd_0339_active = feature_set.is_active(&increase_cpi_account_info_limit::id());
⋮----
.to_budget();
⋮----
create_program_runtime_environment_v1(
&feature_set.runtime_features(),
⋮----
.unwrap(),
⋮----
program_runtime_v2: Arc::new(create_program_runtime_environment_v2(
⋮----
pub fn set_tick_height(&self, tick_height: u64) {
self.tick_height.store(tick_height, Relaxed)
⋮----
pub fn set_inflation(&self, inflation: Inflation) {
*self.inflation.write().unwrap() = inflation;
⋮----
pub fn hard_forks(&self) -> HardForks {
self.hard_forks.read().unwrap().clone()
⋮----
pub fn register_hard_fork(&self, new_hard_fork_slot: Slot) {
let bank_slot = self.slot();
let lock = self.freeze_lock();
⋮----
warn!(
⋮----
.register(new_hard_fork_slot);
⋮----
pub fn get_account_with_fixed_root_no_cache(
⋮----
self.load_account_with(pubkey, false)
.map(|(acc, _slot)| acc)
⋮----
fn load_account_with(
⋮----
self.rc.accounts.accounts_db.load_account_with(
⋮----
pub fn get_account(&self, pubkey: &Pubkey) -> Option<AccountSharedData> {
self.get_account_modified_slot(pubkey)
⋮----
pub fn get_account_with_fixed_root(&self, pubkey: &Pubkey) -> Option<AccountSharedData> {
self.get_account_modified_slot_with_fixed_root(pubkey)
⋮----
pub fn get_account_modified_slot_with_fixed_root(
⋮----
self.load_slow_with_fixed_root(&self.ancestors, pubkey)
⋮----
pub fn get_account_modified_slot(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, Slot)> {
self.load_slow(&self.ancestors, pubkey)
⋮----
fn load_slow(
⋮----
self.rc.accounts.load_without_fixed_root(ancestors, pubkey)
⋮----
fn load_slow_with_fixed_root(
⋮----
self.rc.accounts.load_with_fixed_root(ancestors, pubkey)
⋮----
pub fn get_program_accounts(
⋮----
.load_by_program(&self.ancestors, self.bank_id, program_id, config)
⋮----
pub fn get_filtered_program_accounts<F: Fn(&AccountSharedData) -> bool>(
⋮----
self.rc.accounts.load_by_program_with_filter(
⋮----
pub fn get_filtered_indexed_accounts<F: Fn(&AccountSharedData) -> bool>(
⋮----
self.rc.accounts.load_by_index_key_with_filter(
⋮----
pub fn account_indexes_include_key(&self, key: &Pubkey) -> bool {
self.rc.accounts.account_indexes_include_key(key)
⋮----
pub fn get_all_accounts(&self, sort_results: bool) -> ScanResult<Vec<PubkeyAccountSlot>> {
⋮----
.load_all(&self.ancestors, self.bank_id, sort_results)
⋮----
pub fn scan_all_accounts<F>(&self, scan_func: F, sort_results: bool) -> ScanResult<()>
⋮----
.scan_all(&self.ancestors, self.bank_id, scan_func, sort_results)
⋮----
pub fn get_program_accounts_modified_since_parent(
⋮----
.load_by_program_slot(self.slot(), Some(program_id))
⋮----
pub fn get_transaction_logs(
⋮----
.get_logs_for_address(address)
⋮----
pub fn get_all_accounts_modified_since_parent(&self) -> Vec<KeyedAccountSharedData> {
self.rc.accounts.load_by_program_slot(self.slot(), None)
⋮----
fn get_account_modified_since_parent_with_fixed_root(
⋮----
let just_self: Ancestors = Ancestors::from(vec![self.slot()]);
if let Some((account, slot)) = self.load_slow_with_fixed_root(&just_self, pubkey) {
if slot == self.slot() {
return Some((account, slot));
⋮----
pub fn get_largest_accounts(
⋮----
self.rc.accounts.load_largest_accounts(
⋮----
pub fn transaction_count(&self) -> u64 {
self.transaction_count.load(Relaxed)
⋮----
pub fn non_vote_transaction_count_since_restart(&self) -> u64 {
self.non_vote_transaction_count_since_restart.load(Relaxed)
⋮----
pub fn executed_transaction_count(&self) -> u64 {
self.transaction_count()
.saturating_sub(self.parent().map_or(0, |parent| parent.transaction_count()))
⋮----
pub fn transaction_error_count(&self) -> u64 {
self.transaction_error_count.load(Relaxed)
⋮----
pub fn transaction_entries_count(&self) -> u64 {
self.transaction_entries_count.load(Relaxed)
⋮----
pub fn transactions_per_entry_max(&self) -> u64 {
self.transactions_per_entry_max.load(Relaxed)
⋮----
fn increment_transaction_count(&self, tx_count: u64) {
self.transaction_count.fetch_add(tx_count, Relaxed);
⋮----
fn increment_non_vote_transaction_count_since_restart(&self, tx_count: u64) {
⋮----
.fetch_add(tx_count, Relaxed);
⋮----
pub fn signature_count(&self) -> u64 {
self.signature_count.load(Relaxed)
⋮----
fn increment_signature_count(&self, signature_count: u64) {
self.signature_count.fetch_add(signature_count, Relaxed);
⋮----
pub fn get_signature_status_processed_since_parent(
⋮----
if let Some((slot, status)) = self.get_signature_status_slot(signature) {
if slot <= self.slot() {
return Some(status);
⋮----
pub fn get_signature_status_with_blockhash(
⋮----
let rcache = self.status_cache.read().unwrap();
⋮----
.get_status(signature, blockhash, &self.ancestors)
.map(|v| v.1)
⋮----
pub fn get_committed_transaction_status_and_slot(
⋮----
.get_status(message_hash, transaction_blockhash, &self.ancestors)
.map(|(slot, status)| (slot, status.is_ok()))
⋮----
pub fn get_signature_status_slot(&self, signature: &Signature) -> Option<(Slot, Result<()>)> {
⋮----
rcache.get_status_any_blockhash(signature, &self.ancestors)
⋮----
pub fn get_signature_status(&self, signature: &Signature) -> Option<Result<()>> {
self.get_signature_status_slot(signature).map(|v| v.1)
⋮----
pub fn has_signature(&self, signature: &Signature) -> bool {
self.get_signature_status_slot(signature).is_some()
⋮----
fn hash_internal_state(&self) -> Hash {
⋮----
let mut hash = hashv(&[
self.parent_hash.as_ref(),
&self.signature_count().to_le_bytes(),
self.last_blockhash().as_ref(),
⋮----
let accounts_lt_hash = &*self.accounts_lt_hash.lock().unwrap();
⋮----
hash = hashv(&[hash.as_ref(), lt_hash_bytes]);
accounts_lt_hash.0.checksum()
⋮----
.get_hash_data(slot, self.parent_slot());
⋮----
let hard_forked_hash = hashv(&[hash.as_ref(), &buf]);
warn!("hard fork at slot {slot} by hashing {buf:?}: {hash} => {hard_forked_hash}");
⋮----
.get_bank_hash_override(slot)
⋮----
.inspect(|&hash_override| {
⋮----
let hash = hash_override.unwrap_or(std::hint::black_box(hash));
let bank_hash_stats = self.bank_hash_stats.load();
let total_us = measure_total.end_as_us();
⋮----
pub fn collector_fees(&self) -> u64 {
self.collector_fees.load(Relaxed)
⋮----
pub fn run_final_hash_calc(&self) {
self.force_flush_accounts_cache();
_ = self.verify_accounts(
⋮----
fn verify_accounts(
⋮----
if config.require_rooted_bank && !accounts_db.accounts_index.is_alive_root(slot) {
if let Some(parent) = self.parent() {
⋮----
return parent.verify_accounts(config, None);
⋮----
panic!("cannot verify accounts hash because slot {slot} is not a root");
⋮----
fn check_lt_hash(
⋮----
let expected = expected_accounts_lt_hash.0.checksum();
let calculated = calculated_accounts_lt_hash.0.checksum();
error!(
⋮----
info!("Verifying accounts...");
⋮----
let expected_accounts_lt_hash = self.accounts_lt_hash.lock().unwrap().clone();
⋮----
check_lt_hash(&expected_accounts_lt_hash, calculated_accounts_lt_hash)
⋮----
accounts_db.calculate_accounts_lt_hash_at_startup_from_index(&self.ancestors, slot);
check_lt_hash(&expected_accounts_lt_hash, &calculated_accounts_lt_hash)
⋮----
info!("Verifying accounts... Done in {:?}", start.elapsed());
⋮----
pub fn get_snapshot_storages(&self, base_slot: Option<Slot>) -> Vec<Arc<AccountStorageEntry>> {
let start_slot = base_slot.map_or(0, |slot| slot.saturating_add(1));
let requested_slots = start_slot..=self.slot();
self.rc.accounts.accounts_db.get_storages(requested_slots).0
⋮----
fn verify_hash(&self) -> bool {
assert!(self.is_frozen());
let calculated_hash = self.hash_internal_state();
let expected_hash = self.hash();
⋮----
pub fn verify_transaction(
⋮----
bincode::serialized_size(&tx).map_err(|_| TransactionError::SanitizeFailure)?;
⋮----
return Err(TransactionError::SanitizeFailure);
⋮----
&& tx.message.instructions().len()
⋮----
return Err(solana_transaction_error::TransactionError::SanitizeFailure);
⋮----
tx.verify_and_hash_message()?
⋮----
tx.message.hash()
⋮----
Ok(sanitized_tx)
⋮----
pub fn fully_verify_transaction(
⋮----
self.verify_transaction(tx, TransactionVerificationMode::FullVerification)
⋮----
pub fn check_reserved_keys(&self, tx: &impl SVMMessage) -> Result<()> {
let reserved_keys = self.get_reserved_account_keys();
for (index, key) in tx.account_keys().iter().enumerate() {
if tx.is_writable(index) && reserved_keys.contains(key) {
return Err(TransactionError::ResanitizationNeeded);
⋮----
pub fn calculate_capitalization_for_tests(&self) -> u64 {
⋮----
.calculate_capitalization_at_startup_from_index(&self.ancestors, self.slot())
⋮----
pub fn set_capitalization_for_tests(&self, capitalization: u64) {
self.capitalization.store(capitalization, Relaxed);
⋮----
pub fn get_snapshot_hash(&self) -> SnapshotHash {
SnapshotHash::new(self.accounts_lt_hash.lock().unwrap().0.checksum())
⋮----
pub fn load_account_into_read_cache(&self, key: &Pubkey) {
⋮----
.load_account_into_read_cache(&self.ancestors, key);
⋮----
pub fn verify_snapshot_bank(
⋮----
let (verified_accounts, verify_accounts_time_us) = measure_us!({
⋮----
let (_, clean_time_us) = measure_us!({
⋮----
let (_, shrink_time_us) = measure_us!({
⋮----
info!("Verifying bank...");
let (verified_bank, verify_bank_time_us) = measure_us!(self.verify_hash());
info!("Verifying bank... Done.");
⋮----
pub fn hashes_per_tick(&self) -> &Option<u64> {
⋮----
pub fn ticks_per_slot(&self) -> u64 {
⋮----
pub fn slots_per_year(&self) -> f64 {
⋮----
pub fn tick_height(&self) -> u64 {
self.tick_height.load(Relaxed)
⋮----
pub fn inflation(&self) -> Inflation {
*self.inflation.read().unwrap()
⋮----
pub fn rent_collector(&self) -> &RentCollector {
⋮----
pub fn capitalization(&self) -> u64 {
self.capitalization.load(Relaxed)
⋮----
pub fn max_tick_height(&self) -> u64 {
⋮----
pub fn block_height(&self) -> u64 {
⋮----
pub fn get_slots_in_epoch(&self, epoch: Epoch) -> u64 {
self.epoch_schedule().get_slots_in_epoch(epoch)
⋮----
pub fn get_leader_schedule_epoch(&self, slot: Slot) -> Epoch {
self.epoch_schedule().get_leader_schedule_epoch(slot)
⋮----
pub fn should_use_vote_keyed_leader_schedule(&self, epoch: Epoch) -> Option<bool> {
⋮----
.activated_slot(&agave_feature_set::enable_vote_address_leader_schedule::id())
.map(|activation_slot| {
⋮----
let activation_epoch = self.epoch_schedule.get_epoch(activation_slot);
activation_epoch.wrapping_add(1)
⋮----
return Some(epoch >= effective_epoch);
⋮----
let max_cached_leader_schedule = self.get_leader_schedule_epoch(self.slot());
⋮----
Some(false)
⋮----
fn update_stakes_cache(
⋮----
debug_assert_eq!(txs.len(), processing_results.len());
⋮----
txs.iter()
.zip(processing_results)
.filter_map(|(tx, processing_result)| {
⋮----
.processed_transaction()
.map(|processed_tx| (tx, processed_tx))
⋮----
.filter_map(|(tx, processed_tx)| {
⋮----
.executed_transaction()
.map(|executed_tx| (tx, executed_tx))
⋮----
.filter(|(_, executed_tx)| executed_tx.was_successful())
.flat_map(|(tx, executed_tx)| {
let num_account_keys = tx.account_keys().len();
⋮----
loaded_tx.accounts.iter().take(num_account_keys)
⋮----
.for_each(|(pubkey, account)| {
⋮----
.check_and_store(pubkey, account, new_warmup_cooldown_rate_epoch);
⋮----
pub fn vote_accounts(&self) -> Arc<VoteAccountsHashMap> {
⋮----
Arc::from(stakes.vote_accounts())
⋮----
pub fn get_vote_account(&self, vote_account: &Pubkey) -> Option<VoteAccount> {
⋮----
let vote_account = stakes.vote_accounts().get(vote_account)?;
Some(vote_account.clone())
⋮----
pub fn current_epoch_stakes(&self) -> &VersionedEpochStakes {
⋮----
.get(&self.epoch.saturating_add(1))
.expect("Current epoch stakes must exist")
⋮----
pub fn epoch_stakes(&self, epoch: Epoch) -> Option<&VersionedEpochStakes> {
self.epoch_stakes.get(&epoch)
⋮----
pub fn epoch_stakes_map(&self) -> &HashMap<Epoch, VersionedEpochStakes> {
⋮----
pub fn current_epoch_staked_nodes(&self) -> Arc<HashMap<Pubkey, u64>> {
self.current_epoch_stakes().stakes().staked_nodes()
⋮----
pub fn epoch_staked_nodes(&self, epoch: Epoch) -> Option<Arc<HashMap<Pubkey, u64>>> {
Some(self.epoch_stakes.get(&epoch)?.stakes().staked_nodes())
⋮----
pub fn epoch_total_stake(&self, epoch: Epoch) -> Option<u64> {
⋮----
.get(&epoch)
.map(|epoch_stakes| epoch_stakes.total_stake())
⋮----
pub fn get_current_epoch_total_stake(&self) -> u64 {
self.current_epoch_stakes().total_stake()
⋮----
pub fn epoch_vote_accounts(&self, epoch: Epoch) -> Option<&VoteAccountsHashMap> {
let epoch_stakes = self.epoch_stakes.get(&epoch)?.stakes();
Some(epoch_stakes.vote_accounts().as_ref())
⋮----
pub fn get_current_epoch_vote_accounts(&self) -> &VoteAccountsHashMap {
self.current_epoch_stakes()
⋮----
pub fn epoch_authorized_voter(&self, vote_account: &Pubkey) -> Option<&Pubkey> {
⋮----
.get(&self.epoch)
.expect("Epoch stakes for bank's own epoch must exist")
.epoch_authorized_voters()
.get(vote_account)
⋮----
pub fn epoch_vote_accounts_for_node_id(&self, node_id: &Pubkey) -> Option<&NodeVoteAccounts> {
⋮----
.node_id_to_vote_accounts()
.get(node_id)
⋮----
pub fn epoch_node_id_to_stake(&self, epoch: Epoch, node_id: &Pubkey) -> Option<u64> {
self.epoch_stakes(epoch)
.and_then(|epoch_stakes| epoch_stakes.node_id_to_stake(node_id))
⋮----
pub fn total_epoch_stake(&self) -> u64 {
⋮----
.total_stake()
⋮----
pub fn epoch_vote_account_stake(&self, vote_account: &Pubkey) -> u64 {
⋮----
.epoch_vote_accounts(self.epoch())
.expect("Bank epoch vote accounts must contain entry for the bank's own epoch")
⋮----
.map(|(stake, _)| stake)
.unwrap_or(&0)
⋮----
pub fn get_epoch_and_slot_index(&self, slot: Slot) -> (Epoch, SlotIndex) {
self.epoch_schedule().get_epoch_and_slot_index(slot)
⋮----
pub fn get_epoch_info(&self) -> EpochInfo {
let absolute_slot = self.slot();
let block_height = self.block_height();
let (epoch, slot_index) = self.get_epoch_and_slot_index(absolute_slot);
let slots_in_epoch = self.get_slots_in_epoch(epoch);
let transaction_count = Some(self.transaction_count());
⋮----
pub fn is_empty(&self) -> bool {
!self.is_delta.load(Relaxed)
⋮----
pub fn add_mockup_builtin(
⋮----
self.add_builtin(
⋮----
pub fn add_precompile(&mut self, program_id: &Pubkey) {
debug!("Adding precompiled program {program_id}");
self.add_precompiled_account(program_id);
debug!("Added precompiled program {program_id:?}");
⋮----
pub(crate) fn clean_accounts(&self) {
let highest_slot_to_clean = self.slot().saturating_sub(1);
self.rc.accounts.accounts_db.clean_accounts(
Some(highest_slot_to_clean),
⋮----
pub fn print_accounts_stats(&self) {
self.rc.accounts.accounts_db.print_accounts_stats("");
⋮----
pub fn shrink_candidate_slots(&self) -> usize {
⋮----
.shrink_candidate_slots(self.epoch_schedule())
⋮----
pub(crate) fn shrink_ancient_slots(&self) {
⋮----
.shrink_ancient_slots(self.epoch_schedule())
⋮----
pub fn read_cost_tracker(&self) -> LockResult<RwLockReadGuard<'_, CostTracker>> {
self.cost_tracker.read()
⋮----
pub fn write_cost_tracker(&self) -> LockResult<RwLockWriteGuard<'_, CostTracker>> {
self.cost_tracker.write()
⋮----
// Check if the wallclock time from bank creation to now has exceeded the allotted
// time for transaction processing
pub fn should_bank_still_be_processing_txs(
⋮----
// Do this check outside of the PoH lock, hence not a method on PohRecorder
bank_creation_time.elapsed().as_nanos() <= max_tx_ingestion_nanos
⋮----
pub fn deactivate_feature(&mut self, id: &Pubkey) {
let mut feature_set = Arc::make_mut(&mut self.feature_set).clone();
feature_set.active_mut().remove(id);
feature_set.inactive_mut().insert(*id);
⋮----
pub fn activate_feature(&mut self, id: &Pubkey) {
⋮----
feature_set.inactive_mut().remove(id);
feature_set.active_mut().insert(*id, 0);
⋮----
pub fn fill_bank_with_ticks_for_tests(&self) {
self.do_fill_bank_with_ticks_for_tests(&BankWithScheduler::no_scheduler_available())
⋮----
pub(crate) fn do_fill_bank_with_ticks_for_tests(&self, scheduler: &InstalledSchedulerRwLock) {
if self.tick_height.load(Relaxed) < self.max_tick_height {
let last_blockhash = self.last_blockhash();
while self.last_blockhash() == last_blockhash {
self.register_tick(&Hash::new_unique(), scheduler)
⋮----
warn!("Bank already reached max tick height, cannot fill it with more ticks");
⋮----
pub fn get_reserved_account_keys(&self) -> &HashSet<Pubkey> {
⋮----
fn initialize_after_snapshot_restore<F, TP>(&mut self, rewards_thread_pool_builder: F)
⋮----
self.compute_and_apply_features_after_snapshot_restore();
self.recalculate_partitioned_rewards_if_active(rewards_thread_pool_builder);
⋮----
fn compute_and_apply_genesis_features(&mut self) {
let feature_set = self.compute_active_feature_set(false).0;
⋮----
self.add_builtin_program_accounts();
self.apply_activated_features();
⋮----
fn compute_and_apply_features_after_snapshot_restore(&mut self) {
⋮----
fn compute_and_apply_new_feature_activations(&mut self) {
⋮----
self.compute_active_feature_set(include_pending);
⋮----
for feature_id in new_feature_activations.iter() {
if let Some(mut account) = self.get_account_with_fixed_root(feature_id) {
⋮----
feature.activated_at = Some(self.slot());
if feature::state::to_account(&feature, &mut account).is_some() {
self.store_account(feature_id, &account);
⋮----
info!("Feature {} activated at slot {}", feature_id, self.slot());
⋮----
if new_feature_activations.contains(&feature_set::deprecate_rent_exemption_threshold::id())
⋮----
self.update_rent();
⋮----
if new_feature_activations.contains(&feature_set::pico_inflation::id()) {
*self.inflation.write().unwrap() = Inflation::pico();
⋮----
if !new_feature_activations.is_disjoint(&self.feature_set.full_inflation_features_enabled())
⋮----
*self.inflation.write().unwrap() = Inflation::full();
⋮----
self.apply_new_builtin_program_feature_transitions(&new_feature_activations);
if new_feature_activations.contains(&feature_set::raise_block_limits_to_100m::id()) {
⋮----
drop(cost_tracker);
⋮----
if new_feature_activations.contains(&feature_set::raise_account_cu_limit::id()) {
⋮----
if new_feature_activations.contains(&feature_set::vote_state_v4::id()) {
if let Err(e) = self.upgrade_core_bpf_program(
⋮----
error!("Failed to upgrade Core BPF Stake program: {e}");
⋮----
if new_feature_activations.contains(&feature_set::increase_cpi_account_info_limit::id()) {
⋮----
if new_feature_activations.contains(&feature_set::replace_spl_token_with_p_token::id()) {
if let Err(e) = self.upgrade_loader_v2_program_with_loader_v3_program(
⋮----
fn apply_new_builtin_program_feature_transitions(
⋮----
for builtin in BUILTINS.iter() {
⋮----
if new_feature_activations.contains(&feature_id) {
⋮----
self.feature_set.activated_slot(&feature_id).unwrap_or(0),
builtin.name.len(),
⋮----
if new_feature_activations.contains(&core_bpf_migration_config.feature_id) {
⋮----
.migrate_builtin_to_core_bpf(&builtin.program_id, core_bpf_migration_config)
⋮----
for stateless_builtin in STATELESS_BUILTINS.iter() {
⋮----
if let Err(e) = self.migrate_builtin_to_core_bpf(
⋮----
for precompile in get_precompiles() {
⋮----
if new_feature_activations.contains(feature_id) {
self.add_precompile(&precompile.program_id);
⋮----
fn adjust_sysvar_balance_for_rent(&self, account: &mut AccountSharedData) {
account.set_lamports(
self.get_minimum_balance_for_rent_exemption(account.data().len())
.max(account.lamports()),
⋮----
fn compute_active_feature_set(&self, include_pending: bool) -> (FeatureSet, AHashSet<Pubkey>) {
let mut active = self.feature_set.active().clone();
⋮----
for feature_id in self.feature_set.inactive() {
⋮----
if let Some(account) = self.get_account_with_fixed_root(feature_id) {
⋮----
pending.insert(*feature_id);
activated = Some(slot);
⋮----
activated = Some(activation_slot);
⋮----
active.insert(*feature_id, slot);
⋮----
inactive.insert(*feature_id);
⋮----
pub fn compute_pending_activation_slot(&self, feature_id: &Pubkey) -> Option<Slot> {
let account = self.get_account_with_fixed_root(feature_id)?;
⋮----
if feature.activated_at.is_some() {
⋮----
Some(self.epoch_schedule.get_first_slot_in_epoch(active_epoch))
⋮----
fn add_active_builtin_programs(&mut self) {
⋮----
let builtin_is_bpf = builtin.core_bpf_migration_config.is_some() && {
self.get_account(&builtin.program_id)
.map(|a| a.owner() == &bpf_loader_upgradeable::id())
.unwrap_or(false)
⋮----
.map(|feature_id| self.feature_set.is_active(&feature_id))
.unwrap_or(true);
⋮----
.and_then(|feature_id| self.feature_set.activated_slot(&feature_id))
.unwrap_or(0);
self.transaction_processor.add_builtin(
⋮----
fn add_builtin_program_accounts(&mut self) {
⋮----
self.add_builtin_account(builtin.name, &builtin.program_id);
⋮----
.map(|feature_id| self.feature_set.is_active(feature_id))
⋮----
fn replace_program_account(
⋮----
if let Some(old_account) = self.get_account_with_fixed_root(old_address) {
if let Some(new_account) = self.get_account_with_fixed_root(new_address) {
datapoint_info!(datapoint_name, ("slot", self.slot, i64));
// Burn lamports in the old account
⋮----
.fetch_sub(old_account.lamports(), Relaxed);
// Transfer new account to old account
self.store_account(old_address, &new_account);
// Clear new account
self.store_account(new_address, &AccountSharedData::default());
// Unload a program from the bank's cache
⋮----
.remove_programs([*old_address].into_iter());
⋮----
old_account.data().len(),
⋮----
pub fn calculate_accounts_data_size(&self) -> ScanResult<u64> {
let accounts = self.get_all_accounts(false)?;
⋮----
.map(|(_pubkey, account, _slot)| account.data().len() as u64)
.try_fold(0, u64::checked_add)
.expect("accounts data size cannot overflow");
Ok(accounts_data_size)
⋮----
pub fn is_in_slot_hashes_history(&self, slot: &Slot) -> bool {
⋮----
if let Ok(slot_hashes) = self.transaction_processor.sysvar_cache().get_slot_hashes() {
return slot_hashes.get(slot).is_some();
⋮----
pub fn check_program_modification_slot(&self) -> bool {
⋮----
pub fn set_check_program_modification_slot(&mut self, check: bool) {
⋮----
pub fn fee_structure(&self) -> &FeeStructure {
⋮----
pub fn parent_block_id(&self) -> Option<Hash> {
self.parent().and_then(|p| p.block_id())
⋮----
pub fn block_id(&self) -> Option<Hash> {
*self.block_id.read().unwrap()
⋮----
pub fn set_block_id(&self, block_id: Option<Hash>) {
*self.block_id.write().unwrap() = block_id;
⋮----
pub fn compute_budget(&self) -> Option<ComputeBudget> {
⋮----
pub fn add_builtin(&self, program_id: Pubkey, name: &str, builtin: ProgramCacheEntry) {
debug!("Adding program {name} under {program_id:?}");
self.add_builtin_account(name, &program_id);
self.transaction_processor.add_builtin(program_id, builtin);
debug!("Added program {name} under {program_id:?}");
⋮----
fn add_builtin_account(&self, name: &str, program_id: &Pubkey) {
⋮----
self.get_account_with_fixed_root(program_id)
⋮----
if native_loader::check_id(account.owner()) {
Some(account)
⋮----
if existing_genuine_program.is_some() {
⋮----
self.inherit_specially_retained_account_fields(&existing_genuine_program);
⋮----
data: name.as_bytes().to_vec(),
⋮----
pub fn get_bank_hash_stats(&self) -> BankHashStats {
self.bank_hash_stats.load()
⋮----
pub fn clear_epoch_rewards_cache(&self) {
self.epoch_rewards_calculation_cache.lock().unwrap().clear();
⋮----
pub fn set_accounts_lt_hash_for_snapshot_minimizer(&self, accounts_lt_hash: AccountsLtHash) {
*self.accounts_lt_hash.lock().unwrap() = accounts_lt_hash;
⋮----
pub fn get_collector_fee_details(&self) -> CollectorFeeDetails {
self.collector_fee_details.read().unwrap().clone()
⋮----
pub fn priority_fee_total(&self) -> u64 {
self.collector_fee_details.read().unwrap().priority_fee
⋮----
pub fn new_for_benches(genesis_config: &GenesisConfig) -> Self {
⋮----
pub fn new_with_paths_for_benches(genesis_config: &GenesisConfig, paths: Vec<PathBuf>) -> Self {
⋮----
Some(Pubkey::new_unique()),
⋮----
impl InvokeContextCallback for Bank {
fn get_epoch_stake(&self) -> u64 {
self.get_current_epoch_total_stake()
⋮----
fn get_epoch_stake_for_vote_account(&self, vote_address: &Pubkey) -> u64 {
self.get_current_epoch_vote_accounts()
.get(vote_address)
.map(|(stake, _)| *stake)
⋮----
fn is_precompile(&self, program_id: &Pubkey) -> bool {
is_precompile(program_id, |feature_id: &Pubkey| {
self.feature_set.is_active(feature_id)
⋮----
fn process_precompile(
⋮----
if let Some(precompile) = get_precompile(program_id, |feature_id: &Pubkey| {
⋮----
precompile.verify(data, &instruction_datas, &self.feature_set)
⋮----
Err(PrecompileError::InvalidPublicKey)
⋮----
impl TransactionProcessingCallback for Bank {
fn get_account_shared_data(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, Slot)> {
⋮----
.load_with_fixed_root(&self.ancestors, pubkey)
⋮----
fn inspect_account(&self, address: &Pubkey, account_state: AccountState, is_writable: bool) {
self.inspect_account_for_accounts_lt_hash(address, &account_state, is_writable);
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
f.debug_struct("Bank")
.field("slot", &self.slot)
.field("bank_id", &self.bank_id)
.field("block_height", &self.block_height)
.field("parent_slot", &self.parent_slot)
.field("capitalization", &self.capitalization())
.finish_non_exhaustive()
⋮----
pub fn wrap_with_bank_forks_for_tests(self) -> (Arc<Self>, Arc<RwLock<BankForks>>) {
⋮----
let bank = bank_forks.read().unwrap().root_bank();
⋮----
pub fn new_with_bank_forks_for_tests(
⋮----
bank.wrap_with_bank_forks_for_tests()
⋮----
pub fn new_for_tests(genesis_config: &GenesisConfig) -> Self {
⋮----
pub fn new_with_mockup_builtin_for_tests(
⋮----
bank.add_mockup_builtin(program_id, builtin_function);
⋮----
pub fn new_no_wallclock_throttle_for_tests(
⋮----
pub fn new_with_config_for_tests(
⋮----
pub fn new_with_paths_for_tests(
⋮----
pub fn new_from_parent_with_bank_forks(
⋮----
.insert(bank)
.clone_without_scheduler()
⋮----
/// Prepare a transaction batch from a list of legacy transactions. Used for tests only.
    pub fn prepare_batch_for_tests(
⋮----
pub fn prepare_batch_for_tests(
⋮----
.map(RuntimeTransaction::from_transaction_for_tests)
⋮----
/// Set the initial accounts data size
    /// NOTE: This fn is *ONLY FOR TESTS*
⋮----
/// NOTE: This fn is *ONLY FOR TESTS*
    pub fn set_accounts_data_size_initial_for_tests(&mut self, amount: u64) {
⋮----
pub fn set_accounts_data_size_initial_for_tests(&mut self, amount: u64) {
⋮----
/// Update the accounts data size off-chain delta
    /// NOTE: This fn is *ONLY FOR TESTS*
⋮----
/// NOTE: This fn is *ONLY FOR TESTS*
    pub fn update_accounts_data_size_delta_off_chain_for_tests(&self, amount: i64) {
⋮----
pub fn update_accounts_data_size_delta_off_chain_for_tests(&self, amount: i64) {
self.update_accounts_data_size_delta_off_chain(amount)
⋮----
/// Process multiple transaction in a single batch. This is used for benches and unit tests.
    ///
⋮----
///
    /// # Panics
⋮----
/// # Panics
    ///
⋮----
///
    /// Panics if any of the transactions do not pass sanitization checks.
⋮----
/// Panics if any of the transactions do not pass sanitization checks.
    #[must_use]
pub fn process_transactions<'a>(
⋮----
self.try_process_transactions(txs).unwrap()
⋮----
/// Process entry transactions in a single batch. This is used for benches and unit tests.
    ///
⋮----
pub fn process_entry_transactions(&self, txs: Vec<VersionedTransaction>) -> Vec<Result<()>> {
self.try_process_entry_transactions(txs).unwrap()
⋮----
pub fn flush_accounts_cache_slot_for_tests(&self) {
⋮----
.flush_accounts_cache_slot_for_tests(self.slot())
⋮----
pub fn get_sysvar_cache_for_tests(&self) -> SysvarCache {
self.transaction_processor.get_sysvar_cache_for_tests()
⋮----
pub fn calculate_accounts_lt_hash_for_tests(&self) -> AccountsLtHash {
⋮----
.calculate_accounts_lt_hash_at_startup_from_index(&self.ancestors, self.slot)
⋮----
pub fn get_transaction_processor(&self) -> &TransactionBatchProcessor<BankForks> {
⋮----
pub fn set_fee_structure(&mut self, fee_structure: &FeeStructure) {
self.fee_structure = fee_structure.clone();
⋮----
pub fn load_program(
⋮----
.get_environments_for_epoch(effective_epoch);
load_program_with_pubkey(
⋮----
&mut ExecuteTimings::default(), // Called by ledger-tool, metrics not accumulated.
⋮----
pub fn withdraw(&self, pubkey: &Pubkey, lamports: u64) -> Result<()> {
match self.get_account_with_fixed_root(pubkey) {
⋮----
let min_balance = match get_system_account_kind(&account) {
⋮----
.minimum_balance(nonce::state::State::size()),
⋮----
.checked_add(min_balance)
.filter(|required_balance| *required_balance <= account.lamports())
.ok_or(TransactionError::InsufficientFundsForFee)?;
⋮----
.checked_sub_lamports(lamports)
.map_err(|_| TransactionError::InsufficientFundsForFee)?;
self.store_account(pubkey, &account);
⋮----
None => Err(TransactionError::AccountNotFound),
⋮----
pub fn set_hash_overrides(&self, hash_overrides: HashOverrides) {
*self.hash_overrides.lock().unwrap() = hash_overrides;
⋮----
/// Get stake and stake node accounts
    pub(crate) fn get_stake_accounts(&self, minimized_account_set: &DashSet<Pubkey>) {
⋮----
pub(crate) fn get_stake_accounts(&self, minimized_account_set: &DashSet<Pubkey>) {
⋮----
.stake_delegations()
⋮----
.for_each(|(pubkey, _)| {
minimized_account_set.insert(*pubkey);
⋮----
.staked_nodes()
.par_iter()
⋮----
/// Compute how much an account has changed size.  This function is useful when the data size delta
/// needs to be computed and passed to an `update_accounts_data_size_delta` function.
⋮----
/// needs to be computed and passed to an `update_accounts_data_size_delta` function.
fn calculate_data_size_delta(old_data_size: usize, new_data_size: usize) -> i64 {
⋮----
fn calculate_data_size_delta(old_data_size: usize, new_data_size: usize) -> i64 {
assert!(old_data_size <= i64::MAX as usize);
assert!(new_data_size <= i64::MAX as usize);
⋮----
new_data_size.saturating_sub(old_data_size)
⋮----
impl Drop for Bank {
fn drop(&mut self) {
if let Some(drop_callback) = self.drop_callback.read().unwrap().0.as_ref() {
drop_callback.callback(self);
⋮----
// Default case for tests
⋮----
.purge_slot(self.slot(), self.bank_id(), false);
⋮----
/// utility function used for testing and benchmarking.
pub mod test_utils {
⋮----
pub mod test_utils {
⋮----
pub fn goto_end_of_slot(bank: Arc<Bank>) {
goto_end_of_slot_with_scheduler(&BankWithScheduler::new_without_scheduler(bank))
⋮----
pub fn goto_end_of_slot_with_scheduler(bank: &BankWithScheduler) {
let mut tick_hash = bank.last_blockhash();
⋮----
tick_hash = hashv(&[tick_hash.as_ref(), &[42]]);
bank.register_tick(&tick_hash);
if tick_hash == bank.last_blockhash() {
bank.freeze();
⋮----
pub fn update_vote_account_timestamp(
⋮----
let mut vote_account = bank.get_account(vote_pubkey).unwrap_or_default();
let mut vote_state = VoteStateV4::deserialize(vote_account.data(), vote_pubkey)
.ok()
⋮----
vote_account.set_state(&versioned).unwrap();
bank.store_account(vote_pubkey, &vote_account);
⋮----
pub fn deposit(
⋮----
// This doesn't collect rents intentionally.
⋮----
.get_account_with_fixed_root_no_cache(pubkey)
⋮----
account.checked_add_lamports(lamports)?;
bank.store_account(pubkey, &account);
Ok(account.lamports())

================
File: runtime/src/commitment.rs
================
pub type BlockCommitmentArray = [u64; MAX_LOCKOUT_HISTORY + 1];
⋮----
pub struct BlockCommitment {
⋮----
impl BlockCommitment {
pub fn increase_confirmation_stake(&mut self, confirmation_count: usize, stake: u64) {
assert!(confirmation_count > 0 && confirmation_count <= MAX_LOCKOUT_HISTORY);
⋮----
pub fn get_confirmation_stake(&mut self, confirmation_count: usize) -> u64 {
⋮----
pub fn increase_rooted_stake(&mut self, stake: u64) {
⋮----
pub fn get_rooted_stake(&self) -> u64 {
⋮----
pub fn new(commitment: BlockCommitmentArray) -> Self {
⋮----
pub struct BlockCommitmentCache {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
f.debug_struct("BlockCommitmentCache")
.field("block_commitment", &self.block_commitment)
.field("total_stake", &self.total_stake)
.field(
⋮----
&format_args!("Bank({{current_slot: {:?}}})", self.commitment_slots.slot),
⋮----
.field("root", &self.commitment_slots.root)
.finish()
⋮----
impl BlockCommitmentCache {
pub fn new(
⋮----
pub fn get_block_commitment(&self, slot: Slot) -> Option<&BlockCommitment> {
self.block_commitment.get(&slot)
⋮----
pub fn total_stake(&self) -> u64 {
⋮----
pub fn slot(&self) -> Slot {
⋮----
pub fn root(&self) -> Slot {
⋮----
pub fn highest_confirmed_slot(&self) -> Slot {
⋮----
pub fn highest_super_majority_root(&self) -> Slot {
⋮----
pub fn commitment_slots(&self) -> CommitmentSlots {
⋮----
pub fn highest_gossip_confirmed_slot(&self) -> Slot {
self.highest_confirmed_slot()
⋮----
pub fn slot_with_commitment(&self, commitment_level: CommitmentLevel) -> Slot {
⋮----
CommitmentLevel::Processed => self.slot(),
CommitmentLevel::Confirmed => self.highest_gossip_confirmed_slot(),
CommitmentLevel::Finalized => self.highest_super_majority_root(),
⋮----
fn highest_slot_with_confirmation_count(&self, confirmation_count: usize) -> Slot {
⋮----
for slot in (self.root()..self.slot()).rev() {
if let Some(count) = self.get_confirmation_count(slot) {
⋮----
pub fn calculate_highest_confirmed_slot(&self) -> Slot {
self.highest_slot_with_confirmation_count(1)
⋮----
pub fn get_confirmation_count(&self, slot: Slot) -> Option<usize> {
self.get_lockout_count(slot, VOTE_THRESHOLD_SIZE)
⋮----
fn get_lockout_count(&self, slot: Slot, minimum_stake_percentage: f64) -> Option<usize> {
self.get_block_commitment(slot).map(|block_commitment| {
let iterator = block_commitment.commitment.iter().enumerate().rev();
⋮----
pub fn new_for_tests() -> Self {
⋮----
block_commitment.insert(0, BlockCommitment::default());
⋮----
pub fn new_for_tests_with_slots(slot: Slot, root: Slot) -> Self {
⋮----
pub fn set_highest_confirmed_slot(&mut self, slot: Slot) {
⋮----
pub fn set_highest_super_majority_root(&mut self, root: Slot) {
⋮----
pub fn initialize_slots(&mut self, slot: Slot, root: Slot) {
⋮----
pub fn set_all_slots(&mut self, slot: Slot, root: Slot) {
⋮----
pub struct CommitmentSlots {
⋮----
impl CommitmentSlots {
pub fn new_from_slot(slot: Slot) -> Self {
⋮----
mod tests {
⋮----
fn test_block_commitment() {
⋮----
assert_eq!(cache.get_confirmation_stake(1), 0);
cache.increase_confirmation_stake(1, 10);
assert_eq!(cache.get_confirmation_stake(1), 10);
cache.increase_confirmation_stake(1, 20);
assert_eq!(cache.get_confirmation_stake(1), 30);
⋮----
fn test_get_confirmations() {
⋮----
cache0.increase_confirmation_stake(1, 5);
cache0.increase_confirmation_stake(2, 40);
⋮----
cache1.increase_confirmation_stake(1, 40);
cache1.increase_confirmation_stake(2, 5);
⋮----
cache2.increase_confirmation_stake(1, 20);
cache2.increase_confirmation_stake(2, 5);
⋮----
block_commitment.entry(0).or_insert(cache0);
block_commitment.entry(1).or_insert(cache1);
block_commitment.entry(2).or_insert(cache2);
⋮----
assert_eq!(block_commitment_cache.get_confirmation_count(0), Some(2));
assert_eq!(block_commitment_cache.get_confirmation_count(1), Some(1));
assert_eq!(block_commitment_cache.get_confirmation_count(2), Some(0),);
assert_eq!(block_commitment_cache.get_confirmation_count(3), None,);
⋮----
fn test_highest_confirmed_slot() {
⋮----
block_commitment.entry(1).or_insert_with(|| cache0.clone());
block_commitment.entry(2).or_insert_with(|| cache1.clone());
block_commitment.entry(3).or_insert_with(|| cache2.clone());
⋮----
assert_eq!(block_commitment_cache.calculate_highest_confirmed_slot(), 2);
⋮----
block_commitment.entry(1).or_insert_with(|| cache1.clone());
⋮----
block_commitment.entry(3).or_insert(cache1);
block_commitment.entry(5).or_insert_with(|| cache2.clone());
⋮----
assert_eq!(block_commitment_cache.calculate_highest_confirmed_slot(), 3);
⋮----
block_commitment.entry(1).or_insert(cache0);
block_commitment.entry(2).or_insert_with(|| cache2.clone());
⋮----
assert_eq!(block_commitment_cache.calculate_highest_confirmed_slot(), 1);
⋮----
block_commitment.entry(1).or_insert_with(|| cache2.clone());
⋮----
block_commitment.entry(3).or_insert(cache2);
⋮----
assert_eq!(block_commitment_cache.calculate_highest_confirmed_slot(), 0);

================
File: runtime/src/dependency_tracker.rs
================
pub struct DependencyTracker {
⋮----
fn less_than(a: &Option<u64>, b: u64) -> bool {
a.is_none_or(|a| a < b)
⋮----
impl DependencyTracker {
pub fn declare_work(&self) -> u64 {
⋮----
.fetch_add(1, std::sync::atomic::Ordering::SeqCst)
⋮----
pub fn mark_this_and_all_previous_work_processed(&self, work_id: u64) {
let mut processed_work_id = self.processed_work_id.lock().unwrap();
if less_than(&processed_work_id, work_id) {
*processed_work_id = Some(work_id);
self.condvar.notify_all();
⋮----
pub fn wait_for_dependency(&self, work_id: u64) {
⋮----
while less_than(&processed_work_id, work_id) {
processed_work_id = self.condvar.wait(processed_work_id).unwrap();
⋮----
pub fn get_current_declared_work(&self) -> u64 {
self.work_id.load(std::sync::atomic::Ordering::SeqCst)
⋮----
mod tests {
⋮----
fn test_less_than() {
assert!(less_than(&None, 0));
assert!(less_than(&Some(0), 1));
assert!(!less_than(&Some(1), 1));
assert!(!less_than(&Some(2), 1));
⋮----
fn test_get_new_work_id() {
⋮----
assert_eq!(dependency_tracker.declare_work(), 1);
assert_eq!(dependency_tracker.declare_work(), 2);
assert_eq!(dependency_tracker.get_current_declared_work(), 2);
⋮----
fn test_notify_work_processed() {
⋮----
dependency_tracker.mark_this_and_all_previous_work_processed(1);
let processed_work_id = *dependency_tracker.processed_work_id.lock().unwrap();
assert_eq!(processed_work_id, Some(1));
dependency_tracker.mark_this_and_all_previous_work_processed(0);
⋮----
dependency_tracker.mark_this_and_all_previous_work_processed(2);
⋮----
assert_eq!(processed_work_id, Some(2));
⋮----
fn test_wait_and_notify_work_processed() {
⋮----
let work = dependency_tracker.declare_work();
assert_eq!(work, 1);
⋮----
assert_eq!(work, 2);
let work_to_wait = dependency_tracker.get_current_declared_work();
⋮----
tracker_clone.wait_for_dependency(work_to_wait);
⋮----
dependency_tracker.mark_this_and_all_previous_work_processed(work);
handle.join().unwrap();

================
File: runtime/src/epoch_stakes.rs
================
pub type NodeIdToVoteAccounts = HashMap<Pubkey, NodeVoteAccounts>;
pub type EpochAuthorizedVoters = HashMap<Pubkey, Pubkey>;
⋮----
pub struct BLSPubkeyToRankMap {
⋮----
impl BLSPubkeyToRankMap {
pub fn new(epoch_vote_accounts_hash_map: &VoteAccountsHashMap) -> Self {
⋮----
.iter()
.filter_map(|(pubkey, (stake, account))| {
⋮----
.vote_state_view()
.bls_pubkey_compressed()
.and_then(|bls_pubkey_compressed_bytes| {
⋮----
BLSPubkeyCompressed(bls_pubkey_compressed_bytes);
BLSPubkey::try_from(bls_pubkey_compressed).ok()
⋮----
.map(|bls_pubkey| (*pubkey, bls_pubkey, *stake))
⋮----
.collect();
pubkey_stake_pair_vec.sort_by(|(_, a_pubkey, a_stake), (_, b_pubkey, b_stake)| {
b_stake.cmp(a_stake).then(a_pubkey.cmp(b_pubkey))
⋮----
for (rank, (pubkey, bls_pubkey, _stake)) in pubkey_stake_pair_vec.into_iter().enumerate() {
sorted_pubkeys.push((pubkey, bls_pubkey));
bls_pubkey_to_rank_map.insert(bls_pubkey, rank as u16);
⋮----
pub fn is_empty(&self) -> bool {
self.rank_map.is_empty()
⋮----
pub fn len(&self) -> usize {
self.rank_map.len()
⋮----
pub fn get_rank(&self, bls_pubkey: &BLSPubkey) -> Option<&u16> {
self.rank_map.get(bls_pubkey)
⋮----
pub fn get_pubkey(&self, index: usize) -> Option<&(Pubkey, BLSPubkey)> {
self.sorted_pubkeys.get(index)
⋮----
pub struct NodeVoteAccounts {
⋮----
pub enum VersionedEpochStakes {
⋮----
impl VersionedEpochStakes {
pub(crate) fn new(stakes: SerdeStakesToStakeFormat, leader_schedule_epoch: Epoch) -> Self {
let epoch_vote_accounts = stakes.vote_accounts();
⋮----
Self::parse_epoch_vote_accounts(epoch_vote_accounts.as_ref(), leader_schedule_epoch);
⋮----
pub fn new_for_tests(
⋮----
pub fn stakes(&self) -> &SerdeStakesToStakeFormat {
⋮----
pub fn total_stake(&self) -> u64 {
⋮----
pub fn set_total_stake(&mut self, total_stake: u64) {
⋮----
pub fn node_id_to_vote_accounts(&self) -> &Arc<NodeIdToVoteAccounts> {
⋮----
pub fn node_id_to_stake(&self, node_id: &Pubkey) -> Option<u64> {
self.node_id_to_vote_accounts()
.get(node_id)
.map(|x| x.total_stake)
⋮----
pub fn epoch_authorized_voters(&self) -> &Arc<EpochAuthorizedVoters> {
⋮----
pub fn bls_pubkey_to_rank_map(&self) -> &Arc<BLSPubkeyToRankMap> {
⋮----
} => bls_pubkey_to_rank_map.get_or_init(|| {
⋮----
self.stakes().vote_accounts().as_ref(),
⋮----
pub fn vote_account_stake(&self, vote_account: &Pubkey) -> u64 {
self.stakes()
.vote_accounts()
.get_delegated_stake(vote_account)
⋮----
fn parse_epoch_vote_accounts(
⋮----
.map(|(_, (stake, _))| stake)
.sum();
⋮----
.filter_map(|(key, (stake, account))| {
let vote_state = account.vote_state_view();
⋮----
vote_state.get_authorized_voter(leader_schedule_epoch)
⋮----
.entry(*vote_state.node_pubkey())
.or_default();
⋮----
node_vote_accounts.vote_accounts.push(*key);
Some((*key, *authorized_voter))
⋮----
pub(crate) mod tests {
⋮----
struct VoteAccountInfo {
⋮----
fn new_vote_accounts(
⋮----
.map(|_| {
⋮----
BLSKeypair::new().public.try_into().unwrap();
⋮----
.unwrap()
.try_into()
.unwrap();
⋮----
create_v4_account_with_authorized(
⋮----
Some(bls_pubkey_compressed_serialized),
⋮----
.take(num_vote_accounts_per_node)
.collect(),
⋮----
.collect()
⋮----
fn new_epoch_vote_accounts(
⋮----
.flat_map(|(node_id, vote_accounts)| {
vote_accounts.iter().map(|v| {
let vote_account = VoteAccount::try_from(v.account.clone()).unwrap();
(v.vote_account, (node_id_to_stake_fn(node_id), vote_account))
⋮----
fn test_parse_epoch_vote_accounts(is_alpenglow: bool) {
⋮----
new_vote_accounts(num_nodes, num_vote_accounts_per_node, is_alpenglow);
⋮----
.flat_map(|(_, vote_accounts)| {
⋮----
.map(|v| (v.vote_account, v.authorized_voter))
⋮----
.map(|(node_pubkey, vote_accounts)| {
⋮----
.map(|v| v.vote_account)
⋮----
vote_accounts.sort();
⋮----
new_epoch_vote_accounts(&vote_accounts_map, |_| stake_per_account);
⋮----
.iter_mut()
.for_each(|(_, node_vote_accounts)| node_vote_accounts.vote_accounts.sort());
assert!(
⋮----
assert_eq!(
⋮----
fn test_node_id_to_stake(is_alpenglow: bool) {
⋮----
.keys()
.enumerate()
.map(|(index, node_id)| (*node_id, ((index + 1) * 100) as u64))
⋮----
let epoch_vote_accounts = new_epoch_vote_accounts(&vote_accounts_map, |node_id| {
*node_id_to_stake_map.get(node_id).unwrap()
⋮----
assert_eq!(epoch_stakes.total_stake(), 11000);
for (node_id, stake) in node_id_to_stake_map.iter() {
⋮----
fn test_bls_pubkey_rank_map(num_vote_accounts_per_node: usize) {
⋮----
let vote_accounts_map = new_vote_accounts(num_nodes, num_vote_accounts_per_node, true);
⋮----
let epoch_stakes = VersionedEpochStakes::new_for_tests(epoch_vote_accounts.clone(), 0);
let bls_pubkey_to_rank_map = epoch_stakes.bls_pubkey_to_rank_map();
assert_eq!(bls_pubkey_to_rank_map.len(), num_vote_accounts);
⋮----
let vote_state_view = vote_account.vote_state_view();
⋮----
&vote_state_view.bls_pubkey_compressed().unwrap(),
⋮----
let bls_pubkey = BLSPubkey::try_from(bls_pubkey_compressed).unwrap();
let index = bls_pubkey_to_rank_map.get_rank(&bls_pubkey).unwrap();
assert!(index >= &0 && index < &(num_vote_accounts as u16));
⋮----
bank_epoch_stakes.insert(0, epoch_stakes.clone());
⋮----
.get(&0)
.expect("Epoch stakes should exist");
let bls_pubkey_to_rank_map2 = epoch_stakes.bls_pubkey_to_rank_map();
assert_eq!(bls_pubkey_to_rank_map2, bls_pubkey_to_rank_map);

================
File: runtime/src/genesis_utils.rs
================
pub fn bootstrap_validator_stake_lamports() -> u64 {
Rent::default().minimum_balance(StakeStateV2::size_of())
⋮----
pub const fn genesis_sysvar_and_builtin_program_lamports() -> u64 {
⋮----
pub struct ValidatorVoteKeypairs {
⋮----
impl ValidatorVoteKeypairs {
pub fn new(node_keypair: Keypair, vote_keypair: Keypair, stake_keypair: Keypair) -> Self {
⋮----
pub fn new_rand() -> Self {
⋮----
pub struct GenesisConfigInfo {
⋮----
pub fn create_genesis_config(mint_lamports: u64) -> GenesisConfigInfo {
create_genesis_config_with_leader(
⋮----
pub fn create_genesis_config_with_vote_accounts(
⋮----
create_genesis_config_with_vote_accounts_and_cluster_type(
⋮----
pub fn create_genesis_config_with_alpenglow_vote_accounts(
⋮----
pub fn create_genesis_config_with_vote_accounts_and_cluster_type(
⋮----
assert!(!voting_keypairs.is_empty());
assert_eq!(voting_keypairs.len(), stakes.len());
⋮----
let voting_keypair = voting_keypairs[0].borrow().vote_keypair.insecure_clone();
let validator_pubkey = voting_keypairs[0].borrow().node_keypair.pubkey();
⋮----
&voting_keypairs[0].borrow().vote_keypair,
⋮----
.unwrap();
Some(bls_pubkey_to_compressed_bytes(&bls_keypair.public))
⋮----
let genesis_config = create_genesis_config_with_leader_ex(
⋮----
&mint_keypair.pubkey(),
⋮----
&voting_keypairs[0].borrow().vote_keypair.pubkey(),
&voting_keypairs[0].borrow().stake_keypair.pubkey(),
⋮----
vec![],
⋮----
for (validator_voting_keypairs, stake) in voting_keypairs[1..].iter().zip(&stakes[1..]) {
let node_pubkey = validator_voting_keypairs.borrow().node_keypair.pubkey();
let vote_pubkey = validator_voting_keypairs.borrow().vote_keypair.pubkey();
let stake_pubkey = validator_voting_keypairs.borrow().stake_keypair.pubkey();
⋮----
&validator_voting_keypairs.borrow().vote_keypair,
⋮----
genesis_config_info.genesis_config.accounts.extend(vec![
⋮----
pub fn create_genesis_config_with_leader(
⋮----
create_genesis_config_with_leader_with_mint_keypair(
⋮----
pub fn create_genesis_config_with_leader_with_mint_keypair(
⋮----
&voting_keypair.pubkey(),
⋮----
pub fn activate_all_features_alpenglow(genesis_config: &mut GenesisConfig) {
⋮----
pub fn activate_all_features(genesis_config: &mut GenesisConfig) {
⋮----
fn do_activate_all_features<const IS_ALPENGLOW: bool>(genesis_config: &mut GenesisConfig) {
for feature_id in FeatureSet::default().inactive() {
⋮----
activate_feature(genesis_config, *feature_id);
⋮----
pub fn deactivate_features(
⋮----
if FEATURE_NAMES.contains_key(deactivate_feature_pk) {
genesis_config.accounts.remove(deactivate_feature_pk);
⋮----
warn!(
⋮----
pub fn activate_feature(genesis_config: &mut GenesisConfig, feature_id: Pubkey) {
genesis_config.accounts.insert(
⋮----
activated_at: Some(0),
⋮----
std::cmp::max(genesis_config.rent.minimum_balance(Feature::size_of()), 1),
⋮----
pub fn bls_pubkey_to_compressed_bytes(
⋮----
let key = BLSPubkeyCompressed::try_from(bls_pubkey).unwrap();
bincode::serialize(&key).unwrap().try_into().unwrap()
⋮----
pub fn create_genesis_config_with_leader_ex_no_features(
⋮----
initial_accounts.push((
⋮----
initial_accounts.push((*validator_vote_account_pubkey, validator_vote_account));
initial_accounts.push((*validator_stake_account_pubkey, validator_stake_account));
⋮----
data: spl_generic_token::token::native_mint::ACCOUNT_DATA.to_vec(),
⋮----
.iter()
.cloned()
.map(|(key, account)| (key, Account::from(account)))
.collect(),
⋮----
add_genesis_stake_config_account(&mut genesis_config);
add_genesis_epoch_rewards_account(&mut genesis_config);
⋮----
pub fn create_genesis_config_with_leader_ex(
⋮----
let mut genesis_config = create_genesis_config_with_leader_ex_no_features(
⋮----
activate_all_features(&mut genesis_config);
⋮----
pub fn add_genesis_stake_config_account(genesis_config: &mut GenesisConfig) -> u64 {
let mut data = serialize(&ConfigKeys { keys: vec![] }).unwrap();
data.extend_from_slice(&serialize(&StakeConfig::default()).unwrap());
let lamports = std::cmp::max(genesis_config.rent.minimum_balance(data.len()), 1);
⋮----
genesis_config.add_account(solana_stake_interface::config::id(), account);
⋮----
pub fn add_genesis_epoch_rewards_account(genesis_config: &mut GenesisConfig) -> u64 {
let data = vec![0; EpochRewards::size_of()];
⋮----
genesis_config.add_account(epoch_rewards::id(), account);
⋮----
pub fn create_lockup_stake_account(
⋮----
let rent_exempt_reserve = rent.minimum_balance(stake_account.data().len());
assert!(
⋮----
.set_state(&StakeStateV2::Initialized(Meta {
⋮----
.expect("set_state");

================
File: runtime/src/installed_scheduler_pool.rs
================
pub fn initialized_result_with_timings() -> ResultWithTimings {
(Ok(()), ExecuteTimings::default())
⋮----
pub trait InstalledSchedulerPool: Send + Sync + Debug {
fn take_scheduler(&self, context: SchedulingContext) -> InstalledSchedulerBox {
self.take_resumed_scheduler(context, initialized_result_with_timings())
⋮----
pub struct SchedulerAborted;
pub type ScheduleResult = std::result::Result<(), SchedulerAborted>;
pub struct TimeoutListener {
⋮----
impl TimeoutListener {
pub(crate) fn new(f: impl FnOnce(InstalledSchedulerPoolArc) + Sync + Send + 'static) -> Self {
⋮----
pub fn trigger(self, pool: InstalledSchedulerPoolArc) {
⋮----
impl Debug for TimeoutListener {
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
write!(f, "TimeoutListener({self:p})")
⋮----
pub trait InstalledScheduler: Send + Sync + Debug + 'static {
⋮----
/// Schedule transaction for execution.
    ///
⋮----
///
    /// This non-blocking function will return immediately without waiting for actual execution.
⋮----
/// This non-blocking function will return immediately without waiting for actual execution.
    ///
⋮----
///
    /// Calling this is illegal as soon as `wait_for_termination()` is called. It would result in
⋮----
/// Calling this is illegal as soon as `wait_for_termination()` is called. It would result in
    /// fatal logic error.
⋮----
/// fatal logic error.
    ///
⋮----
///
    /// Note that the returned result indicates whether the scheduler has been aborted due to a
⋮----
/// Note that the returned result indicates whether the scheduler has been aborted due to a
    /// previously-scheduled bad transaction, which terminates further block verification. So,
⋮----
/// previously-scheduled bad transaction, which terminates further block verification. So,
    /// almost always, the returned error isn't due to the merely scheduling of the current
⋮----
/// almost always, the returned error isn't due to the merely scheduling of the current
    fn schedule_execution(
⋮----
pub trait UninstalledScheduler: Send + Sync + Debug + 'static {
⋮----
pub type InstalledSchedulerBox = Box<dyn InstalledScheduler>;
pub type UninstalledSchedulerBox = Box<dyn UninstalledScheduler>;
pub type InstalledSchedulerPoolArc = Arc<dyn InstalledSchedulerPool>;
pub type SchedulerId = u64;
/// A small context to propagate a bank and its scheduling mode to the scheduler subsystem.
///
⋮----
///
/// Note that this isn't called `SchedulerContext` because the contexts aren't associated with
⋮----
/// Note that this isn't called `SchedulerContext` because the contexts aren't associated with
#[derive(Clone, Debug)]
pub struct SchedulingContext {
⋮----
impl SchedulingContext {
pub fn for_preallocation() -> Self {
⋮----
pub(crate) fn new_with_mode(mode: SchedulingMode, bank: Arc<Bank>) -> Self {
⋮----
bank: Some(bank),
⋮----
fn for_verification(bank: Arc<Bank>) -> Self {
⋮----
pub fn for_production(bank: Arc<Bank>) -> Self {
⋮----
pub fn is_preallocated(&self) -> bool {
self.bank.is_none()
⋮----
pub fn mode(&self) -> SchedulingMode {
⋮----
pub fn bank(&self) -> Option<&Arc<Bank>> {
self.bank.as_ref()
⋮----
pub fn slot(&self) -> Option<Slot> {
self.bank.as_ref().map(|bank| bank.slot())
⋮----
pub type ResultWithTimings = (Result<()>, ExecuteTimings);
⋮----
enum WaitReason {
⋮----
impl WaitReason {
pub fn is_paused(&self) -> bool {
⋮----
pub fn is_dropped(&self) -> bool {
⋮----
pub enum SchedulerStatus {
⋮----
impl SchedulerStatus {
fn new(scheduler: Option<InstalledSchedulerBox>) -> Self {
⋮----
fn transition_from_stale_to_active(
⋮----
panic!("transition to Active failed: {self:?}");
⋮----
*self = Self::Active(f(pool, result_with_timings));
⋮----
fn maybe_transition_from_active_to_stale(
⋮----
if !matches!(self, Self::Active(_scheduler)) {
⋮----
unreachable!("not active: {self:?}");
⋮----
let (pool, result_with_timings) = f(scheduler);
⋮----
fn transition_from_active_to_unavailable(&mut self) -> InstalledSchedulerBox {
⋮----
panic!("transition to Unavailable failed: {self:?}");
⋮----
fn transition_from_stale_to_unavailable(&mut self) -> ResultWithTimings {
⋮----
fn active_scheduler(&self) -> &InstalledSchedulerBox {
⋮----
panic!("not active: {self:?}");
⋮----
pub struct BankWithScheduler {
⋮----
pub struct BankWithSchedulerInner {
⋮----
pub type InstalledSchedulerRwLock = RwLock<SchedulerStatus>;
impl BankWithScheduler {
⋮----
pub(crate) fn new(bank: Arc<Bank>, scheduler: Option<InstalledSchedulerBox>) -> Self {
⋮----
.as_ref()
.map(|scheduler| scheduler.context().bank().unwrap())
⋮----
assert!(Arc::ptr_eq(&bank, bank_in_context));
⋮----
pub fn new_without_scheduler(bank: Arc<Bank>) -> Self {
⋮----
pub fn clone_with_scheduler(&self) -> BankWithScheduler {
⋮----
inner: self.inner.clone(),
⋮----
pub fn clone_without_scheduler(&self) -> Arc<Bank> {
self.inner.bank.clone()
⋮----
pub fn register_tick(&self, hash: &Hash) {
self.inner.bank.register_tick(hash, &self.inner.scheduler);
⋮----
pub fn fill_bank_with_ticks_for_tests(&self) {
self.do_fill_bank_with_ticks_for_tests(&self.inner.scheduler);
⋮----
pub fn has_installed_scheduler(&self) -> bool {
!matches!(
⋮----
pub fn schedule_transaction_executions(
⋮----
trace!(
⋮----
let schedule_result: ScheduleResult = self.inner.with_active_scheduler(|scheduler| {
⋮----
scheduler.schedule_execution(sanitized_transaction, task_id)?;
⋮----
Ok(())
⋮----
if schedule_result.is_err() {
return Err(self.inner.retrieve_error_after_schedule_failure());
⋮----
pub(crate) fn create_timeout_listener(&self) -> TimeoutListener {
self.inner.do_create_timeout_listener()
⋮----
pub fn drop_scheduler(&mut self) {
self.inner.drop_scheduler();
⋮----
pub fn unpause_new_block_production_scheduler(&self) {
if let SchedulerStatus::Active(scheduler) = &*self.inner.scheduler.read().unwrap() {
assert_matches!(scheduler.context().mode(), SchedulingMode::BlockProduction);
scheduler.unpause_after_taken();
⋮----
pub(crate) fn wait_for_paused_scheduler(bank: &Bank, scheduler: &InstalledSchedulerRwLock) {
⋮----
assert!(
⋮----
pub fn wait_for_completed_scheduler(&self) -> Option<ResultWithTimings> {
⋮----
pub const fn no_scheduler_available() -> InstalledSchedulerRwLock {
⋮----
impl BankWithSchedulerInner {
fn with_active_scheduler(
⋮----
let scheduler = self.scheduler.read().unwrap();
⋮----
f(scheduler)
⋮----
SchedulerStatus::Stale(_pool, (result, _timings)) if result.is_err() => {
⋮----
Err(SchedulerAborted)
⋮----
let pool = pool.clone();
drop(scheduler);
let context = SchedulingContext::for_verification(self.bank.clone());
let mut scheduler = self.scheduler.write().unwrap();
trace!("with_active_scheduler: {scheduler:?}");
scheduler.transition_from_stale_to_active(|pool, result_with_timings| {
let scheduler = pool.take_resumed_scheduler(context, result_with_timings);
info!(
⋮----
pool.register_timeout_listener(self.do_create_timeout_listener());
f(scheduler.active_scheduler())
⋮----
SchedulerStatus::Unavailable => unreachable!("no installed scheduler"),
⋮----
fn do_create_timeout_listener(self: &Arc<Self>) -> TimeoutListener {
⋮----
let Some(bank) = weak_bank.upgrade() else {
⋮----
let Ok(mut scheduler) = bank.scheduler.write() else {
⋮----
scheduler.maybe_transition_from_active_to_stale(|scheduler| {
let id = scheduler.id();
⋮----
scheduler.wait_for_termination(false);
uninstalled_scheduler.return_to_pool();
⋮----
trace!("timeout_listener: {scheduler:?}");
⋮----
fn retrieve_error_after_schedule_failure(&self) -> TransactionError {
⋮----
SchedulerStatus::Active(scheduler) => scheduler.recover_error_after_abort(),
⋮----
result.clone().unwrap_err()
⋮----
_ => unreachable!("no error in {:?}", self.scheduler),
⋮----
fn wait_for_completed_scheduler_from_drop(&self) -> Option<ResultWithTimings> {
⋮----
fn wait_for_scheduler_termination(
⋮----
debug!(
⋮----
let mut scheduler = scheduler.write().unwrap();
⋮----
SchedulerStatus::Active(scheduler) if reason.is_paused() => {
scheduler.pause_for_recent_blockhash();
⋮----
let scheduler = scheduler.transition_from_active_to_unavailable();
⋮----
scheduler.wait_for_termination(reason.is_dropped());
⋮----
(false, Some(result_with_timings))
⋮----
SchedulerStatus::Stale(_pool, _result_with_timings) if reason.is_paused() => {
⋮----
let result_with_timings = scheduler.transition_from_stale_to_unavailable();
(true, Some(result_with_timings))
⋮----
trace!("wait_for_scheduler_termination(result_with_timings: {result_with_timings:?})",);
⋮----
fn drop_scheduler(&self) {
⋮----
error!(
⋮----
.wait_for_completed_scheduler_from_drop()
.map(|(result, _timings)| result)
⋮----
warn!(
⋮----
impl Drop for BankWithSchedulerInner {
fn drop(&mut self) {
self.drop_scheduler();
⋮----
impl Deref for BankWithScheduler {
type Target = Arc<Bank>;
fn deref(&self) -> &Self::Target {
⋮----
mod tests {
⋮----
fn setup_mocked_scheduler_with_extra(
⋮----
mock.expect_context()
.times(1)
.in_sequence(&mut seq.lock().unwrap())
.return_const(SchedulingContext::for_verification(bank));
⋮----
let seq_cloned = seq.clone();
mock.expect_wait_for_termination()
.with(mockall::predicate::eq(wait_reason))
⋮----
.returning(move |_| {
⋮----
.expect_return_to_pool()
⋮----
.in_sequence(&mut seq_cloned.lock().unwrap())
.returning(|| ());
⋮----
(Ok(()), ExecuteTimings::default()),
⋮----
f(&mut mock);
⋮----
fn setup_mocked_scheduler(
⋮----
setup_mocked_scheduler_with_extra(
⋮----
fn test_scheduler_normal_termination() {
⋮----
bank.clone(),
Some(setup_mocked_scheduler(bank, [false].into_iter())),
⋮----
assert!(bank.has_installed_scheduler());
assert_matches!(bank.wait_for_completed_scheduler(), Some(_));
assert!(!bank.has_installed_scheduler());
assert_matches!(bank.wait_for_completed_scheduler(), None);
⋮----
fn test_no_scheduler_termination() {
⋮----
fn test_scheduler_termination_from_drop() {
⋮----
Some(setup_mocked_scheduler(bank, [true].into_iter())),
⋮----
drop(bank);
⋮----
fn test_scheduler_pause() {
⋮----
Some(setup_mocked_scheduler_with_extra(
⋮----
[false].into_iter(),
Some(|mocked: &mut MockInstalledScheduler| {
⋮----
.expect_pause_for_recent_blockhash()
⋮----
goto_end_of_slot_with_scheduler(&bank);
⋮----
fn do_test_schedule_execution(should_succeed: bool) {
⋮----
} = create_genesis_config(10_000);
⋮----
genesis_config.hash(),
⋮----
let mocked_scheduler = setup_mocked_scheduler_with_extra(
⋮----
[true].into_iter(),
⋮----
.expect_schedule_execution()
⋮----
.returning(|_, _| Ok(()));
⋮----
.returning(|_, _| Err(SchedulerAborted));
⋮----
.expect_recover_error_after_abort()
⋮----
.returning(|| TransactionError::InsufficientFundsForFee);
⋮----
let bank = BankWithScheduler::new(bank, Some(mocked_scheduler));
let result = bank.schedule_transaction_executions([(tx0, 0)].into_iter());
⋮----
assert_matches!(result, Ok(()));
⋮----
assert_matches!(result, Err(TransactionError::InsufficientFundsForFee));
⋮----
fn test_schedule_execution_success() {
do_test_schedule_execution(true);
⋮----
fn test_schedule_execution_failure() {
do_test_schedule_execution(false);

================
File: runtime/src/lib.rs
================
pub mod account_saver;
pub mod accounts_background_service;
pub mod bank;
pub mod bank_client;
pub mod bank_forks;
pub mod bank_hash_cache;
pub mod bank_utils;
pub mod commitment;
pub mod dependency_tracker;
pub mod epoch_stakes;
pub mod genesis_utils;
pub mod inflation_rewards;
pub mod installed_scheduler_pool;
pub mod loader_utils;
pub mod non_circulating_supply;
pub mod prioritization_fee;
pub mod prioritization_fee_cache;
mod read_optimized_dashmap;
pub mod rent_collector;
pub mod runtime_config;
pub mod serde_snapshot;
pub mod snapshot_bank_utils;
pub mod snapshot_controller;
pub mod snapshot_minimizer;
pub mod snapshot_package;
pub mod snapshot_utils;
mod stake_account;
pub mod stake_history;
pub mod stake_utils;
pub mod stake_weighted_timestamp;
pub mod stakes;
pub mod static_ids;
pub mod status_cache;
pub mod transaction_batch;
pub mod vote_sender_types;
⋮----
extern crate solana_metrics;
⋮----
extern crate solana_frozen_abi_macro;

================
File: runtime/src/loader_utils.rs
================
pub fn load_program_from_file(name: &str) -> Vec<u8> {
⋮----
let current_exe = env::current_exe().unwrap();
⋮----
.parent()
.unwrap()
⋮----
.unwrap(),
⋮----
pathbuf.push("deploy");
pathbuf.push(name);
pathbuf.set_extension("so");
let mut file = File::open(&pathbuf).unwrap_or_else(|err| {
panic!("Failed to open {}: {}", pathbuf.display(), err);
⋮----
file.read_to_end(&mut program).unwrap();
⋮----
pub fn create_program(bank: &Bank, loader_id: &Pubkey, name: &str) -> Pubkey {
⋮----
let elf = load_program_from_file(name);
let mut program_account = AccountSharedData::new(1, elf.len(), loader_id);
⋮----
.data_as_mut_slice()
.get_mut(..)
⋮----
.copy_from_slice(&elf);
program_account.set_executable(true);
bank.store_account(&program_id, &program_account);
⋮----
pub fn load_upgradeable_buffer<T: Client>(
⋮----
let program = load_program_from_file(name);
let buffer_pubkey = buffer_keypair.pubkey();
let buffer_authority_pubkey = buffer_authority_keypair.pubkey();
let program_buffer_bytes = UpgradeableLoaderState::size_of_buffer(program.len());
⋮----
.send_and_confirm_message(
⋮----
&from_keypair.pubkey(),
⋮----
1.max(
⋮----
.get_minimum_balance_for_rent_exemption(program_buffer_bytes)
⋮----
program.len(),
⋮----
Some(&from_keypair.pubkey()),
⋮----
.unwrap();
⋮----
for chunk in program.chunks(chunk_size) {
⋮----
chunk.to_vec(),
⋮----
.send_and_confirm_message(&[from_keypair, buffer_authority_keypair], message)
⋮----
pub fn load_upgradeable_program(
⋮----
let program = load_upgradeable_buffer(
⋮----
&executable_keypair.pubkey(),
&buffer_keypair.pubkey(),
&authority_keypair.pubkey(),
⋮----
.get_minimum_balance_for_rent_exemption(
⋮----
program.len() * 2,
⋮----
bank_client.set_sysvar_for_tests(&Clock {
⋮----
pub fn load_upgradeable_program_wrapper(
⋮----
load_upgradeable_program(
⋮----
program_keypair.pubkey()
⋮----
pub fn load_upgradeable_program_and_advance_slot(
⋮----
load_upgradeable_program_wrapper(bank_client, mint_keypair, authority_keypair, name);
⋮----
.advance_slot(1, bank_forks, &Pubkey::default())
.expect("Failed to advance the slot");
⋮----
pub fn upgrade_program<T: Client>(
⋮----
load_upgradeable_buffer(
⋮----
&payer_keypair.pubkey(),
⋮----
Some(&payer_keypair.pubkey()),
⋮----
.send_and_confirm_message(&[payer_keypair, authority_keypair], message)
⋮----
pub fn set_upgrade_authority<T: Client>(
⋮----
&current_authority_keypair.pubkey(),
⋮----
.send_and_confirm_message(&[from_keypair, current_authority_keypair], message)
⋮----
pub fn instructions_to_load_program_of_loader_v4<T: Client>(
⋮----
let program_keypair = program_keypair.unwrap_or_else(|| {
⋮----
instructions.push(system_instruction::create_account(
⋮----
&program_keypair.pubkey(),
⋮----
.saturating_add(program.len()),
⋮----
instructions.push(instruction::set_program_length(
⋮----
program.len() as u32,
⋮----
instructions.push(instruction::write(
⋮----
instructions.push(instruction::copy(
⋮----
instructions.push(instruction::deploy(
⋮----
pub fn load_program_of_loader_v4(
⋮----
let (program_keypair, instructions) = instructions_to_load_program_of_loader_v4(
⋮----
let signers = std::iter::once(signers[0]).chain(std::iter::repeat(signers[1]));
for (instruction, signers) in instructions.into_iter().zip(signers) {
let message = Message::new(&[instruction], Some(&payer_keypair.pubkey()));
⋮----
.send_and_confirm_message(signers, message)
⋮----
(bank, program_keypair.pubkey())
⋮----
pub fn create_invoke_instruction<T: Serialize>(
⋮----
let account_metas = vec![AccountMeta::new(from_pubkey, true)];

================
File: runtime/src/non_circulating_supply.rs
================
pub struct NonCirculatingSupply {
⋮----
pub fn calculate_non_circulating_supply(bank: &Bank) -> ScanResult<NonCirculatingSupply> {
debug!("Updating Bank supply, epoch: {}", bank.epoch());
⋮----
for key in non_circulating_accounts() {
non_circulating_accounts_set.insert(key);
⋮----
let withdraw_authority_list = withdraw_authority();
let clock = bank.clock();
⋮----
.contains(&AccountIndex::ProgramId)
⋮----
bank.get_filtered_indexed_accounts(
⋮----
|account| account.owner() == &stake::program::id(),
⋮----
bank.get_program_accounts(&stake::program::id(), config)?
⋮----
for (pubkey, account) in stake_accounts.iter() {
⋮----
.unwrap_or_default();
⋮----
if meta.lockup.is_in_force(&clock, None)
|| withdraw_authority_list.contains(&meta.authorized.withdrawer)
⋮----
non_circulating_accounts_set.insert(*pubkey);
⋮----
.iter()
.map(|pubkey| bank.get_balance(pubkey))
.sum();
Ok(NonCirculatingSupply {
⋮----
accounts: non_circulating_accounts_set.into_iter().collect(),
⋮----
pub fn non_circulating_accounts() -> Vec<Pubkey> {
⋮----
.into()
⋮----
pub fn withdraw_authority() -> Vec<Pubkey> {
⋮----
mod tests {
⋮----
fn new_from_parent(parent: Arc<Bank>) -> Bank {
let slot = parent.slot() + 1;
⋮----
fn test_calculate_non_circulating_supply() {
⋮----
accounts.insert(
⋮----
let non_circulating_accounts = non_circulating_accounts();
let num_non_circulating_accounts = non_circulating_accounts.len() as u64;
for key in non_circulating_accounts.clone() {
accounts.insert(key, Account::new(balance, 0, &Pubkey::default()));
⋮----
.unwrap();
accounts.insert(pubkey, stake_account);
⋮----
assert_eq!(
⋮----
let non_circulating_supply = calculate_non_circulating_supply(&bank).unwrap();
⋮----
bank = Arc::new(new_from_parent(bank));
⋮----
bank.store_account(
⋮----
assert_eq!(bank.epoch(), 1);

================
File: runtime/src/prioritization_fee_cache.rs
================
type UnfinalizedPrioritizationFees = BTreeMap<Slot, HashMap<BankId, PrioritizationFee>>;
⋮----
struct PrioritizationFeeCacheMetrics {
⋮----
impl PrioritizationFeeCacheMetrics {
fn accumulate_successful_transaction_update_count(&self, val: u64) {
⋮----
.fetch_add(val, Ordering::Relaxed);
⋮----
fn accumulate_total_purged_duplicated_bank_count(&self, val: u64) {
⋮----
fn accumulate_total_update_elapsed_us(&self, val: u64) {
⋮----
fn accumulate_total_cache_lock_elapsed_us(&self, val: u64) {
⋮----
fn accumulate_total_entry_update_elapsed_us(&self, val: u64) {
⋮----
fn accumulate_total_block_finalize_elapsed_us(&self, val: u64) {
⋮----
fn accumulate_total_calculate_prioritization_fee_elapsed_us(&self, val: u64) {
⋮----
fn report(&self, slot: Slot) {
datapoint_info!(
⋮----
enum CacheServiceUpdate {
⋮----
pub struct PrioritizationFeeCache {
⋮----
impl Default for PrioritizationFeeCache {
fn default() -> Self {
⋮----
impl Drop for PrioritizationFeeCache {
fn drop(&mut self) {
let _ = self.sender.send(CacheServiceUpdate::Exit);
⋮----
.take()
.unwrap()
.join()
.expect("Prioritization fee cache servicing thread failed to join");
⋮----
impl PrioritizationFeeCache {
pub fn new(capacity: u64) -> Self {
⋮----
let (sender, receiver) = unbounded();
⋮----
let service_thread = Some(
⋮----
.name("solPrFeeCachSvc".to_string())
.spawn({
let cache = cache.clone();
let metrics = metrics.clone();
⋮----
.unwrap(),
⋮----
pub fn update<'a, Tx: TransactionWithMeta + 'a>(
⋮----
let (_, send_updates_us) = measure_us!({
⋮----
// Vote transactions are not prioritized, therefore they are excluded from
// updating fee_cache.
⋮----
// filter out any transaction that requests zero compute_unit_limit
// since its priority fee amount is not instructive
⋮----
.accumulate_total_update_elapsed_us(send_updates_us);
⋮----
/// Finalize prioritization fee when it's bank is completely replayed from blockstore,
    pub fn finalize_priority_fee(&self, slot: Slot, bank_id: BankId) {
⋮----
pub fn finalize_priority_fee(&self, slot: Slot, bank_id: BankId) {
⋮----
.send(CacheServiceUpdate::BankFinalized { slot, bank_id })
.unwrap_or_else(|err| {
warn!("prioritization fee cache signalling bank frozen failed: {err:?}")
⋮----
fn update_cache(
⋮----
let (_, entry_update_us) = measure_us!(unfinalized
⋮----
metrics.accumulate_total_entry_update_elapsed_us(entry_update_us);
metrics.accumulate_successful_transaction_update_count(1);
⋮----
fn finalize_slot(
⋮----
if unfinalized.is_empty() {
⋮----
let (slot_prioritization_fee, slot_finalize_us) = measure_us!({
⋮----
metrics.accumulate_total_block_finalize_elapsed_us(slot_finalize_us);
⋮----
let (_, cache_lock_us) = measure_us!({
⋮----
metrics.accumulate_total_cache_lock_elapsed_us(cache_lock_us);
⋮----
fn service_loop(
⋮----
let update = match receiver.try_recv() {
⋮----
sleep(Duration::from_millis(5));
⋮----
info!("PrioritizationFeeCache::service_loop() is stopping because: {err}");
⋮----
metrics.report(slot);
⋮----
pub fn available_block_count(&self) -> usize {
self.cache.read().unwrap().len()
⋮----
pub fn get_prioritization_fees(&self, account_keys: &[Pubkey]) -> Vec<(Slot, u64)> {
⋮----
.read()
⋮----
.iter()
.map(|(slot, slot_prioritization_fee)| {
⋮----
.get_min_compute_unit_price()
.unwrap_or_default();
⋮----
slot_prioritization_fee.get_writable_account_fee(account_key)
⋮----
.collect()
⋮----
mod tests {
⋮----
fn build_sanitized_transaction_for_test(
⋮----
Some(signer_account),
⋮----
fn sync_update<'a>(
⋮----
.load(Ordering::Relaxed)
.saturating_add(txs.len() as u64);
prioritization_fee_cache.update(&bank, txs);
⋮----
fn sync_finalize_priority_fee_for_test(
⋮----
prioritization_fee_cache.finalize_priority_fee(slot, bank_id);
⋮----
let cache = prioritization_fee_cache.cache.read().unwrap();
if let Some(slot_cache) = cache.get(&slot) {
if slot_cache.is_finalized() {
⋮----
drop(cache);
⋮----
fn test_prioritization_fee_cache_update() {
⋮----
build_sanitized_transaction_for_test(5, &write_account_a, &write_account_b),
build_sanitized_transaction_for_test(9, &write_account_b, &write_account_c),
build_sanitized_transaction_for_test(2, &write_account_a, &write_account_c),
⋮----
let slot = bank.slot();
⋮----
sync_update(&prioritization_fee_cache, bank.clone(), txs.iter());
⋮----
let lock = prioritization_fee_cache.cache.read().unwrap();
assert!(lock.get(&slot).is_none());
⋮----
sync_finalize_priority_fee_for_test(&prioritization_fee_cache, slot, bank.bank_id());
⋮----
let fee = lock.get(&slot).unwrap();
assert_eq!(2, fee.get_min_compute_unit_price().unwrap());
assert!(fee.get_writable_account_fee(&write_account_a).is_none());
assert_eq!(5, fee.get_writable_account_fee(&write_account_b).unwrap());
assert!(fee.get_writable_account_fee(&write_account_c).is_none());
⋮----
fn test_available_block_count() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let bank = bank_forks.read().unwrap().working_bank();
⋮----
let bank1 = Arc::new(Bank::new_from_parent(bank.clone(), &collector, 1));
sync_update(
⋮----
bank1.clone(),
[build_sanitized_transaction_for_test(
⋮----
.iter(),
⋮----
sync_finalize_priority_fee_for_test(&prioritization_fee_cache, 1, bank1.bank_id());
let bank2 = Arc::new(Bank::new_from_parent(bank.clone(), &collector, 2));
let txs = [build_sanitized_transaction_for_test(
⋮----
sync_update(&prioritization_fee_cache, bank2.clone(), txs.iter());
let bank3 = Arc::new(Bank::new_from_parent(bank.clone(), &collector, 3));
⋮----
bank3.clone(),
⋮----
sync_finalize_priority_fee_for_test(&prioritization_fee_cache, 3, bank3.bank_id());
assert_eq!(2, prioritization_fee_cache.available_block_count());
⋮----
fn test_get_prioritization_fees() {
⋮----
assert!(prioritization_fee_cache
⋮----
build_sanitized_transaction_for_test(2, &write_account_a, &write_account_b),
build_sanitized_transaction_for_test(
⋮----
sync_update(&prioritization_fee_cache, bank1.clone(), txs.iter());
⋮----
assert_eq!(
⋮----
build_sanitized_transaction_for_test(4, &write_account_b, &write_account_c),
⋮----
sync_finalize_priority_fee_for_test(&prioritization_fee_cache, 2, bank2.bank_id());
⋮----
build_sanitized_transaction_for_test(6, &write_account_a, &write_account_c),
⋮----
sync_update(&prioritization_fee_cache, bank3.clone(), txs.iter());
⋮----
fn test_purge_duplicated_bank() {
⋮----
let bank1 = Arc::new(Bank::new_from_parent(bank.clone(), &collector, slot));
⋮----
sync_finalize_priority_fee_for_test(&prioritization_fee_cache, slot, bank1.bank_id());

================
File: runtime/src/prioritization_fee.rs
================
struct PrioritizationFeeMetrics {
⋮----
impl PrioritizationFeeMetrics {
fn accumulate_total_prioritization_fee(&mut self, val: u64) {
⋮----
fn accumulate_total_update_elapsed_us(&mut self, val: u64) {
⋮----
fn increment_attempted_update_on_finalized_fee_count(&mut self, val: u64) {
⋮----
fn update_compute_unit_price(&mut self, cu_price: u64) {
⋮----
self.max_compute_unit_price = self.max_compute_unit_price.max(cu_price);
self.min_compute_unit_price = Some(
⋮----
.map_or(cu_price, |min_cu_price| min_cu_price.min(cu_price)),
⋮----
fn report(&self, slot: Slot) {
⋮----
datapoint_info!(
⋮----
pub enum PrioritizationFeeError {
⋮----
pub struct PrioritizationFee {
⋮----
impl Default for PrioritizationFee {
fn default() -> Self {
⋮----
impl PrioritizationFee {
pub fn update(
⋮----
let (_, update_us) = measure_us!({
⋮----
self.metrics.accumulate_total_update_elapsed_us(update_us);
⋮----
fn prune_irrelevant_writable_accounts(&mut self) {
self.metrics.total_writable_accounts_count = self.get_writable_accounts_count() as u64;
⋮----
.retain(|_, account_fee| account_fee > &mut self.min_compute_unit_price);
self.metrics.relevant_writable_accounts_count = self.get_writable_accounts_count() as u64;
⋮----
pub fn mark_block_completed(&mut self) -> Result<(), PrioritizationFeeError> {
⋮----
return Err(PrioritizationFeeError::BlockIsAlreadyFinalized);
⋮----
self.prune_irrelevant_writable_accounts();
⋮----
Ok(())
⋮----
pub fn get_min_compute_unit_price(&self) -> Option<u64> {
(self.min_compute_unit_price != u64::MAX).then_some(self.min_compute_unit_price)
⋮----
pub fn get_writable_account_fee(&self, key: &Pubkey) -> Option<u64> {
self.min_writable_account_fees.get(key).copied()
⋮----
pub fn get_writable_account_fees(&self) -> impl Iterator<Item = (&Pubkey, &u64)> {
self.min_writable_account_fees.iter()
⋮----
pub fn get_writable_accounts_count(&self) -> usize {
self.min_writable_account_fees.len()
⋮----
pub fn is_finalized(&self) -> bool {
⋮----
pub fn report_metrics(&self, slot: Slot) {
self.metrics.report(slot);
⋮----
mod tests {
⋮----
fn test_update_compute_unit_price() {
⋮----
assert!(prioritization_fee.get_min_compute_unit_price().is_none());
⋮----
prioritization_fee.update(5, tx_fee, vec![write_account_a, write_account_b]);
assert_eq!(5, prioritization_fee.get_min_compute_unit_price().unwrap());
assert_eq!(
⋮----
assert!(prioritization_fee
⋮----
prioritization_fee.update(9, tx_fee, vec![write_account_b, write_account_c]);
⋮----
prioritization_fee.update(2, tx_fee, vec![write_account_a, write_account_c]);
assert_eq!(2, prioritization_fee.get_min_compute_unit_price().unwrap());
⋮----
prioritization_fee.prune_irrelevant_writable_accounts();
assert_eq!(1, prioritization_fee.min_writable_account_fees.len());
⋮----
fn test_total_prioritization_fee() {
⋮----
prioritization_fee.update(0, 10, vec![]);
assert_eq!(10, prioritization_fee.metrics.total_prioritization_fee.0);
prioritization_fee.update(10, u64::MAX, vec![]);
⋮----
prioritization_fee.update(10, 100, vec![]);
⋮----
fn test_mark_block_completed() {
⋮----
assert!(prioritization_fee.mark_block_completed().is_ok());
assert!(prioritization_fee.mark_block_completed().is_err());

================
File: runtime/src/read_optimized_dashmap.rs
================
use shuttle::sync::Arc;
⋮----
use std::sync::Arc;
⋮----
type DashmapIteratorItem<'a, K, V, S> = RefMulti<'a, K, ROValue<V>, S>;
⋮----
pub struct ReadOptimizedDashMap<K, V, S>
⋮----
pub fn new(inner: DashMap<K, ROValue<V>, S>) -> Self {
⋮----
pub fn get_or_insert_with(&self, k: &K, default: impl FnOnce() -> V) -> ROValue<V> {
match self.inner.get(k) {
⋮----
.entry(k.clone())
.or_insert_with(|| ROValue::new(default()))
.value(),
⋮----
pub fn get(&self, k: &K) -> Option<ROValue<V>> {
self.inner.get(k).map(|v| ROValue::clone(&v))
⋮----
pub fn iter(&self) -> impl Iterator<Item = DashmapIteratorItem<'_, K, V, S>> {
self.inner.iter()
⋮----
pub fn remove_if_not_accessed(&self, k: &K) -> Result<Option<ROValue<V>>, ()> {
self.remove_if_not_accessed_and(k, |_| true)
⋮----
pub fn remove_if_not_accessed_and(
⋮----
let entry = self.inner.entry(k.clone());
⋮----
let v = e.get();
if pred(v) && !v.shared() {
return Ok(Some(e.remove()));
⋮----
return Err(());
⋮----
Ok(None)
⋮----
pub fn retain_if_accessed_or(&self, mut f: impl FnMut(&K, &mut ROValue<V>) -> bool) {
self.inner.retain(|k, v| v.shared() || f(k, v))
⋮----
pub unsafe fn retain(&self, f: impl FnMut(&K, &mut ROValue<V>) -> bool) {
self.inner.retain(f)
⋮----
pub fn clear(&self) {
self.inner.clear();
⋮----
pub struct ROValue<V> {
⋮----
impl<V> Clone for ROValue<V> {
fn clone(&self) -> Self {
⋮----
fn new(v: V) -> Self {
⋮----
pub fn shared(&self) -> bool {
⋮----
pub fn inner(&self) -> &Arc<V> {
⋮----
impl<V> Deref for ROValue<V> {
type Target = V;
fn deref(&self) -> &Self::Target {
⋮----
mod tests {
⋮----
fn test_get() {
⋮----
let v1 = map.get_or_insert_with(&1, || 10);
assert_eq!(*v1, 10);
assert!(v1.shared());
let v2 = map.get(&1).unwrap();
assert_eq!(*v2, 10);
assert!(v2.shared());
let v3 = map.get(&2);
assert!(v3.is_none());
⋮----
fn test_remove_if_not_accessed() {
⋮----
assert!(map.remove_if_not_accessed(&1).is_err());
drop(v1);
let removed = map.remove_if_not_accessed_and(&1, |_| false);
assert!(removed.is_err());
let removed = map.remove_if_not_accessed(&1).unwrap();
assert!(removed.is_some());
assert_eq!(*removed.unwrap(), 10);
⋮----
assert!(removed.is_none());
⋮----
fn test_retain_if_accessed_or() {
⋮----
let v2 = map.get_or_insert_with(&2, || 20);
drop(v2);
let v3 = map.get_or_insert_with(&3, || 30);
assert_eq!(map.inner.len(), 3);
map.retain_if_accessed_or(|_k, v| **v >= 30);
assert_eq!(map.inner.len(), 2);
⋮----
drop(v3);
⋮----
assert_eq!(map.inner.len(), 1);
⋮----
mod shuttle_tests {
⋮----
fn do_test_shuttle_concurrent_inserts() {
⋮----
.map(|_| {
⋮----
map.get_or_insert_with(&0, || AtomicU64::new(30))
.fetch_add(10, Ordering::Relaxed);
⋮----
handle.join().unwrap();
⋮----
let v = map.get(&0).unwrap();
assert_eq!(v.load(Ordering::Relaxed), 50);
⋮----
fn test_shuttle_concurrent_inserts_dfs() {
⋮----
fn test_shuttle_concurrent_inserts_random() {
⋮----
fn do_test_shuttle_insert_retain() {
⋮----
map.get_or_insert_with(&1, || 10);
⋮----
map.retain_if_accessed_or(|_k, v| **v >= 20);
⋮----
map.get_or_insert_with(&2, || 20);
⋮----
retain_th.join().unwrap();
insert_th.join().unwrap();
assert_eq!(map.get(&1), None);
let v = map.get(&2).unwrap();
assert_eq!(*v, 20);
⋮----
fn test_shuttle_insert_retain_dfs() {
⋮----
fn test_shuttle_insert_retain_random() {

================
File: runtime/src/rent_collector.rs
================
pub struct RentCollector {
⋮----
impl Default for RentCollector {
fn default() -> Self {
⋮----
slots_per_year: GenesisConfig::default().slots_per_year(),
⋮----
impl RentCollector {
pub(crate) fn new(
⋮----
pub(crate) fn clone_with_epoch(&self, epoch: Epoch) -> Self {
⋮----
..self.clone()

================
File: runtime/src/runtime_config.rs
================
use solana_compute_budget::compute_budget::ComputeBudget;
⋮----
fn example() -> Self {
⋮----
pub struct RuntimeConfig {

================
File: runtime/src/serde_snapshot.rs
================
mod obsolete_accounts;
mod status_cache;
mod storage;
mod tests;
mod types;
mod utils;
⋮----
pub struct AccountsDbFields<T>(
⋮----
pub struct ObsoleteIncrementalSnapshotPersistence {
⋮----
struct BankHashInfo {
⋮----
struct UnusedAccounts {
⋮----
struct DeserializableVersionedBank {
⋮----
fn from(dvb: DeserializableVersionedBank) -> Self {
const LT_HASH_CANARY: LtHash = LtHash([0xCAFE; LtHash::NUM_ELEMENTS]);
⋮----
accounts_lt_hash: AccountsLtHash(LT_HASH_CANARY),
⋮----
struct SerializableVersionedBank {
⋮----
fn from(rhs: BankFieldsToSerialize) -> Self {
⋮----
pub struct SnapshotStreams<'a, R> {
⋮----
/// Helper type to wrap BankFields when reconstructing Bank from either just a full
/// snapshot, or both a full and incremental snapshot
⋮----
/// snapshot, or both a full and incremental snapshot
#[derive(Debug)]
pub struct SnapshotBankFields {
⋮----
impl SnapshotBankFields {
pub fn new(
⋮----
/// Collapse the SnapshotBankFields into a single (the latest) BankFieldsToDeserialize.
    pub fn collapse_into(self) -> BankFieldsToDeserialize {
⋮----
pub fn collapse_into(self) -> BankFieldsToDeserialize {
self.incremental.unwrap_or(self.full)
⋮----
/// Helper type to wrap AccountsDbFields when reconstructing AccountsDb from either just a full
/// snapshot, or both a full and incremental snapshot
⋮----
pub struct SnapshotAccountsDbFields<T> {
⋮----
/// Collapse the SnapshotAccountsDbFields into a single AccountsDbFields.  If there is no
    /// incremental snapshot, this returns the AccountsDbFields from the full snapshot.
⋮----
/// incremental snapshot, this returns the AccountsDbFields from the full snapshot.
    /// Otherwise, use the AccountsDbFields from the incremental snapshot, and a combination
⋮----
/// Otherwise, use the AccountsDbFields from the incremental snapshot, and a combination
    /// of the storages from both the full and incremental snapshots.
⋮----
/// of the storages from both the full and incremental snapshots.
    pub fn collapse_into(self) -> Result<AccountsDbFields<T>, Error> {
⋮----
pub fn collapse_into(self) -> Result<AccountsDbFields<T>, Error> {
⋮----
None => Ok(self.full_snapshot_accounts_db_fields),
⋮----
// filter out incremental snapshot storages with slot <= full snapshot slot
incremental_snapshot_storages.retain(|slot, _| *slot > full_snapshot_slot);
// There must not be any overlap in the slots of storages between the full snapshot and the incremental snapshot
⋮----
.iter()
.all(|storage_entry| !full_snapshot_storages.contains_key(storage_entry.0))
.then_some(())
.ok_or_else(|| {
⋮----
combined_storages.extend(incremental_snapshot_storages);
Ok(AccountsDbFields(
⋮----
pub(crate) fn serialize_into<W, T>(writer: W, value: &T) -> bincode::Result<()>
⋮----
.with_fixint_encoding()
.with_limit(MAX_STREAM_SIZE)
.serialize_into(writer, value)
⋮----
pub(crate) fn deserialize_from<R, T>(reader: R) -> bincode::Result<T>
⋮----
.allow_trailing_bytes()
⋮----
fn deserialize_accounts_db_fields<R>(
⋮----
/// Extra fields that are deserialized from the end of snapshots.
///
⋮----
///
/// Note that this struct's fields should stay synced with the fields in
⋮----
/// Note that this struct's fields should stay synced with the fields in
#[cfg_attr(feature = "frozen-abi", derive(AbiExample))]
⋮----
struct ExtraFieldsToDeserialize {
⋮----
pub struct ExtraFieldsToSerialize {
⋮----
fn deserialize_bank_fields<R>(
⋮----
if !deserializable_bank.unused_epoch_stakes.is_empty() {
return Err(Box::new(bincode::ErrorKind::Custom(
"Expected deserialized bank's unused_epoch_stakes field to be empty".to_string(),
⋮----
let accounts_db_fields = deserialize_accounts_db_fields(stream)?;
let extra_fields = deserialize_from(stream)?;
⋮----
.clone_with_lamports_per_signature(lamports_per_signature);
⋮----
.expect("snapshot must have accounts_lt_hash")
.into();
Ok((bank_fields, accounts_db_fields))
⋮----
pub(crate) fn snapshot_storage_lengths_from_fields(
⋮----
.map(|(slot, slot_storage)| {
⋮----
.map(|storage_entry| (storage_entry.id(), storage_entry.current_len()))
.collect(),
⋮----
.collect()
⋮----
pub(crate) fn fields_from_stream<R: Read>(
⋮----
deserialize_bank_fields(snapshot_stream)
⋮----
pub(crate) fn fields_from_streams(
⋮----
fields_from_stream(snapshot_streams.full_snapshot_stream)?;
⋮----
.as_mut()
.map(|stream| fields_from_stream(stream))
.transpose()?
.unzip();
⋮----
Ok((snapshot_bank_fields, snapshot_accounts_db_fields))
⋮----
pub struct BankFromStreamsInfo {
⋮----
pub(crate) fn bank_from_streams<R>(
⋮----
let (bank_fields, accounts_db_fields) = fields_from_streams(snapshot_streams)?;
let (bank, info) = reconstruct_bank_from_fields(
⋮----
Ok((
⋮----
pub(crate) fn bank_to_stream<W>(
⋮----
pub fn serialize_bank_snapshot_into<W>(
⋮----
bincode::DefaultOptions::new().with_fixint_encoding(),
⋮----
serialize_bank_snapshot_with(
⋮----
pub fn serialize_bank_snapshot_with<S>(
⋮----
(serializable_bank, serializable_accounts_db, extra_fields).serialize(serializer)
⋮----
struct SerializableBankAndStorage<'a> {
⋮----
impl Serialize for SerializableBankAndStorage<'_> {
fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>
⋮----
let slot = self.bank.slot();
let mut bank_fields = self.bank.get_fields_to_serialize();
⋮----
let bank_hash_stats = self.bank.get_bank_hash_stats();
let write_version = accounts_db.write_version.load(Ordering::Acquire);
⋮----
let accounts_lt_hash = Some(bank_fields.accounts_lt_hash.clone().into());
⋮----
bank_fields_to_serialize.serialize(serializer)
⋮----
struct SerializableBankAndStorageNoExtra<'a> {
⋮----
impl Serialize for SerializableBankAndStorageNoExtra<'_> {
⋮----
let bank_fields = self.bank.get_fields_to_serialize();
⋮----
.serialize(serializer)
⋮----
fn from(s: SerializableBankAndStorageNoExtra<'a>) -> SerializableBankAndStorage<'a> {
⋮----
struct SerializableAccountsDb<'a> {
⋮----
impl Serialize for SerializableAccountsDb<'_> {
⋮----
// (1st of 3 elements) write the list of account storage entry lists out as a map
⋮----
let entries = utils::serialize_iter_as_map(self.account_storage_entries.iter().map(|x| {
*entry_count.borrow_mut() += x.len();
⋮----
x.first().unwrap().slot(),
⋮----
x.iter()
.map(|x| SerializableAccountStorageEntry::new(x.as_ref(), self.slot)),
⋮----
stats: self.bank_hash_stats.clone(),
⋮----
.serialize(serializer);
serialize_account_storage_timer.stop();
datapoint_info!(
⋮----
pub(crate) struct ReconstructedBankInfo {
⋮----
pub(crate) fn reconstruct_bank_from_fields<E>(
⋮----
let mut bank_fields = bank_fields.collapse_into();
let (accounts_db, reconstructed_accounts_db_info) = reconstruct_accountsdb_from_fields(
⋮----
let runtime_config = Arc::new(runtime_config.clone());
⋮----
info!("rent_collector: {:?}", bank.rent_collector());
⋮----
pub(crate) fn reconstruct_single_storage(
⋮----
return Err(SnapshotError::MismatchedAccountsFileId(
⋮----
Ok(Arc::new(AccountStorageEntry::new_existing(
⋮----
pub(crate) fn remap_append_vec_file(
⋮----
let append_vec_path_cstr = cstring_from_path(append_vec_path)?;
let mut remapped_append_vec_path = append_vec_path.to_path_buf();
⋮----
let remapped_append_vec_id = next_append_vec_id.fetch_add(1, Ordering::AcqRel);
⋮----
remapped_append_vec_path = append_vec_path.parent().unwrap().join(remapped_file_name);
⋮----
let remapped_append_vec_path_cstr = cstring_from_path(&remapped_append_vec_path)?;
match rename_no_replace(&append_vec_path_cstr, &remapped_append_vec_path_cstr) {
⋮----
Err(e) if e.kind() == io::ErrorKind::AlreadyExists => {}
Err(e) => return Err(e),
⋮----
if std::fs::metadata(&remapped_append_vec_path).is_err() {
⋮----
num_collisions.fetch_add(1, Ordering::Relaxed);
⋮----
Ok((remapped_append_vec_id, remapped_append_vec_path))
⋮----
pub(crate) fn remap_and_reconstruct_single_storage(
⋮----
let (remapped_append_vec_id, remapped_append_vec_path) = remap_append_vec_file(
⋮----
let storage = reconstruct_single_storage(
⋮----
Ok(storage)
⋮----
pub struct ReconstructedAccountsDbInfo {
⋮----
fn reconstruct_accountsdb_from_fields<E>(
⋮----
account_paths.to_vec(),
⋮----
) = snapshot_accounts_db_fields.collapse_into()?;
⋮----
.unwrap_or_else(|err| panic!("Failed to create directory {}: {}", path.display(), err));
⋮----
assert!(
⋮----
let next_append_vec_id = next_append_vec_id.load(Ordering::Acquire);
⋮----
accounts_db.storage.initialize(storage);
⋮----
.store(next_append_vec_id, Ordering::Release);
⋮----
.fetch_add(snapshot_version, Ordering::Release);
info!("Building accounts index...");
⋮----
} = accounts_db.generate_index(limit_load_slot_count_from_snapshot, verify_index);
info!("Building accounts index... Done in {:?}", start.elapsed());
⋮----
fn rename_no_replace(src: &CStr, dest: &CStr) -> io::Result<()> {
⋮----
src.as_ptr() as *const _,
⋮----
dest.as_ptr() as *const _,
⋮----
return Err(io::Error::last_os_error());
⋮----
Ok(())
⋮----
fn cstring_from_path(path: &Path) -> io::Result<CString> {
CString::new(path.as_os_str().as_encoded_bytes())
.map_err(|e| io::Error::new(io::ErrorKind::InvalidInput, e))

================
File: runtime/src/snapshot_bank_utils.rs
================
pub fn bank_fields_from_snapshot_archives(
⋮----
get_highest_full_snapshot_archive_info(&full_snapshot_archives_dir).ok_or_else(|| {
SnapshotError::NoSnapshotArchives(full_snapshot_archives_dir.as_ref().to_path_buf())
⋮----
let incremental_snapshot_archive_info = get_highest_incremental_snapshot_archive_info(
⋮----
full_snapshot_archive_info.slot(),
⋮----
let account_paths = vec![temp_accounts_dir.path().to_path_buf()];
⋮----
) = verify_and_unarchive_snapshots(
⋮----
incremental_snapshot_archive_info.as_ref(),
⋮----
bank_fields_from_snapshots(
⋮----
incremental_unpacked_snapshots_dir_and_version.as_ref(),
⋮----
fn bank_fields_from_snapshots(
⋮----
let (snapshot_version, snapshot_root_paths) = snapshot_version_and_root_paths(
⋮----
info!(
⋮----
deserialize_snapshot_data_files(&snapshot_root_paths, |snapshot_streams| {
Ok(match snapshot_version {
SnapshotVersion::V1_2_0 => fields_from_streams(snapshot_streams)
.map(|(bank_fields, _accountsdb_fields)| bank_fields.collapse_into()),
⋮----
pub fn bank_from_snapshot_archives(
⋮----
storage.extend(incremental_storage);
⋮----
let (bank, info) = reconstruct_bank_from_fields(
⋮----
measure_rebuild.stop();
info!("{measure_rebuild}");
verify_epoch_stakes(&bank)?;
⋮----
.as_ref()
.unwrap_or(&full_unpacked_snapshots_dir_and_version)
⋮----
.join(snapshot_paths::SNAPSHOT_STATUS_CACHE_FILENAME);
⋮----
verify_slot_deltas(slot_deltas.as_slice(), &bank)?;
bank.status_cache.write().unwrap().append(&slot_deltas);
let snapshot_archive_info = incremental_snapshot_archive_info.map_or_else(
|| full_snapshot_archive_info.snapshot_archive_info(),
⋮----
incremental_snapshot_archive_info.snapshot_archive_info()
⋮----
verify_bank_against_expected_slot_hash(
⋮----
if !bank.verify_snapshot_bank(
accounts_db_skip_shrink || !full_snapshot_archive_info.is_remote(),
⋮----
Some(&info.calculated_accounts_lt_hash),
) && limit_load_slot_count_from_snapshot.is_none()
⋮----
panic!("Snapshot bank for slot {} failed to verify", bank.slot());
⋮----
measure_verify.stop();
datapoint_info!(
⋮----
Ok(bank)
⋮----
pub fn bank_from_latest_snapshot_archives(
⋮----
let bank = bank_from_snapshot_archives(
⋮----
bank_snapshots_dir.as_ref(),
⋮----
Ok((
⋮----
pub fn bank_from_snapshot_dir(
⋮----
let ((storage, bank_fields, accounts_db_fields), measure_rebuild_storages) = measure_time!(
⋮----
info!("{measure_rebuild_storages}");
⋮----
Arc::try_unwrap(next_append_vec_id).expect("this is the only strong reference");
⋮----
let ((bank, info), measure_rebuild_bank) = measure_time!(
⋮----
info!("{measure_rebuild_bank}");
⋮----
fn verify_bank_against_expected_slot_hash(
⋮----
let bank_slot = bank.slot();
⋮----
return Err(SnapshotError::MismatchedSlot(bank_slot, snapshot_slot));
⋮----
let bank_hash = bank.get_snapshot_hash();
⋮----
Ok(())
⋮----
Err(SnapshotError::MismatchedHash(bank_hash, snapshot_hash))
⋮----
fn snapshot_version_and_root_paths(
⋮----
verify_unpacked_snapshots_dir_and_version(
⋮----
Some(verify_unpacked_snapshots_dir_and_version(
⋮----
.unzip();
let snapshot_version = incremental_snapshot_version.unwrap_or(full_snapshot_version);
⋮----
full_snapshot_root_file_path: full_snapshot_root_paths.snapshot_path(),
⋮----
.map(|root_paths| root_paths.snapshot_path()),
⋮----
Ok((snapshot_version, snapshot_root_paths))
⋮----
fn verify_slot_deltas(
⋮----
let info = verify_slot_deltas_structural(slot_deltas, bank.slot())?;
verify_slot_deltas_with_history(&info.slots, &bank.get_slot_history(), bank.slot())
⋮----
fn verify_slot_deltas_structural(
⋮----
let num_entries = slot_deltas.len();
⋮----
return Err(VerifySlotDeltasError::TooManyEntries(
⋮----
return Err(VerifySlotDeltasError::SlotIsNotRoot(slot));
⋮----
return Err(VerifySlotDeltasError::SlotGreaterThanMaxRoot(
⋮----
let is_duplicate = !slots_seen_so_far.insert(slot);
⋮----
return Err(VerifySlotDeltasError::SlotHasMultipleEntries(slot));
⋮----
assert_eq!(slots_seen_so_far.len(), slot_deltas.len());
Ok(VerifySlotDeltasStructuralInfo {
⋮----
struct VerifySlotDeltasStructuralInfo {
⋮----
fn verify_slot_deltas_with_history(
⋮----
verify_slot_history(slot_history, bank_slot)?;
⋮----
.iter()
.find(|slot| slot_history.check(**slot) != Check::Found);
⋮----
return Err(VerifySlotDeltasError::SlotNotFoundInHistory(*slot));
⋮----
let slot_missing_from_deltas = (slot_history.oldest()..=slot_history.newest())
.rev()
.filter(|slot| slot_history.check(*slot) == Check::Found)
.take(status_cache::MAX_CACHE_ENTRIES)
.find(|slot| !slots_from_slot_deltas.contains(slot));
⋮----
return Err(VerifySlotDeltasError::SlotNotFoundInDeltas(slot));
⋮----
fn verify_slot_history(
⋮----
if slot_history.newest() != bank_slot {
return Err(VerifySlotHistoryError::InvalidNewestSlot);
⋮----
if slot_history.bits.len() != solana_slot_history::MAX_ENTRIES {
return Err(VerifySlotHistoryError::InvalidNumEntries);
⋮----
fn verify_epoch_stakes(bank: &Bank) -> std::result::Result<(), VerifyEpochStakesError> {
let current_epoch = bank.epoch();
let leader_schedule_epoch = bank.get_leader_schedule_epoch(bank.slot());
⋮----
_verify_epoch_stakes(bank.epoch_stakes_map(), required_epochs)
⋮----
fn _verify_epoch_stakes(
⋮----
let max_epoch = *required_epochs.end();
if let Some(invalid_epoch) = epoch_stakes_map.keys().find(|epoch| **epoch > max_epoch) {
return Err(VerifyEpochStakesError::EpochGreaterThanMax(
⋮----
.clone()
.find(|epoch| !epoch_stakes_map.contains_key(epoch))
⋮----
return Err(VerifyEpochStakesError::StakesNotFound(
⋮----
pub fn bank_to_full_snapshot_archive(
⋮----
let snapshot_version = snapshot_version.unwrap_or_default();
⋮----
bank_to_full_snapshot_archive_with(
⋮----
fn bank_to_full_snapshot_archive_with(
⋮----
assert!(bank.is_complete());
⋮----
.set_latest_full_snapshot_slot(bank.slot());
bank.squash();
bank.rehash();
bank.force_flush_accounts_cache();
bank.clean_accounts();
⋮----
bank.get_snapshot_storages(None),
bank.status_cache.read().unwrap().root_slot_deltas(),
⋮----
full_snapshot_archives_dir: full_snapshot_archives_dir.as_ref().to_path_buf(),
incremental_snapshot_archives_dir: incremental_snapshot_archives_dir.as_ref().to_path_buf(),
bank_snapshots_dir: bank_snapshots_dir.as_ref().to_path_buf(),
⋮----
Ok(FullSnapshotArchiveInfo::new(snapshot_archive_info))
⋮----
pub fn bank_to_incremental_snapshot_archive(
⋮----
assert!(bank.slot() > full_snapshot_slot);
⋮----
.set_latest_full_snapshot_slot(full_snapshot_slot);
⋮----
bank.get_snapshot_storages(Some(full_snapshot_slot)),
⋮----
bank_snapshots_dir: temp_bank_snapshots_dir.path().to_path_buf(),
⋮----
Ok(IncrementalSnapshotArchiveInfo::new(
⋮----
mod tests {
⋮----
fn create_snapshot_dirs_for_tests(
⋮----
let snapshot_archives_dir = TempDir::new().unwrap();
⋮----
let slot = bank.slot() + 1;
⋮----
bank.fill_bank_with_ticks_for_tests();
⋮----
.unwrap();
⋮----
Arc::into_inner(bank).unwrap()
⋮----
fn test_roundtrip_bank_to_and_from_full_snapshot_simple() {
⋮----
original_bank.fill_bank_with_ticks_for_tests();
let (_tmp_dir, accounts_dir) = create_tmp_accounts_dir_for_tests();
let bank_snapshots_dir = tempfile::TempDir::new().unwrap();
let full_snapshot_archives_dir = tempfile::TempDir::new().unwrap();
let incremental_snapshot_archives_dir = tempfile::TempDir::new().unwrap();
⋮----
let snapshot_archive_info = bank_to_full_snapshot_archive(
⋮----
full_snapshot_archives_dir.path(),
incremental_snapshot_archives_dir.path(),
⋮----
let roundtrip_bank = bank_from_snapshot_archives(
⋮----
bank_snapshots_dir.path(),
⋮----
assert_eq!(original_bank, roundtrip_bank);
⋮----
fn test_roundtrip_bank_to_and_from_full_snapshot_with_obsolete_account() {
⋮----
let (genesis_config, mint_keypair) = create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
.transfer(LAMPORTS_PER_SOL, &mint_keypair, &key1.pubkey())
⋮----
.transfer(2 * LAMPORTS_PER_SOL, &mint_keypair, &key2.pubkey())
⋮----
.transfer(3 * LAMPORTS_PER_SOL, &mint_keypair, &key3.pubkey())
⋮----
bank0.fill_bank_with_ticks_for_tests();
bank0.squash();
bank0.force_flush_accounts_cache();
⋮----
Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank0, &collector, slot);
⋮----
.transfer(LAMPORTS_PER_SOL, &key3, &key1.pubkey())
⋮----
bank1.fill_bank_with_ticks_for_tests();
⋮----
let snapshot_archives_dir = tempfile::TempDir::new().unwrap();
⋮----
let full_snapshot_archive_info = bank_to_full_snapshot_archive(
⋮----
snapshot_archives_dir.path(),
⋮----
assert_eq!(*bank1, roundtrip_bank);
⋮----
fn test_roundtrip_bank_to_and_from_snapshot_complex() {
⋮----
.transfer(4 * LAMPORTS_PER_SOL, &mint_keypair, &key4.pubkey())
⋮----
.transfer(5 * LAMPORTS_PER_SOL, &mint_keypair, &key5.pubkey())
⋮----
Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank1, &collector, slot);
⋮----
bank2.fill_bank_with_ticks_for_tests();
⋮----
Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank2, &collector, slot);
⋮----
bank3.fill_bank_with_ticks_for_tests();
⋮----
Bank::new_from_parent_with_bank_forks(bank_forks.as_ref(), bank3, &collector, slot);
⋮----
bank4.fill_bank_with_ticks_for_tests();
⋮----
assert_eq!(*bank4, roundtrip_bank);
⋮----
fn test_roundtrip_bank_to_and_from_incremental_snapshot() {
⋮----
let incremental_snapshot_archive_info = bank_to_incremental_snapshot_archive(
⋮----
Some(&incremental_snapshot_archive_info),
⋮----
fn test_bank_from_latest_snapshot_archives() {
⋮----
bank_to_full_snapshot_archive(
⋮----
bank_to_incremental_snapshot_archive(
⋮----
let (deserialized_bank, ..) = bank_from_latest_snapshot_archives(
⋮----
assert_eq!(deserialized_bank, *bank4);
⋮----
fn test_incremental_snapshots_handle_zero_lamport_accounts() {
⋮----
create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
vec![accounts_dir.clone()],
⋮----
.wrap_with_bank_forks_for_tests();
⋮----
.transfer(lamports_to_transfer, &mint_keypair, &key2.pubkey())
⋮----
.transfer(lamports_to_transfer, &key2, &key1.pubkey())
⋮----
let blockhash = bank2.last_blockhash();
⋮----
&key2.pubkey(),
⋮----
let fee = bank2.get_fee_for_message(tx.message()).unwrap();
⋮----
bank2.process_transaction(&tx).unwrap();
assert_eq!(
⋮----
let deserialized_bank = bank_from_snapshot_archives(
⋮----
bank4.squash();
bank4.clean_accounts();
assert!(
⋮----
fn test_bank_fields_from_snapshot(storage_access: StorageAccess) {
⋮----
let all_snapshots_dir = tempfile::TempDir::new().unwrap();
⋮----
let bank_fields = bank_fields_from_snapshot_archives(
⋮----
assert_eq!(bank_fields.slot, bank2.slot());
assert_eq!(bank_fields.parent_slot, bank2.parent_slot());
⋮----
fn test_bank_snapshot_dir_accounts_hardlinks() {
⋮----
let accounts_hardlinks_dir = get_bank_snapshot_dir(&bank_snapshots_dir, bank.slot())
.join(snapshot_paths::SNAPSHOT_ACCOUNTS_HARDLINKS);
assert!(fs::metadata(&accounts_hardlinks_dir).is_ok());
⋮----
for entry in fs::read_dir(accounts_hardlinks_dir).unwrap() {
let entry = entry.unwrap();
let symlink = entry.path();
let dst_path = fs::read_link(symlink).unwrap();
assert!(fs::metadata(&dst_path).is_ok());
hardlink_dirs.push(dst_path);
⋮----
let bank_snapshot_dir = get_bank_snapshot_dir(&bank_snapshots_dir, bank.slot());
assert!(purge_bank_snapshot(bank_snapshot_dir).is_ok());
assert!(hardlink_dirs.iter().all(|dir| fs::metadata(dir).is_err()));
⋮----
fn test_fastboot_versioning() {
⋮----
let _bank = create_snapshot_dirs_for_tests(&genesis_config, &bank_snapshots_dir, 3, true);
⋮----
full_snapshot_archives_dir: bank_snapshots_dir.as_ref().to_path_buf(),
incremental_snapshot_archives_dir: bank_snapshots_dir.as_ref().to_path_buf(),
⋮----
let snapshot = get_highest_loadable_bank_snapshot(&snapshot_config).unwrap();
assert_eq!(snapshot.slot, 3);
⋮----
.join(snapshot_paths::SNAPSHOT_FASTBOOT_VERSION_FILENAME);
let version = fs::read_to_string(&complete_flag_file).unwrap();
let version = Version::parse(&version).unwrap();
⋮----
fs::write(&complete_flag_file, new_version.to_string()).unwrap();
let new_snapshot = get_highest_loadable_bank_snapshot(&snapshot_config);
assert!(new_snapshot.is_none());
⋮----
.join(snapshot_paths::SNAPSHOT_VERSION_FILENAME);
fs::remove_file(complete_flag_file).unwrap();
⋮----
assert_eq!(snapshot.slot, 2);
⋮----
fs::remove_file(fastboot_version_file).unwrap();
let snapshot = get_highest_loadable_bank_snapshot(&snapshot_config);
assert!(snapshot.is_none());
⋮----
fn test_get_highest_bank_snapshot(should_flush_and_hard_link_storages: bool) {
⋮----
let _bank = create_snapshot_dirs_for_tests(
⋮----
let snapshot = get_highest_bank_snapshot(&bank_snapshots_dir).unwrap();
assert_eq!(snapshot.slot, 4);
⋮----
fs::remove_file(version_file).unwrap();
⋮----
assert!(snapshot_dir_4.exists());
⋮----
fs::remove_file(snapshot_version_file).unwrap();
⋮----
fs::remove_file(status_cache_file).unwrap();
⋮----
assert_eq!(snapshot.slot, 1);
⋮----
fn test_clean_orphaned_account_snapshot_dirs() {
⋮----
let _bank = create_snapshot_dirs_for_tests(&genesis_config, &bank_snapshots_dir, 2, true);
let snapshot_dir_slot_2 = bank_snapshots_dir.path().join("2");
⋮----
snapshot_dir_slot_2.join(snapshot_paths::SNAPSHOT_ACCOUNTS_HARDLINKS);
⋮----
.unwrap()
.map(|entry| {
let symlink = entry.unwrap().path();
fs::read_link(symlink).unwrap()
⋮----
.collect();
fs::remove_dir_all(snapshot_dir_slot_2).unwrap();
assert!(hardlink_dirs_slot_2
⋮----
.map(|dir| dir.parent().unwrap().parent().unwrap().to_path_buf())
⋮----
clean_orphaned_account_snapshot_dirs(&bank_snapshots_dir, &account_snapshot_paths).unwrap();
⋮----
fn test_clean_orphaned_account_snapshot_dirs_no_hard_link() {
⋮----
let _bank = create_snapshot_dirs_for_tests(&genesis_config, &bank_snapshots_dir, 2, false);
let bank_snapshot_dir = get_bank_snapshot_dir(&bank_snapshots_dir, 2);
assert!(fs::exists(&bank_snapshot_dir).unwrap());
⋮----
bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_ACCOUNTS_HARDLINKS);
assert!(!fs::exists(&bank_snapshot_accounts_hard_link_dir).unwrap());
clean_orphaned_account_snapshot_dirs(&bank_snapshots_dir, &[]).unwrap();
⋮----
fn test_purge_incomplete_bank_snapshots(should_flush_and_hard_link_storages: bool) {
⋮----
let bank_snapshot_dir = get_bank_snapshot_dir(&bank_snapshots_dir, slot);
let version_file = bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_VERSION_FILENAME);
⋮----
purge_incomplete_bank_snapshots(&bank_snapshots_dir);
⋮----
assert!(!bank_snapshot_dir.exists());
⋮----
fn test_snapshots_handle_zero_lamport_accounts(storage_access: StorageAccess) {
⋮----
.transfer(lamports_to_transfer, &mint_keypair, &key3.pubkey())
⋮----
bank1.squash();
bank1.force_flush_accounts_cache();
⋮----
.transfer(lamports_to_transfer - fee, &key1, &key2.pubkey())
⋮----
let accounts_dir = tempfile::TempDir::new().unwrap();
let other_bank_snapshots_dir = tempfile::TempDir::new().unwrap();
⋮----
&[accounts_dir.path().to_path_buf()],
other_bank_snapshots_dir.path(),
⋮----
assert_eq!(*bank3, deserialized_bank);
⋮----
fn test_fastboot_handle_zero_lamport_accounts(mark_obsolete_accounts: MarkObsoleteAccounts) {
⋮----
let (mut genesis_config, mint) = create_genesis_config(1_000_000 * LAMPORTS_PER_SOL);
⋮----
bank0.transfer(lamports, &mint, &key2.pubkey()).unwrap();
bank0.transfer(lamports, &mint, &key1.pubkey()).unwrap();
⋮----
bank1.transfer(lamports, &key1, &key2.pubkey()).unwrap();
assert_eq!(bank1.get_balance(&key1.pubkey()), 0,);
⋮----
bank2.transfer(lamports * 2, &key2, &mint.pubkey()).unwrap();
⋮----
assert_eq!(bank2.get_balance(&key2.pubkey()), 0);
⋮----
let bank_snapshot = get_highest_bank_snapshot(&bank_snapshots_dir).unwrap();
let deserialized_bank = bank_from_snapshot_dir(
⋮----
assert_eq!(deserialized_bank.get_balance(&key1.pubkey()), 0);
assert_eq!(deserialized_bank.get_balance(&key2.pubkey()), 0);
assert_eq!(*bank2, deserialized_bank);
⋮----
fn test_fastboot_missing_obsolete_accounts() {
⋮----
let bank = create_snapshot_dirs_for_tests(&genesis_config, &bank_snapshots_dir, 3, true);
⋮----
.join(snapshot_paths::SNAPSHOT_OBSOLETE_ACCOUNTS_FILENAME);
fs::remove_file(obsolete_accounts_file).unwrap();
bank_from_snapshot_dir(
⋮----
fn test_bank_from_snapshot_dir(storage_access: StorageAccess) {
⋮----
let bank_constructed = bank_from_snapshot_dir(
⋮----
assert_eq!(bank_constructed, bank);
⋮----
fs::read_dir(path).unwrap().for_each(|entry| {
let path = entry.unwrap().path();
let filename = path.file_name().unwrap();
⋮----
get_slot_and_append_vec_id(filename.to_str().unwrap()).unwrap();
⋮----
let next_id = bank.accounts().accounts_db.next_id.load(Ordering::Relaxed) as usize;
assert_eq!(max_id, next_id - 1);
⋮----
fn test_purge_all_bank_snapshots(should_flush_and_hard_link_storages: bool) {
⋮----
assert_eq!(get_bank_snapshots(&bank_snapshots_dir).len(), 10);
purge_all_bank_snapshots(&bank_snapshots_dir);
assert_eq!(get_bank_snapshots(&bank_snapshots_dir).len(), 0);
⋮----
fn test_purge_old_bank_snapshots(should_flush_and_hard_link_storages: bool) {
⋮----
purge_old_bank_snapshots(&bank_snapshots_dir, 2);
assert_eq!(get_bank_snapshots(&bank_snapshots_dir).len(), 2);
⋮----
purge_old_bank_snapshots(&bank_snapshots_dir, 0);
⋮----
fn test_purge_bank_snapshots_older_than_slot(should_flush_and_hard_link_storages: bool) {
⋮----
let bank_snapshots_before = get_bank_snapshots(&bank_snapshots_dir);
purge_bank_snapshots_older_than_slot(&bank_snapshots_dir, 0);
let bank_snapshots_after = get_bank_snapshots(&bank_snapshots_dir);
assert_eq!(bank_snapshots_before.len(), bank_snapshots_after.len());
purge_bank_snapshots_older_than_slot(&bank_snapshots_dir, 3);
⋮----
assert_eq!(bank_snapshots_before.len(), bank_snapshots_after.len() + 2);
purge_bank_snapshots_older_than_slot(&bank_snapshots_dir, 8);
⋮----
assert_eq!(bank_snapshots_before.len(), bank_snapshots_after.len() + 7);
purge_bank_snapshots_older_than_slot(&bank_snapshots_dir, Slot::MAX);
⋮----
assert_eq!(bank_snapshots_before.len(), bank_snapshots_after.len() + 9);
assert!(bank_snapshots_after.is_empty());
⋮----
fn test_purge_old_bank_snapshots_at_startup(should_flush_and_hard_link_storages: bool) {
⋮----
purge_old_bank_snapshots_at_startup(&bank_snapshots_dir);
let bank_snapshots = get_bank_snapshots(&bank_snapshots_dir);
assert_eq!(bank_snapshots.len(), 1);
assert_eq!(bank_snapshots.first().unwrap().slot, 9);
⋮----
fn test_verify_slot_deltas_structural_bad_too_many_entries() {
⋮----
.map(|slot| (slot, true, Status::default()))
⋮----
let result = verify_slot_deltas_structural(slot_deltas.as_slice(), bank_slot);
⋮----
fn test_verify_slot_deltas_structural_good() {
let slot_deltas = vec![
⋮----
fn test_verify_slot_deltas_structural_bad_slot_not_root() {
⋮----
assert_eq!(result, Err(VerifySlotDeltasError::SlotIsNotRoot(222)));
⋮----
fn test_verify_slot_deltas_structural_bad_slot_greater_than_bank() {
⋮----
fn test_verify_slot_deltas_structural_bad_slot_has_multiple_entries() {
⋮----
fn test_verify_slot_deltas_with_history_good() {
⋮----
slots_from_slot_deltas.insert(slot);
slot_history.add(slot);
⋮----
verify_slot_deltas_with_history(&slots_from_slot_deltas, &slot_history, bank_slot);
assert_eq!(result, Ok(()));
⋮----
fn test_verify_slot_deltas_with_history_bad_slot_not_in_history() {
⋮----
slot_history.add(444);
⋮----
fn test_verify_slot_deltas_with_history_bad_slot_not_in_deltas() {
⋮----
slot_history.add(222);
slot_history.add(333);
⋮----
fn test_verify_slot_history_good() {
⋮----
let result = verify_slot_history(&slot_history, bank_slot);
⋮----
fn test_verify_slot_history_bad_invalid_newest_slot() {
⋮----
assert_eq!(result, Err(VerifySlotHistoryError::InvalidNewestSlot));
⋮----
fn test_verify_slot_history_bad_invalid_num_entries() {
⋮----
slot_history.bits.truncate(slot_history.bits.len() - 1);
⋮----
assert_eq!(result, Err(VerifySlotHistoryError::InvalidNumEntries));
⋮----
fn test_verify_epoch_stakes_good() {
let bank = create_simple_test_bank(100 * LAMPORTS_PER_SOL);
assert_eq!(verify_epoch_stakes(&bank), Ok(()));
⋮----
fn test_verify_epoch_stakes_bad() {
⋮----
let mut epoch_stakes_map = bank.epoch_stakes_map().clone();
let invalid_epoch = *required_epochs.end() + 1;
epoch_stakes_map.insert(
⋮----
bank.epoch_stakes(bank.epoch()).cloned().unwrap(),
⋮----
for removed_epoch in required_epochs.clone() {
⋮----
let removed_stakes = epoch_stakes_map.remove(&removed_epoch);
assert!(removed_stakes.is_some());
⋮----
fn test_get_highest_loadable_bank_snapshot(snapshot_config: SnapshotConfig) {
let bank_snapshots_dir = TempDir::new().unwrap();
⋮----
full_snapshot_archives_dir: snapshot_archives_dir.as_ref().to_path_buf(),
incremental_snapshot_archives_dir: snapshot_archives_dir.as_ref().to_path_buf(),
⋮----
.get()
⋮----
let highest_bank_snapshot = get_highest_bank_snapshot(&bank_snapshots_dir).unwrap();
assert!(get_highest_loadable_bank_snapshot(&SnapshotConfig::default()).is_none());
assert!(get_highest_loadable_bank_snapshot(&snapshot_config).is_none());
⋮----
let bank_snapshot = get_highest_loadable_bank_snapshot(&snapshot_config).unwrap();
assert_eq!(bank_snapshot.slot, highest_bank_snapshot.slot);
fs::remove_dir_all(&highest_bank_snapshot.snapshot_dir).unwrap();
⋮----
snapshot_utils::mark_bank_snapshot_as_loadable(get_bank_snapshot_dir(
⋮----
assert_eq!(bank_snapshot.slot, highest_bank_snapshot.slot - 1);

================
File: runtime/src/snapshot_controller.rs
================
struct SnapshotGenerationIntervals {
⋮----
pub struct SnapshotController {
⋮----
impl SnapshotController {
pub fn new(
⋮----
pub fn snapshot_config(&self) -> &SnapshotConfig {
⋮----
pub fn request_sender(&self) -> &SnapshotRequestSender {
⋮----
fn latest_abs_request_slot(&self) -> Slot {
self.latest_abs_request_slot.load(Ordering::Relaxed)
⋮----
fn set_latest_abs_request_slot(&self, slot: Slot) {
self.latest_abs_request_slot.store(slot, Ordering::Relaxed);
⋮----
pub fn handle_new_roots(&self, root: Slot, banks: &[&Arc<Bank>]) -> (bool, SquashTiming, u64) {
⋮----
}) = self.snapshot_generation_intervals()
⋮----
if let Some((bank, request_kind)) = banks.iter().find_map(|bank| {
⋮----
bank.block_height() % snapshot_interval == 0
⋮----
if bank.slot() <= self.latest_abs_request_slot() {
⋮----
Some((bank, SnapshotRequestKind::FullSnapshot))
⋮----
Some((bank, SnapshotRequestKind::IncrementalSnapshot))
⋮----
let bank_slot = bank.slot();
self.set_latest_abs_request_slot(bank_slot);
squash_timing += bank.squash();
⋮----
let status_cache_slot_deltas = bank.status_cache.read().unwrap().root_slot_deltas();
if let Err(e) = self.abs_request_sender.send(SnapshotRequest {
⋮----
warn!("Error sending snapshot request for bank: {bank_slot}, err: {e:?}");
⋮----
snapshot_time.stop();
total_snapshot_ms += snapshot_time.as_ms();
⋮----
fn snapshot_generation_intervals(&self) -> Option<SnapshotGenerationIntervals> {
⋮----
.should_generate_snapshots()
.then_some(SnapshotGenerationIntervals {

================
File: runtime/src/snapshot_minimizer.rs
================
pub struct SnapshotMinimizer<'a> {
⋮----
pub fn minimize(
⋮----
minimizer.add_accounts(Self::get_active_bank_features, "active bank features");
minimizer.add_accounts(Self::get_inactive_bank_features, "inactive bank features");
minimizer.add_accounts(Self::get_static_runtime_accounts, "static runtime accounts");
minimizer.add_accounts(Self::get_reserved_accounts, "reserved accounts");
minimizer.add_accounts(Self::get_vote_accounts, "vote accounts");
minimizer.add_accounts(Self::get_stake_accounts, "stake accounts");
minimizer.add_accounts(Self::get_owner_accounts, "owner accounts");
minimizer.add_accounts(Self::get_programdata_accounts, "programdata accounts");
minimizer.minimize_accounts_db();
// Update accounts_cache and capitalization
minimizer.bank.force_flush_accounts_cache();
⋮----
.set_capitalization_for_tests(minimizer.bank.calculate_capitalization_for_tests());
⋮----
// Since the account state has changed, the accounts lt hash must be recalculated
⋮----
.accounts_db()
.calculate_accounts_lt_hash_at_startup_from_index(
⋮----
minimizer.bank.slot(),
⋮----
bank.set_accounts_lt_hash_for_snapshot_minimizer(new_accounts_lt_hash);
⋮----
/// Helper function to measure time and number of accounts added
    fn add_accounts<F>(&self, add_accounts_fn: F, name: &'static str)
⋮----
fn add_accounts<F>(&self, add_accounts_fn: F, name: &'static str)
⋮----
let initial_accounts_len = self.minimized_account_set.len();
let (_, measure) = measure_time!(add_accounts_fn(self), name);
let total_accounts_len = self.minimized_account_set.len();
⋮----
info!(
⋮----
/// Used to get active bank feature accounts in `minimize`.
    fn get_active_bank_features(&self) {
⋮----
fn get_active_bank_features(&self) {
⋮----
.active()
.iter()
.for_each(|(pubkey, _)| {
self.minimized_account_set.insert(*pubkey);
⋮----
/// Used to get inactive bank feature accounts in `minimize`
    fn get_inactive_bank_features(&self) {
⋮----
fn get_inactive_bank_features(&self) {
self.bank.feature_set.inactive().iter().for_each(|pubkey| {
⋮----
/// Used to get static runtime accounts in `minimize`
    fn get_static_runtime_accounts(&self) {
⋮----
fn get_static_runtime_accounts(&self) {
static_ids::STATIC_IDS.iter().for_each(|pubkey| {
⋮----
/// Used to get reserved accounts in `minimize`
    fn get_reserved_accounts(&self) {
⋮----
fn get_reserved_accounts(&self) {
ReservedAccountKeys::all_keys_iter().for_each(|pubkey| {
⋮----
/// Used to get vote and node pubkeys in `minimize`
    /// Add all pubkeys from vote accounts and nodes to `minimized_account_set`
⋮----
/// Add all pubkeys from vote accounts and nodes to `minimized_account_set`
    fn get_vote_accounts(&self) {
⋮----
fn get_vote_accounts(&self) {
⋮----
.vote_accounts()
.par_iter()
.for_each(|(pubkey, (_stake, vote_account))| {
⋮----
.insert(*vote_account.node_pubkey());
⋮----
/// Used to get stake accounts in `minimize`
    /// Add all pubkeys from stake accounts to `minimized_account_set`
⋮----
/// Add all pubkeys from stake accounts to `minimized_account_set`
    fn get_stake_accounts(&self) {
⋮----
fn get_stake_accounts(&self) {
self.bank.get_stake_accounts(&self.minimized_account_set);
⋮----
/// Used to get owner accounts in `minimize`
    /// For each account in `minimized_account_set` adds the owner account's pubkey to `minimized_account_set`.
⋮----
/// For each account in `minimized_account_set` adds the owner account's pubkey to `minimized_account_set`.
    fn get_owner_accounts(&self) {
⋮----
fn get_owner_accounts(&self) {
⋮----
.filter_map(|pubkey| self.bank.get_account(&pubkey))
.map(|account| *account.owner())
.collect();
owner_accounts.into_par_iter().for_each(|pubkey| {
self.minimized_account_set.insert(pubkey);
⋮----
fn get_programdata_accounts(&self) {
⋮----
.filter(|account| account.executable())
.filter(|account| bpf_loader_upgradeable::check_id(account.owner()))
.filter_map(|account| {
⋮----
}) = account.state()
⋮----
Some(programdata_address)
⋮----
programdata_accounts.into_par_iter().for_each(|pubkey| {
⋮----
fn minimize_accounts_db(&self) {
⋮----
measure_time!(self.get_minimized_slot_set(), "generate minimized slot set");
info!("{minimized_slot_set_measure}");
let ((dead_slots, dead_storages), process_snapshot_storages_measure) = measure_time!(
⋮----
info!("{process_snapshot_storages_measure}");
self.accounts_db()
⋮----
.store(false, Ordering::Relaxed);
⋮----
measure_time!(self.purge_dead_slots(dead_slots), "purge dead slots");
info!("{purge_dead_slots_measure}");
let (_, drop_storages_measure) = measure_time!(drop(dead_storages), "drop storages");
info!("{drop_storages_measure}");
⋮----
.store(true, Ordering::Relaxed);
⋮----
fn get_minimized_slot_set(&self) -> DashSet<Slot> {
⋮----
self.minimized_account_set.par_iter().for_each(|pubkey| {
⋮----
.get_and_then(&pubkey, |entry| {
⋮----
.slot_list_read_lock()
⋮----
.map(|(slot, _)| *slot)
.max();
⋮----
minimized_slot_set.insert(max_slot);
⋮----
fn process_snapshot_storages(
⋮----
let snapshot_storages = self.accounts_db().get_storages(..=self.starting_slot).0;
⋮----
snapshot_storages.into_par_iter().for_each(|storage| {
let slot = storage.slot();
⋮----
if minimized_slot_set.contains(&slot) {
self.filter_storage(&storage, &dead_storages);
⋮----
dead_slots.lock().unwrap().push(slot);
⋮----
let dead_slots = dead_slots.into_inner().unwrap();
let dead_storages = dead_storages.into_inner().unwrap();
⋮----
fn filter_storage(
⋮----
} = self.accounts_db().get_unique_accounts_from_storage(storage);
let keep_accounts_collect = Mutex::new(Vec::with_capacity(stored_accounts.len()));
let purge_pubkeys_collect = Mutex::new(Vec::with_capacity(stored_accounts.len()));
⋮----
stored_accounts.par_chunks(CHUNK_SIZE).for_each(|chunk| {
⋮----
chunk.iter().for_each(|account| {
if self.minimized_account_set.contains(account.pubkey()) {
chunk_bytes += account.stored_size();
keep_accounts.push(account);
} else if self.accounts_db().accounts_index.contains(account.pubkey()) {
purge_pubkeys.push(account.pubkey());
⋮----
.lock()
.unwrap()
.append(&mut keep_accounts);
⋮----
.append(&mut purge_pubkeys);
total_bytes_collect.fetch_add(chunk_bytes, Ordering::Relaxed);
⋮----
let keep_accounts = keep_accounts_collect.into_inner().unwrap();
let remove_pubkeys = purge_pubkeys_collect.into_inner().unwrap();
let total_bytes = total_bytes_collect.load(Ordering::Relaxed);
let purge_pubkeys = remove_pubkeys.into_iter().map(|pubkey| (*pubkey, slot));
let _ = self.accounts_db().purge_keys_exact(purge_pubkeys);
⋮----
shrink_in_progress = Some(
⋮----
.get_store_for_shrink(slot, total_bytes as u64),
⋮----
let new_storage = shrink_in_progress.as_ref().unwrap().new_storage();
⋮----
StorableAccountsBySlot::new(slot, &accounts, self.accounts_db());
self.accounts_db().store_accounts_frozen(
⋮----
new_storage.flush().unwrap();
⋮----
let mut dead_storages_this_time = self.accounts_db().mark_dirty_dead_stores(
⋮----
.append(&mut dead_storages_this_time);
⋮----
fn purge_dead_slots(&self, dead_slots: Vec<Slot>) {
⋮----
.purge_slots_from_cache_and_store(dead_slots.iter(), &stats);
⋮----
fn accounts_db(&self) -> &AccountsDb {
⋮----
mod tests {
⋮----
fn test_minimization_get_vote_accounts() {
⋮----
let genesis_config_info = create_genesis_config_with_leader(
⋮----
minimizer.get_vote_accounts();
assert!(minimizer
⋮----
fn test_minimization_get_stake_accounts() {
⋮----
minimizer.get_stake_accounts();
⋮----
.filter_map(|(pubkey, account)| {
stake::program::check_id(account.owner()).then_some(*pubkey)
⋮----
expected_stake_accounts.push(bootstrap_validator_pubkey);
assert_eq!(
⋮----
assert!(minimizer.minimized_account_set.contains(&stake_pubkey));
⋮----
fn test_minimization_get_owner_accounts() {
⋮----
let (genesis_config, _) = create_genesis_config(1_000_000);
⋮----
bank.store_account(&pubkey, &AccountSharedData::new(1, 0, &owner_pubkey));
⋮----
owner_accounts.insert(pubkey);
⋮----
minimizer.get_owner_accounts();
assert!(minimizer.minimized_account_set.contains(&pubkey));
assert!(minimizer.minimized_account_set.contains(&owner_pubkey));
⋮----
fn test_minimization_add_programdata_accounts() {
⋮----
AccountSharedData::new_data(40, &program, &bpf_loader_upgradeable::id()).unwrap();
program_account.set_executable(true);
bank.store_account(&non_program_id, &non_program_account);
bank.store_account(&program_id, &program_account);
⋮----
programdata_accounts.insert(non_program_id);
⋮----
minimizer.get_programdata_accounts();
assert_eq!(minimizer.minimized_account_set.len(), 1);
assert!(minimizer.minimized_account_set.contains(&non_program_id));
minimizer.minimized_account_set.insert(program_id);
⋮----
assert_eq!(minimizer.minimized_account_set.len(), 3);
⋮----
assert!(minimizer.minimized_account_set.contains(&program_id));
⋮----
fn test_minimize_accounts_db() {
⋮----
let accounts = &bank.accounts().accounts_db;
⋮----
.map(|_| solana_pubkey::new_rand())
⋮----
let owner = *AccountSharedData::default().owner();
⋮----
for (index, pubkey) in pubkeys.iter().enumerate() {
accounts.store_for_tests((current_slot, [(pubkey, &account)].as_slice()));
⋮----
minimized_account_set.insert(*pubkey);
⋮----
accounts.add_root_and_flush_write_cache(current_slot);
⋮----
assert_eq!(minimized_account_set.len(), 6);
⋮----
let snapshot_storages = accounts.get_storages(..=current_slot).0;
assert_eq!(snapshot_storages.len(), 3);
⋮----
snapshot_storages.into_iter().for_each(|storage| {
⋮----
.scan_pubkeys(|_| {
⋮----
.expect("must scan accounts storage");
⋮----
fn test_minimize_and_recalculate_accounts_lt_hash(should_recalculate_accounts_lt_hash: bool) {
⋮----
let slot = bank.slot() + 1;
⋮----
.write()
⋮----
.insert(bank)
.clone_without_scheduler();
bank.register_unique_recent_blockhash_for_test();
bank.transfer(
⋮----
.unwrap();
⋮----
bank.fill_bank_with_ticks_for_tests();
bank.squash();
bank.force_flush_accounts_cache();
⋮----
bank.slot(),
⋮----
let bank_snapshots_dir = TempDir::new().unwrap();
let snapshot_archives_dir = TempDir::new().unwrap();
⋮----
Some(snapshot_config.snapshot_version),
⋮----
assert_eq!(roundtrip_bank, *bank);

================
File: runtime/src/snapshot_package.rs
================
use solana_hash::Hash;
⋮----
mod compare;
⋮----
pub struct SnapshotPackage {
⋮----
impl SnapshotPackage {
pub fn new(
⋮----
let slot = bank.slot();
⋮----
assert!(
⋮----
let bank_fields_to_serialize = bank.get_fields_to_serialize();
let hash = SnapshotHash::new(bank_fields_to_serialize.accounts_lt_hash.0.checksum());
⋮----
bank_hash_stats: bank.get_bank_hash_stats(),
⋮----
.load(Ordering::Acquire),
⋮----
pub fn default_for_tests() -> Self {
⋮----
hash: SnapshotHash(Hash::default()),
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
f.debug_struct("SnapshotPackage")
.field("kind", &self.snapshot_kind)
.field("slot", &self.slot)
.finish_non_exhaustive()
⋮----
pub struct BankSnapshotPackage {

================
File: runtime/src/snapshot_utils.rs
================
use solana_accounts_db::utils::create_accounts_run_and_snapshot_dirs;
⋮----
pub mod snapshot_storage_rebuilder;
⋮----
pub struct BankSnapshotInfo {
⋮----
impl PartialOrd for BankSnapshotInfo {
fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
Some(self.cmp(other))
⋮----
impl Ord for BankSnapshotInfo {
fn cmp(&self, other: &Self) -> Ordering {
self.slot.cmp(&other.slot)
⋮----
impl BankSnapshotInfo {
pub fn new_from_dir(
⋮----
if !bank_snapshot_dir.is_dir() {
return Err(SnapshotNewFromDirError::InvalidBankSnapshotDir(
⋮----
let version_path = bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_VERSION_FILENAME);
let version_str = snapshot_version_from_file(&version_path).map_err(|err| {
SnapshotNewFromDirError::IncompleteDir(err, bank_snapshot_dir.clone())
⋮----
let snapshot_version = SnapshotVersion::from_str(version_str.as_str())
.or(Err(SnapshotNewFromDirError::InvalidVersion(version_str)))?;
⋮----
bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_STATUS_CACHE_FILENAME);
if !status_cache_file.is_file() {
return Err(SnapshotNewFromDirError::MissingStatusCacheFile(
⋮----
bank_snapshot_dir.join(snapshot_paths::get_snapshot_file_name(slot));
if !bank_snapshot_path.is_file() {
return Err(SnapshotNewFromDirError::MissingSnapshotFile(
⋮----
bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_FASTBOOT_VERSION_FILENAME);
⋮----
.ok()
.map(|version_string| {
Version::from_str(version_string.trim())
.map_err(|_| SnapshotNewFromDirError::InvalidFastbootVersion(version_string))
⋮----
.transpose()?;
Ok(BankSnapshotInfo {
⋮----
pub fn snapshot_path(&self) -> PathBuf {
⋮----
.join(snapshot_paths::get_snapshot_file_name(self.slot))
⋮----
pub enum SnapshotFrom {
⋮----
pub struct SnapshotRootPaths {
⋮----
pub struct UnarchivedSnapshot {
⋮----
pub struct UnarchivedSnapshots {
⋮----
pub struct UnarchivedSnapshotsGuard {
⋮----
pub struct UnpackedSnapshotsDirAndVersion {
⋮----
pub(crate) struct StorageAndNextAccountsFileId {
⋮----
pub fn clean_orphaned_account_snapshot_dirs(
⋮----
let snapshots = get_bank_snapshots(bank_snapshots_dir);
⋮----
.join(snapshot_paths::SNAPSHOT_ACCOUNTS_HARDLINKS);
⋮----
debug!(
⋮----
let path = entry?.path();
let target = fs::read_link(&path).map_err(|err| {
IoError::other(format!(
⋮----
account_snapshot_dirs_referenced.insert(target);
⋮----
let read_dir = fs::read_dir(account_snapshot_path).map_err(|err| {
⋮----
if !account_snapshot_dirs_referenced.contains(&path) {
info!(
⋮----
move_and_async_delete_path(&path);
⋮----
Ok(())
⋮----
pub fn purge_incomplete_bank_snapshots(bank_snapshots_dir: impl AsRef<Path>) {
⋮----
let is_incomplete = |dir: &PathBuf| !is_bank_snapshot_complete(dir);
⋮----
.filter_map(|entry| entry.ok())
.map(|entry| entry.path())
.filter(|path| path.is_dir())
.filter(is_incomplete)
.collect();
⋮----
let result = purge_bank_snapshot(&incomplete_dir);
⋮----
Ok(_) => info!(
⋮----
Err(err) => warn!("Failed to purge incomplete snapshot dir: {err}"),
⋮----
fn is_bank_snapshot_complete(bank_snapshot_dir: impl AsRef<Path>) -> bool {
⋮----
.as_ref()
.join(snapshot_paths::SNAPSHOT_VERSION_FILENAME);
version_path.is_file()
⋮----
pub fn mark_bank_snapshot_as_loadable(bank_snapshot_dir: impl AsRef<Path>) -> io::Result<()> {
⋮----
.join(snapshot_paths::SNAPSHOT_FASTBOOT_VERSION_FILENAME);
⋮----
SNAPSHOT_FASTBOOT_VERSION.to_string(),
⋮----
.map_err(|err| {
⋮----
fn is_bank_snapshot_loadable(
⋮----
is_snapshot_fastboot_compatible(fastboot_version)
⋮----
Ok(false)
⋮----
fn is_snapshot_fastboot_compatible(
⋮----
Ok(true)
⋮----
Err(SnapshotFastbootError::IncompatibleVersion(version.clone()))
⋮----
pub fn get_highest_loadable_bank_snapshot(
⋮----
let highest_bank_snapshot = get_highest_bank_snapshot(&snapshot_config.bank_snapshots_dir)?;
⋮----
is_bank_snapshot_loadable(highest_bank_snapshot.fastboot_version.as_ref());
⋮----
Ok(true) => Some(highest_bank_snapshot),
⋮----
warn!(
⋮----
pub fn remove_tmp_snapshot_archives(snapshot_archives_dir: impl AsRef<Path>) {
⋮----
for entry in entries.flatten() {
⋮----
.file_name()
.to_str()
.map(|file_name| file_name.starts_with(snapshot_paths::TMP_SNAPSHOT_ARCHIVE_PREFIX))
.unwrap_or(false)
⋮----
let path = entry.path();
let result = if path.is_dir() {
⋮----
pub fn serialize_and_archive_snapshot_package(
⋮----
let bank_snapshot_info = serialize_snapshot(
⋮----
snapshot_storages.retain(|storage| storage.slot() > incremental_snapshot_base_slot);
⋮----
let snapshot_archive_info = archive_snapshot(
⋮----
snapshot_storages.as_slice(),
⋮----
Ok(snapshot_archive_info)
⋮----
fn serialize_snapshot(
⋮----
let status_cache_slot_deltas = status_cache_slot_deltas.as_slice();
⋮----
if bank_snapshot_dir.exists() {
return Err(AddBankSnapshotError::SnapshotDirAlreadyExists(
⋮----
fs::create_dir_all(&bank_snapshot_dir).map_err(|err| {
AddBankSnapshotError::CreateSnapshotDir(err, bank_snapshot_dir.clone())
⋮----
// the bank snapshot is stored as bank_snapshots_dir/slot/slot
⋮----
accounts_lt_hash: Some(bank_fields.accounts_lt_hash.clone().into()),
⋮----
&get_storages_to_serialize(snapshot_storages),
⋮----
let (bank_snapshot_consumed_size, bank_serialize) = measure_time!(
⋮----
let (status_cache_consumed_size, status_cache_serialize_us) = measure_us!(
⋮----
let (_, write_version_file_us) = measure_us!(fs::write(
⋮----
storage.flush().map_err(|err| {
AddBankSnapshotError::FlushStorage(err, storage.path().to_path_buf())
⋮----
let flush_us = flush_measure.end_as_us();
let (_, hard_link_us) = measure_us!(hard_link_storages_to_snapshot(
⋮----
let (_, serialize_obsolete_accounts_us) = measure_us!({
⋮----
mark_bank_snapshot_as_loadable(&bank_snapshot_dir)
.map_err(AddBankSnapshotError::MarkSnapshotLoadable)?;
⋮----
Some(flush_us),
Some(hard_link_us),
Some(serialize_obsolete_accounts_us),
⋮----
measure_everything.stop();
// Monitor sizes because they're capped to MAX_SNAPSHOT_DATA_FILE_SIZE
datapoint_info!(
⋮----
do_serialize_snapshot().map_err(|err| SnapshotError::AddBankSnapshot(err, slot))
⋮----
pub fn get_bank_snapshots(bank_snapshots_dir: impl AsRef<Path>) -> Vec<BankSnapshotInfo> {
⋮----
.filter_map(|entry| {
⋮----
.filter(|entry| entry.path().is_dir())
.and_then(|entry| {
⋮----
.path()
⋮----
.and_then(|file_name| file_name.to_str())
.and_then(|file_name| file_name.parse::<Slot>().ok())
⋮----
.for_each(
⋮----
Ok(snapshot_info) => bank_snapshots.push(snapshot_info),
Err(err) => debug!("Unable to read bank snapshot for slot {slot}: {err}"),
⋮----
pub fn get_highest_bank_snapshot(bank_snapshots_dir: impl AsRef<Path>) -> Option<BankSnapshotInfo> {
do_get_highest_bank_snapshot(get_bank_snapshots(&bank_snapshots_dir))
⋮----
fn do_get_highest_bank_snapshot(
⋮----
bank_snapshots.sort_unstable();
bank_snapshots.into_iter().next_back()
⋮----
pub fn write_obsolete_accounts_to_snapshot(
⋮----
serialize_obsolete_accounts(
⋮----
fn serialize_obsolete_accounts(
⋮----
.join(snapshot_paths::SNAPSHOT_OBSOLETE_ACCOUNTS_FILENAME);
⋮----
file_stream.flush()?;
let consumed_size = file_stream.stream_position()?;
⋮----
let error_message = format!(
⋮----
return Err(IoError::other(error_message).into());
⋮----
Ok(consumed_size)
⋮----
fn deserialize_obsolete_accounts(
⋮----
if obsolete_accounts_file_metadata.len() > maximum_obsolete_accounts_file_size {
⋮----
Ok(obsolete_accounts)
⋮----
pub fn serialize_snapshot_data_file<F>(data_file_path: &Path, serializer: F) -> Result<u64>
⋮----
pub fn deserialize_snapshot_data_file<T: Sized>(
⋮----
deserializer(streams.full_snapshot_stream)
⋮----
full_snapshot_root_file_path: data_file_path.to_path_buf(),
⋮----
deserialize_snapshot_data_files_capped(
⋮----
pub fn deserialize_snapshot_data_files<T: Sized>(
⋮----
fn serialize_snapshot_data_file_capped<F>(
⋮----
serializer(&mut data_file_stream)?;
data_file_stream.flush()?;
let consumed_size = data_file_stream.stream_position()?;
⋮----
fn deserialize_snapshot_data_files_capped<T: Sized>(
⋮----
create_snapshot_data_file_stream(
⋮----
Some(create_snapshot_data_file_stream(
⋮----
.unzip();
⋮----
incremental_snapshot_stream: incremental_snapshot_data_file_stream.as_mut(),
⋮----
let ret = deserializer(&mut snapshot_streams)?;
check_deserialize_file_consumed(
⋮----
incremental_snapshot_file_size.unwrap(),
⋮----
incremental_snapshot_data_file_stream.as_mut().unwrap(),
⋮----
Ok(ret)
⋮----
fn create_snapshot_data_file_stream(
⋮----
let snapshot_file_size = fs::metadata(&snapshot_root_file_path)?.len();
⋮----
Ok((snapshot_file_size, snapshot_data_file_stream))
⋮----
fn check_deserialize_file_consumed(
⋮----
fn get_account_path_from_appendvec_path(appendvec_path: &Path) -> Option<PathBuf> {
let run_path = appendvec_path.parent()?;
let run_file_name = run_path.file_name()?;
⋮----
error!(
⋮----
let account_path = run_path.parent()?;
Some(account_path.to_path_buf())
⋮----
fn get_snapshot_accounts_hardlink_dir(
⋮----
let account_path = get_account_path_from_appendvec_path(appendvec_path).ok_or_else(|| {
GetSnapshotAccountsHardLinkDirError::GetAccountPath(appendvec_path.to_path_buf())
⋮----
.join(ACCOUNTS_SNAPSHOT_DIR)
.join(bank_slot.to_string());
if !account_paths.contains(&account_path) {
let idx = account_paths.len();
⋮----
fs::create_dir_all(&snapshot_hardlink_dir).map_err(|err| {
⋮----
snapshot_hardlink_dir.clone(),
⋮----
let symlink_path = hardlinks_dir.as_ref().join(format!("account_path_{idx}"));
symlink::symlink_dir(&snapshot_hardlink_dir, &symlink_path).map_err(|err| {
⋮----
original: snapshot_hardlink_dir.clone(),
⋮----
account_paths.insert(account_path);
⋮----
Ok(snapshot_hardlink_dir)
⋮----
pub fn hard_link_storages_to_snapshot(
⋮----
fs::create_dir_all(&accounts_hardlinks_dir).map_err(|err| {
⋮----
accounts_hardlinks_dir.clone(),
⋮----
let storage_path = storage.accounts.path();
let snapshot_hardlink_dir = get_snapshot_accounts_hardlink_dir(
⋮----
let hardlink_filename = AccountsFile::file_name(storage.slot(), storage.id());
let hard_link_path = snapshot_hardlink_dir.join(hardlink_filename);
fs::hard_link(storage_path, &hard_link_path).map_err(|err| {
⋮----
storage_path.to_path_buf(),
⋮----
pub(crate) fn get_storages_to_serialize(
⋮----
.iter()
.map(|storage| vec![Arc::clone(storage)])
⋮----
pub fn verify_and_unarchive_snapshots(
⋮----
check_are_snapshots_compatible(
⋮----
} = unarchive_snapshot(
⋮----
full_snapshot_archive_info.path(),
⋮----
full_snapshot_archive_info.archive_format(),
next_append_vec_id.clone(),
⋮----
incremental_snapshot_archive_info.path(),
⋮----
incremental_snapshot_archive_info.archive_format(),
⋮----
Some(unpack_dir),
Some(storage),
Some(bank_fields),
Some(accounts_db_fields),
Some(unpacked_snapshots_dir_and_version),
Some(measure_untar),
⋮----
let next_append_vec_id = Arc::try_unwrap(next_append_vec_id).unwrap();
Ok((
⋮----
enum SnapshotFileKind {
⋮----
fn get_snapshot_file_kind(filename: &str) -> Option<SnapshotFileKind> {
⋮----
LazyLock::new(|| Regex::new(r"^version$").unwrap());
⋮----
LazyLock::new(|| Regex::new(r"^[0-9]+(\.pre)?$").unwrap());
if VERSION_FILE_REGEX.is_match(filename) {
Some(SnapshotFileKind::Version)
} else if BANK_FIELDS_FILE_REGEX.is_match(filename) {
Some(SnapshotFileKind::BankFields)
} else if get_slot_and_append_vec_id(filename).is_ok() {
Some(SnapshotFileKind::Storage)
⋮----
/// Waits for snapshot file
/// Due to parallel unpacking, we may receive some append_vec files before the snapshot file
⋮----
/// Due to parallel unpacking, we may receive some append_vec files before the snapshot file
/// This function will push append_vec files into a buffer until we receive the snapshot file
⋮----
/// This function will push append_vec files into a buffer until we receive the snapshot file
fn get_version_and_snapshot_files(
⋮----
fn get_version_and_snapshot_files(
⋮----
if let Ok(path) = file_receiver.recv() {
let filename = path.file_name().unwrap().to_str().unwrap();
match get_snapshot_file_kind(filename) {
⋮----
snapshot_version_path = Some(path);
// break if we have both the snapshot file and the version file
if snapshot_file_path.is_some() {
⋮----
snapshot_file_path = Some(path);
⋮----
if snapshot_version_path.is_some() {
⋮----
append_vec_files.push(path);
⋮----
None => {} // do nothing for other kinds of files
⋮----
return Err(SnapshotError::RebuildStorages(
"did not receive snapshot file from unpacking threads".to_string(),
⋮----
let snapshot_version_path = snapshot_version_path.unwrap();
let snapshot_file_path = snapshot_file_path.unwrap();
Ok((snapshot_version_path, snapshot_file_path, append_vec_files))
⋮----
struct SnapshotFieldsBundle {
⋮----
fn snapshot_fields_from_files(file_receiver: &Receiver<PathBuf>) -> Result<SnapshotFieldsBundle> {
⋮----
get_version_and_snapshot_files(file_receiver)?;
let snapshot_version_str = snapshot_version_from_file(snapshot_version_path)?;
let snapshot_version = snapshot_version_str.parse().map_err(|err| {
⋮----
let snapshot_file = fs::File::open(snapshot_file_path).unwrap();
⋮----
Ok(SnapshotFieldsBundle {
⋮----
fn create_snapshot_meta_files_for_unarchived_snapshot(unpack_dir: impl AsRef<Path>) -> Result<()> {
let snapshots_dir = unpack_dir.as_ref().join(snapshot_paths::BANK_SNAPSHOTS_DIR);
if !snapshots_dir.is_dir() {
return Err(SnapshotError::NoSnapshotSlotDir(snapshots_dir));
⋮----
.map_err(|_| SnapshotError::NoSnapshotSlotDir(snapshots_dir.clone()))?
.find(|entry| entry.as_ref().unwrap().path().is_dir())
.ok_or_else(|| SnapshotError::NoSnapshotSlotDir(snapshots_dir.clone()))?
⋮----
.path();
⋮----
slot_dir.join(snapshot_paths::SNAPSHOT_VERSION_FILENAME),
⋮----
let status_cache_file = snapshots_dir.join(snapshot_paths::SNAPSHOT_STATUS_CACHE_FILENAME);
⋮----
slot_dir.join(snapshot_paths::SNAPSHOT_STATUS_CACHE_FILENAME),
⋮----
fn unarchive_snapshot(
⋮----
.prefix(unpacked_snapshots_dir_prefix)
.tempdir_in(bank_snapshots_dir)?;
let unpacked_snapshots_dir = unpack_dir.path().join(snapshot_paths::BANK_SNAPSHOTS_DIR);
⋮----
let unarchive_handle = streaming_unarchive_snapshot(
⋮----
account_paths.to_vec(),
unpack_dir.path().to_path_buf(),
snapshot_archive_path.as_ref().to_path_buf(),
⋮----
let num_rebuilder_threads = num_cpus::get_physical().saturating_sub(1).max(1);
let snapshot_result = snapshot_fields_from_files(&file_receiver).and_then(
⋮----
let (storage, measure_untar) = measure_time!(
⋮----
info!("{measure_untar}");
create_snapshot_meta_files_for_unarchived_snapshot(&unpack_dir)?;
Ok(UnarchivedSnapshot {
⋮----
unarchive_handle.join().unwrap()?;
⋮----
fn streaming_snapshot_dir_files(
⋮----
file_sender.send(snapshot_file_path.into())?;
file_sender.send(snapshot_version_path.into())?;
⋮----
file_sender.send(file?.path())?;
⋮----
pub fn rebuild_storages_from_snapshot_dir(
⋮----
let accounts_hardlinks = bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_ACCOUNTS_HARDLINKS);
⋮----
.is_some_and(|fastboot_version| fastboot_version.major >= 2)
.then(|| deserialize_obsolete_accounts(bank_snapshot_dir, MAX_OBSOLETE_ACCOUNTS_FILE_SIZE))
.transpose()
⋮----
let read_dir = fs::read_dir(&accounts_hardlinks).map_err(|err| {
⋮----
let symlink_path = dir_entry?.path();
let account_snapshot_path = fs::read_link(&symlink_path).map_err(|err| {
⋮----
.parent()
.ok_or_else(|| SnapshotError::InvalidAccountPath(account_snapshot_path.clone()))?
⋮----
.join(ACCOUNTS_RUN_DIR);
if !account_run_paths.contains(&account_run_path) {
return Err(SnapshotError::AccountPathsMismatch);
⋮----
let read_dir = fs::read_dir(&account_snapshot_path).map_err(|err| {
⋮----
let file_path = file?.path();
⋮----
.ok_or_else(|| SnapshotError::InvalidAppendVecPath(file_path.to_path_buf()))?;
let dest_path = account_run_path.join(file_name);
fs::hard_link(&file_path, &dest_path).map_err(|err| {
⋮----
let snapshot_file_path = &snapshot_info.snapshot_path();
let snapshot_version_path = bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_VERSION_FILENAME);
streaming_snapshot_dir_files(
⋮----
} = snapshot_fields_from_files(&file_receiver)?;
⋮----
Ok((storage, bank_fields, accounts_db_fields))
⋮----
fn snapshot_version_from_file(path: impl AsRef<Path>) -> io::Result<String> {
let file_metadata = fs::metadata(&path).map_err(|err| {
⋮----
let file_size = file_metadata.len();
⋮----
return Err(IoError::other(error_message));
⋮----
let mut file = fs::File::open(&path).map_err(|err| {
⋮----
file.read_to_string(&mut snapshot_version).map_err(|err| {
⋮----
Ok(snapshot_version.trim().to_string())
⋮----
fn check_are_snapshots_compatible(
⋮----
if incremental_snapshot_archive_info.is_none() {
return Ok(());
⋮----
let incremental_snapshot_archive_info = incremental_snapshot_archive_info.unwrap();
(full_snapshot_archive_info.slot() == incremental_snapshot_archive_info.base_slot())
.then_some(())
.ok_or_else(|| {
⋮----
full_snapshot_archive_info.slot(),
incremental_snapshot_archive_info.base_slot(),
⋮----
pub fn purge_old_snapshot_archives(
⋮----
full_snapshot_archives.sort_unstable();
full_snapshot_archives.reverse();
⋮----
.len()
.min(maximum_full_snapshot_archives_to_retain.get());
trace!(
⋮----
if full_snapshot_archives.is_empty() {
⋮----
Some(full_snapshot_archives.split_at(num_to_retain))
⋮----
.unwrap_or_default();
⋮----
.map(|ai| ai.slot())
⋮----
fn remove_archives<T: SnapshotArchiveInfoGetter>(archives: &[T]) {
for path in archives.iter().map(|a| a.path()) {
trace!("Removing snapshot archive: {}", path.display());
⋮----
remove_archives(full_snapshot_archives_to_remove);
⋮----
get_incremental_snapshot_archives(&incremental_snapshot_archives_dir)
⋮----
.entry(incremental_snapshot_archive.base_slot())
.or_default()
.push(incremental_snapshot_archive)
⋮----
let highest_full_snapshot_slot = retained_full_snapshot_slots.iter().max().copied();
⋮----
incremental_snapshot_archives.sort_unstable();
let num_to_retain = if Some(base_slot) == highest_full_snapshot_slot {
maximum_incremental_snapshot_archives_to_retain.get()
⋮----
usize::from(retained_full_snapshot_slots.contains(&base_slot))
⋮----
incremental_snapshot_archives.truncate(
⋮----
.saturating_sub(num_to_retain),
⋮----
remove_archives(&incremental_snapshot_archives);
⋮----
pub fn verify_unpacked_snapshots_dir_and_version(
⋮----
get_bank_snapshots(&unpacked_snapshots_dir_and_version.unpacked_snapshots_dir);
if bank_snapshots.len() > 1 {
return Err(IoError::other(format!(
⋮----
.into());
⋮----
let root_paths = bank_snapshots.pop().ok_or_else(|| {
⋮----
Ok((snapshot_version, root_paths))
⋮----
pub enum VerifyBank {
⋮----
pub fn purge_all_bank_snapshots(bank_snapshots_dir: impl AsRef<Path>) {
let bank_snapshots = get_bank_snapshots(&bank_snapshots_dir);
purge_bank_snapshots(&bank_snapshots);
⋮----
pub fn purge_old_bank_snapshots(
⋮----
let mut bank_snapshots = get_bank_snapshots(&bank_snapshots_dir);
⋮----
purge_bank_snapshots(
⋮----
.rev()
.skip(num_bank_snapshots_to_retain),
⋮----
pub fn purge_old_bank_snapshots_at_startup(bank_snapshots_dir: impl AsRef<Path>) {
purge_old_bank_snapshots(&bank_snapshots_dir, 1);
let highest_bank_snapshot = get_highest_bank_snapshot(&bank_snapshots_dir);
⋮----
pub fn purge_bank_snapshots_older_than_slot(bank_snapshots_dir: impl AsRef<Path>, slot: Slot) {
⋮----
bank_snapshots.retain(|bank_snapshot| bank_snapshot.slot < slot);
⋮----
fn purge_bank_snapshots<'a>(bank_snapshots: impl IntoIterator<Item = &'a BankSnapshotInfo>) {
for snapshot_dir in bank_snapshots.into_iter().map(|s| &s.snapshot_dir) {
if purge_bank_snapshot(snapshot_dir).is_err() {
warn!("Failed to purge bank snapshot: {}", snapshot_dir.display());
⋮----
pub fn purge_bank_snapshot(bank_snapshot_dir: impl AsRef<Path>) -> Result<()> {
⋮----
if accounts_hardlinks_dir.is_dir() {
let read_dir = fs::read_dir(&accounts_hardlinks_dir).map_err(|err| {
⋮----
let accounts_hardlink_dir = entry?.path();
let accounts_hardlink_dir = fs::read_link(&accounts_hardlink_dir).map_err(|err| {
⋮----
move_and_async_delete_path(&accounts_hardlink_dir);
⋮----
fs::remove_dir_all(&bank_snapshot_dir).map_err(|err| {
⋮----
pub fn should_take_full_snapshot(
⋮----
block_height.is_multiple_of(full_snapshot_archive_interval_slots)
⋮----
pub fn should_take_incremental_snapshot(
⋮----
block_height.is_multiple_of(incremental_snapshot_archive_interval_slots)
&& latest_full_snapshot_slot.is_some()
⋮----
pub fn create_tmp_accounts_dir_for_tests() -> (TempDir, PathBuf) {
let tmp_dir = tempfile::TempDir::new().unwrap();
let account_dir = create_accounts_run_and_snapshot_dirs(&tmp_dir).unwrap().0;
⋮----
mod tests {
⋮----
fn test_serialize_snapshot_data_file_under_limit() {
let temp_dir = tempfile::TempDir::new().unwrap();
⋮----
let consumed_size = serialize_snapshot_data_file_capped(
&temp_dir.path().join("data-file"),
⋮----
serialize_into(stream, &2323_u32)?;
⋮----
.unwrap();
assert_eq!(consumed_size, expected_consumed_size);
⋮----
fn test_serialize_snapshot_data_file_over_limit() {
⋮----
let result = serialize_snapshot_data_file_capped(
⋮----
assert_matches!(result, Err(SnapshotError::Io(ref message)) if message.to_string().starts_with("too large snapshot data file to serialize"));
⋮----
fn test_deserialize_snapshot_data_file_under_limit() {
⋮----
serialize_snapshot_data_file_capped(
⋮----
serialize_into(stream, &expected_data)?;
⋮----
full_snapshot_root_file_path: temp_dir.path().join("data-file"),
⋮----
let actual_data = deserialize_snapshot_data_files_capped(
⋮----
Ok(deserialize_from::<_, u32>(
⋮----
assert_eq!(actual_data, expected_data);
⋮----
fn test_deserialize_snapshot_data_file_over_limit() {
⋮----
let result = deserialize_snapshot_data_files_capped(
⋮----
assert_matches!(result, Err(SnapshotError::Io(ref message)) if message.to_string().starts_with("too large snapshot data file to deserialize"));
⋮----
fn test_deserialize_snapshot_data_file_extra_data() {
⋮----
serialize_into(stream.by_ref(), &expected_data)?;
⋮----
assert_matches!(result, Err(SnapshotError::Io(ref message)) if message.to_string().starts_with("invalid snapshot data file"));
⋮----
fn test_snapshot_version_from_file_under_limit() {
let file_content = SnapshotVersion::default().as_str();
let mut file = NamedTempFile::new().unwrap();
file.write_all(file_content.as_bytes()).unwrap();
let version_from_file = snapshot_version_from_file(file.path()).unwrap();
assert_eq!(version_from_file, file_content);
⋮----
fn test_snapshot_version_from_file_over_limit() {
let over_limit_size = usize::try_from(MAX_SNAPSHOT_VERSION_FILE_SIZE + 1).unwrap();
let file_content = vec![7u8; over_limit_size];
⋮----
file.write_all(&file_content).unwrap();
assert_matches!(
⋮----
fn test_check_are_snapshots_compatible() {
⋮----
format!("/dir/snapshot-{}-{}.tar.zst", slot1, Hash::new_unique()),
⋮----
assert!(check_are_snapshots_compatible(&full_snapshot_archive_info, None,).is_ok());
⋮----
IncrementalSnapshotArchiveInfo::new_from_path(PathBuf::from(format!(
⋮----
assert!(check_are_snapshots_compatible(
⋮----
fn common_create_bank_snapshot_files(
⋮----
fs::create_dir_all(&snapshot_dir).unwrap();
⋮----
let snapshot_path = snapshot_dir.join(snapshot_filename);
fs::File::create(snapshot_path).unwrap();
⋮----
snapshot_dir.join(snapshot_paths::SNAPSHOT_STATUS_CACHE_FILENAME);
fs::File::create(status_cache_file).unwrap();
let version_path = snapshot_dir.join(snapshot_paths::SNAPSHOT_VERSION_FILENAME);
fs::write(version_path, SnapshotVersion::default().as_str().as_bytes()).unwrap();
⋮----
fn test_get_bank_snapshots() {
let temp_snapshots_dir = tempfile::TempDir::new().unwrap();
⋮----
common_create_bank_snapshot_files(temp_snapshots_dir.path(), min_slot, max_slot);
let bank_snapshots = get_bank_snapshots(temp_snapshots_dir.path());
assert_eq!(bank_snapshots.len() as Slot, max_slot - min_slot);
⋮----
fn test_get_highest_bank_snapshot() {
⋮----
let highest_bank_snapshot = get_highest_bank_snapshot(temp_snapshots_dir.path());
assert!(highest_bank_snapshot.is_some());
assert_eq!(highest_bank_snapshot.unwrap().slot, max_slot - 1);
⋮----
fn common_create_snapshot_archive_files(
⋮----
fs::create_dir_all(full_snapshot_archives_dir).unwrap();
fs::create_dir_all(incremental_snapshot_archives_dir).unwrap();
⋮----
let snapshot_filename = format!(
⋮----
let snapshot_filepath = incremental_snapshot_archives_dir.join(snapshot_filename);
fs::File::create(snapshot_filepath).unwrap();
⋮----
let snapshot_filepath = full_snapshot_archives_dir.join(snapshot_filename);
⋮----
let bad_filename = format!(
⋮----
let bad_filepath = incremental_snapshot_archives_dir.join(bad_filename);
fs::File::create(bad_filepath).unwrap();
⋮----
let bad_filename = format!("snapshot-{}-bad!hash.tar.zst", max_full_snapshot_slot + 1);
let bad_filepath = full_snapshot_archives_dir.join(bad_filename);
⋮----
fn test_get_full_snapshot_archives() {
let full_snapshot_archives_dir = tempfile::TempDir::new().unwrap();
let incremental_snapshot_archives_dir = tempfile::TempDir::new().unwrap();
⋮----
common_create_snapshot_archive_files(
full_snapshot_archives_dir.path(),
incremental_snapshot_archives_dir.path(),
⋮----
let snapshot_archives = get_full_snapshot_archives(full_snapshot_archives_dir);
assert_eq!(snapshot_archives.len() as Slot, max_slot - min_slot);
⋮----
fn test_get_full_snapshot_archives_remote() {
⋮----
.join(snapshot_paths::SNAPSHOT_ARCHIVE_DOWNLOAD_DIR),
⋮----
assert!(snapshot_archives.iter().all(|info| info.is_remote()));
⋮----
fn test_get_incremental_snapshot_archives() {
⋮----
get_incremental_snapshot_archives(incremental_snapshot_archives_dir);
assert_eq!(
⋮----
fn test_get_incremental_snapshot_archives_remote() {
⋮----
assert!(incremental_snapshot_archives
⋮----
fn test_get_highest_full_snapshot_archive_slot() {
⋮----
fn test_get_highest_incremental_snapshot_slot() {
⋮----
fn common_test_purge_old_snapshot_archives(
⋮----
let temp_snap_dir = tempfile::TempDir::new().unwrap();
⋮----
let snap_path = temp_snap_dir.path().join(snap_name);
⋮----
purge_old_snapshot_archives(
temp_snap_dir.path(),
⋮----
for entry in fs::read_dir(temp_snap_dir.path()).unwrap() {
let entry_path_buf = entry.unwrap().path();
let entry_path = entry_path_buf.as_path();
⋮----
.unwrap()
⋮----
.to_string();
retained_snaps.insert(snapshot_name);
⋮----
assert!(
⋮----
assert_eq!(retained_snaps.len(), expected_snapshots.len());
⋮----
fn test_purge_old_full_snapshot_archives() {
let snap1_name = format!("snapshot-1-{}.tar.zst", Hash::default());
let snap2_name = format!("snapshot-3-{}.tar.zst", Hash::default());
let snap3_name = format!("snapshot-50-{}.tar.zst", Hash::default());
let snapshot_names = vec![&snap1_name, &snap2_name, &snap3_name];
let expected_snapshots = vec![&snap3_name];
common_test_purge_old_snapshot_archives(
⋮----
NonZeroUsize::new(1).unwrap(),
⋮----
let expected_snapshots = vec![&snap2_name, &snap3_name];
⋮----
NonZeroUsize::new(2).unwrap(),
⋮----
let expected_snapshots = vec![&snap1_name, &snap2_name, &snap3_name];
⋮----
NonZeroUsize::new(3).unwrap(),
⋮----
fn test_purge_old_full_snapshot_archives_in_the_loop() {
⋮----
let maximum_snapshots_to_retain = NonZeroUsize::new(5).unwrap();
⋮----
for slot in (starting_slot..).take(100) {
⋮----
format!("snapshot-{}-{}.tar.zst", slot, Hash::default());
⋮----
.join(full_snapshot_archive_file_name);
fs::File::create(full_snapshot_archive_path).unwrap();
if slot < starting_slot + maximum_snapshots_to_retain.get() as Slot {
⋮----
if slot % (maximum_snapshots_to_retain.get() as Slot * 2) != 0 {
⋮----
NonZeroUsize::new(usize::MAX).unwrap(),
⋮----
get_full_snapshot_archives(&full_snapshot_archives_dir);
⋮----
assert_eq!(full_snapshot_archives.last().unwrap().slot(), slot);
for (i, full_snapshot_archive) in full_snapshot_archives.iter().rev().enumerate() {
assert_eq!(full_snapshot_archive.slot(), slot - i as Slot);
⋮----
fn test_purge_old_incremental_snapshot_archives() {
⋮----
maximum_incremental_snapshot_archives_to_retain.get() * 2;
⋮----
let mut snapshot_filenames = vec![];
⋮----
.step_by(full_snapshot_interval)
.take(
⋮----
.checked_mul(NonZeroUsize::new(2).unwrap())
⋮----
.get(),
⋮----
.for_each(|full_snapshot_slot| {
⋮----
let snapshot_path = full_snapshot_archives_dir.path().join(&snapshot_filename);
⋮----
snapshot_filenames.push(snapshot_filename);
⋮----
.step_by(incremental_snapshot_interval)
.take(num_incremental_snapshots_per_full_snapshot)
.skip(1)
.for_each(|incremental_snapshot_slot| {
⋮----
.join(&snapshot_filename);
⋮----
get_full_snapshot_archives(full_snapshot_archives_dir.path());
⋮----
remaining_full_snapshot_archives.sort_unstable();
⋮----
remaining_full_snapshot_archives.last().unwrap().slot();
⋮----
get_incremental_snapshot_archives(incremental_snapshot_archives_dir.path());
⋮----
remaining_incremental_snapshot_archives.sort_unstable();
remaining_incremental_snapshot_archives.reverse();
for i in (1..maximum_full_snapshot_archives_to_retain.get()).rev() {
⋮----
remaining_incremental_snapshot_archives.pop().unwrap();
⋮----
assert_eq!(incremental_snapshot_archive.base_slot(), expected_base_slot);
⋮----
assert_eq!(incremental_snapshot_archive.slot(), expected_slot);
⋮----
.skip(
⋮----
- maximum_incremental_snapshot_archives_to_retain.get(),
⋮----
.map(|snapshot| snapshot.slot())
⋮----
fn test_purge_all_incremental_snapshot_archives_when_no_full_snapshot_archives() {
⋮----
format!("incremental-snapshot-100-120-{}.tar.zst", Hash::default()),
format!("incremental-snapshot-100-140-{}.tar.zst", Hash::default()),
format!("incremental-snapshot-100-160-{}.tar.zst", Hash::default()),
format!("incremental-snapshot-100-180-{}.tar.zst", Hash::default()),
format!("incremental-snapshot-200-220-{}.tar.zst", Hash::default()),
format!("incremental-snapshot-200-240-{}.tar.zst", Hash::default()),
format!("incremental-snapshot-200-260-{}.tar.zst", Hash::default()),
format!("incremental-snapshot-200-280-{}.tar.zst", Hash::default()),
⋮----
.join(snapshot_filenames);
⋮----
assert!(remaining_incremental_snapshot_archives.is_empty());
⋮----
fn test_get_snapshot_accounts_hardlink_dir() {
⋮----
let bank_snapshots_dir_tmp = tempfile::TempDir::new().unwrap();
let bank_snapshot_dir = bank_snapshots_dir_tmp.path().join(slot.to_string());
⋮----
bank_snapshot_dir.join(snapshot_paths::SNAPSHOT_ACCOUNTS_HARDLINKS);
fs::create_dir_all(&accounts_hardlinks_dir).unwrap();
let (_tmp_dir, accounts_dir) = create_tmp_accounts_dir_for_tests();
let appendvec_filename = format!("{slot}.0");
let appendvec_path = accounts_dir.join(appendvec_filename);
let ret = get_snapshot_accounts_hardlink_dir(
⋮----
assert!(ret.is_ok());
⋮----
.join(appendvec_path.file_name().unwrap());
⋮----
fn test_get_snapshot_file_kind() {
assert_eq!(None, get_snapshot_file_kind("file.txt"));
⋮----
fn test_serialize_deserialize_account_storage_entries(num_storages: u64) {
let temp_dir = tempfile::tempdir().unwrap();
let bank_snapshot_dir = temp_dir.path();
⋮----
snapshot_storages.push(storage);
⋮----
write_obsolete_accounts_to_snapshot(bank_snapshot_dir, &snapshot_storages, snapshot_slot)
⋮----
deserialize_obsolete_accounts(bank_snapshot_dir, MAX_OBSOLETE_ACCOUNTS_FILE_SIZE)
⋮----
assert!(deserialized_accounts.remove(&storage.slot()).unwrap().2 == 0);
⋮----
fn test_serialize_obsolete_accounts_too_large_file() {
⋮----
serialize_obsolete_accounts(bank_snapshot_dir, &obsolete_accounts, 100).unwrap();
⋮----
fn test_deserialize_obsolete_accounts_too_large_file() {
⋮----
deserialize_obsolete_accounts(bank_snapshot_dir, 100).unwrap();

================
File: runtime/src/stake_account.rs
================
use solana_frozen_abi::abi_example::AbiExample;
⋮----
pub struct StakeAccount<T> {
⋮----
pub enum Error {
⋮----
pub(crate) fn lamports(&self) -> u64 {
self.account.lamports()
⋮----
pub fn stake_state(&self) -> &StakeStateV2 {
⋮----
pub fn delegation(&self) -> &Delegation {
self.stake_state.delegation_ref().unwrap()
⋮----
pub(crate) fn stake(&self) -> &Stake {
self.stake_state.stake_ref().unwrap()
⋮----
type Error = Error;
fn try_from(account: AccountSharedData) -> Result<Self, Self::Error> {
if account.owner() != &stake_program::id() {
return Err(Error::InvalidOwner(*account.owner()));
⋮----
let stake_state: StakeStateV2 = account.state()?;
if stake_state.delegation().is_none() {
return Err(Error::InvalidDelegation(Box::new(stake_state)));
⋮----
Ok(Self {
⋮----
fn from(stake_account: StakeAccount<T>) -> Self {
⋮----
fn eq(&self, other: &StakeAccount<S>) -> bool {
⋮----
impl AbiExample for StakeAccount<Delegation> {
fn example() -> Self {
⋮----
account.data.resize(200, 0u8);
⋮----
account.set_state(&stake_state).unwrap();
Self::try_from(AccountSharedData::from(account)).unwrap()

================
File: runtime/src/stake_history.rs
================
pub use solana_stake_interface::stake_history::StakeHistoryGetEntry;
⋮----
pub struct StakeHistory(Arc<StakeHistoryInner>);
impl Deref for StakeHistory {
type Target = StakeHistoryInner;
fn deref(&self) -> &Self::Target {
⋮----
impl DerefMut for StakeHistory {
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
type StakeHistoryInner = stake_history::StakeHistory;
impl StakeHistoryGetEntry for StakeHistory {
fn get_entry(&self, epoch: Epoch) -> Option<StakeHistoryEntry> {
self.0.get_entry(epoch)
⋮----
mod tests {
⋮----
fn rand_stake_history_entry() -> StakeHistoryEntry {
⋮----
fn test_stake_history_is_cow() {
⋮----
(100..109).for_each(|epoch| {
let entry = rand_stake_history_entry();
stake_history.add(epoch, entry);
⋮----
let stake_history2 = stake_history.clone();
assert_eq!(stake_history, stake_history2);
assert!(
⋮----
let mut stake_history2 = stake_history.clone();
⋮----
(200..209).for_each(|epoch| {
⋮----
stake_history2.add(epoch, entry);
⋮----
assert_ne!(stake_history, stake_history2);
⋮----
fn test_stake_history_serde() {
⋮----
(2134..).take(11).for_each(|epoch| {
⋮----
stake_history_outer.add(epoch, entry.clone());
stake_history_inner.add(epoch, entry);
⋮----
assert_eq!(
⋮----
let data = bincode::serialize(&stake_history_outer).unwrap();
let deserialized_inner: StakeHistoryInner = bincode::deserialize(&data).unwrap();
assert_eq!(&deserialized_inner, stake_history_outer.deref());
⋮----
let data = bincode::serialize(&stake_history_inner).unwrap();
let deserialized_outer: StakeHistory = bincode::deserialize(&data).unwrap();
assert_eq!(deserialized_outer.deref(), &stake_history_inner);

================
File: runtime/src/stake_utils.rs
================
pub fn get_minimum_delegation(is_stake_raise_minimum_delegation_to_1_sol_active: bool) -> u64 {
⋮----
pub fn create_stake_account(
⋮----
let vote_state = VoteStateV4::deserialize(vote_account.data(), voter_pubkey).unwrap();
let credits_observed = vote_state.credits();
let rent_exempt_reserve = rent.minimum_balance(stake_account.data().len());
⋮----
.checked_sub(rent_exempt_reserve)
.expect("lamports >= rent_exempt_reserve");
⋮----
.set_state(&StakeStateV2::Stake(meta, stake, StakeFlags::empty()))
.expect("set_state");

================
File: runtime/src/stake_weighted_timestamp.rs
================
pub(crate) struct MaxAllowableDrift {
⋮----
pub(crate) fn calculate_stake_weighted_timestamp<I, K, V, T>(
⋮----
let (timestamp_slot, timestamp) = slot_timestamp.borrow();
let offset = slot_duration.saturating_mul(slot.saturating_sub(*timestamp_slot) as u32);
let estimate = timestamp.saturating_add(offset.as_secs() as i64);
⋮----
.get(vote_pubkey.borrow())
.map(|(stake, _account)| stake)
.unwrap_or(&0);
⋮----
.entry(estimate)
.and_modify(|stake_sum| *stake_sum = stake_sum.saturating_add(*stake as u128))
.or_insert(*stake as u128);
total_stake = total_stake.saturating_add(*stake as u128);
⋮----
for (timestamp, stake) in stake_per_timestamp.into_iter() {
stake_accumulator = stake_accumulator.saturating_add(stake);
⋮----
slot_duration.saturating_mul(slot.saturating_sub(epoch_start_slot) as u32);
⋮----
(estimate as u64).saturating_sub(epoch_start_timestamp as u64)
⋮----
estimate.saturating_sub(epoch_start_timestamp) as u64
⋮----
poh_estimate_offset.saturating_mul(max_allowable_drift.fast) / 100;
⋮----
poh_estimate_offset.saturating_mul(max_allowable_drift.slow) / 100;
⋮----
&& estimate_offset.saturating_sub(poh_estimate_offset) > max_allowable_drift_slow
⋮----
.saturating_add(poh_estimate_offset.as_secs() as i64)
.saturating_add(max_allowable_drift_slow.as_secs() as i64);
⋮----
&& poh_estimate_offset.saturating_sub(estimate_offset) > max_allowable_drift_fast
⋮----
.saturating_sub(max_allowable_drift_fast.as_secs() as i64);
⋮----
Some(estimate)
⋮----
pub mod tests {
⋮----
fn test_calculate_stake_weighted_timestamp_uses_median() {
⋮----
.iter()
.cloned()
.collect();
⋮----
let bounded = calculate_stake_weighted_timestamp(
⋮----
.unwrap();
assert_eq!(bounded, recent_timestamp);
⋮----
assert_eq!(recent_timestamp - bounded, 1578909061);
⋮----
fn test_calculate_stake_weighted_timestamp_poh() {
⋮----
let poh_offset = (slot * slot_duration).as_secs();
⋮----
Some((0, epoch_start_timestamp)),
⋮----
assert_eq!(bounded, poh_estimate + acceptable_delta);
⋮----
assert_eq!(bounded, poh_estimate - acceptable_delta);
⋮----
fn test_calculate_stake_weighted_timestamp_levels() {
⋮----
assert!(acceptable_delta_50 > acceptable_delta_25 + 1);
⋮----
assert_eq!(bounded, poh_estimate + acceptable_delta_25);
⋮----
assert_eq!(bounded, poh_estimate + acceptable_delta_25 + 1);
⋮----
assert_eq!(bounded, poh_estimate + acceptable_delta_50);
⋮----
fn test_calculate_stake_weighted_timestamp_fast_slow() {
⋮----
assert!(acceptable_delta_slow > acceptable_delta_fast + 1);
⋮----
assert_eq!(bounded, poh_estimate - acceptable_delta_fast);
⋮----
assert_eq!(bounded, poh_estimate + acceptable_delta_fast + 1);
⋮----
assert_eq!(bounded, poh_estimate + acceptable_delta_slow);
⋮----
fn test_calculate_stake_weighted_timestamp_early() {

================
File: runtime/src/stakes.rs
================
use solana_stake_interface::state::Stake;
⋮----
mod serde_stakes;
pub(crate) use serde_stakes::serialize_stake_accounts_to_delegation_format;
pub use serde_stakes::SerdeStakesToStakeFormat;
⋮----
pub enum Error {
⋮----
pub enum InvalidCacheEntryReason {
⋮----
pub type StakeAccount = stake_account::StakeAccount<Delegation>;
⋮----
pub struct StakesCache(RwLock<Stakes<StakeAccount>>);
impl StakesCache {
pub(crate) fn new(stakes: Stakes<StakeAccount>) -> Self {
Self(RwLock::new(stakes))
⋮----
pub(crate) fn stakes(&self) -> RwLockReadGuard<'_, Stakes<StakeAccount>> {
self.0.read().unwrap()
⋮----
pub(crate) fn check_and_store(
⋮----
// TODO: If the account is already cached as a vote or stake account
// but the owner changes, then this needs to evict the account from
// the cache. see:
// https://github.com/solana-labs/solana/pull/24200#discussion_r849935444
let owner = account.owner();
// Zero lamport accounts are not stored in accounts-db
// and so should be removed from cache as well.
if account.lamports() == 0 {
⋮----
let mut stakes = self.0.write().unwrap();
stakes.remove_vote_account(pubkey)
⋮----
stakes.remove_stake_delegation(pubkey, new_rate_activation_epoch);
⋮----
debug_assert_ne!(account.lamports(), 0u64);
⋮----
if VoteStateVersions::is_correct_size_and_initialized(account.data()) {
match VoteAccount::try_from(create_account_shared_data(account)) {
⋮----
// drop the old account after releasing the lock
⋮----
stakes.upsert_vote_account(
⋮----
match StakeAccount::try_from(create_account_shared_data(account)) {
⋮----
stakes.upsert_stake_delegation(
⋮----
pub(crate) fn activate_epoch(
⋮----
stakes.activate_epoch(next_epoch, stake_history, vote_accounts)
⋮----
/// The generic type T is either Delegation or StakeAccount.
/// [`Stakes<Delegation>`] is equivalent to the old code and is used for backward
⋮----
/// [`Stakes<Delegation>`] is equivalent to the old code and is used for backward
/// compatibility in [`crate::bank::BankFieldsToDeserialize`].
⋮----
/// compatibility in [`crate::bank::BankFieldsToDeserialize`].
/// But banks cache [`Stakes<StakeAccount>`] which includes the entire stake
⋮----
/// But banks cache [`Stakes<StakeAccount>`] which includes the entire stake
/// account and StakeStateV2 deserialized from the account. Doing so, will remove
⋮----
/// account and StakeStateV2 deserialized from the account. Doing so, will remove
/// the need to load the stake account from accounts-db when working with
⋮----
/// the need to load the stake account from accounts-db when working with
/// stake-delegations.
⋮----
/// stake-delegations.
#[cfg_attr(feature = "frozen-abi", derive(AbiExample))]
⋮----
pub struct Stakes<T: Clone> {
/// vote accounts
    vote_accounts: VoteAccounts,
/// stake_delegations
    pub stake_delegations: ImHashMap<Pubkey, T>,
/// unused
    unused: u64,
/// current epoch, used to calculate current stake
    epoch: Epoch,
/// history of staking levels
    stake_history: StakeHistory,
⋮----
pub fn vote_accounts(&self) -> &VoteAccounts {
⋮----
pub(crate) fn staked_nodes(&self) -> Arc<HashMap<Pubkey, u64>> {
self.vote_accounts.staked_nodes()
⋮----
/// Creates a Stake<StakeAccount> from Stake<Delegation> by loading the
    /// full account state for respective stake pubkeys. get_account function
⋮----
/// full account state for respective stake pubkeys. get_account function
    /// should return the account at the respective slot where stakes where
⋮----
/// should return the account at the respective slot where stakes where
    /// cached.
⋮----
/// cached.
    pub fn new<F>(stakes: &Stakes<Delegation>, get_account: F) -> Result<Self, Error>
⋮----
pub fn new<F>(stakes: &Stakes<Delegation>, get_account: F) -> Result<Self, Error>
⋮----
.iter()
// im::HashMap doesn't support rayon so we manually build a temporary vector. Note this is
⋮----
.into_par_iter()
.try_fold(ImHashMap::new, |mut map, (pubkey, delegation)| {
let Some(stake_account) = get_account(pubkey) else {
return Err(Error::StakeAccountNotFound(*pubkey));
⋮----
if stakes.vote_accounts.get(voter_pubkey).is_none() {
if let Some(account) = get_account(voter_pubkey) {
if VoteStateVersions::is_correct_size_and_initialized(account.data())
&& VoteAccount::try_from(account.clone()).is_ok()
⋮----
error!("vote account not cached: {voter_pubkey}, {account:?}");
return Err(Error::VoteAccountNotCached(*voter_pubkey));
⋮----
if stake_account.delegation() == delegation {
map.insert(*pubkey, stake_account);
Ok(map)
⋮----
Err(Error::InvalidDelegation(*pubkey))
⋮----
.try_reduce(ImHashMap::new, |a, b| Ok(a.union(b)))?;
for (pubkey, vote_account) in stakes.vote_accounts.iter() {
let Some(account) = get_account(pubkey) else {
return Err(Error::VoteAccountNotFound(*pubkey));
⋮----
let vote_account = vote_account.account();
⋮----
error!("vote account mismatch: {pubkey}, {vote_account:?}, {account:?}");
return Err(Error::VoteAccountMismatch(*pubkey));
⋮----
Ok(Self {
vote_accounts: stakes.vote_accounts.clone(),
⋮----
stake_history: stakes.stake_history.clone(),
⋮----
pub fn new_for_tests(
⋮----
pub(crate) fn history(&self) -> &StakeHistory {
⋮----
pub(crate) fn calculate_activated_stake(
⋮----
let stake_history_entry = thread_pool.install(|| {
⋮----
.par_iter()
.fold(
⋮----
let delegation = stake_account.delegation();
acc + delegation.stake_activating_and_deactivating(
⋮----
.reduce(StakeActivationStatus::default, Add::add)
⋮----
let mut stake_history = self.stake_history.clone();
stake_history.add(self.epoch, stake_history_entry);
let vote_accounts = refresh_vote_accounts(
⋮----
fn calculate_stake(
⋮----
.values()
.map(StakeAccount::delegation)
.filter(|delegation| &delegation.voter_pubkey == voter_pubkey)
.map(|delegation| delegation.stake(epoch, stake_history, new_rate_activation_epoch))
.sum()
⋮----
fn remove_vote_account(&mut self, vote_pubkey: &Pubkey) -> Option<VoteAccount> {
self.vote_accounts.remove(vote_pubkey).map(|(_, a)| a)
⋮----
fn remove_stake_delegation(
⋮----
if let Some(stake_account) = self.stake_delegations.remove(stake_pubkey) {
let removed_delegation = stake_account.delegation();
let removed_stake = removed_delegation.stake(
⋮----
.sub_stake(&removed_delegation.voter_pubkey, removed_stake);
⋮----
fn upsert_vote_account(
⋮----
debug_assert_ne!(vote_account.lamports(), 0u64);
⋮----
self.vote_accounts.insert(*vote_pubkey, vote_account, || {
⋮----
fn upsert_stake_delegation(
⋮----
debug_assert_ne!(stake_account.lamports(), 0u64);
⋮----
let stake = delegation.stake(self.epoch, &self.stake_history, new_rate_activation_epoch);
match self.stake_delegations.insert(stake_pubkey, stake_account) {
None => self.vote_accounts.add_stake(&voter_pubkey, stake),
⋮----
let old_delegation = old_stake_account.delegation();
⋮----
let old_stake = old_delegation.stake(
⋮----
self.vote_accounts.sub_stake(&old_voter_pubkey, old_stake);
self.vote_accounts.add_stake(&voter_pubkey, stake);
⋮----
pub fn stake_delegations(&self) -> &ImHashMap<Pubkey, StakeAccount> {
⋮----
pub(crate) fn stake_delegations_vec(&self) -> Vec<(&Pubkey, &StakeAccount)> {
self.stake_delegations.iter().collect()
⋮----
pub(crate) fn highest_staked_node(&self) -> Option<&Pubkey> {
let vote_account = self.vote_accounts.find_max_by_delegated_stake()?;
Some(vote_account.node_pubkey())
⋮----
fn from(stakes: Stakes<StakeAccount>) -> Self {
⋮----
.into_iter()
.map(|(pubkey, stake_account)| (pubkey, *stake_account.delegation()))
.collect();
⋮----
.map(|(pubkey, stake_account)| (pubkey, *stake_account.stake()))
⋮----
fn from(stakes: Stakes<Stake>) -> Self {
⋮----
.map(|(pubkey, stake)| (pubkey, stake.delegation))
⋮----
fn refresh_vote_accounts(
⋮----
type StakesHashMap = HashMap< Pubkey,  u64>;
fn merge(mut stakes: StakesHashMap, other: StakesHashMap) -> StakesHashMap {
if stakes.len() < other.len() {
return merge(other, stakes);
⋮----
*stakes.entry(pubkey).or_default() += stake;
⋮----
let delegated_stakes = thread_pool.install(|| {
⋮----
let entry = delegated_stakes.entry(delegation.voter_pubkey).or_default();
*entry += delegation.stake(epoch, stake_history, new_rate_activation_epoch);
⋮----
.reduce(HashMap::default, merge)
⋮----
.map(|(&vote_pubkey, vote_account)| {
⋮----
.get(&vote_pubkey)
.copied()
.unwrap_or_default();
(vote_pubkey, (delegated_stake, vote_account.clone()))
⋮----
.collect()
⋮----
pub(crate) mod tests {
⋮----
pub(crate) fn create_staked_node_accounts(
⋮----
create_stake_account(stake, &vote_pubkey, &stake_pubkey),
⋮----
pub(crate) fn create_stake_account(
⋮----
fn test_stakes_basic() {
⋮----
create_staked_node_accounts(10);
stakes_cache.check_and_store(&vote_pubkey, &vote_account, None);
stakes_cache.check_and_store(&stake_pubkey, &stake_account, None);
⋮----
.unwrap()
.stake()
.unwrap();
⋮----
let stakes = stakes_cache.stakes();
let vote_accounts = stakes.vote_accounts();
assert!(vote_accounts.get(&vote_pubkey).is_some());
assert_eq!(
⋮----
stake_account.set_lamports(42);
⋮----
create_stake_account(42, &vote_pubkey, &solana_pubkey::new_rand());
⋮----
stake_account.set_lamports(0);
⋮----
assert_eq!(vote_accounts.get_delegated_stake(&vote_pubkey), 0);
⋮----
fn test_stakes_highest() {
⋮----
assert_eq!(stakes_cache.stakes().highest_staked_node(), None);
⋮----
create_staked_node_accounts(20);
stakes_cache.check_and_store(&vote11_pubkey, &vote11_account, None);
stakes_cache.check_and_store(&stake11_pubkey, &stake11_account, None);
let vote11_node_pubkey = VoteStateV4::deserialize(vote11_account.data(), &vote11_pubkey)
⋮----
let highest_staked_node = stakes_cache.stakes().highest_staked_node().copied();
assert_eq!(highest_staked_node, Some(vote11_node_pubkey));
⋮----
fn test_stakes_vote_account_disappear_reappear() {
⋮----
assert_eq!(vote_accounts.get_delegated_stake(&vote_pubkey), 10);
⋮----
vote_account.set_lamports(0);
⋮----
assert!(vote_accounts.get(&vote_pubkey).is_none());
⋮----
vote_account.set_lamports(1);
⋮----
let cache_data = vote_account.data().to_vec();
let mut pushed = vote_account.data().to_vec();
pushed.push(0);
vote_account.set_data(pushed);
⋮----
vote_account.set_data(vec![0; VoteStateV4::size_of()]);
⋮----
vote_account.set_data(cache_data);
⋮----
fn test_stakes_change_delegate() {
⋮----
stakes_cache.check_and_store(&vote_pubkey2, &vote_account2, None);
⋮----
assert!(vote_accounts.get(&vote_pubkey2).is_some());
assert_eq!(vote_accounts.get_delegated_stake(&vote_pubkey2), 0);
⋮----
stakes_cache.check_and_store(&stake_pubkey, &stake_account2, None);
⋮----
fn test_stakes_multiple_stakers() {
⋮----
let stake_account2 = create_stake_account(10, &vote_pubkey, &stake_pubkey2);
⋮----
stakes_cache.check_and_store(&stake_pubkey2, &stake_account2, None);
⋮----
assert_eq!(vote_accounts.get_delegated_stake(&vote_pubkey), 20);
⋮----
fn test_activate_epoch() {
⋮----
let thread_pool = ThreadPoolBuilder::new().num_threads(1).build().unwrap();
⋮----
let stake_delegations = stakes.stake_delegations_vec();
stakes.calculate_activated_stake(next_epoch, &thread_pool, None, &stake_delegations)
⋮----
stakes_cache.activate_epoch(next_epoch, stake_history, vote_accounts);
⋮----
fn test_stakes_not_delegate() {
⋮----
stakes_cache.check_and_store(

================
File: runtime/src/static_ids.rs
================
vec![

================
File: runtime/src/status_cache.rs
================
pub type ForkStatus<T> = Vec<(Slot, T)>;
pub(crate) type KeySlice = [u8; CACHED_KEY_SIZE];
type KeyMap<T> = HashMap<KeySlice, ForkStatus<T>>;
pub type Status<T> = Arc<Mutex<HashMap<Hash, (usize, Vec<(KeySlice, T)>)>>>;
type KeyStatusMap<T> = HashMap<Hash, (Slot, usize, KeyMap<T>)>;
type SlotDeltaMap<T> = HashMap<Slot, Status<T>>;
pub type SlotDelta<T> = (Slot, bool, Status<T>);
⋮----
pub struct StatusCache<T: Serialize + Clone> {
⋮----
impl<T: Serialize + Clone> Default for StatusCache<T> {
fn default() -> Self {
⋮----
pub fn clear_slot_entries(&mut self, slot: Slot) {
let slot_deltas = self.slot_deltas.remove(&slot);
⋮----
let slot_deltas = slot_deltas.lock().unwrap();
for (blockhash, (_, key_list)) in slot_deltas.iter() {
if let Entry::Occupied(mut o_blockhash_entries) = self.cache.entry(*blockhash) {
let (_, _, all_hash_maps) = o_blockhash_entries.get_mut();
⋮----
if let Entry::Occupied(mut o_key_list) = all_hash_maps.entry(*key_slice) {
let key_list = o_key_list.get_mut();
key_list.retain(|(updated_slot, _)| *updated_slot != slot);
if key_list.is_empty() {
o_key_list.remove_entry();
⋮----
panic!(
⋮----
if all_hash_maps.is_empty() {
o_blockhash_entries.remove_entry();
⋮----
panic!("Blockhash must exist if it exists in self.slot_deltas, slot: {slot}")
⋮----
pub fn get_status<K: AsRef<[u8]>>(
⋮----
let map = self.cache.get(transaction_blockhash)?;
⋮----
let max_key_index = key.as_ref().len().saturating_sub(CACHED_KEY_SIZE + 1);
let index = (*index).min(max_key_index);
⋮----
if let Some(stored_forks) = keymap.get(key_slice) {
⋮----
.iter()
.find(|(f, _)| ancestors.contains_key(f) || self.roots.contains(f))
.cloned();
if res.is_some() {
⋮----
pub fn get_status_any_blockhash<K: AsRef<[u8]>>(
⋮----
let keys: Vec<_> = self.cache.keys().copied().collect();
for blockhash in keys.iter() {
trace!("get_status_any_blockhash: trying {blockhash}");
let status = self.get_status(&key, blockhash, ancestors);
if status.is_some() {
⋮----
pub fn add_root(&mut self, fork: Slot) {
self.roots.insert(fork);
self.purge_roots();
⋮----
pub fn roots(&self) -> &HashSet<Slot> {
⋮----
pub fn insert<K: AsRef<[u8]>>(
⋮----
self.cache.entry(*transaction_blockhash).or_insert_with(|| {
⋮----
let key_index = rng().random_range(0..max_key_index + 1);
⋮----
let key_index = (*key_index).min(max_key_index);
⋮----
key_slice.clone_from_slice(&key.as_ref()[key_index..key_index + CACHED_KEY_SIZE]);
let forks = hash_map.entry(key_slice).or_default();
forks.push((slot, res.clone()));
self.add_to_slot_delta(transaction_blockhash, slot, key_index, key_slice, res);
⋮----
pub fn purge_roots(&mut self) {
if self.roots.len() > MAX_CACHE_ENTRIES {
if let Some(min) = self.roots.iter().min().cloned() {
self.roots.remove(&min);
self.cache.retain(|_, (fork, _, _)| *fork > min);
self.slot_deltas.retain(|slot, _| *slot > min);
⋮----
pub fn clear(&mut self) {
for v in self.cache.values_mut() {
⋮----
.iter_mut()
.for_each(|(_, status)| status.lock().unwrap().clear());
⋮----
pub fn root_slot_deltas(&self) -> Vec<SlotDelta<T>> {
self.roots()
⋮----
.map(|root| {
⋮----
self.slot_deltas.get(root).cloned().unwrap_or_default(),
⋮----
.collect()
⋮----
pub fn append(&mut self, slot_deltas: &[SlotDelta<T>]) {
⋮----
.lock()
.unwrap()
⋮----
.for_each(|(tx_hash, (key_index, statuses))| {
for (key_slice, res) in statuses.iter() {
self.insert_with_slice(tx_hash, *slot, *key_index, *key_slice, res.clone())
⋮----
self.add_root(*slot);
⋮----
fn insert_with_slice(
⋮----
.entry(*transaction_blockhash)
.or_insert((slot, key_index, HashMap::new()));
⋮----
let forks = hash_map.2.entry(key_slice).or_default();
⋮----
fn add_to_slot_delta(
⋮----
let mut fork_entry = self.slot_deltas.entry(slot).or_default().lock().unwrap();
⋮----
.or_insert((key_index, vec![]));
hash_entry.push((key_slice, res))
⋮----
mod tests {
⋮----
type BankStatusCache = StatusCache<()>;
⋮----
fn from_slot_deltas(slot_deltas: &[SlotDelta<T>]) -> Self {
⋮----
cache.append(slot_deltas);
⋮----
impl<T: Serialize + Clone + PartialEq> PartialEq for StatusCache<T> {
fn eq(&self, other: &Self) -> bool {
⋮----
.all(|(hash, (slot, key_index, hash_map))| {
⋮----
other.cache.get(hash)
⋮----
return hash_map.iter().all(|(slice, fork_map)| {
if let Some(other_fork_map) = other_hash_map.get(slice) {
return fork_map.last() == other_fork_map.last();
⋮----
fn test_empty_has_no_sigs() {
⋮----
let blockhash = hash(Hash::default().as_ref());
⋮----
assert_eq!(
⋮----
fn test_find_sig_with_ancestor_fork() {
⋮----
let ancestors = vec![(0, 1)].into_iter().collect();
status_cache.insert(&blockhash, sig, 0, ());
⋮----
fn test_find_sig_without_ancestor_fork() {
⋮----
status_cache.insert(&blockhash, sig, 1, ());
assert_eq!(status_cache.get_status(sig, &blockhash, &ancestors), None);
assert_eq!(status_cache.get_status_any_blockhash(sig, &ancestors), None);
⋮----
fn test_find_sig_with_root_ancestor_fork() {
⋮----
status_cache.add_root(0);
⋮----
fn test_insert_picks_latest_blockhash_fork() {
⋮----
let ancestors = vec![(0, 0)].into_iter().collect();
⋮----
status_cache.add_root(i as u64);
⋮----
assert!(status_cache
⋮----
fn test_root_expires() {
⋮----
fn test_clear_signatures_sigs_are_gone() {
⋮----
status_cache.clear();
⋮----
fn test_clear_signatures_insert_works() {
⋮----
fn test_signatures_slice() {
⋮----
let (_, index, sig_map) = status_cache.cache.get(&blockhash).unwrap();
⋮----
assert!(sig_map.get(sig_slice).is_some());
⋮----
fn test_slot_deltas() {
⋮----
assert!(status_cache.roots().contains(&0));
let slot_deltas = status_cache.root_slot_deltas();
⋮----
assert_eq!(cache, status_cache);
let slot_deltas = cache.root_slot_deltas();
⋮----
fn test_roots_deltas() {
⋮----
let blockhash2 = hash(blockhash.as_ref());
⋮----
status_cache.insert(&blockhash2, sig, 1, ());
⋮----
assert_eq!(status_cache.slot_deltas.len(), 1);
assert!(status_cache.slot_deltas.contains_key(&1));
⋮----
fn test_age_sanity() {
assert!(MAX_CACHE_ENTRIES <= MAX_RECENT_BLOCKHASHES);
⋮----
fn test_clear_slot_signatures() {
⋮----
ancestors0.insert(0, 0);
⋮----
ancestors1.insert(1, 0);
⋮----
status_cache.clear_slot_entries(0);
⋮----
assert!(!status_cache.slot_deltas.contains_key(&0));
⋮----
status_cache.clear_slot_entries(1);
assert!(status_cache.slot_deltas.is_empty());
⋮----
assert!(status_cache.cache.is_empty());
⋮----
fn test_different_sized_keys() {
⋮----
let blockhash = hash(blockhash.as_ref());
⋮----
status_cache.insert(&blockhash, sig_key, 0, ());
status_cache.insert(&blockhash, hash_key, 0, ());
⋮----
mod shuttle_tests {
⋮----
type BankStatusCache = RwLock<StatusCache<()>>;
⋮----
const INSERT_DFS_ITERATIONS: Option<usize> = Some(20000);
⋮----
fn do_test_shuttle_clear_slots_blockhash_overlap() {
⋮----
.write()
⋮----
.insert(&blockhash1, key1, 1, ());
⋮----
let status_cache = status_cache.clone();
⋮----
status_cache.write().unwrap().clear_slot_entries(1);
⋮----
.insert(&blockhash1, key2, 2, ());
⋮----
th_clear.join().unwrap();
th_insert.join().unwrap();
⋮----
ancestors2.insert(2, 0);
⋮----
fn test_shuttle_clear_slots_blockhash_overlap_random() {
⋮----
fn test_shuttle_clear_slots_blockhash_overlap_dfs() {
⋮----
fn do_test_shuttle_purge_nonce_overlap() {
⋮----
status_cache.write().unwrap().add_root(i as u64);
⋮----
.insert(&blockhash1, key1, 0, ());
⋮----
.add_root(MAX_CACHE_ENTRIES as Slot + 1);
⋮----
status_cache.write().unwrap().insert(
⋮----
th_purge.join().unwrap();
⋮----
ancestors2.insert(MAX_CACHE_ENTRIES as Slot + 2, 0);
⋮----
fn test_shuttle_purge_nonce_overlap_random() {
⋮----
fn test_shuttle_purge_nonce_overlap_dfs() {
⋮----
fn do_test_shuttle_concurrent_inserts() {
⋮----
handles.push(shuttle::thread::spawn(move || {
⋮----
.insert(&bh, key, slot as Slot, ());
⋮----
handle.join().unwrap();
⋮----
ancestors.insert(1, 0);
ancestors.insert(2, 0);
ancestors.insert(3, 0);
⋮----
assert!(
⋮----
fn test_shuttle_concurrent_inserts_dfs() {
⋮----
fn test_shuttle_concurrent_inserts_random() {

================
File: runtime/src/transaction_batch.rs
================
pub enum OwnedOrBorrowed<'a, T> {
⋮----
impl<T> Deref for OwnedOrBorrowed<'_, T> {
type Target = [T];
fn deref(&self) -> &Self::Target {
⋮----
// Represents the results of trying to lock a set of accounts
pub struct TransactionBatch<'a, 'b, Tx: SVMMessage> {
⋮----
pub fn new(
⋮----
assert_eq!(lock_results.len(), sanitized_txs.len());
⋮----
pub fn lock_results(&self) -> &Vec<Result<()>> {
⋮----
pub fn sanitized_transactions(&self) -> &[Tx] {
⋮----
pub fn bank(&self) -> &Bank {
⋮----
pub fn set_needs_unlock(&mut self, needs_unlock: bool) {
⋮----
pub fn needs_unlock(&self) -> bool {
⋮----
/// For every error result, if the corresponding transaction is
    /// still locked, unlock the transaction and then record the new error.
⋮----
/// still locked, unlock the transaction and then record the new error.
    pub fn unlock_failures(&mut self, transaction_results: Vec<Result<()>>) {
⋮----
pub fn unlock_failures(&mut self, transaction_results: Vec<Result<()>>) {
assert_eq!(self.lock_results.len(), transaction_results.len());
// Shouldn't happen but if a batch was marked as not needing an unlock,
if !self.needs_unlock() {
⋮----
.iter()
.enumerate()
.inspect(|(index, result)| {
assert!(!(result.is_ok() && self.lock_results[*index].is_err()))
⋮----
.filter(|(index, result)| result.is_err() && self.lock_results[*index].is_ok())
.map(|(index, _)| (&self.sanitized_txs[index], &self.lock_results[index]));
self.bank.unlock_accounts(txs_and_results);
⋮----
impl<Tx: SVMMessage> Drop for TransactionBatch<'_, '_, Tx> {
fn drop(&mut self) {
if self.needs_unlock() {
self.set_needs_unlock(false);
self.bank.unlock_accounts(
self.sanitized_transactions()
⋮----
.zip(self.lock_results()),
⋮----
mod tests {
⋮----
fn test_transaction_batch(relax_intrabatch_account_locks: bool) {
let (bank, txs) = setup(false, relax_intrabatch_account_locks);
let batch = bank.prepare_sanitized_batch(&txs);
assert!(batch.lock_results().iter().all(|x| x.is_ok()));
let batch2 = bank.prepare_sanitized_batch(&txs);
assert!(batch2.lock_results().iter().all(|x| x.is_err()));
drop(batch);
⋮----
assert!(batch2.lock_results().iter().all(|x| x.is_ok()));
⋮----
fn test_simulation_batch(relax_intrabatch_account_locks: bool) {
⋮----
let batch = bank.prepare_unlocked_batch_from_single_tx(&txs[0]);
⋮----
let batch3 = bank.prepare_unlocked_batch_from_single_tx(&txs[0]);
assert!(batch3.lock_results().iter().all(|x| x.is_ok()));
⋮----
fn test_unlock_failures(relax_intrabatch_account_locks: bool) {
let (bank, txs) = setup(true, relax_intrabatch_account_locks);
⋮----
vec![Ok(()), Ok(()), Ok(())]
⋮----
vec![Ok(()), Err(TransactionError::AccountInUse), Ok(())]
⋮----
let mut batch = bank.prepare_sanitized_batch(&txs);
assert_eq!(batch.lock_results, expected_lock_results,);
let qos_results = vec![
⋮----
batch.unlock_failures(qos_results.clone());
assert_eq!(batch.lock_results, qos_results);
⋮----
assert_eq!(batch2.lock_results, expected_lock_results,);
⋮----
fn setup(
⋮----
} = create_genesis_config_with_leader(500, &dummy_leader_pubkey, 100);
⋮----
bank.deactivate_feature(&agave_feature_set::relax_intrabatch_account_locks::id());
⋮----
let mut txs = vec![RuntimeTransaction::from_transaction_for_tests(
⋮----
txs.push(RuntimeTransaction::from_transaction_for_tests(
system_transaction::transfer(&mint_keypair, &pubkey2, 1, genesis_config.hash()),
⋮----
system_transaction::transfer(&keypair2, &pubkey2, 1, genesis_config.hash()),
⋮----
fn test_prepare_sanitized_batch_relax_intrabatch_account_locks() {
⋮----
} = create_genesis_config(500);
⋮----
let tx_batch_1 = vec![
⋮----
let tx_batch_2 = vec![RuntimeTransaction::from_transaction_for_tests(transfer(
⋮----
let batch_1 = bank.prepare_sanitized_batch_relax_intrabatch_account_locks(&tx_batch_1);
assert!(batch_1.lock_results().iter().all(|x| x.is_ok()));
let batch_2 = bank.prepare_sanitized_batch(&tx_batch_2);
assert_eq!(
⋮----
drop(batch_1);
drop(batch_2);
⋮----
assert!(batch_2.lock_results().iter().all(|x| x.is_ok()));

================
File: runtime/src/vote_sender_types.rs
================
pub type ReplayVoteSender = Sender<ParsedVote>;
pub type ReplayVoteReceiver = Receiver<ParsedVote>;

================
File: runtime/.gitignore
================
/target/
/farf/

================
File: runtime/Cargo.toml
================
[package]
name = "solana-runtime"
description = "Solana runtime"
documentation = "https://docs.rs/solana-runtime"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_runtime"

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "dep:solana-system-program",
    "solana-accounts-db/dev-context-only-utils",
    "solana-svm/dev-context-only-utils",
    "solana-runtime-transaction/dev-context-only-utils",
    "solana-vote/dev-context-only-utils",
]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-account/frozen-abi",
    "solana-accounts-db/frozen-abi",
    "solana-bls-signatures/frozen-abi",
    "solana-compute-budget/frozen-abi",
    "solana-cost-model/frozen-abi",
    "solana-epoch-schedule/frozen-abi",
    "solana-hard-forks/frozen-abi",
    "solana-inflation/frozen-abi",
    "solana-instruction/frozen-abi",
    "solana-instruction-error/frozen-abi",
    "solana-perf/frozen-abi",
    "solana-program-runtime/frozen-abi",
    "solana-rent/frozen-abi",
    "solana-stake-interface/frozen-abi",
    "solana-svm/frozen-abi",
    "solana-transaction-error/frozen-abi",
    "solana-version/frozen-abi",
    "solana-vote/frozen-abi",
    "solana-vote-program/frozen-abi",
]
shuttle-test = ["dep:shuttle"]

[dependencies]
agave-feature-set = { workspace = true }
agave-fs = { workspace = true }
agave-precompiles = { workspace = true }
agave-reserved-account-keys = { workspace = true }
agave-snapshots = { workspace = true }
agave-syscalls = { workspace = true }
agave-votor-messages = { workspace = true }
ahash = { workspace = true }
aquamarine = { workspace = true }
arc-swap = { workspace = true }
arrayref = { workspace = true }
assert_matches = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
blake3 = { workspace = true }
bv = { workspace = true, features = ["serde"] }
bytemuck = { workspace = true }
crossbeam-channel = { workspace = true }
dashmap = { workspace = true, features = ["rayon", "raw-api", "serde"] }
dir-diff = { workspace = true }
fnv = { workspace = true }
im = { workspace = true, features = ["rayon", "serde"] }
itertools = { workspace = true }
libc = { workspace = true }
log = { workspace = true }
lz4 = { workspace = true }
memmap2 = { workspace = true }
mockall = { workspace = true }
modular-bitfield = { workspace = true }
num-derive = { workspace = true }
num-traits = { workspace = true }
num_cpus = { workspace = true }
num_enum = { workspace = true }
percentage = { workspace = true }
qualifier_attr = { workspace = true }
rand = { workspace = true }
rayon = { workspace = true }
regex = { workspace = true }
semver = { workspace = true }
serde = { workspace = true, features = ["rc"] }
serde_json = { workspace = true }
serde_with = { workspace = true }
shuttle = { workspace = true, optional = true }
solana-account = { workspace = true }
solana-account-info = { workspace = true }
solana-accounts-db = { workspace = true }
solana-address-lookup-table-interface = { workspace = true }
solana-bls-signatures = { workspace = true }
solana-bpf-loader-program = { workspace = true }
solana-bucket-map = { workspace = true }
solana-builtins = { workspace = true }
solana-client-traits = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-instruction = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-config-interface = { workspace = true, features = ["bincode"] }
solana-cost-model = { workspace = true }
solana-cpi = { workspace = true }
solana-ed25519-program = { workspace = true }
solana-epoch-info = { workspace = true }
solana-epoch-rewards-hasher = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-feature-gate-interface = { workspace = true, features = ["bincode"] }
solana-fee = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-fee-structure = { workspace = true, features = ["serde"] }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-genesis-config = { workspace = true }
solana-hard-forks = { workspace = true, features = ["serde"] }
solana-hash = { workspace = true }
solana-inflation = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true, features = ["seed-derivable"] }
solana-lattice-hash = { workspace = true }
solana-loader-v3-interface = { workspace = true, features = ["bincode"] }
solana-loader-v4-interface = { workspace = true, features = ["serde"] }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-native-token = { workspace = true }
solana-nohash-hasher = { workspace = true }
solana-nonce = { workspace = true }
solana-nonce-account = { workspace = true }
solana-packet = { workspace = true }
solana-perf = { workspace = true }
solana-poh-config = { workspace = true }
solana-precompile-error = { workspace = true }
solana-program-runtime = { workspace = true, features = ["metrics"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-rayon-threadlimit = { workspace = true }
solana-rent = { workspace = true }
solana-reward-info = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-secp256k1-program = { workspace = true }
solana-seed-derivable = { workspace = true }
solana-serde = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-slot-history = { workspace = true }
solana-stake-interface = { workspace = true }
solana-svm = { workspace = true }
solana-svm-callback = { workspace = true }
solana-svm-timings = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-program = { workspace = true, optional = true }
solana-system-transaction = { workspace = true }
solana-sysvar = { workspace = true }
solana-sysvar-id = { workspace = true }
solana-time-utils = { workspace = true }
solana-transaction = { workspace = true, features = ["verify"] }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status-client-types = { workspace = true }
solana-unified-scheduler-logic = { workspace = true }
solana-version = { workspace = true }
solana-vote = { workspace = true }
solana-vote-interface = { workspace = true }
solana-vote-program = { workspace = true }
spl-generic-token = { workspace = true }
static_assertions = { workspace = true }
strum = { workspace = true, features = ["derive"] }
strum_macros = { workspace = true }
symlink = { workspace = true }
tempfile = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
agave-transaction-view = { workspace = true }
ed25519-dalek = { workspace = true }
libsecp256k1 = { workspace = true }
memoffset = { workspace = true }
rand_chacha = { workspace = true }
solana-accounts-db = { workspace = true, features = ["dev-context-only-utils"] }
solana-builtins = { workspace = true, features = ["dev-context-only-utils"] }
solana-instruction-error = { workspace = true }
solana-program-binaries = { workspace = true }
# See order-crates-for-publishing.py for using this unusual `path = "."`
solana-runtime = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-runtime-transaction = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-sdk-ids = { workspace = true }
solana-secp256k1-program = { workspace = true, features = ["bincode"] }
solana-signature = { workspace = true, features = ["std"] }
solana-stake-interface = { workspace = true, features = ["sysvar"] }
solana-svm = { workspace = true, features = ["dev-context-only-utils"] }
solana-transaction-context = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-vote-program = { workspace = true, features = ["dev-context-only-utils"] }
static_assertions = { workspace = true }
test-case = { workspace = true }

[[bench]]
name = "prioritization_fee_cache"

[lints]
workspace = true

================
File: runtime-transaction/benches/get_signature_details.rs
================
fn bench_get_signature_details_empty(c: &mut Criterion) {
⋮----
c.benchmark_group("bench_get_signature_details_empty")
.throughput(Throughput::Elements(1))
.bench_function("0 instructions", |bencher| {
bencher.iter(|| {
let instructions = black_box(instructions.clone());
let _ = get_precompile_signature_details(instructions);
⋮----
fn bench_get_signature_details_no_sigs_unique(c: &mut Criterion) {
let program_ids = vec![Pubkey::new_unique(); 32];
⋮----
.map(|i| {
⋮----
accounts: vec![],
data: vec![],
⋮----
c.benchmark_group("bench_get_signature_details_no_sigs_unique")
⋮----
.bench_function(format!("{num_instructions} instructions"), |bencher| {
⋮----
black_box(instructions.iter().map(|(program_id, instruction)| {
⋮----
fn bench_get_signature_details_packed_sigs(c: &mut Criterion) {
⋮----
data: vec![4],
⋮----
c.benchmark_group("bench_get_signature_details_packed_sigs")
⋮----
fn bench_get_signature_details_mixed_sigs(c: &mut Criterion) {
⋮----
.into_iter()
.chain((0..6).map(|_| Pubkey::new_unique()))
⋮----
c.benchmark_group("bench_get_signature_details_mixed_sigs")
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: runtime-transaction/src/runtime_transaction/sdk_transactions.rs
================
pub fn try_from(
⋮----
MessageHash::Compute => sanitized_versioned_tx.get_message().message.hash(),
⋮----
.unwrap_or_else(|| is_simple_vote_transaction(&sanitized_versioned_tx));
⋮----
.get_message()
.program_instructions_iter()
.map(|(program_id, ix)| (program_id, SVMInstruction::from(ix))),
⋮----
.header()
⋮----
Ok(Self {
⋮----
pub fn try_create(
⋮----
&& tx.message.instructions().len()
⋮----
return Err(solana_transaction_error::TransactionError::SanitizeFailure);
⋮----
let hash = *statically_loaded_runtime_tx.message_hash();
let is_simple_vote_tx = statically_loaded_runtime_tx.is_simple_vote_transaction();
⋮----
tx.load_dynamic_metadata()?;
Ok(tx)
⋮----
fn load_dynamic_metadata(&mut self) -> Result<()> {
Ok(())
⋮----
impl TransactionWithMeta for RuntimeTransaction<SanitizedTransaction> {
⋮----
fn as_sanitized_transaction(&self) -> Cow<'_, SanitizedTransaction> {
⋮----
fn to_versioned_transaction(&self) -> VersionedTransaction {
self.transaction.to_versioned_transaction()
⋮----
pub fn from_transaction_for_tests(transaction: solana_transaction::Transaction) -> Self {
⋮----
.expect("failed to create RuntimeTransaction from Transaction")
⋮----
mod tests {
⋮----
fn vote_sanitized_versioned_transaction() -> SanitizedVersionedTransaction {
⋮----
let votes = Vote::new(vec![1, 2, 3], bank_hash);
⋮----
vote::instruction::vote(&vote_keypair.pubkey(), &auth_keypair.pubkey(), votes);
let mut vote_tx = Transaction::new_with_payer(&[vote_ix], Some(&node_keypair.pubkey()));
vote_tx.partial_sign(&[&node_keypair], block_hash);
vote_tx.partial_sign(&[&auth_keypair], block_hash);
SanitizedVersionedTransaction::try_from(VersionedTransaction::from(vote_tx)).unwrap()
⋮----
fn non_vote_sanitized_versioned_transaction() -> SanitizedVersionedTransaction {
TestTransaction::new().to_sanitized_versioned_transaction()
⋮----
struct TestTransaction {
⋮----
impl TestTransaction {
fn new() -> Self {
⋮----
let instructions = vec![system_instruction::transfer(
⋮----
fn add_compute_unit_limit(&mut self, val: u32) -> &mut TestTransaction {
⋮----
.push(ComputeBudgetInstruction::set_compute_unit_limit(val));
⋮----
fn add_compute_unit_price(&mut self, val: u64) -> &mut TestTransaction {
⋮----
.push(ComputeBudgetInstruction::set_compute_unit_price(val));
⋮----
fn add_loaded_accounts_bytes(&mut self, val: u32) -> &mut TestTransaction {
⋮----
.push(ComputeBudgetInstruction::set_loaded_accounts_data_size_limit(val));
⋮----
fn to_sanitized_versioned_transaction(&self) -> SanitizedVersionedTransaction {
let message = Message::new(&self.instructions, Some(&self.from_keypair.pubkey()));
⋮----
SanitizedVersionedTransaction::try_from(VersionedTransaction::from(tx)).unwrap()
⋮----
fn test_runtime_transaction_is_vote_meta() {
fn get_is_simple_vote(
⋮----
.unwrap()
⋮----
assert!(!get_is_simple_vote(
⋮----
assert!(get_is_simple_vote(
⋮----
fn test_advancing_transaction_type() {
⋮----
non_vote_sanitized_versioned_transaction(),
⋮----
.unwrap();
assert_eq!(hash, *statically_loaded_transaction.message_hash());
assert!(!statically_loaded_transaction.is_simple_vote_transaction());
⋮----
dynamically_loaded_transaction.expect("created from statically loaded tx");
assert_eq!(hash, *dynamically_loaded_transaction.message_hash());
assert!(!dynamically_loaded_transaction.is_simple_vote_transaction());
⋮----
fn test_runtime_transaction_static_meta() {
⋮----
.add_compute_unit_limit(compute_unit_limit)
.add_compute_unit_price(compute_unit_price)
.add_loaded_accounts_bytes(loaded_accounts_bytes)
.to_sanitized_versioned_transaction(),
⋮----
assert_eq!(&hash, runtime_transaction_static.message_hash());
assert!(!runtime_transaction_static.is_simple_vote_transaction());
⋮----
assert_eq!(1, signature_details.num_transaction_signatures());
assert_eq!(0, signature_details.num_secp256k1_instruction_signatures());
assert_eq!(0, signature_details.num_ed25519_instruction_signatures());
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(&feature_set)
⋮----
assert_eq!(compute_unit_limit, compute_budget_limits.compute_unit_limit);
assert_eq!(compute_unit_price, compute_budget_limits.compute_unit_price);
assert_eq!(

================
File: runtime-transaction/src/runtime_transaction/transaction_view.rs
================
fn is_simple_vote_transaction<D: TransactionData>(
⋮----
let signatures = transaction.signatures();
let is_legacy_message = matches!(transaction.version(), TransactionVersion::Legacy);
⋮----
.program_instructions_iter()
.map(|(program_id, _ix)| program_id);
is_simple_vote_transaction_impl(signatures, is_legacy_message, instruction_programs)
⋮----
pub fn try_new(
⋮----
from_sanitized_transaction_view(&transaction, message_hash, is_simple_vote_tx)
.map(|meta| RuntimeTransaction { transaction, meta })
⋮----
from_sanitized_transaction_view(transaction, message_hash, is_simple_vote_tx)
⋮----
fn from_sanitized_transaction_view<D>(
⋮----
MessageHash::Compute => VersionedMessage::hash_raw_message(transaction.message_data()),
⋮----
is_simple_vote_tx.unwrap_or_else(|| is_simple_vote_transaction(transaction));
⋮----
} = InstructionMeta::try_new(transaction.program_instructions_iter())?;
⋮----
u64::from(transaction.num_required_signatures()),
⋮----
ComputeBudgetInstructionDetails::try_from(transaction.program_instructions_iter())?;
Ok(TransactionMeta {
⋮----
/// Create a new `RuntimeTransaction<ResolvedTransactionView>` from a
    /// `RuntimeTransaction<SanitizedTransactionView>` that already has
⋮----
/// `RuntimeTransaction<SanitizedTransactionView>` that already has
    /// static metadata loaded.
⋮----
/// static metadata loaded.
    pub fn try_new(
⋮----
// transaction-view does not distinguish between different types of errors here.
// return generic sanitize failure error here.
// these transactions should be immediately dropped, and we generally
// will not care about the specific error at this point.
⋮----
.map_err(|_| TransactionError::SanitizeFailure)?;
⋮----
tx.load_dynamic_metadata()?;
Ok(tx)
⋮----
fn load_dynamic_metadata(&mut self) -> Result<()> {
Ok(())
⋮----
impl<D: TransactionData> TransactionWithMeta for RuntimeTransaction<ResolvedTransactionView<D>> {
fn as_sanitized_transaction(&self) -> Cow<'_, SanitizedTransaction> {
⋮----
} = self.to_versioned_transaction();
let is_writable_account_cache = (0..self.transaction.total_num_accounts())
.map(|index| self.is_writable(usize::from(index)))
.collect();
⋮----
loaded_addresses: Cow::Owned(self.loaded_addresses().unwrap().clone()),
⋮----
*self.message_hash(),
self.is_simple_vote_transaction(),
⋮----
.expect("transaction view is sanitized"),
⋮----
fn to_versioned_transaction(&self) -> VersionedTransaction {
⋮----
num_required_signatures: self.num_required_signatures(),
num_readonly_signed_accounts: self.num_readonly_signed_static_accounts(),
num_readonly_unsigned_accounts: self.num_readonly_unsigned_static_accounts(),
⋮----
let static_account_keys = self.static_account_keys().to_vec();
let recent_blockhash = *self.recent_blockhash();
⋮----
.instructions_iter()
.map(|ix| CompiledInstruction {
⋮----
accounts: ix.accounts.to_vec(),
data: ix.data.to_vec(),
⋮----
let message = match self.version() {
⋮----
.address_table_lookup_iter()
.map(|atl| MessageAddressTableLookup {
⋮----
writable_indexes: atl.writable_indexes.to_vec(),
readonly_indexes: atl.readonly_indexes.to_vec(),
⋮----
.collect(),
⋮----
signatures: self.signatures().to_vec(),
⋮----
mod tests {
⋮----
fn test_advancing_transaction_type() {
⋮----
bincode::serialize(&transaction).unwrap()
⋮----
SanitizedTransactionView::try_new_sanitized(&serialized_transaction[..], true).unwrap();
⋮----
.unwrap();
assert_eq!(hash, *static_runtime_transaction.message_hash());
assert!(!static_runtime_transaction.is_simple_vote_transaction());
⋮----
assert_eq!(hash, *dynamic_runtime_transaction.message_hash());
assert!(!dynamic_runtime_transaction.is_simple_vote_transaction());
⋮----
fn test_to_versioned_transaction() {
fn assert_translation(
⋮----
let bytes = bincode::serialize(&original_transaction).unwrap();
⋮----
SanitizedTransactionView::try_new_sanitized(&bytes[..], true).unwrap();
⋮----
let versioned_transaction = runtime_transaction.to_versioned_transaction();
assert_eq!(original_transaction, versioned_transaction);
⋮----
assert_translation(original_transaction, None, &reserved_key_set);
⋮----
signatures: vec![Signature::default()],
⋮----
addresses: vec![to],
⋮----
.unwrap(),
⋮----
assert_translation(
⋮----
Some(LoadedAddresses {
writable: vec![to],
readonly: vec![],
⋮----
fn test_as_sanitized_transaction() {
⋮----
bincode::serialize(&original_transaction.to_versioned_transaction()).unwrap();
⋮----
let sanitized_transaction = runtime_transaction.as_sanitized_transaction();
assert_eq!(
⋮----
assert_translation(sanitized_transaction, None, &reserved_key_set);
⋮----
SimpleAddressLoader::Enabled(loaded_addresses.clone()),
⋮----
Some(loaded_addresses),

================
File: runtime-transaction/src/instruction_data_len.rs
================
pub struct InstructionDataLenBuilder {
⋮----
impl InstructionDataLenBuilder {
pub fn process_instruction(&mut self, _program_id: &Pubkey, instruction: &SVMInstruction) {
self.value = self.value.saturating_add(instruction.data.len() as u16);
⋮----
pub fn build(self) -> u16 {

================
File: runtime-transaction/src/instruction_meta.rs
================
pub struct InstructionMeta {
⋮----
impl InstructionMeta {
pub fn try_new<'a>(
⋮----
precompile_signature_details_builder.process_instruction(program_id, &instruction);
instruction_data_len_builder.process_instruction(program_id, &instruction);
⋮----
Ok(Self {
precompile_signature_details: precompile_signature_details_builder.build(),
instruction_data_len: instruction_data_len_builder.build(),

================
File: runtime-transaction/src/lib.rs
================
mod instruction_data_len;
pub(crate) mod instruction_meta;
pub mod runtime_transaction;
pub mod signature_details;
pub mod transaction_meta;
pub mod transaction_with_meta;

================
File: runtime-transaction/src/runtime_transaction.rs
================
mod sdk_transactions;
mod transaction_view;
⋮----
pub struct RuntimeTransaction<T> {
⋮----
pub fn into_inner_transaction(self) -> T {
⋮----
impl<T> StaticMeta for RuntimeTransaction<T> {
fn message_hash(&self) -> &Hash {
⋮----
fn is_simple_vote_transaction(&self) -> bool {
⋮----
fn signature_details(&self) -> &TransactionSignatureDetails {
⋮----
fn compute_budget_instruction_details(&self) -> &ComputeBudgetInstructionDetails {
⋮----
fn instruction_data_len(&self) -> u16 {
⋮----
impl<T: SVMMessage> DynamicMeta for RuntimeTransaction<T> {}
impl<T> Deref for RuntimeTransaction<T> {
type Target = T;
fn deref(&self) -> &Self::Target {
⋮----
impl<T: SVMStaticMessage> SVMStaticMessage for RuntimeTransaction<T> {
fn num_transaction_signatures(&self) -> u64 {
self.transaction.num_transaction_signatures()
⋮----
fn num_ed25519_signatures(&self) -> u64 {
⋮----
.num_ed25519_instruction_signatures()
⋮----
fn num_secp256k1_signatures(&self) -> u64 {
⋮----
.num_secp256k1_instruction_signatures()
⋮----
fn num_secp256r1_signatures(&self) -> u64 {
⋮----
.num_secp256r1_instruction_signatures()
⋮----
fn num_write_locks(&self) -> u64 {
self.transaction.num_write_locks()
⋮----
fn recent_blockhash(&self) -> &Hash {
self.transaction.recent_blockhash()
⋮----
fn num_instructions(&self) -> usize {
self.transaction.num_instructions()
⋮----
fn instructions_iter(&self) -> impl Iterator<Item = SVMInstruction<'_>> {
self.transaction.instructions_iter()
⋮----
fn program_instructions_iter(
⋮----
self.transaction.program_instructions_iter()
⋮----
fn static_account_keys(&self) -> &[Pubkey] {
self.transaction.static_account_keys()
⋮----
fn fee_payer(&self) -> &Pubkey {
self.transaction.fee_payer()
⋮----
fn num_lookup_tables(&self) -> usize {
self.transaction.num_lookup_tables()
⋮----
fn message_address_table_lookups(
⋮----
self.transaction.message_address_table_lookups()
⋮----
impl<T: SVMMessage> SVMMessage for RuntimeTransaction<T> {
fn account_keys(&self) -> AccountKeys<'_> {
self.transaction.account_keys()
⋮----
fn is_writable(&self, index: usize) -> bool {
self.transaction.is_writable(index)
⋮----
fn is_signer(&self, index: usize) -> bool {
self.transaction.is_signer(index)
⋮----
fn is_invoked(&self, key_index: usize) -> bool {
self.transaction.is_invoked(key_index)
⋮----
impl<T: SVMTransaction> SVMTransaction for RuntimeTransaction<T> {
fn signature(&self) -> &Signature {
self.transaction.signature()
⋮----
fn signatures(&self) -> &[Signature] {
self.transaction.signatures()

================
File: runtime-transaction/src/signature_details.rs
================
pub struct PrecompileSignatureDetails {
⋮----
pub(crate) struct PrecompileSignatureDetailsBuilder {
⋮----
impl Default for PrecompileSignatureDetailsBuilder {
fn default() -> Self {
⋮----
impl PrecompileSignatureDetailsBuilder {
pub fn process_instruction(&mut self, program_id: &Pubkey, instruction: &SVMInstruction) {
⋮----
match self.filter.is_signature(program_id_index, program_id) {
⋮----
.wrapping_add(get_num_signatures_in_instruction(instruction));
⋮----
pub fn build(self) -> PrecompileSignatureDetails {
⋮----
pub fn get_precompile_signature_details<'a>(
⋮----
builder.process_instruction(program_id, &instruction);
⋮----
builder.build()
⋮----
fn get_num_signatures_in_instruction(instruction: &SVMInstruction) -> u64 {
u64::from(instruction.data.first().copied().unwrap_or(0))
⋮----
enum ProgramIdStatus {
⋮----
struct SignatureDetailsFilter {
// array of slots for all possible static and sanitized program_id_index,
// each slot indicates if a program_id_index has not been checked, or is
// already checked with result that can be reused.
⋮----
impl SignatureDetailsFilter {
⋮----
fn new() -> Self {
⋮----
fn is_signature(&mut self, index: u8, program_id: &Pubkey) -> ProgramIdStatus {
⋮----
*flag = Some(Self::check_program_id(program_id));
*flag.as_ref().unwrap()
⋮----
fn check_program_id(program_id: &Pubkey) -> ProgramIdStatus {
⋮----
mod tests {
⋮----
// simple convenience function so avoid having inconsistent program_id and program_id_index
fn make_instruction<'a>(
⋮----
fn test_get_signature_details_no_instructions() {
⋮----
let signature_details = get_precompile_signature_details(instructions);
assert_eq!(signature_details.num_secp256k1_instruction_signatures, 0);
assert_eq!(signature_details.num_ed25519_instruction_signatures, 0);
⋮----
fn test_get_signature_details_no_sigs_unique() {
⋮----
make_instruction(&program_ids, 0, &[]),
make_instruction(&program_ids, 1, &[]),
⋮----
let signature_details = get_precompile_signature_details(instructions.into_iter());
⋮----
fn test_get_signature_details_signatures_mixed() {
⋮----
make_instruction(&program_ids, 1, &[5]),
make_instruction(&program_ids, 2, &[3]),
make_instruction(&program_ids, 3, &[4]),
⋮----
make_instruction(&program_ids, 2, &[2]),
make_instruction(&program_ids, 1, &[1]),
⋮----
make_instruction(&program_ids, 3, &[3]),
⋮----
assert_eq!(signature_details.num_secp256k1_instruction_signatures, 6);
assert_eq!(signature_details.num_ed25519_instruction_signatures, 5);
assert_eq!(signature_details.num_secp256r1_instruction_signatures, 7);
⋮----
fn test_get_signature_details_missing_num_signatures() {
⋮----
assert_eq!(signature_details.num_secp256r1_instruction_signatures, 0);

================
File: runtime-transaction/src/transaction_meta.rs
================
pub trait StaticMeta {
⋮----
pub trait DynamicMeta: StaticMeta {}
⋮----
pub struct TransactionMeta {

================
File: runtime-transaction/src/transaction_with_meta.rs
================
pub trait TransactionWithMeta: StaticMeta + SVMTransaction {

================
File: runtime-transaction/Cargo.toml
================
[package]
name = "solana-runtime-transaction"
description = "Solana runtime-transaction"
documentation = "https://docs.rs/solana-runtime-transaction"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_runtime_transaction"

[features]
agave-unstable-api = []
dev-context-only-utils = ["solana-compute-budget-instruction/dev-context-only-utils"]

[dependencies]
agave-transaction-view = { workspace = true }
log = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-instruction = { workspace = true }
solana-hash = { workspace = true }
solana-message = { workspace = true, features = ["blake3"] }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signature = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
agave-feature-set = { workspace = true }
agave-reserved-account-keys = { workspace = true }
bincode = { workspace = true }
criterion = { workspace = true }
rand = { workspace = true }
solana-compute-budget-instruction = { workspace = true, features = ["dev-context-only-utils"] }
solana-compute-budget-interface = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-runtime-transaction = { path = ".", features = ["agave-unstable-api"] }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true, features = ["bincode"] }
solana-system-transaction = { workspace = true }
solana-transaction = { workspace = true, features = ["blake3"] }
solana-vote-interface = { workspace = true }

[[bench]]
name = "get_signature_details"
harness = false

[lints]
workspace = true

================
File: rust-toolchain.toml
================
[toolchain]
channel = "1.90.0"

================
File: rustfmt.toml
================
imports_granularity = "One"
format_strings = true
group_imports = "One"

ignore = [
    "jito-programs",
    "anchor"
]

================
File: s
================
#!/usr/bin/env bash

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" &>/dev/null && pwd)"

if [ -f .env ]; then
  export $(cat .env | grep -v '#' | awk '/=/ {print $1}')
else
  echo "Missing .env file"
  exit 0
fi

echo "Syncing to host: $HOST"

# sync to build server, ignoring local builds and local/remote dev ledger
rsync -avh --delete --exclude target --exclude docker-output "$SCRIPT_DIR" "$HOST":~/

================
File: scheduler-bindings/src/lib.rs
================
pub struct SharableTransactionRegion {
⋮----
pub struct SharablePubkeys {
⋮----
pub struct SharableTransactionBatchRegion {
⋮----
pub struct TransactionResponseRegion {
⋮----
pub struct TpuToPackMessage {
⋮----
pub mod tpu_message_flags {
⋮----
pub struct ProgressMessage {
⋮----
pub struct PackToWorkerMessage {
⋮----
pub mod pack_message_flags {
⋮----
pub mod execution_flags {
⋮----
pub mod check_flags {
⋮----
pub mod processed_codes {
⋮----
pub struct WorkerToPackMessage {
⋮----
pub mod worker_message_types {
use crate::SharablePubkeys;
⋮----
pub struct ExecutionResponse {
⋮----
pub mod not_included_reasons {
⋮----
pub mod parsing_and_sanitization_flags {
⋮----
pub mod status_check_flags {
⋮----
pub mod fee_payer_balance_flags {
⋮----
pub mod resolve_flags {
⋮----
pub struct CheckResponse {

================
File: scheduler-bindings/Cargo.toml
================
[package]
name = "agave-scheduler-bindings"
description = "Agave scheduler-binding message types for external pack process integration"
documentation = "https://docs.rs/agave-scheduler-bindings"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]

[lints]
workspace = true

================
File: scheduling-utils/src/handshake/client.rs
================
type RtsError = rts_alloc::error::Error;
type ShaqError = shaq::error::Error;
⋮----
pub fn connect(
⋮----
connect_path(path.as_ref(), logon, timeout)
⋮----
fn connect_path(
⋮----
stream.set_read_timeout(Some(timeout))?;
stream.set_write_timeout(Some(timeout))?;
send_logon(&mut stream, logon)?;
let fds = recv_response(&mut stream)?;
let session = setup_session(&logon, fds)?;
Ok(session)
⋮----
fn send_logon(stream: &mut UnixStream, logon: ClientLogon) -> Result<(), ClientHandshakeError> {
⋮----
buf[..8].copy_from_slice(&VERSION.to_le_bytes());
⋮----
let ptr = buf[8..LOGON_END].as_mut_ptr().cast::<ClientLogon>();
⋮----
stream.write_all(&buf)?;
Ok(())
⋮----
fn recv_response(stream: &mut UnixStream) -> Result<Vec<i32>, ClientHandshakeError> {
⋮----
let mut cmsgs = [0u8; unsafe { CMSG_LEN(CMSG_MAX_SIZE as u32) as usize }];
⋮----
stream.as_raw_fd(),
⋮----
Some(&mut cmsgs),
⋮----
let buf = msg.iovs().next().unwrap();
⋮----
let reason = std::str::from_utf8(&buf[2..2 + reason_len]).unwrap();
return Err(ClientHandshakeError::Rejected(reason.to_string()));
⋮----
let mut cmsgs = msg.cmsgs().unwrap();
let fds = match cmsgs.next() {
⋮----
Some(msg) => panic!("Unexpected; msg={msg:?}"),
None => panic!(),
⋮----
Ok(fds)
⋮----
pub fn setup_session(
⋮----
panic!();
⋮----
.map(|offset| {
⋮----
.checked_add(logon.worker_count)
.unwrap()
.checked_add(offset)
.unwrap();
unsafe { Allocator::join(&allocator_file, u32::try_from(id).unwrap()) }
⋮----
if worker_fds.is_empty()
|| !worker_fds.len().is_multiple_of(2)
|| worker_fds.len() / 2 != logon.worker_count
⋮----
return Err(ClientHandshakeError::ProtocolViolation);
⋮----
.chunks(2)
.map(|window| {
⋮----
Ok(ClientWorkerSession {
⋮----
pub struct ClientSession {
⋮----
pub struct ClientWorkerSession {
⋮----
pub enum ClientHandshakeError {
⋮----
fn from(value: nix::Error) -> Self {
Self::Io(value.into())

================
File: scheduling-utils/src/handshake/mod.rs
================
pub mod client;
pub mod server;
mod shared;
⋮----
mod tests;

================
File: scheduling-utils/src/handshake/server.rs
================
type ShaqError = shaq::error::Error;
type RtsAllocError = rts_alloc::error::Error;
⋮----
pub struct Server {
⋮----
impl Server {
pub fn new(path: impl AsRef<Path>) -> Result<Self, std::io::Error> {
⋮----
Ok(Self {
⋮----
pub fn accept(&mut self) -> Result<AgaveSession, AgaveHandshakeError> {
let (mut stream, _) = self.listener.accept()?;
stream.set_read_timeout(Some(HANDSHAKE_TIMEOUT))?;
match self.handle_logon(&mut stream) {
Ok(session) => Ok(session),
⋮----
let reason = err.to_string();
let reason_len = u8::try_from(reason.len()).unwrap_or(u8::MAX);
let buffer_len = 2usize.checked_add(usize::from(reason_len)).unwrap();
⋮----
.copy_from_slice(&reason.as_bytes()[..usize::from(reason_len)]);
stream.set_nonblocking(true)?;
let _ = stream.write(&self.buffer[..buffer_len])?;
Err(err)
⋮----
fn handle_logon(
⋮----
let logon = self.recv_logon(stream)?;
⋮----
let fds_raw: Vec<_> = files.iter().map(|file| file.as_raw_fd()).collect();
⋮----
socket::sendmsg::<UnixAddr>(stream.as_raw_fd(), &iov, &cmsgs, MsgFlags::empty(), None)
.map_err(std::io::Error::from)?;
debug_assert_eq!(sent, 1);
Ok(session)
⋮----
fn recv_logon(&mut self, stream: &mut UnixStream) -> Result<ClientLogon, AgaveHandshakeError> {
⋮----
while buffer_len < self.buffer.len() {
let read = stream.read(&mut self.buffer[buffer_len..])?;
⋮----
return Err(AgaveHandshakeError::EofDuringHandshake);
⋮----
buffer_len = buffer_len.checked_add(read).unwrap();
if handshake_start.elapsed() > HANDSHAKE_TIMEOUT {
return Err(AgaveHandshakeError::Timeout);
⋮----
let version = u64::from_le_bytes(self.buffer[..8].try_into().unwrap());
⋮----
return Err(AgaveHandshakeError::Version {
⋮----
let logon = ClientLogon::try_from_bytes(&self.buffer[8..LOGON_END]).unwrap();
if !(1..=MAX_WORKERS).contains(&logon.worker_count) {
return Err(AgaveHandshakeError::WorkerCount(logon.worker_count));
⋮----
if !(1..=MAX_ALLOCATOR_HANDLES).contains(&logon.allocator_handles) {
return Err(AgaveHandshakeError::AllocatorHandles(
⋮----
Ok(logon)
⋮----
pub fn setup_session(
⋮----
let (worker_files, workers) = (0..logon.worker_count).try_fold(
⋮----
let worker_index = GLOBAL_ALLOCATORS.checked_add(offset).unwrap();
let worker_index = u32::try_from(worker_index).unwrap();
⋮----
fds.extend([pack_to_worker_file, worker_to_pack_file]);
workers.push(AgaveWorkerSession {
⋮----
Ok((
⋮----
.into_iter()
.chain(worker_files)
.collect(),
⋮----
fn create_allocator(logon: &ClientLogon) -> Result<(File, Allocator), RtsAllocError> {
⋮----
.checked_add(logon.worker_count)
.unwrap()
.checked_add(logon.allocator_handles)
.unwrap();
⋮----
u32::try_from(allocator_count).unwrap(),
⋮----
.map(|allocator| (allocator_file, allocator))
⋮----
create(true).or_else(|_| create(false))
⋮----
fn create_producer<T>(
⋮----
unsafe { shaq::Producer::create(&file, file_size) }.map(|producer| (file, producer))
⋮----
true => create(true).or_else(|_| create(false)),
false => create(false),
⋮----
fn create_consumer(
⋮----
unsafe { shaq::Consumer::create(&file, file_size) }.map(|producer| (file, producer))
⋮----
fn create_shmem(huge: bool) -> Result<File, std::io::Error> {
⋮----
let ret = libc::memfd_create(SHMEM_NAME.as_ptr(), flags);
⋮----
return Err(std::io::Error::last_os_error());
⋮----
Ok(File::from_raw_fd(ret))
⋮----
return Err(std::io::ErrorKind::Unsupported.into());
⋮----
let ret = libc::shm_unlink(SHMEM_NAME.as_ptr());
⋮----
if err.kind() != std::io::ErrorKind::NotFound {
return Err(err);
⋮----
SHMEM_NAME.as_ptr(),
⋮----
Ok(file)
⋮----
fn align_file_size(size: usize, huge: bool) -> usize {
⋮----
true => size.next_multiple_of(2 * 1024 * 1024),
false => size.next_multiple_of(4096),
⋮----
pub struct AgaveSession {
⋮----
pub struct AgaveTpuToPackSession {
⋮----
pub struct AgaveWorkerSession {
⋮----
pub enum AgaveHandshakeError {

================
File: scheduling-utils/src/handshake/shared.rs
================
pub struct ClientLogon {
⋮----
impl ClientLogon {
pub fn try_from_bytes(buffer: &[u8]) -> Option<Self> {
if buffer.len() != core::mem::size_of::<Self>() {
⋮----
Some(unsafe { core::ptr::read_unaligned(buffer.as_ptr().cast()) })
⋮----
pub mod logon_flags {

================
File: scheduling-utils/src/handshake/tests.rs
================
fn message_passing_on_all_queues() {
let ipc = NamedTempFile::new().unwrap();
std::fs::remove_file(ipc.path()).unwrap();
let mut server = Server::new(ipc.path()).unwrap();
⋮----
let mut session = server.accept().unwrap();
session.tpu_to_pack.producer.try_write(tpu_to_pack).unwrap();
session.tpu_to_pack.producer.commit();
⋮----
.try_write(progress_tracker)
.unwrap();
session.progress_tracker.commit();
for (i, worker) in session.workers.iter_mut().enumerate() {
⋮----
worker.pack_to_worker.sync();
if let Some(msg) = worker.pack_to_worker.try_read() {
⋮----
assert_eq!(
⋮----
.try_write(WorkerToPackMessage {
⋮----
worker.worker_to_pack.commit();
⋮----
let mut session = connect(
⋮----
session.tpu_to_pack.sync();
if let Some(msg) = session.tpu_to_pack.try_read() {
⋮----
assert_eq!(msg, tpu_to_pack);
⋮----
session.progress_tracker.sync();
if let Some(msg) = session.progress_tracker.try_read() {
⋮----
assert_eq!(msg, progress_tracker);
⋮----
.try_write(PackToWorkerMessage {
⋮----
worker.pack_to_worker.commit();
⋮----
worker.worker_to_pack.sync();
if let Some(msg) = worker.worker_to_pack.try_read() {
⋮----
client_handle.join().unwrap();
server_handle.join().unwrap();
⋮----
fn accept_worker_count_max() {
⋮----
let res = server.accept();
assert!(res.is_ok());
⋮----
let res = connect(
⋮----
fn reject_worker_count_low() {
⋮----
panic!();
⋮----
assert_eq!(count, 0);
⋮----
assert_eq!(reason, "Worker count; count=0");
⋮----
fn reject_worker_count_high() {
⋮----
assert_eq!(count, 100);
⋮----
assert_eq!(reason, "Worker count; count=100");

================
File: scheduling-utils/src/error.rs
================
pub fn transaction_result_to_not_included_reason(result: &Result<(), TransactionError>) -> u8 {
⋮----
Err(err) => transaction_error_to_not_included_reason(err),
⋮----
pub fn transaction_error_to_not_included_reason(error: &TransactionError) -> u8 {

================
File: scheduling-utils/src/lib.rs
================
pub mod error;
pub mod thread_aware_account_locks;
⋮----
pub mod handshake;
⋮----
pub mod pubkeys_ptr;
⋮----
pub mod responses_region;
⋮----
pub mod transaction_ptr;

================
File: scheduling-utils/src/pubkeys_ptr.rs
================
pub struct PubkeysPtr {
⋮----
impl PubkeysPtr {
⋮----
pub unsafe fn from_raw_parts(ptr: NonNull<Pubkey>, count: usize) -> Self {
⋮----
pub unsafe fn from_sharable_pubkeys(
⋮----
assert_ne!(sharable_pubkeys.num_pubkeys, 0);
let ptr = allocator.ptr_from_offset(sharable_pubkeys.offset).cast();
⋮----
pub fn as_slice(&self) -> &[Pubkey] {
unsafe { core::slice::from_raw_parts(self.ptr.as_ptr(), self.count) }
⋮----
pub unsafe fn free(self, allocator: &Allocator) {
unsafe { allocator.free(self.ptr.cast()) };

================
File: scheduling-utils/src/responses_region.rs
================
pub fn execution_responses_from_iter(
⋮----
unsafe { from_iterator(allocator, EXECUTION_RESPONSE, iter) }
⋮----
pub fn resolve_responses_from_iter(
⋮----
unsafe { from_iterator(allocator, CHECK_RESPONSE, iter) }
⋮----
pub fn allocate_check_response_region(
⋮----
unsafe fn allocate_response_region<T: Sized>(
⋮----
let size = num_transaction_responses.wrapping_mul(core::mem::size_of::<T>());
let response_ptr = allocator.allocate(size as u32)?.cast::<T>();
debug_assert!(
⋮----
let transaction_responses_offset = unsafe { allocator.offset(response_ptr.cast()) };
Some((
⋮----
unsafe fn from_iterator<T: Sized>(
⋮----
let num_transaction_responses = iter.len();
⋮----
unsafe { allocate_response_region(allocator, tag, num_transaction_responses)? };
for (index, response) in iter.enumerate() {
unsafe { response_ptr.add(index).write(response) };
⋮----
Some(region)
⋮----
pub struct CheckResponsesPtr {
⋮----
impl CheckResponsesPtr {
⋮----
pub unsafe fn from_raw_parts(ptr: NonNull<CheckResponse>, count: usize) -> Self {
⋮----
pub unsafe fn from_transaction_response_region(
⋮----
debug_assert!(transaction_response_region.tag == worker_message_types::CHECK_RESPONSE);
⋮----
.ptr_from_offset(transaction_response_region.transaction_responses_offset)
.cast(),
⋮----
pub const fn len(&self) -> usize {
⋮----
pub const fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub fn iter(&self) -> impl Iterator<Item = &CheckResponse> {
unsafe { core::slice::from_raw_parts(self.ptr.as_ptr(), self.count) }.iter()
⋮----
pub unsafe fn free(self, allocator: &Allocator) {
unsafe { allocator.free(self.ptr.cast()) }
⋮----
pub struct ExecutionResponsesPtr {
⋮----
impl ExecutionResponsesPtr {
⋮----
pub unsafe fn from_raw_parts(ptr: NonNull<ExecutionResponse>, count: usize) -> Self {
⋮----
debug_assert!(transaction_response_region.tag == worker_message_types::EXECUTION_RESPONSE);
⋮----
pub fn iter(&self) -> impl Iterator<Item = &ExecutionResponse> {

================
File: scheduling-utils/src/thread_aware_account_locks.rs
================
pub type ThreadId = usize;
type LockCount = u32;
⋮----
pub struct ThreadSet(u64);
⋮----
struct AccountWriteLocks {
⋮----
struct AccountReadLocks {
⋮----
struct AccountLocks {
⋮----
pub enum TryLockError {
⋮----
pub struct ThreadAwareAccountLocks {
⋮----
impl ThreadAwareAccountLocks {
pub fn new(num_threads: usize) -> Self {
assert!(num_threads > 0, "num threads must be > 0");
assert!(
⋮----
pub fn try_lock_accounts<'a>(
⋮----
.accounts_schedulable_threads(write_account_locks.clone(), read_account_locks.clone())
.ok_or(TryLockError::MultipleConflicts)?;
⋮----
if schedulable_threads.is_empty() {
return Err(TryLockError::ThreadNotAllowed);
⋮----
let thread_id = thread_selector(schedulable_threads);
self.lock_accounts(write_account_locks, read_account_locks, thread_id);
Ok(thread_id)
⋮----
/// Unlocks the accounts for the given thread.
    pub fn unlock_accounts<'a>(
⋮----
pub fn unlock_accounts<'a>(
⋮----
self.write_unlock_account(account, thread_id);
⋮----
self.read_unlock_account(account, thread_id);
⋮----
fn accounts_schedulable_threads<'a>(
⋮----
schedulable_threads &= self.write_schedulable_threads(account);
⋮----
schedulable_threads &= self.read_schedulable_threads(account);
⋮----
Some(schedulable_threads)
⋮----
/// Returns `ThreadSet` of schedulable threads for the given readable account.
    fn read_schedulable_threads(&self, account: &Pubkey) -> ThreadSet {
⋮----
fn read_schedulable_threads(&self, account: &Pubkey) -> ThreadSet {
⋮----
/// Returns `ThreadSet` of schedulable threads for the given writable account.
    fn write_schedulable_threads(&self, account: &Pubkey) -> ThreadSet {
⋮----
fn write_schedulable_threads(&self, account: &Pubkey) -> ThreadSet {
⋮----
/// Returns `ThreadSet` of schedulable threads.
    /// If there are no locks, then all threads are schedulable.
⋮----
/// If there are no locks, then all threads are schedulable.
    /// If only write-locked, then only the thread holding the write lock is schedulable.
⋮----
/// If only write-locked, then only the thread holding the write lock is schedulable.
    /// If a mix of locks, then only the write thread is schedulable.
⋮----
/// If a mix of locks, then only the write thread is schedulable.
    /// If only read-locked, the only write-schedulable thread is if a single thread
⋮----
/// If only read-locked, the only write-schedulable thread is if a single thread
    ///   holds all read locks. Otherwise, no threads are write-schedulable.
⋮----
///   holds all read locks. Otherwise, no threads are write-schedulable.
    /// If only read-locked, all threads are read-schedulable.
⋮----
/// If only read-locked, all threads are read-schedulable.
    fn schedulable_threads<const WRITE: bool>(&self, account: &Pubkey) -> ThreadSet {
⋮----
fn schedulable_threads<const WRITE: bool>(&self, account: &Pubkey) -> ThreadSet {
match self.locks.get(account) {
⋮----
.only_one_contained()
.map(ThreadSet::only)
.unwrap_or_else(ThreadSet::none)
⋮----
assert_eq!(
⋮----
}) => unreachable!(),
⋮----
/// Add locks for all writable and readable accounts on `thread_id`.
    fn lock_accounts<'a>(
⋮----
fn lock_accounts<'a>(
⋮----
self.write_lock_account(account, thread_id);
⋮----
self.read_lock_account(account, thread_id);
⋮----
fn write_lock_account(&mut self, account: &Pubkey, thread_id: ThreadId) {
let entry = self.locks.entry(*account).or_default();
⋮----
write_locks.lock_count = write_locks.lock_count.wrapping_add(1);
⋮----
*write_locks = Some(AccountWriteLocks {
⋮----
fn write_unlock_account(&mut self, account: &Pubkey, thread_id: ThreadId) {
let Entry::Occupied(mut entry) = self.locks.entry(*account) else {
panic!("write lock must exist for account: {account}");
⋮----
} = entry.get_mut();
⋮----
write_locks.lock_count = write_locks.lock_count.wrapping_sub(1);
⋮----
if read_locks.is_none() {
entry.remove();
⋮----
fn read_lock_account(&mut self, account: &Pubkey, thread_id: ThreadId) {
⋮----
} = self.locks.entry(*account).or_default();
⋮----
read_locks.thread_set.insert(thread_id);
⋮----
read_locks.lock_counts[thread_id].wrapping_add(1);
⋮----
*read_locks = Some(AccountReadLocks {
⋮----
fn read_unlock_account(&mut self, account: &Pubkey, thread_id: ThreadId) {
⋮----
panic!("read lock must exist for account: {account}");
⋮----
read_locks.lock_counts[thread_id] = read_locks.lock_counts[thread_id].wrapping_sub(1);
⋮----
read_locks.thread_set.remove(thread_id);
if read_locks.thread_set.is_empty() {
⋮----
if write_locks.is_none() {
⋮----
impl BitAnd for ThreadSet {
type Output = Self;
fn bitand(self, rhs: Self) -> Self::Output {
Self(self.0 & rhs.0)
⋮----
impl BitAndAssign for ThreadSet {
fn bitand_assign(&mut self, rhs: Self) {
⋮----
impl Sub for ThreadSet {
⋮----
fn sub(self, rhs: Self) -> Self::Output {
Self(self.0 & !rhs.0)
⋮----
impl Display for ThreadSet {
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
write!(f, "ThreadSet({:#0width$b})", self.0, width = MAX_THREADS)
⋮----
impl Debug for ThreadSet {
⋮----
impl ThreadSet {
⋮----
pub const fn none() -> Self {
Self(0b0)
⋮----
pub const fn any(num_threads: usize) -> Self {
⋮----
Self(u64::MAX)
⋮----
Self(Self::as_flag(num_threads).wrapping_sub(1))
⋮----
pub const fn only(thread_id: ThreadId) -> Self {
Self(Self::as_flag(thread_id))
⋮----
pub fn num_threads(&self) -> u32 {
self.0.count_ones()
⋮----
pub fn only_one_contained(&self) -> Option<ThreadId> {
(self.num_threads() == 1).then_some(self.0.trailing_zeros() as ThreadId)
⋮----
pub fn is_empty(&self) -> bool {
⋮----
pub fn contains(&self, thread_id: ThreadId) -> bool {
⋮----
pub fn insert(&mut self, thread_id: ThreadId) {
⋮----
pub fn remove(&mut self, thread_id: ThreadId) {
⋮----
pub fn contained_threads_iter(self) -> impl Iterator<Item = ThreadId> {
ThreadSetIterator(self.0)
⋮----
const fn as_flag(thread_id: ThreadId) -> u64 {
⋮----
struct ThreadSetIterator(u64);
impl Iterator for ThreadSetIterator {
type Item = ThreadId;
fn next(&mut self) -> Option<Self::Item> {
⋮----
let thread_id = self.0.trailing_zeros() as ThreadId;
self.0 &= self.0.wrapping_sub(1);
Some(thread_id)
⋮----
mod tests {
⋮----
fn test_thread_selector(thread_set: ThreadSet) -> ThreadId {
thread_set.contained_threads_iter().next().unwrap()
⋮----
fn test_too_few_num_threads() {
⋮----
fn test_too_many_num_threads() {
⋮----
fn test_try_lock_accounts_none() {
⋮----
locks.read_lock_account(&pk1, 2);
locks.read_lock_account(&pk1, 3);
⋮----
fn test_try_lock_accounts_one() {
⋮----
locks.write_lock_account(&pk2, 3);
⋮----
fn test_try_lock_accounts_one_not_allowed() {
⋮----
fn test_try_lock_accounts_multiple() {
⋮----
locks.read_lock_account(&pk2, 0);
⋮----
fn test_try_lock_accounts_any() {
⋮----
fn test_accounts_schedulable_threads_no_outstanding_locks() {
⋮----
fn test_accounts_schedulable_threads_outstanding_write_only() {
⋮----
locks.write_lock_account(&pk1, 2);
⋮----
fn test_accounts_schedulable_threads_outstanding_read_only() {
⋮----
locks.read_lock_account(&pk1, 0);
⋮----
fn test_accounts_schedulable_threads_outstanding_mixed() {
⋮----
fn test_write_lock_account_write_conflict_panic() {
⋮----
locks.write_lock_account(&pk1, 0);
locks.write_lock_account(&pk1, 1);
⋮----
fn test_write_lock_account_read_conflict_panic() {
⋮----
fn test_write_unlock_account_not_locked() {
⋮----
locks.write_unlock_account(&pk1, 0);
⋮----
fn test_write_unlock_account_thread_mismatch() {
⋮----
fn test_read_lock_account_write_conflict_panic() {
⋮----
locks.read_lock_account(&pk1, 1);
⋮----
fn test_read_unlock_account_not_locked() {
⋮----
locks.read_unlock_account(&pk1, 1);
⋮----
fn test_read_unlock_account_thread_mismatch() {
⋮----
fn test_write_locking() {
⋮----
locks.write_unlock_account(&pk1, 1);
⋮----
assert!(locks.locks.is_empty());
⋮----
fn test_read_locking() {
⋮----
fn test_lock_accounts_invalid_thread() {
⋮----
locks.lock_accounts([&pk1].into_iter(), std::iter::empty(), TEST_NUM_THREADS);
⋮----
fn test_thread_set() {
⋮----
assert!(thread_set.is_empty());
assert_eq!(thread_set.num_threads(), 0);
assert_eq!(thread_set.only_one_contained(), None);
⋮----
assert!(!thread_set.contains(idx));
⋮----
thread_set.insert(4);
assert!(!thread_set.is_empty());
assert_eq!(thread_set.num_threads(), 1);
assert_eq!(thread_set.only_one_contained(), Some(4));
⋮----
assert_eq!(thread_set.contains(idx), idx == 4);
⋮----
thread_set.insert(2);
⋮----
assert_eq!(thread_set.num_threads(), 2);
⋮----
assert_eq!(thread_set.contains(idx), idx == 2 || idx == 4);
⋮----
thread_set.remove(4);
⋮----
assert_eq!(thread_set.only_one_contained(), Some(2));
⋮----
assert_eq!(thread_set.contains(idx), idx == 2);
⋮----
fn test_thread_set_any_zero() {
⋮----
assert_eq!(any_threads.num_threads(), 0);
⋮----
fn test_thread_set_any_max() {
⋮----
assert_eq!(any_threads.num_threads(), MAX_THREADS as u32);
⋮----
fn test_thread_set_iter() {
⋮----
assert!(thread_set.contained_threads_iter().next().is_none());
⋮----
thread_set.insert(5);
⋮----
thread_set.insert(63);
⋮----
thread_set.remove(5);

================
File: scheduling-utils/src/transaction_ptr.rs
================
pub struct TransactionPtr {
⋮----
impl TransactionData for TransactionPtr {
fn data(&self) -> &[u8] {
unsafe { core::slice::from_raw_parts(self.ptr.as_ptr(), self.count) }
⋮----
impl TransactionData for &TransactionPtr {
⋮----
impl TransactionPtr {
⋮----
pub unsafe fn from_raw_parts(ptr: NonNull<u8>, count: usize) -> Self {
⋮----
pub unsafe fn from_sharable_transaction_region(
⋮----
let ptr = allocator.ptr_from_offset(sharable_transaction_region.offset);
⋮----
pub unsafe fn to_sharable_transaction_region(
⋮----
let offset = unsafe { allocator.offset(self.ptr) };
⋮----
pub unsafe fn free(self, allocator: &Allocator) {
unsafe { allocator.free(self.ptr) }
⋮----
pub struct TransactionPtrBatch<'a, M = ()> {
⋮----
assert!(Self::TX_TOTAL_SIZE * MAX_TRANSACTIONS_PER_MESSAGE < 4096);
⋮----
pub unsafe fn from_sharable_transaction_batch_region(
⋮----
let base = allocator.ptr_from_offset(sharable_transaction_batch_region.transactions_offset);
let tx_ptr = base.cast();
// SAFETY:
// - Assuming the batch was originally allocated to support `M`, this call will also be
//   safe.
let meta_ptr = unsafe { base.byte_add(Self::TX_BATCH_META_OFFSET).cast() };
⋮----
/// The number of transactions in this batch.
    pub const fn len(&self) -> usize {
⋮----
pub const fn len(&self) -> usize {
⋮----
/// Whether the batch is empty.
    pub const fn is_empty(&self) -> bool {
⋮----
pub const fn is_empty(&self) -> bool {
self.len() == 0
⋮----
/// Iterator returning [`TransactionPtr`] for each transaction in the batch.
    pub fn iter(&'a self) -> impl Iterator<Item = (TransactionPtr, M)> + 'a {
⋮----
pub fn iter(&'a self) -> impl Iterator<Item = (TransactionPtr, M)> + 'a {
(0..self.num_transactions).map(|idx| unsafe {
let tx = self.tx_ptr.add(idx);
let tx = TransactionPtr::from_sharable_transaction_region(tx.as_ref(), self.allocator);
let meta = self.meta_ptr.add(idx).read();
⋮----
pub unsafe fn free(self) {
unsafe { self.allocator.free(self.tx_ptr.cast()) }

================
File: scheduling-utils/Cargo.toml
================
[package]
name = "agave-scheduling-utils"
description = "Common utilities for building Agave scheduler implementations"
documentation = "https://docs.rs/agave-scheduling-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
agave-scheduler-bindings = { workspace = true }
ahash = { workspace = true }
solana-pubkey = { workspace = true }
solana-transaction-error = { workspace = true }

[target."cfg(unix)".dependencies]
agave-transaction-view = { workspace = true }
libc = { workspace = true }
nix = { workspace = true, features = ["socket", "uio"] }
rts-alloc = { workspace = true }
shaq = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
tempfile = { workspace = true }

[target."cfg(unix)".dev-dependencies]
agave-scheduler-bindings = { workspace = true, features = ["dev-context-only-utils"] }

[lints]
workspace = true

================
File: scripts/agave-build-lists.sh
================
AGAVE_BINS_DEV=(
  cargo-build-sbf
  cargo-test-sbf
  solana-test-validator
)
AGAVE_BINS_END_USER=(
  agave-install
  solana
  solana-keygen
)
AGAVE_BINS_VAL_OP=(
  agave-validator
  agave-watchtower
  solana-gossip
  solana-genesis
  solana-faucet
)
AGAVE_BINS_DCOU=(
  agave-ledger-tool
  solana-bench-tps
)
AGAVE_BINS_DEPRECATED=(
  solana-stake-accounts
  solana-tokens
  agave-install-init
)
DCOU_TAINTED_PACKAGES=(
  agave-ledger-tool
  agave-store-histogram
  agave-store-tool
  solana-accounts-cluster-bench
  solana-banking-bench
  solana-bench-tps
  solana-dos
  solana-local-cluster
  solana-transaction-dos
  solana-vortexor
)

================
File: scripts/agave-install-deploy.sh
================
set -e
SOLANA_ROOT="$(cd "$(dirname "$0")"/..; pwd)"
maybeKeypair=
while [[ ${1:0:2} = -- ]]; do
  if [[ $1 = --keypair && -n $2 ]]; then
    maybeKeypair="$1 $2"
    shift 2
  else
    echo "Error: Unknown option: $1"
    exit 1
  fi
done
URL=$1
TAG=$2
OS=${3:-linux}
if [[ -z $URL || -z $TAG ]]; then
  echo "Usage: $0 [stable|localhost|RPC URL] [edge|beta|release tag] [linux|osx|windows]"
  exit 0
fi
if [[ ! -f update_manifest_keypair.json ]]; then
  "$SOLANA_ROOT"/scripts/agave-install-update-manifest-keypair.sh "$OS"
fi
source "$SOLANA_ROOT"/scripts/generate-target-triple.sh
TARGET="$BUILD_TARGET_TRIPLE"
case $URL in
stable)
  URL=http://api.devnet.solana.com
  ;;
localhost)
  URL=http://localhost:8899
  ;;
*)
  ;;
esac
case $TAG in
edge|beta)
  DOWNLOAD_URL=https://release.jito.wtf/"$TAG"/solana-release-$TARGET.tar.bz2
  ;;
*)
  DOWNLOAD_URL=https://github.com/jito-foundation/jito-solana/releases/download/"$TAG"/solana-release-$TARGET.tar.bz2
  ;;
esac
PATH="$SOLANA_ROOT"/target/debug:$PATH
set -x
balance=$(solana $maybeKeypair --url "$URL" balance --lamports)
if [[ $balance = "0 lamports" ]]; then
  solana $maybeKeypair --url "$URL" airdrop 0.000000042
fi
agave-install deploy $maybeKeypair --url "$URL" "$DOWNLOAD_URL" update_manifest_keypair.json

================
File: scripts/agave-install-update-manifest-keypair.sh
================
set -e
SOLANA_ROOT="$(cd "$(dirname "$0")"/..; pwd)"
source "$SOLANA_ROOT"/scripts/generate-target-triple.sh
TARGET="$BUILD_TARGET_TRIPLE"
SOLANA_INSTALL_UPDATE_MANIFEST_KEYPAIR="SOLANA_INSTALL_UPDATE_MANIFEST_KEYPAIR_${TARGET//-/_}"
if [[ -z ${!SOLANA_INSTALL_UPDATE_MANIFEST_KEYPAIR} ]]; then
  echo "$SOLANA_INSTALL_UPDATE_MANIFEST_KEYPAIR not defined"
  exit 1
fi
echo "${!SOLANA_INSTALL_UPDATE_MANIFEST_KEYPAIR}" > update_manifest_keypair.json
ls -l update_manifest_keypair.json

================
File: scripts/build-agave-xdp-ebpf.sh
================
set -e
SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
REPO_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
BEFORE_HASH=$(sha256sum "$REPO_ROOT/xdp-ebpf/agave-xdp-prog" | awk '{print $1}')
echo "Hash before rebuild: $BEFORE_HASH"
source "$REPO_ROOT/ci/rust-version.sh"
echo "Using nightly toolchain: $rust_nightly"
if ! command -v bpf-linker &> /dev/null; then
    echo "Installing bpf-linker..."
    cargo install bpf-linker==0.9.15
fi
rustup component add rust-src --toolchain "$rust_nightly"
cargo +"$rust_nightly" rustc --manifest-path "$REPO_ROOT/xdp-ebpf/Cargo.toml" \
    --target bpfel-unknown-none --release --features agave-unstable-api,ebpf \
    -Z build-std=core
llvm-objcopy --strip-unneeded "$REPO_ROOT/target/bpfel-unknown-none/release/agave-xdp-prog" "$REPO_ROOT/xdp-ebpf/agave-xdp-prog"
AFTER_HASH=$(sha256sum "$REPO_ROOT/xdp-ebpf/agave-xdp-prog" | awk '{print $1}')
echo "Hash after rebuild:  $AFTER_HASH"
if [ "$BEFORE_HASH" = "$AFTER_HASH" ]; then
    echo "✓ Hash unchanged"
else
    echo "✗ Hash changed"
fi

================
File: scripts/build-downstream-anchor-projects.sh
================
set -e
cd "$(dirname "$0")"/..
source ci/_
source scripts/patch-crates.sh
source scripts/read-cargo-variable.sh
source scripts/patch-spl-crates-for-anchor.sh
anchor_version=$1
solana_ver=$(readCargoVariable version Cargo.toml)
solana_dir=$PWD
cargo="$solana_dir"/cargo
cargo_build_sbf="$solana_dir"/cargo-build-sbf
cargo_test_sbf="$solana_dir"/cargo-test-sbf
mkdir -p target/downstream-projects-anchor
cd target/downstream-projects-anchor
update_anchor_dependencies() {
  declare project_root="$1"
  declare anchor_ver="$2"
  declare tomls=()
  while IFS='' read -r line; do tomls+=("$line"); done < <(find "$project_root" -name Cargo.toml)
  sed -i -e "s#\(anchor-lang = \"\)[^\"]*\(\"\)#\1=$anchor_ver\2#g" "${tomls[@]}" || return $?
  sed -i -e "s#\(anchor-spl = \"\)[^\"]*\(\"\)#\1=$anchor_ver\2#g" "${tomls[@]}" || return $?
  sed -i -e "s#\(anchor-lang = { version = \"\)[^\"]*\(\"\)#\1=$anchor_ver\2#g" "${tomls[@]}" || return $?
  sed -i -e "s#\(anchor-spl = { version = \"\)[^\"]*\(\"\)#\1=$anchor_ver\2#g" "${tomls[@]}" || return $?
}
patch_crates_io_anchor() {
  declare Cargo_toml="$1"
  declare anchor_dir="$2"
  cat >> "$Cargo_toml" <<EOF
anchor-lang = { path = "$anchor_dir/lang" }
anchor-spl = { path = "$anchor_dir/spl" }
EOF
}
# NOTE This isn't run in a subshell to get $anchor_dir and $anchor_ver
anchor() {
  set -x
  rm -rf spl
  git clone https://github.com/solana-labs/solana-program-library.git spl
  cd spl || exit 1
  ./patch.crates-io.sh "$solana_dir"
  spl_dir=$PWD
  get_spl_versions "$spl_dir"
  cd ..
  rm -rf anchor
  git clone https://github.com/coral-xyz/anchor.git
  cd anchor || exit 1
  if [[ -n "$anchor_version" ]]; then
    git checkout "$anchor_version"
  fi
  cp "$solana_dir"/rust-toolchain.toml .
  update_solana_dependencies . "$solana_ver"
  patch_crates_io_solana Cargo.toml "$solana_dir"
  patch_spl_crates . Cargo.toml "$spl_dir"
  $cargo test --workspace --exclude avm
  (cd spl && $cargo_build_sbf --features stake)
  (cd client && $cargo test --all-features)
  anchor_dir=$PWD
  anchor_ver=$(readCargoVariable version "$anchor_dir"/lang/Cargo.toml)
  cd "$solana_dir"/target/downstream-projects-anchor
}
openbook() {
  rm -rf openbook-v2
  git clone https://github.com/openbook-dex/openbook-v2.git
  cd openbook-v2
  update_solana_dependencies . "$solana_ver"
  patch_crates_io_solana Cargo.toml "$solana_dir"
  $cargo_build_sbf --features enable-gpl
  cd programs/openbook-v2
  $cargo_test_sbf  --features enable-gpl
}
mango() {
  (
    set -x
    rm -rf mango-v4
    git clone https://github.com/blockworks-foundation/mango-v4.git
    cd mango-v4
    update_solana_dependencies . "$solana_ver"
    patch_crates_io_solana_no_header Cargo.toml "$solana_dir"
    $cargo_test_sbf --features enable-gpl
  )
}
metaplex() {
  (
    set -x
    rm -rf mpl-token-metadata
    git clone https://github.com/metaplex-foundation/mpl-token-metadata
    cp "$solana_dir"/rust-toolchain.toml mpl-token-metadata/
    cd mpl-token-metadata
    ./configs/program-scripts/dump.sh ./programs/bin
    ROOT_DIR=$(pwd)
    cd programs/token-metadata
    update_solana_dependencies . "$solana_ver"
    patch_crates_io_solana Cargo.toml "$solana_dir"
    OUT_DIR="$ROOT_DIR"/programs/bin
    export SBF_OUT_DIR="$OUT_DIR"
    $cargo_test_sbf --sbf-out-dir "${OUT_DIR}"
  )
}
_ anchor

================
File: scripts/cargo-clippy-nightly.sh
================
set -o errexit
here="$(dirname "$0")"
cargo="$(readlink -f "${here}/../cargo")"
if [[ -z $cargo ]]; then
  echo >&2 "Failed to find cargo. Mac readlink doesn't support -f. Consider switching
  to gnu readlink with 'brew install coreutils' and then symlink greadlink as
  /usr/local/bin/readlink."
  exit 1
fi
# shellcheck source=ci/rust-version.sh
source "$here/../ci/rust-version.sh" nightly
"$here/cargo-for-all-lock-files.sh" -- \
  "+${rust_nightly}" clippy \
  --workspace --all-targets --features dummy-for-ci-check,frozen-abi -- \
  --deny=warnings \
  --deny=clippy::default_trait_access \
  --deny=clippy::arithmetic_side_effects \
  --deny=clippy::manual_let_else \
  --deny=clippy::uninlined-format-args \
  --deny=clippy::used_underscore_binding

================
File: scripts/cargo-clippy.sh
================
set -o errexit
here="$(dirname "$0")"
# nightly
"$here/cargo-clippy-nightly.sh"

================
File: scripts/cargo-for-all-lock-files.sh
================
if ! command -v cargo &> /dev/null ; then
  >&2 echo "Failed to find cargo. Mac readlink doesn't support -f. Consider switching
  to gnu readlink with 'brew install coreutils' and then symlink greadlink as
  /usr/local/bin/readlink."
  exit 1
fi
set -e
shifted_args=()
while [[ -n $1 ]]; do
  if [[ $1 = -- ]]; then
    escape_marker=found
    shift
    break
  elif [[ $1 = "--ignore-exit-code" ]]; then
    ignore=1
    shift
  else
    shifted_args+=("$1")
    shift
  fi
done
if [[ -n $escape_marker && ${
  files="${shifted_args[*]}"
  for file in $files; do
    if [[ $file = "${file%Cargo.lock}" ]]; then
      echo "$0: unrecognizable as Cargo.lock path (prepend \"--\"?): $file" >&2
      exit 1
    fi
  done
  shifted_args=()
else
  files="$(git ls-files :**Cargo.lock)"
fi
for lock_file in $files; do
  if [[ -n $CI ]]; then
    echo "--- [$lock_file]: cargo " "${shifted_args[@]}" "$@"
  fi
  if (set -x && cd "$(dirname "$lock_file")" && cargo "${shifted_args[@]}" "$@"); then
    # noop
    true
  else
    failed_exit_code=$?
    if [[ -n $ignore ]]; then
      echo "$0: WARN: ignoring last cargo command failed exit code as requested:" $failed_exit_code
      true
    else
      exit $failed_exit_code
    fi
  fi
done

================
File: scripts/cargo-install-all.sh
================
here="$(dirname "$0")"
readlink_cmd="readlink"
echo "OSTYPE IS: $OSTYPE"
if [[ $OSTYPE == darwin* ]]; then
  readlink_cmd="greadlink"
  if ! command -v ${readlink_cmd} &> /dev/null
  then
    echo "${readlink_cmd} could not be found. You may need to install coreutils: \`brew install coreutils\`"
    exit 1
  fi
fi
SOLANA_ROOT="$("${readlink_cmd}" -f "${here}/..")"
cargo="${SOLANA_ROOT}/cargo"
set -e
usage() {
  exitcode=0
  if [[ -n "$1" ]]; then
    exitcode=1
    echo "Error: $*"
  fi
  cat <<EOF
usage: $0 [+<cargo version>] [--debug] [--validator-only] [--release-with-debug] [--no-spl-token] <install directory>
EOF
  exit $exitcode
}
maybeRustVersion=
installDir=
buildProfileArg='--profile release'
buildProfile='release'
validatorOnly=
noSPLToken=
while [[ -n $1 ]]; do
  if [[ ${1:0:1} = - ]]; then
    if [[ $1 = --debug ]]; then
      buildProfileArg=
      buildProfile='debug'
      shift
    elif [[ $1 = --release-with-debug ]]; then
      buildProfileArg='--profile release-with-debug'
      buildProfile='release-with-debug'
      shift
    elif [[ $1 = --release-with-lto ]]; then
      buildProfileArg='--profile release-with-lto'
      buildProfile='release-with-lto'
      shift
    elif [[ $1 = --validator-only ]]; then
      validatorOnly=true
      shift
    elif [[ $1 = --no-spl-token ]]; then
      noSPLToken=true
      shift
    else
      usage "Unknown option: $1"
    fi
  elif [[ ${1:0:1} = \+ ]]; then
    maybeRustVersion=$1
    shift
  else
    installDir=$1
    shift
  fi
done
if [[ -z "$installDir" ]]; then
  usage "Install directory not specified"
  exit 1
fi
installDir="$(mkdir -p "$installDir"; cd "$installDir"; pwd)"
mkdir -p "$installDir/bin/deps"
echo "Install location: $installDir ($buildProfile)"
cd "$(dirname "$0")"/..
SECONDS=0
source "$SOLANA_ROOT"/scripts/agave-build-lists.sh
BINS=()
DCOU_BINS=()
if [[ -n "$validatorOnly" ]]; then
  echo "Building binaries for net.sh deploys: ${AGAVE_BINS_END_USER[*]} ${AGAVE_BINS_VAL_OP[*]} ${AGAVE_BINS_DCOU[*]}"
  BINS+=("${AGAVE_BINS_END_USER[@]}" "${AGAVE_BINS_VAL_OP[@]}")
  DCOU_BINS+=("${AGAVE_BINS_DCOU[@]}")
else
  echo "Building binaries for all platforms: ${AGAVE_BINS_DEV[*]} ${AGAVE_BINS_END_USER[*]} ${AGAVE_BINS_DEPRECATED[*]}"
  BINS+=("${AGAVE_BINS_DEV[@]}" "${AGAVE_BINS_END_USER[@]}" "${AGAVE_BINS_DEPRECATED[@]}")
  if [[ $OSTYPE != msys ]]; then
    echo "Building binaries for linux and osx only: ${AGAVE_BINS_VAL_OP[*]}, ${AGAVE_BINS_DCOU[*]}"
    BINS+=("${AGAVE_BINS_VAL_OP[@]}")
    DCOU_BINS+=("${AGAVE_BINS_DCOU[@]}")
  fi
fi
binArgs=()
for bin in "${BINS[@]}"; do
  binArgs+=(--bin "$bin")
done
dcouBinArgs=()
for bin in "${DCOU_BINS[@]}"; do
  dcouBinArgs+=(--bin "$bin")
done
cargo_build() {
  "$cargo" $maybeRustVersion build $buildProfileArg "$@"
}
check_dcou() {
  RUSTC_BOOTSTRAP=1 \
    cargo_build -Z unstable-options --build-plan "$@" | \
    grep -q -F '"feature=\"dev-context-only-utils\""'
}
(
  set -x
  if check_dcou "${binArgs[@]}" --workspace; then
     echo 'dcou feature activation is incorrectly activated!'
     exit 1
  fi
  cargo_build "${binArgs[@]}" --workspace
  if [[ ${
    if ! check_dcou --manifest-path "dev-bins/Cargo.toml" "${dcouBinArgs[@]}"; then
       echo 'dcou feature activation is incorrectly remain to be deactivated!'
       exit 1
    fi
    cargo_build --manifest-path "dev-bins/Cargo.toml" "${dcouBinArgs[@]}"
  fi
  if [[ -z "$noSPLToken" ]]; then
    source "$SOLANA_ROOT"/scripts/spl-token-cli-version.sh
    "$cargo" $maybeRustVersion install --locked spl-token-cli --root "$installDir" $maybeSplTokenCliVersionArg
  fi
)
for bin in "${BINS[@]}"; do
  cp -fv "target/$buildProfile/$bin" "$installDir"/bin
done
for bin in "${DCOU_BINS[@]}"; do
  cp -fv "dev-bins/target/$buildProfile/$bin" "$installDir"/bin
done
if [[ $OSTYPE != msys ]]; then
  ./fetch-perf-libs.sh
  if [[ -d target/perf-libs ]]; then
    cp -a target/perf-libs "$installDir"/bin/perf-libs
  fi
fi
if [[ -z "$validatorOnly" ]]; then
  "$cargo" $maybeRustVersion build --manifest-path syscalls/gen-syscall-list/Cargo.toml
  "$cargo" $maybeRustVersion run --bin gen-headers
  mkdir -p "$installDir"/bin/platform-tools-sdk/sbf
  cp -a platform-tools-sdk/sbf/* "$installDir"/bin/platform-tools-sdk/sbf
fi
(
  set -x
  shopt -s nullglob
  for dep in target/"$buildProfile"/deps/libsolana*program.*; do
    cp -fv "$dep" "$installDir"/bin/deps
  done
)
echo "Done after $SECONDS seconds"
echo
echo "To use these binaries:"
echo "  export PATH=\"$installDir\"/bin:\"\$PATH\""

================
File: scripts/check-dev-context-only-utils.sh
================
set -eo pipefail
cd "$(dirname "$0")/.."
source ci/_
# only nightly is used uniformly as we contain good amount of nightly-only code
# (benches, frozen abi...)
source ci/rust-version.sh nightly
# There's a special common feature called `dev-context-only-utils` to
# overcome cargo's issue: https://github.com/rust-lang/cargo/issues/8379
# This feature is like `cfg(test)`, which works between crates.
#
# Unfortunately, this in turn needs some special checks to avoid common
# pitfalls of `dev-context-only-utils` itself.
#
# Firstly, detect any misuse of dev-context-only-utils as normal/build
# dependencies.  Also, allow some exceptions for special purpose crates. This
# white-listing mechanism can be used for core-development-oriented crates like
# bench bins.
#
# Put differently, use of dev-context-only-utils is forbidden for non-dev
# dependencies in general. However, allow its use for non-dev dependencies only
# if its use is confined under a dep. subgraph with all nodes being marked as
# dev-context-only-utils.
# Add your troubled package which seems to want to use `dev-context-only-utils`
# as normal (not dev) dependencies, only if you're sure that there's good
# reason to bend dev-context-only-utils's original intention and that listed
# package isn't part of released binaries.
source scripts/agave-build-lists.sh
# convert to comma separeted (ref: https://stackoverflow.com/a/53839433)
printf -v allowed '"%s",' "${DCOU_TAINTED_PACKAGES[@]}"
allowed="${allowed%,}"
mode=${1:-full}
# consume the mode, so that other arguments are forwarded to cargo-hack
shift
case "$mode" in
  tree | check-bins-and-lib | check-all-targets | full)
    ;;
  *)
    echo "$0: unrecognized mode: $mode";
    exit 1
    ;;
esac
if [[ $mode = "tree" || $mode = "full" ]]; then
  query=$(cat <<EOF
.packages
  | map(.name as \$crate
    | (.dependencies
      | map(select((.kind // "normal") == "normal"))
      | map({
        "crate" : \$crate,
        "dependency" : .name,
        "dependencyFeatures" : .features,
      })
    )
  )
  | flatten
  | map(select(
    (.dependencyFeatures
      | index("dev-context-only-utils")
    ) and (.crate as \$needle
      | ([$allowed] | index(\$needle))
      | not
    )
  ))
  | map([.crate, .dependency] | join(": "))
  | join("\n    ")
EOF
  )
  abusers="$(_ cargo "+${rust_nightly}" metadata --format-version=1 |
    jq -r "$query")"
  if [[ -n "$abusers" ]]; then
    cat <<EOF 1>&2
\`dev-context-only-utils\` must not be used as normal dependencies, but is by \
"([crate]: [dependency])":
    $abusers
EOF
    exit 1
  fi
  # Sanity-check that tainted packages has undergone the proper tedious rituals
  # to be justified as such.
  query=$(cat <<EOF
.packages
  | map([.name, (.features | keys)] as [\$this_crate, \$this_feature]
  | if .name as \$needle | ([$allowed] | index(\$needle))
  then
    {
      "crate": \$this_crate,
      "crateFeatures": \$this_feature,
    }
  elif .dependencies | any(
    .name as \$needle | ([$allowed] | index(\$needle))
  )
  then
    .dependencies
      | map({
        "crate": \$this_crate,
        "crateFeatures": \$this_feature,
      })
  else
    []
  end)
  | flatten
  | map(select(
    (.crateFeatures | index("dev-context-only-utils")) | not
  ))
  | map(.crate)
  | join("\n    ")
EOF
    )
    misconfigured_crates=$(
      _ cargo "+${rust_nightly}" metadata \
        --format-version=1 \
        | jq -r "$query"
    )
    if [[ -n "$misconfigured_crates" ]]; then
      cat <<EOF 1>&2
All crates marked \`tainted\`, as well as their dependents, MUST declare the \
\`dev-context-only-utils\`. The following crates are in violation:
    $misconfigured_crates
EOF
    exit 1
  fi
fi
# Detect possible compilation errors of problematic usage of
# `dev-context-only-utils`-gated code without being explicitly declared as such
# in respective workspace member `Cargo.toml`s. This cannot be detected with
# `--workspace --all-targets`, due to unintentional `dev-context-only-utils`
# feature activation by cargo's feature unification mechanism.  So, we use
export RUSTFLAGS="-D warnings -Z threads=8 $RUSTFLAGS"
unset CI_COMMIT
if [[ $mode = "check-bins-and-lib" || $mode = "full" ]]; then
  _ cargo "+${rust_nightly}" hack "$@" check
fi
if [[ $mode = "check-all-targets" || $mode = "full" ]]; then
  _ cargo "+${rust_nightly}" hack "$@" check --all-targets
fi

================
File: scripts/configure-metrics.sh
================
configureMetrics() {
  [[ -n $SOLANA_METRICS_CONFIG ]] || return 0
  declare metricsParams
  IFS=',' read -r -a metricsParams <<< "$SOLANA_METRICS_CONFIG"
  for param in "${metricsParams[@]}"; do
    IFS='=' read -r -a pair <<< "$param"
    if [[ ${
      echo Error: invalid metrics parameter: "$param" >&2
    else
      declare name="${pair[0]}"
      declare value="${pair[1]}"
      case "$name" in
      host)
        export INFLUX_HOST="$value"
        echo INFLUX_HOST="$INFLUX_HOST" >&2
        ;;
      db)
        export INFLUX_DATABASE="$value"
        echo INFLUX_DATABASE="$INFLUX_DATABASE" >&2
        ;;
      u)
        export INFLUX_USERNAME="$value"
        echo INFLUX_USERNAME="$INFLUX_USERNAME" >&2
        ;;
      p)
        export INFLUX_PASSWORD="$value"
        echo INFLUX_PASSWORD="********" >&2
        ;;
      *)
        echo Error: Unknown metrics parameter name: "$name" >&2
        ;;
      esac
    fi
  done
}
configureMetrics
metricsWriteDatapoint="$(dirname "${BASH_SOURCE[0]}")"/metrics-write-datapoint.sh

================
File: scripts/confirm-cargo-version-numbers-before-bump.sh
================
set -e
usage() {
  cat <<EOF
usage: $0 branch tag
Checks that the tag matches the branch (unless branch is master) and the Cargo.toml versions match the tag.
EOF
  exit 0
}
branch="$1"
tag="$2"
[[ -n $tag ]] || usage
echo "branch: $branch tag: $tag"
 if [[ "$tag" != "$branch"* && $branch != "master" ]]; then
    >&2 echo "Tag must start with the branch name (unless branch is master). Tag: $tag   Branch: $branch"
    exit 1
fi
here="$(dirname "$0")"
cd "$here"/..
source scripts/read-cargo-variable.sh
ignores=(
  .cache
  .cargo
  target
  node_modules
)
not_paths=()
for ignore in "${ignores[@]}"; do
  not_paths+=(-not -path "*/$ignore/*")
done
Cargo_tomls=($(find . -mindepth 2 -name Cargo.toml "${not_paths[@]}"))
for Cargo_toml in "${Cargo_tomls[@]}"; do
  manifest_version="$(readCargoVariable version "${Cargo_toml}")"
  if ! [[ "v$manifest_version" == "$tag" ]]; then
    >&2 echo "Tag must match the crate version in the manifest files. Mismatch found in $Cargo_toml. Tag: $tag   Manifest version: $manifest_version"
    exit 1
  else
    echo "tag matches manifest: $Cargo_toml $manifest_version $tag"
  fi
done

================
File: scripts/coverage.sh
================
set -e
here=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
if ! command -v grcov >/dev/null 2>&1; then
  echo "Error: grcov not found.  Try |cargo install grcov|"
  exit 1
fi
source "$here/../ci/rust-version.sh" nightly
llvm_profdata="$(find "$(rustc +"$rust_nightly" --print sysroot)" -name llvm-profdata)"
if [ -z "$llvm_profdata" ]; then
  echo "Error: couldn't find llvm-profdata. Try installing the llvm-tools component with \`rustup component add llvm-tools-preview --toolchain=$rust_nightly\`"
  exit 1
fi
llvm_path="$(dirname "$llvm_profdata")"
# get commit hash. it will be used to name output folder
if [ -z "$COMMIT_HASH" ]; then
  COMMIT_HASH=$(git rev-parse --short=9 HEAD)
fi
rm -rf "$here/../target/cov/$COMMIT_HASH"
export RUSTFLAGS="-C instrument-coverage $RUSTFLAGS"
export RUSTFLAGS="--cfg curve25519_dalek_backend=\"serial\" $RUSTFLAGS"
export LLVM_PROFILE_FILE="$here/../target/cov/${COMMIT_HASH}/profraw/default-%p-%m.profraw"
if [[ -z $1 ]]; then
  EXTRA_ARGS=(
    --features frozen-abi
    --lib
    --all
    --exclude solana-local-cluster
    --
    --skip shred::merkle::test::test_make_shreds_from_data::
    --skip shred::merkle::test::test_make_shreds_from_data_rand::
    --skip shred::merkle::test::test_recover_merkle_shreds::
  )
else
  EXTRA_ARGS=("$@")
fi
RUST_LOG="solana=trace,agave=trace,$RUST_LOG" INTERCEPT_OUTPUT=/dev/null "$here/../ci/intercept.sh" \
  cargo +"$rust_nightly" test --target-dir "$here/../target/cov" "${EXTRA_ARGS[@]}"
echo "--- grcov"
grcov_common_args=(
  "$here/../target/cov/${COMMIT_HASH}"
  --source-dir "$here/.."
  --binary-path "$here/../target/cov/debug"
  --llvm
  --llvm-path "$llvm_path"
  --ignore /\*
)
grcov "${grcov_common_args[@]}" -t html -o "$here/../target/cov/${COMMIT_HASH}/coverage/html"
echo "html: $here/../target/cov/${COMMIT_HASH}/coverage/html"
grcov "${grcov_common_args[@]}" -t lcov -o "$here/../target/cov/${COMMIT_HASH}/coverage/lcov.info"
echo "lcov: $here/../target/cov/${COMMIT_HASH}/coverage/lcov.info"
ln -sfT "$here/../target/cov/${COMMIT_HASH}" "$here/../target/cov/LATEST"

================
File: scripts/create-release-tarball.sh
================
set -euo pipefail
print_usage() {
  cat <<'EOF'
Usage:
  create-release-tarball.sh [options] --target TRIPLE
Options:
  --build-dir DIR           Build directory (default: solana-release)
  --channel-or-tag VAL      Channel or tag to embed in version.yml (default: "unknown")
  --target TRIPLE           Target triple for version.yml and artifact names (required)
  --tarball-basename NAME   Base name for tarball and .yml outputs (default: build-dir)
  --include-val-bins        Include validator/operator binaries in the tarball (default: exclude)
  --help, -h                Show this help and exit
EOF
}
cd "$(dirname "$0")/.."
build_dir="solana-release"
channel_or_tag="unknown"
target=""
tarball_basename=""
include_val_bins=0
commit="$(git rev-parse HEAD)"
while [[ $# -gt 0 ]]; do
  case "$1" in
    --build-dir)
      if [[ $
        echo "Error: --build-dir requires a value"
        print_usage
        exit 1
      fi
      build_dir="$2"
      shift 2
      ;;
    --channel-or-tag)
      if [[ $
        echo "Error: --channel-or-tag requires a value"
        print_usage
        exit 1
      fi
      channel_or_tag="$2"
      shift 2
      ;;
    --target)
      if [[ $
        echo "Error: --target requires a value"
        print_usage
        exit 1
      fi
      target="$2"
      shift 2
      ;;
    --tarball-basename)
      if [[ $
        echo "Error: --tarball-basename requires a value"
        print_usage
        exit 1
      fi
      tarball_basename="$2"
      shift 2
      ;;
    --include-val-bins)
      include_val_bins=1
      shift
      ;;
    --help|-h)
      print_usage
      exit 0
      ;;
    *)
      echo "Unknown argument: $1"
      print_usage
      exit 1
      ;;
  esac
done
if [[ -z "$target" ]]; then
  echo "Error: --target is required"
  print_usage
  exit 1
fi
tarball_basename="${tarball_basename:-${build_dir}}"
rm -rf "${build_dir:?}"/
mkdir -p "${build_dir}"/
cat > "${build_dir}/version.yml" <<EOF
channel: ${channel_or_tag}
commit: ${commit}
target: ${target}
EOF
source ci/rust-version.sh stable
scripts/cargo-install-all.sh stable "${build_dir}"
source scripts/agave-build-lists.sh
tmp_excludes="$(mktemp)"
trap 'rm -f "$tmp_excludes"' EXIT
if [[ "$include_val_bins" -eq 0 ]]; then
  for bin in "${AGAVE_BINS_VAL_OP[@]}"; do
    find "${build_dir}" -type f -name "$bin" -print -quit >> "$tmp_excludes" || true
  done
fi
cp "${build_dir}/bin/agave-install-init" "agave-install-init-${target}"
cp "${build_dir}/version.yml" "${tarball_basename}-${target}.yml"
output_tar="${tarball_basename}-${target}.tar.bz2"
echo "--- Creating tarball"
tar -cjvf "$output_tar" -X "$tmp_excludes" "${build_dir}"
echo "Done. Output: $(pwd)/$output_tar"

================
File: scripts/elf-hash-symbol.sh
================
set -euo pipefail
if [ "$#" -ne 2 ]; then
    echo "Usage: $0 <path/to/elf_file> <symbol_name>" >&2
    exit 1
fi
ELF_FILE="$1"
SYMBOL_NAME="$2"
SYMBOL_DETAILS=$(readelf -Ws "$ELF_FILE" | grep -w "$SYMBOL_NAME" | head -n 1)
if [ -z "$SYMBOL_DETAILS" ]; then
    echo "Error: Symbol '$SYMBOL_NAME' not found in '$ELF_FILE'." >&2
    exit 2
fi
read -r _ SYMBOL_VA SYMBOL_SIZE _ _ _ SYMBOL_NDX _ <<< "$SYMBOL_DETAILS"
if ! [[ "$SYMBOL_SIZE" =~ ^[0-9]+$ ]] || [ "$SYMBOL_SIZE" -eq 0 ]; then
    echo "Error: Symbol '$SYMBOL_NAME' has an invalid or zero size ($SYMBOL_SIZE)." >&2
    exit 3
fi
SECTION_LINE=$(readelf -S "$ELF_FILE" | awk -v ndx="[$SYMBOL_NDX]" '$1 == ndx {print; exit}')
if [ -z "$SECTION_LINE" ]; then
    echo "Error: Could not find section index $SYMBOL_NDX for symbol '$SYMBOL_NAME'." >&2
    exit 4
fi
read -r _ _ _ SECTION_VA SECTION_OFFSET _ <<< "$SECTION_LINE"
SYMBOL_VA_UPPER="${SYMBOL_VA^^}"
SECTION_VA_UPPER="${SECTION_VA^^}"
SECTION_OFFSET_UPPER="${SECTION_OFFSET^^}"
FILE_OFFSET_DEC=$(( 16
echo "--- Symbol Details ---"
echo "Symbol: $SYMBOL_NAME"
echo "Size: $SYMBOL_SIZE bytes"
echo "File Offset (Dec): $FILE_OFFSET_DEC"
echo "----------------------"
echo -n "Hash (SHA256): "
dd if="$ELF_FILE" bs=1 skip="$FILE_OFFSET_DEC" count="$SYMBOL_SIZE" status=none 2>/dev/null | sha256sum | awk '{print $1}'

================
File: scripts/fd-monitor.sh
================
set -e
[[ $(uname) == Linux ]] || exit 0
cd "$(dirname "$0")"
# shellcheck source=scripts/configure-metrics.sh
source configure-metrics.sh
while true; do
  count=$(lsof -u $UID | wc -l)
  ./metrics-write-datapoint.sh "open-files,hostname=$HOSTNAME count=$count"
  sleep 10
done
exit 1

================
File: scripts/generate-target-triple.sh
================
_arch="$(uname -m)"
if [[ $_arch = arm64 ]]; then
  _arch=aarch64
fi
case $(uname | tr '[:upper:]' '[:lower:]') in
linux*)
  export BUILD_TARGET_TRIPLE="$_arch-unknown-linux-gnu"
  ;;
darwin*)
  export BUILD_TARGET_TRIPLE="$_arch-apple-darwin"
  ;;
msys*|mingw*)
  export BUILD_TARGET_TRIPLE="$_arch-pc-windows-msvc"
  ;;
*)
  ;;
esac

================
File: scripts/iftop.sh
================
set -e
[[ $(uname) == Linux ]] || exit 0
cd "$(dirname "$0")"
sudo=
if sudo true; then
sudo="sudo -n"
fi
exec $sudo iftop -i "$(ifconfig | grep mtu | grep -iv loopback | grep -i running | awk 'BEGIN { FS = ":" } ; {print $1}')" -nNbBP -t -L 1000

================
File: scripts/increment-cargo-version.sh
================
set -e
usage() {
  cat <<EOF
usage: $0 [major|minor|patch|-preXYZ]
Increments the Cargo.toml version.
Default:
* Removes the prerelease tag if present, otherwise the minor version is incremented.
EOF
  exit 0
}
here="$(dirname "$0")"
cd "$here"/..
source ci/semver_bash/semver.sh
source scripts/read-cargo-variable.sh
ignores=(
  .cache
  .cargo
  target
  node_modules
  ci/xtask
)
not_paths=()
for ignore in "${ignores[@]}"; do
  not_paths+=(-not -path "*/$ignore/*")
done
Cargo_tomls=($(find . -name Cargo.toml "${not_paths[@]}"))
crates=()
for Cargo_toml in "${Cargo_tomls[@]}"; do
  crates+=("$(readCargoVariable name "$Cargo_toml")")
done
# Read the current version
MAJOR=0
MINOR=0
PATCH=0
SPECIAL=""
semverParseInto "$(readCargoVariable version Cargo.toml)" MAJOR MINOR PATCH SPECIAL
[[ -n $MAJOR ]] || usage
currentVersion="$MAJOR\.$MINOR\.$PATCH$SPECIAL"
bump=$1
if [[ -z $bump ]]; then
  if [[ -n $SPECIAL ]]; then
    bump=dropspecial
  else
    bump=minor
  fi
fi
SPECIAL=""
# Figure out what to increment
case $bump in
patch)
  PATCH=$((PATCH + 1))
  ;;
major)
  MAJOR=$((MAJOR+ 1))
  MINOR=0
  PATCH=0
  ;;
minor)
  MINOR=$((MINOR+ 1))
  PATCH=0
  ;;
dropspecial)
  ;;
check)
  badTomls=()
  for Cargo_toml in "${Cargo_tomls[@]}"; do
    if grep "^version = { workspace = true }" "$Cargo_toml" &>/dev/null; then
      continue
    fi
    if ! grep "^version *= *\"$currentVersion\"$" "$Cargo_toml" &>/dev/null; then
      badTomls+=("$Cargo_toml")
    fi
  done
  if [[ ${
    echo "Error: Incorrect crate version specified in: ${badTomls[*]}"
    exit 1
  fi
  exit 0
  ;;
-*)
  if [[ $1 =~ ^-[\.A-Za-z0-9]*$ ]]; then
    SPECIAL="$1"
  else
    echo "Error: Unsupported characters found in $1"
    exit 1
  fi
  ;;
*)
  echo "Error: unknown argument: $1"
  usage
  ;;
esac
(
  set +e
  if ! git diff --exit-code; then
    echo -e "\nError: Working tree is dirty. Commit or discard changes before bumping version." 1>&2
    exit 1
  fi
)
newVersion="$MAJOR.$MINOR.$PATCH$SPECIAL"
for Cargo_toml in "${Cargo_tomls[@]}"; do
  if ! grep "$currentVersion" "$Cargo_toml"; then
    echo "$Cargo_toml (skipped)"
    continue
  fi
  (
    set -x
    sed -i "$Cargo_toml" -e "s/^version = \"$currentVersion\"$/version = \"$newVersion\"/"
  )
  for crate in "${crates[@]}"; do
    (
      set -x
      sed -i "$Cargo_toml" -e "
        s/^$crate = { *path *= *\"\([^\"]*\)\" *, *version *= *\"[^\"]*\"\(.*\)} *\$/$crate = \{ path = \"\1\", version = \"=$newVersion\"\2\}/
      "
    )
  done
done
scripts/cargo-for-all-lock-files.sh tree >/dev/null
echo "$currentVersion -> $newVersion"
exit 0

================
File: scripts/metrics-write-datapoint.sh
================
point=$1
if [[ -z $point ]]; then
  echo "Data point not specified"
  exit 1
fi
echo "[$(date -u +"%Y-%m-%dT%H:%M:%SZ")] Influx data point: $point"
if [[ -z $INFLUX_DATABASE || -z $INFLUX_USERNAME || -z $INFLUX_PASSWORD ]]; then
  echo Influx user credentials not found
  exit 0
fi
host="https://internal-metrics.solana.com:8086"
if [[ -n $INFLUX_HOST ]]; then
  host="$INFLUX_HOST"
fi
echo "${host}/write?db=${INFLUX_DATABASE}&u=${INFLUX_USERNAME}&p=${INFLUX_PASSWORD}" \
  | xargs curl --max-time 5 --silent --show-error -XPOST --data-binary "$point"
exit 0

================
File: scripts/net-shaper.sh
================
set -e
[[ $(uname) == Linux ]] || exit 0
cd "$(dirname "$0")"
sudo=
if sudo true; then
  sudo="sudo -n"
fi
set -x
iface="$(ip link show | grep mtu | grep -iv loopback | grep "state UP" | awk 'BEGIN { FS = ": " } ; {print $2}')"
if [[ "$1" = cleanup ]]; then
  $sudo ~solana/.cargo/bin/solana-net-shaper cleanup -f "$2" -s "$3" -p "$4" -i "$iface"
else
  $sudo ~solana/.cargo/bin/solana-net-shaper shape -f "$2" -s "$3" -p "$4" -i "$iface"
fi

================
File: scripts/net-stats.sh
================
set -e
[[ $(uname) == Linux ]] || exit 0
cd "$(dirname "$0")"
# shellcheck source=scripts/configure-metrics.sh
source configure-metrics.sh
packets_sent=0
packets_sent_diff=0
packets_received=0
packets_received_diff=0
receive_errors=0
receive_errors_diff=0
receive_buffer_errors=0
receive_buffer_errors_diff=0
send_buffer_errors=0
send_buffer_errors_diff=0
rcvbuf_errors=0
rcvbuf_errors_diff=0
in_octets=0
in_octets_diff=0
out_octets=0
out_octets_diff=0
update_netstat() {
  declare net_stat
  net_stat=$(netstat -suna)
  declare stats
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /packets sent/ {tmp_var = $1} END { print tmp_var }')
  packets_sent_diff=$((stats - packets_sent))
  packets_sent="$stats"
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /packets received/ {tmp_var = $1} END { print tmp_var }')
  packets_received_diff=$((stats - packets_received))
  packets_received="$stats"
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /packet receive errors/ {tmp_var = $1} END { print tmp_var }')
  receive_errors_diff=$((stats - receive_errors))
  receive_errors="$stats"
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /receive buffer errors/ {tmp_var = $1} END { print tmp_var }')
  receive_buffer_errors_diff=$((stats - receive_buffer_errors))
  receive_buffer_errors="$stats"
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /send buffer errors/ {tmp_var = $1} END { print tmp_var }')
  send_buffer_errors_diff=$((stats - send_buffer_errors))
  send_buffer_errors="$stats"
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /RcvbufErrors/ {tmp_var = $2} END { print tmp_var }')
  rcvbuf_errors_diff=$((stats - rcvbuf_errors))
  rcvbuf_errors="$stats"
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /InOctets/ {tmp_var = $2} END { print tmp_var }')
  in_octets_diff=$((stats - in_octets))
  in_octets="$stats"
  stats=$(echo "$net_stat" | awk 'BEGIN {tmp_var = 0} /OutOctets/ {tmp_var = $2} END { print tmp_var }')
  out_octets_diff=$((stats - out_octets))
  out_octets="$stats"
}
update_netstat
while true; do
  update_netstat
  report="packets_sent=$packets_sent_diff,packets_received=$packets_received_diff,receive_errors=$receive_errors_diff,receive_buffer_errors=$receive_buffer_errors_diff,send_buffer_errors=$send_buffer_errors_diff,rcvbuf_errors=$rcvbuf_errors_diff,in_octets=$in_octets_diff,out_octets=$out_octets_diff"
  echo "$report"
  ./metrics-write-datapoint.sh "net-stats,hostname=$HOSTNAME $report"
  sleep 1
done
exit 1

================
File: scripts/oom-monitor.sh
================
set -e
cd "$(dirname "$0")"
# shellcheck source=scripts/oom-score-adj.sh
source oom-score-adj.sh
# shellcheck source=scripts/configure-metrics.sh
source configure-metrics.sh
[[ $(uname) = Linux ]] || exit 0
syslog=/var/log/syslog
[[ -r $syslog ]] || {
  echo Unable to read $syslog
  exit 1
}
# Adjust OOM score to reduce the chance that this script will be killed
# during an Out of Memory event since the purpose of this script is to
# report such events
oom_score_adj "self" -500
while read -r victim; do
  echo "Out of memory event detected, $victim killed"
  ./metrics-write-datapoint.sh "oom-killer,victim=$victim,hostname=$HOSTNAME killed=1"
done < <( \
  tail --follow=name --retry -n0 $syslog \
  | sed --unbuffered -n "s/^.* earlyoom\[[0-9]*\]: Killing process .\(.*\). with signal .*/\1/p" \
)
exit 1

================
File: scripts/oom-score-adj.sh
================
oom_score_adj() {
  declare pid=$1
  declare score=$2
  if [[ $(uname) != Linux ]]; then
    return
  fi
  echo "$score" > "/proc/$pid/oom_score_adj" || true
  declare currentScore
  currentScore=$(cat "/proc/$pid/oom_score_adj" || true)
  if [[ $score != "$currentScore" ]]; then
    echo "Failed to set oom_score_adj to $score for pid $pid (current score: $currentScore)"
  fi
}

================
File: scripts/patch-crates.sh
================
update_solana_dependencies() {
  declare project_root="$1"
  declare solana_ver="$2"
  declare tomls=()
  while IFS='' read -r line; do tomls+=("$line"); done < <(find "$project_root" -name Cargo.toml)
  crates=(
    solana-account-decoder
    solana-account-decoder-client-types
    solana-banks-client
    solana-banks-interface
    solana-banks-server
    solana-bloom
    solana-bucket-map
    solana-builtins-default-costs
    solana-clap-utils
    solana-clap-v3-utils
    solana-cli-config
    solana-cli-output
    solana-client
    solana-compute-budget
    solana-connection-cache
    solana-core
    solana-entry
    solana-faucet
    solana-fee
    agave-geyser-plugin-interface
    solana-geyser-plugin-manager
    solana-gossip
    solana-lattice-hash
    solana-ledger
    solana-log-collector
    solana-measure
    solana-merkle-tree
    solana-metrics
    solana-net-utils
    solana-perf
    solana-poh
    solana-program-runtime
    solana-program-test
    solana-bpf-loader-program
    solana-compute-budget-program
    solana-stake-program
    solana-system-program
    solana-vote-program
    solana-zk-elgamal-proof-program
    solana-zk-token-proof-program
    solana-pubsub-client
    solana-quic-client
    solana-rayon-threadlimit
    solana-remote-wallet
    solana-rpc
    solana-rpc-client
    solana-rpc-client-api
    solana-rpc-client-nonce-utils
    solana-runtime
    solana-runtime-transaction
    solana-send-transaction-service
    solana-storage-bigtable
    solana-storage-proto
    solana-streamer
    solana-svm-rent-calculator
    solana-svm-transaction
    solana-test-validator
    solana-tpu-client
    solana-transaction-status
    solana-transaction-status-client-types
    solana-udp-client
    solana-version
    solana-zk-token-sdk
    solana-curve25519
  )
  set -x
  for crate in "${crates[@]}"; do
    sed -E -i'' -e "s:(${crate} = \")([=<>]*)[0-9.]+([^\"]*)\".*:\1\2${solana_ver}\3\":" "${tomls[@]}"
    sed -E -i'' -e "s:(${crate} = \{ version = \")([=<>]*)[0-9.]+([^\"]*)(\".*):\1\2${solana_ver}\3\4:" "${tomls[@]}"
  done
}
patch_crates_io_solana() {
  declare Cargo_toml="$1"
  declare solana_dir="$2"
  cat >> "$Cargo_toml" <<EOF
[patch.crates-io]
EOF
  patch_crates_io_solana_no_header "$Cargo_toml" "$solana_dir"
}
patch_crates_io_solana_no_header() {
  declare Cargo_toml="$1"
  declare solana_dir="$2"
  crates_map=()
  crates_map+=("solana-account-decoder account-decoder")
  crates_map+=("solana-account-decoder-client-types account-decoder-client-types")
  crates_map+=("solana-banks-client banks-client")
  crates_map+=("solana-banks-interface banks-interface")
  crates_map+=("solana-banks-server banks-server")
  crates_map+=("solana-bloom bloom")
  crates_map+=("solana-bucket-map bucket_map")
  crates_map+=("solana-builtins-default-costs builtins-default-costs")
  crates_map+=("solana-clap-utils clap-utils")
  crates_map+=("solana-clap-v3-utils clap-v3-utils")
  crates_map+=("solana-cli-config cli-config")
  crates_map+=("solana-cli-output cli-output")
  crates_map+=("solana-client client")
  crates_map+=("solana-compute-budget compute-budget")
  crates_map+=("solana-connection-cache connection-cache")
  crates_map+=("solana-core core")
  crates_map+=("solana-entry entry")
  crates_map+=("solana-faucet faucet")
  crates_map+=("solana-fee fee")
  crates_map+=("agave-geyser-plugin-interface geyser-plugin-interface")
  crates_map+=("solana-geyser-plugin-manager geyser-plugin-manager")
  crates_map+=("solana-gossip gossip")
  crates_map+=("solana-lattice-hash lattice-hash")
  crates_map+=("solana-ledger ledger")
  crates_map+=("solana-log-collector log-collector")
  crates_map+=("solana-measure measure")
  crates_map+=("solana-merkle-tree merkle-tree")
  crates_map+=("solana-metrics metrics")
  crates_map+=("solana-net-utils net-utils")
  crates_map+=("solana-perf perf")
  crates_map+=("solana-poh poh")
  crates_map+=("solana-program-runtime program-runtime")
  crates_map+=("solana-program-test program-test")
  crates_map+=("solana-bpf-loader-program programs/bpf_loader")
  crates_map+=("solana-compute-budget-program programs/compute-budget")
  crates_map+=("solana-stake-program programs/stake")
  crates_map+=("solana-system-program programs/system")
  crates_map+=("solana-vote-program programs/vote")
  crates_map+=("solana-zk-elgamal-proof-program programs/zk-elgamal-proof")
  crates_map+=("solana-zk-token-proof-program programs/zk-token-proof")
  crates_map+=("solana-pubsub-client pubsub-client")
  crates_map+=("solana-quic-client quic-client")
  crates_map+=("solana-rayon-threadlimit rayon-threadlimit")
  crates_map+=("solana-remote-wallet remote-wallet")
  crates_map+=("solana-rpc rpc")
  crates_map+=("solana-rpc-client rpc-client")
  crates_map+=("solana-rpc-client-api rpc-client-api")
  crates_map+=("solana-rpc-client-nonce-utils rpc-client-nonce-utils")
  crates_map+=("solana-runtime runtime")
  crates_map+=("solana-runtime-transaction runtime-transaction")
  crates_map+=("solana-send-transaction-service send-transaction-service")
  crates_map+=("solana-storage-bigtable storage-bigtable")
  crates_map+=("solana-storage-proto storage-proto")
  crates_map+=("solana-streamer streamer")
  crates_map+=("solana-svm-rent-collector svm-rent-collector")
  crates_map+=("solana-svm-transaction svm-transaction")
  crates_map+=("solana-test-validator test-validator")
  crates_map+=("solana-tpu-client tpu-client")
  crates_map+=("solana-transaction-status transaction-status")
  crates_map+=("solana-transaction-status-client-types transaction-status-client-types")
  crates_map+=("solana-udp-client udp-client")
  crates_map+=("solana-version version")
  crates_map+=("solana-zk-token-sdk zk-token-sdk")
  crates_map+=("solana-bn254 curves/bn254")
  crates_map+=("solana-curve25519 curves/curve25519")
  crates_map+=("solana-secp256k1-recover curves/secp256k1-recover")
  patch_crates=()
  for map_entry in "${crates_map[@]}"; do
    read -r crate_name crate_path <<<"$map_entry"
    full_path="$solana_dir/$crate_path"
    if [[ -r "$full_path/Cargo.toml" ]]; then
      patch_crates+=("$crate_name = { path = \"$full_path\" }")
    fi
  done
  echo "Patching in $solana_ver from $solana_dir"
  echo
  if grep -q "# The following entries are auto-generated by $0" "$Cargo_toml"; then
    echo "$Cargo_toml is already patched"
  else
    if ! grep -q '\[patch.crates-io\]' "$Cargo_toml"; then
      echo "[patch.crates-io]" >> "$Cargo_toml"
    fi
    cat >> "$Cargo_toml" <<PATCH
$(printf "%s\n" "${patch_crates[@]}")
PATCH
  fi
}

================
File: scripts/patch-spl-crates-for-anchor.sh
================
spl_associated_token_account_version=
spl_pod_version=
spl_token_version=
spl_token_2022_version=
spl_token_group_interface_version=
spl_token_metadata_interface_version=
spl_tlv_account_resolution_version=
spl_transfer_hook_interface_version=
spl_type_length_value_version=
get_spl_versions() {
    declare spl_dir="$1"
    spl_associated_token_account_version=$(readCargoVariable version "$spl_dir/associated-token-account/program/Cargo.toml")
    spl_pod_version=$(readCargoVariable version "$spl_dir/libraries/pod/Cargo.toml")
    spl_token_version=$(readCargoVariable version "$spl_dir/token/program/Cargo.toml")
    spl_token_2022_version=$(readCargoVariable version "$spl_dir/token/program-2022/Cargo.toml"| head -c1)
    spl_token_group_interface_version=$(readCargoVariable version "$spl_dir/token-group/interface/Cargo.toml")
    spl_token_metadata_interface_version=$(readCargoVariable version "$spl_dir/token-metadata/interface/Cargo.toml")
    spl_tlv_account_resolution_version=$(readCargoVariable version "$spl_dir/libraries/tlv-account-resolution/Cargo.toml")
    spl_transfer_hook_interface_version=$(readCargoVariable version "$spl_dir/token/transfer-hook/interface/Cargo.toml")
    spl_type_length_value_version=$(readCargoVariable version "$spl_dir/libraries/type-length-value/Cargo.toml")
}
patch_spl_crates() {
    declare project_root="$1"
    declare Cargo_toml="$2"
    declare spl_dir="$3"
    update_spl_dependencies "$project_root"
    patch_crates_io "$Cargo_toml" "$spl_dir"
}
update_spl_dependencies() {
    declare project_root="$1"
    declare tomls=()
    while IFS='' read -r line; do tomls+=("$line"); done < <(find "$project_root" -name Cargo.toml)
    sed -i -e "s#\(spl-associated-token-account = \"\)[^\"]*\(\"\)#\1$spl_associated_token_account_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-associated-token-account = { version = \"\)[^\"]*\(\"\)#\1$spl_associated_token_account_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-pod = \"\)[^\"]*\(\"\)#\1$spl_pod_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-pod = { version = \"\)[^\"]*\(\"\)#\1$spl_pod_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token = \"\)[^\"]*\(\"\)#\1$spl_token_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token = { version = \"\)[^\"]*\(\"\)#\1$spl_token_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token-2022 = \"\).*\(\"\)#\1$spl_token_2022_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token-2022 = { version = \"\)[^\"]*\(\"\)#\1$spl_token_2022_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token-group-interface = \"\)[^\"]*\(\"\)#\1=$spl_token_group_interface_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token-group-interface = { version = \"\)[^\"]*\(\"\)#\1=$spl_token_group_interface_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token-metadata-interface = \"\)[^\"]*\(\"\)#\1=$spl_token_metadata_interface_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-token-metadata-interface = { version = \"\)[^\"]*\(\"\)#\1=$spl_token_metadata_interface_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-tlv-account-resolution = \"\)[^\"]*\(\"\)#\1=$spl_tlv_account_resolution_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-tlv-account-resolution = { version = \"\)[^\"]*\(\"\)#\1=$spl_tlv_account_resolution_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-transfer-hook-interface = \"\)[^\"]*\(\"\)#\1=$spl_transfer_hook_interface_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-transfer-hook-interface = { version = \"\)[^\"]*\(\"\)#\1=$spl_transfer_hook_interface_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-type-length-value = \"\)[^\"]*\(\"\)#\1=$spl_type_length_value_version\2#g" "${tomls[@]}" || return $?
    sed -i -e "s#\(spl-type-length-value = { version = \"\)[^\"]*\(\"\)#\1=$spl_type_length_value_version\2#g" "${tomls[@]}" || return $?
    # patch ahash. This is super brittle; putting here for convenience, since we are already iterating through the tomls
    ahash_minor_version="0.8"
    sed -i -e "s#\(ahash = \"\)[^\"]*\(\"\)#\1$ahash_minor_version\2#g" "${tomls[@]}" || return $?
}
patch_crates_io() {
    declare Cargo_toml="$1"
    declare spl_dir="$2"
    cat >> "$Cargo_toml" <<EOF
    spl-associated-token-account = { path = "$spl_dir/associated-token-account/program" }
    spl-pod = { path = "$spl_dir/libraries/pod" }
    spl-token = { path = "$spl_dir/token/program" }
    # Avoid patching spl-token-2022 to avoid forcing anchor to use 4.0.1, which
    # doesn't work with the monorepo forcing 4.0.0. Allow the patching again once
    spl-token-group-interface = { path = "$spl_dir/token-group/interface" }
    spl-token-metadata-interface = { path = "$spl_dir/token-metadata/interface" }
    spl-tlv-account-resolution = { path = "$spl_dir/libraries/tlv-account-resolution" }
    spl-transfer-hook-interface = { path = "$spl_dir/token/transfer-hook/interface" }
    spl-type-length-value = { path = "$spl_dir/libraries/type-length-value" }
EOF
}

================
File: scripts/perf-plot.py
================
stages_to_counters = {}
stages_to_time = {}
⋮----
json_part = line[line.find("{"):]
x = json.loads(json_part)
counter = x['name']

================
File: scripts/perf-stats.py
================
stages_data = {}
⋮----
json_part = line[line.find("{"):]
x = json.loads(json_part)
counter = x['name']
⋮----
count_since_last = x['counts'] - stages_data[counter]['last_count']
time_since_last = float(x['now'] - stages_data[counter]['last_ts'])
⋮----
speed = 1000.0 * (count_since_last / time_since_last)
⋮----
mean = 0
average = 0
eightieth = 0
data_len = len(stages_data[stage]['data'])
mean_index = int(data_len / 2)
eightieth_index = int(data_len * 0.8)
⋮----
mean = stages_data[stage]['data'][mean_index]
average = float(sum(stages_data[stage]['data'])) / data_len
eightieth = stages_data[stage]['data'][eightieth_index]
⋮----
num = 5
idx = -1

================
File: scripts/read-cargo-variable.sh
================
readCargoVariable() {
  declare variable="$1"
  declare Cargo_toml="$2"
  while read -r name equals value _; do
    if [[ $name = "$variable" && $equals = = ]]; then
      echo "${value//\"/}"
      return
    fi
  done < <(cat "$Cargo_toml")
  echo "Unable to locate $variable in $Cargo_toml" 1>&2
}

================
File: scripts/reserve-cratesio-package-name.sh
================
display_help() {
  local bin
  bin="$(basename --suffix='.sh' "$0")"
  cat <<EOF 1>&2
$bin
Reserve a Rust package name on crates.io
USAGE:
    $bin [FLAGS] [OPTIONS] <TARGET_TYPE> <PACKAGE_NAME>
FLAGS:
        --help                Display this help message
        --no-prefix           Do not require \`agave-\` or \`solana-\` prefix on PACKAGE_NAME
        --publish             Upload the reserved package. Without this flag, a
                              dry-run is performed
OPTIONS:
        --token <TOKEN>       Token used to authenticate with crates.io
ARGS:
    TARGET_TYPE     The package target type [possible values: bin, lib]
    PACKAGE_NAME    The desired package name [see:
                    https://doc.rust-lang.org/cargo/reference/manifest.html#the-name-field]
EOF
}
require_prefix=true
maybe_publish='--dry-run'
positional=()
while [[ -n "$1" ]]; do
  case "$1" in
    --)
      break
      ;;
    --help)
      display_help
      exit 0
      ;;
    --no-prefix)
      require_prefix=false
      ;;
    --publish)
      maybe_publish=''
      ;;
    --token)
      maybe_crates_token="--token $2"
      shift
      ;;
    --* | -*)
      echo "error: unexpected argument \`$1\`" 1>&2
      display_help
      exit 1
      ;;
    *)
      positional+=("$1")
      ;;
  esac
  shift
done
while [[ -n "$1" ]]; do
  positional+=("$1")
  shift
done
target_type="${positional[0]:?'TARGET_TYPE must be declared'}"
package_name="${positional[1]:?'PACKAGE_NAME must be declared'}"
case "${target_type}" in
  bin)
    src_filename='main.rs'
    src_file_body='fn main() {}'
    ;;
  lib)
    src_filename='lib.rs'
    src_file_body=''
    ;;
  *)
    echo "error: unexpected TARGET_TYPE: \`${target_type}\`" 1>&2
    display_help
    exit 1
    ;;
esac
if ! [[ "${package_name}" =~ ^[a-zA-Z0-9_-]{1,64} ]]; then
  echo "error: illegal PACKAGE_NAME: \`${package_name}\`" 1>&2
  display_help
  exit 1
fi
if ${require_prefix} && ! [[ "${package_name}" =~ ^(agave|solana)- ]]; then
  # shellcheck disable=SC2016 # backticks are not a command here
  echo 'error: PACKAGE_NAME MUST start with `agave-` or `solana-`' 1>&2
  display_help
  exit 1
fi
tmpdir="$(mktemp -d)"
if pushd "${tmpdir}" &>/dev/null; then
  cat <<EOF > "Cargo.toml"
[package]
name = "${package_name}"
version = "0.0.0"
description = "reserved for future use"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
repository = "https://github.com/anza-xyz/agave"
license = "Apache-2.0"
homepage = "https://anza.xyz"
documentation = "https://docs.rs/${package_name}"
edition = "2021"
EOF
  mkdir -p src
  echo "${src_file_body}" > "src/${src_filename}"
  cargo publish ${maybe_publish} ${maybe_crates_token}
  popd &>/dev/null || true
fi
rm -rf "${tmpdir}"

================
File: scripts/run.sh
================
set -e
script_dir="$(readlink -f "$(dirname "$0")")"
if [[ "$script_dir" =~ /scripts$ ]]; then
  cd "$script_dir/.."
else
  cd "$script_dir"
fi
profile=debug
if [[ -n $CARGO_BUILD_PROFILE ]]; then
  profile=$CARGO_BUILD_PROFILE
fi
PATH=$PWD/target/$profile:$PATH
ok=true
for program in solana-{faucet,genesis,keygen}; do
  $program -V || ok=false
done
agave-validator -V || ok=false
$ok || {
  echo
  echo "Unable to locate required programs.  Try building them first with:"
  echo
  echo "  $ cargo build --all"
  echo
  exit 1
}
export RUST_LOG=${RUST_LOG:-solana=info,agave=info,solana_runtime::message_processor=debug}
export RUST_BACKTRACE=1
dataDir=$PWD/config/"$(basename "$0" .sh)"
ledgerDir=$PWD/config/ledger
SOLANA_RUN_SH_CLUSTER_TYPE=${SOLANA_RUN_SH_CLUSTER_TYPE:-development}
set -x
if ! solana address; then
  echo Generating default keypair
  solana-keygen new --no-passphrase
fi
validator_identity="$dataDir/validator-identity.json"
if [[ -e $validator_identity ]]; then
  echo "Use existing validator keypair"
else
  solana-keygen new --no-passphrase -so "$validator_identity"
fi
validator_vote_account="$dataDir/validator-vote-account.json"
if [[ -e $validator_vote_account ]]; then
  echo "Use existing validator vote account keypair"
else
  solana-keygen new --no-passphrase -so "$validator_vote_account"
fi
validator_stake_account="$dataDir/validator-stake-account.json"
if [[ -e $validator_stake_account ]]; then
  echo "Use existing validator stake account keypair"
else
  solana-keygen new --no-passphrase -so "$validator_stake_account"
fi
if [[ -e "$ledgerDir"/genesis.bin || -e "$ledgerDir"/genesis.tar.bz2 ]]; then
  echo "Use existing genesis"
else
  ./fetch-core-bpf.sh
  if [[ -r core-bpf-genesis-args.sh ]]; then
    CORE_BPF_GENESIS_ARGS=$(cat core-bpf-genesis-args.sh)
  fi
  ./fetch-spl.sh
  if [[ -r spl-genesis-args.sh ]]; then
    SPL_GENESIS_ARGS=$(cat spl-genesis-args.sh)
  fi
  solana-genesis \
    --hashes-per-tick sleep \
    --faucet-lamports 500000000000000000 \
    --bootstrap-validator \
      "$validator_identity" \
      "$validator_vote_account" \
      "$validator_stake_account" \
    --ledger "$ledgerDir" \
    --cluster-type "$SOLANA_RUN_SH_CLUSTER_TYPE" \
    $CORE_BPF_GENESIS_ARGS \
    $SPL_GENESIS_ARGS \
    $SOLANA_RUN_SH_GENESIS_ARGS
fi
abort() {
  set +e
  kill "$faucet" "$validator"
  wait "$validator"
}
trap abort INT TERM EXIT
solana-faucet &
faucet=$!
args=(
  --identity "$validator_identity"
  --vote-account "$validator_vote_account"
  --ledger "$ledgerDir"
  --tip-payment-program-pubkey "T1pyyaTNZsKv2WcRAB8oVnk93mLJw2XzjtVYqCsaHqt"
  --tip-distribution-program-pubkey "4R3gSG8BpU4t19KYj8CfnbtRpnT8gtk4dvTHxVRwc2r7"
  --merkle-root-upload-authority "$validator_identity"
  --commission-bps 0
  --gossip-port 8001
  --full-rpc-api
  --rpc-port 8899
  --rpc-faucet-address 127.0.0.1:9900
  --log -
  --enable-rpc-transaction-history
  --enable-extended-tx-metadata-storage
  --init-complete-file "$dataDir"/init-completed
  --require-tower
  --no-wait-for-vote-to-start-leader
  --no-os-network-limits-test
)
agave-validator "${args[@]}" $SOLANA_RUN_SH_VALIDATOR_ARGS &
validator=$!
wait "$validator"

================
File: scripts/sed-i-all-rs-files-for-rust-analyzer.sh
================
set -e
if [[ $1 = "doit" ]]; then
  true &&
    sed -i -e 's/#\[cfg(test)\]/#[cfg(escaped_cfg_test)]/g' $(git ls-files :**.rs :^**/build.rs) &&
    sed -i -e 's/#\[bench\]/#[cfg(escaped_bench)]/g' $(git ls-files :**.rs :^**/build.rs) &&
    sed -i -e 's/#\[test\]/#[cfg(escaped_test)]/g' $(git ls-files :**.rs :^**/build.rs) &&
    sed -i -e 's/#\[tokio::test\]/#[cfg(escaped_tokio_test)]/g' $(git ls-files :**.rs :^**/build.rs)
elif [[ $1 = "undoit" ]]; then
  true &&
    sed -i -e 's/#\[cfg(escaped_cfg_test)\]/#[cfg(test)]/g' $(git ls-files :**.rs :^**/build.rs) &&
    sed -i -e 's/#\[cfg(escaped_bench)\]/#[bench]/g' $(git ls-files :**.rs :^**/build.rs) &&
    sed -i -e 's/#\[cfg(escaped_test)\]/#[test]/g' $(git ls-files :**.rs :^**/build.rs) &&
    sed -i -e 's/#\[cfg(escaped_tokio_test)\]/#[tokio::test]/g' $(git ls-files :**.rs :^**/build.rs)
else
  echo "usage: $0 [doit|undoit]" > /dev/stderr
  exit 1
fi

================
File: scripts/spl-token-cli-version.sh
================
splTokenCliVersion=
maybeSplTokenCliVersionArg=
if [[ -n "$splTokenCliVersion" ]]; then
    maybeSplTokenCliVersionArg="--version $splTokenCliVersion"
fi

================
File: scripts/system-stats.sh
================
set -e
[[ $(uname) == Linux ]] || exit 0
cd "$(dirname "$0")/.."
source scripts/configure-metrics.sh
while true; do
  # collect top twice because the first time is inaccurate
  top_output="$(top -bn2 -d1)"
  # collect the total cpu usage by subtracting idle usage from 100%
  cpu_usage=$(echo "${top_output}" | grep '%Cpu(s):' | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | tail -1 | awk '{print 100 - $1}')
  # collect the total ram usage by dividing used memory / total memory
  ram_total_and_usage=$(echo "${top_output}" | grep '.*B Mem'| tail -1 | sed "s/.*: *\([0-9.]*\)%* total.*, *\([0-9.]*\)%* used.*/\1 \2/")
  read -r total used <<< "$ram_total_and_usage"
  ram_usage=$(awk "BEGIN {print $used / $total * 100}")
  cpu_report="cpu_usage=$cpu_usage,ram_usage=$ram_usage"
  report="${cpu_report}"
  ./scripts/metrics-write-datapoint.sh "system-stats,hostname=$HOSTNAME $report"
  sleep 1
done

================
File: scripts/ulimit-n.sh
================
maxOpenFds=65000
if [[ $(ulimit -n) -lt $maxOpenFds ]]; then
  ulimit -n $maxOpenFds 2>/dev/null || {
    echo "Error: nofiles too small: $(ulimit -n). Failed to run \"ulimit -n $maxOpenFds\"";
    if [[ $(uname) = Darwin ]]; then
      echo "Try running |sudo launchctl limit maxfiles 65536 200000| first"
    fi
  }
fi

================
File: scripts/wallet-sanity.sh
================
set -e
cd "$(dirname "$0")"/..
# shellcheck source=multinode-demo/common.sh
source multinode-demo/common.sh
if [[ -z $1 ]]; then # no network argument, use localhost by default
  args=(--url http://127.0.0.1:8899)
else
  args=("$@")
fi
args+=(--keypair "$SOLANA_CONFIG_DIR"/faucet.json)
node_readiness=false
timeout=60
while [[ $timeout -gt 0 ]]; do
  set +e
  output=$($solana_cli "${args[@]}" transaction-count --commitment finalized)
  rc=$?
  set -e
  if [[ $rc -eq 0 && -n $output ]]; then
    node_readiness=true
    break
  fi
  sleep 2
  (( timeout=timeout-2 ))
done
if ! "$node_readiness"; then
  echo "Timed out waiting for cluster to start"
  exit 1
fi
(
  set -x
  $solana_cli "${args[@]}" address
  $solana_cli "${args[@]}" balance
  $solana_cli "${args[@]}" ping --count 5 --interval 0
  $solana_cli "${args[@]}" balance
)
echo PASS
exit 0

================
File: sdk/README.md
================
# PLEASE READ: This repo no longer contains the Solana SDK

The solana-sdk is currently developed at https://github.com/anza-xyz/solana-sdk

================
File: SECURITY.md
================
# Jito-Solana Security

The bug bounty program for Jito-Solana is managed by Immunefi. More details can be
found [here](https://immunefi.com/bug-bounty/jito/information/).

================
File: send-transaction-service/src/lib.rs
================
pub mod send_transaction_service;
pub mod send_transaction_service_stats;
⋮----
pub mod test_utils;
pub mod tpu_info;
pub mod transaction_client;
⋮----
extern crate solana_metrics;

================
File: send-transaction-service/src/send_transaction_service_stats.rs
================
pub struct SendTransactionServiceStats {
⋮----
pub(crate) struct SendTransactionServiceStatsReport {
⋮----
impl SendTransactionServiceStatsReport {
pub fn report(&self) {
⋮----
.should_update(SEND_TRANSACTION_METRICS_REPORT_RATE_MS)
⋮----
datapoint_info!(

================
File: send-transaction-service/src/send_transaction_service.rs
================
pub struct SendTransactionService {
⋮----
pub struct TransactionInfo {
⋮----
impl TransactionInfo {
pub fn new(
⋮----
fn get_max_retries(
⋮----
.or(default_max_retries)
.map(|max_retries| max_retries.min(service_max_retries))
⋮----
struct ProcessTransactionsResult {
⋮----
pub struct Config {
⋮----
impl Default for Config {
fn default() -> Self {
⋮----
impl SendTransactionService {
pub fn new<Client: TransactionClient + Clone + std::marker::Send + 'static>(
⋮----
client.clone(),
retry_transactions.clone(),
config.clone(),
stats_report.clone(),
exit.clone(),
⋮----
bank_forks.clone(),
⋮----
/// Thread responsible for receiving transactions from RPC clients.
    fn receive_txn_thread<Client: TransactionClient + std::marker::Send + 'static>(
⋮----
fn receive_txn_thread<Client: TransactionClient + std::marker::Send + 'static>(
⋮----
debug!("Starting send-transaction-service::receive_txn_thread");
⋮----
.name("solStxReceive".to_string())
.spawn(move || loop {
⋮----
let recv_result = receiver.recv_timeout(Duration::from_millis(batch_send_rate_ms));
if exit.load(Ordering::Relaxed) {
⋮----
info!("Terminating send-transaction-service.");
exit.store(true, Ordering::Relaxed);
⋮----
stats.received_transactions.fetch_add(1, Ordering::Relaxed);
let entry = transactions.entry(transaction_info.signature);
⋮----
.lock()
.unwrap()
.contains_key(&transaction_info.signature)
⋮----
entry.or_insert(transaction_info);
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
if (!transactions.is_empty()
&& last_batch_sent.elapsed().as_millis() as u64 >= batch_send_rate_ms)
|| transactions.len() >= batch_size
⋮----
.fetch_add(transactions.len() as u64, Ordering::Relaxed);
⋮----
.values()
.map(|transaction_info| transaction_info.wire_transaction.clone())
⋮----
client.send_transactions_in_batch(wire_transactions, stats);
⋮----
let mut retry_transactions = retry_transactions.lock().unwrap();
⋮----
for (signature, mut transaction_info) in transactions.drain() {
⋮----
.get_max_retries(default_max_retries, service_max_retries);
if max_retries == Some(0) {
⋮----
let retry_len = retry_transactions.len();
let entry = retry_transactions.entry(signature);
⋮----
transaction_info.last_sent_time = Some(last_sent_time);
⋮----
Saturating(transactions_to_retry) - transactions_added_to_retry;
⋮----
.fetch_add(retry_queue_overflow as u64, Ordering::Relaxed);
⋮----
.store(retry_transactions.len() as u64, Ordering::Relaxed);
⋮----
stats_report.report();
⋮----
fn retry_thread<Client: TransactionClient + std::marker::Send + 'static>(
⋮----
debug!("Starting send-transaction-service::retry_thread.");
let sharable_banks = bank_forks.read().unwrap().sharable_banks();
let retry_interval_ms_default = MAX_RETRY_SLEEP_MS.min(config.retry_rate_ms);
⋮----
.name("solStxRetry".to_string())
⋮----
sleep(Duration::from_millis(retry_interval_ms));
⋮----
let mut transactions = retry_transactions.lock().unwrap();
if transactions.is_empty() {
⋮----
.store(transactions.len() as u64, Ordering::Relaxed);
⋮----
} = sharable_banks.load();
⋮----
// Adjust retry interval taking into account the time since the last send.
⋮----
.checked_sub(
⋮----
.and_then(|last| Instant::now().checked_duration_since(last))
.and_then(|interval| interval.as_millis().try_into().ok())
.unwrap_or(0),
⋮----
.unwrap_or(retry_interval_ms_default);
⋮----
/// Retry transactions sent before.
    fn process_transactions<Client: TransactionClient + std::marker::Send + 'static>(
⋮----
fn process_transactions<Client: TransactionClient + std::marker::Send + 'static>(
⋮----
transactions.retain(|signature, transaction_info| {
if transaction_info.durable_nonce_info.is_some() {
stats.nonced_transactions.fetch_add(1, Ordering::Relaxed);
⋮----
.get_committed_transaction_status_and_slot(
⋮----
.is_some()
⋮----
info!("Transaction is rooted: {signature}");
⋮----
stats.rooted_transactions.fetch_add(1, Ordering::Relaxed);
⋮----
let signature_status = working_bank.get_committed_transaction_status_and_slot(
⋮----
let nonce_account = working_bank.get_account(&nonce_pubkey).unwrap_or_default();
⋮----
.and_then(|last| now.checked_duration_since(last))
.map(|elapsed| elapsed >= retry_rate)
.unwrap_or(false);
⋮----
if verify_nonce_account.is_none() && signature_status.is_none() && expired {
info!("Dropping expired durable-nonce transaction: {signature}");
⋮----
stats.expired_transactions.fetch_add(1, Ordering::Relaxed);
⋮----
if transaction_info.last_valid_block_height < root_bank.block_height() {
info!("Dropping expired transaction: {signature}");
⋮----
transaction_info.get_max_retries(default_max_retries, service_max_retries);
⋮----
info!("Dropping transaction due to max retries: {signature}");
⋮----
.unwrap_or(true);
⋮----
if transaction_info.last_sent_time.is_some() {
info!("Retrying transaction: {signature}");
⋮----
batched_transactions.push(*signature);
transaction_info.last_sent_time = Some(now);
⋮----
exceeded_retries_transactions.push(*signature);
⋮----
result.last_sent_time = Some(
⋮----
.map(|result_last| result_last.min(last))
.unwrap_or(last),
⋮----
info!("Dropping failed transaction: {signature}");
⋮----
stats.failed_transactions.fetch_add(1, Ordering::Relaxed);
⋮----
stats.retries.fetch_add(result.retried, Ordering::Relaxed);
if !batched_transactions.is_empty() {
⋮----
.iter()
.filter_map(|signature| transactions.get(signature))
.map(|transaction_info| transaction_info.wire_transaction.clone());
let iter = wire_transactions.chunks(batch_size);
⋮----
let chunk = chunk.collect();
client.send_transactions_in_batch(chunk, stats);
⋮----
result.max_retries_elapsed += exceeded_retries_transactions.len() as u64;
⋮----
.fetch_add(result.max_retries_elapsed, Ordering::Relaxed);
⋮----
transactions.remove(&signature);
⋮----
pub fn join(self) -> thread::Result<()> {
self.receive_txn_thread.join()?;
self.exit.store(true, Ordering::Relaxed);
self.retry_thread.join()
⋮----
mod test {
⋮----
async fn service_exit() {
⋮----
let (sender, receiver) = unbounded();
⋮----
ContactInfo::new_localhost(&node_keypair.pubkey(), timestamp()),
⋮----
TpuClientNextClient::create_client(Some(Handle::current()), cluster_info, None, 1);
⋮----
drop(sender);
send_transaction_service.join().unwrap();
client.stop();
⋮----
async fn validator_exit() {
⋮----
let (sender, receiver) = bounded(0);
⋮----
wire_transaction: vec![0; 128],
⋮----
&keypair.pubkey(),
&socketaddr!(Ipv4Addr::LOCALHOST, 1234),
⋮----
sender.send(dummy_tx_info()).unwrap();
⋮----
let mut option = Ok(());
while option.is_ok() {
option = sender.send(dummy_tx_info());
⋮----
async fn process_transactions() {
⋮----
let (mut genesis_config, mint_keypair) = create_genesis_config(4);
⋮----
bank_forks.read().unwrap().working_bank(),
⋮----
.write()
⋮----
.insert(root_bank)
.clone_without_scheduler();
⋮----
&mint_keypair.pubkey(),
⋮----
root_bank.last_blockhash(),
⋮----
root_bank.process_transaction(&transaction).unwrap();
⋮----
.insert(Bank::new_from_parent(
root_bank.clone(),
⋮----
working_bank.last_blockhash(),
⋮----
working_bank.process_transaction(&transaction).unwrap();
⋮----
working_bank.process_transaction(&transaction).unwrap_err();
⋮----
info!("Expired transactions are dropped...");
⋮----
transactions.insert(
⋮----
vec![],
root_bank.block_height() - 1,
⋮----
Some(Instant::now()),
⋮----
Some(Handle::current()),
⋮----
config.tpu_peers.clone(),
⋮----
assert!(transactions.is_empty());
assert_eq!(
⋮----
info!("Rooted transactions are dropped...");
⋮----
rooted_transaction.message.hash(),
⋮----
working_bank.block_height(),
⋮----
info!("Failed transactions are dropped...");
⋮----
failed_transaction.message.hash(),
⋮----
info!("Non-rooted transactions are kept...");
⋮----
non_rooted_transaction.message.hash(),
⋮----
assert_eq!(transactions.len(), 1);
⋮----
transactions.clear();
info!("Unknown transactions are retried...");
⋮----
Some(Instant::now().sub(Duration::from_millis(4000))),
⋮----
info!("Transactions are only retried until max_retries");
⋮----
Some(0),
⋮----
Some(1),
⋮----
async fn retry_durable_nonce_transactions() {
⋮----
AccountSharedData::new_data(43, &nonce_state, &system_program::id()).unwrap();
root_bank.store_account(&nonce_address, &nonce_account);
⋮----
let last_valid_block_height = working_bank.block_height() + 300;
⋮----
let blockhash = working_bank.last_blockhash();
⋮----
info!("Rooted durable-nonce transactions are dropped...");
⋮----
Some((nonce_address, *durable_nonce.as_hash())),
⋮----
Some((nonce_address, Hash::new_unique())),
⋮----
info!("Expired durable-nonce transactions are dropped...");
⋮----
info!("Failed durable-nonce transactions are dropped...");
⋮----
info!("Non-rooted durable-nonce transactions are kept...");
⋮----
info!("Unknown durable-nonce transactions are retried until nonce advances...");
⋮----
for transaction in transactions.values_mut() {
transaction.last_sent_time = Some(Instant::now().sub(Duration::from_millis(4000)));
⋮----
AccountSharedData::new_data(43, &new_nonce_state, &system_program::id()).unwrap();
working_bank.store_account(&nonce_address, &nonce_account);
⋮----
assert_eq!(transactions.len(), 0);

================
File: send-transaction-service/src/test_utils.rs
================
pub trait CreateClient: TransactionClient + Clone {
⋮----
impl CreateClient for TpuClientNextClient {
fn create_client(
⋮----
maybe_runtime.expect("Runtime should be provided for the TpuClientNextClient.");
let port_range = localhost_port_range_for_tests();
let bind_socket = bind_to(IpAddr::V4(Ipv4Addr::LOCALHOST), port_range.0)
.expect("Should be able to open UdpSocket for tests.");
⋮----
pub trait Stoppable {
⋮----
impl Stoppable for TpuClientNextClient {
fn stop(&self) {
self.cancel();
⋮----
pub trait ClientWithCreator:
⋮----
impl<T> ClientWithCreator for T where

================
File: send-transaction-service/src/tpu_info.rs
================
use std::net::SocketAddr;
pub trait TpuInfo {
⋮----
pub struct NullTpuInfo;
impl TpuInfo for NullTpuInfo {
fn refresh_recent_peers(&mut self) {}
fn get_leader_tpus(&self, _max_count: u64) -> Vec<&SocketAddr> {
vec![]
⋮----
fn get_not_unique_leader_tpus(&self, _max_count: u64) -> Vec<&SocketAddr> {

================
File: send-transaction-service/src/transaction_client.rs
================
pub trait TpuInfoWithSendStatic: TpuInfo + std::marker::Send + 'static {}
impl<T> TpuInfoWithSendStatic for T where T: TpuInfo + std::marker::Send + 'static {}
pub trait TransactionClient {
⋮----
pub struct CurrentLeaderInfo<T>
⋮----
pub fn get_leader_info(&mut self) -> Option<&T> {
if let Some(leader_info) = self.leader_info.as_mut() {
⋮----
.map(|last| now.duration_since(last) >= self.refresh_rate)
.unwrap_or(true);
⋮----
leader_info.refresh_recent_peers();
self.last_leader_refresh = Some(now);
⋮----
self.leader_info.as_ref()
⋮----
pub fn new(leader_info: Option<T>) -> Self {
⋮----
pub struct TpuClientNextClient {
⋮----
impl TpuClientNextClient {
pub fn new<T>(
⋮----
cancel.clone(),
⋮----
runtime_handle.spawn(scheduler.get_stats().report_to_influxdb(
⋮----
let _handle = runtime_handle.spawn(scheduler.run(config));
⋮----
fn create_config(
⋮----
stake_identity: stake_identity.map(StakeIdentity::new),
⋮----
pub fn cancel(&self) {
self.cancel.cancel();
⋮----
impl NotifyKeyUpdate for TpuClientNextClient {
fn update_key(&self, identity: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
⋮----
.send(Some(stake_identity))
.map_err(|e| Box::new(e) as Box<dyn std::error::Error>)
⋮----
impl TransactionClient for TpuClientNextClient {
fn send_transactions_in_batch(
⋮----
self.runtime_handle.spawn({
let sender = self.sender.clone();
⋮----
let res = sender.send(TransactionBatch::new(wire_transactions)).await;
if res.is_err() {
warn!("Failed to send transaction to channel: it is closed.");
⋮----
measure.stop();
stats.send_us.fetch_add(measure.as_us(), Ordering::Relaxed);
stats.send_attempt_count.fetch_add(1, Ordering::Relaxed);
⋮----
pub struct SendTransactionServiceLeaderUpdater<T: TpuInfoWithSendStatic> {
⋮----
impl<T> LeaderUpdater for SendTransactionServiceLeaderUpdater<T>
⋮----
fn next_leaders(&mut self, lookahead_leaders: usize) -> Vec<SocketAddr> {
⋮----
.get_leader_info()
.map(|leader_info| leader_info.get_not_unique_leader_tpus(lookahead_leaders as u64))
.filter(|addresses| !addresses.is_empty());
let mut all_peers = self.tpu_peers.clone().unwrap_or_default();
⋮----
all_peers.extend(discovered_peers.into_iter().cloned());
⋮----
all_peers.push(
⋮----
.my_contact_info()
.tpu(Protocol::QUIC)
.unwrap(),
⋮----
async fn stop(&mut self) {}

================
File: send-transaction-service/Cargo.toml
================
[package]
name = "solana-send-transaction-service"
description = "Solana send transaction service"
documentation = "https://docs.rs/solana-send-transaction-service"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = ["solana-net-utils"]

[dependencies]
async-trait = { workspace = true }
crossbeam-channel = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-connection-cache = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-metrics = { workspace = true }
solana-net-utils = { workspace = true, optional = true }
solana-nonce-account = { workspace = true }
solana-pubkey = { workspace = true }
solana-quic-definitions = { workspace = true }
solana-runtime = { workspace = true }
solana-signature = { workspace = true }
solana-time-utils = { workspace = true }
solana-tpu-client-next = { workspace = true, features = ["metrics"] }
tokio = { workspace = true, features = ["full"] }
tokio-util = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
solana-account = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-genesis-config = { workspace = true }
solana-net-utils = { workspace = true }
solana-nonce = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }

================
File: snapshots/src/archive_format.rs
================
pub enum ArchiveFormat {
⋮----
impl ArchiveFormat {
pub fn extension(&self) -> &str {
⋮----
pub fn from_cli_arg(archive_format_str: &str) -> Option<ArchiveFormat> {
⋮----
"zstd" => Some(ArchiveFormat::TarZstd {
⋮----
"lz4" => Some(ArchiveFormat::TarLz4),
⋮----
type Error = ParseError;
fn try_from(extension: &str) -> Result<Self, Self::Error> {
⋮----
TAR_ZSTD_EXTENSION => Ok(ArchiveFormat::TarZstd {
⋮----
TAR_LZ4_EXTENSION => Ok(ArchiveFormat::TarLz4),
_ => Err(ParseError::InvalidExtension(extension.to_string())),
⋮----
impl FromStr for ArchiveFormat {
type Err = ParseError;
fn from_str(extension: &str) -> Result<Self, Self::Err> {
⋮----
pub enum ArchiveFormatDecompressor<R> {
⋮----
pub fn new(format: ArchiveFormat, input: R) -> std::io::Result<Self> {
Ok(match format {
⋮----
Self::Lz4(lz4::Decoder::new(input).map_err(std::io::Error::other)?)
⋮----
fn read(&mut self, buf: &mut [u8]) -> std::io::Result<usize> {
⋮----
Self::Zstd(decoder) => decoder.read(buf),
Self::Lz4(decoder) => decoder.read(buf),
⋮----
pub enum ParseError {
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
⋮----
write!(f, "Invalid archive extension: {extension}")
⋮----
pub struct ZstdConfig {
⋮----
mod tests {
⋮----
fn test_extension() {
assert_eq!(
⋮----
assert_eq!(ArchiveFormat::TarLz4.extension(), TAR_LZ4_EXTENSION);
⋮----
fn test_try_from() {
⋮----
fn test_from_str() {
⋮----
fn test_from_cli_arg() {
⋮----
Some(ArchiveFormat::TarZstd {
⋮----
Some(ArchiveFormat::TarLz4),
⋮----
for (arg, expected) in zip(SUPPORTED_ARCHIVE_COMPRESSION.iter(), golden.into_iter()) {
assert_eq!(ArchiveFormat::from_cli_arg(arg), expected);
⋮----
assert_eq!(ArchiveFormat::from_cli_arg("bad"), None);

================
File: snapshots/src/archive.rs
================
pub fn archive_snapshot(
⋮----
info!("Generating snapshot archive for slot {snapshot_slot}, kind: {snapshot_archive_kind:?}");
⋮----
.as_ref()
.parent()
.expect("Tar output path is invalid");
fs::create_dir_all(tar_dir).map_err(|err| E::CreateArchiveDir(err, tar_dir.to_path_buf()))?;
⋮----
.prefix(&format!("{staging_dir_prefix}{snapshot_slot}-"))
.tempdir_in(tar_dir)
.map_err(|err| E::CreateStagingDir(err, tar_dir.to_path_buf()))?;
let staging_snapshots_dir = staging_dir.path().join(paths::BANK_SNAPSHOTS_DIR);
let slot_str = snapshot_slot.to_string();
let staging_snapshot_dir = staging_snapshots_dir.join(&slot_str);
⋮----
.map_err(|err| E::CreateSnapshotStagingDir(err, staging_snapshot_dir.clone()))?;
let src_snapshot_dir = bank_snapshot_dir.as_ref().canonicalize().map_err(|err| {
E::CanonicalizeSnapshotSourceDir(err, bank_snapshot_dir.as_ref().to_path_buf())
⋮----
let staging_snapshot_file = staging_snapshot_dir.join(&slot_str);
let src_snapshot_file = src_snapshot_dir.join(slot_str);
⋮----
.map_err(|err| E::SymlinkSnapshot(err, src_snapshot_file, staging_snapshot_file))?;
let staging_status_cache = staging_snapshots_dir.join(paths::SNAPSHOT_STATUS_CACHE_FILENAME);
let src_status_cache = src_snapshot_dir.join(paths::SNAPSHOT_STATUS_CACHE_FILENAME);
⋮----
.map_err(|err| E::SymlinkStatusCache(err, src_status_cache, staging_status_cache))?;
let staging_version_file = staging_dir.path().join(paths::SNAPSHOT_VERSION_FILENAME);
let src_version_file = src_snapshot_dir.join(paths::SNAPSHOT_VERSION_FILENAME);
symlink::symlink_file(&src_version_file, &staging_version_file).map_err(|err| {
E::SymlinkVersionFile(err, src_version_file, staging_version_file.clone())
⋮----
let staging_archive_path = tar_dir.join(format!(
⋮----
.map_err(|err| E::CreateArchiveFile(err, staging_archive_path.clone()))?;
⋮----
archive.sparse(false);
⋮----
.append_path_with_name(&staging_version_file, paths::SNAPSHOT_VERSION_FILENAME)
.map_err(E::ArchiveVersionFile)?;
⋮----
.append_dir_all(paths::BANK_SNAPSHOTS_DIR, &staging_snapshots_dir)
.map_err(E::ArchiveSnapshotsDir)?;
⋮----
for storage in storages_orderer.iter() {
⋮----
.join(AccountsFile::file_name(storage.slot(), storage.id()));
⋮----
AccountStorageReader::new(storage, Some(snapshot_slot)).map_err(|err| {
E::AccountStorageReaderError(err, storage.path().to_path_buf())
⋮----
header.set_path(path_in_archive).map_err(|err| {
E::ArchiveAccountStorageFile(err, storage.path().to_path_buf())
⋮----
header.set_size(reader.len() as u64);
header.set_cksum();
archive.append(&header, reader).map_err(|err| {
⋮----
archive.into_inner().map_err(E::FinishArchive)?;
Ok(())
⋮----
.map_err(E::CreateEncoder)?;
do_archive_files(&mut encoder)?;
encoder.finish().map_err(E::FinishEncoder)?;
⋮----
.level(1)
.build(archive_file)
⋮----
let (_output, result) = encoder.finish();
result.map_err(E::FinishEncoder)?;
⋮----
.map_err(|err| E::QueryArchiveMetadata(err, staging_archive_path.clone()))?;
let archive_path = archive_path.as_ref().to_path_buf();
⋮----
.map_err(|err| E::MoveArchive(err, staging_archive_path, archive_path.clone()))?;
timer.stop();
info!(
⋮----
datapoint_info!(
⋮----
Ok(SnapshotArchiveInfo {

================
File: snapshots/src/error.rs
================
pub enum SnapshotError {
⋮----
pub enum SnapshotFastbootError {
⋮----
pub enum SnapshotNewFromDirError {
⋮----
pub enum VerifySlotDeltasError {
⋮----
pub enum VerifySlotHistoryError {
⋮----
pub enum VerifyEpochStakesError {
⋮----
pub enum AddBankSnapshotError {
⋮----
pub enum ArchiveSnapshotPackageError {
⋮----
pub enum HardLinkStoragesToSnapshotError {
⋮----
pub enum GetSnapshotAccountsHardLinkDirError {

================
File: snapshots/src/hardened_unpack.rs
================
pub enum UnpackError {
⋮----
pub type Result<T> = std::result::Result<T, UnpackError>;
⋮----
fn checked_total_size_sum(total_size: u64, entry_size: u64, limit_size: u64) -> Result<u64> {
trace!("checked_total_size_sum: {total_size} + {entry_size} < {limit_size}");
let total_size = total_size.saturating_add(entry_size);
⋮----
return Err(UnpackError::Archive(format!(
⋮----
Ok(total_size)
⋮----
fn checked_total_count_increment(total_count: u64, limit_count: u64) -> Result<u64> {
⋮----
Ok(total_count)
⋮----
fn check_unpack_result(unpack_result: Result<()>, path: String) -> Result<()> {
⋮----
Ok(())
⋮----
enum UnpackPath<'a> {
⋮----
fn unpack_archive<'a, C>(
⋮----
mut entry_checker: C, // checks if entry is valid
⋮----
for entry in archive.entries()? {
⋮----
let path = entry.path()?;
let path_str = path.display().to_string();
⋮----
.components()
.map(|p| match p {
CurDir => Ok("."),
Normal(c) => c.to_str().ok_or(()),
_ => Err(()),
⋮----
entry.header().as_ustar().is_none() && entry.path_bytes().ends_with(b"/");
let kind = entry.header().entry_type();
⋮----
let unpack_dir = match entry_checker(parts.as_slice(), kind) {
⋮----
apparent_total_size = checked_total_size_sum(
⋮----
entry.header().size()?,
⋮----
actual_total_size = checked_total_size_sum(
⋮----
entry.header().entry_size()?,
⋮----
total_count = checked_total_count_increment(total_count, limit_count)?;
let account_filename = match parts.as_slice() {
["accounts", account_filename] => Some(PathBuf::from(account_filename)),
⋮----
sanitize_path_and_open_dir(&account, unpack_dir, &mut open_dirs)
⋮----
sanitize_path_and_open_dir(&path, unpack_dir, &mut open_dirs)
⋮----
let unpack = unpack_entry(&mut file_creator, entry, entry_path, open_dir);
check_unpack_result(unpack, path_str)?;
⋮----
file_creator.drain()?;
info!("unpacked {total_entries} entries total");
⋮----
fn unpack_entry<'a, R: Read>(
⋮----
let mode = match entry.header().entry_type() {
⋮----
if should_fallback_to_tar_unpack(&entry) {
entry.unpack(&dst)?;
// Sanitize permissions.
⋮----
if !entry.header().entry_type().is_dir() {
// Process file after setting permissions
files_creator.file_complete(dst);
⋮----
return Ok(());
⋮----
files_creator.schedule_create_at_dir(dst, mode, dst_open_dir, &mut entry)?;
⋮----
fn should_fallback_to_tar_unpack<R: io::Read>(entry: &tar::Entry<'_, R>) -> bool {
matches!(
⋮----
) || entry.header().as_ustar().is_none() && entry.path_bytes().ends_with(b"/")
⋮----
// return Err on file system error
// return Some((path, open_dir)) if path is good
// return None if we should skip this file
fn sanitize_path_and_open_dir(
⋮----
// We cannot call unpack_in because it errors if we try to use 2 account paths.
// So, this code is borrowed from unpack_in
// ref: https://docs.rs/tar/*/tar/struct.Entry.html#method.unpack_in
let mut file_dst = dst.to_path_buf();
const SKIP: Result<Option<(PathBuf, Arc<File>)>> = Ok(None);
⋮----
for part in path.components() {
⋮----
// Leading '/' characters, root paths, and '.'
// components are just ignored and treated as "empty
⋮----
Component::Normal(part) => file_dst.push(part),
⋮----
let Some(parent) = file_dst.parent() else {
⋮----
let open_dst_dir = match open_dirs.binary_search_by(|(key, _)| parent.cmp(key)) {
⋮----
validate_inside_dst(dst, parent)?;
⋮----
open_dirs.insert(insert_at, (parent.to_path_buf(), opened_dir.clone()));
⋮----
Ok(index) => open_dirs[index].1.clone(),
⋮----
Ok(Some((file_dst, open_dst_dir)))
⋮----
fn validate_inside_dst(dst: &Path, file_dst: &Path) -> Result<PathBuf> {
let canon_parent = file_dst.canonicalize().map_err(|err| {
UnpackError::Archive(format!("{err} while canonicalizing {}", file_dst.display()))
⋮----
let canon_target = dst.canonicalize().map_err(|err| {
UnpackError::Archive(format!("{err} while canonicalizing {}", dst.display()))
⋮----
if !canon_parent.starts_with(&canon_target) {
⋮----
Ok(canon_target)
⋮----
pub(super) fn streaming_unpack_snapshot(
⋮----
unpack_snapshot_with_processors(input, file_creator, ledger_dir, account_paths, |_, _| {})
⋮----
fn unpack_snapshot_with_processors<F>(
⋮----
assert!(!account_paths.is_empty());
unpack_archive(
⋮----
if is_valid_snapshot_archive_entry(parts, kind) {
⋮----
let path_index = rng().random_range(0..account_paths.len());
⋮----
.get(path_index)
.map(|path_buf| path_buf.as_path())
⋮----
accounts_path_processor(file, path);
⋮----
fn all_digits(v: &str) -> bool {
if v.is_empty() {
⋮----
for x in v.chars() {
if !x.is_ascii_digit() {
⋮----
fn like_storage(v: &str) -> bool {
⋮----
fn is_valid_snapshot_archive_entry(parts: &[&str], kind: tar::EntryType) -> bool {
⋮----
(["accounts", file], GNUSparse) if like_storage(file) => true,
(["accounts", file], Regular) if like_storage(file) => true,
⋮----
(["snapshots", dir, file], GNUSparse) if all_digits(dir) && all_digits(file) => true,
(["snapshots", dir, file], Regular) if all_digits(dir) && all_digits(file) => true,
(["snapshots", dir], Directory) if all_digits(dir) => true,
⋮----
pub(super) fn unpack_genesis(
⋮----
|p, k| is_valid_genesis_archive_entry(unpack_dir, p, k),
⋮----
fn is_valid_genesis_archive_entry<'a>(
⋮----
trace!("validating: {parts:?} {kind:?}");
⋮----
mod tests {
⋮----
fn test_archive_is_valid_entry() {
assert!(is_valid_snapshot_archive_entry(
⋮----
assert!(!is_valid_snapshot_archive_entry(
⋮----
fn test_valid_snapshot_accounts() {
⋮----
fn test_archive_is_valid_archive_entry() {
⋮----
assert_eq!(
⋮----
fn with_finalize_and_unpack<C>(archive: tar::Builder<Vec<u8>>, checker: C) -> Result<()>
⋮----
let data = archive.into_inner().unwrap();
let temp_dir = tempfile::TempDir::new().unwrap();
checker(data.as_slice(), temp_dir.path())?;
let result = temp_dir.close();
assert_matches!(result, Ok(()));
⋮----
fn finalize_and_unpack_snapshot(archive: tar::Builder<Vec<u8>>) -> Result<()> {
let file_creator = file_creator(256, |_| {})?;
with_finalize_and_unpack(archive, move |a, b| {
unpack_snapshot_with_processors(a, file_creator, b, &[PathBuf::new()], |_, _| {})
.map(|_| ())
⋮----
fn finalize_and_unpack_genesis(archive: tar::Builder<Vec<u8>>) -> Result<()> {
let file_creator = file_creator(0, |_| {})?;
⋮----
unpack_genesis(a, file_creator, b, MAX_GENESIS_SIZE_FOR_TESTS)
⋮----
fn test_archive_unpack_snapshot_ok() {
⋮----
header.set_path("version").unwrap();
header.set_size(4);
header.set_cksum();
⋮----
archive.append(&header, data).unwrap();
let result = finalize_and_unpack_snapshot(archive);
⋮----
fn test_archive_unpack_genesis_ok() {
⋮----
header.set_path("genesis.bin").unwrap();
⋮----
let result = finalize_and_unpack_genesis(archive);
⋮----
fn test_archive_unpack_genesis_size_limit() {
⋮----
.map(|x| x as u8)
.collect();
⋮----
header.set_entry_type(tar::EntryType::Regular);
⋮----
header.set_size(data.len() as u64);
⋮----
archive.append(&header, data.as_slice()).unwrap();
finalize_and_unpack_genesis(archive).expect_err(&format!(
⋮----
finalize_and_unpack_genesis(archive).expect("should unpack max size genesis");
⋮----
fn test_archive_unpack_genesis_bad_perms() {
⋮----
header.set_path("rocksdb").unwrap();
header.set_entry_type(Directory);
header.set_size(0);
⋮----
header.set_path("rocksdb/test").unwrap();
⋮----
header.set_mode(0o000);
⋮----
fn test_archive_unpack_genesis_bad_rocksdb_subdir() {
⋮----
header.set_path("rocksdb/test/").unwrap();
header.set_entry_type(Regular);
⋮----
assert_matches!(result, Err(UnpackError::Archive(ref message)) if message == "invalid path found: \"rocksdb/test/\"");
⋮----
fn test_archive_unpack_snapshot_invalid_path() {
⋮----
.as_old_mut()
⋮----
.iter_mut()
.zip(b"foo/../../../dangerous".iter().chain(Some(&0)))
⋮----
assert_matches!(result, Err(UnpackError::Archive(ref message)) if message == "invalid path found: \"foo/../../../dangerous\"");
⋮----
fn with_archive_unpack_snapshot_invalid_path(path: &str) -> Result<()> {
⋮----
.zip(path.as_bytes().iter().chain(Some(&0)))
⋮----
with_finalize_and_unpack(archive, |data, path| {
⋮----
for entry in unpacking_archive.entries()? {
if !entry?.unpack_in(path)? {
return Err(UnpackError::Archive("failed!".to_string()));
} else if !path.join(path).exists() {
return Err(UnpackError::Archive("not existing!".to_string()));
⋮----
fn test_archive_unpack_itself() {
assert_matches!(
⋮----
assert_matches!(with_archive_unpack_snapshot_invalid_path("../../../dangerous"), Err(UnpackError::Archive(ref message)) if message == "failed!");
⋮----
fn test_archive_unpack_snapshot_invalid_entry() {
⋮----
header.set_path("foo").unwrap();
⋮----
assert_matches!(result, Err(UnpackError::Archive(ref message)) if message == "extra entry found: \"foo\" Regular");
⋮----
fn test_archive_unpack_snapshot_too_large() {
⋮----
header.set_size(1024 * 1024 * 1024 * 1024 * 1024);
⋮----
fn test_archive_unpack_snapshot_bad_unpack() {
let result = check_unpack_result(
Err(UnpackError::Io(io::ErrorKind::FileTooLarge.into())),
"abc".to_string(),
⋮----
assert_matches!(result, Err(UnpackError::Archive(ref message)) if message == "failed to unpack \"abc\": IO error: file too large");
⋮----
fn test_archive_checked_total_size_sum() {
let result = checked_total_size_sum(500, 500, MAX_SNAPSHOT_ARCHIVE_UNPACKED_ACTUAL_SIZE);
assert_matches!(result, Ok(1000));
⋮----
checked_total_size_sum(u64::MAX - 2, 2, MAX_SNAPSHOT_ARCHIVE_UNPACKED_ACTUAL_SIZE);
⋮----
fn test_archive_checked_total_size_count() {
let result = checked_total_count_increment(101, MAX_SNAPSHOT_ARCHIVE_UNPACKED_COUNT);
assert_matches!(result, Ok(102));
⋮----
checked_total_count_increment(999_999_999_999, MAX_SNAPSHOT_ARCHIVE_UNPACKED_COUNT);
⋮----
fn test_archive_unpack_account_path() {
⋮----
header.set_path("accounts/123.456").unwrap();
⋮----
let result = with_finalize_and_unpack(archive, |ar, tmp| {
let tmp_path_buf = tmp.to_path_buf();
let file_creator = file_creator(256, move |path| {
assert_eq!(path, tmp_path_buf.join("accounts_dest/123.456"))
⋮----
.expect("must make file_creator");
unpack_snapshot_with_processors(
⋮----
&[tmp.join("accounts_dest")],

================
File: snapshots/src/kind.rs
================
use solana_clock::Slot;
⋮----
pub enum SnapshotKind {
⋮----
impl SnapshotKind {
pub fn is_full_snapshot(&self) -> bool {
matches!(self, SnapshotKind::Archive(SnapshotArchiveKind::Full))
⋮----
pub fn is_incremental_snapshot(&self) -> bool {
matches!(
⋮----
pub enum SnapshotArchiveKind {

================
File: snapshots/src/lib.rs
================
mod archive;
mod archive_format;
pub mod error;
pub mod hardened_unpack;
mod kind;
pub mod paths;
pub mod snapshot_archive_info;
pub mod snapshot_config;
pub mod snapshot_hash;
mod snapshot_interval;
mod snapshot_version;
mod unarchive;
pub type Result<T> = std::result::Result<T, error::SnapshotError>;

================
File: snapshots/src/paths.rs
================
/// Get the `&str` from a `&Path`
pub fn path_to_file_name_str(path: &Path) -> Result<&str> {
⋮----
pub fn path_to_file_name_str(path: &Path) -> Result<&str> {
path.file_name()
.ok_or_else(|| SnapshotError::PathToFileNameError(path.to_path_buf()))?
.to_str()
.ok_or_else(|| SnapshotError::FileNameToStrError(path.to_path_buf()))
⋮----
pub fn build_snapshot_archives_remote_dir(snapshot_archives_dir: impl AsRef<Path>) -> PathBuf {
⋮----
.as_ref()
.join(SNAPSHOT_ARCHIVE_DOWNLOAD_DIR)
⋮----
/// Build the full snapshot archive path from its components: the snapshot archives directory, the
/// snapshot slot, the accounts hash, and the archive format.
⋮----
/// snapshot slot, the accounts hash, and the archive format.
pub fn build_full_snapshot_archive_path(
⋮----
pub fn build_full_snapshot_archive_path(
⋮----
full_snapshot_archives_dir.as_ref().join(format!(
⋮----
/// Build the incremental snapshot archive path from its components: the snapshot archives
/// directory, the snapshot base slot, the snapshot slot, the accounts hash, and the archive
⋮----
/// directory, the snapshot base slot, the snapshot slot, the accounts hash, and the archive
/// format.
⋮----
/// format.
pub fn build_incremental_snapshot_archive_path(
⋮----
pub fn build_incremental_snapshot_archive_path(
⋮----
incremental_snapshot_archives_dir.as_ref().join(format!(
⋮----
/// Parse a full snapshot archive filename into its Slot, Hash, and Archive Format
pub fn parse_full_snapshot_archive_filename(
⋮----
pub fn parse_full_snapshot_archive_filename(
⋮----
LazyLock::new(|| Regex::new(FULL_SNAPSHOT_ARCHIVE_FILENAME_REGEX).unwrap());
⋮----
RE.captures(archive_filename).and_then(|captures| {
⋮----
.name("slot")
.map(|x| x.as_str().parse::<Slot>())?
.ok()?;
⋮----
.name("hash")
.map(|x| x.as_str().parse::<Hash>())?
⋮----
.name("ext")
.map(|x| x.as_str().parse::<ArchiveFormat>())?
⋮----
Some((slot, SnapshotHash(hash), archive_format))
⋮----
do_parse().ok_or_else(|| {
SnapshotError::ParseSnapshotArchiveFileNameError(archive_filename.to_string())
⋮----
pub fn parse_incremental_snapshot_archive_filename(
⋮----
LazyLock::new(|| Regex::new(INCREMENTAL_SNAPSHOT_ARCHIVE_FILENAME_REGEX).unwrap());
⋮----
.name("base")
⋮----
Some((base_slot, slot, SnapshotHash(hash), archive_format))
⋮----
pub fn get_snapshot_file_name(slot: Slot) -> String {
slot.to_string()
⋮----
pub fn get_bank_snapshot_dir(bank_snapshots_dir: impl AsRef<Path>, slot: Slot) -> PathBuf {
⋮----
.join(get_snapshot_file_name(slot))
⋮----
fn get_snapshot_archives<T, F>(snapshot_archives_dir: &Path, cb: F) -> Vec<T>
⋮----
info!(
⋮----
vec![]
⋮----
.filter_map(|entry| entry.map_or(None, |entry| cb(entry.path()).ok()))
.collect(),
⋮----
let mut ret = walk_dir(snapshot_archives_dir);
let remote_dir = build_snapshot_archives_remote_dir(snapshot_archives_dir);
if remote_dir.exists() {
ret.append(&mut walk_dir(remote_dir.as_ref()));
⋮----
pub fn get_full_snapshot_archives(
⋮----
get_snapshot_archives(
full_snapshot_archives_dir.as_ref(),
⋮----
pub fn get_incremental_snapshot_archives(
⋮----
incremental_snapshot_archives_dir.as_ref(),
⋮----
pub fn get_highest_full_snapshot_archive_slot(
⋮----
get_highest_full_snapshot_archive_info(full_snapshot_archives_dir)
.map(|full_snapshot_archive_info| full_snapshot_archive_info.slot())
⋮----
pub fn get_highest_incremental_snapshot_archive_slot(
⋮----
get_highest_incremental_snapshot_archive_info(
⋮----
.map(|incremental_snapshot_archive_info| incremental_snapshot_archive_info.slot())
⋮----
pub fn get_highest_full_snapshot_archive_info(
⋮----
let mut full_snapshot_archives = get_full_snapshot_archives(full_snapshot_archives_dir);
full_snapshot_archives.sort_unstable();
full_snapshot_archives.into_iter().next_back()
⋮----
pub fn get_highest_incremental_snapshot_archive_info(
⋮----
get_incremental_snapshot_archives(incremental_snapshot_archives_dir)
.into_iter()
.filter(|incremental_snapshot_archive_info| {
incremental_snapshot_archive_info.base_slot() == full_snapshot_slot
⋮----
incremental_snapshot_archives.sort_unstable();
incremental_snapshot_archives.into_iter().next_back()
⋮----
mod tests {
⋮----
fn test_parse_full_snapshot_archive_filename() {
assert_eq!(
⋮----
assert!(parse_full_snapshot_archive_filename("invalid").is_err());
assert!(
⋮----
assert!(parse_full_snapshot_archive_filename(&format!(
⋮----
fn test_parse_incremental_snapshot_archive_filename() {
⋮----
assert!(parse_incremental_snapshot_archive_filename("invalid").is_err());
assert!(parse_incremental_snapshot_archive_filename(&format!(
⋮----
assert!(parse_incremental_snapshot_archive_filename(

================
File: snapshots/src/snapshot_archive_info.rs
================
pub trait SnapshotArchiveInfoGetter {
⋮----
fn path(&self) -> &PathBuf {
&self.snapshot_archive_info().path
⋮----
fn slot(&self) -> Slot {
self.snapshot_archive_info().slot
⋮----
fn hash(&self) -> &SnapshotHash {
&self.snapshot_archive_info().hash
⋮----
fn archive_format(&self) -> ArchiveFormat {
self.snapshot_archive_info().archive_format
⋮----
fn is_remote(&self) -> bool {
self.snapshot_archive_info()
⋮----
.parent()
.is_some_and(|p| p.ends_with(snapshot_paths::SNAPSHOT_ARCHIVE_DOWNLOAD_DIR))
⋮----
pub struct SnapshotArchiveInfo {
⋮----
pub struct FullSnapshotArchiveInfo(SnapshotArchiveInfo);
impl FullSnapshotArchiveInfo {
pub fn new_from_path(path: PathBuf) -> Result<Self> {
let filename = snapshot_paths::path_to_file_name_str(path.as_path())?;
⋮----
Ok(Self::new(SnapshotArchiveInfo {
⋮----
pub fn new(snapshot_archive_info: SnapshotArchiveInfo) -> Self {
Self(snapshot_archive_info)
⋮----
impl SnapshotArchiveInfoGetter for FullSnapshotArchiveInfo {
fn snapshot_archive_info(&self) -> &SnapshotArchiveInfo {
⋮----
impl PartialOrd for FullSnapshotArchiveInfo {
fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
Some(self.cmp(other))
⋮----
impl Ord for FullSnapshotArchiveInfo {
fn cmp(&self, other: &Self) -> Ordering {
self.slot().cmp(&other.slot())
⋮----
pub struct IncrementalSnapshotArchiveInfo {
⋮----
impl IncrementalSnapshotArchiveInfo {
⋮----
Ok(Self::new(
⋮----
pub fn new(base_slot: Slot, snapshot_archive_info: SnapshotArchiveInfo) -> Self {
⋮----
pub fn base_slot(&self) -> Slot {
⋮----
impl SnapshotArchiveInfoGetter for IncrementalSnapshotArchiveInfo {
⋮----
impl PartialOrd for IncrementalSnapshotArchiveInfo {
⋮----
impl Ord for IncrementalSnapshotArchiveInfo {
⋮----
self.base_slot()
.cmp(&other.base_slot())
.then(self.slot().cmp(&other.slot()))

================
File: snapshots/src/snapshot_config.rs
================
NonZeroU64::new(100_000).unwrap();
⋮----
NonZeroU64::new(100).unwrap();
⋮----
NonZeroUsize::new(2).unwrap();
⋮----
NonZeroUsize::new(4).unwrap();
⋮----
pub struct SnapshotConfig {
⋮----
impl Default for SnapshotConfig {
fn default() -> Self {
⋮----
impl SnapshotConfig {
pub fn new_load_only() -> Self {
⋮----
pub fn new_disabled() -> Self {
⋮----
pub fn should_generate_snapshots(&self) -> bool {
⋮----
pub fn should_load_snapshots(&self) -> bool {
⋮----
pub enum SnapshotUsage {

================
File: snapshots/src/snapshot_hash.rs
================
pub struct StartingSnapshotHashes {
⋮----
pub struct FullSnapshotHash(pub (Slot, SnapshotHash));
⋮----
pub struct IncrementalSnapshotHash(pub (Slot, SnapshotHash));
⋮----
pub struct SnapshotHash(pub Hash);
impl SnapshotHash {
⋮----
pub fn new(accounts_lt_hash_checksum: AccountsLtHashChecksum) -> Self {
⋮----
Self(accounts_hash)

================
File: snapshots/src/snapshot_interval.rs
================
use std::num::NonZeroU64;
⋮----
pub enum SnapshotInterval {

================
File: snapshots/src/snapshot_version.rs
================
pub enum SnapshotVersion {
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
f.write_str(From::from(*self))
⋮----
fn from(snapshot_version: SnapshotVersion) -> &'static str {
⋮----
impl FromStr for SnapshotVersion {
type Err = &'static str;
fn from_str(version_string: &str) -> Result<Self, Self::Err> {
// Remove leading 'v' or 'V' from slice
⋮----
.get(..1)
.is_some_and(|s| s.eq_ignore_ascii_case("v"))
⋮----
VERSION_STRING_V1_2_0 => Ok(SnapshotVersion::V1_2_0),
_ => Err("unsupported snapshot version"),
⋮----
impl SnapshotVersion {
pub fn as_str(self) -> &'static str {

================
File: snapshots/src/unarchive.rs
================
pub fn streaming_unarchive_snapshot(
⋮----
let archive_size = fs::metadata(archive_path)?.len() as usize;
let read_write_budget_size = (memlock_budget_size / 2).min(archive_size);
let read_buf_size = MAX_SNAPSHOT_READER_BUF_SIZE.min(read_write_budget_size as u64);
let decompressor = decompressed_tar_reader(archive_format, archive_path, read_buf_size)?;
let write_buf_size = MAX_UNPACK_WRITE_BUF_SIZE.min(read_write_budget_size);
let file_creator = file_creator(write_buf_size, move |file_path| {
let result = file_sender.send(file_path);
⋮----
panic!(
⋮----
ledger_dir.as_path(),
⋮----
.name("solTarUnpack".to_string())
.spawn(move || {
do_unpack(&snapshot_archive_path)
.map_err(|err| UnpackError::Unpack(Box::new(err), snapshot_archive_path))
⋮----
.unwrap()
⋮----
pub fn unpack_genesis_archive(
⋮----
let file_creator = file_creator(
⋮----
Ok(())
⋮----
fn decompressed_tar_reader(
⋮----
buffered_reader::large_file_buf_reader(archive_path.as_ref(), buf_size as usize)?;

================
File: snapshots/Cargo.toml
================
[package]
name = "agave-snapshots"
description = "Agave snapshot utils"
documentation = "https://docs.rs/agave-snapshots"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "agave_snapshots"

[features]
agave-unstable-api = []

[dependencies]
agave-fs = { workspace = true }
bincode = { workspace = true }
bzip2 = { workspace = true }
crossbeam-channel = { workspace = true }
log = { workspace = true }
lz4 = { workspace = true }
rand = { workspace = true }
regex = { workspace = true }
semver = { workspace = true }
solana-accounts-db = { workspace = true }
solana-clock = { workspace = true }
solana-genesis-config = { workspace = true }
solana-hash = { workspace = true }
solana-lattice-hash = { workspace = true }
solana-measure = { workspace = true }
solana-metrics = { workspace = true }
strum = { workspace = true, features = ["derive"] }
symlink = { workspace = true }
tar = { workspace = true }
tempfile = { workspace = true }
thiserror = { workspace = true }
zstd = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
assert_matches = { workspace = true }

================
File: stake-accounts/src/arg_parser.rs
================
fn fee_payer_arg<'a, 'b>() -> Arg<'a, 'b> {
solana_clap_utils::fee_payer::fee_payer_arg().required(true)
⋮----
fn funding_keypair_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.required(true)
.takes_value(true)
.value_name("FUNDING_KEYPAIR")
.validator(is_valid_signer)
.help("Keypair to fund accounts")
⋮----
fn base_pubkey_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.value_name("BASE_PUBKEY")
.validator(is_valid_pubkey)
.help("Public key which stake account addresses are derived from")
⋮----
fn custodian_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.value_name("KEYPAIR")
⋮----
.help("Authority to modify lockups")
⋮----
fn new_custodian_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.value_name("PUBKEY")
⋮----
.help("New authority to modify lockups")
⋮----
fn new_base_keypair_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.value_name("NEW_BASE_KEYPAIR")
⋮----
.help("New keypair which stake account addresses are derived from")
⋮----
fn stake_authority_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("stake-authority")
⋮----
.help("Stake authority")
⋮----
fn withdraw_authority_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("withdraw-authority")
⋮----
.help("Withdraw authority")
⋮----
fn new_stake_authority_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("new-stake-authority")
⋮----
.help("New stake authority")
⋮----
fn new_withdraw_authority_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("new-withdraw-authority")
⋮----
.help("New withdraw authority")
⋮----
fn lockup_epoch_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("lockup-epoch")
⋮----
.value_name("NUMBER")
.help("The epoch height at which each account will be available for withdrawal")
⋮----
fn lockup_date_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("lockup-date")
.value_name("RFC3339 DATETIME")
.validator(is_rfc3339_datetime)
⋮----
.help("The date and time at which each account will be available for withdrawal")
⋮----
fn num_accounts_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("num-accounts")
⋮----
.help("Number of derived stake accounts")
⋮----
pub(crate) fn get_matches<'a, I, T>(args: I) -> ArgMatches<'a>
⋮----
let default_config_file = CONFIG_FILE.as_ref().unwrap();
App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.long("config")
.global(true)
⋮----
.value_name("FILEPATH")
.default_value(default_config_file)
.help("Config file"),
⋮----
.long("url")
⋮----
.value_name("URL")
.help("RPC entrypoint address. i.e. http://api.devnet.solana.com"),
⋮----
.long("commitment")
⋮----
.value_name("COMMITMENT_LEVEL")
.possible_values(&["processed", "confirmed", "finalized"])
.hide_possible_values(true)
.help(
⋮----
.long("no-wait")
⋮----
.help("Send transactions without waiting for confirmation"),
⋮----
.subcommand(
⋮----
.about("Create derived stake accounts")
.arg(fee_payer_arg())
.arg(funding_keypair_arg().index(1))
⋮----
.index(2)
⋮----
.value_name("BASE_KEYPAIR")
⋮----
.help("Keypair which stake account addresses are derived from"),
⋮----
.index(3)
⋮----
.value_name("AMOUNT")
.validator(is_amount)
.help("Amount to move into the new stake accounts, in SOL"),
⋮----
.help("Stake authority"),
⋮----
.help("Withdraw authority"),
⋮----
.long("index")
⋮----
.default_value("0")
⋮----
.help("Index of the derived account to create"),
⋮----
.about("Count derived stake accounts")
.arg(base_pubkey_arg().index(1)),
⋮----
.about("Show public keys of all derived stake accounts")
.arg(base_pubkey_arg().index(1))
.arg(num_accounts_arg()),
⋮----
.about("Sum balances of all derived stake accounts")
⋮----
.about("Set new authorities in all derived stake accounts")
⋮----
.arg(stake_authority_arg())
.arg(withdraw_authority_arg())
.arg(new_stake_authority_arg())
.arg(new_withdraw_authority_arg())
⋮----
.about("Set new lockups in all derived stake accounts")
⋮----
.arg(custodian_arg())
.arg(lockup_epoch_arg())
.arg(lockup_date_arg())
.arg(new_custodian_arg())
.arg(num_accounts_arg())
⋮----
.long("unlock-years")
⋮----
.help("Years to unlock after the cliff"),
⋮----
.about("Relocate derived stake accounts")
⋮----
.arg(new_base_keypair_arg().index(2))
⋮----
.about("Rebase and set new authorities in all derived stake accounts")
⋮----
.get_matches_from(args)
⋮----
fn parse_new_args(matches: &ArgMatches<'_>) -> NewArgs<String, String> {
⋮----
fee_payer: value_t_or_exit!(matches, "fee_payer", String),
funding_keypair: value_t_or_exit!(matches, "funding_keypair", String),
⋮----
.value_of("amount")
.and_then(sol_str_to_lamports)
.unwrap(),
base_keypair: value_t_or_exit!(matches, "base_keypair", String),
stake_authority: value_t_or_exit!(matches, "stake_authority", String),
withdraw_authority: value_t_or_exit!(matches, "withdraw_authority", String),
index: value_t_or_exit!(matches, "index", usize),
no_wait: matches.is_present("no_wait"),
⋮----
fn parse_count_args(matches: &ArgMatches<'_>) -> CountArgs<String> {
⋮----
base_pubkey: value_t_or_exit!(matches, "base_pubkey", String),
⋮----
fn parse_query_args(matches: &ArgMatches<'_>) -> QueryArgs<String> {
⋮----
num_accounts: value_t_or_exit!(matches, "num_accounts", usize),
⋮----
fn parse_authorize_args(matches: &ArgMatches<'_>) -> AuthorizeArgs<String, String> {
⋮----
new_stake_authority: value_t_or_exit!(matches, "new_stake_authority", String),
new_withdraw_authority: value_t_or_exit!(matches, "new_withdraw_authority", String),
⋮----
fn parse_set_lockup_args(matches: &ArgMatches<'_>) -> SetLockupArgs<String, String> {
⋮----
custodian: value_t_or_exit!(matches, "custodian", String),
lockup_epoch: value_t!(matches, "lockup_epoch", u64).ok(),
lockup_date: unix_timestamp_from_rfc3339_datetime(matches, "lockup_date"),
new_custodian: value_t!(matches, "new_custodian", String).ok(),
⋮----
unlock_years: value_t!(matches, "unlock_years", f64).ok(),
⋮----
fn parse_rebase_args(matches: &ArgMatches<'_>) -> RebaseArgs<String, String> {
⋮----
new_base_keypair: value_t_or_exit!(matches, "new_base_keypair", String),
⋮----
fn parse_move_args(matches: &ArgMatches<'_>) -> MoveArgs<String, String> {
⋮----
rebase_args: parse_rebase_args(matches),
authorize_args: parse_authorize_args(matches),
⋮----
pub(crate) fn parse_args<I, T>(args: I) -> Args<String, String>
⋮----
let matches = get_matches(args);
let config_file = matches.value_of("config_file").unwrap().to_string();
let url = matches.value_of("url").map(|x| x.to_string());
let commitment = matches.value_of("commitment").map(|x| x.to_string());
let command = match matches.subcommand() {
("new", Some(matches)) => Command::New(parse_new_args(matches)),
("count", Some(matches)) => Command::Count(parse_count_args(matches)),
("addresses", Some(matches)) => Command::Addresses(parse_query_args(matches)),
("balance", Some(matches)) => Command::Balance(parse_query_args(matches)),
("authorize", Some(matches)) => Command::Authorize(parse_authorize_args(matches)),
("set-lockup", Some(matches)) => Command::SetLockup(parse_set_lockup_args(matches)),
("rebase", Some(matches)) => Command::Rebase(parse_rebase_args(matches)),
("move", Some(matches)) => Command::Move(Box::new(parse_move_args(matches))),
⋮----
eprintln!("{}", matches.usage());
exit(1);

================
File: stake-accounts/src/args.rs
================
pub(crate) struct NewArgs<P, K> {
⋮----
pub(crate) struct CountArgs<P> {
⋮----
pub(crate) struct QueryArgs<P> {
⋮----
pub(crate) struct AuthorizeArgs<P, K> {
⋮----
pub(crate) struct SetLockupArgs<P, K> {
⋮----
pub(crate) struct RebaseArgs<P, K> {
⋮----
pub(crate) struct MoveArgs<P, K> {
⋮----
pub(crate) enum Command<P, K> {
⋮----
pub(crate) struct Args<P, K> {
⋮----
fn resolve_stake_authority(
⋮----
signer_from_path(&matches, key_url, "stake authority", wallet_manager)
⋮----
fn resolve_withdraw_authority(
⋮----
signer_from_path(&matches, key_url, "withdraw authority", wallet_manager)
⋮----
fn resolve_new_stake_authority(
⋮----
pubkey_from_path(&matches, key_url, "new stake authority", wallet_manager)
⋮----
fn resolve_new_withdraw_authority(
⋮----
pubkey_from_path(&matches, key_url, "new withdraw authority", wallet_manager)
⋮----
fn resolve_fee_payer(
⋮----
signer_from_path(&matches, key_url, "fee-payer", wallet_manager)
⋮----
fn resolve_custodian(
⋮----
signer_from_path(&matches, key_url, "custodian", wallet_manager)
⋮----
fn resolve_new_custodian(
⋮----
let pubkey = pubkey_from_path(&matches, key_url, "new custodian", wallet_manager)?;
Some(pubkey)
⋮----
Ok(pubkey)
⋮----
fn resolve_base_pubkey(
⋮----
pubkey_from_path(&matches, key_url, "base pubkey", wallet_manager)
⋮----
fn resolve_new_base_keypair(
⋮----
signer_from_path(&matches, key_url, "new base pubkey", wallet_manager)
⋮----
fn resolve_authorize_args(
⋮----
fee_payer: resolve_fee_payer(wallet_manager, &args.fee_payer)?,
base_pubkey: resolve_base_pubkey(wallet_manager, &args.base_pubkey)?,
stake_authority: resolve_stake_authority(wallet_manager, &args.stake_authority)?,
withdraw_authority: resolve_withdraw_authority(wallet_manager, &args.withdraw_authority)?,
new_stake_authority: resolve_new_stake_authority(
⋮----
new_withdraw_authority: resolve_new_withdraw_authority(
⋮----
Ok(resolved_args)
⋮----
fn resolve_set_lockup_args(
⋮----
custodian: resolve_custodian(wallet_manager, &args.custodian)?,
⋮----
new_custodian: resolve_new_custodian(wallet_manager, &args.new_custodian)?,
⋮----
fn resolve_rebase_args(
⋮----
new_base_keypair: resolve_new_base_keypair(wallet_manager, &args.new_base_keypair)?,
⋮----
pub(crate) fn resolve_command(
⋮----
fee_payer: resolve_fee_payer(&mut wallet_manager, &args.fee_payer)?,
funding_keypair: signer_from_path(
⋮----
base_keypair: signer_from_path(
⋮----
stake_authority: pubkey_from_path(
⋮----
withdraw_authority: pubkey_from_path(
⋮----
Ok(Command::New(resolved_args))
⋮----
base_pubkey: resolve_base_pubkey(&mut wallet_manager, &args.base_pubkey)?,
⋮----
Ok(Command::Count(resolved_args))
⋮----
Ok(Command::Addresses(resolved_args))
⋮----
Ok(Command::Balance(resolved_args))
⋮----
let resolved_args = resolve_authorize_args(&mut wallet_manager, args)?;
Ok(Command::Authorize(resolved_args))
⋮----
let resolved_args = resolve_set_lockup_args(&mut wallet_manager, args)?;
Ok(Command::SetLockup(resolved_args))
⋮----
let resolved_args = resolve_rebase_args(&mut wallet_manager, args)?;
Ok(Command::Rebase(resolved_args))
⋮----
authorize_args: resolve_authorize_args(&mut wallet_manager, &args.authorize_args)?,
rebase_args: resolve_rebase_args(&mut wallet_manager, &args.rebase_args)?,
⋮----
Ok(Command::Move(Box::new(resolved_args)))

================
File: stake-accounts/src/main.rs
================
mod arg_parser;
mod args;
mod stake_accounts;
⋮----
fn get_balance_at(client: &RpcClient, pubkey: &Pubkey, i: usize) -> Result<u64, ClientError> {
⋮----
client.get_balance(&address)
⋮----
fn count_stake_accounts(client: &RpcClient, base_pubkey: &Pubkey) -> Result<usize, ClientError> {
⋮----
while get_balance_at(client, base_pubkey, i)? > 0 {
⋮----
Ok(i)
⋮----
fn get_balances(
⋮----
.into_iter()
.map(|pubkey| client.get_balance(&pubkey).map(|bal| (pubkey, bal)))
.collect()
⋮----
fn get_lockup(client: &RpcClient, address: &Pubkey) -> Result<Lockup, ClientError> {
⋮----
.get_account(address)
.map(|account| stake_accounts::lockup_from(&account).unwrap())
⋮----
fn get_lockups(
⋮----
.map(|pubkey| get_lockup(client, &pubkey).map(|bal| (pubkey, bal)))
⋮----
fn process_new_stake_account(
⋮----
&args.fee_payer.pubkey(),
&args.funding_keypair.pubkey(),
&args.base_keypair.pubkey(),
⋮----
let signers = unique_signers(vec![
⋮----
let signature = send_and_confirm_message(client, message, &signers, args.no_wait)?;
Ok(signature)
⋮----
fn process_authorize_stake_accounts(
⋮----
&args.stake_authority.pubkey(),
&args.withdraw_authority.pubkey(),
⋮----
send_and_confirm_messages(client, messages, &signers, args.no_wait)?;
Ok(())
⋮----
fn process_lockup_stake_accounts(
⋮----
let existing_lockups = get_lockups(client, addresses)?;
⋮----
&args.custodian.pubkey(),
⋮----
if messages.is_empty() {
eprintln!("No work to do");
return Ok(());
⋮----
let signers = unique_signers(vec![&*args.fee_payer, &*args.custodian]);
⋮----
fn process_rebase_stake_accounts(
⋮----
let balances = get_balances(client, addresses)?;
⋮----
&args.new_base_keypair.pubkey(),
⋮----
eprintln!("No accounts found");
⋮----
fn process_move_stake_accounts(
⋮----
&authorize_args.withdraw_authority.pubkey(),
⋮----
fn send_and_confirm_message<S: Signers>(
⋮----
let blockhash = client.get_new_latest_blockhash(&transaction.message().recent_blockhash)?;
transaction.try_sign(signers, blockhash)?;
⋮----
client.send_transaction(&transaction)
⋮----
client.send_and_confirm_transaction_with_spinner(&transaction)
⋮----
fn send_and_confirm_messages<S: Signers>(
⋮----
let mut signatures = vec![];
⋮----
let signature = send_and_confirm_message(client, message, signers, no_wait)?;
signatures.push(signature);
println!("{signature}");
⋮----
Ok(signatures)
⋮----
fn main() -> Result<(), Box<dyn Error>> {
let command_args = parse_args(env::args_os());
let config = Config::load(&command_args.config_file).unwrap_or_default();
let json_rpc_url = command_args.url.unwrap_or(config.json_rpc_url);
⋮----
.as_ref()
.unwrap_or(&config.commitment),
⋮----
match resolve_command(&command_args.command)? {
⋮----
process_new_stake_account(&client, &args)?;
⋮----
let num_accounts = count_stake_accounts(&client, &args.base_pubkey)?;
println!("{num_accounts}");
⋮----
println!("{address:?}");
⋮----
let balances = get_balances(&client, addresses)?;
let lamports: u64 = balances.into_iter().map(|(_, bal)| bal).sum();
let sol = build_balance_message(lamports, false, false);
println!("{sol} SOL");
⋮----
process_authorize_stake_accounts(&client, &args)?;
⋮----
process_lockup_stake_accounts(&client, &args)?;
⋮----
process_rebase_stake_accounts(&client, &args)?;
⋮----
process_move_stake_accounts(&client, &args)?;

================
File: stake-accounts/src/stake_accounts.rs
================
pub(crate) fn derive_stake_account_address(base_pubkey: &Pubkey, i: usize) -> Pubkey {
Pubkey::create_with_seed(base_pubkey, &i.to_string(), &stake::program::id()).unwrap()
⋮----
fn from<T: ReadableAccount + StateMut<StakeStateV2>>(account: &T) -> Option<StakeStateV2> {
account.state().ok()
⋮----
pub(crate) fn lockup_from<T: ReadableAccount + StateMut<StakeStateV2>>(
⋮----
from(account).and_then(|state: StakeStateV2| state.lockup())
⋮----
pub(crate) fn derive_stake_account_addresses(
⋮----
.map(|i| derive_stake_account_address(base_pubkey, i))
.collect()
⋮----
pub(crate) fn new_stake_account(
⋮----
let stake_account_address = derive_stake_account_address(base_pubkey, index);
⋮----
&index.to_string(),
⋮----
Message::new(&instructions, Some(fee_payer_pubkey))
⋮----
fn authorize_stake_accounts_instructions(
⋮----
vec![instruction0, instruction1]
⋮----
fn rebase_stake_account(
⋮----
let new_stake_account_address = derive_stake_account_address(new_base_pubkey, i);
⋮----
&i.to_string(),
⋮----
let message = Message::new(&instructions, Some(fee_payer_pubkey));
Some(message)
⋮----
fn move_stake_account(
⋮----
let authorize_instructions = authorize_stake_accounts_instructions(
⋮----
instructions.extend(authorize_instructions);
⋮----
pub(crate) fn authorize_stake_accounts(
⋮----
let stake_account_addresses = derive_stake_account_addresses(base_pubkey, num_accounts);
⋮----
.iter()
.map(|stake_account_address| {
let instructions = authorize_stake_accounts_instructions(
⋮----
fn extend_lockup(lockup: &LockupArgs, years: f64) -> LockupArgs {
⋮----
let unix_timestamp = lockup.unix_timestamp.map(|x| x + offset);
let epoch = lockup.epoch.map(|_| todo!());
⋮----
fn apply_lockup_changes(lockup: &LockupArgs, existing_lockup: &Lockup) -> LockupArgs {
⋮----
pub(crate) fn lockup_stake_accounts(
⋮----
.enumerate()
.filter_map(|(index, (address, existing_lockup))| {
⋮----
let unlocks = existing_lockups.len() - 1;
⋮----
extend_lockup(lockup, years)
⋮----
let lockup = apply_lockup_changes(&lockup, existing_lockup);
⋮----
let message = Message::new(&[instruction], Some(fee_payer_pubkey));
⋮----
pub(crate) fn rebase_stake_accounts(
⋮----
.filter_map(|(i, (stake_account_address, lamports))| {
rebase_stake_account(
⋮----
pub(crate) fn move_stake_accounts(
⋮----
move_stake_account(
⋮----
mod tests {
⋮----
fn authorized_from(account: &AccountSharedData) -> Option<Authorized> {
from(account).and_then(|state: StakeStateV2| state.authorized())
⋮----
fn create_bank(lamports: u64) -> (Arc<Bank>, Arc<RwLock<BankForks>>, Keypair, u64, u64) {
let (mut genesis_config, mint_keypair) = create_genesis_config(lamports);
⋮----
.unwrap()
.into_iter()
⋮----
genesis_config.add_account(pubkey, account);
⋮----
bank.squash();
⋮----
bank.set_sysvar_for_tests(&EpochRewards::default());
let stake_rent = bank.get_minimum_balance_for_rent_exemption(StakeStateV2::size_of());
let system_rent = bank.get_minimum_balance_for_rent_exemption(0);
⋮----
bank.into(),
⋮----
fn create_account<C: SyncClient>(
⋮----
.transfer_and_confirm(lamports, funding_keypair, &fee_payer_keypair.pubkey())
.unwrap();
⋮----
fn get_account_at<C: SyncClient>(
⋮----
let account_address = derive_stake_account_address(base_pubkey, i);
AccountSharedData::from(client.get_account(&account_address).unwrap().unwrap())
⋮----
fn get_balances<C: SyncClient>(
⋮----
.map(|i| {
let address = derive_stake_account_address(base_pubkey, i);
(address, client.get_balance(&address).unwrap())
⋮----
fn get_lockups<C: SyncClient>(
⋮----
AccountSharedData::from(client.get_account(&address).unwrap().unwrap());
(address, lockup_from(&account).unwrap())
⋮----
fn test_new_derived_stake_account() {
let (bank, _bank_forks, funding_keypair, stake_rent, system_rent) = create_bank(10_000_000);
let funding_pubkey = funding_keypair.pubkey();
⋮----
let fee_payer_keypair = create_account(&bank_client, &funding_keypair, system_rent);
let fee_payer_pubkey = fee_payer_keypair.pubkey();
⋮----
let base_pubkey = base_keypair.pubkey();
⋮----
let message = new_stake_account(
⋮----
.send_and_confirm_message(&signers, message)
⋮----
let account = get_account_at(&bank_client, &base_pubkey, 0);
assert_eq!(account.lamports(), lamports);
let authorized = authorized_from(&account).unwrap();
assert_eq!(authorized.staker, stake_authority_pubkey);
assert_eq!(authorized.withdrawer, withdraw_authority_pubkey);
⋮----
fn test_authorize_stake_accounts() {
⋮----
let stake_authority_pubkey = stake_authority_keypair.pubkey();
⋮----
let withdraw_authority_pubkey = withdraw_authority_keypair.pubkey();
⋮----
let messages = authorize_stake_accounts(
⋮----
assert_eq!(authorized.staker, new_stake_authority_pubkey);
assert_eq!(authorized.withdrawer, new_withdraw_authority_pubkey);
⋮----
fn test_lockup_stake_accounts() {
⋮----
let custodian_pubkey = custodian_keypair.pubkey();
⋮----
let withdrawer_pubkey = withdrawer_keypair.pubkey();
⋮----
let lockups = get_lockups(&bank_client, &base_pubkey, 1);
let messages = lockup_stake_accounts(
⋮----
unix_timestamp: Some(1),
custodian: Some(custodian_pubkey),
⋮----
let lockup = lockup_from(&account).unwrap();
assert_eq!(lockup.custodian, custodian_pubkey);
assert_eq!(lockup.unix_timestamp, 1);
assert_eq!(lockup.epoch, 0);
⋮----
assert_eq!(messages.len(), 0);
⋮----
fn test_rebase_empty_account() {
⋮----
let message = rebase_stake_account(&pubkey, &pubkey, 0, &pubkey, &pubkey, 0);
assert_eq!(message, None);
⋮----
fn test_move_empty_account() {
⋮----
let message = move_stake_account(
⋮----
fn test_rebase_stake_accounts() {
⋮----
let new_base_pubkey = new_base_keypair.pubkey();
let balances = get_balances(&bank_client, &base_pubkey, num_accounts);
let messages = rebase_stake_accounts(
⋮----
assert_eq!(messages.len(), num_accounts);
⋮----
let account = get_account_at(&bank_client, &new_base_pubkey, 0);
⋮----
fn test_move_stake_accounts() {
⋮----
let messages = move_stake_accounts(
⋮----
fn test_extend_lockup() {
⋮----
unix_timestamp: Some(1 + SECONDS_PER_YEAR),
⋮----
assert_eq!(extend_lockup(&lockup, 1.0), expected_lockup);

================
File: stake-accounts/Cargo.toml
================
[package]
name = "solana-stake-accounts"
documentation = "https://docs.rs/solana-stake-accounts"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = ["remote-wallet-hidraw"]
agave-unstable-api = []
remote-wallet-hidraw = ["solana-remote-wallet/linux-static-hidraw"]
remote-wallet-libusb = ["solana-remote-wallet/linux-static-libusb"]

[dependencies]
clap = { workspace = true }
solana-account = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-cli-output = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-genesis-config = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-pubkey = { workspace = true }
solana-remote-wallet = { workspace = true }
solana-rpc-client = { workspace = true, features = ["default"] }
solana-rpc-client-api = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-stake-interface = { workspace = true }
solana-sysvar = { workspace = true }
solana-transaction = { workspace = true }
solana-version = { workspace = true }

[dev-dependencies]
solana-client-traits = { workspace = true }
solana-program-binaries = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }

================
File: start
================
#!/usr/bin/env bash
set -eu

SOLANA_CONFIG_DIR=./config

mkdir -p $SOLANA_CONFIG_DIR
NDEBUG=1 ./multinode-demo/setup.sh
cargo run --release --bin solana-ledger-tool -- -l config/bootstrap-validator/ create-snapshot 0
NDEBUG=1 ./multinode-demo/faucet.sh

================
File: start_multi
================
#!/usr/bin/env bash
set -eu

SOLANA_KEYGEN="cargo run --release --bin solana-keygen --"
SOLANA_CONFIG_DIR=./config

if [[ ! -d $SOLANA_CONFIG_DIR ]]; then
  echo "New Config! Generating Identities"
  mkdir $SOLANA_CONFIG_DIR
  $SOLANA_KEYGEN new --no-passphrase -so "$SOLANA_CONFIG_DIR"/a/identity.json
  $SOLANA_KEYGEN new --no-passphrase -so "$SOLANA_CONFIG_DIR"/a/stake-account.json
  $SOLANA_KEYGEN new --no-passphrase -so "$SOLANA_CONFIG_DIR"/a/vote-account.json

  $SOLANA_KEYGEN new --no-passphrase -so "$SOLANA_CONFIG_DIR"/b/identity.json
  $SOLANA_KEYGEN new --no-passphrase -so "$SOLANA_CONFIG_DIR"/b/stake-account.json
  $SOLANA_KEYGEN new --no-passphrase -so "$SOLANA_CONFIG_DIR"/b/vote-account.json
fi

NDEBUG=1 ./multinode-demo/setup.sh \
  --bootstrap-validator \
  "$SOLANA_CONFIG_DIR"/a/identity.json \
  "$SOLANA_CONFIG_DIR"/a/vote-account.json \
  "$SOLANA_CONFIG_DIR"/a/stake-account.json \
  --bootstrap-validator \
  "$SOLANA_CONFIG_DIR"/b/identity.json \
  "$SOLANA_CONFIG_DIR"/b/vote-account.json \
  "$SOLANA_CONFIG_DIR"/b/stake-account.json

cargo run --bin solana-ledger-tool -- -l config/bootstrap-validator/ create-snapshot 0
NDEBUG=1 ./multinode-demo/faucet.sh

================
File: storage-bigtable/build-proto/src/main.rs
================
fn main() -> Result<(), std::io::Error> {
⋮----
if std::env::var(PROTOC_ENVAR).is_err() {
⋮----
let manifest_dir = std::path::PathBuf::from(env!("CARGO_MANIFEST_DIR"));
let out_dir = manifest_dir.join("../proto");
let googleapis = manifest_dir.join("googleapis");
println!("Google API directory: {}", googleapis.display());
println!("output directory: {}", out_dir.display());
⋮----
.build_client(true)
.build_server(false)
.out_dir(&out_dir)
.compile(
&[googleapis.join("google/bigtable/v2/bigtable.proto")],

================
File: storage-bigtable/build-proto/.gitignore
================
googleapis/
target/

================
File: storage-bigtable/build-proto/build.sh
================
set -ex
cd "$(dirname "$0")"
cargo="$(readlink -f "../../cargo")"
if [[ ! -d googleapis ]]; then
  git clone https://github.com/googleapis/googleapis.git
fi
exec "$cargo" run

================
File: storage-bigtable/build-proto/Cargo.toml
================
[package]
name = "proto"
publish = false
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
tonic-build = { workspace = true }

# windows users should install the protobuf compiler manually and set the PROTOC
# envar to point to the installed binary
[target."cfg(not(windows))".dependencies]
protobuf-src = { workspace = true }

================
File: storage-bigtable/build-proto/README.md
================
Helper project to build Rust bindings for BigTable, to avoid requiring all
Solana developers have protoc installed

================
File: storage-bigtable/proto/google.api.rs
================
pub struct Http {
⋮----
pub struct HttpRule {
⋮----
pub mod http_rule {
⋮----
pub enum Pattern {
⋮----
pub struct CustomHttpPattern {
⋮----
pub enum LaunchStage {
⋮----
impl LaunchStage {
pub fn as_str_name(&self) -> &'static str {
⋮----
/// Creates an enum from field names used in the ProtoBuf definition.
    pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
⋮----
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
⋮----
"LAUNCH_STAGE_UNSPECIFIED" => Some(Self::Unspecified),
"UNIMPLEMENTED" => Some(Self::Unimplemented),
"PRELAUNCH" => Some(Self::Prelaunch),
"EARLY_ACCESS" => Some(Self::EarlyAccess),
"ALPHA" => Some(Self::Alpha),
"BETA" => Some(Self::Beta),
"GA" => Some(Self::Ga),
"DEPRECATED" => Some(Self::Deprecated),
⋮----
/// Required information for every language.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct CommonLanguageSettings {
/// Link to automatically generated reference documentation.  Example:
    /// <https://cloud.google.com/nodejs/docs/reference/asset/latest>
⋮----
/// <https://cloud.google.com/nodejs/docs/reference/asset/latest>
    #[deprecated]
⋮----
/// The destination where API teams want this client library to be published.
    #[prost(enumeration = "ClientLibraryDestination", repeated, tag = "2")]
⋮----
/// Details about how and where to publish client libraries.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ClientLibrarySettings {
/// Version of the API to apply these settings to. This is the full protobuf
    /// package for the API, ending in the version element.
⋮----
/// package for the API, ending in the version element.
    /// Examples: "google.cloud.speech.v1" and "google.spanner.admin.database.v1".
⋮----
/// Examples: "google.cloud.speech.v1" and "google.spanner.admin.database.v1".
    #[prost(string, tag = "1")]
⋮----
/// Launch stage of this version of the API.
    #[prost(enumeration = "LaunchStage", tag = "2")]
⋮----
/// When using transport=rest, the client request will encode enums as
    /// numbers rather than strings.
⋮----
/// numbers rather than strings.
    #[prost(bool, tag = "3")]
⋮----
/// Settings for legacy Java features, supported in the Service YAML.
    #[prost(message, optional, tag = "21")]
⋮----
/// Settings for C++ client libraries.
    #[prost(message, optional, tag = "22")]
⋮----
/// Settings for PHP client libraries.
    #[prost(message, optional, tag = "23")]
⋮----
/// Settings for Python client libraries.
    #[prost(message, optional, tag = "24")]
⋮----
/// Settings for Node client libraries.
    #[prost(message, optional, tag = "25")]
⋮----
/// Settings for .NET client libraries.
    #[prost(message, optional, tag = "26")]
⋮----
/// Settings for Ruby client libraries.
    #[prost(message, optional, tag = "27")]
⋮----
/// Settings for Go client libraries.
    #[prost(message, optional, tag = "28")]
⋮----
/// This message configures the settings for publishing [Google Cloud Client
/// libraries](<https://cloud.google.com/apis/docs/cloud-client-libraries>)
⋮----
/// libraries](<https://cloud.google.com/apis/docs/cloud-client-libraries>)
/// generated from the service config.
⋮----
/// generated from the service config.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Publishing {
/// A list of API method settings, e.g. the behavior for methods that use the
    /// long-running operation pattern.
⋮----
/// long-running operation pattern.
    #[prost(message, repeated, tag = "2")]
⋮----
/// Link to a *public* URI where users can report issues.  Example:
    /// <https://issuetracker.google.com/issues/new?component=190865&template=1161103>
⋮----
/// <https://issuetracker.google.com/issues/new?component=190865&template=1161103>
    #[prost(string, tag = "101")]
⋮----
/// Link to product home page.  Example:
    /// <https://cloud.google.com/asset-inventory/docs/overview>
⋮----
/// <https://cloud.google.com/asset-inventory/docs/overview>
    #[prost(string, tag = "102")]
⋮----
/// Used as a tracking tag when collecting data about the APIs developer
    /// relations artifacts like docs, packages delivered to package managers,
⋮----
/// relations artifacts like docs, packages delivered to package managers,
    /// etc.  Example: "speech".
⋮----
/// etc.  Example: "speech".
    #[prost(string, tag = "103")]
⋮----
/// GitHub label to apply to issues and pull requests opened for this API.
    #[prost(string, tag = "104")]
⋮----
/// GitHub teams to be added to CODEOWNERS in the directory in GitHub
    /// containing source code for the client libraries for this API.
⋮----
/// containing source code for the client libraries for this API.
    #[prost(string, repeated, tag = "105")]
⋮----
/// A prefix used in sample code when demarking regions to be included in
    /// documentation.
⋮----
/// documentation.
    #[prost(string, tag = "106")]
⋮----
/// For whom the client library is being published.
    #[prost(enumeration = "ClientLibraryOrganization", tag = "107")]
⋮----
/// Client library settings.  If the same version string appears multiple
    /// times in this list, then the last one wins.  Settings from earlier
⋮----
/// times in this list, then the last one wins.  Settings from earlier
    /// settings with the same version string are discarded.
⋮----
/// settings with the same version string are discarded.
    #[prost(message, repeated, tag = "109")]
⋮----
/// Optional link to proto reference documentation.  Example:
    /// <https://cloud.google.com/pubsub/lite/docs/reference/rpc>
⋮----
/// <https://cloud.google.com/pubsub/lite/docs/reference/rpc>
    #[prost(string, tag = "110")]
⋮----
/// Settings for Java client libraries.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct JavaSettings {
/// The package name to use in Java. Clobbers the java_package option
    /// set in the protobuf. This should be used **only** by APIs
⋮----
/// set in the protobuf. This should be used **only** by APIs
    /// who have already set the language_settings.java.package_name" field
⋮----
/// who have already set the language_settings.java.package_name" field
    /// in gapic.yaml. API teams should use the protobuf java_package option
⋮----
/// in gapic.yaml. API teams should use the protobuf java_package option
    /// where possible.
⋮----
/// where possible.
    ///
⋮----
///
    /// Example of a YAML configuration::
⋮----
/// Example of a YAML configuration::
    ///
⋮----
///
    ///   publishing:
⋮----
///   publishing:
    ///     java_settings:
⋮----
///     java_settings:
    ///       library_package: com.google.cloud.pubsub.v1
⋮----
///       library_package: com.google.cloud.pubsub.v1
    #[prost(string, tag = "1")]
⋮----
/// Configure the Java class name to use instead of the service's for its
    #[prost(map = "string, string", tag = "2")]
⋮----
pub struct CppSettings {
⋮----
pub struct PhpSettings {
⋮----
pub struct PythonSettings {
⋮----
pub struct NodeSettings {
⋮----
pub struct DotnetSettings {
⋮----
pub struct RubySettings {
⋮----
pub struct GoSettings {
⋮----
pub struct MethodSettings {
⋮----
pub mod method_settings {
⋮----
pub struct LongRunning {
⋮----
pub enum ClientLibraryOrganization {
⋮----
impl ClientLibraryOrganization {
⋮----
"CLIENT_LIBRARY_ORGANIZATION_UNSPECIFIED" => Some(Self::Unspecified),
"CLOUD" => Some(Self::Cloud),
"ADS" => Some(Self::Ads),
"PHOTOS" => Some(Self::Photos),
"STREET_VIEW" => Some(Self::StreetView),
"SHOPPING" => Some(Self::Shopping),
"GEO" => Some(Self::Geo),
"GENERATIVE_AI" => Some(Self::GenerativeAi),
⋮----
/// To where should client libraries be published?
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
⋮----
pub enum ClientLibraryDestination {
/// Client libraries will neither be generated nor published to package
    /// managers.
⋮----
/// managers.
    Unspecified = 0,
/// Generate the client library in a repo under github.com/googleapis,
    /// but don't publish it to package managers.
⋮----
/// but don't publish it to package managers.
    Github = 10,
⋮----
impl ClientLibraryDestination {
⋮----
"CLIENT_LIBRARY_DESTINATION_UNSPECIFIED" => Some(Self::Unspecified),
"GITHUB" => Some(Self::Github),
"PACKAGE_MANAGER" => Some(Self::PackageManager),
⋮----
/// An indicator of the behavior of a given field (for example, that a field
/// is required in requests, or given as output but ignored as input).
⋮----
/// is required in requests, or given as output but ignored as input).
/// This **does not** change the behavior in protocol buffers itself; it only
⋮----
/// This **does not** change the behavior in protocol buffers itself; it only
/// denotes the behavior and may affect how API tooling handles the field.
⋮----
/// denotes the behavior and may affect how API tooling handles the field.
///
⋮----
///
/// Note: This enum **may** receive new values in the future.
⋮----
/// Note: This enum **may** receive new values in the future.
#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]
⋮----
pub enum FieldBehavior {
/// Conventional default for enums. Do not use this.
    Unspecified = 0,
/// Specifically denotes a field as optional.
    /// While all fields in protocol buffers are optional, this may be specified
⋮----
/// While all fields in protocol buffers are optional, this may be specified
    /// for emphasis if appropriate.
⋮----
/// for emphasis if appropriate.
    Optional = 1,
/// Denotes a field as required.
    /// This indicates that the field **must** be provided as part of the request,
⋮----
/// This indicates that the field **must** be provided as part of the request,
    /// and failure to do so will cause an error (usually `INVALID_ARGUMENT`).
⋮----
/// and failure to do so will cause an error (usually `INVALID_ARGUMENT`).
    Required = 2,
/// Denotes a field as output only.
    /// This indicates that the field is provided in responses, but including the
⋮----
/// This indicates that the field is provided in responses, but including the
    /// field in a request does nothing (the server *must* ignore it and
⋮----
/// field in a request does nothing (the server *must* ignore it and
    /// *must not* throw an error as a result of the field's presence).
⋮----
/// *must not* throw an error as a result of the field's presence).
    OutputOnly = 3,
⋮----
impl FieldBehavior {
⋮----
"FIELD_BEHAVIOR_UNSPECIFIED" => Some(Self::Unspecified),
"OPTIONAL" => Some(Self::Optional),
"REQUIRED" => Some(Self::Required),
"OUTPUT_ONLY" => Some(Self::OutputOnly),
"INPUT_ONLY" => Some(Self::InputOnly),
"IMMUTABLE" => Some(Self::Immutable),
"UNORDERED_LIST" => Some(Self::UnorderedList),
"NON_EMPTY_DEFAULT" => Some(Self::NonEmptyDefault),
"IDENTIFIER" => Some(Self::Identifier),
⋮----
/// A simple descriptor of a resource type.
///
⋮----
///
/// ResourceDescriptor annotates a resource message (either by means of a
⋮----
/// ResourceDescriptor annotates a resource message (either by means of a
/// protobuf annotation or use in the service config), and associates the
⋮----
/// protobuf annotation or use in the service config), and associates the
/// resource's schema, the resource type, and the pattern of the resource name.
⋮----
/// resource's schema, the resource type, and the pattern of the resource name.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ResourceDescriptor {
⋮----
pub mod resource_descriptor {
⋮----
pub enum History {
⋮----
impl History {
⋮----
/// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
⋮----
"HISTORY_UNSPECIFIED" => Some(Self::Unspecified),
"ORIGINALLY_SINGLE_PATTERN" => Some(Self::OriginallySinglePattern),
"FUTURE_MULTI_PATTERN" => Some(Self::FutureMultiPattern),
⋮----
/// A flag representing a specific style that a resource claims to conform to.
    #[derive(
⋮----
pub enum Style {
/// The unspecified value. Do not use.
        Unspecified = 0,
/// This resource is intended to be "declarative-friendly".
        ///
⋮----
///
        /// Declarative-friendly resources must be more strictly consistent, and
⋮----
/// Declarative-friendly resources must be more strictly consistent, and
        /// setting this to true communicates to tools that this resource should
⋮----
/// setting this to true communicates to tools that this resource should
        /// adhere to declarative-friendly expectations.
⋮----
/// adhere to declarative-friendly expectations.
        ///
⋮----
///
        /// Note: This is used by the API linter (linter.aip.dev) to enable
⋮----
/// Note: This is used by the API linter (linter.aip.dev) to enable
        /// additional checks.
⋮----
/// additional checks.
        DeclarativeFriendly = 1,
⋮----
impl Style {
/// String value of the enum field names used in the ProtoBuf definition.
        ///
⋮----
///
        /// The values are not transformed in any way and thus are considered stable
⋮----
/// The values are not transformed in any way and thus are considered stable
        /// (if the ProtoBuf definition does not change) and safe for programmatic use.
⋮----
/// (if the ProtoBuf definition does not change) and safe for programmatic use.
        pub fn as_str_name(&self) -> &'static str {
⋮----
"STYLE_UNSPECIFIED" => Some(Self::Unspecified),
"DECLARATIVE_FRIENDLY" => Some(Self::DeclarativeFriendly),
⋮----
pub struct ResourceReference {
⋮----
pub struct RoutingRule {
⋮----
pub struct RoutingParameter {

================
File: storage-bigtable/proto/google.bigtable.v2.rs
================
pub struct Row {
⋮----
pub struct Family {
⋮----
pub struct Column {
⋮----
pub struct Cell {
⋮----
pub struct RowRange {
⋮----
pub mod row_range {
⋮----
pub enum StartKey {
⋮----
pub enum EndKey {
⋮----
pub struct RowSet {
⋮----
pub struct ColumnRange {
⋮----
pub mod column_range {
⋮----
pub enum StartQualifier {
⋮----
pub enum EndQualifier {
⋮----
pub struct TimestampRange {
⋮----
pub struct ValueRange {
⋮----
pub mod value_range {
⋮----
pub enum StartValue {
⋮----
pub enum EndValue {
⋮----
pub struct RowFilter {
⋮----
pub mod row_filter {
⋮----
pub struct Chain {
⋮----
pub struct Interleave {
⋮----
pub struct Condition {
⋮----
pub enum Filter {
⋮----
pub struct Mutation {
⋮----
pub mod mutation {
⋮----
pub struct SetCell {
⋮----
pub struct DeleteFromColumn {
⋮----
pub struct DeleteFromFamily {
⋮----
pub struct DeleteFromRow {}
⋮----
pub enum Mutation {
⋮----
pub struct ReadModifyWriteRule {
⋮----
pub mod read_modify_write_rule {
⋮----
pub enum Rule {
⋮----
pub struct StreamPartition {
⋮----
pub struct StreamContinuationTokens {
⋮----
pub struct StreamContinuationToken {
⋮----
pub struct ReadIterationStats {
⋮----
pub struct RequestLatencyStats {
⋮----
pub struct FullReadStatsView {
⋮----
pub struct RequestStats {
⋮----
pub mod request_stats {
⋮----
pub enum StatsView {
⋮----
pub struct ReadRowsRequest {
⋮----
pub mod read_rows_request {
⋮----
pub enum RequestStatsView {
⋮----
impl RequestStatsView {
pub fn as_str_name(&self) -> &'static str {
⋮----
/// Creates an enum from field names used in the ProtoBuf definition.
        pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
⋮----
pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
⋮----
"REQUEST_STATS_VIEW_UNSPECIFIED" => Some(Self::Unspecified),
"REQUEST_STATS_NONE" => Some(Self::RequestStatsNone),
"REQUEST_STATS_FULL" => Some(Self::RequestStatsFull),
⋮----
/// Response message for Bigtable.ReadRows.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct ReadRowsResponse {
/// A collection of a row's contents as part of the read request.
    #[prost(message, repeated, tag = "1")]
⋮----
pub mod read_rows_response {
⋮----
pub struct CellChunk {
⋮----
pub mod cell_chunk {
⋮----
pub enum RowStatus {
⋮----
pub struct SampleRowKeysRequest {
⋮----
pub struct SampleRowKeysResponse {
⋮----
pub struct MutateRowRequest {
⋮----
pub struct MutateRowResponse {}
⋮----
pub struct MutateRowsRequest {
⋮----
pub mod mutate_rows_request {
⋮----
pub struct Entry {
⋮----
pub struct MutateRowsResponse {
⋮----
pub mod mutate_rows_response {
⋮----
pub struct RateLimitInfo {
⋮----
pub struct CheckAndMutateRowRequest {
⋮----
pub struct CheckAndMutateRowResponse {
⋮----
pub struct PingAndWarmRequest {
⋮----
pub struct PingAndWarmResponse {}
⋮----
pub struct ReadModifyWriteRowRequest {
⋮----
pub struct ReadModifyWriteRowResponse {
⋮----
pub struct GenerateInitialChangeStreamPartitionsRequest {
⋮----
pub struct GenerateInitialChangeStreamPartitionsResponse {
⋮----
pub struct ReadChangeStreamRequest {
⋮----
pub mod read_change_stream_request {
⋮----
pub enum StartFrom {
⋮----
pub struct ReadChangeStreamResponse {
⋮----
pub mod read_change_stream_response {
⋮----
pub struct MutationChunk {
⋮----
pub mod mutation_chunk {
⋮----
pub struct ChunkInfo {
⋮----
pub struct DataChange {
⋮----
pub mod data_change {
⋮----
pub enum Type {
⋮----
impl Type {
⋮----
/// Creates an enum from field names used in the ProtoBuf definition.
            pub fn from_str_name(value: &str) -> ::core::option::Option<Self> {
⋮----
"TYPE_UNSPECIFIED" => Some(Self::Unspecified),
"USER" => Some(Self::User),
"GARBAGE_COLLECTION" => Some(Self::GarbageCollection),
"CONTINUATION" => Some(Self::Continuation),
⋮----
/// A periodic message with information that can be used to checkpoint
    /// the state of a stream.
⋮----
/// the state of a stream.
    #[derive(Clone, PartialEq, ::prost::Message)]
pub struct Heartbeat {
/// A token that can be provided to a subsequent `ReadChangeStream` call
        /// to pick up reading at the current stream position.
⋮----
/// to pick up reading at the current stream position.
        #[prost(message, optional, tag = "1")]
⋮----
/// An estimate of the commit timestamp that is usually lower than or equal
        /// to any timestamp for a record that will be delivered in the future on the
⋮----
/// to any timestamp for a record that will be delivered in the future on the
        /// stream. It is possible that, under particular circumstances that a future
⋮----
/// stream. It is possible that, under particular circumstances that a future
        /// record has a timestamp is lower than a previously seen timestamp. For
⋮----
/// record has a timestamp is lower than a previously seen timestamp. For
        /// an example usage see
⋮----
/// an example usage see
        /// <https://beam.apache.org/documentation/basics/#watermarks>
⋮----
/// <https://beam.apache.org/documentation/basics/#watermarks>
        #[prost(message, optional, tag = "2")]
⋮----
/// A message indicating that the client should stop reading from the stream.
    /// If status is OK and `continuation_tokens` & `new_partitions` are empty, the
⋮----
/// If status is OK and `continuation_tokens` & `new_partitions` are empty, the
    /// stream has finished (for example if there was an `end_time` specified).
⋮----
/// stream has finished (for example if there was an `end_time` specified).
    /// If `continuation_tokens` & `new_partitions` are present, then a change in
⋮----
/// If `continuation_tokens` & `new_partitions` are present, then a change in
    /// partitioning requires the client to open a new stream for each token to
⋮----
/// partitioning requires the client to open a new stream for each token to
    /// resume reading. Example:
⋮----
/// resume reading. Example:
    ///                                   [B,      D) ends
⋮----
///                                   [B,      D) ends
    ///                                        |
⋮----
///                                        |
    ///                                        v
⋮----
///                                        v
    ///                new_partitions:  [A,  C) [C,  E)
⋮----
///                new_partitions:  [A,  C) [C,  E)
    /// continuation_tokens.partitions:  [B,C) [C,D)
⋮----
/// continuation_tokens.partitions:  [B,C) [C,D)
    ///                                   ^---^ ^---^
⋮----
///                                   ^---^ ^---^
    ///                                   ^     ^
⋮----
///                                   ^     ^
    ///                                   |     |
⋮----
///                                   |     |
    ///                                   |     StreamContinuationToken 2
⋮----
///                                   |     StreamContinuationToken 2
    ///                                   |
⋮----
///                                   |
    ///                                   StreamContinuationToken 1
⋮----
///                                   StreamContinuationToken 1
    /// To read the new partition [A,C), supply the continuation tokens whose
⋮----
/// To read the new partition [A,C), supply the continuation tokens whose
    /// ranges cover the new partition, for example ContinuationToken[A,B) &
⋮----
/// ranges cover the new partition, for example ContinuationToken[A,B) &
    /// ContinuationToken[B,C).
⋮----
/// ContinuationToken[B,C).
    #[derive(Clone, PartialEq, ::prost::Message)]
pub struct CloseStream {
/// The status of the stream.
        #[prost(message, optional, tag = "1")]
⋮----
/// If non-empty, contains the information needed to resume reading their
        /// associated partitions.
⋮----
/// associated partitions.
        #[prost(message, repeated, tag = "2")]
⋮----
/// If non-empty, contains the new partitions to start reading from, which
        /// are related to but not necessarily identical to the partitions for the
⋮----
/// are related to but not necessarily identical to the partitions for the
        /// above `continuation_tokens`.
⋮----
/// above `continuation_tokens`.
        #[prost(message, repeated, tag = "3")]
⋮----
/// The data or control message on the stream.
    #[derive(Clone, PartialEq, ::prost::Oneof)]
pub enum StreamRecord {
/// A mutation to the partition.
        #[prost(message, tag = "1")]
⋮----
/// A periodic heartbeat message.
        #[prost(message, tag = "2")]
⋮----
/// An indication that the stream should be closed.
        #[prost(message, tag = "3")]
⋮----
/// Generated client implementations.
pub mod bigtable_client {
⋮----
pub mod bigtable_client {
⋮----
use tonic::codegen::http::Uri;
/// Service for reading from and writing to existing Bigtable tables.
    #[derive(Debug, Clone)]
pub struct BigtableClient<T> {
⋮----
/// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
⋮----
pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
⋮----
let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
Ok(Self::new(conn))
⋮----
pub fn new(inner: T) -> Self {
⋮----
pub fn with_origin(inner: T, origin: Uri) -> Self {
⋮----
pub fn with_interceptor<F>(
⋮----
pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.send_compressed(encoding);
⋮----
pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
self.inner = self.inner.accept_compressed(encoding);
⋮----
pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_decoding_message_size(limit);
⋮----
pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
self.inner = self.inner.max_encoding_message_size(limit);
⋮----
pub async fn read_rows(
⋮----
.ready()
⋮----
.map_err(|e| {
⋮----
format!("Service was not ready: {}", e.into()),
⋮----
let mut req = request.into_request();
req.extensions_mut()
.insert(GrpcMethod::new("google.bigtable.v2.Bigtable", "ReadRows"));
self.inner.server_streaming(req, path, codec).await
⋮----
pub async fn sample_row_keys(
⋮----
.insert(GrpcMethod::new("google.bigtable.v2.Bigtable", "SampleRowKeys"));
⋮----
pub async fn mutate_row(
⋮----
.insert(GrpcMethod::new("google.bigtable.v2.Bigtable", "MutateRow"));
self.inner.unary(req, path, codec).await
⋮----
pub async fn mutate_rows(
⋮----
.insert(GrpcMethod::new("google.bigtable.v2.Bigtable", "MutateRows"));
⋮----
pub async fn check_and_mutate_row(
⋮----
.insert(
⋮----
pub async fn ping_and_warm(
⋮----
.insert(GrpcMethod::new("google.bigtable.v2.Bigtable", "PingAndWarm"));
⋮----
pub async fn read_modify_write_row(
⋮----
pub async fn generate_initial_change_stream_partitions(
⋮----
pub async fn read_change_stream(

================
File: storage-bigtable/proto/google.protobuf.rs
================


================
File: storage-bigtable/proto/google.rpc.rs
================
pub struct Status {

================
File: storage-bigtable/src/access_token.rs
================
pub use goauth::scopes::Scope;
⋮----
fn load_credentials(filepath: Option<String>) -> Result<Credentials, String> {
⋮----
None => std::env::var("GOOGLE_APPLICATION_CREDENTIALS").map_err(|_| {
"GOOGLE_APPLICATION_CREDENTIALS environment variable not found".to_string()
⋮----
.map_err(|err| format!("Failed to read GCP credentials from {path}: {err}"))
⋮----
fn load_stringified_credentials(credential: String) -> Result<Credentials, String> {
Credentials::from_str(&credential).map_err(|err| format!("{err}"))
⋮----
pub struct AccessTokenInner {
⋮----
pub struct AccessToken {
⋮----
type Target = AccessTokenInner;
fn deref(&self) -> &Self::Target {
⋮----
impl AccessToken {
pub async fn new(scope: Scope, credential_type: CredentialType) -> Result<Self, String> {
⋮----
CredentialType::Filepath(fp) => load_credentials(fp)?,
CredentialType::Stringified(s) => load_stringified_credentials(s)?,
⋮----
if let Err(err) = credentials.rsa_key() {
Err(format!("Invalid rsa key: {err}"))
⋮----
Ok(access_token)
⋮----
pub fn project(&self) -> String {
self.credentials.project()
⋮----
async fn get_token(
⋮----
info!("Requesting token for {scope:?} scope");
⋮----
credentials.iss(),
⋮----
credentials.token_uri(),
⋮----
let jwt = Jwt::new(claims, credentials.rsa_key().unwrap(), None);
⋮----
.map_err(|err| format!("Failed to refresh access token: {err}"))?;
info!("Token expires in {} seconds", token.expires_in());
Ok((token, Instant::now()))
⋮----
pub fn refresh(&self) {
let token_r = self.token.read().unwrap();
if token_r.1.elapsed().as_secs() < token_r.0.expires_in() as u64 / 2 {
debug!("Token is not expired yet");
⋮----
drop(token_r);
⋮----
.compare_exchange(false, true, Ordering::Relaxed, Ordering::Relaxed);
if refresh_progress.is_err() {
debug!("Token update is already in progress");
⋮----
let this = self.clone();
⋮----
let mut token_w = this.token.write().unwrap();
⋮----
Err(err) => error!("Failed to fetch new token: {err}"),
⋮----
warn!("Token refresh timeout")
⋮----
this.refresh_active.store(false, Ordering::Relaxed);
info!("Token refreshed");
⋮----
pub fn get(&self) -> String {
⋮----
format!("{} {}", token_r.0.token_type(), token_r.0.access_token())

================
File: storage-bigtable/src/bigtable.rs
================
mod google {
mod rpc {
include!(concat!(
⋮----
pub mod bigtable {
pub mod v2 {
⋮----
pub type RowKey = String;
pub type RowData = Vec<(CellName, CellValue)>;
pub type RowDataSlice<'a> = &'a [(CellName, CellValue)];
pub type CellName = String;
pub type CellValue = Vec<u8>;
pub enum CellData<B, P> {
⋮----
pub enum Error {
⋮----
fn to_backoff_err(err: Error) -> BackoffError<Error> {
⋮----
if status.code() == tonic::Code::NotFound && status.message().starts_with("table") {
⋮----
err.into()
⋮----
fn from(err: std::io::Error) -> Self {
⋮----
fn from(err: tonic::transport::Error) -> Self {
⋮----
fn from(err: tonic::Status) -> Self {
⋮----
pub type Result<T> = std::result::Result<T, Error>;
type InterceptedRequestResult = std::result::Result<Request<()>, Status>;
⋮----
pub struct BigTableConnection {
⋮----
impl BigTableConnection {
pub async fn new(
⋮----
info!("Connecting to bigtable emulator at {endpoint}");
⋮----
.map_err(Error::AccessToken)?;
let table_prefix = format!(
⋮----
.tls_config(
⋮----
.ca_certificate(
root_ca_certificate::load().map_err(Error::Certificate)?,
⋮----
.domain_name("bigtable.googleapis.com"),
⋮----
endpoint.timeout(timeout)
⋮----
http.enforce_http(false);
http.set_nodelay(true);
⋮----
.map_err(|err| Error::InvalidUri(proxy_uri, err.to_string()))?,
⋮----
proxy_connector.set_tls(None);
endpoint.connect_with_connector_lazy(proxy_connector)
⋮----
_ => endpoint.connect_with_connector_lazy(http),
⋮----
Ok(Self {
access_token: Some(access_token),
⋮----
app_profile_id: app_profile_id.to_string(),
⋮----
pub fn new_for_emulator(
⋮----
channel: tonic::transport::Channel::from_shared(format!("http://{endpoint}"))
.map_err(|err| Error::InvalidUri(String::from(endpoint), err.to_string()))?
.connect_lazy(),
table_prefix: format!("projects/emulator/instances/{instance_name}/tables/"),
⋮----
pub fn client(&self) -> BigTable<impl FnMut(Request<()>) -> InterceptedRequestResult + use<>> {
let access_token = self.access_token.clone();
⋮----
self.channel.clone(),
⋮----
match FromStr::from_str(&access_token.get()) {
⋮----
req.metadata_mut()
.insert("authorization", authorization_header);
⋮----
warn!("Failed to set authorization header: {err}");
⋮----
Ok(req)
⋮----
.max_decoding_message_size(self.max_message_size)
.max_encoding_message_size(self.max_message_size);
⋮----
access_token: self.access_token.clone(),
⋮----
table_prefix: self.table_prefix.clone(),
app_profile_id: self.app_profile_id.clone(),
⋮----
pub async fn put_bincode_cells_with_retry<T>(
⋮----
retry(ExponentialBackoff::default(), || async {
let mut client = self.client();
let result = client.put_bincode_cells(table, cells).await;
result.map_err(to_backoff_err)
⋮----
pub async fn delete_rows_with_retry(&self, table: &str, row_keys: &[RowKey]) -> Result<()> {
⋮----
Ok(client.delete_rows(table, row_keys).await?)
⋮----
pub async fn get_bincode_cells_with_retry<T>(
⋮----
Ok(client.get_bincode_cells(table, row_keys).await?)
⋮----
pub async fn put_protobuf_cells_with_retry<T>(
⋮----
let result = client.put_protobuf_cells(table, cells).await;
⋮----
pub struct BigTable<F: FnMut(Request<()>) -> InterceptedRequestResult> {
⋮----
async fn decode_read_rows_response(
⋮----
let mut rows: Vec<(RowKey, RowData)> = vec![];
⋮----
let mut row_data = vec![];
⋮----
let mut cell_value = vec![];
⋮----
while let Some(res) = rrr.message().await? {
⋮----
if Instant::now().duration_since(started) > timeout {
return Err(Error::Timeout);
⋮----
for (i, mut chunk) in res.chunks.into_iter().enumerate() {
trace!("chunk {i}: {chunk:?}");
if !chunk.row_key.is_empty() {
row_key = String::from_utf8(chunk.row_key).ok();
⋮----
row_data.push((cell_name, cell_value));
cell_value = vec![];
⋮----
cell_name = String::from_utf8(qualifier).ok();
⋮----
cell_value.append(&mut chunk.value);
⋮----
if chunk.row_status.is_some() {
⋮----
rows.push((row_key, row_data))
⋮----
row_data = vec![];
⋮----
Ok(rows)
⋮----
fn refresh_access_token(&self) {
⋮----
access_token.refresh();
⋮----
pub async fn get_row_keys(
⋮----
return Ok(vec![]);
⋮----
self.refresh_access_token();
⋮----
.read_rows(
⋮----
table_name: format!("{}{}", self.table_prefix, table_name),
⋮----
rows: Some(RowSet {
row_keys: vec![],
row_ranges: vec![RowRange {
⋮----
filter: Some(RowFilter {
filter: Some(row_filter::Filter::Chain(row_filter::Chain {
filters: vec![
⋮----
.into_inner();
let rows = self.decode_read_rows_response(response).await?;
Ok(rows.into_iter().map(|r| r.0).collect())
⋮----
pub async fn row_key_exists(&mut self, table_name: &str, row_key: RowKey) -> Result<bool> {
⋮----
row_keys: vec![row_key.into_bytes()],
row_ranges: vec![],
⋮----
filter: Some(row_filter::Filter::StripValueTransformer(true)),
⋮----
Ok(!rows.is_empty())
⋮----
pub async fn get_row_data(
⋮----
filter: Some(row_filter::Filter::CellsPerColumnLimitFilter(1)),
⋮----
self.decode_read_rows_response(response).await
⋮----
pub async fn get_multi_row_data(
⋮----
.iter()
.map(|k| k.as_bytes().to_vec())
⋮----
pub async fn get_single_row_data(
⋮----
rows.into_iter()
.next()
.map(|r| r.1)
.ok_or(Error::RowNotFound)
⋮----
async fn delete_rows(&mut self, table_name: &str, row_keys: &[RowKey]) -> Result<()> {
⋮----
let mut entries = vec![];
⋮----
entries.push(mutate_rows_request::Entry {
row_key: row_key.as_bytes().to_vec(),
mutations: vec![Mutation {
⋮----
.mutate_rows(MutateRowsRequest {
⋮----
while let Some(res) = response.message().await? {
⋮----
eprintln!("delete_rows error {}: {}", status.code, status.message);
warn!("delete_rows error {}: {}", status.code, status.message);
return Err(Error::RowDeleteFailed);
⋮----
Ok(())
⋮----
async fn put_row_data(
⋮----
.map(|(column_key, column_value)| Mutation {
mutation: Some(mutation::Mutation::SetCell(mutation::SetCell {
family_name: family_name.to_string(),
column_qualifier: column_key.clone().into_bytes(),
⋮----
value: column_value.to_vec(),
⋮----
.collect();
⋮----
row_key: (*row_key).clone().into_bytes(),
⋮----
eprintln!("put_row_data error {}: {}", status.code, status.message);
warn!("put_row_data error {}: {}", status.code, status.message);
return Err(Error::RowWriteFailed);
⋮----
pub async fn get_bincode_cell<T>(&mut self, table: &str, key: RowKey) -> Result<T>
⋮----
let row_data = self.get_single_row_data(table, key.clone()).await?;
deserialize_bincode_cell_data(&row_data, table, key.to_string())
⋮----
pub async fn get_bincode_cells<T>(
⋮----
Ok(self
.get_multi_row_data(table, keys)
⋮----
.into_iter()
.map(|(key, row_data)| {
let key_str = key.to_string();
⋮----
deserialize_bincode_cell_data(&row_data, table, key_str),
⋮----
.collect())
⋮----
pub async fn get_protobuf_cell<P>(&mut self, table: &str, key: RowKey) -> Result<P>
⋮----
deserialize_protobuf_cell_data(&row_data, table, key.to_string())
⋮----
pub async fn get_protobuf_or_bincode_cell<B, P>(
⋮----
deserialize_protobuf_or_bincode_cell_data(&row_data, table, key)
⋮----
pub async fn get_protobuf_or_bincode_cells<'a, B, P, R>(
⋮----
.get_multi_row_data(
⋮----
row_keys.into_iter().collect::<Vec<RowKey>>().as_slice(),
⋮----
deserialize_protobuf_or_bincode_cell_data(&row_data, table, key_str).unwrap(),
⋮----
pub async fn put_bincode_cells<T>(
⋮----
let mut new_row_data = vec![];
⋮----
let data = compress_best(&bincode::serialize(&data).unwrap())?;
bytes_written += data.len();
new_row_data.push((row_key, vec![("bin".to_string(), data)]));
⋮----
self.put_row_data(table, "x", &new_row_data).await?;
Ok(bytes_written)
⋮----
pub async fn put_protobuf_cells<T>(
⋮----
let mut buf = Vec::with_capacity(data.encoded_len());
data.encode(&mut buf).unwrap();
let data = compress_best(&buf)?;
⋮----
new_row_data.push((row_key, vec![("proto".to_string(), data)]));
⋮----
async fn read_rows(
⋮----
datapoint_info!(datapoint_bigtable, ("read_rows", 1, i64));
⋮----
self.timeout.unwrap_or(Duration::from_secs(30)),
self.client.read_rows(request),
⋮----
.map_err(|_| {
datapoint_error!(datapoint_bigtable, ("timeout", 1, i64));
⋮----
.map_err(Error::from)
⋮----
pub(crate) fn deserialize_protobuf_or_bincode_cell_data<B, P>(
⋮----
match deserialize_protobuf_cell_data(row_data, table, key.to_string()) {
Ok(result) => return Ok(CellData::Protobuf(result)),
⋮----
_ => return Err(err),
⋮----
deserialize_bincode_cell_data(row_data, table, key).map(CellData::Bincode)
⋮----
pub(crate) fn deserialize_protobuf_cell_data<T>(
⋮----
.find(|(name, _)| name == "proto")
.ok_or_else(|| Error::ObjectNotFound(format!("{table}/{key}")))?
⋮----
let data = decompress(value)?;
T::decode(&data[..]).map_err(|err| {
warn!("Failed to deserialize {table}/{key}: {err}");
Error::ObjectCorrupt(format!("{table}/{key}"))
⋮----
pub(crate) fn deserialize_bincode_cell_data<T>(
⋮----
.find(|(name, _)| name == "bin")
⋮----
bincode::deserialize(&data).map_err(|err| {
⋮----
mod tests {
⋮----
fn confirmed_block_into_protobuf(confirmed_block: ConfirmedBlock) -> generated::ConfirmedBlock {
⋮----
transactions: transactions.into_iter().map(|tx| tx.into()).collect(),
rewards: rewards.into_iter().map(|r| r.into()).collect(),
⋮----
.map(|num_partitions| generated::NumPartitions { num_partitions }),
block_time: block_time.map(|timestamp| generated::UnixTimestamp { timestamp }),
block_height: block_height.map(|block_height| generated::BlockHeight { block_height }),
⋮----
fn test_deserialize_protobuf_or_bincode_cell_data() {
⋮----
status: Ok(()),
⋮----
pre_balances: vec![43, 0, 1],
post_balances: vec![0, 42, 1],
inner_instructions: Some(vec![]),
log_messages: Some(vec![]),
pre_token_balances: Some(vec![]),
post_token_balances: Some(vec![]),
rewards: Some(vec![]),
⋮----
return_data: Some(TransactionReturnData::default()),
compute_units_consumed: Some(1234),
cost_units: Some(5678),
⋮----
transactions: vec![with_meta],
⋮----
blockhash: Hash::default().to_string(),
previous_blockhash: Hash::default().to_string(),
rewards: vec![],
⋮----
block_time: Some(1_234_567_890),
block_height: Some(1),
⋮----
let bincode_block = compress_best(
&bincode::serialize::<StoredConfirmedBlock>(&expected_block.clone().into()).unwrap(),
⋮----
.unwrap();
let protobuf_block = confirmed_block_into_protobuf(expected_block.clone());
let mut buf = Vec::with_capacity(protobuf_block.encoded_len());
protobuf_block.encode(&mut buf).unwrap();
let protobuf_block = compress_best(&buf).unwrap();
⋮----
&[("proto".to_string(), protobuf_block.clone())],
⋮----
"".to_string(),
⋮----
assert_eq!(expected_block, protobuf_block.try_into().unwrap());
⋮----
panic!("deserialization should produce CellData::Protobuf");
⋮----
&[("bin".to_string(), bincode_block.clone())],
⋮----
meta.inner_instructions = None; // Legacy bincode implementation does not support inner_instructions
meta.log_messages = None; // Legacy bincode implementation does not support log_messages
meta.pre_token_balances = None; // Legacy bincode implementation does not support token balances
meta.post_token_balances = None; // Legacy bincode implementation does not support token balances
meta.rewards = None; // Legacy bincode implementation does not support rewards
meta.return_data = None; // Legacy bincode implementation does not support return data
meta.compute_units_consumed = None; // Legacy bincode implementation does not support CU consumed
meta.cost_units = None; // Legacy bincode implementation does not support CU
⋮----
assert_eq!(block, bincode_block.into());
⋮----
panic!("deserialization should produce CellData::Bincode");
⋮----
>(&[("proto".to_string(), bincode_block)], "", "".to_string());
assert!(result.is_err());
⋮----
&[("proto".to_string(), vec![1, 2, 3, 4])],
⋮----
>(&[("bin".to_string(), protobuf_block)], "", "".to_string());
⋮----
>(&[("bin".to_string(), vec![1, 2, 3, 4])], "", "".to_string());

================
File: storage-bigtable/src/compression.rs
================
pub enum CompressionMethod {
⋮----
fn decompress_reader<'a, R: Read + 'a>(
⋮----
Ok(decompress_reader)
⋮----
pub fn decompress(data: &[u8]) -> Result<Vec<u8>, io::Error> {
let method_size = bincode::serialized_size(&CompressionMethod::NoCompression).unwrap();
if (data.len() as u64) < method_size {
return Err(io::Error::other(format!(
⋮----
.map_err(|err| io::Error::other(format!("method deserialize failed: {err}")))?;
let mut reader = decompress_reader(method, &data[method_size as usize..])?;
let mut uncompressed_data = vec![];
reader.read_to_end(&mut uncompressed_data)?;
Ok(uncompressed_data)
⋮----
pub fn compress(method: CompressionMethod, data: &[u8]) -> Result<Vec<u8>, io::Error> {
let mut compressed_data = bincode::serialize(&method).unwrap();
compressed_data.extend(match method {
⋮----
e.write_all(data)?;
e.finish()?
⋮----
let mut e = zstd::stream::write::Encoder::new(Vec::new(), 0).unwrap();
⋮----
CompressionMethod::NoCompression => data.to_vec(),
⋮----
Ok(compressed_data)
⋮----
pub fn compress_best(data: &[u8]) -> Result<Vec<u8>, io::Error> {
let mut candidates = vec![];
⋮----
candidates.push(compress(method, data)?);
⋮----
Ok(candidates
.into_iter()
.min_by(|a, b| a.len().cmp(&b.len()))
.unwrap())
⋮----
mod test {
⋮----
fn test_compress_uncompress() {
let data = vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9];
assert_eq!(
⋮----
fn test_compress() {
let data = vec![0; 256];
assert!(compress_best(&data).expect("compress_best").len() < data.len());

================
File: storage-bigtable/src/lib.rs
================
extern crate solana_metrics;
mod access_token;
mod bigtable;
mod compression;
mod root_ca_certificate;
⋮----
pub enum Error {
⋮----
fn from(err: bigtable::Error) -> Self {
⋮----
fn from(err: std::io::Error) -> Self {
⋮----
pub type Result<T> = std::result::Result<T, Error>;
fn slot_to_key(slot: Slot) -> String {
format!("{slot:016x}")
⋮----
fn slot_to_blocks_key(slot: Slot) -> String {
slot_to_key(slot)
⋮----
fn slot_to_entries_key(slot: Slot) -> String {
⋮----
fn slot_to_tx_by_addr_key(slot: Slot) -> String {
slot_to_key(!slot)
⋮----
fn key_to_slot(key: &str) -> Option<Slot> {
⋮----
Ok(slot) => Some(slot),
⋮----
warn!("Failed to parse object key as a slot: {key}: {err}");
⋮----
struct StoredConfirmedBlock {
⋮----
fn from(confirmed_block: ConfirmedBlock) -> Self {
⋮----
transactions: transactions.into_iter().map(|tx| tx.into()).collect(),
rewards: rewards.into_iter().map(|reward| reward.into()).collect(),
⋮----
fn from(confirmed_block: StoredConfirmedBlock) -> Self {
⋮----
struct StoredConfirmedBlockTransaction {
⋮----
fn from(value: TransactionWithStatusMeta) -> Self {
⋮----
meta: Some(meta.into()),
⋮----
fn from(tx_with_meta: StoredConfirmedBlockTransaction) -> Self {
⋮----
.into_legacy_transaction()
.expect("versioned transactions always have meta"),
⋮----
meta: meta.into(),
⋮----
struct StoredConfirmedBlockTransactionStatusMeta {
⋮----
fn from(value: StoredConfirmedBlockTransactionStatusMeta) -> Self {
⋮----
None => Ok(()),
Some(err) => Err(err.clone()),
⋮----
fn from(value: TransactionStatusMeta) -> Self {
⋮----
err: status.err(),
⋮----
type StoredConfirmedBlockRewards = Vec<StoredConfirmedBlockReward>;
⋮----
struct StoredConfirmedBlockReward {
⋮----
fn from(value: StoredConfirmedBlockReward) -> Self {
⋮----
fn from(value: Reward) -> Self {
⋮----
struct TransactionInfo {
⋮----
struct UploadedTransaction {
⋮----
fn from(transaction_info: TransactionInfo) -> Self {
⋮----
confirmation_status: Some(TransactionConfirmationStatus::Finalized),
⋮----
struct LegacyTransactionByAddrInfo {
⋮----
fn from(legacy: LegacyTransactionByAddrInfo) -> Self {
⋮----
pub enum CredentialType {
⋮----
pub struct LedgerStorageConfig {
⋮----
impl Default for LedgerStorageConfig {
fn default() -> Self {
⋮----
instance_name: DEFAULT_INSTANCE_NAME.to_string(),
app_profile_id: DEFAULT_APP_PROFILE_ID.to_string(),
⋮----
struct LedgerStorageStats {
⋮----
impl LedgerStorageStats {
fn increment_num_queries(&self) {
self.num_queries.fetch_add(1, Ordering::Relaxed);
self.maybe_report();
⋮----
fn maybe_report(&self) {
if self.last_report.should_update(METRICS_REPORT_INTERVAL_MS) {
datapoint_debug!(
⋮----
pub struct LedgerStorage {
⋮----
impl LedgerStorage {
pub async fn new(
⋮----
pub fn new_for_emulator(
⋮----
Ok(Self {
⋮----
pub async fn new_with_config(config: LedgerStorageConfig) -> Result<Self> {
⋮----
instance_name.as_str(),
app_profile_id.as_str(),
⋮----
Ok(Self { stats, connection })
⋮----
pub async fn new_with_stringified_credential(credential: String) -> Result<Self> {
⋮----
pub async fn get_first_available_block(&self) -> Result<Option<Slot>> {
trace!("LedgerStorage::get_first_available_block request received");
self.stats.increment_num_queries();
let mut bigtable = self.connection.client();
let blocks = bigtable.get_row_keys("blocks", None, None, 1).await?;
if blocks.is_empty() {
return Ok(None);
⋮----
Ok(key_to_slot(&blocks[0]))
⋮----
pub async fn get_confirmed_blocks(&self, start_slot: Slot, limit: usize) -> Result<Vec<Slot>> {
trace!("LedgerStorage::get_confirmed_blocks request received: {start_slot:?} {limit:?}");
⋮----
.get_row_keys(
⋮----
Some(slot_to_blocks_key(start_slot)),
⋮----
Ok(blocks.into_iter().filter_map(|s| key_to_slot(&s)).collect())
⋮----
pub async fn get_confirmed_blocks_with_data(
⋮----
trace!("LedgerStorage::get_confirmed_blocks_with_data request received: {slots:?}");
⋮----
let row_keys = slots.into_iter().map(slot_to_blocks_key);
⋮----
.get_protobuf_or_bincode_cells("blocks", row_keys)
⋮----
.filter_map(
⋮----
bigtable::CellData::Bincode(block) => block.into(),
bigtable::CellData::Protobuf(block) => block.try_into().ok()?,
⋮----
Some((key_to_slot(&row_key).unwrap(), block))
⋮----
Ok(data)
⋮----
pub async fn get_confirmed_block(&self, slot: Slot) -> Result<ConfirmedBlock> {
trace!("LedgerStorage::get_confirmed_block request received: {slot:?}");
⋮----
slot_to_blocks_key(slot),
⋮----
.map_err(|err| match err {
⋮----
_ => err.into(),
⋮----
Ok(match block_cell_data {
⋮----
bigtable::CellData::Protobuf(block) => block.try_into().map_err(|_err| {
bigtable::Error::ObjectCorrupt(format!("blocks/{}", slot_to_blocks_key(slot)))
⋮----
pub async fn confirmed_block_exists(&self, slot: Slot) -> Result<bool> {
trace!("LedgerStorage::confirmed_block_exists request received: {slot:?}");
⋮----
.row_key_exists("blocks", slot_to_blocks_key(slot))
⋮----
Ok(block_exists)
⋮----
pub async fn get_entries(
⋮----
trace!("LedgerStorage::get_block_entries request received: {slot:?}");
⋮----
.get_protobuf_cell::<entries::Entries>("entries", slot_to_entries_key(slot))
⋮----
let entries = entry_cell_data.entries.into_iter().map(Into::into);
Ok(entries)
⋮----
pub async fn get_signature_status(&self, signature: &Signature) -> Result<TransactionStatus> {
trace!("LedgerStorage::get_signature_status request received: {signature:?}");
⋮----
.get_bincode_cell::<TransactionInfo>("tx", signature.to_string())
⋮----
Ok(transaction_info.into())
⋮----
pub async fn get_confirmed_transactions(
⋮----
trace!("LedgerStorage::get_confirmed_transactions request received: {signatures:?}");
⋮----
let keys = signatures.iter().map(|s| s.to_string()).collect::<Vec<_>>();
⋮----
order.push((slot, index, signature));
slots.insert(slot);
⋮----
.get_confirmed_blocks_with_data(slots)
⋮----
Ok(order
.into_iter()
.filter_map(|(slot, index, signature)| {
blocks.get(&slot).and_then(|block| {
⋮----
.get(index as usize)
.and_then(|tx_with_meta| {
if tx_with_meta.transaction_signature().to_string() != *signature {
warn!(
⋮----
Some(ConfirmedTransactionWithStatusMeta {
⋮----
tx_with_meta: tx_with_meta.clone(),
⋮----
pub async fn get_confirmed_transaction(
⋮----
trace!("LedgerStorage::get_confirmed_transaction request received: {signature:?}");
⋮----
.get_bincode_cell("tx", signature.to_string())
⋮----
let block = self.get_confirmed_block(slot).await?;
match block.transactions.into_iter().nth(index as usize) {
⋮----
warn!("Transaction info for {signature} is corrupt");
Ok(None)
⋮----
if tx_with_meta.transaction_signature() != signature {
warn!("Transaction info or confirmed block for {signature} is corrupt");
⋮----
Ok(Some(ConfirmedTransactionWithStatusMeta {
⋮----
pub async fn get_confirmed_signatures_for_address(
⋮----
trace!("LedgerStorage::get_confirmed_signatures_for_address request received: {address:?}");
⋮----
let address_prefix = format!("{address}/");
⋮----
.get_bincode_cell("tx", before_signature.to_string())
⋮----
.get_bincode_cell("tx", until_signature.to_string())
⋮----
let mut infos = vec![];
⋮----
format!("{}{}", address_prefix, slot_to_tx_by_addr_key(first_slot)),
⋮----
.map(|cell_data| {
⋮----
bigtable::CellData::Bincode(tx_by_addr) => tx_by_addr.len(),
bigtable::CellData::Protobuf(tx_by_addr) => tx_by_addr.tx_by_addrs.len(),
⋮----
.unwrap_or(0);
⋮----
.get_row_data(
⋮----
Some(format!(
⋮----
let slot = !key_to_slot(&row_key[address_prefix.len()..]).ok_or_else(|| {
bigtable::Error::ObjectCorrupt(format!(
⋮----
>(&data, "tx-by-addr", row_key.clone())?;
⋮----
tx_by_addr.into_iter().map(|legacy| legacy.into()).collect()
⋮----
tx_by_addr.try_into().map_err(|error| {
⋮----
cell_data.reverse();
for tx_by_addr_info in cell_data.into_iter() {
// Filter out records before `before_transaction_index`
⋮----
// Filter out records after `until_transaction_index`
⋮----
infos.push((
⋮----
// Respect limit
if infos.len() >= limit {
⋮----
Ok(infos)
⋮----
pub async fn upload_confirmed_block(
⋮----
trace!("LedgerStorage::upload_confirmed_block request received: {slot:?}");
self.upload_confirmed_block_with_entries(
⋮----
entries: vec![],
⋮----
pub async fn upload_confirmed_block_with_entries(
⋮----
trace!("LedgerStorage::upload_confirmed_block_with_entries request received: {slot:?}");
⋮----
let mut tx_cells = Vec::with_capacity(confirmed_block.transactions.len());
for (index, transaction_with_meta) in confirmed_block.transactions.iter().enumerate() {
⋮----
let err = meta.status.clone().err();
⋮----
let memo = extract_and_fmt_memos(transaction_with_meta);
for address in transaction_with_meta.account_keys().iter() {
if !reserved_account_keys.is_reserved(address) {
⋮----
.entry(address)
.or_default()
.push(TransactionByAddrInfo {
⋮----
err: err.clone(),
⋮----
memo: memo.clone(),
⋮----
tx_cells.push((
signature.to_string(),
⋮----
.map(|(address, transaction_info_by_addr)| {
⋮----
format!("{}/{}", address, slot_to_tx_by_addr_key(slot)),
⋮----
.map(|by_addr| by_addr.into())
.collect(),
⋮----
.collect();
let num_entries = entries.len();
⋮----
slot_to_entries_key(slot),
⋮----
entries: entries.into_iter().enumerate().map(Into::into).collect(),
⋮----
let mut tasks = vec![];
if !tx_cells.is_empty() {
let conn = self.connection.clone();
tasks.push(tokio::spawn(async move {
⋮----
if !tx_by_addr_cells.is_empty() {
⋮----
if maybe_first_err.is_none() {
maybe_first_err = Some(Error::TokioJoinError(err));
⋮----
maybe_first_err = Some(Error::BigTableError(err));
⋮----
return Err(err);
⋮----
let num_transactions = confirmed_block.transactions.len();
let blocks_cells = [(slot_to_blocks_key(slot), confirmed_block.into())];
⋮----
datapoint_info!(
⋮----
Ok(())
⋮----
pub async fn delete_confirmed_block(&self, slot: Slot, dry_run: bool) -> Result<()> {
⋮----
let confirmed_block = self.get_confirmed_block(slot).await?;
⋮----
for address in transaction.message.account_keys.iter() {
addresses.insert(address);
⋮----
expected_tx_infos.insert(
⋮----
for address in tx_with_meta.account_keys().iter() {
⋮----
.map(|address| format!("{}/{}", address, slot_to_tx_by_addr_key(slot)))
⋮----
let tx_deletion_rows = if !expected_tx_infos.is_empty() {
let signatures = expected_tx_infos.keys().cloned().collect::<Vec<_>>();
⋮----
.map(|(signature, tx_info_res)| (signature, tx_info_res.map(Into::into)))
⋮----
let mut deletion_rows = Vec::with_capacity(expected_tx_infos.len());
⋮----
match fetched_tx_infos.get(&signature) {
⋮----
deletion_rows.push(signature);
⋮----
warn!("skipped tx row {signature} because it was not found");
⋮----
vec![]
⋮----
.client()
.row_key_exists("entries", slot_to_entries_key(slot))
⋮----
.is_ok_and(|x| x);
⋮----
if !address_slot_rows.is_empty() {
⋮----
.delete_rows_with_retry("tx-by-addr", &address_slot_rows)
⋮----
if !tx_deletion_rows.is_empty() {
⋮----
.delete_rows_with_retry("tx", &tx_deletion_rows)
⋮----
.delete_rows_with_retry("entries", &[slot_to_entries_key(slot)])
⋮----
.delete_rows_with_retry("blocks", &[slot_to_blocks_key(slot)])
⋮----
info!(
⋮----
mod test {
⋮----
fn test_slot_to_key() {
assert_eq!(slot_to_key(0), "0000000000000000");
assert_eq!(slot_to_key(!0), "ffffffffffffffff");

================
File: storage-bigtable/src/pki-goog-roots.pem
================
# Operating CA: DigiCert
# Issuer: CN=Baltimore CyberTrust Root O=Baltimore OU=CyberTrust
# Subject: CN=Baltimore CyberTrust Root O=Baltimore OU=CyberTrust
# Label: "Baltimore CyberTrust Root"
# Serial: 33554617
# MD5 Fingerprint: ac:b6:94:a5:9c:17:e0:d7:91:52:9b:b1:97:06:a6:e4
# SHA1 Fingerprint: d4:de:20:d0:5e:66:fc:53:fe:1a:50:88:2c:78:db:28:52:ca:e4:74
# SHA256 Fingerprint: 16:af:57:a9:f6:76:b0:ab:12:60:95:aa:5e:ba:de:f2:2a:b3:11:19:d6:44:ac:95:cd:4b:93:db:f3:f2:6a:eb
-----BEGIN CERTIFICATE-----
MIIDdzCCAl+gAwIBAgIEAgAAuTANBgkqhkiG9w0BAQUFADBaMQswCQYDVQQGEwJJ
RTESMBAGA1UEChMJQmFsdGltb3JlMRMwEQYDVQQLEwpDeWJlclRydXN0MSIwIAYD
VQQDExlCYWx0aW1vcmUgQ3liZXJUcnVzdCBSb290MB4XDTAwMDUxMjE4NDYwMFoX
DTI1MDUxMjIzNTkwMFowWjELMAkGA1UEBhMCSUUxEjAQBgNVBAoTCUJhbHRpbW9y
ZTETMBEGA1UECxMKQ3liZXJUcnVzdDEiMCAGA1UEAxMZQmFsdGltb3JlIEN5YmVy
VHJ1c3QgUm9vdDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKMEuyKr
mD1X6CZymrV51Cni4eiVgLGw41uOKymaZN+hXe2wCQVt2yguzmKiYv60iNoS6zjr
IZ3AQSsBUnuId9Mcj8e6uYi1agnnc+gRQKfRzMpijS3ljwumUNKoUMMo6vWrJYeK
mpYcqWe4PwzV9/lSEy/CG9VwcPCPwBLKBsua4dnKM3p31vjsufFoREJIE9LAwqSu
XmD+tqYF/LTdB1kC1FkYmGP1pWPgkAx9XbIGevOF6uvUA65ehD5f/xXtabz5OTZy
dc93Uk3zyZAsuT3lySNTPx8kmCFcB5kpvcY67Oduhjprl3RjM71oGDHweI12v/ye
jl0qhqdNkNwnGjkCAwEAAaNFMEMwHQYDVR0OBBYEFOWdWTCCR1jMrPoIVDaGezq1
BE3wMBIGA1UdEwEB/wQIMAYBAf8CAQMwDgYDVR0PAQH/BAQDAgEGMA0GCSqGSIb3
DQEBBQUAA4IBAQCFDF2O5G9RaEIFoN27TyclhAO992T9Ldcw46QQF+vaKSm2eT92
9hkTI7gQCvlYpNRhcL0EYWoSihfVCr3FvDB81ukMJY2GQE/szKN+OMY3EU/t3Wgx
jkzSswF07r51XgdIGn9w/xZchMB5hbgF/X++ZRGjD8ACtPhSNzkE1akxehi/oCr0
Epn3o0WC4zxe9Z2etciefC7IpJ5OCBRLbf1wbWsaY71k5h+3zvDyny67G7fyUIhz
ksLi4xaNmjICq44Y3ekQEe5+NauQrz4wlHrQMz2nZQ/1/I6eYs9HRCwBXbsdtTLS
R9I4LtD+gdwyah617jzV/OeBHRnDJELqYzmp
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=Cybertrust Global Root O=Cybertrust, Inc
# Subject: CN=Cybertrust Global Root O=Cybertrust, Inc
# Label: "Cybertrust Global Root"
# Serial: 4835703278459682877484360
# MD5 Fingerprint: 72:e4:4a:87:e3:69:40:80:77:ea:bc:e3:f4:ff:f0:e1
# SHA1 Fingerprint: 5f:43:e5:b1:bf:f8:78:8c:ac:1c:c7:ca:4a:9a:c6:22:2b:cc:34:c6
# SHA256 Fingerprint: 96:0a:df:00:63:e9:63:56:75:0c:29:65:dd:0a:08:67:da:0b:9c:bd:6e:77:71:4a:ea:fb:23:49:ab:39:3d:a3
-----BEGIN CERTIFICATE-----
MIIDoTCCAomgAwIBAgILBAAAAAABD4WqLUgwDQYJKoZIhvcNAQEFBQAwOzEYMBYG
A1UEChMPQ3liZXJ0cnVzdCwgSW5jMR8wHQYDVQQDExZDeWJlcnRydXN0IEdsb2Jh
bCBSb290MB4XDTA2MTIxNTA4MDAwMFoXDTIxMTIxNTA4MDAwMFowOzEYMBYGA1UE
ChMPQ3liZXJ0cnVzdCwgSW5jMR8wHQYDVQQDExZDeWJlcnRydXN0IEdsb2JhbCBS
b290MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA+Mi8vRRQZhP/8NN5
7CPytxrHjoXxEnOmGaoQ25yiZXRadz5RfVb23CO21O1fWLE3TdVJDm71aofW0ozS
J8bi/zafmGWgE07GKmSb1ZASzxQG9Dvj1Ci+6A74q05IlG2OlTEQXO2iLb3VOm2y
HLtgwEZLAfVJrn5GitB0jaEMAs7u/OePuGtm839EAL9mJRQr3RAwHQeWP032a7iP
t3sMpTjr3kfb1V05/Iin89cqdPHoWqI7n1C6poxFNcJQZZXcY4Lv3b93TZxiyWNz
FtApD0mpSPCzqrdsxacwOUBdrsTiXSZT8M4cIwhhqJQZugRiQOwfOHB3EgZxpzAY
XSUnpQIDAQABo4GlMIGiMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTADAQH/
MB0GA1UdDgQWBBS2CHsNesysIEyGVjJez6tuhS1wVzA/BgNVHR8EODA2MDSgMqAw
hi5odHRwOi8vd3d3Mi5wdWJsaWMtdHJ1c3QuY29tL2NybC9jdC9jdHJvb3QuY3Js
MB8GA1UdIwQYMBaAFLYIew16zKwgTIZWMl7Pq26FLXBXMA0GCSqGSIb3DQEBBQUA
A4IBAQBW7wojoFROlZfJ+InaRcHUowAl9B8Tq7ejhVhpwjCt2BWKLePJzYFa+HMj
Wqd8BfP9IjsO0QbE2zZMcwSO5bAi5MXzLqXZI+O4Tkogp24CJJ8iYGd7ix1yCcUx
XOl5n4BHPa2hCwcUPUf/A2kaDAtE52Mlp3+yybh2hO0j9n0Hq0V+09+zv+mKts2o
omcrUtW3ZfA5TGOgkXmTUg9U3YO7n9GPp1Nzw8v/MOx8BLjYRB+TX3EJIrduPuoc
A06dGiBh+4E37F78CkWr1+cXVdCg6mCbpvbjjFspwgZgFJ0tl0ypkxWdYcQBX0jW
WL1WMRJOEcgh4LMRkWXbtKaIOM5V
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert Assured ID Root CA O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert Assured ID Root CA O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert Assured ID Root CA"
# Serial: 17154717934120587862167794914071425081
# MD5 Fingerprint: 87:ce:0b:7b:2a:0e:49:00:e1:58:71:9b:37:a8:93:72
# SHA1 Fingerprint: 05:63:b8:63:0d:62:d7:5a:bb:c8:ab:1e:4b:df:b5:a8:99:b2:4d:43
# SHA256 Fingerprint: 3e:90:99:b5:01:5e:8f:48:6c:00:bc:ea:9d:11:1e:e7:21:fa:ba:35:5a:89:bc:f1:df:69:56:1e:3d:c6:32:5c
-----BEGIN CERTIFICATE-----
MIIDtzCCAp+gAwIBAgIQDOfg5RfYRv6P5WD8G/AwOTANBgkqhkiG9w0BAQUFADBl
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
d3cuZGlnaWNlcnQuY29tMSQwIgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJv
b3QgQ0EwHhcNMDYxMTEwMDAwMDAwWhcNMzExMTEwMDAwMDAwWjBlMQswCQYDVQQG
EwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNl
cnQuY29tMSQwIgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJvb3QgQ0EwggEi
MA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCtDhXO5EOAXLGH87dg+XESpa7c
JpSIqvTO9SA5KFhgDPiA2qkVlTJhPLWxKISKityfCgyDF3qPkKyK53lTXDGEKvYP
mDI2dsze3Tyoou9q+yHyUmHfnyDXH+Kx2f4YZNISW1/5WBg1vEfNoTb5a3/UsDg+
wRvDjDPZ2C8Y/igPs6eD1sNuRMBhNZYW/lmci3Zt1/GiSw0r/wty2p5g0I6QNcZ4
VYcgoc/lbQrISXwxmDNsIumH0DJaoroTghHtORedmTpyoeb6pNnVFzF1roV9Iq4/
AUaG9ih5yLHa5FcXxH4cDrC0kqZWs72yl+2qp/C3xag/lRbQ/6GW6whfGHdPAgMB
AAGjYzBhMA4GA1UdDwEB/wQEAwIBhjAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQW
BBRF66Kv9JLLgjEtUYunpyGd823IDzAfBgNVHSMEGDAWgBRF66Kv9JLLgjEtUYun
pyGd823IDzANBgkqhkiG9w0BAQUFAAOCAQEAog683+Lt8ONyc3pklL/3cmbYMuRC
dWKuh+vy1dneVrOfzM4UKLkNl2BcEkxY5NM9g0lFWJc1aRqoR+pWxnmrEthngYTf
fwk8lOa4JiwgvT2zKIn3X/8i4peEH+ll74fg38FnSbNd67IJKusm7Xi+fT8r87cm
NW1fiQG2SVufAQWbqz0lwcy2f8Lxb4bG+mRo64EtlOtCt/qMHt1i8b5QZ7dsvfPx
H2sMNgcWfzd8qVttevESRmCD1ycEvkvOl77DZypoEd+A5wwzZr8TDRRu838fYxAe
+o0bJW1sj6W3YQGx0qMmoRBxna3iw/nDmVG3KwcIzi7mULKn+gpFL6Lw8g==
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert Assured ID Root G2 O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert Assured ID Root G2 O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert Assured ID Root G2"
# Serial: 15385348160840213938643033620894905419
# MD5 Fingerprint: 92:38:b9:f8:63:24:82:65:2c:57:33:e6:fe:81:8f:9d
# SHA1 Fingerprint: a1:4b:48:d9:43:ee:0a:0e:40:90:4f:3c:e0:a4:c0:91:93:51:5d:3f
# SHA256 Fingerprint: 7d:05:eb:b6:82:33:9f:8c:94:51:ee:09:4e:eb:fe:fa:79:53:a1:14:ed:b2:f4:49:49:45:2f:ab:7d:2f:c1:85
-----BEGIN CERTIFICATE-----
MIIDljCCAn6gAwIBAgIQC5McOtY5Z+pnI7/Dr5r0SzANBgkqhkiG9w0BAQsFADBl
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
d3cuZGlnaWNlcnQuY29tMSQwIgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJv
b3QgRzIwHhcNMTMwODAxMTIwMDAwWhcNMzgwMTE1MTIwMDAwWjBlMQswCQYDVQQG
EwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNl
cnQuY29tMSQwIgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJvb3QgRzIwggEi
MA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDZ5ygvUj82ckmIkzTz+GoeMVSA
n61UQbVH35ao1K+ALbkKz3X9iaV9JPrjIgwrvJUXCzO/GU1BBpAAvQxNEP4Htecc
biJVMWWXvdMX0h5i89vqbFCMP4QMls+3ywPgym2hFEwbid3tALBSfK+RbLE4E9Hp
EgjAALAcKxHad3A2m67OeYfcgnDmCXRwVWmvo2ifv922ebPynXApVfSr/5Vh88lA
bx3RvpO704gqu52/clpWcTs/1PPRCv4o76Pu2ZmvA9OPYLfykqGxvYmJHzDNw6Yu
YjOuFgJ3RFrngQo8p0Quebg/BLxcoIfhG69Rjs3sLPr4/m3wOnyqi+RnlTGNAgMB
AAGjQjBAMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgGGMB0GA1UdDgQW
BBTOw0q5mVXyuNtgv6l+vVa1lzan1jANBgkqhkiG9w0BAQsFAAOCAQEAyqVVjOPI
QW5pJ6d1Ee88hjZv0p3GeDgdaZaikmkuOGybfQTUiaWxMTeKySHMq2zNixya1r9I
0jJmwYrA8y8678Dj1JGG0VDjA9tzd29KOVPt3ibHtX2vK0LRdWLjSisCx1BL4Gni
lmwORGYQRI+tBev4eaymG+g3NJ1TyWGqolKvSnAWhsI6yLETcDbYz+70CjTVW0z9
B5yiutkBclzzTcHdDrEcDcRjvq30FPuJ7KJBDkzMyFdA0G4Dqs0MjomZmWzwPDCv
ON9vvKO+KSAnq3T/EyJ43pdSVR6DtVQgA+6uwE9W3jfMw3+qBCe703e4YtsXfJwo
IhNzbM8m9Yop5w==
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert Assured ID Root G3 O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert Assured ID Root G3 O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert Assured ID Root G3"
# Serial: 15459312981008553731928384953135426796
# MD5 Fingerprint: 7c:7f:65:31:0c:81:df:8d:ba:3e:99:e2:5c:ad:6e:fb
# SHA1 Fingerprint: f5:17:a2:4f:9a:48:c6:c9:f8:a2:00:26:9f:dc:0f:48:2c:ab:30:89
# SHA256 Fingerprint: 7e:37:cb:8b:4c:47:09:0c:ab:36:55:1b:a6:f4:5d:b8:40:68:0f:ba:16:6a:95:2d:b1:00:71:7f:43:05:3f:c2
-----BEGIN CERTIFICATE-----
MIICRjCCAc2gAwIBAgIQC6Fa+h3foLVJRK/NJKBs7DAKBggqhkjOPQQDAzBlMQsw
CQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cu
ZGlnaWNlcnQuY29tMSQwIgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJvb3Qg
RzMwHhcNMTMwODAxMTIwMDAwWhcNMzgwMTE1MTIwMDAwWjBlMQswCQYDVQQGEwJV
UzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQu
Y29tMSQwIgYDVQQDExtEaWdpQ2VydCBBc3N1cmVkIElEIFJvb3QgRzMwdjAQBgcq
hkjOPQIBBgUrgQQAIgNiAAQZ57ysRGXtzbg/WPuNsVepRC0FFfLvC/8QdJ+1YlJf
Zn4f5dwbRXkLzMZTCp2NXQLZqVneAlr2lSoOjThKiknGvMYDOAdfVdp+CW7if17Q
RSAPWXYQ1qAk8C3eNvJsKTmjQjBAMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/
BAQDAgGGMB0GA1UdDgQWBBTL0L2p4ZgFUaFNN6KDec6NHSrkhDAKBggqhkjOPQQD
AwNnADBkAjAlpIFFAmsSS3V0T8gj43DydXLefInwz5FyYZ5eEJJZVrmDxxDnOOlY
JjZ91eQ0hjkCMHw2U/Aw5WJjOpnitqM7mzT6HtoQknFekROn3aRukswy1vUhZscv
6pZjamVFkpUBtA==
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert Global Root CA O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert Global Root CA O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert Global Root CA"
# Serial: 10944719598952040374951832963794454346
# MD5 Fingerprint: 79:e4:a9:84:0d:7d:3a:96:d7:c0:4f:e2:43:4c:89:2e
# SHA1 Fingerprint: a8:98:5d:3a:65:e5:e5:c4:b2:d7:d6:6d:40:c6:dd:2f:b1:9c:54:36
# SHA256 Fingerprint: 43:48:a0:e9:44:4c:78:cb:26:5e:05:8d:5e:89:44:b4:d8:4f:96:62:bd:26:db:25:7f:89:34:a4:43:c7:01:61
-----BEGIN CERTIFICATE-----
MIIDrzCCApegAwIBAgIQCDvgVpBCRrGhdWrJWZHHSjANBgkqhkiG9w0BAQUFADBh
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBD
QTAeFw0wNjExMTAwMDAwMDBaFw0zMTExMTAwMDAwMDBaMGExCzAJBgNVBAYTAlVT
MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsTEHd3dy5kaWdpY2VydC5j
b20xIDAeBgNVBAMTF0RpZ2lDZXJ0IEdsb2JhbCBSb290IENBMIIBIjANBgkqhkiG
9w0BAQEFAAOCAQ8AMIIBCgKCAQEA4jvhEXLeqKTTo1eqUKKPC3eQyaKl7hLOllsB
CSDMAZOnTjC3U/dDxGkAV53ijSLdhwZAAIEJzs4bg7/fzTtxRuLWZscFs3YnFo97
nh6Vfe63SKMI2tavegw5BmV/Sl0fvBf4q77uKNd0f3p4mVmFaG5cIzJLv07A6Fpt
43C/dxC//AH2hdmoRBBYMql1GNXRor5H4idq9Joz+EkIYIvUX7Q6hL+hqkpMfT7P
T19sdl6gSzeRntwi5m3OFBqOasv+zbMUZBfHWymeMr/y7vrTC0LUq7dBMtoM1O/4
gdW7jVg/tRvoSSiicNoxBN33shbyTApOB6jtSj1etX+jkMOvJwIDAQABo2MwYTAO
BgNVHQ8BAf8EBAMCAYYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUA95QNVbR
TLtm8KPiGxvDl7I90VUwHwYDVR0jBBgwFoAUA95QNVbRTLtm8KPiGxvDl7I90VUw
DQYJKoZIhvcNAQEFBQADggEBAMucN6pIExIK+t1EnE9SsPTfrgT1eXkIoyQY/Esr
hMAtudXH/vTBH1jLuG2cenTnmCmrEbXjcKChzUyImZOMkXDiqw8cvpOp/2PV5Adg
06O/nVsJ8dWO41P0jmP6P6fbtGbfYmbW0W5BjfIttep3Sp+dWOIrWcBAI+0tKIJF
PnlUkiaY4IBIqDfv8NZ5YBberOgOzW6sRBc4L0na4UU+Krk2U886UAb3LujEV0ls
YSEY1QSteDwsOoBrp+uvFRTp2InBuThs4pFsiv9kuXclVzDAGySj4dzp30d8tbQk
CAUw7C29C79Fv1C5qfPrmAESrciIxpg0X40KPMbp1ZWVbd4=
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert Global Root G2 O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert Global Root G2 O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert Global Root G2"
# Serial: 4293743540046975378534879503202253541
# MD5 Fingerprint: e4:a6:8a:c8:54:ac:52:42:46:0a:fd:72:48:1b:2a:44
# SHA1 Fingerprint: df:3c:24:f9:bf:d6:66:76:1b:26:80:73:fe:06:d1:cc:8d:4f:82:a4
# SHA256 Fingerprint: cb:3c:cb:b7:60:31:e5:e0:13:8f:8d:d3:9a:23:f9:de:47:ff:c3:5e:43:c1:14:4c:ea:27:d4:6a:5a:b1:cb:5f
-----BEGIN CERTIFICATE-----
MIIDjjCCAnagAwIBAgIQAzrx5qcRqaC7KGSxHQn65TANBgkqhkiG9w0BAQsFADBh
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
d3cuZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBH
MjAeFw0xMzA4MDExMjAwMDBaFw0zODAxMTUxMjAwMDBaMGExCzAJBgNVBAYTAlVT
MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsTEHd3dy5kaWdpY2VydC5j
b20xIDAeBgNVBAMTF0RpZ2lDZXJ0IEdsb2JhbCBSb290IEcyMIIBIjANBgkqhkiG
9w0BAQEFAAOCAQ8AMIIBCgKCAQEAuzfNNNx7a8myaJCtSnX/RrohCgiN9RlUyfuI
2/Ou8jqJkTx65qsGGmvPrC3oXgkkRLpimn7Wo6h+4FR1IAWsULecYxpsMNzaHxmx
1x7e/dfgy5SDN67sH0NO3Xss0r0upS/kqbitOtSZpLYl6ZtrAGCSYP9PIUkY92eQ
q2EGnI/yuum06ZIya7XzV+hdG82MHauVBJVJ8zUtluNJbd134/tJS7SsVQepj5Wz
tCO7TG1F8PapspUwtP1MVYwnSlcUfIKdzXOS0xZKBgyMUNGPHgm+F6HmIcr9g+UQ
vIOlCsRnKPZzFBQ9RnbDhxSJITRNrw9FDKZJobq7nMWxM4MphQIDAQABo0IwQDAP
BgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIBhjAdBgNVHQ4EFgQUTiJUIBiV
5uNu5g/6+rkS7QYXjzkwDQYJKoZIhvcNAQELBQADggEBAGBnKJRvDkhj6zHd6mcY
1Yl9PMWLSn/pvtsrF9+wX3N3KjITOYFnQoQj8kVnNeyIv/iPsGEMNKSuIEyExtv4
NeF22d+mQrvHRAiGfzZ0JFrabA0UWTW98kndth/Jsw1HKj2ZL7tcu7XUIOGZX1NG
Fdtom/DzMNU+MeKNhJ7jitralj41E6Vf8PlwUHBHQRFXGU7Aj64GxJUTFy8bJZ91
8rGOmaFvE7FBcf6IKshPECBV1/MUReXgRPTqh5Uykw7+U0b6LJ3/iyK5S9kJRaTe
pLiaWN0bfVKfjllDiIGknibVb63dDcY3fe0Dkhvld1927jyNxF1WW6LZZm6zNTfl
MrY=
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert Global Root G3 O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert Global Root G3 O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert Global Root G3"
# Serial: 7089244469030293291760083333884364146
# MD5 Fingerprint: f5:5d:a4:50:a5:fb:28:7e:1e:0f:0d:cc:96:57:56:ca
# SHA1 Fingerprint: 7e:04:de:89:6a:3e:66:6d:00:e6:87:d3:3f:fa:d9:3b:e8:3d:34:9e
# SHA256 Fingerprint: 31:ad:66:48:f8:10:41:38:c7:38:f3:9e:a4:32:01:33:39:3e:3a:18:cc:02:29:6e:f9:7c:2a:c9:ef:67:31:d0
-----BEGIN CERTIFICATE-----
MIICPzCCAcWgAwIBAgIQBVVWvPJepDU1w6QP1atFcjAKBggqhkjOPQQDAzBhMQsw
CQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cu
ZGlnaWNlcnQuY29tMSAwHgYDVQQDExdEaWdpQ2VydCBHbG9iYWwgUm9vdCBHMzAe
Fw0xMzA4MDExMjAwMDBaFw0zODAxMTUxMjAwMDBaMGExCzAJBgNVBAYTAlVTMRUw
EwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsTEHd3dy5kaWdpY2VydC5jb20x
IDAeBgNVBAMTF0RpZ2lDZXJ0IEdsb2JhbCBSb290IEczMHYwEAYHKoZIzj0CAQYF
K4EEACIDYgAE3afZu4q4C/sLfyHS8L6+c/MzXRq8NOrexpu80JX28MzQC7phW1FG
fp4tn+6OYwwX7Adw9c+ELkCDnOg/QW07rdOkFFk2eJ0DQ+4QE2xy3q6Ip6FrtUPO
Z9wj/wMco+I+o0IwQDAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIBhjAd
BgNVHQ4EFgQUs9tIpPmhxdiuNkHMEWNpYim8S8YwCgYIKoZIzj0EAwMDaAAwZQIx
AK288mw/EkrRLTnDCgmXc/SINoyIJ7vmiI1Qhadj+Z4y3maTD/HMsQmP3Wyr+mt/
oAIwOWZbwmSNuJ5Q3KjVSaLtx9zRSX8XAbjIho9OjIgrqJqpisXRAL34VOKa5Vt8
sycX
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert High Assurance EV Root CA O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert High Assurance EV Root CA O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert High Assurance EV Root CA"
# Serial: 3553400076410547919724730734378100087
# MD5 Fingerprint: d4:74:de:57:5c:39:b2:d3:9c:85:83:c5:c0:65:49:8a
# SHA1 Fingerprint: 5f:b7:ee:06:33:e2:59:db:ad:0c:4c:9a:e6:d3:8f:1a:61:c7:dc:25
# SHA256 Fingerprint: 74:31:e5:f4:c3:c1:ce:46:90:77:4f:0b:61:e0:54:40:88:3b:a9:a0:1e:d0:0b:a6:ab:d7:80:6e:d3:b1:18:cf
-----BEGIN CERTIFICATE-----
MIIDxTCCAq2gAwIBAgIQAqxcJmoLQJuPC3nyrkYldzANBgkqhkiG9w0BAQUFADBs
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
d3cuZGlnaWNlcnQuY29tMSswKQYDVQQDEyJEaWdpQ2VydCBIaWdoIEFzc3VyYW5j
ZSBFViBSb290IENBMB4XDTA2MTExMDAwMDAwMFoXDTMxMTExMDAwMDAwMFowbDEL
MAkGA1UEBhMCVVMxFTATBgNVBAoTDERpZ2lDZXJ0IEluYzEZMBcGA1UECxMQd3d3
LmRpZ2ljZXJ0LmNvbTErMCkGA1UEAxMiRGlnaUNlcnQgSGlnaCBBc3N1cmFuY2Ug
RVYgUm9vdCBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMbM5XPm
+9S75S0tMqbf5YE/yc0lSbZxKsPVlDRnogocsF9ppkCxxLeyj9CYpKlBWTrT3JTW
PNt0OKRKzE0lgvdKpVMSOO7zSW1xkX5jtqumX8OkhPhPYlG++MXs2ziS4wblCJEM
xChBVfvLWokVfnHoNb9Ncgk9vjo4UFt3MRuNs8ckRZqnrG0AFFoEt7oT61EKmEFB
Ik5lYYeBQVCmeVyJ3hlKV9Uu5l0cUyx+mM0aBhakaHPQNAQTXKFx01p8VdteZOE3
hzBWBOURtCmAEvF5OYiiAhF8J2a3iLd48soKqDirCmTCv2ZdlYTBoSUeh10aUAsg
EsxBu24LUTi4S8sCAwEAAaNjMGEwDgYDVR0PAQH/BAQDAgGGMA8GA1UdEwEB/wQF
MAMBAf8wHQYDVR0OBBYEFLE+w2kD+L9HAdSYJhoIAu9jZCvDMB8GA1UdIwQYMBaA
FLE+w2kD+L9HAdSYJhoIAu9jZCvDMA0GCSqGSIb3DQEBBQUAA4IBAQAcGgaX3Nec
nzyIZgYIVyHbIUf4KmeqvxgydkAQV8GK83rZEWWONfqe/EW1ntlMMUu4kehDLI6z
eM7b41N5cdblIZQB2lWHmiRk9opmzN6cN82oNLFpmyPInngiK3BD41VHMWEZ71jF
hS9OMPagMRYjyOfiZRYzy78aG6A9+MpeizGLYAiJLQwGXFK3xPkKmNEVX58Svnw2
Yzi9RKR/5CYrCsSXaQ3pjOLAEFe4yHYSkVXySGnYvCoCWw9E1CAx2/S6cCZdkGCe
vEsXCS+0yx5DaMkHJ8HSXPfqIbloEpw8nL+e/IBcm2PN7EeqJSdnoDfzAIJ9VNep
+OkuE6N36B9K
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=DigiCert Trusted Root G4 O=DigiCert Inc OU=www.digicert.com
# Subject: CN=DigiCert Trusted Root G4 O=DigiCert Inc OU=www.digicert.com
# Label: "DigiCert Trusted Root G4"
# Serial: 7451500558977370777930084869016614236
# MD5 Fingerprint: 78:f2:fc:aa:60:1f:2f:b4:eb:c9:37:ba:53:2e:75:49
# SHA1 Fingerprint: dd:fb:16:cd:49:31:c9:73:a2:03:7d:3f:c8:3a:4d:7d:77:5d:05:e4
# SHA256 Fingerprint: 55:2f:7b:dc:f1:a7:af:9e:6c:e6:72:01:7f:4f:12:ab:f7:72:40:c7:8e:76:1a:c2:03:d1:d9:d2:0a:c8:99:88
-----BEGIN CERTIFICATE-----
MIIFkDCCA3igAwIBAgIQBZsbV56OITLiOQe9p3d1XDANBgkqhkiG9w0BAQwFADBi
MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3
d3cuZGlnaWNlcnQuY29tMSEwHwYDVQQDExhEaWdpQ2VydCBUcnVzdGVkIFJvb3Qg
RzQwHhcNMTMwODAxMTIwMDAwWhcNMzgwMTE1MTIwMDAwWjBiMQswCQYDVQQGEwJV
UzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQu
Y29tMSEwHwYDVQQDExhEaWdpQ2VydCBUcnVzdGVkIFJvb3QgRzQwggIiMA0GCSqG
SIb3DQEBAQUAA4ICDwAwggIKAoICAQC/5pBzaN675F1KPDAiMGkz7MKnJS7JIT3y
ithZwuEppz1Yq3aaza57G4QNxDAf8xukOBbrVsaXbR2rsnnyyhHS5F/WBTxSD1If
xp4VpX6+n6lXFllVcq9ok3DCsrp1mWpzMpTREEQQLt+C8weE5nQ7bXHiLQwb7iDV
ySAdYyktzuxeTsiT+CFhmzTrBcZe7FsavOvJz82sNEBfsXpm7nfISKhmV1efVFiO
DCu3T6cw2Vbuyntd463JT17lNecxy9qTXtyOj4DatpGYQJB5w3jHtrHEtWoYOAMQ
jdjUN6QuBX2I9YI+EJFwq1WCQTLX2wRzKm6RAXwhTNS8rhsDdV14Ztk6MUSaM0C/
CNdaSaTC5qmgZ92kJ7yhTzm1EVgX9yRcRo9k98FpiHaYdj1ZXUJ2h4mXaXpI8OCi
EhtmmnTK3kse5w5jrubU75KSOp493ADkRSWJtppEGSt+wJS00mFt6zPZxd9LBADM
fRyVw4/3IbKyEbe7f/LVjHAsQWCqsWMYRJUadmJ+9oCw++hkpjPRiQfhvbfmQ6QY
uKZ3AeEPlAwhHbJUKSWJbOUOUlFHdL4mrLZBdd56rF+NP8m800ERElvlEFDrMcXK
chYiCd98THU/Y+whX8QgUWtvsauGi0/C1kVfnSD8oR7FwI+isX4KJpn15GkvmB0t
9dmpsh3lGwIDAQABo0IwQDAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQEAwIB
hjAdBgNVHQ4EFgQU7NfjgtJxXWRM3y5nP+e6mK4cD08wDQYJKoZIhvcNAQEMBQAD
ggIBALth2X2pbL4XxJEbw6GiAI3jZGgPVs93rnD5/ZpKmbnJeFwMDF/k5hQpVgs2
SV1EY+CtnJYYZhsjDT156W1r1lT40jzBQ0CuHVD1UvyQO7uYmWlrx8GnqGikJ9yd
+SeuMIW59mdNOj6PWTkiU0TryF0Dyu1Qen1iIQqAyHNm0aAFYF/opbSnr6j3bTWc
fFqK1qI4mfN4i/RN0iAL3gTujJtHgXINwBQy7zBZLq7gcfJW5GqXb5JQbZaNaHqa
sjYUegbyJLkJEVDXCLG4iXqEI2FCKeWjzaIgQdfRnGTZ6iahixTXTBmyUEFxPT9N
cCOGDErcgdLMMpSEDQgJlxxPwO5rIHQw0uA5NBCFIRUBCOhVMt5xSdkoF1BN5r5N
0XWs0Mr7QbhDparTwwVETyw2m+L64kW4I1NsBm9nVX9GtUw/bihaeSbSpKhil9Ie
4u1Ki7wb/UdKDd9nZn6yW0HQO+T0O/QEY+nvwlQAUaCKKsnOeMzV6ocEGLPOr0mI
r/OSmbaz5mEP0oUA51Aa5BuVnRmhuZyxm7EAHu/QD09CbMkKvO5D+jpxpchNJqU1
/YldvIViHTLSoCtU7ZpXwdv6EM8Zt4tKG48BtieVU+i2iW1bvGjUI+iLUaJW+fCm
gKDWHrO8Dw9TdSmq6hN35N6MgSGtBxBHEa2HPQfRdbzP82Z+
-----END CERTIFICATE-----

# Operating CA: DigiCert
# Issuer: CN=GeoTrust Global CA O=GeoTrust Inc.
# Subject: CN=GeoTrust Global CA O=GeoTrust Inc.
# Label: "GeoTrust Global CA"
# Serial: 144470
# MD5 Fingerprint: f7:75:ab:29:fb:51:4e:b7:77:5e:ff:05:3c:99:8e:f5
# SHA1 Fingerprint: de:28:f4:a4:ff:e5:b9:2f:a3:c5:03:d1:a3:49:a7:f9:96:2a:82:12
# SHA256 Fingerprint: ff:85:6a:2d:25:1d:cd:88:d3:66:56:f4:50:12:67:98:cf:ab:aa:de:40:79:9c:72:2d:e4:d2:b5:db:36:a7:3a
-----BEGIN CERTIFICATE-----
MIIDVDCCAjygAwIBAgIDAjRWMA0GCSqGSIb3DQEBBQUAMEIxCzAJBgNVBAYTAlVT
MRYwFAYDVQQKEw1HZW9UcnVzdCBJbmMuMRswGQYDVQQDExJHZW9UcnVzdCBHbG9i
YWwgQ0EwHhcNMDIwNTIxMDQwMDAwWhcNMjIwNTIxMDQwMDAwWjBCMQswCQYDVQQG
EwJVUzEWMBQGA1UEChMNR2VvVHJ1c3QgSW5jLjEbMBkGA1UEAxMSR2VvVHJ1c3Qg
R2xvYmFsIENBMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2swYYzD9
9BcjGlZ+W988bDjkcbd4kdS8odhM+KhDtgPpTSEHCIjaWC9mOSm9BXiLnTjoBbdq
fnGk5sRgprDvgOSJKA+eJdbtg/OtppHHmMlCGDUUna2YRpIuT8rxh0PBFpVXLVDv
iS2Aelet8u5fa9IAjbkU+BQVNdnARqN7csiRv8lVK83Qlz6cJmTM386DGXHKTubU
1XupGc1V3sjs0l44U+VcT4wt/lAjNvxm5suOpDkZALeVAjmRCw7+OC7RHQWa9k0+
bw8HHa8sHo9gOeL6NlMTOdReJivbPagUvTLrGAMoUgRx5aszPeE4uwc2hGKceeoW
MPRfwCvocWvk+QIDAQABo1MwUTAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBTA
ephojYn7qwVkDBF9qn1luMrMTjAfBgNVHSMEGDAWgBTAephojYn7qwVkDBF9qn1l
uMrMTjANBgkqhkiG9w0BAQUFAAOCAQEANeMpauUvXVSOKVCUn5kaFOSPeCpilKIn
Z57QzxpeR+nBsqTP3UEaBU6bS+5Kb1VSsyShNwrrZHYqLizz/Tt1kL/6cdjHPTfS
tQWVYrmm3ok9Nns4d0iXrKYgjy6myQzCsplFAMfOEVEiIuCl6rYVSAlk6l5PdPcF
PseKUgzbFbS9bZvlxrFUaKnjaZC2mqUPuLk/IH2uSrW4nOQdtqvmlKXBx4Ot2/Un
hw4EbNX/3aBd7YdStysVAq45pmp06drE57xNNB6pXE0zX5IJL4hmXXeXxx12E6nV
5fEWCRE11azbJHFwLJhWC9kXtNHjUStedejV0NxPNO3CBWaAocvmMw==
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=Entrust Root Certification Authority O=Entrust, Inc. OU=www.entrust.net/CPS is incorporated by reference/(c) 2006 Entrust, Inc.
# Subject: CN=Entrust Root Certification Authority O=Entrust, Inc. OU=www.entrust.net/CPS is incorporated by reference/(c) 2006 Entrust, Inc.
# Label: "Entrust Root Certification Authority"
# Serial: 1164660820
# MD5 Fingerprint: d6:a5:c3:ed:5d:dd:3e:00:c1:3d:87:92:1f:1d:3f:e4
# SHA1 Fingerprint: b3:1e:b1:b7:40:e3:6c:84:02:da:dc:37:d4:4d:f5:d4:67:49:52:f9
# SHA256 Fingerprint: 73:c1:76:43:4f:1b:c6:d5:ad:f4:5b:0e:76:e7:27:28:7c:8d:e5:76:16:c1:e6:e6:14:1a:2b:2c:bc:7d:8e:4c
-----BEGIN CERTIFICATE-----
MIIEkTCCA3mgAwIBAgIERWtQVDANBgkqhkiG9w0BAQUFADCBsDELMAkGA1UEBhMC
VVMxFjAUBgNVBAoTDUVudHJ1c3QsIEluYy4xOTA3BgNVBAsTMHd3dy5lbnRydXN0
Lm5ldC9DUFMgaXMgaW5jb3Jwb3JhdGVkIGJ5IHJlZmVyZW5jZTEfMB0GA1UECxMW
KGMpIDIwMDYgRW50cnVzdCwgSW5jLjEtMCsGA1UEAxMkRW50cnVzdCBSb290IENl
cnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTA2MTEyNzIwMjM0MloXDTI2MTEyNzIw
NTM0MlowgbAxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1FbnRydXN0LCBJbmMuMTkw
NwYDVQQLEzB3d3cuZW50cnVzdC5uZXQvQ1BTIGlzIGluY29ycG9yYXRlZCBieSBy
ZWZlcmVuY2UxHzAdBgNVBAsTFihjKSAyMDA2IEVudHJ1c3QsIEluYy4xLTArBgNV
BAMTJEVudHJ1c3QgUm9vdCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTCCASIwDQYJ
KoZIhvcNAQEBBQADggEPADCCAQoCggEBALaVtkNC+sZtKm9I35RMOVcF7sN5EUFo
Nu3s/poBj6E4KPz3EEZmLk0eGrEaTsbRwJWIsMn/MYszA9u3g3s+IIRe7bJWKKf4
4LlAcTfFy0cOlypowCKVYhXbR9n10Cv/gkvJrT7eTNuQgFA/CYqEAOwwCj0Yzfv9
KlmaI5UXLEWeH25DeW0MXJj+SKfFI0dcXv1u5x609mhF0YaDW6KKjbHjKYD+JXGI
rb68j6xSlkuqUY3kEzEZ6E5Nn9uss2rVvDlUccp6en+Q3X0dgNmBu1kmwhH+5pPi
94DkZfs0Nw4pgHBNrziGLp5/V6+eF67rHMsoIV+2HNjnogQi+dPa2MsCAwEAAaOB
sDCBrTAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zArBgNVHRAEJDAi
gA8yMDA2MTEyNzIwMjM0MlqBDzIwMjYxMTI3MjA1MzQyWjAfBgNVHSMEGDAWgBRo
kORnpKZTgMeGZqTx90tD+4S9bTAdBgNVHQ4EFgQUaJDkZ6SmU4DHhmak8fdLQ/uE
vW0wHQYJKoZIhvZ9B0EABBAwDhsIVjcuMTo0LjADAgSQMA0GCSqGSIb3DQEBBQUA
A4IBAQCT1DCw1wMgKtD5Y+iRDAUgqV8ZyntyTtSx29CW+1RaGSwMCPeyvIWonX9t
O1KzKtvn1ISMY/YPyyYBkVBs9F8U4pN0wBOeMDpQ47RgxRzwIkSNcUesyBrJ6Zua
AGAT/3B+XxFNSRuzFVJ7yVTav52Vr2ua2J7p8eRDjeIRRDq/r72DQnNSi6q7pynP
9WQcCk3RvKqsnyrQ/39/2n3qse0wJcGE2jTSW3iDVuycNsMm4hH2Z0kdkquM++v/
eu6FSqdQgPCnXEqULl8FmTxSQeDNtGPPAUO6nIPcj2A781q0tHuu2guQOHXvgR1m
0vdXcDazv/wor3ElhVsT/h5/WrQ8
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=Entrust Root Certification Authority - EC1 O=Entrust, Inc. OU=See www.entrust.net/legal-terms/(c) 2012 Entrust, Inc. - for authorized use only
# Subject: CN=Entrust Root Certification Authority - EC1 O=Entrust, Inc. OU=See www.entrust.net/legal-terms/(c) 2012 Entrust, Inc. - for authorized use only
# Label: "Entrust Root Certification Authority - EC1"
# Serial: 51543124481930649114116133369
# MD5 Fingerprint: b6:7e:1d:f0:58:c5:49:6c:24:3b:3d:ed:98:18:ed:bc
# SHA1 Fingerprint: 20:d8:06:40:df:9b:25:f5:12:25:3a:11:ea:f7:59:8a:eb:14:b5:47
# SHA256 Fingerprint: 02:ed:0e:b2:8c:14:da:45:16:5c:56:67:91:70:0d:64:51:d7:fb:56:f0:b2:ab:1d:3b:8e:b0:70:e5:6e:df:f5
-----BEGIN CERTIFICATE-----
MIIC+TCCAoCgAwIBAgINAKaLeSkAAAAAUNCR+TAKBggqhkjOPQQDAzCBvzELMAkG
A1UEBhMCVVMxFjAUBgNVBAoTDUVudHJ1c3QsIEluYy4xKDAmBgNVBAsTH1NlZSB3
d3cuZW50cnVzdC5uZXQvbGVnYWwtdGVybXMxOTA3BgNVBAsTMChjKSAyMDEyIEVu
dHJ1c3QsIEluYy4gLSBmb3IgYXV0aG9yaXplZCB1c2Ugb25seTEzMDEGA1UEAxMq
RW50cnVzdCBSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5IC0gRUMxMB4XDTEy
MTIxODE1MjUzNloXDTM3MTIxODE1NTUzNlowgb8xCzAJBgNVBAYTAlVTMRYwFAYD
VQQKEw1FbnRydXN0LCBJbmMuMSgwJgYDVQQLEx9TZWUgd3d3LmVudHJ1c3QubmV0
L2xlZ2FsLXRlcm1zMTkwNwYDVQQLEzAoYykgMjAxMiBFbnRydXN0LCBJbmMuIC0g
Zm9yIGF1dGhvcml6ZWQgdXNlIG9ubHkxMzAxBgNVBAMTKkVudHJ1c3QgUm9vdCBD
ZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEVDMTB2MBAGByqGSM49AgEGBSuBBAAi
A2IABIQTydC6bUF74mzQ61VfZgIaJPRbiWlH47jCffHyAsWfoPZb1YsGGYZPUxBt
ByQnoaD41UcZYUx9ypMn6nQM72+WCf5j7HBdNq1nd67JnXxVRDqiY1Ef9eNi1KlH
Bz7MIKNCMEAwDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0O
BBYEFLdj5xrdjekIplWDpOBqUEFlEUJJMAoGCCqGSM49BAMDA2cAMGQCMGF52OVC
R98crlOZF7ZvHH3hvxGU0QOIdeSNiaSKd0bebWHvAvX7td/M/k7//qnmpwIwW5nX
hTcGtXsI/esni0qU+eH6p44mCOh8kmhtc9hvJqwhAriZtyZBWyVgrtBIGu4G
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=Entrust Root Certification Authority - G2 O=Entrust, Inc. OU=See www.entrust.net/legal-terms/(c) 2009 Entrust, Inc. - for authorized use only
# Subject: CN=Entrust Root Certification Authority - G2 O=Entrust, Inc. OU=See www.entrust.net/legal-terms/(c) 2009 Entrust, Inc. - for authorized use only
# Label: "Entrust Root Certification Authority - G2"
# Serial: 1246989352
# MD5 Fingerprint: 4b:e2:c9:91:96:65:0c:f4:0e:5a:93:92:a0:0a:fe:b2
# SHA1 Fingerprint: 8c:f4:27:fd:79:0c:3a:d1:66:06:8d:e8:1e:57:ef:bb:93:22:72:d4
# SHA256 Fingerprint: 43:df:57:74:b0:3e:7f:ef:5f:e4:0d:93:1a:7b:ed:f1:bb:2e:6b:42:73:8c:4e:6d:38:41:10:3d:3a:a7:f3:39
-----BEGIN CERTIFICATE-----
MIIEPjCCAyagAwIBAgIESlOMKDANBgkqhkiG9w0BAQsFADCBvjELMAkGA1UEBhMC
VVMxFjAUBgNVBAoTDUVudHJ1c3QsIEluYy4xKDAmBgNVBAsTH1NlZSB3d3cuZW50
cnVzdC5uZXQvbGVnYWwtdGVybXMxOTA3BgNVBAsTMChjKSAyMDA5IEVudHJ1c3Qs
IEluYy4gLSBmb3IgYXV0aG9yaXplZCB1c2Ugb25seTEyMDAGA1UEAxMpRW50cnVz
dCBSb290IENlcnRpZmljYXRpb24gQXV0aG9yaXR5IC0gRzIwHhcNMDkwNzA3MTcy
NTU0WhcNMzAxMjA3MTc1NTU0WjCBvjELMAkGA1UEBhMCVVMxFjAUBgNVBAoTDUVu
dHJ1c3QsIEluYy4xKDAmBgNVBAsTH1NlZSB3d3cuZW50cnVzdC5uZXQvbGVnYWwt
dGVybXMxOTA3BgNVBAsTMChjKSAyMDA5IEVudHJ1c3QsIEluYy4gLSBmb3IgYXV0
aG9yaXplZCB1c2Ugb25seTEyMDAGA1UEAxMpRW50cnVzdCBSb290IENlcnRpZmlj
YXRpb24gQXV0aG9yaXR5IC0gRzIwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
AoIBAQC6hLZy254Ma+KZ6TABp3bqMriVQRrJ2mFOWHLP/vaCeb9zYQYKpSfYs1/T
RU4cctZOMvJyig/3gxnQaoCAAEUesMfnmr8SVycco2gvCoe9amsOXmXzHHfV1IWN
cCG0szLni6LVhjkCsbjSR87kyUnEO6fe+1R9V77w6G7CebI6C1XiUJgWMhNcL3hW
wcKUs/Ja5CeanyTXxuzQmyWC48zCxEXFjJd6BmsqEZ+pCm5IO2/b1BEZQvePB7/1
U1+cPvQXLOZprE4yTGJ36rfo5bs0vBmLrpxR57d+tVOxMyLlbc9wPBr64ptntoP0
jaWvYkxN4FisZDQSA/i2jZRjJKRxAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBBjAP
BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRqciZ60B7vfec7aVHUbI2fkBJmqzAN
BgkqhkiG9w0BAQsFAAOCAQEAeZ8dlsa2eT8ijYfThwMEYGprmi5ZiXMRrEPR9RP/
jTkrwPK9T3CMqS/qF8QLVJ7UG5aYMzyorWKiAHarWWluBh1+xLlEjZivEtRh2woZ
Rkfz6/djwUAFQKXSt/S1mja/qYh2iARVBCuch38aNzx+LaUa2NSJXsq9rD1s2G2v
1fN2D807iDginWyTmsQ9v4IbZT+mD12q/OWyFcq1rca8PdCE6OoGcrBNOTJ4vz4R
nAuknZoh8/CbCzB428Hch0P+vGOaysXCHMnHjf87ElgI5rY97HosTvuDls4MPGmH
VHOkc8KT/1EQrBVUAdj8BbGJoX90g5pJ19xOe4pIb4tF9g==
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=Entrust.net Certification Authority (2048) O=Entrust.net OU=www.entrust.net/CPS_2048 incorp. by ref. (limits liab.)/(c) 1999 Entrust.net Limited
# Subject: CN=Entrust.net Certification Authority (2048) O=Entrust.net OU=www.entrust.net/CPS_2048 incorp. by ref. (limits liab.)/(c) 1999 Entrust.net Limited
# Label: "Entrust.net Premium 2048 Secure Server CA"
# Serial: 946069240
# MD5 Fingerprint: ee:29:31:bc:32:7e:9a:e6:e8:b5:f7:51:b4:34:71:90
# SHA1 Fingerprint: 50:30:06:09:1d:97:d4:f5:ae:39:f7:cb:e7:92:7d:7d:65:2d:34:31
# SHA256 Fingerprint: 6d:c4:71:72:e0:1c:bc:b0:bf:62:58:0d:89:5f:e2:b8:ac:9a:d4:f8:73:80:1e:0c:10:b9:c8:37:d2:1e:b1:77
-----BEGIN CERTIFICATE-----
MIIEKjCCAxKgAwIBAgIEOGPe+DANBgkqhkiG9w0BAQUFADCBtDEUMBIGA1UEChML
RW50cnVzdC5uZXQxQDA+BgNVBAsUN3d3dy5lbnRydXN0Lm5ldC9DUFNfMjA0OCBp
bmNvcnAuIGJ5IHJlZi4gKGxpbWl0cyBsaWFiLikxJTAjBgNVBAsTHChjKSAxOTk5
IEVudHJ1c3QubmV0IExpbWl0ZWQxMzAxBgNVBAMTKkVudHJ1c3QubmV0IENlcnRp
ZmljYXRpb24gQXV0aG9yaXR5ICgyMDQ4KTAeFw05OTEyMjQxNzUwNTFaFw0yOTA3
MjQxNDE1MTJaMIG0MRQwEgYDVQQKEwtFbnRydXN0Lm5ldDFAMD4GA1UECxQ3d3d3
LmVudHJ1c3QubmV0L0NQU18yMDQ4IGluY29ycC4gYnkgcmVmLiAobGltaXRzIGxp
YWIuKTElMCMGA1UECxMcKGMpIDE5OTkgRW50cnVzdC5uZXQgTGltaXRlZDEzMDEG
A1UEAxMqRW50cnVzdC5uZXQgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkgKDIwNDgp
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEArU1LqRKGsuqjIAcVFmQq
K0vRvwtKTY7tgHalZ7d4QMBzQshowNtTK91euHaYNZOLGp18EzoOH1u3Hs/lJBQe
sYGpjX24zGtLA/ECDNyrpUAkAH90lKGdCCmziAv1h3edVc3kw37XamSrhRSGlVuX
MlBvPci6Zgzj/L24ScF2iUkZ/cCovYmjZy/Gn7xxGWC4LeksyZB2ZnuU4q941mVT
XTzWnLLPKQP5L6RQstRIzgUyVYr9smRMDuSYB3Xbf9+5CFVghTAp+XtIpGmG4zU/
HoZdenoVve8AjhUiVBcAkCaTvA5JaJG/+EfTnZVCwQ5N328mz8MYIWJmQ3DW1cAH
4QIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNV
HQ4EFgQUVeSB0RGAvtiJuQijMfmhJAkWuXAwDQYJKoZIhvcNAQEFBQADggEBADub
j1abMOdTmXx6eadNl9cZlZD7Bh/KM3xGY4+WZiT6QBshJ8rmcnPyT/4xmf3IDExo
U8aAghOY+rat2l098c5u9hURlIIM7j+VrxGrD9cv3h8Dj1csHsm7mhpElesYT6Yf
zX1XEC+bBAlahLVu2B064dae0Wx5XnkcFMXj0EyTO2U87d89vqbllRrDtRnDvV5b
u/8j72gZyxKTJ1wDLW8w0B62GqzeWvfRqqgnpv55gcR5mTNXuhKwqeBCbJPKVt7+
bYQLCIt+jerXmCHG8+c8eS9enNFMFY3h7CI3zJpDC5fcgJCNs2ebb0gIFVbPv/Er
fF6adulZkMV8gzURZVE=
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=AffirmTrust Commercial O=AffirmTrust
# Subject: CN=AffirmTrust Commercial O=AffirmTrust
# Label: "AffirmTrust Commercial"
# Serial: 8608355977964138876
# MD5 Fingerprint: 82:92:ba:5b:ef:cd:8a:6f:a6:3d:55:f9:84:f6:d6:b7
# SHA1 Fingerprint: f9:b5:b6:32:45:5f:9c:be:ec:57:5f:80:dc:e9:6e:2c:c7:b2:78:b7
# SHA256 Fingerprint: 03:76:ab:1d:54:c5:f9:80:3c:e4:b2:e2:01:a0:ee:7e:ef:7b:57:b6:36:e8:a9:3c:9b:8d:48:60:c9:6f:5f:a7
-----BEGIN CERTIFICATE-----
MIIDTDCCAjSgAwIBAgIId3cGJyapsXwwDQYJKoZIhvcNAQELBQAwRDELMAkGA1UE
BhMCVVMxFDASBgNVBAoMC0FmZmlybVRydXN0MR8wHQYDVQQDDBZBZmZpcm1UcnVz
dCBDb21tZXJjaWFsMB4XDTEwMDEyOTE0MDYwNloXDTMwMTIzMTE0MDYwNlowRDEL
MAkGA1UEBhMCVVMxFDASBgNVBAoMC0FmZmlybVRydXN0MR8wHQYDVQQDDBZBZmZp
cm1UcnVzdCBDb21tZXJjaWFsMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC
AQEA9htPZwcroRX1BiLLHwGy43NFBkRJLLtJJRTWzsO3qyxPxkEylFf6EqdbDuKP
Hx6GGaeqtS25Xw2Kwq+FNXkyLbscYjfysVtKPcrNcV/pQr6U6Mje+SJIZMblq8Yr
ba0F8PrVC8+a5fBQpIs7R6UjW3p6+DM/uO+Zl+MgwdYoic+U+7lF7eNAFxHUdPAL
MeIrJmqbTFeurCA+ukV6BfO9m2kVrn1OIGPENXY6BwLJN/3HR+7o8XYdcxXyl6S1
yHp52UKqK39c/s4mT6NmgTWvRLpUHhwwMmWd5jyTXlBOeuM61G7MGvv50jeuJCqr
VwMiKA1JdX+3KNp1v47j3A55MQIDAQABo0IwQDAdBgNVHQ4EFgQUnZPGU4teyq8/
nx4P5ZmVvCT2lI8wDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDQYJ
KoZIhvcNAQELBQADggEBAFis9AQOzcAN/wr91LoWXym9e2iZWEnStB03TX8nfUYG
XUPGhi4+c7ImfU+TqbbEKpqrIZcUsd6M06uJFdhrJNTxFq7YpFzUf1GO7RgBsZNj
vbz4YYCanrHOQnDiqX0GJX0nof5v7LMeJNrjS1UaADs1tDvZ110w/YETifLCBivt
Z8SOyUOyXGsViQK8YvxO8rUzqrJv0wqiUOP2O+guRMLbZjipM1ZI8W0bM40NjD9g
N53Tym1+NH4Nn3J2ixufcv1SNUFFApYvHLKac0khsUlHRUe072o0EclNmsxZt9YC
nlpOZbWUrhvfKbAW8b8Angc6F2S1BLUjIZkKlTuXfO8=
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=AffirmTrust Networking O=AffirmTrust
# Subject: CN=AffirmTrust Networking O=AffirmTrust
# Label: "AffirmTrust Networking"
# Serial: 8957382827206547757
# MD5 Fingerprint: 42:65:ca:be:01:9a:9a:4c:a9:8c:41:49:cd:c0:d5:7f
# SHA1 Fingerprint: 29:36:21:02:8b:20:ed:02:f5:66:c5:32:d1:d6:ed:90:9f:45:00:2f
# SHA256 Fingerprint: 0a:81:ec:5a:92:97:77:f1:45:90:4a:f3:8d:5d:50:9f:66:b5:e2:c5:8f:cd:b5:31:05:8b:0e:17:f3:f0:b4:1b
-----BEGIN CERTIFICATE-----
MIIDTDCCAjSgAwIBAgIIfE8EORzUmS0wDQYJKoZIhvcNAQEFBQAwRDELMAkGA1UE
BhMCVVMxFDASBgNVBAoMC0FmZmlybVRydXN0MR8wHQYDVQQDDBZBZmZpcm1UcnVz
dCBOZXR3b3JraW5nMB4XDTEwMDEyOTE0MDgyNFoXDTMwMTIzMTE0MDgyNFowRDEL
MAkGA1UEBhMCVVMxFDASBgNVBAoMC0FmZmlybVRydXN0MR8wHQYDVQQDDBZBZmZp
cm1UcnVzdCBOZXR3b3JraW5nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC
AQEAtITMMxcua5Rsa2FSoOujz3mUTOWUgJnLVWREZY9nZOIG41w3SfYvm4SEHi3y
YJ0wTsyEheIszx6e/jarM3c1RNg1lho9Nuh6DtjVR6FqaYvZ/Ls6rnla1fTWcbua
kCNrmreIdIcMHl+5ni36q1Mr3Lt2PpNMCAiMHqIjHNRqrSK6mQEubWXLviRmVSRL
QESxG9fhwoXA3hA/Pe24/PHxI1Pcv2WXb9n5QHGNfb2V1M6+oF4nI979ptAmDgAp
6zxG8D1gvz9Q0twmQVGeFDdCBKNwV6gbh+0t+nvujArjqWaJGctB+d1ENmHP4ndG
yH329JKBNv3bNPFyfvMMFr20FQIDAQABo0IwQDAdBgNVHQ4EFgQUBx/S55zawm6i
QLSwelAQUHTEyL0wDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwDQYJ
KoZIhvcNAQEFBQADggEBAIlXshZ6qML91tmbmzTCnLQyFE2npN/svqe++EPbkTfO
tDIuUFUaNU52Q3Eg75N3ThVwLofDwR1t3Mu1J9QsVtFSUzpE0nPIxBsFZVpikpzu
QY0x2+c06lkh1QF612S4ZDnNye2v7UsDSKegmQGA3GWjNq5lWUhPgkvIZfFXHeVZ
Lgo/bNjR9eUJtGxUAArgFU2HdW23WJZa3W3SAKD0m0i+wzekujbgfIeFlxoVot4u
olu9rxj5kFDNcFn4J2dHy8egBzp90SxdbBk6ZrV9/ZFvgrG+CJPbFEfxojfHRZ48
x3evZKiT3/Zpg4Jg8klCNO1aAFSFHBY2kgxc+qatv9s=
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=AffirmTrust Premium O=AffirmTrust
# Subject: CN=AffirmTrust Premium O=AffirmTrust
# Label: "AffirmTrust Premium"
# Serial: 7893706540734352110
# MD5 Fingerprint: c4:5d:0e:48:b6:ac:28:30:4e:0a:bc:f9:38:16:87:57
# SHA1 Fingerprint: d8:a6:33:2c:e0:03:6f:b1:85:f6:63:4f:7d:6a:06:65:26:32:28:27
# SHA256 Fingerprint: 70:a7:3f:7f:37:6b:60:07:42:48:90:45:34:b1:14:82:d5:bf:0e:69:8e:cc:49:8d:f5:25:77:eb:f2:e9:3b:9a
-----BEGIN CERTIFICATE-----
MIIFRjCCAy6gAwIBAgIIbYwURrGmCu4wDQYJKoZIhvcNAQEMBQAwQTELMAkGA1UE
BhMCVVMxFDASBgNVBAoMC0FmZmlybVRydXN0MRwwGgYDVQQDDBNBZmZpcm1UcnVz
dCBQcmVtaXVtMB4XDTEwMDEyOTE0MTAzNloXDTQwMTIzMTE0MTAzNlowQTELMAkG
A1UEBhMCVVMxFDASBgNVBAoMC0FmZmlybVRydXN0MRwwGgYDVQQDDBNBZmZpcm1U
cnVzdCBQcmVtaXVtMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAxBLf
qV/+Qd3d9Z+K4/as4Tx4mrzY8H96oDMq3I0gW64tb+eT2TZwamjPjlGjhVtnBKAQ
JG9dKILBl1fYSCkTtuG+kU3fhQxTGJoeJKJPj/CihQvL9Cl/0qRY7iZNyaqoe5rZ
+jjeRFcV5fiMyNlI4g0WJx0eyIOFJbe6qlVBzAMiSy2RjYvmia9mx+n/K+k8rNrS
s8PhaJyJ+HoAVt70VZVs+7pk3WKL3wt3MutizCaam7uqYoNMtAZ6MMgpv+0GTZe5
HMQxK9VfvFMSF5yZVylmd2EhMQcuJUmdGPLu8ytxjLW6OQdJd/zvLpKQBY0tL3d7
70O/Nbua2Plzpyzy0FfuKE4mX4+QaAkvuPjcBukumj5Rp9EixAqnOEhss/n/fauG
V+O61oV4d7pD6kh/9ti+I20ev9E2bFhc8e6kGVQa9QPSdubhjL08s9NIS+LI+H+S
qHZGnEJlPqQewQcDWkYtuJfzt9WyVSHvutxMAJf7FJUnM7/oQ0dG0giZFmA7mn7S
5u046uwBHjxIVkkJx0w3AJ6IDsBz4W9m6XJHMD4Q5QsDyZpCAGzFlH5hxIrff4Ia
C1nEWTJ3s7xgaVY5/bQGeyzWZDbZvUjthB9+pSKPKrhC9IK31FOQeE4tGv2Bb0TX
OwF0lkLgAOIua+rF7nKsu7/+6qqo+Nz2snmKtmcCAwEAAaNCMEAwHQYDVR0OBBYE
FJ3AZ6YMItkm9UWrpmVSESfYRaxjMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/
BAQDAgEGMA0GCSqGSIb3DQEBDAUAA4ICAQCzV00QYk465KzquByvMiPIs0laUZx2
KI15qldGF9X1Uva3ROgIRL8YhNILgM3FEv0AVQVhh0HctSSePMTYyPtwni94loMg
Nt58D2kTiKV1NpgIpsbfrM7jWNa3Pt668+s0QNiigfV4Py/VpfzZotReBA4Xrf5B
8OWycvpEgjNC6C1Y91aMYj+6QrCcDFx+LmUmXFNPALJ4fqENmS2NuB2OosSw/WDQ
MKSOyARiqcTtNd56l+0OOF6SL5Nwpamcb6d9Ex1+xghIsV5n61EIJenmJWtSKZGc
0jlzCFfemQa0W50QBuHCAKi4HEoCChTQwUHK+4w1IX2COPKpVJEZNZOUbWo6xbLQ
u4mGk+ibyQ86p3q4ofB4Rvr8Ny/lioTz3/4E2aFooC8k4gmVBtWVyuEklut89pMF
u+1z6S3RdTnX5yTb2E5fQ4+e0BQ5v1VwSJlXMbSc7kqYA5YwH2AG7hsj/oFgIxpH
YoWlzBk0gG+zrBrjn/B7SK3VAdlntqlyk+otZrWyuOQ9PLLvTIzq6we/qzWaVYa8
GKa1qF60g2xraUDTn9zxw2lrueFtCfTxqlB2Cnp9ehehVZZCmTEJ3WARjQUwfuaO
RtGdFNrHF+QFlozEJLUbzxQHskD4o55BhrwE0GuWyCqANP2/7waj3VjFhT0+j/6e
KeC2uAloGRwYQw==
-----END CERTIFICATE-----

# Operating CA: Entrust Datacard
# Issuer: CN=AffirmTrust Premium ECC O=AffirmTrust
# Subject: CN=AffirmTrust Premium ECC O=AffirmTrust
# Label: "AffirmTrust Premium ECC"
# Serial: 8401224907861490260
# MD5 Fingerprint: 64:b0:09:55:cf:b1:d5:99:e2:be:13:ab:a6:5d:ea:4d
# SHA1 Fingerprint: b8:23:6b:00:2f:1d:16:86:53:01:55:6c:11:a4:37:ca:eb:ff:c3:bb
# SHA256 Fingerprint: bd:71:fd:f6:da:97:e4:cf:62:d1:64:7a:dd:25:81:b0:7d:79:ad:f8:39:7e:b4:ec:ba:9c:5e:84:88:82:14:23
-----BEGIN CERTIFICATE-----
MIIB/jCCAYWgAwIBAgIIdJclisc/elQwCgYIKoZIzj0EAwMwRTELMAkGA1UEBhMC
VVMxFDASBgNVBAoMC0FmZmlybVRydXN0MSAwHgYDVQQDDBdBZmZpcm1UcnVzdCBQ
cmVtaXVtIEVDQzAeFw0xMDAxMjkxNDIwMjRaFw00MDEyMzExNDIwMjRaMEUxCzAJ
BgNVBAYTAlVTMRQwEgYDVQQKDAtBZmZpcm1UcnVzdDEgMB4GA1UEAwwXQWZmaXJt
VHJ1c3QgUHJlbWl1bSBFQ0MwdjAQBgcqhkjOPQIBBgUrgQQAIgNiAAQNMF4bFZ0D
0KF5Nbc6PJJ6yhUczWLznCZcBz3lVPqj1swS6vQUX+iOGasvLkjmrBhDeKzQN8O9
ss0s5kfiGuZjuD0uL3jET9v0D6RoTFVya5UdThhClXjMNzyR4ptlKymjQjBAMB0G
A1UdDgQWBBSaryl6wBE1NSZRMADDav5A1a7WPDAPBgNVHRMBAf8EBTADAQH/MA4G
A1UdDwEB/wQEAwIBBjAKBggqhkjOPQQDAwNnADBkAjAXCfOHiFBar8jAQr9HX/Vs
aobgxCd05DhT1wV/GzTjxi+zygk8N53X57hG8f2h4nECMEJZh0PUUd+60wkyWs6I
flc9nF9Ca/UHLbXwgpP5WW+uZPpY5Yse42O+tYHNbwKMeQ==
-----END CERTIFICATE-----

# Operating CA: GlobalSign
# Issuer: CN=GlobalSign Root CA O=GlobalSign nv-sa OU=Root CA
# Subject: CN=GlobalSign Root CA O=GlobalSign nv-sa OU=Root CA
# Label: "GlobalSign Root CA"
# Serial: 4835703278459707669005204
# MD5 Fingerprint: 3e:45:52:15:09:51:92:e1:b7:5d:37:9f:b1:87:29:8a
# SHA1 Fingerprint: b1:bc:96:8b:d4:f4:9d:62:2a:a8:9a:81:f2:15:01:52:a4:1d:82:9c
# SHA256 Fingerprint: eb:d4:10:40:e4:bb:3e:c7:42:c9:e3:81:d3:1e:f2:a4:1a:48:b6:68:5c:96:e7:ce:f3:c1:df:6c:d4:33:1c:99
-----BEGIN CERTIFICATE-----
MIIDdTCCAl2gAwIBAgILBAAAAAABFUtaw5QwDQYJKoZIhvcNAQEFBQAwVzELMAkG
A1UEBhMCQkUxGTAXBgNVBAoTEEdsb2JhbFNpZ24gbnYtc2ExEDAOBgNVBAsTB1Jv
b3QgQ0ExGzAZBgNVBAMTEkdsb2JhbFNpZ24gUm9vdCBDQTAeFw05ODA5MDExMjAw
MDBaFw0yODAxMjgxMjAwMDBaMFcxCzAJBgNVBAYTAkJFMRkwFwYDVQQKExBHbG9i
YWxTaWduIG52LXNhMRAwDgYDVQQLEwdSb290IENBMRswGQYDVQQDExJHbG9iYWxT
aWduIFJvb3QgQ0EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDaDuaZ
jc6j40+Kfvvxi4Mla+pIH/EqsLmVEQS98GPR4mdmzxzdzxtIK+6NiY6arymAZavp
xy0Sy6scTHAHoT0KMM0VjU/43dSMUBUc71DuxC73/OlS8pF94G3VNTCOXkNz8kHp
1Wrjsok6Vjk4bwY8iGlbKk3Fp1S4bInMm/k8yuX9ifUSPJJ4ltbcdG6TRGHRjcdG
snUOhugZitVtbNV4FpWi6cgKOOvyJBNPc1STE4U6G7weNLWLBYy5d4ux2x8gkasJ
U26Qzns3dLlwR5EiUWMWea6xrkEmCMgZK9FGqkjWZCrXgzT/LCrBbBlDSgeF59N8
9iFo7+ryUp9/k5DPAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8E
BTADAQH/MB0GA1UdDgQWBBRge2YaRQ2XyolQL30EzTSo//z9SzANBgkqhkiG9w0B
AQUFAAOCAQEA1nPnfE920I2/7LqivjTFKDK1fPxsnCwrvQmeU79rXqoRSLblCKOz
yj1hTdNGCbM+w6DjY1Ub8rrvrTnhQ7k4o+YviiY776BQVvnGCv04zcQLcFGUl5gE
38NflNUVyRRBnMRddWQVDf9VMOyGj/8N7yy5Y0b2qvzfvGn9LhJIZJrglfCm7ymP
AbEVtQwdpf5pLGkkeB6zpxxxYu7KyJesF12KwvhHhm4qxFYxldBniYUr+WymXUad
DKqC5JlR3XC321Y9YeRq4VzW9v493kHMB65jUr9TU/Qr6cf9tveCX4XSQRjbgbME
HMUfpIBvFSDJ3gyICh3WZlXi/EjJKSZp4A==
-----END CERTIFICATE-----

# Operating CA: GlobalSign
# Issuer: CN=GlobalSign O=GlobalSign OU=GlobalSign Root CA - R3
# Subject: CN=GlobalSign O=GlobalSign OU=GlobalSign Root CA - R3
# Label: "GlobalSign Root CA - R3"
# Serial: 4835703278459759426209954
# MD5 Fingerprint: c5:df:b8:49:ca:05:13:55:ee:2d:ba:1a:c3:3e:b0:28
# SHA1 Fingerprint: d6:9b:56:11:48:f0:1c:77:c5:45:78:c1:09:26:df:5b:85:69:76:ad
# SHA256 Fingerprint: cb:b5:22:d7:b7:f1:27:ad:6a:01:13:86:5b:df:1c:d4:10:2e:7d:07:59:af:63:5a:7c:f4:72:0d:c9:63:c5:3b
-----BEGIN CERTIFICATE-----
MIIDXzCCAkegAwIBAgILBAAAAAABIVhTCKIwDQYJKoZIhvcNAQELBQAwTDEgMB4G
A1UECxMXR2xvYmFsU2lnbiBSb290IENBIC0gUjMxEzARBgNVBAoTCkdsb2JhbFNp
Z24xEzARBgNVBAMTCkdsb2JhbFNpZ24wHhcNMDkwMzE4MTAwMDAwWhcNMjkwMzE4
MTAwMDAwWjBMMSAwHgYDVQQLExdHbG9iYWxTaWduIFJvb3QgQ0EgLSBSMzETMBEG
A1UEChMKR2xvYmFsU2lnbjETMBEGA1UEAxMKR2xvYmFsU2lnbjCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAMwldpB5BngiFvXAg7aEyiie/QV2EcWtiHL8
RgJDx7KKnQRfJMsuS+FggkbhUqsMgUdwbN1k0ev1LKMPgj0MK66X17YUhhB5uzsT
gHeMCOFJ0mpiLx9e+pZo34knlTifBtc+ycsmWQ1z3rDI6SYOgxXG71uL0gRgykmm
KPZpO/bLyCiR5Z2KYVc3rHQU3HTgOu5yLy6c+9C7v/U9AOEGM+iCK65TpjoWc4zd
QQ4gOsC0p6Hpsk+QLjJg6VfLuQSSaGjlOCZgdbKfd/+RFO+uIEn8rUAVSNECMWEZ
XriX7613t2Saer9fwRPvm2L7DWzgVGkWqQPabumDk3F2xmmFghcCAwEAAaNCMEAw
DgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFI/wS3+o
LkUkrk1Q+mOai97i3Ru8MA0GCSqGSIb3DQEBCwUAA4IBAQBLQNvAUKr+yAzv95ZU
RUm7lgAJQayzE4aGKAczymvmdLm6AC2upArT9fHxD4q/c2dKg8dEe3jgr25sbwMp
jjM5RcOO5LlXbKr8EpbsU8Yt5CRsuZRj+9xTaGdWPoO4zzUhw8lo/s7awlOqzJCK
6fBdRoyV3XpYKBovHd7NADdBj+1EbddTKJd+82cEHhXXipa0095MJ6RMG3NzdvQX
mcIfeg7jLQitChws/zyrVQ4PkX4268NXSb7hLi18YIvDQVETI53O9zJrlAGomecs
Mx86OyXShkDOOyyGeMlhLxS67ttVb9+E7gUJTb0o2HLO02JQZR7rkpeDMdmztcpH
WD9f
-----END CERTIFICATE-----

# Operating CA: GlobalSign
# Issuer: CN=GlobalSign O=GlobalSign OU=GlobalSign ECC Root CA - R5
# Subject: CN=GlobalSign O=GlobalSign OU=GlobalSign ECC Root CA - R5
# Label: "GlobalSign ECC Root CA - R5"
# Serial: 32785792099990507226680698011560947931244
# MD5 Fingerprint: 9f:ad:3b:1c:02:1e:8a:ba:17:74:38:81:0c:a2:bc:08
# SHA1 Fingerprint: 1f:24:c6:30:cd:a4:18:ef:20:69:ff:ad:4f:dd:5f:46:3a:1b:69:aa
# SHA256 Fingerprint: 17:9f:bc:14:8a:3d:d0:0f:d2:4e:a1:34:58:cc:43:bf:a7:f5:9c:81:82:d7:83:a5:13:f6:eb:ec:10:0c:89:24
-----BEGIN CERTIFICATE-----
MIICHjCCAaSgAwIBAgIRYFlJ4CYuu1X5CneKcflK2GwwCgYIKoZIzj0EAwMwUDEk
MCIGA1UECxMbR2xvYmFsU2lnbiBFQ0MgUm9vdCBDQSAtIFI1MRMwEQYDVQQKEwpH
bG9iYWxTaWduMRMwEQYDVQQDEwpHbG9iYWxTaWduMB4XDTEyMTExMzAwMDAwMFoX
DTM4MDExOTAzMTQwN1owUDEkMCIGA1UECxMbR2xvYmFsU2lnbiBFQ0MgUm9vdCBD
QSAtIFI1MRMwEQYDVQQKEwpHbG9iYWxTaWduMRMwEQYDVQQDEwpHbG9iYWxTaWdu
MHYwEAYHKoZIzj0CAQYFK4EEACIDYgAER0UOlvt9Xb/pOdEh+J8LttV7HpI6SFkc
8GIxLcB6KP4ap1yztsyX50XUWPrRd21DosCHZTQKH3rd6zwzocWdTaRvQZU4f8ke
hOvRnkmSh5SHDDqFSmafnVmTTZdhBoZKo0IwQDAOBgNVHQ8BAf8EBAMCAQYwDwYD
VR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUPeYpSJvqB8ohREom3m7e0oPQn1kwCgYI
KoZIzj0EAwMDaAAwZQIxAOVpEslu28YxuglB4Zf4+/2a4n0Sye18ZNPLBSWLVtmg
515dTguDnFt2KaAJJiFqYgIwcdK1j1zqO+F4CYWodZI7yFz9SO8NdCKoCOJuxUnO
xwy8p2Fp8fc74SrL+SvzZpA3
-----END CERTIFICATE-----

# Operating CA: GlobalSign
# Issuer: CN=GlobalSign O=GlobalSign OU=GlobalSign Root CA - R6
# Subject: CN=GlobalSign O=GlobalSign OU=GlobalSign Root CA - R6
# Label: "GlobalSign Root CA - R6"
# Serial: 1417766617973444989252670301619537
# MD5 Fingerprint: 4f:dd:07:e4:d4:22:64:39:1e:0c:37:42:ea:d1:c6:ae
# SHA1 Fingerprint: 80:94:64:0e:b5:a7:a1:ca:11:9c:1f:dd:d5:9f:81:02:63:a7:fb:d1
# SHA256 Fingerprint: 2c:ab:ea:fe:37:d0:6c:a2:2a:ba:73:91:c0:03:3d:25:98:29:52:c4:53:64:73:49:76:3a:3a:b5:ad:6c:cf:69
-----BEGIN CERTIFICATE-----
MIIFgzCCA2ugAwIBAgIORea7A4Mzw4VlSOb/RVEwDQYJKoZIhvcNAQEMBQAwTDEg
MB4GA1UECxMXR2xvYmFsU2lnbiBSb290IENBIC0gUjYxEzARBgNVBAoTCkdsb2Jh
bFNpZ24xEzARBgNVBAMTCkdsb2JhbFNpZ24wHhcNMTQxMjEwMDAwMDAwWhcNMzQx
MjEwMDAwMDAwWjBMMSAwHgYDVQQLExdHbG9iYWxTaWduIFJvb3QgQ0EgLSBSNjET
MBEGA1UEChMKR2xvYmFsU2lnbjETMBEGA1UEAxMKR2xvYmFsU2lnbjCCAiIwDQYJ
KoZIhvcNAQEBBQADggIPADCCAgoCggIBAJUH6HPKZvnsFMp7PPcNCPG0RQssgrRI
xutbPK6DuEGSMxSkb3/pKszGsIhrxbaJ0cay/xTOURQh7ErdG1rG1ofuTToVBu1k
ZguSgMpE3nOUTvOniX9PeGMIyBJQbUJmL025eShNUhqKGoC3GYEOfsSKvGRMIRxD
aNc9PIrFsmbVkJq3MQbFvuJtMgamHvm566qjuL++gmNQ0PAYid/kD3n16qIfKtJw
LnvnvJO7bVPiSHyMEAc4/2ayd2F+4OqMPKq0pPbzlUoSB239jLKJz9CgYXfIWHSw
1CM69106yqLbnQneXUQtkPGBzVeS+n68UARjNN9rkxi+azayOeSsJDa38O+2HBNX
k7besvjihbdzorg1qkXy4J02oW9UivFyVm4uiMVRQkQVlO6jxTiWm05OWgtH8wY2
SXcwvHE35absIQh1/OZhFj931dmRl4QKbNQCTXTAFO39OfuD8l4UoQSwC+n+7o/h
bguyCLNhZglqsQY6ZZZZwPA1/cnaKI0aEYdwgQqomnUdnjqGBQCe24DWJfncBZ4n
WUx2OVvq+aWh2IMP0f/fMBH5hc8zSPXKbWQULHpYT9NLCEnFlWQaYw55PfWzjMpY
rZxCRXluDocZXFSxZba/jJvcE+kNb7gu3GduyYsRtYQUigAZcIN5kZeR1Bonvzce
MgfYFGM8KEyvAgMBAAGjYzBhMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTAD
AQH/MB0GA1UdDgQWBBSubAWjkxPioufi1xzWx/B/yGdToDAfBgNVHSMEGDAWgBSu
bAWjkxPioufi1xzWx/B/yGdToDANBgkqhkiG9w0BAQwFAAOCAgEAgyXt6NH9lVLN
nsAEoJFp5lzQhN7craJP6Ed41mWYqVuoPId8AorRbrcWc+ZfwFSY1XS+wc3iEZGt
Ixg93eFyRJa0lV7Ae46ZeBZDE1ZXs6KzO7V33EByrKPrmzU+sQghoefEQzd5Mr61
55wsTLxDKZmOMNOsIeDjHfrYBzN2VAAiKrlNIC5waNrlU/yDXNOd8v9EDERm8tLj
vUYAGm0CuiVdjaExUd1URhxN25mW7xocBFymFe944Hn+Xds+qkxV/ZoVqW/hpvvf
cDDpw+5CRu3CkwWJ+n1jez/QcYF8AOiYrg54NMMl+68KnyBr3TsTjxKM4kEaSHpz
oHdpx7Zcf4LIHv5YGygrqGytXm3ABdJ7t+uA/iU3/gKbaKxCXcPu9czc8FB10jZp
nOZ7BN9uBmm23goJSFmH63sUYHpkqmlD75HHTOwY3WzvUy2MmeFe8nI+z1TIvWfs
pA9MRf/TuTAjB0yPEL+GltmZWrSZVxykzLsViVO6LAUP5MSeGbEYNNVMnbrt9x+v
JJUEeKgDu+6B5dpffItKoZB0JaezPkvILFa9x8jvOOJckvB595yEunQtYQEgfn7R
8k8HWV+LLUNS60YMlOH1Zkd5d9VUWx+tJDfLRVpOoERIyNiwmcUVhAn21klJwGW4
5hpxbqCo8YLoRT5s1gLXCmeDBVrJpBA=
-----END CERTIFICATE-----

# Note: "GlobalSign Root CA - R7" not added on purpose. It is P-521.

# Operating CA: GoDaddy
# Issuer: CN=Go Daddy Root Certificate Authority - G2 O=GoDaddy.com, Inc.
# Subject: CN=Go Daddy Root Certificate Authority - G2 O=GoDaddy.com, Inc.
# Label: "Go Daddy Root Certificate Authority - G2"
# Serial: 0
# MD5 Fingerprint: 80:3a:bc:22:c1:e6:fb:8d:9b:3b:27:4a:32:1b:9a:01
# SHA1 Fingerprint: 47:be:ab:c9:22:ea:e8:0e:78:78:34:62:a7:9f:45:c2:54:fd:e6:8b
# SHA256 Fingerprint: 45:14:0b:32:47:eb:9c:c8:c5:b4:f0:d7:b5:30:91:f7:32:92:08:9e:6e:5a:63:e2:74:9d:d3:ac:a9:19:8e:da
-----BEGIN CERTIFICATE-----
MIIDxTCCAq2gAwIBAgIBADANBgkqhkiG9w0BAQsFADCBgzELMAkGA1UEBhMCVVMx
EDAOBgNVBAgTB0FyaXpvbmExEzARBgNVBAcTClNjb3R0c2RhbGUxGjAYBgNVBAoT
EUdvRGFkZHkuY29tLCBJbmMuMTEwLwYDVQQDEyhHbyBEYWRkeSBSb290IENlcnRp
ZmljYXRlIEF1dGhvcml0eSAtIEcyMB4XDTA5MDkwMTAwMDAwMFoXDTM3MTIzMTIz
NTk1OVowgYMxCzAJBgNVBAYTAlVTMRAwDgYDVQQIEwdBcml6b25hMRMwEQYDVQQH
EwpTY290dHNkYWxlMRowGAYDVQQKExFHb0RhZGR5LmNvbSwgSW5jLjExMC8GA1UE
AxMoR28gRGFkZHkgUm9vdCBDZXJ0aWZpY2F0ZSBBdXRob3JpdHkgLSBHMjCCASIw
DQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAL9xYgjx+lk09xvJGKP3gElY6SKD
E6bFIEMBO4Tx5oVJnyfq9oQbTqC023CYxzIBsQU+B07u9PpPL1kwIuerGVZr4oAH
/PMWdYA5UXvl+TW2dE6pjYIT5LY/qQOD+qK+ihVqf94Lw7YZFAXK6sOoBJQ7Rnwy
DfMAZiLIjWltNowRGLfTshxgtDj6AozO091GB94KPutdfMh8+7ArU6SSYmlRJQVh
GkSBjCypQ5Yj36w6gZoOKcUcqeldHraenjAKOc7xiID7S13MMuyFYkMlNAJWJwGR
tDtwKj9useiciAF9n9T521NtYJ2/LOdYq7hfRvzOxBsDPAnrSTFcaUaz4EcCAwEA
AaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYE
FDqahQcQZyi27/a9BUFuIMGU2g/eMA0GCSqGSIb3DQEBCwUAA4IBAQCZ21151fmX
WWcDYfF+OwYxdS2hII5PZYe096acvNjpL9DbWu7PdIxztDhC2gV7+AJ1uP2lsdeu
9tfeE8tTEH6KRtGX+rcuKxGrkLAngPnon1rpN5+r5N9ss4UXnT3ZJE95kTXWXwTr
gIOrmgIttRD02JDHBHNA7XIloKmf7J6raBKZV8aPEjoJpL1E/QYVN8Gb5DKj7Tjo
2GTzLH4U/ALqn83/B2gX2yKQOC16jdFU8WnjXzPKej17CuPKf1855eJ1usV2GDPO
LPAvTK33sefOT6jEm0pUBsV/fdUID+Ic/n4XuKxe9tQWskMJDE32p2u0mYRlynqI
4uJEvlz36hz1
-----END CERTIFICATE-----

# Operating CA: GoDaddy
# Issuer: CN=Starfield Root Certificate Authority - G2 O=Starfield Technologies, Inc.
# Subject: CN=Starfield Root Certificate Authority - G2 O=Starfield Technologies, Inc.
# Label: "Starfield Root Certificate Authority - G2"
# Serial: 0
# MD5 Fingerprint: d6:39:81:c6:52:7e:96:69:fc:fc:ca:66:ed:05:f2:96
# SHA1 Fingerprint: b5:1c:06:7c:ee:2b:0c:3d:f8:55:ab:2d:92:f4:fe:39:d4:e7:0f:0e
# SHA256 Fingerprint: 2c:e1:cb:0b:f9:d2:f9:e1:02:99:3f:be:21:51:52:c3:b2:dd:0c:ab:de:1c:68:e5:31:9b:83:91:54:db:b7:f5
-----BEGIN CERTIFICATE-----
MIID3TCCAsWgAwIBAgIBADANBgkqhkiG9w0BAQsFADCBjzELMAkGA1UEBhMCVVMx
EDAOBgNVBAgTB0FyaXpvbmExEzARBgNVBAcTClNjb3R0c2RhbGUxJTAjBgNVBAoT
HFN0YXJmaWVsZCBUZWNobm9sb2dpZXMsIEluYy4xMjAwBgNVBAMTKVN0YXJmaWVs
ZCBSb290IENlcnRpZmljYXRlIEF1dGhvcml0eSAtIEcyMB4XDTA5MDkwMTAwMDAw
MFoXDTM3MTIzMTIzNTk1OVowgY8xCzAJBgNVBAYTAlVTMRAwDgYDVQQIEwdBcml6
b25hMRMwEQYDVQQHEwpTY290dHNkYWxlMSUwIwYDVQQKExxTdGFyZmllbGQgVGVj
aG5vbG9naWVzLCBJbmMuMTIwMAYDVQQDEylTdGFyZmllbGQgUm9vdCBDZXJ0aWZp
Y2F0ZSBBdXRob3JpdHkgLSBHMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC
ggEBAL3twQP89o/8ArFvW59I2Z154qK3A2FWGMNHttfKPTUuiUP3oWmb3ooa/RMg
nLRJdzIpVv257IzdIvpy3Cdhl+72WoTsbhm5iSzchFvVdPtrX8WJpRBSiUZV9Lh1
HOZ/5FSuS/hVclcCGfgXcVnrHigHdMWdSL5stPSksPNkN3mSwOxGXn/hbVNMYq/N
Hwtjuzqd+/x5AJhhdM8mgkBj87JyahkNmcrUDnXMN/uLicFZ8WJ/X7NfZTD4p7dN
dloedl40wOiWVpmKs/B/pM293DIxfJHP4F8R+GuqSVzRmZTRouNjWwl2tVZi4Ut0
HZbUJtQIBFnQmA4O5t78w+wfkPECAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAO
BgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFHwMMh+n2TB/xH1oo2Kooc6rB1snMA0G
CSqGSIb3DQEBCwUAA4IBAQARWfolTwNvlJk7mh+ChTnUdgWUXuEok21iXQnCoKjU
sHU48TRqneSfioYmUeYs0cYtbpUgSpIB7LiKZ3sx4mcujJUDJi5DnUox9g61DLu3
4jd/IroAow57UvtruzvE03lRTs2Q9GcHGcg8RnoNAX3FWOdt5oUwF5okxBDgBPfg
8n/Uqgr/Qh037ZTlZFkSIHc40zI+OIF1lnP6aI+xy84fxez6nH7PfrHxBy22/L/K
pL/QlwVKvOoYKAKQvVR4CSFx09F9HdkWsKlhPdAKACL8x3vLCWRFCztAgfd9fDL1
mMpYjn0q7pBZc2T5NnReJaH1ZgUufzkVqSr7UIuOhWn0
-----END CERTIFICATE-----

# Operating CA: GoDaddy
# Issuer: O=Starfield Technologies, Inc. OU=Starfield Class 2 Certification Authority
# Subject: O=Starfield Technologies, Inc. OU=Starfield Class 2 Certification Authority
# Label: "Starfield Class 2 CA"
# Serial: 0
# MD5 Fingerprint: 32:4a:4b:bb:c8:63:69:9b:be:74:9a:c6:dd:1d:46:24
# SHA1 Fingerprint: ad:7e:1c:28:b0:64:ef:8f:60:03:40:20:14:c3:d0:e3:37:0e:b5:8a
# SHA256 Fingerprint: 14:65:fa:20:53:97:b8:76:fa:a6:f0:a9:95:8e:55:90:e4:0f:cc:7f:aa:4f:b7:c2:c8:67:75:21:fb:5f:b6:58
-----BEGIN CERTIFICATE-----
MIIEDzCCAvegAwIBAgIBADANBgkqhkiG9w0BAQUFADBoMQswCQYDVQQGEwJVUzEl
MCMGA1UEChMcU3RhcmZpZWxkIFRlY2hub2xvZ2llcywgSW5jLjEyMDAGA1UECxMp
U3RhcmZpZWxkIENsYXNzIDIgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwHhcNMDQw
NjI5MTczOTE2WhcNMzQwNjI5MTczOTE2WjBoMQswCQYDVQQGEwJVUzElMCMGA1UE
ChMcU3RhcmZpZWxkIFRlY2hub2xvZ2llcywgSW5jLjEyMDAGA1UECxMpU3RhcmZp
ZWxkIENsYXNzIDIgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwggEgMA0GCSqGSIb3
DQEBAQUAA4IBDQAwggEIAoIBAQC3Msj+6XGmBIWtDBFk385N78gDGIc/oav7PKaf
8MOh2tTYbitTkPskpD6E8J7oX+zlJ0T1KKY/e97gKvDIr1MvnsoFAZMej2YcOadN
+lq2cwQlZut3f+dZxkqZJRRU6ybH838Z1TBwj6+wRir/resp7defqgSHo9T5iaU0
X9tDkYI22WY8sbi5gv2cOj4QyDvvBmVmepsZGD3/cVE8MC5fvj13c7JdBmzDI1aa
K4UmkhynArPkPw2vCHmCuDY96pzTNbO8acr1zJ3o/WSNF4Azbl5KXZnJHoe0nRrA
1W4TNSNe35tfPe/W93bC6j67eA0cQmdrBNj41tpvi/JEoAGrAgEDo4HFMIHCMB0G
A1UdDgQWBBS/X7fRzt0fhvRbVazc1xDCDqmI5zCBkgYDVR0jBIGKMIGHgBS/X7fR
zt0fhvRbVazc1xDCDqmI56FspGowaDELMAkGA1UEBhMCVVMxJTAjBgNVBAoTHFN0
YXJmaWVsZCBUZWNobm9sb2dpZXMsIEluYy4xMjAwBgNVBAsTKVN0YXJmaWVsZCBD
bGFzcyAyIENlcnRpZmljYXRpb24gQXV0aG9yaXR5ggEAMAwGA1UdEwQFMAMBAf8w
DQYJKoZIhvcNAQEFBQADggEBAAWdP4id0ckaVaGsafPzWdqbAYcaT1epoXkJKtv3
L7IezMdeatiDh6GX70k1PncGQVhiv45YuApnP+yz3SFmH8lU+nLMPUxA2IGvd56D
eruix/U0F47ZEUD0/CwqTRV/p2JdLiXTAAsgGh1o+Re49L2L7ShZ3U0WixeDyLJl
xy16paq8U4Zt3VekyvggQQto8PT7dL5WXXp59fkdheMtlb71cZBDzI0fmgAKhynp
VSJYACPq4xJDKVtHCN2MQWplBqjlIapBtJUhlbl90TSrE9atvNziPTnNvT51cKEY
WQPJIrSPnNVeKtelttQKbfi3QBFGmh95DmK/D5fs4C8fF5Q=
-----END CERTIFICATE-----

# Operating CA: GoDaddy
# Issuer: O=The Go Daddy Group, Inc. OU=Go Daddy Class 2 Certification Authority
# Subject: O=The Go Daddy Group, Inc. OU=Go Daddy Class 2 Certification Authority
# Label: "Go Daddy Class 2 CA"
# Serial: 0
# MD5 Fingerprint: 91:de:06:25:ab:da:fd:32:17:0c:bb:25:17:2a:84:67
# SHA1 Fingerprint: 27:96:ba:e6:3f:18:01:e2:77:26:1b:a0:d7:77:70:02:8f:20:ee:e4
# SHA256 Fingerprint: c3:84:6b:f2:4b:9e:93:ca:64:27:4c:0e:c6:7c:1e:cc:5e:02:4f:fc:ac:d2:d7:40:19:35:0e:81:fe:54:6a:e4
-----BEGIN CERTIFICATE-----
MIIEADCCAuigAwIBAgIBADANBgkqhkiG9w0BAQUFADBjMQswCQYDVQQGEwJVUzEh
MB8GA1UEChMYVGhlIEdvIERhZGR5IEdyb3VwLCBJbmMuMTEwLwYDVQQLEyhHbyBE
YWRkeSBDbGFzcyAyIENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTA0MDYyOTE3
MDYyMFoXDTM0MDYyOTE3MDYyMFowYzELMAkGA1UEBhMCVVMxITAfBgNVBAoTGFRo
ZSBHbyBEYWRkeSBHcm91cCwgSW5jLjExMC8GA1UECxMoR28gRGFkZHkgQ2xhc3Mg
MiBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTCCASAwDQYJKoZIhvcNAQEBBQADggEN
ADCCAQgCggEBAN6d1+pXGEmhW+vXX0iG6r7d/+TvZxz0ZWizV3GgXne77ZtJ6XCA
PVYYYwhv2vLM0D9/AlQiVBDYsoHUwHU9S3/Hd8M+eKsaA7Ugay9qK7HFiH7Eux6w
wdhFJ2+qN1j3hybX2C32qRe3H3I2TqYXP2WYktsqbl2i/ojgC95/5Y0V4evLOtXi
EqITLdiOr18SPaAIBQi2XKVlOARFmR6jYGB0xUGlcmIbYsUfb18aQr4CUWWoriMY
avx4A6lNf4DD+qta/KFApMoZFv6yyO9ecw3ud72a9nmYvLEHZ6IVDd2gWMZEewo+
YihfukEHU1jPEX44dMX4/7VpkI+EdOqXG68CAQOjgcAwgb0wHQYDVR0OBBYEFNLE
sNKR1EwRcbNhyz2h/t2oatTjMIGNBgNVHSMEgYUwgYKAFNLEsNKR1EwRcbNhyz2h
/t2oatTjoWekZTBjMQswCQYDVQQGEwJVUzEhMB8GA1UEChMYVGhlIEdvIERhZGR5
IEdyb3VwLCBJbmMuMTEwLwYDVQQLEyhHbyBEYWRkeSBDbGFzcyAyIENlcnRpZmlj
YXRpb24gQXV0aG9yaXR5ggEAMAwGA1UdEwQFMAMBAf8wDQYJKoZIhvcNAQEFBQAD
ggEBADJL87LKPpH8EsahB4yOd6AzBhRckB4Y9wimPQoZ+YeAEW5p5JYXMP80kWNy
OO7MHAGjHZQopDH2esRU1/blMVgDoszOYtuURXO1v0XJJLXVggKtI3lpjbi2Tc7P
TMozI+gciKqdi0FuFskg5YmezTvacPd+mSYgFFQlq25zheabIZ0KbIIOqPjCDPoQ
HmyW74cNxA9hi63ugyuV+I6ShHI56yDqg+2DzZduCLzrTia2cyvk0/ZM/iZx4mER
dEr/VxqHD3VILs9RaRegAhJhldXRQLIQTO7ErBBDpqWeCtWVYpoNz4iCxTIM5Cuf
ReYNnyicsbkqWletNw+vHX/bvZ8=
-----END CERTIFICATE-----

# Operating CA: Google Trust Services LLC
# Issuer: CN=GlobalSign O=GlobalSign OU=GlobalSign Root CA - R2
# Subject: CN=GlobalSign O=GlobalSign OU=GlobalSign Root CA - R2
# Label: "GlobalSign Root CA - R2"
# Serial: 4835703278459682885658125
# MD5 Fingerprint: 94:14:77:7e:3e:5e:fd:8f:30:bd:41:b0:cf:e7:d0:30
# SHA1 Fingerprint: 75:e0:ab:b6:13:85:12:27:1c:04:f8:5f:dd:de:38:e4:b7:24:2e:fe
# SHA256 Fingerprint: ca:42:dd:41:74:5f:d0:b8:1e:b9:02:36:2c:f9:d8:bf:71:9d:a1:bd:1b:1e:fc:94:6f:5b:4c:99:f4:2c:1b:9e
-----BEGIN CERTIFICATE-----
MIIDujCCAqKgAwIBAgILBAAAAAABD4Ym5g0wDQYJKoZIhvcNAQEFBQAwTDEgMB4G
A1UECxMXR2xvYmFsU2lnbiBSb290IENBIC0gUjIxEzARBgNVBAoTCkdsb2JhbFNp
Z24xEzARBgNVBAMTCkdsb2JhbFNpZ24wHhcNMDYxMjE1MDgwMDAwWhcNMjExMjE1
MDgwMDAwWjBMMSAwHgYDVQQLExdHbG9iYWxTaWduIFJvb3QgQ0EgLSBSMjETMBEG
A1UEChMKR2xvYmFsU2lnbjETMBEGA1UEAxMKR2xvYmFsU2lnbjCCASIwDQYJKoZI
hvcNAQEBBQADggEPADCCAQoCggEBAKbPJA6+Lm8omUVCxKs+IVSbC9N/hHD6ErPL
v4dfxn+G07IwXNb9rfF73OX4YJYJkhD10FPe+3t+c4isUoh7SqbKSaZeqKeMWhG8
eoLrvozps6yWJQeXSpkqBy+0Hne/ig+1AnwblrjFuTosvNYSuetZfeLQBoZfXklq
tTleiDTsvHgMCJiEbKjNS7SgfQx5TfC4LcshytVsW33hoCmEofnTlEnLJGKRILzd
C9XZzPnqJworc5HGnRusyMvo4KD0L5CLTfuwNhv2GXqF4G3yYROIXJ/gkwpRl4pa
zq+r1feqCapgvdzZX99yqWATXgAByUr6P6TqBwMhAo6CygPCm48CAwEAAaOBnDCB
mTAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUm+IH
V2ccHsBqBt5ZtJot39wZhi4wNgYDVR0fBC8wLTAroCmgJ4YlaHR0cDovL2NybC5n
bG9iYWxzaWduLm5ldC9yb290LXIyLmNybDAfBgNVHSMEGDAWgBSb4gdXZxwewGoG
3lm0mi3f3BmGLjANBgkqhkiG9w0BAQUFAAOCAQEAmYFThxxol4aR7OBKuEQLq4Gs
J0/WwbgcQ3izDJr86iw8bmEbTUsp9Z8FHSbBuOmDAGJFtqkIk7mpM0sYmsL4h4hO
291xNBrBVNpGP+DTKqttVCL1OmLNIG+6KYnX3ZHu01yiPqFbQfXf5WRDLenVOavS
ot+3i9DAgBkcRcAtjOj4LaR0VknFBbVPFd5uRHg5h6h+u/N5GJG79G+dwfCMNYxd
AfvDbbnvRG15RjF+Cv6pgsH/76tuIMRQyV+dTZsXjAzlAcmgQWpzU/qlULRuJQ/7
TBj0/VLZjmmx6BEP3ojY+x1J96relc8geMJgEtslQIxq/H5COEBkEveegeGTLg==
-----END CERTIFICATE-----

# Operating CA: Google Trust Services LLC
# Issuer: CN=GlobalSign O=GlobalSign OU=GlobalSign ECC Root CA - R4
# Subject: CN=GlobalSign O=GlobalSign OU=GlobalSign ECC Root CA - R4
# Label: "GlobalSign ECC Root CA - R4"
# Serial: 14367148294922964480859022125800977897474
# MD5 Fingerprint: 20:f0:27:68:d1:7e:a0:9d:0e:e6:2a:ca:df:5c:89:8e
# SHA1 Fingerprint: 69:69:56:2e:40:80:f4:24:a1:e7:19:9f:14:ba:f3:ee:58:ab:6a:bb
# SHA256 Fingerprint: be:c9:49:11:c2:95:56:76:db:6c:0a:55:09:86:d7:6e:3b:a0:05:66:7c:44:2c:97:62:b4:fb:b7:73:de:22:8c
-----BEGIN CERTIFICATE-----
MIIB4TCCAYegAwIBAgIRKjikHJYKBN5CsiilC+g0mAIwCgYIKoZIzj0EAwIwUDEk
MCIGA1UECxMbR2xvYmFsU2lnbiBFQ0MgUm9vdCBDQSAtIFI0MRMwEQYDVQQKEwpH
bG9iYWxTaWduMRMwEQYDVQQDEwpHbG9iYWxTaWduMB4XDTEyMTExMzAwMDAwMFoX
DTM4MDExOTAzMTQwN1owUDEkMCIGA1UECxMbR2xvYmFsU2lnbiBFQ0MgUm9vdCBD
QSAtIFI0MRMwEQYDVQQKEwpHbG9iYWxTaWduMRMwEQYDVQQDEwpHbG9iYWxTaWdu
MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEuMZ5049sJQ6fLjkZHAOkrprlOQcJ
FspjsbmG+IpXwVfOQvpzofdlQv8ewQCybnMO/8ch5RikqtlxP6jUuc6MHaNCMEAw
DgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFFSwe61F
uOJAf/sKbvu+M8k8o4TVMAoGCCqGSM49BAMCA0gAMEUCIQDckqGgE6bPA7DmxCGX
kPoUVy0D7O48027KqGx2vKLeuwIgJ6iFJzWbVsaj8kfSt24bAgAXqmemFZHe+pTs
ewv4n4Q=
-----END CERTIFICATE-----

# Operating CA: Google Trust Services LLC
# Issuer: C=US, O=Google Trust Services LLC, CN=GTS Root R1
# Subject: C=US, O=Google Trust Services LLC, CN=GTS Root R1
# Label: "GTS Root R1"
# Serial: 6e:47:a9:c5:4b:47:0c:0d:ec:33:d0:89:b9:1c:f4:e1
# MD5 Fingerprint: 82:1A:EF:D4:D2:4A:F2:9F:E2:3D:97:06:14:70:72:85
# SHA1 Fingerprint: E1:C9:50:E6:EF:22:F8:4C:56:45:72:8B:92:20:60:D7:D5:A7:A3:E8
# SHA256 Fingerprint: 2A:57:54:71:E3:13:40:BC:21:58:1C:BD:2C:F1:3E:15:84:63:20:3E:CE:94:BC:F9:D3:CC:19:6B:F0:9A:54:72
-----BEGIN CERTIFICATE-----
MIIFWjCCA0KgAwIBAgIQbkepxUtHDA3sM9CJuRz04TANBgkqhkiG9w0BAQwFADBH
MQswCQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNlcnZpY2VzIExM
QzEUMBIGA1UEAxMLR1RTIFJvb3QgUjEwHhcNMTYwNjIyMDAwMDAwWhcNMzYwNjIy
MDAwMDAwWjBHMQswCQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNl
cnZpY2VzIExMQzEUMBIGA1UEAxMLR1RTIFJvb3QgUjEwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQC2EQKLHuOhd5s73L+UPreVp0A8of2C+X0yBoJx9vaM
f/vo27xqLpeXo4xL+Sv2sfnOhB2x+cWX3u+58qPpvBKJXqeqUqv4IyfLpLGcY9vX
mX7wCl7raKb0xlpHDU0QM+NOsROjyBhsS+z8CZDfnWQpJSMHobTSPS5g4M/SCYe7
zUjwTcLCeoiKu7rPWRnWr4+wB7CeMfGCwcDfLqZtbBkOtdh+JhpFAz2weaSUKK0P
fyblqAj+lug8aJRT7oM6iCsVlgmy4HqMLnXWnOunVmSPlk9orj2XwoSPwLxAwAtc
vfaHszVsrBhQf4TgTM2S0yDpM7xSma8ytSmzJSq0SPly4cpk9+aCEI3oncKKiPo4
Zor8Y/kB+Xj9e1x3+naH+uzfsQ55lVe0vSbv1gHR6xYKu44LtcXFilWr06zqkUsp
zBmkMiVOKvFlRNACzqrOSbTqn3yDsEB750Orp2yjj32JgfpMpf/VjsPOS+C12LOO
Rc92wO1AK/1TD7Cn1TsNsYqiA94xrcx36m97PtbfkSIS5r762DL8EGMUUXLeXdYW
k70paDPvOmbsB4om3xPXV2V4J95eSRQAogB/mqghtqmxlbCluQ0WEdrHbEg8QOB+
DVrNVjzRlwW5y0vtOUucxD/SVRNuJLDWcfr0wbrM7Rv1/oFB2ACYPTrIrnqYNxgF
lQIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNV
HQ4EFgQU5K8rJnEaK0gnhS9SZizv8IkTcT4wDQYJKoZIhvcNAQEMBQADggIBADiW
Cu49tJYeX++dnAsznyvgyv3SjgofQXSlfKqE1OXyHuY3UjKcC9FhHb8owbZEKTV1
d5iyfNm9dKyKaOOpMQkpAWBz40d8U6iQSifvS9efk+eCNs6aaAyC58/UEBZvXw6Z
XPYfcX3v73svfuo21pdwCxXu11xWajOl40k4DLh9+42FpLFZXvRq4d2h9mREruZR
gyFmxhE+885H7pwoHyXa/6xmld01D1zvICxi/ZG6qcz8WpyTgYMpl0p8WnK0OdC3
d8t5/Wk6kjftbjhlRn7pYL15iJdfOBL07q9bgsiG1eGZbYwE8na6SfZu6W0eX6Dv
J4J2QPim01hcDyxC2kLGe4g0x8HYRZvBPsVhHdljUEn2NIVq4BjFbkerQUIpm/Zg
DdIx02OYI5NaAIFItO/Nis3Jz5nu2Z6qNuFoS3FJFDYoOj0dzpqPJeaAcWErtXvM
+SUWgeExX6GjfhaknBZqlxi9dnKlC54dNuYvoS++cJEPqOba+MSSQGwlfnuzCdyy
F62ARPBopY+Udf90WuioAnwMCeKpSwughQtiue+hMZL77/ZRBIls6Kl0obsXs7X9
SQ98POyDGCBDTtWTurQ0sR8WNh8M5mQ5Fkzc4P4dyKliPUDqysU0ArSuiYgzNdws
E3PYJ/HQcu51OyLemGhmW/HGY0dVHLqlCFF1pkgl
-----END CERTIFICATE-----

# Operating CA: Google Trust Services LLC
# Issuer: C=US, O=Google Trust Services LLC, CN=GTS Root R2
# Subject: C=US, O=Google Trust Services LLC, CN=GTS Root R2
# Label: "GTS Root R2"
# Serial: 6e:47:a9:c6:5a:b3:e7:20:c5:30:9a:3f:68:52:f2:6f
# MD5 Fingerprint: 44:ED:9A:0E:A4:09:3B:00:F2:AE:4C:A3:C6:61:B0:8B
# SHA1 Fingerprint: D2:73:96:2A:2A:5E:39:9F:73:3F:E1:C7:1E:64:3F:03:38:34:FC:4D
# SHA256 Fingerprint: C4:5D:7B:B0:8E:6D:67:E6:2E:42:35:11:0B:56:4E:5F:78:FD:92:EF:05:8C:84:0A:EA:4E:64:55:D7:58:5C:60
-----BEGIN CERTIFICATE-----
MIIFWjCCA0KgAwIBAgIQbkepxlqz5yDFMJo/aFLybzANBgkqhkiG9w0BAQwFADBH
MQswCQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNlcnZpY2VzIExM
QzEUMBIGA1UEAxMLR1RTIFJvb3QgUjIwHhcNMTYwNjIyMDAwMDAwWhcNMzYwNjIy
MDAwMDAwWjBHMQswCQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNl
cnZpY2VzIExMQzEUMBIGA1UEAxMLR1RTIFJvb3QgUjIwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQDO3v2m++zsFDQ8BwZabFn3GTXd98GdVarTzTukk3Lv
CvptnfbwhYBboUhSnznFt+4orO/LdmgUud+tAWyZH8QiHZ/+cnfgLFuv5AS/T3Kg
GjSY6Dlo7JUle3ah5mm5hRm9iYz+re026nO8/4Piy33B0s5Ks40FnotJk9/BW9Bu
XvAuMC6C/Pq8tBcKSOWIm8Wba96wyrQD8Nr0kLhlZPdcTK3ofmZemde4wj7I0BOd
re7kRXuJVfeKH2JShBKzwkCX44ofR5GmdFrS+LFjKBC4swm4VndAoiaYecb+3yXu
PuWgf9RhD1FLPD+M2uFwdNjCaKH5wQzpoeJ/u1U8dgbuak7MkogwTZq9TwtImoS1
mKPV+3PBV2HdKFZ1E66HjucMUQkQdYhMvI35ezzUIkgfKtzra7tEscszcTJGr61K
8YzodDqs5xoic4DSMPclQsciOzsSrZYuxsN2B6ogtzVJV+mSSeh2FnIxZyuWfoqj
x5RWIr9qS34BIbIjMt/kmkRtWVtd9QCgHJvGeJeNkP+byKq0rxFROV7Z+2et1VsR
nTKaG73VululycslaVNVJ1zgyjbLiGH7HrfQy+4W+9OmTN6SpdTi3/UGVN4unUu0
kzCqgc7dGtxRcw1PcOnlthYhGXmy5okLdWTK1au8CcEYof/UVKGFPP0UJAOyh9Ok
twIDAQABo0IwQDAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zAdBgNV
HQ4EFgQUu//KjiOfT5nK2+JopqUVJxce2Q4wDQYJKoZIhvcNAQEMBQADggIBALZp
8KZ3/p7uC4Gt4cCpx/k1HUCCq+YEtN/L9x0Pg/B+E02NjO7jMyLDOfxA325BS0JT
vhaI8dI4XsRomRyYUpOM52jtG2pzegVATX9lO9ZY8c6DR2Dj/5epnGB3GFW1fgiT
z9D2PGcDFWEJ+YF59exTpJ/JjwGLc8R3dtyDovUMSRqodt6Sm2T4syzFJ9MHwAiA
pJiS4wGWAqoC7o87xdFtCjMwc3i5T1QWvwsHoaRc5svJXISPD+AVdyx+Jn7axEvb
pxZ3B7DNdehyQtaVhJ2Gg/LkkM0JR9SLA3DaWsYDQvTtN6LwG1BUSw7YhN4ZKJmB
R64JGz9I0cNv4rBgF/XuIwKl2gBbbZCr7qLpGzvpx0QnRY5rn/WkhLx3+WuXrD5R
RaIRpsyF7gpo8j5QOHokYh4XIDdtak23CZvJ/KRY9bb7nE4Yu5UC56GtmwfuNmsk
0jmGwZODUNKBRqhfYlcsu2xkiAhu7xNUX90txGdj08+JN7+dIPT7eoOboB6BAFDC
5AwiWVIQ7UNWhwD4FFKnHYuTjKJNRn8nxnGbJN7k2oaLDX5rIMHAnuFl2GqjpuiF
izoHCBy69Y9Vmhh1fuXsgWbRIXOhNUQLgD1bnF5vKheW0YMjiGZt5obicDIvUiLn
yOd/xCxgXS/Dr55FBcOEArf9LAhST4Ldo/DUhgkC
-----END CERTIFICATE-----

# Operating CA: Google Trust Services LLC
# Issuer: C=US, O=Google Trust Services LLC, CN=GTS Root R3
# Subject: C=US, O=Google Trust Services LLC, CN=GTS Root R3
# Label: "GTS Root R3"
# Serial: 6e:47:a9:c7:6c:a9:73:24:40:89:0f:03:55:dd:8d:1d
# MD5 Fingerprint: 1A:79:5B:6B:04:52:9C:5D:C7:74:33:1B:25:9A:F9:25
# SHA1 Fingerprint: 30:D4:24:6F:07:FF:DB:91:89:8A:0B:E9:49:66:11:EB:8C:5E:46:E5
# SHA256 Fingerprint: 15:D5:B8:77:46:19:EA:7D:54:CE:1C:A6:D0:B0:C4:03:E0:37:A9:17:F1:31:E8:A0:4E:1E:6B:7A:71:BA:BC:E5
-----BEGIN CERTIFICATE-----
MIICDDCCAZGgAwIBAgIQbkepx2ypcyRAiQ8DVd2NHTAKBggqhkjOPQQDAzBHMQsw
CQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNlcnZpY2VzIExMQzEU
MBIGA1UEAxMLR1RTIFJvb3QgUjMwHhcNMTYwNjIyMDAwMDAwWhcNMzYwNjIyMDAw
MDAwWjBHMQswCQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNlcnZp
Y2VzIExMQzEUMBIGA1UEAxMLR1RTIFJvb3QgUjMwdjAQBgcqhkjOPQIBBgUrgQQA
IgNiAAQfTzOHMymKoYTey8chWEGJ6ladK0uFxh1MJ7x/JlFyb+Kf1qPKzEUURout
736GjOyxfi//qXGdGIRFBEFVbivqJn+7kAHjSxm65FSWRQmx1WyRRK2EE46ajA2A
DDL24CejQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTADAQH/MB0GA1Ud
DgQWBBTB8Sa6oC2uhYHP0/EqEr24Cmf9vDAKBggqhkjOPQQDAwNpADBmAjEAgFuk
fCPAlaUs3L6JbyO5o91lAFJekazInXJ0glMLfalAvWhgxeG4VDvBNhcl2MG9AjEA
njWSdIUlUfUk7GRSJFClH9voy8l27OyCbvWFGFPouOOaKaqW04MjyaR7YbPMAuhd
-----END CERTIFICATE-----

# Operating CA: Google Trust Services LLC
# Issuer: C=US, O=Google Trust Services LLC, CN=GTS Root R4
# Subject: C=US, O=Google Trust Services LLC, CN=GTS Root R4
# Label: "GTS Root R4"
# Serial: 6e:47:a9:c8:8b:94:b6:e8:bb:3b:2a:d8:a2:b2:c1:99
# MD5 Fingerprint: 5D:B6:6A:C4:60:17:24:6A:1A:99:A8:4B:EE:5E:B4:26
# SHA1 Fingerprint: 2A:1D:60:27:D9:4A:B1:0A:1C:4D:91:5C:CD:33:A0:CB:3E:2D:54:CB
# SHA256 Fingerprint: 71:CC:A5:39:1F:9E:79:4B:04:80:25:30:B3:63:E1:21:DA:8A:30:43:BB:26:66:2F:EA:4D:CA:7F:C9:51:A4:BD
-----BEGIN CERTIFICATE-----
MIICCjCCAZGgAwIBAgIQbkepyIuUtui7OyrYorLBmTAKBggqhkjOPQQDAzBHMQsw
CQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNlcnZpY2VzIExMQzEU
MBIGA1UEAxMLR1RTIFJvb3QgUjQwHhcNMTYwNjIyMDAwMDAwWhcNMzYwNjIyMDAw
MDAwWjBHMQswCQYDVQQGEwJVUzEiMCAGA1UEChMZR29vZ2xlIFRydXN0IFNlcnZp
Y2VzIExMQzEUMBIGA1UEAxMLR1RTIFJvb3QgUjQwdjAQBgcqhkjOPQIBBgUrgQQA
IgNiAATzdHOnaItgrkO4NcWBMHtLSZ37wWHO5t5GvWvVYRg1rkDdc/eJkTBa6zzu
hXyiQHY7qca4R9gq55KRanPpsXI5nymfopjTX15YhmUPoYRlBtHci8nHc8iMai/l
xKvRHYqjQjBAMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTADAQH/MB0GA1Ud
DgQWBBSATNbrdP9JNqPV2Py1PsVq8JQdjDAKBggqhkjOPQQDAwNnADBkAjBqUFJ0
CMRw3J5QdCHojXohw0+WbhXRIjVhLfoIN+4Zba3bssx9BzT1YBkstTTZbyACMANx
sbqjYAuG7ZoIapVon+Kz4ZNkfF6Tpt95LY2F45TPI11xzPKwTdb+mciUqXWi4w==
-----END CERTIFICATE-----

# Operating CA: Sectigo
# Issuer: CN=AAA Certificate Services O=Comodo CA Limited
# Subject: CN=AAA Certificate Services O=Comodo CA Limited
# Label: "Comodo AAA Services root"
# Serial: 1
# MD5 Fingerprint: 49:79:04:b0:eb:87:19:ac:47:b0:bc:11:51:9b:74:d0
# SHA1 Fingerprint: d1:eb:23:a4:6d:17:d6:8f:d9:25:64:c2:f1:f1:60:17:64:d8:e3:49
# SHA256 Fingerprint: d7:a7:a0:fb:5d:7e:27:31:d7:71:e9:48:4e:bc:de:f7:1d:5f:0c:3e:0a:29:48:78:2b:c8:3e:e0:ea:69:9e:f4
-----BEGIN CERTIFICATE-----
MIIEMjCCAxqgAwIBAgIBATANBgkqhkiG9w0BAQUFADB7MQswCQYDVQQGEwJHQjEb
MBkGA1UECAwSR3JlYXRlciBNYW5jaGVzdGVyMRAwDgYDVQQHDAdTYWxmb3JkMRow
GAYDVQQKDBFDb21vZG8gQ0EgTGltaXRlZDEhMB8GA1UEAwwYQUFBIENlcnRpZmlj
YXRlIFNlcnZpY2VzMB4XDTA0MDEwMTAwMDAwMFoXDTI4MTIzMTIzNTk1OVowezEL
MAkGA1UEBhMCR0IxGzAZBgNVBAgMEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UE
BwwHU2FsZm9yZDEaMBgGA1UECgwRQ29tb2RvIENBIExpbWl0ZWQxITAfBgNVBAMM
GEFBQSBDZXJ0aWZpY2F0ZSBTZXJ2aWNlczCCASIwDQYJKoZIhvcNAQEBBQADggEP
ADCCAQoCggEBAL5AnfRu4ep2hxxNRUSOvkbIgwadwSr+GB+O5AL686tdUIoWMQua
BtDFcCLNSS1UY8y2bmhGC1Pqy0wkwLxyTurxFa70VJoSCsN6sjNg4tqJVfMiWPPe
3M/vg4aijJRPn2jymJBGhCfHdr/jzDUsi14HZGWCwEiwqJH5YZ92IFCokcdmtet4
YgNW8IoaE+oxox6gmf049vYnMlhvB/VruPsUK6+3qszWY19zjNoFmag4qMsXeDZR
rOme9Hg6jc8P2ULimAyrL58OAd7vn5lJ8S3frHRNG5i1R8XlKdH5kBjHYpy+g8cm
ez6KJcfA3Z3mNWgQIJ2P2N7Sw4ScDV7oL8kCAwEAAaOBwDCBvTAdBgNVHQ4EFgQU
oBEKIz6W8Qfs4q8p74Klf9AwpLQwDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQF
MAMBAf8wewYDVR0fBHQwcjA4oDagNIYyaHR0cDovL2NybC5jb21vZG9jYS5jb20v
QUFBQ2VydGlmaWNhdGVTZXJ2aWNlcy5jcmwwNqA0oDKGMGh0dHA6Ly9jcmwuY29t
b2RvLm5ldC9BQUFDZXJ0aWZpY2F0ZVNlcnZpY2VzLmNybDANBgkqhkiG9w0BAQUF
AAOCAQEACFb8AvCb6P+k+tZ7xkSAzk/ExfYAWMymtrwUSWgEdujm7l3sAg9g1o1Q
GE8mTgHj5rCl7r+8dFRBv/38ErjHT1r0iWAFf2C3BUrz9vHCv8S5dIa2LX1rzNLz
Rt0vxuBqw8M0Ayx9lt1awg6nCpnBBYurDC/zXDrPbDdVCYfeU0BsWO/8tqtlbgT2
G9w84FoVxp7Z8VlIMCFlA2zs6SFz7JsDoeA3raAVGI/6ugLOpyypEBMs1OUIJqsi
l2D4kF501KKaU73yqWjgom7C12yxow+ev+to51byrvLjKzg6CYG1a4XXvi3tPxq3
smPi9WIsgtRqAEFQ8TmDn5XpNpaYbg==
-----END CERTIFICATE-----

# Operating CA: Sectigo
# Issuer: CN=COMODO Certification Authority O=COMODO CA Limited
# Subject: CN=COMODO Certification Authority O=COMODO CA Limited
# Label: "COMODO Certification Authority"
# Serial: 104350513648249232941998508985834464573
# MD5 Fingerprint: 5c:48:dc:f7:42:72:ec:56:94:6d:1c:cc:71:35:80:75
# SHA1 Fingerprint: 66:31:bf:9e:f7:4f:9e:b6:c9:d5:a6:0c:ba:6a:be:d1:f7:bd:ef:7b
# SHA256 Fingerprint: 0c:2c:d6:3d:f7:80:6f:a3:99:ed:e8:09:11:6b:57:5b:f8:79:89:f0:65:18:f9:80:8c:86:05:03:17:8b:af:66
-----BEGIN CERTIFICATE-----
MIIEHTCCAwWgAwIBAgIQToEtioJl4AsC7j41AkblPTANBgkqhkiG9w0BAQUFADCB
gTELMAkGA1UEBhMCR0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4G
A1UEBxMHU2FsZm9yZDEaMBgGA1UEChMRQ09NT0RPIENBIExpbWl0ZWQxJzAlBgNV
BAMTHkNPTU9ETyBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTAeFw0wNjEyMDEwMDAw
MDBaFw0yOTEyMzEyMzU5NTlaMIGBMQswCQYDVQQGEwJHQjEbMBkGA1UECBMSR3Jl
YXRlciBNYW5jaGVzdGVyMRAwDgYDVQQHEwdTYWxmb3JkMRowGAYDVQQKExFDT01P
RE8gQ0EgTGltaXRlZDEnMCUGA1UEAxMeQ09NT0RPIENlcnRpZmljYXRpb24gQXV0
aG9yaXR5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA0ECLi3LjkRv3
UcEbVASY06m/weaKXTuH+7uIzg3jLz8GlvCiKVCZrts7oVewdFFxze1CkU1B/qnI
2GqGd0S7WWaXUF601CxwRM/aN5VCaTwwxHGzUvAhTaHYujl8HJ6jJJ3ygxaYqhZ8
Q5sVW7euNJH+1GImGEaaP+vB+fGQV+useg2L23IwambV4EajcNxo2f8ESIl33rXp
+2dtQem8Ob0y2WIC8bGoPW43nOIv4tOiJovGuFVDiOEjPqXSJDlqR6sA1KGzqSX+
DT+nHbrTUcELpNqsOO9VUCQFZUaTNE8tja3G1CEZ0o7KBWFxB3NH5YoZEr0ETc5O
nKVIrLsm9wIDAQABo4GOMIGLMB0GA1UdDgQWBBQLWOWLxkwVN6RAqTCpIb5HNlpW
/zAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/BAUwAwEB/zBJBgNVHR8EQjBAMD6g
PKA6hjhodHRwOi8vY3JsLmNvbW9kb2NhLmNvbS9DT01PRE9DZXJ0aWZpY2F0aW9u
QXV0aG9yaXR5LmNybDANBgkqhkiG9w0BAQUFAAOCAQEAPpiem/Yb6dc5t3iuHXIY
SdOH5EOC6z/JqvWote9VfCFSZfnVDeFs9D6Mk3ORLgLETgdxb8CPOGEIqB6BCsAv
IC9Bi5HcSEW88cbeunZrM8gALTFGTO3nnc+IlP8zwFboJIYmuNg4ON8qa90SzMc/
RxdMosIGlgnW2/4/PEZB31jiVg88O8EckzXZOFKs7sjsLjBOlDW0JB9LeGna8gI4
zJVSk/BwJVmcIGfE7vmLV2H0knZ9P4SNVbfo5azV8fUZVqZa+5Acr5Pr5RzUZ5dd
BA6+C4OmF4O5MBKgxTMVBbkN+8cFduPYSo38NBejxiEovjBFMR7HeL5YYTisO+IB
ZQ==
-----END CERTIFICATE-----

# Operating CA: Sectigo
# Issuer: CN=COMODO ECC Certification Authority O=COMODO CA Limited
# Subject: CN=COMODO ECC Certification Authority O=COMODO CA Limited
# Label: "COMODO ECC Certification Authority"
# Serial: 41578283867086692638256921589707938090
# MD5 Fingerprint: 7c:62:ff:74:9d:31:53:5e:68:4a:d5:78:aa:1e:bf:23
# SHA1 Fingerprint: 9f:74:4e:9f:2b:4d:ba:ec:0f:31:2c:50:b6:56:3b:8e:2d:93:c3:11
# SHA256 Fingerprint: 17:93:92:7a:06:14:54:97:89:ad:ce:2f:8f:34:f7:f0:b6:6d:0f:3a:e3:a3:b8:4d:21:ec:15:db:ba:4f:ad:c7
-----BEGIN CERTIFICATE-----
MIICiTCCAg+gAwIBAgIQH0evqmIAcFBUTAGem2OZKjAKBggqhkjOPQQDAzCBhTEL
MAkGA1UEBhMCR0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UE
BxMHU2FsZm9yZDEaMBgGA1UEChMRQ09NT0RPIENBIExpbWl0ZWQxKzApBgNVBAMT
IkNPTU9ETyBFQ0MgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwHhcNMDgwMzA2MDAw
MDAwWhcNMzgwMTE4MjM1OTU5WjCBhTELMAkGA1UEBhMCR0IxGzAZBgNVBAgTEkdy
ZWF0ZXIgTWFuY2hlc3RlcjEQMA4GA1UEBxMHU2FsZm9yZDEaMBgGA1UEChMRQ09N
T0RPIENBIExpbWl0ZWQxKzApBgNVBAMTIkNPTU9ETyBFQ0MgQ2VydGlmaWNhdGlv
biBBdXRob3JpdHkwdjAQBgcqhkjOPQIBBgUrgQQAIgNiAAQDR3svdcmCFYX7deSR
FtSrYpn1PlILBs5BAH+X4QokPB0BBO490o0JlwzgdeT6+3eKKvUDYEs2ixYjFq0J
cfRK9ChQtP6IHG4/bC8vCVlbpVsLM5niwz2J+Wos77LTBumjQjBAMB0GA1UdDgQW
BBR1cacZSBm8nZ3qQUfflMRId5nTeTAOBgNVHQ8BAf8EBAMCAQYwDwYDVR0TAQH/
BAUwAwEB/zAKBggqhkjOPQQDAwNoADBlAjEA7wNbeqy3eApyt4jf/7VGFAkK+qDm
fQjGGoe9GKhzvSbKYAydzpmfz1wPMOG+FDHqAjAU9JM8SaczepBGR7NjfRObTrdv
GDeAU/7dIOA1mjbRxwG55tzd8/8dLDoWV9mSOdY=
-----END CERTIFICATE-----

# Operating CA: Sectigo
# Issuer: CN=COMODO RSA Certification Authority O=COMODO CA Limited
# Subject: CN=COMODO RSA Certification Authority O=COMODO CA Limited
# Label: "COMODO RSA Certification Authority"
# Serial: 101909084537582093308941363524873193117
# MD5 Fingerprint: 1b:31:b0:71:40:36:cc:14:36:91:ad:c4:3e:fd:ec:18
# SHA1 Fingerprint: af:e5:d2:44:a8:d1:19:42:30:ff:47:9f:e2:f8:97:bb:cd:7a:8c:b4
# SHA256 Fingerprint: 52:f0:e1:c4:e5:8e:c6:29:29:1b:60:31:7f:07:46:71:b8:5d:7e:a8:0d:5b:07:27:34:63:53:4b:32:b4:02:34
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----

# Operating CA: Sectigo
# Issuer: CN=USERTrust ECC Certification Authority O=The USERTRUST Network
# Subject: CN=USERTrust ECC Certification Authority O=The USERTRUST Network
# Label: "USERTrust ECC Certification Authority"
# Serial: 123013823720199481456569720443997572134
# MD5 Fingerprint: fa:68:bc:d9:b5:7f:ad:fd:c9:1d:06:83:28:cc:24:c1
# SHA1 Fingerprint: d1:cb:ca:5d:b2:d5:2a:7f:69:3b:67:4d:e5:f0:5a:1d:0c:95:7d:f0
# SHA256 Fingerprint: 4f:f4:60:d5:4b:9c:86:da:bf:bc:fc:57:12:e0:40:0d:2b:ed:3f:bc:4d:4f:bd:aa:86:e0:6a:dc:d2:a9:ad:7a
-----BEGIN CERTIFICATE-----
MIICjzCCAhWgAwIBAgIQXIuZxVqUxdJxVt7NiYDMJjAKBggqhkjOPQQDAzCBiDEL
MAkGA1UEBhMCVVMxEzARBgNVBAgTCk5ldyBKZXJzZXkxFDASBgNVBAcTC0plcnNl
eSBDaXR5MR4wHAYDVQQKExVUaGUgVVNFUlRSVVNUIE5ldHdvcmsxLjAsBgNVBAMT
JVVTRVJUcnVzdCBFQ0MgQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwHhcNMTAwMjAx
MDAwMDAwWhcNMzgwMTE4MjM1OTU5WjCBiDELMAkGA1UEBhMCVVMxEzARBgNVBAgT
Ck5ldyBKZXJzZXkxFDASBgNVBAcTC0plcnNleSBDaXR5MR4wHAYDVQQKExVUaGUg
VVNFUlRSVVNUIE5ldHdvcmsxLjAsBgNVBAMTJVVTRVJUcnVzdCBFQ0MgQ2VydGlm
aWNhdGlvbiBBdXRob3JpdHkwdjAQBgcqhkjOPQIBBgUrgQQAIgNiAAQarFRaqflo
I+d61SRvU8Za2EurxtW20eZzca7dnNYMYf3boIkDuAUU7FfO7l0/4iGzzvfUinng
o4N+LZfQYcTxmdwlkWOrfzCjtHDix6EznPO/LlxTsV+zfTJ/ijTjeXmjQjBAMB0G
A1UdDgQWBBQ64QmG1M8ZwpZ2dEl23OA1xmNjmjAOBgNVHQ8BAf8EBAMCAQYwDwYD
VR0TAQH/BAUwAwEB/zAKBggqhkjOPQQDAwNoADBlAjA2Z6EWCNzklwBBHU6+4WMB
zzuqQhFkoJ2UOQIReVx7Hfpkue4WQrO/isIJxOzksU0CMQDpKmFHjFJKS04YcPbW
RNZu9YO6bVi9JNlWSOrvxKJGgYhqOkbRqZtNyWHa0V1Xahg=
-----END CERTIFICATE-----

# Operating CA: Sectigo
# Issuer: CN=USERTrust RSA Certification Authority O=The USERTRUST Network
# Subject: CN=USERTrust RSA Certification Authority O=The USERTRUST Network
# Label: "USERTrust RSA Certification Authority"
# Serial: 2645093764781058787591871645665788717
# MD5 Fingerprint: 1b:fe:69:d1:91:b7:19:33:a3:72:a8:0f:e1:55:e5:b5
# SHA1 Fingerprint: 2b:8f:1b:57:33:0d:bb:a2:d0:7a:6c:51:f7:0e:e9:0d:da:b9:ad:8e
# SHA256 Fingerprint: e7:93:c9:b0:2f:d8:aa:13:e2:1c:31:22:8a:cc:b0:81:19:64:3b:74:9c:89:89:64:b1:74:6d:46:c3:d4:cb:d2
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----

================
File: storage-bigtable/src/root_ca_certificate.rs
================
pub fn load() -> Result<Certificate, String> {
let pem = match std::env::var("GRPC_DEFAULT_SSL_ROOTS_FILE_PATH").ok() {
⋮----
.and_then(|mut file| {
⋮----
file.read_to_end(&mut pem).map(|_| pem)
⋮----
.map_err(|err| format!("Failed to read {cert_file}: {err}"))?,
⋮----
include_bytes!("pki-goog-roots.pem").to_vec()
⋮----
Ok(Certificate::from_pem(pem))

================
File: storage-bigtable/Cargo.toml
================
[package]
name = "solana-storage-bigtable"
description = "Solana Storage BigTable"
documentation = "https://docs.rs/solana-storage-bigtable"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_storage_bigtable"

[features]
agave-unstable-api = []

[dependencies]
agave-reserved-account-keys = { workspace = true }
backoff = { workspace = true, features = ["tokio"] }
bincode = { workspace = true }
bytes = { workspace = true }
bzip2 = { workspace = true }
enum-iterator = { workspace = true }
flate2 = { workspace = true }
futures = { workspace = true }
goauth = { workspace = true }
http = { workspace = true }
hyper = { workspace = true }
hyper-proxy = { workspace = true }
log = { workspace = true }
# openssl is a dependency of the goauth and smpl_jwt crates, but explicitly
# declare it here as well to activate the "vendored" feature that builds OpenSSL
openssl = { workspace = true, features = ["vendored"] }
prost = { workspace = true }
prost-types = { workspace = true }
serde = { workspace = true }
smpl_jwt = { workspace = true }
solana-clock = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-pubkey = { workspace = true }
solana-serde = { workspace = true }
solana-signature = { workspace = true }
solana-storage-proto = { workspace = true }
solana-time-utils = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true }
tonic = { workspace = true, features = ["tls", "transport"] }
zstd = { workspace = true }

[dev-dependencies]
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-system-transaction = { workspace = true }
solana-transaction-context = { workspace = true }

================
File: storage-bigtable/init-bigtable.sh
================
set -e
instance=solana-ledger
cbt=(
  cbt
  -instance
  "$instance"
)
if [[ -n $BIGTABLE_EMULATOR_HOST ]]; then
  cbt+=(-project emulator)
fi
for table in blocks entries tx tx-by-addr; do
  (
    set -x
    "${cbt[@]}" createtable $table
    "${cbt[@]}" createfamily $table x
    "${cbt[@]}" setgcpolicy $table x maxversions=1
    "${cbt[@]}" setgcpolicy $table x maxage=360d
  )
done

================
File: storage-bigtable/README.md
================
## BigTable Setup

### Development Environment
The Cloud BigTable emulator can be used during development/test.  See
https://cloud.google.com/bigtable/docs/emulator for general setup information.

Process:
1. Run `gcloud beta emulators bigtable start` in the background
2. Run `$(gcloud beta emulators bigtable env-init)` to establish the `BIGTABLE_EMULATOR_HOST` environment variable
3. Run `./init-bigtable.sh` to configure the emulator
4. Develop/test

### Production Environment
Export a standard `GOOGLE_APPLICATION_CREDENTIALS` environment variable to your
service account credentials.  The project should contain a BigTable instance
called `solana-ledger` that has been initialized by running the `./init-bigtable.sh` script.

Depending on what operation mode is required, either the
`https://www.googleapis.com/auth/bigtable.data` or
`https://www.googleapis.com/auth/bigtable.data.readonly` OAuth scope will be
requested using the provided credentials.

#### Forward proxy
Export `BIGTABLE_PROXY` environment variable for the forward proxy as you would
for `HTTP_PROXY`. This will establish a tunnel through the forward proxy for
gRPC traffic (the tunneled traffic will still use TLS as normal).

================
File: storage-proto/proto/confirmed_block.proto
================
syntax = "proto3";

package solana.storage.ConfirmedBlock;

message ConfirmedBlock {
    string previous_blockhash = 1;
    string blockhash = 2;
    uint64 parent_slot = 3;
    repeated ConfirmedTransaction transactions = 4;
    repeated Reward rewards = 5;
    UnixTimestamp block_time = 6;
    BlockHeight block_height = 7;
    NumPartitions num_partitions = 8;
}

message ConfirmedTransaction {
    Transaction transaction = 1;
    TransactionStatusMeta meta = 2;
}

message Transaction {
    repeated bytes signatures = 1;
    Message message = 2;
}

message Message {
    MessageHeader header = 1;
    repeated bytes account_keys = 2;
    bytes recent_blockhash = 3;
    repeated CompiledInstruction instructions = 4;
    bool versioned = 5;
    repeated MessageAddressTableLookup address_table_lookups = 6;
}

message MessageHeader {
    uint32 num_required_signatures = 1;
    uint32 num_readonly_signed_accounts = 2;
    uint32 num_readonly_unsigned_accounts = 3;
}

message MessageAddressTableLookup {
    bytes account_key = 1;
    bytes writable_indexes = 2;
    bytes readonly_indexes = 3;
}

message TransactionStatusMeta {
    TransactionError err = 1;
    uint64 fee = 2;
    repeated uint64 pre_balances = 3;
    repeated uint64 post_balances = 4;
    repeated InnerInstructions inner_instructions = 5;
    bool inner_instructions_none = 10;
    repeated string log_messages = 6;
    bool log_messages_none = 11;
    repeated TokenBalance pre_token_balances = 7;
    repeated TokenBalance post_token_balances = 8;
    repeated Reward rewards = 9;
    repeated bytes loaded_writable_addresses = 12;
    repeated bytes loaded_readonly_addresses = 13;
    ReturnData return_data = 14;
    bool return_data_none = 15;

    // Sum of compute units consumed by all instructions.
    // Available since Solana v1.10.35 / v1.11.6.
    // Set to `None` for txs executed on earlier versions.
    optional uint64 compute_units_consumed = 16;
    // Total transaction cost
    optional uint64 cost_units = 17;
}

message TransactionError {
    bytes err = 1;
}

message InnerInstructions {
    uint32 index = 1;
    repeated InnerInstruction instructions = 2;
}

message InnerInstruction {
    uint32 program_id_index = 1;
    bytes accounts = 2;
    bytes data = 3;

    // Invocation stack height of an inner instruction.
    // Available since Solana v1.14.6
    // Set to `None` for txs executed on earlier versions.
    optional uint32 stack_height = 4;
}

message CompiledInstruction {
    uint32 program_id_index = 1;
    bytes accounts = 2;
    bytes data = 3;
}

message TokenBalance {
    uint32 account_index = 1;
    string mint = 2;
    UiTokenAmount ui_token_amount = 3;
    string owner = 4;
    string program_id = 5;
}

message UiTokenAmount {
    double ui_amount = 1;
    uint32 decimals = 2;
    string amount = 3;
    string ui_amount_string = 4;
}

message ReturnData {
    bytes program_id = 1;
    bytes data = 2;
}

enum RewardType {
    Unspecified = 0;
    Fee = 1;
    Rent = 2;
    Staking = 3;
    Voting = 4;
}

message Reward {
    string pubkey = 1;
    int64 lamports = 2;
    uint64 post_balance = 3;
    RewardType reward_type = 4;
    string commission = 5;
}

message Rewards {
  repeated Reward rewards = 1;
  NumPartitions num_partitions = 2;
}

message UnixTimestamp {
    int64 timestamp = 1;
}

message BlockHeight {
    uint64 block_height = 1;
}

message NumPartitions {
    uint64 num_partitions = 1;
}

================
File: storage-proto/proto/entries.proto
================
syntax = "proto3";

package solana.storage.Entries;

message Entries {
    repeated Entry entries = 1;
}

message Entry {
    uint32 index = 1;
    uint64 num_hashes = 2;
    bytes hash = 3;
    uint64 num_transactions = 4;
    uint32 starting_transaction_index = 5;
}

================
File: storage-proto/proto/transaction_by_addr.proto
================
syntax = "proto3";

package solana.storage.TransactionByAddr;

message TransactionByAddr {
    repeated TransactionByAddrInfo tx_by_addrs = 1;
}

message TransactionByAddrInfo {
    bytes signature = 1;
    TransactionError err = 2;
    uint32 index = 3;
    Memo memo = 4;
    UnixTimestamp block_time = 5;
}

message Memo {
    string memo = 1;
}

message TransactionError {
    TransactionErrorType transaction_error = 1;
    InstructionError instruction_error = 2;
    TransactionDetails transaction_details = 3;
}

enum TransactionErrorType {
    ACCOUNT_IN_USE = 0;
    ACCOUNT_LOADED_TWICE = 1;
    ACCOUNT_NOT_FOUND = 2;
    PROGRAM_ACCOUNT_NOT_FOUND = 3;
    INSUFFICIENT_FUNDS_FOR_FEE = 4;
    INVALID_ACCOUNT_FOR_FEE = 5;
    ALREADY_PROCESSED = 6;
    BLOCKHASH_NOT_FOUND = 7;
    INSTRUCTION_ERROR = 8;
    CALL_CHAIN_TOO_DEEP = 9;
    MISSING_SIGNATURE_FOR_FEE = 10;
    INVALID_ACCOUNT_INDEX = 11;
    SIGNATURE_FAILURE = 12;
    INVALID_PROGRAM_FOR_EXECUTION = 13;
    SANITIZE_FAILURE = 14;
    CLUSTER_MAINTENANCE = 15;
    ACCOUNT_BORROW_OUTSTANDING_TX = 16;
    WOULD_EXCEED_MAX_BLOCK_COST_LIMIT = 17;
    UNSUPPORTED_VERSION = 18;
    INVALID_WRITABLE_ACCOUNT = 19;
    WOULD_EXCEED_MAX_ACCOUNT_COST_LIMIT = 20;
    WOULD_EXCEED_ACCOUNT_DATA_BLOCK_LIMIT = 21;
    TOO_MANY_ACCOUNT_LOCKS = 22;
    ADDRESS_LOOKUP_TABLE_NOT_FOUND = 23;
    INVALID_ADDRESS_LOOKUP_TABLE_OWNER = 24;
    INVALID_ADDRESS_LOOKUP_TABLE_DATA = 25;
    INVALID_ADDRESS_LOOKUP_TABLE_INDEX = 26;
    INVALID_RENT_PAYING_ACCOUNT = 27;
    WOULD_EXCEED_MAX_VOTE_COST_LIMIT = 28;
    WOULD_EXCEED_ACCOUNT_DATA_TOTAL_LIMIT = 29;
    DUPLICATE_INSTRUCTION = 30;
    INSUFFICIENT_FUNDS_FOR_RENT = 31;
    MAX_LOADED_ACCOUNTS_DATA_SIZE_EXCEEDED = 32;
    INVALID_LOADED_ACCOUNTS_DATA_SIZE_LIMIT = 33;
    RESANITIZATION_NEEDED = 34;
    PROGRAM_EXECUTION_TEMPORARILY_RESTRICTED = 35;
    UNBALANCED_TRANSACTION = 36;
    PROGRAM_CACHE_HIT_MAX_LIMIT = 37;
    COMMIT_CANCELLED = 38;
}

message InstructionError {
    uint32 index = 1;
    InstructionErrorType error = 2;
    CustomError custom = 3;
}

message TransactionDetails {
    uint32 index = 1;
}

enum InstructionErrorType {
    GENERIC_ERROR = 0;
    INVALID_ARGUMENT = 1;
    INVALID_INSTRUCTION_DATA = 2;
    INVALID_ACCOUNT_DATA = 3;
    ACCOUNT_DATA_TOO_SMALL = 4;
    INSUFFICIENT_FUNDS = 5;
    INCORRECT_PROGRAM_ID = 6;
    MISSING_REQUIRED_SIGNATURE = 7;
    ACCOUNT_ALREADY_INITIALIZED = 8;
    UNINITIALIZED_ACCOUNT = 9;
    UNBALANCED_INSTRUCTION = 10;
    MODIFIED_PROGRAM_ID = 11;
    EXTERNAL_ACCOUNT_LAMPORT_SPEND = 12;
    EXTERNAL_ACCOUNT_DATA_MODIFIED = 13;
    READONLY_LAMPORT_CHANGE = 14;
    READONLY_DATA_MODIFIED = 15;
    DUPLICATE_ACCOUNT_INDEX = 16;
    EXECUTABLE_MODIFIED = 17;
    RENT_EPOCH_MODIFIED = 18;
    NOT_ENOUGH_ACCOUNT_KEYS = 19;
    ACCOUNT_DATA_SIZE_CHANGED = 20;
    ACCOUNT_NOT_EXECUTABLE = 21;
    ACCOUNT_BORROW_FAILED = 22;
    ACCOUNT_BORROW_OUTSTANDING = 23;
    DUPLICATE_ACCOUNT_OUT_OF_SYNC = 24;
    CUSTOM = 25;
    INVALID_ERROR = 26;
    EXECUTABLE_DATA_MODIFIED = 27;
    EXECUTABLE_LAMPORT_CHANGE = 28;
    EXECUTABLE_ACCOUNT_NOT_RENT_EXEMPT = 29;
    UNSUPPORTED_PROGRAM_ID = 30;
    CALL_DEPTH = 31;
    MISSING_ACCOUNT = 32;
    REENTRANCY_NOT_ALLOWED = 33;
    MAX_SEED_LENGTH_EXCEEDED = 34;
    INVALID_SEEDS = 35;
    INVALID_REALLOC = 36;
    COMPUTATIONAL_BUDGET_EXCEEDED = 37;
    PRIVILEGE_ESCALATION = 38;
    PROGRAM_ENVIRONMENT_SETUP_FAILURE = 39;
    PROGRAM_FAILED_TO_COMPLETE = 40;
    PROGRAM_FAILED_TO_COMPILE = 41;
    IMMUTABLE = 42;
    INCORRECT_AUTHORITY = 43;
    BORSH_IO_ERROR = 44;
    ACCOUNT_NOT_RENT_EXEMPT = 45;
    INVALID_ACCOUNT_OWNER = 46;
    ARITHMETIC_OVERFLOW = 47;
    UNSUPPORTED_SYSVAR = 48;
    ILLEGAL_OWNER = 49;
    MAX_ACCOUNTS_DATA_ALLOCATIONS_EXCEEDED = 50;
    MAX_ACCOUNTS_EXCEEDED = 51;
    MAX_INSTRUCTION_TRACE_LENGTH_EXCEEDED = 52;
    BUILTIN_PROGRAMS_MUST_CONSUME_COMPUTE_UNITS = 53;
}

message UnixTimestamp {
    int64 timestamp = 1;
}

message CustomError {
    uint32 custom = 1;
}

================
File: storage-proto/src/convert.rs
================
pub mod generated {
include!(concat!(
⋮----
pub mod tx_by_addr {
⋮----
pub mod entries {
include!(concat!(env!("OUT_DIR"), "/solana.storage.entries.rs"));
⋮----
fn from(rewards: Vec<Reward>) -> Self {
⋮----
rewards: rewards.into_iter().map(|r| r.into()).collect(),
⋮----
fn from(input: RewardsAndNumPartitions) -> Self {
⋮----
rewards: input.rewards.into_iter().map(|r| r.into()).collect(),
num_partitions: input.num_partitions.map(|n| n.into()),
⋮----
fn from(rewards: generated::Rewards) -> Self {
rewards.rewards.into_iter().map(|r| r.into()).collect()
⋮----
rewards.rewards.into_iter().map(|r| r.into()).collect(),
⋮----
.map(|generated::NumPartitions { num_partitions }| num_partitions),
⋮----
fn from(rewards: StoredExtendedRewards) -> Self {
⋮----
.into_iter()
.map(|r| {
let r: Reward = r.into();
r.into()
⋮----
.collect(),
⋮----
.collect()
⋮----
fn from(reward: Reward) -> Self {
⋮----
commission: reward.commission.map(|c| c.to_string()).unwrap_or_default(),
⋮----
fn from(reward: generated::Reward) -> Self {
⋮----
1 => Some(RewardType::Fee),
2 => Some(RewardType::Rent),
3 => Some(RewardType::Staking),
4 => Some(RewardType::Voting),
⋮----
commission: reward.commission.parse::<u8>().ok(),
⋮----
fn from(num_partitions: u64) -> Self {
⋮----
fn from(confirmed_block: VersionedConfirmedBlock) -> Self {
⋮----
transactions: transactions.into_iter().map(|tx| tx.into()).collect(),
⋮----
num_partitions: num_partitions.map(Into::into),
block_time: block_time.map(|timestamp| generated::UnixTimestamp { timestamp }),
block_height: block_height.map(|block_height| generated::BlockHeight { block_height }),
⋮----
type Error = bincode::Error;
fn try_from(
⋮----
Ok(Self {
⋮----
.map(|tx| tx.try_into())
⋮----
block_time: block_time.map(|generated::UnixTimestamp { timestamp }| timestamp),
block_height: block_height.map(|generated::BlockHeight { block_height }| block_height),
⋮----
fn from(tx_with_meta: TransactionWithStatusMeta) -> Self {
⋮----
transaction: Some(generated::Transaction::from(transaction)),
⋮----
fn from(value: VersionedTransactionWithStatusMeta) -> Self {
⋮----
transaction: Some(value.transaction.into()),
meta: Some(value.meta.into()),
⋮----
fn try_from(value: generated::ConfirmedTransaction) -> std::result::Result<Self, Self::Error> {
let meta = value.meta.map(|meta| meta.try_into()).transpose()?;
let transaction = value.transaction.expect("transaction is required").into();
Ok(match meta {
⋮----
.into_legacy_transaction()
.expect("meta is required for versioned transactions"),
⋮----
fn from(value: Transaction) -> Self {
⋮----
.map(|signature| <Signature as AsRef<[u8]>>::as_ref(&signature).into())
⋮----
message: Some(value.message.into()),
⋮----
fn from(value: VersionedTransaction) -> Self {
⋮----
fn from(value: generated::Transaction) -> Self {
⋮----
.map(Signature::try_from)
⋮----
.unwrap(),
message: value.message.expect("message is required").into(),
⋮----
fn from(value: TransactionError) -> Self {
⋮----
fn from(value: generated::TransactionError) -> Self {
let stored_error = StoredTransactionError(value.err);
stored_error.into()
⋮----
fn from(message: LegacyMessage) -> Self {
⋮----
header: Some(message.header.into()),
⋮----
.iter()
.map(|key| <Pubkey as AsRef<[u8]>>::as_ref(key).into())
⋮----
recent_blockhash: message.recent_blockhash.to_bytes().into(),
⋮----
.map(|ix| ix.into())
⋮----
address_table_lookups: vec![],
⋮----
fn from(message: VersionedMessage) -> Self {
⋮----
.map(|lookup| lookup.into())
⋮----
fn from(value: generated::Message) -> Self {
let header = value.header.expect("header is required").into();
⋮----
.map(|key| Pubkey::try_from(key).unwrap())
.collect();
⋮----
.map(Hash::new_from_array)
.unwrap();
let instructions = value.instructions.into_iter().map(|ix| ix.into()).collect();
⋮----
fn from(value: MessageHeader) -> Self {
⋮----
fn from(value: generated::MessageHeader) -> Self {
⋮----
fn from(value: TransactionStatusMeta) -> Self {
⋮----
Err(err) => Some(generated::TransactionError {
err: bincode::serialize(&err).expect("transaction error to serialize to bytes"),
⋮----
let inner_instructions_none = inner_instructions.is_none();
⋮----
.unwrap_or_default()
⋮----
.map(|ii| ii.into())
⋮----
let log_messages_none = log_messages.is_none();
let log_messages = log_messages.unwrap_or_default();
⋮----
.map(|balance| balance.into())
⋮----
.map(|reward| reward.into())
⋮----
.map(|key| <Pubkey as AsRef<[u8]>>::as_ref(&key).into())
⋮----
let return_data_none = return_data.is_none();
let return_data = return_data.map(|return_data| return_data.into());
⋮----
fn from(meta: StoredTransactionStatusMeta) -> Self {
let meta: TransactionStatusMeta = meta.into();
meta.into()
⋮----
fn try_from(value: generated::TransactionStatusMeta) -> std::result::Result<Self, Self::Error> {
⋮----
None => Ok(()),
Some(tx_error) => Err(bincode::deserialize(&tx_error.err)?),
⋮----
Some(
⋮----
.map(|inner| inner.into())
⋮----
Some(log_messages)
⋮----
let pre_token_balances = Some(
⋮----
let post_token_balances = Some(
⋮----
let rewards = Some(rewards.into_iter().map(|reward| reward.into()).collect());
⋮----
.map(Pubkey::try_from)
⋮----
.map_err(|err| {
let err = format!("Invalid writable address: {err:?}");
⋮----
let err = format!("Invalid readonly address: {err:?}");
⋮----
return_data.map(|return_data| return_data.into())
⋮----
fn from(value: InnerInstructions) -> Self {
⋮----
instructions: value.instructions.into_iter().map(|i| i.into()).collect(),
⋮----
fn from(value: generated::InnerInstructions) -> Self {
⋮----
fn from(value: TransactionTokenBalance) -> Self {
⋮----
ui_token_amount: Some(generated::UiTokenAmount {
ui_amount: value.ui_token_amount.ui_amount.unwrap_or_default(),
⋮----
fn from(value: generated::TokenBalance) -> Self {
let ui_token_amount = value.ui_token_amount.unwrap_or_default();
⋮----
ui_amount: if (ui_token_amount.ui_amount - f64::default()).abs() > f64::EPSILON {
Some(ui_token_amount.ui_amount)
⋮----
amount: ui_token_amount.amount.clone(),
ui_amount_string: if !ui_token_amount.ui_amount_string.is_empty() {
⋮----
real_number_string_trimmed(
u64::from_str(&ui_token_amount.amount).unwrap_or_default(),
⋮----
fn from(lookup: MessageAddressTableLookup) -> Self {
⋮----
account_key: <Pubkey as AsRef<[u8]>>::as_ref(&lookup.account_key).into(),
⋮----
fn from(value: generated::MessageAddressTableLookup) -> Self {
⋮----
account_key: Pubkey::try_from(value.account_key).unwrap(),
⋮----
fn from(value: TransactionReturnData) -> Self {
⋮----
program_id: <Pubkey as AsRef<[u8]>>::as_ref(&value.program_id).into(),
⋮----
fn from(value: generated::ReturnData) -> Self {
⋮----
program_id: Pubkey::try_from(value.program_id).unwrap(),
⋮----
fn from(value: CompiledInstruction) -> Self {
⋮----
fn from(value: generated::CompiledInstruction) -> Self {
⋮----
fn from(value: InnerInstruction) -> Self {
⋮----
fn from(value: generated::InnerInstruction) -> Self {
⋮----
type Error = &'static str;
fn try_from(transaction_error: tx_by_addr::TransactionError) -> Result<Self, Self::Error> {
⋮----
return Ok(TransactionError::InstructionError(
⋮----
_ => return Err("Invalid InstructionError"),
⋮----
return Ok(TransactionError::DuplicateInstruction(
⋮----
return Ok(TransactionError::InsufficientFundsForRent {
⋮----
return Ok(TransactionError::ProgramExecutionTemporarilyRestricted {
⋮----
Ok(match transaction_error.transaction_error {
⋮----
_ => return Err("Invalid TransactionError"),
⋮----
fn from(transaction_error: TransactionError) -> Self {
⋮----
Some(tx_by_addr::InstructionError {
⋮----
Some(tx_by_addr::CustomError { custom: *custom })
⋮----
Some(tx_by_addr::TransactionDetails {
⋮----
fn from(by_addr: TransactionByAddrInfo) -> Self {
⋮----
signature: <Signature as AsRef<[u8]>>::as_ref(&signature).into(),
err: err.map(|e| e.into()),
⋮----
memo: memo.map(|memo| tx_by_addr::Memo { memo }),
block_time: block_time.map(|timestamp| tx_by_addr::UnixTimestamp { timestamp }),
⋮----
.map(|err| err.try_into())
.transpose()?;
⋮----
.map_err(|_| "Invalid Signature")?,
⋮----
.map(|tx_by_addr::Memo { memo }| memo),
⋮----
.map(|tx_by_addr::UnixTimestamp { timestamp }| timestamp),
⋮----
fn try_from(collection: tx_by_addr::TransactionByAddr) -> Result<Self, Self::Error> {
⋮----
.map(|tx_by_addr| tx_by_addr.try_into())
⋮----
fn from((index, entry_summary): (usize, EntrySummary)) -> Self {
⋮----
hash: entry_summary.hash.as_ref().into(),
⋮----
fn from(entry: entries::Entry) -> Self {
⋮----
mod test {
⋮----
fn test_reward_type_encode() {
⋮----
pubkey: "invalid".to_string(),
⋮----
let gen_reward: generated::Reward = reward.clone().into();
assert_eq!(reward, gen_reward.into());
reward.reward_type = Some(RewardType::Fee);
⋮----
reward.reward_type = Some(RewardType::Rent);
⋮----
reward.reward_type = Some(RewardType::Voting);
⋮----
reward.reward_type = Some(RewardType::Staking);
⋮----
fn test_transaction_by_addr_encode() {
⋮----
.into_vec()
⋮----
.unwrap()
⋮----
memo: Some("string".to_string()),
block_time: Some(1610674861)
⋮----
let tx_by_addr_transaction_info: tx_by_addr::TransactionByAddrInfo = info.clone().into();
assert_eq!(info, tx_by_addr_transaction_info.try_into().unwrap());
⋮----
fn test_transaction_error_encode() {
⋮----
transaction_error.clone().into();
assert_eq!(
⋮----
fn test_error_enums() {
⋮----
transaction_details: Some(tx_by_addr::TransactionDetails {
⋮----
.clone()
.try_into()
.unwrap_or_else(|_| panic!("{error:?} conversion implemented?"));
assert_eq!(tx_by_addr_error, transaction_error.into());
⋮----
instruction_error: Some(tx_by_addr::InstructionError {
⋮----
.unwrap_or_else(|_| panic!("{ix_error:?} conversion implemented?"));
⋮----
custom: Some(tx_by_addr::CustomError {
⋮----
tx_by_addr_error.clone().try_into().unwrap();

================
File: storage-proto/src/lib.rs
================
pub mod convert;
pub type StoredExtendedRewards = Vec<StoredExtendedReward>;
⋮----
pub struct StoredExtendedReward {
⋮----
fn from(value: StoredExtendedReward) -> Self {
⋮----
fn from(value: Reward) -> Self {
⋮----
pub struct StoredTokenAmount {
⋮----
fn from(value: StoredTokenAmount) -> Self {
⋮----
real_number_string_trimmed(u64::from_str(&amount).unwrap_or(0), decimals);
⋮----
ui_amount: Some(ui_amount),
⋮----
fn from(value: UiTokenAmount) -> Self {
⋮----
ui_amount: ui_amount.unwrap_or(0.0),
⋮----
struct StoredTransactionError(Vec<u8>);
⋮----
fn from(value: StoredTransactionError) -> Self {
⋮----
bincode::deserialize(&bytes).expect("transaction error to deserialize from bytes")
⋮----
fn from(value: TransactionError) -> Self {
let bytes = bincode::serialize(&value).expect("transaction error to serialize to bytes");
StoredTransactionError(bytes)
⋮----
pub struct StoredTransactionTokenBalance {
⋮----
fn from(value: StoredTransactionTokenBalance) -> Self {
⋮----
ui_token_amount: ui_token_amount.into(),
⋮----
fn from(value: TransactionTokenBalance) -> Self {
⋮----
pub struct StoredTransactionStatusMeta {
⋮----
fn from(value: StoredTransactionStatusMeta) -> Self {
⋮----
.map(|balances| balances.into_iter().map(|balance| balance.into()).collect()),
⋮----
.map(|rewards| rewards.into_iter().map(|reward| reward.into()).collect()),
⋮----
type Error = bincode::Error;
fn try_from(value: TransactionStatusMeta) -> std::result::Result<Self, Self::Error> {
⋮----
if !loaded_addresses.is_empty() {
return Err(
bincode::ErrorKind::Custom("Bincode serialization is deprecated".into()).into(),
⋮----
Ok(Self {
⋮----
mod tests {
⋮----
fn test_serialize_transaction_error_to_stored_transaction_error_round_trip(
⋮----
let serialized: StoredTransactionError = err.clone().into();
let deserialized: TransactionError = serialized.into();
assert_eq!(deserialized, err);
⋮----
fn test_deserialize_stored_transaction_error(
⋮----
let stored_transaction = StoredTransactionError(stored_bytes);
let deserialized: TransactionError = stored_transaction.into();
assert_eq!(deserialized, expected_transaction_error);
⋮----
fn test_seserialize_stored_transaction_error(
⋮----
let StoredTransactionError(serialized_bytes) = transaction_error.into();
assert_eq!(serialized_bytes, expected_serialized_bytes);

================
File: storage-proto/build.rs
================
fn main() -> Result<(), std::io::Error> {
⋮----
if std::env::var(PROTOC_ENVAR).is_err() {
⋮----
let proto = proto_base_path.join(proto_file);
println!("cargo:rerun-if-changed={}", proto.display());
protos.push(proto);
⋮----
.build_client(true)
.build_server(false)
.type_attribute(
⋮----
.compile(&protos, &[proto_base_path])

================
File: storage-proto/Cargo.toml
================
[package]
name = "solana-storage-proto"
description = "Solana Storage Protobuf Definitions"
documentation = "https://docs.rs/solana-storage-proto"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_storage_proto"

[features]
agave-unstable-api = []

[dependencies]
bincode = { workspace = true }
bs58 = { workspace = true }
prost = { workspace = true }
serde = { workspace = true }
solana-account-decoder = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-message = { workspace = true }
solana-pubkey = { workspace = true }
solana-serde = { workspace = true }
solana-signature = { workspace = true, features = ["std"] }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true, features = ["serde"] }
solana-transaction-error = { workspace = true }
solana-transaction-status = { workspace = true }

[build-dependencies]
tonic-build = { workspace = true }

# windows users should install the protobuf compiler manually and set the PROTOC
# envar to point to the installed binary
[target."cfg(not(windows))".build-dependencies]
protobuf-src = { workspace = true }

[dev-dependencies]
enum-iterator = { workspace = true }
test-case = { workspace = true }

================
File: storage-proto/README.md
================
# Storage Protobufs

The `solana-storage-proto` structs used in `src/convert.rs` and elsewhere are
auto-generated from protobuf definitions on build. To update these structs,
simply make the desired edits to `proto/*.proto` files.

================
File: streamer/examples/swqos.rs
================
fn parse_duration(arg: &str) -> Result<std::time::Duration, std::num::ParseFloatError> {
let seconds = arg.parse()?;
Ok(std::time::Duration::from_secs_f64(seconds))
⋮----
pub fn load_staked_nodes_overrides(path: &String) -> anyhow::Result<HashMap<Pubkey, u64>> {
debug!("Loading staked nodes overrides configuration from {path}");
if Path::new(&path).exists() {
⋮----
for (line_num, line) in reader.lines().enumerate() {
⋮----
let parts: Vec<&str> = line.split_whitespace().collect();
if parts.len() != 2 {
⋮----
.map_err(|_| anyhow::anyhow!("invalid pubkey at line {line_num}"))?;
⋮----
.parse()
.map_err(|_| anyhow::anyhow!("invalid number at line {line_num}"))?;
map.insert(pubkey, value.saturating_mul(LAMPORTS_PER_SOL));
⋮----
Ok(map)
⋮----
struct Cli {
⋮----
async fn main() -> anyhow::Result<()> {
⋮----
let socket = bind_to_with_config(
cli.bind_to.ip(),
cli.bind_to.port(),
⋮----
.expect("should bind");
let (sender, receiver) = bounded(1024);
⋮----
load_staked_nodes_overrides(&cli.stake_amounts)?,
⋮----
[socket.try_clone()?],
⋮----
cancel.clone(),
⋮----
info!("Server listening on {}", socket.local_addr()?);
let path = cli.log_file.clone();
⋮----
NaiveDate::from_ymd_opt(2020, 3, 16).unwrap(),
⋮----
info!("Logfile in {}", &path);
⋮----
let now = Utc::now().naive_utc();
let delta_time = (now - solana_epoch).num_microseconds().unwrap() as u64;
for pkt in batch.iter() {
let pkt = pkt.to_bytes_packet();
if pkt.buffer().len() < 32 {
⋮----
let pubkey: [u8; 32] = pkt.buffer()[0..32].try_into()?;
logfile.write_all(&pubkey)?;
let pkt_len = pkt.buffer().len() as u64;
logfile.write_all(&pkt_len.to_ne_bytes())?;
logfile.write_all(&delta_time.to_ne_bytes())?;
⋮----
debug!("{pubkey}: {pkt_len} bytes");
⋮----
info!("Server captured {sum} TXs");
logfile.flush()?;
Ok(())
⋮----
info!("Server terminating");
cancel.cancel();
drop(endpoints);
stats.report("final_stats");

================
File: streamer/src/nonblocking/connection_rate_limiter.rs
================
pub struct ConnectionRateLimiter {
⋮----
impl ConnectionRateLimiter {
pub fn new(limit_per_minute: u64, max_burst: u64, num_shards: usize) -> Self {
⋮----
pub fn is_allowed(&self, ip: &IpAddr) -> bool {
match self.limiter.current_tokens(ip) {
⋮----
pub fn register_connection(&self, ip: &IpAddr) -> bool {
if self.limiter.consume_tokens(*ip, 1).is_ok() {
debug!("Request from IP {ip:?} allowed");
⋮----
debug!("Request from IP {ip:?} blocked");
⋮----
pub mod test {
⋮----
async fn test_connection_rate_limiter() {
⋮----
assert!(limiter.is_allowed(&ip1));
assert!(limiter.register_connection(&ip1));
⋮----
assert!(!limiter.is_allowed(&ip1));
assert!(!limiter.register_connection(&ip1));
⋮----
assert!(
⋮----
assert!(limiter.register_connection(&ip2));
⋮----
assert!(limiter.is_allowed(&ip2));
⋮----
assert!(!limiter.is_allowed(&ip2));

================
File: streamer/src/nonblocking/mod.rs
================
pub mod connection_rate_limiter;
pub mod qos;
pub mod quic;
⋮----
pub mod recvmmsg;
pub mod sendmmsg;
pub mod simple_qos;
mod stream_throttle;
pub mod swqos;
⋮----
pub mod testing_utilities;

================
File: streamer/src/nonblocking/qos.rs
================
pub(crate) trait ConnectionContext: Clone + Send + Sync {
⋮----
pub(crate) trait QosController<C: ConnectionContext> {
⋮----
pub(crate) trait OpaqueStreamerCounter: Send + Sync + 'static {}
⋮----
pub(crate) struct NullStreamerCounter;
⋮----
impl OpaqueStreamerCounter for NullStreamerCounter {}

================
File: streamer/src/nonblocking/quic.rs
================
struct PacketAccumulator {
⋮----
impl PacketAccumulator {
fn new(meta: Meta) -> Self {
⋮----
pub enum ConnectionPeerType {
⋮----
impl ConnectionPeerType {
pub(crate) fn is_staked(&self) -> bool {
matches!(self, ConnectionPeerType::Staked(_))
⋮----
pub struct SpawnNonBlockingServerResult {
⋮----
pub(crate) fn spawn_server<Q, C>(
⋮----
let sockets: Vec<_> = sockets.into_iter().collect();
info!("Start {name} quic server on {sockets:?}");
let (config, _) = configure_server(keypair)?;
⋮----
.into_iter()
.map(|sock| {
⋮----
Some(config.clone()),
⋮----
.map_err(QuicServerError::EndpointFailed)
⋮----
let max_concurrent_connections = qos.max_concurrent_connections();
⋮----
let endpoints = endpoints.clone();
let stats = stats.clone();
⋮----
let tasks = run_server(
⋮----
endpoints.clone(),
⋮----
stats.clone(),
⋮----
tasks.close();
tasks.wait().await;
⋮----
Ok(SpawnNonBlockingServerResult {
⋮----
/// struct ease tracking connections of all stages, so that we do not have to
/// litter the code with open connection tracking. This is added into the
⋮----
/// litter the code with open connection tracking. This is added into the
/// connection table as part of the ConnectionEntry. The reference is auto
⋮----
/// connection table as part of the ConnectionEntry. The reference is auto
/// reduced when it is dropped.
⋮----
/// reduced when it is dropped.
pub struct ClientConnectionTracker {
⋮----
pub struct ClientConnectionTracker {
⋮----
/// This is required by ConnectionEntry for supporting debug format.
impl fmt::Debug for ClientConnectionTracker {
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
f.debug_struct("StreamerClientConnection")
.field(
⋮----
&self.stats.open_connections.load(Ordering::Relaxed),
⋮----
.finish()
⋮----
impl Drop for ClientConnectionTracker {
fn drop(&mut self) {
self.stats.open_connections.fetch_sub(1, Ordering::Relaxed);
⋮----
impl ClientConnectionTracker {
fn new(stats: Arc<StreamerStats>, max_concurrent_connections: usize) -> Result<Self, ()> {
let open_connections = stats.open_connections.fetch_add(1, Ordering::Relaxed);
⋮----
stats.open_connections.fetch_sub(1, Ordering::Relaxed);
debug!(
⋮----
return Err(());
⋮----
Ok(Self { stats })
⋮----
async fn run_server<Q, C>(
⋮----
// allow for 10x burst to make sure we can accommodate legitimate
// bursts from container environments running multiple pods on same IP
⋮----
quic_server_params.num_threads.get() * 2,
⋮----
debug!("spawn quic server");
⋮----
.store(endpoints.len(), Ordering::Relaxed);
⋮----
.iter()
.enumerate()
.map(|(i, incoming)| {
⋮----
accept: incoming.accept(),
⋮----
let timeout_connection = select! {
⋮----
// we can't really get here - we never poll an empty FuturesUnordered
⋮----
if last_datapoint.elapsed().as_secs() >= 5 {
stats.report(name);
⋮----
.fetch_add(1, Ordering::Relaxed);
if overall_connection_rate_limiter.current_tokens() == 0 {
⋮----
incoming.ignore();
⋮----
if !rate_limiter.is_allowed(&incoming.remote_address().ip()) {
⋮----
ClientConnectionTracker::new(stats.clone(), qos.max_concurrent_connections())
⋮----
incoming.refuse();
⋮----
let connecting = incoming.accept();
⋮----
let rate_limiter = rate_limiter.clone();
let overall_connection_rate_limiter = overall_connection_rate_limiter.clone();
tasks.spawn(setup_connection(
⋮----
packet_batch_sender.clone(),
⋮----
quic_server_params.clone(),
qos.clone(),
tasks.clone(),
⋮----
.fetch_sub(1, Ordering::Relaxed);
debug!("Incoming::accept(): error {err:?}");
⋮----
debug!("accept(): Timed out waiting for connection");
⋮----
pub fn get_remote_pubkey(connection: &Connection) -> Option<Pubkey> {
⋮----
.peer_identity()?
⋮----
.ok()
.filter(|certs| certs.len() == 1)?
.first()
.and_then(get_pubkey_from_tls_certificate)
⋮----
pub fn get_connection_stake(
⋮----
let pubkey = get_remote_pubkey(connection)?;
debug!("Peer public key is {pubkey:?}");
let staked_nodes = staked_nodes.read().unwrap();
Some((
⋮----
staked_nodes.get_node_stake(&pubkey)?,
staked_nodes.total_stake(),
⋮----
pub(crate) enum ConnectionHandlerError {
⋮----
pub(crate) fn update_open_connections_stat<S: OpaqueStreamerCounter>(
⋮----
if connection_table.is_staked() {
⋮----
.store(connection_table.table_size(), Ordering::Relaxed);
⋮----
.fetch_max(connection_table.table_size(), Ordering::Relaxed);
⋮----
async fn setup_connection<Q, C>(
⋮----
let from = connecting.remote_address();
let res = timeout(QUIC_CONNECTION_HANDSHAKE_TIMEOUT, connecting).await;
⋮----
debug!("Got a connection {from:?}");
if !rate_limiter.register_connection(&from.ip()) {
debug!("Reject connection from {from:?} -- rate limiting exceeded");
⋮----
new_connection.close(
CONNECTION_CLOSE_CODE_DISALLOWED.into(),
⋮----
if overall_connection_rate_limiter.consume_tokens(1).is_err() {
⋮----
stats.total_new_connections.fetch_add(1, Ordering::Relaxed);
let mut conn_context = qos.build_connection_context(&new_connection);
⋮----
.try_add_connection(
⋮----
tasks.spawn(handle_connection(
packet_sender.clone(),
⋮----
conn_context.clone(),
⋮----
handle_connection_error(e, &stats, from);
⋮----
fn handle_connection_error(e: quinn::ConnectionError, stats: &StreamerStats, from: SocketAddr) {
debug!("error: {e:?} from: {from:?}");
stats.connection_setup_error.fetch_add(1, Ordering::Relaxed);
⋮----
fn track_streamer_fetch_packet_performance(
⋮----
if packet_perf_measure.is_empty() {
⋮----
let mut process_sampled_packets_us_hist = stats.process_sampled_packets_us_hist.lock().unwrap();
⋮----
let duration = now.duration_since(*start_time);
⋮----
.increment(duration.as_micros() as u64)
.unwrap();
⋮----
drop(process_sampled_packets_us_hist);
measure.stop();
⋮----
.fetch_add(measure.as_us(), Ordering::Relaxed);
⋮----
async fn handle_connection<Q, C>(
⋮----
let peer_type = context.peer_type();
let remote_addr = connection.remote_address();
⋮----
stats.total_connections.fetch_add(1, Ordering::Relaxed);
⋮----
// Wait for new streams. If the peer is disconnected we get a cancellation signal and stop
// the connection task.
let mut stream = select! {
⋮----
qos.on_new_stream(&context).await;
qos.on_stream_accepted(&context);
stats.active_streams.fetch_add(1, Ordering::Relaxed);
stats.total_new_streams.fetch_add(1, Ordering::Relaxed);
⋮----
meta.set_socket_addr(&remote_addr);
meta.set_from_staked_node(matches!(peer_type, ConnectionPeerType::Staked(_)));
if let Some(pubkey) = context.remote_pubkey() {
meta.set_remote_pubkey(pubkey);
⋮----
// Virtually all small transactions will fit in 1 chunk. Larger transactions will fit in 1
// or 2 chunks if the first chunk starts towards the end of a datagram. A small number of
// transaction will have other protocol frames inserted in the middle. Empirically it's been
⋮----
Ok(Ok(chunk)) => chunk.unwrap_or(0),
⋮----
debug!("Received stream error: {e:?}");
⋮----
debug!("Timeout in receiving on stream");
⋮----
match handle_chunks(
chunks.iter().take(n_chunks).cloned(),
⋮----
qos.on_stream_finished(&context);
⋮----
connection.close(
CONNECTION_CLOSE_CODE_INVALID_STREAM.into(),
⋮----
stats.active_streams.fetch_sub(1, Ordering::Relaxed);
qos.on_stream_error(&context);
⋮----
qos.on_stream_closed(&context);
⋮----
let removed_connection_count = qos.remove_connection(&context, connection).await;
⋮----
.fetch_add(removed_connection_count, Ordering::Relaxed);
⋮----
stats.total_connections.fetch_sub(1, Ordering::Relaxed);
⋮----
enum StreamState {
// Stream is not finished, keep receiving chunks
⋮----
// Stream is finished
⋮----
// Handle the chunks received from the stream. If the stream is finished, send the packet to the
// packet sender.
//
// Returns Err(()) if the stream is invalid.
fn handle_chunks(
⋮----
let n_chunks = chunks.len();
⋮----
accum.meta.size += chunk.len();
⋮----
// The stream window size is set to PACKET_DATA_SIZE, so one individual chunk can
// never exceed this size. A peer can send two chunks that together exceed the size
// tho, in which case we report the error.
stats.invalid_stream_size.fetch_add(1, Ordering::Relaxed);
debug!("invalid stream size {}", accum.meta.size);
⋮----
accum.chunks.push(chunk);
if peer_type.is_staked() {
⋮----
// n_chunks == 0 marks the end of a stream
⋮----
return Ok(StreamState::Receiving);
⋮----
if accum.chunks.is_empty() {
debug!("stream is empty");
⋮----
// done receiving chunks
⋮----
// 86% of transactions/packets come in one chunk. In that case,
// we can just move the chunk to the `Packet` and no copy is
// made.
// 14% of them come in multiple chunks. In that case, we copy
// them into one `Bytes` buffer. We make a copy once, with
// intention to not do it again.
let mut packet = if accum.chunks.len() == 1 {
⋮----
accum.chunks.pop().expect("expected one chunk"),
accum.meta.clone(),
⋮----
let size: usize = accum.chunks.iter().map(Bytes::len).sum();
⋮----
buf.put_slice(chunk);
⋮----
BytesPacket::new(buf.freeze(), accum.meta.clone())
⋮----
let packet_size = packet.meta().size;
⋮----
if let Some(signature) = signature_if_should_track_packet(&packet).ok().flatten() {
packet_perf_measure = Some((*signature, accum.start_time));
// we set the PERF_TRACK_PACKET on
packet.meta_mut().set_track_performance(true);
⋮----
if let Err(err) = packet_sender.try_send(packet_batch) {
⋮----
trace!("packet batch send error {err:?}");
⋮----
track_streamer_fetch_packet_performance(core::array::from_ref(ppm), stats);
⋮----
.fetch_add(packet_size, Ordering::Relaxed);
⋮----
trace!("sent {bytes_sent} byte packet for batching");
⋮----
Ok(StreamState::Finished)
⋮----
struct ConnectionEntry<S: OpaqueStreamerCounter> {
⋮----
// We do not explicitly use it, but its drop is triggered when ConnectionEntry is dropped.
⋮----
fn new(
⋮----
fn last_update(&self) -> u64 {
self.last_update.load(Ordering::Relaxed)
⋮----
fn stake(&self) -> u64 {
⋮----
impl<S: OpaqueStreamerCounter> Drop for ConnectionEntry<S> {
⋮----
if let Some(conn) = self.connection.take() {
conn.close(
CONNECTION_CLOSE_CODE_DROPPED_ENTRY.into(),
⋮----
self.cancel.cancel();
⋮----
pub(crate) enum ConnectionTableKey {
⋮----
impl ConnectionTableKey {
pub(crate) fn new(ip: IpAddr, maybe_pubkey: Option<Pubkey>) -> Self {
maybe_pubkey.map_or(ConnectionTableKey::IP(ip), |pubkey| {
⋮----
pub(crate) enum ConnectionTableType {
⋮----
// Map of IP to list of connection entries
pub(crate) struct ConnectionTable<S: OpaqueStreamerCounter> {
⋮----
/// Prune the connection which has the oldest update
///
⋮----
///
/// Return number pruned
⋮----
/// Return number pruned
impl<S: OpaqueStreamerCounter> ConnectionTable<S> {
pub(crate) fn new(table_type: ConnectionTableType, cancel: CancellationToken) -> Self {
⋮----
fn table_size(&self) -> usize {
⋮----
fn is_staked(&self) -> bool {
matches!(self.table_type, ConnectionTableType::Staked)
⋮----
pub(crate) fn prune_oldest(&mut self, max_size: usize) -> usize {
⋮----
connections.iter().map(ConnectionEntry::last_update).min()
⋮----
while self.total_size.saturating_sub(num_pruned) > max_size {
match self.table.values().enumerate().min_by_key(key) {
⋮----
num_pruned += connections.len();
self.table.swap_remove_index(index);
⋮----
self.total_size = self.total_size.saturating_sub(num_pruned);
⋮----
// Randomly selects sample_size many connections, evicts the one with the
// lowest stake, and returns the number of pruned connections.
// If the stakes of all the sampled connections are higher than the
// threshold_stake, rejects the pruning attempt, and returns 0.
pub(crate) fn prune_random(&mut self, sample_size: usize, threshold_stake: u64) -> usize {
let num_pruned = std::iter::once(self.table.len())
.filter(|&size| size > 0)
.flat_map(|size| {
let mut rng = rng();
repeat_with(move || rng.random_range(0..size))
⋮----
.map(|index| {
let connection = self.table[index].first();
let stake = connection.map(|connection: &ConnectionEntry<S>| connection.stake());
⋮----
.take(sample_size)
.min_by_key(|&(_, stake)| stake)
.filter(|&(_, stake)| stake < Some(threshold_stake))
.and_then(|(index, _)| self.table.swap_remove_index(index))
.map(|(_, connections)| connections.len())
.unwrap_or_default();
⋮----
pub(crate) fn try_add_connection<F: FnOnce() -> Arc<S>>(
⋮----
let connection_entry = self.table.entry(key).or_default();
⋮----
.len()
.checked_add(1)
.map(|c| c <= max_connections_per_peer)
.unwrap_or(false);
⋮----
let cancel = self.cancel.child_token();
⋮----
.map(|entry| entry.stream_counter.clone())
.unwrap_or_else(stream_counter_factory);
connection_entry.push(ConnectionEntry::new(
cancel.clone(),
⋮----
last_update.clone(),
⋮----
stream_counter.clone(),
⋮----
Some((last_update, cancel, stream_counter))
⋮----
CONNECTION_CLOSE_CODE_TOO_MANY.into(),
⋮----
// Returns number of connections that were removed
pub(crate) fn remove_connection(
⋮----
if let Entry::Occupied(mut e) = self.table.entry(key) {
let e_ref = e.get_mut();
let old_size = e_ref.len();
e_ref.retain(|connection_entry| {
// Retain the connection entry if the port is different, or if the connection's
⋮----
.as_ref()
.and_then(|connection| (connection.stable_id() != stable_id).then_some(0))
.is_some()
⋮----
let new_size = e_ref.len();
if e_ref.is_empty() {
e.swap_remove_entry();
⋮----
let connections_removed = old_size.saturating_sub(new_size);
self.total_size = self.total_size.saturating_sub(connections_removed);
⋮----
struct EndpointAccept<'a> {
⋮----
impl Future for EndpointAccept<'_> {
type Output = (Option<quinn::Incoming>, usize);
fn poll(self: Pin<&mut Self>, cx: &mut std::task::Context) -> Poll<Self::Output> {
⋮----
// Safety:
// self is pinned and accept is a field so it can't get moved out. See safety docs of
unsafe { self.map_unchecked_mut(|this| &mut this.accept) }
.poll(cx)
.map(|r| (r, i))
⋮----
pub mod test {
⋮----
pub async fn check_timeout(receiver: Receiver<PacketBatch>, server_address: SocketAddr) {
let conn1 = make_client_endpoint(&server_address, None).await;
⋮----
let mut s1 = conn1.open_uni().await.unwrap();
s1.write_all(&[0u8]).await.unwrap();
s1.finish().unwrap();
info!("done {i}");
sleep(Duration::from_millis(1000)).await;
⋮----
if let Ok(_x) = receiver.try_recv() {
⋮----
info!("got {received}");
⋮----
sleep(Duration::from_millis(500)).await;
⋮----
pub async fn check_block_multiple_connections(server_address: SocketAddr) {
⋮----
let conn2 = make_client_endpoint(&server_address, None).await;
⋮----
let s2 = conn2.open_uni().await;
⋮----
let data = vec![1u8; PACKET_DATA_SIZE * 2];
s2.write_all(&data)
⋮----
.expect_err("shouldn't be able to open 2 connections");
⋮----
assert_matches!(s2, Err(quinn::ConnectionError::ApplicationClosed(_)));
⋮----
pub async fn check_multiple_writes(
⋮----
let conn1 = Arc::new(make_client_endpoint(&server_address, client_keypair).await);
⋮----
check_received_packets(receiver, num_expected_packets, num_bytes).await;
⋮----
pub async fn check_multiple_packets(
⋮----
let packet = vec![1u8; num_bytes];
⋮----
s1.write_all(&packet).await.unwrap();
⋮----
async fn check_received_packets(
⋮----
let mut all_packets = vec![];
⋮----
while now.elapsed().as_secs() < 5 {
if let Ok(packets) = receiver.try_recv() {
total_packets += packets.len();
all_packets.push(packets)
⋮----
sleep(Duration::from_secs(1)).await;
⋮----
for p in batch.iter() {
assert_eq!(p.meta().size, num_bytes);
⋮----
assert_eq!(total_packets, num_expected_packets);
⋮----
pub async fn check_unstaked_node_connect_failure(server_address: SocketAddr) {
let conn1 = Arc::new(make_client_endpoint(&server_address, None).await);
if let Ok(mut s1) = conn1.open_uni().await {
⋮----
s1.write_all(&[0u8]).await.unwrap_or_default();
⋮----
s1.finish().unwrap_or_default();
s1.stopped().await.unwrap_err();
⋮----
async fn test_quic_server_exit_on_cancel() {
⋮----
} = setup_quic_server(
⋮----
cancel.cancel();
join_handle.await.unwrap();
drop(receiver);
⋮----
async fn test_quic_timeout() {
⋮----
check_timeout(receiver, server_address).await;
⋮----
async fn test_quic_stream_timeout() {
⋮----
assert_eq!(stats.active_streams.load(Ordering::Relaxed), 0);
assert_eq!(stats.total_stream_read_timeouts.load(Ordering::Relaxed), 0);
⋮----
sleep(sleep_time).await;
⋮----
assert_ne!(stats.total_stream_read_timeouts.load(Ordering::Relaxed), 0);
assert!(s1.write_all(&[0u8]).await.is_err());
⋮----
async fn test_quic_server_block_multiple_connections() {
⋮----
check_block_multiple_connections(server_address).await;
⋮----
async fn test_quic_server_multiple_connections_on_single_client_endpoint() {
⋮----
let client_socket = bind_to_localhost_unique().expect("should bind - client");
⋮----
endpoint.set_default_client_config(get_client_config(&default_keypair));
⋮----
.connect(server_address, "localhost")
.expect("Failed in connecting")
⋮----
.expect("Failed in waiting");
⋮----
let mut s2 = conn2.open_uni().await.unwrap();
conn1.close(
⋮----
while stats.connection_removed.load(Ordering::Relaxed) != 1 && start.elapsed().as_secs() < 1
⋮----
debug!("First connection not removed yet");
sleep(Duration::from_millis(10)).await;
⋮----
assert!(start.elapsed().as_secs() < 1);
s2.write_all(&[0u8]).await.unwrap();
s2.finish().unwrap();
conn2.close(
⋮----
while stats.connection_removed.load(Ordering::Relaxed) != 2 && start.elapsed().as_secs() < 1
⋮----
debug!("Second connection not removed yet");
⋮----
async fn test_quic_server_multiple_writes() {
⋮----
check_multiple_writes(receiver, server_address, None).await;
⋮----
async fn test_quic_server_staked_connection_removal() {
⋮----
let stakes = HashMap::from([(client_keypair.pubkey(), 100_000)]);
⋮----
Some(staked_nodes),
⋮----
check_multiple_writes(receiver, server_address, Some(&client_keypair)).await;
⋮----
assert_eq!(
⋮----
assert_eq!(stats.connection_removed.load(Ordering::Relaxed), 1);
assert_eq!(stats.connection_remove_failed.load(Ordering::Relaxed), 0);
⋮----
async fn test_quic_server_zero_staked_connection_removal() {
⋮----
let stakes = HashMap::from([(client_keypair.pubkey(), 0)]);
⋮----
async fn test_quic_server_unstaked_connection_removal() {
⋮----
async fn test_quic_server_unstaked_node_connect_failure() {
⋮----
let s = bind_to_localhost_unique().expect("should bind");
let (sender, _) = unbounded();
⋮----
let server_address = s.local_addr().unwrap();
⋮----
} = spawn_stake_weighted_qos_server(
⋮----
check_unstaked_node_connect_failure(server_address).await;
⋮----
t.await.unwrap();
⋮----
async fn test_quic_server_multiple_streams() {
⋮----
let (sender, receiver) = unbounded();
⋮----
check_multiple_streams(receiver, server_address, None).await;
⋮----
assert_eq!(stats.total_new_streams.load(Ordering::Relaxed), 20);
assert_eq!(stats.total_connections.load(Ordering::Relaxed), 2);
assert_eq!(stats.total_new_connections.load(Ordering::Relaxed), 2);
⋮----
assert_eq!(stats.total_connections.load(Ordering::Relaxed), 0);
⋮----
fn test_prune_table_with_ip() {
use std::net::Ipv4Addr;
⋮----
.map(|i| SocketAddr::new(IpAddr::V4(Ipv4Addr::new(i, 0, 0, 0)), 0))
.collect();
⋮----
for (i, socket) in sockets.iter().enumerate() {
⋮----
ConnectionTableKey::IP(socket.ip()),
socket.port(),
ClientConnectionTracker::new(stats.clone(), 1000).unwrap(),
⋮----
ConnectionTableKey::IP(sockets[0].ip()),
sockets[0].port(),
⋮----
let pruned = table.prune_oldest(new_size);
assert_eq!(pruned, num_entries as usize - new_size);
for v in table.table.values() {
⋮----
assert!((x.last_update() + 1) >= (num_entries as u64 - new_size as u64));
⋮----
assert_eq!(table.table.len(), new_size);
assert_eq!(table.total_size, new_size);
for socket in sockets.iter().take(num_entries as usize).skip(new_size - 1) {
table.remove_connection(ConnectionTableKey::IP(socket.ip()), socket.port(), 0);
⋮----
assert_eq!(table.total_size, 0);
assert_eq!(stats.open_connections.load(Ordering::Relaxed), 0);
⋮----
fn test_prune_table_with_unique_pubkeys() {
⋮----
let pubkeys: Vec<_> = (0..num_entries).map(|_| Pubkey::new_unique()).collect();
for (i, pubkey) in pubkeys.iter().enumerate() {
⋮----
for pubkey in pubkeys.iter().take(num_entries as usize).skip(new_size - 1) {
table.remove_connection(ConnectionTableKey::Pubkey(*pubkey), 0, 0);
⋮----
fn test_prune_table_with_non_unique_pubkeys() {
⋮----
(0..max_connections_per_peer).for_each(|i| {
⋮----
assert!(table
⋮----
assert_eq!(table.total_size, num_entries);
⋮----
let pruned = table.prune_oldest(new_max_size);
assert!(pruned >= num_entries - new_max_size);
assert!(table.table.len() <= new_max_size);
assert!(table.total_size <= new_max_size);
table.remove_connection(ConnectionTableKey::Pubkey(pubkey2), 0, 0);
⋮----
fn test_prune_table_random() {
⋮----
let pruned = table.prune_random( 2,  0);
assert_eq!(pruned, 0);
let pruned = table.prune_random(
⋮----
assert_eq!(pruned, 1);
assert_eq!(stats.open_connections.load(Ordering::Relaxed), 4);
⋮----
fn test_remove_connections() {
⋮----
ConnectionTableKey::IP(single_connection_addr.ip()),
single_connection_addr.port(),
⋮----
sockets.push(single_connection_addr);
sockets.push(zero_connection_addr);
for socket in sockets.iter() {
⋮----
async fn test_throttling_check_no_packet_drop() {
⋮----
let client_connection = make_client_endpoint(&server_address, None).await;
⋮----
let mut send_stream = client_connection.open_uni().await.unwrap();
let data = format!("{i}").into_bytes();
send_stream.write_all(&data).await.unwrap();
send_stream.finish().unwrap();
⋮----
let elapsed_sending: f64 = start_time.elapsed().as_secs_f64();
info!("Elapsed sending: {elapsed_sending}");
⋮----
while num_txs_received < expected_num_txs && start_time.elapsed() < Duration::from_secs(2) {
⋮----
num_txs_received += packets.len();
⋮----
sleep(Duration::from_millis(100)).await;
⋮----
assert_eq!(expected_num_txs, num_txs_received);
⋮----
assert!(stats.throttled_unstaked_streams.load(Ordering::Relaxed) > 0);
⋮----
fn test_client_connection_tracker() {
⋮----
let tracker_1 = ClientConnectionTracker::new(stats.clone(), 1);
assert!(tracker_1.is_ok());
assert!(ClientConnectionTracker::new(stats.clone(), 1).is_err());
assert_eq!(stats.open_connections.load(Ordering::Relaxed), 1);
drop(tracker_1);
⋮----
async fn test_client_connection_close_invalid_stream() {
⋮----
.write_all(&[42; PACKET_DATA_SIZE + 1])
⋮----
match client_connection.closed().await {
⋮----
assert_eq!(error_code, CONNECTION_CLOSE_CODE_INVALID_STREAM.into());
assert_eq!(reason, CONNECTION_CLOSE_REASON_INVALID_STREAM);
⋮----
_ => panic!("unexpected close"),
⋮----
assert_eq!(stats.invalid_stream_size.load(Ordering::Relaxed), 1);

================
File: streamer/src/nonblocking/recvmmsg.rs
================
pub async fn recv_mmsg(
⋮----
debug_assert!(packets.iter().all(|pkt| pkt.meta() == &Meta::default()));
let count = cmp::min(PACKETS_PER_BATCH, packets.len());
socket.readable().await?;
⋮----
for p in packets.iter_mut().take(count) {
p.meta_mut().size = 0;
match socket.try_recv_from(p.buffer_mut()) {
Err(ref e) if e.kind() == io::ErrorKind::WouldBlock => {
⋮----
return Err(e);
⋮----
p.meta_mut().size = nrecv;
p.meta_mut().set_socket_addr(&from);
⋮----
Ok(i)
⋮----
pub async fn recv_mmsg_exact(
⋮----
let total = packets.len();
⋮----
let res = recv_mmsg(socket, &mut packets[first..]).await?;
⋮----
Ok(packets.len())
⋮----
mod tests {
⋮----
type TestConfig = (UdpSocket, SocketAddr, UdpSocket, SocketAddr);
async fn test_setup_reader_sender(ip_str: &str) -> io::Result<TestConfig> {
⋮----
.parse()
.map_err(|e| io::Error::new(io::ErrorKind::InvalidInput, e))?;
let reader = bind_to_async(sock_addr.ip(), sock_addr.port()).await?;
let addr = reader.local_addr()?;
let sender = bind_to_async(sock_addr.ip(), sock_addr.port()).await?;
let saddr = sender.local_addr()?;
Ok((reader, addr, sender, saddr))
⋮----
async fn test_one_iter((reader, addr, sender, saddr): TestConfig) {
⋮----
sender.send_to(&data[..], &addr).await.unwrap();
⋮----
let mut packets = vec![Packet::default(); sent];
let recv = recv_mmsg_exact(&reader, &mut packets[..]).await.unwrap();
assert_eq!(sent, recv);
for packet in packets.iter().take(recv) {
assert_eq!(packet.meta().size, PACKET_DATA_SIZE);
assert_eq!(packet.meta().socket_addr(), saddr);
⋮----
async fn test_recv_mmsg_one_iter() {
test_one_iter(test_setup_reader_sender("127.0.0.1:0").await.unwrap()).await;
match test_setup_reader_sender("::1:0").await {
Ok(config) => test_one_iter(config).await,
Err(e) => warn!("Failed to configure IPv6: {e:?}"),
⋮----
async fn test_multi_iter((reader, addr, sender, saddr): TestConfig) {
⋮----
let mut packets = vec![Packet::default(); TEST_NUM_MSGS];
⋮----
assert_eq!(TEST_NUM_MSGS, recv);
⋮----
let mut packets = vec![Packet::default(); sent - TEST_NUM_MSGS];
⋮----
.iter_mut()
.for_each(|pkt| *pkt.meta_mut() = Meta::default());
⋮----
assert_eq!(sent - TEST_NUM_MSGS, recv);
⋮----
async fn test_recv_mmsg_multi_iter() {
test_multi_iter(test_setup_reader_sender("127.0.0.1:0").await.unwrap()).await;
⋮----
Ok(config) => test_multi_iter(config).await,
⋮----
async fn test_recv_mmsg_exact_multi_iter_timeout() {
let reader = bind_to_localhost_async().await.expect("bind");
let addr = reader.local_addr().unwrap();
let sender = bind_to_localhost_async().await.expect("bind");
let saddr = sender.local_addr().unwrap();
⋮----
let _recv = recv_mmsg(&reader, &mut packets[..]).await;
assert!(start.elapsed().as_secs() < 5);
⋮----
async fn test_recv_mmsg_multi_addrs() {
⋮----
let sender1 = bind_to_localhost_async().await.expect("bind");
let saddr1 = sender1.local_addr().unwrap();
⋮----
let sender2 = bind_to_localhost_async().await.expect("bind");
let saddr2 = sender2.local_addr().unwrap();
⋮----
sender1.send_to(&data[..], &addr).await.unwrap();
⋮----
sender2.send_to(&data[..], &addr).await.unwrap();
⋮----
let recv = recv_mmsg(&reader, &mut packets[..]).await.unwrap();
⋮----
for packet in packets.iter().take(sent1) {
⋮----
assert_eq!(packet.meta().socket_addr(), saddr1);
⋮----
for packet in packets.iter().skip(sent1).take(recv - sent1) {
⋮----
assert_eq!(packet.meta().socket_addr(), saddr2);
⋮----
assert_eq!(sent1 + sent2 - TEST_NUM_MSGS, recv);

================
File: streamer/src/nonblocking/sendmmsg.rs
================
pub async fn batch_send<S, T>(sock: &UdpSocket, packets: &[(T, S)]) -> Result<(), SendPktsError>
⋮----
.iter()
.map(|(p, a)| sock.send_to(p.as_ref(), a.borrow()))
⋮----
let results = join_all(futures).await;
⋮----
if erropt.is_none() {
erropt = Some(e);
⋮----
Err(SendPktsError::IoError(err, num_failed))
⋮----
Ok(())
⋮----
pub async fn multi_target_send<S, T>(
⋮----
let dests = dests.iter().map(Borrow::borrow);
let pkts: Vec<_> = repeat(&packet).zip(dests).collect();
batch_send(sock, &pkts).await
⋮----
mod tests {
⋮----
async fn test_send_mmsg_one_dest() {
let reader = bind_to_localhost_async().await.expect("bind");
let addr = reader.local_addr().unwrap();
let sender = bind_to_localhost_async().await.expect("bind");
let packets: Vec<_> = (0..32).map(|_| vec![0u8; PACKET_DATA_SIZE]).collect();
let packet_refs: Vec<_> = packets.iter().map(|p| (&p[..], &addr)).collect();
let sent = batch_send(&sender, &packet_refs[..]).await.ok();
assert_eq!(sent, Some(()));
let mut packets = vec![Packet::default(); 32];
let recv = recv_mmsg_exact(&reader, &mut packets[..]).await.unwrap();
assert_eq!(32, recv);
⋮----
async fn test_send_mmsg_multi_dest() {
⋮----
let reader2 = bind_to_localhost_async().await.expect("bind");
let addr2 = reader2.local_addr().unwrap();
⋮----
.enumerate()
.map(|(i, p)| {
⋮----
.collect();
⋮----
let mut packets = vec![Packet::default(); 16];
⋮----
assert_eq!(16, recv);
⋮----
let recv = recv_mmsg_exact(&reader2, &mut packets[..]).await.unwrap();
⋮----
async fn test_multicast_msg() {
⋮----
let reader3 = bind_to_localhost_async().await.expect("bind");
let addr3 = reader3.local_addr().unwrap();
let reader4 = bind_to_localhost_async().await.expect("bind");
let addr4 = reader4.local_addr().unwrap();
⋮----
let sent = multi_target_send(
⋮----
packet.data(..).unwrap(),
⋮----
.ok();
⋮----
let recv = recv_mmsg(&reader, &mut packets[..]).await.unwrap();
assert_eq!(1, recv);
⋮----
let recv = recv_mmsg(&reader2, &mut packets[..]).await.unwrap();
⋮----
let recv = recv_mmsg(&reader3, &mut packets[..]).await.unwrap();
⋮----
let recv = recv_mmsg(&reader4, &mut packets[..]).await.unwrap();
⋮----
async fn test_intermediate_failures_mismatched_bind() {
let packets: Vec<_> = (0..3).map(|_| vec![0u8; PACKET_DATA_SIZE]).collect();
⋮----
let packet_refs: Vec<_> = vec![
⋮----
let dest_refs: Vec<_> = vec![&ip4, &ip6, &ip4];
let sender = bind_to_unspecified_async().await.expect("bind");
let res = batch_send(&sender, &packet_refs[..]).await;
assert_matches!(res, Err(SendPktsError::IoError(_,  1)));
let res = multi_target_send(&sender, &packets[0], &dest_refs).await;
⋮----
async fn test_intermediate_failures_unreachable_address() {
let packets: Vec<_> = (0..5).map(|_| vec![0u8; PACKET_DATA_SIZE]).collect();
⋮----
match batch_send(&sender, &packet_refs[..]).await {
Ok(()) => panic!(),
⋮----
assert_matches!(ioerror.kind(), ErrorKind::PermissionDenied);
assert_eq!(num_failed, 2);
⋮----
assert_eq!(num_failed, 3);
⋮----
let dest_refs: Vec<_> = vec![
⋮----
match multi_target_send(&sender, &packets[0], &dest_refs).await {

================
File: streamer/src/nonblocking/simple_qos.rs
================
pub struct SimpleQosConfig {
⋮----
impl Default for SimpleQosConfig {
fn default() -> Self {
⋮----
impl OpaqueStreamerCounter for TokenBucket {}
pub struct SimpleQos {
⋮----
impl SimpleQos {
pub fn new(
⋮----
fn cache_new_connection(
⋮----
let remote_addr = connection.remote_address();
let rtt = connection.rtt().clamp(MIN_RTT, MAX_RTT).as_millis() as u32;
let max_streams_in_flight = (self.config.max_streams_per_second as u32).saturating_mul(rtt)
⋮----
let max_streams_in_flight = max_streams_in_flight.max(STREAMS_IN_FLIGHT_MARGIN);
connection.set_max_concurrent_uni_streams(VarInt::from_u32(max_streams_in_flight));
debug!(
⋮----
let key = ConnectionTableKey::new(remote_addr.ip(), conn_context.remote_pubkey);
⋮----
.try_add_connection(
⋮----
remote_addr.port(),
⋮----
Some(connection.clone()),
conn_context.peer_type(),
conn_context.last_update.clone(),
⋮----
update_open_connections_stat(&self.stats, &connection_table_l);
drop(connection_table_l);
Ok((last_update, cancel_connection, stream_counter))
⋮----
.fetch_add(1, Ordering::Relaxed);
Err(ConnectionHandlerError::ConnectionAddError)
⋮----
pub struct SimpleQosConnectionContext {
⋮----
impl ConnectionContext for SimpleQosConnectionContext {
fn peer_type(&self) -> ConnectionPeerType {
⋮----
fn remote_pubkey(&self) -> Option<solana_pubkey::Pubkey> {
⋮----
fn build_connection_context(&self, connection: &Connection) -> SimpleQosConnectionContext {
⋮----
get_connection_stake(connection, &self.staked_nodes).map_or(
⋮----
(ConnectionPeerType::Staked(stake), Some(pubkey), total_stake)
⋮----
remote_address: connection.remote_address(),
⋮----
fn try_add_connection(
⋮----
match conn_context.peer_type() {
⋮----
let mut connection_table_l = self.staked_connection_table.lock().await;
⋮----
connection_table_l.prune_random(PRUNE_RANDOM_SAMPLE_SIZE, stake);
⋮----
.fetch_add(num_pruned, Ordering::Relaxed);
⋮----
.cache_new_connection(
⋮----
conn_context.stream_counter = Some(stream_counter);
return Some(cancel_connection);
⋮----
fn on_stream_accepted(&self, _conn_context: &SimpleQosConnectionContext) {}
fn on_stream_error(&self, _conn_context: &SimpleQosConnectionContext) {}
fn on_stream_closed(&self, _conn_context: &SimpleQosConnectionContext) {}
⋮----
fn remove_connection(
⋮----
let stable_id = connection.stable_id();
⋮----
let mut connection_table = self.staked_connection_table.lock().await;
let removed_connection_count = connection_table.remove_connection(
ConnectionTableKey::new(remote_addr.ip(), conn_context.remote_pubkey()),
⋮----
update_open_connections_stat(&self.stats, &connection_table);
⋮----
fn on_stream_finished(&self, context: &SimpleQosConnectionContext) {
⋮----
.store(timing::timestamp(), Ordering::Relaxed);
⋮----
fn on_new_stream(
⋮----
let peer_type = context.peer_type();
⋮----
.as_ref()
.expect("This will always be populated before streams are opened");
while stream_counter.consume_tokens(1).is_err() {
debug!("Throttling stream from {remote_addr:?}");
self.stats.throttled_streams.fetch_add(1, Ordering::Relaxed);
⋮----
let min_sleep = stream_counter.us_to_have_tokens(1).expect(
⋮----
sleep(Duration::from_micros(min_sleep)).await;
⋮----
fn max_concurrent_connections(&self) -> usize {
⋮----
mod tests {
⋮----
async fn create_connection_with_keypairs(
⋮----
let (server_config, _) = configure_server(server_keypair).unwrap();
let server_socket = bind_to_localhost_unique().expect("should bind - server");
let server_addr = server_socket.local_addr().unwrap();
⋮----
Some(server_config),
⋮----
.unwrap();
let client_socket = bind_to_localhost_unique().expect("should bind - client");
⋮----
let client_config = get_client_config(client_keypair);
client_endpoint.set_default_client_config(client_config);
⋮----
let incoming = server_endpoint.accept().await.unwrap();
incoming.await.unwrap()
⋮----
let client_connect_future = client_endpoint.connect(server_addr, "localhost").unwrap();
⋮----
let _client_connection = client_connection.unwrap();
⋮----
async fn create_server_side_connection() -> (Connection, Endpoint, Endpoint) {
⋮----
create_connection_with_keypairs(&server_keypair, &client_keypair).await
⋮----
fn create_staked_nodes_with_keypairs(
⋮----
stakes.insert(server_keypair.pubkey(), stake_amount);
stakes.insert(client_keypair.pubkey(), stake_amount);
⋮----
async fn test_cache_new_connection_success() {
⋮----
stats.clone(),
⋮----
cancel.clone(),
⋮----
let connection_table_l = connection_table_guard.lock().await;
⋮----
stats: stats.clone(),
⋮----
create_server_side_connection().await;
let remote_addr = server_connection.remote_address();
⋮----
remote_pubkey: Some(solana_pubkey::Pubkey::new_unique()),
⋮----
let result = simple_qos.cache_new_connection(
⋮----
assert!(result.is_ok());
let (_last_update, cancel_token, _stream_counter) = result.unwrap();
assert!(!cancel_token.is_cancelled());
⋮----
async fn test_cache_new_connection_max_connections_reached() {
⋮----
ConnectionTable::new(ConnectionTableType::Staked, cancel.clone());
⋮----
let remote_addr = connection1.remote_address();
let key = ConnectionTableKey::new(remote_addr.ip(), None);
⋮----
let _ = connection_table.try_add_connection(
⋮----
Some(connection1),
⋮----
assert!(result.is_err());
assert_eq!(stats.connection_add_failed.load(Ordering::Relaxed), 1);
⋮----
async fn test_cache_new_connection_updates_stats() {
⋮----
let initial_open_connections = stats.open_staked_connections.load(Ordering::Relaxed);
⋮----
if result.is_ok() {
assert!(
⋮----
async fn test_build_connection_context_unstaked_peer() {
⋮----
let context = simple_qos.build_connection_context(&server_connection);
assert!(matches!(context.peer_type(), ConnectionPeerType::Unstaked));
assert_eq!(context.remote_pubkey(), None);
assert_eq!(context.remote_address, server_connection.remote_address());
assert!(context.last_update.load(Ordering::Relaxed) > 0);
assert!(context.stream_counter.is_none());
⋮----
async fn test_build_connection_context_staked_peer() {
⋮----
create_staked_nodes_with_keypairs(&server_keypair, &client_keypair, stake_amount);
⋮----
create_connection_with_keypairs(&server_keypair, &client_keypair).await;
⋮----
assert!(matches!(context.peer_type(), ConnectionPeerType::Staked(_)));
if let ConnectionPeerType::Staked(stake) = context.peer_type() {
assert_eq!(stake, stake_amount);
⋮----
assert_eq!(context.remote_pubkey(), Some(client_keypair.pubkey()));
⋮----
async fn test_try_add_connection_staked_peer_success() {
⋮----
let mut conn_context = simple_qos.build_connection_context(&server_connection);
⋮----
.try_add_connection(client_tracker, &server_connection, &mut conn_context)
⋮----
assert!(result.is_some());
let cancel_token = result.unwrap();
⋮----
assert!(conn_context.stream_counter.is_some());
assert_eq!(
⋮----
async fn test_try_add_connection_unstaked_peer_rejected() {
⋮----
assert!(result.is_none());
assert!(conn_context.stream_counter.is_none());
⋮----
async fn test_try_add_connection_max_staked_connections_with_pruning() {
⋮----
stakes.insert(server_keypair1.pubkey(), stake_amount);
stakes.insert(client_keypair1.pubkey(), stake_amount);
stakes.insert(server_keypair2.pubkey(), stake_amount);
stakes.insert(client_keypair2.pubkey(), stake_amount * 2);
⋮----
create_connection_with_keypairs(&server_keypair1, &client_keypair1).await;
let mut conn_context1 = simple_qos.build_connection_context(&server_connection1);
⋮----
.try_add_connection(client_tracker1, &server_connection1, &mut conn_context1)
⋮----
assert!(result1.is_some());
⋮----
create_connection_with_keypairs(&server_keypair2, &client_keypair2).await;
let mut conn_context2 = simple_qos.build_connection_context(&server_connection2);
⋮----
.try_add_connection(client_tracker2, &server_connection2, &mut conn_context2)
⋮----
assert!(result2.is_some());
assert!(stats.num_evictions_staked.load(Ordering::Relaxed) > 0);
⋮----
async fn test_try_add_connection_max_staked_connections_no_pruning_possible() {
⋮----
stakes.insert(server_keypair1.pubkey(), high_stake);
stakes.insert(client_keypair1.pubkey(), high_stake);
stakes.insert(server_keypair2.pubkey(), low_stake);
stakes.insert(client_keypair2.pubkey(), low_stake);
⋮----
assert!(result2.is_none());
assert!(conn_context2.stream_counter.is_none());
⋮----
async fn test_try_add_connection_context_updates() {
⋮----
let initial_last_update = conn_context.last_update.load(Ordering::Relaxed);
⋮----
let updated_last_update = conn_context.last_update.load(Ordering::Relaxed);
assert!(updated_last_update >= initial_last_update);
⋮----
async fn test_on_stream_accepted_increments_counter() {
⋮----
async fn test_remove_connection_success() {
⋮----
assert!(add_result.is_some());
⋮----
assert!(initial_open_connections > 0);
⋮----
.remove_connection(&conn_context, server_connection.clone())
⋮----
assert_eq!(removed_count, 1);
let final_open_connections = stats.open_staked_connections.load(Ordering::Relaxed);
assert!(final_open_connections < initial_open_connections);
assert_eq!(final_open_connections, initial_open_connections - 1);
⋮----
async fn test_on_new_stream_throttles_correctly() {
⋮----
let simple_qos = SimpleQos::new(qos_config, stats.clone(), staked_nodes, cancel.clone());
⋮----
simple_qos.on_new_stream(&conn_context).await;
⋮----
let elapsed = start_time.elapsed();
assert!(elapsed > std::time::Duration::from_millis(950));
assert!(elapsed < std::time::Duration::from_millis(1200));

================
File: streamer/src/nonblocking/stream_throttle.rs
================
pub(crate) struct StakedStreamLoadEMA {
⋮----
impl StakedStreamLoadEMA {
pub(crate) fn new(
⋮----
fn ema_function(current_ema: u128, recent_load: u128) -> u128 {
⋮----
fn update_ema(&self, time_since_last_update_ms: u128) {
⋮----
time_since_last_update_ms.saturating_sub(1) / u128::from(STREAM_LOAD_EMA_INTERVAL_MS);
⋮----
u128::from(self.load_in_recent_interval.swap(0, Ordering::Relaxed));
⋮----
u128::from(self.current_load_ema.load(Ordering::Relaxed)),
⋮----
error!("Failed to convert EMA {updated_load_ema} to a u64. Not updating the load EMA");
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
.store(updated_load_ema, Ordering::Relaxed);
⋮----
.store(updated_load_ema as usize, Ordering::Relaxed);
⋮----
pub(crate) fn update_ema_if_needed(&self) {
⋮----
if Instant::now().duration_since(*self.last_update.read().unwrap()) >= EMA_DURATION {
let mut last_update_w = self.last_update.write().unwrap();
let since_last_update = Instant::now().duration_since(*last_update_w);
⋮----
self.update_ema(since_last_update.as_millis());
⋮----
pub(crate) fn increment_load(&self, peer_type: ConnectionPeerType) {
if peer_type.is_staked() {
self.load_in_recent_interval.fetch_add(1, Ordering::Relaxed);
⋮----
self.update_ema_if_needed();
⋮----
pub(crate) fn available_load_capacity_in_throttling_duration(
⋮----
self.current_load_ema.load(Ordering::Relaxed),
⋮----
let calculated_capacity = u64::try_from(calculated_capacity).unwrap_or_else(|_| {
error!(
⋮----
.saturating_add(1)
⋮----
.saturating_add(1),
⋮----
pub(crate) fn max_streams_per_ms(&self) -> u64 {
⋮----
pub struct ConnectionStreamCounter {
⋮----
impl OpaqueStreamerCounter for ConnectionStreamCounter {}
impl ConnectionStreamCounter {
pub fn new() -> Self {
⋮----
pub(crate) fn reset_throttling_params_if_needed(&self) -> tokio::time::Instant {
let last_throttling_instant = *self.last_throttling_instant.read().unwrap();
if tokio::time::Instant::now().duration_since(last_throttling_instant)
⋮----
let mut last_throttling_instant = self.last_throttling_instant.write().unwrap();
if tokio::time::Instant::now().duration_since(*last_throttling_instant)
⋮----
self.stream_count.store(0, Ordering::Relaxed);
⋮----
pub(crate) async fn throttle_stream(
⋮----
let throttle_interval_start = stream_counter.reset_throttling_params_if_needed();
let streams_read_in_throttle_interval = stream_counter.stream_count.load(Ordering::Relaxed);
⋮----
STREAM_THROTTLING_INTERVAL.saturating_sub(throttle_interval_start.elapsed());
if !throttle_duration.is_zero() {
debug!(
⋮----
stats.throttled_streams.fetch_add(1, Ordering::Relaxed);
⋮----
sleep(throttle_duration).await;
⋮----
pub mod test {
⋮----
fn test_max_streams_for_unstaked_connection() {
⋮----
assert_eq!(
⋮----
fn test_max_streams_for_staked_connection() {
⋮----
load_ema.current_load_ema.store(10000, Ordering::Relaxed);
⋮----
load_ema.current_load_ema.store(5000, Ordering::Relaxed);
⋮----
load_ema.current_load_ema.store(4000, Ordering::Relaxed);
⋮----
fn test_max_streams_for_staked_connection_with_no_unstaked_connections() {
⋮----
load_ema.current_load_ema.store(20000, Ordering::Relaxed);
assert!(
⋮----
assert!((6249u64..=6250).contains(
⋮----
assert!((12499u64..=12500).contains(
⋮----
fn test_update_ema() {
⋮----
.store(2500, Ordering::Relaxed);
⋮----
.store(2000, Ordering::Relaxed);
stream_load_ema.update_ema(5);
let updated_ema = stream_load_ema.current_load_ema.load(Ordering::Relaxed);
assert_eq!(updated_ema, 2090);
⋮----
assert_eq!(updated_ema, 2164);
⋮----
fn test_update_ema_missing_interval() {
⋮----
stream_load_ema.update_ema(8);
⋮----
fn test_update_ema_if_needed() {
⋮----
stream_load_ema.update_ema_if_needed();
⋮----
assert_eq!(updated_ema, 2000);
⋮----
*stream_load_ema.last_update.write().unwrap() =
Instant::now().checked_sub(ema_interval).unwrap();

================
File: streamer/src/nonblocking/swqos.rs
================
pub struct SwQosConfig {
⋮----
impl Default for SwQosConfig {
fn default() -> Self {
⋮----
impl SwQosConfig {
⋮----
pub fn default_for_tests() -> Self {
⋮----
pub struct SwQos {
⋮----
pub struct SwQosConnectionContext {
⋮----
impl ConnectionContext for SwQosConnectionContext {
fn peer_type(&self) -> ConnectionPeerType {
⋮----
fn remote_pubkey(&self) -> Option<solana_pubkey::Pubkey> {
⋮----
impl SwQos {
pub fn new(
⋮----
config: config.clone(),
⋮----
stats.clone(),
⋮----
cancel.clone(),
⋮----
fn compute_max_allowed_uni_streams(peer_type: ConnectionPeerType, total_stake: u64) -> usize {
⋮----
warn!(
⋮----
.clamp(
⋮----
fn cache_new_connection(
⋮----
if let Ok(max_uni_streams) = VarInt::from_u64(compute_max_allowed_uni_streams(
conn_context.peer_type(),
⋮----
let remote_addr = connection.remote_address();
debug!(
⋮----
let max_connections_per_peer = match conn_context.peer_type() {
⋮----
.try_add_connection(
ConnectionTableKey::new(remote_addr.ip(), conn_context.remote_pubkey),
remote_addr.port(),
⋮----
Some(connection.clone()),
⋮----
conn_context.last_update.clone(),
⋮----
update_open_connections_stat(&self.stats, &connection_table_l);
drop(connection_table_l);
connection.set_max_concurrent_uni_streams(max_uni_streams);
Ok((last_update, cancel_connection, stream_counter))
⋮----
.fetch_add(1, Ordering::Relaxed);
Err(ConnectionHandlerError::ConnectionAddError)
⋮----
connection.close(
CONNECTION_CLOSE_CODE_EXCEED_MAX_STREAM_COUNT.into(),
⋮----
Err(ConnectionHandlerError::MaxStreamError)
⋮----
fn prune_unstaked_connection_table(
⋮----
let max_connections = max_percentage_full.apply_to(max_unstaked_connections);
let num_pruned = unstaked_connection_table.prune_oldest(max_connections);
⋮----
.fetch_add(num_pruned, Ordering::Relaxed);
⋮----
async fn prune_unstaked_connections_and_add_new_connection(
⋮----
let stats = self.stats.clone();
⋮----
let mut connection_table = connection_table.lock().await;
self.prune_unstaked_connection_table(&mut connection_table, max_connections, stats);
self.cache_new_connection(
⋮----
CONNECTION_CLOSE_CODE_DISALLOWED.into(),
⋮----
fn max_streams_per_throttling_interval(&self, conn_context: &SwQosConnectionContext) -> u64 {
⋮----
.available_load_capacity_in_throttling_duration(
⋮----
fn build_connection_context(&self, connection: &Connection) -> SwQosConnectionContext {
get_connection_stake(connection, &self.staked_nodes).map_or(
⋮----
remote_address: connection.remote_address(),
⋮----
let max_streams_per_ms = self.staked_stream_load_ema.max_streams_per_ms();
⋮----
remote_pubkey: Some(pubkey),
⋮----
fn try_add_connection(
⋮----
match conn_context.peer_type() {
⋮----
let mut connection_table_l = self.staked_connection_table.lock().await;
⋮----
connection_table_l.prune_random(PRUNE_RANDOM_SAMPLE_SIZE, stake);
⋮----
.cache_new_connection(
⋮----
conn_context.stream_counter = Some(stream_counter);
return Some(cancel_connection);
⋮----
.prune_unstaked_connections_and_add_new_connection(
⋮----
self.unstaked_connection_table.clone(),
⋮----
fn on_stream_accepted(&self, conn_context: &SwQosConnectionContext) {
⋮----
.increment_load(conn_context.peer_type);
⋮----
.as_ref()
.unwrap()
⋮----
fn on_stream_error(&self, _conn_context: &SwQosConnectionContext) {
self.staked_stream_load_ema.update_ema_if_needed();
⋮----
fn on_stream_closed(&self, _conn_context: &SwQosConnectionContext) {
⋮----
fn remove_connection(
⋮----
self.staked_connection_table.lock().await
⋮----
self.unstaked_connection_table.lock().await
⋮----
let stable_id = connection.stable_id();
⋮----
let removed_count = lock.remove_connection(
ConnectionTableKey::new(remote_addr.ip(), conn_context.remote_pubkey()),
⋮----
update_open_connections_stat(&self.stats, &lock);
⋮----
fn on_stream_finished(&self, context: &SwQosConnectionContext) {
⋮----
.store(timing::timestamp(), Ordering::Relaxed);
⋮----
fn on_new_stream(&self, context: &SwQosConnectionContext) -> impl Future<Output = ()> + Send {
⋮----
let peer_type = context.peer_type();
⋮----
context.stream_counter.as_ref().unwrap();
⋮----
self.max_streams_per_throttling_interval(context);
throttle_stream(
⋮----
fn max_concurrent_connections(&self) -> usize {
⋮----
pub mod test {
⋮----
fn test_max_allowed_uni_streams() {
assert_eq!(

================
File: streamer/src/nonblocking/testing_utilities.rs
================
pub fn spawn_stake_weighted_qos_server(
⋮----
stats.clone(),
⋮----
cancel.clone(),
⋮----
spawn_server(
⋮----
pub fn get_client_config(keypair: &Keypair) -> ClientConfig {
let (cert, key) = new_dummy_x509_certificate(keypair);
let mut crypto = tls_client_config_builder()
.with_client_auth_cert(vec![cert], key)
.expect("Failed to use client certificate");
⋮----
crypto.alpn_protocols = vec![ALPN_TPU_PROTOCOL_ID.to_vec()];
let mut config = ClientConfig::new(Arc::new(QuicClientConfig::try_from(crypto).unwrap()));
⋮----
let timeout = IdleTimeout::try_from(QUIC_MAX_TIMEOUT).unwrap();
transport_config.max_idle_timeout(Some(timeout));
transport_config.keep_alive_interval(Some(QUIC_KEEP_ALIVE));
transport_config.send_fairness(QUIC_SEND_FAIRNESS);
config.transport_config(Arc::new(transport_config));
⋮----
pub struct SpawnTestServerResult {
⋮----
pub fn create_quic_server_sockets() -> Vec<UdpSocket> {
let num = if cfg!(not(target_os = "windows")) {
⋮----
let port_range = localhost_port_range_for_tests();
multi_bind_in_range_with_config(
⋮----
.expect("bind operation for quic server sockets should succeed")
⋮----
pub fn setup_quic_server(
⋮----
let sockets = create_quic_server_sockets();
let (sender, receiver) = unbounded();
⋮----
let server_address = sockets[0].local_addr().unwrap();
let staked_nodes = Arc::new(RwLock::new(option_staked_nodes.unwrap_or_default()));
⋮----
} = spawn_stake_weighted_qos_server(
⋮----
.unwrap();
⋮----
pub async fn make_client_endpoint(
⋮----
let client_socket = bind_to_localhost_unique().expect("should bind - client");
⋮----
endpoint.set_default_client_config(get_client_config(
client_keypair.unwrap_or(&default_keypair),
⋮----
.connect(*addr, "localhost")
.expect("Endpoint configuration should be correct")
⋮----
.expect("Test server should be already listening on 'localhost'")
⋮----
pub async fn check_multiple_streams(
⋮----
let conn1 = Arc::new(make_client_endpoint(&server_address, client_keypair).await);
let conn2 = Arc::new(make_client_endpoint(&server_address, client_keypair).await);
⋮----
info!("sending: {i}");
let c1 = conn1.clone();
let c2 = conn2.clone();
let mut s1 = c1.open_uni().await.unwrap();
let mut s2 = c2.open_uni().await.unwrap();
s1.write_all(&[0u8]).await.unwrap();
s1.finish().unwrap();
s2.write_all(&[0u8]).await.unwrap();
s2.finish().unwrap();
⋮----
sleep(Duration::from_millis(200)).await;
⋮----
let mut all_packets = vec![];
⋮----
while now.elapsed().as_secs() < 10 {
if let Ok(packets) = receiver.try_recv() {
total_packets += packets.len();
all_packets.push(packets)
⋮----
sleep(Duration::from_secs(1)).await;
⋮----
for p in batch.iter() {
assert_eq!(p.meta().size, 1);
⋮----
assert_eq!(total_packets, num_expected_packets);

================
File: streamer/src/evicting_sender.rs
================
pub struct EvictingSender<T> {
⋮----
pub fn new(sender: Sender<T>, receiver: Receiver<T>) -> Self {
⋮----
pub fn new_bounded(capacity: usize) -> (Self, Receiver<T>) {
let (sender, receiver) = bounded(capacity);
(Self::new(sender, receiver.clone()), receiver)
⋮----
fn send(&self, msg: T) -> std::result::Result<(), SendError<T>> {
self.sender.send(msg)
⋮----
fn try_send(&self, msg: T) -> std::result::Result<(), TrySendError<T>> {
let Err(e) = self.sender.try_send(msg) else {
return Ok(());
⋮----
// Prefer newer messages over older messages.
TrySendError::Full(msg) => match self.receiver.try_recv() {
⋮----
// Attempt to requeue the newer message.
// NB: if multiple senders are used, and another sender is faster than us to send() after we've popped `older`,
self.sender.try_send(msg)?;
Err(TrySendError::Full(older))
⋮----
Err(TryRecvError::Empty) => self.sender.try_send(msg),
Err(TryRecvError::Disconnected) => unreachable!(),
⋮----
TrySendError::Disconnected(_) => unreachable!(),
⋮----
fn is_empty(&self) -> bool {
self.receiver.is_empty()
⋮----
fn len(&self) -> usize {
self.receiver.len()

================
File: streamer/src/lib.rs
================
pub mod evicting_sender;
pub mod msghdr;
pub mod nonblocking;
pub mod packet;
pub mod quic;
pub mod recvmmsg;
pub mod sendmmsg;
pub mod streamer;
⋮----
extern crate log;
⋮----
extern crate solana_metrics;

================
File: streamer/src/msghdr.rs
================
pub(crate) fn create_msghdr(
⋮----
let mut msg_hdr: msghdr = unsafe { zeroed() };
msg_hdr.msg_name = msg_name.as_mut_ptr() as *mut _;
⋮----
msg_hdr.msg_iov = iov.as_mut_ptr();

================
File: streamer/src/packet.rs
================
pub(crate) fn recv_from(
⋮----
socket.set_nonblocking(false)?;
trace!("receiving on {}", socket.local_addr().unwrap());
let should_wait = max_wait.is_some();
let start = should_wait.then(Instant::now);
⋮----
batch.resize(PACKETS_PER_BATCH, Packet::default());
match recv_mmsg(socket, &mut batch[i..]) {
⋮----
if !should_wait && err.kind() == ErrorKind::WouldBlock {
⋮----
trace!("recv_from err {e:?}");
return Err(e);
⋮----
socket.set_nonblocking(true)?;
⋮----
trace!("got {npkts} packets");
⋮----
if start.as_ref().map(Instant::elapsed) > max_wait {
⋮----
batch.truncate(i);
Ok(i)
⋮----
use crate::streamer::SOCKET_READ_TIMEOUT;
const SOCKET_READ_TIMEOUT_MS: u16 = SOCKET_READ_TIMEOUT.as_millis() as u16;
fn recv_from_once(
⋮----
Err(e) if e.kind() == ErrorKind::WouldBlock => {
⋮----
if poll(poll_fd, PollTimeout::from(SOCKET_READ_TIMEOUT_MS))? == 0 {
⋮----
Err(e) => return Err(e),
⋮----
fn recv_from_coalesce(
⋮----
let remaining = deadline.saturating_duration_since(Instant::now());
⋮----
if ppoll(poll_fd, Some(TimeSpec::from_duration(timeout)), None)? == 0 {
⋮----
if poll(poll_fd, PollTimeout::from(timeout.as_millis() as u16))? == 0 {
⋮----
Some(max_wait) => recv_from_coalesce(batch, socket, max_wait, poll_fd),
None => recv_from_once(batch, socket, poll_fd),
⋮----
pub fn send_to(
⋮----
for p in batch.iter() {
let addr = p.meta().socket_addr();
if socket_addr_space.check(&addr) {
if let Some(data) = p.data(..) {
socket.send_to(data, addr)?;
⋮----
Ok(())
⋮----
mod tests {
⋮----
fn test_packets_set_addr() {
let send_addr: SocketAddr = "127.0.0.1:123".parse().unwrap();
let packets = vec![Packet::default()];
⋮----
packet_batch.set_addr(&send_addr);
assert_eq!(packet_batch[0].meta().socket_addr(), send_addr);
⋮----
fn recv_from(
⋮----
let mut poll_fd = [PollFd::new(socket.as_fd(), PollFlags::POLLIN)];
recv_from_impl(batch, socket, max_wait, &mut poll_fd)
⋮----
recv_from_impl(batch, socket, max_wait)
⋮----
pub fn packet_send_recv() {
⋮----
let recv_socket = bind_to_localhost_unique().expect("should bind - receiver");
let addr = recv_socket.local_addr().unwrap();
let send_socket = bind_to_localhost_unique().expect("should bind - sender");
let saddr = send_socket.local_addr().unwrap();
⋮----
for m in batch.iter_mut() {
m.meta_mut().set_socket_addr(&addr);
m.meta_mut().size = PACKET_DATA_SIZE;
⋮----
send_to(&batch, &send_socket, &SocketAddrSpace::Unspecified).unwrap();
⋮----
.iter_mut()
.for_each(|pkt| *pkt.meta_mut() = Meta::default());
let recvd = recv_from(
⋮----
Some(Duration::from_millis(1)),
⋮----
.unwrap();
assert_eq!(recvd, batch.len());
for m in batch.iter() {
assert_eq!(m.meta().size, PACKET_DATA_SIZE);
assert_eq!(m.meta().socket_addr(), saddr);
⋮----
pub fn debug_trait() {
write!(io::sink(), "{:?}", Packet::default()).unwrap();
write!(io::sink(), "{:?}", RecycledPacketBatch::default()).unwrap();
⋮----
fn test_packet_partial_eq() {
⋮----
p1.meta_mut().size = 1;
p1.buffer_mut()[0] = 0;
p2.meta_mut().size = 1;
p2.buffer_mut()[0] = 0;
assert!(p1 == p2);
p2.buffer_mut()[0] = 4;
assert!(p1 != p2);
⋮----
fn test_packet_resize() {
⋮----
batch.resize(batch_size, Packet::default());
for p in batch.iter_mut() {
p.meta_mut().set_socket_addr(&addr);
p.meta_mut().size = 1;
⋮----
Some(Duration::from_millis(100)),
⋮----
assert_eq!(recvd, PACKETS_PER_BATCH);
assert_eq!(batch.capacity(), PACKETS_PER_BATCH);

================
File: streamer/src/quic.rs
================
pub fn default_num_tpu_transaction_forward_receive_threads() -> usize {
num_cpus::get().min(16)
⋮----
pub fn default_num_tpu_transaction_receive_threads() -> usize {
num_cpus::get().min(8)
⋮----
pub fn default_num_tpu_vote_transaction_receive_threads() -> usize {
⋮----
pub struct SpawnServerResult {
⋮----
pub(crate) fn configure_server(
⋮----
let (cert, priv_key) = new_dummy_x509_certificate(identity_keypair);
let cert_chain_pem_parts = vec![Pem {
⋮----
tls_server_config_builder().with_single_cert(vec![cert], priv_key)?;
server_tls_config.alpn_protocols = vec![ALPN_TPU_PROTOCOL_ID.to_vec()];
⋮----
server_config.migration(false);
let config = Arc::get_mut(&mut server_config.transport).unwrap();
config.stream_receive_window((PACKET_DATA_SIZE as u32).into());
config.receive_window((PACKET_DATA_SIZE as u32).into());
config.max_concurrent_uni_streams(0u32.into());
config.receive_window(CONNECTION_RECEIVE_WINDOW_BYTES);
let timeout = IdleTimeout::try_from(QUIC_MAX_TIMEOUT).unwrap();
config.max_idle_timeout(Some(timeout));
config.max_concurrent_bidi_streams(0u32.into());
config.datagram_receive_buffer_size(None);
config.enable_segmentation_offload(false);
Ok((server_config, cert_chain_pem))
⋮----
fn rt(name: String, num_threads: NonZeroUsize) -> Runtime {
⋮----
.thread_name(name)
.worker_threads(num_threads.get())
.enable_all()
.build()
.unwrap()
⋮----
pub enum QuicServerError {
⋮----
pub struct EndpointKeyUpdater {
⋮----
impl NotifyKeyUpdate for EndpointKeyUpdater {
fn update_key(&self, key: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
let (config, _) = configure_server(key)?;
⋮----
endpoint.set_server_config(Some(config.clone()));
⋮----
Ok(())
⋮----
pub struct StreamerStats {
⋮----
impl StreamerStats {
pub fn report(&self, name: &'static str) {
⋮----
let mut metrics = self.process_sampled_packets_us_hist.lock().unwrap();
let process_sampled_packets_us_hist = metrics.clone();
metrics.clear();
⋮----
datapoint_info!(
⋮----
pub struct QuicStreamerConfig {
⋮----
pub struct SwQosQuicStreamerConfig {
⋮----
pub struct SimpleQosQuicStreamerConfig {
⋮----
impl Default for QuicStreamerConfig {
fn default() -> Self {
⋮----
num_threads: NonZeroUsize::new(num_cpus::get().min(1)).expect("1 is non-zero"),
⋮----
impl QuicStreamerConfig {
⋮----
pub const DEFAULT_NUM_SERVER_THREADS_FOR_TEST: NonZeroUsize = NonZeroUsize::new(8).unwrap();
⋮----
pub fn default_for_tests() -> Self {
⋮----
/// Generic function to spawn a tokio runtime with a QUIC server
/// Generic over QoS implementation
⋮----
/// Generic over QoS implementation
fn spawn_runtime_and_server<Q, C>(
⋮----
fn spawn_runtime_and_server<Q, C>(
⋮----
let runtime = rt(format!("{thread_name}Rt"), quic_server_params.num_threads);
⋮----
let _guard = runtime.enter();
⋮----
.name(thread_name.into())
.spawn(move || {
if let Err(e) = runtime.block_on(result.thread) {
warn!("error from runtime.block_on: {e:?}");
⋮----
.unwrap();
⋮----
endpoints: result.endpoints.clone(),
⋮----
Ok(SpawnServerResult {
⋮----
/// Spawns a tokio runtime and a streamer instance inside it.
/// Uses Stake Weighted QoS
⋮----
/// Uses Stake Weighted QoS
pub fn spawn_stake_wighted_qos_server(
⋮----
pub fn spawn_stake_wighted_qos_server(
⋮----
stats.clone(),
⋮----
cancel.clone(),
⋮----
spawn_runtime_and_server(
⋮----
/// Spawns a tokio runtime and a streamer instance inside it.
pub fn spawn_simple_qos_server(
⋮----
pub fn spawn_simple_qos_server(
⋮----
mod test {
⋮----
fn rt_for_test() -> Runtime {
rt(
"solQuicTestRt".to_string(),
⋮----
fn setup_simple_qos_quic_server(
⋮----
let s = bind_to_localhost_unique().expect("should bind");
let (sender, receiver) = unbounded();
⋮----
let server_address = s.local_addr().unwrap();
⋮----
} = spawn_simple_qos_server(
⋮----
fn setup_swqos_quic_server() -> (
⋮----
} = spawn_stake_wighted_qos_server(
⋮----
fn test_quic_server_exit() {
let (t, _receiver, _server_address, cancel) = setup_swqos_quic_server();
cancel.cancel();
t.join().unwrap();
⋮----
fn test_quic_timeout() {
⋮----
let (t, receiver, server_address, cancel) = setup_swqos_quic_server();
let runtime = rt_for_test();
runtime.block_on(check_timeout(receiver, server_address));
⋮----
fn test_quic_server_block_multiple_connections() {
⋮----
let (t, _receiver, server_address, cancel) = setup_swqos_quic_server();
⋮----
runtime.block_on(check_block_multiple_connections(server_address));
⋮----
fn test_quic_server_multiple_streams() {
⋮----
runtime.block_on(check_multiple_streams(receiver, server_address, None));
⋮----
fn test_quic_server_multiple_writes() {
⋮----
runtime.block_on(check_multiple_writes(receiver, server_address, None));
⋮----
fn test_quic_server_multiple_packets_with_simple_qos() {
⋮----
(client_keypair.pubkey(), 1_000),
(rich_node_keypair.pubkey(), 1_000_000_000),
⋮----
setup_simple_qos_quic_server(server_params, Arc::new(RwLock::new(staked_nodes)));
⋮----
runtime.block_on(check_multiple_packets_with_client_id(
⋮----
Some(&client_keypair),
Some(&rich_node_keypair),
⋮----
fn test_quic_server_unstaked_node_connect_failure() {
⋮----
let (sender, _) = unbounded();
⋮----
runtime.block_on(check_unstaked_node_connect_failure(server_address));
⋮----
async fn check_multiple_packets_with_client_id(
⋮----
let conn1 = Arc::new(make_client_endpoint(&server_address, client_keypair1).await);
let conn2 = Arc::new(make_client_endpoint(&server_address, client_keypair2).await);
debug!(
⋮----
let expected_client_pubkey_1 = client_keypair1.map(|kp| kp.pubkey());
let expected_client_pubkey_2 = client_keypair2.map(|kp| kp.pubkey());
⋮----
debug!("Sending stream pair {i}");
let c1 = conn1.clone();
let c2 = conn2.clone();
let mut s1 = c1.open_uni().await.unwrap();
let mut s2 = c2.open_uni().await.unwrap();
s1.write_all(&[0u8]).await.unwrap();
s1.finish().unwrap();
debug!("Stream {i}.1 sent and finished");
⋮----
s2.write_all(&[1u8]).await.unwrap();
s2.finish().unwrap();
debug!("Stream {i}.2 sent and finished");
⋮----
debug!("All streams sent, expecting {num_packets_sent} packets with client ID");
⋮----
while now.elapsed().as_secs() < 2 {
⋮----
match receiver.try_recv() {
⋮----
debug!("Received packet batch (iteration {iterations})");
⋮----
panic!("Expected PacketBatch::Simple but got PacketBatch::Bytes");
⋮----
panic!("Expected PacketBatch::Simple but got PacketBatch::Pinned");
⋮----
if *packet.data(0).unwrap() == 0u8 {
debug!("Packet from stream with client 1");
assert_eq!(packet.meta().remote_pubkey(), expected_client_pubkey_1);
} else if *packet.data(0).unwrap() == 1u8 {
debug!("Packet from stream with client 2");
assert_eq!(packet.meta().remote_pubkey(), expected_client_pubkey_2);
⋮----
panic!("Unexpected data in packet: {:?}", packet.data(0));
⋮----
debug!("No packets yet (iteration {iterations}): {e:?}");
⋮----
sleep(Duration::from_millis(100)).await;
⋮----
debug!("Received all expected packets with client ID!");
⋮----
assert!(

================
File: streamer/src/recvmmsg.rs
================
pub use solana_perf::packet::PACKETS_PER_BATCH;
⋮----
pub fn recv_mmsg(socket: &UdpSocket, packets: &mut [Packet]) -> io::Result< usize> {
debug_assert!(packets.iter().all(|pkt| pkt.meta() == &Meta::default()));
⋮----
let count = cmp::min(PACKETS_PER_BATCH, packets.len());
for p in packets.iter_mut().take(count) {
p.meta_mut().size = 0;
match socket.recv_from(p.buffer_mut()) {
⋮----
return Err(e);
⋮----
p.meta_mut().size = nrecv;
p.meta_mut().set_socket_addr(&from);
⋮----
socket.set_nonblocking(true)?;
⋮----
Ok(i)
⋮----
fn cast_socket_addr(addr: &sockaddr_storage, hdr: &mmsghdr) -> Option<SocketAddr> {
⋮----
return Some(SocketAddr::V4(SocketAddrV4::new(
std::net::Ipv4Addr::from(addr.sin_addr.s_addr.to_ne_bytes()),
⋮----
return Some(SocketAddr::V6(SocketAddrV6::new(
⋮----
error!(
⋮----
pub fn recv_mmsg(sock: &UdpSocket, packets: &mut [Packet]) -> io::Result< usize> {
if packets.is_empty() {
return Ok(0);
⋮----
let sock_fd = sock.as_raw_fd();
let count = cmp::min(iovs.len(), packets.len());
⋮----
izip!(packets.iter_mut(), &mut hdrs, &mut iovs, &mut addrs).take(count)
⋮----
let buffer = packet.buffer_mut();
iov.write(iovec {
iov_base: buffer.as_mut_ptr() as *mut libc::c_void,
iov_len: buffer.len(),
⋮----
let msg_hdr = create_msghdr(addr, SOCKADDR_STORAGE_SIZE, iov);
hdr.write(mmsghdr {
⋮----
hdrs[0].assume_init_mut(),
⋮----
MSG_WAITFORONE.try_into().unwrap(),
⋮----
return Err(io::Error::last_os_error());
⋮----
usize::try_from(nrecv).unwrap()
⋮----
for (addr, hdr, pkt) in izip!(addrs, hdrs, packets.iter_mut()).take(nrecv) {
let hdr_ref = unsafe { hdr.assume_init_ref() };
let addr_ref = unsafe { addr.assume_init_ref() };
pkt.meta_mut().size = hdr_ref.msg_len as usize;
if let Some(addr) = cast_socket_addr(addr_ref, hdr_ref) {
pkt.meta_mut().set_socket_addr(&addr);
⋮----
for (iov, addr, hdr) in izip!(&mut iovs, &mut addrs, &mut hdrs).take(count) {
⋮----
iov.assume_init_drop();
addr.assume_init_drop();
hdr.assume_init_drop();
⋮----
Ok(nrecv)
⋮----
mod tests {
⋮----
type TestConfig = (UdpSocket, SocketAddr, UdpSocket, SocketAddr);
fn test_setup_reader_sender(ip: IpAddr) -> io::Result<TestConfig> {
let port_range = unique_port_range_for_tests(2);
let reader = bind_in_range_with_config(
⋮----
let reader_addr = reader.local_addr()?;
let sender = bind_in_range_with_config(
⋮----
let sender_addr = sender.local_addr()?;
Ok((reader, reader_addr, sender, sender_addr))
⋮----
pub fn test_recv_mmsg_one_iter() {
⋮----
sender.send_to(&data[..], addr).unwrap();
⋮----
let mut packets = vec![Packet::default(); TEST_NUM_MSGS];
let recv = recv_mmsg(&reader, &mut packets[..]).unwrap();
assert_eq!(sent, recv);
for packet in packets.iter().take(recv) {
assert_eq!(packet.meta().size, PACKET_DATA_SIZE);
assert_eq!(packet.meta().socket_addr(), saddr);
⋮----
test_one_iter(test_setup_reader_sender(IpAddr::V4(Ipv4Addr::LOCALHOST)).unwrap());
match test_setup_reader_sender(IpAddr::V6(Ipv6Addr::LOCALHOST)) {
Ok(config) => test_one_iter(config),
Err(e) => warn!("Failed to configure IPv6: {e:?}"),
⋮----
pub fn test_recv_mmsg_multi_iter() {
⋮----
assert_eq!(TEST_NUM_MSGS, recv);
⋮----
.iter_mut()
.for_each(|pkt| *pkt.meta_mut() = Meta::default());
⋮----
assert_eq!(sent - TEST_NUM_MSGS, recv);
⋮----
test_multi_iter(test_setup_reader_sender(IpAddr::V4(Ipv4Addr::LOCALHOST)).unwrap());
⋮----
Ok(config) => test_multi_iter(config),
⋮----
pub fn test_recv_mmsg_multi_iter_timeout() {
⋮----
test_setup_reader_sender(IpAddr::V4(Ipv4Addr::LOCALHOST)).unwrap();
reader.set_read_timeout(Some(Duration::new(5, 0))).unwrap();
reader.set_nonblocking(false).unwrap();
⋮----
sender.send_to(&data[..], reader_addr).unwrap();
⋮----
assert_eq!(packet.meta().socket_addr(), sender_addr);
⋮----
reader.set_nonblocking(true).unwrap();
⋮----
let _recv = recv_mmsg(&reader, &mut packets[..]);
assert!(start.elapsed().as_secs() < 5);
⋮----
pub fn test_recv_mmsg_multi_addrs() {
⋮----
let port_range = localhost_port_range_for_tests();
let reader = bind_in_range_with_config(ip, port_range, SocketConfig::default())
.unwrap()
⋮----
let reader_addr = reader.local_addr().unwrap();
let sender1 = bind_in_range_with_config(ip, port_range, SocketConfig::default())
⋮----
let sender1_addr = sender1.local_addr().unwrap();
⋮----
let sender2 = bind_in_range_with_config(ip, port_range, SocketConfig::default())
⋮----
let sender_addr = sender2.local_addr().unwrap();
⋮----
sender1.send_to(&data[..], reader_addr).unwrap();
⋮----
sender2.send_to(&data[..], reader_addr).unwrap();
⋮----
for packet in packets.iter().take(sent1) {
⋮----
assert_eq!(packet.meta().socket_addr(), sender1_addr);
⋮----
for packet in packets.iter().skip(sent1).take(recv - sent1) {
⋮----
assert_eq!(sent1 + sent2 - TEST_NUM_MSGS, recv);

================
File: streamer/src/sendmmsg.rs
================
pub enum SendPktsError {
⋮----
fn from(err: SendPktsError) -> Self {
Self::Custom(format!("{err:?}"))
⋮----
pub fn batch_send<'a, S, T: 'a + ?Sized>(
⋮----
if let Err(e) = sock.send_to(p.as_ref(), a.borrow()) {
⋮----
if erropt.is_none() {
erropt = Some(e);
⋮----
Err(SendPktsError::IoError(err, num_failed))
⋮----
Ok(())
⋮----
fn mmsghdr_for_packet(
⋮----
iov.write(iovec {
iov_base: packet.as_ptr() as *mut libc::c_void,
iov_len: packet.len(),
⋮----
let ptr: *mut sockaddr_in = addr.as_mut_ptr() as *mut _;
⋮----
*nix::sys::socket::SockaddrIn::from(*socket_addr_v4).as_ref(),
⋮----
(ptr as *mut u8).add(SIZE_OF_SOCKADDR_IN),
⋮----
let ptr: *mut sockaddr_in6 = addr.as_mut_ptr() as *mut _;
⋮----
*nix::sys::socket::SockaddrIn6::from(*socket_addr_v6).as_ref(),
⋮----
(ptr as *mut u8).add(SIZE_OF_SOCKADDR_IN6),
⋮----
let msg_hdr = create_msghdr(addr, msg_namelen, iov);
hdr.write(mmsghdr {
⋮----
fn sendmmsg_retry(sock: &UdpSocket, hdrs: &mut [mmsghdr]) -> Result<(), SendPktsError> {
let sock_fd = sock.as_raw_fd();
⋮----
while !pkts.is_empty() {
let npkts = match unsafe { libc::sendmmsg(sock_fd, &mut pkts[0], pkts.len() as u32, 0) } {
⋮----
erropt = Some(io::Error::last_os_error());
⋮----
Err(SendPktsError::IoError(err, hdrs.len() - total_sent))
⋮----
fn batch_send_max_iov<'a, S, T: 'a + ?Sized>(
⋮----
let packets = packets.into_iter();
let num_packets = packets.len();
debug_assert!(num_packets <= MAX_IOV);
⋮----
for ((pkt, dest), hdr, iov, addr) in izip!(packets, &mut hdrs, &mut iovs, &mut addrs) {
mmsghdr_for_packet(pkt.as_ref(), dest.borrow(), iov, addr, hdr);
⋮----
unsafe { std::slice::from_raw_parts_mut(hdrs.as_mut_ptr() as *mut mmsghdr, num_packets) };
let result = sendmmsg_retry(sock, hdrs_slice);
for (hdr, iov, addr) in izip!(&mut hdrs, &mut iovs, &mut addrs).take(num_packets) {
⋮----
hdr.assume_init_drop();
iov.assume_init_drop();
addr.assume_init_drop();
⋮----
let mut packets = packets.into_iter();
⋮----
let chunk = packets.by_ref().take(MAX_IOV);
if chunk.len() == 0 {
⋮----
batch_send_max_iov(sock, chunk)?;
⋮----
pub fn multi_target_send<S, T>(
⋮----
let dests = dests.iter().map(Borrow::borrow);
let pkts = dests.map(|addr| (&packet, addr));
batch_send(sock, pkts)
⋮----
mod tests {
⋮----
pub fn test_send_mmsg_one_dest() {
let reader = bind_to_localhost_unique().expect("should bind - reader");
let addr = reader.local_addr().unwrap();
let sender = bind_to_localhost_unique().expect("should bind - sender");
let packets: Vec<_> = (0..32).map(|_| vec![0u8; PACKET_DATA_SIZE]).collect();
let packet_refs: Vec<_> = packets.iter().map(|p| (&p[..], &addr)).collect();
let sent = batch_send(&sender, packet_refs).ok();
assert_eq!(sent, Some(()));
let mut packets = vec![Packet::default(); 32];
let recv = recv_mmsg(&reader, &mut packets[..]).unwrap();
assert_eq!(32, recv);
⋮----
pub fn test_send_mmsg_multi_dest() {
let reader = bind_to_localhost_unique().expect("should bind - reader 1");
⋮----
let reader2 = bind_to_localhost_unique().expect("should bind - reader 2");
let addr2 = reader2.local_addr().unwrap();
⋮----
.iter()
.enumerate()
.map(|(i, p)| {
⋮----
.collect();
⋮----
assert_eq!(16, recv);
⋮----
let recv = recv_mmsg(&reader2, &mut packets[..]).unwrap();
⋮----
pub fn test_multicast_msg() {
⋮----
let reader3 = bind_to_localhost_unique().expect("should bind - reader 3");
let addr3 = reader3.local_addr().unwrap();
let reader4 = bind_to_localhost_unique().expect("should bind - reader 4");
let addr4 = reader4.local_addr().unwrap();
let sender = bind_to_localhost_unique().expect("should bind - reader 5");
⋮----
let sent = multi_target_send(
⋮----
packet.data(..).unwrap(),
⋮----
.ok();
⋮----
assert_eq!(1, recv);
⋮----
let recv = recv_mmsg(&reader3, &mut packets[..]).unwrap();
⋮----
let recv = recv_mmsg(&reader4, &mut packets[..]).unwrap();
⋮----
fn test_intermediate_failures_mismatched_bind() {
let packets: Vec<_> = (0..3).map(|_| vec![0u8; PACKET_DATA_SIZE]).collect();
⋮----
let packet_refs: Vec<_> = vec![
⋮----
let dest_refs: Vec<_> = vec![&ip4, &ip6, &ip4];
⋮----
let res = batch_send(&sender, packet_refs);
assert_matches!(res, Err(SendPktsError::IoError(_,  1)));
let res = multi_target_send(&sender, &packets[0], &dest_refs);
⋮----
fn test_intermediate_failures_unreachable_address() {
let packets: Vec<_> = (0..5).map(|_| vec![0u8; PACKET_DATA_SIZE]).collect();
⋮----
match batch_send(&sender, packet_refs) {
Ok(()) => panic!(),
⋮----
assert_matches!(ioerror.kind(), ErrorKind::PermissionDenied);
assert_eq!(num_failed, 2);
⋮----
assert_eq!(num_failed, 3);
⋮----
let dest_refs: Vec<_> = vec![
⋮----
match multi_target_send(&sender, &packets[0], &dest_refs) {

================
File: streamer/src/streamer.rs
================
pub trait ChannelSend<T>: Send + 'static {
⋮----
fn send(&self, msg: T) -> std::result::Result<(), SendError<T>> {
self.send(msg)
⋮----
fn try_send(&self, msg: T) -> std::result::Result<(), TrySendError<T>> {
self.try_send(msg)
⋮----
fn is_empty(&self) -> bool {
self.is_empty()
⋮----
fn len(&self) -> usize {
self.len()
⋮----
pub struct StakedNodes {
⋮----
pub type PacketBatchReceiver = Receiver<PacketBatch>;
pub type PacketBatchSender = Sender<PacketBatch>;
⋮----
pub enum StreamerError {
⋮----
pub struct StreamerReceiveStats {
⋮----
impl StreamerReceiveStats {
pub fn new(name: &'static str) -> Self {
⋮----
pub fn report(&self) {
datapoint_info!(
⋮----
pub type Result<T> = std::result::Result<T, StreamerError>;
fn recv_loop<P: SocketProvider>(
⋮----
fn setup_socket(socket: &UdpSocket) -> Result<()> {
⋮----
socket.set_read_timeout(Some(SOCKET_READ_TIMEOUT))?;
⋮----
socket.set_nonblocking(true)?;
Ok(())
⋮----
let mut socket = provider.current_socket_ref();
setup_socket(socket)?;
⋮----
let mut poll_fd = [PollFd::new(socket.as_fd(), PollFlags::POLLIN)];
⋮----
packet_batch.resize(PACKETS_PER_BATCH, Packet::default());
⋮----
if exit.load(Ordering::Relaxed) {
return Ok(());
⋮----
if in_vote_only_mode.load(Ordering::Relaxed) {
sleep(Duration::from_millis(1));
⋮----
packets_count.fetch_add(len, Ordering::Relaxed);
packet_batches_count.fetch_add(1, Ordering::Relaxed);
max_channel_len.fetch_max(packet_batch_sender.len(), Ordering::Relaxed);
⋮----
full_packet_batches_count.fetch_add(1, Ordering::Relaxed);
⋮----
.iter_mut()
.for_each(|p| p.meta_mut().set_from_staked_node(is_staked_service));
match packet_batch_sender.try_send(packet_batch.into()) {
⋮----
stats.num_packets_dropped.fetch_add(len, Ordering::Relaxed);
⋮----
return Err(StreamerError::Send(SendError(err)))
⋮----
if let CurrentSocket::Changed(s) = provider.current_socket() {
⋮----
poll_fd = [PollFd::new(socket.as_fd(), PollFlags::POLLIN)];
⋮----
pub fn receiver(
⋮----
.name(thread_name)
.spawn(move || {
⋮----
let _ = recv_loop(
⋮----
.unwrap()
⋮----
pub fn receiver_atomic(
⋮----
struct SendStats {
⋮----
struct StreamerSendStats {
⋮----
impl StreamerSendStats {
fn report_stats(
⋮----
let sample_ms = sample_duration.map(|d| d.as_millis()).unwrap_or_default();
⋮----
host_map.iter().for_each(|(_addr, host_stats)| {
hist.increment(host_stats.bytes).unwrap();
⋮----
let num_entries = host_map.len();
let mut entries: Vec<_> = host_map.into_iter().collect();
if entries.len() > MAX_REPORT_ENTRIES {
entries.select_nth_unstable_by_key(MAX_REPORT_ENTRIES, |(_addr, stats)| {
Reverse(stats.bytes)
⋮----
entries.truncate(MAX_REPORT_ENTRIES);
⋮----
info!("streamer send {name} hosts: count:{num_entries} {entries:?}");
⋮----
fn maybe_submit(&mut self, name: &'static str, sender: &Sender<Box<dyn FnOnce() + Send>>) {
⋮----
let elapsed = self.since.as_ref().map(Instant::elapsed);
if elapsed.map(|e| e < SUBMIT_CADENCE).unwrap_or_default()
&& self.host_map.len() < MAP_SIZE_REPORTING_THRESHOLD
⋮----
let _ = sender.send(Box::new(move || {
⋮----
since: Some(Instant::now()),
⋮----
fn record(&mut self, pkt: PacketRef) {
let ent = self.host_map.entry(pkt.meta().addr).or_default();
⋮----
ent.bytes += pkt.data(..).map(<[u8]>::len).unwrap_or_default() as u64;
⋮----
impl StakedNodes {
fn calculate_total_stake(
⋮----
.iter()
.filter(|(pubkey, _)| !overrides.contains_key(pubkey))
.map(|(_, &stake)| stake)
.chain(overrides.values().copied())
.sum()
⋮----
pub fn new(stakes: Arc<HashMap<Pubkey, u64>>, overrides: HashMap<Pubkey, u64>) -> Self {
⋮----
pub fn get_node_stake(&self, pubkey: &Pubkey) -> Option<u64> {
⋮----
.get(pubkey)
.or_else(|| self.stakes.get(pubkey))
.filter(|&&stake| stake > 0)
.copied()
⋮----
pub fn total_stake(&self) -> u64 {
⋮----
pub fn update_stake_map(&mut self, stakes: Arc<HashMap<Pubkey, u64>>) {
⋮----
fn recv_send(
⋮----
let packet_batch = r.recv_timeout(timer)?;
⋮----
packet_batch.iter().for_each(|p| stats.record(p));
⋮----
let packets = packet_batch.iter().filter_map(|pkt| {
let addr = pkt.meta().socket_addr();
let data = pkt.data(..)?;
socket_addr_space.check(&addr).then_some((data, addr))
⋮----
batch_send(sock, packets.collect::<Vec<_>>())?;
⋮----
pub fn recv_packet_batches(
⋮----
let packet_batch = recvr.recv_timeout(timer)?;
trace!("got packets");
let mut num_packets = packet_batch.len();
let mut packet_batches = vec![packet_batch];
while let Ok(packet_batch) = recvr.try_recv() {
trace!("got more packets");
num_packets += packet_batch.len();
packet_batches.push(packet_batch);
⋮----
let recv_duration = recv_start.elapsed();
trace!(
⋮----
Ok((packet_batches, num_packets, recv_duration))
⋮----
pub fn responder_atomic(
⋮----
.name(format!("solRspndr{name}"))
⋮----
responder_loop(
⋮----
pub fn responder(
⋮----
fn responder_loop<P: SocketProvider>(
⋮----
if stats_reporter_sender.is_some() {
stats = Some(StreamerSendStats::default());
⋮----
let sock = provider.current_socket_ref();
if let Err(e) = recv_send(sock, &r, &socket_addr_space, &mut stats) {
⋮----
last_error = Some(e);
⋮----
let now = timestamp();
⋮----
datapoint_info!(name, ("errors", errors, i64),);
info!("{name} last-error: {last_error:?} count: {errors}");
⋮----
stats.maybe_submit(name, stats_reporter_sender);
⋮----
mod test {
⋮----
fn get_packet_batches(r: PacketBatchReceiver, num_packets: &mut usize) {
⋮----
let packet_batch_res = r.recv_timeout(Duration::new(1, 0));
if packet_batch_res.is_err() {
⋮----
*num_packets -= packet_batch_res.unwrap().len();
⋮----
fn streamer_debug() {
write!(io::sink(), "{:?}", Packet::default()).unwrap();
write!(io::sink(), "{:?}", RecycledPacketBatch::default()).unwrap();
⋮----
fn streamer_send_test() {
let read = bind_to_localhost_unique().expect("should bind reader");
read.set_read_timeout(Some(SOCKET_READ_TIMEOUT)).unwrap();
let addr = read.local_addr().unwrap();
let send = bind_to_localhost_unique().expect("should bind sender");
⋮----
let (s_reader, r_reader) = unbounded();
⋮----
let t_receiver = receiver(
"solRcvrTest".to_string(),
⋮----
exit.clone(),
⋮----
stats.clone(),
Some(Duration::from_millis(1)),
⋮----
let (s_responder, r_responder) = unbounded();
let t_responder = responder(
⋮----
p.buffer_mut()[0] = i as u8;
p.meta_mut().size = PACKET_DATA_SIZE;
p.meta_mut().set_socket_addr(&addr);
⋮----
packet_batch.push(p);
⋮----
s_responder.send(packet_batch).expect("send");
⋮----
get_packet_batches(r_reader, &mut packets_remaining);
assert_eq!(packets_remaining, 0);
exit.store(true, Ordering::Relaxed);
assert!(stats.packet_batches_count.load(Ordering::Relaxed) >= 1);
assert_eq!(stats.packets_count.load(Ordering::Relaxed), NUM_PACKETS);
assert_eq!(stats.full_packet_batches_count.load(Ordering::Relaxed), 0);
t_receiver.join().expect("join");
t_responder.join().expect("join");

================
File: streamer/tests/recvmmsg.rs
================
pub fn test_recv_mmsg_batch_size() {
let reader = bind_to_localhost_unique().expect("should bind - reader");
let addr = reader.local_addr().unwrap();
let sender = bind_to_localhost_unique().expect("should bind - sender");
⋮----
(0..1000).for_each(|_| {
⋮----
sender.send_to(&data[..], addr).unwrap();
⋮----
let mut packets = vec![Packet::default(); TEST_BATCH_SIZE];
⋮----
let recv = recv_mmsg(&reader, &mut packets[..]).unwrap();
elapsed_in_max_batch += now.elapsed().as_nanos();
⋮----
assert!(num_max_batches > 990);
⋮----
let mut packets = vec![Packet::default(); 4];
⋮----
while let Ok(num) = recv_mmsg(&reader, &mut packets[..]) {
⋮----
.iter_mut()
.for_each(|pkt| *pkt.meta_mut() = Meta::default());
⋮----
elapsed_in_small_batch += now.elapsed().as_nanos();
assert_eq!(TEST_BATCH_SIZE, recv);
⋮----
assert!(elapsed_in_max_batch <= elapsed_in_small_batch);

================
File: streamer/Cargo.toml
================
[package]
name = "solana-streamer"
description = "Solana Streamer"
documentation = "https://docs.rs/solana-streamer"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_streamer"

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
arc-swap = { workspace = true }
bytes = { workspace = true }
crossbeam-channel = { workspace = true }
dashmap = { workspace = true }
futures = { workspace = true }
futures-util = { workspace = true }
histogram = { workspace = true }
indexmap = { workspace = true }
itertools = { workspace = true }
libc = { workspace = true }
log = { workspace = true }
nix = { workspace = true, features = ["net", "poll", "signal"] }
num_cpus = { workspace = true }
pem = { workspace = true }
percentage = { workspace = true }
quinn = { workspace = true }
quinn-proto = { workspace = true }
rand = { workspace = true }
rustls = { workspace = true }
smallvec = { workspace = true }
socket2 = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-metrics = { workspace = true }
solana-net-utils = { workspace = true, features = ["agave-unstable-api"] }
solana-packet = { workspace = true }
solana-perf = { workspace = true }
solana-pubkey = { workspace = true }
solana-quic-definitions = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-time-utils = { workspace = true }
solana-tls-utils = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-metrics-tracker = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tokio-util = { workspace = true, features = ["rt"] }
x509-parser = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
anyhow = { workspace = true }
assert_matches = { workspace = true }
chrono = { workspace = true, features = ["now"] }
clap = { version = "4.5.31", features = ["cargo", "derive", "error-context"] }
solana-net-utils = { workspace = true, features = ["dev-context-only-utils"] }
solana-streamer = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }

================
File: svm/doc/diagrams/context.svg
================
<?xml version='1.0' encoding='UTF-8'?>
<!-- This file was generated by dvisvgm 2.13.1 -->
<svg version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' width='394.227615pt' height='241.8755pt' viewBox='19.925635 -.00025 394.227615 241.8755'>
<defs>
<clipPath id='clip1'>
<path d='M74.1133 17.0703H34.4258C29.7305 17.0703 25.9219 20.875 25.9219 25.5742V65.2578C25.9219 69.9531 29.7305 73.7617 34.4258 73.7617H74.1133C78.8086 73.7617 82.6172 69.9531 82.6172 65.2578V25.5742C82.6172 20.875 78.8086 17.0703 74.1133 17.0703ZM25.9219 73.7617'/>
</clipPath>
<clipPath id='clip2'>
<path d='M269.0702 17.0703H169.5975C164.8983 17.0703 161.0936 20.875 161.0936 25.5742V65.2578C161.0936 69.9531 164.8983 73.7617 169.5975 73.7617H269.0702C273.7655 73.7617 277.5741 69.9531 277.5741 65.2578V25.5742C277.5741 20.875 273.7655 17.0703 269.0702 17.0703ZM161.0936 73.7617'/>
</clipPath>
<clipPath id='clip3'>
<path d='M345.0704 109.6879H276.9805C272.2852 109.6879 268.4766 113.4965 268.4766 118.1918V157.8793C268.4766 162.5746 272.2852 166.3832 276.9805 166.3832H345.0704C349.7696 166.3832 353.5743 162.5746 353.5743 157.8793V118.1918C353.5743 113.4965 349.7696 109.6879 345.0704 109.6879ZM268.4766 166.3832'/>
</clipPath>
<clipPath id='clip4'>
<path d='M401.5308 181.0663H361.8472C357.148 181.0663 353.3433 184.871 353.3433 189.5702V229.2538C353.3433 233.9531 357.148 237.7577 361.8472 237.7577H401.5308C406.23 237.7577 410.0347 233.9531 410.0347 229.2538V189.5702C410.0347 184.871 406.23 181.0663 401.5308 181.0663ZM353.3433 237.7577'/>
</clipPath>
<clipPath id='clip5'>
<path d='M76.1953 182.1246H29.0273C24.3281 182.1246 20.5234 185.9293 20.5234 190.6285V230.3121C20.5234 235.0113 24.3281 238.816 29.0273 238.816H76.1953C80.8906 238.816 84.6992 235.0113 84.6992 230.3121V190.6285C84.6992 185.9293 80.8906 182.1246 76.1953 182.1246ZM20.5234 238.816'/>
</clipPath>
<font id='rm-lmr10' horiz-adv-x='0'>
<font-face font-family='rm-lmr10' units-per-em='1000' ascent='1127' descent='290'/>
<glyph unicode='M' horiz-adv-x='917' vert-adv-y='917' glyph-name='M' d='M879 0V31H855C778 31 776 42 776 78V605C776 641 778 652 855 652H879V683H710C684 683 684 682 677 664L458 101L241 661C232 683 229 683 206 683H37V652H61C138 652 140 641 140 605V105C140 78 140 31 37 31V0L154 3L271 0V31C168 31 168 78 168 105V644H169L410 22C415 9 420 0 430 0C441 0 444 8 448 19L694 652H695V78C695 42 693 31 616 31H592V0C629 3 697 3 736 3S842 3 879 0Z'/>
<glyph unicode='S' horiz-adv-x='556' vert-adv-y='556' glyph-name='S' d='M499 186C499 286 433 368 349 388L221 419C159 434 120 488 120 546C120 616 174 677 252 677C419 677 441 513 447 468C448 462 448 456 459 456C472 456 472 461 472 480V681C472 698 472 705 461 705C454 705 453 704 446 692L411 635C381 664 340 705 251 705C140 705 56 617 56 511C56 428 109 355 187 328C198 324 249 312 319 295C346 288 376 281 404 244C425 218 435 185 435 152C435 81 385 9 301 9C272 9 196 14 143 63C85 117 82 181 81 217C80 227 72 227 69 227C56 227 56 220 56 202V2C56-15 56-22 67-22C74-22 75-20 82-9C82-9 85-5 118 48C149 14 213-22 302-22C419-22 499 76 499 186Z'/>
<glyph unicode='V' horiz-adv-x='750' vert-adv-y='750' glyph-name='V' d='M730 652V683C699 681 659 680 633 680L519 683V652C571 651 592 625 592 602C592 594 589 588 587 582L404 100L213 605C207 619 207 623 207 623C207 652 264 652 289 652V683C253 680 184 680 146 680L19 683V652C84 652 103 652 117 614L349 0C356-19 361-22 374-22C391-22 393-17 398-3L621 585C635 622 662 651 730 652Z'/>
<glyph unicode='e' horiz-adv-x='444' vert-adv-y='444' glyph-name='e' d='M415 119C415 129 407 131 402 131C393 131 391 125 389 117C354 14 264 14 254 14C204 14 164 44 141 81C111 129 111 195 111 231H390C412 231 415 231 415 252C415 351 361 448 236 448C120 448 28 345 28 220C28 86 133-11 248-11C370-11 415 100 415 119ZM349 252H112C118 401 202 426 236 426C339 426 349 291 349 252Z'/>
<glyph unicode='i' horiz-adv-x='278' vert-adv-y='278' glyph-name='i' d='M247 0V31C181 31 177 36 177 75V442L37 431V400C102 400 111 394 111 345V76C111 31 100 31 33 31V0L143 3C178 3 213 1 247 0ZM192 604C192 631 169 657 139 657C105 657 85 629 85 604C85 577 108 551 138 551C172 551 192 579 192 604Z'/>
<glyph unicode='m' horiz-adv-x='833' vert-adv-y='833' glyph-name='m' d='M813 0V31C761 31 736 31 735 61V252C735 338 735 369 704 405C690 422 657 442 599 442C515 442 471 382 454 344C440 431 366 442 321 442C248 442 201 399 173 337V442L32 431V400C102 400 110 393 110 344V76C110 31 99 31 32 31V0L145 3L257 0V31C190 31 179 31 179 76V260C179 364 250 420 314 420C377 420 388 366 388 309V76C388 31 377 31 310 31V0L423 3L535 0V31C468 31 457 31 457 76V260C457 364 528 420 592 420C655 420 666 366 666 309V76C666 31 655 31 588 31V0L701 3L813 0Z'/>
<glyph unicode='n' horiz-adv-x='556' vert-adv-y='556' glyph-name='n' d='M535 0V31C483 31 458 31 457 61V252C457 338 457 369 426 405C412 422 379 442 321 442C248 442 201 399 173 337V442L32 431V400C102 400 110 393 110 344V76C110 31 99 31 32 31V0L145 3L257 0V31C190 31 179 31 179 76V260C179 364 250 420 314 420C377 420 388 366 388 309V76C388 31 377 31 310 31V0L423 3L535 0Z'/>
<glyph unicode='r' horiz-adv-x='392' vert-adv-y='392' glyph-name='r' d='M364 381C364 413 333 442 290 442C217 442 181 375 167 332V442L28 431V400C98 400 106 393 106 344V76C106 31 95 31 28 31V0L142 3C182 3 229 3 269 0V31H248C174 31 172 42 172 78V232C172 331 214 420 290 420C297 420 299 420 301 419C298 418 278 406 278 380C278 352 299 337 321 337C339 337 364 349 364 381Z'/>
<glyph unicode='t' horiz-adv-x='389' vert-adv-y='389' glyph-name='t' d='M332 124V181H307V126C307 52 277 14 240 14C173 14 173 105 173 122V400H316V431H173V615H148C147 533 117 426 19 422V400H104V124C104 1 197-11 233-11C304-11 332 60 332 124Z'/>
<glyph unicode='u' horiz-adv-x='556' vert-adv-y='556' glyph-name='u' d='M535 0V31C465 31 457 38 457 87V442L310 431V400C380 400 388 393 388 344V166C388 79 340 11 267 11C183 11 179 58 179 110V442L32 431V400C110 400 110 397 110 308V158C110 80 110-11 262-11C318-11 362 17 391 79V-11L535 0Z'/>
</font>
<font id='rm-lmtt10' horiz-adv-x='0'>
<font-face font-family='rm-lmtt10' units-per-em='1000' ascent='1016' descent='316'/>
<glyph unicode='-' horiz-adv-x='525' vert-adv-y='525' glyph-name='hyphen' d='M468 306C468 341 433 341 421 341H103C91 341 56 341 56 306S91 271 103 271H421C433 271 468 271 468 306Z'/>
<glyph unicode='B' horiz-adv-x='525' vert-adv-y='525' glyph-name='B' d='M482 167C482 236 434 301 353 320C417 342 461 394 461 456C461 529 400 611 289 611H63C49 611 23 611 23 581C23 550 48 550 64 550H91V61H64C48 61 23 61 23 30C23 0 49 0 63 0H309C422 0 482 89 482 167ZM392 455C392 414 360 348 263 348H160V550H285C370 550 392 488 392 455ZM413 169C413 120 374 61 287 61H160V287H298C383 287 413 214 413 169Z'/>
<glyph unicode='F' horiz-adv-x='525' vert-adv-y='525' glyph-name='F' d='M490 497V570C490 601 484 611 450 611H69C53 611 28 611 28 580C28 550 54 550 69 550H102V61H69C53 61 28 61 28 30C28 0 54 0 69 0H223C238 0 263 0 263 30C263 61 239 61 223 61H171V275H307C307 234 307 218 342 218C376 218 376 240 376 259V352C376 371 376 393 342 393C307 393 307 377 307 336H171V550H421V497C421 478 421 456 455 456C490 456 490 477 490 497Z'/>
<glyph unicode='M' horiz-adv-x='525' vert-adv-y='525' glyph-name='M' d='M507 30C507 61 486 61 454 61V550C486 550 507 550 507 581C507 611 482 611 467 611H411C370 611 366 597 357 571L290 369C278 333 269 304 263 275H262C252 322 164 581 162 587C152 611 127 611 113 611H57C42 611 17 611 17 581C17 550 38 550 70 550V61C38 61 17 61 17 30C17 0 42 0 57 0H139C154 0 179 0 179 30C179 61 158 61 126 61V538H127C138 492 202 300 206 287C214 264 227 226 232 219C238 211 250 204 262 204C275 204 289 212 297 226C300 232 385 485 397 538H398V61C366 61 345 61 345 30C345 0 370 0 385 0H467C482 0 507 0 507 30Z'/>
<glyph unicode='P' horiz-adv-x='525' vert-adv-y='525' glyph-name='P' d='M480 428C480 521 406 611 291 611H67C52 611 26 611 26 581C26 550 52 550 67 550H97V61H67C51 61 26 61 26 30C26 0 52 0 67 0H196C211 0 237 0 237 30C237 61 211 61 196 61H166V245H291C407 245 480 335 480 428ZM411 428C411 375 368 306 272 306H166V550H272C368 550 411 481 411 428Z'/>
<glyph unicode='V' horiz-adv-x='525' vert-adv-y='525' glyph-name='V' d='M506 581C506 611 480 611 465 611H363C348 611 322 611 322 581C322 550 343 550 378 550L301 244C288 191 269 115 262 72H261C256 107 252 124 243 161L146 550C181 550 202 550 202 581C202 611 176 611 161 611H59C44 611 18 611 18 581C18 550 39 550 75 550L207 30C217-8 233-8 262-8C289-8 308-8 317 29L449 550C485 550 506 550 506 581Z'/>
<glyph unicode='a' horiz-adv-x='525' vert-adv-y='525' glyph-name='a' d='M519 31C519 61 494 61 479 61C437 61 427 66 419 69V285C419 356 365 440 221 440C178 440 76 440 76 367C76 337 97 321 121 321C136 321 164 330 165 367C165 375 166 376 186 378C200 379 213 379 222 379C298 379 350 348 350 277C173 274 50 224 50 128C50 59 113-6 215-6C252-6 314 1 361 32C383 1 431 0 469 0C497 0 519 0 519 31ZM350 134C350 111 350 90 311 72C275 55 230 55 222 55C160 55 119 89 119 128C119 177 205 214 350 218V134Z'/>
<glyph unicode='b' horiz-adv-x='525' vert-adv-y='525' glyph-name='b' d='M488 216C488 339 399 437 293 437C245 437 200 419 166 388V570C166 601 160 611 126 611H53C37 611 12 611 12 580C12 550 38 550 52 550H97V41C97 21 97 0 132 0C166 0 166 20 166 45C207 3 250-6 282-6C391-6 488 89 488 216ZM419 216C419 120 349 55 279 55C201 55 166 143 166 191V264C166 323 224 376 287 376C361 376 419 303 419 216Z'/>
<glyph unicode='c' horiz-adv-x='525' vert-adv-y='525' glyph-name='c' d='M466 109C466 137 437 137 431 137C415 137 405 135 398 115C392 102 373 55 299 55C214 55 142 125 142 216C142 264 170 379 305 379C326 379 365 379 365 370C366 335 385 321 409 321S454 338 454 367C454 440 350 440 305 440C133 440 73 304 73 216C73 96 167-6 293-6C432-6 466 92 466 109Z'/>
<glyph unicode='d' horiz-adv-x='525' vert-adv-y='525' glyph-name='d' d='M512 31C512 61 486 61 472 61H427V570C427 601 421 611 387 611H314C298 611 273 611 273 580C273 550 299 550 313 550H358V392C325 421 284 437 241 437C132 437 36 342 36 215C36 91 126-6 232-6C288-6 330 21 358 50C358 14 358 0 398 0H471C487 0 512 0 512 31ZM358 194C358 138 313 55 236 55C165 55 105 126 105 215C105 311 175 376 245 376C309 376 358 320 358 265V194Z'/>
<glyph unicode='e' horiz-adv-x='525' vert-adv-y='525' glyph-name='e' d='M464 109C464 137 435 137 429 137C411 137 403 134 396 115C374 64 320 55 292 55C217 55 142 105 126 191H424C445 191 464 191 464 228C464 342 400 440 270 440C151 440 55 340 55 217C55 95 156-6 285-6C417-6 464 84 464 109ZM393 251H127C141 324 200 379 270 379C322 379 384 354 393 251Z'/>
<glyph unicode='g' horiz-adv-x='525' vert-adv-y='525' glyph-name='g' d='M509 388C509 411 490 442 436 442C425 442 376 441 330 407C314 418 279 437 233 437C139 437 68 362 68 277C68 234 85 201 100 182C89 166 80 144 80 114C80 79 94 54 103 42C29-3 29-71 29-82C29-168 134-229 262-229S495-167 495-82C495-45 477 5 426 32C413 39 371 61 281 61H211C203 61 190 61 182 63C167 63 161 63 149 77C138 91 137 113 137 113C137 117 139 131 143 141C146 139 184 116 233 116C325 116 398 188 398 277C398 307 389 336 372 364C393 376 417 380 430 381C436 354 459 347 469 347C486 347 509 359 509 388ZM329 277C329 220 285 176 233 176C179 176 137 223 137 276C137 333 181 377 233 377C287 377 329 330 329 277ZM437-82C437-126 362-169 262-169S87-126 87-82C87-64 96-32 128-12C153 4 162 4 235 4C324 4 437 4 437-82Z'/>
<glyph unicode='i' horiz-adv-x='525' vert-adv-y='525' glyph-name='i' d='M455 30C455 61 429 61 414 61H309V390C309 421 303 431 269 431H127C112 431 86 431 86 401C86 370 112 370 127 370H240V61H119C103 61 78 61 78 30C78 0 104 0 119 0H414C429 0 455 0 455 30ZM313 555C313 583 291 605 263 605S213 583 213 555S235 505 263 505S313 527 313 555Z'/>
<glyph unicode='k' horiz-adv-x='525' vert-adv-y='525' glyph-name='k' d='M508 30C508 61 482 61 467 61H436L287 255L410 370H445C460 370 486 370 486 400C486 431 460 431 445 431H291C275 431 251 431 251 400C251 370 276 370 291 370H328L166 218V570C166 601 160 611 126 611H61C46 611 21 611 21 581C21 550 45 550 61 550H110V61H61C46 61 21 61 21 31C21 0 45 0 61 0H215C230 0 255 0 255 30C255 61 231 61 215 61H166V142L246 216L365 61C329 61 309 61 309 30C309 0 334 0 349 0H467C482 0 508 0 508 30Z'/>
<glyph unicode='m' horiz-adv-x='525' vert-adv-y='525' glyph-name='m' d='M516 30C516 61 495 61 459 61V303C459 323 459 437 370 437C340 437 299 424 271 385C256 419 228 437 195 437C163 437 133 423 109 401C107 431 87 431 69 431H37C22 431-4 431-4 401C-4 370 17 370 53 370V61C17 61-4 61-4 30C-4 0 23 0 37 0H125C140 0 166 0 166 30C166 61 145 61 109 61V240C109 329 150 376 191 376C214 376 228 359 228 294V61C209 61 184 61 184 30C184 0 211 0 225 0H300C315 0 341 0 341 30C341 61 320 61 284 61V240C284 329 325 376 366 376C389 376 403 359 403 294V61C384 61 359 61 359 30C359 0 386 0 400 0H475C490 0 516 0 516 30Z'/>
<glyph unicode='n' horiz-adv-x='525' vert-adv-y='525' glyph-name='n' d='M512 30C512 61 487 61 471 61H427V293C427 394 376 437 297 437C230 437 185 403 166 384C166 416 166 431 126 431H53C37 431 12 431 12 400C12 370 38 370 52 370H97V61H53C37 61 12 61 12 30C12 0 38 0 52 0H211C225 0 251 0 251 30C251 61 226 61 210 61H166V238C166 338 240 376 291 376C344 376 358 348 358 288V61H319C303 61 278 61 278 30C278 0 305 0 319 0H472C486 0 512 0 512 30Z'/>
<glyph unicode='o' horiz-adv-x='525' vert-adv-y='525' glyph-name='o' d='M467 216C467 341 374 440 262 440S57 341 57 216C57 89 152-6 262-6S467 90 467 216ZM398 223C398 130 336 55 262 55S126 130 126 223C126 314 191 379 262 379C334 379 398 314 398 223Z'/>
<glyph unicode='p' horiz-adv-x='525' vert-adv-y='525' glyph-name='p' d='M488 216C488 339 399 437 293 437C245 437 200 419 166 388C166 417 164 431 126 431H53C37 431 12 431 12 400C12 370 38 370 52 370H97V-161H53C37-161 12-161 12-192C12-222 38-222 52-222H211C225-222 251-222 251-192C251-161 226-161 210-161H166V45C207 3 250-6 282-6C391-6 488 89 488 216ZM419 216C419 120 349 55 279 55C201 55 166 143 166 191V264C166 323 224 376 287 376C361 376 419 303 419 216Z'/>
<glyph unicode='r' horiz-adv-x='525' vert-adv-y='525' glyph-name='r' d='M487 375C487 395 475 437 392 437C341 437 277 419 222 356V390C222 421 216 431 182 431H72C57 431 32 431 32 401C32 370 56 370 72 370H153V61H72C57 61 32 61 32 31C32 0 56 0 72 0H333C348 0 374 0 374 30C374 61 348 61 333 61H222V186C222 280 281 376 402 376C403 352 420 332 445 332C467 332 487 348 487 375Z'/>
<glyph unicode='s' horiz-adv-x='525' vert-adv-y='525' glyph-name='s' d='M459 125C459 229 330 250 298 255L230 267C201 271 133 283 133 322C133 348 165 379 260 379C343 379 357 349 360 323C361 306 363 289 394 289C429 289 429 310 429 330V399C429 415 429 440 399 440C375 440 371 426 369 419C325 440 281 440 262 440C95 440 72 358 72 322C72 230 177 213 269 199C318 191 398 178 398 125C398 88 361 55 270 55C223 55 167 66 142 144C137 162 133 173 107 173C72 173 72 152 72 132V35C72 19 72-6 102-6C111-6 127-5 139 32C188-4 241-6 269-6C427-6 459 77 459 125Z'/>
<glyph unicode='t' horiz-adv-x='525' vert-adv-y='525' glyph-name='t' d='M449 126C449 144 449 165 414 165C381 165 380 144 380 127C379 65 322 55 299 55C222 55 222 107 222 132V370H386C401 370 426 370 426 400C426 431 402 431 386 431H222V513C222 532 222 554 188 554C153 554 153 533 153 513V431H66C50 431 25 431 25 400C25 370 50 370 65 370H153V126C153 30 221-6 294-6C368-6 449 37 449 126Z'/>
<glyph unicode='u' horiz-adv-x='525' vert-adv-y='525' glyph-name='u' d='M512 31C512 61 486 61 472 61H427V390C427 421 421 431 387 431H314C298 431 273 431 273 400C273 370 299 370 313 370H358V157C358 67 277 55 245 55C166 55 166 88 166 120V390C166 421 160 431 126 431H53C37 431 12 431 12 400C12 370 38 370 52 370H97V114C97 18 166-6 239-6C280-6 321 4 358 32C359 0 380 0 398 0H471C487 0 512 0 512 31Z'/>
</font>
</defs>
<style type='text/css'>
<![CDATA[text.f0 {font-family:rm-lmtt10;font-size:9.96264px}
text.f1 {font-family:rm-lmr10;font-size:9.96264px}
]]>
</style>
<g id='page1'>
<path d='M247.9878 74.3594L282.0508 108.7656' stroke='#000' fill='none' stroke-width='.3985' stroke-miterlimit='10'/>
<path d='M282.339351 106.792619C281.987799 107.284817 282.120604 108.694972 282.257316 108.976227C281.979982 108.835602 280.569821 108.687152 280.073715 109.03872' stroke='#000' fill='none' stroke-width='.318773' stroke-miterlimit='10' stroke-linecap='round' stroke-linejoin='round'/>
<path d='M339.6838 166.9804L355.2228 182.6754' stroke='#000' fill='none' stroke-width='.3985' stroke-miterlimit='10'/>
<path d='M355.511985 180.707134C355.160419 181.199318 355.293234 182.605557 355.433869 182.89071C355.152612 182.746187 353.742465 182.597774 353.246368 182.949318' stroke='#000' fill='none' stroke-width='.318773' stroke-miterlimit='10' stroke-linecap='round' stroke-linejoin='round'/>
<path d='M83.2148 45.414061H160.0348' stroke='#000' fill='none' stroke-width='.3985' stroke-miterlimit='10'/>
<path d='M158.8399 43.82031C158.94146 44.417967 160.035215 45.316405 160.335996 45.414061C160.035215 45.515624 158.94146 46.410155 158.8399 47.00781' stroke='#000' fill='none' stroke-width='.31879' stroke-miterlimit='10' stroke-linecap='round' stroke-linejoin='round'/>
<path d='M53.98047 74.3594L52.90625 181.0664' stroke='#000' fill='none' stroke-width='.3985' stroke-miterlimit='10'/>
<path d='M54.511718 179.887131C53.914066 179.980889 53.003909 181.070724 52.902349 181.367603C52.80469 181.066827 51.921877 179.961356 51.324217 179.855891' stroke='#000' fill='none' stroke-width='.318778' stroke-miterlimit='10' stroke-linecap='round' stroke-linejoin='round'/>
<path d='M74.1133 17.0703H34.4258C29.7305 17.0703 25.9219 20.875 25.9219 25.5742V65.2578C25.9219 69.9531 29.7305 73.7617 34.4258 73.7617H74.1133C78.8086 73.7617 82.6172 69.9531 82.6172 65.2578V25.5742C82.6172 20.875 78.8086 17.0703 74.1133 17.0703Z' stroke='#808080' fill='none' stroke-width='1.19553' stroke-miterlimit='10'/>
<text class='f0' x='54.269831' y='45.415446' transform='matrix(1 0 0 1 -10.4609 2.9841)'>bank</text>
<path d='M269.0702 17.0703H169.5975C164.8983 17.0703 161.0936 20.875 161.0936 25.5742V65.2578C161.0936 69.9531 164.8983 73.7617 169.5975 73.7617H269.0702C273.7655 73.7617 277.5741 69.9531 277.5741 65.2578V25.5742C277.5741 20.875 273.7655 17.0703 269.0702 17.0703Z' stroke='#808080' fill='none' stroke-width='1.19553' stroke-miterlimit='10'/>
<text class='f0' x='54.269831' y='45.415446' transform='matrix(1 0 0 1 110.1442 1.8771)'>transaction<tspan x='117.034407'>processor</tspan></text>
<path d='M345.0704 109.6879H276.9805C272.2852 109.6879 268.4766 113.4965 268.4766 118.1918V157.8793C268.4766 162.5746 272.2852 166.3832 276.9805 166.3832H345.0704C349.7696 166.3832 353.5743 162.5746 353.5743 157.8793V118.1918C353.5743 113.4965 349.7696 109.6879 345.0704 109.6879Z' stroke='#808080' fill='none' stroke-width='1.19553' stroke-miterlimit='10'/>
<text class='f0' x='54.269831' y='45.415446' transform='matrix(1 0 0 1 217.5292 94.4976)'>program<tspan x='96.112878'>runtime</tspan></text>
<path d='M401.5308 181.0663H361.8472C357.148 181.0663 353.3433 184.871 353.3433 189.5702V229.2538C353.3433 233.9531 357.148 237.7577 361.8472 237.7577H401.5308C406.23 237.7577 410.0347 233.9531 410.0347 229.2538V189.5702C410.0347 184.871 406.23 181.0663 401.5308 181.0663Z' stroke='#808080' fill='none' stroke-width='1.19553' stroke-miterlimit='10'/>
<text class='f0' x='54.269831' y='45.415446' transform='matrix(1 0 0 1 311.7282 166.9806)'>BPF<tspan x='75.191349'>VM</tspan></text>
<path d='M76.1953 182.1246H29.0273C24.3281 182.1246 20.5234 185.9293 20.5234 190.6285V230.3121C20.5234 235.0113 24.3281 238.816 29.0273 238.816H76.1953C80.8906 238.816 84.6992 235.0113 84.6992 230.3121V190.6285C84.6992 185.9293 80.8906 182.1246 76.1953 182.1246Z' stroke='#808080' fill='none' stroke-width='1.19553' stroke-miterlimit='10'/>
<text class='f0' x='54.269831' y='45.415446' transform='matrix(1 0 0 1 -30.4267 168.0386)'>accounts-db</text>
<path d='M22.0039 77.6797H86.5352V.1992H22.0039Z' stroke='#000' fill='none' stroke-width='.3985' stroke-miterlimit='10'/>
<text class='f1' x='54.269831' y='11.888356' transform='matrix(1 0 0 1 -28.9446 0)'>run<tspan x='68.964605'>time</tspan></text>
<path d='M157.176 241.676H413.954V.199H157.176Z' stroke='#000' fill='none' stroke-width='.3985' stroke-miterlimit='10'/>
<text class='f1' x='54.269831' y='-70.108918' transform='matrix(1 0 0 1 106.2252 81.9986)'>SVM</text>
</g>
</svg>

================
File: svm/doc/diagrams/context.tex
================
%%\documentclass[dvisvgm]{minimal}
\documentclass{minimal}

\usepackage{tikz}
\usetikzlibrary{graphs, graphdrawing, shapes.misc}
\usegdlibrary{trees}

\begin{document}

\tikzset{terminal/.style={
    % The shape:
    rectangle,
    rounded corners=3mm,
    minimum size=20mm,
    % The rest
    very thick,draw=black!50,
    top color=white,bottom color=black!20,
    font=\ttfamily},
}

\begin{tikzpicture}
  \graph[tree layout, grow'=right, level sep=10mm] {
    bank [terminal]
    -> {
         { a/"transaction processor"[terminal, orient=right, orient tail=bank]
        -> b/"program runtime"[terminal]
        -> c/"BPF VM"[terminal] }, , , , , , , ,
           d/"accounts-db"[terminal, orient=down, orient tail=bank]
    };
    runtime [draw] // { bank };
    SVM [draw] // { a, b, c }
  };
\end{tikzpicture}

\end{document}

================
File: svm/doc/spec.md
================
# Solana Virtual Machine specification

# Introduction

Several components of the Solana Validator are involved in processing
a transaction (or a batch of transactions).  Collectively, the
components responsible for transaction execution are designated as
Solana Virtual Machine (SVM). SVM packaged as a stand-alone library
can be used in applications outside the Solana Validator.

This document represents the SVM specification. It covers the API
of using SVM in projects unrelated to Solana Validator and the
internal workings of the SVM, including the descriptions of the inner
data flow, data structures, and algorithms involved in the execution
of transactions. The document’s target audience includes both external
users and the developers of the SVM.

## Use cases

We envision the following applications for SVM

- **Transaction execution in Solana Validator**

    This is the primary use case for the SVM. It remains a major
    component of the Agave Validator, but with clear interface and
    isolated from dependencies on other components.

    The SVM is currently viewed as realizing two stages of the
    Transaction Engine Execution pipeline as described in Solana
    Architecture documentation
    [https://docs.solana.com/validator/runtime#execution](https://docs.solana.com/validator/runtime#execution),
    namely ‘load accounts’ and ‘execute’ stages.

- **SVM Rollups**

    Rollups that need to execute a block but don’t need the other
    components of the validator can benefit from SVM, as it can reduce
    hardware requirements and decentralize the network. This is
    especially useful for Ephemeral Rollups since the cost of compute
    will be higher as a new rollup is created for every user session
    in applications like gaming.

- **SVM Fraud Proofs for Diet Clients**

    A succinct proof of an invalid state transition by the supermajority (SIMD-65)

- **Validator Sidecar for JSON-RPC**

    The RPC needs to be separated from the validator.
    `simulateTransaction` requires replaying the transactions and
    accessing necessary account data.

- **SVM-based Avalanche subnet**

    The SVM would need to be isolated to run within a subnet since the
    consensus and networking functionality would rely on Avalanche
    modules.

- **Modified SVM (SVM+)**

    An SVM type with all the current functionality and extended
    instructions for custom use cases. This would form a superset of
    the current SVM.

# System Context

In this section, SVM is represented as a single entity. We describe its
interfaces to the parts of the Solana Validator external to SVM.

In the context of Solana Validator, the main entity external to SVM is
bank. It creates an SVM, submits transactions for execution and
receives results of transaction execution from SVM.

![context diagram](/svm/doc/diagrams/context.svg "System Context")

## Interfaces

In this section, we describe the API of using the SVM both in Solana
Validator and in third-party applications.

The interface to SVM is represented by the
`transaction_processor::TransactionBatchProcessor` struct.  To create
a `TransactionBatchProcessor` object the client need to specify the
`slot`, `epoch`, and `program_cache`.

- `slot: Slot` is a u64 value representing the ordinal number of a
    particular blockchain state in context of which the transactions
    are executed. This value is used to locate the on-chain program
    versions used in the transaction execution.
- `epoch: Epoch` is a u64 value representing the ordinal number of
    a Solana epoch, in which the slot was created. This is another
    index used to locate the onchain programs used in the execution of
    transactions in the batch.
- `program_cache: Arc<RwLock<ProgramCache<FG>>>` is a reference to
    a ProgramCache instance. All on chain programs used in transaction
    batch execution are loaded from the program cache.

In addition, `TransactionBatchProcessor` needs an instance of
`SysvarCache` and a set of pubkeys of builtin program IDs.

The main entry point to the SVM is the method
`load_and_execute_sanitized_transactions`.

The method `load_and_execute_sanitized_transactions` takes the
following arguments:

- `callbacks`: A `TransactionProcessingCallback` trait instance which allows
  the transaction processor to summon information about accounts, most
  importantly loading them for transaction execution.
- `sanitized_txs`: A slice of sanitized transactions.
- `check_results`: A mutable slice of transaction check results.
- `environment`: The runtime environment for transaction batch processing.
- `config`: Configurations for customizing transaction processing behavior.

The method returns a `LoadAndExecuteSanitizedTransactionsOutput`, which is
defined below in more detail.

An integration test `svm_integration` contains an example of
instantiating `TransactionBatchProcessor` and calling its method
`load_and_execute_sanitized_transactions`.

### `TransactionProcessingCallback`

Downstream consumers of the SVM must implement the
`TransactionProcessingCallback` trait in order to provide the transaction
processor with the ability to load accounts and retrieve other account-related
information.

```rust
pub trait TransactionProcessingCallback {
    fn get_account_shared_data(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, Slot)>;

    fn add_builtin_account(&self, _name: &str, _program_id: &Pubkey) {}
}
```

Consumers can customize this plug-in to use their own Solana account source,
caching, and more.

### `SVMTransaction`

An SVM transaction is a transaction that has undergone the
various checks required to evaluate a transaction against the Solana protocol
ruleset. Some of these rules include signature verification and validation
of account indices (`num_readonly_signers`, etc.).

A `SVMTransaction` is a trait that can access:

- `signatures`: the hash of the transaction message encrypted using
  the signing key (for each signer in the transaction).
- `static_account_keys`: Slice of `Pubkey` of accounts used in the transaction.
- `account_keys`: Pubkeys of all accounts used in the transaction, including
  those from address table lookups.
- `recent_blockhash`: Hash of a recent block.
- `instructions_iter`: An iterator over the transaction's instructions.
- `message_address_table_lookups`: An iterator over the transaction's
  address table lookups. These are only used in V0 transactions, for legacy
  transactions the iterator is empty.

### `TransactionCheckResult`

Simply stores details about a transaction, including whether or not it contains
a nonce, the nonce it contains (if applicable), and the lamports per signature
to charge for fees.

### `TransactionProcessingEnvironment`

The transaction processor requires consumers to provide values describing
the runtime environment to use for processing transactions.

- `blockhash`: The blockhash to use for the transaction batch.
- `feature_set`: Runtime feature set to use for the transaction batch.
- `epoch_total_stake`: The total stake for the current epoch.
- `fee_structure`: Fee structure to use for assessing transaction fees.
- `lamports_per_signature`: Lamports per signature to charge per transaction.
- `rent_collector`: Rent collector to use for the transaction batch.

### `TransactionProcessingConfig`

Consumers can provide various configurations to adjust the default behavior of
the transaction processor.

- `account_overrides`: Encapsulates overridden accounts, typically used for
  transaction simulation.
- `compute_budget`: The compute budget to use for transaction execution.
- `check_program_modification_slot`: Whether or not to check a program's
  modification slot when replenishing a program cache instance.
- `log_messages_bytes_limit`: The maximum number of bytes that log messages can
  consume.
- `limit_to_load_programs`: Whether to limit the number of programs loaded for
  the transaction batch.
- `recording_config`: Recording capabilities for transaction execution.

### `LoadAndExecuteSanitizedTransactionsOutput`

The output of the transaction batch processor's
`load_and_execute_sanitized_transactions` method.

- `error_metrics`: Error metrics for transactions that were processed.
- `execute_timings`: Timings for transaction batch execution.
- `processing_results`: Vector of results indicating whether a transaction was
  processed or could not be processed for some reason. Note that processed
  transactions can still have failed!

# Functional Model

In this section, we describe the functionality (logic) of the SVM in
terms of its components, relationships among components, and their
interactions.

On a high level the control flow of SVM consists of loading program
accounts, checking and verifying the loaded accounts, creating
invocation context and invoking RBPF on programs implementing the
instructions of a transaction. The SVM needs to have access to an account
database, and a sysvar cache via traits implemented for the corresponding
objects passed to it. The results of transaction execution are
consumed by bank in Solana Validator use case. However, bank structure
should not be part of the SVM.

In bank context `load_and_execute_sanitized_transactions` is called from
`simulate_transaction` where a single transaction is executed, and
from `load_execute_and_commit_transactions` which receives a batch of
transactions from its caller.

Steps of `load_and_execute_sanitized_transactions`

1. Steps of preparation for execution
   - filter executable program accounts and build program accounts map (explain)
   - add builtin programs to program accounts map
   - replenish program cache using the program accounts map
        - Gather all required programs to load from the cache.
        - Lock the global program cache and initialize the local program cache.
        - Perform loading tasks to load all required programs from the cache,
          loading, verifying, and compiling (where necessary) each program.
        - A helper module - `program_loader` - provides utilities for loading
          programs from on-chain, namely `load_program_with_pubkey`.
        - Return the replenished local program cache.

2. Load accounts (call to `load_accounts` function)
   - For each `SVMTransaction` and `TransactionCheckResult`, we:
        - Calculate the number of signatures in transaction and its cost.
        - Call `load_transaction_accounts`
            - The function is interwined with the struct `SVMInstruction`
            - Load accounts from accounts DB
            - Extract data from accounts
            - Verify if we've reached the maximum account data size
            - Validate the fee payer and the loaded accounts
            - Validate the programs accounts that have been loaded and checks if they are builtin programs.
            - Return `struct LoadedTransaction` containing the accounts (pubkey and data),
              indices to the executable accounts in `TransactionContext` (or `InstructionContext`),
              the transaction rent, and the `struct RentDebit`.
            - Generate a `RollbackAccounts` struct which holds fee-subtracted fee payer account and pre-execution nonce state used for rolling back account state on execution failure.
    - Returns `TransactionLoadedResult`, containing the `LoadTransaction` we obtained from `loaded_transaction_accounts`

3. Execute each loaded transactions
   1. Compute the sum of transaction accounts' balances. This sum is
      invariant in the transaction execution.
   2. Obtain rent state of each account before the transaction
      execution. This is later used in verifying the account state
      changes (step #7).
   3. Create a new log_collector.  `LogCollector` is defined in
      solana-program-runtime crate.
   4. Obtain last blockhash and lamports per signature. This
      information is read from blockhash_queue maintained in Bank. The
      information is taken in parameters to
      `MessageProcessor::process_message`.
   5. Make two local variables that will be used as output parameters
      of `MessageProcessor::process_message`. One will contain the
      number of executed units (the number of compute unites consumed
      in the transaction). Another is a container of `ProgramCacheForTxBatch`.
      The latter is initialized with the slot, and
      the clone of environments of `programs_loaded_for_tx_batch`
         - `programs_loaded_for_tx_batch` contains a reference to all the `ProgramCacheEntry`s
            necessary for the transaction. It maintains an `Arc` to the programs in the global
            `ProgramCacheEntry` data structure.
   6. Call `MessageProcessor::process_message` to execute the
      transaction. `MessageProcessor` is contained in
      solana-program-runtime crate. The result of processing message
      is either `ProcessedMessageInfo` which is an i64 wrapped in a
      struct meaning the change in accounts data length, or a
      `TransactionError`, if any of instructions failed to execute
      correctly.
   7. Verify transaction accounts' `RentState` changes (`verify_changes` function)
      - If the account `RentState` pre-transaction processing is rent exempt or unitiliazed, the verification will pass.
      - If the account `RentState` pre-transaction is rent paying:
         - A transition to a state uninitialized or rent exempt post-transaction is not allowed.
         - If its size has changed or its balance has increased, it cannot remain rent paying.
   8. Extract log messages.
   9. Extract inner instructions (`Vec<Vec<InnerInstruction>>`).
   10. Extract `ExecutionRecord` components from transaction context.
   11. Check balances of accounts to match the sum of balances before
       transaction execution.
   12. Update loaded transaction accounts to new accounts.
   13. Extract changes in accounts data sizes
   14. Extract return data
   15. Return `TransactionExecutionResult` with wrapping the extracted
       information in `TransactionExecutionDetails`.

4. Prepare the results of loading and executing transactions.

   This includes the following steps for each transactions
   1. Dump flattened result to info log for an account whose pubkey is
      in the transaction's debug keys.
   2. Collect logs of the transaction execution for each executed
      transaction, unless Bank's `transaction_log_collector_config` is
      set to `None`.
   3. Finally, increment various statistical counters, and update
      timings passed as a mutable reference to
      `load_and_execute_transactions` in arguments. The counters are
      packed in the struct `LoadAndExecuteTransactionsOutput`.

================
File: svm/src/account_loader.rs
================
pub type TransactionCheckResult = Result<CheckedTransactionDetails>;
type TransactionValidationResult = Result<ValidatedTransactionDetails>;
⋮----
pub(crate) enum TransactionLoadResult {
⋮----
pub struct CheckedTransactionDetails {
⋮----
impl Default for CheckedTransactionDetails {
fn default() -> Self {
⋮----
.expect("Failed to set loaded_accounts_bytes"),
⋮----
impl CheckedTransactionDetails {
pub fn new(
⋮----
pub(crate) struct ValidatedTransactionDetails {
⋮----
impl Default for ValidatedTransactionDetails {
⋮----
pub(crate) struct LoadedTransactionAccount {
⋮----
pub struct LoadedTransaction {
⋮----
pub struct FeesOnlyTransaction {
⋮----
pub(crate) struct AccountLoader<'a, CB: TransactionProcessingCallback> {
⋮----
// create a new AccountLoader for the transaction batch
⋮----
pub(crate) fn new_with_loaded_accounts_capacity(
⋮----
// SlotHistory may be overridden for simulation.
// No other uses of AccountOverrides are expected.
⋮----
account_overrides.and_then(|overrides| overrides.get(&slot_history::id()))
⋮----
loaded_accounts.insert(slot_history::id(), slot_history.clone());
⋮----
// Jito added: let's use pre-execution accounts
⋮----
for (pubkey, account) in overrides.accounts().iter() {
loaded_accounts.insert(*pubkey, account.clone());
⋮----
pub(crate) fn load_transaction_account(
⋮----
let account = self.load_account(account_key);
self.callbacks.inspect_account(
⋮----
account.map(|account| LoadedTransactionAccount {
loaded_size: base_account_size.saturating_add(account.data().len()),
⋮----
pub(crate) fn load_account(&mut self, account_key: &Pubkey) -> Option<AccountSharedData> {
match self.do_load(account_key) {
(Some(account), false) => Some(account),
⋮----
self.loaded_accounts.insert(*account_key, account.clone());
Some(account)
⋮----
.insert(*account_key, AccountSharedData::default());
⋮----
fn do_load(&self, account_key: &Pubkey) -> (Option<AccountSharedData>, bool) {
if let Some(account) = self.loaded_accounts.get(account_key) {
let option_account = if account.lamports() == 0 {
⋮----
Some(account.clone())
⋮----
} else if let Some((account, _slot)) = self.callbacks.get_account_shared_data(account_key) {
(Some(account), true)
⋮----
pub(crate) fn update_accounts_for_failed_tx(&mut self, rollback_accounts: &RollbackAccounts) {
⋮----
.insert(*account_address, account.clone());
⋮----
pub(crate) fn update_accounts_for_successful_tx(
⋮----
for (i, (address, account)) in (0..message.account_keys().len()).zip(transaction_accounts) {
if !message.is_writable(i) {
⋮----
if message.is_invoked(i) && !message.is_instruction_account(i) {
⋮----
self.loaded_accounts.insert(*address, account.clone());
⋮----
impl<CB: TransactionProcessingCallback> TransactionProcessingCallback for AccountLoader<'_, CB> {
fn get_account_shared_data(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, Slot)> {
// The returned last-modification-slot is a dummy value for now,
// but will later be used in IndexImplementation::V2 of the global program cache.
self.do_load(pubkey).0.map(|account| (account, 0))
⋮----
// NOTE this is a required subtrait of TransactionProcessingCallback.
// It may make sense to break out a second subtrait just for the above two functions,
// but this would be a nontrivial breaking change and require careful consideration.
⋮----
pub fn update_rent_exempt_status_for_account(rent: &Rent, account: &mut AccountSharedData) {
if account.rent_epoch() != RENT_EXEMPT_RENT_EPOCH
&& rent.is_exempt(account.lamports(), account.data().len())
⋮----
account.set_rent_epoch(RENT_EXEMPT_RENT_EPOCH);
⋮----
pub fn validate_fee_payer(
⋮----
if payer_account.lamports() == 0 {
⋮----
return Err(TransactionError::AccountNotFound);
⋮----
let system_account_kind = get_system_account_kind(payer_account).ok_or_else(|| {
⋮----
rent.minimum_balance(NonceState::size())
⋮----
.lamports()
.checked_sub(min_balance)
.and_then(|v| v.checked_sub(fee))
.ok_or_else(|| {
⋮----
get_account_rent_state(rent, payer_account.lamports(), payer_account.data().len());
⋮----
.checked_sub_lamports(fee)
.map_err(|_| TransactionError::InsufficientFundsForFee)?;
⋮----
check_rent_state_with_account(
⋮----
pub(crate) fn load_transaction<CB: TransactionProcessingCallback>(
⋮----
let load_result = load_transaction_accounts(
⋮----
struct LoadedTransactionAccounts {
⋮----
impl LoadedTransactionAccounts {
fn increase_calculated_data_size(
⋮----
return Err(TransactionError::MaxLoadedAccountsDataSizeExceeded);
⋮----
.saturating_add(data_size_delta);
if self.loaded_accounts_data_size > requested_loaded_accounts_data_size_limit.get() {
⋮----
Err(TransactionError::MaxLoadedAccountsDataSizeExceeded)
⋮----
Ok(())
⋮----
fn load_transaction_accounts<CB: TransactionProcessingCallback>(
⋮----
load_transaction_accounts_simd186(
⋮----
load_transaction_accounts_old(
⋮----
fn load_transaction_accounts_simd186<CB: TransactionProcessingCallback>(
⋮----
let account_keys = message.account_keys();
⋮----
accounts: Vec::with_capacity(account_keys.len()),
program_indices: Vec::with_capacity(message.num_instructions()),
⋮----
loaded_transaction_accounts.increase_calculated_data_size(
⋮----
.num_lookup_tables()
.saturating_mul(ADDRESS_LOOKUP_TABLE_BASE_SIZE),
⋮----
if bpf_loader_upgradeable::check_id(account.owner()) {
⋮----
}) = account.state()
⋮----
if !account_keys.iter().any(|key| programdata_address == *key)
&& !additional_loaded_accounts.contains(&programdata_address)
⋮----
account_loader.load_account(&programdata_address)
⋮----
.saturating_add(programdata_account.data().len()),
⋮----
additional_loaded_accounts.insert(programdata_address);
⋮----
loaded_transaction_accounts.accounts.push((*key, account));
⋮----
collect_loaded_account(
⋮----
message.fee_payer(),
⋮----
for (account_index, account_key) in account_keys.iter().enumerate().skip(1) {
⋮----
load_transaction_account(account_loader, message, account_key, account_index, rent);
collect_loaded_account(account_loader, account_key, loaded_account)?;
⋮----
for (program_id, instruction) in message.program_instructions_iter() {
let Some(program_account) = account_loader.load_account(program_id) else {
⋮----
return Err(TransactionError::ProgramAccountNotFound);
⋮----
let owner_id = program_account.owner();
if !native_loader::check_id(owner_id) && !PROGRAM_OWNERS.contains(owner_id) {
⋮----
return Err(TransactionError::InvalidProgramForExecution);
⋮----
.push(instruction.program_id_index as IndexOfAccount);
⋮----
Ok(loaded_transaction_accounts)
⋮----
fn load_transaction_accounts_old<CB: TransactionProcessingCallback>(
⋮----
let mut accounts = Vec::with_capacity(account_keys.len());
let mut validated_loaders = AHashSet::with_capacity(PROGRAM_OWNERS.len());
let mut accumulated_accounts_data_size: Saturating<u32> = Saturating(0);
⋮----
accumulate_and_check_loaded_account_data_size(
⋮----
accounts.push((*key, account));
⋮----
collect_loaded_account(message.fee_payer(), loaded_fee_payer_account)?;
⋮----
collect_loaded_account(account_key, loaded_account)?;
⋮----
.program_instructions_iter()
.map(|(program_id, instruction)| {
⋮----
return Ok(u16::MAX as IndexOfAccount);
⋮----
return Ok(program_index as IndexOfAccount);
⋮----
if !validated_loaders.contains(owner_id) {
if let Some(owner_account) = account_loader.load_account(owner_id) {
if !native_loader::check_id(owner_account.owner()) {
⋮----
owner_account.data().len(),
⋮----
validated_loaders.insert(*owner_id);
⋮----
Ok(program_index as IndexOfAccount)
⋮----
Ok(LoadedTransactionAccounts {
⋮----
fn load_transaction_account<CB: TransactionProcessingCallback>(
⋮----
let is_writable = message.is_writable(account_index);
⋮----
account: construct_instructions_account(message),
⋮----
account_loader.load_transaction_account(account_key, is_writable)
⋮----
update_rent_exempt_status_for_account(rent, &mut loaded_account.account);
⋮----
default_account.set_rent_epoch(RENT_EXEMPT_RENT_EPOCH);
⋮----
loaded_size: default_account.data().len(),
⋮----
fn accumulate_and_check_loaded_account_data_size(
⋮----
if accumulated_loaded_accounts_data_size.0 > requested_loaded_accounts_data_size_limit.get() {
⋮----
fn construct_instructions_account(message: &impl SVMMessage) -> AccountSharedData {
⋮----
let mut decompiled_instructions = Vec::with_capacity(message.num_instructions());
⋮----
.iter()
.map(|account_index| {
⋮----
is_signer: message.is_signer(account_index),
is_writable: message.is_writable(account_index),
pubkey: account_keys.get(account_index).unwrap(),
⋮----
.collect();
decompiled_instructions.push(BorrowedInstruction {
⋮----
data: construct_instructions_data(&decompiled_instructions),
⋮----
mod tests {
⋮----
struct TestCallbacks {
⋮----
impl Default for TestCallbacks {
⋮----
impl InvokeContextCallback for TestCallbacks {}
impl TransactionProcessingCallback for TestCallbacks {
⋮----
.get(pubkey)
.map(|account| (account.clone(), 0))
⋮----
fn inspect_account(
⋮----
AccountState::Alive(account) => Some(account.clone()),
⋮----
.borrow_mut()
.entry(*address)
.or_default()
.push((account, is_writable));
⋮----
fn from(callbacks: &'a TestCallbacks) -> AccountLoader<'a, TestCallbacks> {
⋮----
fn load_accounts_with_features_and_rent(
⋮----
let fee_payer_account = accounts[0].1.clone();
⋮----
accounts_map.insert(*pubkey, account.clone());
⋮----
let mut account_loader: AccountLoader<TestCallbacks> = (&callbacks).into();
⋮----
load_transaction(
⋮----
Ok(ValidatedTransactionDetails {
⋮----
fn new_unchecked_sanitized_message(message: Message) -> SanitizedMessage {
⋮----
fn test_load_accounts_unknown_program_id(formalize_loaded_transaction_data_size: bool) {
⋮----
let key0 = keypair.pubkey();
⋮----
accounts.push((key0, account));
⋮----
accounts.push((key1, account));
let instructions = vec![CompiledInstruction::new(1, &(), vec![0])];
⋮----
vec![Pubkey::default()],
⋮----
let load_results = load_accounts_with_features_and_rent(
⋮----
assert_eq!(error_metrics.account_not_found.0, 1);
assert!(matches!(
⋮----
fn test_load_accounts_no_loaders(formalize_loaded_transaction_data_size: bool) {
⋮----
account.set_rent_epoch(1);
⋮----
let instructions = vec![CompiledInstruction::new(2, &(), vec![0, 1])];
⋮----
vec![native_loader::id()],
⋮----
let loaded_accounts = load_accounts_with_features_and_rent(
⋮----
assert_eq!(error_metrics.account_not_found.0, 0);
assert_eq!(loaded_transaction.accounts.len(), 3);
assert_eq!(loaded_transaction.accounts[0].1, accounts[0].1);
assert_eq!(loaded_transaction.program_indices.len(), 1);
assert_eq!(loaded_transaction.program_indices[0], u16::MAX);
⋮----
assert_eq!(
⋮----
result => panic!("unexpected result: {result:?}"),
⋮----
fn test_load_accounts_bad_owner(formalize_loaded_transaction_data_size: bool) {
⋮----
account.set_executable(true);
⋮----
vec![key1],
⋮----
assert_eq!(error_metrics.invalid_program_for_execution.0, 1);
⋮----
fn test_load_accounts_not_executable(formalize_loaded_transaction_data_size: bool) {
⋮----
assert_eq!(error_metrics.invalid_program_for_execution.0, 0);
⋮----
assert_eq!(loaded_transaction.accounts.len(), 2);
⋮----
assert_eq!(loaded_transaction.accounts[1].1, accounts[1].1);
⋮----
assert_eq!(loaded_transaction.program_indices[0], 1);
⋮----
TransactionLoadResult::FeesOnly(fees_only_tx) => panic!("{}", fees_only_tx.load_error),
TransactionLoadResult::NotLoaded(e) => panic!("{e}"),
⋮----
fn test_load_accounts_multiple_loaders(formalize_loaded_transaction_data_size: bool) {
⋮----
account.set_owner(native_loader::id());
⋮----
account.set_owner(key1);
accounts.push((key2, account));
let instructions = vec![
⋮----
vec![key1, key2],
⋮----
assert_eq!(loaded_transaction.program_indices.len(), 2);
⋮----
assert_eq!(loaded_transaction.program_indices[1], 2);
⋮----
fn load_accounts_no_store(
⋮----
Ok(ValidatedTransactionDetails::default()),
⋮----
fn test_instructions() {
⋮----
let instructions = vec![CompiledInstruction::new(1, &(), vec![0, 1])];
⋮----
let load_results = load_accounts_no_store(&[], tx, None);
⋮----
fn test_overrides() {
⋮----
account_overrides.set_slot_history(Some(account));
⋮----
program_account.set_lamports(1);
program_account.set_executable(true);
program_account.set_owner(native_loader::id());
let instructions = vec![CompiledInstruction::new(2, &(), vec![0])];
⋮----
vec![bpf_loader::id()],
⋮----
let loaded_accounts = load_accounts_no_store(
⋮----
(keypair.pubkey(), account),
⋮----
Some(&account_overrides),
⋮----
assert_eq!(loaded_transaction.accounts[0].0, keypair.pubkey());
assert_eq!(loaded_transaction.accounts[1].0, slot_history_id);
assert_eq!(loaded_transaction.accounts[1].1.lamports(), 42);
⋮----
fn test_accumulate_and_check_loaded_account_data_size() {
⋮----
let mut accumulated_data_size: Saturating<u32> = Saturating(0);
⋮----
let requested_data_size_limit = NonZeroU32::new(data_size as u32).unwrap();
// OK - loaded data size is up to limit
assert!(accumulate_and_check_loaded_account_data_size(
⋮----
assert_eq!(data_size as u32, accumulated_data_size.0);
// fail - loading more data that would exceed limit
⋮----
struct ValidateFeePayerTestParameter {
⋮----
fn validate_fee_payer_account(test_parameter: ValidateFeePayerTestParameter, rent: &Rent) {
⋮----
.unwrap()
⋮----
let result = validate_fee_payer(
&payer_account_keys.pubkey(),
⋮----
assert_eq!(result, test_parameter.expected_result);
assert_eq!(account.lamports(), test_parameter.payer_post_balance);
⋮----
fn test_validate_fee_payer() {
⋮----
let min_balance = rent.minimum_balance(NonceState::size());
⋮----
// If payer account has sufficient balance, expect successful fee deduction,
// regardless feature gate status, or if payer is nonce account.
⋮----
validate_fee_payer_account(
⋮----
expected_result: Ok(()),
⋮----
// If payer account has no balance, expected AccountNotFound Error
⋮----
expected_result: Err(TransactionError::AccountNotFound),
⋮----
// If payer account has insufficient balance, expect InsufficientFundsForFee error
⋮----
expected_result: Err(TransactionError::InsufficientFundsForFee),
⋮----
// normal payer account has balance of u64::MAX, so does fee; since it does not  require
// min_balance, expect successful fee deduction, regardless of feature gate status
⋮----
fn test_validate_nonce_fee_payer_with_checked_arithmetic() {
⋮----
// nonce payer account has balance of u64::MAX, so does fee; due to nonce account
// requires additional min_balance, expect InsufficientFundsForFee error if feature gate is
// enabled
⋮----
fn test_construct_instructions_account() {
⋮----
is_writable_account_cache: vec![false],
⋮----
let shared_data = construct_instructions_account(&message);
⋮----
data: construct_instructions_data(&message.decompile_instructions()),
⋮----
assert_eq!(shared_data, expected);
⋮----
fn test_load_transaction_accounts_fee_payer() {
⋮----
account_keys: vec![fee_payer_address],
⋮----
instructions: vec![],
⋮----
let sanitized_message = new_unchecked_sanitized_message(message);
⋮----
fee_payer_account.set_lamports(fee_payer_balance);
⋮----
.insert(fee_payer_address, fee_payer_account.clone());
let mut account_loader = (&mock_bank).into();
⋮----
vec![Signature::new_unique()],
⋮----
let result = load_transaction_accounts(
⋮----
sanitized_transaction.message(),
⋮----
loaded_size: fee_payer_account.data().len(),
account: fee_payer_account.clone(),
⋮----
fn test_load_transaction_accounts_native_loader(formalize_loaded_transaction_data_size: bool) {
⋮----
account_keys: vec![key1.pubkey(), native_loader::id()],
⋮----
instructions: vec![CompiledInstruction {
⋮----
.insert(native_loader::id(), AccountSharedData::default());
⋮----
fee_payer_account.set_lamports(200);
⋮----
.insert(key1.pubkey(), fee_payer_account.clone());
⋮----
fn test_load_transaction_accounts_program_account_no_data() {
⋮----
account_keys: vec![key1.pubkey(), key2.pubkey()],
⋮----
account_data.set_lamports(200);
mock_bank.accounts_map.insert(key1.pubkey(), account_data);
⋮----
assert_eq!(result.err(), Some(TransactionError::ProgramAccountNotFound));
⋮----
fn test_load_transaction_accounts_invalid_program_for_execution() {
⋮----
fn test_load_transaction_accounts_native_loader_owner(
⋮----
account_keys: vec![key2.pubkey(), key1.pubkey()],
⋮----
account_data.set_owner(native_loader::id());
account_data.set_lamports(1);
account_data.set_executable(true);
⋮----
.insert(key2.pubkey(), fee_payer_account.clone());
⋮----
fn test_load_transaction_accounts_program_account_not_found_after_all_checks() {
⋮----
mock_bank.accounts_map.insert(key2.pubkey(), account_data);
⋮----
fn test_load_transaction_accounts_program_account_invalid_program_for_execution_last_check() {
⋮----
account_data.set_owner(key3.pubkey());
⋮----
.insert(key3.pubkey(), AccountSharedData::default());
⋮----
fn test_load_transaction_accounts_program_success_complete(
⋮----
account_data.set_owner(bpf_loader::id());
⋮----
.insert(bpf_loader::id(), account_data);
⋮----
fn test_load_transaction_accounts_program_builtin_saturating_add(
⋮----
account_keys: vec![key2.pubkey(), key1.pubkey(), key3.pubkey()],
⋮----
instructions: vec![
⋮----
account_data.set_rent_epoch(RENT_EXEMPT_RENT_EPOCH);
⋮----
fn test_rent_state_list_len() {
⋮----
system_data.set_lamports(1);
system_data.set_executable(true);
system_data.set_owner(native_loader::id());
⋮----
.insert(Pubkey::new_from_array([0u8; 32]), system_data);
⋮----
mint_data.set_lamports(2);
bank.accounts_map.insert(mint_keypair.pubkey(), mint_data);
⋮----
.insert(recipient, AccountSharedData::default());
let mut account_loader = (&bank).into();
let tx = transfer(&mint_keypair, &recipient, LAMPORTS_PER_SOL, last_block_hash);
let num_accounts = tx.message().account_keys.len();
⋮----
let load_result = load_transaction(
⋮----
&sanitized_tx.clone(),
⋮----
panic!("transaction loading failed");
⋮----
rent.clone(),
⋮----
fn test_load_accounts_success(formalize_loaded_transaction_data_size: bool) {
⋮----
let validation_result = Ok(ValidatedTransactionDetails {
⋮----
fn test_load_accounts_error() {
⋮----
account_keys: vec![Pubkey::new_from_array([0; 32])],
⋮----
let validation_result = Ok(ValidatedTransactionDetails::default());
⋮----
validation_result.clone(),
⋮----
let validation_result = Err(TransactionError::InvalidWritableAccount);
⋮----
fn test_update_rent_exempt_status_for_account() {
⋮----
let min_exempt_balance = rent.minimum_balance(0);
⋮----
update_rent_exempt_status_for_account(&rent, &mut account);
assert_eq!(account.rent_epoch(), RENT_EXEMPT_RENT_EPOCH);
⋮----
fn test_update_rent_exempt_status_for_rent_paying_account() {
⋮----
assert_eq!(account.rent_epoch(), 0);
assert_eq!(account.lamports(), 1);
⋮----
// Ensure `TransactionProcessingCallback::inspect_account()` is called when
// loading accounts for transaction processing.
⋮----
fn test_inspect_account_non_fee_payer() {
⋮----
let address0 = Pubkey::new_unique(); // <-- fee payer
let address1 = Pubkey::new_unique(); // <-- initially alive
let address2 = Pubkey::new_unique(); // <-- initially dead
let address3 = Pubkey::new_unique(); // <-- program
⋮----
account0.set_lamports(1_000_000_000);
mock_bank.accounts_map.insert(address0, account0.clone());
⋮----
account1.set_lamports(2_000_000_000);
mock_bank.accounts_map.insert(address1, account1.clone());
// account2 *not* added to the bank's accounts_map
⋮----
account3.set_lamports(4_000_000_000);
account3.set_executable(true);
account3.set_owner(bpf_loader::id());
mock_bank.accounts_map.insert(address3, account3.clone());
⋮----
account_keys: vec![address0, address1, address2, address3],
⋮----
account: account0.clone(),
⋮----
let _load_results = load_transaction(
⋮----
.borrow()
⋮----
.map(|(k, v)| (*k, v.clone()))
⋮----
actual_inspected_accounts.sort_unstable_by(|a, b| a.0.cmp(&b.0));
let mut expected_inspected_accounts = vec![
⋮----
expected_inspected_accounts.sort_unstable_by(|a, b| a.0.cmp(&b.0));
assert_eq!(actual_inspected_accounts, expected_inspected_accounts,);
⋮----
fn test_load_transaction_accounts_data_sizes() {
⋮----
let program1 = program1_keypair.pubkey();
⋮----
use solana_account::state_traits::StateMut;
⋮----
program2_account.set_owner(loader_v3);
program2_account.set_lamports(LAMPORTS_PER_SOL);
program2_account.set_executable(true);
program2_account.set_data(vec![0; program2_size as usize]);
⋮----
.set_state(&UpgradeableLoaderState::Program {
⋮----
.unwrap();
mock_bank.accounts_map.insert(program2, program2_account);
⋮----
programdata2_account.set_owner(loader_v3);
programdata2_account.set_lamports(LAMPORTS_PER_SOL);
programdata2_account.set_data(vec![0; program2_size as usize]);
⋮----
.set_state(&UpgradeableLoaderState::ProgramData {
⋮----
let mut programdata = programdata2_account.data().to_vec();
⋮----
File::open("tests/example-programs/hello-solana/hello_solana_program.so").unwrap();
file.read_to_end(&mut programdata).unwrap();
let programdata2_size = programdata.len() as u32;
programdata2_account.set_data(programdata);
⋮----
.insert(programdata2, programdata2_account);
⋮----
vec![0; size],
⋮----
mock_bank.accounts_map.insert(pubkey, account.clone());
⋮----
let fee_payer = fee_payer_keypair.pubkey();
⋮----
make_account(fee_payer, system_program::id(), false);
⋮----
let (account1_size, _) = make_account(account1, program1, false);
⋮----
let (account2_size, _) = make_account(account2, program2, false);
let (native_loader_size, _) = make_account(native_loader::id(), native_loader::id(), true);
let (bpf_loader_size, _) = make_account(loader_v2, native_loader::id(), true);
let (upgradeable_loader_size, _) = make_account(loader_v3, native_loader::id(), true);
let (program1_size, _) = make_account(program1, loader_v2, true);
⋮----
program_accounts.insert(program1, (&loader_v2, 0));
program_accounts.insert(program2, (&loader_v3, 0));
⋮----
let loaded_transaction_accounts = load_transaction_accounts(
⋮----
Some(&fee_payer),
⋮----
test_transaction_data_size(transaction, expected_size)
⋮----
let ixns = vec![Instruction::new_with_bytes(program1, &[], vec![])];
test_data_size(ixns, program1_size + bpf_loader_size + fee_payer_size);
let ixns = vec![
⋮----
test_data_size(
⋮----
let ixns = vec![Instruction::new_with_bytes(bpf_loader::id(), &[], vec![])];
test_data_size(ixns, bpf_loader_size + fee_payer_size);
let ixns = vec![Instruction::new_with_bytes(
⋮----
test_data_size(ixns, bpf_loader_size + native_loader_size + fee_payer_size);
⋮----
test_data_size(ixns, native_loader_size + fee_payer_size);
⋮----
fn test_account_loader_wrappers() {
⋮----
fee_payer_account.set_rent_epoch(u64::MAX);
fee_payer_account.set_lamports(5000);
⋮----
.insert(fee_payer, fee_payer_account.clone());
let mut account_loader: AccountLoader<_> = (&mock_bank).into();
⋮----
let account_loader: AccountLoader<_> = (&mock_bank).into();
⋮----
account_loader.load_account(&fee_payer).unwrap();
⋮----
fee_payer_account.set_lamports(0);
account_loader.update_accounts_for_failed_tx(&RollbackAccounts::FeePayerOnly {
⋮----
assert_eq!(account_loader.load_account(&fee_payer), None);
assert_eq!(account_loader.get_account_shared_data(&fee_payer), None);
⋮----
fn test_load_transaction_accounts_data_sizes_simd186() {
⋮----
vec![0; rng.gen_range(0, 128)],
⋮----
rng.gen(),
⋮----
mock_bank.accounts_map.insert(Pubkey::new_unique(), account);
⋮----
let mut fee_payers = vec![];
⋮----
vec![0; rng.gen_range(0, 32)],
⋮----
mock_bank.accounts_map.insert(fee_payer, account);
fee_payers.push(fee_payer);
⋮----
let mut loader_owned_accounts = vec![];
⋮----
vec![0; rng.gen_range(0, 512)],
⋮----
if *loader == bpf_loader_upgradeable::id() && account.data().len() >= 64 {
⋮----
let has_programdata = rng.gen();
⋮----
programdata_tracker.insert(
⋮----
(programdata_address, programdata_account.data().len()),
⋮----
.insert(programdata_address, programdata_account);
loader_owned_accounts.push(programdata_address);
⋮----
if has_programdata || rng.gen() {
⋮----
mock_bank.accounts_map.insert(program_id, account);
loader_owned_accounts.push(program_id);
⋮----
let mut all_accounts = mock_bank.accounts_map.keys().copied().collect::<Vec<_>>();
⋮----
all_accounts.push(Pubkey::new_unique());
⋮----
let mut instructions = vec![];
for _ in 0..rng.gen_range(1, 8) {
let mut accounts = vec![];
for _ in 0..rng.gen_range(1, 16) {
all_accounts.shuffle(&mut rng);
⋮----
accounts.push(AccountMeta {
⋮----
is_writable: rng.gen(),
is_signer: rng.gen() && rng.gen(),
⋮----
loader_owned_accounts.shuffle(&mut rng);
⋮----
instructions.push(Instruction {
⋮----
data: vec![],
⋮----
fee_payers.shuffle(&mut rng);
⋮----
let fee_payer_account = mock_bank.accounts_map.get(&fee_payer).cloned().unwrap();
⋮----
Transaction::new_with_payer(&instructions, Some(&fee_payer)),
⋮----
.account_keys()
⋮----
.copied()
⋮----
for pubkey in transaction.account_keys().iter() {
if let Some(account) = mock_bank.accounts_map.get(pubkey) {
expected_size += TRANSACTION_ACCOUNT_BASE_SIZE + account.data().len();
⋮----
programdata_tracker.get(pubkey)
⋮----
if counted_programdatas.get(programdata_address).is_none() {
⋮----
counted_programdatas.insert(*programdata_address);
⋮----
assert!(expected_size <= MAX_LOADED_ACCOUNTS_DATA_SIZE_BYTES.get() as usize);
⋮----
loaded_size: TRANSACTION_ACCOUNT_BASE_SIZE + fee_payer_account.data().len(),
⋮----
fn test_loader_aliasing() {
⋮----
.insert(hit_address, expected_hit_account.clone());
⋮----
account_loader.load_account(&hit_address);
let actual_hit_account = account_loader.loaded_accounts.get(&hit_address);
assert_eq!(actual_hit_account, Some(&expected_hit_account));
assert!(Arc::ptr_eq(
⋮----
account_loader.load_account(&miss_address);
⋮----
.get(&miss_address)
⋮----
.clone();
assert!(!Arc::ptr_eq(
⋮----
let actual_miss_account = account_loader.loaded_accounts.get(&miss_address);
assert_eq!(actual_miss_account, Some(&expected_miss_account));

================
File: svm/src/account_overrides.rs
================
pub struct AccountOverrides {
⋮----
impl AccountOverrides {
pub fn set_account(&mut self, pubkey: &Pubkey, account: Option<AccountSharedData>) {
⋮----
Some(account) => self.accounts.insert(*pubkey, account),
None => self.accounts.remove(pubkey),
⋮----
pub fn set_slot_history(&mut self, slot_history: Option<AccountSharedData>) {
self.set_account(&sysvar::slot_history::id(), slot_history);
⋮----
pub fn get(&self, pubkey: &Pubkey) -> Option<&AccountSharedData> {
self.accounts.get(pubkey)
⋮----
pub fn len(&self) -> usize {
self.accounts.len()
⋮----
pub fn is_empty(&self) -> bool {
self.accounts.is_empty()
⋮----
pub fn accounts(&self) -> &HashMap<Pubkey, AccountSharedData> {
⋮----
pub fn merge(&mut self, other: AccountOverrides) {
self.accounts.extend(other.accounts);
⋮----
mod test {
⋮----
fn test_set_account() {
⋮----
accounts.set_account(&key, Some(data.clone()));
assert_eq!(accounts.get(&key), Some(&data));
accounts.set_account(&key, None);
assert!(accounts.get(&key).is_none());
⋮----
fn test_slot_history() {
⋮----
assert_eq!(accounts.get(&sysvar::slot_history::id()), None);
accounts.set_slot_history(Some(data.clone()));
assert_eq!(accounts.get(&sysvar::slot_history::id()), Some(&data));

================
File: svm/src/lib.rs
================
pub mod account_loader;
pub mod account_overrides;
pub mod message_processor;
pub mod nonce_info;
pub mod program_loader;
pub mod rent_calculator;
pub mod rollback_accounts;
pub mod transaction_account_state_info;
pub mod transaction_balances;
pub mod transaction_commit_result;
pub mod transaction_error_metrics;
pub mod transaction_execution_result;
pub mod transaction_processing_callback;
pub mod transaction_processing_result;
pub mod transaction_processor;
⋮----
extern crate solana_frozen_abi_macro;

================
File: svm/src/message_processor.rs
================
pub(crate) fn process_message<'ix_data>(
⋮----
debug_assert_eq!(program_indices.len(), message.num_instructions());
⋮----
.program_instructions_iter()
.zip(program_indices.iter())
.enumerate()
⋮----
.prepare_next_top_level_instruction(
⋮----
.map_err(|err| {
⋮----
let (result, process_instruction_us) = measure_us!({
⋮----
accumulated_consumed_units.saturating_add(compute_units_consumed);
⋮----
execute_timings.details.accumulate_program(
⋮----
result.is_err(),
⋮----
execute_timings.details.accumulate(&invoke_context.timings);
⋮----
result.map_err(|err| {
⋮----
Ok(())
⋮----
mod tests {
⋮----
struct MockCallback {}
impl InvokeContextCallback for MockCallback {}
fn create_loadable_account_for_test(name: &str) -> AccountSharedData {
⋮----
data: name.as_bytes().to_vec(),
⋮----
fn new_sanitized_message(message: Message) -> SanitizedMessage {
SanitizedMessage::try_from_legacy_message(message, &HashSet::new()).unwrap()
⋮----
fn test_process_message_readonly_handling() {
⋮----
enum MockSystemInstruction {
⋮----
declare_process_instruction!(MockBuiltin, 1, |invoke_context| {
⋮----
let accounts = vec![
⋮----
let program_indices = vec![2];
⋮----
program_cache_for_tx_batch.replenish(
⋮----
let account_keys = (0..transaction_context.get_number_of_accounts())
.map(|index| {
⋮----
.get_key_of_account_at_index(index)
.unwrap()
⋮----
let account_metas = vec![
⋮----
let message = new_sanitized_message(Message::new_with_compiled_instructions(
⋮----
account_keys.clone(),
⋮----
AccountKeys::new(&account_keys, None).compile_instructions(&[
⋮----
account_metas.clone(),
⋮----
let result = process_message(
⋮----
assert!(result.is_ok());
assert_eq!(
⋮----
fn test_process_message_duplicate_accounts() {
⋮----
let message = new_sanitized_message(Message::new(
⋮----
Some(transaction_context.get_key_of_account_at_index(0).unwrap()),
⋮----
fn secp256k1_instruction_for_test() -> Instruction {
⋮----
let secret_key = libsecp256k1::SecretKey::random(&mut thread_rng());
⋮----
let eth_address = eth_address_from_pubkey(&pubkey.serialize()[1..].try_into().unwrap());
⋮----
solana_secp256k1_program::sign_message(&secret_key.serialize(), &message[..]).unwrap();
new_secp256k1_instruction_with_signature(
⋮----
fn ed25519_instruction_for_test() -> Instruction {
let secret_key = ed25519_dalek::Keypair::generate(&mut thread_rng());
let signature = secret_key.sign(b"hello").to_bytes();
let pubkey = secret_key.public.to_bytes();
new_ed25519_instruction_with_signature(b"hello", &signature, &pubkey)
⋮----
fn secp256r1_instruction_for_test() -> Instruction {
let group = EcGroup::from_curve_name(Nid::X9_62_PRIME256V1).unwrap();
let secret_key = EcKey::generate(&group).unwrap();
let signature = sign_message(b"hello", &secret_key.private_key_to_der().unwrap()).unwrap();
let mut ctx = openssl::bn::BigNumContext::new().unwrap();
⋮----
.public_key()
.to_bytes(
⋮----
.unwrap();
new_secp256r1_instruction_with_signature(b"hello", &signature, &pubkey.try_into().unwrap())
⋮----
fn test_precompile() {
⋮----
declare_process_instruction!(MockBuiltin, 1, |_invoke_context| {
⋮----
secp256k1_account.set_executable(true);
⋮----
ed25519_account.set_executable(true);
⋮----
secp256r1_account.set_executable(true);
⋮----
mock_program_account.set_executable(true);
⋮----
secp256k1_instruction_for_test(),
ed25519_instruction_for_test(),
secp256r1_instruction_for_test(),
Instruction::new_with_bytes(mock_program_id, &[], vec![]),
⋮----
impl InvokeContextCallback for MockCallback {
fn is_precompile(&self, program_id: &Pubkey) -> bool {
⋮----
fn process_precompile(
⋮----
if self.is_precompile(program_id) {
⋮----
Err(PrecompileError::InvalidPublicKey)
⋮----
assert_eq!(transaction_context.get_instruction_trace_length(), 4);

================
File: svm/src/nonce_info.rs
================
pub struct NonceInfo {
⋮----
enum AdvanceNonceError {
⋮----
impl NonceInfo {
pub fn new(address: Pubkey, account: AccountSharedData) -> Self {
⋮----
fn try_advance_nonce(
⋮----
.map_err(|_| AdvanceNonceError::Invalid)?;
if let NonceState::Initialized(data) = nonce_versions.state() {
⋮----
self.account.set_state(&nonce_versions).unwrap();
Ok(())
⋮----
Err(AdvanceNonceError::Uninitialized)
⋮----
pub fn address(&self) -> &Pubkey {
⋮----
pub fn account(&self) -> &AccountSharedData {
⋮----
mod tests {
⋮----
fn create_nonce_account(state: NonceState) -> AccountSharedData {
⋮----
.unwrap()
⋮----
fn test_nonce_info() {
⋮----
let nonce_account = create_nonce_account(NonceState::Initialized(NonceData::new(
⋮----
let nonce_info = NonceInfo::new(nonce_address, nonce_account.clone());
assert_eq!(*nonce_info.address(), nonce_address);
assert_eq!(*nonce_info.account(), nonce_account);
⋮----
fn test_try_advance_nonce_success() {
⋮----
create_nonce_account(NonceState::Initialized(NonceData::new(
⋮----
let result = nonce_info.try_advance_nonce(new_nonce, new_lamports_per_signature);
assert_eq!(result, Ok(()));
let nonce_versions = StateMut::<NonceVersions>::state(&nonce_info.account).unwrap();
assert_eq!(
⋮----
fn test_try_advance_nonce_invalid() {
⋮----
let result = nonce_info.try_advance_nonce(durable_nonce, 5000);
assert_eq!(result, Err(AdvanceNonceError::Invalid));
⋮----
fn test_try_advance_nonce_uninitialized() {
⋮----
create_nonce_account(NonceState::Uninitialized),
⋮----
assert_eq!(result, Err(AdvanceNonceError::Uninitialized));

================
File: svm/src/program_loader.rs
================
pub(crate) enum ProgramAccountLoadResult {
⋮----
pub(crate) fn load_program_from_bytes(
⋮----
program_runtime_environment.clone(),
⋮----
deployment_slot.saturating_add(DELAY_VISIBILITY_SLOT_OFFSET),
⋮----
pub(crate) fn load_program_accounts<CB: TransactionProcessingCallback>(
⋮----
let (program_account, _slot) = callbacks.get_account_shared_data(pubkey)?;
if loader_v4::check_id(program_account.owner()) {
return Some(
solana_loader_v4_program::get_state(program_account.data())
.ok()
.and_then(|state| {
(!matches!(state.status, LoaderV4Status::Retracted)).then_some(state.slot)
⋮----
.map(|slot| ProgramAccountLoadResult::ProgramOfLoaderV4(program_account, slot))
.unwrap_or(ProgramAccountLoadResult::InvalidAccountData(
⋮----
if bpf_loader_upgradeable::check_id(program_account.owner()) {
⋮----
}) = program_account.state()
⋮----
callbacks.get_account_shared_data(&programdata_address)
⋮----
}) = programdata_account.state()
⋮----
return Some(ProgramAccountLoadResult::ProgramOfLoaderV3(
⋮----
return Some(ProgramAccountLoadResult::InvalidAccountData(
⋮----
if bpf_loader::check_id(program_account.owner()) {
return Some(ProgramAccountLoadResult::ProgramOfLoaderV2(program_account));
⋮----
if bpf_loader_deprecated::check_id(program_account.owner()) {
return Some(ProgramAccountLoadResult::ProgramOfLoaderV1(program_account));
⋮----
pub fn load_program_with_pubkey<CB: TransactionProcessingCallback>(
⋮----
program_id: pubkey.to_string(),
⋮----
let loaded_program = match load_program_accounts(callbacks, pubkey)? {
ProgramAccountLoadResult::InvalidAccountData(owner) => Ok(
⋮----
ProgramAccountLoadResult::ProgramOfLoaderV1(program_account) => load_program_from_bytes(
⋮----
program_account.data(),
program_account.owner(),
program_account.data().len(),
⋮----
environments.program_runtime_v1.clone(),
⋮----
.map_err(|_| (0, ProgramCacheEntryOwner::LoaderV1)),
ProgramAccountLoadResult::ProgramOfLoaderV2(program_account) => load_program_from_bytes(
⋮----
.map_err(|_| (0, ProgramCacheEntryOwner::LoaderV2)),
⋮----
.data()
.get(UpgradeableLoaderState::size_of_programdata_metadata()..)
.ok_or(Box::new(InstructionError::InvalidAccountData).into())
.and_then(|programdata| {
load_program_from_bytes(
⋮----
.len()
.saturating_add(programdata_account.data().len()),
⋮----
.map_err(|_| (slot, ProgramCacheEntryOwner::LoaderV3))
⋮----
.get(LoaderV4State::program_data_offset()..)
⋮----
.and_then(|elf_bytes| {
⋮----
.map_err(|_| (slot, ProgramCacheEntryOwner::LoaderV4)),
⋮----
.unwrap_or_else(|(slot, owner)| {
let env = environments.program_runtime_v1.clone();
⋮----
load_program_metrics.submit_datapoint(&mut execute_timings.details);
loaded_program.update_access_slot(slot);
Some(Arc::new(loaded_program))
⋮----
pub(crate) fn get_program_modification_slot<CB: TransactionProcessingCallback>(
⋮----
.get_account_shared_data(pubkey)
.ok_or(TransactionError::ProgramAccountNotFound)?;
if bpf_loader_upgradeable::check_id(program.owner()) {
⋮----
}) = program.state()
⋮----
.get_account_shared_data(&programdata_address)
⋮----
}) = programdata.state()
⋮----
return Ok(slot);
⋮----
Err(TransactionError::ProgramAccountNotFound)
} else if loader_v4::check_id(program.owner()) {
let state = solana_loader_v4_program::get_state(program.data())
.map_err(|_| TransactionError::ProgramAccountNotFound)?;
Ok(state.slot)
⋮----
Ok(0)
⋮----
mod tests {
⋮----
struct TestForkGraph {}
impl ForkGraph for TestForkGraph {
fn relationship(&self, _a: Slot, _b: Slot) -> BlockRelation {
⋮----
pub(crate) struct MockBankCallback {
⋮----
impl InvokeContextCallback for MockBankCallback {}
impl TransactionProcessingCallback for MockBankCallback {
fn get_account_shared_data(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, Slot)> {
⋮----
.borrow()
.get(pubkey)
.map(|account| (account.clone(), 0))
⋮----
fn test_load_program_accounts_account_not_found() {
⋮----
let result = load_program_accounts(&mock_bank, &key);
assert!(result.is_none());
⋮----
account_data.set_owner(bpf_loader_upgradeable::id());
⋮----
account_data.set_data(bincode::serialize(&state).unwrap());
⋮----
.borrow_mut()
.insert(key, account_data.clone());
⋮----
assert!(matches!(
⋮----
account_data.set_data(Vec::new());
⋮----
.insert(key, account_data);
⋮----
fn test_load_program_accounts_loader_v4() {
⋮----
account_data.set_owner(loader_v4::id());
⋮----
account_data.set_data(vec![0; 64]);
⋮----
account_data.set_data(encoded.to_vec());
⋮----
assert_eq!(data, account_data);
assert_eq!(slot, 25);
⋮----
_ => panic!("Invalid result"),
⋮----
fn test_load_program_accounts_loader_v1_or_v2() {
⋮----
account_data.set_owner(bpf_loader::id());
⋮----
fn test_load_program_accounts_success() {
⋮----
.insert(key1, account_data.clone());
⋮----
account_data2.set_data(bincode::serialize(&state).unwrap());
⋮----
.insert(key2, account_data2.clone());
let result = load_program_accounts(&mock_bank, &key1);
⋮----
assert_eq!(data1, account_data);
assert_eq!(data2, account_data2);
⋮----
fn load_test_program() -> Vec<u8> {
let mut dir = env::current_dir().unwrap();
dir.push("tests");
dir.push("example-programs");
dir.push("hello-solana");
dir.push("hello_solana_program.so");
let mut file = File::open(dir.clone()).expect("file not found");
let metadata = fs::metadata(dir).expect("Unable to read metadata");
let mut buffer = vec![0; metadata.len() as usize];
file.read_exact(&mut buffer).expect("Buffer overflow");
⋮----
fn test_load_program_from_bytes() {
let buffer = load_test_program();
⋮----
let size = buffer.len();
⋮----
let result = load_program_from_bytes(
⋮----
environment.clone(),
⋮----
assert!(result.is_ok());
⋮----
fn test_load_program_not_found() {
⋮----
let result = load_program_with_pubkey(
⋮----
&batch_processor.get_environments_for_epoch(50),
⋮----
fn test_load_program_invalid_account_data() {
⋮----
&batch_processor.get_environments_for_epoch(20),
⋮----
.get_environments_for_epoch(20)
⋮----
assert_eq!(result.unwrap(), Arc::new(loaded_program));
⋮----
fn test_load_program_program_loader_v1_or_v2() {
⋮----
account_data.set_data(buffer);
⋮----
let expected = load_program_from_bytes(
⋮----
account_data.data(),
account_data.owner(),
account_data.data().len(),
⋮----
assert_eq!(result.unwrap(), Arc::new(expected.unwrap()));
⋮----
fn test_load_program_program_loader_v3() {
⋮----
&batch_processor.get_environments_for_epoch(0),
⋮----
.get_environments_for_epoch(0)
⋮----
let mut buffer = load_test_program();
let mut header = bincode::serialize(&state).unwrap();
let mut complement = vec![
⋮----
header.append(&mut complement);
header.append(&mut buffer);
account_data.set_data(header);
⋮----
.insert(key2, account_data.clone());
⋮----
let data = account_data.data();
⋮----
.set_data(data[UpgradeableLoaderState::size_of_programdata_metadata()..].to_vec());
⋮----
fn test_load_program_of_loader_v4() {
⋮----
let mut header = account_data.data().to_vec();
⋮----
vec![0; std::cmp::max(0, LoaderV4State::program_data_offset() - header.len())];
⋮----
let data = account_data.data()[LoaderV4State::program_data_offset()..].to_vec();
account_data.set_data(data);
⋮----
fn test_load_program_environment() {
⋮----
let current_environments = batch_processor.environments.clone();
⋮----
batch_processor.epoch_boundary_preparation.write().unwrap();
⋮----
epoch_boundary_preparation.upcoming_environments = Some(upcoming_environments.clone());
⋮----
&batch_processor.get_environments_for_epoch(is_upcoming_env as u64),
⋮----
.unwrap();
assert_ne!(
⋮----
assert_eq!(
⋮----
fn test_program_modification_slot_account_not_found() {
⋮----
let result = get_program_modification_slot(&mock_bank, &key);
assert_eq!(result.err(), Some(TransactionError::ProgramAccountNotFound));
⋮----
fn test_program_modification_slot_success() {
⋮----
.insert(key1, account_data);
⋮----
.insert(key2, account_data);
let result = get_program_modification_slot(&mock_bank, &key1);
assert_eq!(result.unwrap(), 77);
⋮----
let mut account_data = AccountSharedData::new(100, encoded.len(), &loader_v4::id());
⋮----
assert_eq!(result.unwrap(), 58);
account_data.set_owner(Pubkey::new_unique());
⋮----
let result = get_program_modification_slot(&mock_bank, &key2);
assert_eq!(result.unwrap(), 0);

================
File: svm/src/rent_calculator.rs
================
pub enum RentState {
⋮----
pub fn check_rent_state(
⋮----
if let Some((pre_rent_state, post_rent_state)) = pre_rent_state.zip(post_rent_state) {
⋮----
check_rent_state_with_account(
⋮----
.get_key_of_account_at_index(index)
.expect(expect_msg),
⋮----
Ok(())
⋮----
pub fn check_rent_state_with_account(
⋮----
&& !transition_allowed(pre_rent_state, post_rent_state)
⋮----
Err(TransactionError::InsufficientFundsForRent { account_index })
⋮----
pub fn get_account_rent_state(
⋮----
} else if rent.is_exempt(account_lamports, account_size) {
⋮----
pub fn transition_allowed(pre_rent_state: &RentState, post_rent_state: &RentState) -> bool {

================
File: svm/src/rollback_accounts.rs
================
pub enum RollbackAccounts {
⋮----
impl Default for RollbackAccounts {
fn default() -> Self {
⋮----
pub struct RollbackAccountsIter<'a> {
⋮----
impl<'a> Iterator for RollbackAccountsIter<'a> {
type Item = &'a KeyedAccountSharedData;
fn next(&mut self) -> Option<Self::Item> {
if let Some(fee_payer) = self.fee_payer.take() {
return Some(fee_payer);
⋮----
if let Some(nonce) = self.nonce.take() {
return Some(nonce);
⋮----
impl<'a> IntoIterator for &'a RollbackAccounts {
⋮----
type IntoIter = RollbackAccountsIter<'a>;
fn into_iter(self) -> Self::IntoIter {
self.iter()
⋮----
impl RollbackAccounts {
pub(crate) fn new(
⋮----
if &fee_payer_address == nonce.address() {
fee_payer_account.set_data_from_slice(nonce.account().data());
⋮----
fee_payer_account.set_rent_epoch(fee_payer_loaded_rent_epoch);
⋮----
pub fn fee_payer(&self) -> &KeyedAccountSharedData {
⋮----
pub fn count(&self) -> usize {
⋮----
pub fn iter(&self) -> RollbackAccountsIter<'_> {
⋮----
fee_payer: Some(fee_payer),
⋮----
nonce: Some(nonce),
⋮----
pub fn data_size(&self) -> usize {
⋮----
for (_, account) in self.iter() {
total_size = total_size.saturating_add(account.data().len());
⋮----
mod tests {
⋮----
fn test_new_fee_payer_only() {
⋮----
let fee_payer_rent_epoch = fee_payer_account.rent_epoch();
⋮----
let mut account = fee_payer_account.clone();
account.set_lamports(fee_payer_account.lamports());
account.set_rent_epoch(fee_payer_rent_epoch + 1);
⋮----
assert_eq!(expected_fee_payer, fee_payer);
⋮----
_ => panic!("Expected FeePayerOnly variant"),
⋮----
fn test_new_same_nonce_and_fee_payer() {
⋮----
.unwrap();
⋮----
let mut account = nonce_account.clone();
account.set_lamports(nonce_account.lamports());
⋮----
let nonce = NonceInfo::new(nonce_address, rent_epoch_updated_fee_payer_account.clone());
⋮----
Some(nonce),
⋮----
assert_eq!(expected_rollback_accounts, rollback_accounts);
⋮----
fn test_separate_nonce_and_fee_payer() {
⋮----
let nonce = NonceInfo::new(nonce_address, nonce_account.clone());
⋮----
rent_epoch_updated_fee_payer_account.clone(),
⋮----
assert_eq!(expected_nonce, nonce);
⋮----
_ => panic!("Expected SeparateNonceAndFeePayer variant"),

================
File: svm/src/transaction_account_state_info.rs
================
pub(crate) struct TransactionAccountStateInfo {
⋮----
impl TransactionAccountStateInfo {
pub(crate) fn new(
⋮----
(0..message.account_keys().len())
.map(|i| {
let rent_state = if message.is_writable(i) {
⋮----
.accounts()
.try_borrow(i as IndexOfAccount)
⋮----
debug_assert!(!native_loader::check_id(account.owner()));
Some(get_account_rent_state(
⋮----
account.lamports(),
account.data().len(),
⋮----
debug_assert!(
⋮----
.collect()
⋮----
pub(crate) fn verify_changes(
⋮----
pre_state_infos.iter().zip(post_state_infos).enumerate()
⋮----
check_rent_state(
pre_state_info.rent_state.as_ref(),
post_state_info.rent_state.as_ref(),
⋮----
Ok(())
⋮----
mod test {
⋮----
fn test_new() {
⋮----
account_keys: vec![key2.pubkey(), key1.pubkey(), key4.pubkey()],
⋮----
instructions: vec![
⋮----
let transaction_accounts = vec![
⋮----
let context = TransactionContext::new(transaction_accounts, rent.clone(), 20, 20);
⋮----
assert_eq!(
⋮----
fn test_new_panic() {
⋮----
account_keys: vec![key2.pubkey(), key1.pubkey(), key4.pubkey(), key3.pubkey()],
⋮----
fn test_verify_changes() {
⋮----
let pre_rent_state = vec![
⋮----
let post_rent_state = vec![TransactionAccountStateInfo {
⋮----
assert!(result.is_ok());
let pre_rent_state = vec![TransactionAccountStateInfo {

================
File: svm/src/transaction_balances.rs
================
use qualifier_attr::field_qualifiers;
⋮----
type TxNativeBalances = Vec<u64>;
type TxTokenBalances = Vec<SvmTokenInfo>;
type BatchNativeBalances = Vec<TxNativeBalances>;
type BatchTokenBalances = Vec<TxTokenBalances>;
pub(crate) trait BalanceCollectionRoutines {
⋮----
pub struct BalanceCollector {
⋮----
impl BalanceCollector {
pub(crate) fn new_with_transaction_count(transaction_count: usize) -> Self {
⋮----
pub fn into_vecs(
⋮----
fn collect_balances<CB: TransactionProcessingCallback>(
⋮----
let mut native_balances = Vec::with_capacity(transaction.account_keys().len());
let mut token_balances = vec![];
let has_token_program = transaction.account_keys().iter().any(is_known_spl_token_id);
for (index, key) in transaction.account_keys().iter().enumerate() {
let Some(account) = account_loader.load_account(key) else {
native_balances.push(0);
⋮----
native_balances.push(account.lamports());
⋮----
&& !transaction.is_invoked(index)
&& !is_known_spl_token_id(key)
&& is_known_spl_token_id(account.owner())
⋮----
token_balances.push(token_info);
⋮----
pub(crate) fn lengths_match_expected(&self, expected_len: usize) -> bool {
self.native_pre.len() == expected_len
&& self.native_post.len() == expected_len
&& self.token_pre.len() == expected_len
&& self.token_post.len() == expected_len
⋮----
impl BalanceCollectionRoutines for BalanceCollector {
fn collect_pre_balances<CB: TransactionProcessingCallback>(
⋮----
let (native_balances, token_balances) = self.collect_balances(account_loader, transaction);
self.native_pre.push(native_balances);
self.token_pre.push(token_balances);
⋮----
fn collect_post_balances<CB: TransactionProcessingCallback>(
⋮----
self.native_post.push(native_balances);
self.token_post.push(token_balances);
⋮----
impl BalanceCollectionRoutines for Option<BalanceCollector> {
⋮----
inner.collect_pre_balances(account_loader, transaction)
⋮----
inner.collect_post_balances(account_loader, transaction)
⋮----
pub struct SvmTokenInfo {
⋮----
impl SvmTokenInfo {
fn unpack_token_account<CB: TransactionProcessingCallback>(
⋮----
let program_id = *account.owner();
⋮----
} = generic_token::Account::unpack(account.data(), &program_id)?;
let mint_account = account_loader.load_account(&mint)?;
if *mint_account.owner() != program_id {
⋮----
generic_token::Mint::unpack(mint_account.data(), &program_id)?;
Some(Self {
account_index: index.try_into().ok()?,

================
File: svm/src/transaction_commit_result.rs
================
pub type TransactionCommitResult = TransactionResult<CommittedTransaction>;
⋮----
pub struct CommittedTransaction {
⋮----
pub trait TransactionCommitResultExtensions {
⋮----
impl TransactionCommitResultExtensions for TransactionCommitResult {
fn was_committed(&self) -> bool {
self.is_ok()
⋮----
fn was_executed_successfully(&self) -> bool {
⋮----
Ok(committed_tx) => committed_tx.status.is_ok(),

================
File: svm/src/transaction_error_metrics.rs
================
use std::num::Saturating;
⋮----
pub struct TransactionErrorMetrics {
⋮----
impl TransactionErrorMetrics {
pub fn new() -> Self {
⋮----
pub fn accumulate(&mut self, other: &TransactionErrorMetrics) {

================
File: svm/src/transaction_execution_result.rs
================
pub struct TransactionLoadedAccountsStats {
⋮----
pub struct ExecutedTransaction {
⋮----
impl ExecutedTransaction {
pub fn was_successful(&self) -> bool {
self.execution_details.was_successful()
⋮----
pub struct TransactionExecutionDetails {
⋮----
impl TransactionExecutionDetails {
⋮----
self.status.is_ok()

================
File: svm/src/transaction_processing_callback.rs
================


================
File: svm/src/transaction_processing_result.rs
================
pub type TransactionProcessingResult = TransactionResult<ProcessedTransaction>;
pub trait TransactionProcessingResultExtensions {
⋮----
pub enum ProcessedTransaction {
⋮----
impl TransactionProcessingResultExtensions for TransactionProcessingResult {
fn was_processed(&self) -> bool {
self.is_ok()
⋮----
fn was_processed_with_successful_result(&self) -> bool {
⋮----
Ok(processed_tx) => processed_tx.was_processed_with_successful_result(),
⋮----
fn processed_transaction(&self) -> Option<&ProcessedTransaction> {
self.as_ref().ok()
⋮----
fn flattened_result(&self) -> TransactionResult<()> {
self.as_ref()
.map_err(|err| err.clone())
.and_then(|processed_tx| processed_tx.status())
⋮----
impl ProcessedTransaction {
⋮----
Self::Executed(executed_tx) => executed_tx.execution_details.status.is_ok(),
⋮----
pub fn status(&self) -> TransactionResult<()> {
⋮----
Self::Executed(executed_tx) => executed_tx.execution_details.status.clone(),
Self::FeesOnly(details) => Err(TransactionError::clone(&details.load_error)),
⋮----
pub fn fee_details(&self) -> FeeDetails {
⋮----
pub fn executed_transaction(&self) -> Option<&ExecutedTransaction> {
⋮----
Self::Executed(context) => Some(context),
⋮----
pub fn execution_details(&self) -> Option<&TransactionExecutionDetails> {
⋮----
Self::Executed(context) => Some(&context.execution_details),
⋮----
pub fn executed_units(&self) -> u64 {
self.execution_details()
.map(|detail| detail.executed_units)
.unwrap_or_default()
⋮----
pub fn loaded_accounts_data_size(&self) -> u32 {
⋮----
Self::FeesOnly(details) => details.rollback_accounts.data_size() as u32,

================
File: svm/src/transaction_processor.rs
================
pub type TransactionLogMessages = Vec<String>;
pub struct LoadAndExecuteSanitizedTransactionsOutput {
⋮----
pub struct ExecutionRecordingConfig {
⋮----
impl ExecutionRecordingConfig {
pub fn new_single_setting(option: bool) -> Self {
⋮----
pub struct TransactionProcessingConfig<'a> {
/// Encapsulates overridden accounts, typically used for transaction
    /// simulation.
⋮----
/// simulation.
    pub account_overrides: Option<&'a AccountOverrides>,
⋮----
pub struct TransactionProcessingEnvironment {
⋮----
pub struct TransactionBatchProcessor<FG: ForkGraph> {
⋮----
impl<FG: ForkGraph> Debug for TransactionBatchProcessor<FG> {
fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
f.debug_struct("TransactionBatchProcessor")
.field("slot", &self.slot)
.field("epoch", &self.epoch)
.field("sysvar_cache", &self.sysvar_cache)
.field("global_program_cache", &self.global_program_cache)
.finish()
⋮----
impl<FG: ForkGraph> Default for TransactionBatchProcessor<FG> {
fn default() -> Self {
⋮----
/// Create a new, uninitialized `TransactionBatchProcessor`.
    ///
⋮----
///
    /// In this context, uninitialized means that the `TransactionBatchProcessor`
⋮----
/// In this context, uninitialized means that the `TransactionBatchProcessor`
    /// has been initialized with an empty program cache. The cache contains no
⋮----
/// has been initialized with an empty program cache. The cache contains no
    /// programs (including builtins) and has not been configured with a valid
⋮----
/// programs (including builtins) and has not been configured with a valid
    /// fork graph.
⋮----
/// fork graph.
    ///
⋮----
///
    /// When using this method, it's advisable to call `set_fork_graph_in_program_cache`
⋮----
/// When using this method, it's advisable to call `set_fork_graph_in_program_cache`
    pub fn new_uninitialized(slot: Slot, epoch: Epoch) -> Self {
⋮----
pub fn new_uninitialized(slot: Slot, epoch: Epoch) -> Self {
⋮----
pub fn new(
⋮----
.write()
.unwrap()
.set_fork_graph(fork_graph);
⋮----
program_runtime_environment_v1.unwrap_or(empty_loader());
⋮----
program_runtime_environment_v2.unwrap_or(empty_loader());
⋮----
pub fn new_from(&self, slot: Slot, epoch: Epoch) -> Self {
⋮----
epoch_boundary_preparation: self.epoch_boundary_preparation.clone(),
global_program_cache: self.global_program_cache.clone(),
environments: self.environments.clone(),
builtin_program_ids: RwLock::new(self.builtin_program_ids.read().unwrap().clone()),
⋮----
pub fn set_execution_cost(&mut self, cost: SVMTransactionExecutionCost) {
⋮----
pub fn set_environments(&mut self, new_environments: ProgramRuntimeEnvironments) {
⋮----
.read()
⋮----
upcoming_environments.program_runtime_v1.clone();
⋮----
upcoming_environments.program_runtime_v2.clone();
⋮----
pub fn get_environments_for_epoch(&self, epoch: Epoch) -> ProgramRuntimeEnvironments {
⋮----
.get_upcoming_environments_for_epoch(epoch)
.unwrap_or_else(|| self.environments.clone())
⋮----
pub fn sysvar_cache(&self) -> RwLockReadGuard<'_, SysvarCache> {
self.sysvar_cache.read().unwrap()
⋮----
/// Main entrypoint to the SVM.
    pub fn load_and_execute_sanitized_transactions<CB: TransactionProcessingCallback>(
⋮----
pub fn load_and_execute_sanitized_transactions<CB: TransactionProcessingCallback>(
⋮----
// If `check_results` does not have the same length as `sanitized_txs`,
// transactions could be truncated as a result of `.iter().zip()` in
// many of the below methods.
// See <https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.zip>.
debug_assert_eq!(
⋮----
// Initialize metrics.
⋮----
let mut processing_results = Vec::with_capacity(sanitized_txs.len());
// Determine a capacity for the internal account cache. This
// over-allocates but avoids ever reallocating, and spares us from
// deduplicating the account keys lists.
⋮----
sanitized_txs.iter().map(|tx| tx.account_keys().len()).sum();
// Create the account loader, which wraps all external account fetching.
⋮----
account_keys_in_batch.saturating_add(
⋮----
.map(|a| a.len())
.unwrap_or_default(),
⋮----
// Create the transaction balance collector if recording is enabled.
⋮----
.then(|| BalanceCollector::new_with_transaction_count(sanitized_txs.len()));
// Create the batch-local program cache.
⋮----
let builtins = self.builtin_program_ids.read().unwrap().clone();
let ((), program_cache_us) = measure_us!({
⋮----
false, // increment_usage_counter
⋮----
.saturating_add_in_place(ExecuteTimingType::ProgramCacheUs, program_cache_us);
⋮----
processing_results: (0..sanitized_txs.len())
.map(|_| Err(TransactionError::ProgramCacheHitMaxLimit))
.collect(),
// If we abort the batch and balance recording is enabled, no balances should be
// collected. If this is a leader thread, no batch will be committed.
⋮----
// Validate, execute, and collect results from each transaction in order.
// With SIMD83, transactions must be executed in order, because transactions
// in the same batch may modify the same accounts. Transaction order is
// preserved within entries written to the ledger.
for (tx, check_result) in sanitized_txs.iter().zip(check_results) {
⋮----
measure_us!(check_result.and_then(|tx_details| {
⋮----
.saturating_add_in_place(ExecuteTimingType::ValidateFeesUs, validate_fees_us);
let (load_result, single_load_us) = measure_us!(load_transaction(
⋮----
load_us = load_us.saturating_add(single_load_us);
⋮----
measure_us!(balance_collector.collect_pre_balances(&mut account_loader, tx));
⋮----
.saturating_add_in_place(ExecuteTimingType::CollectBalancesUs, collect_balances_us);
let (processing_result, single_execution_us) = measure_us!(match load_result {
⋮----
// Update loaded accounts cache with nonce and fee-payer
⋮----
true, // increment_usage_counter
⋮----
// Successful transactions need to update the account loader cache as future
// transactions in the batch may depend on them.
⋮----
// Also update local program cache with modifications made by the
// transaction, if it executed successfully.
⋮----
// If the transaction failed & drop on failure is set then we don't want to
⋮----
execution_us = execution_us.saturating_add(single_execution_us);
⋮----
measure_us!(balance_collector.collect_post_balances(&mut account_loader, tx));
⋮----
if config.all_or_nothing && processing_result.is_err() {
for res in processing_results.iter_mut() {
*res = Err(TransactionError::CommitCancelled);
⋮----
processing_results.push(processing_result);
processing_results.extend(
(0..sanitized_txs.len() - processing_results.len())
.map(|_| Err(TransactionError::CommitCancelled)),
⋮----
.evict_using_2s_random_selection(
⋮----
debug!(
⋮----
execute_timings.saturating_add_in_place(ExecuteTimingType::LoadUs, load_us);
execute_timings.saturating_add_in_place(ExecuteTimingType::ExecuteUs, execution_us);
⋮----
debug_assert!(balance_collector.lengths_match_expected(sanitized_txs.len()));
⋮----
fn validate_transaction_nonce_and_fee_payer<CB: TransactionProcessingCallback>(
⋮----
fn validate_transaction_fee_payer<CB: TransactionProcessingCallback>(
⋮----
let fee_payer_address = message.fee_payer();
⋮----
account_loader.load_transaction_account(fee_payer_address, true)
⋮----
return Err(TransactionError::AccountNotFound);
⋮----
let fee_payer_loaded_rent_epoch = loaded_fee_payer.account.rent_epoch();
update_rent_exempt_status_for_account(rent, &mut loaded_fee_payer.account);
⋮----
validate_fee_payer(
⋮----
compute_budget_and_limits.fee_details.total_fee(),
⋮----
loaded_fee_payer.account.clone(),
⋮----
Ok(ValidatedTransactionDetails {
⋮----
fn validate_transaction_nonce<CB: TransactionProcessingCallback>(
⋮----
.load_transaction_account(nonce_info.address(), true)
.and_then(|ref current_nonce| {
system_program::check_id(current_nonce.account.owner()).then_some(())?;
StateMut::<NonceVersions>::state(&current_nonce.account).ok()
⋮----
.and_then(
|current_nonce_versions| match current_nonce_versions.state() {
⋮----
.account_keys()
.iter()
.enumerate()
.any(|(i, address)| {
address == &current_nonce_data.authority && message.is_signer(i)
⋮----
Some(nonce_can_be_advanced)
⋮----
Err(TransactionError::BlockhashNotFound)
⋮----
Err(TransactionError::AccountNotFound)
⋮----
Some(true) => Ok(()),
⋮----
fn filter_executable_program_accounts<CB: TransactionProcessingCallback>(
⋮----
for account_key in tx.account_keys().iter() {
if let Some(cache_entry) = program_cache_for_tx_batch.find(account_key) {
cache_entry.tx_usage_counter.fetch_add(1, Ordering::Relaxed);
⋮----
.get_account_shared_data(account_key)
.map(|(account, _slot)| PROGRAM_OWNERS.contains(account.owner()))
.unwrap_or(false)
⋮----
program_accounts_set.insert(*account_key);
⋮----
fn replenish_program_cache<CB: TransactionProcessingCallback>(
⋮----
.map(|pubkey| {
⋮----
get_program_modification_slot(account_loader, pubkey)
.map_or(ProgramCacheMatchCriteria::Tombstone, |slot| {
⋮----
.collect();
⋮----
let global_program_cache = self.global_program_cache.read().unwrap();
let program_to_load = global_program_cache.extract(
⋮----
let program_to_store = program_to_load.map(|key| {
let program = load_program_with_pubkey(
⋮----
.expect("called load_program_with_pubkey() with nonexistent account");
⋮----
(program_to_store, task_waiter.cookie(), task_waiter)
⋮----
let mut global_program_cache = self.global_program_cache.write().unwrap();
if global_program_cache.finish_cooperative_loading_task(
⋮----
} else if missing_programs.is_empty() {
⋮----
let _new_cookie = task_waiter.wait(task_cookie);
⋮----
fn execute_loaded_transaction<CB: TransactionProcessingCallback>(
⋮----
debug_assert!(transaction_accounts.len() == tx.account_keys().len());
fn transaction_accounts_lamports_sum(
⋮----
accounts.iter().try_fold(0u128, |sum, (_, account)| {
sum.checked_add(u128::from(account.lamports()))
⋮----
transaction_accounts_lamports_sum(&transaction_accounts).unwrap_or(0);
⋮----
environment.rent.clone(),
⋮----
None => Some(LogCollector::new_ref()),
Some(log_messages_bytes_limit) => Some(LogCollector::new_ref_with_limit(Some(
⋮----
let sysvar_cache = &self.sysvar_cache.read().unwrap();
⋮----
log_collector.clone(),
⋮----
let process_result = process_message(
⋮----
process_message_time.stop();
drop(invoke_context);
execute_timings.execute_accessories.process_message_us += process_message_time.as_us();
⋮----
.and_then(|info| {
⋮----
.map(|_| info)
⋮----
.map_err(|err| {
⋮----
log_collector.and_then(|log_collector| {
⋮----
.map(|log_collector| log_collector.into_inner().into_messages())
.ok()
⋮----
if status.is_ok()
&& transaction_accounts_lamports_sum(&accounts)
.filter(|lamports_after_tx| lamports_before_tx == *lamports_after_tx)
.is_none()
⋮----
status = Err(TransactionError::UnbalancedTransaction);
⋮----
let status = status.map(|_| ());
⋮----
execute_timings.details.total_account_count += loaded_transaction.accounts.len() as u64;
⋮----
&& !return_data.data.is_empty()
⋮----
Some(return_data)
⋮----
programs_modified_by_tx: program_cache_for_tx_batch.drain_modified_entries(),
⋮----
fn deconstruct_transaction(
⋮----
debug_assert!(transaction_context
⋮----
let ix_trace = transaction_context.take_instruction_trace();
⋮----
for ix_in_trace in ix_trace.into_iter() {
let stack_height = ix_in_trace.nesting_level.saturating_add(1);
⋮----
outer_instructions.push(Vec::new());
} else if let Some(inner_instructions) = outer_instructions.last_mut() {
let stack_height = u8::try_from(stack_height).unwrap_or(u8::MAX);
inner_instructions.push(InnerInstruction {
⋮----
ix_in_trace.instruction_data.into_owned(),
⋮----
.map(|acc| acc.index_in_transaction as u8)
⋮----
debug_assert!(false);
⋮----
Some(outer_instructions)
⋮----
let record: ExecutionRecord = transaction_context.into();
⋮----
pub fn fill_missing_sysvar_cache_entries<CB: TransactionProcessingCallback>(
⋮----
let mut sysvar_cache = self.sysvar_cache.write().unwrap();
sysvar_cache.fill_missing_entries(|pubkey, set_sysvar| {
if let Some((account, _slot)) = callbacks.get_account_shared_data(pubkey) {
set_sysvar(account.data());
⋮----
pub fn reset_sysvar_cache(&self) {
⋮----
sysvar_cache.reset();
⋮----
pub fn get_sysvar_cache_for_tests(&self) -> SysvarCache {
self.sysvar_cache.read().unwrap().clone()
⋮----
pub fn add_builtin(&self, program_id: Pubkey, builtin: ProgramCacheEntry) {
self.builtin_program_ids.write().unwrap().insert(program_id);
self.global_program_cache.write().unwrap().assign_program(
⋮----
fn writable_sysvar_cache(&self) -> &RwLock<SysvarCache> {
⋮----
mod tests {
⋮----
use solana_sysvar::fees::Fees;
⋮----
fn new_unchecked_sanitized_message(message: Message) -> SanitizedMessage {
⋮----
struct TestForkGraph {}
impl ForkGraph for TestForkGraph {
fn relationship(&self, _a: Slot, _b: Slot) -> BlockRelation {
⋮----
struct MockBankCallback {
⋮----
impl Default for MockBankCallback {
⋮----
impl InvokeContextCallback for MockBankCallback {}
impl TransactionProcessingCallback for MockBankCallback {
fn get_account_shared_data(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, Slot)> {
⋮----
.get(pubkey)
.map(|account| (account.clone(), 0))
⋮----
fn inspect_account(
⋮----
AccountState::Alive(account) => Some(account.clone()),
⋮----
.entry(*address)
.or_default()
.push((account, is_writable));
⋮----
impl MockBankCallback {
pub fn calculate_fee_details(
⋮----
.num_transaction_signatures()
.saturating_add(message.num_ed25519_signatures())
.saturating_add(message.num_secp256k1_signatures())
.saturating_add(message.num_secp256r1_signatures());
⋮----
signature_count.saturating_mul(lamports_per_signature),
⋮----
fn from(callbacks: &'a MockBankCallback) -> AccountLoader<'a, MockBankCallback> {
⋮----
fn test_check_results_txs_length_mismatch(check_results_len: usize) {
let sanitized_message = new_unchecked_sanitized_message(Message {
account_keys: vec![Pubkey::new_from_array([0; 32])],
⋮----
instructions: vec![CompiledInstruction {
⋮----
// Transactions, length 2.
let sanitized_txs = vec![
⋮----
let check_results = vec![
⋮----
batch_processor.load_and_execute_sanitized_transactions(
⋮----
fn test_inner_instructions_list_from_instruction_trace() {
⋮----
vec![(
⋮----
instruction_trace.len(),
⋮----
for (index_in_trace, stack_height) in instruction_trace.into_iter().enumerate() {
while stack_height <= transaction_context.get_instruction_stack_height() {
transaction_context.pop().unwrap();
⋮----
if stack_height > transaction_context.get_instruction_stack_height() {
⋮----
.configure_next_instruction_for_tests(0, vec![], vec![index_in_trace as u8])
.unwrap();
transaction_context.push().unwrap();
⋮----
assert_eq!(
⋮----
fn test_execute_loaded_transaction_recordings() {
// Setting all the arguments correctly is too burdensome for testing
// execute_loaded_transaction separately.This function will be tested in an integration
// test with load_and_execute_sanitized_transactions
⋮----
let sanitized_message = new_unchecked_sanitized_message(message);
⋮----
vec![Signature::new_unique()],
⋮----
accounts: vec![(Pubkey::new_unique(), AccountSharedData::default())],
program_indices: vec![0],
⋮----
let executed_tx = batch_processor.execute_loaded_transaction(
⋮----
loaded_transaction.clone(),
⋮----
assert!(executed_tx.execution_details.log_messages.is_some());
processing_config.log_messages_bytes_limit = Some(2);
⋮----
assert!(executed_tx.execution_details.inner_instructions.is_none());
⋮----
assert!(executed_tx.execution_details.log_messages.is_none());
assert!(executed_tx.execution_details.inner_instructions.is_some());
⋮----
fn test_execute_loaded_transaction_error_metrics() {
⋮----
account_keys: vec![key1, key2],
⋮----
account_data.set_owner(bpf_loader::id());
⋮----
accounts: vec![
⋮----
let _ = batch_processor.execute_loaded_transaction(
⋮----
assert_eq!(error_metrics.instruction_error.0, 1);
⋮----
fn test_replenish_program_cache_with_nonexistent_accounts() {
⋮----
let account_loader = (&mock_bank).into();
⋮----
account_set.insert(key);
⋮----
batch_processor.replenish_program_cache(
⋮----
fn test_replenish_program_cache() {
⋮----
batch_processor.get_environments_for_epoch(0);
⋮----
.insert(key, account_data);
⋮----
assert!(!program_cache_for_tx_batch.hit_max_limit);
⋮----
let program = program_cache_for_tx_batch.find(&key).unwrap();
assert!(matches!(
⋮----
assert!(loaded_missing > 0);
⋮----
fn test_filter_executable_program_accounts() {
⋮----
data1.set_owner(owner1);
data1.set_lamports(93);
⋮----
.insert(key1, data1);
⋮----
data2.set_owner(owner2);
data2.set_lamports(90);
⋮----
.insert(key2, data2);
⋮----
let program_accounts_set = batch_processor.filter_executable_program_accounts(
⋮----
assert_eq!(program_accounts_set.len(), 2);
assert!(program_accounts_set.contains(&key1));
assert!(program_accounts_set.contains(&key2));
⋮----
fn test_filter_executable_program_accounts_no_errors() {
⋮----
bank.account_shared_data.write().unwrap().insert(
⋮----
let account_loader = (&bank).into();
⋮----
vec![account1_pubkey, account2_pubkey, account3_pubkey],
vec![CompiledInstruction::new(1, &(), vec![0])],
⋮----
vec![account4_pubkey, account3_pubkey, account2_pubkey],
⋮----
let tx1_programs = batch_processor.filter_executable_program_accounts(
⋮----
assert_eq!(tx1_programs.len(), 1);
assert!(
⋮----
let tx2_programs = batch_processor.filter_executable_program_accounts(
⋮----
assert_eq!(tx2_programs.len(), 2);
⋮----
fn test_sysvar_cache_initialization1() {
⋮----
let clock_account = create_account_shared_data_for_test(&clock);
⋮----
.insert(sysvar::clock::id(), clock_account);
⋮----
let epoch_schedule_account = create_account_shared_data_for_test(&epoch_schedule);
⋮----
.insert(sysvar::epoch_schedule::id(), epoch_schedule_account);
⋮----
let fees_account = create_account_shared_data_for_test(&fees);
⋮----
.insert(sysvar::fees::id(), fees_account);
⋮----
let rent_account = create_account_shared_data_for_test(&rent);
⋮----
.insert(sysvar::rent::id(), rent_account);
⋮----
transaction_processor.fill_missing_sysvar_cache_entries(&mock_bank);
let sysvar_cache = transaction_processor.sysvar_cache.read().unwrap();
let cached_clock = sysvar_cache.get_clock();
let cached_epoch_schedule = sysvar_cache.get_epoch_schedule();
let cached_fees = sysvar_cache.get_fees();
let cached_rent = sysvar_cache.get_rent();
⋮----
assert!(sysvar_cache.get_slot_hashes().is_err());
assert!(sysvar_cache.get_epoch_rewards().is_err());
⋮----
fn test_reset_and_fill_sysvar_cache() {
⋮----
// Fill the sysvar cache
⋮----
// Reset the sysvar cache
transaction_processor.reset_sysvar_cache();
⋮----
// Test that sysvar cache is empty and none of the values are found
assert!(sysvar_cache.get_clock().is_err());
assert!(sysvar_cache.get_epoch_schedule().is_err());
assert!(sysvar_cache.get_fees().is_err());
⋮----
assert!(sysvar_cache.get_rent().is_err());
⋮----
// Refill the cache and test the values are available.
⋮----
fn test_add_builtin() {
⋮----
name.len(),
⋮----
batch_processor.add_builtin(key, program);
⋮----
batch_processor.get_environments_for_epoch(batch_processor.epoch);
⋮----
.extract(
&mut vec![(key, ProgramCacheMatchCriteria::NoCriteria)],
⋮----
let entry = loaded_programs_for_tx_batch.find(&key).unwrap();
// Repeating code because ProgramCacheEntry does not implement clone.
⋮----
assert_eq!(entry, Arc::new(program));
⋮----
fn test_validate_transaction_fee_payer_exact_balance(
⋮----
let message = new_unchecked_sanitized_message(Message::new_with_blockhash(
⋮----
Some(&Pubkey::new_unique()),
⋮----
let min_balance = rent.minimum_balance(nonce::state::State::size());
⋮----
mock_accounts.insert(*fee_payer_address, fee_payer_account.clone());
⋮----
let mut account_loader = (&mock_bank).into();
⋮----
let mut account = fee_payer_account.clone();
account.set_rent_epoch(RENT_EXEMPT_RENT_EPOCH);
account.set_lamports(0);
⋮----
fn test_validate_transaction_fee_payer_rent_paying(
⋮----
let min_balance = rent.minimum_balance(0);
⋮----
account.set_lamports(starting_balance - transaction_fee);
⋮----
fn test_validate_transaction_fee_payer_not_found() {
⋮----
new_unchecked_sanitized_message(Message::new(&[], Some(&Pubkey::new_unique())));
⋮----
assert_eq!(error_counters.account_not_found.0, 1);
assert_eq!(result, Err(TransactionError::AccountNotFound));
⋮----
fn test_validate_transaction_fee_payer_insufficient_funds() {
⋮----
assert_eq!(error_counters.insufficient_funds.0, 1);
assert_eq!(result, Err(TransactionError::InsufficientFundsForFee));
⋮----
fn test_validate_transaction_fee_payer_insufficient_rent() {
⋮----
fn test_validate_transaction_fee_payer_invalid() {
⋮----
assert_eq!(error_counters.invalid_account_for_fee.0, 1);
assert_eq!(result, Err(TransactionError::InvalidAccountForFee));
⋮----
fn test_validate_transaction_fee_payer_is_nonce(formalize_loaded_transaction_data_size: bool) {
⋮----
let min_balance = Rent::default().minimum_balance(nonce::state::State::size());
⋮----
let mut future_nonce = NonceInfo::new(*fee_payer_address, fee_payer_account.clone());
⋮----
.try_advance_nonce(next_durable_nonce, lamports_per_signature)
⋮----
Some(future_nonce.clone()),
⋮----
account.set_lamports(min_balance);
⋮----
fn test_inspect_account_fee_payer() {
⋮----
.insert(fee_payer_address, fee_payer_account.clone());
⋮----
Some(&fee_payer_address),
⋮----
.map(|(k, v)| (*k, v.clone()))

================
File: svm/tests/example-programs/clock-sysvar/src/lib.rs
================
entrypoint!(process_instruction);
fn process_instruction(
⋮----
let time_now = Clock::get().unwrap().unix_timestamp;
let return_data = time_now.to_be_bytes();
set_return_data(&return_data);
Ok(())

================
File: svm/tests/example-programs/clock-sysvar/Cargo.toml
================
[package]
name = "clock-sysvar-program"
version = "4.0.0-alpha.0"
edition = "2021"

[dependencies]

[lib]
crate-type = ["cdylib", "rlib"]

[workspace]

================
File: svm/tests/example-programs/hello-solana/src/lib.rs
================
entrypoint!(process_instruction);
fn process_instruction(
⋮----
msg!("Hello, Solana!");
Ok(())

================
File: svm/tests/example-programs/hello-solana/Cargo.toml
================
[package]
name = "hello-solana-program"
version = "4.0.0-alpha.0"
edition = "2021"

[dependencies]

[lib]
crate-type = ["cdylib", "rlib"]

[workspace]

================
File: svm/tests/example-programs/simple-transfer/src/lib.rs
================
entrypoint!(process_instruction);
fn process_instruction(
⋮----
let amount = u64::from_be_bytes(data[0..8].try_into().unwrap());
let accounts_iter = &mut accounts.iter();
let payer = next_account_info(accounts_iter)?;
let recipient = next_account_info(accounts_iter)?;
let system_program = next_account_info(accounts_iter)?;
invoke(
⋮----
&[payer.clone(), recipient.clone(), system_program.clone()],
⋮----
Ok(())

================
File: svm/tests/example-programs/simple-transfer/Cargo.toml
================
[package]
name = "simple-transfer-program"
version = "4.0.0-alpha.0"
edition = "2021"

[dependencies]

[lib]
crate-type = ["cdylib", "rlib"]

[workspace]

================
File: svm/tests/example-programs/transfer-from-account/src/lib.rs
================
entrypoint!(process_instruction);
fn process_instruction(
⋮----
let accounts_iter = &mut accounts.iter();
let payer = next_account_info(accounts_iter)?;
let recipient = next_account_info(accounts_iter)?;
let data_account = next_account_info(accounts_iter)?;
let system_program = next_account_info(accounts_iter)?;
let amount = u64::from_le_bytes(data_account.data.borrow()[0..8].try_into().unwrap());
invoke(
⋮----
&[payer.clone(), recipient.clone(), system_program.clone()],
⋮----
Ok(())

================
File: svm/tests/example-programs/transfer-from-account/Cargo.toml
================
[package]
name = "transfer-from-account"
version = "4.0.0-alpha.0"
edition = "2021"

[dependencies]

[lib]
crate-type = ["cdylib", "rlib"]

[workspace]

================
File: svm/tests/example-programs/write-to-account/src/lib.rs
================
entrypoint!(process_instruction);
fn process_instruction(
⋮----
let accounts_iter = &mut accounts.iter();
let target_account_info = next_account_info(accounts_iter)?;
⋮----
msg!(
⋮----
let mut account_data = target_account_info.try_borrow_mut_data()?;
⋮----
let incinerator_info = next_account_info(accounts_iter)?;
⋮----
return Err(ProgramError::InvalidAccountData);
⋮----
let mut target_lamports = target_account_info.try_borrow_mut_lamports()?;
let mut incinerator_lamports = incinerator_info.try_borrow_mut_lamports()?;
⋮----
.checked_add(**target_lamports)
.ok_or(ProgramError::ArithmeticOverflow)?;
⋮----
.checked_sub(**target_lamports)
.ok_or(ProgramError::InsufficientFunds)?;
⋮----
let new_size = usize::from_le_bytes(data[1..9].try_into().unwrap());
target_account_info.realloc(new_size, true)?;
⋮----
return Err(ProgramError::InvalidArgument);
⋮----
Ok(())

================
File: svm/tests/example-programs/write-to-account/Cargo.toml
================
[package]
name = "write-to-account"
version = "4.0.0-alpha.0"
edition = "2021"

[dependencies]

[lib]
crate-type = ["cdylib", "rlib"]

[workspace]

================
File: svm/tests/concurrent_tests.rs
================
mod mock_bank;
⋮----
fn program_cache_execution(threads: usize) {
⋮----
let programs = vec![
⋮----
let account_maps: HashSet<Pubkey> = programs.iter().copied().collect();
⋮----
.map(|_| {
let local_bank = mock_bank.clone();
⋮----
let maps = account_maps.clone();
let programs = programs.clone();
⋮----
processor.get_environments_for_epoch(processor.epoch);
processor.replenish_program_cache(
⋮----
let cache_entry = result.find(key);
assert!(matches!(
⋮----
.collect();
⋮----
th.join().unwrap();
⋮----
fn test_program_cache_with_probabilistic_scheduler() {
⋮----
program_cache_execution(4);
⋮----
fn test_program_cache_with_random_scheduler() {
shuttle::check_random(move || program_cache_execution(4), MAX_ITERATIONS);
⋮----
fn test_program_cache_with_exhaustive_scheduler() {
let scheduler = shuttle::scheduler::DfsScheduler::new(Some(MAX_ITERATIONS), true);
⋮----
runner.run(move || program_cache_execution(4));
⋮----
fn svm_concurrent() {
⋮----
Some(Arc::new(create_custom_loader())),
⋮----
mock_bank.configure_sysvars();
batch_processor.fill_missing_sysvar_cache_entries(&*mock_bank);
register_builtins(&mock_bank, &batch_processor, false);
let program_id = deploy_program("transfer-from-account".to_string(), 0, &mock_bank);
⋮----
let mut transactions = vec![Vec::new(); THREADS];
let mut check_data = vec![Vec::new(); THREADS];
⋮----
account_data.set_data(AMOUNT.to_le_bytes().to_vec());
account_data.set_rent_epoch(u64::MAX);
account_data.set_lamports(1);
⋮----
.write()
.unwrap()
.insert(read_account, account_data);
⋮----
struct CheckTxData {
⋮----
account_data.set_lamports(BALANCE);
⋮----
let shared_data = &mut mock_bank.account_shared_data.write().unwrap();
shared_data.insert(sender, account_data.clone());
shared_data.insert(recipient, account_data.clone());
shared_data.insert(fee_payer, account_data);
⋮----
let accounts = vec![
⋮----
let legacy_transaction = Transaction::new_with_payer(&[instruction], Some(&fee_payer));
⋮----
transactions[idx % THREADS].push(sanitized_transaction.unwrap());
check_data[idx % THREADS].push(CheckTxData {
⋮----
.map(|idx| {
let local_batch = batch_processor.clone();
⋮----
.iter()
.map(|tx| {
Ok(CheckedTransactionDetails::new(
⋮----
let result = local_batch.load_and_execute_sanitized_transactions(
⋮----
.clone(),
⋮----
for (idx, processing_result) in result.processing_results.iter().enumerate() {
assert!(processing_result.was_processed());
let processed_tx = processing_result.processed_transaction().unwrap();
assert_matches!(processed_tx, &ProcessedTransaction::Executed(_));
let executed_tx = processed_tx.executed_transaction().unwrap();
⋮----
assert_eq!(account_data.lamports(), BALANCE - 10000);
⋮----
assert_eq!(account_data.lamports(), BALANCE - AMOUNT);
⋮----
assert_eq!(account_data.lamports(), BALANCE + AMOUNT);
⋮----
fn test_svm_with_probabilistic_scheduler() {
⋮----
svm_concurrent();

================
File: svm/tests/integration_test.rs
================
mod mock_bank;
fn process_test_compute_budget_instructions<'a>(
⋮----
// Scan for compute budget instructions.
// Only key on `SetLoadedAccountsDataSizeLimit`.
⋮----
&& instruction.data.len() >= 5
⋮----
loaded_accounts_data_size_limit = Some(size);
⋮----
.ok_or(TransactionError::InvalidLoadedAccountsDataSizeLimit)?
⋮----
.min(MAX_LOADED_ACCOUNTS_DATA_SIZE_BYTES);
Ok(ComputeBudgetLimits {
⋮----
const LAST_BLOCKHASH: Hash = Hash::new_from_array([7; 32]); // Arbitrary constant hash for advancing nonces
pub type AccountsMap = HashMap<Pubkey, AccountSharedData>;
// container for everything needed to execute a test entry
// care should be taken if reused, because we update bank account states, but otherwise leave it as-is
// the environment is made available for tests that check it after processing
pub struct SvmTestEnvironment<'a> {
⋮----
pub fn create(test_entry: SvmTestEntry) -> Self {
⋮----
deploy_program_with_upgrade_authority(name.to_string(), *slot, &mock_bank, *authority);
⋮----
.write()
.unwrap()
.insert(*pubkey, account.clone());
⋮----
Some(Arc::new(create_custom_loader())),
⋮----
mock_bank.configure_sysvars();
batch_processor.fill_missing_sysvar_cache_entries(&mock_bank);
register_builtins(&mock_bank, &batch_processor, test_entry.with_loader_v4);
⋮----
.get_environments_for_epoch(EXECUTION_EPOCH),
⋮----
pub fn execute(&self) -> LoadAndExecuteSanitizedTransactionsOutput {
let (transactions, check_results) = self.test_entry.prepare_transactions();
⋮----
.load_and_execute_sanitized_transactions(
⋮----
let mut final_accounts_actual = self.test_entry.initial_accounts.clone();
⋮----
if account.lamports() == 0 {
final_accounts.insert(pubkey, AccountSharedData::default());
⋮----
final_accounts.insert(pubkey, account);
⋮----
for (tx_index, processed_transaction) in batch_output.processing_results.iter().enumerate()
⋮----
.iter()
.enumerate()
⋮----
if sanitized_transaction.is_writable(index) {
update_or_dealloc_account(
⋮----
account_data.clone(),
⋮----
.zip(self.test_entry.asserts())
.map(|(processing_result, test_item_assert)| {
⋮----
.unzip();
assert_eq!(
⋮----
for (pubkey, expected_account_data) in self.test_entry.final_accounts.iter() {
let actual_account_data = final_accounts_actual.get(pubkey);
⋮----
.check_executed_transaction(&executed_transaction.execution_details),
⋮----
assert!(test_item_asserts.processed());
assert!(!test_item_asserts.executed());
⋮----
Err(_) => assert!(test_item_asserts.discarded()),
⋮----
let mut mock_bank_accounts = self.mock_bank.account_shared_data.write().unwrap();
mock_bank_accounts.extend(final_accounts_actual);
for processing_result in batch_output.processing_results.iter() {
⋮----
processing_result.processed_transaction()
⋮----
if executed_tx.was_successful() && !programs_modified_by_tx.is_empty() {
⋮----
.merge(&self.batch_processor.environments, programs_modified_by_tx);
⋮----
pub fn is_program_blocked(&self, program_id: &Pubkey) -> bool {
⋮----
.read()
⋮----
.get_flattened_entries_for_tests()
.into_iter()
.rev()
.find(|(key, _)| key == program_id)
.unwrap();
program_cache_entry.effective_slot > EXECUTION_SLOT || program_cache_entry.is_tombstone()
⋮----
pub struct SvmTestEntry {
⋮----
impl Default for SvmTestEntry {
fn default() -> Self {
⋮----
impl SvmTestEntry {
pub fn with_loader_v4() -> Self {
⋮----
pub fn add_initial_account(&mut self, pubkey: Pubkey, account: &AccountSharedData) {
assert!(self
⋮----
self.create_expected_account(pubkey, account);
⋮----
pub fn add_initial_program(&mut self, program_name: &str) {
⋮----
.push((program_name.to_string(), DEPLOYMENT_SLOT, None));
⋮----
pub fn create_expected_account(&mut self, pubkey: Pubkey, account: &AccountSharedData) {
let mut account = account.clone();
account.set_rent_epoch(u64::MAX);
assert!(self.final_accounts.insert(pubkey, account).is_none());
⋮----
pub fn update_expected_account_data(&mut self, pubkey: Pubkey, account: &AccountSharedData) {
⋮----
assert!(self.final_accounts.insert(pubkey, account).is_some());
⋮----
pub fn drop_expected_account(&mut self, pubkey: Pubkey) {
⋮----
pub fn increase_expected_lamports(&mut self, pubkey: &Pubkey, lamports: u64) {
⋮----
.get_mut(pubkey)
⋮----
.checked_add_lamports(lamports)
⋮----
pub fn decrease_expected_lamports(&mut self, pubkey: &Pubkey, lamports: u64) {
⋮----
.checked_sub_lamports(lamports)
⋮----
pub fn push_transaction(&mut self, transaction: Transaction) {
self.push_transaction_with_status(transaction, ExecutionStatus::Succeeded)
⋮----
pub fn push_transaction_with_status(
⋮----
self.transaction_batch.push(TransactionBatchItem {
⋮----
pub fn push_nonce_transaction(&mut self, transaction: Transaction, nonce_info: NonceInfo) {
self.push_nonce_transaction_with_status(transaction, nonce_info, ExecutionStatus::Succeeded)
⋮----
pub fn push_nonce_transaction_with_status(
⋮----
.try_advance_nonce(
⋮----
fn prepare_transactions(&self) -> (Vec<SanitizedTransaction>, Vec<TransactionCheckResult>) {
⋮----
.cloned()
.map(|item| {
⋮----
let check_result = item.check_result.map(|tx_details| {
let compute_budget_limits = process_test_compute_budget_instructions(
⋮----
.num_transaction_signatures()
.saturating_add(message.num_ed25519_signatures())
.saturating_add(message.num_secp256k1_signatures())
.saturating_add(message.num_secp256r1_signatures());
⋮----
.map(|v| {
v.get_compute_budget_and_limits(
⋮----
signature_count.saturating_mul(LAMPORTS_PER_SIGNATURE),
v.get_prioritization_fee(),
⋮----
.unzip()
⋮----
fn asserts(&self) -> Vec<TransactionBatchItemAsserts> {
⋮----
.map(|item| item.asserts)
.collect()
⋮----
pub struct TransactionBatchItem {
⋮----
impl TransactionBatchItem {
fn with_nonce(nonce_info: NonceInfo) -> Self {
⋮----
check_result: Ok(CheckedTransactionDetails::new(
Some(nonce_info),
⋮----
impl Default for TransactionBatchItem {
⋮----
pub struct TransactionBatchItemAsserts {
⋮----
impl TransactionBatchItemAsserts {
pub fn succeeded(&self) -> bool {
self.status.succeeded()
⋮----
pub fn executed(&self) -> bool {
self.status.executed()
⋮----
pub fn processed(&self) -> bool {
self.status.processed()
⋮----
pub fn discarded(&self) -> bool {
self.status.discarded()
⋮----
pub fn check_executed_transaction(&self, execution_details: &TransactionExecutionDetails) {
assert!(self.executed());
assert_eq!(self.succeeded(), execution_details.status.is_ok());
if !self.logs.is_empty() {
let actual_logs = execution_details.log_messages.as_ref().unwrap();
⋮----
assert!(actual_logs.contains(expected_log));
⋮----
fn from(status: ExecutionStatus) -> Self {
⋮----
pub enum ExecutionStatus {
⋮----
impl ExecutionStatus {
pub fn succeeded(self) -> bool {
⋮----
pub fn executed(self) -> bool {
⋮----
pub fn processed(self) -> bool {
⋮----
pub fn discarded(self) -> bool {
⋮----
fn from(processing_result: &TransactionProcessingResult) -> Self {
⋮----
if executed_transaction.execution_details.status.is_ok() {
⋮----
pub enum ReturnDataAssert {
⋮----
fn from(option_ro_data: Option<TransactionReturnData>) -> Self {
⋮----
fn program_medley(drop_on_failure: bool) -> Vec<SvmTestEntry> {
⋮----
let program_id = program_address(program_name);
test_entry.add_initial_program(program_name);
⋮----
let fee_payer = fee_payer_keypair.pubkey();
⋮----
fee_payer_data.set_lamports(LAMPORTS_PER_SOL);
test_entry.add_initial_account(fee_payer, &fee_payer_data);
let instruction = Instruction::new_with_bytes(program_id, &[], vec![]);
test_entry.push_transaction(Transaction::new_signed_with_payer(
⋮----
Some(&fee_payer),
⋮----
.push("Program log: Hello, Solana!".to_string());
test_entry.decrease_expected_lamports(&fee_payer, LAMPORTS_PER_SIGNATURE);
⋮----
let sender = sender_keypair.pubkey();
⋮----
sender_data.set_lamports(LAMPORTS_PER_SOL);
test_entry.add_initial_account(sender, &sender_data);
⋮----
recipient_data.set_lamports(LAMPORTS_PER_SOL);
test_entry.add_initial_account(recipient, &recipient_data);
⋮----
vec![
⋮----
test_entry.increase_expected_lamports(&recipient, transfer_amount);
test_entry.decrease_expected_lamports(&sender, transfer_amount);
test_entry.decrease_expected_lamports(&fee_payer, LAMPORTS_PER_SIGNATURE * 2);
⋮----
data: i64::to_be_bytes(WALLCLOCK_TIME).to_vec(),
⋮----
let program_id = program_address("simple-transfer");
⋮----
test_entry.final_accounts.insert(fee_payer, fee_payer_data);
⋮----
sender_data.set_lamports(base_amount);
⋮----
test_entry.final_accounts.insert(sender, sender_data);
⋮----
recipient_data.set_lamports(base_amount);
⋮----
test_entry.final_accounts.insert(recipient, recipient_data);
⋮----
test_entry.push_transaction_with_status(
⋮----
.push("Transfer: insufficient lamports 900000, need 900050".to_string());
⋮----
test_entry.transaction_batch.push(TransactionBatchItem {
⋮----
check_result: Err(TransactionError::BlockhashNotFound),
asserts: ExecutionStatus::Discarded.into(),
⋮----
vec![test_entry]
⋮----
fn simple_transfer(drop_on_failure: bool) -> Vec<SvmTestEntry> {
⋮----
let source = source_keypair.pubkey();
⋮----
source_data.set_lamports(LAMPORTS_PER_SOL * 10);
test_entry.add_initial_account(source, &source_data);
test_entry.push_transaction(system_transaction::transfer(
⋮----
.checked_add_lamports(transfer_amount)
⋮----
test_entry.create_expected_account(destination, &destination_data);
test_entry.decrease_expected_lamports(&source, transfer_amount + LAMPORTS_PER_SIGNATURE);
⋮----
source_data.set_lamports(transfer_amount - 1);
⋮----
test_entry.final_accounts.insert(source, source_data);
⋮----
drop_on_failure_status(ExecutionStatus::ExecutedFailed),
⋮----
test_entry.decrease_expected_lamports(&source, LAMPORTS_PER_SIGNATURE);
⋮----
source_data.set_lamports(transfer_amount * 10);
⋮----
.insert(source, source_data.clone());
⋮----
Some(&source),
⋮----
drop_on_failure_status(ExecutionStatus::ProcessedFailed),
⋮----
fn simple_nonce(fee_paying_nonce: bool) -> Vec<SvmTestEntry> {
⋮----
let real_program_id = program_address(program_name);
⋮----
let mut nonce_balance = Rent::default().minimum_balance(nonce_size);
⋮----
assert!(fee_paying_nonce);
⋮----
&nonce::versions::Versions::new(nonce::state::State::Initialized(nonce_data.clone())),
⋮----
let nonce_info = NonceInfo::new(nonce_pubkey, nonce_account.clone());
⋮----
test_entry.add_initial_account(nonce_pubkey, &nonce_account);
⋮----
let instructions = vec![
⋮----
nonce_data.blockhash(),
⋮----
mk_nonce_transaction(&mut test_entry, real_program_id, false, false);
test_entry.push_nonce_transaction(transaction, nonce_info.clone());
⋮----
.get_mut(nonce_info.address())
⋮----
.data_as_mut_slice()
.copy_from_slice(nonce_info.account().data());
⋮----
mk_nonce_transaction(&mut test_entry, real_program_id, true, false);
⋮----
.entry(*nonce_info.address())
.and_modify(|account| account.set_rent_epoch(0));
test_entry.push_nonce_transaction_with_status(
⋮----
mk_nonce_transaction(&mut test_entry, system_program::id(), false, false);
⋮----
nonce_info.clone(),
⋮----
mk_nonce_transaction(&mut test_entry, Pubkey::new_unique(), false, false);
⋮----
.set_rent_epoch(0);
⋮----
mk_nonce_transaction(&mut test_entry, real_program_id, false, true);
⋮----
mk_nonce_transaction(&mut test_entry, Pubkey::new_unique(), false, true);
⋮----
fn simd83_intrabatch_account_reuse() -> Vec<SvmTestEntry> {
let mut test_entries = vec![];
⋮----
let wallet_rent = Rent::default().minimum_balance(0);
⋮----
.decrease_expected_lamports(&source, transfer_amount + LAMPORTS_PER_SIGNATURE);
⋮----
test_entries.push(test_entry);
⋮----
source_data.set_lamports(transfer_amount + LAMPORTS_PER_SIGNATURE + wallet_rent);
⋮----
let grandparent = grandparent_keypair.pubkey();
⋮----
let parent = parent_keypair.pubkey();
⋮----
grandparent_data.set_lamports(LAMPORTS_PER_SOL * 10);
test_entry.add_initial_account(grandparent, &grandparent_data);
⋮----
.checked_add_lamports(first_transfer_amount)
⋮----
test_entry.create_expected_account(parent, &parent_data);
test_entry.decrease_expected_lamports(
⋮----
.checked_add_lamports(second_transfer_amount)
⋮----
test_entry.create_expected_account(child, &child_data);
⋮----
.decrease_expected_lamports(&parent, second_transfer_amount + LAMPORTS_PER_SIGNATURE);
⋮----
let feepayer = feepayer_keypair.pubkey();
⋮----
let separate_source = separate_source_keypair.pubkey();
⋮----
feepayer_data.set_lamports(1 + LAMPORTS_PER_SIGNATURE + wallet_rent);
test_entry.add_initial_account(feepayer, &feepayer_data);
separate_source_data.set_lamports(LAMPORTS_PER_SOL * 10);
test_entry.add_initial_account(separate_source, &separate_source_data);
⋮----
Some(&feepayer),
⋮----
destination_data.checked_add_lamports(1).unwrap();
⋮----
test_entry.decrease_expected_lamports(&feepayer, 1 + LAMPORTS_PER_SIGNATURE);
⋮----
.decrease_expected_lamports(&source, transfer_amount + LAMPORTS_PER_SIGNATURE * 2);
⋮----
fn simd83_nonce_reuse(fee_paying_nonce: bool) -> Vec<SvmTestEntry> {
⋮----
non_fee_nonce_keypair.pubkey()
⋮----
initial_nonce_data.clone(),
⋮----
let initial_nonce_info = NonceInfo::new(nonce_pubkey, initial_nonce_account.clone());
⋮----
let mut advanced_nonce_info = initial_nonce_info.clone();
⋮----
.try_advance_nonce(advanced_durable, LAMPORTS_PER_SIGNATURE)
⋮----
let successful_noop_instruction = Instruction::new_with_bytes(program_id, &[], vec![]);
let failing_noop_instruction = Instruction::new_with_bytes(system_program::id(), &[], vec![]);
let fee_only_noop_instruction = Instruction::new_with_bytes(Pubkey::new_unique(), &[], vec![]);
⋮----
advance_instruction.clone(),
successful_noop_instruction.clone(),
⋮----
*advanced_durable.as_hash(),
⋮----
common_test_entry.add_initial_account(nonce_pubkey, &initial_nonce_account);
⋮----
common_test_entry.add_initial_account(fee_payer, &fee_payer_data);
⋮----
.get_mut(&nonce_pubkey)
⋮----
.copy_from_slice(advanced_nonce_info.account().data());
common_test_entry.decrease_expected_lamports(&fee_payer, LAMPORTS_PER_SIGNATURE);
⋮----
let mut test_entry = common_test_entry.clone();
⋮----
&[advance_instruction.clone(), advance_instruction.clone()],
⋮----
*initial_durable.as_hash(),
⋮----
initial_nonce_info.clone(),
⋮----
test_entry.push_nonce_transaction(first_transaction, initial_nonce_info.clone());
⋮----
second_transaction.clone(),
advanced_nonce_info.clone(),
⋮----
&[advance_instruction.clone(), failing_noop_instruction],
⋮----
&[advance_instruction.clone(), fee_only_noop_instruction],
⋮----
test_entry.push_transaction(first_transaction);
⋮----
test_entry.increase_expected_lamports(&fee_payer, LAMPORTS_PER_SOL);
test_entry.drop_expected_account(nonce_pubkey);
⋮----
test_entry.push_transaction(middle_transaction);
⋮----
new_nonce_state.set_lamports(LAMPORTS_PER_SOL);
test_entry.update_expected_account_data(nonce_pubkey, &new_nonce_state);
⋮----
vec![0; nonce_size],
⋮----
test_entry.initial_accounts.remove(&nonce_pubkey);
test_entry.final_accounts.remove(&nonce_pubkey);
let mut fake_nonce_account = initial_nonce_account.clone();
fake_nonce_account.set_rent_epoch(u64::MAX);
fake_nonce_account.set_owner(Pubkey::new_unique());
test_entry.add_initial_account(nonce_pubkey, &fake_nonce_account);
⋮----
test_entry.update_expected_account_data(nonce_pubkey, &final_nonce_account);
⋮----
let new_authority = new_authority_keypair.pubkey();
⋮----
test_entry.push_nonce_transaction(second_transaction.clone(), advanced_nonce_info.clone());
⋮----
enum WriteProgramInstruction {
⋮----
impl WriteProgramInstruction {
fn create_transaction(
⋮----
Self::Print => (vec![0], vec![AccountMeta::new_readonly(target, false)]),
Self::Set => (vec![1], vec![AccountMeta::new(target, false)]),
⋮----
vec![2],
⋮----
let mut instruction_data = vec![3];
instruction_data.extend_from_slice(&new_size.to_le_bytes());
(instruction_data, vec![AccountMeta::new(target, false)])
⋮----
let mut instructions = vec![];
⋮----
instructions.push(ComputeBudgetInstruction::set_loaded_accounts_data_size_limit(size));
⋮----
instructions.push(Instruction::new_with_bytes(
⋮----
Some(&fee_payer.pubkey()),
⋮----
fn simd83_account_deallocate() -> Vec<SvmTestEntry> {
⋮----
Rent::default().minimum_balance(1),
vec![0],
⋮----
test_entry.add_initial_account(target, &target_data);
let set_data_transaction = WriteProgramInstruction::Set.create_transaction(
⋮----
test_entry.push_transaction(set_data_transaction);
target_data.data_as_mut_slice()[0] = 100;
⋮----
test_entry.update_expected_account_data(target, &target_data);
⋮----
let dealloc_transaction = WriteProgramInstruction::Dealloc.create_transaction(
⋮----
test_entry.push_transaction(dealloc_transaction);
let print_transaction = WriteProgramInstruction::Print.create_transaction(
⋮----
test_entry.push_transaction(print_transaction);
⋮----
.push("Program log: account size 0".to_string());
⋮----
test_entry.drop_expected_account(target);
⋮----
fn simd83_fee_payer_deallocate() -> Vec<SvmTestEntry> {
⋮----
let dealloc_fee_payer = dealloc_fee_payer_keypair.pubkey();
⋮----
dealloc_fee_payer_data.set_lamports(LAMPORTS_PER_SIGNATURE);
dealloc_fee_payer_data.set_rent_epoch(u64::MAX - 1);
test_entry.add_initial_account(dealloc_fee_payer, &dealloc_fee_payer_data);
⋮----
let stable_fee_payer = stable_fee_payer_keypair.pubkey();
⋮----
stable_fee_payer_data.set_lamports(LAMPORTS_PER_SOL);
test_entry.add_initial_account(stable_fee_payer, &stable_fee_payer_data);
⋮----
vec![],
⋮----
Some(&dealloc_fee_payer),
⋮----
test_entry.decrease_expected_lamports(&dealloc_fee_payer, LAMPORTS_PER_SIGNATURE);
⋮----
vec![AccountMeta::new_readonly(dealloc_fee_payer, false)],
⋮----
Some(&stable_fee_payer),
⋮----
test_entry.decrease_expected_lamports(&stable_fee_payer, LAMPORTS_PER_SIGNATURE);
test_entry.drop_expected_account(dealloc_fee_payer);
⋮----
Instruction::new_with_bytes(Pubkey::new_unique(), &[], vec![]);
⋮----
test_entry.push_transaction_with_status(transaction, ExecutionStatus::ProcessedFailed);
⋮----
fn simd83_account_reallocate(formalize_loaded_transaction_data_size: bool) -> Vec<SvmTestEntry> {
⋮----
let program_size = program_data_size(program_name);
⋮----
common_test_entry.add_initial_program(program_name);
⋮----
vec![0; size],
⋮----
common_test_entry.add_initial_account(target, &mk_target(target_start_size));
let size_budget = Some(if formalize_loaded_transaction_data_size {
⋮----
common_test_entry.decrease_expected_lamports(&fee_payer, LAMPORTS_PER_SIGNATURE * 2);
⋮----
.create_transaction(program_id, &fee_payer_keypair, target, None);
test_entry.push_transaction(realloc_transaction);
test_entry.push_transaction(print_transaction.clone());
⋮----
.push(format!("Program log: account size {new_target_size}"));
test_entry.update_expected_account_data(target, &mk_target(new_target_size));
⋮----
print_transaction.clone(),
⋮----
enum AbortReason {
⋮----
fn all_or_nothing(abort: AbortReason) -> Vec<SvmTestEntry> {
⋮----
drop_on_failure: matches!(abort, AbortReason::DropOnFailure),
⋮----
if matches!(abort, AbortReason::DropOnFailure) {
⋮----
if matches!(abort, AbortReason::Unprocessable) {
⋮----
fn drop_on_failure_batch(statuses: &[bool]) -> Vec<SvmTestEntry> {
⋮----
source_data.set_lamports(LAMPORTS_PER_SOL * 100);
⋮----
println!("source: {source}");
println!("destination: {destination}");
⋮----
destination_data.set_rent_epoch(u64::MAX);
⋮----
false => test_entry.push_transaction_with_status(
⋮----
source_data.lamports() + 1,
⋮----
if statuses.iter().all(|success| !*success) {
⋮----
.get_mut(&source)
⋮----
if statuses.iter().any(|success| *success) {
assert!(test_entry
⋮----
fn svm_integration(test_entries: Vec<SvmTestEntry>) {
⋮----
env.execute();
⋮----
fn program_cache_create_account() {
⋮----
fee_payer_data.set_lamports(LAMPORTS_PER_SOL * 10);
⋮----
let program_id = new_account_keypair.pubkey();
⋮----
test_entry.push_transaction(create_transaction);
⋮----
.decrease_expected_lamports(&fee_payer, LAMPORTS_PER_SOL + LAMPORTS_PER_SIGNATURE * 2);
⋮----
&[Instruction::new_with_bytes(program_id, &[], vec![])],
⋮----
invoke_transaction.clone(),
⋮----
initial_accounts: env.test_entry.final_accounts.clone(),
final_accounts: env.test_entry.final_accounts.clone(),
⋮----
.push_transaction_with_status(invoke_transaction, ExecutionStatus::ExecutedFailed);
⋮----
fn program_cache_loaderv3_update_tombstone(upgrade_program: bool, invoke_changed_program: bool) {
⋮----
.push((program_name.to_string(), DEPLOYMENT_SLOT, Some(fee_payer)));
⋮----
authority_address: Some(fee_payer),
⋮----
let mut program_bytecode = load_program(program_name.to_string());
data.append(&mut program_bytecode);
⋮----
test_entry.add_initial_account(buffer_address, &buffer_account);
test_entry.drop_expected_account(buffer_address);
⋮----
&get_program_data_address(&program_id),
⋮----
Some(&program_id),
⋮----
assert!(env.is_program_blocked(&program_id));
⋮----
test_entry.push_transaction_with_status(invoke_transaction, ExecutionStatus::ExecutedFailed);
⋮----
fn program_cache_loaderv3_buffer_swap(invoke_changed_program: bool) {
⋮----
let target = target_keypair.pubkey();
let programdata_address = get_program_data_address(&target);
⋮----
let deploy = deploy_keypair.pubkey();
⋮----
buffer_data.append(&mut program_bytecode);
⋮----
buffer_data.clone(),
⋮----
test_entry.add_initial_account(target, &buffer_account);
test_entry.add_initial_account(deploy, &buffer_account);
⋮----
test_entry.update_expected_account_data(target, &program_account);
test_entry.drop_expected_account(deploy);
⋮----
loaderv3_instruction::close_any(&target, &Pubkey::new_unique(), Some(&fee_payer), None);
⋮----
buffer_data.len(),
⋮----
Rent::default().minimum_balance(
UpgradeableLoaderState::size_of_programdata_metadata() + buffer_data.len(),
⋮----
&[Instruction::new_with_bytes(target, &[], vec![])],
⋮----
assert!(env.is_program_blocked(&target));
⋮----
fn program_cache_stats() {
⋮----
let noop_program = program_address(program_name);
⋮----
fee_payer_data.set_lamports(LAMPORTS_PER_SOL * 100);
⋮----
let succesful_noop_instruction = Instruction::new_with_bytes(noop_program, &[], vec![]);
⋮----
let fee_only_noop_instruction = Instruction::new_with_bytes(missing_program, &[], vec![]);
⋮----
test_entry.push_transaction(make_transaction(slice::from_ref(
⋮----
make_transaction(slice::from_ref(&failing_transfer_instruction)),
⋮----
test_entry.push_transaction(make_transaction(&[
succesful_noop_instruction.clone(),
⋮----
succesful_transfer_instruction.clone(),
⋮----
make_transaction(&[
failing_transfer_instruction.clone(),
⋮----
fee_only_noop_instruction.clone(),
⋮----
LAMPORTS_PER_SIGNATURE * test_entry.transaction_batch.len() as u64
⋮----
transaction: make_transaction(slice::from_ref(&succesful_transfer_instruction)),
⋮----
.find(|(pubkey, _)| *pubkey == noop_program)
⋮----
.find(|(pubkey, _)| *pubkey == system_program::id())
⋮----
assert!(
⋮----
make_transaction(slice::from_ref(&succesful_noop_instruction)),
⋮----
enum Inspect<'a> {
⋮----
fn from(inspect: Inspect) -> Self {
⋮----
Inspect::LiveRead(account) => (Some(account.clone()), false),
Inspect::LiveWrite(account) => (Some(account.clone()), true),
⋮----
struct InspectedAccounts(pub HashMap<Pubkey, Vec<(Option<AccountSharedData>, bool)>>);
impl InspectedAccounts {
fn inspect(&mut self, pubkey: Pubkey, inspect: Inspect) {
self.0.entry(pubkey).or_default().push(inspect.into())
⋮----
fn svm_inspect_nonce_load_failure(
⋮----
let dummy = dummy_keypair.pubkey();
⋮----
separate_nonce_keypair.pubkey()
⋮----
initial_nonce_account.set_rent_epoch(u64::MAX);
⋮----
vec![AccountMeta {
⋮----
test_entry.add_initial_account(nonce_pubkey, &initial_nonce_account);
⋮----
separate_fee_payer_account.set_lamports(LAMPORTS_PER_SOL);
⋮----
AccountSharedData::create(1, vec![0; 2], system_program::id(), false, u64::MAX);
test_entry.add_initial_account(dummy, &dummy_account);
expected_inspected_accounts.inspect(nonce_pubkey, Inspect::LiveWrite(&initial_nonce_account));
⋮----
.inspect(nonce_pubkey, Inspect::LiveWrite(&initial_nonce_account));
⋮----
test_entry.add_initial_account(fee_payer, &separate_fee_payer_account);
⋮----
.inspect(fee_payer, Inspect::LiveWrite(&separate_fee_payer_account));
⋮----
expected_inspected_accounts.inspect(dummy, Inspect::LiveWrite(&dummy_account));
⋮----
let sanitized = SanitizedTransaction::from_transaction_for_tests(transaction.clone());
⋮----
.account_keys()
⋮----
.position(|key| *key == dummy)
⋮----
.position(|key| *key == nonce_pubkey)
⋮----
assert!(dummy_index < nonce_index);
⋮----
let env = SvmTestEnvironment::create(test_entry.clone());
⋮----
let actual_inspected_accounts = env.mock_bank.inspected_accounts.read().unwrap().clone();
⋮----
let actual_account = actual_inspected_accounts.get(expected_pubkey).unwrap();
⋮----
fn svm_inspect_account() {
⋮----
fee_payer_account.set_lamports(85_000);
fee_payer_account.set_rent_epoch(u64::MAX);
initial_test_entry.add_initial_account(fee_payer, &fee_payer_account);
expected_inspected_accounts.inspect(fee_payer, Inspect::LiveWrite(&fee_payer_account));
⋮----
sender_account.set_lamports(11_000_000);
sender_account.set_rent_epoch(u64::MAX);
initial_test_entry.add_initial_account(sender, &sender_account);
expected_inspected_accounts.inspect(sender, Inspect::LiveWrite(&sender_account));
expected_inspected_accounts.inspect(recipient, Inspect::DeadWrite);
⋮----
"system_program".as_bytes().to_vec(),
⋮----
expected_inspected_accounts.inspect(system_program::id(), Inspect::LiveRead(&system_account));
⋮----
initial_test_entry.push_transaction(transaction);
⋮----
recipient_account.set_lamports(transfer_amount);
initial_test_entry.decrease_expected_lamports(&fee_payer, LAMPORTS_PER_SIGNATURE * 2);
initial_test_entry.decrease_expected_lamports(&sender, transfer_amount);
initial_test_entry.create_expected_account(recipient, &recipient_account);
⋮----
let mut env = SvmTestEnvironment::create(initial_test_entry.clone());
⋮----
.get(&fee_payer)
⋮----
expected_inspected_accounts.inspect(
⋮----
.get(&sender)
⋮----
expected_inspected_accounts.inspect(sender, Inspect::LiveWrite(&intermediate_sender_account));
⋮----
.get(&recipient)
⋮----
initial_accounts: initial_test_entry.final_accounts.clone(),
final_accounts: initial_test_entry.final_accounts.clone(),
⋮----
final_test_entry.push_transaction(transaction);
final_test_entry.decrease_expected_lamports(&fee_payer, LAMPORTS_PER_SIGNATURE * 2);
final_test_entry.decrease_expected_lamports(&sender, transfer_amount);
final_test_entry.increase_expected_lamports(&recipient, transfer_amount);
⋮----
expected_inspected_accounts.0.values().map(Vec::len).sum();
⋮----
actual_inspected_accounts.values().map(Vec::len).sum();
⋮----
fn svm_metrics_accumulation() {
for test_entry in program_medley(false) {
⋮----
let (transactions, check_results) = env.test_entry.prepare_transactions();
let result = env.batch_processor.load_and_execute_sanitized_transactions(
⋮----
assert_ne!(
⋮----
mod balance_collector {
⋮----
struct Transfer {
⋮----
impl Transfer {
fn new_rand(users: &[Pubkey]) -> Self {
⋮----
let [from_idx, to_idx] = (0..users.len()).choose_multiple(&mut rng, 2)[..] else {
unreachable!()
⋮----
let amount = rng.gen_range(1, STARTING_BALANCE / 100);
⋮----
fn to_system_instruction(&self) -> Instruction {
⋮----
fn to_token_instruction(&self, fee_payer: &Pubkey) -> Instruction {
⋮----
fn to_instruction(&self, fee_payer: &Pubkey, use_tokens: bool) -> Instruction {
⋮----
self.to_token_instruction(fee_payer)
⋮----
self.to_system_instruction()
⋮----
fn svm_collect_balances(use_tokens: bool) {
⋮----
let fake_fee_payer = fake_fee_payer_keypair.pubkey();
⋮----
let alice = alice_keypair.pubkey();
let bob = bob_keypair.pubkey();
let charlie = charlie_keypair.pubkey();
⋮----
let mut mint_buf = vec![0; Mint::get_packed_len()];
⋮----
.pack_into_slice(&mut mint_buf);
⋮----
let mut token_buf = vec![0; TokenAccount::get_packed_len()];
token_account_for_tests().pack_into_slice(&mut token_buf);
⋮----
.swap_remove(0);
⋮----
test_entry.add_initial_account(fee_payer, &native_state.clone());
⋮----
test_entry.add_initial_account(spl_token_interface::id(), &spl_token);
test_entry.add_initial_account(mint, &mint_state);
test_entry.add_initial_account(alice, &token_state);
test_entry.add_initial_account(bob, &token_state);
test_entry.add_initial_account(charlie, &token_state);
⋮----
test_entry.add_initial_account(alice, &native_state);
test_entry.add_initial_account(bob, &native_state);
test_entry.add_initial_account(charlie, &native_state);
⋮----
let mut transaction_discards = vec![];
⋮----
user_balances.insert(alice, STARTING_BALANCE);
user_balances.insert(bob, STARTING_BALANCE);
user_balances.insert(charlie, STARTING_BALANCE);
let mut user_balance_history = vec![(Transfer::default(), user_balances.clone())];
⋮----
transaction_discards.push(expected_status == ExecutionStatus::Discarded);
⋮----
let from_signer = vec![&alice_keypair, &bob_keypair, &charlie_keypair]
⋮----
.find(|k| k.pubkey() == transfer.from)
⋮----
.entry(transfer.from)
.and_modify(|v| *v -= transfer.amount);
⋮----
.entry(transfer.to)
.and_modify(|v| *v += transfer.amount);
vec![transfer.to_instruction(&fee_payer, use_tokens)]
⋮----
let instruction = transfer.to_instruction(&fee_payer, use_tokens);
⋮----
vec![instruction]
⋮----
let mut instruction = transfer.to_instruction(&fee_payer, use_tokens);
⋮----
let transaction = if expected_status.discarded() {
⋮----
Some(&fake_fee_payer),
⋮----
test_entry.push_transaction_with_status(transaction, expected_status);
user_balance_history.push((transfer, user_balances.clone()));
⋮----
let mut token_account = token_account_for_tests();
⋮----
token_account.amount = *user_balances.get(&alice).unwrap();
token_account.pack_into_slice(&mut token_buf);
⋮----
token_buf.clone(),
⋮----
test_entry.update_expected_account_data(alice, &final_token_state);
token_account.amount = *user_balances.get(&bob).unwrap();
⋮----
test_entry.update_expected_account_data(bob, &final_token_state);
token_account.amount = *user_balances.get(&charlie).unwrap();
⋮----
test_entry.update_expected_account_data(charlie, &final_token_state);
⋮----
let mut alice_final_state = native_state.clone();
alice_final_state.set_lamports(*user_balances.get(&alice).unwrap());
test_entry.update_expected_account_data(alice, &alice_final_state);
let mut bob_final_state = native_state.clone();
bob_final_state.set_lamports(*user_balances.get(&bob).unwrap());
test_entry.update_expected_account_data(bob, &bob_final_state);
let mut charlie_final_state = native_state.clone();
charlie_final_state.set_lamports(*user_balances.get(&charlie).unwrap());
test_entry.update_expected_account_data(charlie, &charlie_final_state);
⋮----
let batch_output = env.execute();
⋮----
batch_output.balance_collector.unwrap().into_vecs();
⋮----
.zip(post_lamport_vecs.clone())
.zip(transaction_discards)
.map(|((pres, posts), discard)| (pres[0], posts[0], discard))
⋮----
assert_eq!(pre_bal, 0);
assert_eq!(post_bal, 0);
⋮----
assert_eq!(pre_bal, running_fee_payer_balance);
assert_eq!(post_bal, expected_post_balance);
⋮----
.map(|bals| (bals[0].amount, bals[1].amount))
.collect();
⋮----
.map(|bals| (bals[1], bals[2]))
⋮----
assert_eq!(user_balance_history.len(), batch_pre.len() + 1);
assert_eq!(user_balance_history.len(), batch_post.len() + 1);
⋮----
batch_pre.into_iter().zip(batch_post).enumerate()

================
File: svm/tests/mock_bank.rs
================
pub struct MockForkGraph {}
impl ForkGraph for MockForkGraph {
fn relationship(&self, a: Slot, b: Slot) -> BlockRelation {
match a.cmp(&b) {
⋮----
pub struct MockBankCallback {
⋮----
impl InvokeContextCallback for MockBankCallback {}
impl TransactionProcessingCallback for MockBankCallback {
fn get_account_shared_data(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, Slot)> {
⋮----
.read()
.unwrap()
.get(pubkey)
.map(|account| (account.clone(), 0))
⋮----
fn inspect_account(&self, address: &Pubkey, account_state: AccountState, is_writable: bool) {
⋮----
AccountState::Alive(account) => Some(account.clone()),
⋮----
.write()
⋮----
.entry(*address)
.or_default()
.push((account, is_writable));
⋮----
impl MockBankCallback {
pub fn calculate_fee_details(message: &impl SVMMessage, prioritization_fee: u64) -> FeeDetails {
⋮----
.num_transaction_signatures()
.saturating_add(message.num_ed25519_signatures())
.saturating_add(message.num_secp256k1_signatures())
.saturating_add(message.num_secp256r1_signatures());
⋮----
signature_count.saturating_mul(FeeStructure::default().lamports_per_signature),
⋮----
pub fn add_builtin(
⋮----
data: name.as_bytes().to_vec(),
⋮----
.insert(program_id, account_data);
batch_processor.add_builtin(program_id, builtin);
⋮----
pub fn override_feature_set(&mut self, new_set: SVMFeatureSet) {
⋮----
pub fn configure_sysvars(&self) {
⋮----
epoch_start_timestamp: WALLCLOCK_TIME.saturating_sub(10) as UnixTimestamp,
⋮----
account_data.set_data(bincode::serialize(&clock).unwrap());
⋮----
.insert(Clock::id(), account_data);
⋮----
account_data.set_data(bincode::serialize(&rent).unwrap());
⋮----
.insert(Rent::id(), account_data);
⋮----
let recent_blockhashes = vec![BlockhashesEntry::default()];
⋮----
account_data.set_data(bincode::serialize(&recent_blockhashes).unwrap());
⋮----
.insert(RecentBlockhashes::id(), account_data);
⋮----
account_data.set_data(bincode::serialize(&epoch_schedule).unwrap());
⋮----
.insert(EpochSchedule::id(), account_data);
⋮----
pub fn load_program(name: String) -> Vec<u8> {
let mut dir = env::current_dir().unwrap();
dir.push("tests");
dir.push("example-programs");
dir.push(name.as_str());
let name = name.replace('-', "_");
dir.push(name + "_program.so");
let mut file = File::open(dir.clone()).expect("file not found");
let metadata = fs::metadata(dir).expect("Unable to read metadata");
let mut buffer = vec![0; metadata.len() as usize];
file.read_exact(&mut buffer).expect("Buffer overflow");
⋮----
pub fn program_address(program_name: &str) -> Pubkey {
Pubkey::create_with_seed(&Pubkey::default(), program_name, &Pubkey::default()).unwrap()
⋮----
pub fn program_data_size(program_name: &str) -> usize {
load_program(program_name.to_string()).len()
⋮----
pub fn deploy_program(name: String, deployment_slot: Slot, mock_bank: &MockBankCallback) -> Pubkey {
deploy_program_with_upgrade_authority(name, deployment_slot, mock_bank, None)
⋮----
pub fn deploy_program_with_upgrade_authority(
⋮----
let program_account = program_address(&name);
⋮----
let buffer = bincode::serialize(&state).unwrap();
account_data.set_lamports(rent.minimum_balance(buffer.len()));
account_data.set_owner(solana_sdk_ids::bpf_loader_upgradeable::id());
account_data.set_executable(true);
account_data.set_data(buffer);
⋮----
.insert(program_account, account_data);
⋮----
let mut header = bincode::serialize(&state).unwrap();
let mut complement = vec![
⋮----
let mut buffer = load_program(name);
header.append(&mut complement);
header.append(&mut buffer);
account_data.set_lamports(rent.minimum_balance(header.len()));
⋮----
account_data.set_data(header);
⋮----
.insert(program_data_account, account_data);
⋮----
pub fn register_builtins(
⋮----
mock_bank.add_builtin(
⋮----
loader_v3_name.len(),
⋮----
loader_v1_name.len(),
⋮----
loader_v2_name.len(),
⋮----
loader_v4_name.len(),
⋮----
system_program_name.len(),
⋮----
compute_budget_program_name.len(),
⋮----
pub fn create_custom_loader<'a>() -> BuiltinProgram<InvokeContext<'a, 'a>> {
⋮----
.register_function("abort", SyscallAbort::vm)
.expect("Registration failed");
⋮----
.register_function("sol_log_", SyscallLog::vm)
⋮----
.register_function("sol_memcpy_", SyscallMemcpy::vm)
⋮----
.register_function("sol_memset_", SyscallMemset::vm)
⋮----
.register_function("sol_memcmp_", SyscallMemcmp::vm)
⋮----
.register_function("sol_memmove_", SyscallMemmove::vm)
⋮----
.register_function("sol_invoke_signed_rust", SyscallInvokeSignedRust::vm)
⋮----
.register_function("sol_set_return_data", SyscallSetReturnData::vm)
⋮----
.register_function("sol_get_clock_sysvar", SyscallGetClockSysvar::vm)
⋮----
.register_function("sol_get_rent_sysvar", SyscallGetRentSysvar::vm)
⋮----
.register_function(

================
File: svm/Cargo.toml
================
[package]
name = "solana-svm"
description = "Solana SVM"
documentation = "https://docs.rs/solana-svm"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_svm"

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "dep:qualifier_attr",
    "solana-program-runtime/dev-context-only-utils",
]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-program-runtime/frozen-abi",
]
shuttle-test = [
    "solana-bpf-loader-program/shuttle-test",
    "solana-loader-v4-program/shuttle-test",
    "solana-program-runtime/shuttle-test",
    "solana-svm-type-overrides/shuttle-test",
]
svm-internal = []

[dependencies]
ahash = { workspace = true }
log = { workspace = true }
percentage = { workspace = true }
qualifier_attr = { workspace = true, optional = true }
serde = { workspace = true, features = ["rc"] }
solana-account = { workspace = true }
solana-clock = { workspace = true }
solana-fee-structure = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-hash = { workspace = true }
solana-instruction = { workspace = true, features = ["std"] }
solana-instructions-sysvar = { workspace = true }
solana-loader-v3-interface = { workspace = true, features = ["bincode"] }
solana-loader-v4-interface = { workspace = true }
solana-loader-v4-program = { workspace = true }
solana-message = { workspace = true }
solana-nonce = { workspace = true }
solana-nonce-account = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-pack = { workspace = true }
solana-program-runtime = { workspace = true, features = ["metrics"] }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-svm-callback = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-measure = { workspace = true }
solana-svm-timings = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-system-interface = { workspace = true }
solana-sysvar-id = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
spl-generic-token = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
agave-syscalls = { workspace = true }
assert_matches = { workspace = true }
bincode = { workspace = true }
ed25519-dalek = { workspace = true }
libsecp256k1 = { workspace = true }
openssl = { workspace = true }
rand0-7 = { workspace = true }
shuttle = { workspace = true }
solana-bpf-loader-program = { workspace = true }
solana-clock = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-compute-budget-program = { workspace = true }
solana-ed25519-program = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-keypair = { workspace = true }
solana-native-token = { workspace = true }
solana-precompile-error = { workspace = true }
solana-program-binaries = { workspace = true }
solana-program-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-rent = { workspace = true }
solana-sbpf = { workspace = true, features = ["jit"] }
solana-secp256k1-program = { workspace = true, features = ["bincode"] }
solana-secp256r1-program = { workspace = true, features = ["openssl-vendored"] }
solana-signature = { workspace = true, features = ["rand"] }
solana-signer = { workspace = true }
# See order-crates-for-publishing.py for using this unusual `path = "."`
solana-svm = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils", "svm-internal"] }
solana-system-program = { workspace = true }
solana-system-transaction = { workspace = true }
solana-sysvar = { workspace = true }
solana-transaction = { workspace = true, features = ["dev-context-only-utils"] }
solana-transaction-context = { workspace = true, features = ["dev-context-only-utils"] }
spl-token-interface = { workspace = true }
test-case = { workspace = true }

[lints]
workspace = true

================
File: svm-callback/src/lib.rs
================
pub trait InvokeContextCallback {
fn get_epoch_stake(&self) -> u64 {
⋮----
fn get_epoch_stake_for_vote_account(&self, _vote_address: &Pubkey) -> u64 {
⋮----
fn is_precompile(&self, _program_id: &Pubkey) -> bool {
⋮----
fn process_precompile(
⋮----
Err(PrecompileError::InvalidPublicKey)
⋮----
pub trait TransactionProcessingCallback: InvokeContextCallback {
⋮----
fn inspect_account(&self, _address: &Pubkey, _account_state: AccountState, _is_writable: bool) {
⋮----
pub enum AccountState<'a> {
/// This account is dead, and will be created by this transaction
    Dead,
/// This account is alive, and already existed prior to this transaction
    Alive(&'a AccountSharedData),

================
File: svm-callback/Cargo.toml
================
[package]
name = "solana-svm-callback"
description = "Solana SVM callback"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
readme = false

[features]
agave-unstable-api = []

[dependencies]
solana-account = { workspace = true }
solana-clock = { workspace = true }
solana-precompile-error = { workspace = true }
solana-pubkey = { workspace = true }

[lints]
workspace = true

================
File: svm-feature-set/src/lib.rs
================
pub struct SVMFeatureSet {
⋮----
impl SVMFeatureSet {
pub fn all_enabled() -> Self {

================
File: svm-feature-set/Cargo.toml
================
[package]
name = "solana-svm-feature-set"
description = "Solana SVM Feature Set"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
readme = false

[features]
agave-unstable-api = []

[dependencies]

[lints]
workspace = true

================
File: svm-log-collector/src/lib.rs
================
pub use log;
⋮----
pub struct LogCollector {
⋮----
impl Default for LogCollector {
fn default() -> Self {
⋮----
bytes_limit: Some(LOG_MESSAGES_BYTES_LIMIT),
⋮----
impl LogCollector {
pub fn log(&mut self, message: &str) {
⋮----
self.messages.push(message.to_string());
⋮----
let bytes_written = self.bytes_written.saturating_add(message.len());
⋮----
self.messages.push(String::from("Log truncated"));
⋮----
pub fn get_recorded_content(&self) -> &[String] {
self.messages.as_slice()
⋮----
pub fn new_ref() -> Rc<RefCell<Self>> {
⋮----
pub fn new_ref_with_limit(bytes_limit: Option<usize>) -> Rc<RefCell<Self>> {
⋮----
pub fn into_messages(self) -> Vec<String> {
⋮----
macro_rules! ic_logger_msg {
⋮----
macro_rules! ic_msg {
⋮----
pub(crate) mod tests {
⋮----
fn test_log_messages_bytes_limit() {
⋮----
lc.log("x");
⋮----
let logs: Vec<_> = lc.into_messages();
assert_eq!(logs.len(), LOG_MESSAGES_BYTES_LIMIT);
for log in logs.iter().take(LOG_MESSAGES_BYTES_LIMIT - 1) {
assert_eq!(*log, "x".to_string());
⋮----
assert_eq!(logs.last(), Some(&"Log truncated".to_string()));

================
File: svm-log-collector/Cargo.toml
================
[package]
name = "solana-svm-log-collector"
description = "Solana log collector"
documentation = "https://docs.rs/solana-svm-log-collector"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
log = { workspace = true }

================
File: svm-measure/src/lib.rs
================
pub mod macros;
pub mod measure;

================
File: svm-measure/src/macros.rs
================
macro_rules! measure_time {
⋮----
macro_rules! measure_us {
⋮----
/// Measures how long it takes to execute an expression, and returns a Duration
///
⋮----
///
/// # Examples
⋮----
/// # Examples
///
⋮----
///
/// ```
⋮----
/// ```
/// # use solana_svm_measure::meas_dur;
⋮----
/// # use solana_svm_measure::meas_dur;
/// # fn meow(x: i32, y: i32) -> i32 {x + y}
⋮----
/// # fn meow(x: i32, y: i32) -> i32 {x + y}
/// let (result, duration) = meas_dur!(meow(1, 2) + 3);
⋮----
/// let (result, duration) = meas_dur!(meow(1, 2) + 3);
/// # assert_eq!(result, 1 + 2 + 3);
⋮----
/// # assert_eq!(result, 1 + 2 + 3);
/// ```
⋮----
/// ```
//
⋮----
//
// The macro name, `meas_dur`, is "measure" + "duration".
⋮----
macro_rules! meas_dur {
⋮----
mod tests {
⋮----
fn my_multiply(x: i32, y: i32) -> i32 {
⋮----
fn square(x: i32) -> i32 {
my_multiply(x, x)
⋮----
struct SomeStruct {
⋮----
impl SomeStruct {
fn add_to(&self, x: i32) -> i32 {
⋮----
fn test_measure_macro() {
⋮----
let (_result, measure) = measure_time!(sleep(Duration::from_millis(1)), "test");
assert!(measure.as_s() > 0.0);
assert!(measure.as_ms() > 0);
assert!(measure.as_us() > 0);
⋮----
let (result, _measure) = measure_time!(my_multiply(3, 4), "test");
assert_eq!(result, 3 * 4);
let (result, _measure) = measure_time!(square(5), "test");
assert_eq!(result, 5 * 5)
⋮----
let (result, _measure) = measure_time!(some_struct.add_to(4), "test");
assert_eq!(result, 42 + 4);
⋮----
let (result, _measure) = measure_time!({ 1 + 2 }, "test");
assert_eq!(result, 3);
⋮----
let (result, _measure) = measure_time!(square(5), "test",);
⋮----
let (result, _measure) = measure_time!(square(5));
⋮----
fn test_measure_us_macro() {
⋮----
let (_result, measure) = measure_us!(sleep(Duration::from_millis(1)));
assert!(measure > 0);
⋮----
let (result, _measure) = measure_us!(my_multiply(3, 4));
⋮----
let (result, _measure) = measure_us!(square(5));
⋮----
let (result, _measure) = measure_us!(some_struct.add_to(4));
⋮----
let (result, _measure) = measure_us!({ 1 + 2 });
⋮----
fn test_meas_dur_macro() {
⋮----
let (result, _duration) = meas_dur!(my_multiply(3, 4));
⋮----
let (result, _duration) = meas_dur!(square(5));
⋮----
let (result, _duration) = meas_dur!(some_struct.add_to(4));
⋮----
let (result, _duration) = meas_dur!({ 1 + 2 });

================
File: svm-measure/src/measure.rs
================
pub struct Measure {
⋮----
impl Measure {
pub fn start(name: &'static str) -> Self {
⋮----
pub fn stop(&mut self) {
self.duration = self.start.elapsed().as_nanos() as u64;
⋮----
pub fn as_ns(&self) -> u64 {
⋮----
pub fn as_us(&self) -> u64 {
⋮----
pub fn as_ms(&self) -> u64 {
⋮----
pub fn as_s(&self) -> f32 {
⋮----
pub fn as_duration(&self) -> Duration {
Duration::from_nanos(self.as_ns())
⋮----
pub fn end_as_ns(self) -> u64 {
self.start.elapsed().as_nanos() as u64
⋮----
pub fn end_as_us(self) -> u64 {
self.start.elapsed().as_micros() as u64
⋮----
pub fn end_as_ms(self) -> u64 {
self.start.elapsed().as_millis() as u64
⋮----
pub fn end_as_s(self) -> f32 {
self.start.elapsed().as_secs_f32()
⋮----
pub fn end_as_duration(self) -> Duration {
self.start.elapsed()
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
⋮----
write!(f, "{} running", self.name)
} else if self.as_us() < 1 {
write!(f, "{} took {}ns", self.name, self.duration)
} else if self.as_ms() < 1 {
write!(f, "{} took {}us", self.name, self.as_us())
} else if self.as_s() < 1. {
write!(f, "{} took {}ms", self.name, self.as_ms())
⋮----
write!(f, "{} took {:.1}s", self.name, self.as_s())
⋮----
mod tests {
⋮----
fn test_measure() {
⋮----
sleep(test_duration);
measure.stop();
assert!(measure.as_duration() >= test_duration);
⋮----
fn test_measure_as() {
⋮----
duration: test_duration.as_nanos() as u64,
⋮----
assert!(f32::abs(measure.as_s() - 0.1f32) <= f32::EPSILON);
assert_eq!(measure.as_ms(), 100);
assert_eq!(measure.as_us(), 100_000);
assert_eq!(measure.as_ns(), 100_000_000);
assert_eq!(measure.as_duration(), test_duration);
⋮----
fn test_measure_display() {
⋮----
assert_eq!(format!("{measure}"), "test_ns took 1ns");
⋮----
assert_eq!(format!("{measure}"), "test_us took 1us");
⋮----
assert_eq!(format!("{measure}"), "test_ms took 1ms");
⋮----
assert_eq!(format!("{measure}"), "test_s took 1.0s");
⋮----
assert_eq!(format!("{measure}"), "test_not_stopped running");

================
File: svm-measure/Cargo.toml
================
[package]
name = "solana-svm-measure"
description = "Timing measurement utilities for SVM"
documentation = "https://docs.rs/solana-svm-measure"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

================
File: svm-rent-calculator/src/lib.rs
================
pub mod rent_state;
pub mod svm_rent_calculator;

================
File: svm-rent-calculator/src/rent_state.rs
================
pub enum RentState {

================
File: svm-rent-calculator/src/svm_rent_calculator.rs
================
pub fn check_rent_state(
⋮----
if let Some((pre_rent_state, post_rent_state)) = pre_rent_state.zip(post_rent_state) {
⋮----
check_rent_state_with_account(
⋮----
.get_key_of_account_at_index(index)
.expect(expect_msg),
⋮----
.accounts()
.try_borrow(index)
⋮----
Ok(())
⋮----
pub fn check_rent_state_with_account(
⋮----
&& !transition_allowed(pre_rent_state, post_rent_state)
⋮----
Err(TransactionError::InsufficientFundsForRent { account_index })
⋮----
pub fn get_account_rent_state(rent: &Rent, account: &AccountSharedData) -> RentState {
if account.lamports() == 0 {
⋮----
} else if rent.is_exempt(account.lamports(), account.data().len()) {
⋮----
data_size: account.data().len(),
lamports: account.lamports(),
⋮----
pub fn transition_allowed(pre_rent_state: &RentState, post_rent_state: &RentState) -> bool {

================
File: svm-rent-calculator/Cargo.toml
================
[package]
name = "solana-svm-rent-calculator"
description = "Solana SVM Rent Calculator"
documentation = "https://docs.rs/solana-svm-rent-calculator"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
solana-account = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }

================
File: svm-test-harness/bin/test_exec_instr.rs
================
struct Cli {
⋮----
fn exec(input: &PathBuf) -> bool {
let blob = std::fs::read(input).unwrap();
let fixture = ProtoInstrFixture::decode(&blob[..]).unwrap();
⋮----
println!("No context found.");
⋮----
println!("No fixture found.");
⋮----
let Some(effects) = execute_instr_proto(context) else {
println!("FAIL: No instruction effects returned for input: {input:?}",);
⋮----
println!("OK: {input:?}");
⋮----
println!("FAIL: {input:?}");
⋮----
fn main() {
⋮----
if !exec(&input) {
fail_cnt = fail_cnt.saturating_add(1);

================
File: svm-test-harness/src/fixture/account_state.rs
================
type Error = FixtureError;
fn try_from(value: ProtoAccount) -> Result<Self, Self::Error> {
⋮----
let pubkey = Pubkey::try_from(address).map_err(FixtureError::InvalidPubkeyBytes)?;
let owner = Pubkey::try_from(owner).map_err(FixtureError::InvalidPubkeyBytes)?;
Ok((
⋮----
fn from(value: (Pubkey, Account)) -> Self {
⋮----
address: value.0.to_bytes().to_vec(),
owner: owner.to_bytes().to_vec(),

================
File: svm-test-harness/src/fixture/error.rs
================
use thiserror::Error;
⋮----
pub enum FixtureError {

================
File: svm-test-harness/src/fixture/feature_set.rs
================
const fn feature_u64(feature: &Pubkey) -> u64 {
let feature_id = feature.to_bytes();
⋮----
.keys()
.map(|pubkey| (feature_u64(pubkey), *pubkey))
.collect()
⋮----
fn from(value: &ProtoFeatureSet) -> Self {
⋮----
if let Some(pubkey) = INDEXED_FEATURES.get(id) {
feature_set.activate(pubkey, 0);

================
File: svm-test-harness/src/fixture/instr_context.rs
================
pub struct InstrContext {
⋮----
type Error = FixtureError;
fn try_from(value: ProtoInstrContext) -> Result<Self, Self::Error> {
⋮----
.try_into()
.map_err(FixtureError::InvalidPubkeyBytes)?,
⋮----
.as_ref()
.and_then(|epoch_ctx| epoch_ctx.features.as_ref())
.map(|fs| fs.into())
.unwrap_or_default();
⋮----
.into_iter()
.map(|acct_state| acct_state.try_into())
⋮----
.map(|acct| {
if acct.index as usize >= accounts.len() {
return Err(FixtureError::AccountMissingForInstrAccount(
⋮----
Ok(AccountMeta {
⋮----
if instruction_accounts.len() > 128 {
return Err(FixtureError::InvalidFixtureInput);
⋮----
accounts: instruction_accounts.into(),
data: value.data.into(),
⋮----
Ok(Self {

================
File: svm-test-harness/src/fixture/instr_effects.rs
================
pub struct InstrEffects {
⋮----
fn from(value: InstrEffects) -> Self {
⋮----
.as_ref()
.map(|error| {
let serialized_err = bincode::serialize(error).unwrap();
i32::from_le_bytes((&serialized_err[0..4]).try_into().unwrap())
.saturating_add(1)
⋮----
.unwrap_or_default(),
custom_err: custom_err.unwrap_or_default(),
modified_accounts: modified_accounts.into_iter().map(Into::into).collect(),

================
File: svm-test-harness/src/fixture/mod.rs
================
pub mod account_state;
pub mod error;
pub mod feature_set;
pub mod instr_context;
pub mod instr_effects;
⋮----
pub mod proto {
include!(concat!(env!("OUT_DIR"), "/org.solana.sealevel.v1.rs"));

================
File: svm-test-harness/src/file.rs
================
fn default_shared_object_dirs() -> Vec<PathBuf> {
let mut search_path = vec![PathBuf::from("tests/fixtures")];
⋮----
search_path.push(PathBuf::from(bpf_out_dir));
⋮----
search_path.push(PathBuf::from(sbf_out_dir));
⋮----
search_path.push(dir);
⋮----
fn find_file(filename: &str) -> Option<PathBuf> {
for dir in default_shared_object_dirs() {
let candidate = dir.join(filename);
if candidate.exists() {
return Some(candidate);
⋮----
pub fn read_file<P: AsRef<Path>>(path: P) -> Vec<u8> {
let path = path.as_ref();
⋮----
.unwrap_or_else(|e| panic!("Failed to open file {}: {}", path.display(), e));
⋮----
file.read_to_end(&mut file_data)
.unwrap_or_else(|e| panic!("Failed to read file {}: {}", path.display(), e));
⋮----
pub fn load_program_elf(program_name: &str) -> Vec<u8> {
let file_name = format!("{program_name}.so");
let program_file = find_file(&file_name)
.unwrap_or_else(|| panic!("Failed to find program ELF file: {file_name}"));
read_file(program_file)

================
File: svm-test-harness/src/fuzz.rs
================
pub unsafe extern "C" fn sol_compat_init(_log_level: i32) {
⋮----
if env::var("ENABLE_SOLANA_LOGGER").is_ok() {
⋮----
pub unsafe extern "C" fn sol_compat_fini() {}
pub fn execute_instr_proto(input: ProtoInstrContext) -> Option<ProtoInstrEffects> {
⋮----
let simd_0268_active = feature_set.is_active(&raise_cpi_nesting_limit_to_8::id());
let simd_0339_active = feature_set.is_active(&increase_cpi_account_info_limit::id());
⋮----
let slot = sysvar_cache.get_clock().unwrap().slot;
⋮----
create_program_runtime_environment_v1(
&instr_context.feature_set.runtime_features(),
&compute_budget.to_budget(),
⋮----
.unwrap(),
⋮----
.unwrap();
⋮----
let instr_effects = execute_instr(
⋮----
instr_effects.map(Into::into)
⋮----
pub unsafe extern "C" fn sol_compat_instr_execute_v1(
⋮----
let Some(instr_effects) = execute_instr_proto(instr_context) else {
⋮----
let out_vec = instr_effects.encode_to_vec();
if out_vec.len() > out_slice.len() {
⋮----
out_slice[..out_vec.len()].copy_from_slice(&out_vec);
⋮----
*out_psz = out_vec.len() as u64;

================
File: svm-test-harness/src/instr.rs
================
struct InstrContextCallback<'a>(&'a InstrContext);
impl InvokeContextCallback for InstrContextCallback<'_> {
fn is_precompile(&self, program_id: &Pubkey) -> bool {
is_precompile(program_id, |feature_id: &Pubkey| {
self.0.feature_set.is_active(feature_id)
⋮----
fn process_precompile(
⋮----
if let Some(precompile) = get_precompile(program_id, |feature_id: &Pubkey| {
⋮----
precompile.verify(data, &instruction_datas, &self.0.feature_set)
⋮----
Err(PrecompileError::InvalidPublicKey)
⋮----
fn compile_accounts<'a>(
⋮----
.iter()
.map(|(pubkey, account)| (*pubkey, AccountSharedData::from(account.clone())))
.collect();
⋮----
.any(|(pubkey, _)| pubkey == &input.instruction.program_id)
⋮----
transaction_accounts.push((input.instruction.program_id, AccountSharedData::default()));
⋮----
transaction_accounts.clone(),
⋮----
let num_transaction_accounts = transaction_context.get_number_of_accounts();
⋮----
.map(|meta| {
⋮----
.find_index_of_account(&meta.pubkey)
.unwrap_or(num_transaction_accounts)
⋮----
pub fn execute_instr(
⋮----
let runtime_features = input.feature_set.runtime_features();
let rent = sysvar_cache.get_rent().unwrap();
⋮----
compile_accounts(&input, compute_budget, (*rent).clone());
⋮----
create_program_runtime_environment_v1(
&input.feature_set.runtime_features(),
&compute_budget.to_budget(),
⋮----
.unwrap(),
⋮----
let instruction_data = input.instruction.data.iter().copied().collect::<Vec<_>>();
⋮----
.get_recent_blockhashes()
.ok()
.and_then(|x| (*x).last().cloned())
.map(|x| (x.blockhash, x.fee_calculator.lamports_per_signature))
.unwrap_or_default();
let callback = InstrContextCallback(&input);
⋮----
transaction_context.find_index_of_account(&input.instruction.program_id)?;
⋮----
Some(log_collector.clone()),
compute_budget.to_budget(),
compute_budget.to_cost(),
⋮----
.configure_next_instruction_for_tests(
⋮----
input.instruction.data.to_vec(),
⋮----
.unwrap();
if invoke_context.is_precompile(&input.instruction.program_id) {
invoke_context.process_precompile(
⋮----
[instruction_data.as_slice()].into_iter(),
⋮----
invoke_context.process_instruction(&mut compute_units_consumed, &mut timings)
⋮----
let cu_avail = input.cu_avail.saturating_sub(compute_units_consumed);
let return_data = transaction_context.get_return_data().1.to_vec();
let account_keys: Vec<Pubkey> = (0..transaction_context.get_number_of_accounts())
.map(|index| {
⋮----
.get_key_of_account_at_index(index)
.clone()
.unwrap()
⋮----
Some(InstrEffects {
⋮----
if get_precompile(&input.instruction.program_id, |_| true).is_some() {
Some(0)
⋮----
Some(code)
⋮----
result: result.err(),
⋮----
.deconstruct_without_keys()
⋮----
.into_iter()
.zip(account_keys)
.map(|(account, key)| (key, account.into()))
.collect(),
⋮----
mod tests {
⋮----
fn test_system_program_exec() {
⋮----
let clock_data = bincode::serialize(&clock).unwrap();
⋮----
let rent_data = bincode::serialize(&rent).unwrap();
⋮----
feature_set: feature_set.clone(),
accounts: vec![
⋮----
.into(),
data: vec![
⋮----
&feature_set.runtime_features(),
⋮----
let effects = execute_instr(context, &compute_budget, &mut program_cache, &sysvar_cache)
.expect("Instruction execution should succeed");
assert_eq!(effects.result, None);
assert_eq!(effects.custom_err, None);
assert_eq!(effects.cu_avail, 9850u64);
assert_eq!(effects.return_data, Vec::<u8>::new(),);
⋮----
.find(|(k, _)| k == &from_pubkey)
⋮----
assert_eq!(from_account.1.lamports, 999);
⋮----
.find(|(k, _)| k == &to_pubkey)
⋮----
assert_eq!(to_account.1.lamports, 1);

================
File: svm-test-harness/src/lib.rs
================
pub mod file;
pub mod fixture;
pub mod instr;
pub mod program_cache;
pub mod sysvar_cache;
⋮----
pub mod fuzz;

================
File: svm-test-harness/src/program_cache.rs
================
pub fn new_with_builtins(feature_set: &FeatureSet, slot: u64) -> ProgramCacheForTxBatch {
⋮----
cache.set_slot_for_tests(slot);
⋮----
&& !feature_set.is_active(&enable_loader_v4::id())
⋮----
&& !feature_set.is_active(&zk_elgamal_proof_program_enabled::id())
⋮----
&& !feature_set.is_active(&zk_token_sdk_enabled::id())
⋮----
cache.replenish(
⋮----
builtin.name.len(),
⋮----
pub fn add_program(
⋮----
create_program_runtime_environment_v1(
&feature_set.runtime_features(),
&compute_budget.to_budget(),
⋮----
.unwrap(),
⋮----
elf.len(),
⋮----
.unwrap();
cache.replenish(*program_id, Arc::new(entry));
⋮----
pub fn fill_from_accounts(
⋮----
if !newly_loaded_programs.insert(acc.0) {
return Err(InstructionError::UnsupportedProgramId);
⋮----
if program_cache.find(&acc.0).is_none() {
⋮----
&FillFromAccountsCallback(accounts),
⋮----
program_cache.replenish(acc.0, loaded_program);
⋮----
Ok(())
⋮----
struct FillFromAccountsCallback<'a>(&'a [(Pubkey, Account)]);
impl InvokeContextCallback for FillFromAccountsCallback<'_> {}
impl TransactionProcessingCallback for FillFromAccountsCallback<'_> {
fn get_account_shared_data(&self, pubkey: &Pubkey) -> Option<(AccountSharedData, u64)> {
⋮----
.iter()
.find(|(found_pubkey, _)| *found_pubkey == *pubkey)
.map(|(_, account)| (AccountSharedData::from(account.clone()), 0u64))

================
File: svm-test-harness/src/sysvar_cache.rs
================
pub fn fill_from_accounts(sysvar_cache: &mut SysvarCache, accounts: &[(Pubkey, Account)]) {
sysvar_cache.fill_missing_entries(|pubkey, callbackback| {
if let Some(account) = accounts.iter().find(|(key, _)| key == pubkey) {
if account.1.lamports() > 0 {
callbackback(account.1.data());

================
File: svm-test-harness/.gitignore
================
dump

================
File: svm-test-harness/build.rs
================
fn main() -> Result<(), Box<dyn std::error::Error>> {
⋮----
.expect("protosol did not expose PROTO_DIR, did protosol build.rs run first?"),
⋮----
println!("cargo:rerun-if-env-changed=DEP_PROTOSOL_PROTO_DIR");
println!("cargo:rerun-if-changed={}", proto_dir.display());
let mut proto_files = vec![];
⋮----
let path = entry?.path();
if path.extension().and_then(|e| e.to_str()) == Some("proto") {
println!("cargo:rerun-if-changed={}", path.display());
proto_files.push(path);
⋮----
proto_files.sort();
⋮----
config.out_dir(&out_dir);
config.compile_protos(
⋮----
.iter()
.map(|p| p.display().to_string())
⋮----
&[proto_dir.to_str().unwrap()],
⋮----
Ok(())

================
File: svm-test-harness/Cargo.toml
================
[package]
name = "solana-svm-test-harness"
description = "Solana SVM test harness."
documentation = "https://docs.rs/solana-svm-test-harness"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
publish = false

[[bin]]
name = "test_exec_instr"
path = "bin/test_exec_instr.rs"
required-features = ["fuzz"]

[features]
agave-unstable-api = []
dummy-for-ci-check = ["fuzz"]
fuzz = ["dep:clap", "dep:prost", "dep:prost-build", "dep:protosol"]

[dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
agave-precompiles = { workspace = true }
agave-syscalls = { workspace = true }
bincode = { workspace = true }
clap = { version = "4.5.2", features = ["derive"], optional = true }
prost = { workspace = true, optional = true }
protosol = { workspace = true, optional = true }
solana-account = { workspace = true }
solana-builtins = { workspace = true }
solana-clock = { workspace = true, features = ["sysvar"] }
solana-compute-budget = { workspace = true }
solana-epoch-schedule = { workspace = true, features = ["sysvar"] }
solana-instruction = { workspace = true }
solana-instruction-error = { workspace = true, features = ["serde"] }
solana-last-restart-slot = { workspace = true, features = ["sysvar"] }
solana-precompile-error = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true, features = ["sysvar"] }
solana-sdk-ids = { workspace = true }
solana-stable-layout = { workspace = true }
solana-svm = { workspace = true }
solana-svm-callback = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-timings = { workspace = true }
solana-sysvar-id = { workspace = true }
solana-transaction-context = { workspace = true }
thiserror = { workspace = true }

[build-dependencies]
prost-build = { workspace = true, optional = true }

[lints]
workspace = true

================
File: svm-test-harness/Makefile
================
CARGO?=cargo

test: | binaries

binaries:
	$(CARGO) rustc --manifest-path ./Cargo.toml --lib --release --features fuzz --crate-type cdylib
	$(CARGO) build --manifest-path ./Cargo.toml --bins --release --features fuzz

================
File: svm-timings/src/lib.rs
================
extern crate eager;
⋮----
pub struct ProgramTiming {
⋮----
impl ProgramTiming {
pub fn coalesce_error_timings(&mut self, current_estimated_program_cost: u64) {
for tx_error_compute_consumed in self.errored_txs_compute_consumed.drain(..) {
⋮----
pub fn accumulate_program_timings(&mut self, other: &ProgramTiming) {
⋮----
.extend(other.errored_txs_compute_consumed.clone());
⋮----
pub enum ExecuteTimingType {
⋮----
pub struct Metrics([Saturating<u64>; ExecuteTimingType::CARDINALITY]);
⋮----
type Output = Saturating<u64>;
fn index(&self, index: ExecuteTimingType) -> &Self::Output {
self.0.index(index as usize)
⋮----
fn index_mut(&mut self, index: ExecuteTimingType) -> &mut Self::Output {
self.0.index_mut(index as usize)
⋮----
impl Default for Metrics {
fn default() -> Self {
Metrics([Saturating(0); ExecuteTimingType::CARDINALITY])
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
self.0.fmt(f)
⋮----
eager_macro_rules! { $eager_1
⋮----
pub struct ExecuteTimings {
⋮----
impl ExecuteTimings {
pub fn accumulate(&mut self, other: &ExecuteTimings) {
for (t1, t2) in self.metrics.0.iter_mut().zip(other.metrics.0.iter()) {
⋮----
self.details.accumulate(&other.details);
⋮----
.accumulate(&other.execute_accessories);
⋮----
pub fn saturating_add_in_place(&mut self, timing_type: ExecuteTimingType, value_to_add: u64) {
⋮----
match self.metrics.0.get_mut(idx) {
⋮----
None => debug_assert!(idx < ExecuteTimingType::CARDINALITY, "Index out of bounds"),
⋮----
pub struct ExecuteProcessInstructionTimings {
⋮----
impl ExecuteProcessInstructionTimings {
pub fn accumulate(&mut self, other: &ExecuteProcessInstructionTimings) {
⋮----
pub struct ExecuteAccessoryTimings {
⋮----
impl ExecuteAccessoryTimings {
pub fn accumulate(&mut self, other: &ExecuteAccessoryTimings) {
⋮----
.accumulate(&other.process_instructions);
⋮----
pub struct ExecuteDetailsTimings {
⋮----
impl ExecuteDetailsTimings {
pub fn accumulate(&mut self, other: &ExecuteDetailsTimings) {
⋮----
let program_timing = self.per_program_timings.entry(*id).or_default();
program_timing.accumulate_program_timings(other);
⋮----
pub fn accumulate_program(
⋮----
let program_timing = self.per_program_timings.entry(*program_id).or_default();
⋮----
.push(compute_units_consumed);
⋮----
mod tests {
⋮----
fn construct_execute_timings_with_program(
⋮----
execute_details_timings.accumulate_program(
⋮----
.get(program_id)
.unwrap();
assert_eq!(program_timings.accumulated_us.0, us.saturating_mul(2));
assert_eq!(program_timings.accumulated_units.0, compute_units_consumed);
assert_eq!(program_timings.count.0, 1,);
assert_eq!(
⋮----
fn test_execute_details_timing_acumulate_program() {
⋮----
construct_execute_timings_with_program(&program_id, us, compute_units_consumed);
⋮----
fn test_execute_details_timing_acumulate() {
⋮----
execute_details_timings.accumulate(&other_execute_details_timings);
assert_eq!(execute_details_timings, other_execute_details_timings);
⋮----
fn execute_timings_saturating_add_in_place() {
⋮----
timings.saturating_add_in_place(ExecuteTimingType::CheckUs, 1);
let check_us = timings.metrics.index(ExecuteTimingType::CheckUs);
assert_eq!(1, check_us.0);
timings.saturating_add_in_place(ExecuteTimingType::CheckUs, 2);
⋮----
assert_eq!(3, check_us.0);

================
File: svm-timings/Cargo.toml
================
[package]
name = "solana-svm-timings"
description = "Solana Execution Timings"
documentation = "https://docs.rs/solana-svm-timings"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
eager = { workspace = true }
enum-iterator = { workspace = true }
solana-pubkey = { workspace = true }

================
File: svm-transaction/src/svm_message/sanitized_message.rs
================
impl SVMStaticMessage for SanitizedMessage {
fn num_transaction_signatures(&self) -> u64 {
u64::from(self.header().num_required_signatures)
⋮----
fn num_write_locks(&self) -> u64 {
⋮----
fn recent_blockhash(&self) -> &Hash {
⋮----
fn num_instructions(&self) -> usize {
SanitizedMessage::instructions(self).len()
⋮----
fn instructions_iter(&self) -> impl Iterator<Item = SVMInstruction<'_>> {
⋮----
.iter()
.map(SVMInstruction::from)
⋮----
fn program_instructions_iter(
⋮----
.map(|(pubkey, ix)| (pubkey, SVMInstruction::from(ix)))
⋮----
fn static_account_keys(&self) -> &[Pubkey] {
⋮----
fn fee_payer(&self) -> &Pubkey {
⋮----
fn num_lookup_tables(&self) -> usize {
SanitizedMessage::message_address_table_lookups(self).len()
⋮----
fn message_address_table_lookups(
⋮----
.map(SVMMessageAddressTableLookup::from)
⋮----
// Implement for the "reference" `SanitizedMessage` type.
impl SVMMessage for SanitizedMessage {
fn account_keys(&self) -> AccountKeys<'_> {
⋮----
fn is_writable(&self, index: usize) -> bool {
⋮----
fn is_signer(&self, index: usize) -> bool {
⋮----
fn is_invoked(&self, key_index: usize) -> bool {

================
File: svm-transaction/src/svm_message/sanitized_transaction.rs
================
impl SVMStaticMessage for SanitizedTransaction {
fn num_transaction_signatures(&self) -> u64 {
⋮----
fn num_write_locks(&self) -> u64 {
⋮----
fn recent_blockhash(&self) -> &Hash {
⋮----
fn num_instructions(&self) -> usize {
⋮----
fn instructions_iter(&self) -> impl Iterator<Item = SVMInstruction<'_>> {
⋮----
fn program_instructions_iter(
⋮----
fn static_account_keys(&self) -> &[Pubkey] {
⋮----
fn fee_payer(&self) -> &Pubkey {
⋮----
fn num_lookup_tables(&self) -> usize {
⋮----
fn message_address_table_lookups(
⋮----
impl SVMMessage for SanitizedTransaction {
fn account_keys(&self) -> AccountKeys<'_> {
⋮----
fn is_writable(&self, index: usize) -> bool {
⋮----
fn is_signer(&self, index: usize) -> bool {
⋮----
fn is_invoked(&self, key_index: usize) -> bool {

================
File: svm-transaction/src/svm_transaction/sanitized_transaction.rs
================
impl SVMTransaction for SanitizedTransaction {
fn signature(&self) -> &Signature {
⋮----
fn signatures(&self) -> &[Signature] {

================
File: svm-transaction/src/instruction.rs
================
use solana_message::compiled_instruction::CompiledInstruction;
⋮----
pub struct SVMInstruction<'a> {
/// Index into the transaction keys array indicating the program account that executes this instruction.
    pub program_id_index: u8,
/// Ordered indices into the transaction keys array indicating which accounts to pass to the program.
    pub accounts: &'a [u8],
⋮----
fn from(ix: &'a CompiledInstruction) -> Self {
⋮----
accounts: ix.accounts.as_slice(),
data: ix.data.as_slice(),

================
File: svm-transaction/src/lib.rs
================
pub mod instruction;
pub mod message_address_table_lookup;
pub mod svm_message;
pub mod svm_transaction;
mod tests;

================
File: svm-transaction/src/message_address_table_lookup.rs
================
pub struct SVMMessageAddressTableLookup<'a> {
/// Address lookup table account key
    pub account_key: &'a Pubkey,
⋮----
/// List of indexes used to load readonly account addresses
    pub readonly_indexes: &'a [u8],
⋮----
fn from(lookup: &'a v0::MessageAddressTableLookup) -> Self {

================
File: svm-transaction/src/svm_message.rs
================
mod sanitized_message;
mod sanitized_transaction;
⋮----
pub trait SVMStaticMessage {
⋮----
fn num_ed25519_signatures(&self) -> u64 {
default_precompile_signature_count(&ed25519_program::ID, self.program_instructions_iter())
⋮----
fn num_secp256k1_signatures(&self) -> u64 {
default_precompile_signature_count(&secp256k1_program::ID, self.program_instructions_iter())
⋮----
fn num_secp256r1_signatures(&self) -> u64 {
default_precompile_signature_count(&secp256r1_program::ID, self.program_instructions_iter())
⋮----
/// Return an iterator over the instructions in the message, paired with
    /// the pubkey of the program.
⋮----
/// the pubkey of the program.
    fn program_instructions_iter(
⋮----
// - Debug to support legacy logging
pub trait SVMMessage: Debug + SVMStaticMessage {
/// Return the account keys.
    fn account_keys(&self) -> AccountKeys<'_>;
⋮----
fn is_instruction_account(&self, key_index: usize) -> bool {
⋮----
self.instructions_iter()
.any(|ix| ix.accounts.contains(&key_index))
⋮----
fn get_durable_nonce(&self, require_static_nonce_account: bool) -> Option<&Pubkey> {
let account_keys = self.account_keys();
⋮----
.nth(usize::from(NONCED_TX_MARKER_IX_INDEX))
.filter(
|ix| match account_keys.get(usize::from(ix.program_id_index)) {
⋮----
.filter(|ix| {
const SERIALIZED_ADVANCE_NONCE_ACCOUNT: [u8; 4] = 4u32.to_le_bytes();
const SERIALIZED_SIZE: usize = SERIALIZED_ADVANCE_NONCE_ACCOUNT.len();
⋮----
.get(..SERIALIZED_SIZE)
.map(|data| data == SERIALIZED_ADVANCE_NONCE_ACCOUNT)
.unwrap_or(false)
⋮----
.and_then(|ix| {
ix.accounts.first().and_then(|idx| {
⋮----
if (require_static_nonce_account && index >= self.static_account_keys().len())
|| !self.is_writable(index)
⋮----
account_keys.get(index)
⋮----
fn get_ix_signers(&self, index: usize) -> impl Iterator<Item = &Pubkey> {
⋮----
.nth(index)
.into_iter()
.flat_map(|ix| {
⋮----
.iter()
.copied()
.map(usize::from)
.filter(|index| self.is_signer(*index))
.filter_map(|signer_index| self.account_keys().get(signer_index))
⋮----
fn default_precompile_signature_count<'a>(
⋮----
.filter(|(program_id, _)| *program_id == precompile)
.map(|(_, ix)| u64::from(ix.data.first().copied().unwrap_or(0)))
.sum()

================
File: svm-transaction/src/svm_transaction.rs
================
mod sanitized_transaction;
pub trait SVMTransaction: SVMMessage {

================
File: svm-transaction/src/tests.rs
================
fn test_get_durable_nonce(require_static_nonce_account: bool) {
fn create_message_for_test(
⋮----
num_readonly_unsigned_accounts: u8::try_from(account_keys.len())
.unwrap()
.checked_sub(num_writable)
.unwrap(),
⋮----
address_table_lookups: vec![MessageAddressTableLookup {
⋮----
SanitizedVersionedMessage::try_new(versioned_message).unwrap(),
⋮----
let message = create_message_for_test(1, 1, vec![Pubkey::new_unique()], vec![], None);
assert!(SanitizedMessage::get_durable_nonce(&message).is_none());
assert!(SVMMessage::get_durable_nonce(&message, require_static_nonce_account).is_none());
⋮----
let message = create_message_for_test(
⋮----
vec![Pubkey::new_unique(), system_program::id()],
vec![CompiledInstruction::new_from_raw_parts(1, vec![], vec![])],
⋮----
vec![CompiledInstruction::new(
⋮----
vec![payer, nonce, system_program::id()],
⋮----
vec![payer_nonce, system_program::id()],
⋮----
assert_eq!(
⋮----
let mut instruction_bytes = vec![4, 0, 0, 0];
instruction_bytes.push(0);
⋮----
vec![CompiledInstruction::new_from_raw_parts(
⋮----
assert_eq!(SanitizedMessage::get_durable_nonce(&message), Some(&nonce));
⋮----
vec![payer, other, nonce, system_program::id()],
⋮----
vec![payer, system_program::id()],
⋮----
Some(LoadedAddresses {
writable: vec![nonce],
readonly: vec![],
⋮----
fn test_get_ix_signers() {
⋮----
let instructions = vec![
⋮----
vec![signer0, signer1, non_signer, loader_key],
⋮----
.unwrap();

================
File: svm-transaction/Cargo.toml
================
[package]
name = "solana-svm-transaction"
description = "Solana SVM Transaction"
documentation = "https://docs.rs/solana-svm-transaction"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
solana-hash = { workspace = true }
solana-message = { workspace = true }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signature = { workspace = true }
solana-transaction = { workspace = true }

[dev-dependencies]
solana-message = { workspace = true, features = ["bincode"] }
solana-nonce = { workspace = true }
solana-system-interface = { workspace = true, features = ["bincode"] }
static_assertions = { workspace = true }
test-case = { workspace = true }

================
File: svm-type-overrides/src/lib.rs
================
pub mod executor {
⋮----
pub mod hint {
⋮----
pub mod rand {
⋮----
pub mod sync {
⋮----
pub mod thread {

================
File: svm-type-overrides/Cargo.toml
================
[package]
name = "solana-svm-type-overrides"
description = "Type overrides for specialized testing"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
shuttle-test = ["dep:shuttle"]
executor = ["dep:futures"]

[dependencies]
futures = { workspace = true, optional = true }
rand = { workspace = true }
shuttle = { workspace = true, optional = true }

================
File: syscalls/gen-syscall-list/src/main.rs
================
fn main() {

================
File: syscalls/gen-syscall-list/build.rs
================
fn main() {
⋮----
println!(
⋮----
Err(err) => panic!("Failed to open {}: {}", syscalls_rs_path.display(), err),
⋮----
let mut text = vec![];
file.read_to_end(&mut text).unwrap();
let text = str::from_utf8(&text).unwrap();
⋮----
Err(err) => panic!("Failed to create {}: {}", syscalls_txt_path.display(), err),
⋮----
let sysc_re = Regex::new(r#"register_syscall_by_name\([[:space:]]*b"([^"]+)","#).unwrap();
for caps in sysc_re.captures_iter(text) {
let name = caps[1].to_string();
writeln!(out, "{name}").unwrap();

================
File: syscalls/gen-syscall-list/Cargo.toml
================
[package]
name = "gen-syscall-list"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[build-dependencies]
regex = { workspace = true }

================
File: syscalls/src/cpi.rs
================
declare_builtin_function!(
⋮----
impl SyscallInvokeSigned for SyscallInvokeSignedRust {
fn translate_instruction(
⋮----
translate_instruction_rust(addr, memory_mapping, invoke_context, check_aligned)
⋮----
fn translate_accounts<'a>(
⋮----
translate_accounts_rust(
⋮----
fn translate_signers(
⋮----
translate_signers_rust(
⋮----
/// Cross-program invocation called from C
    SyscallInvokeSignedC,
⋮----
impl SyscallInvokeSigned for SyscallInvokeSignedC {
⋮----
translate_instruction_c(addr, memory_mapping, invoke_context, check_aligned)
⋮----
translate_accounts_c(
⋮----
translate_signers_c(

================
File: syscalls/src/lib.rs
================
use solana_program_runtime::memory::translate_vm_slice;
⋮----
mod cpi;
mod logging;
mod mem_ops;
mod sysvar;
⋮----
pub enum SyscallError {
⋮----
fn from(error: MemoryTranslationError) -> Self {
⋮----
fn from(error: CpiError) -> Self {
⋮----
type Error = Box<dyn std::error::Error>;
trait HasherImpl {
⋮----
struct Sha256Hasher(Hasher);
struct Blake3Hasher(blake3::Hasher);
struct Keccak256Hasher(keccak::Hasher);
impl HasherImpl for Sha256Hasher {
⋮----
type Output = Hash;
fn create_hasher() -> Self {
Sha256Hasher(Hasher::default())
⋮----
fn hash(&mut self, val: &[u8]) {
self.0.hash(val);
⋮----
fn result(self) -> Self::Output {
self.0.result()
⋮----
fn get_base_cost(compute_cost: &SVMTransactionExecutionCost) -> u64 {
⋮----
fn get_byte_cost(compute_cost: &SVMTransactionExecutionCost) -> u64 {
⋮----
fn get_max_slices(compute_budget: &SVMTransactionExecutionBudget) -> u64 {
⋮----
impl HasherImpl for Blake3Hasher {
⋮----
type Output = blake3::Hash;
⋮----
Blake3Hasher(blake3::Hasher::default())
⋮----
impl HasherImpl for Keccak256Hasher {
⋮----
type Output = keccak::Hash;
⋮----
Keccak256Hasher(keccak::Hasher::default())
⋮----
fn consume_compute_meter(invoke_context: &InvokeContext, amount: u64) -> Result<(), Error> {
invoke_context.consume_checked(amount)?;
Ok(())
⋮----
macro_rules! register_feature_gated_function {
⋮----
pub fn create_program_runtime_environment_v1<'a, 'ix_data>(
⋮----
debug_assert!(min_sbpf_version <= max_sbpf_version);
⋮----
result.register_function("abort", SyscallAbort::vm)?;
result.register_function("sol_panic_", SyscallPanic::vm)?;
result.register_function("sol_log_", SyscallLog::vm)?;
result.register_function("sol_log_64_", SyscallLogU64::vm)?;
result.register_function("sol_log_pubkey", SyscallLogPubkey::vm)?;
result.register_function("sol_log_compute_units_", SyscallLogBpfComputeUnits::vm)?;
result.register_function(
⋮----
result.register_function("sol_sha256", SyscallHash::vm::<Sha256Hasher>)?;
result.register_function("sol_keccak256", SyscallHash::vm::<Keccak256Hasher>)?;
result.register_function("sol_secp256k1_recover", SyscallSecp256k1Recover::vm)?;
register_feature_gated_function!(
⋮----
result.register_function("sol_get_clock_sysvar", SyscallGetClockSysvar::vm)?;
⋮----
result.register_function("sol_get_rent_sysvar", SyscallGetRentSysvar::vm)?;
⋮----
result.register_function("sol_memcpy_", SyscallMemcpy::vm)?;
result.register_function("sol_memmove_", SyscallMemmove::vm)?;
result.register_function("sol_memset_", SyscallMemset::vm)?;
result.register_function("sol_memcmp_", SyscallMemcmp::vm)?;
⋮----
result.register_function("sol_get_stack_height", SyscallGetStackHeight::vm)?;
result.register_function("sol_set_return_data", SyscallSetReturnData::vm)?;
result.register_function("sol_get_return_data", SyscallGetReturnData::vm)?;
result.register_function("sol_invoke_signed_c", SyscallInvokeSignedC::vm)?;
result.register_function("sol_invoke_signed_rust", SyscallInvokeSignedRust::vm)?;
⋮----
result.register_function("sol_log_data", SyscallLogData::vm)?;
Ok(result)
⋮----
pub fn create_program_runtime_environment_v2<'a, 'ix_data>(
⋮----
fn translate_type<'a, T>(
⋮----
translate_type_inner!(memory_mapping, AccessType::Load, vm_addr, T, check_aligned)
.map(|value| &*value)
⋮----
fn translate_slice<'a, T>(
⋮----
translate_slice_inner!(
⋮----
fn translate_string_and_do(
⋮----
match from_utf8(buf) {
Ok(message) => work(message),
Err(err) => Err(SyscallError::InvalidString(err, buf.to_vec()).into()),
⋮----
fn translate_type_mut<'a, T>(
⋮----
translate_type_inner!(memory_mapping, AccessType::Store, vm_addr, T, check_aligned)
⋮----
// Do not use this directly
⋮----
fn translate_slice_mut<'a, T>(
⋮----
fn touch_type_mut<T>(memory_mapping: &mut MemoryMapping, vm_addr: u64) -> Result<(), Error> {
translate_inner!(
⋮----
.map(|_| ())
⋮----
fn touch_slice_mut<T>(
⋮----
return Ok(());
⋮----
macro_rules! translate_mut {
⋮----
declare_builtin_function!(
⋮----
fn translate_and_check_program_address_inputs<'a>(
⋮----
if untranslated_seeds.len() > MAX_SEEDS {
return Err(SyscallError::BadSeeds(PubkeyError::MaxSeedLengthExceeded).into());
⋮----
.iter()
.map(|untranslated_seed| {
if untranslated_seed.len() > MAX_SEED_LEN as u64 {
⋮----
translate_vm_slice(untranslated_seed, memory_mapping, check_aligned)
⋮----
Ok((seeds, program_id))
⋮----
mod tests {
⋮----
use solana_sysvar::fees::Fees;
⋮----
macro_rules! assert_access_violation {
⋮----
macro_rules! prepare_mockup {
⋮----
struct MockSlice {
⋮----
fn test_translate() {
⋮----
let data = vec![0u8; LENGTH as usize];
let addr = data.as_ptr() as u64;
⋮----
vec![MemoryRegion::new_readonly(&data, START)],
⋮----
.unwrap();
let cases = vec![
⋮----
assert_eq!(
⋮----
assert!(
⋮----
fn test_translate_type() {
⋮----
vec![MemoryRegion::new_readonly(bytes_of(&pubkey), 0x100000000)],
⋮----
translate_type::<Pubkey>(&memory_mapping, 0x100000000, true).unwrap();
assert_eq!(pubkey, *translated_pubkey);
⋮----
vec![AccountMeta::new(solana_pubkey::new_rand(), false)],
⋮----
let memory_region = MemoryRegion::new_readonly(bytes_of(&instruction), 0x100000000);
⋮----
MemoryMapping::new(vec![memory_region], &config, SBPFVersion::V3).unwrap();
⋮----
translate_type::<StableInstruction>(&memory_mapping, 0x100000000, true).unwrap();
assert_eq!(instruction, *translated_instruction);
let memory_region = MemoryRegion::new_readonly(&bytes_of(&instruction)[..1], 0x100000000);
⋮----
assert!(translate_type::<Instruction>(&memory_mapping, 0x100000000, true).is_err());
⋮----
fn test_translate_slice() {
⋮----
let good_data = vec![1u8, 2, 3, 4, 5];
let data: Vec<u8> = vec![];
assert_eq!(std::ptr::dangling::<u8>(), data.as_ptr());
⋮----
vec![MemoryRegion::new_readonly(&good_data, 0x100000000)],
⋮----
translate_slice::<u8>(&memory_mapping, data.as_ptr() as u64, 0, true).unwrap();
assert_eq!(data, translated_data);
assert_eq!(0, translated_data.len());
let mut data = vec![1u8, 2, 3, 4, 5];
⋮----
vec![MemoryRegion::new_readonly(&data, 0x100000000)],
⋮----
translate_slice::<u8>(&memory_mapping, 0x100000000, data.len() as u64, true).unwrap();
⋮----
*data.first_mut().unwrap() = 10;
⋮----
let mut data = vec![1u64, 2, 3, 4, 5];
⋮----
vec![MemoryRegion::new_readonly(
⋮----
translate_slice::<u64>(&memory_mapping, 0x100000000, data.len() as u64, true).unwrap();
⋮----
assert!(translate_slice::<u64>(&memory_mapping, 0x100000000, u64::MAX, true).is_err());
let mut data = vec![solana_pubkey::new_rand(); 5];
⋮----
translate_slice::<Pubkey>(&memory_mapping, 0x100000000, data.len() as u64, true)
⋮----
*data.first_mut().unwrap() = solana_pubkey::new_rand();
⋮----
fn test_translate_string_and_do() {
⋮----
vec![MemoryRegion::new_readonly(string.as_bytes(), 0x100000000)],
⋮----
fn test_syscall_abort() {
prepare_mockup!(invoke_context, program_id, bpf_loader::id());
⋮----
let mut memory_mapping = MemoryMapping::new(vec![], &config, SBPFVersion::V3).unwrap();
⋮----
result.unwrap();
⋮----
fn test_syscall_sol_panic() {
⋮----
invoke_context.mock_set_remaining(string.len() as u64 - 1);
⋮----
string.len() as u64,
⋮----
assert_matches!(
⋮----
invoke_context.mock_set_remaining(string.len() as u64);
⋮----
fn test_syscall_sol_log() {
⋮----
invoke_context.mock_set_remaining(400 - 1);
⋮----
assert_access_violation!(result, 0x100000001, string.len() as u64);
⋮----
string.len() as u64 * 2,
⋮----
assert_access_violation!(result, 0x100000000, string.len() as u64 * 2);
⋮----
fn test_syscall_sol_log_u64() {
⋮----
let cost = invoke_context.get_execution_cost().log_64_units;
invoke_context.mock_set_remaining(cost);
⋮----
fn test_syscall_sol_pubkey() {
⋮----
let cost = invoke_context.get_execution_cost().log_pubkey_units;
let pubkey = Pubkey::from_str("MoqiU1vryuCGQSxFKA1SZ316JdLEFFhoAu6cKUNk7dN").unwrap();
⋮----
assert_access_violation!(result, 0x100000001, 32);
invoke_context.mock_set_remaining(1);
⋮----
macro_rules! setup_alloc_test {
⋮----
fn test_syscall_sol_alloc_free() {
⋮----
setup_alloc_test!(invoke_context, memory_mapping, heap);
⋮----
assert_ne!(result.unwrap(), 0);
⋮----
assert_eq!(result.unwrap(), 0);
⋮----
fn aligned<T>() {
⋮----
let address = result.unwrap();
assert_ne!(address, 0);
assert!(address_is_aligned::<T>(address));
⋮----
fn test_syscall_sha256() {
⋮----
prepare_mockup!(invoke_context, program_id, bpf_loader_deprecated::id());
⋮----
len: bytes1.len(),
⋮----
len: bytes2.len(),
⋮----
let ro_len = bytes_to_hash.len() as u64;
⋮----
vec![
⋮----
invoke_context.mock_set_remaining(
(invoke_context.get_execution_cost().sha256_base_cost
+ invoke_context.get_execution_cost().mem_op_base_cost.max(
⋮----
.get_execution_cost()
⋮----
.saturating_mul((bytes1.len() + bytes2.len()) as u64 / 2),
⋮----
let hash_local = hashv(&[bytes1.as_ref(), bytes2.as_ref()]).to_bytes();
assert_eq!(hash_result, hash_local);
⋮----
assert_access_violation!(result, ro_va - 1, 32);
⋮----
assert_access_violation!(result, ro_va, 48);
⋮----
assert_access_violation!(result, rw_va - 1, HASH_BYTES as u64);
⋮----
fn test_syscall_edwards_curve_point_validation() {
use solana_curve25519::curve_syscall_traits::CURVE25519_EDWARDS;
⋮----
assert_eq!(0, result.unwrap());
⋮----
assert_eq!(1, result.unwrap());
⋮----
fn test_syscall_ristretto_curve_point_validation() {
use solana_curve25519::curve_syscall_traits::CURVE25519_RISTRETTO;
⋮----
fn test_syscall_edwards_curve_group_ops() {
⋮----
assert_eq!(expected_sum, result_point);
⋮----
assert_eq!(expected_difference, result_point);
⋮----
assert_eq!(expected_product, result_point);
⋮----
fn test_syscall_ristretto_curve_group_ops() {
⋮----
fn test_syscall_multiscalar_multiplication() {
⋮----
fn test_syscall_multiscalar_multiplication_maximum_length_exceeded() {
⋮----
invoke_context.mock_set_remaining(500_000);
⋮----
.unwrap_err()
⋮----
assert_eq!(*result, SyscallError::InvalidLength);
⋮----
fn create_filled_type<T: Default>(zero_init: bool) -> T {
⋮----
*p.offset(i) = if zero_init { 0 } else { i as u8 };
⋮----
fn are_bytes_equal<T>(first: &T, second: &T) -> bool {
⋮----
if *p_first.offset(i) != *p_second.offset(i) {
⋮----
fn test_syscall_get_sysvar() {
⋮----
let transaction_accounts = vec![
⋮----
with_mock_invoke_context!(invoke_context, transaction_context, transaction_accounts);
⋮----
let mut got_clock_buf = vec![0; Clock::size_of()];
⋮----
let clock_id = Clock::id().to_bytes();
⋮----
assert_eq!(got_clock_obj, src_clock);
⋮----
assert!(are_bytes_equal(&got_clock_obj, &clean_clock));
⋮----
let clock_from_buf = bincode::deserialize::<Clock>(&got_clock_buf).unwrap();
assert_eq!(clock_from_buf, src_clock);
assert!(are_bytes_equal(&clock_from_buf, &clean_clock));
⋮----
let mut got_epochschedule_buf = vec![0; EpochSchedule::size_of()];
⋮----
let epochschedule_id = EpochSchedule::id().to_bytes();
⋮----
assert_eq!(got_epochschedule_obj, src_epochschedule);
⋮----
assert!(are_bytes_equal(
⋮----
bincode::deserialize::<EpochSchedule>(&got_epochschedule_buf).unwrap();
assert_eq!(epochschedule_from_buf, src_epochschedule);
⋮----
vec![MemoryRegion::new_writable(
⋮----
assert_eq!(got_fees, src_fees);
⋮----
assert!(are_bytes_equal(&got_fees, &clean_fees));
⋮----
let mut got_rent_buf = vec![0; Rent::size_of()];
⋮----
let rent_id = Rent::id().to_bytes();
⋮----
assert_eq!(got_rent_obj, src_rent);
⋮----
assert!(are_bytes_equal(&got_rent_obj, &clean_rent));
⋮----
let rent_from_buf = bincode::deserialize::<Rent>(&got_rent_buf).unwrap();
assert_eq!(rent_from_buf, src_rent);
assert!(are_bytes_equal(&rent_from_buf.clone(), &clean_rent));
⋮----
let mut got_rewards_buf = vec![0; EpochRewards::size_of()];
⋮----
let rewards_id = EpochRewards::id().to_bytes();
⋮----
assert_eq!(got_rewards_obj, src_rewards);
⋮----
assert!(are_bytes_equal(&got_rewards_obj, &clean_rewards));
⋮----
let rewards_from_buf = bincode::deserialize::<EpochRewards>(&got_rewards_buf).unwrap();
assert_eq!(rewards_from_buf, src_rewards);
assert!(are_bytes_equal(&rewards_from_buf.clone(), &clean_rewards));
⋮----
let mut got_restart_buf = vec![0; LastRestartSlot::size_of()];
⋮----
let restart_id = LastRestartSlot::id().to_bytes();
⋮----
assert_eq!(got_restart_obj, src_restart);
⋮----
assert!(are_bytes_equal(&got_restart_obj, &clean_restart));
⋮----
bincode::deserialize::<LastRestartSlot>(&got_restart_buf).unwrap();
assert_eq!(restart_from_buf, src_restart);
assert!(are_bytes_equal(&restart_from_buf, &clean_restart));
⋮----
fn test_syscall_get_stake_history(filled: bool) {
⋮----
src_history.add(
⋮----
let mut src_history_buf = vec![0; StakeHistory::size_of()];
bincode::serialize_into(&mut src_history_buf, &src_history).unwrap();
let transaction_accounts = vec![(
⋮----
let mut got_history_buf = vec![0; StakeHistory::size_of()];
⋮----
let history_id = StakeHistory::id().to_bytes();
⋮----
let history_from_buf = bincode::deserialize::<StakeHistory>(&got_history_buf).unwrap();
assert_eq!(history_from_buf, src_history);
⋮----
fn test_syscall_get_slot_hashes(filled: bool) {
⋮----
src_hashes.add(slot, hashv(&[&slot.to_le_bytes()]));
⋮----
let mut src_hashes_buf = vec![0; SlotHashes::size_of()];
bincode::serialize_into(&mut src_hashes_buf, &src_hashes).unwrap();
⋮----
let mut got_hashes_buf = vec![0; SlotHashes::size_of()];
⋮----
let hashes_id = SlotHashes::id().to_bytes();
⋮----
let hashes_from_buf = bincode::deserialize::<SlotHashes>(&got_hashes_buf).unwrap();
assert_eq!(hashes_from_buf, src_hashes);
⋮----
fn test_syscall_get_sysvar_errors() {
⋮----
let mut got_clock_buf_rw = vec![0; Clock::size_of()];
⋮----
let got_clock_buf_ro = vec![0; Clock::size_of()];
⋮----
let got_clock_empty = vec![0; Clock::size_of()];
⋮----
// start without the clock sysvar because we expect to hit specific errors before loading it
with_mock_invoke_context!(invoke_context, transaction_context, vec![]);
// Abort: "Not all bytes in VM memory range `[sysvar_id, sysvar_id + 32)` are readable."
⋮----
.unwrap_err();
⋮----
assert_eq!(got_clock_buf_rw, got_clock_empty);
// Abort: "Not all bytes in VM memory range `[var_addr, var_addr + length)` are writable."
⋮----
// Abort: "`offset + length` is not in `[0, 2^64)`."
⋮----
// "`var_addr + length` is not in `[0, 2^64)`" is theoretically impossible to trigger
// because if the sum extended outside u64::MAX then it would not be writable and translate would fail
// "`2` if the sysvar data is not present in the Sysvar Cache."
⋮----
assert_eq!(result, 2);
⋮----
// "`1` if `offset + length` is greater than the length of the sysvar data."
⋮----
assert_eq!(result, 1);
⋮----
// and now lets succeed
⋮----
let clock_from_buf = bincode::deserialize::<Clock>(&got_clock_buf_rw).unwrap();
⋮----
type BuiltinFunctionRustInterface<'a> = fn(
⋮----
fn call_program_address_common<'a, 'b: 'a>(
⋮----
let mut regions = vec![
⋮----
let mut mock_slices = Vec::with_capacity(seeds.len());
for (i, seed) in seeds.iter().enumerate() {
let vm_addr = SEED_VA.saturating_add((i as u64).saturating_mul(0x100000000));
⋮----
len: seed.len(),
⋮----
mock_slices.push(mock_slice);
regions.push(MemoryRegion::new_readonly(bytes_of_slice(seed), vm_addr));
⋮----
regions.push(MemoryRegion::new_readonly(
bytes_of_slice(&mock_slices),
⋮----
let mut memory_mapping = MemoryMapping::new(regions, &config, SBPFVersion::V3).unwrap();
let result = syscall(
⋮----
seeds.len() as u64,
⋮----
result.map(|_| (address, bump_seed))
⋮----
fn create_program_address<'a>(
⋮----
let (address, _) = call_program_address_common(
⋮----
Ok(address)
⋮----
fn try_find_program_address<'a>(
⋮----
call_program_address_common(
⋮----
fn test_set_and_get_return_data() {
⋮----
let data = vec![42; 24];
let mut data_buffer = vec![0; 16];
let mut id_buffer = vec![0; 32];
⋮----
data.len() as u64,
⋮----
data_buffer.len() as u64,
⋮----
assert_eq!(result.unwrap() as usize, data.len());
assert_eq!(data.get(0..data_buffer.len()).unwrap(), data_buffer);
assert_eq!(id_buffer, program_id.to_bytes());
⋮----
fn test_syscall_sol_get_processed_sibling_instruction() {
⋮----
.map(|_| {
⋮----
for (index_in_trace, stack_height) in instruction_trace.into_iter().enumerate() {
⋮----
.get_instruction_stack_height()
⋮----
invoke_context.transaction_context.pop().unwrap();
⋮----
let instruction_accounts = vec![InstructionAccount::new(
⋮----
.configure_next_instruction_for_tests(
⋮----
vec![index_in_trace as u8],
⋮----
invoke_context.transaction_context.push().unwrap();
⋮----
let syscall_base_cost = invoke_context.get_execution_cost().syscall_base_cost;
⋮----
vec![MemoryRegion::new_writable(&mut memory, VM_BASE_ADDRESS)],
⋮----
unsafe { &mut *memory.as_mut_ptr().cast::<ProcessedSiblingInstruction>() };
⋮----
invoke_context.mock_set_remaining(syscall_base_cost);
⋮----
VM_BASE_ADDRESS.saturating_add(META_OFFSET as u64),
VM_BASE_ADDRESS.saturating_add(PROGRAM_ID_OFFSET as u64),
VM_BASE_ADDRESS.saturating_add(DATA_OFFSET as u64),
VM_BASE_ADDRESS.saturating_add(ACCOUNTS_OFFSET as u64),
⋮----
assert_eq!(result.unwrap(), 1);
⋮----
assert_eq!(processed_sibling_instruction.data_len, 1);
assert_eq!(processed_sibling_instruction.accounts_len, 1);
⋮----
assert_eq!(data, &[5]);
⋮----
fn test_create_program_address() {
// These tests duplicate the direct tests in solana_pubkey
⋮----
assert!(create_program_address(&mut invoke_context, &[max_seed], &address).is_ok());
⋮----
assert!(create_program_address(&mut invoke_context, exceeded_seeds, &address).is_ok());
⋮----
let public_key = Pubkey::from_str("SeedPubey1111111111111111111111111111111111").unwrap();
⋮----
assert_ne!(
⋮----
invoke_context.mock_set_remaining(0);
⋮----
fn test_find_program_address() {
⋮----
let max_tries = 256; // one per seed
⋮----
invoke_context.mock_set_remaining(cost * max_tries);
⋮----
try_find_program_address(&mut invoke_context, &[b"Lil'", b"Bits"], &address)
⋮----
try_find_program_address(&mut invoke_context, seeds, &address).unwrap();
invoke_context.mock_set_remaining(cost * (max_tries - bump_seed as u64));
⋮----
invoke_context.mock_set_remaining(cost * (max_tries - bump_seed as u64 - 1));
⋮----
invoke_context.mock_set_remaining(cost * (max_tries - 1));
⋮----
fn test_syscall_big_mod_exp() {
⋮----
let budget = invoke_context.get_execution_cost();
⋮----
fn test_syscall_get_epoch_stake_total_stake() {
⋮----
struct MockCallback {}
impl InvokeContextCallback for MockCallback {
fn get_epoch_stake(&self) -> u64 {
⋮----
assert_eq!(result, EXPECTED_TOTAL_STAKE);
⋮----
fn test_syscall_get_epoch_stake_vote_account_stake() {
⋮----
fn get_epoch_stake_for_vote_account(&self, vote_address: &Pubkey) -> u64 {
⋮----
assert_access_violation!(result, vote_address_var, 32);
⋮----
invoke_context.mock_set_remaining(compute_budget.compute_unit_limit);
⋮----
assert_eq!(result, EXPECTED_EPOCH_STAKE);
⋮----
assert_eq!(result, 0);
⋮----
fn test_check_type_assumptions() {
check_type_assumptions();
⋮----
fn bytes_of<T>(val: &T) -> &[u8] {
⋮----
unsafe { slice::from_raw_parts(std::slice::from_ref(val).as_ptr().cast(), size) }
⋮----
fn bytes_of_mut<T>(val: &mut T) -> &mut [u8] {
⋮----
unsafe { slice::from_raw_parts_mut(slice::from_mut(val).as_mut_ptr().cast(), size) }
⋮----
fn bytes_of_slice<T>(val: &[T]) -> &[u8] {
let size = val.len().wrapping_mul(mem::size_of::<T>());
unsafe { slice::from_raw_parts(val.as_ptr().cast(), size) }
⋮----
fn bytes_of_slice_mut<T>(val: &mut [T]) -> &mut [u8] {
⋮----
unsafe { slice::from_raw_parts_mut(val.as_mut_ptr().cast(), size) }
⋮----
fn test_address_is_aligned() {
⋮----
assert_eq!(address_is_aligned::<u64>(address as u64), address == 0);
⋮----
fn test_memcmp_success(src_a: u64, src_b: u64, expected_result: &[u8; 4]) {
⋮----
let mut result_mem = vec![0; 4];
⋮----
assert_eq!(result_mem, expected_result);
⋮----
fn test_memmove_success(dst: u64, src: u64, expected_hash: u64) {
⋮----
mem.hash(&mut hasher);
assert_eq!(hasher.finish(), expected_hash);
⋮----
fn test_memset_success(dst: u64, value: u64, expected_hash: u64) {
⋮----
vec![MemoryRegion::new_writable(&mut mem, 0x100000000)],
⋮----
fn test_memcpy_overlapping(dst: u64, src: u64) {
⋮----
fn test_memops_access_violation(dst: u64, src: u64, fault_address: u64) {
⋮----
assert_access_violation!(result, fault_address, 4);
⋮----
fn test_memset_access_violation(dst: u64) {
⋮----
assert_access_violation!(result, dst, 4);
⋮----
fn test_memcmp_result_access_violation() {
⋮----
vec![MemoryRegion::new_readonly(&mem, 0x100000000)],
⋮----
assert_access_violation!(result, 0x100000000, 4);

================
File: syscalls/src/logging.rs
================
declare_builtin_function!(

================
File: syscalls/src/mem_ops.rs
================
fn mem_op_consume(invoke_context: &mut InvokeContext, n: u64) -> Result<(), Error> {
let compute_cost = invoke_context.get_execution_cost();
let cost = compute_cost.mem_op_base_cost.max(
n.checked_div(compute_cost.cpi_bytes_per_unit)
.unwrap_or(u64::MAX),
⋮----
consume_compute_meter(invoke_context, cost)
⋮----
pub(crate) fn is_nonoverlapping<N>(src: N, src_len: N, dst: N, dst_len: N) -> bool
⋮----
src.saturating_sub(&dst) >= dst_len
⋮----
dst.saturating_sub(&src) >= src_len
⋮----
declare_builtin_function!(
⋮----
fn memmove(
⋮----
translate_mut!(
⋮----
let dst_ptr = dst_ref_mut.as_mut_ptr();
⋮----
invoke_context.get_check_aligned(),
⋮----
.as_ptr();
⋮----
Ok(0)
⋮----
unsafe fn memcmp(s1: &[u8], s2: &[u8], n: usize) -> i32 {
⋮----
let a = *s1.get_unchecked(i);
let b = *s2.get_unchecked(i);
⋮----
return (a as i32).saturating_sub(b as i32);
⋮----
mod tests {
⋮----
fn test_is_nonoverlapping() {
⋮----
assert!(is_nonoverlapping(10, 3, dst, 3));
⋮----
assert!(!is_nonoverlapping(10, 3, dst, 3));
⋮----
assert!(is_nonoverlapping::<u8>(255, 3, 254, 1));
assert!(!is_nonoverlapping::<u8>(255, 2, 254, 3));

================
File: syscalls/src/sysvar.rs
================
fn get_sysvar<T: std::fmt::Debug + SysvarSerialize + Clone>(
⋮----
consume_compute_meter(
⋮----
.get_execution_cost()
⋮----
.saturating_add(size_of::<T>() as u64),
⋮----
.get_feature_set()
⋮----
return Err(SyscallError::InvalidPointer.into());
⋮----
translate_mut!(
⋮----
*var = T::clone(sysvar.as_ref());
Ok(SUCCESS)
⋮----
declare_builtin_function!(

================
File: syscalls/Cargo.toml
================
[package]
name = "agave-syscalls"
description = "Agave implementation of the Solana syscalls."
documentation = "https://docs.rs/agave-syscalls"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = ["metrics"]
agave-unstable-api = []
metrics = ["solana-program-runtime/metrics"]
shuttle-test = [
    "solana-program-runtime/shuttle-test",
    "solana-sbpf/shuttle-test",
    "solana-svm-type-overrides/shuttle-test",
]
svm-internal = []

[dependencies]
bincode = { workspace = true }
libsecp256k1 = { workspace = true }
num-traits = { workspace = true }
solana-account = { workspace = true }
solana-account-info = { workspace = true }
solana-big-mod-exp = { workspace = true }
solana-blake3-hasher = { workspace = true, features = ["blake3"] }
solana-bn254 = { workspace = true }
solana-clock = { workspace = true }
solana-cpi = { workspace = true }
solana-curve25519 = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keccak-hasher = { workspace = true, features = ["sha3"] }
solana-loader-v3-interface = { workspace = true, features = ["serde"] }
solana-poseidon = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true }
solana-sbpf = { workspace = true, features = ["jit"] }
solana-sdk-ids = { workspace = true }
solana-secp256k1-recover = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-stable-layout = { workspace = true }
solana-stake-interface = { workspace = true }
solana-svm-callback = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-measure = { workspace = true }
solana-svm-timings = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-sysvar = { workspace = true }
solana-sysvar-id = { workspace = true }
solana-transaction-context = { workspace = true, features = ["bincode"] }
thiserror = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
solana-epoch-rewards = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-last-restart-slot = { workspace = true }
solana-program = { workspace = true }
solana-program-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-rent = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-transaction-context = { workspace = true, features = ["dev-context-only-utils"] }
static_assertions = { workspace = true }
test-case = { workspace = true }

[lints]
workspace = true

================
File: test-validator/src/lib.rs
================
pub struct AccountInfo<'a> {
⋮----
pub struct UpgradeableProgramInfo {
⋮----
pub struct TestValidatorNodeConfig {
⋮----
impl Default for TestValidatorNodeConfig {
fn default() -> Self {
⋮----
pub struct TestValidatorGenesis {
⋮----
impl Default for TestValidatorGenesis {
⋮----
let deactivate_feature_set = [alpenglow::id()].into_iter().collect();
⋮----
fn try_transform_program_data(
⋮----
if account.owner() == &solana_sdk_ids::bpf_loader_upgradeable::id() {
⋮----
let programdata_meta = account.data().get(0..programdata_offset).ok_or(format!(
⋮----
account.data_as_mut_slice(),
⋮----
.map_err(|_| format!("Failed to write to upgradeable programdata account {address}"))
⋮----
Err(format!(
⋮----
Err(format!("Account {address} not owned by upgradeable loader"))
⋮----
impl TestValidatorGenesis {
pub fn deactivate_features(&mut self, deactivate_list: &[Pubkey]) -> &mut Self {
self.deactivate_feature_set.extend(deactivate_list);
⋮----
pub fn ledger_path<P: Into<PathBuf>>(&mut self, ledger_path: P) -> &mut Self {
self.ledger_path = Some(ledger_path.into());
⋮----
pub fn tower_storage(&mut self, tower_storage: Arc<dyn TowerStorage>) -> &mut Self {
self.tower_storage = Some(tower_storage);
⋮----
pub fn ledger_exists(ledger_path: &Path) -> bool {
ledger_path.join("vote-account-keypair.json").exists()
⋮----
pub fn tpu_enable_udp(&mut self, tpu_enable_udp: bool) -> &mut Self {
⋮----
pub fn fee_rate_governor(&mut self, fee_rate_governor: FeeRateGovernor) -> &mut Self {
⋮----
pub fn ticks_per_slot(&mut self, ticks_per_slot: u64) -> &mut Self {
self.ticks_per_slot = Some(ticks_per_slot);
⋮----
pub fn epoch_schedule(&mut self, epoch_schedule: EpochSchedule) -> &mut Self {
self.epoch_schedule = Some(epoch_schedule);
⋮----
pub fn inflation(&mut self, inflation: Inflation) -> &mut Self {
self.inflation = Some(inflation);
⋮----
pub fn rent(&mut self, rent: Rent) -> &mut Self {
⋮----
pub fn rpc_config(&mut self, rpc_config: JsonRpcConfig) -> &mut Self {
⋮----
pub fn pubsub_config(&mut self, pubsub_config: PubSubConfig) -> &mut Self {
⋮----
pub fn rpc_port(&mut self, rpc_port: u16) -> &mut Self {
self.rpc_ports = Some((rpc_port, rpc_port + 1));
⋮----
pub fn faucet_addr(&mut self, faucet_addr: Option<SocketAddr>) -> &mut Self {
⋮----
pub fn warp_slot(&mut self, warp_slot: Slot) -> &mut Self {
self.warp_slot = Some(warp_slot);
⋮----
pub fn gossip_host(&mut self, gossip_host: IpAddr) -> &mut Self {
self.node_config.gossip_addr.set_ip(gossip_host);
⋮----
pub fn gossip_port(&mut self, gossip_port: u16) -> &mut Self {
self.node_config.gossip_addr.set_port(gossip_port);
⋮----
pub fn port_range(&mut self, port_range: PortRange) -> &mut Self {
⋮----
pub fn bind_ip_addr(&mut self, bind_ip_addr: IpAddr) -> &mut Self {
⋮----
pub fn compute_unit_limit(&mut self, compute_unit_limit: u64) -> &mut Self {
self.compute_unit_limit = Some(compute_unit_limit);
⋮----
pub fn add_account(&mut self, address: Pubkey, account: AccountSharedData) -> &mut Self {
self.accounts.insert(address, account);
⋮----
pub fn add_accounts<T>(&mut self, accounts: T) -> &mut Self
⋮----
self.add_account(address, account);
⋮----
fn clone_accounts_and_transform<T, F>(
⋮----
let addresses: Vec<Pubkey> = addresses.into_iter().collect();
for chunk in addresses.chunks(MAX_MULTIPLE_ACCOUNTS) {
info!("Fetching {chunk:?} over RPC...");
⋮----
.get_multiple_accounts(chunk)
.map_err(|err| format!("Failed to fetch: {err}"))?;
for (address, res) in chunk.iter().zip(responses) {
⋮----
self.add_account(*address, transform(address, account)?);
⋮----
warn!("Could not find {address}, skipping.");
⋮----
return Err(format!("Failed to fetch {address}"));
⋮----
Ok(self)
⋮----
pub fn clone_accounts<T>(
⋮----
self.clone_accounts_and_transform(
⋮----
try_transform_program_data(address, &mut account_shared_data).ok();
Ok(account_shared_data)
⋮----
pub fn deep_clone_address_lookup_table_accounts<T>(
⋮----
if address_lookup_table::check_id(account.owner()) {
⋮----
.data()
.get(LOOKUP_TABLE_META_SIZE..)
.ok_or(format!("Failed to get addresses data from {address}"))?;
if raw_addresses_data.len() % std::mem::size_of::<Pubkey>() != 0 {
return Err(format!("Invalid alt account data length for {address}"));
⋮----
raw_addresses_data.chunks_exact(std::mem::size_of::<Pubkey>())
⋮----
let address = Pubkey::try_from(address_slice).unwrap();
alt_entries.push(address);
⋮----
self.add_account(*address, AccountSharedData::from(account));
⋮----
return Err(format!("Account {address} is not an address lookup table"));
⋮----
self.clone_accounts(alt_entries, rpc_client, true)
⋮----
pub fn clone_programdata_accounts<T>(
⋮----
try_transform_program_data(address, &mut account_shared_data)?;
⋮----
pub fn clone_upgradeable_programs<T>(
⋮----
self.clone_accounts(addresses.clone(), rpc_client, false)?;
⋮----
let account = self.accounts.get(&address).unwrap();
⋮----
}) = account.deserialize_data()
⋮----
programdata_addresses.insert(programdata_address);
⋮----
return Err(format!(
⋮----
self.clone_programdata_accounts(programdata_addresses, rpc_client, false)?;
⋮----
pub fn clone_feature_set(&mut self, rpc_client: &RpcClient) -> Result<&mut Self, String> {
⋮----
.keys()
.cloned()
⋮----
.chunks(MAX_MULTIPLE_ACCOUNTS)
⋮----
.get_multiple_accounts(feature_ids)
.map_err(|err| format!("Failed to fetch: {err}"))?
.into_iter()
.zip(feature_ids)
.for_each(|(maybe_account, feature_id)| {
⋮----
.as_ref()
.and_then(solana_feature_gate_interface::from_account)
.and_then(|feature| feature.activated_at)
.is_none()
⋮----
self.deactivate_feature_set.insert(*feature_id);
⋮----
pub fn add_accounts_from_json_files(
⋮----
return Err(format!("Unable to locate {}", account.filename));
⋮----
let mut file = File::open(&account_path).unwrap();
⋮----
file.read_to_string(&mut account_info_raw).unwrap();
⋮----
let address = account.address.unwrap_or_else(|| {
Pubkey::from_str(account_info.keyed_account.pubkey.as_str()).unwrap()
⋮----
.unwrap();
⋮----
pub fn add_accounts_from_directories<T, P>(&mut self, dirs: T) -> Result<&mut Self, String>
⋮----
Err(e) => return Err(format!("Cannot read directory {}: {}", &dir, e)),
⋮----
.flatten()
.map(|entry| entry.path())
.filter(|path| path.is_file() && path.extension() == Some(OsStr::new("json")))
.map(|path| String::from(path.to_string_lossy()));
json_files.extend(matched_files);
⋮----
debug!("account files found: {json_files:?}");
⋮----
.iter()
.map(|filename| AccountInfo {
⋮----
.collect();
self.add_accounts_from_json_files(&accounts)?;
⋮----
pub fn add_account_with_file_data(
⋮----
self.add_account(
⋮----
solana_program_test::find_file(filename).unwrap_or_else(|| {
panic!("Unable to locate {filename}");
⋮----
pub fn add_account_with_base64_data(
⋮----
.decode(data_base64)
.unwrap_or_else(|err| panic!("Failed to base64 decode: {err}")),
⋮----
pub fn add_program(&mut self, program_name: &str, program_id: Pubkey) -> &mut Self {
let program_path = solana_program_test::find_file(&format!("{program_name}.so"))
.unwrap_or_else(|| panic!("Unable to locate program {program_name}"));
self.upgradeable_programs.push(UpgradeableProgramInfo {
⋮----
pub fn add_upgradeable_programs_with_path(
⋮----
self.upgradeable_programs.push(program.clone());
⋮----
pub fn start_with_mint_address(
⋮----
self.start_with_mint_address_and_geyser_plugin_rpc(mint_address, socket_addr_space, None)
⋮----
pub fn start_with_mint_address_and_geyser_plugin_rpc(
⋮----
.inspect(|test_validator| {
⋮----
.enable_io()
.enable_time()
.build()
⋮----
runtime.block_on(test_validator.wait_for_nonzero_fees());
⋮----
pub fn start(&self) -> (TestValidator, Keypair) {
self.start_with_socket_addr_space(SocketAddrSpace::new( true))
⋮----
pub fn start_with_socket_addr_space(
⋮----
self.start_with_mint_address(mint_keypair.pubkey(), socket_addr_space)
⋮----
.map(|p| &p.program_id)
⋮----
.block_on(test_validator.wait_for_upgradeable_programs_deployed(
⋮----
.unwrap_or_else(|err| {
panic!("Failed to wait for programs to be deployed: {err:?}")
⋮----
.map(|test_validator| (test_validator, mint_keypair))
.unwrap_or_else(|err| panic!("Test validator failed to start: {err}"))
⋮----
pub async fn start_async_with_mint_address(
⋮----
TestValidator::start(mint_keypair.pubkey(), self, socket_addr_space, None)?;
test_validator.wait_for_nonzero_fees().await;
⋮----
.wait_for_upgradeable_programs_deployed(&upgradeable_program_ids, mint_keypair)
⋮----
.unwrap_or_else(|err| panic!("Failed to wait for programs to be deployed: {err:?}"));
Ok(test_validator)
⋮----
pub async fn start_async(&self) -> (TestValidator, Keypair) {
self.start_async_with_socket_addr_space(SocketAddrSpace::new(
⋮----
pub async fn start_async_with_socket_addr_space(
⋮----
.start_async_with_mint_address(&mint_keypair, socket_addr_space)
⋮----
.unwrap_or_else(|err| panic!("Test validator failed to start: {err}"));
⋮----
pub struct TestValidator {
⋮----
impl TestValidator {
fn start_with_config(
⋮----
.tpu_enable_udp(tpu_enable_udp)
.fee_rate_governor(FeeRateGovernor::new(target_lamports_per_signature, 0))
.rent(Rent {
⋮----
.faucet_addr(faucet_addr)
.start_with_mint_address(mint_address, socket_addr_space)
.expect("validator start failed");
⋮----
async fn async_start_with_config(
⋮----
.start_async_with_mint_address(mint_keypair, socket_addr_space)
⋮----
.expect("validator start failed")
⋮----
pub fn with_no_fees(
⋮----
pub fn with_no_fees_udp(
⋮----
pub fn with_custom_fees(
⋮----
pub async fn async_with_no_fees(
⋮----
pub async fn async_with_custom_fees(
⋮----
fn initialize_ledger(
⋮----
let mut feature_set = FeatureSet::default().inactive().clone();
⋮----
if feature_set.remove(feature) {
info!("Feature for {feature:?} deactivated")
⋮----
warn!("Feature {feature:?} set for deactivation is not a known Feature public key",)
⋮----
let mut accounts = config.accounts.clone();
⋮----
accounts.entry(address).or_insert(account);
⋮----
feature_set.contains(feature_id)
⋮----
&[upgradeable_program.program_id.as_ref()],
⋮----
upgrade_authority_address: Some(upgradeable_program.upgrade_authority),
⋮----
program_data.extend_from_slice(&data);
accounts.insert(
⋮----
lamports: Rent::default().minimum_balance(program_data.len()).max(1),
⋮----
lamports: Rent::default().minimum_balance(data.len()).max(1),
⋮----
let mut genesis_config = create_genesis_config_with_leader_ex_no_features(
⋮----
&validator_identity.pubkey(),
&validator_vote_account.pubkey(),
&validator_stake_account.pubkey(),
⋮----
config.fee_rate_governor.clone(),
config.rent.clone(),
⋮----
accounts.into_iter().collect(),
⋮----
.unwrap_or_else(EpochSchedule::without_warmup);
⋮----
None => create_new_tmp_ledger!(&genesis_config).0,
⋮----
return Ok(ledger_path.to_path_buf());
⋮----
let _ = create_new_ledger(
⋮----
.unwrap_or(MAX_GENESIS_ARCHIVE_UNPACKED_SIZE),
⋮----
.map_err(|err| {
format!(
⋮----
ledger_path.to_path_buf()
⋮----
write_keypair_file(
⋮----
ledger_path.join("validator-keypair.json").to_str().unwrap(),
⋮----
.join("stake-account-keypair.json")
.to_str()
.unwrap(),
⋮----
assert!(!TestValidatorGenesis::ledger_exists(&ledger_path));
⋮----
.join("vote-account-keypair.json")
⋮----
Ok(ledger_path)
⋮----
fn start(
⋮----
let preserve_ledger = config.ledger_path.is_some();
⋮----
read_keypair_file(ledger_path.join("validator-keypair.json").to_str().unwrap())?;
let validator_vote_account = read_keypair_file(
⋮----
bind_ip_addrs: BindIpAddrs::new(vec![bind_ip_addr])?,
gossip_port: config.node_config.gossip_addr.port(),
⋮----
num_tvu_receive_sockets: NonZero::new(1).unwrap(),
num_tvu_retransmit_sockets: NonZero::new(1).unwrap(),
⋮----
.expect("Number of QUIC endpoints can not be zero"),
⋮----
Node::new_with_external_ip(&validator_identity.pubkey(), validator_node_config);
let (rpc, rpc_pubsub) = config.rpc_ports.unwrap_or_else(|| {
⋮----
find_available_ports_in_range(bind_ip_addr, config.node_config.port_range)
⋮----
node.info.set_rpc((bind_ip_addr, rpc)).unwrap();
⋮----
.set_rpc_pubsub((bind_ip_addr, rpc_pubsub))
⋮----
let vote_account_address = validator_vote_account.pubkey();
let rpc_url = format!("http://{}", node.info.rpc().unwrap());
let rpc_pubsub_url = format!("ws://{}/", node.info.rpc_pubsub().unwrap());
let tpu = node.info.tpu(Protocol::UDP).unwrap();
let gossip = node.info.gossip().unwrap();
⋮----
let mut authorized_voter_keypairs = config.authorized_voter_keypairs.write().unwrap();
⋮----
.any(|x| x.pubkey() == vote_account_address)
⋮----
authorized_voter_keypairs.push(Arc::new(validator_vote_account))
⋮----
index: Some(AccountsIndexConfig::default()),
account_indexes: Some(config.rpc_config.account_indexes.clone()),
⋮----
.map(|compute_unit_limit| ComputeBudget {
⋮----
.contains(&raise_cpi_nesting_limit_to_8::id()),
⋮----
.contains(&increase_cpi_account_info_limit::id()),
⋮----
on_start_geyser_plugin_config_files: config.geyser_plugin_config_files.clone(),
rpc_addrs: Some((
⋮----
node.info.rpc().unwrap().port(),
⋮----
node.info.rpc_pubsub().unwrap().port(),
⋮----
rpc_config: config.rpc_config.clone(),
pubsub_config: config.pubsub_config.clone(),
account_paths: vec![
⋮----
NonZeroU64::new(100).unwrap(),
⋮----
bank_snapshots_dir: ledger_path.join(BANK_SNAPSHOTS_DIR),
full_snapshot_archives_dir: ledger_path.to_path_buf(),
incremental_snapshot_archives_dir: ledger_path.to_path_buf(),
⋮----
validator_exit: config.validator_exit.clone(),
⋮----
staked_nodes_overrides: config.staked_nodes_overrides.clone(),
⋮----
merkle_root_upload_authority: validator_identity.pubkey(),
⋮----
bam_url: config.bam_url.clone(),
⋮----
validator_config.tower_storage = tower_storage.clone();
⋮----
let validator = Some(Validator::new(
⋮----
config.authorized_voter_keypairs.clone(),
vec![],
⋮----
config.start_progress.clone(),
⋮----
config.admin_rpc_service_post_init.clone(),
⋮----
async fn wait_for_nonzero_fees(&self) {
⋮----
self.rpc_url.clone(),
⋮----
vec![AccountMeta::new(Pubkey::new_unique(), true)],
⋮----
println!("Waiting for fees to stabilize {num_tries:?}...");
match rpc_client.get_latest_blockhash().await {
⋮----
match rpc_client.get_fee_for_message(&message).await {
⋮----
warn!("get_fee_for_message() failed: {err:?}");
⋮----
warn!("get_latest_blockhash() failed: {err:?}");
⋮----
sleep(Duration::from_millis(DEFAULT_MS_PER_SLOT)).await;
⋮----
async fn wait_for_upgradeable_programs_deployed(
⋮----
let mut deployed = vec![false; upgradeable_programs.len()];
⋮----
let blockhash = rpc_client.get_latest_blockhash().await.unwrap();
for (program_id, is_deployed) in upgradeable_programs.iter().zip(deployed.iter_mut()) {
⋮----
accounts: vec![],
data: vec![],
⋮----
Some(&payer.pubkey()),
⋮----
match rpc_client.simulate_transaction(&transaction).await {
⋮----
let err_string = format!("{e:?}");
if err_string.contains("Program is not deployed") {
debug!("{program_id:?} - not deployed");
} else if err_string.contains("AccountNotFound") {
return Err(RpcClientError::from(
⋮----
debug!("{program_id:?} - Unexpected error: {e:?}");
⋮----
warn!("Failed to simulate transaction: {e:?}");
⋮----
return Err(e);
⋮----
if deployed.iter().all(|&deployed| deployed) {
return Ok(());
⋮----
println!("Waiting for programs to be fully deployed {attempt} ...");
⋮----
panic!("Timeout waiting for program to become usable");
⋮----
pub fn tpu(&self) -> &SocketAddr {
⋮----
pub fn gossip(&self) -> &SocketAddr {
⋮----
pub fn rpc_url(&self) -> String {
self.rpc_url.clone()
⋮----
pub fn rpc_pubsub_url(&self) -> String {
self.rpc_pubsub_url.clone()
⋮----
pub fn vote_account_address(&self) -> Pubkey {
⋮----
pub fn get_rpc_client(&self) -> RpcClient {
RpcClient::new_with_commitment(self.rpc_url.clone(), CommitmentConfig::processed())
⋮----
pub fn get_async_rpc_client(&self) -> nonblocking::rpc_client::RpcClient {
⋮----
pub fn join(mut self) {
if let Some(validator) = self.validator.take() {
validator.join();
⋮----
pub fn cluster_info(&self) -> Arc<ClusterInfo> {
self.validator.as_ref().unwrap().cluster_info.clone()
⋮----
pub fn bank_forks(&self) -> Arc<RwLock<BankForks>> {
self.validator.as_ref().unwrap().bank_forks.clone()
⋮----
pub fn repair_whitelist(&self) -> Arc<RwLock<HashSet<Pubkey>>> {
⋮----
impl Drop for TestValidator {
fn drop(&mut self) {
⋮----
validator.close();
⋮----
remove_dir_all(&self.ledger_path).unwrap_or_else(|err| {
panic!(
⋮----
mod test {
⋮----
fn get_health() {
let (test_validator, _payer) = TestValidatorGenesis::default().start();
let rpc_client = test_validator.get_rpc_client();
rpc_client.get_health().expect("health");
⋮----
async fn nonblocking_get_health() {
let (test_validator, _payer) = TestValidatorGenesis::default().start_async().await;
let rpc_client = test_validator.get_async_rpc_client();
rpc_client.get_health().await.expect("health");
⋮----
fn test_upgradeable_program_deploayment() {
⋮----
.add_program("../programs/bpf-loader-tests/noop", program_id)
.start();
⋮----
let blockhash = rpc_client.get_latest_blockhash().unwrap();
⋮----
assert!(rpc_client
⋮----
async fn test_nonblocking_upgradeable_program_deploayment() {
⋮----
.start_async()
⋮----
async fn document_tokio_panic() {
let (_test_validator, _payer) = TestValidatorGenesis::default().start();
⋮----
async fn test_deactivate_features() {
let mut control = FeatureSet::default().inactive().clone();
⋮----
.for_each(|feature| {
control.remove(&feature);
deactivate_features.push(feature);
⋮----
let control: Vec<Pubkey> = control.into_iter().collect();
⋮----
.deactivate_features(&deactivate_features)
⋮----
.get_multiple_accounts(&deactivate_features)
⋮----
assert!(f.is_none());
⋮----
for chunk in control.chunks(100) {
let active_feature_accounts = rpc_client.get_multiple_accounts(chunk).await.unwrap();
⋮----
let account = f.unwrap();
let feature_state: Feature = bincode::deserialize(account.data()).unwrap();
assert!(feature_state.activated_at.is_some());
⋮----
async fn test_override_feature_account() {
⋮----
.deactivate_features(&[with_deactivate_flag])
.add_accounts([
(with_deactivate_flag, account()),
(without_deactivate_flag, account()),
⋮----
.get_multiple_accounts(&[with_deactivate_flag, without_deactivate_flag])
⋮----
let overridden_account = our_accounts[0].as_ref().unwrap();
assert_eq!(overridden_account.lamports, 100_000);
assert_eq!(overridden_account.data.len(), 0);
assert_eq!(overridden_account.owner, owner);
let feature_account = our_accounts[1].as_ref().unwrap();
assert_eq!(feature_account.owner, solana_sdk_ids::feature::id());
let feature_state: Feature = bincode::deserialize(feature_account.data()).unwrap();
⋮----
async fn test_core_bpf_programs() {
⋮----
.get_multiple_accounts(&[
⋮----
let account = fetched_programs[0].as_ref().unwrap();
assert_eq!(account.owner, solana_sdk_ids::bpf_loader_upgradeable::id());
assert!(account.executable);
let account = fetched_programs[1].as_ref().unwrap();
⋮----
let account = fetched_programs[2].as_ref().unwrap();
⋮----
let account = fetched_programs[3].as_ref().unwrap();
⋮----
async fn test_wait_for_program_with_unfunded_payer() {
⋮----
.wait_for_upgradeable_programs_deployed(&[&program_id], &unfunded_payer)
⋮----
let err = result.unwrap_err();
assert!(matches!(

================
File: test-validator/Cargo.toml
================
[package]
name = "solana-test-validator"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
agave-snapshots = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
crossbeam-channel = { workspace = true }
log = { workspace = true }
serde_json = { workspace = true }
solana-account = { workspace = true }
solana-accounts-db = { workspace = true }
solana-cli-output = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-compute-budget = { workspace = true }
solana-core = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-feature-gate-interface = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-genesis-utils = { workspace = true }
solana-geyser-plugin-manager = { workspace = true }
solana-gossip = { workspace = true }
solana-inflation = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true }
solana-loader-v3-interface = { workspace = true }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-net-utils = { workspace = true }
solana-program-binaries = { workspace = true }
solana-program-test = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-rpc = { workspace = true }
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-runtime = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-validator-exit = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[dev-dependencies]
solana-sdk-ids = { workspace = true }

================
File: thread-manager/examples/common/mod.rs
================
pub async fn axum_main(port: u16, ready: Sender<()>) {
async fn root() -> &'static str {
⋮----
let app = Router::new().route("/", get(root));
⋮----
.unwrap();
info!("Server on port {port} ready");
ready.send(()).unwrap();
⋮----
axum::serve(listener, app).into_future(),
⋮----
v.unwrap();
⋮----
info!("Terminating server on port {port}");
⋮----
pub struct Stats {
⋮----
pub async fn workload_main(ports: &[u16], tasks: usize) -> anyhow::Result<Stats> {
struct ControlBlock {
⋮----
async fn connection(port: u16, control_block: Arc<ControlBlock>) -> anyhow::Result<()> {
⋮----
while control_block.start_time.elapsed() < Duration::from_secs(TEST_SECONDS) {
⋮----
.uri(path)
.method("GET")
.body(Body::from(""))?;
⋮----
let res = timeout(Duration::from_millis(100), request_sender.send_request(req)).await;
⋮----
let _ = res.body();
if res.status() != 200 {
⋮----
.fetch_add(start.elapsed().as_micros() as usize, Ordering::Relaxed);
control_block.requests.fetch_add(1, Ordering::Relaxed);
request_sender.ready().await?;
⋮----
Ok(())
⋮----
info!("Starting load generation on port {port}");
⋮----
join_set.spawn(connection(*port, control_block.clone()));
⋮----
while let Some(jr) = join_set.join_next().await {
⋮----
let requests = control_block.requests.load(Ordering::Relaxed);
let latency_accumulator_us = control_block.cumulative_latency_us.load(Ordering::Relaxed);
Ok(Stats {

================
File: thread-manager/examples/core_contention_basics.rs
================
mod common;
⋮----
fn main() -> anyhow::Result<()> {
env_logger::Builder::from_env(env_logger::Env::default().default_filter_or("info")).init();
⋮----
info!("===================");
info!("Running {exp}");
let mut conf_file = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
conf_file.push(exp);
⋮----
std::fs::File::open(conf_file)?.read_to_string(&mut buf)?;
⋮----
let manager = ThreadManager::new(cfg).unwrap();
let tokio1 = manager.get_tokio("axum1");
tokio1.start_metrics_sampling(Duration::from_secs(1));
let tokio2 = manager.get_tokio("axum2");
tokio2.start_metrics_sampling(Duration::from_secs(1));
⋮----
"LoadGenerator".to_owned(),
⋮----
scope.spawn(|| {
tokio1.tokio.block_on(axum_main(8888, tx1));
⋮----
tokio2.tokio.block_on(axum_main(8889, tx2));
⋮----
rx1.blocking_recv().unwrap();
rx2.blocking_recv().unwrap();
⋮----
scope.spawn(|| workload_runtime.block_on(workload_main(&[8888, 8889], 1000)));
join_handle.join().expect("Load generator crashed!")
⋮----
info!("Results are: {results:?}");
⋮----
Ok(())

================
File: thread-manager/examples/core_contention_contending_set.toml
================
[native_configs]

[rayon_configs]

[tokio_configs.axum1]
worker_threads = 8
max_blocking_threads = 1
core_allocation.DedicatedCoreSet = { min = 0, max = 8 }

[tokio_configs.axum2]
worker_threads = 8
max_blocking_threads = 1
core_allocation.DedicatedCoreSet = { min = 0, max = 8 }

================
File: thread-manager/examples/core_contention_dedicated_set.toml
================
[native_configs]

[rayon_configs]

[tokio_configs.axum1]
worker_threads = 4
max_blocking_threads = 1
core_allocation.DedicatedCoreSet = { min = 0, max = 4 }

[tokio_configs.axum2]
worker_threads = 4
max_blocking_threads = 1
core_allocation.DedicatedCoreSet = { min = 4, max = 8 }

================
File: thread-manager/examples/core_contention_sweep.rs
================
mod common;
⋮----
fn make_config_shared(cc: usize) -> ThreadManagerConfig {
⋮----
let tokio_cfg_2 = tokio_cfg_1.clone();
⋮----
("axum1".into(), tokio_cfg_1),
("axum2".into(), tokio_cfg_2),
⋮----
fn make_config_dedicated(core_count: usize) -> ThreadManagerConfig {
⋮----
enum Regime {
⋮----
impl Regime {
⋮----
struct Results {
⋮----
fn main() -> anyhow::Result<()> {
env_logger::Builder::from_env(env_logger::Env::default().default_filter_or("info")).init();
⋮----
info!("===================");
info!("Running {core_count} cores under {regime:?}");
⋮----
manager = ThreadManager::new(make_config_shared(core_count)).unwrap();
(manager.get_tokio("axum1"), manager.get_tokio("axum2"))
⋮----
manager = ThreadManager::new(make_config_dedicated(core_count)).unwrap();
⋮----
"LoadGenerator".to_owned(),
⋮----
s.spawn(|| {
tokio1.start_metrics_sampling(Duration::from_secs(1));
tokio1.tokio.block_on(axum_main(8888, tx1));
⋮----
Regime::Single => s.spawn(|| {
rx1.blocking_recv().unwrap();
workload_runtime.block_on(workload_main(&[8888, 8888], 3000))
⋮----
tokio2.start_metrics_sampling(Duration::from_secs(1));
tokio2.tokio.block_on(axum_main(8889, tx2));
⋮----
rx2.blocking_recv().unwrap();
workload_runtime.block_on(workload_main(&[8888, 8889], 3000))
⋮----
jh.join().expect("Some of the threads crashed!")
⋮----
info!("Results are: {measurement:?}");
results.latencies_s.push(measurement.latency_s);
⋮----
.push(measurement.requests_per_second);
⋮----
all_results.insert(format!("{regime:?}"), results);
⋮----
println!("{}", serde_json::to_string_pretty(&all_results)?);
Ok(())

================
File: thread-manager/src/lib.rs
================
pub mod native_thread_runtime;
pub mod policy;
pub mod rayon_runtime;
pub mod tokio_runtime;
⋮----
pub struct ThreadManagerInner {
⋮----
impl ThreadManagerInner {
fn populate_mappings(&mut self, config: &ThreadManagerConfig) {
for name in config.native_configs.keys() {
⋮----
.insert(name.clone(), name.clone());
⋮----
for (k, v) in config.native_runtime_mapping.iter() {
self.native_runtime_mapping.insert(k.clone(), v.clone());
⋮----
for name in config.tokio_configs.keys() {
⋮----
for (k, v) in config.tokio_runtime_mapping.iter() {
self.tokio_runtime_mapping.insert(k.clone(), v.clone());
⋮----
for name in config.rayon_configs.keys() {
⋮----
for (k, v) in config.rayon_runtime_mapping.iter() {
self.rayon_runtime_mapping.insert(k.clone(), v.clone());
⋮----
pub struct ThreadManager {
⋮----
impl Deref for ThreadManager {
type Target = ThreadManagerInner;
fn deref(&self) -> &Self::Target {
⋮----
pub struct ThreadManagerConfig {
⋮----
impl Default for ThreadManagerConfig {
fn default() -> Self {
⋮----
native_configs: HashMap::from([("default".to_owned(), NativeConfig::default())]),
⋮----
rayon_configs: HashMap::from([("default".to_owned(), RayonConfig::default())]),
⋮----
tokio_configs: HashMap::from([("default".to_owned(), TokioConfig::default())]),
⋮----
impl ThreadManager {
fn lookup<'a, T>(
⋮----
match mapping.get(name) {
Some(n) => runtimes.get(n),
None => match mapping.get("default") {
⋮----
runtimes.get(n)
⋮----
pub fn try_get_native(&self, name: &str) -> Option<&NativeThreadRuntime> {
self.lookup(
⋮----
pub fn get_native(&self, name: &str) -> &NativeThreadRuntime {
if let Some(runtime) = self.try_get_native(name) {
⋮----
panic!("Native thread pool for {name} can not be found!");
⋮----
pub fn try_get_rayon(&self, name: &str) -> Option<&RayonRuntime> {
self.lookup(name, &self.rayon_runtime_mapping, &self.rayon_runtimes)
⋮----
pub fn get_rayon(&self, name: &str) -> &RayonRuntime {
if let Some(runtime) = self.try_get_rayon(name) {
⋮----
panic!("Rayon thread pool for {name} can not be found!");
⋮----
pub fn try_get_tokio(&self, name: &str) -> Option<&TokioRuntime> {
self.lookup(name, &self.tokio_runtime_mapping, &self.tokio_runtimes)
⋮----
pub fn get_tokio(&self, name: &str) -> &TokioRuntime {
if let Some(runtime) = self.try_get_tokio(name) {
⋮----
panic!("Tokio thread pool for {name} can not be found!");
⋮----
pub fn set_process_affinity(config: &ThreadManagerConfig) -> anyhow::Result<Vec<usize>> {
let chosen_cores_mask = config.default_core_allocation.as_core_mask_vector();
⋮----
Ok(chosen_cores_mask)
⋮----
pub fn new(config: ThreadManagerConfig) -> anyhow::Result<Self> {
⋮----
manager.populate_mappings(&config);
for (name, cfg) in config.native_configs.iter() {
let nrt = NativeThreadRuntime::new(name.clone(), cfg.clone());
manager.native_thread_runtimes.insert(name.clone(), nrt);
⋮----
for (name, cfg) in config.rayon_configs.iter() {
let rrt = RayonRuntime::new(name.clone(), cfg.clone())?;
manager.rayon_runtimes.insert(name.clone(), rrt);
⋮----
for (name, cfg) in config.tokio_configs.iter() {
let tokiort = TokioRuntime::new(name.clone(), cfg.clone())?;
core_allocations.insert(name.clone(), cfg.core_allocation.as_core_mask_vector());
manager.tokio_runtimes.insert(name.clone(), tokiort);
⋮----
Ok(Self {
⋮----
mod tests {
⋮----
fn test_config_files() {
⋮----
println!("Loading config {exp}");
let mut conffile = std::path::PathBuf::from(env!("CARGO_MANIFEST_DIR"));
conffile.push(exp);
⋮----
.unwrap()
.read_to_string(&mut buf)
.unwrap();
let cfg: ThreadManagerConfig = toml::from_str(&buf).unwrap();
println!("{cfg:?}");
⋮----
fn validate_affinity(expect_cores: &[usize], error_msg: &str) {
let affinity = affinity::get_thread_affinity().unwrap();
assert_eq!(affinity, expect_cores, "{error_msg}");
⋮----
fn test_thread_priority() {
⋮----
"high".to_owned(),
⋮----
"default".to_owned(),
⋮----
"low".to_owned(),
⋮----
let manager = ThreadManager::new(conf).unwrap();
let high = manager.get_native("high");
let low = manager.get_native("low");
let default = manager.get_native("default");
high.spawn(move || {
⋮----
thread_priority::get_thread_priority(thread_priority::thread_native_id()).unwrap();
assert_eq!(
⋮----
.join()
⋮----
low.spawn(move || {
⋮----
.spawn(move || {
⋮----
fn test_process_affinity() {
⋮----
"pool1".to_owned(),
⋮----
native_runtime_mapping: HashMap::from([("test".to_owned(), "pool1".to_owned())]),
⋮----
let runtime = manager.get_native("test");
⋮----
.spawn(|| {
validate_affinity(&[0, 1, 2, 3], "Managed thread allocation should be 0-3");
⋮----
validate_affinity(&[4, 5, 6, 7], "Default thread allocation should be 4-7");
⋮----
validate_affinity(
⋮----
inner_thread.join().unwrap();
⋮----
thread1.join().unwrap();
thread2.join().unwrap();
⋮----
fn test_rayon_affinity() {
⋮----
"test".to_owned(),
⋮----
let rayon_runtime = manager.get_rayon("test");
let _rr = rayon_runtime.rayon_pool.broadcast(|ctx| {
println!("Rayon thread {} reporting", ctx.index());
validate_affinity(&[1, 2, 3], "Rayon thread allocation should still be 1-3");

================
File: thread-manager/src/native_thread_runtime.rs
================
pub struct NativeConfig {
⋮----
impl Default for NativeConfig {
fn default() -> Self {
⋮----
policy: "OTHER".to_owned(),
⋮----
pub struct NativeThreadRuntimeInner {
⋮----
pub struct NativeThreadRuntime {
⋮----
impl Deref for NativeThreadRuntime {
type Target = NativeThreadRuntimeInner;
fn deref(&self) -> &Self::Target {
⋮----
pub struct JoinHandle<T> {
⋮----
fn join_inner(&mut self) -> std::thread::Result<T> {
match self.std_handle.take() {
⋮----
let result = jh.join();
let rc = self.running_count.fetch_sub(1, Ordering::Relaxed);
datapoint_info!("thread-manager-native", ("threads-running", rc, i64),);
⋮----
panic!("Thread already joined");
⋮----
pub fn join(mut self) -> std::thread::Result<T> {
self.join_inner()
⋮----
pub fn is_finished(&self) -> bool {
⋮----
Some(ref jh) => jh.is_finished(),
⋮----
impl<T> Drop for JoinHandle<T> {
fn drop(&mut self) {
if self.std_handle.is_some() {
warn!(
⋮----
self.join_inner().expect("Child thread panicked");
⋮----
impl NativeThreadRuntime {
pub fn new(name: String, cfg: NativeConfig) -> Self {
debug_assert!(name.len() < MAX_THREAD_NAME_CHARS, "Thread name too long");
⋮----
pub fn spawn<F, T>(&self, f: F) -> anyhow::Result<JoinHandle<T>>
⋮----
let n = self.id_count.fetch_add(1, Ordering::Relaxed);
let name = format!("{}-{}", &self.name, n);
self.spawn_named(name, f)
⋮----
pub fn spawn_named<F, T>(&self, name: String, f: F) -> anyhow::Result<JoinHandle<T>>
⋮----
let spawned = self.running_count.load(Ordering::Relaxed);
⋮----
bail!("All allowed threads in this pool are already spawned");
⋮----
let core_alloc = self.config.core_allocation.clone();
⋮----
let policy = parse_policy(&self.config.policy);
let chosen_cores_mask = Mutex::new(self.config.core_allocation.as_core_mask_vector());
⋮----
.name(name)
.stack_size(self.config.stack_size_bytes)
.spawn(move || {
apply_policy(&core_alloc, policy, priority, &chosen_cores_mask);
f()
⋮----
let rc = self.running_count.fetch_add(1, Ordering::Relaxed);
datapoint_info!("thread-manager-native", ("threads-running", rc as i64, i64),);
Ok(JoinHandle {
std_handle: Some(jh),
running_count: self.running_count.clone(),
⋮----
pub fn new_for_tests(name: &str) -> Self {
Self::new(name.to_owned(), NativeConfig::default())

================
File: thread-manager/src/policy.rs
================
pub enum CoreAllocation {
⋮----
impl CoreAllocation {
pub fn as_core_mask_vector(&self) -> Vec<usize> {
⋮----
CoreAllocation::PinnedCores { min, max } => (min..max).collect(),
CoreAllocation::DedicatedCoreSet { min, max } => (min..max).collect(),
CoreAllocation::OsDefault => Vec::from_iter(0..*CORE_COUNT.get_or_init(num_cpus::get)),
⋮----
pub(crate) fn apply_policy(
⋮----
apply_thread_scheduler_policy(policy, priority);
⋮----
.lock()
.expect("Can not lock core mask mutex");
⋮----
.pop()
.expect("Not enough cores provided for pinned allocation");
set_thread_affinity(&[core]);
⋮----
set_thread_affinity(&lg);

================
File: thread-manager/src/rayon_runtime.rs
================
pub struct RayonConfig {
⋮----
impl Default for RayonConfig {
fn default() -> Self {
⋮----
policy: "BATCH".to_owned(),
⋮----
pub struct RayonRuntimeInner {
⋮----
impl Deref for RayonRuntimeInner {
type Target = rayon::ThreadPool;
fn deref(&self) -> &Self::Target {
⋮----
pub struct RayonRuntime {
⋮----
impl Deref for RayonRuntime {
type Target = RayonRuntimeInner;
⋮----
self.inner.deref()
⋮----
impl RayonRuntime {
pub fn new(name: String, config: RayonConfig) -> anyhow::Result<Self> {
debug_assert!(name.len() < MAX_THREAD_NAME_CHARS, "Thread name too long");
let core_allocation = config.core_allocation.clone();
let chosen_cores_mask = Mutex::new(core_allocation.as_core_mask_vector());
⋮----
let policy = parse_policy(&config.policy);
⋮----
.num_threads(config.worker_threads)
.thread_name(move |i| format!("{}_{}", &name, i))
.stack_size(config.stack_size_bytes)
.start_handler(move |_idx| {
apply_policy(&core_allocation, policy, priority, &chosen_cores_mask);
⋮----
.build()?;
Ok(Self {
⋮----
pub fn new_for_tests(name: &str) -> Self {
Self::new(name.to_owned(), RayonConfig::default())
.expect("Failed to create rayon runtime for tests")

================
File: thread-manager/src/tokio_runtime.rs
================
pub struct TokioConfig {
⋮----
impl Default for TokioConfig {
fn default() -> Self {
⋮----
policy: "OTHER".to_owned(),
⋮----
pub struct TokioRuntime {
⋮----
impl Deref for TokioRuntime {
type Target = tokio::runtime::Runtime;
fn deref(&self) -> &Self::Target {
⋮----
impl TokioRuntime {
pub fn start_metrics_sampling(&self, period: Duration) {
let counters = self.counters.clone();
self.tokio.spawn(metrics_sampler(counters, period));
⋮----
pub fn new(name: String, cfg: TokioConfig) -> anyhow::Result<Self> {
debug_assert!(name.len() < MAX_THREAD_NAME_CHARS, "Thread name too long");
⋮----
let chosen_cores_mask = cfg.core_allocation.as_core_mask_vector();
let base_name = name.clone();
⋮----
builder.worker_threads(num_workers);
⋮----
namespace: format!("thread-manager-tokio-{}", &base_name).leak(),
⋮----
(num_workers.wrapping_add(cfg.max_blocking_threads)) as u64,
⋮----
.event_interval(cfg.event_interval)
.thread_name_fn(move || {
let id = atomic_id.fetch_add(1, Ordering::Relaxed);
format!("{base_name}-{id}")
⋮----
.on_thread_park({
let counters = counters.clone();
⋮----
counters.on_park();
⋮----
.on_thread_unpark({
⋮----
counters.on_unpark();
⋮----
.thread_stack_size(cfg.stack_size_bytes)
.enable_all()
.max_blocking_threads(cfg.max_blocking_threads);
let c = cfg.clone();
⋮----
builder.on_thread_start(move || {
⋮----
.get_native_id()
.expect("Can not get thread id for newly created thread");
apply_policy(
⋮----
parse_policy(&c.policy),
⋮----
Ok(TokioRuntime {
tokio: builder.build()?,
config: cfg.clone(),
⋮----
pub fn new_for_tests() -> Self {
⋮----
TokioRuntime::new("solNetTest".to_owned(), cfg.clone())
.expect("Failed to create Tokio runtime for tests")
⋮----
pub struct ThreadCounters {
⋮----
impl ThreadCounters {
pub fn on_park(&self) {
self.active_threads_cnt.fetch_sub(1, Ordering::Relaxed);
⋮----
pub fn on_unpark(&self) {
self.active_threads_cnt.fetch_add(1, Ordering::Relaxed);
⋮----
async fn metrics_sampler(counters: Arc<ThreadCounters>, period: Duration) {
⋮----
interval.tick().await;
let active = counters.active_threads_cnt.load(Ordering::Relaxed) as i64;
let parked = (counters.total_threads_cnt as i64).saturating_sub(active);
datapoint_info!(

================
File: thread-manager/Cargo.toml
================
[package]
name = "agave-thread-manager"
description = "Thread pool manager for agave"

version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
anyhow = { workspace = true }
cfg-if = { workspace = true }
log = { workspace = true }
num_cpus = { workspace = true }
rayon = { workspace = true }
serde = { workspace = true, features = ["derive"] }
solana-metrics = { workspace = true }
thread-priority = { workspace = true }
tokio = { workspace = true, features = ["time", "rt-multi-thread"] }

[target.'cfg(target_os = "linux")'.dependencies]
affinity = "0.1.2"

[dev-dependencies]
agave-thread-manager = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
axum = { workspace = true }
env_logger = { workspace = true }
hyper = { workspace = true, features = ["http1", "client", "stream", "tcp"] }
serde_json = { workspace = true }
toml = { workspace = true }
tower = { workspace = true }

================
File: thread-manager/README.md
================
# thread-manager
Balances machine resources across multiple threaded runtimes to optimize performance.
The goal is to manage thread contention effectively between different parts of the code, ensuring each can benefit from tailored management strategies.
For example, we may want to have cores 1-4 handling networking via
Tokio, core 5 handling file IO via Tokio, cores 9-16 allocated for
Rayon thread pool, and cores 6-8 available for general use by std::thread.
This will minimize contention for CPU caches and context switches that
would occur if Rayon was entirely unaware it was running side-by-side with
tokio, and each was to spawn as many threads as there are cores.

## Thread pool mapping
Thread manager will, by default, look for a particular named pool, e.g. "solGossip".
Matching is done independently for each type of runtime.
However, if no named pool is found, it will fall back to the "default" thread pool
of the same type (if specified in the config). If the default pool is not specified,
thread pool lookup will fail.

Multiple names can point to the same pool. For example, "solGossipConsume" and
"solSigverify" can both be executed on the same rayon pool named "rayonSigverify".
This, in principle, allows some degree of runtime sharing between different crates
in the codebase without having to manually patch the pointers through.

# Supported threading models
## Affinity
All threading models allow setting core affinity, but only on Linux.

For core affinity you can set e.g.
```toml
core_allocation.DedicatedCoreSet = { min = 16, max = 64 }
```
to pin the pool to cores 16-64.

## Scheduling policy and priority
You can configure the thread scheduling policy and priority if desired. Keep in mind that this will likely require
```bash
 sudo setcap cap_sys_nice+ep
 ```
or root privileges to run the resulting process.
To see which policies are supported check (the sources)[./src/policy.rs]
If you use realtime policies, priority to values from 1 (lowest) to 99 (highest) are possible.

## Tokio
You can create multiple Tokio runtimes, each with its own dedicated pool of CPU cores. The number of worker and blocking threads, along with thread priorities for the pool, can be fully customized.

## Native
Native threads (`std::thread`) can be spawned from managed pools, allowing them to inherit specific
affinity from the pool, along with providing control over the total number of threads in each pool.

## Rayon
Rayon already manages thread pools well enough, all thread_manager does on top is enforce affinity and
priority for rayon threads. Normally one would only ever have one rayon pool, but for priority allocations
one may want to spawn many rayon pools.

# Limitations

 * Thread pools can only be created at process startup
 * Once thread pool is created, its policy can not be modified at runtime
 * Thread affinity & priority are not supported outside of linux
 * Thread priority generally requires kernel level support and extra capabilities

# TODO:

 * even more tests
 * better thread priority support


# Examples
 * core_contention_basics will demonstrate why core contention is bad, and how thread configs can help
 * core_contention_sweep will sweep across a range of core counts to show how benefits scale with core counts

================
File: tls-utils/src/config.rs
================
pub fn tls_client_config_builder() -> ConfigBuilder<ClientConfig, WantsClientCert> {
⋮----
.with_safe_default_protocol_versions()
.unwrap()
.dangerous()
.with_custom_certificate_verifier(crate::SkipServerVerification::new())
⋮----
pub fn tls_server_config_builder() -> ConfigBuilder<ServerConfig, WantsServerCert> {
⋮----
.with_client_cert_verifier(crate::SkipClientVerification::new())

================
File: tls-utils/src/crypto_provider.rs
================
pub fn crypto_provider() -> CryptoProvider {
⋮----
.retain(|kx| kx.name() == NamedGroup::X25519);

================
File: tls-utils/src/lib.rs
================
mod config;
⋮----
mod crypto_provider;
⋮----
mod tls_certificates;
⋮----
mod quic_client_certificate;
⋮----
mod skip_server_verification;
pub use skip_server_verification::SkipServerVerification;
mod skip_client_verification;
pub use skip_client_verification::SkipClientVerification;

================
File: tls-utils/src/quic_client_certificate.rs
================
pub struct QuicClientCertificate {
⋮----
impl QuicClientCertificate {
pub fn new(keypair: Option<&Keypair>) -> Self {
⋮----
let (certificate, key) = new_dummy_x509_certificate(keypair);

================
File: tls-utils/src/skip_client_verification.rs
================
pub struct SkipClientVerification(Arc<CryptoProvider>);
impl SkipClientVerification {
pub fn new() -> Arc<Self> {
Arc::new(Self(Arc::new(crypto_provider())))
⋮----
impl ClientCertVerifier for SkipClientVerification {
fn verify_client_cert(
⋮----
Ok(ClientCertVerified::assertion())
⋮----
fn root_hint_subjects(&self) -> &[DistinguishedName] {
⋮----
fn verify_tls12_signature(
⋮----
fn verify_tls13_signature(
⋮----
fn supported_verify_schemes(&self) -> Vec<SignatureScheme> {
self.0.signature_verification_algorithms.supported_schemes()
⋮----
fn offer_client_auth(&self) -> bool {
⋮----
fn client_auth_mandatory(&self) -> bool {
self.offer_client_auth()

================
File: tls-utils/src/skip_server_verification.rs
================
pub struct SkipServerVerification(Arc<CryptoProvider>);
impl SkipServerVerification {
pub fn new() -> Arc<Self> {
Arc::new(Self(Arc::new(crypto_provider())))
⋮----
impl ServerCertVerifier for SkipServerVerification {
fn verify_tls12_signature(
⋮----
verify_tls12_signature(
⋮----
fn verify_tls13_signature(
⋮----
verify_tls13_signature(
⋮----
fn supported_verify_schemes(&self) -> Vec<SignatureScheme> {
self.0.signature_verification_algorithms.supported_schemes()
⋮----
fn verify_server_cert(
⋮----
Ok(ServerCertVerified::assertion())
⋮----
impl Debug for SkipServerVerification {
fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
f.debug_struct("SkipServerVerification")
.finish_non_exhaustive()

================
File: tls-utils/src/tls_certificates.rs
================
pub fn new_dummy_x509_certificate(
⋮----
let keypair_secret_bytes = keypair.secret_bytes();
let keypair_secret_len = keypair_secret_bytes.len();
⋮----
panic!("Unexpected secret key length!");
⋮----
.len()
.checked_add(keypair_secret_len)
.expect("Unexpected secret key length!");
⋮----
key_pkcs8_der.extend_from_slice(&PKCS8_PREFIX);
key_pkcs8_der.extend_from_slice(keypair_secret_bytes);
⋮----
cert_der.extend_from_slice(&[
⋮----
cert_der.extend_from_slice(&keypair.pubkey().to_bytes());
⋮----
rustls::pki_types::PrivateKeyDer::try_from(key_pkcs8_der).unwrap(),
⋮----
pub fn get_pubkey_from_tls_certificate(
⋮----
let (_, cert) = X509Certificate::from_der(der_cert.as_ref()).ok()?;
match cert.public_key().parsed().ok()? {
PublicKey::Unknown(key) => Pubkey::try_from(key).ok(),
⋮----
pub fn socket_addr_to_quic_server_name(peer: SocketAddr) -> String {
format!("{}.{}.sol", peer.ip(), peer.port())
⋮----
mod tests {
⋮----
fn test_socket_addr_to_quic_server_name() {
⋮----
assert_eq!(socket_addr_to_quic_server_name(socket), "1.2.3.4.8004.sol");
⋮----
fn test_generate_tls_certificate() {
⋮----
let (cert, _) = new_dummy_x509_certificate(&keypair);
if let Some(pubkey) = get_pubkey_from_tls_certificate(&cert) {
assert_eq!(pubkey, keypair.pubkey());
⋮----
panic!("Failed to get certificate pubkey");

================
File: tls-utils/Cargo.toml
================
[package]
name = "solana-tls-utils"
description = "Solana TLS utilities"
documentation = "https://docs.rs/solana-tls-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
rustls = { workspace = true, features = ["ring"] }
solana-keypair = { workspace = true }
solana-pubkey = { workspace = true }
solana-signer = { workspace = true }
x509-parser = { workspace = true }

[lints]
workspace = true

================
File: tls-utils/README
================
A collection of utility functions and structures needed to bridge the conceptual gap between conventional TLS security models in protocols like QUIC and Agave use cases

================
File: tokens/src/arg_parser.rs
================
fn get_matches<'a, I, T>(args: I) -> ArgMatches<'a>
⋮----
let default_config_file = CONFIG_FILE.as_ref().unwrap();
App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.short("C")
.long("config")
.takes_value(true)
.value_name("FILEPATH")
.default_value(default_config_file)
.help("Config file"),
⋮----
.short("u")
.long("url")
.value_name("URL_OR_MONIKER")
⋮----
.global(true)
.validator(is_url_or_moniker)
.help(
⋮----
.subcommand(
⋮----
.about("Distribute SOL")
⋮----
.long("db-path")
.required(true)
⋮----
.value_name("FILE")
⋮----
.long("input-csv")
⋮----
.help("Input CSV file"),
⋮----
.long("transfer-amount")
⋮----
.value_name("AMOUNT")
.validator(is_amount)
.help("The amount to send to each recipient, in SOL"),
⋮----
.long("dry-run")
.help("Do not execute any transfers"),
⋮----
.long("output-path")
.short("o")
⋮----
.help("Write the transaction log to this file"),
⋮----
.long("from")
⋮----
.value_name("SENDING_KEYPAIR")
.validator(is_valid_signer)
.help("Keypair to fund accounts"),
⋮----
.long("fee-payer")
⋮----
.value_name("KEYPAIR")
⋮----
.help("Fee payer"),
⋮----
.about("Create stake accounts")
⋮----
.help("Allocations CSV file"),
⋮----
.default_value("1.0")
.long("unlocked-sol")
⋮----
.value_name("SOL_AMOUNT")
.help("Amount of SOL to put in system account to pay for fees"),
⋮----
.long("lockup-authority")
⋮----
.value_name("PUBKEY")
.validator(is_valid_pubkey)
.help("Lockup Authority Address"),
⋮----
.about("Split to stake accounts")
⋮----
.long("stake-account-address")
⋮----
.value_name("ACCOUNT_ADDRESS")
⋮----
.help("Stake Account Address"),
⋮----
.long("stake-authority")
⋮----
.help("Stake Authority Keypair"),
⋮----
.long("withdraw-authority")
⋮----
.help("Withdraw Authority Keypair"),
⋮----
.help("Lockup Authority Keypair"),
⋮----
.about("Distribute SPL tokens")
⋮----
.help("The amount of SPL tokens to send to each recipient"),
⋮----
.value_name("TOKEN_ACCOUNT_ADDRESS")
⋮----
.help("SPL token account to send from"),
⋮----
.long("owner")
⋮----
.value_name("TOKEN_ACCOUNT_OWNER_KEYPAIR")
⋮----
.help("SPL token account owner"),
⋮----
.about("Balance of each account")
⋮----
.about("Balance of SPL token associated accounts")
⋮----
.long("mint")
⋮----
.value_name("MINT_ADDRESS")
⋮----
.help("SPL token mint of distribution"),
⋮----
.about("Print the database to a CSV file")
⋮----
.help("Location of database to query"),
⋮----
.help("Output file"),
⋮----
.get_matches_from(args)
⋮----
fn parse_distribute_tokens_args(
⋮----
let mut wallet_manager = maybe_wallet_manager()?;
let signer_matches = ArgMatches::default(); // No default signer
let sender_keypair_str = value_t_or_exit!(matches, "sender_keypair", String);
let sender_keypair = signer_from_path(
⋮----
let fee_payer_str = value_t_or_exit!(matches, "fee_payer", String);
let fee_payer = signer_from_path(
⋮----
Ok(DistributeTokensArgs {
input_csv: value_t_or_exit!(matches, "input_csv", String),
transaction_db: value_t_or_exit!(matches, "db_path", String),
output_path: matches.value_of("output_path").map(|path| path.to_string()),
dry_run: matches.is_present("dry_run"),
⋮----
.value_of("transfer_amount")
.and_then(sol_str_to_lamports),
⋮----
fn parse_create_stake_args(
⋮----
let lockup_authority_str = value_t!(matches, "lockup_authority", String).ok();
⋮----
.map(|path| {
pubkey_from_path(
⋮----
.transpose()?;
⋮----
.value_of("unlocked_sol")
.and_then(sol_str_to_lamports)
.unwrap(),
⋮----
stake_args: Some(stake_args),
⋮----
fn parse_distribute_stake_args(
⋮----
let stake_account_address_str = value_t_or_exit!(matches, "stake_account_address", String);
let stake_account_address = pubkey_from_path(
⋮----
let stake_authority_str = value_t_or_exit!(matches, "stake_authority", String);
let stake_authority = signer_from_path(
⋮----
let withdraw_authority_str = value_t_or_exit!(matches, "withdraw_authority", String);
let withdraw_authority = signer_from_path(
⋮----
signer_from_path(
⋮----
let lockup_authority_address = lockup_authority.as_ref().map(|keypair| keypair.pubkey());
⋮----
sender_stake_args: Some(sender_stake_args),
⋮----
fn parse_distribute_spl_tokens_args(
⋮----
let token_owner_str = value_t_or_exit!(matches, "token_owner", String);
let token_owner = signer_from_path(
⋮----
let token_account_address_str = value_t_or_exit!(matches, "token_account_address", String);
let token_account_address = pubkey_from_path(
⋮----
spl_token_args: Some(SplTokenArgs {
⋮----
transfer_amount: value_of(matches, "transfer_amount"),
⋮----
fn parse_balances_args(matches: &ArgMatches<'_>) -> Result<BalancesArgs, Box<dyn Error>> {
⋮----
pubkey_of_signer(matches, "mint_address", &mut wallet_manager)?.map(|mint| SplTokenArgs {
⋮----
Ok(BalancesArgs {
⋮----
fn parse_transaction_log_args(matches: &ArgMatches<'_>) -> TransactionLogArgs {
⋮----
output_path: value_t_or_exit!(matches, "output_path", String),
⋮----
pub fn parse_args<I, T>(args: I) -> Result<Args, Box<dyn Error>>
⋮----
let matches = get_matches(args);
let config_file = matches.value_of("config_file").unwrap().to_string();
let url = matches.value_of("json_rpc_url").map(|x| x.to_string());
let command = match matches.subcommand() {
⋮----
Command::DistributeTokens(parse_distribute_tokens_args(matches)?)
⋮----
Command::DistributeTokens(parse_create_stake_args(matches)?)
⋮----
Command::DistributeTokens(parse_distribute_stake_args(matches)?)
⋮----
Command::DistributeTokens(parse_distribute_spl_tokens_args(matches)?)
⋮----
("balances", Some(matches)) => Command::Balances(parse_balances_args(matches)?),
("spl-token-balances", Some(matches)) => Command::Balances(parse_balances_args(matches)?),
⋮----
Command::TransactionLog(parse_transaction_log_args(matches))
⋮----
eprintln!("{}", matches.usage());
exit(1);
⋮----
Ok(args)

================
File: tokens/src/args.rs
================
pub struct SenderStakeArgs {
⋮----
pub struct StakeArgs {
⋮----
pub struct DistributeTokensArgs {
⋮----
pub struct SplTokenArgs {
⋮----
pub struct BalancesArgs {
⋮----
pub struct TransactionLogArgs {
⋮----
pub enum Command {
⋮----
pub struct Args {

================
File: tokens/src/commands.rs
================
pub struct Allocation {
⋮----
pub struct TypedAllocation {
⋮----
pub enum FundingSource {
⋮----
pub struct FundingSources(Vec<FundingSource>);
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
for (i, source) in self.0.iter().enumerate() {
⋮----
write!(f, "/")?;
⋮----
write!(f, "{source:?}")?;
⋮----
Ok(())
⋮----
impl PartialEq for FundingSources {
fn eq(&self, other: &Self) -> bool {
⋮----
fn from(sources_vec: Vec<FundingSource>) -> Self {
Self(sources_vec)
⋮----
type StakeExtras = Vec<(Keypair, Option<DateTime<Utc>>)>;
⋮----
pub enum Error {
⋮----
fn merge_allocations(allocations: &[TypedAllocation]) -> Vec<TypedAllocation> {
⋮----
.entry(&allocation.recipient)
.or_insert(TypedAllocation {
⋮----
allocation_map.values().cloned().collect()
⋮----
/// Return true if the recipient and lockups are the same
fn has_same_recipient(allocation: &TypedAllocation, transaction_info: &TransactionInfo) -> bool {
⋮----
fn has_same_recipient(allocation: &TypedAllocation, transaction_info: &TransactionInfo) -> bool {
⋮----
fn apply_previous_transactions(
⋮----
for allocation in allocations.iter_mut() {
if !has_same_recipient(allocation, transaction_info) {
⋮----
allocations.retain(|x| x.amount > 0);
⋮----
fn transfer<S: Signer>(
⋮----
system_instruction::transfer(&sender_keypair.pubkey(), to_pubkey, lamports);
let message = Message::new(&[create_instruction], Some(&sender_keypair.pubkey()));
let recent_blockhash = client.get_latest_blockhash()?;
Ok(Transaction::new(
⋮----
fn distribution_instructions(
⋮----
if args.spl_token_args.is_some() {
return build_spl_token_instructions(allocation, args, do_create_associated_token_account);
⋮----
// No stake args; a simple token transfer.
⋮----
let from = args.sender_keypair.pubkey();
⋮----
vec![instruction]
⋮----
// Stake args provided, so create a recipient stake account.
⋮----
let sender_pubkey = args.sender_keypair.pubkey();
⋮----
// No source stake account, so create a recipient stake account directly.
⋮----
// Make the recipient both the new stake and withdraw authority
⋮----
lockup.unix_timestamp = lockup_date.timestamp();
⋮----
// A sender stake account was provided, so create a recipient stake account by
// splitting the sender account.
⋮----
let stake_authority = sender_stake_args.stake_authority.pubkey();
let withdraw_authority = sender_stake_args.withdraw_authority.pubkey();
⋮----
.expect("SenderStakeArgs.rent_exempt_reserve should be populated");
// Transfer some tokens to stake account to cover rent-exempt reserve.
let mut instructions = vec![system_instruction::transfer(
⋮----
// Split to stake account
instructions.append(&mut stake_instruction::split(
⋮----
// Make the recipient the new stake authority
instructions.push(stake_instruction::authorize(
⋮----
// Make the recipient the new withdraw authority
⋮----
// Add lockup
⋮----
unix_timestamp: Some(lockup_date.timestamp()),
⋮----
instructions.push(stake_instruction::set_lockup(
⋮----
&stake_args.lockup_authority.unwrap(),
⋮----
// Transfer some unlocked tokens to recipient, which they can use for transaction fees.
instructions.push(system_instruction::transfer(
⋮----
fn build_messages(
⋮----
let mut existing_associated_token_accounts = vec![];
⋮----
let allocation_chunks = allocations.chunks(MAX_MULTIPLE_ACCOUNTS);
⋮----
.iter()
.map(|x| {
⋮----
get_associated_token_address(&wallet_address, &spl_token_args.mint)
⋮----
let mut maybe_accounts = client.get_multiple_accounts(&associated_token_addresses)?;
existing_associated_token_accounts.append(&mut maybe_accounts);
⋮----
for (i, allocation) in allocations.iter().enumerate() {
if exit.load(Ordering::SeqCst) {
db.dump()?;
return Err(Error::ExitSignal);
⋮----
existing_associated_token_accounts[i].is_none();
⋮----
println!(
⋮----
let instructions = distribution_instructions(
⋮----
&new_stake_account_keypair.pubkey(),
⋮----
let fee_payer_pubkey = args.fee_payer.pubkey();
⋮----
Some(&fee_payer_pubkey),
&Hash::default(), // populated by a real blockhash for balance check and submission
⋮----
messages.push(message);
stake_extras.push((new_stake_account_keypair, lockup_date));
⋮----
fn send_messages(
⋮----
allocations.iter().zip(messages).zip(stake_extras)
⋮----
let new_stake_account_address = new_stake_account_keypair.pubkey();
let mut signers = vec![&*args.fee_payer, &*args.sender_keypair];
⋮----
signers.push(&new_stake_account_keypair);
⋮----
signers.push(&*sender_stake_args.stake_authority);
signers.push(&*sender_stake_args.withdraw_authority);
⋮----
if allocation.lockup_date.is_some() {
⋮----
signers.push(&**lockup_authority);
⋮----
return Err(Error::MissingLockupAuthority);
⋮----
let signers = unique_signers(signers);
⋮----
Ok((Transaction::new_unsigned(message), u64::MAX))
⋮----
client.get_latest_blockhash_with_commitment(CommitmentConfig::default())?;
⋮----
client.send_transaction_with_config(&transaction, config)?;
Ok((transaction, last_valid_block_height))
⋮----
args.stake_args.as_ref().map(|_| &new_stake_account_address);
⋮----
eprintln!("Error sending tokens to {}: {}", allocation.recipient, e);
⋮----
fn distribute_allocations(
⋮----
let mut messages: Vec<Message> = vec![];
let mut stake_extras: StakeExtras = vec![];
⋮----
build_messages(
⋮----
exit.clone(),
⋮----
check_spl_token_balances(&messages, allocations, client, args, created_accounts)?;
⋮----
check_payer_balances(&messages, allocations, client, args)?;
⋮----
send_messages(client, db, allocations, args, exit, messages, stake_extras)?;
⋮----
fn read_allocations(
⋮----
let mut rdr = ReaderBuilder::new().trim(Trim::All).from_path(input_csv)?;
⋮----
rdr.deserialize()
.map(|recipient| {
⋮----
Pubkey::from_str(&recipient).map_err(|err| Error::BadInputPubkeyError {
⋮----
Ok(TypedAllocation {
⋮----
// We only support SOL token in "require lockup" mode.
⋮----
let lockup_date = if !lockup_date.is_empty() {
let lockup_date = lockup_date.parse::<DateTime<Utc>>().map_err(|err| {
⋮----
Some(lockup_date)
⋮----
// empty lockup date means no lockup, it's okay to have only some lockups specified
⋮----
amount: sol_str_to_lamports(&amount)
.ok_or(Error::BadInputNumberError { input: amount })?,
⋮----
if allocations.is_empty() {
return Err(Error::CsvIsEmptyError);
⋮----
Ok(allocations)
⋮----
fn new_spinner_progress_bar() -> ProgressBar {
⋮----
progress_bar.set_style(
⋮----
.template("{spinner:.green} {wide_msg}")
.expect("ProgresStyle::template direct input to be correct"),
⋮----
progress_bar.enable_steady_tick(Duration::from_millis(100));
⋮----
pub fn process_allocations(
⋮----
let with_lockup = args.stake_args.is_some();
let mut allocations: Vec<TypedAllocation> = read_allocations(
⋮----
args.spl_token_args.is_some(),
⋮----
let starting_total_tokens = allocations.iter().map(|x| x.amount).sum();
⋮----
let confirmations = finalize_transactions(client, &mut db, args.dry_run, exit.clone())?;
⋮----
apply_previous_transactions(&mut allocations, &transaction_infos);
⋮----
eprintln!("No work to do");
return Ok(confirmations);
⋮----
let distributed_tokens = transaction_infos.iter().map(|x| x.amount).sum();
let undistributed_tokens = allocations.iter().map(|x| x.amount).sum();
⋮----
println!("{} {}", style("Distributed:").bold(), distributed_tokens,);
⋮----
distribute_allocations(client, &mut db, &allocations, args, exit.clone())?;
let opt_confirmations = finalize_transactions(client, &mut db, args.dry_run, exit)?;
⋮----
Ok(opt_confirmations)
⋮----
fn finalize_transactions(
⋮----
return Ok(None);
⋮----
let mut opt_confirmations = update_finalized_transactions(client, db, exit.clone())?;
let progress_bar = new_spinner_progress_bar();
while opt_confirmations.is_some() {
⋮----
progress_bar.set_message(format!(
⋮----
sleep(Duration::from_millis(500));
let opt_conf = update_finalized_transactions(client, db, exit.clone())?;
⋮----
fn update_finalized_transactions(
⋮----
.filter_map(|info| {
if info.finalized_date.is_some() {
⋮----
Some((&info.transaction, info.last_valid_block_height))
⋮----
.collect();
⋮----
.map(|(tx, _slot)| tx.signatures[0])
.filter(|sig| *sig != Signature::default())
⋮----
let mut statuses = vec![];
⋮----
unconfirmed_signatures.chunks(MAX_GET_SIGNATURE_STATUSES_QUERY_ITEMS - 1)
⋮----
statuses.extend(
⋮----
.get_signature_statuses(unconfirmed_signatures_chunk)?
⋮----
.into_iter(),
⋮----
log_transaction_confirmations(
⋮----
Ok(confirmations)
⋮----
fn log_transaction_confirmations(
⋮----
let finalized_block_height = client.get_block_height()?;
⋮----
.into_iter()
.zip(statuses.into_iter())
⋮----
*confirmations = Some(cmp::min(confs, confirmations.unwrap_or(usize::MAX)));
⋮----
pub fn get_fee_estimate_for_messages(
⋮----
let mut message = messages.first().ok_or(Error::MissingMessages)?.clone();
let latest_blockhash = client.get_latest_blockhash()?;
⋮----
let fee = client.get_fee_for_message(&message)?;
⋮----
.checked_mul(messages.len() as u64)
.ok_or(Error::FeeEstimationError)?;
Ok(fee_estimate)
⋮----
fn check_payer_balances(
⋮----
let mut undistributed_tokens: u64 = allocations.iter().map(|x| x.amount).sum();
let fees = get_fee_estimate_for_messages(messages, client)?;
⋮----
let total_unlocked_sol = allocations.len() as u64 * stake_args.unlocked_sol;
⋮----
args.sender_keypair.pubkey()
⋮----
Some((args.sender_keypair.pubkey(), total_unlocked_sol)),
⋮----
(args.sender_keypair.pubkey(), None)
⋮----
let fee_payer_balance = client.get_balance(&args.fee_payer.pubkey())?;
⋮----
let staker_balance = client.get_balance(&distribution_source)?;
⋮----
return Err(Error::InsufficientFunds(
vec![FundingSource::StakeAccount].into(),
build_balance_message(undistributed_tokens, false, false).to_string(),
⋮----
if args.fee_payer.pubkey() == unlocked_sol_source {
⋮----
vec![FundingSource::SystemAccount, FundingSource::FeePayer].into(),
build_balance_message(fees + total_unlocked_sol, false, false).to_string(),
⋮----
vec![FundingSource::FeePayer].into(),
build_balance_message(fees, false, false).to_string(),
⋮----
let unlocked_sol_balance = client.get_balance(&unlocked_sol_source)?;
⋮----
vec![FundingSource::SystemAccount].into(),
build_balance_message(total_unlocked_sol, false, false).to_string(),
⋮----
} else if args.fee_payer.pubkey() == distribution_source {
⋮----
build_balance_message(fees + undistributed_tokens, false, false).to_string(),
⋮----
let sender_balance = client.get_balance(&distribution_source)?;
⋮----
pub fn process_balances(
⋮----
read_allocations(&args.input_csv, None, false, args.spl_token_args.is_some())?;
let allocations = merge_allocations(&allocations);
⋮----
spl_token_args.mint.to_string()
⋮----
"◎".to_string()
⋮----
println!("{} {}", style("Token:").bold(), token);
⋮----
print_token_balances(client, allocation, spl_token_args)?;
⋮----
let expected = build_balance_message(allocation.amount, false, false);
let actual_amount = client.get_balance(&address).unwrap();
let actual = build_balance_message(actual_amount, false, false);
let diff = build_balance_message(actual_amount - allocation.amount, false, false);
⋮----
pub fn process_transaction_log(args: &TransactionLogArgs) -> Result<(), Error> {
⋮----
pub fn test_process_distribute_tokens_with_client(
⋮----
let transaction = transfer(
⋮----
sol_str_to_lamports("1.0").unwrap(),
⋮----
&fee_payer.pubkey(),
⋮----
.unwrap();
⋮----
.send_and_confirm_transaction_with_spinner(&transaction)
⋮----
assert_eq!(
⋮----
sol_str_to_lamports("1000.0").unwrap()
⋮----
let allocations_file = NamedTempFile::new().unwrap();
let input_csv = allocations_file.path().to_str().unwrap().to_string();
let mut wtr = csv::WriterBuilder::new().from_writer(allocations_file);
wtr.write_record(["recipient", "amount"]).unwrap();
wtr.write_record([
alice_pubkey.to_string(),
build_balance_message(expected_amount, false, false).to_string(),
⋮----
wtr.flush().unwrap();
let dir = tempdir().unwrap();
⋮----
.path()
.join("transactions.db")
.to_str()
.unwrap()
.to_string();
let output_file = NamedTempFile::new().unwrap();
let output_path = output_file.path().to_str().unwrap().to_string();
⋮----
transaction_db: transaction_db.clone(),
output_path: Some(output_path.clone()),
⋮----
let confirmations = process_allocations(client, &args, exit.clone()).unwrap();
assert_eq!(confirmations, None);
⋮----
db::read_transaction_infos(&db::open_db(&transaction_db, true).unwrap());
assert_eq!(transaction_infos.len(), 1);
assert_eq!(transaction_infos[0].recipient, alice_pubkey);
assert_eq!(transaction_infos[0].amount, expected_amount);
assert_eq!(client.get_balance(&alice_pubkey).unwrap(), expected_amount);
check_output_file(&output_path, &db::open_db(&transaction_db, true).unwrap());
process_allocations(client, &args, exit).unwrap();
⋮----
pub fn test_process_create_stake_with_client(client: &RpcClient, sender_keypair: Keypair) {
⋮----
let stake_account_address = stake_account_keypair.pubkey();
⋮----
staker: stake_authority.pubkey(),
withdrawer: withdraw_authority.pubkey(),
⋮----
&sender_keypair.pubkey(),
⋮----
sol_str_to_lamports("3000.0").unwrap(),
⋮----
let message = Message::new(&instructions, Some(&sender_keypair.pubkey()));
⋮----
let blockhash = client.get_latest_blockhash().unwrap();
⋮----
let expected_amount = sol_str_to_lamports("1000.0").unwrap();
⋮----
let file = NamedTempFile::new().unwrap();
let input_csv = file.path().to_str().unwrap().to_string();
let mut wtr = csv::WriterBuilder::new().from_writer(file);
wtr.write_record(["recipient", "amount", "lockup_date"])
⋮----
"".to_string(),
⋮----
unlocked_sol: sol_str_to_lamports("1.0").unwrap(),
⋮----
stake_args: Some(stake_args),
⋮----
let new_stake_account_address = transaction_infos[0].new_stake_account_address.unwrap();
⋮----
pub fn test_process_distribute_stake_with_client(client: &RpcClient, sender_keypair: Keypair) {
⋮----
.get_minimum_balance_for_rent_exemption(StakeStateV2::size_of())
⋮----
rent_exempt_reserve: Some(rent_exempt_reserve),
⋮----
sender_stake_args: Some(sender_stake_args),
⋮----
mod tests {
⋮----
fn one_signer_message(client: &RpcClient) -> Message {
⋮----
vec![AccountMeta::new(Pubkey::default(), true)],
⋮----
&client.get_latest_blockhash().unwrap(),
⋮----
fn test_process_token_allocations() {
⋮----
let test_validator = simple_test_validator_no_fees(alice.pubkey());
let url = test_validator.rpc_url();
⋮----
test_process_distribute_tokens_with_client(&client, alice, None);
⋮----
fn test_process_transfer_amount_allocations() {
⋮----
test_process_distribute_tokens_with_client(&client, alice, sol_str_to_lamports("1.5"));
⋮----
fn simple_test_validator_no_fees(pubkey: Pubkey) -> TestValidator {
⋮----
fn test_create_stake_allocations() {
⋮----
test_process_create_stake_with_client(&client, alice);
⋮----
fn test_process_stake_allocations() {
⋮----
test_process_distribute_stake_with_client(&client, alice);
⋮----
fn test_read_allocations() {
⋮----
wtr.serialize((
"recipient".to_string(),
"amount".to_string(),
"require_lockup".to_string(),
⋮----
allocation.recipient.to_string(),
⋮----
amount: sol_str_to_lamports("42.0").unwrap(),
⋮----
fn test_read_allocations_no_lockup() {
⋮----
wtr.serialize(("recipient".to_string(), "amount".to_string()))
⋮----
wtr.serialize((&pubkey0.to_string(), 42.0)).unwrap();
wtr.serialize((&pubkey1.to_string(), 43.0)).unwrap();
⋮----
let expected_allocations = vec![
⋮----
fn test_read_allocations_malformed() {
⋮----
let mut wtr = csv::WriterBuilder::new().from_writer(&file);
⋮----
let got = read_allocations(&input_csv, None, false, false);
assert!(matches!(got, Err(Error::CsvIsEmptyError)));
⋮----
wtr.serialize("recipient".to_string()).unwrap();
wtr.serialize(pubkey0.to_string()).unwrap();
wtr.serialize(pubkey1.to_string()).unwrap();
⋮----
assert!(matches!(got, Err(Error::CsvError(..))));
⋮----
wtr.serialize((pubkey0.to_string(), "42.0".to_string()))
⋮----
wtr.serialize((pubkey1.to_string(), "43.0".to_string()))
⋮----
let got = read_allocations(&input_csv, None, true, false);
⋮----
wtr.serialize(header).unwrap();
wtr.serialize(&data[0]).unwrap();
wtr.serialize(&data[1]).unwrap();
⋮----
generate_csv_file(
default_header.clone(),
vec![
⋮----
let got_err = read_allocations(&input_csv, None, false, false).unwrap_err();
assert!(
⋮----
let got_err = read_allocations(&input_csv, Some(123), false, false).unwrap_err();
⋮----
let got_err = read_allocations(&input_csv, None, true, false).unwrap_err();
⋮----
let got_err = read_allocations(&input_csv, None, false, true).unwrap_err();
⋮----
assert!(matches!(got, Err(Error::BadInputNumberError { .. })));
// Bad value in 2nd column (with require lockup).
⋮----
// Bad value in 2nd column (with raw amount).
⋮----
(pubkey1.to_string(), "43.0".to_string(), "".to_string()), // bad raw amount
⋮----
let got = read_allocations(&input_csv, None, false, true);
⋮----
// Bad value in 3rd column.
⋮----
fn test_read_allocations_transfer_amount() {
⋮----
wtr.serialize(pubkey2.to_string()).unwrap();
⋮----
let amount = sol_str_to_lamports("1.5").unwrap();
⋮----
fn test_apply_previous_transactions() {
⋮----
let mut allocations = vec![
⋮----
let transaction_infos = vec![TransactionInfo {
⋮----
assert_eq!(allocations.len(), 1);
assert_eq!(allocations[0].recipient, alice);
⋮----
fn test_has_same_recipient() {
⋮----
let lockup0 = "2021-01-07T00:00:00Z".to_string();
let lockup1 = "9999-12-31T23:59:59Z".to_string();
⋮----
amount: sol_str_to_lamports("1.0").unwrap(),
⋮----
lockup_date: lockup0.parse().ok(),
⋮----
lockup_date: lockup1.parse().ok(),
⋮----
assert!(!has_same_recipient(&alice_alloc, &bob_info));
assert!(!has_same_recipient(&alice_alloc, &alice_info_lockup0));
assert!(!has_same_recipient(
⋮----
assert!(has_same_recipient(&alice_alloc, &alice_info));
assert!(has_same_recipient(
⋮----
fn test_set_split_stake_lockup() {
⋮----
amount: sol_str_to_lamports("1.002282880").unwrap(),
lockup_date: lockup_date_str.parse().ok(),
⋮----
let lockup_authority_address = lockup_authority.pubkey();
⋮----
lockup_authority: Some(Box::new(lockup_authority)),
rent_exempt_reserve: Some(2_282_880),
⋮----
lockup_authority: Some(lockup_authority_address),
⋮----
input_csv: "".to_string(),
transaction_db: "".to_string(),
⋮----
let lockup_date = lockup_date_str.parse().unwrap();
⋮----
Some(lockup_date),
⋮----
bincode::deserialize(&instructions[SET_LOCKUP_INDEX].data).unwrap();
⋮----
assert_eq!(lockup_args.unix_timestamp, Some(lockup_date.timestamp()));
assert_eq!(lockup_args.epoch, None); // Don't change the epoch
assert_eq!(lockup_args.custodian, None); // Don't change the lockup authority
⋮----
panic!("expected SetLockup instruction");
⋮----
fn tmp_file_path(name: &str, pubkey: &Pubkey) -> String {
use std::env;
let out_dir = env::var("FARF_DIR").unwrap_or_else(|_| "farf".to_string());
format!("{out_dir}/tmp/{name}-{pubkey}")
⋮----
fn initialize_check_payer_balances_inputs(
⋮----
let allocations = vec![TypedAllocation {
⋮----
sender_keypair: Box::new(read_keypair_file(sender_keypair_file).unwrap()),
fee_payer: Box::new(read_keypair_file(fee_payer).unwrap()),
⋮----
fn test_check_payer_balances_distribute_tokens_single_payer() {
⋮----
let test_validator = simple_test_validator(alice.pubkey());
⋮----
let sender_keypair_file = tmp_file_path("keypair_file", &alice.pubkey());
write_keypair_file(&alice, &sender_keypair_file).unwrap();
⋮----
.get_fee_for_message(&one_signer_message(&client))
⋮----
let (allocations, mut args) = initialize_check_payer_balances_inputs(
sol_str_to_lamports(&allocation_amount.to_string()).unwrap(),
⋮----
check_payer_balances(&[one_signer_message(&client)], &allocations, &client, &args).unwrap();
⋮----
let unfunded_payer_keypair_file = tmp_file_path("keypair_file", &unfunded_payer.pubkey());
write_keypair_file(&unfunded_payer, &unfunded_payer_keypair_file).unwrap();
args.sender_keypair = Box::new(read_keypair_file(&unfunded_payer_keypair_file).unwrap());
args.fee_payer = Box::new(read_keypair_file(&unfunded_payer_keypair_file).unwrap());
⋮----
check_payer_balances(&[one_signer_message(&client)], &allocations, &client, &args)
.unwrap_err();
⋮----
assert_eq!(amount, (allocation_amount + fees_in_sol).to_string());
⋮----
panic!("check_payer_balances should have errored");
⋮----
tmp_file_path("keypair_file", &partially_funded_payer.pubkey());
write_keypair_file(
⋮----
&partially_funded_payer.pubkey(),
⋮----
Box::new(read_keypair_file(&partially_funded_payer_keypair_file).unwrap());
args.fee_payer = Box::new(read_keypair_file(&partially_funded_payer_keypair_file).unwrap());
⋮----
fn test_check_payer_balances_distribute_tokens_separate_payers() {
⋮----
let funded_payer_keypair_file = tmp_file_path("keypair_file", &funded_payer.pubkey());
write_keypair_file(&funded_payer, &funded_payer_keypair_file).unwrap();
⋮----
&funded_payer.pubkey(),
⋮----
args.fee_payer = Box::new(read_keypair_file(&sender_keypair_file).unwrap());
⋮----
assert_eq!(sources, vec![FundingSource::SystemAccount].into());
assert_eq!(amount, allocation_amount.to_string());
⋮----
args.sender_keypair = Box::new(read_keypair_file(&sender_keypair_file).unwrap());
⋮----
assert_eq!(sources, vec![FundingSource::FeePayer].into());
assert_eq!(amount, fees_in_sol.to_string());
⋮----
fn initialize_stake_account(
⋮----
fn simple_test_validator(alice: Pubkey) -> TestValidator {
⋮----
fn test_check_payer_balances_distribute_stakes_single_payer() {
⋮----
let stake_args = initialize_stake_account(
⋮----
sol_str_to_lamports(&unlocked_sol.to_string()).unwrap(),
⋮----
Some(stake_args),
⋮----
let expensive_allocations = vec![TypedAllocation {
⋮----
let err_result = check_payer_balances(
&[one_signer_message(&client)],
⋮----
assert_eq!(sources, vec![FundingSource::StakeAccount].into());
⋮----
assert_eq!(amount, (unlocked_sol + fees_in_sol).to_string());
⋮----
fn test_check_payer_balances_distribute_stakes_separate_payers() {
⋮----
assert_eq!(amount, unlocked_sol.to_string());
⋮----
fn test_build_messages_dump_db() {
let client = RpcClient::new_mock("mock_client".to_string());
⋮----
.join("build_messages.db")
⋮----
let mut db = db::open_db(&db_file, false).unwrap();
⋮----
let amount = sol_str_to_lamports("1.0").unwrap();
⋮----
let transaction = transfer(&client, amount, &sender, &recipient).unwrap();
⋮----
let read_db = db::open_db(&db_file, true).unwrap();
assert!(db::read_transaction_infos(&read_db).is_empty());
⋮----
assert_eq!(messages.len(), 1);
⋮----
assert!(messages.is_empty());
⋮----
assert_eq!(transaction_info.len(), 1);
⋮----
assert_eq!(messages.len(), 0);
⋮----
fn test_send_messages_dump_db() {
⋮----
.join("send_messages.db")
⋮----
let message = transaction.message.clone();
send_messages(
⋮----
vec![message.clone()],
vec![(Keypair::new(), None)],
⋮----
let num_records = db::read_transaction_infos(&db).len();
⋮----
send_messages(&client, &mut db, &[], &args, exit.clone(), vec![], vec![]).unwrap();
⋮----
assert_eq!(transaction_info.len(), num_records);
assert!(transaction_info.contains(&TransactionInfo {
⋮----
db.dump().unwrap();
⋮----
fn test_distribute_allocations_dump_db() {
⋮----
let test_validator = simple_test_validator_no_fees(sender_keypair.pubkey());
⋮----
.join("dist_allocations.db")
⋮----
// Ensure data is always dumped after distribute_allocations
distribute_allocations(&client, &mut db, &[allocation], &args, exit).unwrap();
⋮----
fn test_log_transaction_confirmations_dump_db() {
⋮----
.join("log_transaction_confirmations.db")
⋮----
vec![],
⋮----
vec![(&transaction, 111)],
vec![Some(TransactionStatus {
⋮----
assert_eq!(confirmations, Some(15));
⋮----
assert!(transaction_info[0].finalized_date.is_some());
⋮----
fn test_update_finalized_transactions_dump_db() {
⋮----
.join("update_finalized_transactions.db")
⋮----
update_finalized_transactions(&client, &mut db, Arc::new(AtomicBool::new(false)))
⋮----
assert_eq!(confs, None);

================
File: tokens/src/db.rs
================
pub struct TransactionInfo {
⋮----
struct SignedTransactionInfo {
⋮----
impl Default for TransactionInfo {
fn default() -> Self {
⋮----
signatures: vec![Signature::default()],
⋮----
pub fn open_db(path: &str, dry_run: bool) -> Result<PickleDb, Error> {
⋮----
let db = if path.exists() {
⋮----
if let Some(parent) = path.parent() {
fs::create_dir_all(parent).unwrap();
⋮----
Ok(db)
⋮----
pub fn compare_transaction_infos(a: &TransactionInfo, b: &TransactionInfo) -> Ordering {
⋮----
(Some(a), Some(b)) => a.cmp(&b),
⋮----
return a.recipient.to_string().cmp(&b.recipient.to_string());
⋮----
pub fn write_transaction_log<P: AsRef<Path>>(db: &PickleDb, path: &P) -> Result<(), io::Error> {
let mut wtr = csv::WriterBuilder::new().from_path(path).unwrap();
let mut transaction_infos = read_transaction_infos(db);
transaction_infos.sort_by(compare_transaction_infos);
⋮----
recipient: info.recipient.to_string(),
⋮----
.map(|x| x.to_string())
.unwrap_or_else(|| "".to_string()),
⋮----
signature: info.transaction.signatures[0].to_string(),
⋮----
wtr.serialize(&signed_info)?;
⋮----
wtr.flush()
⋮----
pub fn read_transaction_infos(db: &PickleDb) -> Vec<TransactionInfo> {
db.iter()
.map(|kv| kv.get_value::<TransactionInfo>().unwrap())
.collect()
⋮----
pub fn set_transaction_info(
⋮----
let finalized_date = if finalized { Some(Utc::now()) } else { None };
⋮----
new_stake_account_address: new_stake_account_address.cloned(),
⋮----
transaction: transaction.clone(),
⋮----
db.set(&signature.to_string(), &transaction_info)?;
Ok(())
⋮----
// Set the finalized bit in the database if the transaction is rooted.
// Remove the TransactionInfo from the database if the transaction failed.
// Return the number of confirmations on the transaction or None if either
// finalized or discarded.
pub fn update_finalized_transaction(
⋮----
if opt_transaction_status.is_none() {
⋮----
eprintln!(
⋮----
eprintln!();
// Don't discard the transaction, because we are not certain the
// blockhash is expired. Instead, return None to signal that
// we don't need to wait for confirmations.
return Ok(None);
⋮----
// Return zero to signal the transaction may still be in flight.
return Ok(Some(0));
⋮----
let transaction_status = opt_transaction_status.unwrap();
⋮----
// The transaction was found but is not yet finalized.
return Ok(Some(confirmations));
⋮----
// The transaction was finalized, but execution failed. Drop it.
eprintln!("Error in transaction with signature {signature}: {e}");
eprintln!("Discarding transaction record");
⋮----
db.rem(&signature.to_string())?;
⋮----
let mut transaction_info = db.get::<TransactionInfo>(&signature.to_string()).unwrap();
transaction_info.finalized_date = Some(Utc::now());
⋮----
Ok(None)
⋮----
pub(crate) fn check_output_file(path: &str, db: &PickleDb) {
⋮----
.trim(Trim::All)
.from_path(path)
.unwrap();
⋮----
rdr.deserialize().map(|entry| entry.unwrap()).collect();
⋮----
.iter()
.map(|info| SignedTransactionInfo {
⋮----
.unwrap_or_default(),
⋮----
.collect();
assert_eq!(logged_infos, transaction_infos);
⋮----
mod tests {
⋮----
fn test_sort_transaction_infos_finalized_first() {
⋮----
finalized_date: Some(Utc.with_ymd_and_hms(2014, 7, 8, 9, 10, 11).unwrap()),
⋮----
finalized_date: Some(Utc.with_ymd_and_hms(2014, 7, 8, 9, 10, 42).unwrap()),
⋮----
assert_eq!(compare_transaction_infos(&info0, &info1), Ordering::Less);
assert_eq!(compare_transaction_infos(&info1, &info2), Ordering::Less);
assert_eq!(compare_transaction_infos(&info2, &info3), Ordering::Less);
⋮----
fn test_write_transaction_log() {
⋮----
PickleDb::new_yaml(NamedTempFile::new().unwrap(), PickleDbDumpPolicy::NeverDump);
⋮----
db.set(&signature.to_string(), &transaction_info).unwrap();
let csv_file = NamedTempFile::new().unwrap();
write_transaction_log(&db, &csv_file).unwrap();
let mut rdr = ReaderBuilder::new().trim(Trim::All).from_reader(csv_file);
⋮----
recipient: Pubkey::default().to_string(),
signature: Signature::default().to_string(),
⋮----
assert_eq!(signed_infos, vec![signed_info]);
⋮----
fn test_update_finalized_transaction_not_landed() {
⋮----
assert_matches!(
⋮----
assert_eq!(
⋮----
fn test_update_finalized_transaction_confirming() {
⋮----
confirmations: Some(1),
⋮----
status: Ok(()),
confirmation_status: Some(TransactionConfirmationStatus::Confirmed),
⋮----
fn test_update_finalized_transaction_failed() {
⋮----
err: Some(TransactionError::AccountNotFound),
⋮----
confirmation_status: Some(TransactionConfirmationStatus::Finalized),
⋮----
assert_eq!(db.get::<TransactionInfo>(&signature.to_string()), None);
⋮----
fn test_update_finalized_transaction_finalized() {
⋮----
assert!(db

================
File: tokens/src/lib.rs
================
pub mod arg_parser;
pub mod args;
pub mod commands;
mod db;
pub mod spl_token;
pub mod stake;
pub mod token_display;

================
File: tokens/src/main.rs
================
fn main() -> Result<(), Box<dyn Error>> {
let command_args = parse_args(env::args_os())?;
let config = if Path::new(&command_args.config_file).exists() {
⋮----
let default_config_file = CONFIG_FILE.as_ref().unwrap();
⋮----
eprintln!("Error: config file not found");
⋮----
let json_rpc_url = normalize_to_url_if_moniker(command_args.url.unwrap_or(config.json_rpc_url));
⋮----
let exit = exit.clone();
⋮----
exit.store(true, Ordering::SeqCst);
⋮----
.expect("Error setting Ctrl-C handler");
⋮----
Ok(())

================
File: tokens/src/spl_token.rs
================
pub fn update_token_args(client: &RpcClient, args: &mut Option<SplTokenArgs>) -> Result<(), Error> {
⋮----
.get_account(&spl_token_args.token_account_address)
.unwrap_or_default();
⋮----
update_decimals(client, args)?;
⋮----
Ok(())
⋮----
pub fn update_decimals(client: &RpcClient, args: &mut Option<SplTokenArgs>) -> Result<(), Error> {
⋮----
let mint_account = client.get_account(&spl_token_args.mint).unwrap_or_default();
⋮----
pub(crate) fn build_spl_token_instructions(
⋮----
.as_ref()
.expect("spl_token_args must be some");
⋮----
get_associated_token_address(&wallet_address, &spl_token_args.mint);
let mut instructions = vec![];
⋮----
instructions.push(create_associated_token_account(
&args.fee_payer.pubkey(),
⋮----
instructions.push(
⋮----
&args.sender_keypair.pubkey(),
⋮----
.unwrap(),
⋮----
pub(crate) fn check_spl_token_balances(
⋮----
let allocation_amount: u64 = allocations.iter().map(|x| x.amount).sum();
let fees = get_fee_estimate_for_messages(messages, client)?;
⋮----
client.get_minimum_balance_for_rent_exemption(SplTokenAccount::LEN)?;
⋮----
let fee_payer_balance = client.get_balance(&args.fee_payer.pubkey())?;
⋮----
return Err(Error::InsufficientFunds(
vec![FundingSource::FeePayer].into(),
build_balance_message(fees + account_creation_amount, false, false).to_string(),
⋮----
vec![FundingSource::SplTokenAccount].into(),
real_number_string_trimmed(allocation_amount, spl_token_args.decimals),
⋮----
pub(crate) fn print_token_balances(
⋮----
let associated_token_address = get_associated_token_address(&address, &spl_token_args.mint);
⋮----
.get_account(&associated_token_address)
⋮----
let actual_ui_amount = real_number_string(recipient_token.amount, spl_token_args.decimals);
⋮----
real_number_string(recipient_token.amount - expected, spl_token_args.decimals);
⋮----
style(format!("{actual_ui_amount:>24}")),
format!("{delta_string:>24}"),
⋮----
style("Associated token account not yet created".to_string()).yellow(),
"".to_string(),
⋮----
println!(
⋮----
mod tests {

================
File: tokens/src/stake.rs
================
pub fn update_stake_args(client: &RpcClient, args: &mut Option<StakeArgs>) -> Result<(), Error> {
⋮----
let rent = client.get_minimum_balance_for_rent_exemption(StakeStateV2::size_of())?;
sender_args.rent_exempt_reserve = Some(rent);
⋮----
Ok(())

================
File: tokens/src/token_display.rs
================
pub enum TokenType {
⋮----
pub struct Token {
⋮----
impl Token {
fn write_with_symbol(&self, f: &mut Formatter) -> Result {
⋮----
let amount = build_balance_message(self.amount, false, false);
write!(f, "{SOL_SYMBOL}{amount}")
⋮----
let amount = real_number_string_trimmed(self.amount, self.decimals);
write!(f, "{amount} tokens")
⋮----
pub fn sol(amount: u64) -> Self {
⋮----
pub fn spl_token(amount: u64, decimals: u8) -> Self {
⋮----
impl Display for Token {
fn fmt(&self, f: &mut Formatter) -> Result {
self.write_with_symbol(f)
⋮----
impl Debug for Token {
⋮----
impl Add for Token {
type Output = Token;
fn add(self, other: Self) -> Self {

================
File: tokens/tests/commands.rs
================
fn test_process_distribute_with_rpc_client() {
⋮----
TestValidator::with_no_fees(mint_keypair.pubkey(), None, SocketAddrSpace::Unspecified);
let client = RpcClient::new(test_validator.rpc_url());
test_process_distribute_tokens_with_client(&client, mint_keypair, None);

================
File: tokens/.gitignore
================
target/
*.csv
/farf/

================
File: tokens/Cargo.toml
================
[package]
name = "solana-tokens"
documentation = "https://docs.rs/solana-tokens"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
default = ["remote-wallet-hidraw"]
agave-unstable-api = []
remote-wallet-hidraw = ["solana-remote-wallet/linux-static-hidraw"]
remote-wallet-libusb = ["solana-remote-wallet/linux-static-libusb"]

[dependencies]
chrono = { workspace = true, features = ["default", "serde"] }
clap = "2.33.0"
console = { workspace = true }
csv = { workspace = true }
ctrlc = { workspace = true, features = ["termination"] }
indexmap = { workspace = true }
indicatif = { workspace = true }
pickledb = { workspace = true, features = ["yaml"] }
serde = { workspace = true }
solana-account-decoder = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-cli-output = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-net-utils = { workspace = true }
solana-program-error = { workspace = true }
solana-program-pack = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-remote-wallet = { workspace = true }
solana-rpc-client = { workspace = true, features = ["default"] }
solana-rpc-client-api = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-stake-interface = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-status = { workspace = true }
solana-version = { workspace = true }
spl-associated-token-account-interface = { version = "=2.0.0" }
spl-token-interface = { workspace = true }
tempfile = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
assert_matches = { workspace = true }
bincode = { workspace = true }
solana-test-validator = { workspace = true }
solana-transaction-error = { workspace = true }

================
File: tokens/README.md
================
# Distribute Solana tokens

A user may want to make payments to multiple accounts over multiple iterations.
The user will have a spreadsheet listing public keys and token amounts, and
some process for transferring tokens to them, and ensuring that no more than the
expected amount are sent. The command-line tool here automates that process.

## Distribute tokens

Send tokens to the recipients in `<RECIPIENTS_CSV>`.

Example recipients.csv:

```text
recipient,amount,lockup_date
3ihfUy1n9gaqihM5bJCiTAGLgWc5zo3DqVUS6T736NLM,42.0,
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT,43.0,
```

```bash
solana-tokens distribute-tokens --from <KEYPAIR> --input-csv <RECIPIENTS_CSV> --fee-payer <KEYPAIR>
```

Example transaction log before:

```text
recipient,amount,finalized_date,signature
6Vo87BaDhp4v4GHwVDhw5huhxVF8CyxSXYtkUwVHbbPv,70.0,2020-09-15T23:29:26.879747Z,UB168XhBhecxzeD1w2ZRUhwTHpPSqv2WNh8NrZHqz1F2EqxxbSW6iFfVtsg3HkU9NX2cD7R92D8VRLSyArZ9xKQ
```

Send tokens to the recipients in `<RECIPIENTS_CSV>` if the distribution is
not already recorded in the transaction log.

```bash
solana-tokens distribute-tokens --from <KEYPAIR> --input-csv <RECIPIENTS_CSV> --fee-payer <KEYPAIR>
```

Example output:

```text
Recipient                                     Expected Balance
3ihfUy1n9gaqihM5bJCiTAGLgWc5zo3DqVUS6T736NLM  42
UKUcTXgbeTYh65RaVV5gSf6xBHevqHvAXMo3e8Q6np8k  43
```


Example transaction log after:

```bash
solana-tokens transaction-log --output-path transactions.csv
```

```text
recipient,amount,signature
6Vo87BaDhp4v4GHwVDhw5huhxVF8CyxSXYtkUwVHbbPv,70.0,2020-09-15T23:29:26.879747Z,UB168XhBhecxzeD1w2ZRUhwTHpPSqv2WNh8NrZHqz1F2EqxxbSW6iFfVtsg3HkU9NX2cD7R92D8VRLSyArZ9xKQ
3ihfUy1n9gaqihM5bJCiTAGLgWc5zo3DqVUS6T736NLM,42.0,2020-09-15T23:31:50.264241Z,53AVNEVpQBteJBRAKp6naxXsgESDjqe1ge9Dg2HeCSpYWTuGTLqHrBpkHTnpvPJURNgKWxkJfihuRa5STVRjL2hy
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT,43.0,2020-09-15T23:33:53.680821Z,4XsMfLx9D2ZxVpdJ5xdkV2w4X4SKEQ5zbQhcH4NcRwgZDkdRNiZjvnMFaWaWHUh5eF1LwFPpQdjn6mzSsiCVj3L7
```

### Calculate what tokens should be sent

List the differences between a list of expected distributions and the record of what
transactions have already been sent.

```bash
solana-tokens distribute-tokens --dry-run --input-csv <RECIPIENTS_CSV>
```

Example recipients.csv:

```text
recipient,amount,lockup_date
6Vo87BaDhp4v4GHwVDhw5huhxVF8CyxSXYtkUwVHbbPv,80,
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr,42,
```

Example output:

```text
Recipient                                     Expected Balance
6Vo87BaDhp4v4GHwVDhw5huhxVF8CyxSXYtkUwVHbbPv  10
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr  42
```

## Distribute tokens: transfer-amount

This tool also makes it straightforward to transfer the same amount of tokens to a simple list of recipients. Just add the `--transfer-amount` arg to specify the amount:

Example recipients.csv:

```text
recipient
6Vo87BaDhp4v4GHwVDhw5huhxVF8CyxSXYtkUwVHbbPv
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT
```

```bash
solana-tokens distribute-tokens --transfer-amount 10 --from <KEYPAIR> --input-csv <RECIPIENTS_CSV> --fee-payer <KEYPAIR>
```

Example output:

```text
Recipient                                     Expected Balance
6Vo87BaDhp4v4GHwVDhw5huhxVF8CyxSXYtkUwVHbbPv  10
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr  10
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT  10
```

## Distribute stake accounts

Distributing tokens via stake accounts works similarly to how tokens are distributed. The
big difference is that new stake accounts are split from existing ones. By splitting,
the new accounts inherit any lockup or custodian settings of the original.

```bash
solana-tokens distribute-stake --stake-account-address <ACCOUNT_ADDRESS> \
    --input-csv <ALLOCATIONS_CSV> \
    --stake-authority <KEYPAIR> --withdraw-authority <KEYPAIR> --fee-payer <KEYPAIR>
```

Currently, this will subtract 1 SOL from each allocation and store it in the
recipient address. That SOL can be used to pay transaction fees on staking
operations such as delegating stake. The rest of the allocation is put in
a stake account. The new stake account address is output in the transaction
log.

## Distribute SPL tokens

Distributing SPL Tokens works very similarly to distributing SOL, but requires
the `--owner` parameter to sign transactions. Each recipient account must be an
system account that will own an Associated Token Account for the SPL Token mint.
The Associated Token Account will be created, and funded by the fee_payer, if it
does not already exist.

Send SPL tokens to the recipients in `<RECIPIENTS_CSV>`.
*NOTE:* the CSV expects SPL-token amounts in raw format (no decimals)

Example recipients.csv:

```text
recipient,amount
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT,75400
C56nwrDVFpPrqwGYsTgQxv1ZraTh81H14PV4RHvZe36s,10000
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr,42100
7qQPmVAQxEQ5djPDCtiEUrxaPf8wKtLG1m6SB1brejJ1,20000
```

You can check the status of the recipients before beginning a distribution. You
must include the SPL Token mint address:

```bash
solana-tokens spl-token-balances --mint <ADDRESS> --input-csv <RECIPIENTS_CSV>
```

Example output:

```text
Token: JDte736XZ1jGUtfAS32DLpBUWBR7WGSHy1hSZ36VRQ5V
Recipient                                             Expected Balance            Actual Balance                Difference
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT                    75.400                      0.000                   -75.400
C56nwrDVFpPrqwGYsTgQxv1ZraTh81H14PV4RHvZe36s                    10.000  Associated token account not yet created
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr                    42.100                      0.000                   -42.100
7qQPmVAQxEQ5djPDCtiEUrxaPf8wKtLG1m6SB1brejJ1                    20.000  Associated token account not yet created
```

To run the distribution:

```bash
solana-tokens distribute-spl-tokens --from <ADDRESS> --owner <KEYPAIR> \
    --input-csv <RECIPIENTS_CSV> --fee-payer <KEYPAIR>
```

Example output:

```text
Total in input_csv: 147.5 tokens
Distributed: 0 tokens
Undistributed: 147.5 tokens
Total: 147.5 tokens
Recipient                                             Expected Balance
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT                    75.400
C56nwrDVFpPrqwGYsTgQxv1ZraTh81H14PV4RHvZe36s                    10.000
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr                    42.100
7qQPmVAQxEQ5djPDCtiEUrxaPf8wKtLG1m6SB1brejJ1                    20.000
```

### Calculate what tokens should be sent

As with SOL, you can List the differences between a list of expected
distributions and the record of what transactions have already been sent using
the `--dry-run` parameter, or `solana-tokens balances`.

Example updated recipients.csv:

```text
recipient,amount
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT,100000
C56nwrDVFpPrqwGYsTgQxv1ZraTh81H14PV4RHvZe36s,100000
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr,100000
7qQPmVAQxEQ5djPDCtiEUrxaPf8wKtLG1m6SB1brejJ1,100000
```

Using dry-run:

```bash
solana-tokens distribute-tokens --dry-run --input-csv <RECIPIENTS_CSV>
```

Example output:

```text
Total in input_csv: 400 tokens
Distributed: 147.5 tokens
Undistributed: 252.5 tokens
Total: 400 tokens
Recipient                                             Expected Balance
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT                    24.600
C56nwrDVFpPrqwGYsTgQxv1ZraTh81H14PV4RHvZe36s                    90.000
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr                    57.900
7qQPmVAQxEQ5djPDCtiEUrxaPf8wKtLG1m6SB1brejJ1                    80.000
```

Or:

```bash
solana-tokens balances --mint <ADDRESS> --input-csv <RECIPIENTS_CSV>
```

Example output:

```text
Token: JDte736XZ1jGUtfAS32DLpBUWBR7WGSHy1hSZ36VRQ5V
Recipient                                             Expected Balance            Actual Balance                Difference
CYRJWqiSjLitBAcRxPvWpgX3s5TvmN2SuRY3eEYypFvT                   100.000                    75.400                   -24.600
C56nwrDVFpPrqwGYsTgQxv1ZraTh81H14PV4RHvZe36s                   100.000                    10.000                   -90.000
7aHDubg5FBYj1SgmyBgU3ZJdtfuqYCQsJQK2pTR5JUqr                   100.000                    42.100                   -57.900
7qQPmVAQxEQ5djPDCtiEUrxaPf8wKtLG1m6SB1brejJ1                   100.000                    20.000                   -80.000
```

================
File: tps-client/src/bank_client.rs
================
impl TpsClient for BankClient {
fn send_transaction(&self, transaction: Transaction) -> TpsClientResult<Signature> {
AsyncClient::async_send_transaction(self, transaction).map_err(|err| err.into())
⋮----
fn send_batch(&self, transactions: Vec<Transaction>) -> TpsClientResult<()> {
AsyncClient::async_send_batch(self, transactions).map_err(|err| err.into())
⋮----
fn get_latest_blockhash(&self) -> TpsClientResult<Hash> {
SyncClient::get_latest_blockhash(self).map_err(|err| err.into())
⋮----
fn get_latest_blockhash_with_commitment(
⋮----
.map_err(|err| err.into())
⋮----
fn get_transaction_count(&self) -> TpsClientResult<u64> {
SyncClient::get_transaction_count(self).map_err(|err| err.into())
⋮----
fn get_signature_status(&self, signature: &Signature) -> TpsClientResult<Option<Result<()>>> {
SyncClient::get_signature_status(self, signature).map_err(|err| err.into())
⋮----
fn get_transaction_count_with_commitment(
⋮----
fn get_epoch_info(&self) -> TpsClientResult<EpochInfo> {
SyncClient::get_epoch_info(self).map_err(|err| err.into())
⋮----
fn get_balance(&self, pubkey: &Pubkey) -> TpsClientResult<u64> {
SyncClient::get_balance(self, pubkey).map_err(|err| err.into())
⋮----
fn get_balance_with_commitment(
⋮----
fn get_fee_for_message(&self, message: &Message) -> TpsClientResult<u64> {
SyncClient::get_fee_for_message(self, message).map_err(|err| err.into())
⋮----
fn get_minimum_balance_for_rent_exemption(&self, data_len: usize) -> TpsClientResult<u64> {
SyncClient::get_minimum_balance_for_rent_exemption(self, data_len).map_err(|err| err.into())
⋮----
fn addr(&self) -> String {
"Local BankClient".to_string()
⋮----
fn request_airdrop_with_blockhash(
⋮----
Err(TpsClientError::AirdropFailure)
⋮----
fn get_account(&self, pubkey: &Pubkey) -> TpsClientResult<Account> {
⋮----
.and_then(|account| {
account.ok_or_else(|| {
TpsClientError::Custom(format!("AccountNotFound: pubkey={pubkey}"))
⋮----
fn get_account_with_commitment(
⋮----
fn get_multiple_accounts(&self, _pubkeys: &[Pubkey]) -> TpsClientResult<Vec<Option<Account>>> {
unimplemented!("BankClient doesn't support get_multiple_accounts");
⋮----
fn get_slot_with_commitment(
⋮----
SyncClient::get_slot_with_commitment(self, commitment_config).map_err(|err| err.into())
⋮----
fn get_blocks_with_commitment(
⋮----
unimplemented!("BankClient doesn't support get_blocks");
⋮----
fn get_block_with_config(
⋮----
unimplemented!("BankClient doesn't support get_block_with_config");

================
File: tps-client/src/lib.rs
================
pub enum TpsClientError {
⋮----
pub type TpsClientResult<T> = std::result::Result<T, TpsClientError>;
pub trait TpsClient {
⋮----
fn get_new_latest_blockhash(&self, blockhash: &Hash) -> TpsClientResult<Hash> {
⋮----
while start.elapsed().as_secs() < 5 {
if let Ok(new_blockhash) = self.get_latest_blockhash() {
⋮----
return Ok(new_blockhash);
⋮----
debug!("Got same blockhash ({blockhash:?}), will retry...");
sleep(Duration::from_millis(DEFAULT_MS_PER_SLOT / 2));
⋮----
Err(TpsClientError::Custom("Timeout".to_string()))
⋮----
mod bank_client;
mod rpc_client;
mod tpu_client;
pub mod utils;

================
File: tps-client/src/rpc_client.rs
================
impl TpsClient for RpcClient {
fn send_transaction(&self, transaction: Transaction) -> TpsClientResult<Signature> {
⋮----
.map_err(|err| err.into())
⋮----
fn send_batch(&self, transactions: Vec<Transaction>) -> TpsClientResult<()> {
⋮----
Ok(())
⋮----
fn get_latest_blockhash(&self) -> TpsClientResult<Hash> {
RpcClient::get_latest_blockhash(self).map_err(|err| err.into())
⋮----
fn get_latest_blockhash_with_commitment(
⋮----
fn get_new_latest_blockhash(&self, blockhash: &Hash) -> TpsClientResult<Hash> {
RpcClient::get_new_latest_blockhash(self, blockhash).map_err(|err| err.into())
⋮----
fn get_signature_status(&self, signature: &Signature) -> TpsClientResult<Option<Result<()>>> {
RpcClient::get_signature_status(self, signature).map_err(|err| err.into())
⋮----
fn get_transaction_count(&self) -> TpsClientResult<u64> {
RpcClient::get_transaction_count(self).map_err(|err| err.into())
⋮----
fn get_transaction_count_with_commitment(
⋮----
fn get_epoch_info(&self) -> TpsClientResult<EpochInfo> {
RpcClient::get_epoch_info(self).map_err(|err| err.into())
⋮----
fn get_balance(&self, pubkey: &Pubkey) -> TpsClientResult<u64> {
RpcClient::get_balance(self, pubkey).map_err(|err| err.into())
⋮----
fn get_balance_with_commitment(
⋮----
.map(|res| res.value)
⋮----
fn get_fee_for_message(&self, message: &Message) -> TpsClientResult<u64> {
RpcClient::get_fee_for_message(self, message).map_err(|err| err.into())
⋮----
fn get_minimum_balance_for_rent_exemption(&self, data_len: usize) -> TpsClientResult<u64> {
RpcClient::get_minimum_balance_for_rent_exemption(self, data_len).map_err(|err| err.into())
⋮----
fn addr(&self) -> String {
self.url()
⋮----
fn request_airdrop_with_blockhash(
⋮----
fn get_account(&self, pubkey: &Pubkey) -> TpsClientResult<Account> {
RpcClient::get_account(self, pubkey).map_err(|err| err.into())
⋮----
fn get_account_with_commitment(
⋮----
.and_then(|account| {
account.ok_or_else(|| {
TpsClientError::Custom(format!("AccountNotFound: pubkey={pubkey}"))
⋮----
fn get_multiple_accounts(&self, pubkeys: &[Pubkey]) -> TpsClientResult<Vec<Option<Account>>> {
RpcClient::get_multiple_accounts(self, pubkeys).map_err(|err| err.into())
⋮----
fn get_slot_with_commitment(
⋮----
RpcClient::get_slot_with_commitment(self, commitment_config).map_err(|err| err.into())
⋮----
fn get_blocks_with_commitment(
⋮----
fn get_block_with_config(
⋮----
RpcClient::get_block_with_config(self, slot, rpc_block_config).map_err(|err| err.into())

================
File: tps-client/src/tpu_client.rs
================
impl<P, M, C> TpsClient for TpuClient<P, M, C>
⋮----
fn send_transaction(&self, transaction: Transaction) -> TpsClientResult<Signature> {
⋮----
self.try_send_transaction(&transaction)?;
Ok(signature)
⋮----
fn send_batch(&self, transactions: Vec<Transaction>) -> TpsClientResult<()> {
self.try_send_transaction_batch(&transactions)?;
Ok(())
⋮----
fn get_latest_blockhash(&self) -> TpsClientResult<Hash> {
self.rpc_client()
.get_latest_blockhash()
.map_err(|err| err.into())
⋮----
fn get_latest_blockhash_with_commitment(
⋮----
.get_latest_blockhash_with_commitment(commitment_config)
⋮----
fn get_new_latest_blockhash(&self, blockhash: &Hash) -> TpsClientResult<Hash> {
⋮----
.get_new_latest_blockhash(blockhash)
⋮----
fn get_signature_status(&self, signature: &Signature) -> TpsClientResult<Option<Result<()>>> {
⋮----
.get_signature_status(signature)
⋮----
fn get_transaction_count(&self) -> TpsClientResult<u64> {
⋮----
.get_transaction_count()
⋮----
fn get_transaction_count_with_commitment(
⋮----
.get_transaction_count_with_commitment(commitment_config)
⋮----
fn get_epoch_info(&self) -> TpsClientResult<EpochInfo> {
self.rpc_client().get_epoch_info().map_err(|err| err.into())
⋮----
fn get_balance(&self, pubkey: &Pubkey) -> TpsClientResult<u64> {
⋮----
.get_balance(pubkey)
⋮----
fn get_balance_with_commitment(
⋮----
.get_balance_with_commitment(pubkey, commitment_config)
.map(|res| res.value)
⋮----
fn get_fee_for_message(&self, message: &Message) -> TpsClientResult<u64> {
⋮----
.get_fee_for_message(message)
⋮----
fn get_minimum_balance_for_rent_exemption(&self, data_len: usize) -> TpsClientResult<u64> {
⋮----
.get_minimum_balance_for_rent_exemption(data_len)
⋮----
fn addr(&self) -> String {
self.rpc_client().url()
⋮----
fn request_airdrop_with_blockhash(
⋮----
.request_airdrop_with_blockhash(pubkey, lamports, recent_blockhash)
⋮----
fn get_account(&self, pubkey: &Pubkey) -> TpsClientResult<Account> {
⋮----
.get_account(pubkey)
⋮----
fn get_account_with_commitment(
⋮----
.get_account_with_commitment(pubkey, commitment_config)
⋮----
.and_then(|account| {
account.ok_or_else(|| {
TpsClientError::Custom(format!("AccountNotFound: pubkey={pubkey}"))
⋮----
fn get_multiple_accounts(&self, pubkeys: &[Pubkey]) -> TpsClientResult<Vec<Option<Account>>> {
⋮----
.get_multiple_accounts(pubkeys)
⋮----
fn get_slot_with_commitment(
⋮----
.get_slot_with_commitment(commitment_config)
⋮----
fn get_blocks_with_commitment(
⋮----
.get_blocks_with_commitment(start_slot, end_slot, commitment_config)
⋮----
fn get_block_with_config(
⋮----
.get_block_with_config(slot, rpc_block_config)

================
File: tps-client/src/utils.rs
================
fn find_node_activated_stake(
⋮----
let vote_accounts = rpc_client.get_vote_accounts();
⋮----
error!("Failed to get vote accounts, error: {error}");
return Err(());
⋮----
let vote_accounts = vote_accounts.unwrap();
⋮----
.iter()
.map(|vote_account| vote_account.activated_stake)
.sum();
let node_id_as_str = node_id.to_string();
⋮----
.find(|&vote_account| vote_account.node_pubkey == node_id_as_str);
⋮----
Some(value) => Ok((value.activated_stake, total_active_stake)),
⋮----
error!("Failed to find stake for requested node");
Err(())
⋮----
pub fn create_connection_cache(
⋮----
create_connection_cache_with_client_socket_option(
⋮----
pub fn create_connection_cache_for_tests(
⋮----
Some(solana_net_utils::sockets::bind_to_localhost_unique().unwrap()),
⋮----
fn create_connection_cache_with_client_socket_option(
⋮----
if client_node_id.is_none() {
⋮----
let client_node_id = client_node_id.unwrap();
⋮----
find_node_activated_stake(rpc_client, client_node_id.pubkey()).unwrap_or_default();
info!("Stake for specified client_node_id: {stake}, total stake: {total_stake}");
⋮----
(client_node_id.pubkey(), stake),
⋮----
total_stake.checked_sub(stake).unwrap(),
⋮----
Some((client_node_id, bind_address)),
Some((&staked_nodes, &client_node_id.pubkey())),

================
File: tps-client/Cargo.toml
================
[package]
name = "solana-tps-client"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]
all-features = true
rustdoc-args = ["--cfg=docsrs"]

[features]
agave-unstable-api = []
bank-client = ["dep:solana-client-traits", "dep:solana-runtime"]
dev-context-only-utils = []

[dependencies]
log = { workspace = true }
solana-account = { workspace = true }
solana-client = { workspace = true }
solana-client-traits = { workspace = true, optional = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-connection-cache = { workspace = true }
solana-epoch-info = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-message = { workspace = true }
solana-net-utils = { workspace = true }
solana-pubkey = { workspace = true }
solana-quic-client = { workspace = true }
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-runtime = { workspace = true, optional = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
tempfile = { workspace = true }

================
File: tpu-client/src/nonblocking/mod.rs
================
pub mod tpu_client;

================
File: tpu-client/src/nonblocking/tpu_client.rs
================
pub use crate::tpu_client::Result;
⋮----
pub enum TpuSenderError {
⋮----
struct LeaderTpuCacheUpdateInfo {
⋮----
impl LeaderTpuCacheUpdateInfo {
pub fn has_some(&self) -> bool {
self.maybe_cluster_nodes.is_some()
|| self.maybe_epoch_schedule.is_some()
|| self.maybe_slot_leaders.is_some()
⋮----
struct LeaderTpuCache {
⋮----
impl LeaderTpuCache {
pub fn new(
⋮----
pub fn last_slot(&self) -> Slot {
self.first_slot + self.leaders.len().saturating_sub(1) as u64
⋮----
pub fn slot_info(&self) -> (Slot, Slot, Slot) {
⋮----
self.last_slot(),
⋮----
fn get_unique_leader_sockets(
⋮----
let all_leader_sockets = self.get_leader_sockets(estimated_current_slot, fanout_slots);
⋮----
if seen.insert(socket) {
unique_sockets.push(socket);
⋮----
fn get_leader_sockets(
⋮----
.step_by(NUM_CONSECUTIVE_LEADER_SLOTS as usize)
⋮----
if let Some(leader) = self.get_slot_leader(leader_slot) {
if let Some(tpu_socket) = self.leader_tpu_map.get(leader) {
leader_sockets.push(*tpu_socket);
⋮----
trace!("TPU not available for leader {leader}");
⋮----
warn!(
⋮----
pub fn get_slot_leader(&self, slot: Slot) -> Option<&Pubkey> {
⋮----
self.leaders.get(index as usize)
⋮----
fn extract_cluster_tpu_sockets(
⋮----
.into_iter()
.filter_map(|contact_info| {
let pubkey = Pubkey::from_str(&contact_info.pubkey).ok()?;
⋮----
Some((pubkey, socket))
⋮----
.collect()
⋮----
pub fn fanout(slots_in_epoch: Slot) -> Slot {
(2 * MAX_FANOUT_SLOTS).min(slots_in_epoch)
⋮----
pub fn update_all(&mut self, cache_update_info: LeaderTpuCacheUpdateInfo) -> (bool, bool) {
⋮----
warn!("Failed to fetch cluster tpu sockets: {err}");
⋮----
let epoch = epoch_schedule.get_epoch(cache_update_info.first_slot);
self.slots_in_epoch = epoch_schedule.get_slots_in_epoch(epoch);
self.last_slot_in_epoch = epoch_schedule.get_last_slot_in_epoch(epoch);
⋮----
pub struct TpuClient<
⋮----
fn send_wire_transaction_futures<'a, P, M, C>(
⋮----
let sleep_duration = SEND_TRANSACTION_INTERVAL.saturating_mul(index as u32);
let send_timeout = SEND_TIMEOUT_INTERVAL.saturating_add(sleep_duration);
⋮----
.map(|addr| {
timeout_future(
⋮----
sleep_and_send_wire_transaction_to_addr(
⋮----
wire_transaction.clone(),
⋮----
.boxed_local() // required to make types work simply
⋮----
.chain(iter::once(
⋮----
sleep_and_set_message(
⋮----
.boxed_local(), // required to make types work simply
⋮----
// Wrap an existing future with a timeout.
//
// Useful for end-users who don't need a persistent connection to each validator,
⋮----
async fn timeout_future<Fut: Future<Output = TransportResult<()>>>(
⋮----
timeout(timeout_duration, future)
⋮----
.unwrap_or_else(|_| Err(TransportError::Custom("Timed out".to_string())))
⋮----
async fn sleep_and_set_message(
⋮----
sleep(sleep_duration).await;
progress.set_message_for_confirmed_transactions(
⋮----
&format!("Sending {}/{} transactions", index + 1, num_transactions,),
⋮----
Ok(())
⋮----
async fn sleep_and_send_wire_transaction_to_addr<P, M, C>(
⋮----
send_wire_transaction_to_addr(connection_cache, &addr, wire_transaction).await
⋮----
async fn send_wire_transaction_to_addr<P, M, C>(
⋮----
let conn = connection_cache.get_nonblocking_connection(addr);
conn.send_data(&wire_transaction).await
⋮----
async fn send_wire_transaction_batch_to_addr<P, M, C>(
⋮----
conn.send_data_batch(wire_transactions).await
⋮----
pub async fn send_transaction(&self, transaction: &Transaction) -> bool {
let wire_transaction = serialize(transaction).expect("serialization should succeed");
self.send_wire_transaction(wire_transaction).await
⋮----
pub async fn send_wire_transaction(&self, wire_transaction: Vec<u8>) -> bool {
self.try_send_wire_transaction(wire_transaction)
⋮----
.is_ok()
⋮----
pub async fn try_send_transaction(&self, transaction: &Transaction) -> TransportResult<()> {
⋮----
self.try_send_wire_transaction(wire_transaction).await
⋮----
pub async fn try_send_wire_transaction(
⋮----
.unique_leader_tpu_sockets(self.fanout_slots);
⋮----
.iter()
⋮----
send_wire_transaction_to_addr(
⋮----
let results: Vec<TransportResult<()>> = join_all(futures).await;
⋮----
if last_error.is_none() {
last_error = Some(e);
⋮----
Err(if let Some(err) = last_error {
⋮----
std::io::Error::other("No sends attempted").into()
⋮----
pub async fn try_send_wire_transaction_batch(
⋮----
send_wire_transaction_batch_to_addr(
⋮----
pub async fn new(
⋮----
ConnectionCache::new(name, connection_manager, DEFAULT_CONNECTION_POOL_SIZE).unwrap(),
); // TODO: Handle error properly, as the ConnectionCache ctor is now fallible.
⋮----
/// Create a new client that disconnects when dropped
    pub async fn new_with_connection_cache(
⋮----
pub async fn new_with_connection_cache(
⋮----
LeaderTpuService::new(rpc_client.clone(), websocket_url, M::PROTOCOL, exit.clone())
⋮----
Ok(Self {
fanout_slots: config.fanout_slots.clamp(1, MAX_FANOUT_SLOTS),
⋮----
pub async fn send_and_confirm_messages_with_spinner<T: Signers + ?Sized>(
⋮----
progress_bar.set_message("Setting up...");
⋮----
.enumerate()
.map(|(i, message)| (i, Transaction::new_unsigned(message.clone())))
⋮----
progress.total_transactions = transactions.len();
let mut transaction_errors = vec![None; transactions.len()];
progress.block_height = self.rpc_client.get_block_height().await?;
for expired_blockhash_retries in (0..5).rev() {
⋮----
.get_latest_blockhash_with_commitment(self.rpc_client.commitment())
⋮----
transaction.try_sign(signers, blockhash)?;
pending_transactions.insert(transaction.signatures[0], (i, transaction));
⋮----
let num_transactions = pending_transactions.len();
// Periodically re-send all pending transactions
if Instant::now().duration_since(last_resend) > TRANSACTION_RESEND_INTERVAL {
// Prepare futures for all transactions
let mut futures = vec![];
for (index, (_i, transaction)) in pending_transactions.values().enumerate() {
let wire_transaction = serialize(transaction).unwrap();
⋮----
futures.extend(send_wire_transaction_futures(
⋮----
// Start the process of sending them all
let results = join_all(futures).await;
⋮----
.chunks(self.fanout_slots as usize)
.zip(pending_transactions.values())
⋮----
// Only report an error if every future in the chunk errored
if tx_results.iter().all(|r| r.is_err()) {
⋮----
&format!(
⋮----
let _result = self.rpc_client.send_transaction(transaction).await.ok();
⋮----
// Wait for the next block before checking for transaction statuses
⋮----
&format!("Waiting for next block, {num_transactions} transactions pending..."),
⋮----
sleep(Duration::from_millis(500)).await;
new_block_height = self.rpc_client.get_block_height().await?;
⋮----
// Collect statuses for the transactions, drop those that are confirmed
let pending_signatures = pending_transactions.keys().cloned().collect::<Vec<_>>();
⋮----
pending_signatures.chunks(MAX_GET_SIGNATURE_STATUSES_QUERY_ITEMS)
⋮----
.get_signature_statuses(pending_signatures_chunk)
⋮----
pending_signatures_chunk.iter().zip(statuses.into_iter())
⋮----
if status.satisfies_commitment(self.rpc_client.commitment()) {
if let Some((i, _)) = pending_transactions.remove(signature) {
⋮----
if status.err.is_some() {
⋮----
.println(format!("Failed transaction: {status:?}"));
⋮----
if pending_transactions.is_empty() {
return Ok(transaction_errors);
⋮----
transactions = pending_transactions.into_values().collect();
progress_bar.println(format!(
⋮----
Err(TpuSenderError::Custom("Max retries exceeded".into()))
⋮----
pub fn rpc_client(&self) -> &RpcClient {
⋮----
pub async fn shutdown(&mut self) {
self.exit.store(true, Ordering::Relaxed);
self.leader_tpu_service.join().await;
⋮----
pub fn get_connection_cache(&self) -> &Arc<ConnectionCache<P, M, C>>
⋮----
pub fn get_leader_tpu_service(&self) -> &LeaderTpuService {
⋮----
pub fn get_fanout_slots(&self) -> u64 {
⋮----
impl<P, M, C> Drop for TpuClient<P, M, C> {
fn drop(&mut self) {
⋮----
/// Service that tracks upcoming leaders and maintains an up-to-date mapping
/// of leader id to TPU socket address.
⋮----
/// of leader id to TPU socket address.
pub struct LeaderTpuService {
⋮----
pub struct LeaderTpuService {
⋮----
impl LeaderTpuService {
⋮----
let epoch_schedule = rpc_client.get_epoch_schedule().await?;
⋮----
.get_slot_with_commitment(CommitmentConfig::processed())
⋮----
let epoch = epoch_schedule.get_epoch(start_slot);
let slots_in_epoch = epoch_schedule.get_slots_in_epoch(epoch);
let last_slot_in_epoch = epoch_schedule.get_last_slot_in_epoch(epoch);
// When a cluster is starting, we observe an invalid slot range failure that goes away after a
// retry. It seems as if the leader schedule is not available, but it should be. The logic
// below retries the RPC call in case of an invalid slot range error.
⋮----
let leaders = timeout(tpu_leader_service_creation_timeout, async {
⋮----
// TODO: The root cause appears to lie within the `rpc_client.get_slot_leaders()`.
// It might be worth debugging further and trying to understand why the RPC
// call fails. There may be a bug in the `get_slot_leaders()` logic or in the
// RPC implementation
⋮----
.get_slot_leaders(start_slot, LeaderTpuCache::fanout(slots_in_epoch))
⋮----
Ok(leaders) => return Ok(leaders),
⋮----
if is_invalid_slot_range_error(&client_error) {
sleep(retry_interval).await;
⋮----
return Err(client_error);
⋮----
.map_err(|_| {
TpuSenderError::Custom(format!(
⋮----
let cluster_nodes = timeout(tpu_leader_service_creation_timeout, async {
⋮----
let cluster_nodes = rpc_client.get_cluster_nodes().await?;
// Stop once we find at least one leader's contact info
if cluster_nodes.iter().any(|rpc_contact_info| {
⋮----
.map(|pubkey| leaders.contains(&pubkey))
.unwrap_or(false)
⋮----
let pubsub_client = if !websocket_url.is_empty() {
Some(PubsubClient::new(websocket_url).await?)
⋮----
let t_leader_tpu_service = Some({
let recent_slots = recent_slots.clone();
let leader_tpu_cache = leader_tpu_cache.clone();
⋮----
Ok(LeaderTpuService {
⋮----
pub async fn join(&mut self) {
if let Some(t_handle) = self.t_leader_tpu_service.take() {
t_handle.await.unwrap().unwrap();
⋮----
pub fn estimated_current_slot(&self) -> Slot {
self.recent_slots.estimated_current_slot()
⋮----
pub fn unique_leader_tpu_sockets(&self, fanout_slots: u64) -> Vec<SocketAddr> {
let current_slot = self.recent_slots.estimated_current_slot();
⋮----
.read()
.unwrap()
.get_unique_leader_sockets(current_slot, fanout_slots)
⋮----
pub fn leader_tpu_sockets(&self, fanout_slots: u64) -> Vec<SocketAddr> {
⋮----
.get_leader_sockets(current_slot, fanout_slots)
⋮----
async fn run(
⋮----
async fn run_cache_refresher(
⋮----
while !exit.load(Ordering::Relaxed) {
sleep(Duration::from_millis(sleep_ms)).await;
⋮----
let cache_update_info = maybe_fetch_cache_info(
⋮----
if cache_update_info.has_some() {
let mut leader_tpu_cache = leader_tpu_cache.write().unwrap();
let (has_error, cluster_refreshed) = leader_tpu_cache.update_all(cache_update_info);
⋮----
async fn run_slot_watcher(
⋮----
return Ok(());
⋮----
let (mut notifications, unsubscribe) = pubsub_client.slot_updates_subscribe().await?;
⋮----
while let Ok(Some(update)) = timeout(SLOT_UPDATE_TIMEOUT, notifications.next()).await {
⋮----
SlotUpdate::Completed { slot, .. } => slot.saturating_add(1),
⋮----
recent_slots.record_slot(current_slot);
⋮----
drop(notifications);
unsubscribe().await;
pubsub_client.shutdown().await?;
⋮----
async fn maybe_fetch_cache_info(
⋮----
let maybe_cluster_nodes = if last_cluster_refresh.elapsed() > Duration::from_secs(5 * 60) {
Some(rpc_client.get_cluster_nodes().await)
⋮----
let estimated_current_slot = recent_slots.estimated_current_slot();
⋮----
let leader_tpu_cache = leader_tpu_cache.read().unwrap();
leader_tpu_cache.slot_info()
⋮----
Some(rpc_client.get_epoch_schedule().await)
⋮----
let maybe_slot_leaders = if estimated_current_slot >= last_slot.saturating_sub(MAX_FANOUT_SLOTS)
⋮----
Some(
⋮----
.get_slot_leaders(
⋮----
fn is_invalid_slot_range_error(client_error: &ClientError) -> bool {
⋮----
client_error.kind()
⋮----
&& message.contains("Invalid slot range: leader schedule for epoch");

================
File: tpu-client/src/lib.rs
================
pub mod nonblocking;
pub mod tpu_client;

================
File: tpu-client/src/tpu_client.rs
================
pub use crate::nonblocking::tpu_client::TpuSenderError;
⋮----
pub type Result<T> = std::result::Result<T, TpuSenderError>;
⋮----
pub struct TpuClientConfig {
⋮----
impl Default for TpuClientConfig {
fn default() -> Self {
⋮----
pub struct TpuClient<
⋮----
pub fn send_transaction(&self, transaction: &Transaction) -> bool {
self.invoke(self.tpu_client.send_transaction(transaction))
⋮----
pub fn send_wire_transaction(&self, wire_transaction: Vec<u8>) -> bool {
self.invoke(self.tpu_client.send_wire_transaction(wire_transaction))
⋮----
pub fn try_send_transaction(&self, transaction: &Transaction) -> TransportResult<()> {
self.invoke(self.tpu_client.try_send_transaction(transaction))
⋮----
pub fn send_transaction_to_upcoming_leaders(
⋮----
Arc::new(bincode::serialize(&transaction).expect("should serialize transaction"));
⋮----
.get_leader_tpu_service()
.unique_leader_tpu_sockets(self.tpu_client.get_fanout_slots());
⋮----
let cache = self.tpu_client.get_connection_cache();
let conn = cache.get_connection(tpu_address);
if let Err(err) = conn.send_data_async(wire_transaction.clone()) {
last_error = Some(err);
⋮----
Err(err)
⋮----
Err(std::io::Error::other("No sends attempted").into())
⋮----
Ok(())
⋮----
pub fn try_send_transaction_batch(&self, transactions: &[Transaction]) -> TransportResult<()> {
⋮----
.into_par_iter()
.map(|tx| bincode::serialize(&tx).expect("serialize Transaction in send_batch"))
⋮----
self.invoke(
⋮----
.try_send_wire_transaction_batch(wire_transactions),
⋮----
pub fn try_send_wire_transaction(&self, wire_transaction: Vec<u8>) -> TransportResult<()> {
self.invoke(self.tpu_client.try_send_wire_transaction(wire_transaction))
⋮----
pub fn try_send_wire_transaction_batch(
⋮----
pub fn new(
⋮----
rpc_client.get_inner_client().clone(),
⋮----
tokio::task::block_in_place(|| rpc_client.runtime().block_on(create_tpu_client))?;
Ok(Self {
⋮----
/// Create a new client that disconnects when dropped
    pub fn new_with_connection_cache(
⋮----
pub fn new_with_connection_cache(
⋮----
pub fn send_and_confirm_messages_with_spinner<T: Signers + ?Sized>(
⋮----
.send_and_confirm_messages_with_spinner(messages, signers),
⋮----
pub fn rpc_client(&self) -> &RpcClient {
⋮----
fn invoke<T, F: std::future::Future<Output = T>>(&self, f: F) -> T {
// `block_on()` panics if called within an asynchronous execution context. Whereas
// `block_in_place()` only panics if called from a current_thread runtime, which is the
// lesser evil.
tokio::task::block_in_place(move || self.rpc_client.runtime().block_on(f))
⋮----
// Methods below are required for calls to client.async_transfer()
// where client is of type TpuClient<P, M, C>
impl<P, M, C> AsyncClient for TpuClient<P, M, C>
⋮----
fn async_send_versioned_transaction(
⋮----
bincode::serialize(&transaction).expect("serialize Transaction in send_batch");
self.send_wire_transaction(wire_transaction);
Ok(transaction.signatures[0])
⋮----
fn async_send_versioned_transaction_batch(
⋮----
self.try_send_wire_transaction_batch(buffers)?;
⋮----
// 48 chosen because it's unlikely that 12 leaders in a row will miss their slots
⋮----
pub(crate) struct RecentLeaderSlots(Arc<RwLock<VecDeque<Slot>>>);
impl RecentLeaderSlots {
pub(crate) fn new(current_slot: Slot) -> Self {
⋮----
recent_slots.push_back(current_slot);
Self(Arc::new(RwLock::new(recent_slots)))
⋮----
pub(crate) fn record_slot(&self, current_slot: Slot) {
let mut recent_slots = self.0.write().unwrap();
⋮----
while recent_slots.len() > 12 {
recent_slots.pop_front();
⋮----
pub(crate) fn estimated_current_slot(&self) -> Slot {
let mut recent_slots: Vec<Slot> = self.0.read().unwrap().iter().cloned().collect();
assert!(!recent_slots.is_empty());
recent_slots.sort_unstable();
let max_index = recent_slots.len() - 1;
⋮----
.into_iter()
.rev()
.find(|slot| *slot <= max_reasonable_current_slot)
.unwrap()
⋮----
fn from(recent_slots: Vec<Slot>) -> Self {
⋮----
Self(Arc::new(RwLock::new(recent_slots.into_iter().collect())))
⋮----
mod tests {
⋮----
fn assert_slot(recent_slots: RecentLeaderSlots, expected_slot: Slot) {
assert_eq!(recent_slots.estimated_current_slot(), expected_slot);
⋮----
fn test_recent_leader_slots() {
assert_slot(RecentLeaderSlots::new(0), 0);
let mut recent_slots: Vec<Slot> = (1..=12).collect();
assert_slot(RecentLeaderSlots::from(recent_slots.clone()), 12);
recent_slots.reverse();
assert_slot(RecentLeaderSlots::from(recent_slots), 12);
assert_slot(
RecentLeaderSlots::from(vec![0, 1 + MAX_SLOT_SKIP_DISTANCE]),
⋮----
RecentLeaderSlots::from(vec![0, 2 + MAX_SLOT_SKIP_DISTANCE]),
⋮----
assert_slot(RecentLeaderSlots::from(vec![1]), 1);
assert_slot(RecentLeaderSlots::from(vec![1, 100]), 1);
assert_slot(RecentLeaderSlots::from(vec![1, 2, 100]), 2);
assert_slot(RecentLeaderSlots::from(vec![1, 2, 3, 100]), 3);
assert_slot(RecentLeaderSlots::from(vec![1, 2, 3, 99, 100]), 3);

================
File: tpu-client/.gitignore
================
/target/
/farf/

================
File: tpu-client/Cargo.toml
================
[package]
name = "solana-tpu-client"
description = "Solana TPU Client"
documentation = "https://docs.rs/solana-tpu-client"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = ["spinner"]
agave-unstable-api = []
# Support tpu-client methods that feature a spinner progress bar for
# command-line interfaces
spinner = [
    "dep:indicatif",
    "dep:solana-message",
    "solana-rpc-client/spinner",
]

[dependencies]
async-trait = { workspace = true }
bincode = { workspace = true }
futures-util = { workspace = true }
indexmap = { workspace = true }
indicatif = { workspace = true, optional = true }
log = { workspace = true }
rayon = { workspace = true }
solana-client-traits = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-connection-cache = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true, optional = true }
solana-net-utils = { workspace = true }
solana-pubkey = { workspace = true }
solana-pubsub-client = { workspace = true }
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }





================================================================
End of Codebase
================================================================
