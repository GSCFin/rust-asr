This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
.buildkite/
  hooks/
    post-checkout
    post-command
    pre-command
  scripts/
    build-stable.sh
    build-stable.test.sh
    common.sh
    common.test.sh
    func-assert-eq.sh
    test-all.sh
    trigger-github-actions-windows-build.sh
  pipeline-upload.sh
  solana-private.sh
.config/
  nextest.toml
.github/
  ISSUE_TEMPLATE/
    0-community.md
    1-core-contributor.md
    2-feature-gate.yml
  scripts/
    add-team-to-ghsa.sh
    check-changelog.sh
    downstream-project-spl-common.sh
    downstream-project-spl-install-deps.sh
    install-all-deps.sh
    install-openssl.sh
    install-proto.sh
    purge-ubuntu-runner.sh
  workflows/
    add-team-to-ghsa.yml
    benchmark.yml
    cargo.yml
    changelog-label.yml
    client-targets.yml
    crate-check.yml
    dependabot-pr.yml
    docs.yml
    downstream-project-anchor.yml
    downstream-project-spl-nightly.yml
    downstream-project-spl.yml
    error-reporting.yml
    label-actions.yml
    publish-windows-tarball.yml
    rebase.yaml
    release.yml
    verify-packets.yml
  CODEOWNERS
  dependabot.yml
  label-actions.yml
  PULL_REQUEST_TEMPLATE.md
  RELEASE_TEMPLATE.md
account-decoder/
  src/
    lib.rs
    parse_account_data.rs
    parse_address_lookup_table.rs
    parse_bpf_loader.rs
    parse_config.rs
    parse_nonce.rs
    parse_stake.rs
    parse_sysvar.rs
    parse_token_extension.rs
    parse_token.rs
    parse_vote.rs
    validator_info.rs
  Cargo.toml
account-decoder-client-types/
  src/
    lib.rs
    token.rs
  Cargo.toml
accounts-cluster-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
accounts-db/
  benches/
    accounts_index.rs
    accounts.rs
    bench_accounts_file.rs
    bench_hashing.rs
    bench_lock_accounts.rs
    bench_serde.rs
    read_only_accounts_cache.rs
    utils.rs
  src/
    account_storage/
      stored_account_info.rs
    accounts_db/
      accounts_db_config.rs
      geyser_plugin_utils.rs
      stats.rs
      tests.rs
    accounts_index/
      account_map_entry.rs
      accounts_index_storage.rs
      bucket_map_holder.rs
      in_mem_accounts_index.rs
      iter.rs
      roots_tracker.rs
      secondary.rs
      stats.rs
    append_vec/
      meta.rs
      test_utils.rs
    rolling_bit_field/
      iterators.rs
    tiered_storage/
      byte_block.rs
      error.rs
      file.rs
      footer.rs
      hot.rs
      index.rs
      meta.rs
      mmap_utils.rs
      owners.rs
      readable.rs
      test_utils.rs
    account_info.rs
    account_locks.rs
    account_storage_reader.rs
    account_storage.rs
    accounts_cache.rs
    accounts_db.rs
    accounts_file.rs
    accounts_hash.rs
    accounts_index.rs
    accounts_update_notifier_interface.rs
    accounts.rs
    active_stats.rs
    ancestors.rs
    ancient_append_vecs.rs
    append_vec.rs
    blockhash_queue.rs
    contains.rs
    is_loadable.rs
    is_zero_lamport.rs
    lib.rs
    obsolete_accounts.rs
    partitioned_rewards.rs
    pubkey_bins.rs
    read_only_accounts_cache.rs
    rolling_bit_field.rs
    sorted_storages.rs
    stake_rewards.rs
    storable_accounts.rs
    tiered_storage.rs
    utils.rs
    waitable_condvar.rs
  store-histogram/
    src/
      main.rs
    Cargo.toml
  store-tool/
    src/
      main.rs
    Cargo.toml
  tests/
    read_only_accounts_cache.rs
  Cargo.toml
bam-banking-bench/
  src/
    main.rs
    mock_bam_server.rs
  .gitignore
  Cargo.toml
bam-local-cluster/
  examples/
    example_config.toml
  src/
    cluster_manager.rs
    config.rs
    lib.rs
    main.rs
  Cargo.toml
  README.md
banking-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
banking-stage-ingress-types/
  src/
    lib.rs
  Cargo.toml
banks-client/
  src/
    error.rs
    lib.rs
  Cargo.toml
banks-interface/
  src/
    lib.rs
  Cargo.toml
banks-server/
  src/
    banks_server.rs
    lib.rs
  Cargo.toml
bench-streamer/
  src/
    main.rs
  .gitignore
  Cargo.toml
bench-tps/
  src/
    bench.rs
    cli.rs
    keypairs.rs
    lib.rs
    log_transaction_service.rs
    main.rs
    perf_utils.rs
    rpc_with_retry_utils.rs
    send_batch.rs
  tests/
    fixtures/
      spl_instruction_padding.so
    bench_tps.rs
  .gitignore
  Cargo.toml
bench-vote/
  src/
    main.rs
  Cargo.toml
bloom/
  benches/
    bloom.rs
  src/
    bloom.rs
    lib.rs
  Cargo.toml
bucket_map/
  src/
    bucket_api.rs
    bucket_item.rs
    bucket_map.rs
    bucket_stats.rs
    bucket_storage.rs
    bucket.rs
    index_entry.rs
    lib.rs
    restart.rs
  tests/
    bucket_map.rs
  Cargo.toml
builtins/
  src/
    core_bpf_migration.rs
    lib.rs
    prototype.rs
  Cargo.toml
builtins-default-costs/
  src/
    lib.rs
  Cargo.toml
bundle/
  src/
    lib.rs
  Cargo.toml
cargo-registry/
  src/
    client.rs
    crate_handler.rs
    main.rs
    response_builder.rs
    sparse_index.rs
  Cargo.toml
ci/
  bench/
    common.sh
    part1.sh
    part2.sh
  common/
    limit-threads.sh
    shared-functions.sh
  coverage/
    common.sh
    part-1.sh
    part-2.sh
    part-3.sh
  docker/
    build.sh
    Dockerfile
    env.sh
    README.md
  downstream-projects/
    common.sh
    func-openbook-dex.sh
    func-spl.sh
    run-all.sh
    run-openbook-dex.sh
    run-spl.sh
  semver_bash/
    LICENSE
    README.md
    semver_test.sh
    semver.sh
  stable/
    common.sh
    run-all.sh
    run-local-cluster-partially.sh
    run-localnet.sh
    run-partition.sh
  xtask/
    src/
      commands/
        bump_version.rs
        hello.rs
      commands.rs
      common.rs
      main.rs
    Cargo.toml
  _
  .gitignore
  buildkite-pipeline.sh
  buildkite-secondary.yml
  buildkite-solana-private.sh
  channel_restriction.sh
  channel-info.sh
  check-channel-version.sh
  check-crates.sh
  check-install-all.sh
  crate-version.sh
  do-audit.sh
  docker-run-default-image.sh
  docker-run.sh
  env.sh
  format-url.sh
  hoover.sh
  intercept.sh
  localnet-sanity.sh
  nits.sh
  order-crates-for-publishing.py
  platform-tools-info.sh
  publish-crate.sh
  publish-installer.sh
  publish-metrics-dashboard.sh
  publish-tarball.sh
  run-local.sh
  run-sanity.sh
  rust-version.sh
  shellcheck.sh
  test-checks.sh
  test-coverage.sh
  test-dev-context-only-utils.sh
  test-downstream-builds.sh
  test-frozen-abi.sh
  test-miri.sh
  test-sanity.sh
  test-shuttle.sh
  test-stable.sh
  test-verify-packets-gossip.sh
  test.sh
  upload-benchmark.sh
  upload-ci-artifact.sh
  upload-github-release-asset.sh
clap-utils/
  src/
    compute_budget.rs
    compute_unit_price.rs
    fee_payer.rs
    input_parsers.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
clap-v3-utils/
  src/
    input_parsers/
      mod.rs
      signer.rs
    keygen/
      derivation_path.rs
      mnemonic.rs
      mod.rs
    compute_budget.rs
    fee_payer.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
cli/
  src/
    address_lookup_table.rs
    checks.rs
    clap_app.rs
    cli.rs
    cluster_query.rs
    compute_budget.rs
    feature.rs
    inflation.rs
    lib.rs
    main.rs
    memo.rs
    nonce.rs
    program_v4.rs
    program.rs
    spend_utils.rs
    stake.rs
    test_utils.rs
    validator_info.rs
    vote.rs
    wallet.rs
  tests/
    fixtures/
      alt_bn128.so
      build.sh
      noop_large.so
      noop.so
    address_lookup_table.rs
    cluster_query.rs
    nonce.rs
    program.rs
    request_airdrop.rs
    stake.rs
    transfer.rs
    validator_info.rs
    vote.rs
  .gitignore
  Cargo.toml
cli-config/
  src/
    config_input.rs
    config.rs
    lib.rs
  Cargo.toml
cli-output/
  src/
    cli_output.rs
    cli_version.rs
    display.rs
    lib.rs
  Cargo.toml
client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    connection_cache.rs
    lib.rs
    send_and_confirm_transactions_in_parallel.rs
    tpu_client.rs
    transaction_executor.rs
  .gitignore
  Cargo.toml
client-test/
  tests/
    client.rs
    send_and_confirm_transactions_in_parallel.rs
  .gitignore
  Cargo.toml
compute-budget/
  src/
    compute_budget_limits.rs
    compute_budget.rs
    lib.rs
  Cargo.toml
compute-budget-instruction/
  benches/
    process_compute_budget_instructions.rs
  src/
    builtin_programs_filter.rs
    compute_budget_instruction_details.rs
    compute_budget_program_id_filter.rs
    instructions_processor.rs
    lib.rs
  Cargo.toml
connection-cache/
  src/
    nonblocking/
      client_connection.rs
      mod.rs
    client_connection.rs
    connection_cache_stats.rs
    connection_cache.rs
    lib.rs
  Cargo.toml
core/
  benches/
    banking_stage.rs
    banking_trace.rs
    consensus.rs
    consumer.rs
    gen_keys.rs
    proto_to_packet.rs
    receive_and_buffer_utils.rs
    receive_and_buffer.rs
    scheduler.rs
    shredder.rs
    sigverify_stage.rs
  src/
    banking_stage/
      transaction_scheduler/
        bam_receive_and_buffer.rs
        bam_scheduler.rs
        bam_utils.rs
        batch_id_generator.rs
        greedy_scheduler.rs
        in_flight_tracker.rs
        mod.rs
        prio_graph_scheduler.rs
        receive_and_buffer.rs
        scheduler_common.rs
        scheduler_controller.rs
        scheduler_error.rs
        scheduler_metrics.rs
        scheduler.rs
        transaction_priority_id.rs
        transaction_state_container.rs
        transaction_state.rs
      committer.rs
      consume_worker.rs
      consumer.rs
      decision_maker.rs
      latest_validator_vote_packet.rs
      leader_slot_metrics.rs
      leader_slot_timing_metrics.rs
      progress_tracker.rs
      qos_service.rs
      read_write_account_set.rs
      scheduler_messages.rs
      tpu_to_pack.rs
      unified_scheduler.rs
      vote_packet_receiver.rs
      vote_storage.rs
      vote_worker.rs
    block_creation_loop/
      stats.rs
    bundle_stage/
      bundle_account_locker.rs
      bundle_consumer.rs
      bundle_packet_deserializer.rs
      bundle_stage_leader_metrics.rs
      bundle_storage.rs
    cluster_slots_service/
      cluster_slots.rs
      slot_supporters.rs
    consensus/
      fork_choice.rs
      heaviest_subtree_fork_choice.rs
      latest_validator_votes_for_frozen_banks.rs
      progress_map.rs
      tower_storage.rs
      tower_vote_state.rs
      tower1_14_11.rs
      tower1_7_14.rs
      tree_diff.rs
      vote_stake_tracker.rs
    forwarding_stage/
      packet_container.rs
    proxy/
      auth.rs
      block_engine_stage.rs
      fetch_stage_manager.rs
      mod.rs
      relayer_stage.rs
    repair/
      ancestor_hashes_service.rs
      cluster_slot_state_verifier.rs
      duplicate_repair_status.rs
      malicious_repair_handler.rs
      mod.rs
      outstanding_requests.rs
      packet_threshold.rs
      quic_endpoint.rs
      repair_generic_traversal.rs
      repair_handler.rs
      repair_response.rs
      repair_service.rs
      repair_weight.rs
      repair_weighted_traversal.rs
      request_response.rs
      result.rs
      serve_repair_service.rs
      serve_repair.rs
      standard_repair_handler.rs
    snapshot_packager_service/
      snapshot_gossip_manager.rs
    tip_manager/
      tip_distribution.rs
      tip_payment.rs
    admin_rpc_post_init.rs
    bam_connection.rs
    bam_dependencies.rs
    bam_manager.rs
    banking_simulation.rs
    banking_stage.rs
    banking_trace.rs
    block_creation_loop.rs
    bundle_sigverify_stage.rs
    bundle_stage.rs
    bundle.rs
    cluster_info_vote_listener.rs
    cluster_slots_service.rs
    commitment_service.rs
    completed_data_sets_service.rs
    consensus.rs
    cost_update_service.rs
    drop_bank_service.rs
    fetch_stage.rs
    forwarding_stage.rs
    gen_keys.rs
    lib.rs
    mock_alpenglow_consensus.rs
    next_leader.rs
    optimistic_confirmation_verifier.rs
    packet_bundle.rs
    replay_stage.rs
    resource_limits.rs
    result.rs
    sample_performance_service.rs
    scheduler_bindings_server.rs
    shred_fetch_stage.rs
    sigverify_stage.rs
    sigverify.rs
    snapshot_packager_service.rs
    staked_nodes_updater_service.rs
    stats_reporter_service.rs
    system_monitor_service.rs
    tip_manager.rs
    tpu_entry_notifier.rs
    tpu.rs
    tvu.rs
    unfrozen_gossip_verified_vote_hashes.rs
    validator.rs
    vortexor_receiver_adapter.rs
    vote_simulator.rs
    voting_service.rs
    warm_quic_cache_service.rs
    window_service.rs
  tests/
    bam_connection.rs
    fork-selection.rs
    scheduler_cost_adjustment.rs
    snapshots.rs
    unified_scheduler.rs
  .gitignore
  Cargo.toml
cost-model/
  benches/
    cost_model.rs
    cost_tracker.rs
  src/
    block_cost_limits.rs
    cost_model.rs
    cost_tracker_post_analysis.rs
    cost_tracker.rs
    lib.rs
    transaction_cost.rs
  Cargo.toml
curves/
  curve25519/
    src/
      curve_syscall_traits.rs
      edwards.rs
      errors.rs
      lib.rs
      ristretto.rs
      scalar.rs
    .gitignore
    Cargo.toml
dev/
  Dockerfile
dev-bins/
  .config/
    nextest.toml
  Cargo.toml
docker-solana/
  .gitignore
  build.sh
  Dockerfile
  README.md
docs/
  art/
    fork-generation.bob
    forks-pruned.bob
    forks-pruned2.bob
    forks.bob
    passive-staking-callflow.msc
    retransmit_stage.bob
    runtime.bob
    sdk-tools.bob
    spv-bank-hash.bob
    spv-block-merkle.bob
    tpu.bob
    transaction.bob
    tvu.bob
    validator-proposal.bob
    validator.bob
  components/
    Card.jsx
    HomeCtaLinks.jsx
  src/
    cli/
      examples/
        _category_.json
        choose-a-cluster.md
        delegate-stake.md
        deploy-a-program.md
        durable-nonce.md
        offline-signing.md
        sign-offchain-message.md
        test-validator.md
        transfer-tokens.md
      wallets/
        hardware/
          _category_.json
          index.md
          ledger.md
        _category_.json
        file-system.md
        index.md
        paper.md
      .usage.md.header
      index.md
      install.md
      intro.md
    clusters/
      available.md
      benchmark.md
      index.md
      metrics.md
      testnet.md
    consensus/
      commitments.md
      fork-generation.md
      leader-rotation.md
      managing-forks.md
      stake-delegation-and-rewards.md
      synchronization.md
      turbine-block-propagation.md
      vote-signing.md
    css/
      custom.css
    implemented-proposals/
      ed_overview/
        ed_validation_client_economics/
          ed_vce_overview.md
          ed_vce_state_validation_protocol_based_rewards.md
          ed_vce_state_validation_transaction_fees.md
          ed_vce_validation_stake_delegation.md
        ed_economic_sustainability.md
        ed_mvp.md
        ed_overview.md
        ed_references.md
        ed_storage_rent_economics.md
      abi-management.md
      bank-timestamp-correction.md
      commitment.md
      durable-tx-nonces.md
      epoch_accounts_hash.md
      index.md
      installer.md
      instruction_introspection.md
      leader-leader-transition.md
      leader-validator-transition.md
      persistent-account-storage.md
      readonly-accounts.md
      reliable-vote-transmission.md
      rent.md
      repair-service.md
      rpc-transaction-history.md
      snapshot-verification.md
      staking-rewards.md
      testing-programs.md
      tower-bft.md
      transaction-fees.md
      validator-timestamp-oracle.md
    operations/
      best-practices/
        _category_.json
        general.md
        monitoring.md
        security.md
      guides/
        _category_.json
        restart-cluster.md
        validator-failover.md
        validator-info.md
        validator-monitor.md
        validator-stake.md
        validator-start.md
        validator-troubleshoot.md
        vote-accounts.md
      _category_.json
      index.md
      prerequisites.md
      requirements.md
      setup-a-validator.md
      setup-an-rpc-node.md
      validator-or-rpc-node.md
    pages/
      styles.module.css
    proposals/
      accepted-design-proposals.md
      accounts-db-replication.md
      bankless-leader.md
      block-confirmation.md
      cluster-test-framework.md
      comprehensive-compute-fees.md
      embedding-move.md
      fee_transaction_priority.md
      handle-duplicate-block.md
      interchain-transaction-verification.md
      ledger-replication-to-implement.md
      log_data.md
      off-chain-message-signing.md
      optimistic_confirmation.md
      optimistic-confirmation-and-slashing.md
      optimistic-transaction-propagation-signal.md
      partitioned-inflationary-rewards-distribution.md
      program-instruction-macro.md
      return-data.md
      rip-curl.md
      simple-payment-and-state-verification.md
      slashing.md
      snapshot-verification.md
      tick-verification.md
      timely-vote-credits.md
      validator-proposal.md
      versioned-transactions.md
      vote-signing-to-implement.md
    runtime/
      zk-docs/
        ciphertext_ciphertext_equality.pdf
        ciphertext_commitment_equality.pdf
        ciphertext_validity.pdf
        percentage_with_cap.pdf
        pubkey_proof.pdf
        twisted_elgamal.pdf
        zero_proof.pdf
      programs.md
      sysvars.md
      zk-elgamal-proof.md
    theme/
      Footer/
        index.js
        styles.module.css
    validator/
      anatomy.md
      blockstore.md
      geyser.md
      gossip.md
      runtime.md
      tpu.md
      tvu.md
    architecture.md
    backwards-compatibility.md
    faq.md
    index.mdx
    proposals.md
    what-is-a-validator.md
    what-is-an-rpc-node.md
  static/
    img/
      favicon.ico
    katex/
      contrib/
        auto-render.js
        auto-render.min.js
        auto-render.mjs
        copy-tex.css
        copy-tex.js
        copy-tex.min.css
        copy-tex.min.js
        copy-tex.mjs
        mathtex-script-type.js
        mathtex-script-type.min.js
        mathtex-script-type.mjs
        mhchem.js
        mhchem.min.js
        mhchem.mjs
        render-a11y-string.js
        render-a11y-string.min.js
        render-a11y-string.mjs
      fonts/
        KaTeX_AMS-Regular.ttf
        KaTeX_AMS-Regular.woff
        KaTeX_AMS-Regular.woff2
        KaTeX_Caligraphic-Bold.ttf
        KaTeX_Caligraphic-Bold.woff
        KaTeX_Caligraphic-Bold.woff2
        KaTeX_Caligraphic-Regular.ttf
        KaTeX_Caligraphic-Regular.woff
        KaTeX_Caligraphic-Regular.woff2
        KaTeX_Fraktur-Bold.ttf
        KaTeX_Fraktur-Bold.woff
        KaTeX_Fraktur-Bold.woff2
        KaTeX_Fraktur-Regular.ttf
        KaTeX_Fraktur-Regular.woff
        KaTeX_Fraktur-Regular.woff2
        KaTeX_Main-Bold.ttf
        KaTeX_Main-Bold.woff
        KaTeX_Main-Bold.woff2
        KaTeX_Main-BoldItalic.ttf
        KaTeX_Main-BoldItalic.woff
        KaTeX_Main-BoldItalic.woff2
        KaTeX_Main-Italic.ttf
        KaTeX_Main-Italic.woff
        KaTeX_Main-Italic.woff2
        KaTeX_Main-Regular.ttf
        KaTeX_Main-Regular.woff
        KaTeX_Main-Regular.woff2
        KaTeX_Math-BoldItalic.ttf
        KaTeX_Math-BoldItalic.woff
        KaTeX_Math-BoldItalic.woff2
        KaTeX_Math-Italic.ttf
        KaTeX_Math-Italic.woff
        KaTeX_Math-Italic.woff2
        KaTeX_SansSerif-Bold.ttf
        KaTeX_SansSerif-Bold.woff
        KaTeX_SansSerif-Bold.woff2
        KaTeX_SansSerif-Italic.ttf
        KaTeX_SansSerif-Italic.woff
        KaTeX_SansSerif-Italic.woff2
        KaTeX_SansSerif-Regular.ttf
        KaTeX_SansSerif-Regular.woff
        KaTeX_SansSerif-Regular.woff2
        KaTeX_Script-Regular.ttf
        KaTeX_Script-Regular.woff
        KaTeX_Script-Regular.woff2
        KaTeX_Size1-Regular.ttf
        KaTeX_Size1-Regular.woff
        KaTeX_Size1-Regular.woff2
        KaTeX_Size2-Regular.ttf
        KaTeX_Size2-Regular.woff
        KaTeX_Size2-Regular.woff2
        KaTeX_Size3-Regular.ttf
        KaTeX_Size3-Regular.woff
        KaTeX_Size3-Regular.woff2
        KaTeX_Size4-Regular.ttf
        KaTeX_Size4-Regular.woff
        KaTeX_Size4-Regular.woff2
        KaTeX_Typewriter-Regular.ttf
        KaTeX_Typewriter-Regular.woff
        KaTeX_Typewriter-Regular.woff2
      katex.css
      katex.js
      katex.min.css
      katex.min.js
      katex.mjs
      README.md
    .nojekyll
  .eslintignore
  .eslintrc
  .gitignore
  .prettierignore
  .prettierrc.json
  babel.config.js
  build-cli-usage.sh
  build.sh
  convert-ascii-to-svg.sh
  deploy.sh
  docusaurus.config.js
  offline-cmd-md-links.sh
  package.json
  README.md
  set-solana-release-tag.sh
  sidebars.js
dos/
  src/
    cli.rs
    lib.rs
    main.rs
  Cargo.toml
download-utils/
  src/
    lib.rs
  Cargo.toml
entry/
  benches/
    entry_sigverify.rs
  src/
    entry.rs
    lib.rs
    poh.rs
    wincode.rs
  Cargo.toml
faucet/
  src/
    bin/
      faucet.rs
    faucet_mock.rs
    faucet.rs
    lib.rs
  tests/
    local-faucet.rs
  .gitignore
  Cargo.toml
feature-set/
  src/
    lib.rs
  Cargo.toml
fee/
  src/
    lib.rs
  Cargo.toml
fs/
  src/
    io_uring/
      dir_remover.rs
      file_creator.rs
      memory.rs
      mod.rs
      sequential_file_reader.rs
    buffered_reader.rs
    dirs.rs
    file_io.rs
    lib.rs
  Cargo.toml
genesis/
  src/
    address_generator.rs
    genesis_accounts.rs
    lib.rs
    main.rs
    stakes.rs
    unlocks.rs
  .gitignore
  Cargo.toml
  README.md
genesis-utils/
  src/
    lib.rs
    open.rs
  Cargo.toml
geyser-plugin-interface/
  src/
    geyser_plugin_interface.rs
    lib.rs
  Cargo.toml
  README.md
geyser-plugin-manager/
  src/
    accounts_update_notifier.rs
    block_metadata_notifier_interface.rs
    block_metadata_notifier.rs
    entry_notifier.rs
    geyser_plugin_manager.rs
    geyser_plugin_service.rs
    lib.rs
    slot_status_notifier.rs
    slot_status_observer.rs
    transaction_notifier.rs
  Cargo.toml
gossip/
  benches/
    crds_gossip_pull.rs
    crds_shards.rs
    crds.rs
    weighted_shuffle.rs
  src/
    cluster_info_metrics.rs
    cluster_info.rs
    contact_info.rs
    crds_data.rs
    crds_entry.rs
    crds_filter.rs
    crds_gossip_error.rs
    crds_gossip_pull.rs
    crds_gossip_push.rs
    crds_gossip.rs
    crds_shards.rs
    crds_value.rs
    crds.rs
    deprecated.rs
    duplicate_shred_handler.rs
    duplicate_shred_listener.rs
    duplicate_shred.rs
    epoch_slots.rs
    epoch_specs.rs
    gossip_error.rs
    gossip_service.rs
    legacy_contact_info.rs
    lib.rs
    node.rs
    ping_pong.rs
    protocol.rs
    push_active_set.rs
    received_cache.rs
    restart_crds_values.rs
    tlv.rs
    weighted_shuffle.rs
    wire_format_tests.rs
  tests/
    crds_gossip.rs
    gossip.rs
  .gitignore
  Cargo.toml
gossip-bin/
  src/
    main.rs
  Cargo.toml
install/
  src/
    bin/
      agave-install-init.rs
    build_env.rs
    command.rs
    config.rs
    defaults.rs
    lib.rs
    main.rs
    stop_process.rs
    update_manifest.rs
  .gitignore
  agave-install-init.sh
  build.rs
  Cargo.toml
  install-help.sh
io-uring/
  src/
    lib.rs
    ring.rs
    slab.rs
  Cargo.toml
jito-protos/
  src/
    lib.rs
  build.rs
  Cargo.toml
keygen/
  src/
    keygen.rs
  .gitignore
  Cargo.toml
lattice-hash/
  benches/
    bench_lt_hash.rs
  src/
    lib.rs
    lt_hash.rs
  Cargo.toml
ledger/
  benches/
    blockstore.rs
    make_shreds_from_entries.rs
    protobuf.rs
  proptest-regressions/
    blockstore_meta.txt
  src/
    blockstore/
      blockstore_purge.rs
      column.rs
      error.rs
    leader_schedule/
      identity_keyed.rs
      vote_keyed.rs
    shred/
      common.rs
      merkle_tree.rs
      merkle.rs
      payload.rs
      shred_code.rs
      shred_data.rs
      stats.rs
      traits.rs
      wire.rs
    ancestor_iterator.rs
    bank_forks_utils.rs
    bigtable_delete.rs
    bigtable_upload_service.rs
    bigtable_upload.rs
    bit_vec.rs
    block_error.rs
    blockstore_cleanup_service.rs
    blockstore_db.rs
    blockstore_meta.rs
    blockstore_metric_report_service.rs
    blockstore_metrics.rs
    blockstore_options.rs
    blockstore_processor.rs
    blockstore.rs
    entry_notifier_interface.rs
    entry_notifier_service.rs
    genesis_utils.rs
    leader_schedule_cache.rs
    leader_schedule_utils.rs
    leader_schedule.rs
    lib.rs
    next_slots_iterator.rs
    rooted_slot_iterator.rs
    shred.rs
    shredder.rs
    sigverify_shreds.rs
    slot_stats.rs
    staking_utils.rs
    token_balances.rs
    transaction_address_lookup_table_scanner.rs
    transaction_balances.rs
    use_snapshot_archives_at_startup.rs
    wire_format_tests.rs
  tests/
    blockstore.rs
    shred.rs
  .gitignore
  Cargo.toml
ledger-tool/
  src/
    args.rs
    bigtable.rs
    blockstore.rs
    error.rs
    ledger_path.rs
    ledger_utils.rs
    main.rs
    output.rs
    program.rs
  tests/
    basic.rs
  .gitignore
  Cargo.toml
local-cluster/
  src/
    cluster_tests.rs
    cluster.rs
    integration_tests.rs
    lib.rs
    local_cluster_snapshot_utils.rs
    local_cluster.rs
    validator_configs.rs
  tests/
    local_cluster.rs
  .gitignore
  Cargo.toml
logger/
  src/
    lib.rs
  Cargo.toml
measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  .gitignore
  Cargo.toml
merkle-tree/
  src/
    lib.rs
    merkle_tree.rs
  .gitignore
  Cargo.toml
metrics/
  benches/
    metrics.rs
  scripts/
    grafana-provisioning/
      dashboards/
        cluster-monitor.json
        dashboard.yml
    .gitignore
    adjust-dashboard-for-channel.py
    enable.sh
    grafana.ini
    influxdb.conf
    README.md
    start.sh
    status.sh
    stop.sh
    test.sh
  src/
    counter.rs
    datapoint.rs
    lib.rs
    metrics.rs
  .gitignore
  Cargo.toml
  README.md
multinode-demo/
  bench-tps.sh
  bootstrap-validator.sh
  common.sh
  delegate-stake.sh
  faucet.sh
  setup-from-mainnet-beta.sh
  setup-from-testnet.sh
  setup.sh
  validator-x.sh
  validator.sh
net/
  remote/
    cleanup.sh
    README.md
    remote-client.sh
    remote-deploy-update.sh
    remote-node-wait-init.sh
    remote-node.sh
    remote-sanity.sh
  scripts/
    azure-provider.sh
    colo_nodes
    colo-node-onacquire.sh
    colo-node-onfree.sh
    colo-provider.sh
    colo-utils.sh
    create-solana-user.sh
    disable-background-upgrades.sh
    ec2-provider.sh
    ec2-security-group-config.json
    gce-provider.sh
    gce-self-destruct.sh
    install-ag.sh
    install-at.sh
    install-certbot.sh
    install-docker.sh
    install-earlyoom.sh
    install-iftop.sh
    install-jq.sh
    install-libssl.sh
    install-perf.sh
    install-rsync.sh
    localtime.sh
    mount-additional-disk.sh
    network-config.sh
    remove-docker-interface.sh
    rsync-retry.sh
  .gitignore
  common.sh
  gce.sh
  init-metrics.sh
  net.sh
  scp.sh
  ssh.sh
net-utils/
  benches/
    token_bucket.rs
  src/
    ip_echo_client.rs
    ip_echo_server.rs
    lib.rs
    multihomed_sockets.rs
    socket_addr_space.rs
    sockets.rs
    token_bucket.rs
    tooling_for_tests.rs
  .gitignore
  Cargo.toml
notifier/
  src/
    lib.rs
  .gitignore
  Cargo.toml
perf/
  benches/
    dedup.rs
    discard.rs
    recycler.rs
    reset.rs
    shrink.rs
    sigverify.rs
  src/
    data_budget.rs
    deduper.rs
    discard.rs
    lib.rs
    packet.rs
    perf_libs.rs
    recycled_vec.rs
    recycler_cache.rs
    recycler.rs
    sigverify.rs
    test_tx.rs
    thread.rs
  build.rs
  Cargo.toml
platform-tools-sdk/
  cargo-build-sbf/
    src/
      main.rs
      post_processing.rs
      toolchain.rs
      utils.rs
    tests/
      crates/
        fail/
          src/
            lib.rs
          Cargo.toml
        noop/
          src/
            lib.rs
          Cargo.toml
        package-metadata/
          src/
            lib.rs
          Cargo.toml
        workspace-metadata/
          src/
            lib.rs
          Cargo.toml
      crates.rs
    .gitignore
    Cargo.toml
  cargo-test-sbf/
    src/
      main.rs
    Cargo.toml
  gen-headers/
    src/
      main.rs
    Cargo.toml
  sbf/
    c/
      inc/
        sol/
          inc/
            alt_bn128_compression.inc
            alt_bn128.inc
            assert.inc
            big_mod_exp.inc
            blake3.inc
            compute_units.inc
            cpi.inc
            keccak.inc
            last_restart_slot.inc
            log.inc
            poseidon.inc
            pubkey.inc
            return_data.inc
            secp256k1.inc
            sha.inc
          alt_bn128_compression.h
          alt_bn128.h
          assert.h
          big_mod_exp.h
          blake3.h
          compute_units.h
          constants.h
          cpi.h
          deserialize_deprecated.h
          deserialize.h
          entrypoint.h
          keccak.h
          last_restart_slot.h
          log.h
          poseidon.h
          pubkey.h
          return_data.h
          secp256k1.h
          sha.h
          string.h
          types.h
        sys/
          param.h
        deserialize_deprecated.h
        solana_sdk.h
        stdio.h
        stdlib.h
        string.h
        wchar.h
      README.md
      sbf.ld
      sbf.mk
    scripts/
      dump.sh
      install.sh
      objcopy.sh
      package.sh
      strip.sh
    .gitignore
    env.sh
poh/
  benches/
    poh_verify.rs
    poh.rs
    transaction_recorder.rs
  src/
    lib.rs
    poh_controller.rs
    poh_recorder.rs
    poh_service.rs
    record_channels.rs
    transaction_recorder.rs
  .gitignore
  Cargo.toml
poh-bench/
  src/
    main.rs
  Cargo.toml
poseidon/
  src/
    legacy.rs
    lib.rs
  Cargo.toml
precompiles/
  benches/
    ed25519_instructions.rs
    secp256k1_instructions.rs
    secp256r1_instructions.rs
  src/
    ed25519.rs
    lib.rs
    secp256k1.rs
    secp256r1.rs
  Cargo.toml
program-binaries/
  src/
    programs/
      core_bpf_address_lookup_table-3.0.0.so
      core_bpf_config-3.0.0.so
      core_bpf_feature_gate-0.0.1.so
      core_bpf_stake-1.0.1.so
      spl_associated_token_account-1.1.1.so
      spl_memo-1.0.0.so
      spl_memo-3.0.0.so
      spl_token_2022-8.0.0.so
      spl_token-3.5.0.so
      spl-jito_tip_distribution-0.1.10.so
      spl-jito_tip_distribution-0.1.4.so
      spl-jito_tip_distribution-0.1.7.so
      spl-jito_tip_payment-0.1.10.so
      spl-jito_tip_payment-0.1.4.so
      spl-jito_tip_payment-0.1.7.so
    lib.rs
  Cargo.toml
program-runtime/
  src/
    cpi.rs
    execution_budget.rs
    invoke_context.rs
    lib.rs
    loaded_programs.rs
    mem_pool.rs
    memory.rs
    serialization.rs
    stable_log.rs
    sysvar_cache.rs
  Cargo.toml
program-test/
  src/
    lib.rs
  tests/
    fixtures/
      noop_program.so
    bpf.rs
    builtins.rs
    compute_units.rs
    core_bpf.rs
    cpi.rs
    fuzz.rs
    genesis_accounts.rs
    lamports.rs
    panic.rs
    realloc.rs
    return_data.rs
    setup.rs
    spl.rs
    sysvar_last_restart_slot.rs
    sysvar.rs
    warp.rs
  Cargo.toml
programs/
  bpf_loader/
    benches/
      bpf_loader_upgradeable.rs
      serialization.rs
    src/
      lib.rs
    test_elfs/
      out/
        noop_aligned.so
        noop_unaligned.so
        sbpfv0_verifier_err.so
        sbpfv3_return_err.so
        sbpfv3_return_ok.so
      src/
        noop_aligned/
          noop_aligned.c
        noop_unaligned/
          noop_unaligned.c
      makefile
    Cargo.toml
  bpf-loader-tests/
    tests/
      common.rs
      extend_program_ix.rs
    Cargo.toml
    noop.so
  compute-budget/
    src/
      lib.rs
    Cargo.toml
  compute-budget-bench/
    benches/
      compute_budget.rs
    Cargo.toml
  ed25519-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  loader-v4/
    src/
      lib.rs
    Cargo.toml
  sbf/
    benches/
      bpf_loader.rs
    c/
      src/
        alloc/
          alloc.c
        alt_bn128/
          alt_bn128.c
        alt_bn128_compression/
          alt_bn128.c
        bench_alu/
          bench_alu.c
          test_bench_alu.c
        big_mod_exp/
          big_mod_exp.c
        deprecated_loader/
          deprecated_loader.c
        dup_accounts/
          dup_accounts.c
        error_handling/
          error_handling.c
        float/
          float.c
        invoke/
          invoke.c
        invoked/
          instruction.h
          invoked.c
        log_data/
          log_data.c
        move_funds/
          move_funds.c
        multiple_static/
          multiple_static.c
        noop/
          noop.c
        noop++/
          noop++.cc
        panic/
          panic.c
        poseidon/
          poseidon.c
        read_program/
          read_program.c
        relative_call/
          relative_call.c
        remaining_compute_units/
          remaining_compute_units.c
        return_data/
          return_data.c
        sanity/
          sanity.c
        sanity++/
          sanity++.cc
        sbf_to_sbf/
          entrypoint.c
          helper.c
          helper.h
        secp256k1_recover/
          secp256k1_recover.c
        ser/
          ser.c
        sha/
          sha.c
        stdlib/
          stdlib.c
        struct_pass/
          struct_pass.c
        struct_ret/
          struct_ret.c
        tuner/
          tuner.c
        tuner-variable-iterations/
          tuner-variable-iterations.c
      .gitignore
    rust/
      128bit/
        src/
          lib.rs
        Cargo.toml
      128bit_dep/
        src/
          lib.rs
        Cargo.toml
      account_mem/
        src/
          lib.rs
        Cargo.toml
      account_mem_deprecated/
        src/
          lib.rs
        Cargo.toml
      alloc/
        src/
          lib.rs
        Cargo.toml
      alt_bn128/
        src/
          lib.rs
        Cargo.toml
      alt_bn128_compression/
        src/
          lib.rs
        Cargo.toml
      big_mod_exp/
        src/
          lib.rs
        Cargo.toml
      call_args/
        src/
          lib.rs
        Cargo.toml
      call_depth/
        src/
          lib.rs
        Cargo.toml
      caller_access/
        src/
          lib.rs
        Cargo.toml
      curve25519/
        src/
          lib.rs
        Cargo.toml
      custom_heap/
        src/
          lib.rs
        Cargo.toml
      dep_crate/
        src/
          lib.rs
        Cargo.toml
      deprecated_loader/
        src/
          lib.rs
        Cargo.toml
      divide_by_zero/
        src/
          lib.rs
        Cargo.toml
      dup_accounts/
        src/
          lib.rs
        Cargo.toml
      error_handling/
        src/
          lib.rs
        Cargo.toml
      external_spend/
        src/
          lib.rs
        Cargo.toml
      get_minimum_delegation/
        src/
          lib.rs
        Cargo.toml
      inner_instruction_alignment_check/
        src/
          lib.rs
        Cargo.toml
      instruction_introspection/
        src/
          lib.rs
        Cargo.toml
      invoke/
        src/
          lib.rs
        Cargo.toml
      invoke_and_error/
        src/
          lib.rs
        Cargo.toml
      invoke_and_ok/
        src/
          lib.rs
        Cargo.toml
      invoke_and_return/
        src/
          lib.rs
        Cargo.toml
      invoke_dep/
        src/
          lib.rs
        Cargo.toml
      invoked/
        src/
          lib.rs
        Cargo.toml
      invoked_dep/
        src/
          lib.rs
        Cargo.toml
      iter/
        src/
          lib.rs
        Cargo.toml
      log_data/
        src/
          lib.rs
        Cargo.toml
      many_args/
        src/
          helper.rs
          lib.rs
        Cargo.toml
      many_args_dep/
        src/
          lib.rs
        Cargo.toml
      mem/
        src/
          lib.rs
        Cargo.toml
      mem_dep/
        src/
          lib.rs
        Cargo.toml
      membuiltins/
        src/
          lib.rs
        Cargo.toml
      noop/
        src/
          lib.rs
        Cargo.toml
      panic/
        src/
          lib.rs
        Cargo.toml
      param_passing/
        src/
          lib.rs
        Cargo.toml
      param_passing_dep/
        src/
          lib.rs
        Cargo.toml
      poseidon/
        src/
          lib.rs
        Cargo.toml
      r2_instruction_data_pointer/
        src/
          lib.rs
        Cargo.toml
      rand/
        src/
          lib.rs
        Cargo.toml
      realloc/
        src/
          lib.rs
        Cargo.toml
      realloc_dep/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke_dep/
        src/
          lib.rs
        Cargo.toml
      remaining_compute_units/
        src/
          lib.rs
        Cargo.toml
      ro_account_modify/
        src/
          lib.rs
        Cargo.toml
      ro_modify/
        src/
          lib.rs
        Cargo.toml
      sanity/
        src/
          lib.rs
        Cargo.toml
      secp256k1_recover/
        src/
          lib.rs
        Cargo.toml
      sha/
        src/
          lib.rs
        Cargo.toml
      sibling_inner_instructions/
        src/
          lib.rs
        Cargo.toml
      sibling_instructions/
        src/
          lib.rs
        Cargo.toml
      simulation/
        src/
          lib.rs
        Cargo.toml
      spoof1/
        src/
          lib.rs
        Cargo.toml
      spoof1_system/
        src/
          lib.rs
        Cargo.toml
      syscall-get-epoch-stake/
        src/
          lib.rs
        Cargo.toml
      sysvar/
        src/
          lib.rs
        Cargo.toml
      upgradeable/
        src/
          lib.rs
        Cargo.toml
      upgraded/
        src/
          lib.rs
        Cargo.toml
    tests/
      programs.rs
      simulation.rs
      syscall_get_epoch_stake.rs
      sysvar.rs
    .gitignore
    Cargo.toml
    Makefile
  system/
    benches/
      system.rs
    src/
      lib.rs
      system_instruction.rs
      system_processor.rs
    Cargo.toml
  vote/
    benches/
      process_vote.rs
      vote_instructions.rs
    src/
      vote_state/
        handler.rs
        mod.rs
      lib.rs
      vote_processor.rs
    Cargo.toml
  zk-elgamal-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
  zk-elgamal-proof-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  zk-token-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
pubsub-client/
  src/
    nonblocking/
      mod.rs
      pubsub_client.rs
    lib.rs
    pubsub_client.rs
  Cargo.toml
quic-client/
  src/
    nonblocking/
      mod.rs
      quic_client.rs
    lib.rs
    quic_client.rs
  tests/
    quic_client.rs
  Cargo.toml
rayon-threadlimit/
  src/
    lib.rs
  .gitignore
  Cargo.toml
rbpf-cli/
  src/
    main.rs
  Cargo.toml
remote-wallet/
  src/
    ledger_error.rs
    ledger.rs
    lib.rs
    locator.rs
    remote_keypair.rs
    remote_wallet.rs
  Cargo.toml
  README.md
reserved-account-keys/
  src/
    lib.rs
  Cargo.toml
rpc/
  src/
    rpc/
      account_resolver.rs
    cluster_tpu_info.rs
    filter.rs
    lib.rs
    max_slots.rs
    optimistically_confirmed_bank_tracker.rs
    parsed_token_accounts.rs
    rpc_cache.rs
    rpc_completed_slots_service.rs
    rpc_health.rs
    rpc_pubsub_service.rs
    rpc_pubsub.rs
    rpc_service.rs
    rpc_subscription_tracker.rs
    rpc_subscriptions.rs
    rpc.rs
    slot_status_notifier.rs
    transaction_notifier_interface.rs
    transaction_status_service.rs
  .gitignore
  Cargo.toml
rpc-client/
  src/
    nonblocking/
      mod.rs
      rpc_client.rs
    http_sender.rs
    lib.rs
    mock_sender.rs
    rpc_client.rs
    rpc_sender.rs
    spinner.rs
  Cargo.toml
rpc-client-api/
  src/
    bundles.rs
    client_error.rs
    custom_error.rs
    lib.rs
    response.rs
  Cargo.toml
rpc-client-nonce-utils/
  src/
    nonblocking/
      blockhash_query.rs
      mod.rs
    blockhash_query.rs
    lib.rs
  Cargo.toml
rpc-client-types/
  src/
    config.rs
    error_object.rs
    filter.rs
    lib.rs
    request.rs
    response.rs
  Cargo.toml
rpc-test/
  tests/
    nonblocking.rs
    rpc.rs
  .gitignore
  Cargo.toml
runtime/
  benches/
    accounts.rs
    prioritization_fee_cache.rs
    status_cache.rs
  src/
    accounts_background_service/
      pending_snapshot_packages.rs
      stats.rs
    bank/
      builtins/
        core_bpf_migration/
          error.rs
          mod.rs
          source_buffer.rs
          target_bpf_v2.rs
          target_builtin.rs
          target_core_bpf.rs
        mod.rs
      partitioned_epoch_rewards/
        calculation.rs
        distribution.rs
        epoch_rewards_hasher.rs
        mod.rs
        sysvar.rs
      accounts_lt_hash.rs
      address_lookup_table.rs
      bank_hash_details.rs
      check_transactions.rs
      fee_distribution.rs
      metrics.rs
      recent_blockhashes_account.rs
      serde_snapshot.rs
      sysvar_cache.rs
      tests.rs
    inflation_rewards/
      mod.rs
      points.rs
    serde_snapshot/
      obsolete_accounts.rs
      status_cache.rs
      storage.rs
      tests.rs
      types.rs
      utils.rs
    snapshot_package/
      compare.rs
    snapshot_utils/
      snapshot_storage_rebuilder.rs
    stakes/
      serde_stakes.rs
    account_saver.rs
    accounts_background_service.rs
    bank_client.rs
    bank_forks.rs
    bank_hash_cache.rs
    bank_utils.rs
    bank.rs
    commitment.rs
    dependency_tracker.rs
    epoch_stakes.rs
    genesis_utils.rs
    installed_scheduler_pool.rs
    lib.rs
    loader_utils.rs
    non_circulating_supply.rs
    prioritization_fee_cache.rs
    prioritization_fee.rs
    read_optimized_dashmap.rs
    rent_collector.rs
    runtime_config.rs
    serde_snapshot.rs
    snapshot_bank_utils.rs
    snapshot_controller.rs
    snapshot_minimizer.rs
    snapshot_package.rs
    snapshot_utils.rs
    stake_account.rs
    stake_history.rs
    stake_utils.rs
    stake_weighted_timestamp.rs
    stakes.rs
    static_ids.rs
    status_cache.rs
    transaction_batch.rs
    vote_sender_types.rs
  .gitignore
  Cargo.toml
runtime-transaction/
  benches/
    get_signature_details.rs
  src/
    runtime_transaction/
      sdk_transactions.rs
      transaction_view.rs
    instruction_data_len.rs
    instruction_meta.rs
    lib.rs
    runtime_transaction.rs
    signature_details.rs
    transaction_meta.rs
    transaction_with_meta.rs
  Cargo.toml
scheduler-bindings/
  src/
    lib.rs
  Cargo.toml
scheduling-utils/
  src/
    handshake/
      client.rs
      mod.rs
      server.rs
      shared.rs
      tests.rs
    error.rs
    lib.rs
    pubkeys_ptr.rs
    responses_region.rs
    thread_aware_account_locks.rs
    transaction_ptr.rs
  Cargo.toml
scripts/
  agave-build-lists.sh
  agave-install-deploy.sh
  agave-install-update-manifest-keypair.sh
  build-agave-xdp-ebpf.sh
  build-downstream-anchor-projects.sh
  cargo-clippy-nightly.sh
  cargo-clippy.sh
  cargo-for-all-lock-files.sh
  cargo-install-all.sh
  check-dev-context-only-utils.sh
  configure-metrics.sh
  confirm-cargo-version-numbers-before-bump.sh
  coverage.sh
  create-release-tarball.sh
  elf-hash-symbol.sh
  fd-monitor.sh
  generate-target-triple.sh
  iftop.sh
  increment-cargo-version.sh
  metrics-write-datapoint.sh
  net-shaper.sh
  net-stats.sh
  oom-monitor.sh
  oom-score-adj.sh
  patch-crates.sh
  patch-spl-crates-for-anchor.sh
  perf-plot.py
  perf-stats.py
  read-cargo-variable.sh
  reserve-cratesio-package-name.sh
  run.sh
  sed-i-all-rs-files-for-rust-analyzer.sh
  spl-token-cli-version.sh
  system-stats.sh
  ulimit-n.sh
  wallet-sanity.sh
sdk/
  README.md
send-transaction-service/
  src/
    lib.rs
    send_transaction_service_stats.rs
    send_transaction_service.rs
    test_utils.rs
    tpu_info.rs
    transaction_client.rs
  Cargo.toml
snapshots/
  src/
    archive_format.rs
    archive.rs
    error.rs
    hardened_unpack.rs
    kind.rs
    lib.rs
    paths.rs
    snapshot_archive_info.rs
    snapshot_config.rs
    snapshot_hash.rs
    snapshot_interval.rs
    snapshot_version.rs
    unarchive.rs
  Cargo.toml
stake-accounts/
  src/
    arg_parser.rs
    args.rs
    main.rs
    stake_accounts.rs
  Cargo.toml
storage-bigtable/
  build-proto/
    src/
      main.rs
    .gitignore
    build.sh
    Cargo.toml
    README.md
  proto/
    google.api.rs
    google.bigtable.v2.rs
    google.protobuf.rs
    google.rpc.rs
  src/
    access_token.rs
    bigtable.rs
    compression.rs
    lib.rs
    pki-goog-roots.pem
    root_ca_certificate.rs
  Cargo.toml
  init-bigtable.sh
  README.md
storage-proto/
  proto/
    confirmed_block.proto
    entries.proto
    transaction_by_addr.proto
  src/
    convert.rs
    lib.rs
  build.rs
  Cargo.toml
  README.md
streamer/
  examples/
    swqos.rs
  src/
    nonblocking/
      connection_rate_limiter.rs
      mod.rs
      qos.rs
      quic.rs
      recvmmsg.rs
      sendmmsg.rs
      simple_qos.rs
      stream_throttle.rs
      swqos.rs
      testing_utilities.rs
    evicting_sender.rs
    lib.rs
    msghdr.rs
    packet.rs
    quic.rs
    recvmmsg.rs
    sendmmsg.rs
    streamer.rs
  tests/
    recvmmsg.rs
  Cargo.toml
svm/
  doc/
    diagrams/
      context.svg
      context.tex
    spec.md
  src/
    account_loader.rs
    account_overrides.rs
    lib.rs
    message_processor.rs
    nonce_info.rs
    program_loader.rs
    rent_calculator.rs
    rollback_accounts.rs
    transaction_account_state_info.rs
    transaction_balances.rs
    transaction_commit_result.rs
    transaction_error_metrics.rs
    transaction_execution_result.rs
    transaction_processing_callback.rs
    transaction_processing_result.rs
    transaction_processor.rs
  tests/
    example-programs/
      clock-sysvar/
        src/
          lib.rs
        Cargo.toml
        clock_sysvar_program.so
      hello-solana/
        src/
          lib.rs
        Cargo.toml
        hello_solana_program.so
      simple-transfer/
        src/
          lib.rs
        Cargo.toml
        simple_transfer_program.so
      transfer-from-account/
        src/
          lib.rs
        Cargo.toml
        transfer_from_account_program.so
      write-to-account/
        src/
          lib.rs
        Cargo.toml
        write_to_account_program.so
    concurrent_tests.rs
    integration_test.rs
    mock_bank.rs
  Cargo.toml
svm-callback/
  src/
    lib.rs
  Cargo.toml
svm-feature-set/
  src/
    lib.rs
  Cargo.toml
svm-log-collector/
  src/
    lib.rs
  Cargo.toml
svm-measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  Cargo.toml
svm-rent-calculator/
  src/
    lib.rs
    rent_state.rs
    svm_rent_calculator.rs
  Cargo.toml
svm-test-harness/
  bin/
    test_exec_instr.rs
  src/
    fixture/
      account_state.rs
      error.rs
      feature_set.rs
      instr_context.rs
      instr_effects.rs
      mod.rs
    file.rs
    fuzz.rs
    instr.rs
    lib.rs
    program_cache.rs
    sysvar_cache.rs
  .gitignore
  build.rs
  Cargo.toml
  Makefile
svm-timings/
  src/
    lib.rs
  Cargo.toml
svm-transaction/
  src/
    svm_message/
      sanitized_message.rs
      sanitized_transaction.rs
    svm_transaction/
      sanitized_transaction.rs
    instruction.rs
    lib.rs
    message_address_table_lookup.rs
    svm_message.rs
    svm_transaction.rs
    tests.rs
  Cargo.toml
svm-type-overrides/
  src/
    lib.rs
  Cargo.toml
syscalls/
  gen-syscall-list/
    src/
      main.rs
    build.rs
    Cargo.toml
  src/
    cpi.rs
    lib.rs
    logging.rs
    mem_ops.rs
    sysvar.rs
  Cargo.toml
test-validator/
  src/
    lib.rs
  Cargo.toml
thread-manager/
  examples/
    common/
      mod.rs
    core_contention_basics.rs
    core_contention_contending_set.toml
    core_contention_dedicated_set.toml
    core_contention_sweep.rs
  src/
    lib.rs
    native_thread_runtime.rs
    policy.rs
    rayon_runtime.rs
    tokio_runtime.rs
  Cargo.toml
  README.md
tls-utils/
  src/
    config.rs
    crypto_provider.rs
    lib.rs
    quic_client_certificate.rs
    skip_client_verification.rs
    skip_server_verification.rs
    tls_certificates.rs
  Cargo.toml
  README
tokens/
  src/
    arg_parser.rs
    args.rs
    commands.rs
    db.rs
    lib.rs
    main.rs
    spl_token.rs
    stake.rs
    token_display.rs
  tests/
    commands.rs
  .gitignore
  Cargo.toml
  README.md
tps-client/
  src/
    bank_client.rs
    lib.rs
    rpc_client.rs
    tpu_client.rs
    utils.rs
  Cargo.toml
tpu-client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    lib.rs
    tpu_client.rs
  .gitignore
  Cargo.toml
tpu-client-next/
  src/
    node_address_service/
      leader_tpu_cache_service.rs
      recent_leader_slots.rs
      slot_event.rs
      slot_receiver.rs
      slot_update_service.rs
    quic_networking/
      error.rs
    client_builder.rs
    connection_worker.rs
    connection_workers_scheduler.rs
    leader_updater.rs
    lib.rs
    logging.rs
    metrics.rs
    node_address_service.rs
    quic_networking.rs
    send_transaction_stats.rs
    transaction_batch.rs
    websocket_node_address_service.rs
    workers_cache.rs
  tests/
    connection_workers_scheduler_test.rs
  Cargo.toml
transaction-context/
  src/
    instruction_accounts.rs
    instruction.rs
    lib.rs
    transaction_accounts.rs
    vm_slice.rs
  Cargo.toml
transaction-dos/
  src/
    main.rs
  .gitignore
  Cargo.toml
transaction-metrics-tracker/
  src/
    lib.rs
  Cargo.toml
transaction-status/
  benches/
    extract_memos.rs
  src/
    parse_token/
      extension/
        confidential_mint_burn.rs
        confidential_transfer_fee.rs
        confidential_transfer.rs
        cpi_guard.rs
        default_account_state.rs
        group_member_pointer.rs
        group_pointer.rs
        interest_bearing_mint.rs
        memo_transfer.rs
        metadata_pointer.rs
        mint_close_authority.rs
        mod.rs
        pausable.rs
        permanent_delegate.rs
        reallocate.rs
        scaled_ui_amount.rs
        token_group.rs
        token_metadata.rs
        transfer_fee.rs
        transfer_hook.rs
    extract_memos.rs
    lib.rs
    parse_accounts.rs
    parse_address_lookup_table.rs
    parse_associated_token.rs
    parse_bpf_loader.rs
    parse_instruction.rs
    parse_stake.rs
    parse_system.rs
    parse_token.rs
    parse_vote.rs
    token_balances.rs
  Cargo.toml
transaction-status-client-types/
  src/
    lib.rs
    option_serializer.rs
  Cargo.toml
transaction-view/
  benches/
    bytes.rs
    transaction_view.rs
  src/
    address_table_lookup_frame.rs
    bytes.rs
    instructions_frame.rs
    lib.rs
    message_header_frame.rs
    resolved_transaction_view.rs
    result.rs
    sanitize.rs
    signature_frame.rs
    static_account_keys_frame.rs
    transaction_data.rs
    transaction_frame.rs
    transaction_version.rs
    transaction_view.rs
  Cargo.toml
turbine/
  benches/
    cluster_info.rs
    cluster_nodes.rs
  src/
    broadcast_stage/
      broadcast_duplicates_run.rs
      broadcast_fake_shreds_run.rs
      broadcast_metrics.rs
      broadcast_utils.rs
      fail_entry_verification_broadcast_run.rs
      standard_broadcast_run.rs
    addr_cache.rs
    broadcast_stage.rs
    cluster_nodes.rs
    lib.rs
    quic_endpoint.rs
    retransmit_stage.rs
    sigverify_shreds.rs
    xdp.rs
  Cargo.toml
udp-client/
  src/
    nonblocking/
      mod.rs
      udp_client.rs
    lib.rs
    udp_client.rs
  Cargo.toml
unified-scheduler-logic/
  src/
    lib.rs
  Cargo.toml
unified-scheduler-pool/
  src/
    lib.rs
    sleepless_testing.rs
  Cargo.toml
validator/
  src/
    bin/
      solana-test-validator.rs
    cli/
      thread_args.rs
    commands/
      authorized_voter/
        mod.rs
      bam/
        mod.rs
      block_engine/
        mod.rs
      contact_info/
        mod.rs
      exit/
        mod.rs
      manage_block_production/
        mod.rs
      monitor/
        mod.rs
      plugin/
        mod.rs
      relayer/
        mod.rs
      repair_shred_from_peer/
        mod.rs
      repair_whitelist/
        mod.rs
      run/
        args/
          account_secondary_indexes.rs
          blockstore_options.rs
          json_rpc_config.rs
          pub_sub_config.rs
          rpc_bigtable_config.rs
          rpc_bootstrap_config.rs
          send_transaction_config.rs
        args.rs
        execute.rs
        mod.rs
      set_identity/
        mod.rs
      set_log_filter/
        mod.rs
      set_public_address/
        mod.rs
      shred/
        mod.rs
      staked_nodes_overrides/
        mod.rs
      wait_for_restart_window/
        mod.rs
      mod.rs
    admin_rpc_service.rs
    bootstrap.rs
    cli.rs
    dashboard.rs
    lib.rs
    main.rs
  tests/
    cli.rs
  .gitignore
  Cargo.toml
  solana-test-validator
verified-packet-receiver/
  src/
    lib.rs
    receiver.rs
  Cargo.toml
  Readme.md
version/
  src/
    legacy.rs
    lib.rs
  .gitignore
  build.rs
  Cargo.toml
vortexor/
  src/
    cli.rs
    lib.rs
    main.rs
    rpc_load_balancer.rs
    sender.rs
    stake_updater.rs
    vortexor.rs
  tests/
    vortexor.rs
  Cargo.toml
  README.md
vote/
  benches/
    vote_account.rs
  src/
    vote_state_view/
      field_frames.rs
      frame_v1_14_11.rs
      frame_v3.rs
      frame_v4.rs
      list_view.rs
    lib.rs
    vote_account.rs
    vote_parser.rs
    vote_state_view.rs
    vote_transaction.rs
  Cargo.toml
votor/
  src/
    consensus_pool/
      certificate_builder.rs
      parent_ready_tracker.rs
      slot_stake_counters.rs
      stats.rs
      vote_pool.rs
    consensus_pool_service/
      stats.rs
    event_handler/
      stats.rs
    timer_manager/
      stats.rs
      timers.rs
    commitment.rs
    common.rs
    consensus_metrics.rs
    consensus_pool_service.rs
    consensus_pool.rs
    event_handler.rs
    event.rs
    lib.rs
    root_utils.rs
    staked_validators_cache.rs
    timer_manager.rs
    vote_history_storage.rs
    vote_history.rs
    voting_service.rs
    voting_utils.rs
    votor.rs
  Cargo.toml
votor-messages/
  src/
    consensus_message.rs
    lib.rs
    vote.rs
  Cargo.toml
watchtower/
  src/
    main.rs
  .gitignore
  Cargo.toml
  README.md
web3.js/
  README.md
wen-restart/
  proto/
    wen_restart.proto
  src/
    heaviest_fork_aggregate.rs
    last_voted_fork_slots_aggregate.rs
    lib.rs
    wen_restart.rs
  build.rs
  Cargo.toml
xdp/
  src/
    device.rs
    lib.rs
    netlink.rs
    packet.rs
    program.rs
    route.rs
    socket.rs
    tx_loop.rs
    umem.rs
  Cargo.toml
xdp-ebpf/
  src/
    bin/
      agave-xdp-prog.rs
    lib.rs
  agave-xdp-prog
  Cargo.toml
  README
zk-keygen/
  README.md
zk-sdk/
  README.md
zk-token-sdk/
  src/
    encryption/
      auth_encryption.rs
      decode_u32_precomputation_for_G.bincode
      discrete_log.rs
      elgamal.rs
      grouped_elgamal.rs
      mod.rs
      pedersen.rs
    instruction/
      batched_grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      batched_range_proof/
        batched_range_proof_u128.rs
        batched_range_proof_u256.rs
        batched_range_proof_u64.rs
        mod.rs
      grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      transfer/
        encryption.rs
        mod.rs
        with_fee.rs
        without_fee.rs
      ciphertext_ciphertext_equality.rs
      ciphertext_commitment_equality.rs
      errors.rs
      fee_sigma.rs
      mod.rs
      pubkey_validity.rs
      range_proof.rs
      withdraw.rs
      zero_balance.rs
    range_proof/
      errors.rs
      generators.rs
      inner_product.rs
      mod.rs
      util.rs
    sigma_proofs/
      batched_grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      ciphertext_ciphertext_equality_proof.rs
      ciphertext_commitment_equality_proof.rs
      errors.rs
      fee_proof.rs
      mod.rs
      pubkey_proof.rs
      zero_balance_proof.rs
    zk_token_elgamal/
      pod/
        auth_encryption.rs
        elgamal.rs
        grouped_elgamal.rs
        instruction.rs
        mod.rs
        pedersen.rs
        range_proof.rs
        sigma_proofs.rs
      convert.rs
      decryption.rs
      mod.rs
      ops.rs
    errors.rs
    lib.rs
    macros.rs
    transcript.rs
    zk_token_proof_instruction.rs
    zk_token_proof_program.rs
    zk_token_proof_state.rs
  .gitignore
  Cargo.toml
  README.md
.codecov.yml
.dockerignore
.gitignore
.gitmodules
.mergify.yml
bootstrap
cargo
cargo-build-sbf
cargo-test-sbf
Cargo.toml
CHANGELOG.md
clippy.toml
CONTRIBUTING.md
deploy_programs
f
fetch-core-bpf.sh
fetch-perf-libs.sh
fetch-programs.sh
fetch-spl.sh
legal.md
LICENSE
privacy.md
README.md
RELEASE.md
rust-toolchain.toml
rustfmt.toml
s
SECURITY.md
start
start_multi
vercel.json

================================================================
Files
================================================================

================
File: gossip/benches/crds_gossip_pull.rs
================
fn bench_hash_as_u64(c: &mut Criterion) {
⋮----
.take(1000)
.collect();
c.bench_function("bench_hash_as_u64", |b| {
b.iter(|| {
⋮----
.iter()
.map(CrdsFilter::hash_as_u64)
⋮----
fn bench_build_crds_filters(c: &mut Criterion) {
let thread_pool = ThreadPoolBuilder::new().build().unwrap();
let mut rng = thread_rng();
⋮----
.filter(|_| {
crds.insert(
⋮----
rng.gen(),
⋮----
.is_ok()
⋮----
.count();
assert_eq!(num_inserts, 90_000);
⋮----
c.bench_function("bench_build_crds_filters", |b| {
⋮----
let filters = crds_gossip_pull.build_crds_filters(
⋮----
assert_eq!(filters.len(), 16);
⋮----
criterion_group!(benches, bench_hash_as_u64, bench_build_crds_filters);
criterion_main!(benches);

================
File: gossip/benches/crds_shards.rs
================
fn new_test_crds_value<R: Rng>(rng: &mut R) -> VersionedCrdsValue {
⋮----
let label = value.label();
⋮----
crds.insert(value, timestamp(), GossipRoute::LocalMessage)
.unwrap();
crds.get::<&VersionedCrdsValue>(&label).cloned().unwrap()
⋮----
fn bench_crds_shards_find(c: &mut Criterion, num_values: usize, mask_bits: u32) {
let mut rng = thread_rng();
let values: Vec<_> = repeat_with(|| new_test_crds_value(&mut rng))
.take(num_values)
.collect();
⋮----
for (index, value) in values.iter().enumerate() {
assert!(shards.insert(index, value));
⋮----
c.bench_function(
&format!("bench_crds_shards_find: mask_bits: {mask_bits:?}"),
⋮----
b.iter(|| {
let mask = rng.gen();
let _hits = shards.find(mask, mask_bits).count();
⋮----
fn bench_crds_shards_find_0(c: &mut Criterion) {
bench_crds_shards_find(c, 100_000, 0);
⋮----
fn bench_crds_shards_find_1(c: &mut Criterion) {
bench_crds_shards_find(c, 100_000, 1);
⋮----
fn bench_crds_shards_find_3(c: &mut Criterion) {
bench_crds_shards_find(c, 100_000, 3);
⋮----
fn bench_crds_shards_find_5(c: &mut Criterion) {
bench_crds_shards_find(c, 100_000, 5);
⋮----
fn bench_crds_shards_find_7(c: &mut Criterion) {
bench_crds_shards_find(c, 100_000, 7);
⋮----
fn bench_crds_shards_find_8(c: &mut Criterion) {
bench_crds_shards_find(c, 100_000, 8);
⋮----
fn bench_crds_shards_find_9(c: &mut Criterion) {
bench_crds_shards_find(c, 100_000, 9);
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: gossip/benches/crds.rs
================
fn bench_find_old_labels(c: &mut Criterion) {
let thread_pool = ThreadPoolBuilder::new().build().unwrap();
let mut rng = thread_rng();
⋮----
std::iter::repeat_with(|| (CrdsValue::new_rand(&mut rng, None), rng.gen_range(0..now)))
.take(50_000)
.for_each(|(v, ts)| assert!(crds.insert(v, ts, GossipRoute::LocalMessage).is_ok()));
⋮----
c.bench_function("bench_find_old_labels", |b| {
b.iter(|| {
let out = crds.find_old_labels(&thread_pool, now, &timeouts);
assert!(out.len() > 10);
assert!(out.len() < 250);
⋮----
criterion_group!(benches, bench_find_old_labels);
criterion_main!(benches);

================
File: gossip/benches/weighted_shuffle.rs
================
fn make_weights<R: Rng>(rng: &mut R) -> Vec<u64> {
repeat_with(|| rng.gen_range(1..10_000))
.take(4_000)
.collect()
⋮----
fn bench_weighted_shuffle_new(c: &mut Criterion) {
⋮----
c.bench_function("bench_weighted_shuffle_new", |b| {
b.iter(|| {
let weights = make_weights(&mut rng);
black_box(WeightedShuffle::new("", &weights));
⋮----
fn bench_weighted_shuffle_shuffle(c: &mut Criterion) {
⋮----
c.bench_function("bench_weighted_shuffle_shuffle", |b| {
⋮----
rng.fill(&mut seed[..]);
⋮----
.clone()
.shuffle(&mut rng)
.for_each(|index| {
black_box(index);
⋮----
c.bench_function("bench_weighted_shuffle_collect", |b| {
⋮----
let mut weighted_shuffle = weighted_shuffle.clone();
let shuffle = weighted_shuffle.shuffle(&mut rng);
black_box(shuffle.collect::<Vec<_>>());
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: gossip/src/cluster_info_metrics.rs
================
pub(crate) struct Counter(AtomicU64);
impl Counter {
pub(crate) fn add_measure(&self, x: &mut Measure) {
x.stop();
self.0.fetch_add(x.as_us(), Ordering::Relaxed);
⋮----
pub(crate) fn add_relaxed(&self, x: u64) {
self.0.fetch_add(x, Ordering::Relaxed);
⋮----
fn clear(&self) -> u64 {
self.0.swap(0, Ordering::Relaxed)
⋮----
pub(crate) struct TimedGuard<'a, T> {
⋮----
pub(crate) struct ScopedTimer<'a> {
⋮----
fn from(counter: &'a Counter) -> Self {
⋮----
impl Drop for ScopedTimer<'_> {
fn drop(&mut self) {
let micros = self.clock.elapsed().as_micros();
self.metric.fetch_add(micros as u64, Ordering::Relaxed);
⋮----
pub(crate) fn new(guard: T, label: &'static str, counter: &'a Counter) -> Self {
⋮----
impl<T> Deref for TimedGuard<'_, T> {
type Target = T;
fn deref(&self) -> &Self::Target {
⋮----
impl<T> DerefMut for TimedGuard<'_, T> {
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
impl<T> Drop for TimedGuard<'_, T> {
⋮----
self.counter.add_measure(&mut self.timer);
⋮----
pub struct GossipStats {
⋮----
impl GossipStats {
⋮----
pub(crate) fn record_gossip_packet(&self, protocol: &Protocol) {
⋮----
.add_relaxed(1);
⋮----
pub(crate) fn record_received_packet<E>(
⋮----
self.packets_received_unknown_count.add_relaxed(1);
⋮----
Some(protocol)
⋮----
pub(crate) fn submit_gossip_stats(
⋮----
let gossip_crds = gossip.crds.read().unwrap();
⋮----
gossip_crds.take_stats(),
gossip_crds.len(),
gossip_crds.num_nodes(),
gossip_crds.num_pubkeys(),
gossip_crds.num_purged(),
gossip.pull.failed_inserts_size(),
⋮----
let num_nodes_staked = stakes.values().filter(|stake| **stake > 0).count();
⋮----
.iter()
.map(|counter| counter.0.load(Ordering::Relaxed))
.sum();
datapoint_info!(
⋮----
submit_vote_stats("cluster_info_crds_stats_votes_pull", &crds_stats.pull.votes);
submit_vote_stats("cluster_info_crds_stats_votes_push", &crds_stats.push.votes);
⋮----
.into_iter()
.chain(crds_stats.push.votes)
.into_grouping_map()
.aggregate(|acc, _slot, num_votes| Some(acc.unwrap_or_default() + num_votes));
submit_vote_stats("cluster_info_crds_stats_votes", &votes);
⋮----
fn submit_vote_stats<'a, I>(name: &'static str, votes: I)
⋮----
I: IntoIterator<Item = (&'a Slot, /*num-votes:*/ &'a usize)>,
⋮----
let mut votes: Vec<_> = votes.into_iter().map(|(k, v)| (*k, *v)).collect();
if votes.len() > NUM_SLOTS {
votes.select_nth_unstable_by_key(NUM_SLOTS, |(_, num)| Reverse(*num));
⋮----
for (slot, num_votes) in votes.into_iter().take(NUM_SLOTS) {
datapoint_trace!(name, ("slot", slot, i64), ("num_votes", num_votes, i64));
⋮----
pub(crate) fn should_report_message_signature(signature: &Signature, leading_zeros: u32) -> bool {
let Some(Ok(bytes)) = signature.as_ref().get(..8).map(<[u8; 8]>::try_from) else {
⋮----
u64::from_le_bytes(bytes).trailing_zeros() >= leading_zeros
⋮----
pub(crate) fn last_four_chars(s: &str) -> Option<&str> {
s.get(s.len().saturating_sub(4)..)
⋮----
pub(crate) fn log_gossip_crds_sample_egress(value: &CrdsValue, peer: &Pubkey) {

================
File: gossip/src/cluster_info.rs
================
pub const MINIMUM_NUM_TVU_RECEIVE_SOCKETS: NonZeroUsize = NonZeroUsize::new(1).unwrap();
⋮----
pub const MINIMUM_NUM_TVU_RETRANSMIT_SOCKETS: NonZeroUsize = NonZeroUsize::new(1).unwrap();
pub const DEFAULT_NUM_TVU_RETRANSMIT_SOCKETS: NonZeroUsize = NonZeroUsize::new(12).unwrap();
⋮----
pub enum ClusterInfoError {
⋮----
pub struct ClusterInfo {
⋮----
impl ClusterInfo {
pub fn new(
⋮----
assert_eq!(contact_info.pubkey(), &keypair.pubkey());
⋮----
me.refresh_my_gossip_contact_info();
⋮----
pub fn set_contact_debug_interval(&mut self, new: u64) {
⋮----
pub fn socket_addr_space(&self) -> &SocketAddrSpace {
⋮----
pub fn set_bind_ip_addrs(&mut self, ip_addrs: Arc<BindIpAddrs>) {
⋮----
pub fn bind_ip_addrs(&self) -> Arc<BindIpAddrs> {
self.bind_ip_addrs.clone()
⋮----
fn refresh_push_active_set(
⋮----
let shred_version = self.my_contact_info.read().unwrap().shred_version();
⋮----
self.gossip.refresh_push_active_set(
&self.keypair(),
⋮----
.into_iter()
.map(|(addr, ping)| (addr, Protocol::PingMessage(ping)));
send_gossip_packets(pings, recycler, sender, &self.stats);
⋮----
pub fn insert_info(&self, node: ContactInfo) {
let entry = CrdsValue::new(CrdsData::ContactInfo(node), &self.keypair());
⋮----
let mut gossip_crds = self.gossip.crds.write().unwrap();
gossip_crds.insert(entry, timestamp(), GossipRoute::LocalMessage)
⋮----
error!("ClusterInfo.insert_info: {err:?}");
⋮----
pub fn set_entrypoint(&self, entrypoint: ContactInfo) {
self.set_entrypoints(vec![entrypoint]);
⋮----
pub fn set_entrypoints(&self, entrypoints: Vec<ContactInfo>) {
*self.entrypoints.write().unwrap() = entrypoints;
⋮----
pub fn set_my_contact_info(&self, my_contact_info: ContactInfo) {
*self.my_contact_info.write().unwrap() = my_contact_info;
⋮----
pub fn save_contact_info(&self) {
⋮----
.read()
.unwrap()
.iter()
.filter_map(ContactInfo::gossip)
⋮----
let self_pubkey = self.id();
let gossip_crds = self.gossip.crds.read().unwrap();
⋮----
.get_nodes()
.filter_map(|v| {
let contact_info = v.value.contact_info().unwrap();
if contact_info.pubkey() != &self_pubkey
⋮----
.gossip()
.map(|addr| !entrypoint_gossip_addrs.contains(&addr))
.unwrap_or_default()
⋮----
return Some(v.value.clone());
⋮----
if nodes.is_empty() {
⋮----
let filename = self.contact_info_path.join("contact-info.bin");
let tmp_filename = &filename.with_extension("tmp");
⋮----
warn!(
⋮----
if let Err(err) = writer.flush() {
warn!("Failed to save contact info: {err}");
⋮----
warn!("Failed to create {}: {}", tmp_filename.display(), err);
⋮----
info!(
⋮----
pub fn restore_contact_info(&mut self, contact_info_path: &Path, contact_save_interval: u64) {
self.contact_info_path = contact_info_path.into();
⋮----
let filename = contact_info_path.join("contact-info.bin");
if !filename.exists() {
⋮----
bincode::deserialize_from(&mut BufReader::new(file)).unwrap_or_else(|err| {
warn!("Failed to deserialize {}: {}", filename.display(), err);
vec![]
⋮----
warn!("Failed to open {}: {}", filename.display(), err);
⋮----
let now = timestamp();
⋮----
if let Err(err) = gossip_crds.insert(node, now, GossipRoute::LocalMessage) {
warn!("crds insert failed {err:?}");
⋮----
pub fn id(&self) -> Pubkey {
self.keypair.load().pubkey()
⋮----
pub fn keypair(&self) -> Arc<Keypair> {
self.keypair.load_full()
⋮----
pub fn set_keypair(&self, new_keypair: Arc<Keypair>) {
let id = new_keypair.pubkey();
self.keypair.store(new_keypair);
self.my_contact_info.write().unwrap().hot_swap_pubkey(id);
self.refresh_my_gossip_contact_info();
⋮----
pub fn set_gossip_socket(&self, gossip_addr: SocketAddr) -> Result<(), ContactInfoError> {
⋮----
.write()
⋮----
.set_gossip(gossip_addr)?;
⋮----
Ok(())
⋮----
pub fn set_tvu_socket(&self, tvu_addr: SocketAddr) -> Result<(), ContactInfoError> {
⋮----
.set_tvu(contact_info::Protocol::UDP, tvu_addr)?;
⋮----
pub fn set_tpu_quic(&self, tpu_addr: SocketAddr) -> Result<(), ContactInfoError> {
⋮----
.set_tpu(contact_info::Protocol::QUIC, tpu_addr)?;
⋮----
pub fn set_tpu_forwards_quic(
⋮----
.set_tpu_forwards(contact_info::Protocol::QUIC, tpu_forwards_addr)?;
⋮----
pub fn set_tpu_vote(
⋮----
.set_tpu_vote(protocol, tpu_vote_addr)?;
⋮----
pub fn lookup_contact_info<R>(
⋮----
gossip_crds.get(*id).map(query)
⋮----
pub fn lookup_contact_info_by_gossip_addr(
⋮----
let mut nodes = gossip_crds.get_nodes_contact_info();
⋮----
.find(|node| node.gossip() == Some(*gossip_addr))
.cloned()
⋮----
pub fn my_contact_info(&self) -> ContactInfo {
self.my_contact_info.read().unwrap().clone()
⋮----
pub fn my_shred_version(&self) -> u16 {
self.my_contact_info.read().unwrap().shred_version()
⋮----
fn lookup_epoch_slots(&self, ix: EpochSlotsIndex) -> EpochSlots {
⋮----
.and_then(|v| v.epoch_slots())
⋮----
.unwrap_or_else(|| EpochSlots::new(self_pubkey, timestamp()))
⋮----
fn addr_to_string(&self, default_ip: &Option<IpAddr>, addr: &Option<SocketAddr>) -> String {
addr.filter(|addr| self.socket_addr_space.check(addr))
.map(|addr| {
if &Some(addr.ip()) == default_ip {
addr.port().to_string()
⋮----
addr.to_string()
⋮----
.unwrap_or_else(|| String::from("none"))
⋮----
pub fn rpc_info_trace(&self) -> String {
⋮----
let my_pubkey = self.id();
let my_shred_version = self.my_shred_version();
⋮----
.all_peers()
⋮----
.filter_map(|(node, last_updated)| {
⋮----
.rpc()
.filter(|addr| self.socket_addr_space.check(addr))?;
let node_version = self.get_node_version(node.pubkey());
if node.shred_version() != my_shred_version {
⋮----
let rpc_addr = node_rpc.ip();
Some(format!(
⋮----
.collect();
format!(
⋮----
pub fn contact_info_trace(&self) -> String {
⋮----
total_spy_nodes = total_spy_nodes.saturating_add(1);
⋮----
different_shred_nodes = different_shred_nodes.saturating_add(1);
⋮----
shred_spy_nodes = shred_spy_nodes.saturating_add(1);
⋮----
let ip_addr = node.gossip().as_ref().map(SocketAddr::ip);
⋮----
// TODO: This has a race condition if called from more than one thread.
pub fn push_lowest_slot(&self, min: Slot) {
let self_keypair = self.keypair();
let self_pubkey = self_keypair.pubkey();
⋮----
.map(|x| x.lowest)
⋮----
self.push_message(entry);
⋮----
// TODO: If two threads call into this function then epoch_slot_index has a
// race condition and the threads will overwrite each other in crds table.
pub fn push_epoch_slots(&self, mut update: &[Slot]) {
⋮----
self.time_gossip_read_lock("lookup_epoch_slots", &self.stats.epoch_slots_lookup);
⋮----
.filter_map(|ix| {
⋮----
let epoch_slots = crds_value.epoch_slots()?;
let first_slot = epoch_slots.first_slot()?;
Some((epoch_slots.wallclock, first_slot, ix))
⋮----
.collect()
⋮----
.map(|(_wallclock, slot, _index)| *slot)
.min()
.unwrap_or_default();
let max_slot: Slot = update.iter().max().cloned().unwrap_or(0);
⋮----
&& crds_data::MAX_EPOCH_SLOTS as usize <= current_slots.len()
⋮----
self.stats.epoch_slots_filled.add_relaxed(1);
⋮----
let mut epoch_slot_index = match current_slots.iter().max() {
⋮----
while !update.is_empty() {
⋮----
self.lookup_epoch_slots(epoch_slot_index)
⋮----
let n = slots.fill(update, now);
⋮----
entries.push(entry);
⋮----
epoch_slot_index = epoch_slot_index.wrapping_add(1);
⋮----
if let Err(err) = gossip_crds.insert(entry, now, GossipRoute::LocalMessage) {
error!("push_epoch_slots failed: {err:?}");
⋮----
pub fn push_restart_last_voted_fork_slots(
⋮----
self_keypair.pubkey(),
⋮----
self.my_shred_version(),
⋮----
self.push_message(CrdsValue::new(
⋮----
pub fn push_restart_heaviest_fork(
⋮----
from: self_keypair.pubkey(),
wallclock: timestamp(),
⋮----
shred_version: self.my_shred_version(),
⋮----
fn time_gossip_read_lock<'a>(
⋮----
TimedGuard::new(self.gossip.crds.read().unwrap(), label, counter)
⋮----
fn push_message(&self, message: CrdsValue) {
⋮----
.lock()
⋮----
.push(message);
⋮----
pub fn push_snapshot_hashes(
⋮----
if incremental.len() > MAX_INCREMENTAL_SNAPSHOT_HASHES {
return Err(ClusterInfoError::TooManyIncrementalSnapshotHashes);
⋮----
self.push_message(CrdsValue::new(message, &self_keypair));
⋮----
pub fn push_vote_at_index(&self, vote: Transaction, vote_index: u8, self_keypair: &Keypair) {
assert!(vote_index < MAX_VOTES);
⋮----
let vote = Vote::new(self_pubkey, vote, now).unwrap();
⋮----
if let Err(err) = gossip_crds.insert(vote, now, GossipRoute::LocalMessage) {
error!("push_vote failed: {err:?}");
⋮----
fn find_vote_index_to_evict(&self, new_vote_slot: Slot) -> Option<u8> {
⋮----
self.time_gossip_read_lock("gossip_read_push_vote", &self.stats.push_vote_read);
⋮----
let vote: &CrdsData = gossip_crds.get(&vote)?;
⋮----
CrdsData::Vote(_, vote) if vote.slot() < Some(new_vote_slot) => {
Some((vote.wallclock, ix))
⋮----
_ => panic!("this should not happen!"),
⋮----
.map(|(_ , ix)| ix)
⋮----
Some(num_crds_votes)
⋮----
pub fn push_vote(&self, tower: &[Slot], vote: Transaction) {
⋮----
debug_assert!(tower.iter().tuple_windows().all(|(a, b)| a < b));
⋮----
self.find_vote_index_to_evict(tower.last().copied().expect("Cannot push empty vote"))
⋮----
let (_, vote, hash, _) = vote_parser::parse_vote_transaction(&vote).unwrap();
panic!(
⋮----
debug_assert!(vote_index < MAX_VOTES);
self.push_vote_at_index(vote, vote_index, &self_keypair);
⋮----
pub fn refresh_vote(&self, refresh_vote: Transaction, refresh_vote_slot: Slot) {
⋮----
(0..MAX_VOTES).find(|ix| {
let vote = CrdsValueLabel::Vote(*ix, self_keypair.pubkey());
⋮----
panic!("this should not happen!");
⋮----
match prev_vote.slot() {
⋮----
error!("crds vote with no slots!");
⋮----
self.push_vote_at_index(refresh_vote, vote_index, &self_keypair);
⋮----
let Some(vote_index) = self.find_vote_index_to_evict(refresh_vote_slot) else {
⋮----
pub fn get_votes(&self, cursor: &mut Cursor) -> Vec<Transaction> {
⋮----
.time_gossip_read_lock("get_votes", &self.stats.get_votes)
.get_votes(cursor)
.map(|vote| {
let CrdsData::Vote(_, vote) = vote.value.data() else {
⋮----
vote.transaction().clone()
⋮----
self.stats.get_votes_count.add_relaxed(txs.len() as u64);
⋮----
pub fn get_votes_with_labels(
⋮----
let label = vote.value.label();
⋮----
(label, vote.transaction().clone())
⋮----
.unzip();
⋮----
pub fn push_duplicate_shred(
⋮----
self.gossip.push_duplicate_shred(
⋮----
pub fn get_snapshot_hashes_for_node(&self, pubkey: &Pubkey) -> Option<SnapshotHashes> {
⋮----
pub fn get_epoch_slots(&self, cursor: &mut Cursor) -> Vec<EpochSlots> {
let self_shred_version = Some(self.my_shred_version());
⋮----
.get_epoch_slots(cursor)
.filter(|entry| {
let origin = entry.value.pubkey();
gossip_crds.get_shred_version(&origin) == self_shred_version
⋮----
.map(|entry| match entry.value.data() {
CrdsData::EpochSlots(_, slots) => slots.clone(),
⋮----
pub fn get_restart_last_voted_fork_slots(
⋮----
let self_shred_version = self.my_shred_version();
⋮----
.get_entries(cursor)
.filter_map(|entry| {
let CrdsData::RestartLastVotedForkSlots(slots) = entry.value.data() else {
⋮----
(slots.shred_version == self_shred_version).then_some(slots)
⋮----
pub fn get_restart_heaviest_fork(&self, cursor: &mut Cursor) -> Vec<RestartHeaviestFork> {
⋮----
let CrdsData::RestartHeaviestFork(fork) = entry.value.data() else {
⋮----
(fork.shred_version == self_shred_version).then_some(fork)
⋮----
pub(crate) fn get_duplicate_shreds(&self, cursor: &mut Cursor) -> Vec<DuplicateShred> {
⋮----
.get_duplicate_shreds(cursor)
⋮----
CrdsData::DuplicateShred(_, dup) => dup.clone(),
⋮----
pub fn get_node_version(&self, pubkey: &Pubkey) -> Option<solana_version::Version> {
⋮----
.map(ContactInfo::version)
⋮----
fn check_socket_addr_space(&self, addr: &Option<SocketAddr>) -> bool {
addr.as_ref()
.map(|addr| self.socket_addr_space.check(addr))
⋮----
pub fn rpc_peers(&self) -> Vec<ContactInfo> {
⋮----
.get_nodes_contact_info()
.filter(|node| {
node.pubkey() != &self_pubkey
&& self.check_socket_addr_space(&node.rpc())
&& node.shred_version() == self_shred_version
⋮----
pub fn all_peers(&self) -> Vec<(ContactInfo, u64)> {
⋮----
.filter_map(|node| {
let contact_info = node.value.contact_info()?;
(contact_info.shred_version() == self_shred_version)
.then(|| (contact_info.clone(), node.local_timestamp))
⋮----
pub fn gossip_peers(&self) -> Vec<ContactInfo> {
let me = self.id();
⋮----
node.pubkey() != &me
&& self.check_socket_addr_space(&node.gossip())
⋮----
pub fn tvu_peers<R>(&self, query: impl ContactInfoQuery<R>) -> Vec<R> {
⋮----
self.time_gossip_read_lock("tvu_peers", &self.stats.tvu_peers)
⋮----
&& self.check_socket_addr_space(&node.tvu(contact_info::Protocol::UDP))
⋮----
.map(query)
⋮----
pub fn repair_peers(&self, slot: Slot) -> Vec<ContactInfo> {
⋮----
&& self.check_socket_addr_space(&node.serve_repair(contact_info::Protocol::UDP))
&& match gossip_crds.get::<&LowestSlot>(*node.pubkey()) {
⋮----
fn is_spy_node(node: &ContactInfo, socket_addr_space: &SocketAddrSpace) -> bool {
⋮----
node.tpu(contact_info::Protocol::UDP),
node.gossip(),
node.tvu(contact_info::Protocol::UDP),
⋮----
.all(|addr| {
addr.map(|addr| socket_addr_space.check(&addr))
⋮----
pub fn tpu_peers(&self) -> Vec<ContactInfo> {
⋮----
&& self.check_socket_addr_space(&node.tpu(contact_info::Protocol::UDP))
⋮----
fn refresh_my_gossip_contact_info(&self) {
let keypair = self.keypair();
⋮----
let mut node = self.my_contact_info.write().unwrap();
node.set_wallclock(timestamp());
node.clone()
⋮----
gossip_crds.insert(node, timestamp(), GossipRoute::LocalMessage)
⋮----
error!("refresh_my_gossip_contact_info failed: {err:?}");
⋮----
fn append_entrypoint_to_pulls<T: Iterator<Item = (SocketAddr, CrdsFilter)> + Clone>(
⋮----
let mut pulls = pulls.peekable();
⋮----
let mut entrypoints = self.entrypoints.write().unwrap();
let Some(entrypoint) = entrypoints.choose_mut(&mut rand::thread_rng()) else {
⋮----
if pulls.peek().is_some() {
⋮----
if now <= entrypoint.wallclock().saturating_add(THROTTLE_DELAY) {
⋮----
entrypoint.set_wallclock(now);
if let Some(entrypoint_gossip) = entrypoint.gossip() {
⋮----
.time_gossip_read_lock("entrypoint", &self.stats.entrypoint)
⋮----
.any(|node| node.gossip() == Some(entrypoint_gossip))
⋮----
let Some(entrypoint) = entrypoint.gossip() else {
⋮----
let filters = if pulls.peek().is_none() {
⋮----
.build_crds_filters(thread_pool, &self.gossip.crds, max_bloom_filter_bytes)
.into_iter(),
⋮----
Either::Right(pulls.clone().map(|(_, filter)| filter))
⋮----
self.stats.pull_from_entrypoint_count.add_relaxed(1);
Either::Right(pulls.chain(repeat(entrypoint).zip(filters)))
⋮----
fn new_pull_requests(
⋮----
let mut contact_info = self.my_contact_info();
contact_info.set_wallclock(now);
⋮----
let max_bloom_filter_bytes = get_max_bloom_filter_bytes(&self_info);
⋮----
.new_pull_request(
⋮----
.flatten()
⋮----
self.append_entrypoint_to_pulls(thread_pool, max_bloom_filter_bytes, pulls)
.map(move |(gossip_addr, filter)| {
let request = Protocol::PullRequest(filter, self_info.clone());
⋮----
.chain(pings)
⋮----
pub fn flush_push_queue(&self) {
⋮----
std::mem::take(&mut *self.local_message_pending_push_queue.lock().unwrap());
if !entries.is_empty() {
⋮----
let _ = gossip_crds.insert(entry, now, GossipRoute::LocalMessage);
⋮----
fn new_push_requests(
⋮----
let self_id = self.id();
⋮----
self.flush_push_queue();
⋮----
.new_push_messages(&self_id, timestamp(), stakes, |value| {
should_retain_crds_value(value, stakes, GossipFilterDirection::EgressPush)
⋮----
.add_relaxed(entries.len() as u64);
⋮----
.add_relaxed(num_pushes as u64);
⋮----
self.time_gossip_read_lock("push_req_lookup", &self.stats.new_push_requests2);
⋮----
.filter_map(|(pubkey, messages)| {
let addr = get_node_addr(
⋮----
Some((addr, messages))
⋮----
.flat_map(move |(peer, msgs): (SocketAddr, Vec<usize>)| {
⋮----
let msgs = msgs.into_iter().map(move |k| entries[k].clone());
let msgs = split_gossip_messages(PUSH_MESSAGE_MAX_PAYLOAD_SIZE, msgs)
.map(move |msgs| Protocol::PushMessage(self_id, msgs));
repeat(peer).zip(msgs)
⋮----
fn generate_new_gossip_requests(
⋮----
self.trim_crds_table(CRDS_UNIQUE_PUBKEY_CAPACITY, stakes);
let out = self.new_push_requests(stakes);
⋮----
let reqs = self.new_pull_requests(thread_pool, gossip_validators, stakes);
Either::Right(out.chain(reqs))
⋮----
fn run_gossip(
⋮----
self.generate_new_gossip_requests(
⋮----
.filter_map(|(addr, data)| make_gossip_packet(addr, &data, &self.stats))
.for_each(|pkt| packet_batch.push(pkt));
if !packet_batch.is_empty() {
if let Err(TrySendError::Full(packet_batch)) = sender.try_send(packet_batch.into()) {
⋮----
.add_relaxed(packet_batch.len() as u64);
⋮----
.add_relaxed(1);
⋮----
fn process_entrypoints(&self) -> bool {
⋮----
if entrypoints.is_empty() {
⋮----
for entrypoint in entrypoints.iter_mut() {
if entrypoint.pubkey() == &Pubkey::default() {
⋮----
.and_then(|addr| self.lookup_contact_info_by_gossip_addr(&addr))
⋮----
.all(|entrypoint| entrypoint.pubkey() != &Pubkey::default())
⋮----
fn handle_purge(
⋮----
.make_timeouts(self_pubkey, stakes, epoch_duration);
⋮----
.purge(&self_pubkey, thread_pool, timestamp(), &timeouts)
⋮----
self.stats.purge_count.add_relaxed(num_purged as u64);
⋮----
fn trim_crds_table(&self, cap: usize, stakes: &HashMap<Pubkey, u64>) {
if !self.gossip.crds.read().unwrap().should_trim(cap) {
⋮----
.map(ContactInfo::pubkey)
.copied()
.chain(std::iter::once(self.id()))
⋮----
self.stats.trim_crds_table.add_relaxed(1);
⋮----
match gossip_crds.trim(cap, &keep, stakes, timestamp()) {
⋮----
self.stats.trim_crds_table_failed.add_relaxed(1);
debug!("crds table trim failed: {err:?}");
⋮----
.add_relaxed(num_purged as u64);
⋮----
pub fn gossip(
⋮----
.num_threads(std::cmp::min(get_thread_count(), 8))
.thread_name(|i| format!("solGossipRun{i:02}"))
.build()
.unwrap();
let mut epoch_specs = bank_forks.map(EpochSpecs::from);
⋮----
.name("solGossip".to_string())
.spawn(move || {
⋮----
let mut last_contact_info_trace = timestamp();
let mut last_contact_info_save = timestamp();
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
let start = timestamp();
⋮----
self.save_contact_info();
⋮----
.as_mut()
.map(EpochSpecs::current_epoch_staked_nodes)
⋮----
let _ = self.run_gossip(
⋮----
gossip_validators.as_ref(),
⋮----
.map(EpochSpecs::epoch_duration)
.unwrap_or(DEFAULT_EPOCH_DURATION);
self.handle_purge(&thread_pool, epoch_duration, &stakes);
entrypoints_processed = entrypoints_processed || self.process_entrypoints();
⋮----
self.refresh_push_active_set(
⋮----
last_push = timestamp();
⋮----
let elapsed = timestamp() - start;
⋮----
sleep(Duration::from_millis(time_left));
⋮----
fn handle_batch_prune_messages(&self, messages: Vec<PruneData>, stakes: &HashMap<Pubkey, u64>) {
⋮----
if messages.is_empty() {
⋮----
.add_relaxed(messages.iter().map(|data| data.prunes.len() as u64).sum());
⋮----
match self.gossip.process_prune_msg(
⋮----
.add_relaxed(prune_message_timeout);
⋮----
.add_relaxed(bad_prune_destination);
⋮----
fn handle_batch_pull_requests(
⋮----
if !requests.is_empty() {
let response = self.handle_pull_requests(thread_pool, recycler, requests, stakes);
if !response.is_empty() {
if let Err(TrySendError::Full(response)) = response_sender.try_send(response.into())
⋮----
.add_relaxed(response.len() as u64);
⋮----
fn update_data_budget(&self, num_staked: usize) -> usize {
⋮----
let num_staked = num_staked.max(2);
self.outbound_budget.update(INTERVAL_MS, |bytes| {
⋮----
fn check_pull_request<'a, R>(
⋮----
let mut ping_cache = self.ping_cache.lock().unwrap();
⋮----
let (check, ping) = ping_cache.check(rng, &self.keypair(), now, node);
⋮----
if let Some(pkt) = make_gossip_packet(node.1, &ping, &self.stats) {
packet_batch.push(pkt);
⋮----
.add_relaxed(1)
⋮----
// Because pull-responses are sent back to packet.meta().socket_addr() of
// incoming pull-requests, pings are also sent to request.from_addr (as
// opposed to caller.gossip address).
⋮----
*cache.entry(node).or_insert_with(|| hard_check(node))
⋮----
// Pull requests take an incoming bloom filter of contained entries from a node
// and tries to send back to them the values it detects are missing.
fn handle_pull_requests(
⋮----
self.update_data_budget(stakes.len()) / PULL_RESPONSE_MIN_SERIALIZED_SIZE;
⋮----
requests.retain({
⋮----
self.check_pull_request(now, &mut rng, &mut packet_batch)
⋮----
self.gossip.generate_pull_responses(
⋮----
should_retain_crds_value(
⋮----
// Prioritize more recent values, staked values and ContactInfos.
⋮----
let age = now.saturating_sub(value.wallclock());
// score CrdsValue: 2x score if staked; 2x score if ContactInfo
⋮----
.saturating_sub(age)
.div(CRDS_GOSSIP_PULL_CRDS_TIMEOUT_MS)
.max(1);
let score = if stakes.contains_key(&value.pubkey()) {
⋮----
let score = match value.data() {
⋮----
.zip(pull_responses)
.flat_map(|(PullRequest { addr, .. }, values)| {
num_crds_values += values.len();
split_gossip_messages(PULL_RESPONSE_MAX_PAYLOAD_SIZE, values).map(move |values| {
let score = values.iter().map(get_score).max().unwrap_or_default();
⋮----
.shuffle(&mut rng)
.filter_map(|k| {
⋮----
let num_values = values.len();
⋮----
let packet = make_gossip_packet(*addr, &response, &self.stats)?;
Some((packet, num_values))
⋮----
.take_while(|(packet, _)| {
if self.outbound_budget.take(packet.meta().size) {
⋮----
self.stats.gossip_pull_request_no_budget.add_relaxed(1);
⋮----
.map(|(packet, num_values)| {
let num_bytes = packet.meta().size;
packet_batch.push(packet);
⋮----
.fold((0, 0), |a, b| (a.0 + b.0, a.1 + b.1));
let dropped_responses = num_crds_values.saturating_sub(sent_crds_values);
⋮----
.add_relaxed(dropped_responses as u64);
⋮----
.add_relaxed(total_bytes as u64);
⋮----
fn handle_batch_pull_responses(
⋮----
if !responses.is_empty() {
⋮----
self.handle_pull_response(responses, &timeouts);
⋮----
// Returns (failed, timeout, success)
fn handle_pull_response(
⋮----
let len = crds_values.len();
⋮----
.filter_pull_responses(timeouts, crds_values, timestamp(), &mut pull_stats)
⋮----
if !filtered_pulls.is_empty()
|| !filtered_pulls_expired_timeout.is_empty()
|| !failed_inserts.is_empty()
⋮----
self.gossip.process_pull_responses(
⋮----
timestamp(),
⋮----
self.stats.process_pull_response_count.add_relaxed(1);
self.stats.process_pull_response_len.add_relaxed(len as u64);
⋮----
.add_relaxed(pull_stats.failed_insert as u64);
⋮----
.add_relaxed(pull_stats.failed_timeout as u64);
⋮----
.add_relaxed(pull_stats.success as u64);
⋮----
fn handle_batch_ping_messages<S: Borrow<SocketAddr>>(
⋮----
let pongs = pings.into_iter().map(|(addr, ping)| {
⋮----
send_gossip_packets(pongs, recycler, response_sender, &self.stats);
⋮----
fn handle_batch_pong_messages<I>(&self, pongs: I, now: Instant)
⋮----
let mut pongs = pongs.into_iter().peekable();
if pongs.peek().is_some() {
⋮----
ping_cache.add(&pong, addr, now);
⋮----
fn handle_batch_push_messages(
⋮----
// Origins' pubkeys of upserted crds values.
⋮----
self.gossip.process_push_message(messages, now)
⋮----
let prune_messages = self.generate_prune_messages(thread_pool, origins, stakes);
let mut packet_batch = make_gossip_packet_batch(prune_messages, recycler, &self.stats);
self.new_push_requests(stakes)
⋮----
response_sender.try_send(packet_batch.into())
⋮----
fn generate_prune_messages(
⋮----
.prune_received_cache(&self_pubkey, origins, stakes)
⋮----
thread_pool.install(|| {
⋮----
.into_par_iter()
.filter_map(|(pubkey, prunes)| {
⋮----
Some((pubkey, addr, prunes))
⋮----
let wallclock = timestamp();
⋮----
.flat_map(|(destination, addr, prunes)| {
let prunes = prunes.into_par_iter().chunks(MAX_PRUNE_DATA_NODES);
rayon::iter::repeat((destination, addr)).zip(prunes)
⋮----
.map(|((destination, addr), prunes)| {
⋮----
prune_data.sign(&self_keypair);
⋮----
fn process_packets(
⋮----
discard_different_shred_version(msg, self_shred_version, &gossip_crds, &self.stats)
⋮----
if packets.len() < 4 && packets.iter().map(Vec::len).sum::<usize>() < 16 {
for (_, msg) in packets.iter_mut().flatten() {
discard_different_shred_version(msg);
⋮----
.par_iter_mut()
⋮----
.for_each(|(_, msg)| discard_different_shred_version(msg))
⋮----
let my_contact_info = self.my_contact_info();
⋮----
let mut nodes = values.iter().filter_map(CrdsValue::contact_info);
if nodes.any(|other| my_contact_info.check_duplicate(other)) {
Err(GossipError::DuplicateNodeInstance)
⋮----
if verify_gossip_addr(
⋮----
self.stats.num_unverifed_gossip_addrs.add_relaxed(1);
⋮----
let mut pull_requests = vec![];
let mut pull_responses = vec![];
let mut push_messages = vec![];
let mut prune_messages = vec![];
let mut ping_messages = vec![];
let mut pong_messages = vec![];
for (from_addr, packet) in packets.drain(..).flatten() {
⋮----
if !check_pull_request_shred_version(self_shred_version, &caller) {
self.stats.skip_pull_shred_version.add_relaxed(1);
⋮----
pubkey: caller.pubkey(),
⋮----
wallclock: caller.wallclock(),
⋮----
self.stats.window_request_loopback.add_relaxed(1);
⋮----
pull_requests.push(request);
⋮----
check_duplicate_instance(&data)?;
⋮----
data.retain(&mut verify_gossip_addr);
if !data.is_empty() {
pull_responses.append(&mut data);
⋮----
.add_relaxed(data.len() as u64);
push_messages.push((from, data));
⋮----
Protocol::PruneMessage(_from, data) => prune_messages.push(data),
Protocol::PingMessage(ping) => ping_messages.push((from_addr, ping)),
Protocol::PongMessage(pong) => pong_messages.push((from_addr, pong)),
⋮----
send_gossip_packets(pings, recycler, response_sender, &self.stats);
self.handle_batch_ping_messages(ping_messages, recycler, response_sender);
self.handle_batch_prune_messages(prune_messages, stakes);
self.handle_batch_push_messages(
⋮----
self.handle_batch_pull_responses(pull_responses, stakes, epoch_duration);
⋮----
self.handle_batch_pong_messages(pong_messages, Instant::now());
self.handle_batch_pull_requests(
⋮----
fn run_socket_consume(
⋮----
.recv()
.map(std::iter::once)?
.chain(receiver.try_iter())
⋮----
num_packets += packet_batch.len();
packet_buf.push(packet_batch);
if packet_buf.len() == CHANNEL_CONSUME_CAPACITY {
⋮----
.add_relaxed(num_packets as u64);
fn verify_packet(
⋮----
stats.record_received_packet(packet.deserialize_slice::<Protocol, _>(..))?;
protocol.sanitize().ok()?;
⋮----
values.retain(|value| {
should_retain_crds_value(value, stakes, GossipFilterDirection::Ingress)
⋮----
if values.is_empty() {
⋮----
protocol.verify().then(|| {
stats.packets_received_verified_count.add_relaxed(1);
(packet.meta().socket_addr(), protocol)
⋮----
if packet_buf.len() == 1 {
⋮----
.par_iter()
.filter_map(|packet| verify_packet(packet, &stakes, &self.stats))
⋮----
if let Err(TrySendError::Full(_)) = sender.try_send(packets_verified) {
self.stats.gossip_packets_dropped_count.add_relaxed(
⋮----
.fold(0, |acc, packet_batch| acc + packet_batch.len()) as u64,
⋮----
packet_buf.clear();
⋮----
fn run_listen(
⋮----
packet_buf.push(pkts);
⋮----
.map(|epoch_specs| epoch_specs.current_epoch_staked_nodes())
⋮----
self.process_packets(
⋮----
pub(crate) fn start_socket_consume_thread(
⋮----
.num_threads(get_thread_count().min(8))
.thread_name(|i| format!("solGossipCons{i:02}"))
⋮----
while !exit.load(Ordering::Relaxed) {
let result = self.run_socket_consume(
⋮----
epoch_specs.as_mut(),
⋮----
Err(err) => error!("gossip consume: {err}"),
⋮----
Builder::new().name(thread_name).spawn(run_consume).unwrap()
⋮----
pub(crate) fn listen(
⋮----
.thread_name(|i| format!("solGossipWork{i:02}"))
⋮----
.name("solGossipListen".to_string())
⋮----
let result = self.run_listen(
⋮----
error!(
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
_ => error!("gossip run_listen failed: {err}"),
⋮----
pub fn gossip_contact_info(id: Pubkey, gossip: SocketAddr, shred_version: u16) -> ContactInfo {
let mut node = ContactInfo::new(id,  timestamp(), shred_version);
let _ = node.set_gossip(gossip);
⋮----
pub fn gossip_node(
⋮----
bind_gossip_port_in_range(gossip_addr, VALIDATOR_PORT_RANGE, bind_ip_addr);
⋮----
Self::gossip_contact_info(id, SocketAddr::new(gossip_addr.ip(), port), shred_version);
(contact_info, gossip_socket, Some(ip_echo))
⋮----
pub fn spy_node(
⋮----
let (port, gossip_socket) = bind_in_range(bind_ip_addr, VALIDATOR_PORT_RANGE).unwrap();
⋮----
pub fn set_client_id(&self, client_id: ClientId) {
self.my_contact_info.write().unwrap().version.client = u16::try_from(client_id).unwrap();
⋮----
pub fn get_client_id(&self) -> ClientId {
self.my_contact_info.read().unwrap().version.client.into()
⋮----
pub struct Sockets {
⋮----
pub struct NodeConfig {
⋮----
pub fn push_messages_to_peer_for_tests(
⋮----
let reqs: Vec<_> = split_gossip_messages(PUSH_MESSAGE_MAX_PAYLOAD_SIZE, messages)
.map(move |payload| (peer_gossip, Protocol::PushMessage(self_id, payload)))
⋮----
let packet_batch = make_gossip_packet_batch(
⋮----
let sock = bind_to_localhost_unique().expect("should bind");
⋮----
fn check_pull_request_shred_version(self_shred_version: u16, caller: &CrdsValue) -> bool {
let shred_version = match caller.data() {
CrdsData::ContactInfo(node) => node.shred_version(),
⋮----
fn discard_different_shred_version(
⋮----
values.retain(|value| match value.data() {
CrdsData::ContactInfo(ci) => ci.shred_version() == self_shred_version,
_ => crds.get_shred_version(&value.pubkey()) == Some(self_shred_version),
⋮----
let num_skipped = num_values - values.len();
⋮----
skip_shred_version_counter.add_relaxed(num_skipped as u64);
⋮----
fn get_node_addr(
⋮----
query(node).filter(|addr| socket_addr_space.check(addr))
⋮----
fn verify_gossip_addr<R: Rng + CryptoRng>(
⋮----
let (pubkey, addr) = match value.data() {
CrdsData::ContactInfo(node) => (node.pubkey(), node.gossip()),
⋮----
let Some(addr) = addr.filter(|addr| socket_addr_space.check(addr)) else {
⋮----
let mut ping_cache = ping_cache.lock().unwrap();
ping_cache.check(rng, keypair, Instant::now(), node)
⋮----
pings.push((addr, ping));
⋮----
fn send_gossip_packets<S: Borrow<SocketAddr>>(
⋮----
let pkts = pkts.into_iter();
if pkts.len() != 0 {
let pkts = make_gossip_packet_batch(pkts, recycler, stats);
if let Err(TrySendError::Full(pkts)) = sender.try_send(pkts.into()) {
⋮----
.add_relaxed(pkts.len() as u64);
⋮----
fn make_gossip_packet_batch<S: Borrow<SocketAddr>>(
⋮----
let record_gossip_packet = |(_, pkt): &(_, Protocol)| stats.record_gossip_packet(pkt);
let pkts = pkts.into_iter().inspect(record_gossip_packet);
⋮----
fn make_gossip_packet(
⋮----
match Packet::from_data(Some(addr.borrow()), pkt) {
⋮----
error!("failed to write gossip packet: {err:?}");
⋮----
stats.record_gossip_packet(pkt);
Some(out)
⋮----
mod tests {
⋮----
NonZeroUsize::new(DEFAULT_QUIC_ENDPOINTS).unwrap();
⋮----
fn old_pull_requests(
⋮----
self.new_pull_requests(thread_pool, gossip_validators, stakes)
.partition_map(|(addr, protocol)| {
⋮----
fn test_gossip_node() {
⋮----
assert!(ClusterInfo::is_spy_node(
⋮----
ClusterInfo::gossip_node(solana_pubkey::new_rand(), &"1.1.1.1:0".parse().unwrap(), 0);
⋮----
fn test_handle_pull() {
⋮----
let node = Node::new_localhost_with_pubkey(&keypair.pubkey());
⋮----
let data = test_crds_values(entrypoint_pubkey);
⋮----
cluster_info.id(),
⋮----
assert_eq!(
⋮----
fn test_handle_pong_messages() {
⋮----
ContactInfo::new_localhost(&this_node.pubkey(), timestamp()),
this_node.clone(),
⋮----
repeat_with(|| new_rand_remote_node(&mut rng))
.take(128)
⋮----
let mut ping_cache = cluster_info.ping_cache.lock().unwrap();
⋮----
.map(|(keypair, socket)| {
let node = (keypair.pubkey(), *socket);
let (check, ping) = ping_cache.check(&mut rng, &this_node, now, node);
assert!(!check);
ping.unwrap()
⋮----
.zip(&remote_nodes)
.map(|(ping, (keypair, socket))| (*socket, Pong::new(ping, keypair)))
⋮----
cluster_info.handle_batch_pong_messages(pongs, now);
⋮----
let (check, _) = ping_cache.check(&mut rng, &this_node, now, node);
assert!(check);
⋮----
let (keypair, socket) = new_rand_remote_node(&mut rng);
let node = (keypair.pubkey(), socket);
⋮----
fn test_handle_ping_messages() {
⋮----
.filter(|(_, socket)| socket.port() != 0)
⋮----
.map(|(keypair, _)| Ping::new(rng.gen(), keypair))
⋮----
.map(|ping| Pong::new(ping, &this_node))
⋮----
cluster_info.handle_batch_ping_messages(
remote_nodes.iter().map(|(_, socket)| socket).zip(pings),
⋮----
receiver.recv().unwrap()
⋮----
assert_eq!(remote_nodes.len(), packets.len());
for (packet, (_, socket), pong) in izip!(
⋮----
assert_eq!(packet.meta().socket_addr(), socket);
let bytes = serialize(&pong).unwrap();
match packet.deserialize_slice(..).unwrap() {
Protocol::PongMessage(pong) => assert_eq!(serialize(&pong).unwrap(), bytes),
_ => panic!("invalid packet!"),
⋮----
fn test_crds_values(pubkey: Pubkey) -> Vec<CrdsValue> {
let entrypoint = ContactInfo::new_localhost(&pubkey, timestamp());
⋮----
vec![entrypoint_crdsvalue]
⋮----
fn test_cluster_spy_gossip() {
let thread_pool = ThreadPoolBuilder::new().build().unwrap();
⋮----
cluster_info.insert_info(spy);
cluster_info.gossip.refresh_push_active_set(
&cluster_info.keypair(),
cluster_info.my_shred_version(),
⋮----
let mut reqs = cluster_info.generate_new_gossip_requests(
⋮----
assert!(reqs.all(|(addr, _)| {
⋮----
fn test_cluster_info_new() {
⋮----
let d = ContactInfo::new_localhost(&keypair.pubkey(), timestamp());
let cluster_info = ClusterInfo::new(d.clone(), keypair, SocketAddrSpace::Unspecified);
assert_eq!(d.pubkey(), &cluster_info.id());
⋮----
fn insert_info_test() {
⋮----
let d = ContactInfo::new_localhost(&solana_pubkey::new_rand(), timestamp());
let label = CrdsValueLabel::ContactInfo(*d.pubkey());
cluster_info.insert_info(d);
let gossip_crds = cluster_info.gossip.crds.read().unwrap();
assert!(gossip_crds.get::<&CrdsValue>(&label).is_some());
⋮----
fn assert_in_range(x: u16, range: (u16, u16)) {
assert!(x >= range.0);
assert!(x < range.1);
⋮----
fn check_sockets<T>(sockets: &[T], ip: IpAddr, range: (u16, u16))
⋮----
assert!(!sockets.is_empty());
let port = sockets[0].borrow().local_addr().unwrap().port();
⋮----
let s = s.borrow();
let local_addr = s.local_addr().unwrap();
assert_eq!(local_addr.ip(), ip);
assert_in_range(local_addr.port(), range);
assert_eq!(local_addr.port(), port);
⋮----
fn check_socket<T>(socket: &T, ip: IpAddr, range: (u16, u16))
⋮----
check_sockets(std::slice::from_ref(socket), ip, range);
⋮----
fn check_node_sockets(node: &Node, ip: IpAddr, range: (u16, u16)) {
check_socket(&node.sockets.repair, ip, range);
check_socket(&node.sockets.tvu_quic, ip, range);
⋮----
check_socket(alpenglow_port, ip, range);
⋮----
check_sockets(&node.sockets.gossip, ip, range);
check_sockets(&node.sockets.tvu, ip, range);
check_sockets(&node.sockets.tpu, ip, range);
⋮----
fn new_with_external_ip_test_random() {
⋮----
let port_range = localhost_port_range_for_tests();
⋮----
bind_ip_addrs: BindIpAddrs::new(vec![IpAddr::V4(ip)]).unwrap(),
⋮----
check_node_sockets(&node, IpAddr::V4(ip), port_range);
⋮----
fn new_with_external_ip_test_gossip() {
⋮----
bind_ip_addrs: BindIpAddrs::new(vec![ip]).unwrap(),
⋮----
check_node_sockets(&node, ip, port_range);
check_sockets(&node.sockets.gossip, ip, port_range);
⋮----
fn test_gossip_signature_verification() {
⋮----
let contact_info = ContactInfo::new_localhost(&keypair.pubkey(), 0);
let peer = ContactInfo::new_localhost(&peer_keypair.pubkey(), 0);
⋮----
cluster_info.ping_cache.lock().unwrap().mock_pong(
*peer.pubkey(),
peer.gossip().unwrap(),
⋮----
cluster_info.insert_info(peer);
⋮----
cluster_info.flush_push_queue();
let (entries, push_messages, _) = cluster_info.gossip.new_push_messages(
&cluster_info.id(),
⋮----
.map(|(pubkey, indices)| {
let values = indices.into_iter().map(|k| entries[k].clone()).collect();
⋮----
assert!(!push_messages.is_empty());
⋮----
.values()
.for_each(|v| v.par_iter().for_each(|v| assert!(v.verify())));
⋮----
cluster_info.keypair().deref(),
⋮----
.ok()
⋮----
fn test_refresh_vote_eviction() {
⋮----
let mut prev_votes = vec![];
⋮----
prev_votes.push(slot);
let unrefresh_vote = Vote::new(vec![slot], Hash::new_unique());
⋮----
if first_vote.is_none() {
first_vote = Some(vote_tx.clone());
⋮----
cluster_info.push_vote(&prev_votes, vote_tx);
⋮----
let initial_votes = cluster_info.get_votes(&mut Cursor::default());
assert_eq!(initial_votes.len(), MAX_VOTES as usize);
⋮----
let refresh_vote = Vote::new(vec![refresh_slot], Hash::new_unique());
⋮----
refresh_vote.clone(),
⋮----
cluster_info.refresh_vote(refresh_tx.clone(), refresh_slot);
let current_votes = cluster_info.get_votes(&mut Cursor::default());
assert_eq!(initial_votes, current_votes);
assert!(!current_votes.contains(&refresh_tx));
⋮----
let votes = cluster_info.get_votes(&mut Cursor::default());
assert_eq!(votes.len(), MAX_VOTES as usize);
assert!(votes.contains(&refresh_tx));
assert!(!votes.contains(&first_vote.unwrap()));
⋮----
fn test_refresh_vote() {
⋮----
let unrefresh_tower = vec![1, 3, unrefresh_slot];
let unrefresh_vote = Vote::new(unrefresh_tower.clone(), Hash::new_unique());
⋮----
cluster_info.push_vote(&unrefresh_tower, unrefresh_tx.clone());
⋮----
let votes = cluster_info.get_votes(&mut cursor);
assert_eq!(votes, vec![unrefresh_tx.clone()]);
⋮----
let refresh_tower = vec![1, 3, unrefresh_slot, refresh_slot];
let refresh_vote = Vote::new(refresh_tower.clone(), Hash::new_unique());
⋮----
assert_eq!(votes.len(), 2);
assert!(votes.contains(&unrefresh_tx));
⋮----
&new_signer.pubkey(),
⋮----
cluster_info.refresh_vote(latest_refresh_tx.clone(), refresh_slot);
⋮----
assert_eq!(votes.len(), 1);
assert_eq!(votes[0], latest_refresh_tx);
⋮----
assert!(votes.contains(&latest_refresh_tx));
⋮----
fn test_push_vote() {
⋮----
ClusterInfo::new(contact_info, keypair.clone(), SocketAddrSpace::Unspecified);
⋮----
assert_eq!(votes, vec![]);
⋮----
vec![1, 3, 7],
⋮----
let tower = vec![7];
cluster_info.push_vote(&tower, tx.clone());
let (labels, votes) = cluster_info.get_votes_with_labels(&mut cursor);
assert_eq!(votes, vec![tx]);
assert_eq!(labels.len(), 1);
⋮----
assert_eq!(pubkey, keypair.pubkey());
⋮----
_ => panic!("Bad match"),
⋮----
fn new_vote_transaction(slots: Vec<Slot>) -> Transaction {
⋮----
fn test_push_votes_with_tower() {
⋮----
let (labels, _) = cluster_info.get_votes_with_labels(&mut Cursor::default());
⋮----
let CrdsData::Vote(_, vote) = &gossip_crds.get::<&CrdsData>(&label).unwrap() else {
⋮----
assert!(vote_slots.insert(vote.slot().unwrap()));
⋮----
vote_slots.into_iter().sorted().collect()
⋮----
tower.push(slot);
let vote = new_vote_transaction(vec![slot]);
cluster_info.push_vote(&tower, vote);
let vote_slots = get_vote_slots(&cluster_info);
let min_vote = k.saturating_sub(MAX_VOTES as usize - 1) as u64;
assert!(vote_slots.into_iter().eq(min_vote..=(k as u64)));
sleep(Duration::from_millis(1));
⋮----
tower.clear();
tower.extend(0..=slot);
⋮----
assert!(panic::catch_unwind(|| cluster_info.push_vote(&tower, vote))
⋮----
fn test_push_epoch_slots() {
⋮----
let slots = cluster_info.get_epoch_slots(&mut Cursor::default());
assert!(slots.is_empty());
cluster_info.push_epoch_slots(&[0]);
⋮----
let slots = cluster_info.get_epoch_slots(&mut cursor);
assert_eq!(slots.len(), 1);
⋮----
let mut node = ContactInfo::new_rand(&mut rng, Some(node_pubkey));
node.set_shred_version(42);
let epoch_slots = EpochSlots::new_rand(&mut rng, Some(node_pubkey));
let entries = vec![
⋮----
let mut gossip_crds = cluster_info.gossip.crds.write().unwrap();
⋮----
assert!(gossip_crds
⋮----
assert_eq!(slots[0].from, cluster_info.id());
⋮----
let mut node = cluster_info.my_contact_info.write().unwrap();
⋮----
cluster_info.refresh_my_gossip_contact_info();
⋮----
assert_eq!(slots.len(), 2);
⋮----
assert_eq!(slots[1].from, node_pubkey);
⋮----
fn test_append_entrypoint_to_pulls() {
⋮----
ContactInfo::new_localhost(&node_keypair.pubkey(), timestamp()),
⋮----
let entrypoint = ContactInfo::new_localhost(&entrypoint_pubkey, timestamp());
cluster_info.set_entrypoint(entrypoint.clone());
let (pings, pulls) = cluster_info.old_pull_requests(&thread_pool, None, &HashMap::new());
assert!(pings.is_empty());
assert_eq!(pulls.len(), MIN_NUM_BLOOM_FILTERS);
⋮----
assert_eq!(addr, entrypoint.gossip().unwrap());
⋮----
assert!(value.verify());
assert_eq!(value.pubkey(), cluster_info.id())
⋮----
_ => panic!("wrong protocol"),
⋮----
let timeouts = cluster_info.gossip.make_timeouts(
⋮----
cluster_info.handle_pull_response(vec![entrypoint_crdsvalue], &timeouts);
⋮----
assert_eq!(pings.len(), 1);
⋮----
assert_eq!(*cluster_info.entrypoints.read().unwrap(), vec![entrypoint]);
⋮----
fn test_tvu_peers_and_stakes() {
⋮----
let contact_info = ContactInfo::new_localhost(&id, timestamp());
cluster_info.insert_info(contact_info);
⋮----
let mut contact_info = ContactInfo::new_localhost(&id2, timestamp());
cluster_info.insert_info(contact_info.clone());
stakes.insert(id2, 10);
contact_info.set_wallclock(timestamp() + 1);
⋮----
let mut contact_info = ContactInfo::new_localhost(&id3, timestamp());
contact_info.remove_tvu();
⋮----
stakes.insert(id3, 10);
⋮----
let mut contact_info = ContactInfo::new_localhost(&id4, timestamp());
contact_info.set_shred_version(1);
assert_ne!(contact_info.shred_version(), d.shred_version());
⋮----
stakes.insert(id4, 10);
⋮----
fn test_pull_from_entrypoint_if_not_present() {
⋮----
let mut entrypoint = ContactInfo::new_localhost(&entrypoint_pubkey, timestamp());
⋮----
.set_gossip(socketaddr!("127.0.0.2:1234"))
⋮----
let other_node = ContactInfo::new_localhost(&other_node_pubkey, timestamp());
assert_ne!(other_node.gossip().unwrap(), entrypoint.gossip().unwrap());
⋮----
*other_node.pubkey(),
other_node.gossip().unwrap(),
⋮----
cluster_info.insert_info(other_node.clone());
stakes.insert(other_node_pubkey, 10);
let (pings, pulls) = cluster_info.old_pull_requests(&thread_pool, None, &stakes);
⋮----
assert!(pulls
⋮----
cluster_info.entrypoints.write().unwrap()[0].set_wallclock(0);
⋮----
assert_eq!(pulls.len(), 2 * MIN_NUM_BLOOM_FILTERS);
⋮----
fn test_repair_peers() {
⋮----
LowestSlot::new(other_node_pubkey, peer_lowest, timestamp()),
⋮----
let _ = gossip_crds.insert(value, timestamp(), GossipRoute::LocalMessage);
⋮----
assert_eq!(cluster_info.repair_peers(5).len(), 5);
⋮----
fn test_push_epoch_slots_large() {
⋮----
let range: Vec<Slot> = repeat_with(|| rng.gen_range(1..32))
.scan(0, |slot, step| {
⋮----
Some(*slot)
⋮----
.take(32000)
⋮----
cluster_info.push_epoch_slots(&range[..16000]);
cluster_info.push_epoch_slots(&range[16000..]);
⋮----
let slots: Vec<_> = slots.iter().flat_map(|x| x.to_slots(0)).collect();
assert_eq!(slots, range);
⋮----
fn test_process_entrypoint_without_adopt_shred_version() {
⋮----
ContactInfo::new_localhost(&node_keypair.pubkey(), timestamp());
contact_info.set_shred_version(2);
⋮----
assert_eq!(cluster_info.my_shred_version(), 2);
let entrypoint_gossip_addr = socketaddr!("127.0.0.2:1234");
let mut entrypoint = ContactInfo::new_localhost(&Pubkey::default(), timestamp());
entrypoint.set_gossip(entrypoint_gossip_addr).unwrap();
assert_eq!(entrypoint.shred_version(), 0);
cluster_info.set_entrypoint(entrypoint);
⋮----
ContactInfo::new_localhost(&solana_pubkey::new_rand(), timestamp());
⋮----
.set_gossip(entrypoint_gossip_addr)
⋮----
gossiped_entrypoint_info.set_shred_version(1);
cluster_info.insert_info(gossiped_entrypoint_info.clone());
⋮----
assert_eq!(cluster_info.entrypoints.read().unwrap().len(), 1);
⋮----
assert!(entrypoints_processed);
⋮----
fn test_pull_request_time_pruning() {
⋮----
let shred_version = cluster_info.my_shred_version();
let mut peers: Vec<Pubkey> = vec![];
⋮----
let data: Vec<_> = repeat_with(|| {
⋮----
peers.push(keypair.pubkey());
let mut rand_ci = ContactInfo::new_rand(&mut rng, Some(keypair.pubkey()));
rand_ci.set_shred_version(shred_version);
rand_ci.set_wallclock(timestamp());
⋮----
.take(NO_ENTRIES)
⋮----
fn test_get_duplicate_shreds() {
⋮----
let node = Node::new_localhost_with_pubkey(&host1_key.pubkey());
⋮----
host1_key.clone(),
⋮----
assert!(cluster_info.get_duplicate_shreds(&mut cursor).is_empty());
⋮----
let shredder = Shredder::new(slot, parent_slot, reference_tick, version).unwrap();
⋮----
let shred1 = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
let shred2 = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
assert!(cluster_info
⋮----
let entries = cluster_info.get_duplicate_shreds(&mut cursor);
assert_eq!(3, entries.len());
for (i, shred_data) in entries.iter().enumerate() {
assert_eq!(shred_data.from, host1_key.pubkey());
assert_eq!(shred_data.slot, 53084024);
assert_eq!(shred_data.chunk_index() as usize, i);
⋮----
let shred3 = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
let shred4 = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
⋮----
let entries1 = cluster_info.get_duplicate_shreds(&mut cursor);
assert_eq!(3, entries1.len());
for (i, shred_data) in entries1.iter().enumerate() {
⋮----
assert_eq!(shred_data.slot, 53084025);
⋮----
fn test_push_restart_last_voted_fork_slots() {
⋮----
let slots = cluster_info.get_restart_last_voted_fork_slots(&mut Cursor::default());
⋮----
let mut update: Vec<Slot> = vec![0];
⋮----
update.push(i * 1050 + j);
⋮----
let slots = cluster_info.get_restart_last_voted_fork_slots(&mut cursor);
⋮----
let retrieved_slots = slots[0].to_slots(0);
assert!(retrieved_slots[0] < 69000);
assert_eq!(retrieved_slots.last(), Some(84999).as_ref());
⋮----
let mut slots = RestartLastVotedForkSlots::new_rand(&mut rng, Some(node_pubkey));
⋮----
assert_eq!(slots[0].from, node_pubkey);
assert_eq!(slots[1].from, cluster_info.id());
⋮----
fn test_push_restart_heaviest_fork() {
⋮----
let pubkey = keypair.pubkey();
⋮----
let heaviest_forks = cluster_info.get_restart_heaviest_fork(&mut cursor);
assert_eq!(heaviest_forks, vec![]);
⋮----
cluster_info.push_restart_heaviest_fork(slot1, hash1, stake1);
⋮----
assert_eq!(heaviest_forks.len(), 1);
⋮----
assert_eq!(fork.last_slot, slot1);
assert_eq!(fork.last_slot_hash, hash1);
assert_eq!(fork.observed_stake, stake1);
assert_eq!(fork.from, pubkey);
⋮----
let mut new_node = ContactInfo::new_rand(&mut rng, Some(pubkey2));
new_node.set_shred_version(42);
⋮----
let heaviest_forks = cluster_info.get_restart_heaviest_fork(&mut Cursor::default());
⋮----
assert_eq!(heaviest_forks[0].from, pubkey);
⋮----
assert_eq!(heaviest_forks[0].from, pubkey2);
⋮----
fn test_contact_trace() {
⋮----
.as_ref(),
⋮----
.unwrap(),
⋮----
let node = Node::new_localhost_with_pubkey(&keypair44.pubkey());
info!("{node:?}");
ClusterInfo::new(node.info, keypair44.clone(), SocketAddrSpace::Unspecified)
⋮----
let node = Node::new_localhost_with_pubkey(&keypair43.pubkey());
ClusterInfo::new(node.info, keypair43.clone(), SocketAddrSpace::Unspecified)
⋮----
assert_eq!(keypair43.pubkey().to_string().len(), 43);
assert_eq!(keypair44.pubkey().to_string().len(), 44);
let trace = cluster_info44.contact_info_trace();
info!("cluster:\n{trace}");
assert_eq!(trace.len(), CLUSTER_INFO_TRACE_LENGTH);
let trace = cluster_info44.rpc_info_trace();
info!("rpc:\n{trace}");
assert_eq!(trace.len(), RPC_INFO_TRACE_LENGTH);
let trace = cluster_info43.contact_info_trace();
⋮----
let trace = cluster_info43.rpc_info_trace();
⋮----
fn test_discard_different_shred_version_push_message() {
⋮----
keypair.pubkey(),
⋮----
let mut msg = Protocol::PushMessage(keypair.pubkey(), vec![ci.clone()]);
discard_different_shred_version(&mut msg, self_shred_version, &crds, &stats);
⋮----
assert_eq!(values.len(), 1);
⋮----
ContactInfo::new(keypair.pubkey(),  1234567890, 1);
⋮----
let mut msg = Protocol::PushMessage(keypair.pubkey(), vec![ci_wrong_shred_version]);
⋮----
assert_eq!(values.len(), 0);
⋮----
let epoch_slots = EpochSlots::new_rand(&mut rng, Some(keypair.pubkey()));
⋮----
let mut msg = Protocol::PushMessage(keypair.pubkey(), vec![es]);
⋮----
keypair2.pubkey(),
⋮----
assert!(crds
⋮----
let mut msg = Protocol::PushMessage(keypair.pubkey(), vec![es.clone()]);
⋮----
let mut msg = Protocol::PushMessage(keypair.pubkey(), entries);
⋮----
assert_eq!(values.len(), 3);
⋮----
crds.remove(&ci.label(),  0);
⋮----
assert_eq!(values.len(), 2);

================
File: gossip/src/contact_info.rs
================
pub use solana_client::connection_cache::Protocol;
⋮----
const_assert_eq!(SOCKET_CACHE_SIZE, 14);
⋮----
pub trait ContactInfoQuery<R>: Fn(&ContactInfo) -> R {}
⋮----
pub enum Error {
⋮----
pub struct ContactInfo {
⋮----
struct SocketEntry {
⋮----
define_tlv_enum!(
⋮----
struct ContactInfoLite {
⋮----
macro_rules! get_socket {
⋮----
macro_rules! set_socket {
⋮----
macro_rules! remove_socket {
⋮----
impl ContactInfo {
pub fn new(pubkey: Pubkey, wallclock: u64, shred_version: u16) -> Self {
⋮----
outset: get_node_outset(),
⋮----
pub fn pubkey(&self) -> &Pubkey {
⋮----
pub fn wallclock(&self) -> u64 {
⋮----
pub fn shred_version(&self) -> u16 {
⋮----
pub(crate) fn version(&self) -> &solana_version::Version {
⋮----
pub fn hot_swap_pubkey(&mut self, pubkey: Pubkey) {
⋮----
self.outset = get_node_outset();
⋮----
pub fn set_wallclock(&mut self, wallclock: u64) {
⋮----
pub fn set_shred_version(&mut self, shred_version: u16) {
⋮----
get_socket!(gossip, SOCKET_TAG_GOSSIP);
get_socket!(rpc, SOCKET_TAG_RPC);
get_socket!(rpc_pubsub, SOCKET_TAG_RPC_PUBSUB);
get_socket!(
⋮----
get_socket!(tpu, SOCKET_TAG_TPU, SOCKET_TAG_TPU_QUIC);
⋮----
get_socket!(tpu_vote, SOCKET_TAG_TPU_VOTE, SOCKET_TAG_TPU_VOTE_QUIC);
get_socket!(tvu, SOCKET_TAG_TVU, SOCKET_TAG_TVU_QUIC);
get_socket!(alpenglow, SOCKET_TAG_ALPENGLOW);
set_socket!(set_gossip, SOCKET_TAG_GOSSIP);
set_socket!(set_rpc, SOCKET_TAG_RPC);
set_socket!(set_rpc_pubsub, SOCKET_TAG_RPC_PUBSUB);
set_socket!(set_tpu, SOCKET_TAG_TPU, SOCKET_TAG_TPU_QUIC);
set_socket!(
⋮----
set_socket!(set_tpu_vote, SOCKET_TAG_TPU_VOTE, SOCKET_TAG_TPU_VOTE_QUIC);
set_socket!(set_tvu, SOCKET_TAG_TVU, SOCKET_TAG_TVU_QUIC);
set_socket!(set_alpenglow, SOCKET_TAG_ALPENGLOW);
remove_socket!(
⋮----
remove_socket!(remove_tpu, SOCKET_TAG_TPU, SOCKET_TAG_TPU_QUIC);
⋮----
remove_socket!(remove_tvu, SOCKET_TAG_TVU, SOCKET_TAG_TVU_QUIC);
remove_socket!(remove_alpenglow, SOCKET_TAG_ALPENGLOW);
⋮----
fn get_socket(&self, key: u8) -> Result<SocketAddr, Error> {
⋮----
.get(usize::from(entry.index))
.ok_or(Error::InvalidIpAddrIndex {
⋮----
num_addrs: self.addrs.len(),
⋮----
sanitize_socket(&socket)?;
return Ok(socket);
⋮----
Err(Error::SocketNotFound(key))
⋮----
fn push_addr(&mut self, addr: IpAddr) -> Result<u8, Error> {
match self.addrs.iter().position(|k| k == &addr) {
Some(index) => u8::try_from(index).map_err(|_| Error::IpAddrsSaturated),
⋮----
let index = u8::try_from(self.addrs.len()).map_err(|_| Error::IpAddrsSaturated)?;
self.addrs.push(addr);
Ok(index)
⋮----
pub fn set_socket(&mut self, key: u8, socket: SocketAddr) -> Result<(), Error> {
⋮----
self.remove_socket(key);
let mut offset = socket.port();
let index = self.sockets.iter().position(|entry| {
offset = match offset.checked_sub(entry.offset) {
⋮----
index: self.push_addr(socket.ip())?,
⋮----
None => self.sockets.push(entry),
⋮----
self.sockets.insert(index, entry);
⋮----
if let Some(entry) = self.cache.get_mut(usize::from(key)) {
⋮----
debug_assert_matches!(sanitize_entries(&self.addrs, &self.sockets), Ok(()));
Ok(())
⋮----
fn remove_socket(&mut self, key: u8) {
if let Some(index) = self.sockets.iter().position(|entry| entry.key == key) {
let entry = self.sockets.remove(index);
if let Some(next_entry) = self.sockets.get_mut(index) {
⋮----
self.maybe_remove_addr(entry.index);
⋮----
fn maybe_remove_addr(&mut self, index: u8) {
if !self.sockets.iter().any(|entry| entry.index == index) {
self.addrs.remove(usize::from(index));
⋮----
pub fn is_valid_address(addr: &SocketAddr, socket_addr_space: &SocketAddrSpace) -> bool {
addr.port() != 0u16 && Self::is_valid_ip(addr.ip()) && socket_addr_space.check(addr)
⋮----
fn is_valid_ip(addr: IpAddr) -> bool {
!(addr.is_unspecified() || addr.is_multicast())
⋮----
pub fn new_rand<R: rand::Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
⋮----
let now = solana_time_utils::timestamp() - delay + rng.gen_range(0..2 * delay);
let pubkey = pubkey.unwrap_or_else(solana_pubkey::new_rand);
⋮----
let _ = node.set_gossip((Ipv4Addr::LOCALHOST, rng.gen_range(1024..u16::MAX)));
⋮----
pub fn new_gossip_entry_point(gossip_addr: &SocketAddr) -> Self {
⋮----
if let Err(err) = node.set_gossip(*gossip_addr) {
error!("Invalid entrypoint: {gossip_addr}, {err:?}");
⋮----
pub fn new_localhost(pubkey: &Pubkey, wallclock: u64) -> Self {
⋮----
node.set_gossip((Ipv4Addr::LOCALHOST, 8000)).unwrap();
node.set_tvu(UDP, (Ipv4Addr::LOCALHOST, 8001)).unwrap();
node.set_tvu(QUIC, (Ipv4Addr::LOCALHOST, 8002)).unwrap();
node.set_tpu(UDP, (Ipv4Addr::LOCALHOST, 8003)).unwrap();
node.set_tpu(QUIC, (Ipv4Addr::LOCALHOST, 8009)).unwrap();
node.set_tpu_forwards(UDP, (Ipv4Addr::LOCALHOST, 8004))
.unwrap();
node.set_tpu_forwards(QUIC, (Ipv4Addr::LOCALHOST, 8010))
⋮----
node.set_tpu_vote(UDP, (Ipv4Addr::LOCALHOST, 8005)).unwrap();
node.set_tpu_vote(QUIC, (Ipv4Addr::LOCALHOST, 8007))
⋮----
node.set_rpc((Ipv4Addr::LOCALHOST, DEFAULT_RPC_PORT))
⋮----
node.set_rpc_pubsub((Ipv4Addr::LOCALHOST, DEFAULT_RPC_PUBSUB_PORT))
⋮----
node.set_serve_repair(UDP, (Ipv4Addr::LOCALHOST, 8008))
⋮----
node.set_serve_repair(QUIC, (Ipv4Addr::LOCALHOST, 8006))
⋮----
pub fn new_with_socketaddr(pubkey: &Pubkey, socket: &SocketAddr) -> Self {
⋮----
assert_matches!(sanitize_socket(socket), Ok(()));
⋮----
let (addr, port) = (socket.ip(), socket.port());
node.set_gossip((addr, port + 1)).unwrap();
node.set_tvu(UDP, (addr, port + 2)).unwrap();
node.set_tvu(QUIC, (addr, port + 3)).unwrap();
node.set_tpu(UDP, (addr, port)).unwrap();
node.set_tpu(QUIC, (addr, port + 6)).unwrap();
node.set_tpu_forwards(UDP, (addr, port + 5)).unwrap();
node.set_tpu_forwards(QUIC, (addr, port + 11)).unwrap();
node.set_tpu_vote(UDP, (addr, port + 7)).unwrap();
node.set_tpu_vote(QUIC, (addr, port + 9)).unwrap();
node.set_rpc((addr, DEFAULT_RPC_PORT)).unwrap();
node.set_rpc_pubsub((addr, DEFAULT_RPC_PUBSUB_PORT))
⋮----
node.set_serve_repair(UDP, (addr, port + 8)).unwrap();
node.set_serve_repair(QUIC, (addr, port + 4)).unwrap();
⋮----
pub(crate) fn check_duplicate(&self, other: &ContactInfo) -> bool {
⋮----
pub(crate) fn overrides(&self, other: &ContactInfo) -> Option<bool> {
⋮----
match (self.outset, self.wallclock).cmp(&other) {
Ordering::Less => Some(false),
Ordering::Greater => Some(true),
⋮----
fn get_node_outset() -> u64 {
⋮----
let elapsed = now.duration_since(UNIX_EPOCH).unwrap();
u64::try_from(elapsed.as_micros()).unwrap()
⋮----
impl Default for ContactInfo {
fn default() -> Self {
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
ContactInfo::try_from(node).map_err(serde::de::Error::custom)
⋮----
type Error = Error;
fn try_from(node: ContactInfoLite) -> Result<Self, Self::Error> {
⋮----
sanitize_entries(&addrs, &sockets)?;
⋮----
// Populate node.cache.
// Only sanitized socket-addrs can be cached!
⋮----
let Some(entry) = node.cache.get_mut(usize::from(key)) else {
⋮----
let Some(&addr) = node.addrs.get(usize::from(index)) else {
⋮----
if sanitize_socket(&socket).is_ok() {
⋮----
Ok(node)
⋮----
impl Sanitize for ContactInfo {
fn sanitize(&self) -> Result<(), SanitizeError> {
⋮----
return Err(SanitizeError::ValueOutOfBounds);
⋮----
pub(crate) fn sanitize_socket(socket: &SocketAddr) -> Result<(), Error> {
if socket.port() == 0u16 {
return Err(Error::InvalidPort(socket.port()));
⋮----
let addr = socket.ip();
if addr.is_unspecified() {
return Err(Error::UnspecifiedIpAddr(addr));
⋮----
if addr.is_multicast() {
return Err(Error::MulticastIpAddr(addr));
⋮----
// Sanitizes deserialized IpAddr and socket entries.
fn sanitize_entries(addrs: &[IpAddr], sockets: &[SocketEntry]) -> Result<(), Error> {
// Verify that all IP addresses are unique.
⋮----
let mut seen = HashSet::with_capacity(addrs.len());
⋮----
if !seen.insert(addr) {
return Err(Error::DuplicateIpAddr(*addr));
⋮----
// Verify that all socket entries have unique key.
⋮----
let mut mask = [0u64; 4]; // 256-bit bitmask.
⋮----
return Err(Error::DuplicateSocket(key));
⋮----
// Verify that all socket entries reference a valid IP address, and
// that all IP addresses are referenced in the sockets.
⋮----
let num_addrs = addrs.len();
let mut hits = vec![false; num_addrs];
⋮----
.get_mut(usize::from(index))
.ok_or(Error::InvalidIpAddrIndex { index, num_addrs })? = true;
⋮----
if let Some(index) = hits.into_iter().position(|hit| !hit) {
return Err(Error::UnusedIpAddr(addrs[index]));
⋮----
// Verify that port offsets don't overflow.
⋮----
.iter()
.try_fold(0u16, |offset, entry| offset.checked_add(entry.offset))
.is_none()
⋮----
return Err(Error::PortOffsetsOverflow);
⋮----
fn example() -> Self {
⋮----
extensions: vec![],
⋮----
macro_rules! socketaddr {
⋮----
macro_rules! socketaddr_any {
⋮----
mod tests {
⋮----
fn new_rand_addr<R: Rng>(rng: &mut R) -> IpAddr {
if rng.gen() {
let addr = Ipv4Addr::new(rng.gen(), rng.gen(), rng.gen(), rng.gen());
⋮----
rng.gen(),
⋮----
fn new_rand_port<R: Rng>(rng: &mut R) -> u16 {
⋮----
let bits = u16::BITS - port.leading_zeros();
let shift = rng.gen_range(0u32..bits + 1u32);
port.checked_shr(shift).unwrap_or_default()
⋮----
fn new_rand_socket<R: Rng>(rng: &mut R) -> SocketAddr {
SocketAddr::new(new_rand_addr(rng), new_rand_port(rng))
⋮----
fn test_new_gossip_entry_point() {
⋮----
assert_eq!(ci.gossip().unwrap(), addr);
assert_matches!(ci.rpc(), None);
assert_matches!(ci.rpc_pubsub(), None);
assert_matches!(ci.serve_repair(Protocol::QUIC), None);
assert_matches!(ci.serve_repair(Protocol::UDP), None);
assert_matches!(ci.tpu(Protocol::QUIC), None);
assert_matches!(ci.tpu(Protocol::UDP), None);
assert_matches!(ci.tpu_forwards(Protocol::QUIC), None);
assert_matches!(ci.tpu_forwards(Protocol::UDP), None);
assert_matches!(ci.tpu_vote(Protocol::UDP), None);
assert_matches!(ci.tpu_vote(Protocol::QUIC), None);
assert_matches!(ci.tvu(Protocol::QUIC), None);
assert_matches!(ci.tvu(Protocol::UDP), None);
assert_matches!(ci.alpenglow(), None);
⋮----
fn test_sanitize_entries() {
⋮----
let addrs: Vec<IpAddr> = repeat_with(|| new_rand_addr(&mut rng)).take(5).collect();
let mut keys: Vec<u8> = (0u8..=u8::MAX).collect();
keys.shuffle(&mut rng);
⋮----
assert_matches!(
⋮----
.map(|&key| SocketEntry { key, index, offset })
.collect();
⋮----
.into_iter()
.zip(&keys)
.map(|(index, &key)| SocketEntry { key, index, offset })
⋮----
.map(|key| SocketEntry {
⋮----
.map(|&key| SocketEntry {
⋮----
index: rng.gen_range(0u8..addrs.len() as u8),
offset: rng.gen_range(0u16..u16::MAX / 64),
⋮----
offset: rng.gen_range(0u16..u16::MAX / 256),
⋮----
assert_matches!(sanitize_entries(&addrs, &sockets), Ok(()));
⋮----
fn test_round_trip() {
⋮----
let addrs: Vec<IpAddr> = repeat_with(|| new_rand_addr(&mut rng)).take(8).collect();
⋮----
wallclock: rng.gen(),
outset: rng.gen(),
shred_version: rng.gen(),
⋮----
let addr = addrs.choose(&mut rng).unwrap();
let socket = SocketAddr::new(*addr, new_rand_port(&mut rng));
let key = rng.gen_range(KEYS.start..KEYS.end);
⋮----
sockets.insert(key, socket);
assert_matches!(node.set_socket(key, socket), Ok(()));
assert_matches!(sanitize_entries(&node.addrs, &node.sockets), Ok(()));
⋮----
assert_matches!(node.set_socket(key, socket), Err(_));
⋮----
for key in KEYS.clone() {
let socket = sockets.get(&key);
assert_eq!(node.get_socket(key).ok().as_ref(), socket);
⋮----
assert_eq!(
⋮----
assert_eq!(node.gossip().as_ref(), sockets.get(&SOCKET_TAG_GOSSIP));
assert_eq!(node.rpc().as_ref(), sockets.get(&SOCKET_TAG_RPC));
⋮----
assert!(node
⋮----
assert!(u16::try_from(
⋮----
let bytes = bincode::serialize(&node).unwrap();
let other: ContactInfo = bincode::deserialize(&bytes).unwrap();
assert_eq!(node, other);
⋮----
fn test_set_and_remove_alpenglow() {
⋮----
Keypair::new().pubkey(),
⋮----
let socket = repeat_with(|| new_rand_socket(&mut rng))
.find(|socket| matches!(sanitize_socket(socket), Ok(())))
⋮----
node.set_alpenglow(socket).unwrap();
assert_eq!(node.alpenglow().unwrap(), socket);
node.remove_alpenglow();
assert_matches!(node.alpenglow(), None);
⋮----
fn test_check_duplicate() {
⋮----
let other = node.clone();
assert!(!node.check_duplicate(&other));
assert!(!other.check_duplicate(&node));
assert_eq!(node.overrides(&other), None);
assert_eq!(other.overrides(&node), None);
⋮----
let mut other = node.clone();
while other.set_gossip(new_rand_socket(&mut rng)).is_err() {}
⋮----
.set_serve_repair(Protocol::UDP, new_rand_socket(&mut rng))
.is_err()
⋮----
other.remove_serve_repair();
⋮----
node.set_wallclock(rng.gen());
⋮----
node.hot_swap_pubkey(*other.pubkey());
assert!(node.outset > other.outset);
⋮----
assert!(other.check_duplicate(&node));
assert_eq!(node.overrides(&other), Some(true));
assert_eq!(other.overrides(&node), Some(false));
⋮----
assert!(node.outset < other.outset);
assert!(node.check_duplicate(&other));
⋮----
assert_eq!(node.overrides(&other), Some(false));
assert_eq!(other.overrides(&node), Some(true));
node.set_wallclock(other.wallclock);

================
File: gossip/src/crds_data.rs
================
pub(crate) type VoteIndex = u8;
⋮----
pub(crate) type EpochSlotsIndex = u8;
⋮----
pub enum CrdsData {
⋮----
impl Sanitize for CrdsData {
fn sanitize(&self) -> Result<(), SanitizeError> {
⋮----
CrdsData::LegacyContactInfo(val) => val.sanitize(),
⋮----
return Err(SanitizeError::ValueOutOfBounds);
⋮----
val.sanitize()
⋮----
CrdsData::LegacySnapshotHashes(val) => val.sanitize(),
CrdsData::AccountsHashes(val) => val.sanitize(),
⋮----
CrdsData::LegacyVersion(version) => version.sanitize(),
CrdsData::Version(version) => version.sanitize(),
CrdsData::NodeInstance(node) => node.sanitize(),
⋮----
Err(SanitizeError::ValueOutOfBounds)
⋮----
shred.sanitize()
⋮----
CrdsData::SnapshotHashes(val) => val.sanitize(),
CrdsData::ContactInfo(node) => node.sanitize(),
CrdsData::RestartLastVotedForkSlots(slots) => slots.sanitize(),
CrdsData::RestartHeaviestFork(fork) => fork.sanitize(),
⋮----
pub(crate) fn new_rand_timestamp<R: Rng>(rng: &mut R) -> u64 {
⋮----
timestamp() - DELAY + rng.gen_range(0..2 * DELAY)
⋮----
impl CrdsData {
pub(crate) fn new_rand<R: Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> CrdsData {
let kind = rng.gen_range(0..8);
⋮----
4 => CrdsData::Vote(rng.gen_range(0..MAX_VOTES), Vote::new_rand(rng, pubkey)),
⋮----
rng.gen_range(0..MAX_EPOCH_SLOTS),
⋮----
pub(crate) fn wallclock(&self) -> u64 {
⋮----
CrdsData::LegacyContactInfo(contact_info) => contact_info.wallclock(),
⋮----
CrdsData::ContactInfo(node) => node.wallclock(),
⋮----
pub(crate) fn pubkey(&self) -> Pubkey {
⋮----
CrdsData::LegacyContactInfo(contact_info) => *contact_info.pubkey(),
⋮----
CrdsData::ContactInfo(node) => *node.pubkey(),
⋮----
pub(crate) fn is_deprecated(&self) -> bool {
⋮----
fn from(node: ContactInfo) -> Self {
⋮----
fn from(node: &ContactInfo) -> Self {
Self::ContactInfo(node.clone())
⋮----
pub(crate) struct AccountsHashes {
⋮----
reject_deserialize!(AccountsHashes, "AccountsHashes is deprecated");
impl Sanitize for AccountsHashes {
⋮----
sanitize_wallclock(self.wallclock)?;
⋮----
self.from.sanitize()
⋮----
impl AccountsHashes {
pub(crate) fn new_rand<R: Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
let num_hashes = rng.gen_range(0..MAX_ACCOUNTS_HASHES) + 1;
⋮----
let slot = 47825632 + rng.gen_range(0..512);
⋮----
.take(num_hashes)
.collect();
⋮----
from: pubkey.unwrap_or_else(solana_pubkey::new_rand),
⋮----
wallclock: new_rand_timestamp(rng),
⋮----
type LegacySnapshotHashes = AccountsHashes;
⋮----
pub struct SnapshotHashes {
⋮----
impl Sanitize for SnapshotHashes {
⋮----
return Err(SanitizeError::InvalidValue);
⋮----
pub struct LowestSlot {
⋮----
impl LowestSlot {
pub(crate) fn new(from: Pubkey, lowest: Slot, wallclock: u64) -> Self {
⋮----
stash: vec![],
⋮----
fn new_rand<R: Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
⋮----
root: rng.gen(),
lowest: rng.gen(),
⋮----
impl Sanitize for LowestSlot {
⋮----
if !self.slots.is_empty() {
⋮----
if !self.stash.is_empty() {
⋮----
fn reject_nonzero_u8<'de, D>(de: D) -> Result<u8, D::Error>
⋮----
Ok(v)
⋮----
Err(serde::de::Error::custom(
⋮----
pub struct Vote {
⋮----
impl Sanitize for Vote {
⋮----
self.from.sanitize()?;
self.transaction.sanitize()
⋮----
impl Vote {
pub fn new(from: Pubkey, transaction: Transaction, wallclock: u64) -> Option<Self> {
vote_parser::parse_vote_transaction(&transaction).map(|(_, vote, ..)| Self {
⋮----
slot: vote.last_voted_slot(),
⋮----
pub(crate) fn transaction(&self) -> &Transaction {
⋮----
pub(crate) fn slot(&self) -> Option<Slot> {
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
struct Vote {
⋮----
.sanitize()
.map_err(serde::de::Error::custom)?;
⋮----
.ok_or_else(|| serde::de::Error::custom("invalid vote tx"))
⋮----
pub(crate) struct LegacyVersion {
⋮----
reject_deserialize!(LegacyVersion, "LegacyVersion is deprecated");
impl Sanitize for LegacyVersion {
⋮----
self.version.sanitize()
⋮----
pub(crate) struct Version {
⋮----
reject_deserialize!(Version, "Version is deprecated");
impl Sanitize for Version {
⋮----
pub(crate) struct NodeInstance {
⋮----
timestamp: u64, // Timestamp when the instance was created.
token: u64,     // Randomly generated value at node instantiation.
⋮----
reject_deserialize!(NodeInstance, "NodeInstance is deprecated");
impl Sanitize for NodeInstance {
⋮----
pub(crate) fn sanitize_wallclock(wallclock: u64) -> Result<(), SanitizeError> {
⋮----
Ok(())
⋮----
macro_rules! reject_deserialize {
⋮----
pub(crate) use reject_deserialize;
⋮----
mod test {
⋮----
fn test_lowest_slot_sanitize() {
⋮----
let v = CrdsValue::new_unsigned(CrdsData::LowestSlot(0, ls.clone()));
assert_eq!(v.sanitize(), Ok(()));
let mut o = ls.clone();
⋮----
assert_eq!(v.sanitize(), Err(SanitizeError::InvalidValue));
let o = ls.clone();
⋮----
assert_eq!(v.sanitize(), Err(SanitizeError::ValueOutOfBounds));
⋮----
o.slots.insert(1);
⋮----
o.stash.push(deprecated::EpochIncompleteSlots::default());
⋮----
fn test_max_vote_index() {
⋮----
let vote = Vote::new(keypair.pubkey(), new_test_vote_tx(&mut rng), timestamp()).unwrap();
⋮----
assert!(vote.sanitize().is_err());
⋮----
fn test_vote_round_trip() {
⋮----
vec![1, 3, 7],
⋮----
Some(&Pubkey::new_unique()),
⋮----
rng.gen(),
⋮----
.unwrap();
assert_eq!(vote.slot, Some(7));
let bytes = bincode::serialize(&vote).unwrap();
let other = bincode::deserialize(&bytes[..]).unwrap();
assert_eq!(vote, other);
assert_eq!(other.slot, Some(7));
let bytes = bincode::options().serialize(&vote).unwrap();
let other = bincode::options().deserialize(&bytes[..]).unwrap();
⋮----
fn test_max_epoch_slots_index() {
⋮----
EpochSlots::new(keypair.pubkey(), timestamp()),
⋮----
assert_eq!(item.sanitize(), Err(SanitizeError::ValueOutOfBounds));
⋮----
fn test_deprecated_values_fail_deserialization() {
⋮----
from: keypair.pubkey(),
wallclock: timestamp(),
⋮----
let bytes = bincode::serialize(&node_instance).unwrap();
assert!(bincode::deserialize::<CrdsData>(&bytes[..]).is_err());
⋮----
struct LegacyVersion1Mirror {
⋮----
bincode::deserialize(&bytes).unwrap()
⋮----
let bytes = bincode::serialize(&legacy_version).unwrap();
⋮----
let bytes = bincode::serialize(&version).unwrap();
⋮----
let bytes = bincode::serialize(&legacy_contact_info).unwrap();
⋮----
CrdsData::AccountsHashes(AccountsHashes::new_rand(&mut rng, Some(keypair.pubkey())));
let bytes = bincode::serialize(&accounts_hashes).unwrap();
⋮----
LegacySnapshotHashes::new_rand(&mut rng, Some(keypair.pubkey())),
⋮----
let bytes = bincode::serialize(&legacy_snapshot_hashes).unwrap();
⋮----
CrdsData::LowestSlot(1, LowestSlot::new(keypair.pubkey(), 0, timestamp()));
let bytes = bincode::serialize(&lowest_slot).unwrap();
⋮----
CrdsData::LowestSlot(0, LowestSlot::new(keypair.pubkey(), 0, timestamp()));
⋮----
assert!(bincode::deserialize::<CrdsData>(&bytes[..]).is_ok());

================
File: gossip/src/crds_entry.rs
================
type CrdsTable = IndexMap<CrdsValueLabel, VersionedCrdsValue>;
pub trait CrdsEntry<'a, 'b>: Sized {
⋮----
macro_rules! impl_crds_entry (
// Lookup by CrdsValueLabel.
⋮----
impl_crds_entry!(CrdsData, |entry| Some(entry?.value.data()));
impl_crds_entry!(CrdsValue, |entry| Some(&entry?.value));
impl_crds_entry!(VersionedCrdsValue, |entry| entry);
impl_crds_entry!(ContactInfo, CrdsData::ContactInfo(node), node);
impl_crds_entry!(LowestSlot, CrdsData::LowestSlot(_, slot), slot);
impl_crds_entry!(
⋮----
mod tests {
⋮----
fn test_get_crds_entry() {
⋮----
let keypairs: Vec<_> = std::iter::repeat_with(Keypair::new).take(32).collect();
⋮----
let keypair = keypairs.choose(&mut rng).unwrap();
let value = CrdsValue::new_rand(&mut rng, Some(keypair));
let key = value.label();
if let Ok(()) = crds.insert(
value.clone(),
new_rand_timestamp(&mut rng),
⋮----
entries.insert(key, value);
⋮----
assert!(crds.len() > 64);
assert_eq!(crds.len(), entries.len());
for entry in entries.values() {
let key = entry.label();
assert_eq!(crds.get::<&CrdsValue>(&key), Some(entry));
assert_eq!(crds.get::<&CrdsData>(&key), Some(entry.data()));
assert_eq!(crds.get::<&VersionedCrdsValue>(&key).unwrap().value, *entry);
let key = entry.pubkey();
match entry.data() {
⋮----
assert_eq!(crds.get::<&ContactInfo>(key), Some(node))
⋮----
assert_eq!(crds.get::<&LowestSlot>(key), Some(slot))
⋮----
assert_eq!(crds.get::<&SnapshotHashes>(key), Some(hash))

================
File: gossip/src/crds_filter.rs
================
pub(crate) enum GossipFilterDirection {
⋮----
pub(crate) fn should_retain_crds_value(
⋮----
stakes.len() < MIN_NUM_STAKED_NODES || {
let stake = stakes.get(&value.pubkey()).copied();
stake.unwrap_or_default() >= MIN_STAKE_FOR_GOSSIP
⋮----
match value.data() {
⋮----
| CrdsData::RestartLastVotedForkSlots(_) => retain_if_staked(),
⋮----
EgressPush | EgressPullResponse => retain_if_staked(),
⋮----
EgressPullResponse => retain_if_staked(),

================
File: gossip/src/crds_gossip_error.rs
================
pub enum CrdsGossipError {

================
File: gossip/src/crds_gossip_pull.rs
================
pub struct CrdsFilter {
⋮----
pub struct PullRequest {
⋮----
impl Default for CrdsFilter {
fn default() -> Self {
⋮----
fn sanitize(&self) -> std::result::Result<(), solana_sanitize::SanitizeError> {
self.filter.sanitize()?;
Ok(())
⋮----
impl CrdsFilter {
⋮----
pub(crate) fn new_rand(num_items: usize, max_bytes: usize) -> Self {
⋮----
let seed: u64 = rand::thread_rng().gen_range(0..2u64.pow(mask_bits));
⋮----
fn compute_mask(seed: u64, mask_bits: u32) -> u64 {
assert!(seed <= 2u64.pow(mask_bits));
let seed: u64 = seed.checked_shl(64 - mask_bits).unwrap_or(0x0);
seed | (!0u64).checked_shr(mask_bits).unwrap_or(!0x0)
⋮----
fn max_items(max_bits: f64, false_rate: f64, num_keys: f64) -> f64 {
⋮----
(m / (-k / (1f64 - (p.ln() / k).exp()).ln())).ceil()
⋮----
fn mask_bits(num_items: f64, max_items: f64) -> u32 {
((num_items / max_items).log2().ceil()).max(0.0) as u32
⋮----
pub fn hash_as_u64(item: &Hash) -> u64 {
let buf = item.as_ref()[..8].try_into().unwrap();
⋮----
fn test_mask(&self, item: &Hash) -> bool {
let ones = (!0u64).checked_shr(self.mask_bits).unwrap_or(!0u64);
⋮----
fn add(&mut self, item: &Hash) {
if self.test_mask(item) {
self.filter.add(item);
⋮----
fn contains(&self, item: &Hash) -> bool {
if !self.test_mask(item) {
⋮----
self.filter.contains(item)
⋮----
fn filter_contains(&self, item: &Hash) -> bool {
⋮----
struct CrdsFilterSet {
⋮----
impl CrdsFilterSet {
fn new<R: Rng>(rng: &mut R, num_items: usize, max_bytes: usize) -> Self {
⋮----
let mut filters: Vec<_> = repeat_with(|| None).take(1usize << mask_bits).collect();
let mut indices: Vec<_> = (0..filters.len()).collect();
let size = filters.len().div_ceil(SAMPLE_RATE);
for _ in 0..MAX_NUM_FILTERS.min(size) {
let k = rng.gen_range(0..indices.len());
let k = indices.swap_remove(k);
⋮----
filters[k] = Some(ConcurrentBloom::<Hash>::from(filter));
⋮----
fn add(&self, hash_value: Hash) {
let shift = u64::BITS.checked_sub(self.mask_bits).unwrap();
⋮----
.checked_shr(shift)
.unwrap_or_default(),
⋮----
.unwrap();
⋮----
filter.add(&hash_value);
⋮----
fn from(cfs: CrdsFilterSet) -> Self {
⋮----
.into_iter()
.enumerate()
.filter_map(|(seed, filter)| {
Some(CrdsFilter {
⋮----
.collect()
⋮----
pub struct ProcessPullStats {
⋮----
pub struct CrdsGossipPull {
⋮----
impl Default for CrdsGossipPull {
⋮----
impl CrdsGossipPull {
⋮----
pub(crate) fn new_pull_request(
⋮----
&self_keypair.pubkey(),
⋮----
.get(&self_keypair.pubkey())
.copied()
.unwrap_or_default();
⋮----
.into_values()
.map(|(stake, node)| {
let stake = stake.min(stake_cap) / LAMPORTS_PER_SOL;
let weight = u64::BITS - stake.leading_zeros();
let weight = u64::from(weight).saturating_add(1).saturating_pow(2);
⋮----
.unzip();
if nodes.is_empty() {
return Err(CrdsGossipError::NoPeers);
⋮----
let filters = self.build_crds_filters(thread_pool, crds, bloom_size);
let dist = WeightedIndex::new(weights).unwrap();
Ok(filters.into_iter().filter_map(move |filter| {
let node = &nodes[dist.sample(&mut rng)];
Some((node.gossip()?, filter))
⋮----
pub(crate) fn generate_pull_responses(
⋮----
pub(crate) fn filter_pull_responses(
⋮----
let mut active_values = vec![];
let mut expired_values = vec![];
let crds = crds.read().unwrap();
⋮----
let owner = response.label().pubkey();
⋮----
if !crds.upserts(&response) {
Some(response)
} else if now <= response.wallclock().saturating_add(timeout) {
active_values.push(response);
⋮----
} else if crds.get::<&ContactInfo>(owner).is_some() {
expired_values.push(response);
⋮----
.filter_map(upsert)
.map(|resp| *resp.hash())
.collect();
⋮----
pub(crate) fn process_pull_responses(
⋮----
let mut crds = crds.write().unwrap();
⋮----
let _ = crds.insert(response, now, GossipRoute::PullResponse);
⋮----
let owner = response.pubkey();
if let Ok(()) = crds.insert(response, now, GossipRoute::PullResponse) {
⋮----
owners.insert(owner);
⋮----
self.num_pulls.fetch_add(num_inserts, Ordering::Relaxed);
⋮----
crds.update_record_timestamp(&owner, now);
⋮----
drop(crds);
stats.failed_insert += failed_inserts.len();
self.purge_failed_inserts(now);
let failed_inserts = failed_inserts.into_iter().zip(repeat(now));
self.failed_inserts.write().unwrap().extend(failed_inserts);
⋮----
pub(crate) fn purge_failed_inserts(&self, now: u64) {
⋮----
let mut failed_inserts = self.failed_inserts.write().unwrap();
⋮----
.iter()
.take_while(|(_, ts)| *ts < cutoff)
.count();
failed_inserts.drain(..outdated);
⋮----
pub(crate) fn failed_inserts_size(&self) -> usize {
self.failed_inserts.read().unwrap().len()
⋮----
pub fn build_crds_filters(
⋮----
let failed_inserts = self.failed_inserts.read().unwrap();
⋮----
let num_items = crds.len() + crds.num_purged() + failed_inserts.len();
let num_items = MIN_NUM_BLOOM_ITEMS.max(num_items);
⋮----
thread_pool.install(|| {
crds.par_values()
.with_min_len(PAR_MIN_LENGTH)
.map(|v| *v.value.hash())
.chain(crds.purged().with_min_len(PAR_MIN_LENGTH))
.chain(
⋮----
.par_iter()
⋮----
.map(|(v, _)| *v),
⋮----
.for_each(|v| filters.add(v));
⋮----
drop(failed_inserts);
filters.into()
⋮----
fn filter_crds_values(
⋮----
let jitter = rand::thread_rng().gen_range(0..msg_timeout / 4);
⋮----
now.saturating_sub(msg_timeout)..now.saturating_add(msg_timeout);
⋮----
let output_size_limit = output_size_limit.try_into().unwrap_or(i64::MAX);
⋮----
if output_size_limit.load(Ordering::Relaxed) <= 0 {
⋮----
if !caller_wallclock_window.contains(&caller_wallclock) {
dropped_requests.fetch_add(1, Ordering::Relaxed);
⋮----
let caller_wallclock = caller_wallclock.checked_add(jitter).unwrap_or(0);
⋮----
debug_assert!(filter.test_mask(entry.value.hash()));
if entry.value.wallclock() > caller_wallclock {
total_skipped.fetch_add(1, Ordering::Relaxed);
⋮----
!filter.filter_contains(entry.value.hash())
&& should_retain_crds_value(&entry.value)
⋮----
.filter_bitmask(filter.mask, filter.mask_bits, self_shred_version)
.filter(pred)
.map(|entry| entry.value.clone())
.take(output_size_limit.load(Ordering::Relaxed).max(0) as usize)
⋮----
output_size_limit.fetch_sub(out.len() as i64, Ordering::Relaxed);
⋮----
let ret: Vec<_> = thread_pool.install(|| requests.par_iter().map(apply_filter).collect());
⋮----
.add_relaxed(dropped_requests.into_inner() as u64);
⋮----
.add_relaxed(total_skipped.into_inner() as u64);
⋮----
pub(crate) fn make_timeouts<'a>(
⋮----
/// Purge values from the crds that are older then `active_timeout`
    pub(crate) fn purge_active(
⋮----
pub(crate) fn purge_active(
⋮----
let labels = crds.find_old_labels(thread_pool, now, timeouts);
⋮----
crds.remove(label, now);
⋮----
labels.len()
⋮----
/// For legacy tests
    #[cfg(test)]
fn process_pull_response(
⋮----
self.filter_pull_responses(crds, timeouts, response, now, &mut stats);
self.process_pull_responses(
⋮----
pub struct CrdsTimeouts<'a> {
⋮----
stakes: &'a HashMap<Pubkey, /*lamports:*/ u64>,
⋮----
pub fn new(
⋮----
let extended_timeout = default_timeout.max(epoch_duration.as_millis() as u64);
let default_timeout = if stakes.values().all(|&stake| stake == 0u64) {
⋮----
type Output = u64;
fn index(&self, pubkey: &Pubkey) -> &Self::Output {
⋮----
} else if self.stakes.get(pubkey) > Some(&0u64) {
⋮----
// Returns max_bytes for the bloom filter such that bincode serialized
// Protocol::PullRequest(CrdsFilter, CrdsValue) fits in a packet.
pub(crate) fn get_max_bloom_filter_bytes(caller: &CrdsValue) -> usize {
// Maps serialized size of CrdsFilter to max_bytes of bloom filter.
⋮----
let filters = CrdsFilterSet::new(&mut rng, /*num_items:*/ 1, max_bytes);
⋮----
.map(|filter| {
⋮----
.map(usize::try_from)
.unwrap()
⋮----
.dedup();
let size_of_filter = iter.next().unwrap();
// All filters should have the same serialzied size.
assert_eq!(iter.next(), None);
out[size_of_filter] = u16::try_from(max_bytes).unwrap();
⋮----
// Forward fill zero entries by assigning the last non-zero value.
let _ = out.iter_mut().fold(0, |state, entry| {
⋮----
// Maximum bincode serialized size of CrdsFilter in
// Protocol::PullRequest(CrdsFilter, CrdsValue)
⋮----
.checked_sub(
// 4 bytes for u32 enum variant identifier of Protocol.
4 + caller.bincode_serialized_size(),
⋮----
.get(size_of_filter)
⋮----
.map(usize::from)
⋮----
pub(crate) mod tests {
⋮----
// Wrapper for CrdsGossipPush.new_pull_request replicating old return
// type for legacy tests.
⋮----
fn old_pull_request(
⋮----
let out = self.new_pull_request(
⋮----
.read()
⋮----
.get_nodes_contact_info()
.map(|node| (node.gossip().unwrap(), node.clone()))
⋮----
Ok(out
.into_group_map()
⋮----
.map(|(addr, filters)| (nodes.get(&addr).cloned().unwrap(), filters))
.collect())
⋮----
fn new_ping_cache() -> PingCache {
⋮----
Duration::from_secs(20 * 60),      // ttl
Duration::from_secs(20 * 60) / 64, // rate_limit_delay
128,                               // capacity
⋮----
fn test_hash_as_u64() {
⋮----
assert_eq!(CrdsFilter::hash_as_u64(&hash), 0x807060504030201);
⋮----
fn test_hash_as_u64_random() {
fn hash_as_u64_bitops(hash: &Hash) -> u64 {
⋮----
for (i, val) in hash.as_ref().iter().enumerate().take(8) {
⋮----
assert_eq!(CrdsFilter::hash_as_u64(&hash), hash_as_u64_bitops(&hash));
⋮----
fn test_crds_filter_default() {
⋮----
assert_eq!(filter.mask, mask);
⋮----
assert!(filter.test_mask(&hash));
⋮----
fn test_crds_filter_set_add() {
⋮----
&mut rng, /*num_items=*/ 59672788, /*max_bytes=*/ 8196,
⋮----
let hash_values: Vec<_> = repeat_with(|| {
let buf: [u8; 32] = rng.gen();
⋮----
.take(1024)
⋮----
assert_eq!(crds_filter_set.filters.len(), 8192);
⋮----
crds_filter_set.add(*hash_value);
⋮----
let filters: Vec<CrdsFilter> = crds_filter_set.into();
⋮----
assert_eq!(filters.len(), 1024);
⋮----
if filter.test_mask(&hash_value) {
⋮----
assert!(!hit);
⋮----
assert!(filter.contains(&hash_value));
assert!(filter.filter.contains(&hash_value));
} else if filter.filter.contains(&hash_value) {
⋮----
assert!(false_positives < 5);
⋮----
assert!(num_hits > 96, "num_hits: {num_hits}");
⋮----
fn test_crds_filter_set_new() {
// Validates invariances required by CrdsFilterSet::get in the
// vector of filters generated by CrdsFilterSet::new.
⋮----
55345017, // num_items
4098,     // max_bytes
⋮----
assert_eq!(filters.filters.len(), 16384);
⋮----
// Check that all mask_bits are equal.
assert_eq!(mask_bits, filter.mask_bits);
assert!((0..16384).contains(&(filter.mask >> right_shift)));
assert_eq!(ones, ones & filter.mask);
⋮----
fn test_build_crds_filter() {
⋮----
let thread_pool = ThreadPoolBuilder::new().build().unwrap();
⋮----
let keypairs: Vec<_> = repeat_with(|| {
⋮----
rng.fill(&mut seed[..]);
keypair_from_seed(&seed).unwrap()
⋮----
.take(10_000)
⋮----
let keypair = keypairs.choose(&mut rng).unwrap();
let value = CrdsValue::new_rand(&mut rng, Some(keypair));
⋮----
.insert(value, rng.gen(), GossipRoute::LocalMessage)
.is_ok()
⋮----
assert!(num_inserts > 30_000, "num inserts: {num_inserts}");
let filters = crds_gossip_pull.build_crds_filters(
⋮----
992, // max_bloom_filter_bytes
⋮----
assert_eq!(filters.len(), MIN_NUM_BLOOM_FILTERS.max(4));
⋮----
let purged: Vec<_> = thread_pool.install(|| crds.purged().collect());
⋮----
.values()
⋮----
.chain(purged)
⋮----
// CrdsValue::new_rand may generate exact same value twice in which
// case its hash-value is not added to purged values.
assert!(
⋮----
assert!(num_hits > 4000, "num_hits: {num_hits}");
assert!(false_positives < 20_000, "fp: {false_positives}");
⋮----
fn test_new_pull_request() {
⋮----
&node_keypair.pubkey(),
⋮----
let ping_cache = Mutex::new(new_ping_cache());
assert_eq!(
⋮----
crds.write()
⋮----
.insert(entry, 0, GossipRoute::LocalMessage)
⋮----
new.set_gossip(([127, 0, 0, 1], 8020)).unwrap();
⋮----
.lock()
⋮----
.mock_pong(*new.pubkey(), new.gossip().unwrap(), Instant::now());
⋮----
.insert(new.clone(), now, GossipRoute::LocalMessage)
⋮----
let req = node.old_pull_request(
⋮----
let peers: Vec<_> = req.unwrap().into_iter().map(|(node, _)| node).collect();
assert_eq!(peers, vec![new.contact_info().unwrap().clone()]);
⋮----
offline.set_gossip(([127, 0, 0, 1], 8021)).unwrap();
⋮----
.insert(offline, now, GossipRoute::LocalMessage)
⋮----
// Even though the offline node should have higher weight, we shouldn't request from it
⋮----
fn test_new_mark_creation_time() {
⋮----
let mut ping_cache = new_ping_cache();
⋮----
crds.insert(entry, now, GossipRoute::LocalMessage).unwrap();
⋮----
old.set_gossip(([127, 0, 0, 1], 8020)).unwrap();
ping_cache.mock_pong(*old.pubkey(), old.gossip().unwrap(), Instant::now());
⋮----
crds.insert(old.clone(), now, GossipRoute::LocalMessage)
⋮----
new.set_gossip(([127, 0, 0, 1], 8021)).unwrap();
ping_cache.mock_pong(*new.pubkey(), new.gossip().unwrap(), Instant::now());
⋮----
crds.insert(new, now, GossipRoute::LocalMessage).unwrap();
⋮----
let old = old.contact_info().unwrap();
let count = repeat_with(|| {
⋮----
.old_pull_request(
⋮----
requests.into_iter().map(|(node, _)| node)
⋮----
.flatten()
.take(100)
.filter(|peer| peer != old)
⋮----
assert!(count < 75, "count of peer != old: {count}");
⋮----
fn test_generate_pull_responses() {
⋮----
let now = timestamp();
⋮----
let caller = entry.clone();
⋮----
.insert(entry, now, GossipRoute::LocalMessage)
⋮----
.insert(new, now, GossipRoute::LocalMessage)
⋮----
let filters = req.unwrap().into_iter().flat_map(|(_, filters)| filters);
⋮----
.map(|filter| PullRequest {
pubkey: caller.pubkey(),
⋮----
wallclock: caller.wallclock(),
⋮----
assert_eq!(rsp[0].len(), 0);
⋮----
.write()
⋮----
assert_eq!(requests.len(), MIN_NUM_BLOOM_FILTERS);
requests.extend({
⋮----
.map(|PullRequest { filter, .. }| PullRequest {
⋮----
filter: filter.clone(),
⋮----
assert_eq!(rsp.len(), 2 * MIN_NUM_BLOOM_FILTERS);
assert!(rsp.iter().take(MIN_NUM_BLOOM_FILTERS).all(|r| r.is_empty()));
assert_eq!(rsp.iter().filter(|r| r.is_empty()).count(), rsp.len() - 1);
assert_eq!(rsp.iter().find(|r| r.len() == 1).unwrap().len(), 1);
⋮----
fn test_process_pull_request_response() {
⋮----
let node_pubkey = entry.label().pubkey();
⋮----
node_crds.insert(new, 0, GossipRoute::LocalMessage).unwrap();
⋮----
.insert(new.clone(), 0, GossipRoute::LocalMessage)
⋮----
ping_cache.mock_pong(
*same_key.pubkey(),
same_key.gossip().unwrap(),
⋮----
assert_eq!(same_key.label(), new.label());
assert!(same_key.wallclock() < new.wallclock());
⋮----
.insert(same_key.clone(), 0, GossipRoute::LocalMessage)
⋮----
assert_eq!(0, {
⋮----
if rsp.is_empty() {
⋮----
assert_eq!(rsp.len(), MIN_NUM_BLOOM_FILTERS);
⋮----
.process_pull_response(
⋮----
&node.make_timeouts(node_pubkey, &HashMap::new(), Duration::default()),
rsp.into_iter().flatten().collect(),
⋮----
assert_eq!(failed, 0);
assert_eq!(1, {
⋮----
assert!(done);
⋮----
fn test_gossip_purge() {
⋮----
let node_label = entry.label();
let node_pubkey = node_label.pubkey();
⋮----
.insert(old.clone(), 0, GossipRoute::LocalMessage)
⋮----
let value_hash = *node_crds.get::<&CrdsValue>(&old.label()).unwrap().hash();
⋮----
let timeouts = node.make_timeouts(node_pubkey, &stakes, Duration::default());
⋮----
assert_eq!(node_label, {
⋮----
assert_eq!(node_crds.read().unwrap().num_purged(), 1);
⋮----
let filters = node.build_crds_filters(&thread_pool, &node_crds, PACKET_DATA_SIZE);
assert!(filters.iter().any(|filter| filter.contains(&value_hash)));
⋮----
let mut node_crds = node_crds.write().unwrap();
node_crds.trim_purged(node.crds_timeout + 1);
assert_eq!(node_crds.num_purged(), 0);
⋮----
fn test_crds_filter_mask() {
⋮----
assert_eq!(filter.mask, !0x0);
assert_eq!(CrdsFilter::max_items(80f64, 0.01, 8f64), 9f64);
assert_eq!(CrdsFilter::mask_bits(1000f64, 9f64), 7u32);
⋮----
assert_eq!(filter.mask & 0x00_ffff_ffff, 0x00_ffff_ffff);
⋮----
fn test_crds_filter_add_no_mask() {
⋮----
let h: Hash = hash(Hash::default().as_ref());
assert!(!filter.contains(&h));
filter.add(&h);
assert!(filter.contains(&h));
let h: Hash = hash(h.as_ref());
⋮----
fn test_crds_filter_add_mask() {
⋮----
while !filter.test_mask(&h) {
h = hash(h.as_ref());
⋮----
assert!(filter.test_mask(&h));
⋮----
fn test_crds_filter_complete_set_add_mask() {
⋮----
assert!(filters.iter().all(|f| f.mask_bits > 0));
⋮----
while !filters.iter().rev().any(|f| f.test_mask(&h)) {
⋮----
let filter = filters.iter_mut().find(|f| f.test_mask(&h)).unwrap();
⋮----
fn test_crds_filter_contains_mask() {
⋮----
assert!(filter.mask_bits > 0);
⋮----
while filter.test_mask(&h) {
⋮----
assert!(!filter.test_mask(&h));
⋮----
fn test_mask() {
⋮----
run_test_mask(i);
⋮----
fn run_test_mask(mask_bits: u32) {
⋮----
fn test_process_pull_response() {
⋮----
let peer_vote = Vote::new(peer_pubkey, new_test_vote_tx(&mut rng), 0).unwrap();
⋮----
fn verify_get_max_bloom_filter_bytes<R: Rng>(
⋮----
let max_bytes = get_max_bloom_filter_bytes(caller);
⋮----
let request_bytes = caller.bincode_serialized_size() as u64;
⋮----
let request_bytes = 4 + request_bytes + bincode::serialized_size(&filter).unwrap();
let request = Protocol::PullRequest(filter, caller.clone());
let request = bincode::serialize(&request).unwrap();
assert!(packet_data_size_range.contains(&request.len()));
assert_eq!(request.len() as u64, request_bytes);
⋮----
fn test_get_max_bloom_filter_bytes(num_items: usize) {
⋮----
ContactInfo::new_localhost(&keypair.pubkey(),  timestamp());
node.set_shred_version(rng.gen());
⋮----
assert_eq!(get_max_bloom_filter_bytes(&caller), 1175);
verify_get_max_bloom_filter_bytes(&mut rng, &caller, num_items);
⋮----
let mut node = ContactInfo::new_with_socketaddr(&keypair.pubkey(), &socket);
⋮----
assert_eq!(get_max_bloom_filter_bytes(&caller), 1155);

================
File: gossip/src/crds_gossip_push.rs
================
pub struct CrdsGossipPush {
⋮----
impl Default for CrdsGossipPush {
fn default() -> Self {
⋮----
impl CrdsGossipPush {
pub fn num_pending(&self, crds: &RwLock<Crds>) -> usize {
let mut cursor: Cursor = *self.crds_cursor.lock().unwrap();
crds.read().unwrap().get_entries(&mut cursor).count()
⋮----
pub(crate) fn prune_received_cache<I>(
⋮----
let mut received_cache = self.received_cache.lock().unwrap();
⋮----
.into_iter()
.flat_map(|origin| {
⋮----
.prune(
⋮----
.zip(repeat(origin))
⋮----
.into_group_map()
⋮----
fn wallclock_window(&self, now: u64) -> impl RangeBounds<u64> + use<> {
now.saturating_sub(self.msg_timeout)..=now.saturating_add(self.msg_timeout)
⋮----
pub(crate) fn process_push_message(
⋮----
let mut crds = crds.write().unwrap();
let wallclock_window = self.wallclock_window(now);
⋮----
self.num_total.fetch_add(values.len(), Ordering::Relaxed);
⋮----
if !wallclock_window.contains(&value.wallclock()) {
⋮----
let origin = value.pubkey();
match crds.insert(value, now, GossipRoute::PushMessage(&from)) {
⋮----
received_cache.record(origin, from,  0);
origins.insert(origin);
⋮----
received_cache.record(origin, from, usize::from(num_dups));
self.num_old.fetch_add(1, Ordering::Relaxed);
⋮----
received_cache.record(origin, from,  usize::MAX);
⋮----
pub(crate) fn new_push_messages(
⋮----
let active_set = self.active_set.read().unwrap();
let mut crds_cursor = self.crds_cursor.lock().unwrap();
let crds = crds.read().unwrap();
⋮----
.get_entries(crds_cursor.deref_mut())
.map(|entry| &entry.value)
.filter(|value| wallclock_window.contains(&value.wallclock()))
.filter(|value| should_retain_crds_value(value));
⋮----
.get_nodes(pubkey, &origin, stakes)
.take(self.push_fanout)
.peekable();
let index = values.len();
if nodes.peek().is_some() {
values.push(value.clone())
⋮----
should_report_message_signature(value.signature(), SIGNATURE_SAMPLE_LEADING_ZEROS);
⋮----
log_gossip_crds_sample_egress(value, &node);
⋮----
push_messages.entry(node).or_default().push(index);
⋮----
drop(crds);
drop(crds_cursor);
drop(active_set);
self.num_pushes.fetch_add(num_pushes, Ordering::Relaxed);
⋮----
pub(crate) fn process_prune_msg(
⋮----
active_set.prune(self_pubkey, peer, origins, stakes);
⋮----
pub(crate) fn refresh_push_active_set(
⋮----
timestamp(),
&self_keypair.pubkey(),
⋮----
.into_values()
.map(|(_stake, node)| *node.pubkey())
⋮----
if nodes.is_empty() {
⋮----
let cluster_size = crds.read().unwrap().num_pubkeys().max(stakes.len());
let mut active_set = self.active_set.write().unwrap();
active_set.rotate(
⋮----
mod tests {
⋮----
fn new_ping_cache() -> PingCache {
⋮----
fn old_new_push_messages(
⋮----
let (entries, messages, _) = self.new_push_messages(
⋮----
.map(|(pubkey, indices)| {
let values = indices.into_iter().map(|k| entries[k].clone()).collect();
⋮----
.collect()
⋮----
fn test_process_push_one() {
⋮----
let label = value.label();
assert_eq!(
⋮----
assert_eq!(crds.read().unwrap().get::<&CrdsValue>(&label), Some(&value));
assert!(push
⋮----
fn test_process_push_old_version() {
⋮----
ci.set_wallclock(1);
⋮----
ci.set_wallclock(0);
⋮----
fn test_process_push_timeout() {
⋮----
ci.set_wallclock(timeout + 1);
⋮----
fn test_process_push_update() {
⋮----
let origin = *ci.pubkey();
⋮----
fn test_new_push_messages() {
let now = timestamp();
⋮----
let mut ping_cache = new_ping_cache();
⋮----
ping_cache.mock_pong(*peer.pubkey(), peer.gossip().unwrap(), Instant::now());
⋮----
push.refresh_push_active_set(
⋮----
expected.insert(peer.label().pubkey(), vec![new_msg.clone()]);
let origin = new_msg.pubkey();
⋮----
fn test_personalized_push_messages() {
⋮----
let peers: Vec<_> = vec![0, 0, now]
⋮----
.map(|wallclock| {
⋮----
peer.set_wallclock(wallclock);
⋮----
.collect();
let origin: Vec<_> = peers.iter().map(|node| node.pubkey()).collect();
⋮----
let expected: HashMap<_, _> = vec![
⋮----
fn test_process_prune() {
⋮----
let ping_cache = Mutex::new(new_ping_cache());
⋮----
push.process_prune_msg(
⋮----
&peer.label().pubkey(),
&[new_msg.label().pubkey()],
⋮----
fn test_purge_old_pending_push_messages() {
⋮----
assert_eq!(crds.insert(peer, 0, GossipRoute::LocalMessage), Ok(()));
⋮----
fn test_purge_old_received_cache() {

================
File: gossip/src/crds_gossip.rs
================
pub struct CrdsGossip {
⋮----
impl CrdsGossip {
pub fn process_push_message(
⋮----
self.push.process_push_message(&self.crds, messages, now)
⋮----
pub fn prune_received_cache<I>(
⋮----
self.push.prune_received_cache(self_pubkey, origins, stakes)
⋮----
pub fn new_push_messages(
⋮----
.new_push_messages(pubkey, &self.crds, now, stakes, should_retain_crds_value)
⋮----
pub(crate) fn push_duplicate_shred<F>(
⋮----
let pubkey = keypair.pubkey();
let shred_slot = shred.slot();
let mut crds = self.crds.write().unwrap();
⋮----
.get_records(&pubkey)
.any(|value| match value.value.data() {
⋮----
return Ok(());
⋮----
shred.clone(),
⋮----
timestamp(),
⋮----
.filter_map(|value| match value.value.data() {
⋮----
Some((value.wallclock, *ix))
⋮----
.min()
.map(|(_ , ix)| ix)
.unwrap_or(0);
⋮----
let entries = chunks.enumerate().map(|(k, chunk)| {
⋮----
let now = timestamp();
⋮----
if let Err(err) = crds.insert(entry, now, GossipRoute::LocalMessage) {
error!("push_duplicate_shred failed: {err:?}");
⋮----
Ok(())
⋮----
pub fn process_prune_msg(
⋮----
if now > wallclock.saturating_add(self.push.prune_timeout) {
Err(CrdsGossipError::PruneMessageTimeout)
⋮----
.process_prune_msg(self_pubkey, peer, origin, stakes);
⋮----
Err(CrdsGossipError::BadPruneDestination)
⋮----
pub fn refresh_push_active_set(
⋮----
self.push.refresh_push_active_set(
⋮----
pub fn new_pull_request(
⋮----
self.pull.new_pull_request(
⋮----
pub fn generate_pull_responses(
⋮----
pub fn filter_pull_responses(
⋮----
.filter_pull_responses(&self.crds, timeouts, response, now, process_pull_stats)
⋮----
pub fn process_pull_responses(
⋮----
self.pull.process_pull_responses(
⋮----
pub fn make_timeouts<'a>(
⋮----
self.pull.make_timeouts(self_pubkey, stakes, epoch_duration)
⋮----
pub fn purge(
⋮----
debug_assert_eq!(timeouts[self_pubkey], u64::MAX);
debug_assert_ne!(timeouts[&Pubkey::default()], 0u64);
⋮----
.write()
.unwrap()
.trim_purged(now.saturating_sub(5 * self.pull.crds_timeout));
self.pull.purge_failed_inserts(now);
⋮----
pub(crate) fn get_gossip_nodes<R: Rng>(
⋮----
let active_cutoff = now.saturating_sub(ACTIVE_TIMEOUT.as_millis() as u64);
let crds = crds.read().unwrap();
crds.get_nodes()
.filter_map(|value| {
let node = value.value.contact_info().unwrap();
⋮----
let stake = stakes.get(node.pubkey()).copied().unwrap_or_default();
if stake == 0u64 || !rng.gen_ratio(1, 16) {
⋮----
Some(node)
⋮----
.filter(|node| {
node.pubkey() != pubkey
&& verify_shred_version(node.shred_version())
⋮----
.gossip()
.map(|addr| socket_addr_space.check(&addr))
.unwrap_or_default()
⋮----
Some(nodes) => nodes.contains(node.pubkey()),
⋮----
.cloned()
.collect()
⋮----
pub(crate) fn dedup_gossip_addresses(
⋮----
.into_iter()
.filter_map(|node| Some((node.gossip()?, node)))
.into_grouping_map()
.aggregate(|acc, _node_gossip, node| {
⋮----
Some(_) | None => Some((stake, node)),
⋮----
pub(crate) fn maybe_ping_gossip_addresses<R: Rng + CryptoRng>(
⋮----
let mut ping_cache = ping_cache.lock().unwrap();
⋮----
let Some(node_gossip) = node.gossip() else {
⋮----
let node = (*node.pubkey(), node_gossip);
ping_cache.check(rng, keypair, now, node)
⋮----
pings.push((node_gossip, ping));
⋮----
mod test {
⋮----
fn test_prune_errors() {
⋮----
let id = keypair.pubkey();
⋮----
.insert(
⋮----
.unwrap();
⋮----
crds_gossip.refresh_push_active_set(
⋮----
let mut res = crds_gossip.process_prune_msg(
⋮----
ci.pubkey(),
&Pubkey::from(hash(&[1; 32]).to_bytes()),
⋮----
assert_eq!(res.err(), Some(CrdsGossipError::BadPruneDestination));
res = crds_gossip.process_prune_msg(
⋮----
res.unwrap();
⋮----
assert_eq!(res.err(), Some(CrdsGossipError::PruneMessageTimeout));

================
File: gossip/src/crds_shards.rs
================
pub struct CrdsShards {
⋮----
impl CrdsShards {
pub fn new(shard_bits: u32) -> Self {
⋮----
shards: vec![IndexMap::new(); 1 << shard_bits],
⋮----
pub fn insert(&mut self, index: usize, value: &VersionedCrdsValue) -> bool {
let hash = CrdsFilter::hash_as_u64(value.value.hash());
self.shard_mut(hash).insert(index, hash).is_none()
⋮----
pub fn remove(&mut self, index: usize, value: &VersionedCrdsValue) -> bool {
⋮----
self.shard_mut(hash).swap_remove(&index).is_some()
⋮----
pub fn find(&self, mask: u64, mask_bits: u32) -> impl Iterator<Item = usize> + '_ {
let ones = (!0u64).checked_shr(mask_bits).unwrap_or(0);
⋮----
match self.shard_bits.cmp(&mask_bits) {
⋮----
Some(index)
⋮----
Iter::Less(self.shard(mask).iter().filter_map(pred))
⋮----
Ordering::Equal => Iter::Equal(self.shard(mask).keys().cloned()),
⋮----
let end = self.shard_index(mask) + 1;
⋮----
.iter()
.flat_map(IndexMap::keys)
.cloned(),
⋮----
fn shard_index(&self, hash: u64) -> usize {
hash.checked_shr(64 - self.shard_bits).unwrap_or(0) as usize
⋮----
fn shard(&self, hash: u64) -> &IndexMap<usize, u64> {
let shard_index = self.shard_index(hash);
self.shards.index(shard_index)
⋮----
fn shard_mut(&mut self, hash: u64) -> &mut IndexMap<usize, u64> {
⋮----
self.shards.index_mut(shard_index)
⋮----
pub fn check(&self, crds: &[VersionedCrdsValue]) {
⋮----
.cloned()
.collect();
indices.sort_unstable();
assert_eq!(indices, (0..crds.len()).collect::<Vec<_>>());
for (shard_index, shard) in self.shards.iter().enumerate() {
⋮----
assert_eq!(hash, CrdsFilter::hash_as_u64(crds[index].value.hash()));
assert_eq!(
⋮----
enum Iter<R, S, T> {
⋮----
impl<R, S, T> Iterator for Iter<R, S, T>
⋮----
type Item = usize;
fn next(&mut self) -> Option<Self::Item> {
⋮----
Self::Greater(iter) => iter.next(),
Self::Less(iter) => iter.next(),
Self::Equal(iter) => iter.next(),
⋮----
mod test {
⋮----
fn new_test_crds_value<R: Rng>(rng: &mut R) -> VersionedCrdsValue {
⋮----
let label = value.label();
⋮----
crds.insert(value, timestamp(), GossipRoute::LocalMessage)
.unwrap();
crds.get::<&VersionedCrdsValue>(&label).cloned().unwrap()
⋮----
fn check_mask(value: &VersionedCrdsValue, mask: u64, mask_bits: u32) -> bool {
⋮----
let ones = (!0u64).checked_shr(mask_bits).unwrap_or(0u64);
⋮----
fn filter_crds_values(
⋮----
.enumerate()
.filter_map(|(index, value)| {
if check_mask(value, mask, mask_bits) {
⋮----
.collect()
⋮----
fn test_crds_shards_round_trip() {
let mut rng = thread_rng();
let mut values: Vec<_> = repeat_with(|| new_test_crds_value(&mut rng))
.take(4096)
⋮----
for (index, value) in values.iter().enumerate() {
assert!(shards.insert(index, value));
⋮----
shards.check(&values);
⋮----
let index = rng.gen_range(0..values.len());
let value = values.swap_remove(index);
assert!(shards.remove(index, &value));
if index < values.len() {
let value = values.index(index);
assert!(shards.remove(values.len(), value));
⋮----
let mask = rng.gen();
⋮----
let mut set = filter_crds_values(&values, mask, mask_bits);
for index in shards.find(mask, mask_bits) {
assert!(set.remove(&index));
⋮----
assert!(set.is_empty());
⋮----
let mask = CrdsFilter::hash_as_u64(value.value.hash());
let hits: Vec<_> = shards.find(mask, 64).collect();
assert_eq!(hits, vec![index]);
⋮----
while !values.is_empty() {

================
File: gossip/src/crds_value.rs
================
pub struct CrdsValue {
⋮----
impl Sanitize for CrdsValue {
fn sanitize(&self) -> Result<(), SanitizeError> {
self.signature.sanitize()?;
self.data.sanitize()
⋮----
impl Signable for CrdsValue {
fn pubkey(&self) -> Pubkey {
self.pubkey()
⋮----
fn signable_data(&self) -> Cow<'static, [u8]> {
Cow::Owned(serialize(&self.data).expect("failed to serialize CrdsData"))
⋮----
fn get_signature(&self) -> Signature {
⋮----
fn set_signature(&mut self, signature: Signature) {
⋮----
fn verify(&self) -> bool {
self.get_signature()
.verify(self.pubkey().as_ref(), self.signable_data().borrow())
⋮----
/// Type of the replicated value
/// These are labels for values in a record that is associated with `Pubkey`
⋮----
/// These are labels for values in a record that is associated with `Pubkey`
#[derive(PartialEq, Hash, Eq, Clone, Debug)]
pub enum CrdsValueLabel {
⋮----
impl CrdsValueLabel {
pub fn pubkey(&self) -> Pubkey {
⋮----
impl CrdsValue {
pub fn new(data: CrdsData, keypair: &Keypair) -> Self {
let bincode_serialized_data = bincode::serialize(&data).unwrap();
let signature = keypair.sign_message(&bincode_serialized_data);
let hash = solana_sha256_hasher::hashv(&[signature.as_ref(), &bincode_serialized_data]);
⋮----
pub(crate) fn new_unsigned(data: CrdsData) -> Self {
⋮----
/// New random CrdsValue for tests and benchmarks.
    pub fn new_rand<R: Rng>(rng: &mut R, keypair: Option<&Keypair>) -> CrdsValue {
⋮----
pub fn new_rand<R: Rng>(rng: &mut R, keypair: Option<&Keypair>) -> CrdsValue {
⋮----
let data = CrdsData::new_rand(rng, Some(keypair.pubkey()));
⋮----
pub(crate) fn signature(&self) -> &Signature {
⋮----
pub(crate) fn data(&self) -> &CrdsData {
⋮----
pub(crate) fn hash(&self) -> &Hash {
⋮----
/// Totally unsecure unverifiable wallclock of the node that generated this message
    /// Latest wallclock is always picked.
⋮----
/// Latest wallclock is always picked.
    /// This is used to time out push messages.
⋮----
/// This is used to time out push messages.
    pub(crate) fn wallclock(&self) -> u64 {
⋮----
pub(crate) fn wallclock(&self) -> u64 {
self.data.wallclock()
⋮----
pub(crate) fn pubkey(&self) -> Pubkey {
self.data.pubkey()
⋮----
pub fn label(&self) -> CrdsValueLabel {
let pubkey = self.data.pubkey();
⋮----
pub(crate) fn contact_info(&self) -> Option<&ContactInfo> {
⋮----
Some(node)
⋮----
pub(crate) fn epoch_slots(&self) -> Option<&EpochSlots> {
⋮----
Some(epoch_slots)
⋮----
/// Returns the bincode serialized size (in bytes) of the CrdsValue.
    pub fn bincode_serialized_size(&self) -> usize {
⋮----
pub fn bincode_serialized_size(&self) -> usize {
⋮----
.map(usize::try_from)
.unwrap()
⋮----
// Manual implementation of Deserialize for CrdsValue in order to populate
// CrdsValue.hash which is skipped in serialization.
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
struct CrdsValue {
⋮----
bincode::serialize_into(&mut buffer, &data).map_err(serde::de::Error::custom)?;
let hash = solana_sha256_hasher::hashv(&[signature.as_ref(), &buffer]);
Ok(Self {
⋮----
mod test {
⋮----
fn test_keys_and_values() {
⋮----
assert_eq!(v.wallclock(), 0);
let key = *v.contact_info().unwrap().pubkey();
assert_eq!(v.label(), CrdsValueLabel::ContactInfo(key));
let v = Vote::new(Pubkey::default(), new_test_vote_tx(&mut rng), 0).unwrap();
⋮----
_ => panic!(),
⋮----
assert_eq!(v.label(), CrdsValueLabel::Vote(0, key));
⋮----
assert_eq!(v.label(), CrdsValueLabel::LowestSlot(key));
⋮----
fn test_signature() {
⋮----
&keypair.pubkey(),
timestamp(),
⋮----
verify_signatures(&mut v, &keypair, &wrong_keypair);
let v = Vote::new(keypair.pubkey(), new_test_vote_tx(&mut rng), timestamp()).unwrap();
⋮----
LowestSlot::new(keypair.pubkey(), 0, timestamp()),
⋮----
fn serialize_deserialize_value(value: &mut CrdsValue, keypair: &Keypair) {
⋮----
value.sign(keypair);
let original_signature = value.get_signature();
⋮----
let serialized_value = serialize(value).unwrap();
let deserialized_value: CrdsValue = deserialize(&serialized_value).unwrap();
let deserialized_signature = deserialized_value.get_signature();
assert_eq!(original_signature, deserialized_signature);
assert!(deserialized_value.verify());
⋮----
fn verify_signatures(
⋮----
assert!(!value.verify());
value.sign(correct_keypair);
assert!(value.verify());
value.sign(wrong_keypair);
⋮----
serialize_deserialize_value(value, correct_keypair);
⋮----
fn test_serialize_round_trip() {
⋮----
.into_vec()
.map(<[u8; 32]>::try_from)
⋮----
.unwrap(),
⋮----
let values: Vec<CrdsValue> = vec![
⋮----
let bytes = bincode::serialize(&values).unwrap();
assert_eq!(

================
File: gossip/src/crds.rs
================
pub struct Crds {
⋮----
pub enum CrdsError {
⋮----
pub enum GossipRoute<'a> {
⋮----
PushMessage(/*from:*/ &'a Pubkey),
⋮----
type CrdsCountsArray = [usize; 14];
pub(crate) struct CrdsDataStats {
⋮----
pub(crate) struct CrdsStats {
⋮----
pub struct VersionedCrdsValue {
⋮----
pub struct Cursor(u64);
impl Cursor {
fn ordinal(&self) -> u64 {
⋮----
fn consume(&mut self, ordinal: u64) {
self.0 = self.0.max(ordinal + 1);
⋮----
impl VersionedCrdsValue {
fn new(value: CrdsValue, cursor: Cursor, local_timestamp: u64, route: GossipRoute) -> Self {
⋮----
GossipRoute::PullResponse => Some(0),
GossipRoute::PushMessage(_) => Some(1),
⋮----
ordinal: cursor.ordinal(),
⋮----
impl Default for Crds {
fn default() -> Self {
⋮----
fn overrides(value: &CrdsValue, other: &VersionedCrdsValue) -> bool {
assert_eq!(value.label(), other.value.label(), "labels mismatch!");
if let CrdsData::ContactInfo(value) = value.data() {
if let CrdsData::ContactInfo(other) = other.value.data() {
if let Some(out) = value.overrides(other) {
⋮----
match value.wallclock().cmp(&other.value.wallclock()) {
⋮----
Ordering::Equal => other.value.hash() < value.hash(),
⋮----
impl Crds {
pub(crate) fn upserts(&self, value: &CrdsValue) -> bool {
match self.table.get(&value.label()) {
Some(other) => overrides(value, other),
⋮----
pub fn insert(
⋮----
let label = value.label();
let pubkey = value.pubkey();
⋮----
let mut stats = self.stats.lock().unwrap();
match self.table.entry(label) {
⋮----
stats.record_insert(&value, route);
let entry_index = entry.index();
self.shards.insert(entry_index, &value);
match value.value.data() {
⋮----
self.nodes.insert(entry_index);
self.shred_versions.insert(pubkey, node.shred_version());
⋮----
self.votes.insert(value.ordinal, entry_index);
⋮----
self.epoch_slots.insert(value.ordinal, entry_index);
⋮----
self.duplicate_shreds.insert(value.ordinal, entry_index);
⋮----
self.entries.insert(value.ordinal, entry_index);
self.records.entry(pubkey).or_default().insert(entry_index);
self.cursor.consume(value.ordinal);
entry.insert(value);
Ok(())
⋮----
Entry::Occupied(mut entry) if overrides(&value.value, entry.get()) => {
⋮----
self.shards.remove(entry_index, entry.get());
⋮----
debug_assert_matches!(entry.get().value.data(), CrdsData::ContactInfo(_));
⋮----
self.votes.remove(&entry.get().ordinal);
⋮----
self.epoch_slots.remove(&entry.get().ordinal);
⋮----
self.duplicate_shreds.remove(&entry.get().ordinal);
⋮----
self.entries.remove(&entry.get().ordinal);
⋮----
debug_assert_eq!(entry.get().value.pubkey(), pubkey);
⋮----
self.purged.push_back((*entry.get().value.hash(), now));
⋮----
stats.record_fail(&value, route);
trace!(
⋮----
if entry.get().value.hash() != value.value.hash() {
self.purged.push_back((*value.value.hash(), now));
Err(CrdsError::InsertFailed)
} else if matches!(route, GossipRoute::PushMessage(_)) {
let entry = entry.get_mut();
if entry.num_push_recv == Some(0) {
⋮----
let num_push_dups = entry.num_push_recv.unwrap_or_default();
entry.num_push_recv = Some(num_push_dups.saturating_add(1));
Err(CrdsError::DuplicatePush(num_push_dups))
⋮----
pub fn get<'a, 'b, V>(&'a self, key: V::Key) -> Option<V>
⋮----
pub(crate) fn get_shred_version(&self, pubkey: &Pubkey) -> Option<u16> {
self.shred_versions.get(pubkey).copied()
⋮----
/// Returns all entries which are ContactInfo.
    pub(crate) fn get_nodes(&self) -> impl Iterator<Item = &VersionedCrdsValue> {
⋮----
pub(crate) fn get_nodes(&self) -> impl Iterator<Item = &VersionedCrdsValue> {
self.nodes.iter().map(move |i| self.table.index(*i))
⋮----
/// Returns ContactInfo of all known nodes.
    pub(crate) fn get_nodes_contact_info(&self) -> impl Iterator<Item = &ContactInfo> {
⋮----
pub(crate) fn get_nodes_contact_info(&self) -> impl Iterator<Item = &ContactInfo> {
self.get_nodes().map(|v| match v.value.data() {
⋮----
_ => panic!("this should not happen!"),
⋮----
/// Returns all vote entries inserted since the given cursor.
    /// Updates the cursor as the votes are consumed.
⋮----
/// Updates the cursor as the votes are consumed.
    pub(crate) fn get_votes<'a>(
⋮----
pub(crate) fn get_votes<'a>(
⋮----
let range = (Bound::Included(cursor.ordinal()), Bound::Unbounded);
self.votes.range(range).map(move |(ordinal, index)| {
cursor.consume(*ordinal);
self.table.index(*index)
⋮----
/// Returns epoch-slots inserted since the given cursor.
    /// Updates the cursor as the values are consumed.
⋮----
/// Updates the cursor as the values are consumed.
    pub(crate) fn get_epoch_slots<'a>(
⋮----
pub(crate) fn get_epoch_slots<'a>(
⋮----
self.epoch_slots.range(range).map(move |(ordinal, index)| {
⋮----
/// Returns duplicate-shreds inserted since the given cursor.
    /// Updates the cursor as the values are consumed.
⋮----
/// Updates the cursor as the values are consumed.
    pub(crate) fn get_duplicate_shreds<'a>(
⋮----
pub(crate) fn get_duplicate_shreds<'a>(
⋮----
.range(range)
.map(move |(ordinal, index)| {
⋮----
/// Returns all entries inserted since the given cursor.
    pub(crate) fn get_entries<'a>(
⋮----
pub(crate) fn get_entries<'a>(
⋮----
self.entries.range(range).map(move |(&ordinal, &index)| {
cursor.consume(ordinal);
self.table.index(index)
⋮----
/// Returns all records associated with a pubkey.
    pub(crate) fn get_records(
⋮----
pub(crate) fn get_records(
⋮----
.get(pubkey)
.into_iter()
.flat_map(|records| records.into_iter())
.map(move |i| self.table.index(*i))
⋮----
pub(crate) fn num_nodes(&self) -> usize {
self.nodes.len()
⋮----
pub(crate) fn num_pubkeys(&self) -> usize {
self.records.len()
⋮----
pub fn len(&self) -> usize {
self.table.len()
⋮----
pub fn is_empty(&self) -> bool {
self.table.is_empty()
⋮----
pub(crate) fn values(&self) -> impl Iterator<Item = &VersionedCrdsValue> {
self.table.values()
⋮----
pub(crate) fn par_values(&self) -> ParValues<'_, CrdsValueLabel, VersionedCrdsValue> {
self.table.par_values()
⋮----
pub(crate) fn num_purged(&self) -> usize {
self.purged.len()
⋮----
pub(crate) fn purged(&self) -> impl IndexedParallelIterator<Item = Hash> + '_ {
self.purged.par_iter().map(|(hash, _)| *hash)
⋮----
pub(crate) fn trim_purged(&mut self, timestamp: u64) {
⋮----
.iter()
.take_while(|(_, ts)| *ts < timestamp)
.count();
self.purged.drain(..count);
⋮----
pub(crate) fn filter_bitmask(
⋮----
.find(mask, mask_bits)
.map(move |i| self.table.index(i))
.filter(move |VersionedCrdsValue { value, .. }| {
let data = value.data();
!value.data().is_deprecated()
⋮----
CrdsData::ContactInfo(info) => info.shred_version() == self_shred_version,
⋮----
pub(crate) fn update_record_timestamp(&mut self, pubkey: &Pubkey, now: u64) {
⋮----
match self.table.get_mut(&origin) {
⋮----
if let Some(indices) = self.records.get(pubkey) {
⋮----
let entry = self.table.index_mut(*index);
⋮----
pub fn find_old_labels(
⋮----
if let Some(origin) = self.table.get(&origin) {
⋮----
.wallclock()
.min(origin.local_timestamp)
.saturating_add(timeout)
⋮----
return vec![];
⋮----
.map(|&ix| self.table.get_index(ix).unwrap())
.filter(|(_, entry)| {
⋮----
.min(entry.local_timestamp)
⋮----
.map(|(label, _)| label)
.cloned()
⋮----
thread_pool.install(|| {
⋮----
.par_iter()
.flat_map(|(pubkey, index)| evict(pubkey, index))
.collect()
⋮----
pub fn remove(&mut self, key: &CrdsValueLabel, now: u64) {
let Some((index, _ , value)) = self.table.swap_remove_full(key) else {
⋮----
self.shards.remove(index, &value);
⋮----
self.nodes.swap_remove(&index);
⋮----
self.votes.remove(&value.ordinal);
⋮----
self.epoch_slots.remove(&value.ordinal);
⋮----
self.duplicate_shreds.remove(&value.ordinal);
⋮----
self.entries.remove(&value.ordinal);
let pubkey = value.value.pubkey();
let hash_map::Entry::Occupied(mut records_entry) = self.records.entry(pubkey) else {
panic!("this should not happen!");
⋮----
records_entry.get_mut().swap_remove(&index);
if records_entry.get().is_empty() {
records_entry.remove();
self.shred_versions.remove(&pubkey);
⋮----
let size = self.table.len();
⋮----
let value = self.table.index(index);
self.shards.remove(size, value);
self.shards.insert(index, value);
⋮----
self.nodes.swap_remove(&size);
self.nodes.insert(index);
⋮----
self.votes.insert(value.ordinal, index);
⋮----
self.epoch_slots.insert(value.ordinal, index);
⋮----
self.duplicate_shreds.insert(value.ordinal, index);
⋮----
self.entries.insert(value.ordinal, index);
⋮----
let records = self.records.get_mut(&pubkey).unwrap();
records.swap_remove(&size);
records.insert(index);
⋮----
pub(crate) fn should_trim(&self, cap: usize) -> bool {
10 * self.records.len() > 11 * cap
⋮----
pub(crate) fn trim(
⋮----
if self.should_trim(cap) {
let size = self.records.len().saturating_sub(cap);
self.drop(size, keep, stakes, now)
⋮----
Ok(0)
⋮----
fn drop(
⋮----
if stakes.values().all(|&stake| stake == 0) {
return Err(CrdsError::UnknownStakes);
⋮----
.keys()
.map(|k| (stakes.get(k).copied().unwrap_or_default(), *k))
.collect();
if size < keys.len() {
keys.select_nth_unstable(size);
⋮----
.take(size)
.map(|(_, k)| k)
.filter(|k| !keep.contains(k))
.flat_map(|k| &self.records[&k])
.map(|k| self.table.get_index(*k).unwrap().0.clone())
⋮----
self.remove(key, now);
⋮----
Ok(keys.len())
⋮----
pub(crate) fn take_stats(&self) -> CrdsStats {
std::mem::take(&mut self.stats.lock().unwrap())
⋮----
impl Default for CrdsDataStats {
⋮----
impl CrdsDataStats {
fn record_insert(&mut self, entry: &VersionedCrdsValue, route: GossipRoute) {
⋮----
if let CrdsData::Vote(_, vote) = entry.value.data() {
if let Some(slot) = vote.slot() {
let num_nodes = self.votes.get(&slot).copied().unwrap_or_default();
self.votes.put(slot, num_nodes + 1);
⋮----
if should_report_message_signature(entry.value.signature(), SIGNATURE_SAMPLE_LEADING_ZEROS)
⋮----
datapoint_info!(
⋮----
fn record_fail(&mut self, entry: &VersionedCrdsValue) {
⋮----
fn ordinal(entry: &VersionedCrdsValue) -> usize {
match entry.value.data() {
⋮----
impl CrdsStats {
⋮----
GossipRoute::PushMessage(_) => self.push.record_insert(entry, route),
GossipRoute::PullResponse => self.pull.record_insert(entry, route),
⋮----
fn record_fail(&mut self, entry: &VersionedCrdsValue, route: GossipRoute) {
⋮----
GossipRoute::PushMessage(_) => self.push.record_fail(entry),
GossipRoute::PullResponse => self.pull.record_fail(entry),
⋮----
mod tests {
⋮----
fn test_insert() {
⋮----
assert_eq!(
⋮----
assert_eq!(crds.table.len(), 1);
assert!(crds.table.contains_key(&val.label()));
assert_eq!(crds.table[&val.label()].local_timestamp, 0);
⋮----
fn test_update_old() {
⋮----
assert!(crds.purged.is_empty());
⋮----
fn test_update_new() {
⋮----
let value_hash = *original.hash();
assert_matches!(crds.insert(original, 0, GossipRoute::LocalMessage), Ok(()));
⋮----
assert_eq!(*crds.purged.back().unwrap(), (value_hash, 1));
assert_eq!(crds.table[&val.label()].local_timestamp, 1);
⋮----
fn test_update_timestamp() {
⋮----
let val1_hash = *val1.hash();
⋮----
assert_eq!(crds.table[&val1.label()].local_timestamp, 0);
assert_eq!(crds.table[&val1.label()].ordinal, 0);
⋮----
assert_eq!(val2.label().pubkey(), val1.label().pubkey());
⋮----
assert_eq!(*crds.purged.back().unwrap(), (val1_hash, 1));
assert_eq!(crds.table[&val2.label()].local_timestamp, 1);
assert_eq!(crds.table[&val2.label()].ordinal, 1);
crds.update_record_timestamp(&val2.label().pubkey(), 2);
assert_eq!(crds.table[&val2.label()].local_timestamp, 2);
⋮----
crds.update_record_timestamp(&val2.label().pubkey(), 1);
⋮----
fn test_find_old_records_default() {
let thread_pool = ThreadPoolBuilder::new().build().unwrap();
⋮----
assert!(crds.find_old_labels(&thread_pool, 0, &timeouts).is_empty());
⋮----
fn test_find_old_records_with_override() {
⋮----
let mut rng = thread_rng();
⋮----
assert!(crds.find_old_labels(&thread_pool, 2, &timeouts).is_empty());
stakes.insert(val.pubkey(), 1u64);
⋮----
stakes.remove(&val.pubkey());
⋮----
fn test_remove_default() {
⋮----
assert_matches!(
⋮----
crds.remove(&val.label(),  0);
⋮----
fn test_find_old_records_staked() {
⋮----
fn test_crds_shards() {
fn check_crds_shards(crds: &Crds) {
⋮----
.check(&crds.table.values().cloned().collect::<Vec<_>>());
⋮----
let keypairs: Vec<_> = std::iter::repeat_with(Keypair::new).take(256).collect();
⋮----
let keypair = &keypairs[rng.gen_range(0..keypairs.len())];
let value = CrdsValue::new_rand(&mut rng, Some(keypair));
let local_timestamp = new_rand_timestamp(&mut rng);
if let Ok(()) = crds.insert(value, local_timestamp, GossipRoute::LocalMessage) {
⋮----
check_crds_shards(&crds);
⋮----
assert_eq!(num_inserts, crds.cursor.0 as usize);
assert!(num_inserts > 700);
assert!(crds.num_purged() > 500);
assert_eq!(crds.num_purged() + crds.table.len(), 4096);
assert!(crds.table.len() > 200);
assert!(num_inserts > crds.table.len());
⋮----
while !crds.table.is_empty() {
let index = rng.gen_range(0..crds.table.len());
let key = crds.table.get_index(index).unwrap().0.clone();
crds.remove(&key,  0);
⋮----
fn check_crds_value_indices<R: rand::Rng>(
⋮----
let size = crds.table.len();
let since = if size == 0 || rng.gen() {
rng.gen_range(0..crds.cursor.0 + 1)
⋮----
crds.table[rng.gen_range(0..size)].ordinal
⋮----
.values()
.filter(|v| v.ordinal >= since)
.filter(|v| matches!(v.value.data(), CrdsData::EpochSlots(_, _)))
⋮----
let mut cursor = Cursor(since);
assert_eq!(num_epoch_slots, crds.get_epoch_slots(&mut cursor).count());
⋮----
for value in crds.get_epoch_slots(&mut Cursor(since)) {
assert!(value.ordinal >= since);
assert_matches!(value.value.data(), CrdsData::EpochSlots(_, _));
⋮----
.filter(|v| matches!(v.value.data(), CrdsData::Vote(_, _)))
⋮----
assert_eq!(num_votes, crds.get_votes(&mut cursor).count());
⋮----
for value in crds.get_votes(&mut Cursor(since)) {
⋮----
assert_matches!(value.value.data(), CrdsData::Vote(_, _));
⋮----
.filter(|value| value.ordinal >= since)
⋮----
assert_eq!(num_entries, crds.get_entries(&mut cursor).count());
⋮----
for value in crds.get_entries(&mut Cursor(since)) {
⋮----
.filter(|v| matches!(v.value.data(), CrdsData::ContactInfo(_)))
⋮----
assert_eq!(num_nodes, crds.get_nodes_contact_info().count());
assert_eq!(num_votes, crds.get_votes(&mut Cursor::default()).count());
⋮----
for vote in crds.get_votes(&mut Cursor::default()) {
assert_matches!(vote.value.data(), CrdsData::Vote(_, _));
⋮----
for epoch_slots in crds.get_epoch_slots(&mut Cursor::default()) {
assert_matches!(epoch_slots.value.data(), CrdsData::EpochSlots(_, _));
⋮----
fn test_crds_value_indices() {
⋮----
let keypairs: Vec<_> = repeat_with(Keypair::new).take(128).collect();
⋮----
check_crds_value_indices(&mut rng, &crds);
⋮----
let (num_nodes, num_votes, num_epoch_slots) = check_crds_value_indices(&mut rng, &crds);
assert!(num_nodes * 3 < crds.table.len());
assert!(num_nodes > 100, "num nodes: {num_nodes}");
assert!(num_votes > 100, "num votes: {num_votes}");
assert!(num_epoch_slots > 100, "num epoch slots: {num_epoch_slots}");
⋮----
if crds.table.len() % 16 == 0 {
⋮----
fn test_crds_records() {
fn check_crds_records(crds: &Crds) {
⋮----
let value = crds.table.index(*index);
assert_eq!(*pubkey, value.value.pubkey());
⋮----
let _ = crds.insert(value, local_timestamp, GossipRoute::LocalMessage);
⋮----
check_crds_records(&crds);
⋮----
assert!(crds.records.len() > 96);
assert!(crds.records.len() <= keypairs.len());
⋮----
if crds.table.len() % 64 == 0 {
⋮----
assert!(crds.records.is_empty());
⋮----
fn test_get_shred_version() {
⋮----
assert_eq!(crds.get_shred_version(&pubkey), None);
let mut node = ContactInfo::new_rand(&mut rng, Some(pubkey));
let wallclock = node.wallclock();
node.set_shred_version(42);
⋮----
assert_eq!(crds.get_shred_version(&pubkey), Some(42));
let mut node = node.clone();
node.set_wallclock(wallclock - 1);
node.set_shred_version(8);
⋮----
node.set_wallclock(wallclock + 1);
⋮----
assert_eq!(crds.get_shred_version(&pubkey), Some(8));
let val = AccountsHashes::new_rand(&mut rng, Some(pubkey));
⋮----
crds.remove(&CrdsValueLabel::ContactInfo(pubkey), timestamp());
assert_eq!(crds.get::<&ContactInfo>(pubkey), None);
⋮----
crds.remove(&CrdsValueLabel::AccountsHashes(pubkey), timestamp());
assert_eq!(crds.get_records(&pubkey).count(), 0);
⋮----
fn test_drop() {
fn num_unique_pubkeys<'a, I>(values: I) -> usize
⋮----
.map(|v| v.value.pubkey())
⋮----
.len()
⋮----
let keypairs: Vec<_> = repeat_with(Keypair::new).take(64).collect();
⋮----
.map(|k| (k.pubkey(), rng.gen_range(0..1000)))
⋮----
let num_values = crds.table.len();
let num_pubkeys = num_unique_pubkeys(crds.table.values());
assert!(!crds.should_trim(num_pubkeys));
assert!(crds.should_trim(num_pubkeys * 5 / 6));
let values: Vec<_> = crds.table.values().cloned().collect();
crds.drop(16, &[], &stakes,  0).unwrap();
⋮----
let purged: HashSet<_> = crds.purged.iter().map(|(hash, _)| hash).copied().collect();
⋮----
.filter(|v| purged.contains(v.value.hash()))
⋮----
assert_eq!(purged.len() + crds.table.len(), num_values);
assert_eq!(num_unique_pubkeys(&purged), 16);
assert_eq!(num_unique_pubkeys(crds.table.values()), num_pubkeys - 16);
⋮----
let pk = v.value.pubkey();
⋮----
assert!(
⋮----
assert!(!purged.contains(&k.pubkey()));
assert!(!purged.contains(&v.value.pubkey()));
⋮----
fn test_remove_staked() {
⋮----
fn test_equal() {
⋮----
VersionedCrdsValue::new(val.clone(), Cursor::default(), 1, GossipRoute::LocalMessage);
⋮----
assert_eq!(v1, v2);
assert!(!(v1 != v2));
assert!(!overrides(&v1.value, &v2));
assert!(!overrides(&v2.value, &v1));
⋮----
fn test_hash_order() {
⋮----
node.set_rpc((Ipv4Addr::LOCALHOST, 1244)).unwrap();
⋮----
assert_eq!(v1.value.label(), v2.value.label());
assert_eq!(v1.value.wallclock(), v2.value.wallclock());
assert_ne!(v1.value.hash(), v2.value.hash());
assert!(v1 != v2);
assert!(!(v1 == v2));
if v1.value.hash() > v2.value.hash() {
assert!(overrides(&v1.value, &v2));
⋮----
assert!(overrides(&v2.value, &v1));
⋮----
fn test_wallclock_order() {
⋮----
node.set_wallclock(0);
⋮----
fn test_label_order() {
⋮----
assert_ne!(v1, v2);

================
File: gossip/src/deprecated.rs
================
enum CompressionType {
⋮----
pub(crate) struct EpochIncompleteSlots {

================
File: gossip/src/duplicate_shred_handler.rs
================
type BufferEntry = [Option<DuplicateShred>; MAX_NUM_CHUNKS];
pub struct DuplicateShredHandler {
⋮----
impl DuplicateShredHandlerTrait for DuplicateShredHandler {
fn handle(&mut self, shred_data: DuplicateShred) {
self.cache_root_info();
self.maybe_prune_buffer();
⋮----
if let Err(error) = self.handle_shred_data(shred_data) {
if error.is_non_critical() {
info!(
⋮----
error!(
⋮----
impl DuplicateShredHandler {
pub fn new(
⋮----
fn cache_root_info(&mut self) {
let last_root = self.blockstore.max_root();
if last_root == self.last_root && !self.cached_staked_nodes.is_empty() {
⋮----
if let Ok(bank_fork) = self.bank_forks.try_read() {
let root_bank = bank_fork.root_bank();
let epoch_info = root_bank.get_epoch_info();
if self.cached_staked_nodes.is_empty() || self.cached_on_epoch < epoch_info.epoch {
⋮----
if let Some(cached_staked_nodes) = root_bank.epoch_staked_nodes(epoch_info.epoch) {
⋮----
fn handle_shred_data(&mut self, chunk: DuplicateShred) -> Result<(), Error> {
if !self.should_consume_slot(chunk.slot) {
return Ok(());
⋮----
let num_chunks = chunk.num_chunks();
let chunk_index = chunk.chunk_index();
⋮----
return Err(Error::InvalidChunkIndex {
⋮----
let entry = self.buffer.entry((chunk.slot, chunk.from)).or_default();
⋮----
.get_mut(usize::from(chunk_index))
.ok_or(Error::InvalidChunkIndex {
⋮----
})? = Some(chunk);
if entry.iter().flatten().count() == usize::from(num_chunks) {
let chunks = std::mem::take(entry).into_iter().flatten();
⋮----
.slot_leader_at(slot,  None)
.ok_or(Error::UnknownSlotLeader(slot))?;
⋮----
if !self.blockstore.has_duplicate_shreds_in_slot(slot) {
self.blockstore.store_duplicate_slot(
⋮----
shred1.into_payload(),
shred2.into_payload(),
⋮----
.send(slot)
.map_err(|_| Error::DuplicateSlotSenderFailure)?;
⋮----
self.consumed.insert(slot, true);
⋮----
Ok(())
⋮----
fn should_consume_slot(&mut self, slot: Slot) -> bool {
⋮----
&& slot < self.last_root.saturating_add(self.cached_slots_in_epoch)
&& should_consume_slot(slot, &self.blockstore, &mut self.consumed)
⋮----
fn maybe_prune_buffer(&mut self) {
if self.buffer.len() < BUFFER_CAPACITY.saturating_mul(2) {
⋮----
self.consumed.retain(|&slot, _| slot > self.last_root);
⋮----
self.buffer.retain(|(slot, pubkey), _| {
⋮----
&& should_consume_slot(*slot, &self.blockstore, &mut self.consumed)
⋮----
let count = counts.entry(*pubkey).or_default();
*count = count.saturating_add(1);
⋮----
if self.buffer.len() < BUFFER_CAPACITY {
⋮----
.drain()
.map(|entry @ ((_, pubkey), _)| {
⋮----
.get(&pubkey)
.copied()
.unwrap_or_default();
⋮----
.collect();
buffer.select_nth_unstable_by_key(BUFFER_CAPACITY, |&(stake, _)| Reverse(stake));
self.buffer.extend(
⋮----
.into_iter()
.take(BUFFER_CAPACITY)
.map(|(_, entry)| entry),
⋮----
fn should_consume_slot(
⋮----
.entry(slot)
.or_insert_with(|| blockstore.has_duplicate_shreds_in_slot(slot))
⋮----
mod tests {
⋮----
fn create_duplicate_proof(
⋮----
let shredder = Shredder::new(slot, slot - 1, 0, shred_version).unwrap();
⋮----
let shred1 = new_rand_shred(&mut rng, next_shred_index, &shredder, &my_keypair);
let shredder1 = Shredder::new(slot + 1, slot, 0, shred_version).unwrap();
⋮----
new_rand_shred(&mut rng, next_shred_index, &shredder1, &my_keypair)
⋮----
Some(Error::InvalidDuplicateShreds) => shred1.clone(),
_ => new_rand_shred(&mut rng, next_shred_index, &shredder, &my_keypair),
⋮----
None => my_keypair.pubkey(),
⋮----
let chunks = from_shred(
⋮----
shred2.payload().clone(),
⋮----
timestamp(),
⋮----
Ok(chunks)
⋮----
fn test_handle_mixed_entries() {
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
let my_pubkey = my_keypair.pubkey();
⋮----
let genesis_config_info = create_genesis_config_with_leader(10_000, &my_pubkey, 10_000);
⋮----
let mut bank_forks = bank_forks_arc.write().unwrap();
let bank0 = bank_forks.get(0).unwrap();
bank_forks.insert(Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 9));
bank_forks.set_root(9, None, None);
⋮----
assert!(blockstore.set_roots([0, 9].iter()).is_ok());
⋮----
&bank_forks_arc.read().unwrap().working_bank(),
⋮----
let (sender, receiver) = unbounded();
⋮----
blockstore.clone(),
⋮----
let chunks = create_duplicate_proof(
my_keypair.clone(),
⋮----
.unwrap();
let chunks1 = create_duplicate_proof(
⋮----
assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot));
assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot + 1));
for (chunk1, chunk2) in chunks.zip(chunks1) {
duplicate_shred_handler.handle(chunk1);
duplicate_shred_handler.handle(chunk2);
⋮----
assert!(blockstore.has_duplicate_shreds_in_slot(start_slot));
assert!(blockstore.has_duplicate_shreds_in_slot(start_slot + 1));
assert_eq!(
⋮----
let proof_result = create_duplicate_proof(
⋮----
Some(error),
⋮----
duplicate_shred_handler.handle(chunk);
⋮----
assert!(!blockstore.has_duplicate_shreds_in_slot(start_slot + 2));
assert!(receiver.is_empty());
⋮----
fn test_reject_abuses() {
⋮----
blockstore.set_roots([0, 9].iter()).unwrap();
⋮----
blockstore.max_root() + duplicate_shred_handler.cached_slots_in_epoch + start_slot;
⋮----
assert!(!blockstore.has_duplicate_shreds_in_slot(future_slot));
⋮----
let mut chunks = create_duplicate_proof(
⋮----
duplicate_shred_handler.handle(chunks.next().unwrap());
⋮----
assert_eq!(receiver.try_iter().collect_vec(), vec![start_slot]);

================
File: gossip/src/duplicate_shred_listener.rs
================
pub trait DuplicateShredHandlerTrait: Send {
⋮----
pub struct DuplicateShredListener {
⋮----
impl DuplicateShredListener {
pub fn new(
⋮----
.name("solCiEntryLstnr".to_string())
.spawn(move || {
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
// Here we are sending data one by one rather than in a batch because in the future
// we may send different type of CrdsData to different senders.
fn recv_loop(
⋮----
while !exit.load(Ordering::Relaxed) {
let entries: Vec<DuplicateShred> = cluster_info.get_duplicate_shreds(&mut cursor);
⋮----
handler.handle(x);
⋮----
sleep(Duration::from_millis(GOSSIP_SLEEP_MILLIS));
⋮----
mod tests {
⋮----
struct FakeHandler {
⋮----
impl FakeHandler {
fn new(count: Arc<AtomicU32>) -> Self {
⋮----
impl DuplicateShredHandlerTrait for FakeHandler {
fn handle(&mut self, data: DuplicateShred) {
assert!(data.num_chunks() > 0);
self.count.fetch_add(1, Ordering::Relaxed);
⋮----
fn test_listener_get_entries() {
⋮----
let node = Node::new_localhost_with_pubkey(&host1_key.pubkey());
⋮----
let handler = FakeHandler::new(count.clone());
let listener = DuplicateShredListener::new(exit.clone(), cluster_info.clone(), handler);
⋮----
let shredder = Shredder::new(slot, parent_slot, reference_tick, version).unwrap();
⋮----
let shred1 = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
let shred2 = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
assert!(cluster_info
⋮----
cluster_info.flush_push_queue();
⋮----
assert_eq!(count.load(Ordering::Relaxed), 3);
exit.store(true, Ordering::Relaxed);
assert!(listener.join().is_ok());

================
File: gossip/src/duplicate_shred.rs
================
pub(crate) type DuplicateShredIndex = u16;
⋮----
pub struct DuplicateShred {
⋮----
impl DuplicateShred {
⋮----
pub(crate) fn num_chunks(&self) -> u8 {
⋮----
pub(crate) fn chunk_index(&self) -> u8 {
⋮----
pub enum Error {
⋮----
impl Error {
pub(crate) fn is_non_critical(&self) -> bool {
⋮----
fn check_shreds<F>(
⋮----
if shred1.slot() != shred2.slot() {
return Err(Error::SlotMismatch);
⋮----
if shred1.version() != shred_version {
return Err(Error::InvalidShredVersion(shred1.version()));
⋮----
if shred2.version() != shred_version {
return Err(Error::InvalidShredVersion(shred2.version()));
⋮----
leader_schedule(shred1.slot()).ok_or(Error::UnknownSlotLeader(shred1.slot()))?;
if !shred1.verify(&slot_leader) || !shred2.verify(&slot_leader) {
return Err(Error::InvalidSignature);
⋮----
if shred1.fec_set_index() == shred2.fec_set_index()
&& shred1.merkle_root().ok() != shred2.merkle_root().ok()
⋮----
return Ok(());
⋮----
if shred1.shred_type() != shred2.shred_type() {
return Err(Error::ShredTypeMismatch);
⋮----
if shred1.index() == shred2.index() {
if shred1.is_shred_duplicate(shred2) {
⋮----
return Err(Error::InvalidDuplicateShreds);
⋮----
if shred1.shred_type() == ShredType::Data {
if shred1.last_in_slot() && shred2.index() > shred1.index() {
⋮----
if shred2.last_in_slot() && shred1.index() > shred2.index() {
⋮----
return Err(Error::InvalidLastIndexConflict);
⋮----
Err(Error::InvalidErasureMetaConflict)
⋮----
pub(crate) fn from_shred<T: AsRef<[u8]>, F>(
⋮----
if shred.payload().as_ref() == other_payload.as_ref() {
⋮----
check_shreds(leader_schedule, &shred, &other_shred, shred_version)?;
let slot = shred.slot();
⋮----
shred1: shred.into_payload(),
shred2: other_shred.into_payload(),
⋮----
return Err(Error::InvalidSizeLimit);
⋮----
let chunks: Vec<_> = data.chunks(chunk_size).map(Vec::from).collect();
let num_chunks = u8::try_from(chunks.len())?;
⋮----
.into_iter()
.enumerate()
.map(move |(i, chunk)| DuplicateShred {
⋮----
_unused_shred_type: ShredType::Code.into(),
⋮----
Ok(chunks)
⋮----
fn check_chunk(slot: Slot, num_chunks: u8) -> impl Fn(&DuplicateShred) -> Result<(), Error> {
⋮----
Err(Error::SlotMismatch)
⋮----
Err(Error::NumChunksMismatch)
⋮----
Err(Error::InvalidChunkIndex {
⋮----
Ok(())
⋮----
pub(crate) fn into_shreds(
⋮----
let mut chunks = chunks.into_iter();
⋮----
} = chunks.next().ok_or(Error::InvalidDuplicateShreds)?;
let check_chunk = check_chunk(slot, num_chunks);
⋮----
data.insert(chunk_index, chunk);
⋮----
check_chunk(&chunk)?;
match data.entry(chunk.chunk_index) {
⋮----
entry.insert(chunk.chunk);
⋮----
if *entry.get() != chunk.chunk {
return Err(Error::DataChunkMismatch);
⋮----
if data.len() != num_chunks as usize {
return Err(Error::MissingDataChunk);
⋮----
let data = (0..num_chunks).map(|k| data.remove(&k).unwrap()).concat();
⋮----
return Err(Error::InvalidDuplicateSlotProof);
⋮----
if shred1.slot() != slot || shred2.slot() != slot {
⋮----
check_shreds(
Some(|_| Some(slot_leader).copied()),
⋮----
Ok((shred1, shred2))
⋮----
impl Sanitize for DuplicateShred {
fn sanitize(&self) -> Result<(), SanitizeError> {
sanitize_wallclock(self.wallclock)?;
⋮----
return Err(SanitizeError::IndexOutOfBounds);
⋮----
self.from.sanitize()
⋮----
pub(crate) mod tests {
⋮----
fn test_duplicate_shred_header_size() {
⋮----
_unused_shred_type: ShredType::Data.into(),
⋮----
assert_eq!(
⋮----
pub(crate) fn new_rand_shred<R: Rng>(
⋮----
let (mut data_shreds, _) = new_rand_shreds(
⋮----
data_shreds.pop().unwrap()
⋮----
fn new_rand_data_shred<R: Rng>(
⋮----
fn new_rand_coding_shreds<R: Rng>(
⋮----
let (_, coding_shreds) = new_rand_shreds(
⋮----
fn new_rand_shreds<R: Rng>(
⋮----
let tx = transfer(
⋮----
rng.gen(),
⋮----
vec![tx],
⋮----
.take(num_entries)
.collect();
shredder.entries_to_merkle_shreds_for_tests(
⋮----
Hash::new_from_array(rng.gen()),
⋮----
fn from_shred_bypass_checks(
⋮----
fn test_duplicate_shred_round_trip() {
⋮----
let shredder = Shredder::new(slot, parent_slot, reference_tick, version).unwrap();
let next_shred_index = rng.gen_range(0..32_000);
let shred1 = new_rand_data_shred(&mut rng, next_shred_index, &shredder, &leader, true);
let shred2 = new_rand_data_shred(&mut rng, next_shred_index, &shredder, &leader, true);
⋮----
Some(leader.pubkey())
⋮----
let chunks: Vec<_> = from_shred(
shred1.clone(),
⋮----
shred2.payload().clone(),
Some(leader_schedule),
⋮----
.unwrap()
⋮----
assert!(chunks.len() > 4);
let (shred3, shred4) = into_shreds(&leader.pubkey(), chunks, version).unwrap();
assert_eq!(shred1, shred3);
assert_eq!(shred2, shred4);
⋮----
fn test_duplicate_shred_invalid() {
⋮----
let data_shred = new_rand_data_shred(&mut rng, next_shred_index, &shredder, &leader, true);
⋮----
new_rand_coding_shreds(&mut rng, next_shred_index, 10, &shredder, &leader);
let test_cases = vec![
⋮----
for (shred1, shred2) in test_cases.into_iter() {
assert_matches!(
⋮----
let chunks: Vec<_> = from_shred_bypass_checks(
⋮----
shred2.clone(),
⋮----
fn test_latest_index_conflict_round_trip() {
⋮----
let next_shred_index = rng.gen_range(0..31_000);
⋮----
new_rand_data_shred(&mut rng, next_shred_index, &shredder, &leader, true),
new_rand_data_shred(
⋮----
new_rand_data_shred(&mut rng, next_shred_index + 100, &shredder, &leader, true),
⋮----
for (shred1, shred2) in test_cases.iter().flat_map(|(a, b)| [(a, b), (b, a)]) {
⋮----
assert_eq!(shred1, &shred3);
assert_eq!(shred2, &shred4);
⋮----
fn test_latest_index_conflict_invalid() {
⋮----
fn test_erasure_meta_conflict_round_trip() {
⋮----
new_rand_coding_shreds(&mut rng, next_shred_index, 13, &shredder, &leader);
⋮----
new_rand_coding_shreds(&mut rng, next_shred_index, 7, &shredder, &leader);
⋮----
fn test_erasure_meta_conflict_invalid() {
⋮----
new_rand_coding_shreds(&mut rng, next_shred_index + 1, 10, &shredder, &leader);
⋮----
new_rand_coding_shreds(&mut rng, next_shred_index + 1, 13, &shredder, &leader);
⋮----
fn test_merkle_root_conflict_round_trip() {
⋮----
let (data_shreds, coding_shreds) = new_rand_shreds(
⋮----
let (diff_data_shreds, diff_coding_shreds) = new_rand_shreds(
⋮----
fn test_merkle_root_conflict_invalid() {
⋮----
let (next_data_shreds, next_coding_shreds) = new_rand_shreds(
⋮----
fn test_shred_version() {
⋮----
let shredder = Shredder::new(slot, parent_slot, reference_tick, version + 1).unwrap();
let (wrong_data_shreds_1, wrong_coding_shreds_1) = new_rand_shreds(
⋮----
let shredder = Shredder::new(slot, parent_slot, reference_tick, version + 2).unwrap();
let (wrong_data_shreds_2, wrong_coding_shreds_2) = new_rand_shreds(
⋮----
fn test_retransmitter_signature_invalid() {
⋮----
new_rand_coding_shreds(&mut rng, next_shred_index, 10, &shredder, &leader)[0].clone();
let mut data_shred_different_retransmitter_payload = data_shred.clone().into_payload();
⋮----
&mut data_shred_different_retransmitter_payload.as_mut(),
⋮----
.unwrap();
⋮----
Shred::new_from_serialized_shred(data_shred_different_retransmitter_payload).unwrap();
let mut coding_shred_different_retransmitter_payload = coding_shred.clone().into_payload();
⋮----
&mut coding_shred_different_retransmitter_payload.as_mut(),
⋮----
Shred::new_from_serialized_shred(coding_shred_different_retransmitter_payload).unwrap();

================
File: gossip/src/epoch_slots.rs
================
pub struct Uncompressed {
⋮----
impl Sanitize for Uncompressed {
fn sanitize(&self) -> std::result::Result<(), SanitizeError> {
⋮----
return Err(SanitizeError::ValueOutOfBounds);
⋮----
if !self.slots.len().is_multiple_of(8) {
⋮----
if self.slots.len() != self.slots.capacity() {
⋮----
Ok(())
⋮----
pub struct Flate2 {
⋮----
mod serde_compat_bytes {
⋮----
pub(super) fn serialize<S: Serializer>(
⋮----
serializer.serialize_bytes(bytes)
⋮----
pub(super) fn deserialize<'de, D>(deserializer: D) -> Result<Arc<Vec<u8>>, D::Error>
⋮----
.map(ByteBuf::into_vec)
.map(Arc::new)
⋮----
impl Sanitize for Flate2 {
⋮----
pub enum Error {
⋮----
pub type Result<T> = std::result::Result<T, Error>;
⋮----
fn from(_e: flate2::CompressError) -> Error {
⋮----
fn from(_e: flate2::DecompressError) -> Error {
⋮----
impl Flate2 {
fn deflate(unc: Uncompressed) -> Result<Self> {
let mut compressed = Vec::with_capacity(unc.slots.block_capacity());
⋮----
let block_len = unc.slots.block_len();
let bits = Arc::unwrap_or_clone(unc.slots).into_boxed_slice();
compressor.compress_vec(&bits[0..block_len], &mut compressed, FlushCompress::Finish)?;
⋮----
let _ = rv.inflate()?;
Ok(rv)
⋮----
thread_local! {
⋮----
pub fn inflate(&self) -> Result<Uncompressed> {
⋮----
Self::DECOMPRESS_BUF.with_borrow_mut(|v| {
v.clear();
let _ = decompress.decompress_vec(&self.compressed, v, FlushDecompress::Finish)?;
Ok(Uncompressed {
⋮----
impl Uncompressed {
pub fn new(max_size: usize) -> Self {
⋮----
fn to_slots(&self, min_slot: Slot) -> Vec<Slot> {
Self::get_slots(Cow::Borrowed(self), min_slot).collect()
⋮----
pub fn add(&mut self, slots: &[Slot]) -> usize {
for (i, s) in slots.iter().enumerate() {
⋮----
if *s - self.first_slot >= self.slots.len() {
⋮----
Arc::make_mut(&mut self.slots).set(*s - self.first_slot, true);
⋮----
slots.len()
⋮----
fn get_slots(this: Cow<'_, Self>, min_slot: Slot) -> impl Iterator<Item = Slot> + '_ {
⋮----
let start = min_slot.saturating_sub(first_slot);
let end = this.slots.len().min(this.num as u64);
⋮----
.filter(move |&k| this.slots.get(k))
.map(move |k| first_slot + k)
⋮----
pub enum CompressedSlots {
⋮----
impl Sanitize for CompressedSlots {
⋮----
CompressedSlots::Uncompressed(a) => a.sanitize(),
CompressedSlots::Flate2(b) => b.sanitize(),
⋮----
impl Default for CompressedSlots {
fn default() -> Self {
⋮----
impl CompressedSlots {
pub(crate) fn new(max_size: usize) -> Self {
⋮----
pub fn first_slot(&self) -> Slot {
⋮----
pub fn num_slots(&self) -> usize {
⋮----
CompressedSlots::Uncompressed(vals) => vals.add(slots),
⋮----
fn to_slots(&self, min_slot: Slot) -> Result<impl Iterator<Item = Slot> + '_> {
⋮----
Self::Flate2(slots) => Cow::Owned(slots.inflate()?),
⋮----
Ok(Uncompressed::get_slots(slots, min_slot))
⋮----
pub fn deflate(&mut self) -> Result<()> {
⋮----
let unc = vals.clone();
⋮----
CompressedSlots::Flate2(_) => Ok(()),
⋮----
pub struct EpochSlots {
⋮----
impl Sanitize for EpochSlots {
⋮----
self.from.sanitize()?;
self.slots.sanitize()
⋮----
use std::fmt;
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
let num_slots: usize = self.slots.iter().map(|s| s.num_slots()).sum();
let lowest_slot = self.first_slot().unwrap_or(0);
write!(
⋮----
impl EpochSlots {
pub fn new(from: Pubkey, now: u64) -> Self {
⋮----
slots: vec![],
⋮----
pub fn fill(&mut self, slots: &[Slot], now: u64) -> usize {
⋮----
while num < slots.len() {
num += self.add(&slots[num..]);
if num < slots.len() {
if self.deflate().is_err() {
⋮----
let space = self.max_compressed_slot_size();
⋮----
self.slots.push(cslot);
⋮----
num += s.add(&slots[num..]);
if num >= slots.len() {
⋮----
for s in self.slots.iter_mut() {
s.deflate()?;
⋮----
pub fn max_compressed_slot_size(&self) -> isize {
let len_header = serialized_size(self).unwrap();
let len_slot = serialized_size(&CompressedSlots::default()).unwrap();
⋮----
pub fn first_slot(&self) -> Option<Slot> {
self.slots.iter().map(|s| s.first_slot()).min()
⋮----
pub fn to_slots(&self, min_slot: Slot) -> impl Iterator<Item = Slot> + '_ {
⋮----
.iter()
.filter(move |s| min_slot < s.first_slot() + s.num_slots() as u64)
.filter_map(move |s| s.to_slots(min_slot).ok())
.flatten()
⋮----
/// New random EpochSlots for tests and simulations.
    pub(crate) fn new_rand<R: rand::Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
⋮----
pub(crate) fn new_rand<R: rand::Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
⋮----
let pubkey = pubkey.unwrap_or_else(solana_pubkey::new_rand);
⋮----
let num_slots = rng.gen_range(0..20);
let slots: Vec<_> = std::iter::repeat_with(|| 47825632 + rng.gen_range(0..512))
.take(num_slots)
.collect();
epoch_slots.add(&slots);
⋮----
mod tests {
⋮----
fn test_epoch_slots_max_size() {
⋮----
assert!(epoch_slots.max_compressed_slot_size() > 0);
⋮----
fn test_epoch_slots_uncompressed_add_1() {
⋮----
assert_eq!(slots.slots.capacity(), 8);
assert_eq!(slots.add(&[1]), 1);
assert_eq!(slots.to_slots(1), vec![1]);
assert!(slots.to_slots(2).is_empty());
⋮----
fn test_epoch_slots_to_slots_overflow() {
⋮----
assert!(slots.to_slots(0).is_empty());
⋮----
fn test_epoch_slots_uncompressed_add_2() {
⋮----
assert_eq!(slots.add(&[1, 2]), 2);
assert_eq!(slots.to_slots(1), vec![1, 2]);
⋮----
fn test_epoch_slots_uncompressed_add_3a() {
⋮----
assert_eq!(slots.add(&[1, 3, 2]), 3);
assert_eq!(slots.to_slots(1), vec![1, 2, 3]);
⋮----
fn test_epoch_slots_uncompressed_add_3b() {
⋮----
assert_eq!(slots.add(&[1, 10, 2]), 1);
⋮----
fn test_epoch_slots_uncompressed_add_3c() {
⋮----
assert_eq!(slots.add(&[1, 10, 2]), 3);
assert_eq!(slots.to_slots(1), vec![1, 2, 10]);
assert_eq!(slots.to_slots(2), vec![2, 10]);
assert_eq!(slots.to_slots(3), vec![10]);
assert!(slots.to_slots(11).is_empty());
⋮----
fn test_epoch_slots_compressed() {
⋮----
slots.add(&[1, 701, 2]);
assert_eq!(slots.num, 701);
let compressed = Flate2::deflate(slots).unwrap();
assert_eq!(compressed.first_slot, 1);
assert_eq!(compressed.num, 701);
assert!(compressed.compressed.len() < 32);
let slots = compressed.inflate().unwrap();
assert_eq!(slots.first_slot, 1);
⋮----
assert_eq!(slots.to_slots(1), vec![1, 2, 701]);
⋮----
fn test_epoch_slots_sanitize() {
⋮----
assert!(slots.sanitize().is_ok());
let mut o = slots.clone();
⋮----
assert_eq!(o.sanitize(), Err(SanitizeError::ValueOutOfBounds));
⋮----
o.slots = Arc::new(BitVec::new_fill(false, 7)); // Length not a multiple of 8
⋮----
o.slots = Arc::new(BitVec::with_capacity(8)); // capacity() not equal to len()
⋮----
assert!(compressed.sanitize().is_ok());
let mut o = compressed.clone();
⋮----
let range: Vec<Slot> = (0..5000).collect();
assert_eq!(slots.fill(&range, 1), 5000);
assert_eq!(slots.wallclock, 1);
⋮----
fn test_epoch_slots_fill_range() {
⋮----
assert!(slots.to_slots(0).eq(range));
assert!(slots.to_slots(4999).eq(vec![4999]));
assert_eq!(slots.to_slots(5000).next(), None);
⋮----
fn test_epoch_slots_fill_sparce_range() {
let range: Vec<Slot> = (0..5000).map(|x| x * 3).collect();
⋮----
assert_eq!(slots.fill(&range, 2), 5000);
assert_eq!(slots.wallclock, 2);
assert_eq!(slots.slots.len(), 3);
assert_eq!(slots.slots[0].first_slot(), 0);
assert_ne!(slots.slots[0].num_slots(), 0);
let next = slots.slots[0].num_slots() as u64 + slots.slots[0].first_slot();
assert!(slots.slots[1].first_slot() >= next);
assert_ne!(slots.slots[1].num_slots(), 0);
assert_ne!(slots.slots[2].num_slots(), 0);
⋮----
assert!(slots.to_slots(4999 * 3).eq(vec![4999 * 3]));
⋮----
fn test_epoch_slots_fill_large_sparce_range() {
let range: Vec<Slot> = (0..5000).map(|x| x * 7).collect();
⋮----
fn make_rand_slots<R: Rng>(rng: &mut R) -> impl Iterator<Item = Slot> + '_ {
repeat_with(|| rng.gen_range(1..5)).scan(0, |slot, step| {
⋮----
Some(*slot)
⋮----
fn test_epoch_slots_fill_uncompressed_random_range() {
⋮----
let range: Vec<Slot> = make_rand_slots(&mut rng).take(5000).collect();
let sz = EpochSlots::default().max_compressed_slot_size();
⋮----
let sz = slots.add(&range);
let slots = slots.to_slots(0);
assert_eq!(slots.len(), sz);
assert_eq!(slots[..], range[..sz]);
⋮----
fn test_epoch_slots_fill_compressed_random_range() {
⋮----
slots.deflate().unwrap();
let slots = slots.to_slots(0).unwrap().collect::<Vec<_>>();
⋮----
fn test_epoch_slots_fill_random_range() {
⋮----
let sz = slots.fill(&range, 1);
⋮----
assert_eq!(
⋮----
assert!(s.to_slots(0).is_ok());
⋮----
let slots = slots.to_slots(0).collect::<Vec<_>>();
assert_eq!(slots[..], range[..slots.len()]);
assert_eq!(sz, slots.len())

================
File: gossip/src/epoch_specs.rs
================
pub struct EpochSpecs {
⋮----
impl EpochSpecs {
⋮----
pub fn current_epoch_staked_nodes(&mut self) -> &Arc<HashMap<Pubkey,  u64>> {
self.maybe_refresh();
⋮----
pub(crate) fn epoch_duration(&mut self) -> Duration {
⋮----
fn maybe_refresh(&mut self) {
if self.epoch_schedule.get_epoch(self.root.get()) == self.epoch {
⋮----
let root_bank = self.bank_forks.read().unwrap().root_bank();
debug_assert_eq!(
⋮----
self.epoch = root_bank.epoch();
self.epoch_schedule = root_bank.epoch_schedule().clone();
self.current_epoch_staked_nodes = root_bank.current_epoch_staked_nodes();
self.epoch_duration = get_epoch_duration(&root_bank);
⋮----
impl Clone for EpochSpecs {
fn clone(&self) -> Self {
Self::from(self.bank_forks.clone())
⋮----
fn from(bank_forks: Arc<RwLock<BankForks>>) -> Self {
⋮----
let bank_forks = bank_forks.read().unwrap();
(bank_forks.get_atomic_root(), bank_forks.root_bank())
⋮----
epoch: root_bank.epoch(),
epoch_schedule: root_bank.epoch_schedule().clone(),
⋮----
current_epoch_staked_nodes: root_bank.current_epoch_staked_nodes(),
epoch_duration: get_epoch_duration(&root_bank),
⋮----
fn get_epoch_duration(bank: &Bank) -> Duration {
let num_slots = bank.get_slots_in_epoch(bank.epoch());
⋮----
mod tests {
⋮----
fn test_get_epoch_duration() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
assert_eq!(bank.epoch(), epoch);
assert_eq!(bank.get_slots_in_epoch(epoch), num_slots);
assert_eq!(
⋮----
fn verify_epoch_specs(
⋮----
assert_eq!(epoch_specs.epoch_duration(), get_epoch_duration(root_bank));
assert_eq!(root_bank.slot(), slot);
assert_eq!(root_bank.epoch(), epoch);
assert_eq!(epoch_specs.epoch, epoch);
assert_eq!(&epoch_specs.epoch_schedule, root_bank.epoch_schedule());
assert_eq!(epoch_specs.root.get(), slot);
⋮----
fn test_epoch_specs_refresh() {
⋮----
let mut epoch_specs = EpochSpecs::from(bank_forks.clone());
⋮----
let bank = bank_forks.read().unwrap().get(slot - 1).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank);
⋮----
let root_bank = bank_forks.read().unwrap().get(0).unwrap();
verify_epoch_specs(
⋮----
bank_forks.write().unwrap().set_root(17, None, None);
let root_bank = bank_forks.read().unwrap().get(17).unwrap();
⋮----
bank_forks.write().unwrap().set_root(19, None, None);
let root_bank = bank_forks.read().unwrap().get(19).unwrap();
⋮----
bank_forks.write().unwrap().set_root(37, None, None);
let root_bank = bank_forks.read().unwrap().get(37).unwrap();
⋮----
bank_forks.write().unwrap().set_root(59, None, None);
let root_bank = bank_forks.read().unwrap().get(59).unwrap();
⋮----
bank_forks.write().unwrap().set_root(97, None, None);
let root_bank = bank_forks.read().unwrap().get(97).unwrap();

================
File: gossip/src/gossip_error.rs
================
pub enum GossipError {
⋮----
fn from(_e: SendError<T>) -> GossipError {

================
File: gossip/src/gossip_service.rs
================
pub struct GossipService {
⋮----
impl GossipService {
pub fn new(
⋮----
trace!(
⋮----
let socket_addr_space = *cluster_info.socket_addr_space();
⋮----
"solRcvrGossip".to_string(),
gossip_sockets.clone(),
cluster_info.bind_ip_addrs(),
exit.clone(),
⋮----
gossip_receiver_stats.clone(),
Some(Duration::from_millis(1)),
⋮----
let t_socket_consume = cluster_info.clone().start_socket_consume_thread(
bank_forks.clone(),
⋮----
let t_listen = cluster_info.clone().listen(
⋮----
response_sender.clone(),
⋮----
let t_gossip = cluster_info.clone().gossip(
⋮----
.name("solGossipMetr".to_string())
.spawn({
let cluster_info = cluster_info.clone();
let mut epoch_specs = bank_forks.map(EpochSpecs::from);
⋮----
while !exit.load(Ordering::Relaxed) {
sleep(SUBMIT_GOSSIP_STATS_INTERVAL);
⋮----
.as_mut()
.map(|epoch_specs| epoch_specs.current_epoch_staked_nodes())
.cloned()
.unwrap_or_default();
submit_gossip_stats(&cluster_info.stats, &cluster_info.gossip, &stakes);
gossip_receiver_stats.report();
⋮----
.unwrap();
let thread_hdls = vec![
⋮----
pub fn join(self) -> thread::Result<()> {
⋮----
thread_hdl.join()?;
⋮----
Ok(())
⋮----
pub fn discover_validators(
⋮----
let (_all_peers, validators) = discover_peers(
⋮----
&vec![*entrypoint],
Some(num_nodes),
⋮----
Ok(validators)
⋮----
pub fn discover_peers(
⋮----
let keypair = keypair.unwrap_or_else(Keypair::new);
⋮----
let (gossip_service, ip_echo, spy_ref) = make_node(
⋮----
let id = spy_ref.id();
info!("Entrypoints: {entrypoints:?}");
info!("Node Id: {id:?}");
⋮----
info!("Gossip Address: {my_gossip_addr:?}");
⋮----
let _ip_echo_server = ip_echo.map(|tcp_listener| {
⋮----
Some(my_shred_version),
⋮----
let (met_criteria, elapsed, all_peers, tvu_peers) = spy(
spy_ref.clone(),
⋮----
exit.store(true, Ordering::Relaxed);
gossip_service.join().unwrap();
⋮----
info!(
⋮----
return Ok((all_peers, tvu_peers));
⋮----
if !tvu_peers.is_empty() {
⋮----
info!("discover failed...\n{}", spy_ref.contact_info_trace());
Err(std::io::Error::other("Discover failed"))
⋮----
pub fn get_client(
⋮----
let select = thread_rng().gen_range(0..nodes.len());
let rpc_pubsub_url = format!("ws://{}/", nodes[select].rpc_pubsub().unwrap());
let rpc_url = format!("http://{}", nodes[select].rpc().unwrap());
⋮----
rpc_pubsub_url.as_str(),
⋮----
cache.clone(),
⋮----
.unwrap_or_else(|err| {
panic!("Could not create TpuClient with Quic Cache {err:?}");
⋮----
panic!("Could not create TpuClient with Udp Cache {err:?}");
⋮----
fn spy(
⋮----
while !met_criteria && now.elapsed() < timeout {
⋮----
.all_peers()
.into_iter()
.map(|x| x.0)
⋮----
tvu_peers = spy_ref.tvu_peers(ContactInfo::clone);
⋮----
.iter()
.all(|pubkey| all_peers.iter().any(|node| node.pubkey() == pubkey))
⋮----
let found_nodes_by_gossip_addr = !find_nodes_by_gossip_addr.is_empty()
&& find_nodes_by_gossip_addr.iter().all(|node_addr| {
⋮----
.any(|peer| (*peer).gossip() == Some(*node_addr))
⋮----
let mut nodes: Vec<ContactInfo> = tvu_peers.clone();
nodes.sort_unstable_by_key(|node| *node.pubkey());
nodes.dedup();
if nodes.len() >= num {
⋮----
if find_nodes_by_pubkey.is_none() && find_nodes_by_gossip_addr.is_empty() {
⋮----
info!("discovering...\n{}", spy_ref.contact_info_trace());
⋮----
sleep(Duration::from_millis(
⋮----
(met_criteria, now.elapsed(), all_peers, tvu_peers)
⋮----
pub fn make_node(
⋮----
ClusterInfo::gossip_node(keypair.pubkey(), gossip_addr, shred_version)
⋮----
ClusterInfo::spy_node(keypair.pubkey(), shred_version)
⋮----
cluster_info.set_entrypoints(
⋮----
.map(ContactInfo::new_gossip_entry_point)
⋮----
mod tests {
⋮----
fn test_exit() {
⋮----
let tn = Node::new_localhost_with_pubkey(&kp.pubkey());
⋮----
ClusterInfo::new(tn.info.clone(), Arc::new(kp), SocketAddrSpace::Unspecified);
⋮----
d.join().unwrap();
⋮----
fn test_gossip_services_spy() {
⋮----
let contact_info = ContactInfo::new_localhost(&keypair.pubkey(), 0);
⋮----
cluster_info.insert_info(peer0_info.clone());
cluster_info.insert_info(peer1_info);
⋮----
let (met_criteria, elapsed, _, tvu_peers) = spy(spy_ref.clone(), None, TIMEOUT, None, &[]);
assert!(!met_criteria);
assert!((TIMEOUT..TIMEOUT + Duration::from_secs(1)).contains(&elapsed));
assert_eq!(tvu_peers, spy_ref.tvu_peers(ContactInfo::clone));
let (met_criteria, _, _, _) = spy(spy_ref.clone(), Some(1), TIMEOUT, None, &[]);
assert!(met_criteria);
let (met_criteria, _, _, _) = spy(spy_ref.clone(), Some(2), TIMEOUT, None, &[]);
⋮----
let (met_criteria, _, _, _) = spy(spy_ref.clone(), None, TIMEOUT, Some(&[peer0]), &[]);
⋮----
let (met_criteria, _, _, _) = spy(
⋮----
Some(&[solana_pubkey::new_rand()]),
⋮----
let (met_criteria, _, _, _) = spy(spy_ref.clone(), Some(1), TIMEOUT, Some(&[peer0]), &[]);
⋮----
let (met_criteria, _, _, _) = spy(spy_ref.clone(), Some(3), TIMEOUT, Some(&[peer0]), &[]);
⋮----
Some(1),
⋮----
&[peer0_info.gossip().unwrap()],
⋮----
&["1.1.1.1:1234".parse().unwrap()],

================
File: gossip/src/legacy_contact_info.rs
================
pub(crate) struct LegacyContactInfo {
⋮----
reject_deserialize!(LegacyContactInfo, "LegacyContactInfo is deprecated");
impl Sanitize for LegacyContactInfo {
fn sanitize(&self) -> std::result::Result<(), SanitizeError> {
Err(SanitizeError::ValueOutOfBounds)
⋮----
impl Default for LegacyContactInfo {
fn default() -> Self {
⋮----
gossip: socketaddr_any!(),
tvu: socketaddr_any!(),
tvu_quic: socketaddr_any!(),
serve_repair_quic: socketaddr_any!(),
tpu: socketaddr_any!(),
tpu_forwards: socketaddr_any!(),
tpu_vote: socketaddr_any!(),
rpc: socketaddr_any!(),
rpc_pubsub: socketaddr_any!(),
serve_repair: socketaddr_any!(),
⋮----
impl LegacyContactInfo {
⋮----
pub(crate) fn pubkey(&self) -> &Pubkey {
⋮----
pub(crate) fn wallclock(&self) -> u64 {

================
File: gossip/src/lib.rs
================
pub mod cluster_info;
pub mod cluster_info_metrics;
pub mod contact_info;
pub mod crds;
pub mod crds_data;
pub mod crds_entry;
mod crds_filter;
pub mod crds_gossip;
pub mod crds_gossip_error;
pub mod crds_gossip_pull;
pub mod crds_gossip_push;
pub mod crds_shards;
pub mod crds_value;
mod deprecated;
pub mod duplicate_shred;
pub mod duplicate_shred_handler;
pub mod duplicate_shred_listener;
pub mod epoch_slots;
pub mod epoch_specs;
pub mod gossip_error;
pub mod gossip_service;
pub mod node;
⋮----
mod tlv;
⋮----
mod legacy_contact_info;
pub mod ping_pong;
mod protocol;
mod push_active_set;
mod received_cache;
pub mod restart_crds_values;
pub mod weighted_shuffle;
⋮----
extern crate log;
⋮----
extern crate assert_matches;
⋮----
extern crate solana_frozen_abi_macro;
⋮----
extern crate solana_metrics;
mod wire_format_tests;

================
File: gossip/src/node.rs
================
pub struct MultihomingAddresses {
⋮----
pub struct Node {
⋮----
impl Node {
pub fn new_localhost() -> Self {
⋮----
pub fn new_localhost_with_pubkey(pubkey: &Pubkey) -> Self {
let port_range = localhost_port_range_for_tests();
⋮----
bind_ip_addrs: BindIpAddrs::new(vec![bind_ip_addr]).expect("should bind"),
⋮----
num_tvu_receive_sockets: NonZero::new(1).unwrap(),
num_tvu_retransmit_sockets: NonZero::new(1).unwrap(),
⋮----
.expect("Number of QUIC endpoints can not be zero"),
⋮----
let rpc_ports: [u16; 2] = find_available_ports_in_range(bind_ip_addr, port_range).unwrap();
⋮----
node.info.set_rpc(rpc_addr).unwrap();
node.info.set_rpc_pubsub(rpc_pubsub_addr).unwrap();
⋮----
pub fn new_with_external_ip(pubkey: &Pubkey, config: NodeConfig) -> Node {
⋮----
let bind_ip_addr = bind_ip_addrs.active();
let mut gossip_sockets = Vec::with_capacity(bind_ip_addrs.len());
let mut gossip_ports = Vec::with_capacity(bind_ip_addrs.len());
let mut ip_echo_sockets = Vec::with_capacity(bind_ip_addrs.len());
for ip in bind_ip_addrs.iter() {
⋮----
bind_gossip_port_in_range(&gossip_addr, port_range, *ip);
gossip_sockets.push(gossip);
gossip_ports.push(port);
ip_echo_sockets.push(ip_echo);
⋮----
let (tvu_port, mut tvu_sockets) = multi_bind_in_range_with_config(
⋮----
num_tvu_receive_sockets.get(),
⋮----
.expect("tvu multi_bind");
tvu_sockets.append(
⋮----
.expect("Secondary bind TVU"),
⋮----
bind_in_range_with_config(bind_ip_addr, port_range, socket_config)
.expect("tvu_quic bind");
⋮----
.expect("tpu_socket primary bind");
⋮----
bind_more_with_config(tpu_socket, 32, socket_config).expect("tpu_sockets multi_bind");
⋮----
let mut tpu_quic = bind_more_with_config(tpu_quic, num_quic_endpoints.get(), socket_config)
.expect("tpu_quic bind");
tpu_quic.append(
⋮----
.expect("Secondary bind TPU QUIC"),
⋮----
.expect("tpu_forwards primary bind");
let tpu_forwards_sockets = bind_more_with_config(tpu_forwards_socket, 8, socket_config)
.expect("tpu_forwards multi_bind");
⋮----
bind_more_with_config(tpu_forwards_quic, num_quic_endpoints.get(), socket_config)
.expect("tpu_forwards_quic multi_bind");
tpu_forwards_quic.append(
⋮----
num_quic_endpoints.get(),
⋮----
.expect("Secondary bind TPU forwards"),
⋮----
multi_bind_in_range_with_config(bind_ip_addr, port_range, socket_config, 1)
.expect("tpu_vote multi_bind");
tpu_vote_sockets.extend(
⋮----
.expect("Secondary binds for tpu vote"),
⋮----
.expect("tpu_vote_quic");
⋮----
bind_more_with_config(tpu_vote_quic, num_quic_endpoints.get(), socket_config)
.expect("tpu_vote_quic multi_bind");
tpu_vote_quic.append(
⋮----
.expect("Secondary bind TPU vote"),
⋮----
let (tvu_retransmit_port, mut retransmit_sockets) = multi_bind_in_range_with_config(
⋮----
num_tvu_retransmit_sockets.get(),
⋮----
.expect("tvu retransmit multi_bind");
retransmit_sockets.append(
⋮----
.expect("Secondary bind TVU retransmit"),
⋮----
let (_, repair) = bind_in_range_with_config(bind_ip_addr, port_range, socket_config)
.expect("repair bind");
let (_, repair_quic) = bind_in_range_with_config(bind_ip_addr, port_range, socket_config)
.expect("repair_quic bind");
⋮----
.expect("serve_repair");
⋮----
.expect("serve_repair_quic");
⋮----
multi_bind_in_range_with_config(bind_ip_addr, port_range, socket_config, 4)
.expect("broadcast multi_bind");
broadcast.append(
⋮----
.expect("Secondary bind broadcast"),
⋮----
.expect("ancestor_hashes_requests bind");
⋮----
.expect("ancestor_hashes_requests QUIC bind should succeed");
⋮----
.expect("Alpenglow port bind should succeed");
⋮----
bind_in_range_with_config(bind_ip_addr, port_range, socket_config).unwrap();
⋮----
bind_in_range_with_config(bind_ip_addr, port_range, socket_config).expect(
⋮----
let tpu_transaction_forwarding_clients = once(tpu_transaction_forwarding_clients)
.chain(
⋮----
.expect("Secondary interface binds for tpu forward clients should succeed"),
⋮----
.collect();
⋮----
timestamp(),
⋮----
info.set_gossip((advertised_ip, gossip_ports[0])).unwrap();
info.set_tvu(
⋮----
public_tvu_addr.unwrap_or_else(|| SocketAddr::new(advertised_ip, tvu_port)),
⋮----
.unwrap();
info.set_tvu(QUIC, (advertised_ip, tvu_quic_port)).unwrap();
info.set_tpu(UDP, (advertised_ip, tpu_port)).unwrap();
info.set_tpu(
⋮----
public_tpu_addr.unwrap_or_else(|| SocketAddr::new(advertised_ip, tpu_port_quic)),
⋮----
info.set_tpu_forwards(UDP, (advertised_ip, tpu_forwards_port))
⋮----
info.set_tpu_forwards(
⋮----
.unwrap_or_else(|| SocketAddr::new(advertised_ip, tpu_forwards_port)),
⋮----
info.set_tpu_vote(UDP, (advertised_ip, tpu_vote_port))
⋮----
info.set_tpu_vote(QUIC, (advertised_ip, tpu_vote_quic_port))
⋮----
info.set_serve_repair(UDP, (advertised_ip, serve_repair_port))
⋮----
info.set_alpenglow((advertised_ip, alpenglow_port)).unwrap();
info.set_serve_repair(QUIC, (advertised_ip, serve_repair_quic_port))
⋮----
let vortexor_receivers = vortexor_receiver_addr.map(|vortexor_receiver_addr| {
multi_bind_in_range_with_config(
vortexor_receiver_addr.ip(),
⋮----
vortexor_receiver_addr.port(),
vortexor_receiver_addr.port() + 1,
⋮----
.unwrap_or_else(|_| {
panic!("Could not bind to the set vortexor_receiver_addr {vortexor_receiver_addr}")
⋮----
info!("vortexor_receivers is {vortexor_receivers:?}");
trace!("new ContactInfo: {info:?}");
⋮----
alpenglow: Some(alpenglow),
gossip: gossip_sockets.into_iter().collect(),
⋮----
ip_echo: ip_echo_sockets.into_iter().next(),
⋮----
info!("Bound all network sockets as follows: {:#?}", &sockets);
⋮----
fn get_socket_addrs(sockets: &[UdpSocket]) -> Box<[SocketAddr]> {
⋮----
let addr = socket.local_addr().unwrap();
if seen.insert(addr) {
addresses.push(addr);
⋮----
addresses.into()
⋮----
fn bind_to_extra_ip(
⋮----
let active_ip_addr = bind_ip_addrs.active();
let mut sockets = vec![];
⋮----
.iter()
.cloned()
.filter(|&ip| ip != active_ip_addr)
⋮----
let socket = bind_to_with_config(ip_addr, port, socket_config)?;
sockets.append(&mut bind_more_with_config(socket, num, socket_config)?);
⋮----
Ok(sockets)
⋮----
mod multihoming {
⋮----
pub struct NodeMultihoming {
⋮----
impl NodeMultihoming {
pub fn switch_active_interface(
⋮----
if self.bind_ip_addrs.active() == interface {
return Err(String::from("Specified interface already selected"));
⋮----
.position(|&e| e == interface)
.ok_or_else(|| {
⋮----
format!(
⋮----
.local_addr()
.map_err(|e| e.to_string())?;
⋮----
.set_gossip_socket(gossip_addr)
⋮----
.set_tvu_socket(tvu_ingress_address)
⋮----
.set_tpu_quic(tpu_quic_address)
⋮----
.set_tpu_forwards_quic(tpu_forwards_quic_address)
⋮----
.set_tpu_vote(QUIC, tpu_vote_quic_address)
⋮----
.set_tpu_vote(UDP, tpu_vote_address)
⋮----
.set_active(interface_index)
.expect("Interface index out of range");
Ok(())
⋮----
fn from(node: &Node) -> Self {
⋮----
gossip_socket: node.sockets.gossip.clone(),
addresses: node.addresses.clone(),
bind_ip_addrs: node.bind_ip_addrs.clone(),

================
File: gossip/src/ping_pong.rs
================
const PING_PONG_HASH_PREFIX: &[u8] = "SOLANA_PING_PONG".as_bytes();
⋮----
pub struct Ping<const N: usize> {
⋮----
pub struct Pong {
⋮----
pub struct PingCache<const N: usize> {
⋮----
pub fn new(token: [u8; N], keypair: &Keypair) -> Self {
let signature = keypair.sign_message(&token);
⋮----
from: keypair.pubkey(),
⋮----
impl<const N: usize> Sanitize for Ping<N> {
fn sanitize(&self) -> Result<(), SanitizeError> {
self.from.sanitize()?;
self.signature.sanitize()
⋮----
impl<const N: usize> Signable for Ping<N> {
⋮----
fn pubkey(&self) -> Pubkey {
⋮----
fn signable_data(&self) -> Cow<'_, [u8]> {
⋮----
fn get_signature(&self) -> Signature {
⋮----
fn set_signature(&mut self, signature: Signature) {
⋮----
impl Pong {
pub fn new<const N: usize>(ping: &Ping<N>, keypair: &Keypair) -> Self {
let hash = hash_ping_token(&ping.token);
⋮----
signature: keypair.sign_message(hash.as_ref()),
⋮----
pub fn from(&self) -> &Pubkey {
⋮----
pub(crate) fn signature(&self) -> &Signature {
⋮----
impl Sanitize for Pong {
⋮----
self.hash.sanitize()?;
⋮----
impl Signable for Pong {
⋮----
fn signable_data(&self) -> Cow<'static, [u8]> {
Cow::Owned(self.hash.as_ref().into())
⋮----
pub fn new<R: Rng + CryptoRng>(
⋮----
assert!(rate_limit_delay <= ttl / 2);
⋮----
hashers: std::array::from_fn(|_| SipHasher24::new_with_key(&rng.gen())),
⋮----
pub fn add(&mut self, pong: &Pong, socket: SocketAddr, now: Instant) -> bool {
let remote_node = (pong.pubkey(), socket);
if !self.hashers.iter().copied().any(|hasher| {
⋮----
hash_ping_token(&token) == pong.hash
⋮----
self.pongs.put(remote_node, now);
if let Some(sent_time) = self.ping_times.pop(&socket.ip()) {
if should_report_message_signature(
pong.signature(),
⋮----
let rtt = now.saturating_duration_since(sent_time);
datapoint_info!(
⋮----
fn maybe_ping<R: Rng + CryptoRng>(
⋮----
if matches!(self.pings.peek(&remote_node),
⋮----
self.pings.put(remote_node, now);
self.maybe_refresh_key(rng, now);
⋮----
self.ping_times.put(remote_node.1.ip(), Instant::now());
Some(Ping::new(token, keypair))
⋮----
pub fn check<R: Rng + CryptoRng>(
⋮----
let (check, should_ping) = match self.pongs.get(&remote_node) {
⋮----
let age = now.saturating_duration_since(*t);
⋮----
self.pongs.pop(&remote_node);
⋮----
.then(|| self.maybe_ping(rng, keypair, now, remote_node))
.flatten();
⋮----
fn maybe_refresh_key<R: Rng + CryptoRng>(&mut self, rng: &mut R, now: Instant) {
if now.checked_duration_since(self.key_refresh) > Some(KEY_REFRESH_CADENCE) {
let hasher = SipHasher24::new_with_key(&rng.gen());
⋮----
pub fn mock_pong(&mut self, node: Pubkey, socket: SocketAddr, now: Instant) {
self.pongs.put((node, socket), now);
⋮----
fn make_ping_token<const N: usize>(
⋮----
remote_node.hash(&mut hasher);
let hash = hasher.finish().to_le_bytes();
debug_assert!(N >= std::mem::size_of::<u64>());
⋮----
token[..std::mem::size_of::<u64>()].copy_from_slice(&hash);
⋮----
fn hash_ping_token<const N: usize>(token: &[u8; N]) -> Hash {
⋮----
mod tests {
⋮----
fn test_ping_pong() {
⋮----
let ping = Ping::<32>::new(rng.gen(), &keypair);
assert!(ping.verify());
assert!(ping.sanitize().is_ok());
⋮----
assert!(pong.verify());
assert!(pong.sanitize().is_ok());
assert_eq!(
⋮----
fn test_ping_cache() {
⋮----
let keypairs: Vec<_> = repeat_with(Keypair::new).take(8).collect();
let sockets: Vec<_> = repeat_with(|| {
⋮----
Ipv4Addr::new(rng.gen(), rng.gen(), rng.gen(), rng.gen()),
rng.gen(),
⋮----
.take(8)
.collect();
let remote_nodes: Vec<(&Keypair, SocketAddr)> = repeat_with(|| {
let keypair = &keypairs[rng.gen_range(0..keypairs.len())];
let socket = sockets[rng.gen_range(0..sockets.len())];
⋮----
.take(128)
⋮----
.iter()
.map(|(keypair, socket)| {
let node = (keypair.pubkey(), *socket);
let (check, ping) = cache.check(&mut rng, &this_node, now, node);
assert!(!check);
assert_eq!(seen_nodes.insert(node), ping.is_some());
⋮----
for ((keypair, socket), ping) in remote_nodes.iter().zip(&pings) {
⋮----
assert!(check);
assert!(ping.is_none());
⋮----
assert!(cache.add(&pong, *socket, now));
⋮----
seen_nodes.clear();
⋮----
if seen_nodes.insert(node) {
⋮----
assert!(ping.is_some());

================
File: gossip/src/protocol.rs
================
pub(crate) enum Protocol {
⋮----
pub(crate) type Ping = ping_pong::Ping<GOSSIP_PING_TOKEN_SIZE>;
pub(crate) type PingCache = ping_pong::PingCache<GOSSIP_PING_TOKEN_SIZE>;
⋮----
pub(crate) struct PruneData {
⋮----
impl Protocol {
⋮----
fn bincode_serialized_size(&self) -> usize {
⋮----
.map(usize::try_from)
.unwrap()
⋮----
pub(crate) fn verify(&self) -> bool {
⋮----
Self::PullRequest(_, caller) => caller.verify(),
Self::PullResponse(_, data) => data.iter().all(CrdsValue::verify),
Self::PushMessage(_, data) => data.iter().all(CrdsValue::verify),
Self::PruneMessage(_, data) => data.verify(),
Self::PingMessage(ping) => ping.verify(),
Self::PongMessage(pong) => pong.verify(),
⋮----
impl PruneData {
fn signable_data_without_prefix(&self) -> Cow<'static, [u8]> {
⋮----
struct SignData<'a> {
⋮----
Cow::Owned(serialize(&data).expect("should serialize PruneData"))
⋮----
fn signable_data_with_prefix(&self) -> Cow<'static, [u8]> {
⋮----
struct SignDataWithPrefix<'a> {
⋮----
Cow::Owned(serialize(&data).expect("should serialize PruneDataWithPrefix"))
⋮----
fn verify_data(&self, use_prefix: bool) -> bool {
⋮----
self.signable_data_without_prefix()
⋮----
self.signable_data_with_prefix()
⋮----
self.get_signature()
.verify(self.pubkey().as_ref(), data.borrow())
⋮----
impl Sanitize for Protocol {
fn sanitize(&self) -> Result<(), SanitizeError> {
⋮----
filter.sanitize()?;
// PullRequest is only allowed to have ContactInfo in its CrdsData
match val.data() {
CrdsData::ContactInfo(_) => val.sanitize(),
_ => Err(SanitizeError::InvalidValue),
⋮----
// PullResponse is allowed to carry anything in its CrdsData, including deprecated Crds
// such that a deprecated Crds does not get pulled and then rejected.
val.sanitize()
⋮----
// PushMessage is allowed to carry anything in its CrdsData, including deprecated Crds
// such that a deprecated Crds gets ingested instead of the node having to pull it from
// other nodes that have inserted it into their Crds table
⋮----
Err(SanitizeError::InvalidValue)
⋮----
Protocol::PingMessage(ping) => ping.sanitize(),
Protocol::PongMessage(pong) => pong.sanitize(),
⋮----
impl Sanitize for PruneData {
⋮----
return Err(SanitizeError::ValueOutOfBounds);
⋮----
Ok(())
⋮----
impl Signable for PruneData {
fn pubkey(&self) -> Pubkey {
⋮----
fn signable_data(&self) -> Cow<'static, [u8]> {
⋮----
fn get_signature(&self) -> Signature {
⋮----
fn set_signature(&mut self, signature: Signature) {
⋮----
fn verify(&self) -> bool {
self.verify_data(false) || self.verify_data(true)
⋮----
pub(crate) fn split_gossip_messages<T: Serialize + Debug>(
⋮----
let mut data_feed = data_feed.into_iter().fuse();
let mut buffer = vec![];
⋮----
let Some(data) = data_feed.next() else {
return (!buffer.is_empty()).then(|| std::mem::take(&mut buffer));
⋮----
error!("serialized_size failed: {err:?}");
⋮----
buffer.push(data);
⋮----
return Some(std::mem::replace(&mut buffer, vec![data]));
⋮----
error!("dropping data larger than the maximum chunk size {data:?}",);
⋮----
pub(crate) mod tests {
⋮----
fn new_rand_socket_addr<R: Rng>(rng: &mut R) -> SocketAddr {
let addr = if rng.gen_bool(0.5) {
IpAddr::V4(Ipv4Addr::new(rng.gen(), rng.gen(), rng.gen(), rng.gen()))
⋮----
rng.gen(),
⋮----
SocketAddr::new(addr,  rng.gen())
⋮----
pub(crate) fn new_rand_remote_node<R>(rng: &mut R) -> (Keypair, SocketAddr)
⋮----
let socket = new_rand_socket_addr(rng);
⋮----
fn new_rand_prune_data<R: Rng>(
⋮----
let num_nodes = num_nodes.unwrap_or_else(|| rng.gen_range(0..MAX_PRUNE_DATA_NODES + 1));
⋮----
.take(num_nodes)
.collect();
⋮----
pubkey: self_keypair.pubkey(),
⋮----
prune_data.sign(self_keypair);
⋮----
fn test_max_accounts_hashes_with_push_messages() {
⋮----
let message = Protocol::PushMessage(Pubkey::new_unique(), vec![crds_value]);
let socket = new_rand_socket_addr(&mut rng);
assert!(Packet::from_data(Some(&socket), message).is_ok());
⋮----
fn test_max_accounts_hashes_with_pull_responses() {
⋮----
let response = Protocol::PullResponse(Pubkey::new_unique(), vec![crds_value]);
⋮----
assert!(Packet::from_data(Some(&socket), response).is_ok());
⋮----
fn test_max_snapshot_hashes_with_push_messages() {
⋮----
incremental: vec![(Slot::default(), Hash::default()); MAX_INCREMENTAL_SNAPSHOT_HASHES],
wallclock: timestamp(),
⋮----
fn test_max_snapshot_hashes_with_pull_responses() {
⋮----
fn test_max_prune_data_pubkeys() {
⋮----
new_rand_prune_data(&mut rng, &self_keypair, Some(MAX_PRUNE_DATA_NODES));
let prune_message = Protocol::PruneMessage(self_keypair.pubkey(), prune_data);
⋮----
assert!(Packet::from_data(Some(&socket), prune_message).is_ok());
⋮----
new_rand_prune_data(&mut rng, &self_keypair, Some(MAX_PRUNE_DATA_NODES + 1));
⋮----
assert!(Packet::from_data(Some(&socket), prune_message).is_err());
⋮----
fn test_push_message_max_payload_size() {
⋮----
assert_eq!(
⋮----
fn test_pull_response_max_payload_size() {
⋮----
fn test_duplicate_shred_max_payload_size() {
⋮----
let shredder = Shredder::new(slot, parent_slot, reference_tick, version).unwrap();
let next_shred_index = rng.gen_range(0..32_000);
let shred = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
⋮----
let other_shred = new_rand_shred(&mut rng, next_shred_index, &shredder, &leader);
other_shred.into_payload()
⋮----
Some(leader.pubkey())
⋮----
keypair.pubkey(),
⋮----
Some(leader_schedule),
timestamp(),
⋮----
assert!(chunks.len() > 1);
⋮----
let pull_response = Protocol::PullResponse(keypair.pubkey(), vec![value.clone()]);
assert!(pull_response.bincode_serialized_size() < PACKET_DATA_SIZE);
let push_message = Protocol::PushMessage(keypair.pubkey(), vec![value.clone()]);
assert!(push_message.bincode_serialized_size() < PACKET_DATA_SIZE);
⋮----
fn test_pull_response_min_serialized_size() {
⋮----
let crds_values = vec![CrdsValue::new_rand(&mut rng, None)];
⋮----
let size = pull_response.bincode_serialized_size();
assert!(
⋮----
fn test_split_messages_small() {
⋮----
test_split_messages(value);
⋮----
fn test_split_messages_large() {
⋮----
fn test_split_gossip_messages() {
⋮----
let values: Vec<_> = repeat_with(|| CrdsValue::new_rand(&mut rng, None))
.take(NUM_CRDS_VALUES)
⋮----
split_gossip_messages(PUSH_MESSAGE_MAX_PAYLOAD_SIZE, values.clone()).collect();
⋮----
assert!(splits.len() * 2 < NUM_CRDS_VALUES);
assert_eq!(NUM_CRDS_VALUES, splits.iter().map(Vec::len).sum::<usize>());
⋮----
.iter()
.flat_map(|s| s.iter())
.zip(values)
.for_each(|(a, b)| assert_eq!(*a, b));
⋮----
Ipv4Addr::new(rng.gen(), rng.gen(), rng.gen(), rng.gen()),
⋮----
.map(CrdsValue::bincode_serialized_size)
⋮----
assert_eq!(message.bincode_serialized_size(), size);
⋮----
fn test_split_gossip_messages_pull_response() {
⋮----
split_gossip_messages(PULL_RESPONSE_MAX_PAYLOAD_SIZE, values.clone()).collect();
⋮----
fn test_split_messages_packet_size() {
⋮----
hashes: vec![],
⋮----
while value.bincode_serialized_size() < PUSH_MESSAGE_MAX_PAYLOAD_SIZE {
⋮----
hashes: vec![(0, Hash::default()); i],
⋮----
split_gossip_messages(PUSH_MESSAGE_MAX_PAYLOAD_SIZE, vec![value]).collect();
assert_eq!(split.len(), 0);
⋮----
fn test_split_messages(value: CrdsValue) {
⋮----
let value_size = value.bincode_serialized_size();
let num_values_per_payload = (PUSH_MESSAGE_MAX_PAYLOAD_SIZE / value_size).max(1);
let expected_len = NUM_VALUES.div_ceil(num_values_per_payload);
let msgs = vec![value; NUM_VALUES];
assert!(split_gossip_messages(PUSH_MESSAGE_MAX_PAYLOAD_SIZE, msgs).count() <= expected_len);
⋮----
fn test_protocol_sanitize() {
⋮----
assert_eq!(msg.sanitize(), Err(SanitizeError::ValueOutOfBounds));
⋮----
fn test_protocol_prune_message_sanitize() {
⋮----
pubkey: keypair.pubkey(),
prunes: vec![],
⋮----
prune_data.sign(&keypair);
let prune_message = Protocol::PruneMessage(keypair.pubkey(), prune_data.clone());
assert_eq!(prune_message.sanitize(), Ok(()));
⋮----
assert_eq!(prune_message.sanitize(), Err(SanitizeError::InvalidValue));
⋮----
fn test_vote_size() {
let slots = vec![1; 32];
⋮----
&keypair.pubkey(),
⋮----
let mut vote_tx = Transaction::new_with_payer(&[vote_ix], Some(&keypair.pubkey()));
vote_tx.partial_sign(&[keypair.as_ref()], Hash::default());
⋮----
.unwrap();
⋮----
assert!(vote.bincode_serialized_size() <= PUSH_MESSAGE_MAX_PAYLOAD_SIZE);
⋮----
fn test_prune_data_sign_and_verify_without_prefix() {
⋮----
let mut prune_data = new_rand_prune_data(&mut rng, &keypair, Some(3));
⋮----
let is_valid = prune_data.verify();
assert!(is_valid, "Signature should be valid without prefix");
⋮----
fn test_prune_data_sign_and_verify_with_prefix() {
⋮----
let prefixed_data = prune_data.signable_data_with_prefix();
let signature_with_prefix = keypair.sign_message(prefixed_data.borrow());
prune_data.set_signature(signature_with_prefix);
⋮----
assert!(is_valid, "Signature should be valid with prefix");
⋮----
fn test_prune_data_verify_with_and_without_prefix() {
⋮----
let is_valid_non_prefixed = prune_data.verify();
⋮----
let non_prefixed_data = prune_data.signable_data_without_prefix().into_owned();
⋮----
let is_valid_prefixed = prune_data.verify();
assert!(is_valid_prefixed, "Signature should be valid with prefix");
⋮----
assert_ne!(

================
File: gossip/src/push_active_set.rs
================
pub(crate) struct PushActiveSet([PushActiveSetEntry; NUM_PUSH_ACTIVE_SET_ENTRIES]);
⋮----
struct PushActiveSetEntry(IndexMap< Pubkey,  ConcurrentBloom<Pubkey>>);
impl PushActiveSet {
⋮----
pub(crate) fn get_nodes<'a>(
⋮----
pubkey: &'a Pubkey, // This node.
⋮----
let stake = stakes.get(pubkey).min(stakes.get(origin));
self.get_entry(stake).get_nodes(pubkey, origin)
⋮----
// Prunes origins for the given gossip node.
// We will stop pushing messages from the specified origins to the node.
pub(crate) fn prune(
⋮----
pubkey: &Pubkey,    // This node.
node: &Pubkey,      // Gossip node.
origins: &[Pubkey], // CRDS value owners.
⋮----
let stake = stakes.get(pubkey);
⋮----
let stake = stake.min(stakes.get(origin));
self.get_entry(stake).prune(node, origin)
⋮----
pub(crate) fn rotate<R: Rng>(
⋮----
size: usize, // Number of nodes to retain in each active-set entry.
⋮----
// Gossip nodes to be sampled for each push active set.
⋮----
let num_bloom_filter_items = cluster_size.max(Self::MIN_NUM_BLOOM_ITEMS);
// Active set of nodes to push to are sampled from these gossip nodes,
// using sampling probabilities obtained from the stake bucket of each
// node.
⋮----
.iter()
.map(|node| get_stake_bucket(stakes.get(node)))
.collect();
// (k, entry) represents push active set where the stake bucket of
//     min stake of {this node, crds value owner}
// is equal to `k`. The `entry` maintains set of gossip nodes to
// actively push to for crds values belonging to this bucket.
for (k, entry) in self.0.iter_mut().enumerate() {
⋮----
.map(|&bucket| {
// bucket <- get_stake_bucket(min stake of {
//  this node, crds value owner and gossip peer
// })
// weight <- (bucket + 1)^2
// min stake of {...} is a proxy for how much we care about
// the link, and tries to mirror similar logic on the
// receiving end when pruning incoming links:
// https://github.com/solana-labs/solana/blob/81394cf92/gossip/src/received_cache.rs#L100-L105
let bucket = bucket.min(k) as u64;
bucket.saturating_add(1).saturating_pow(2)
⋮----
entry.rotate(rng, size, num_bloom_filter_items, nodes, &weights);
⋮----
fn get_entry(&self, stake: Option<&u64>) -> &PushActiveSetEntry {
&self.0[get_stake_bucket(stake)]
⋮----
impl PushActiveSetEntry {
⋮----
fn get_nodes<'a>(
⋮----
origin: &'a Pubkey, // CRDS value owner.
⋮----
.filter(move |(node, bloom_filter)| {
!bloom_filter.contains(origin) || (pubkey_eq_origin && &pubkey != node)
⋮----
.map(|(node, _bloom_filter)| node)
⋮----
fn prune(
⋮----
if let Some(bloom_filter) = self.0.get(node) {
bloom_filter.add(origin);
⋮----
fn rotate<R: Rng>(
⋮----
debug_assert_eq!(nodes.len(), weights.len());
debug_assert!(weights.iter().all(|&weight| weight != 0u64));
⋮----
for node in weighted_shuffle.shuffle(rng).map(|k| &nodes[k]) {
if self.0.len() > size {
⋮----
if self.0.contains_key(node) {
⋮----
bloom.add(node);
self.0.insert(*node, bloom);
⋮----
while self.0.len() > size {
self.0.shift_remove_index(0);
⋮----
fn get_stake_bucket(stake: Option<&u64>) -> usize {
let stake = stake.copied().unwrap_or_default() / LAMPORTS_PER_SOL;
let bucket = u64::BITS - stake.leading_zeros();
(bucket as usize).min(NUM_PUSH_ACTIVE_SET_ENTRIES - 1)
⋮----
mod tests {
⋮----
fn test_get_stake_bucket() {
assert_eq!(get_stake_bucket(None), 0);
⋮----
for (k, bucket) in buckets.into_iter().enumerate() {
⋮----
assert_eq!(get_stake_bucket(Some(&stake)), bucket);
⋮----
assert_eq!(
⋮----
fn test_push_active_set() {
⋮----
let nodes: Vec<_> = repeat_with(Pubkey::new_unique).take(20).collect();
let stakes = repeat_with(|| rng.gen_range(1..MAX_STAKE));
let mut stakes: HashMap<_, _> = nodes.iter().copied().zip(stakes).collect();
stakes.insert(pubkey, rng.gen_range(1..MAX_STAKE));
⋮----
assert!(active_set.0.iter().all(|entry| entry.0.is_empty()));
active_set.rotate(&mut rng, 5, CLUSTER_SIZE, &nodes, &stakes);
assert!(active_set.0.iter().all(|entry| entry.0.len() == 5));
⋮----
for (node, filter) in entry.0.iter() {
assert!(filter.contains(node));
⋮----
assert!(active_set
⋮----
active_set.prune(&pubkey, &nodes[5], &[*origin], &stakes);
active_set.prune(&pubkey, &nodes[3], &[*origin], &stakes);
active_set.prune(&pubkey, &nodes[16], &[*origin], &stakes);
⋮----
active_set.rotate(&mut rng, 7, CLUSTER_SIZE, &nodes, &stakes);
assert!(active_set.0.iter().all(|entry| entry.0.len() == 7));
⋮----
active_set.prune(&pubkey, &nodes[18], &origins, &stakes);
active_set.prune(&pubkey, &nodes[0], &origins, &stakes);
active_set.prune(&pubkey, &nodes[15], &origins, &stakes);
⋮----
fn test_push_active_set_entry() {
⋮----
let weights: Vec<_> = repeat_with(|| rng.gen_range(1..1000)).take(20).collect();
⋮----
entry.rotate(
⋮----
assert_eq!(entry.0.len(), 5);
⋮----
assert!(entry.0.keys().eq(keys));
for (pubkey, origin) in iproduct!(&nodes, &nodes) {
if !keys.contains(&origin) {
assert!(entry.get_nodes(pubkey, origin).eq(keys));
⋮----
assert!(entry
⋮----
for (pubkey, origin) in iproduct!(&nodes, keys) {
⋮----
entry.prune(&nodes[11], origin);
entry.prune(&nodes[14], origin);
entry.prune(&nodes[19], origin);
⋮----
assert!(entry.get_nodes(pubkey, origin).eq(keys
⋮----
entry.rotate(&mut rng, 5, NUM_BLOOM_FILTER_ITEMS, &nodes, &weights);
⋮----
entry.rotate(&mut rng, 6, NUM_BLOOM_FILTER_ITEMS, &nodes, &weights);
⋮----
entry.rotate(&mut rng, 4, NUM_BLOOM_FILTER_ITEMS, &nodes, &weights);

================
File: gossip/src/received_cache.rs
================
pub(crate) struct ReceivedCache(LruCache< Pubkey, ReceivedCacheEntry>);
⋮----
struct ReceivedCacheEntry {
⋮----
impl ReceivedCache {
⋮----
pub(crate) fn new(capacity: usize) -> Self {
Self(LruCache::new(capacity))
⋮----
pub(crate) fn record(&mut self, origin: Pubkey, node: Pubkey, num_dups: usize) {
match self.0.get_mut(&origin) {
Some(entry) => entry.record(node, num_dups),
⋮----
entry.record(node, num_dups);
self.0.put(origin, entry);
⋮----
pub(crate) fn prune(
⋮----
match self.0.peek_mut(&origin) {
⋮----
Some(entry) => Some(
⋮----
.prune(pubkey, &origin, stake_threshold, min_ingress_nodes, stakes)
.filter(move |node| node != &origin),
⋮----
.into_iter()
.flatten()
⋮----
fn mock_clone(&self) -> Self {
let mut cache = LruCache::new(self.0.cap());
for (&origin, entry) in self.0.iter().rev() {
cache.put(origin, entry.clone());
⋮----
Self(cache)
⋮----
impl ReceivedCacheEntry {
⋮----
fn record(&mut self, node: Pubkey, num_dups: usize) {
⋮----
self.num_upserts = self.num_upserts.saturating_add(1);
⋮----
let score = self.nodes.entry(node).or_default();
*score = score.saturating_add(1);
} else if self.nodes.len() < Self::CAPACITY {
let _ = self.nodes.entry(node).or_default();
⋮----
fn prune(
⋮----
debug_assert!((0.0..=1.0).contains(&stake_threshold));
debug_assert!(self.num_upserts >= ReceivedCache::MIN_NUM_UPSERTS);
⋮----
let stake = stakes.get(pubkey).min(stakes.get(origin));
(stake.copied().unwrap_or_default() as f64 * stake_threshold) as u64
⋮----
.map(|(node, score)| {
let stake = stakes.get(&node).copied().unwrap_or_default();
⋮----
.sorted_unstable_by_key(|&(_, score, stake)| Reverse((score, stake)))
.scan(0u64, |acc, (node, _score, stake)| {
⋮----
*acc = acc.saturating_add(stake);
Some((node, old))
⋮----
.skip(min_ingress_nodes)
.skip_while(move |&(_, stake)| stake < min_ingress_stake)
.map(|(node, _stake)| node)
⋮----
mod tests {
⋮----
fn test_received_cache() {
⋮----
let records = vec![
⋮----
let nodes: Vec<_> = repeat_with(Pubkey::new_unique)
.take(records.len())
.collect();
for (node, records) in nodes.iter().zip(records) {
for (num_dups, k) in records.into_iter().enumerate() {
⋮----
cache.record(origin, *node, num_dups);
⋮----
assert_eq!(cache.0.get(&origin).unwrap().num_upserts, 21);
⋮----
assert_eq!(cache.0.get(&origin).unwrap().nodes, scores);
⋮----
let prunes: HashSet<Pubkey> = [nodes[0], nodes[2], nodes[3]].into_iter().collect();
assert_eq!(
⋮----
let prunes: HashSet<Pubkey> = [nodes[0], nodes[2]].into_iter().collect();

================
File: gossip/src/restart_crds_values.rs
================
pub struct RestartLastVotedForkSlots {
⋮----
pub enum RestartLastVotedForkSlotsError {
⋮----
pub struct RestartHeaviestFork {
⋮----
enum SlotsOffsets {
⋮----
struct U16(#[serde(with = "serde_varint")] u16);
⋮----
struct RunLengthEncoding(Vec<U16>);
⋮----
struct RawOffsets(BitVec<u8>);
impl Sanitize for RestartLastVotedForkSlots {
fn sanitize(&self) -> std::result::Result<(), SanitizeError> {
sanitize_wallclock(self.wallclock)?;
self.last_voted_hash.sanitize()
⋮----
impl RestartLastVotedForkSlots {
⋮----
pub fn new(
⋮----
last_voted_fork.iter().minmax().into_option()
⋮----
return Err(RestartLastVotedForkSlotsError::LastVotedForkEmpty);
⋮----
let max_size = last_voted_slot.saturating_sub(first_voted_slot) + 1;
⋮----
uncompressed_bitvec.set(last_voted_slot - *slot, true);
⋮----
if run_length_encoding.num_encoded_slots() > RestartLastVotedForkSlots::MAX_BYTES * 8 {
⋮----
Ok(Self {
⋮----
pub(crate) fn new_rand<R: Rng>(rng: &mut R, pubkey: Option<Pubkey>) -> Self {
let pubkey = pubkey.unwrap_or_else(solana_pubkey::new_rand);
let num_slots = rng.gen_range(2..20);
let slots = std::iter::repeat_with(|| 47825632 + rng.gen_range(0..512))
.take(num_slots)
⋮----
new_rand_timestamp(rng),
⋮----
.unwrap()
⋮----
pub fn to_slots(&self, min_slot: Slot) -> Vec<Slot> {
⋮----
run_length_encoding.to_slots(self.last_voted_slot, min_slot)
⋮----
raw_offsets.to_slots(self.last_voted_slot, min_slot)
⋮----
impl Sanitize for RestartHeaviestFork {
fn sanitize(&self) -> Result<(), SanitizeError> {
⋮----
self.last_slot_hash.sanitize()
⋮----
impl RestartHeaviestFork {
pub(crate) fn new_rand<R: Rng>(rng: &mut R, from: Option<Pubkey>) -> Self {
let from = from.unwrap_or_else(solana_pubkey::new_rand);
⋮----
wallclock: new_rand_timestamp(rng),
last_slot: rng.gen_range(0..1000),
⋮----
observed_stake: rng.gen_range(1..u64::MAX),
⋮----
impl RunLengthEncoding {
fn new(bits: &BitVec<u8>) -> Self {
let encoded = (0..bits.len())
.map(|i| bits.get(i))
.dedup_with_count()
.map_while(|(count, _)| u16::try_from(count).ok())
.scan(0, |current_bytes, count| {
*current_bytes += (u16::BITS - count.leading_zeros()).div_ceil(7).max(1) as usize;
(*current_bytes <= RestartLastVotedForkSlots::MAX_BYTES).then_some(U16(count))
⋮----
.collect();
Self(encoded)
⋮----
fn num_encoded_slots(&self) -> usize {
self.0.iter().map(|x| usize::from(x.0)).sum()
⋮----
fn to_slots(&self, last_slot: Slot, min_slot: Slot) -> Vec<Slot> {
⋮----
.iter()
.map(|bit_count| usize::from(bit_count.0))
.zip([1, 0].iter().cycle())
.flat_map(|(bit_count, bit)| std::iter::repeat_n(bit, bit_count))
.enumerate()
.filter(|(_, bit)| **bit == 1)
.map_while(|(offset, _)| {
let offset = Slot::try_from(offset).ok()?;
last_slot.checked_sub(offset)
⋮----
.take(RestartLastVotedForkSlots::MAX_SLOTS)
.take_while(|slot| *slot >= min_slot)
⋮----
slots.reverse();
⋮----
impl RawOffsets {
fn new(mut bits: BitVec<u8>) -> Self {
bits.truncate(RestartLastVotedForkSlots::MAX_BYTES as u64 * 8);
bits.shrink_to_fit();
Self(bits)
⋮----
let mut slots: Vec<Slot> = (0..self.0.len())
.filter(|index| self.0.get(*index))
.map_while(|offset| last_slot.checked_sub(offset))
⋮----
mod test {
⋮----
fn make_rand_slots<R: Rng>(rng: &mut R) -> impl Iterator<Item = Slot> + '_ {
repeat_with(|| rng.gen_range(1..5)).scan(0, |slot, step| {
⋮----
Some(*slot)
⋮----
fn test_restart_last_voted_fork_slots_max_bytes() {
⋮----
keypair.pubkey(),
timestamp(),
⋮----
.unwrap();
assert_eq!(
⋮----
let range: Vec<Slot> = make_rand_slots(&mut rng).take(large_length).collect();
⋮----
assert!(serialized_size(&large_slots).unwrap() <= MAX_CRDS_OBJECT_SIZE as u64);
let retrieved_slots = large_slots.to_slots(0);
assert!(retrieved_slots.len() <= range.len());
assert!(retrieved_slots.last().unwrap() - retrieved_slots.first().unwrap() > 5000);
⋮----
fn test_restart_last_voted_fork_slots() {
⋮----
let value = CrdsValue::new(CrdsData::RestartLastVotedForkSlots(slots.clone()), &keypair);
assert_eq!(value.sanitize(), Ok(()));
let label = value.label();
⋮----
assert_eq!(label.pubkey(), keypair.pubkey());
assert_eq!(value.wallclock(), slots.wallclock);
let retrieved_slots = slots.to_slots(0);
assert_eq!(retrieved_slots.len(), 2);
assert_eq!(retrieved_slots[0], slot_parent);
assert_eq!(retrieved_slots[1], slot);
⋮----
assert!(bad_value.is_err());
⋮----
let large_slots_vec: Vec<Slot> = (0..last_slot + 1).collect();
⋮----
assert!(serialized_size(&large_slots).unwrap() < MAX_CRDS_OBJECT_SIZE as u64);
⋮----
assert_eq!(retrieved_slots, large_slots_vec);
⋮----
fn check_run_length_encoding(slots: Vec<Slot>) {
let last_voted_slot = slots[slots.len() - 1];
⋮----
bitvec.set(last_voted_slot - slot, true);
⋮----
let retrieved_slots = rle.to_slots(last_voted_slot, 0);
assert_eq!(retrieved_slots, slots);
⋮----
fn test_run_length_encoding() {
check_run_length_encoding((1000..16384 + 1000).map(|x| x as Slot).collect_vec());
check_run_length_encoding([1000 as Slot].into());
check_run_length_encoding(
⋮----
.into(),
⋮----
check_run_length_encoding((1000..1800).step_by(2).map(|x| x as Slot).collect_vec());
⋮----
check_run_length_encoding(range);
⋮----
fn test_restart_heaviest_fork() {
⋮----
from: keypair.pubkey(),
wallclock: timestamp(),
⋮----
assert_eq!(fork.sanitize(), Ok(()));
assert_eq!(fork.observed_stake, 800_000);
⋮----
assert_eq!(fork.sanitize(), Err(SanitizeError::ValueOutOfBounds));

================
File: gossip/src/tlv.rs
================
pub(crate) struct TlvRecord {
⋮----
macro_rules! define_tlv_enum {
⋮----
pub enum TlvDecodeError {
⋮----
pub(crate) fn parse<'a, T: TryFrom<&'a TlvRecord>>(entries: &'a [TlvRecord]) -> Vec<T> {
entries.iter().filter_map(|v| T::try_from(v).ok()).collect()
⋮----
mod tests {
⋮----
define_tlv_enum! (pub(crate) enum ExtensionNew {
⋮----
define_tlv_enum! ( pub(crate) enum ExtensionLegacy {
⋮----
fn test_tlv_backwards_compat() {
let new_tlv_data = vec![
⋮----
let new_bytes = bincode::serialize(&new_tlv_data).unwrap();
let tlv_vec: Vec<TlvRecord> = bincode::deserialize(&new_bytes).unwrap();
⋮----
assert!(matches!(new[0], ExtensionNew::Test(42)));
⋮----
assert_eq!(s, "bla");
⋮----
panic!("Wrong deserialization")
⋮----
assert!(matches!(legacy[0], ExtensionLegacy::Test(42)));
assert_eq!(
⋮----
fn test_tlv_forward_compat() {
let legacy_tlv_data = vec![
⋮----
let legacy_bytes = bincode::serialize(&legacy_tlv_data).unwrap();
let tlv_vec: Vec<TlvRecord> = bincode::deserialize(&legacy_bytes).unwrap();
⋮----
assert_eq!(s, "foo");

================
File: gossip/src/weighted_shuffle.rs
================
pub struct WeightedShuffle {
⋮----
impl WeightedShuffle {
pub fn new<I>(name: &'static str, weights: I) -> Self
⋮----
let weights = weights.into_iter();
let (num_nodes, size) = get_num_nodes_and_tree_size(weights.len());
debug_assert!(size <= num_nodes);
let mut tree = vec![[0; FANOUT]; size];
⋮----
for (k, weight) in weights.enumerate() {
let weight = *weight.borrow();
⋮----
zeros.push(k);
⋮----
sum = match sum.checked_add(&weight) {
⋮----
// Traverse the tree from the leaf node upwards to the root,
// updating the sub-tree sums along the way.
let mut index = num_nodes + k; // leaf node
⋮----
index = (index - 1) >> BIT_SHIFT; // parent node
debug_assert!(index < tree.len());
// SAFETY: Index is updated to a lesser value towards zero.
// The bitwise AND operation with BIT_MASK ensures that offset
// is always less than FANOUT, which is the size of the inner
// arrays. As a result, tree[index][offset] never goes out of
// bounds.
unsafe { tree.get_unchecked_mut(index).get_unchecked_mut(offset) }
.add_assign(weight);
⋮----
datapoint_error!("weighted-shuffle-overflow", (name, num_overflow, i64));
⋮----
// Removes given weight at index k.
fn remove(&mut self, k: usize, weight: u64) {
debug_assert!(self.weight >= weight);
⋮----
let mut index = self.num_nodes + k; // leaf node
⋮----
debug_assert!(self.tree[index][offset] >= weight);
// SAFETY: Index is updated to a lesser value towards zero. The
// bitwise AND operation with BIT_MASK ensures that offset is
// always less than FANOUT, which is the size of the inner arrays.
// As a result, tree[index][offset] never goes out of bounds.
unsafe { self.tree.get_unchecked_mut(index).get_unchecked_mut(offset) }
.sub_assign(weight);
⋮----
// Returns smallest index such that sum of weights[..=k] > val,
// along with its respective weight.
fn search(&self, mut val: u64) -> (/*index:*/ usize, /*weight:*/ u64) {
debug_assert!(!self.tree.is_empty());
// Traverse the tree downwards from the root to the target leaf node.
let mut index = 0; // root
⋮----
// SAFETY: function returns if index goes out of bounds.
let (offset, &node) = unsafe { self.tree.get_unchecked(index) }
.iter()
.enumerate()
.find(|&(_, node)| {
⋮----
.unwrap();
// Traverse to the subtree of self.tree[index].
⋮----
if self.tree.len() <= index {
⋮----
pub fn remove_index(&mut self, k: usize) {
let index = self.num_nodes + k; // leaf node
⋮----
let index = (index - 1) >> BIT_SHIFT; // parent node
let Some(weight) = self.tree.get(index).map(|node| node[offset]) else {
error!("WeightedShuffle::remove_index: Invalid index {k}");
⋮----
self.remove_zero(k);
⋮----
self.remove(k, weight);
⋮----
fn remove_zero(&mut self, k: usize) {
if let Some(index) = self.zeros.iter().position(|&ix| ix == k) {
self.zeros.remove(index);
⋮----
// Equivalent to weighted_shuffle.shuffle(&mut rng).next()
pub fn first<R: Rng>(&self, rng: &mut R) -> Option<usize> {
⋮----
let (index, _) = self.search(sample);
return Some(index);
⋮----
if self.zeros.is_empty() {
⋮----
let index = <u64 as SampleUniform>::Sampler::sample_single(0, self.zeros.len() as u64, rng);
self.zeros.get(index as usize).copied()
⋮----
pub fn shuffle<'a, R: Rng>(&'a mut self, rng: &'a mut R) -> impl Iterator<Item = usize> + 'a {
⋮----
let (index, weight) = self.search(sample);
self.remove(index, weight);
⋮----
<u64 as SampleUniform>::Sampler::sample_single(0, self.zeros.len() as u64, rng);
Some(self.zeros.swap_remove(index as usize))
⋮----
// Maps number of items to the number of "internal" nodes of the tree
// which "implicitly" holds those items on the leaves.
// Nodes without children are never accessed and don't need to be
fn get_num_nodes_and_tree_size(count: usize) -> ( usize,  usize) {
⋮----
(size + nodes, size + count.div_ceil(FANOUT))
⋮----
impl Clone for WeightedShuffle {
⋮----
fn clone(&self) -> Self {
⋮----
tree: self.tree.clone(),
⋮----
zeros: self.zeros.clone(),
⋮----
fn clone_from(&mut self, other: &Self) {
⋮----
self.tree.clone_from(&other.tree);
⋮----
self.zeros.clone_from(&other.zeros);
⋮----
mod tests {
⋮----
fn verify_shuffle(shuffle: &[usize], weights: &[u64], mut mask: Vec<bool>) {
assert_eq!(weights.len(), mask.len());
let num_dropped = mask.iter().copied().map(usize::from).sum::<usize>();
assert_eq!(shuffle.len(), weights.len() - num_dropped);
assert!(shuffle.iter().all(|&index| {
⋮----
assert!(mask.iter().all(|&x| x));
assert!(shuffle
⋮----
fn weighted_shuffle_slow<R>(rng: &mut R, mut weights: Vec<u64>) -> Vec<usize>
⋮----
let mut shuffle = Vec::with_capacity(weights.len());
let mut high: u64 = weights.iter().sum();
⋮----
.filter(|(_, w)| **w == 0)
.map(|(i, _)| i)
.collect();
⋮----
let sample = rng.gen_range(0..high);
⋮----
.scan(0, |acc, &w| {
⋮----
Some(*acc)
⋮----
.position(|acc| sample < acc)
⋮----
shuffle.push(index);
⋮----
while !zeros.is_empty() {
let index = <u64 as SampleUniform>::Sampler::sample_single(0, zeros.len() as u64, rng);
shuffle.push(zeros.swap_remove(index as usize));
⋮----
fn test_get_num_nodes_and_tree_size() {
assert_eq!(get_num_nodes_and_tree_size(0), (1, 0));
⋮----
assert_eq!(get_num_nodes_and_tree_size(count), (1, 1));
⋮----
let tree_size = 1 + count.div_ceil(16);
assert_eq!(get_num_nodes_and_tree_size(count), (num_nodes, tree_size));
⋮----
let tree_size = 1 + 16 + count.div_ceil(16);
⋮----
let tree_size = 1 + 16 + 16 * 16 + count.div_ceil(16);
⋮----
fn test_weighted_shuffle_empty_weights() {
⋮----
assert!(shuffle.clone().shuffle(&mut rng).next().is_none());
assert!(shuffle.first(&mut rng).is_none());
⋮----
// Asserts that zero weights will be shuffled.
⋮----
fn test_weighted_shuffle_zero_weights(cha_cha_variant: u8) {
let weights = vec![0u64; 5];
⋮----
assert_eq!(
⋮----
assert_eq!(shuffle.first(&mut rng), Some(4));
⋮----
assert_eq!(shuffle.first(&mut rng), Some(1));
⋮----
_ => unreachable!(),
⋮----
// Asserts that each index is selected proportional to its weight.
⋮----
fn test_weighted_shuffle_sanity() {
let seed: Vec<_> = (1..).step_by(3).take(32).collect();
let seed: [u8; 32] = seed.try_into().unwrap();
⋮----
test_weighted_shuffle_sanity_impl(
⋮----
fn test_weighted_shuffle_sanity_impl<R: Rng>(
⋮----
let mut shuffle = weighted_shuffle.shuffle(rng);
counts[shuffle.next().unwrap()] += 1;
let _ = shuffle.count(); // consume the rest.
⋮----
assert_eq!(counts, counts1);
⋮----
shuffle.remove_index(5);
shuffle.remove_index(3);
shuffle.remove_index(1);
let mut shuffle = shuffle.shuffle(rng);
⋮----
assert_eq!(counts, counts2);
⋮----
fn test_weighted_shuffle_overflow() {
⋮----
fn test_weighted_shuffle_overflow_impl<R: Rng + rand::SeedableRng<Seed = [u8; 32]>>(
⋮----
assert_eq!(shuffle.shuffle(&mut rng).collect::<Vec<_>>(), counts);
⋮----
fn test_weighted_shuffle_hard_coded() {
⋮----
assert_eq!(shuffle.first(&mut rng), Some(10));
⋮----
shuffle.remove_index(11);
⋮----
shuffle.remove_index(15);
shuffle.remove_index(0);
⋮----
assert_eq!(shuffle.first(&mut rng), Some(3));
shuffle.remove_index(16);
shuffle.remove_index(8);
shuffle.remove_index(20);
⋮----
shuffle.remove_index(19);
shuffle.remove_index(4);
⋮----
assert_eq!(shuffle.first(&mut rng), Some(2));
⋮----
// Verifies that changes to the code or dependencies (e.g. rand or
// rand_chacha::ChaChaRng) do not change the deterministic shuffle.
⋮----
fn test_weighted_shuffle_hard_coded_paranoid(seed: u64, expected_hash: &str) {
let expected_hash = Hash::from_str(expected_hash).unwrap();
⋮----
successors(Some(seed), |seed| Some(seed + 1))
.map(u64::to_le_bytes)
.take(32 / 8)
.flatten()
⋮----
.map(ChaChaRng::from_seed)
⋮----
let num_weights = rng.gen_range(1..=100_000);
assert!((8143..=85348).contains(&num_weights), "{num_weights}");
let weights: Vec<u64> = repeat_with(|| {
if rng.gen_ratio(1, 100) {
⋮----
rng.gen_range(0..=(u64::MAX / num_weights as u64))
⋮----
.take(num_weights)
⋮----
let num_zeros = weights.iter().filter(|&&w| w == 0).count();
assert!((72..=846).contains(&num_zeros), "{num_zeros}");
⋮----
let shuffle1 = shuffle.clone().shuffle(&mut rng).collect::<Vec<_>>();
// Assert that all indices appear in the shuffle.
assert_eq!(shuffle1.len(), num_weights);
verify_shuffle(&shuffle1, &weights, vec![false; num_weights]);
// Drop some of the weights and re-shuffle.
let num_drops = rng.gen_range(1..1_000);
assert!((253..=981).contains(&num_drops), "{num_drops}");
let mut mask = vec![false; num_weights];
repeat_with(|| rng.gen_range(0..num_weights))
.filter(|&index| {
⋮----
.take(num_drops)
.for_each(|index| shuffle.remove_index(index));
let shuffle2 = shuffle.shuffle(&mut rng).collect::<Vec<_>>();
assert_eq!(shuffle2.len(), num_weights - num_drops);
verify_shuffle(&shuffle2, &weights, mask);
// Assert that code or dependencies updates do not change the shuffle.
⋮----
.into_iter()
.chain(shuffle2)
.map(usize::to_le_bytes)
⋮----
let bytes = bytes.iter().map(AsRef::as_ref).collect::<Vec<_>>();
assert_eq!(solana_sha256_hasher::hashv(&bytes[..]), expected_hash);
⋮----
fn test_weighted_shuffle_match_slow() {
⋮----
fn test_weighted_shuffle_match_slow_impl<R: Rng + rand::SeedableRng<Seed = [u8; 32]>>() {
⋮----
let weights: Vec<u64> = repeat_with(|| rng.gen_range(0..1000)).take(997).collect();
⋮----
rng.fill(&mut seed[..]);
⋮----
let shuffle: Vec<_> = shuffle.shuffle(&mut rng).collect();
⋮----
let shuffle_slow = weighted_shuffle_slow(&mut rng, weights.clone());
assert_eq!(shuffle, shuffle_slow);
⋮----
assert_eq!(shuffle.first(&mut rng), Some(shuffle_slow[0]));
⋮----
fn test_weighted_shuffle_paranoid() {
⋮----
test_weighted_shuffle_paranoid_impl(rng);
⋮----
fn test_weighted_shuffle_paranoid_impl<R: Rng + Clone>(mut rng: R) {
⋮----
let weights: Vec<_> = repeat_with(|| rng.gen_range(0..1000)).take(size).collect();
let shuffle_slow = weighted_shuffle_slow(&mut rng.clone(), weights.clone());
⋮----
assert_eq!(shuffle.first(&mut rng.clone()), Some(shuffle_slow[0]));
⋮----
assert_eq!(shuffle.shuffle(&mut rng).collect::<Vec<_>>(), shuffle_slow);

================
File: gossip/src/wire_format_tests.rs
================
mod tests {
⋮----
fn parse_gossip(bytes: &[u8]) -> anyhow::Result<Protocol> {
⋮----
pkt.sanitize()?;
Ok(pkt)
⋮----
fn serialize<T: Serialize>(pkt: T) -> Vec<u8> {
bincode::serialize(&pkt).unwrap()
⋮----
fn find_differences(a: &[u8], b: &[u8]) -> Option<usize> {
if a.len() != b.len() {
return Some(a.len().min(b.len()));
⋮----
for (idx, (e1, e2)) in a.iter().zip(b).enumerate() {
⋮----
return Some(idx);
⋮----
fn test_gossip_wire_format() {
⋮----
eprintln!("Test requires GOSSIP_WIRE_FORMAT_PACKETS env variable, skipping!");
⋮----
std::fs::read_dir(path_base).expect("Expecting env var to point to a directory")
⋮----
let entry = entry.expect("Expecting a readable file");
validate_packet_format(
&entry.path(),
⋮----
.unwrap();

================
File: gossip/tests/crds_gossip.rs
================
type PingCache = solana_gossip::ping_pong::PingCache<32>;
⋮----
struct Node {
⋮----
impl Node {
fn new(keypair: Arc<Keypair>, contact_info: ContactInfo, gossip: Arc<CrdsGossip>) -> Self {
⋮----
fn staked(
⋮----
let ping_cache = Arc::new(new_ping_cache());
⋮----
struct Network {
⋮----
impl Network {
fn new(nodes: HashMap<Pubkey, Node>) -> Self {
⋮----
impl Deref for Network {
type Target = HashMap<Pubkey, Node>;
fn deref(&self) -> &Self::Target {
⋮----
fn stakes(network: &Network) -> HashMap<Pubkey, u64> {
⋮----
for (key, Node { stake, .. }) in network.iter() {
stakes.insert(*key, *stake);
⋮----
fn star_network_create(num: usize) -> Network {
⋮----
let contact_info = ContactInfo::new_localhost(&node_keypair.pubkey(), 0);
⋮----
.map(|k| {
⋮----
let mut contact_info = ContactInfo::new_localhost(&node_keypair.pubkey(), 0);
let gossip_port = gossip_port_offset + u16::try_from(k).unwrap();
⋮----
.set_gossip((Ipv4Addr::LOCALHOST, gossip_port))
.unwrap();
⋮----
let mut node_crds = node.crds.write().unwrap();
⋮----
.insert(new.clone(), timestamp(), GossipRoute::LocalMessage)
⋮----
.insert(entry.clone(), timestamp(), GossipRoute::LocalMessage)
⋮----
(new.label().pubkey(), node)
⋮----
.collect();
⋮----
let id = entry.label().pubkey();
⋮----
.write()
.unwrap()
.insert(entry, timestamp(), GossipRoute::LocalMessage)
⋮----
network.insert(id, node);
⋮----
fn rstar_network_create(num: usize) -> Network {
⋮----
.map(|_| {
⋮----
fn ring_network_create(num: usize) -> Network {
⋮----
let keys: Vec<Pubkey> = network.keys().cloned().collect();
for k in 0..keys.len() {
⋮----
let gossip_crds = start.gossip.crds.read().unwrap();
gossip_crds.get::<&CrdsValue>(&label).unwrap().clone()
⋮----
let end = network.get_mut(&keys[(k + 1) % keys.len()]).unwrap();
let mut end_crds = end.gossip.crds.write().unwrap();
⋮----
.insert(start_info, timestamp(), GossipRoute::LocalMessage)
⋮----
fn connected_staked_network_create(stakes: &[u64]) -> Network {
let num = stakes.len();
⋮----
.map(|n| {
⋮----
.iter()
⋮----
gossip_crds.get::<&CrdsValue>(&start_label).unwrap().clone()
⋮----
for (end_pubkey, end) in network.iter_mut() {
⋮----
let start_info = start_entries[k].clone();
⋮----
fn network_simulator_pull_only(thread_pool: &ThreadPool, network: &Network) {
let num = network.len();
⋮----
.map(|(&pubkey, node)| {
⋮----
let crds = node.gossip.crds.read().unwrap();
let entry = crds.get::<&CrdsValue>(&label).unwrap().clone();
⋮----
.unzip();
entries.rotate_right(1);
for (pubkey, entry) in pubkeys.into_iter().zip(entries) {
let mut crds = network.nodes[&pubkey].gossip.crds.write().unwrap();
let _ = crds.insert(entry, timestamp(), GossipRoute::LocalMessage);
⋮----
let (converged, bytes_tx) = network_run_pull(thread_pool, network, 0, num * 2, 0.9);
trace!("network_simulator_pull_{num}: converged: {converged} total_bytes: {bytes_tx}");
assert!(converged >= 0.9);
⋮----
fn network_simulator(thread_pool: &ThreadPool, network: &mut Network, max_convergance: f64) {
⋮----
let (converged, bytes_tx) = network_run_pull(thread_pool, network, 0, 10, 1.0);
trace!("network_simulator_push_{num}: converged: {converged}");
let network_values: Vec<Node> = network.values().cloned().collect();
network_values.par_iter().for_each(|node| {
node.gossip.refresh_push_active_set(
⋮----
let mut ts = timestamp();
⋮----
let start = ts.div_ceil(100) as usize;
⋮----
let node_pubkey = node.keypair.pubkey();
⋮----
let node_crds = node.gossip.crds.read().unwrap();
node_crds.get::<&ContactInfo>(node_pubkey).cloned().unwrap()
⋮----
m.set_wallclock(now);
node.gossip.process_push_message(
vec![(
⋮----
let (queue_size, bytes_tx) = network_run_push(thread_pool, network, start, end);
⋮----
trace!("network_simulator_push_{num}: queue_size: {queue_size} bytes: {bytes_tx}");
let (converged, bytes_tx) = network_run_pull(thread_pool, network, start, end, 1.0);
⋮----
trace!(
⋮----
fn network_run_push(
⋮----
let stakes = stakes(network);
⋮----
.par_iter()
.map(|node| {
⋮----
let timeouts = node.gossip.make_timeouts(
⋮----
node.gossip.purge(&node_pubkey, thread_pool, now, &timeouts);
let (entries, messages, _) = node.gossip.new_push_messages(
⋮----
.into_iter()
.map(|(pubkey, indices)| {
let values = indices.into_iter().map(|k| entries[k].clone()).collect();
⋮----
.into_par_iter()
.map(|(from, push_messages)| {
⋮----
.map(CrdsValue::bincode_serialized_size)
⋮----
.get(&to)
⋮----
.process_push_message(vec![(from, msgs.clone())], now)
⋮----
.prune_received_cache(&node_pubkey, origins, &stakes)
⋮----
let prune_keys: Vec<_> = prune_set.into_iter().collect();
⋮----
pruned.insert((from, *prune_key));
⋮----
bytes += serialized_size(&prune_keys).unwrap() as usize;
⋮----
.get(&from)
⋮----
let now = timestamp();
⋮----
.process_prune_msg(
⋮----
let from_stake = stakes.get(&from).unwrap();
if network.connections_pruned.insert((from, to)) {
⋮----
if now.is_multiple_of(CRDS_GOSSIP_PUSH_MSG_TIMEOUT_MS) && now > 0 {
⋮----
.map(|node| node.gossip.push.num_pending(&node.gossip.crds))
.sum();
⋮----
fn network_run_pull(
⋮----
let mut ping_cache = node.ping_cache.lock().unwrap();
⋮----
if node.keypair.pubkey() != other.keypair.pubkey() {
ping_cache.mock_pong(
other.keypair.pubkey(),
other.contact_info.gossip().unwrap(),
⋮----
.values()
⋮----
(node.gossip().unwrap(), node.clone())
⋮----
.flat_map_iter(|from| {
⋮----
.new_pull_request(
⋮----
from.keypair.deref(),
⋮----
from.ping_cache.deref(),
⋮----
.map(|requests| {
⋮----
.into_group_map()
⋮----
.map(|(addr, filters)| {
(nodes.get(&addr).cloned().unwrap(), filters)
⋮----
.unwrap_or_default();
let from_pubkey = from.keypair.pubkey();
⋮----
let gossip_crds = from.gossip.crds.read().unwrap();
let self_info = gossip_crds.get::<&CrdsValue>(&label).unwrap().clone();
⋮----
.map(move |(peer, filters)| (*peer.pubkey(), filters, self_info.clone()))
⋮----
.collect()
⋮----
.map(|(to, filters, caller_info)| {
⋮----
let from = caller_info.label().pubkey();
bytes += filters.iter().map(|f| f.filter.keys.len()).sum::<usize>();
⋮----
.map(|f| f.filter.bits.len() as usize / 8)
⋮----
bytes += caller_info.bincode_serialized_size();
⋮----
.map(|filter| PullRequest {
⋮----
.generate_pull_responses(
⋮----
.flatten()
⋮----
msgs += rsp.len();
if let Some(node) = network.get(&from) {
⋮----
node.keypair.pubkey(),
⋮----
.filter_pull_responses(&timeouts, rsp, now, &mut stats);
node.gossip.process_pull_responses(
⋮----
.map(|v| v.gossip.crds.read().unwrap().len())
⋮----
fn build_gossip_thread_pool() -> ThreadPool {
⋮----
.num_threads(get_thread_count().min(2))
.thread_name(|i| format!("gossipTest{i:02}"))
.build()
⋮----
fn new_ping_cache() -> Mutex<PingCache> {
⋮----
fn test_star_network_pull_50() {
let network = star_network_create(50);
let thread_pool = build_gossip_thread_pool();
network_simulator_pull_only(&thread_pool, &network);
⋮----
fn test_star_network_pull_100() {
let network = star_network_create(100);
⋮----
fn test_star_network_push_star_200() {
let mut network = star_network_create(200);
⋮----
network_simulator(&thread_pool, &mut network, 0.9);
⋮----
fn test_star_network_push_rstar_200() {
let mut network = rstar_network_create(200);
⋮----
fn test_star_network_push_ring_200() {
let mut network = ring_network_create(200);
⋮----
fn test_connected_staked_network() {
⋮----
[1000; 2].to_vec(),
[100; 3].to_vec(),
[10; 5].to_vec(),
[1; 15].to_vec(),
⋮----
.concat();
let mut network = connected_staked_network_create(&stakes);
network_simulator(&thread_pool, &mut network, 1.0);
let stake_sum: u64 = stakes.iter().sum();
let avg_stake: u64 = stake_sum / stakes.len() as u64;
let avg_stake_pruned = network.stake_pruned / network.connections_pruned.len() as u64;
⋮----
assert!(
⋮----
fn test_star_network_large_pull() {
⋮----
let network = star_network_create(2000);
⋮----
fn test_rstar_network_large_push() {
⋮----
let mut network = rstar_network_create(4000);
⋮----
fn test_ring_network_large_push() {
⋮----
let mut network = ring_network_create(4001);
⋮----
fn test_star_network_large_push() {
⋮----
let mut network = star_network_create(4002);
⋮----
fn test_prune_errors() {
⋮----
let id = keypair.pubkey();
⋮----
.insert(
⋮----
let ping_cache = new_ping_cache();
crds_gossip.refresh_push_active_set(
⋮----
let mut res = crds_gossip.process_prune_msg(
⋮----
ci.pubkey(),
&Pubkey::from(hash(&[1; 32]).to_bytes()),
⋮----
assert_eq!(res.err(), Some(CrdsGossipError::BadPruneDestination));
res = crds_gossip.process_prune_msg(
⋮----
res.unwrap();
⋮----
assert_eq!(res.err(), Some(CrdsGossipError::PruneMessageTimeout));

================
File: gossip/tests/gossip.rs
================
extern crate log;
⋮----
fn test_node(exit: Arc<AtomicBool>) -> (Arc<ClusterInfo>, GossipService, UdpSocket) {
⋮----
let mut test_node = Node::new_localhost_with_pubkey(&keypair.pubkey());
⋮----
test_node.info.clone(),
⋮----
let _ = cluster_info.my_contact_info();
⋮----
test_node.sockets.tvu.pop().unwrap(),
⋮----
fn test_node_with_bank(
⋮----
let mut test_node = Node::new_localhost_with_pubkey(&node_keypair.pubkey());
⋮----
Some(bank_forks),
⋮----
fn run_gossip_topo<F>(num: usize, topo: F)
⋮----
let listen: Vec<_> = (0..num).map(|_| test_node(exit.clone())).collect();
topo(&listen);
⋮----
let total: usize = listen.iter().map(|v| v.0.gossip_peers().len()).sum();
⋮----
trace!("not converged {} {} {}", i, total + num, num * num);
⋮----
sleep(Duration::from_secs(1));
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
dr.join().unwrap();
⋮----
assert!(done);
⋮----
fn retransmit_to(
⋮----
trace!("retransmit orders {}", peers.len());
⋮----
.iter()
.filter_map(|peer| peer.tvu(Protocol::UDP))
.filter(|addr| socket_addr_space.check(addr))
.collect()
⋮----
match multi_target_send(socket, data, &dests) {
⋮----
error!(
⋮----
fn gossip_ring() {
⋮----
run_gossip_topo(40, |listen| {
let num = listen.len();
⋮----
let y = n % listen.len();
let x = (n + 1) % listen.len();
⋮----
let mut d = yv.lookup_contact_info(&yv.id(), |ci| ci.clone()).unwrap();
d.set_wallclock(timestamp());
listen[x].0.insert_info(d);
⋮----
fn gossip_ring_large() {
⋮----
run_gossip_topo(600, |listen| {
⋮----
fn gossip_star() {
⋮----
run_gossip_topo(10, |listen| {
⋮----
let y = (n + 1) % listen.len();
⋮----
let mut yd = yv.lookup_contact_info(&yv.id(), |ci| ci.clone()).unwrap();
yd.set_wallclock(timestamp());
⋮----
xv.insert_info(yd);
trace!("star leader {}", &xv.id());
⋮----
fn gossip_rstar() {
⋮----
xv.lookup_contact_info(&xv.id(), |ci| ci.clone()).unwrap()
⋮----
trace!("rstar leader {}", xd.pubkey());
⋮----
yv.insert_info(xd.clone());
trace!("rstar insert {} into {}", xd.pubkey(), yv.id());
⋮----
pub fn cluster_info_retransmit() {
⋮----
trace!("c1:");
let (c1, dr1, tn1) = test_node(exit.clone());
trace!("c2:");
let (c2, dr2, tn2) = test_node(exit.clone());
trace!("c3:");
let (c3, dr3, tn3) = test_node(exit.clone());
let c1_contact_info = c1.my_contact_info();
c2.insert_info(c1_contact_info.clone());
c3.insert_info(c1_contact_info);
⋮----
trace!("waiting to converge:");
⋮----
done = c1.gossip_peers().len() == num - 1
&& c2.gossip_peers().len() == num - 1
&& c3.gossip_peers().len() == num - 1;
⋮----
p.meta_mut().size = 10;
let peers = c1.tvu_peers(ContactInfo::clone);
let retransmit_peers: Vec<_> = peers.iter().collect();
retransmit_to(
⋮----
p.data(..).unwrap(),
⋮----
.into_par_iter()
.map(|s| {
⋮----
s.set_read_timeout(Some(Duration::from_secs(1))).unwrap();
let res = s.recv_from(p.buffer_mut());
res.is_err()
⋮----
.collect();
assert_eq!(res, [true, false, false]);
⋮----
dr1.join().unwrap();
dr2.join().unwrap();
dr3.join().unwrap();
⋮----
pub fn cluster_info_scale() {
⋮----
.unwrap_or_else(|_| "10".to_string())
.parse()
.expect("could not parse NUM_NODES as a number");
⋮----
.map(|_| ValidatorVoteKeypairs::new_rand())
⋮----
let genesis_config_info = create_genesis_config_with_vote_accounts(
⋮----
vec![100; vote_keypairs.len()],
⋮----
.into_iter()
.map(|keypairs| {
test_node_with_bank(
⋮----
exit.clone(),
bank_forks.clone(),
⋮----
let ci0 = nodes[0].0.my_contact_info();
⋮----
node.0.insert_info(ci0.clone());
⋮----
for (i, node) in nodes.iter().enumerate() {
warn!("node {} peers: {}", i, node.0.gossip_peers().len());
if node.0.gossip_peers().len() != num_nodes - 1 {
⋮----
time.stop();
warn!("found {num_nodes} nodes in {time} success: {success}");
⋮----
let tx = test_tx();
warn!("tx.message.account_keys: {:?}", tx.message.account_keys);
⋮----
vec![1, 3, num_votes + 5],
⋮----
let tower = vec![num_votes + 5];
nodes[0].0.push_vote(&tower, tx.clone());
⋮----
for (node, _, _) in nodes.iter() {
⋮----
.get_votes(&mut Cursor::default())
⋮----
.filter(|v| v.message.account_keys == tx.message.account_keys)
.count();
num_old += node.gossip.push.num_old.load(Ordering::Relaxed);
num_push_total += node.gossip.push.num_total.load(Ordering::Relaxed);
num_pushes += node.gossip.push.num_pushes.load(Ordering::Relaxed);
num_pulls += node.gossip.pull.num_pulls.load(Ordering::Relaxed);
⋮----
warn!("not_done: {}/{}", not_done, nodes.len());
warn!("num_old: {num_old}");
warn!("num_push_total: {num_push_total}");
warn!("num_pushes: {num_pushes}");
warn!("num_pulls: {num_pulls}");
success = not_done < (nodes.len() / 20);
⋮----
sleep(Duration::from_millis(200));
⋮----
warn!("propagated vote {num_votes} in {time} success: {success}");
⋮----
node.gossip.push.num_old.store(0, Ordering::Relaxed);
node.gossip.push.num_total.store(0, Ordering::Relaxed);
node.gossip.push.num_pushes.store(0, Ordering::Relaxed);
node.gossip.pull.num_pulls.store(0, Ordering::Relaxed);
⋮----
node.1.join().unwrap();

================
File: gossip/.gitignore
================
/target/
/farf/

================
File: gossip/Cargo.toml
================
[package]
name = "solana-gossip"
documentation = "https://docs.rs/solana-gossip"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
bench = false

[features]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-bloom/frozen-abi",
    "solana-ledger/frozen-abi",
    "solana-perf/frozen-abi",
    "solana-runtime/frozen-abi",
    "solana-short-vec/frozen-abi",
    "solana-version/frozen-abi",
    "solana-vote/frozen-abi",
    "solana-vote-program/frozen-abi",
]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
arc-swap = { workspace = true }
arrayvec = { workspace = true }
assert_matches = { workspace = true }
bincode = { workspace = true }
bv = { workspace = true, features = ["serde"] }
clap = { workspace = true }
crossbeam-channel = { workspace = true }
flate2 = { workspace = true }
indexmap = { workspace = true, features = ["rayon"] }
itertools = { workspace = true }
log = { workspace = true }
lru = { workspace = true }
num-traits = { workspace = true }
rand = "0.8.5"
rand_chacha = "0.3.1"
rayon = { workspace = true }
serde = { workspace = true }
serde-big-array = { workspace = true }
serde_bytes = { workspace = true }
siphasher = { workspace = true }
solana-bloom = { workspace = true }
solana-clap-utils = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-connection-cache = { workspace = true }
solana-entry = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true, features = ["agave-unstable-api"] }
solana-measure = { workspace = true }
solana-metrics = { workspace = true }
solana-native-token = { workspace = true }
solana-net-utils = { workspace = true, features = ["agave-unstable-api"] }
solana-packet = { workspace = true }
solana-perf = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-quic-definitions = { workspace = true }
solana-rayon-threadlimit = { workspace = true }
solana-rpc-client = { workspace = true }
solana-runtime = { workspace = true }
solana-sanitize = { workspace = true }
solana-serde-varint = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-short-vec = { workspace = true }
solana-signature = { workspace = true, default-features = false }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-time-utils = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-version = { workspace = true }
solana-vote = { workspace = true }
solana-vote-program = { workspace = true }
static_assertions = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
anyhow = { workspace = true }
bs58 = { workspace = true }
criterion = { workspace = true }
num_cpus = { workspace = true }
serial_test = { workspace = true }
solana-gossip = { path = ".", features = ["agave-unstable-api"] }
solana-net-utils = { workspace = true, features = ["dev-context-only-utils"] }
solana-perf = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-signature = { workspace = true, features = ["rand"] }
solana-system-transaction = { workspace = true }
solana-vote-interface = { workspace = true }
static_assertions = { workspace = true }
test-case = { workspace = true }

[[bench]]
name = "crds"
harness = false

[[bench]]
name = "crds_gossip_pull"
harness = false

[[bench]]
name = "crds_shards"
harness = false

[[bench]]
name = "weighted_shuffle"
harness = false

[lints]
workspace = true

================
File: gossip-bin/src/main.rs
================
fn get_clap_app<'ab, 'v>(name: &str, about: &'ab str, version: &'v str) -> App<'ab, 'v> {
⋮----
.long("shred-version")
.value_name("VERSION")
.takes_value(true)
.default_value("0")
.help("Filter gossip nodes by this shred version");
⋮----
.long("gossip-port")
.value_name("PORT")
⋮----
.validator(is_port)
.help("Gossip port number for the node");
⋮----
.long("bind-address")
.value_name("HOST")
⋮----
.validator(solana_net_utils::is_host)
.help("IP address to bind the node to for gossip");
⋮----
.about(about)
.version(version)
.setting(AppSettings::SubcommandRequiredElseHelp)
.arg(
⋮----
.long("allow-private-addr")
.takes_value(false)
.help("Allow contacting private ip addresses")
.hidden(hidden_unless_forced()),
⋮----
.subcommand(
⋮----
.about("Get an RPC URL for the cluster")
⋮----
.short("n")
.long("entrypoint")
.value_name("HOST:PORT")
⋮----
.required(true)
.validator(solana_net_utils::is_host_port)
.help("Rendezvous with the cluster at this entry point"),
⋮----
.long("all")
⋮----
.help("Return all RPC URLs"),
⋮----
.long("any")
⋮----
.conflicts_with("all")
.help("Return any RPC URL"),
⋮----
.long("timeout")
.value_name("SECONDS")
⋮----
.default_value("15")
.help("Timeout in seconds"),
⋮----
.arg(&shred_version_arg)
.arg(&gossip_port_arg)
.arg(&bind_address_arg)
.setting(AppSettings::DisableVersion),
⋮----
.about("Monitor the gossip entrypoint")
.setting(AppSettings::DisableVersion)
⋮----
.multiple(true)
⋮----
.help("Rendezvous with the cluster at this entrypoint"),
⋮----
.short("i")
.long("identity")
.value_name("PATH")
⋮----
.validator(is_keypair_or_ask_keyword)
.help("Identity keypair [default: ephemeral keypair]"),
⋮----
.short("N")
.long("num-nodes")
.value_name("NUM")
⋮----
.conflicts_with("num_nodes_exactly")
.help("Wait for at least NUM nodes to be visible"),
⋮----
.short("E")
.long("num-nodes-exactly")
⋮----
.help("Wait for exactly NUM nodes to be visible"),
⋮----
.short("p")
.long("pubkey")
.value_name("PUBKEY")
⋮----
.validator(is_pubkey)
⋮----
.help("Public key of a specific node to wait for"),
⋮----
.help("Maximum time to wait in seconds [default: wait forever]"),
⋮----
fn parse_matches() -> ArgMatches<'static> {
get_clap_app(
crate_name!(),
crate_description!(),
⋮----
.get_matches()
⋮----
fn parse_bind_address(matches: &ArgMatches, entrypoint_addrs: &[SocketAddr]) -> IpAddr {
if let Some(bind_address) = matches.value_of("bind_address") {
solana_net_utils::parse_host(bind_address).unwrap_or_else(|e| {
eprintln!("failed to parse bind-address: {e}");
exit(1);
⋮----
} else if let Some(bind_addr) = get_bind_address_from_entrypoints(entrypoint_addrs) {
⋮----
eprintln!(
⋮----
fn get_bind_address_from_entrypoints(entrypoint_addrs: &[SocketAddr]) -> Option<IpAddr> {
entrypoint_addrs.iter().find_map(|entrypoint_addr| {
⋮----
.ok()
⋮----
fn process_spy_results(
⋮----
if timeout.is_some() {
⋮----
if validators.len() < num {
let add = if num_nodes_exactly.is_some() {
⋮----
eprintln!("Error: Insufficient validators discovered.  Expecting {num}{add}",);
⋮----
if !validators.iter().any(|x| {
⋮----
let pubkey = x.pubkey();
⋮----
eprintln!("Error: Could not find node {node:?}");
⋮----
if validators.len() > num_nodes_exactly {
eprintln!("Error: Extra nodes discovered.  Expecting exactly {num_nodes_exactly}");
⋮----
fn get_entrypoint_shred_version(entrypoint_addrs: &[SocketAddr]) -> Option<u16> {
⋮----
warn!("get_cluster_shred_version failed: {entrypoint_addr}, {err}");
⋮----
warn!("entrypoint {entrypoint_addr} returned shred-version zero");
⋮----
info!("obtained shred-version {shred_version} from entrypoint: {entrypoint_addr}");
Some(shred_version)
⋮----
fn process_spy(matches: &ArgMatches, socket_addr_space: SocketAddrSpace) -> std::io::Result<()> {
⋮----
.value_of("num_nodes_exactly")
.map(|num| num.to_string().parse().unwrap());
⋮----
.value_of("num_nodes")
.map(|num| num.to_string().parse().unwrap())
.or(num_nodes_exactly);
⋮----
.value_of("timeout")
.map(|secs| secs.to_string().parse().unwrap());
let pubkeys = pubkeys_of(matches, "node_pubkey");
let identity_keypair = keypair_of(matches, "identity");
let entrypoint_addrs = parse_entrypoints(matches);
let gossip_addr = get_gossip_address(matches, &entrypoint_addrs);
let mut shred_version = value_t_or_exit!(matches, "shred_version", u16);
⋮----
shred_version = get_entrypoint_shred_version(&entrypoint_addrs)
.expect("need non-zero shred-version to join the cluster");
⋮----
let discover_timeout = Duration::from_secs(timeout.unwrap_or(u64::MAX));
⋮----
let (_all_peers, validators) = discover_peers(
⋮----
pubkeys.as_deref(),
⋮----
Some(&gossip_addr),
⋮----
process_spy_results(
⋮----
Ok(())
⋮----
fn parse_entrypoints(matches: &ArgMatches) -> Vec<SocketAddr> {
values_t!(matches, "entrypoint", String)
.unwrap_or_default()
.into_iter()
.map(|entrypoint| solana_net_utils::parse_host_port(&entrypoint))
.filter_map(Result::ok)
⋮----
fn process_rpc_url(
⋮----
let any = matches.is_present("any");
let all = matches.is_present("all");
let timeout = value_t_or_exit!(matches, "timeout", u64);
⋮----
Some(1),
⋮----
.iter()
.filter(|node| {
⋮----
let addrs = node.gossip();
⋮----
.map(|addr| entrypoint_addrs.contains(&addr))
⋮----
.filter_map(
⋮----
.filter(|addr| socket_addr_space.check(addr))
.collect();
if rpc_addrs.is_empty() {
eprintln!("No RPC URL found");
⋮----
println!("http://{rpc_addr}");
⋮----
fn get_gossip_address(matches: &ArgMatches, entrypoint_addrs: &[SocketAddr]) -> SocketAddr {
let bind_address = parse_bind_address(matches, entrypoint_addrs);
⋮----
value_t!(matches, "gossip_port", u16).unwrap_or_else(|_| {
⋮----
.expect("unable to find an available gossip port")
⋮----
fn main() -> Result<(), Box<dyn error::Error>> {
⋮----
let matches = parse_matches();
let socket_addr_space = SocketAddrSpace::new(matches.is_present("allow_private_addr"));
match matches.subcommand() {
⋮----
process_spy(matches, socket_addr_space)?;
⋮----
process_rpc_url(matches, socket_addr_space)?;
⋮----
_ => unreachable!(),

================
File: gossip-bin/Cargo.toml
================
[package]
name = "solana-gossip-bin"
version = { workspace = true }
authors = { workspace = true }
description = "Binary crate providing the Solana Gossip command-line tool."
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
publish = false
edition = { workspace = true }

[[bin]]
name = "solana-gossip"
path = "src/main.rs"
bench = false

[features]
agave-unstable-api = []

[dependencies]
agave-logger = { workspace = true }
clap = { workspace = true }
log = { workspace = true }
solana-clap-utils = { workspace = true }
solana-gossip = { workspace = true, features = ["agave-unstable-api"] }
solana-net-utils = { workspace = true, features = ["agave-unstable-api"] }
solana-pubkey = { version = "=3.0.0", features = ["rand"] }
solana-version = { workspace = true }

================
File: install/src/bin/agave-install-init.rs
================
use std::process::exit;
fn press_enter() {
if cfg!(windows) && atty::is(atty::Stream::Stdin) {
println!();
println!("Press the Enter key to continue.");
use std::io::BufRead;
⋮----
let stdin = stdin.lock();
let mut lines = stdin.lines();
lines.next();
⋮----
fn main() {
agave_install::main_init().unwrap_or_else(|err| {
println!("Error: {err}");
press_enter();
exit(1);

================
File: install/src/build_env.rs
================
pub const TARGET: &str = env!("TARGET");
pub const BUILD_SECONDS_SINCE_UNIX_EPOCH: &str = env!("BUILD_SECONDS_SINCE_UNIX_EPOCH");

================
File: install/src/command.rs
================
pub struct ReleaseVersion {
⋮----
static TRUCK: Emoji = Emoji("🚚 ", "");
static LOOKING_GLASS: Emoji = Emoji("🔍 ", "");
static BULLET: Emoji = Emoji("• ", "* ");
static SPARKLE: Emoji = Emoji("✨ ", "");
static WRAPPED_PRESENT: Emoji = Emoji("🎁 ", "");
static PACKAGE: Emoji = Emoji("📦 ", "");
static INFORMATION: Emoji = Emoji("ℹ️  ", "");
static RECYCLING: Emoji = Emoji("♻️  ", "");
/// Creates a new process bar for processing that will take an unknown amount of time
fn new_spinner_progress_bar() -> ProgressBar {
⋮----
fn new_spinner_progress_bar() -> ProgressBar {
⋮----
progress_bar.set_style(
⋮----
.template("{spinner:.green} {wide_msg}")
.expect("ProgresStyle::template direct input to be correct"),
⋮----
progress_bar.enable_steady_tick(Duration::from_millis(100));
⋮----
fn println_name_value(name: &str, value: &str) {
println!("{} {}", style(name).bold(), value);
⋮----
fn download_to_temp(
⋮----
fn sha256_file_digest<P: AsRef<Path>>(path: P) -> Result<Hash, Box<dyn std::error::Error>> {
⋮----
let count = reader.read(&mut buffer)?;
⋮----
hasher.hash(&buffer[..count]);
⋮----
Ok(hasher.result())
⋮----
let url = Url::parse(url).map_err(|err| format!("Unable to parse {url}: {err}"))?;
⋮----
let temp_file = temp_dir.path().join("download");
⋮----
.connect_timeout(Duration::from_secs(30))
.timeout(None)
.build()?;
let progress_bar = new_spinner_progress_bar();
progress_bar.set_message(format!("{TRUCK}Downloading..."));
let response = client.get(url.as_str()).send()?;
⋮----
.headers()
.get(reqwest::header::CONTENT_LENGTH)
.and_then(|content_length| content_length.to_str().ok())
.and_then(|content_length| content_length.parse().ok())
.unwrap_or(0)
⋮----
progress_bar.set_length(download_size);
⋮----
.template(
⋮----
.expect("ProgresStyle::template direct input to be correct")
.progress_chars("=> "),
⋮----
progress_bar.set_message(format!("{TRUCK}Downloading"));
struct DownloadProgress<R> {
⋮----
impl<R: Read> Read for DownloadProgress<R> {
fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {
self.response.read(buf).inspect(|&n| {
self.progress_bar.inc(n as u64);
⋮----
let temp_file_sha256 = sha256_file_digest(&temp_file)
.map_err(|err| format!("Unable to hash {temp_file:?}: {err}"))?;
if expected_sha256.is_some() && expected_sha256 != Some(&temp_file_sha256) {
return Err(io::Error::other("Incorrect hash").into());
⋮----
source.progress_bar.finish_and_clear();
Ok((temp_dir, temp_file, temp_file_sha256))
⋮----
fn extract_release_archive(
⋮----
progress_bar.set_message(format!("{PACKAGE}Extracting..."));
if extract_dir.exists() {
⋮----
let tmp_extract_dir = extract_dir.with_file_name("tmp-extract");
if tmp_extract_dir.exists() {
⋮----
release.unpack(&tmp_extract_dir)?;
⋮----
progress_bar.finish_and_clear();
Ok(())
⋮----
fn load_release_version(version_yml: &Path) -> Result<ReleaseVersion, String> {
⋮----
.map_err(|err| format!("Unable to open {version_yml:?}: {err:?}"))?;
⋮----
.map_err(|err| format!("Unable to parse {version_yml:?}: {err:?}"))?;
Ok(version)
⋮----
fn load_release_target(release_dir: &Path) -> Result<String, String> {
⋮----
version_yml.push("solana-release");
version_yml.push("version.yml");
let version = load_release_version(&version_yml)?;
Ok(version.target)
⋮----
fn timestamp_secs() -> u64 {
⋮----
.duration_since(SystemTime::UNIX_EPOCH)
.unwrap()
.as_secs()
⋮----
fn new_update_manifest(
⋮----
.get_account_data(&update_manifest_keypair.pubkey())
.is_err()
⋮----
let recent_blockhash = rpc_client.get_latest_blockhash()?;
⋮----
.get_minimum_balance_for_rent_exemption(SignedUpdateManifest::MAX_SPACE as usize)?;
⋮----
&from_keypair.pubkey(),
&update_manifest_keypair.pubkey(),
⋮----
vec![],
⋮----
let message = Message::new(&instructions, Some(&from_keypair.pubkey()));
⋮----
rpc_client.send_and_confirm_transaction(&transaction)?;
⋮----
fn store_update_manifest(
⋮----
let message = Message::new(&[instruction], Some(&from_keypair.pubkey()));
⋮----
fn get_update_manifest(
⋮----
.get_account_data(update_manifest_pubkey)
.map_err(|err| format!("Unable to fetch update manifest: {err}"))?;
let config_data = get_config_data(&data)
.map_err(|err| format!("Unable to get at config_data to update manifest: {err}"))?;
⋮----
.map_err(|err| format!("Unable to deserialize update manifest: {err}"))?;
Ok(signed_update_manifest.manifest)
⋮----
fn check_env_path_for_bin_dir(config: &Config) {
use std::env;
⋮----
.active_release_bin_dir()
.canonicalize()
.unwrap_or_default();
⋮----
Some(paths) => env::split_paths(&paths).any(|path| {
if let Ok(path) = path.canonicalize() {
⋮----
println!(
⋮----
pub fn string_to_winreg_bytes(s: &str) -> Vec<u8> {
⋮----
let v: Vec<_> = OsString::from(format!("{s}\x00")).encode_wide().collect();
unsafe { std::slice::from_raw_parts(v.as_ptr() as *const u8, v.len() * 2).to_vec() }
⋮----
pub fn string_from_winreg_value(val: &winreg::RegValue) -> Option<String> {
⋮----
slice::from_raw_parts(val.bytes.as_ptr() as *const u16, val.bytes.len() / 2)
⋮----
while s.ends_with('\u{0}') {
s.pop();
⋮----
Some(s)
⋮----
fn get_windows_path_var() -> Result<Option<String>, String> {
⋮----
.open_subkey_with_flags("Environment", KEY_READ | KEY_WRITE)
.map_err(|err| format!("Unable to open HKEY_CURRENT_USER\\Environment: {err}"))?;
let reg_value = environment.get_raw_value("PATH");
⋮----
if let Some(s) = string_from_winreg_value(&val) {
Ok(Some(s))
⋮----
Ok(None)
⋮----
Err(ref e) if e.kind() == io::ErrorKind::NotFound => Ok(Some(String::new())),
Err(e) => Err(e.to_string()),
⋮----
fn add_to_path(new_path: &str) -> bool {
⋮----
get_windows_path_var().unwrap_or_else(|err| panic!("Unable to get PATH: {err}"))
⋮----
if !old_path.contains(new_path) {
let mut new_path = new_path.to_string();
if !old_path.is_empty() {
new_path.push(';');
new_path.push_str(&old_path);
⋮----
.unwrap_or_else(|err| panic!("Unable to open HKEY_CURRENT_USER\\Environment: {err}"));
⋮----
bytes: string_to_winreg_bytes(&new_path),
⋮----
.set_raw_value("PATH", &reg_value)
.unwrap_or_else(|err| panic!("Unable set HKEY_CURRENT_USER\\Environment\\PATH: {err}"));
⋮----
SendMessageTimeoutA(
⋮----
c"Environment".as_ptr() as LPARAM,
⋮----
let shell_export_string = format!("\nexport PATH=\"{new_path}:$PATH\"");
⋮----
let mut rcfiles = vec![dirs_next::home_dir().map(|p| p.join(".profile"))];
⋮----
if shell.contains("zsh") {
⋮----
.ok()
.map(PathBuf::from)
.or_else(dirs_next::home_dir);
let zprofile = zdotdir.map(|p| p.join(".zprofile"));
rcfiles.push(zprofile);
⋮----
if let Some(bash_profile) = dirs_next::home_dir().map(|p| p.join(".bash_profile")) {
if bash_profile.exists() {
rcfiles.push(Some(bash_profile));
⋮----
let rcfiles = rcfiles.into_iter().filter_map(|f| f.filter(|f| f.exists()));
⋮----
if !rcfile.exists() {
⋮----
fn read_file(path: &Path) -> io::Result<String> {
let mut file = fs::OpenOptions::new().read(true).open(path)?;
⋮----
Ok(contents)
⋮----
match read_file(&rcfile) {
⋮----
println!("Unable to read {rcfile:?}: {err}");
⋮----
if !contents.contains(&shell_export_string) {
⋮----
fn append_file(dest: &Path, line: &str) -> io::Result<()> {
use std::io::Write;
⋮----
.append(true)
.create(true)
.open(dest)?;
writeln!(&mut dest_file, "{line}")?;
dest_file.sync_data()?;
⋮----
append_file(&rcfile, &shell_export_string).unwrap_or_else(|err| {
println!("Unable to append to {rcfile:?}: {err}");
⋮----
pub fn init(
⋮----
let mut current_config = Config::load(config_file).unwrap_or_default();
⋮----
config.save(config_file)?;
⋮----
init_or_update(config_file, true, false)?;
⋮----
add_to_path(config.active_release_bin_dir().to_str().unwrap())
⋮----
check_env_path_for_bin_dir(&config);
⋮----
fn github_release_download_url(release_semver: &str) -> String {
format!(
⋮----
fn release_channel_download_url(release_channel: &str) -> String {
⋮----
fn release_channel_version_url(release_channel: &str) -> String {
⋮----
fn print_update_manifest(update_manifest: &UpdateManifest) {
⋮----
.timestamp_opt(update_manifest.timestamp_secs as i64, 0)
.unwrap();
println_name_value(&format!("{BULLET}release date:"), &when.to_string());
println_name_value(
&format!("{BULLET}download URL:"),
⋮----
pub fn info(config_file: &str, local_info_only: bool, eval: bool) -> Result<(), String> {
⋮----
.map(|er| match er {
⋮----
.and_then(|channel| {
println!("SOLANA_INSTALL_ACTIVE_CHANNEL={channel}",);
⋮----
return Ok(());
⋮----
println_name_value("Configuration:", config_file);
⋮----
config.active_release_dir().to_str().unwrap_or("?"),
⋮----
fn print_release_version(config: &Config) {
⋮----
load_release_version(&config.active_release_dir().join("version.yml"))
⋮----
&format!("{BULLET}Release commit:"),
⋮----
println_name_value(&format!("{BULLET}Release version:"), release_semver);
⋮----
&format!("{BULLET}Release URL:"),
&github_release_download_url(release_semver),
⋮----
println_name_value(&format!("{BULLET}Release channel:"), release_channel);
⋮----
&release_channel_download_url(release_channel),
⋮----
print_release_version(&config);
⋮----
println_name_value("JSON RPC URL:", &config.json_rpc_url);
⋮----
&config.update_manifest_pubkey.to_string(),
⋮----
println_name_value("Installed version:", "");
⋮----
print_update_manifest(update_manifest);
⋮----
println_name_value("Installed version:", "None");
⋮----
update(config_file, true).map(|_| ())
⋮----
pub fn deploy(
⋮----
let from_keypair = read_keypair_file(from_keypair_file)
.map_err(|err| format!("Unable to read {from_keypair_file}: {err}"))?;
let update_manifest_keypair = read_keypair_file(update_manifest_keypair_file)
.map_err(|err| format!("Unable to read {update_manifest_keypair_file}: {err}"))?;
println_name_value("JSON RPC URL:", json_rpc_url);
⋮----
&update_manifest_keypair.pubkey().to_string(),
⋮----
let rpc_client = RpcClient::new(json_rpc_url.to_string());
⋮----
progress_bar.set_message(format!("{LOOKING_GLASS}Checking cluster..."));
⋮----
.get_balance(&from_keypair.pubkey())
.map_err(|err| {
format!("Unable to get the account balance of {from_keypair_file}: {err}")
⋮----
return Err(format!("{from_keypair_file} account balance is empty"));
⋮----
let (temp_dir, temp_archive, temp_archive_sha256) = download_to_temp(download_url, None)
.map_err(|err| format!("Unable to download {download_url}: {err}"))?;
if let Ok(update_manifest) = get_update_manifest(&rpc_client, &update_manifest_keypair.pubkey())
⋮----
let temp_release_dir = temp_dir.path().join("archive");
extract_release_archive(&temp_archive, &temp_release_dir).map_err(|err| {
format!("Unable to extract {temp_archive:?} into {temp_release_dir:?}: {err}")
⋮----
let release_target = load_release_target(&temp_release_dir)
.map_err(|err| format!("Unable to load release target from {temp_release_dir:?}: {err}"))?;
println_name_value("Update target:", &release_target);
⋮----
progress_bar.set_message(format!("{PACKAGE}Deploying update..."));
⋮----
account_pubkey: update_manifest_keypair.pubkey(),
⋮----
update_manifest.manifest.timestamp_secs = timestamp_secs();
update_manifest.manifest.download_url = download_url.to_string();
⋮----
update_manifest.sign(&update_manifest_keypair);
assert!(update_manifest.verify());
new_update_manifest(&rpc_client, &from_keypair, &update_manifest_keypair)
.map_err(|err| format!("Unable to create update manifest: {err}"))?;
store_update_manifest(
⋮----
.map_err(|err| format!("Unable to store update manifest: {err:?}"))?;
⋮----
println!("  {}{}", SPARKLE, style("Deployment successful").bold());
⋮----
fn symlink_dir<P: AsRef<Path>, Q: AsRef<Path>>(src: P, dst: Q) -> std::io::Result<()> {
⋮----
pub fn gc(config_file: &str) -> Result<(), String> {
⋮----
.map_err(|err| format!("Unable to read {}: {}", config.releases_dir.display(), err))?;
⋮----
.filter_map(|entry| entry.ok())
.filter_map(|entry| {
⋮----
.metadata()
⋮----
.map(|metadata| (entry.path(), metadata))
⋮----
.filter_map(|(release_path, metadata)| {
if metadata.is_dir() {
Some((release_path, metadata))
⋮----
.modified()
⋮----
.map(|modified_time| (release_path, modified_time))
⋮----
releases.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
⋮----
if releases.len() > MAX_CACHE_LEN {
let old_releases = releases.split_off(MAX_CACHE_LEN);
if !old_releases.is_empty() {
⋮----
progress_bar.set_length(old_releases.len() as u64);
⋮----
.template("{spinner:.green}{wide_msg} [{bar:40.cyan/blue}] {pos}/{len} ({eta})")
⋮----
progress_bar.set_message(format!("{RECYCLING}Removing old releases"));
⋮----
progress_bar.inc(1);
⋮----
pub struct GithubRelease {
⋮----
pub struct GithubError {
⋮----
pub struct GithubReleases(Vec<GithubRelease>);
fn semver_of(string: &str) -> Result<semver::Version, String> {
if string.starts_with('v') {
semver::Version::parse(string.split_at(1).1)
⋮----
.map_err(|err| err.to_string())
⋮----
fn check_for_newer_github_release(
⋮----
.user_agent("agave-install")
.build()
.map_err(|err| err.to_string())?;
⋮----
let download_url = github_release_download_url(current_release_semver);
⋮----
.head(download_url.as_str())
.send()
⋮----
if response.status() == reqwest::StatusCode::OK {
return Ok(Some(current_release_semver.to_string()));
⋮----
let version_filter = semver::VersionReq::parse(&format!(
⋮----
.ok();
⋮----
let mut all_releases = vec![];
let mut releases = vec![];
while page == 1 || releases.len() == PER_PAGE {
⋮----
("per_page", &format!("{PER_PAGE}")),
("page", &format!("{page}")),
⋮----
let request = client.get(url).build().map_err(|err| err.to_string())?;
let response = client.execute(request).map_err(|err| err.to_string())?;
⋮----
.map_err(|err| err.to_string())?
⋮----
.into_iter()
.filter_map(
⋮----
if let Ok(version) = semver_of(&tag_name) {
⋮----
.as_ref()
.is_none_or(|version_filter| version_filter.matches(&version))
⋮----
return Some(version);
⋮----
all_releases.extend_from_slice(&releases);
⋮----
return Err(response
⋮----
all_releases.sort();
Ok(all_releases.pop().map(|r| r.to_string()))
⋮----
pub enum SemverUpdateType {
⋮----
pub fn update(config_file: &str, check_only: bool) -> Result<bool, String> {
init_or_update(config_file, false, check_only)
⋮----
pub fn init_or_update(config_file: &str, is_init: bool, check_only: bool) -> Result<bool, String> {
⋮----
let release_dir = config.release_dir(current_release_semver);
if is_init && release_dir.exists() {
(current_release_semver.to_owned(), None, release_dir)
⋮----
progress_bar.set_message(format!("{LOOKING_GLASS}Checking for updates..."));
⋮----
let github_release = check_for_newer_github_release(
⋮----
return Err(format!("Unknown release: {current_release_semver}"));
⋮----
if let Ok(active_release_version) = load_release_version(
&config.active_release_dir().join("version.yml"),
⋮----
if format!("v{current_release_semver}")
⋮----
return Ok(false);
⋮----
Some(ExplicitRelease::Semver(release_semver.clone()));
let release_dir = config.release_dir(&release_semver);
let download_url_and_sha256 = if release_dir.exists() {
⋮----
Some((github_release_download_url(&release_semver), None))
⋮----
let version_url = release_channel_version_url(release_channel);
⋮----
download_to_temp(&version_url, None)
.map_err(|err| format!("Unable to download {version_url}: {err}"))?;
let update_release_version = load_release_version(&temp_file)?;
let release_id = format!("{}-{}", release_channel, update_release_version.commit);
let release_dir = config.release_dir(&release_id);
⋮----
release_dir.join("solana-release").join("version.yml");
let download_url = release_channel_download_url(release_channel);
if !current_release_version_yml.exists() {
⋮----
Some((download_url, None)),
⋮----
load_release_version(&current_release_version_yml)?;
⋮----
let rpc_client = RpcClient::new(config.json_rpc_url.clone());
let update_manifest = get_update_manifest(&rpc_client, &config.update_manifest_pubkey)?;
⋮----
if Some(&update_manifest) == config.current_update_manifest.as_ref() {
println!("Install is up to date");
⋮----
println!("\n{}", style("An update is available:").bold());
print_update_manifest(&update_manifest);
if timestamp_secs()
⋮----
return Err("Unable to update as system time seems unreliable".to_string());
⋮----
return Err("Unable to update to an older version".to_string());
⋮----
config.current_update_manifest = Some(update_manifest.clone());
let release_dir = config.release_dir(&update_manifest.download_sha256.to_string());
⋮----
let archive_sha256 = Some(update_manifest.download_sha256);
⋮----
"latest manifest".to_string(),
Some((download_url, archive_sha256)),
⋮----
return Ok(true);
⋮----
download_to_temp(&download_url, archive_sha256.as_ref())
⋮----
extract_release_archive(&temp_archive, &release_dir).map_err(|err| {
format!("Unable to extract {temp_archive:?} to {release_dir:?}: {err}")
⋮----
let release_target = load_release_target(&release_dir)
.map_err(|err| format!("Unable to load release target from {release_dir:?}: {err}"))?;
⋮----
return Err(format!("Incompatible update target: {release_target}"));
⋮----
let path = &release_dir.join(".touch");
⋮----
.create_new(true)
.write(true)
.open(path);
⋮----
let _ = fs::remove_dir_all(config.active_release_dir());
symlink_dir(
release_dir.join("solana-release"),
config.active_release_dir(),
⋮----
.map_err(|err| match err.raw_os_error() {
⋮----
"You need to run this command with administrator privileges.".to_string()
⋮----
_ => format!(
⋮----
gc(config_file)?;
⋮----
Ok(true)
⋮----
pub fn run(
⋮----
let mut full_program_path = config.active_release_bin_dir().join(program_name);
if cfg!(windows) && full_program_path.extension().is_none() {
full_program_path.set_extension("exe");
⋮----
if !full_program_path.exists() {
return Err(format!(
⋮----
let (signal_sender, signal_receiver) = unbounded();
⋮----
let _ = signal_sender.send(());
⋮----
.expect("Error setting Ctrl-C handler");
⋮----
Some(mut child) => match child.try_wait() {
⋮----
&format!("{program_name} exited with:"),
&status.to_string(),
⋮----
Ok(None) => Some(child),
⋮----
eprintln!("Error attempting to wait for program to exit: {err}");
⋮----
.args(&program_arguments)
.spawn()
⋮----
Ok(child) => Some(child),
⋮----
eprintln!("Failed to spawn {program_name}: {err:?}");
⋮----
if config.explicit_release.is_none() && now.elapsed().as_secs() > config.update_poll_secs {
match update(config_file, false) {
⋮----
stop_process(child).unwrap_or_else(|err| {
eprintln!("Failed to stop child: {err:?}");
⋮----
eprintln!("Failed to apply update: {err:?}");
⋮----
if let Ok(()) = signal_receiver.recv_timeout(Duration::from_secs(1)) {
⋮----
pub fn list(config_file: &str) -> Result<(), String> {
⋮----
let entries = fs::read_dir(&config.releases_dir).map_err(|err| {
⋮----
let dir_name = entry.file_name();
⋮----
load_release_version(&config.active_release_dir().join("version.yml"))?.channel;
let current = if current_version.contains(dir_name.to_string_lossy().as_ref()) {
⋮----
println!("{}{}", dir_name.to_string_lossy(), current);
⋮----
eprintln!("error listing installed versions: {err:?}");

================
File: install/src/config.rs
================
pub enum ExplicitRelease {
⋮----
pub struct Config {
⋮----
impl Config {
pub fn new(
⋮----
json_rpc_url: json_rpc_url.to_string(),
⋮----
releases_dir: PathBuf::from(data_dir).join("releases"),
active_release_dir: PathBuf::from(data_dir).join("active_release"),
⋮----
fn _load(config_file: &str) -> Result<Self, io::Error> {
⋮----
serde_yaml::from_reader(file).or_else(|err| {
let err_string = format!("{err:?}");
if err_string.contains(LEGACY_FMT_LOAD_ERR) {
Self::try_migrate_08(config_file).map_err(|_| io::Error::other(err_string))
⋮----
Err(io::Error::other(err_string))
⋮----
fn try_migrate_08(config_file: &str) -> Result<Self, io::Error> {
eprintln!("attempting to upgrade legacy config file");
let bak_filename = config_file.to_string() + ".bak";
⋮----
let result = File::open(config_file).and_then(|file| {
⋮----
.map_err(|err| io::Error::other(format!("{err:?}")))
.and_then(|config_08: Self| {
let save = config_08._save(config_file).map(|_| config_08);
if save.is_ok() {
⋮----
if result.is_err() {
eprintln!("config upgrade failed! restoring original");
⋮----
.and_then(|_| std::fs::remove_file(&bak_filename));
if restored.is_err() {
eprintln!("restoration failed! original: `{bak_filename}`");
⋮----
eprintln!("restoration succeeded!");
⋮----
eprintln!("config upgrade succeeded!");
⋮----
pub fn load(config_file: &str) -> Result<Self, String> {
Self::_load(config_file).map_err(|err| format!("Unable to load {config_file}: {err:?}"))
⋮----
fn _save(&self, config_file: &str) -> Result<(), io::Error> {
⋮----
serde_yaml::to_string(self).map_err(|err| io::Error::other(format!("{err:?}")))?;
if let Some(outdir) = Path::new(&config_file).parent() {
create_dir_all(outdir)?;
⋮----
file.write_all(b"---\n")?;
file.write_all(&serialized.into_bytes())?;
Ok(())
⋮----
pub fn save(&self, config_file: &str) -> Result<(), String> {
self._save(config_file)
.map_err(|err| format!("Unable to save {config_file}: {err:?}"))
⋮----
pub fn active_release_dir(&self) -> &PathBuf {
⋮----
pub fn active_release_bin_dir(&self) -> PathBuf {
self.active_release_dir.join("bin")
⋮----
pub fn release_dir(&self, release_id: &str) -> PathBuf {
self.releases_dir.join(release_id)
⋮----
mod test {
⋮----
fn test_save() {
let root_dir = env::var("CARGO_MANIFEST_DIR").expect("$CARGO_MANIFEST_DIR");
⋮----
let config_path = format!("{root_dir}/{config_name}");
⋮----
assert_eq!(config.save(config_name), Ok(()));
defer! {
⋮----
assert_eq!(
⋮----
fn test_load_serde_yaml_v_0_8_config() {
⋮----
let mut file = File::create(file_name).unwrap();
⋮----
writeln!(
⋮----
.unwrap();
let config = Config::load(file_name).unwrap();

================
File: install/src/defaults.rs
================
use std::sync::LazyLock;
⋮----
dirs_next::home_dir().map(|mut path| {
path.extend([".config", "solana", "install", "config.yml"]);
path.to_str().unwrap().to_string()
⋮----
path.extend([".config", "solana", "id.json"]);
⋮----
path.extend([".local", "share", "solana", "install"]);

================
File: install/src/lib.rs
================
mod build_env;
mod command;
mod config;
mod defaults;
mod stop_process;
mod update_manifest;
pub fn is_semver(semver: &str) -> Result<(), String> {
⋮----
Ok(_) => Ok(()),
Err(err) => Err(format!("{err:?}")),
⋮----
pub fn is_release_channel(channel: &str) -> Result<(), String> {
⋮----
"edge" | "beta" | "stable" => Ok(()),
_ => Err(format!("Invalid release channel {channel}")),
⋮----
pub fn is_explicit_release(string: String) -> Result<(), String> {
if string.starts_with('v') && is_semver(string.split_at(1).1).is_ok() {
return Ok(());
⋮----
is_semver(&string).or_else(|_| is_release_channel(&string))
⋮----
pub fn explicit_release_of(
⋮----
.value_of(name)
.map(ToString::to_string)
.map(|explicit_release| {
if explicit_release.starts_with('v')
&& is_semver(explicit_release.split_at(1).1).is_ok()
⋮----
config::ExplicitRelease::Semver(explicit_release.split_at(1).1.to_string())
} else if is_semver(&explicit_release).is_ok() {
⋮----
fn handle_init(matches: &ArgMatches<'_>, config_file: &str) -> Result<(), String> {
let json_rpc_url = matches.value_of("json_rpc_url").unwrap();
let update_manifest_pubkey = pubkey_of(matches, "update_manifest_pubkey");
let data_dir = matches.value_of("data_dir").unwrap();
let no_modify_path = matches.is_present("no_modify_path");
let explicit_release = explicit_release_of(matches, "explicit_release");
if update_manifest_pubkey.is_none() && explicit_release.is_none() {
Err(format!(
⋮----
&update_manifest_pubkey.unwrap_or_default(),
⋮----
pub fn main() -> Result<(), String> {
⋮----
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.setting(AppSettings::SubcommandRequiredElseHelp)
.arg({
⋮----
.short("c")
.long("config")
.value_name("PATH")
.takes_value(true)
.global(true)
.help("Configuration file to use");
⋮----
Some(ref config_file) => arg.default_value(config_file),
None => arg.required(true),
⋮----
.subcommand(
⋮----
.about("Initializes a new installation")
.setting(AppSettings::DisableVersion)
⋮----
.short("d")
.long("data-dir")
⋮----
.required(true)
.help("Directory to store install data");
⋮----
Some(ref data_dir) => arg.default_value(data_dir),
⋮----
.arg(
⋮----
.short("u")
.long("url")
.value_name("URL")
⋮----
.default_value(defaults::JSON_RPC_URL)
.validator(is_url)
.help("JSON RPC URL for the solana cluster"),
⋮----
.long("no-modify-path")
.help("Don't configure the PATH environment variable"),
⋮----
.short("p")
.long("pubkey")
.value_name("PUBKEY")
⋮----
.validator(is_pubkey)
.help("Public key of the update manifest"),
⋮----
.value_name("release")
.index(1)
.conflicts_with_all(&["json_rpc_url", "update_manifest_pubkey"])
.validator(is_explicit_release)
.help("The release version or channel to install"),
⋮----
.about("Displays information about the current installation")
⋮----
.short("l")
.long("local")
.help("Only display local information, don't check for updates"),
⋮----
.long("eval")
.help("Display information in a format that can be used with `eval`"),
⋮----
.about("Deploys a new update")
⋮----
.short("k")
.long("keypair")
⋮----
.help("Keypair file of the account that funds the deployment");
⋮----
.help("URL to the solana release archive"),
⋮----
.index(2)
⋮----
.help("Keypair file for the update manifest (/path/to/keypair.json)"),
⋮----
.about("Delete older releases from the install cache to reclaim disk space")
.setting(AppSettings::DisableVersion),
⋮----
.about("Checks for an update, and if available downloads and applies it")
⋮----
.about("Runs a program while periodically checking and applying software updates")
.after_help("The program will be restarted upon a successful software update")
⋮----
.help("program to run"),
⋮----
.multiple(true)
.help("Arguments to supply to the program"),
⋮----
.subcommand(SubCommand::with_name("list").about("List installed versions of solana cli"))
.get_matches();
let config_file = matches.value_of("config_file").unwrap();
match matches.subcommand() {
("init", Some(matches)) => handle_init(matches, config_file),
⋮----
let local_info_only = matches.is_present("local_info_only");
let eval = matches.is_present("eval");
command::info(config_file, local_info_only, eval).map(|_| ())
⋮----
let from_keypair_file = matches.value_of("from_keypair_file").unwrap();
⋮----
let download_url = matches.value_of("download_url").unwrap();
⋮----
matches.value_of("update_manifest_keypair_file").unwrap();
⋮----
("update", Some(_matches)) => command::update(config_file, false).map(|_| ()),
⋮----
let program_name = matches.value_of("program_name").unwrap();
⋮----
.values_of("program_arguments")
.map(Iterator::collect)
.unwrap_or_else(Vec::new);
⋮----
_ => unreachable!(),
⋮----
pub fn main_init() -> Result<(), String> {
⋮----
handle_init(&matches, config_file)

================
File: install/src/main.rs
================
fn main() -> Result<(), String> {

================
File: install/src/stop_process.rs
================
fn kill_process(process: &mut Child) -> Result<(), io::Error> {
if let Ok(()) = process.kill() {
process.wait()?;
⋮----
println!("Process {} has already exited", process.id());
⋮----
Ok(())
⋮----
pub fn stop_process(process: &mut Child) -> Result<(), io::Error> {
kill_process(process)
⋮----
let pid = Pid::from_raw(process.id() as i32);
match kill(pid, Signal::SIGINT) {
⋮----
while let Ok(None) = process.try_wait() {
⋮----
if let Ok(None) = process.try_wait() {
kill_process(process)?;
⋮----
println!("Invalid signal. Killing process {pid}");
⋮----
return Err(io::Error::new(
⋮----
format!("Insufficient permissions to signal process {pid}"),
⋮----
format!("Process {pid} does not exist"),
⋮----
format!("Unexpected error {e}"),

================
File: install/src/update_manifest.rs
================
pub struct UpdateManifest {
⋮----
pub struct SignedUpdateManifest {
⋮----
impl Signable for SignedUpdateManifest {
fn pubkey(&self) -> Pubkey {
⋮----
fn signable_data(&self) -> Cow<'_, [u8]> {
Cow::Owned(bincode::serialize(&self.manifest).expect("serialize"))
⋮----
fn get_signature(&self) -> Signature {
⋮----
fn set_signature(&mut self, signature: Signature) {
⋮----
impl SignedUpdateManifest {
⋮----
pub fn deserialize(
⋮----
if !manifest.verify() {
Err(io::Error::other("Manifest failed to verify").into())
⋮----
Ok(manifest)

================
File: install/.gitignore
================
/target/
/farf/

================
File: install/agave-install-init.sh
================
{
if [ -z "$SOLANA_DOWNLOAD_ROOT" ]; then
    SOLANA_DOWNLOAD_ROOT="https://github.com/jito-foundation/jito-solana/releases/download/"
fi
GH_LATEST_RELEASE="https://api.github.com/repos/jito-foundation/jito-solana/releases/latest"
set -e
usage() {
    cat 1>&2 <<EOF
agave-install-init
initializes a new installation
USAGE:
    agave-install-init [FLAGS] [OPTIONS] --data_dir <PATH> --pubkey <PUBKEY>
FLAGS:
    -h, --help              Prints help information
        --no-modify-path    Don't configure the PATH environment variable
OPTIONS:
    -d, --data-dir <PATH>    Directory to store install data
    -u, --url <URL>          JSON RPC URL for the solana cluster
    -p, --pubkey <PUBKEY>    Public key of the update manifest
EOF
}
main() {
    downloader --check
    need_cmd uname
    need_cmd mktemp
    need_cmd chmod
    need_cmd mkdir
    need_cmd rm
    need_cmd sed
    need_cmd grep
    for arg in "$@"; do
      case "$arg" in
        -h|--help)
          usage
          exit 0
          ;;
        *)
          ;;
      esac
    done
    _ostype="$(uname -s)"
    _cputype="$(uname -m)"
    case "$_ostype" in
    Linux)
      _ostype=unknown-linux-gnu
      ;;
    Darwin)
      if [[ $_cputype = arm64 ]]; then
        _cputype=aarch64
      fi
      _ostype=apple-darwin
      ;;
    *)
      err "machine architecture is currently unsupported"
      ;;
    esac
    TARGET="${_cputype}-${_ostype}"
    temp_dir="$(mktemp -d 2>/dev/null || ensure mktemp -d -t agave-install-init)"
    ensure mkdir -p "$temp_dir"
    if [ -n "$SOLANA_RELEASE" ]; then
      release="$SOLANA_RELEASE"
    else
      release_file="$temp_dir/release"
      printf 'looking for latest release\n' 1>&2
      ensure downloader "$GH_LATEST_RELEASE" "$release_file"
      release=$(\
        grep -m 1 \"tag_name\": "$release_file" \
        | sed -ne 's/^ *"tag_name": "\([^"]*\)",$/\1/p' \
      )
      if [ -z "$release" ]; then
        err 'Unable to figure latest release'
      fi
    fi
    download_url="$SOLANA_DOWNLOAD_ROOT/$release/agave-install-init-$TARGET"
    solana_install_init="$temp_dir/agave-install-init"
    printf 'downloading %s installer\n' "$release" 1>&2
    ensure mkdir -p "$temp_dir"
    ensure downloader "$download_url" "$solana_install_init"
    ensure chmod u+x "$solana_install_init"
    if [ ! -x "$solana_install_init" ]; then
        printf '%s\n' "Cannot execute $solana_install_init (likely because of mounting /tmp as noexec)." 1>&2
        printf '%s\n' "Please copy the file to a location where you can execute binaries and run ./agave-install-init." 1>&2
        exit 1
    fi
    if [ -z "$1" ]; then
      ignore "$solana_install_init" $SOLANA_INSTALL_INIT_ARGS
    else
      ignore "$solana_install_init" "$@"
    fi
    retval=$?
    ignore rm "$solana_install_init"
    ignore rm -rf "$temp_dir"
    return "$retval"
}
err() {
    printf 'agave-install-init: %s\n' "$1" >&2
    exit 1
}
need_cmd() {
    if ! check_cmd "$1"; then
        err "need '$1' (command not found)"
    fi
}
check_cmd() {
    command -v "$1" > /dev/null 2>&1
}
ensure() {
    if ! "$@"; then
      err "command failed: $*"
    fi
}
ignore() {
    "$@"
}
downloader() {
    if check_cmd curl; then
        program=curl
    elif check_cmd wget; then
        program=wget
    else
        program='curl or wget'
    fi
    if [ "$1" = --check ]; then
        need_cmd "$program"
    elif [ "$program" = curl ]; then
        curl -sSfL "$1" -o "$2"
    elif [ "$program" = wget ]; then
        wget "$1" -O "$2"
    else
        err "Unknown downloader"
    fi
}
main "$@"
}

================
File: install/build.rs
================
use std::time::SystemTime;
fn main() {
println!(

================
File: install/Cargo.toml
================
[package]
name = "agave-install"
description = "The solana cluster software installer"
documentation = "https://docs.rs/agave-install"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
agave-logger = { workspace = true }
atty = { workspace = true }
bincode = { workspace = true }
bzip2 = { workspace = true }
chrono = { workspace = true, features = ["default", "serde"] }
clap = { workspace = true }
console = { workspace = true }
crossbeam-channel = { workspace = true }
ctrlc = { workspace = true, features = ["termination"] }
dirs-next = { workspace = true }
indicatif = { workspace = true }
nix = { workspace = true, features = ["signal"] }
reqwest = { workspace = true, features = ["blocking", "brotli", "deflate", "gzip", "rustls-tls", "json"] }
scopeguard = { workspace = true }
semver = { workspace = true }
serde = { workspace = true }
serde_yaml = { workspace = true }
serde_yaml_08 = { package = "serde_yaml", version = "0.8.26" }
solana-clap-utils = { workspace = true }
solana-config-interface = { version = "=2.0.0", features = ["bincode"] }
solana-hash = "=3.1.0"
solana-keypair = "=3.0.1"
solana-message = "=3.0.1"
solana-pubkey = { version = "=3.0.0", default-features = false }
solana-rpc-client = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-signature = { version = "=3.1.0", default-features = false }
solana-signer = "=3.0.0"
solana-transaction = "=3.0.2"
solana-version = { workspace = true }
tar = { workspace = true }
tempfile = { workspace = true }
url = { workspace = true }

[target."cfg(windows)".dependencies]
winapi = { workspace = true, features = ["minwindef", "winuser"] }
winreg = { workspace = true }

================
File: install/install-help.sh
================
set -e
cd "$(dirname "$0")"/..
cargo="$(readlink -f "./cargo")"
"$cargo" build --package agave-install
export PATH=$PWD/target/debug:$PATH
echo "\`\`\`manpage"
agave-install --help
echo "\`\`\`"
echo ""
commands=(init info deploy update run)
for x in "${commands[@]}"; do
    echo "\`\`\`manpage"
    agave-install "${x}" --help
    echo "\`\`\`"
    echo ""
done

================
File: io-uring/src/lib.rs
================
mod ring;
mod slab;
⋮----
pub fn io_uring_supported() -> bool {
⋮----
IO_URING_SUPPORTED_ONCE.call_once(|| {
fn check() -> io::Result<()> {
⋮----
if !ring.params().is_feature_nodrop() {
return Err(io::Error::other("no IORING_FEAT_NODROP"));
⋮----
if !ring.params().is_feature_sqpoll_nonfixed() {
return Err(io::Error::other("no IORING_FEAT_SQPOLL_NONFIXED"));
⋮----
Ok(())
⋮----
IO_URING_SUPPORTED = match check() {

================
File: io-uring/src/ring.rs
================
pub struct Ring<T, E: RingOp<T>> {
⋮----
pub fn new(ring: IoUring, ctx: T) -> Self {
⋮----
entries: FixedSlab::with_capacity(ring.params().cq_entries() as usize),
⋮----
pub fn context(&self) -> &T {
⋮----
pub fn context_mut(&mut self) -> &mut T {
⋮----
pub unsafe fn register_buffers(&self, iovecs: &[libc::iovec]) -> io::Result<()> {
unsafe { self.ring.submitter().register_buffers(iovecs) }
⋮----
pub fn register_files(&self, fds: &[RawFd]) -> io::Result<()> {
self.ring.submitter().register_files(fds)
⋮----
pub fn push(&mut self, op: E) -> io::Result<()> {
⋮----
self.process_completions()?;
if !self.entries.is_full() {
⋮----
self.submit_and_wait(1, None)?;
⋮----
let key = self.entries.insert(op);
let entry = self.entries.get_mut(key).unwrap().entry();
let entry = entry.user_data(key as u64);
while unsafe { self.ring.submission().push(&entry) }.is_err() {
self.submit()?;
⋮----
Ok(())
⋮----
pub fn submit(&mut self) -> io::Result<()> {
self.submit_and_wait(0, None).map(|_| ())
⋮----
pub fn submit_and_wait(&mut self, want: usize, timeout: Option<Duration>) -> io::Result<usize> {
⋮----
args = args.timespec(&ts);
⋮----
match self.ring.submitter().submit_with_args(want, &args) {
Ok(n) => return Ok(n),
Err(e) if e.raw_os_error() == Some(libc::ETIME) => return Ok(0),
Err(e) if e.raw_os_error() == Some(libc::EBUSY) => {
⋮----
Err(e) if e.raw_os_error() == Some(libc::EINTR) => return Ok(0),
Err(e) => return Err(e),
⋮----
pub fn process_completions(&mut self) -> io::Result<()> {
let mut completion = self.ring.completion();
let mut new_entries = smallvec![];
while let Some(cqe) = completion.next() {
let completed_key = cqe.user_data() as usize;
let entry = self.entries.get_mut(completed_key).unwrap();
let result = entry.result(cqe.result());
⋮----
let res = entry.complete(&mut comp_ctx, result);
if !cqueue::more(cqe.flags()) {
self.entries.remove(completed_key);
⋮----
if !new_entries.is_empty() {
completion.sync();
drop(completion);
for new_entry in new_entries.drain(..) {
self.push(new_entry)?;
⋮----
completion = self.ring.completion();
⋮----
pub fn drain(&mut self) -> io::Result<()> {
⋮----
if self.entries.is_empty() {
⋮----
match self.ring.submitter().submit_with_args(
⋮----
&SubmitArgs::new().timespec(&Timespec::from(Duration::from_millis(10))),
⋮----
Err(e) if e.raw_os_error() == Some(libc::ETIME) => {}
⋮----
pub trait RingOp<T> {
⋮----
fn result(&self, res: i32) -> io::Result<i32> {
⋮----
Err(io::Error::from_raw_os_error(res.wrapping_neg()))
⋮----
Ok(res)
⋮----
pub struct Completion<'a, T, E: RingOp<T>> {
// Give new_entries a stack size of 2 to avoid heap allocations in the common case where only
// one or two ops are queued from a completion handler.
//
// It's common to want to queue some extra work after a completion, for instance if you've
⋮----
pub fn push(&mut self, op: E) {
self.new_entries.push(op);

================
File: io-uring/src/slab.rs
================
use slab::Slab;
pub struct FixedSlab<T> {
⋮----
pub fn with_capacity(cap: usize) -> Self {
⋮----
pub fn len(&self) -> usize {
self.inner.len()
⋮----
pub fn capacity(&self) -> usize {
self.inner.capacity()
⋮----
pub fn is_empty(&self) -> bool {
self.inner.is_empty()
⋮----
pub fn is_full(&self) -> bool {
self.len() == self.capacity()
⋮----
pub fn insert(&mut self, value: T) -> usize {
if self.is_full() {
panic!("FixedSlab is full, cannot insert new value");
⋮----
self.inner.insert(value)
⋮----
pub fn remove(&mut self, key: usize) -> T {
self.inner.remove(key)
⋮----
pub fn get_mut(&mut self, key: usize) -> Option<&mut T> {
self.inner.get_mut(key)

================
File: io-uring/Cargo.toml
================
[package]
name = "agave-io-uring"
description = "Agave io_uring wrapper"
documentation = "https://docs.rs/agave-io-uring"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
libc = { workspace = true }
log = { workspace = true }
slab = { workspace = true }
smallvec = { workspace = true }

[target.'cfg(target_os = "linux")'.dependencies]
io-uring = { workspace = true }

================
File: jito-protos/src/lib.rs
================
pub mod proto {
pub mod auth {
⋮----
pub mod block_engine {
⋮----
pub mod bundle {
⋮----
pub mod packet {
⋮----
pub mod relayer {
⋮----
pub mod shared {
⋮----
pub mod bam_api {
⋮----
pub mod bam_types {

================
File: jito-protos/build.rs
================
use tonic_build::configure;
fn main() -> Result<(), std::io::Error> {
⋮----
if std::env::var(PROTOC_ENVAR).is_err() {
⋮----
let proto = proto_base_path.join(proto_file);
println!("cargo:rerun-if-changed={}", proto.display());
protos.push(proto);
⋮----
let proto = proto_base_path_jds.join(proto_file);
⋮----
configure()
.build_client(true)
.build_server(true)
.server_mod_attribute(".", "#[allow(clippy::default_trait_access)]")
.server_mod_attribute(".", "#[allow(clippy::mixed_attributes_style)]")
.type_attribute(
⋮----
.compile(&protos, &[proto_base_path, proto_base_path_jds])

================
File: jito-protos/Cargo.toml
================
[package]
name = "jito-protos"
version = { workspace = true }
edition = { workspace = true }
publish = false

[features]
agave-unstable-api = []

[dependencies]
bytes = { workspace = true }
prost = { workspace = true }
prost-types = { workspace = true }
tonic = { workspace = true }

[build-dependencies]
tonic-build = { workspace = true }

# windows users should install the protobuf compiler manually and set the PROTOC
# envar to point to the installed binary
[target."cfg(not(windows))".build-dependencies]
protobuf-src = { workspace = true }

================
File: keygen/src/keygen.rs
================
mod smallest_length_44_public_key {
use solana_pubkey::Pubkey;
⋮----
fn assert_length() {
use crate::smallest_length_44_public_key;
assert_eq!(smallest_length_44_public_key::PUBKEY.to_string().len(), 44);
⋮----
struct GrindMatch {
⋮----
enum GrindType {
⋮----
fn grind_parser(grind_type: GrindType) -> ValueParser {
⋮----
if v.matches(':').count() != required_div_count || (v.starts_with(':') || v.ends_with(':'))
⋮----
return Err(format!("Expected : between {prefix_suffix} and COUNT"));
⋮----
let mut args: Vec<&str> = v.split(':').collect();
let count = args.pop().unwrap().parse::<u64>();
for arg in args.iter() {
⋮----
.into_vec()
.map_err(|err| format!("{}: {:?}", args[0], err))?;
⋮----
if count.is_err() || count.unwrap() == 0 {
return Err(String::from("Expected COUNT to be of type u64"));
⋮----
Ok(v.to_string())
⋮----
fn get_keypair_from_matches(
⋮----
let keypair_source = if matches.try_contains_id("keypair")? {
matches.get_one::<SignerSource>("keypair").unwrap()
} else if !config.keypair_path.is_empty() {
⋮----
let mut path = dirs_next::home_dir().expect("home directory");
path.extend([".config", "solana", "id.json"]);
config_source = SignerSource::parse(path.to_str().unwrap())?;
⋮----
signer_from_source(matches, keypair_source, "pubkey recovery", wallet_manager)
⋮----
fn output_keypair(
⋮----
write_keypair(keypair, &mut stdout)?;
⋮----
write_keypair_file(keypair, outfile)?;
println!("Wrote {source} keypair to {outfile}");
⋮----
Ok(())
⋮----
fn grind_print_info(grind_matches: &[GrindMatch], num_threads: usize) {
println!("Searching with {num_threads} threads for:");
⋮----
if gm.count.load(Ordering::Relaxed) > 1 {
msg.push("pubkeys".to_string());
msg.push("start".to_string());
msg.push("end".to_string());
⋮----
msg.push("pubkey".to_string());
msg.push("starts".to_string());
msg.push("ends".to_string());
⋮----
println!(
⋮----
fn grind_parse_args(
⋮----
let args: Vec<&str> = sw.split(':').collect();
grind_matches.push(GrindMatch {
⋮----
args[0].to_lowercase()
⋮----
args[0].to_string()
⋮----
ends: "".to_string(),
count: AtomicU64::new(args[1].parse::<u64>().unwrap()),
⋮----
let args: Vec<&str> = ew.split(':').collect();
⋮----
starts: "".to_string(),
⋮----
let args: Vec<&str> = swew.split(':').collect();
⋮----
args[1].to_lowercase()
⋮----
args[1].to_string()
⋮----
count: AtomicU64::new(args[2].parse::<u64>().unwrap()),
⋮----
grind_print_info(&grind_matches, num_threads);
⋮----
fn app<'a>(num_threads: &'a str, crate_version: &'a str) -> Command<'a> {
Command::new(crate_name!())
.about(crate_description!())
.version(crate_version)
.subcommand_required(true)
.arg_required_else_help(true)
.arg({
⋮----
.short('C')
.long("config")
.value_name("FILEPATH")
.takes_value(true)
.global(true)
.help("Configuration file to use");
⋮----
arg.default_value(config_file)
⋮----
.subcommand(
⋮----
.about("Verify a keypair can sign and verify a message.")
.arg(
⋮----
.index(1)
.value_name("PUBKEY")
⋮----
.required(true)
.help("Public key"),
⋮----
.index(2)
.value_name("KEYPAIR")
⋮----
.value_parser(SignerSourceParserBuilder::default().allow_all().build())
.help("Filepath or URL to a keypair"),
⋮----
.about(
⋮----
.disable_version_flag(true)
⋮----
.short('o')
.long("outfile")
⋮----
.help("Path to generated file"),
⋮----
.short('f')
.long("force")
.help("Overwrite the output file if it exists"),
⋮----
.arg(Arg::new("silent").short('s').long("silent").help(
⋮----
.arg(derivation_path_arg())
.key_generation_common_args()
.arg(no_outfile_arg().conflicts_with_all(&["outfile", "silent"])),
⋮----
.about("Grind for vanity keypairs")
⋮----
.long("ignore-case")
.help("Performs case insensitive matches"),
⋮----
.long("starts-with")
.value_name("PREFIX:COUNT")
.number_of_values(1)
⋮----
.action(ArgAction::Append)
.multiple_values(true)
.value_parser(grind_parser(GrindType::Starts))
.help(
⋮----
.long("ends-with")
.value_name("SUFFIX:COUNT")
⋮----
.value_parser(grind_parser(GrindType::Ends))
⋮----
.long("starts-and-ends-with")
.value_name("PREFIX:SUFFIX:COUNT")
⋮----
.value_parser(grind_parser(GrindType::StartsAndEnds))
⋮----
.long("num-threads")
.value_name("NUMBER")
⋮----
.value_parser(value_parser!(usize))
.default_value(num_threads)
.help("Specify the number of grind threads"),
⋮----
.arg(Arg::new("use_mnemonic").long("use-mnemonic").help(
⋮----
.arg(derivation_path_arg().requires("use_mnemonic"))
⋮----
no_outfile_arg()
.requires("use_mnemonic"),
⋮----
.about("Display the pubkey from a keypair file")
⋮----
.long(SKIP_SEED_PHRASE_VALIDATION_ARG.long)
.help(SKIP_SEED_PHRASE_VALIDATION_ARG.help),
⋮----
.about("Display the BLS pubkey derived from given ed25519 keypair file")
⋮----
.about("Recover keypair from seed phrase and optional BIP39 passphrase")
⋮----
.value_parser(
⋮----
.allow_prompt()
.allow_legacy()
.build(),
⋮----
.help("`prompt:` URI scheme or `ASK` keyword"),
⋮----
fn write_pubkey_file(outfile: &str, pubkey: Pubkey) -> Result<(), Box<dyn std::error::Error>> {
use std::io::Write;
let printable = format!("{pubkey}");
⋮----
if let Some(outdir) = std::path::Path::new(&outfile).parent() {
⋮----
f.write_all(&serialized.into_bytes())?;
⋮----
fn write_bls_pubkey_file(
⋮----
let printable = format!("{bls_pubkey}");
⋮----
fn main() -> Result<(), Box<dyn error::Error>> {
let default_num_threads = num_cpus::get().to_string();
let matches = app(&default_num_threads, solana_version::version!())
.try_get_matches()
.unwrap_or_else(|e| e.exit());
do_main(&matches).map_err(|err| DisplayError::new_as_boxed(err).into())
⋮----
fn do_main(matches: &ArgMatches) -> Result<(), Box<dyn error::Error>> {
⋮----
Config::load(config_file).unwrap_or_default()
⋮----
let subcommand = matches.subcommand().unwrap();
⋮----
get_keypair_from_matches(matches, config, &mut wallet_manager)?.try_pubkey()?;
if matches.try_contains_id("outfile")? {
let outfile = matches.get_one::<String>("outfile").unwrap();
check_for_overwrite(outfile, matches)?;
write_pubkey_file(outfile, pubkey)?;
⋮----
println!("{pubkey}");
⋮----
let keypair = get_keypair_from_matches(matches, config, &mut wallet_manager)?;
⋮----
write_bls_pubkey_file(outfile, bls_pubkey)?;
⋮----
println!("{bls_pubkey}");
⋮----
let outfile = if matches.try_contains_id("outfile")? {
matches.get_one::<String>("outfile").map(|s| s.as_str())
} else if matches.try_contains_id(NO_OUTFILE_ARG.name)? {
⋮----
Some(path.to_str().unwrap())
⋮----
Some(outfile) => check_for_overwrite(outfile, matches)?,
⋮----
let word_count = try_get_word_count(matches)?.unwrap();
⋮----
let language = try_get_language(matches)?.unwrap();
let silent = matches.try_contains_id("silent")?;
⋮----
println!("Generating a new keypair");
⋮----
let derivation_path = acquire_derivation_path(matches)?;
⋮----
let (passphrase, passphrase_message) = acquire_passphrase_and_message(matches)
.map_err(|err| format!("Unable to acquire passphrase: {err}"))?;
⋮----
Some(_) => keypair_from_seed_and_derivation_path(seed.as_bytes(), derivation_path),
None => keypair_from_seed(seed.as_bytes()),
⋮----
output_keypair(&keypair, outfile, "new")
.map_err(|err| format!("Unable to write {outfile}: {err}"))?;
⋮----
let phrase: &str = mnemonic.phrase();
let divider = String::from_utf8(vec![b'='; phrase.len()]).unwrap();
⋮----
matches.get_one::<String>("outfile").unwrap()
⋮----
path.to_str().unwrap()
⋮----
keypair_from_source(matches, source, keypair_name, true)?
⋮----
matches.try_contains_id(SKIP_SEED_PHRASE_VALIDATION_ARG.name)?;
keypair_from_seed_phrase(keypair_name, skip_validation, true, None, true)?
⋮----
output_keypair(&keypair, outfile, "recovered")?;
⋮----
let ignore_case = matches.try_contains_id("ignore_case")?;
let starts_with_args = if matches.try_contains_id("starts_with")? {
⋮----
.unwrap()
.map(|s| {
⋮----
s.to_lowercase()
⋮----
s.to_owned()
⋮----
.collect()
⋮----
let ends_with_args = if matches.try_contains_id("ends_with")? {
⋮----
let starts_and_ends_with_args = if matches.try_contains_id("starts_and_ends_with")? {
⋮----
if starts_with_args.is_empty()
&& ends_with_args.is_empty()
&& starts_and_ends_with_args.is_empty()
⋮----
return Err(
⋮----
.into(),
⋮----
let num_threads = *matches.get_one::<usize>("num_threads").unwrap();
let grind_matches = grind_parse_args(
⋮----
let use_mnemonic = matches.try_contains_id("use_mnemonic")?;
⋮----
acquire_passphrase_and_message(matches).unwrap()
⋮----
no_passphrase_and_message()
⋮----
let no_outfile = matches.try_contains_id(NO_OUTFILE_ARG.name)?;
// The vast majority of base58 encoded public keys have length 44, but
// these only encapsulate prefixes 1-9 and A-H.  If the user is searching
// for a keypair that starts with a prefix of J-Z or a-z, then there is no
// reason to waste time searching for a keypair that will never match
⋮----
.iter()
.map(|g| {
⋮----
g.starts.to_ascii_uppercase()
⋮----
g.starts.clone()
⋮----
target_key + &(0..44 - g.starts.len()).map(|_| "1").collect::<String>();
bs58::decode(target_key).into_vec()
⋮----
.filter_map(|s| s.ok())
.all(|s| s.len() > 32);
⋮----
.map(|_| {
let done = done.clone();
let attempts = attempts.clone();
let found = found.clone();
let grind_matches_thread_safe = grind_matches_thread_safe.clone();
let passphrase = passphrase.clone();
let passphrase_message = passphrase_message.clone();
let derivation_path = derivation_path.clone();
⋮----
if done.load(Ordering::Relaxed) {
⋮----
let attempts = attempts.fetch_add(1, Ordering::Relaxed);
if attempts.is_multiple_of(1_000_000) {
⋮----
Some(_) => keypair_from_seed_and_derivation_path(
seed.as_bytes(),
derivation_path.clone(),
⋮----
.unwrap();
(keypair, mnemonic.phrase().to_string())
⋮----
(Keypair::new(), "".to_string())
⋮----
// Skip keypairs that will never match the user specified prefix
⋮----
&& keypair.pubkey() >= smallest_length_44_public_key::PUBKEY
⋮----
let mut pubkey = bs58::encode(keypair.pubkey()).into_string();
⋮----
pubkey = pubkey.to_lowercase();
⋮----
for i in 0..grind_matches_thread_safe.len() {
if grind_matches_thread_safe[i].count.load(Ordering::Relaxed) == 0 {
⋮----
if (!grind_matches_thread_safe[i].starts.is_empty()
&& grind_matches_thread_safe[i].ends.is_empty()
&& pubkey.starts_with(&grind_matches_thread_safe[i].starts))
|| (grind_matches_thread_safe[i].starts.is_empty()
&& !grind_matches_thread_safe[i].ends.is_empty()
&& pubkey.ends_with(&grind_matches_thread_safe[i].ends))
|| (!grind_matches_thread_safe[i].starts.is_empty()
⋮----
&& pubkey.starts_with(&grind_matches_thread_safe[i].starts)
⋮----
let _found = found.fetch_add(1, Ordering::Relaxed);
⋮----
.fetch_sub(1, Ordering::Relaxed);
⋮----
write_keypair_file(
⋮----
format!("{}.json", keypair.pubkey()),
⋮----
String::from_utf8(vec![b'='; phrase.len()]).unwrap();
⋮----
if total_matches_found == grind_matches_thread_safe.len() {
done.store(true, Ordering::Relaxed);
⋮----
.collect();
⋮----
thread_handle.join().unwrap();
⋮----
vec![AccountMeta::new(keypair.pubkey(), true)],
⋮----
Some(&keypair.pubkey()),
⋮----
.serialize();
let signature = keypair.try_sign_message(&simple_message)?;
let pubkey_bs58 = matches.try_get_one::<String>("pubkey")?.unwrap();
let pubkey = bs58::decode(pubkey_bs58).into_vec().unwrap();
if signature.verify(&pubkey, &simple_message) {
println!("Verification for public key: {pubkey_bs58}: Success");
⋮----
let err_msg = format!("Verification for public key: {pubkey_bs58}: Failed");
return Err(err_msg.into());
⋮----
_ => unreachable!(),
⋮----
mod tests {
⋮----
fn read_pubkey_file(infile: &str) -> Result<Pubkey, Box<dyn std::error::Error>> {
⋮----
use std::str::FromStr;
Ok(Pubkey::from_str(&printable)?)
⋮----
fn read_bls_pubkey_file(infile: &str) -> Result<BLSPubkey, Box<dyn std::error::Error>> {
⋮----
Ok(BLSPubkey::from_str(&printable)?)
⋮----
fn process_test_command(args: &[&str]) -> Result<(), Box<dyn error::Error>> {
⋮----
let app_matches = app(&default_num_threads, solana_version).get_matches_from(args);
do_main(&app_matches)
⋮----
fn create_tmp_keypair_and_config_file(
⋮----
.path()
.join(format!("{}-keypair", keypair.pubkey()));
let keypair_outfile = keypair_path.into_os_string().into_string().unwrap();
write_keypair_file(&keypair, &keypair_outfile).unwrap();
⋮----
keypair_path: keypair_outfile.clone(),
⋮----
.join(format!("{}-config", keypair.pubkey()));
let config_outfile = config_path.into_os_string().into_string().unwrap();
config.save(&config_outfile).unwrap();
(keypair.pubkey(), keypair_outfile, config_outfile)
⋮----
fn tmp_outfile_path(out_dir: &TempDir, name: &str) -> String {
let path = out_dir.path().join(name);
path.into_os_string().into_string().unwrap()
⋮----
fn test_arguments() {
⋮----
// run clap internal assert statements
app(&default_num_threads, solana_version).debug_assert();
⋮----
fn test_verify() {
let keypair_out_dir = tempdir().unwrap();
let config_out_dir = tempdir().unwrap();
⋮----
create_tmp_keypair_and_config_file(&keypair_out_dir, &config_out_dir);
// success case using a keypair file
process_test_command(&[
⋮----
&correct_pubkey.to_string(),
⋮----
// success case using a config file
⋮----
// fail case using a keypair file
⋮----
let result = process_test_command(&[
⋮----
&incorrect_pubkey.to_string(),
⋮----
.unwrap_err()
.to_string();
let expected = format!("Verification for public key: {incorrect_pubkey}: Failed");
assert_eq!(result, expected);
// fail case using a config file
⋮----
// keypair file takes precedence over config file
let alt_keypair_out_dir = tempdir().unwrap();
let alt_config_out_dir = tempdir().unwrap();
⋮----
create_tmp_keypair_and_config_file(&alt_keypair_out_dir, &alt_config_out_dir);
⋮----
fn test_pubkey() {
⋮----
let outfile_dir = tempdir().unwrap();
let outfile_path = tmp_outfile_path(&outfile_dir, &expected_pubkey.to_string());
⋮----
let result_pubkey = read_pubkey_file(&outfile_path).unwrap();
assert_eq!(result_pubkey, expected_pubkey);
⋮----
// refuse to overwrite file
⋮----
let expected = format!("Refusing to overwrite {outfile_path} without --force flag");
⋮----
fn test_new() {
⋮----
// general success case
⋮----
// no outfile
⋮----
// sanity check on languages and word count combinations
⋮----
// sanity check derivation path
⋮----
// empty derivation path
⋮----
"m/44'/501'/0'/0'", // default derivation path
⋮----
fn test_grind() {
⋮----
fn test_read_write_pubkey() -> Result<(), std::boxed::Box<dyn std::error::Error>> {
⋮----
write_pubkey_file(filename, pubkey)?;
let read = read_pubkey_file(filename)?;
assert_eq!(read, pubkey);
⋮----
fn test_read_write_bls_pubkey() -> Result<(), std::boxed::Box<dyn std::error::Error>> {
⋮----
write_bls_pubkey_file(filename, bls_pubkey)?;
let read = read_bls_pubkey_file(filename)?;
assert_eq!(read, bls_pubkey);
⋮----
fn test_generate_bls_pubkey_from_existing_keypair() {
⋮----
let my_keypair = read_keypair_file(&keypair_file).unwrap();
⋮----
BLSKeypair::derive_from_signer(&my_keypair, BLS_KEYPAIR_DERIVE_SEED).unwrap();
let read_bls_pubkey = read_bls_pubkey_file(&outfile_path).unwrap();
assert_eq!(read_bls_pubkey, bls_keypair.public);

================
File: keygen/.gitignore
================
/target/
/farf/

================
File: keygen/Cargo.toml
================
[package]
name = "solana-keygen"
description = "Solana key generation utility"
documentation = "https://docs.rs/solana-keygen"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[[bin]]
name = "solana-keygen"
path = "src/keygen.rs"

[features]
default = ["remote-wallet-hidraw"]
agave-unstable-api = []
remote-wallet-hidraw = ["solana-remote-wallet/linux-static-hidraw"]
remote-wallet-libusb = ["solana-remote-wallet/linux-static-libusb"]

[dependencies]
agave-votor-messages = { workspace = true }
bs58 = { workspace = true, features = ["std"] }
clap = { version = "3.1.5", features = ["cargo"] }
dirs-next = { workspace = true }
num_cpus = { workspace = true }
serde_json = { workspace = true }
solana-bls-signatures = { workspace = true }
solana-clap-v3-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-derivation-path = "=3.0.0"
solana-instruction = { version = "=3.0.0", features = ["bincode"] }
solana-keypair = "=3.0.1"
solana-message = { version = "=3.0.1", features = ["bincode"] }
solana-pubkey = { version = "=3.0.0", default-features = false }
solana-remote-wallet = { workspace = true }
solana-seed-derivable = "=3.0.0"
solana-signer = "=3.0.0"
solana-version = { workspace = true }
tiny-bip39 = { workspace = true }

[dev-dependencies]
solana-pubkey = { workspace = true, features = ["rand"] }
tempfile = { workspace = true }

================
File: lattice-hash/benches/bench_lt_hash.rs
================
fn new_random_lt_hash(rng: &mut impl Rng) -> LtHash {
⋮----
hasher.update(&rng.random::<u64>().to_le_bytes());
⋮----
fn bench_mix_in(c: &mut Criterion) {
⋮----
let mut lt_hash1 = new_random_lt_hash(&mut rng);
let lt_hash2 = new_random_lt_hash(&mut rng);
c.bench_function("mix_in", |b| {
b.iter(|| lt_hash1.mix_in(&lt_hash2));
⋮----
fn bench_mix_out(c: &mut Criterion) {
⋮----
c.bench_function("mix_out", |b| {
b.iter(|| lt_hash1.mix_out(&lt_hash2));
⋮----
fn bench_checksum(c: &mut Criterion) {
⋮----
let lt_hash = new_random_lt_hash(&mut rng);
c.bench_function("checksum", |b| {
b.iter(|| lt_hash.checksum());
⋮----
fn bench_with(c: &mut Criterion) {
⋮----
c.bench_function("with", |b| {
b.iter(|| LtHash::with(&hasher));
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: lattice-hash/src/lib.rs
================
pub mod lt_hash;

================
File: lattice-hash/src/lt_hash.rs
================
pub struct LtHash(pub [u16; LtHash::NUM_ELEMENTS]);
impl LtHash {
⋮----
pub const fn identity() -> Self {
Self([0; Self::NUM_ELEMENTS])
⋮----
pub fn with(hasher: &blake3::Hasher) -> Self {
let mut reader = hasher.finalize_xof();
⋮----
reader.fill(bytemuck::must_cast_slice_mut(new.0.as_mut_slice()));
⋮----
pub fn mix_in(&mut self, other: &Self) {
for i in 0..self.0.len() {
self.0[i] = self.0[i].wrapping_add(other.0[i]);
⋮----
pub fn mix_out(&mut self, other: &Self) {
⋮----
self.0[i] = self.0[i].wrapping_sub(other.0[i]);
⋮----
pub fn checksum(&self) -> Checksum {
⋮----
Checksum(hash.into())
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
⋮----
write!(f, "{base64}")
⋮----
pub struct Checksum(pub [u8; Checksum::NUM_ELEMENTS]);
impl Checksum {
⋮----
let len = bs58::encode(&self.0).onto(buf.as_mut_slice()).unwrap();
let str = str::from_utf8(&buf[..len]).unwrap();
write!(f, "{str}")
⋮----
mod tests {
⋮----
fn new_random() -> Self {
⋮----
rand::rng().fill(&mut new.0);
⋮----
impl Add for LtHash {
type Output = Self;
fn add(mut self, rhs: Self) -> Self {
self.mix_in(&rhs);
⋮----
impl Sub for LtHash {
⋮----
fn sub(mut self, rhs: Self) -> Self {
self.mix_out(&rhs);
⋮----
impl Copy for LtHash {}
⋮----
fn test_identity() {
⋮----
assert_eq!(a, a + LtHash::identity());
assert_eq!(a, a - LtHash::identity());
⋮----
fn test_inverse() {
⋮----
assert_eq!(a, a + b - b);
assert_eq!(a, a - b + b);
⋮----
fn test_commutative() {
⋮----
assert_eq!(a + b, b + a);
⋮----
fn test_associative() {
⋮----
assert_eq!((a + b) + c, a + (b + c));
⋮----
fn test_distribute() {
⋮----
assert_eq!(a - b - c - d, a - (b + c + d));
⋮----
fn test_hello_world() {
let expected_hello_lt_hash = LtHash([
⋮----
let expected_hello_checksum = Checksum([
⋮----
let expected_world_lt_hash = LtHash([
⋮----
let expected_world_checksum = Checksum([
⋮----
hasher.update(input.as_bytes());
⋮----
assert_eq!(actual_lt_hash, expected_lt_hash);
let actual_checksum = actual_lt_hash.checksum();
assert_eq!(actual_checksum, expected_checksum);
⋮----
fn test_checksum_display() {
⋮----
let checksum = lt_hash.checksum();
let str = checksum.to_string();
assert_eq!(str.as_str(), "DoL6fvKuTpTQCyUh83NxQw2ewKzWYtq9gsTKp1eQiGC2");

================
File: lattice-hash/Cargo.toml
================
[package]
name = "solana-lattice-hash"
description = "Solana Lattice Hash"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
base64 = { workspace = true }
blake3 = { workspace = true }
bs58 = { workspace = true }
bytemuck = { workspace = true, features = ["must_cast"] }

[dev-dependencies]
criterion = { workspace = true }
rand = { workspace = true }
rand_chacha = { workspace = true }
solana-lattice-hash = { path = ".", features = ["agave-unstable-api"] }

[[bench]]
name = "bench_lt_hash"
harness = false

================
File: ledger/benches/blockstore.rs
================
extern crate solana_ledger;
extern crate test;
⋮----
fn bench_write_shreds(bench: &mut Bencher, entries: Vec<Entry>, ledger_path: &Path) {
⋮----
Blockstore::open(ledger_path).expect("Expected to be able to open database ledger");
bench.iter(move || {
let shreds = entries_to_test_shreds(&entries, 0, 0, true, 0);
blockstore.insert_shreds(shreds, None, false).unwrap();
⋮----
fn setup_read_bench(
⋮----
let entries = create_ticks(
⋮----
let shreds = entries_to_test_shreds(
⋮----
slot.saturating_sub(1),
⋮----
.insert_shreds(shreds, None, false)
.expect("Expected successful insertion of shreds into ledger");
⋮----
fn bench_write_small(bench: &mut Bencher) {
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
let entries = create_ticks(num_entries, 0, Hash::default());
bench_write_shreds(bench, entries, ledger_path.path());
⋮----
fn bench_write_big(bench: &mut Bencher) {
⋮----
fn bench_read_sequential(bench: &mut Bencher) {
⋮----
Blockstore::open(ledger_path.path()).expect("Expected to be able to open database ledger");
⋮----
setup_read_bench(&blockstore, num_small_shreds, num_large_shreds, slot);
⋮----
let start_index = rng.random_range(0..num_small_shreds + num_large_shreds);
⋮----
let _ = blockstore.get_data_shred(slot, i % total_shreds);
⋮----
fn bench_read_random(bench: &mut Bencher) {
⋮----
.map(|_| rng.random_range(0..total_shreds) as usize)
.collect();
⋮----
for i in indexes.iter() {
let _ = blockstore.get_data_shred(slot, *i as u64);
⋮----
fn bench_insert_data_shred_small(bench: &mut Bencher) {
⋮----
fn bench_insert_data_shred_big(bench: &mut Bencher) {
⋮----
fn bench_write_transaction_memos(b: &mut Bencher) {
⋮----
let signatures: Vec<Signature> = (0..64).map(|_| Signature::new_unique()).collect();
b.iter(|| {
for (slot, signature) in signatures.iter().enumerate() {
⋮----
.write_transaction_memos(
⋮----
"bench_write_transaction_memos".to_string(),
⋮----
.unwrap();
⋮----
fn bench_add_transaction_memos_to_batch(b: &mut Bencher) {
⋮----
let mut memos_batch = blockstore.get_write_batch().unwrap();
⋮----
.add_transaction_memos_to_batch(
⋮----
blockstore.write_batch(memos_batch).unwrap();
⋮----
fn bench_write_transaction_status(b: &mut Bencher) {
⋮----
.map(|_| {
vec![
⋮----
for (tx_idx, signature) in signatures.iter().enumerate() {
⋮----
.write_transaction_status(
⋮----
keys_with_writable[tx_idx].iter().map(|(k, v)| (k, *v)),
⋮----
fn bench_add_transaction_status_to_batch(b: &mut Bencher) {
⋮----
let mut status_batch = blockstore.get_write_batch().unwrap();
⋮----
.add_transaction_status_to_batch(
⋮----
blockstore.write_batch(status_batch).unwrap();

================
File: ledger/benches/make_shreds_from_entries.rs
================
fn make_dummy_hash<R: Rng>(rng: &mut R) -> Hash {
⋮----
fn make_dummy_transaction<R: Rng>(rng: &mut R) -> Transaction {
⋮----
rng.random(),
make_dummy_hash(rng),
⋮----
fn make_dummy_entry<R: Rng>(rng: &mut R) -> Entry {
let count = rng.random_range(1..20);
let transactions = repeat_with(|| make_dummy_transaction(rng))
.take(count)
.collect();
⋮----
&make_dummy_hash(rng),
⋮----
fn make_dummy_entries<R: Rng>(rng: &mut R, data_size: usize) -> Vec<Entry> {
⋮----
repeat_with(|| make_dummy_entry(rng))
.take_while(|entry| {
serialized_size += bincode::serialized_size(entry).unwrap();
⋮----
.collect()
⋮----
fn make_shreds_from_entries<R: Rng>(
⋮----
let (data, code) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
rng.random_range(0..2_000),
⋮----
(black_box(data), black_box(code))
⋮----
fn run_make_shreds_from_entries(
⋮----
let slot = 315_892_061 + rng.random_range(0..=100_000);
let parent_offset = rng.random_range(1..=u16::MAX);
⋮----
rng.random_range(0..64),
⋮----
.unwrap();
⋮----
let entries = make_dummy_entries(&mut rng, data_size);
let chained_merkle_root = make_dummy_hash(&mut rng);
⋮----
make_shreds_from_entries(
⋮----
c.bench_function(name, |b| {
b.iter(|| {
let (data, code) = make_shreds_from_entries(
⋮----
black_box(data);
black_box(code);
⋮----
fn run_recover_shreds(
⋮----
let fec_set_index = data[0].fec_set_index();
⋮----
.into_iter()
.filter(|shred| shred.fec_set_index() == fec_set_index)
⋮----
let num_code = num_code.min(code.len());
for _ in 0..num_code.min(data.len()) {
let k = rng.random_range(0..data.len());
data.remove(k);
⋮----
let j = rng.random_range(i..code.len());
code.swap(i, j);
⋮----
code.sort_unstable_by_key(|shred| shred.index());
⋮----
shreds.extend(code);
⋮----
let recovered_shreds = shred::recover(shreds.clone(), &reed_solomon_cache)
.unwrap()
⋮----
black_box(recovered_shreds);
⋮----
fn bench_make_shreds_from_entries(c: &mut Criterion) {
⋮----
let name = format!(
⋮----
run_make_shreds_from_entries(&name, c, num_packets, is_last_in_slot);
⋮----
fn bench_recover_shreds(c: &mut Criterion) {
⋮----
run_recover_shreds(&name, c, num_packets, num_code, is_last_in_slot);
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: ledger/benches/protobuf.rs
================
extern crate test;
⋮----
fn create_rewards() -> Rewards {
⋮----
.map(|i| Reward {
pubkey: solana_pubkey::new_rand().to_string(),
⋮----
reward_type: Some(RewardType::Fee),
⋮----
.collect()
⋮----
fn bincode_serialize_rewards(rewards: Rewards) -> Vec<u8> {
serialize(&rewards).unwrap()
⋮----
fn protobuf_serialize_rewards(rewards: Rewards) -> Vec<u8> {
let rewards: solana_storage_proto::convert::generated::Rewards = rewards.into();
let mut buffer = Vec::with_capacity(rewards.encoded_len());
rewards.encode(&mut buffer).unwrap();
⋮----
fn bincode_deserialize_rewards(bytes: &[u8]) -> Rewards {
deserialize(bytes).unwrap()
⋮----
fn protobuf_deserialize_rewards(bytes: &[u8]) -> Rewards {
⋮----
.unwrap()
.into()
⋮----
fn bench_serialize_rewards<S>(bench: &mut Bencher, serialize_method: S)
⋮----
let rewards = create_rewards();
bench.iter(move || {
let _ = serialize_method(rewards.clone());
⋮----
fn bench_deserialize_rewards<S, D>(bench: &mut Bencher, serialize_method: S, deserialize_method: D)
⋮----
let rewards_bytes = serialize_method(rewards);
⋮----
let _ = deserialize_method(&rewards_bytes);
⋮----
fn bench_serialize_bincode(bencher: &mut Bencher) {
bench_serialize_rewards(bencher, bincode_serialize_rewards);
⋮----
fn bench_serialize_protobuf(bencher: &mut Bencher) {
bench_serialize_rewards(bencher, protobuf_serialize_rewards);
⋮----
fn bench_deserialize_bincode(bencher: &mut Bencher) {
bench_deserialize_rewards(
⋮----
fn bench_deserialize_protobuf(bencher: &mut Bencher) {

================
File: ledger/proptest-regressions/blockstore_meta.txt
================
# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc d28b14f167a3950cfc2a5b82dff1e15c65e9ac23a5c249f812e69af96c3489ed # shrinks to coding_indices = 0..0, data_indices = 2984..15152, slot = 0

================
File: ledger/src/blockstore/blockstore_purge.rs
================
pub struct PurgeStats {
⋮----
pub enum PurgeType {
⋮----
impl Blockstore {
pub fn purge_slots(&self, from_slot: Slot, to_slot: Slot, purge_type: PurgeType) {
⋮----
self.run_purge_with_stats(from_slot, to_slot, purge_type, &mut purge_stats);
datapoint_info!(
⋮----
error!("Error: {e:?}; Purge failed in range {from_slot:?} to {to_slot:?}");
⋮----
pub fn set_max_expired_slot(&self, to_slot: Slot) {
let to_slot = to_slot.checked_add(1).unwrap();
self.db.set_oldest_slot(to_slot);
if let Err(err) = self.maybe_cleanup_highest_primary_index_slot(to_slot) {
warn!("Could not clean up TransactionStatusIndex: {err:?}");
⋮----
pub fn purge_and_compact_slots(&self, from_slot: Slot, to_slot: Slot) {
self.purge_slots(from_slot, to_slot, PurgeType::Exact);
⋮----
pub fn purge_from_next_slots(&self, from_slot: Slot, to_slot: Slot) {
⋮----
.slot_meta_iterator(0)
.expect("unable to iterate over meta")
⋮----
if last_print.elapsed().as_millis() > 2000 {
info!(
⋮----
let original_len = meta.next_slots.len();
⋮----
.retain(|slot| *slot < from_slot || *slot > to_slot);
if meta.next_slots.len() != original_len {
⋮----
self.put_meta(slot, &meta).expect("couldn't update meta");
⋮----
time.stop();
total_retain_us += time.as_us();
⋮----
pub(crate) fn run_purge(
⋮----
self.run_purge_with_stats(from_slot, to_slot, purge_type, &mut PurgeStats::default())
⋮----
pub(crate) fn purge_slot_cleanup_chaining(&self, slot: Slot) -> Result<bool> {
let Some(mut slot_meta) = self.meta(slot)? else {
return Err(BlockstoreError::SlotUnavailable);
⋮----
let mut write_batch = self.get_write_batch()?;
let columns_purged = self.purge_range(&mut write_batch, slot, slot, PurgeType::Exact)?;
⋮----
let parent_slot_meta = self.meta(parent_slot)?;
⋮----
.retain(|&next_slot| next_slot != slot);
⋮----
.put_in_batch(&mut write_batch, parent_slot, &parent_slot_meta)?;
⋮----
error!(
⋮----
slot_meta.clear_unconfirmed_slot();
⋮----
.put_in_batch(&mut write_batch, slot, &slot_meta)?;
self.write_batch(write_batch).inspect_err(|e| {
error!("Error: {e:?} while submitting write batch for slot {slot:?}")
⋮----
Ok(columns_purged)
⋮----
pub(crate) fn run_purge_with_stats(
⋮----
let columns_purged = self.purge_range(&mut write_batch, from_slot, to_slot, purge_type)?;
delete_range_timer.stop();
⋮----
write_timer.stop();
⋮----
self.purge_files_in_range(from_slot, to_slot);
⋮----
purge_files_in_range_timer.stop();
purge_stats.delete_range += delete_range_timer.as_us();
purge_stats.write_batch += write_timer.as_us();
purge_stats.delete_file_in_range += purge_files_in_range_timer.as_us();
⋮----
fn purge_range(
⋮----
.delete_range_in_batch(write_batch, from_slot, to_slot)
.is_ok()
⋮----
.is_ok();
⋮----
self.purge_special_columns_exact(write_batch, from_slot, to_slot)?;
⋮----
fn purge_files_in_range(&self, from_slot: Slot, to_slot: Slot) -> bool {
⋮----
.delete_file_in_range(from_slot, to_slot)
⋮----
fn special_columns_empty(&self) -> Result<bool> {
⋮----
.iter(IteratorMode::Start)?
.next()
.is_none();
⋮----
Ok(transaction_status_empty && address_signatures_empty)
⋮----
fn purge_special_columns_exact(
⋮----
if self.special_columns_empty()? {
return Ok(());
⋮----
let mut index0 = self.transaction_status_index_cf.get(0)?.unwrap_or_default();
let mut index1 = self.transaction_status_index_cf.get(1)?.unwrap_or_default();
let highest_primary_index_slot = self.get_highest_primary_index_slot();
⋮----
let mut indexes = vec![];
if highest_primary_index_slot.is_none() {
⋮----
indexes.push(0);
⋮----
indexes.push(1);
⋮----
let primary_indexes = slot_indexes(slot);
⋮----
self.get_slot_entries_with_shred_info(slot, 0, true )?;
⋮----
.into_iter()
.flat_map(|entry| entry.transactions);
for (i, transaction) in transactions.enumerate() {
if let Some(&signature) = transaction.signatures.first() {
⋮----
.delete_in_batch(batch, (signature, slot))?;
⋮----
if !primary_indexes.is_empty() {
⋮----
.delete_deprecated_in_batch(batch, signature)?;
⋮----
.delete_deprecated_in_batch(batch, (*primary_index, signature, slot))?;
⋮----
let meta = self.read_transaction_status((signature, slot))?;
let loaded_addresses = meta.map(|meta| meta.loaded_addresses);
⋮----
transaction.message.static_account_keys(),
loaded_addresses.as_ref(),
⋮----
u32::try_from(i).map_err(|_| BlockstoreError::TransactionIndexOverflow)?;
for pubkey in account_keys.iter() {
self.address_signatures_cf.delete_in_batch(
⋮----
self.address_signatures_cf.delete_deprecated_in_batch(
⋮----
index0.max_slot = from_slot.saturating_sub(1);
⋮----
.put_in_batch(batch, 0, &index0)?;
⋮----
index1.max_slot = from_slot.saturating_sub(1);
⋮----
.put_in_batch(batch, 1, &index1)?;
⋮----
self.set_highest_primary_index_slot(Some(max(index0.max_slot, index1.max_slot)))
⋮----
Ok(())
⋮----
pub mod tests {
⋮----
fn test_purge_slots() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
let (shreds, _) = make_many_slot_entries(0, 50, 5);
blockstore.insert_shreds(shreds, None, false).unwrap();
blockstore.purge_and_compact_slots(0, 5);
test_all_empty_or_min(&blockstore, 6);
blockstore.purge_and_compact_slots(0, 50);
test_all_empty_or_min(&blockstore, 100);
test_all_empty_or_min(&blockstore, 0);
⋮----
.unwrap()
.for_each(|(_, _)| {
panic!();
⋮----
fn test_purge_front_of_ledger() {
⋮----
.write_transaction_status(
⋮----
vec![
⋮----
.into_iter(),
⋮----
.unwrap();
⋮----
blockstore.run_purge(10, 20, PurgeType::Exact).unwrap();
⋮----
.iter(IteratorMode::Start)
⋮----
.collect();
assert_eq!(status_entries.len(), 10);
⋮----
fn clear_and_repopulate_transaction_statuses_for_test(blockstore: &Blockstore, max_slot: u64) {
blockstore.run_purge(0, max_slot, PurgeType::Exact).unwrap();
⋮----
assert_eq!(iter.next(), None);
populate_transaction_statuses_for_test(blockstore, 0, max_slot);
⋮----
fn populate_transaction_statuses_for_test(
⋮----
let entries = make_slot_entries_with_transactions(1);
let shreds = entries_to_test_shreds(
⋮----
x.saturating_sub(1),
⋮----
.iter()
.filter(|entry| !entry.is_tick())
.cloned()
.flat_map(|entry| entry.transactions)
.map(|transaction| transaction.signatures[0])
⋮----
let random_bytes: Vec<u8> = (0..64).map(|_| rand::random::<u8>()).collect();
⋮----
fn populate_deprecated_transaction_statuses_for_test(
⋮----
.write_deprecated_transaction_status(
⋮----
vec![&Pubkey::try_from(&random_bytes[..32]).unwrap()],
vec![&Pubkey::try_from(&random_bytes[32..]).unwrap()],
⋮----
fn test_special_columns_empty() {
⋮----
assert!(blockstore.special_columns_empty().unwrap());
⋮----
let entries = make_slot_entries_with_transactions(num_entries);
⋮----
slot.saturating_sub(1),
⋮----
for transaction in entries.into_iter().flat_map(|entry| entry.transactions) {
assert_eq!(transaction.signatures.len(), 1);
⋮----
.static_account_keys()
⋮----
.map(|key| (key, true)),
⋮----
assert!(!blockstore.special_columns_empty().unwrap());
⋮----
.run_purge(0, max_slot - 5, PurgeType::Exact)
⋮----
fn test_purge_transaction_status_exact() {
⋮----
clear_and_repopulate_transaction_statuses_for_test(&blockstore, max_slot);
blockstore.run_purge(10, 12, PurgeType::Exact).unwrap();
⋮----
let entry = status_entry_iterator.next().unwrap().0;
assert!(entry.1 <= max_slot || entry.1 > 0);
⋮----
assert_eq!(status_entry_iterator.next(), None);
drop(status_entry_iterator);
⋮----
blockstore.run_purge(2, 4, PurgeType::Exact).unwrap();
⋮----
assert!(entry.1 < 2 || entry.1 > 4);
⋮----
.run_purge(0, max_slot - 1, PurgeType::Exact)
⋮----
assert_eq!(entry.1, 9);
⋮----
blockstore.run_purge(0, 22, PurgeType::Exact).unwrap();
⋮----
fn purge_exact(blockstore: &Blockstore, oldest_slot: Slot) {
⋮----
.run_purge(0, oldest_slot - 1, PurgeType::Exact)
⋮----
fn purge_compaction_filter(blockstore: &Blockstore, oldest_slot: Slot) {
blockstore.db.set_oldest_slot(oldest_slot);
blockstore.transaction_status_cf.compact();
⋮----
fn test_purge_special_columns_with_old_data(purge: impl Fn(&Blockstore, Slot)) {
⋮----
populate_deprecated_transaction_statuses_for_test(&blockstore, 0, 0, 4);
populate_deprecated_transaction_statuses_for_test(&blockstore, 1, 5, 9);
populate_transaction_statuses_for_test(&blockstore, 10, 14);
⋮----
.get(0)
⋮----
.unwrap_or_default();
⋮----
.put(0, &index0)
⋮----
.get(1)
⋮----
.put(1, &index1)
⋮----
.count();
assert_eq!(num_statuses, 15);
⋮----
purge(&blockstore, oldest_slot);
⋮----
assert!(slot >= oldest_slot);
⋮----
assert_eq!(count, 12);
⋮----
assert_eq!(count, 10);
⋮----
assert_eq!(count, 7);
⋮----
assert_eq!(count, 5);
⋮----
assert_eq!(count, 2);
⋮----
assert!(status_entry_iterator.next().is_none());
⋮----
fn test_purge_special_columns_exact_no_sigs() {
⋮----
let mut entries: Vec<Entry> = vec![];
⋮----
tx.signatures = vec![];
entries.push(next_entry_mut(&mut Hash::default(), 0, vec![tx]));
let mut tick = create_ticks(1, 0, hash(&serialize(&x).unwrap()));
entries.append(&mut tick);
⋮----
let mut write_batch = blockstore.get_write_batch().unwrap();
⋮----
.purge_special_columns_exact(&mut write_batch, slot, slot + 1)
⋮----
fn test_purge_special_columns_compaction_filter() {
⋮----
assert_eq!(count, max_slot - (oldest_slot - 1));
⋮----
fn test_purge_transaction_memos_compaction_filter() {
⋮----
fn random_signature() -> Signature {
use rand::Rng;
⋮----
rand::rng().fill(&mut key[..]);
⋮----
.put_deprecated(random_signature(), &"this is a memo".to_string())
⋮----
.put_deprecated(random_signature(), &"another memo".to_string())
⋮----
blockstore.db.set_clean_slot_0(false);
⋮----
.put(
(random_signature(), oldest_slot - 1),
&"this is a new memo in slot 4".to_string(),
⋮----
(random_signature(), oldest_slot),
&"this is a memo in slot 5 ".to_string(),
⋮----
blockstore.db.set_oldest_slot(0);
blockstore.transaction_memos_cf.compact();
⋮----
assert_eq!(num_memos, 4);
⋮----
assert!(slot == 0 || slot >= oldest_slot);
⋮----
assert_eq!(count, 3);
blockstore.db.set_clean_slot_0(true);
⋮----
assert_eq!(count, 1);
⋮----
fn test_purge_slot_cleanup_chaining_missing_slot_meta() {
⋮----
let (shreds, _) = make_many_slot_entries(0, 10, 5);
⋮----
assert!(matches!(
⋮----
fn test_purge_slot_cleanup_chaining() {
⋮----
let (slot_11, _) = make_slot_entries(11, 4, 5);
blockstore.insert_shreds(slot_11, None, false).unwrap();
let (slot_12, _) = make_slot_entries(12, 5, 5);
blockstore.insert_shreds(slot_12, None, false).unwrap();
blockstore.purge_slot_cleanup_chaining(5).unwrap();
let slot_meta = blockstore.meta(5).unwrap().unwrap();
⋮----
next_slots: vec![6, 12],
⋮----
assert_eq!(slot_meta, expected_slot_meta);
let parent_slot_meta = blockstore.meta(4).unwrap().unwrap();
assert_eq!(parent_slot_meta.next_slots, vec![11]);
let child_slot_meta = blockstore.meta(6).unwrap().unwrap();
assert_eq!(child_slot_meta.parent_slot.unwrap(), 5);
let child_slot_meta = blockstore.meta(12).unwrap().unwrap();

================
File: ledger/src/blockstore/column.rs
================
pub mod columns {
⋮----
pub struct SlotMeta;
⋮----
pub struct Orphans;
⋮----
pub struct DeadSlots;
⋮----
pub struct DuplicateSlots;
⋮----
pub struct ErasureMeta;
⋮----
pub struct BankHash;
⋮----
pub struct Root;
⋮----
pub struct Index;
⋮----
pub struct ShredData;
⋮----
pub struct ShredCode;
⋮----
pub struct TransactionStatus;
⋮----
pub struct AddressSignatures;
⋮----
pub struct TransactionMemos;
⋮----
pub struct TransactionStatusIndex;
⋮----
pub struct Rewards;
⋮----
pub struct Blocktime;
⋮----
pub struct PerfSamples;
⋮----
pub struct BlockHeight;
⋮----
pub struct OptimisticSlots;
⋮----
pub struct MerkleRootMeta;
⋮----
macro_rules! convert_column_index_to_key_bytes {
⋮----
macro_rules! convert_column_key_bytes_to_index {
⋮----
pub trait Column {
⋮----
pub trait ColumnName {
⋮----
// Columns that serialize data on insertion and deserialize on fetch
pub trait TypedColumn: Column {
⋮----
fn deserialize(data: &[u8]) -> Result<Self::Type> {
Ok(bincode::deserialize(data)?)
⋮----
fn serialize(data: &Self::Type) -> Result<Vec<u8>> {
Ok(bincode::serialize(data)?)
⋮----
pub trait ProtobufColumn: Column {
⋮----
/// SlotColumn is a trait for slot-based column families.  Its index is
/// essentially Slot (or more generally speaking, has a 1:1 mapping to Slot).
⋮----
/// essentially Slot (or more generally speaking, has a 1:1 mapping to Slot).
///
⋮----
///
/// The clean-up of any LedgerColumn that implements SlotColumn is managed by
⋮----
/// The clean-up of any LedgerColumn that implements SlotColumn is managed by
/// `LedgerCleanupService`, which will periodically deprecate and purge
⋮----
/// `LedgerCleanupService`, which will periodically deprecate and purge
/// oldest entries that are older than the latest root in order to maintain the
⋮----
/// oldest entries that are older than the latest root in order to maintain the
/// configured --limit-ledger-size under the validator argument.
⋮----
/// configured --limit-ledger-size under the validator argument.
pub trait SlotColumn<Index = Slot> {}
⋮----
pub trait SlotColumn<Index = Slot> {}
pub enum IndexError {
⋮----
/// Helper trait to transition primary indexes out from the columns that are using them.
pub trait ColumnIndexDeprecation: Column {
⋮----
pub trait ColumnIndexDeprecation: Column {
⋮----
fn index(key: &[u8]) -> Self::Index {
⋮----
// Way back in the day, we broke the TransactionStatus column key. This fallback
// preserves the existing logic for ancient keys, but realistically should never be
// executed.
⋮----
impl TypedColumn for columns::AddressSignatures {
type Type = blockstore_meta::AddressSignatureMeta;
⋮----
impl TypedColumn for columns::TransactionMemos {
type Type = String;
⋮----
impl TypedColumn for columns::TransactionStatusIndex {
type Type = blockstore_meta::TransactionStatusIndexMeta;
⋮----
impl<T: SlotColumn> Column for T {
type Index = Slot;
type Key = [u8; std::mem::size_of::<Slot>()];
⋮----
fn key(slot: &Self::Index) -> Self::Key {
slot.to_be_bytes()
⋮----
/// Converts a RocksDB key to its u64 Index.
    fn index(key: &[u8]) -> Self::Index {
convert_column_key_bytes_to_index!(key, 0..8 => Slot::from_be_bytes)
⋮----
fn slot(index: Self::Index) -> Slot {
⋮----
/// Converts a Slot to its u64 Index.
    fn as_index(slot: Slot) -> u64 {
⋮----
fn as_index(slot: Slot) -> u64 {
⋮----
impl Column for columns::TransactionStatus {
type Index = (Signature, Slot);
type Key = [u8; SIGNATURE_BYTES + std::mem::size_of::<Slot>()];
⋮----
fn key((signature, slot): &Self::Index) -> Self::Key {
convert_column_index_to_key_bytes!(Key,
⋮----
fn index(key: &[u8]) -> (Signature, Slot) {
⋮----
// The TransactionStatus column is not keyed by slot so this method is meaningless
// See Column::as_index() declaration for more details
fn as_index(_index: u64) -> Self::Index {
⋮----
impl ColumnName for columns::TransactionStatus {
⋮----
impl ProtobufColumn for columns::TransactionStatus {
type Type = generated::TransactionStatusMeta;
⋮----
impl ColumnIndexDeprecation for columns::TransactionStatus {
⋮----
type DeprecatedIndex = (u64, Signature, Slot);
type DeprecatedKey = [u8; 80];
fn deprecated_key((index, signature, slot): Self::DeprecatedIndex) -> Self::DeprecatedKey {
convert_column_index_to_key_bytes!(DeprecatedKey,
⋮----
fn try_deprecated_index(key: &[u8]) -> std::result::Result<Self::DeprecatedIndex, IndexError> {
if key.len() != std::mem::size_of::<Self::DeprecatedKey>() {
return Err(IndexError::UnpackError);
⋮----
Ok(convert_column_key_bytes_to_index!(key,
⋮----
fn try_current_index(key: &[u8]) -> std::result::Result<Self::Index, IndexError> {
if key.len() != Self::CURRENT_INDEX_LEN {
⋮----
fn convert_index(deprecated_index: Self::DeprecatedIndex) -> Self::Index {
⋮----
impl Column for columns::AddressSignatures {
type Index = (Pubkey, Slot,  u32, Signature);
type Key = [u8; PUBKEY_BYTES
⋮----
fn key((pubkey, slot, transaction_index, signature): &Self::Index) -> Self::Key {
⋮----
impl ColumnName for columns::AddressSignatures {
⋮----
impl ColumnIndexDeprecation for columns::AddressSignatures {
⋮----
type DeprecatedIndex = (u64, Pubkey, Slot, Signature);
type DeprecatedKey = [u8; 112];
fn deprecated_key(
⋮----
0..8   => u64::from_be_bytes,  // primary index
⋮----
40..44  => u32::from_be_bytes,  // transaction index
⋮----
impl Column for columns::TransactionMemos {
⋮----
fn as_index(index: u64) -> Self::Index {
⋮----
impl ColumnName for columns::TransactionMemos {
⋮----
impl ColumnIndexDeprecation for columns::TransactionMemos {
⋮----
type DeprecatedIndex = Signature;
type DeprecatedKey = [u8; 64];
fn deprecated_key(signature: Self::DeprecatedIndex) -> Self::DeprecatedKey {
⋮----
Signature::try_from(&key[..64]).map_err(|_| IndexError::UnpackError)
⋮----
impl Column for columns::TransactionStatusIndex {
type Index = u64;
type Key = [u8; std::mem::size_of::<u64>()];
⋮----
fn key(index: &Self::Index) -> Self::Key {
index.to_be_bytes()
⋮----
convert_column_key_bytes_to_index!(key, 0..8 => u64::from_be_bytes)
⋮----
fn slot(_index: Self::Index) -> Slot {
unimplemented!()
⋮----
fn as_index(slot: u64) -> u64 {
⋮----
impl ColumnName for columns::TransactionStatusIndex {
⋮----
impl SlotColumn for columns::Rewards {}
impl ColumnName for columns::Rewards {
⋮----
impl ProtobufColumn for columns::Rewards {
type Type = generated::Rewards;
⋮----
impl SlotColumn for columns::Blocktime {}
impl ColumnName for columns::Blocktime {
⋮----
impl TypedColumn for columns::Blocktime {
type Type = UnixTimestamp;
⋮----
impl SlotColumn for columns::PerfSamples {}
impl ColumnName for columns::PerfSamples {
⋮----
impl SlotColumn for columns::BlockHeight {}
impl ColumnName for columns::BlockHeight {
⋮----
impl TypedColumn for columns::BlockHeight {
type Type = u64;
⋮----
impl Column for columns::ShredCode {
type Index = (Slot, /*shred index:*/ u64);
type Key = <columns::ShredData as Column>::Key;
⋮----
// ShredCode and ShredData have the same key format
⋮----
fn as_index(slot: Slot) -> Self::Index {
⋮----
impl ColumnName for columns::ShredCode {
⋮----
impl Column for columns::ShredData {
type Index = (Slot,  u64);
type Key = [u8; std::mem::size_of::<Slot>() + std::mem::size_of::<u64>()];
⋮----
fn key((slot, index): &Self::Index) -> Self::Key {
⋮----
convert_column_key_bytes_to_index!(key,
⋮----
impl ColumnName for columns::ShredData {
⋮----
impl SlotColumn for columns::Index {}
impl ColumnName for columns::Index {
⋮----
impl TypedColumn for columns::Index {
type Type = blockstore_meta::Index;
⋮----
.with_fixint_encoding()
.reject_trailing_bytes();
let index: bincode::Result<blockstore_meta::Index> = config.deserialize(data);
⋮----
Ok(index) => Ok(index),
⋮----
let index: blockstore_meta::IndexFallback = config.deserialize(data)?;
Ok(index.into())
⋮----
impl SlotColumn for columns::DeadSlots {}
impl ColumnName for columns::DeadSlots {
⋮----
impl TypedColumn for columns::DeadSlots {
type Type = bool;
⋮----
impl SlotColumn for columns::DuplicateSlots {}
impl ColumnName for columns::DuplicateSlots {
⋮----
impl TypedColumn for columns::DuplicateSlots {
type Type = blockstore_meta::DuplicateSlotProof;
⋮----
impl SlotColumn for columns::Orphans {}
impl ColumnName for columns::Orphans {
⋮----
impl TypedColumn for columns::Orphans {
⋮----
impl SlotColumn for columns::BankHash {}
impl ColumnName for columns::BankHash {
⋮----
impl TypedColumn for columns::BankHash {
type Type = blockstore_meta::FrozenHashVersioned;
⋮----
impl SlotColumn for columns::Root {}
impl ColumnName for columns::Root {
⋮----
impl TypedColumn for columns::Root {
⋮----
impl SlotColumn for columns::SlotMeta {}
impl ColumnName for columns::SlotMeta {
⋮----
impl TypedColumn for columns::SlotMeta {
type Type = blockstore_meta::SlotMeta;
⋮----
let index: bincode::Result<blockstore_meta::SlotMeta> = config.deserialize(data);
⋮----
let index: blockstore_meta::SlotMetaFallback = config.deserialize(data)?;
⋮----
impl Column for columns::ErasureMeta {
⋮----
fn key((slot, fec_set_index): &Self::Index) -> Self::Key {
⋮----
impl ColumnName for columns::ErasureMeta {
⋮----
impl TypedColumn for columns::ErasureMeta {
type Type = blockstore_meta::ErasureMeta;
⋮----
impl SlotColumn for columns::OptimisticSlots {}
impl ColumnName for columns::OptimisticSlots {
⋮----
impl TypedColumn for columns::OptimisticSlots {
type Type = blockstore_meta::OptimisticSlotMetaVersioned;
⋮----
impl Column for columns::MerkleRootMeta {
type Index = (Slot,  u32);
type Key = [u8; std::mem::size_of::<Slot>() + std::mem::size_of::<u32>()];
⋮----
fn slot((slot, _fec_set_index): Self::Index) -> Slot {
⋮----
impl ColumnName for columns::MerkleRootMeta {
⋮----
impl TypedColumn for columns::MerkleRootMeta {
type Type = blockstore_meta::MerkleRootMeta;

================
File: ledger/src/blockstore/error.rs
================
pub enum BlockstoreError {
⋮----
pub type Result<T> = std::result::Result<T, BlockstoreError>;

================
File: ledger/src/leader_schedule/identity_keyed.rs
================
pub struct LeaderSchedule {
⋮----
impl LeaderSchedule {
pub fn new(
⋮----
.iter()
.map(|(pubkey, stake)| (pubkey, *stake))
.collect();
let slot_leaders = stake_weighted_slot_leaders(keyed_stakes, epoch, len, repeat);
⋮----
pub fn new_from_schedule(slot_leaders: Vec<Pubkey>) -> Self {
⋮----
fn invert_slot_leaders(slot_leaders: &[Pubkey]) -> HashMap<Pubkey, Vec<usize>> {
⋮----
.enumerate()
.map(|(i, pk)| (*pk, i))
.into_group_map()
⋮----
pub fn get_slot_leaders(&self) -> &[Pubkey] {
⋮----
impl LeaderScheduleVariant for LeaderSchedule {
fn get_slot_leaders(&self) -> &[Pubkey] {
⋮----
fn get_leader_slots_map(&self) -> &HashMap<Pubkey, Vec<usize>> {
⋮----
type Output = Pubkey;
fn index(&self, index: u64) -> &Pubkey {
&self.get_slot_leaders()[index as usize % self.num_slots()]
⋮----
mod tests {
⋮----
fn test_leader_schedule_index() {
⋮----
let leader_schedule = LeaderSchedule::new_from_schedule(vec![pubkey0, pubkey1]);
assert_eq!(leader_schedule[0], pubkey0);
assert_eq!(leader_schedule[1], pubkey1);
assert_eq!(leader_schedule[2], pubkey0);
⋮----
fn test_leader_schedule_basic() {
⋮----
.map(|i| (solana_pubkey::new_rand(), i))
⋮----
assert_eq!(leader_schedule.num_slots() as u64, len);
assert_eq!(leader_schedule, leader_schedule2);
⋮----
fn test_repeated_leader_schedule() {
⋮----
for (i, node) in leader_schedule.get_slot_leaders().iter().enumerate() {
⋮----
assert_eq!(leader_node, *node);
⋮----
fn test_repeated_leader_schedule_specific() {
⋮----
let stakes: HashMap<_, _> = [(alice_pubkey, 2), (bob_pubkey, 1)].into_iter().collect();
⋮----
.get_slot_leaders()
.to_vec();
⋮----
assert_eq!(leaders1.len(), leaders2.len());
let leaders1_expected = vec![
⋮----
let leaders2_expected = vec![
⋮----
assert_eq!(leaders1, leaders1_expected);
assert_eq!(leaders2, leaders2_expected);
⋮----
fn test_invert_slot_leaders() {
⋮----
assert_eq!(

================
File: ledger/src/leader_schedule/vote_keyed.rs
================
pub struct LeaderSchedule {
⋮----
impl LeaderSchedule {
pub fn new(
⋮----
.iter()
.filter(|(_pubkey, (stake, _account))| *stake > 0)
.map(|(vote_pubkey, (stake, _account))| (vote_pubkey, *stake))
.collect();
let vote_keyed_slot_leaders = stake_weighted_slot_leaders(keyed_stakes, epoch, len, repeat);
⋮----
fn new_from_schedule(
⋮----
struct SlotLeaderInfo<'a> {
⋮----
.map(|vote_account_address| {
⋮----
.get(vote_account_address)
.expect("vote account must be in vote_accounts_map")
⋮----
.node_pubkey();
⋮----
impl LeaderScheduleVariant for LeaderSchedule {
fn get_slot_leaders(&self) -> &[Pubkey] {
self.identity_keyed_leader_schedule.get_slot_leaders()
⋮----
fn get_leader_slots_map(&self) -> &HashMap<Pubkey, Vec<usize>> {
self.identity_keyed_leader_schedule.get_leader_slots_map()
⋮----
fn get_vote_key_at_slot_index(&self, index: usize) -> Option<&Pubkey> {
⋮----
Some(&slot_vote_addresses[index % slot_vote_addresses.len()])
⋮----
type Output = Pubkey;
fn index(&self, index: u64) -> &Pubkey {
&self.get_slot_leaders()[index as usize % self.num_slots()]
⋮----
mod tests {
⋮----
fn test_index() {
⋮----
let vote_keyed_slot_leaders = vec![pubkey0, pubkey1];
⋮----
.into_iter()
⋮----
assert_eq!(
⋮----
fn test_get_vote_key_at_slot_index() {
⋮----
fn test_leader_schedule_basic() {
⋮----
.map(|i| (solana_pubkey::new_rand(), (i, VoteAccount::new_random())))
⋮----
assert_eq!(leader_schedule.num_slots() as u64, len);
assert_eq!(leader_schedule, leader_schedule2);
⋮----
fn test_repeated_leader_schedule() {
⋮----
for (i, node) in leader_schedule.get_slot_leaders().iter().enumerate() {
⋮----
assert_eq!(leader_node, *node);
⋮----
fn test_repeated_leader_schedule_specific() {
⋮----
let alice_pubkey = *vote_accounts_map.get(&vote_key0).unwrap().1.node_pubkey();
let bob_pubkey = *vote_accounts_map.get(&vote_key1).unwrap().1.node_pubkey();
⋮----
.get_slot_leaders()
.to_vec();
⋮----
assert_eq!(leaders1.len(), leaders2.len());
let leaders1_expected = vec![
⋮----
let leaders2_expected = vec![
⋮----
assert_eq!(leaders1, leaders1_expected);
assert_eq!(leaders2, leaders2_expected);

================
File: ledger/src/shred/common.rs
================
macro_rules! dispatch {
⋮----
macro_rules! impl_shred_common {

================
File: ledger/src/shred/merkle_tree.rs
================
const_assert_eq!(SIZE_OF_MERKLE_ROOT, 32);
const_assert_eq!(SIZE_OF_MERKLE_PROOF_ENTRY, 20);
⋮----
pub(crate) type MerkleProofEntry = [u8; 20];
pub(crate) struct MerkleTree {
⋮----
impl MerkleTree {
pub(crate) fn try_new(
⋮----
if shreds.len() == 0 {
return Err(Error::EmptyIterator);
⋮----
let num_shreds = shreds.len();
let capacity = get_merkle_tree_size(num_shreds);
⋮----
nodes.push(shred?);
⋮----
let init = (num_shreds > 1).then_some(num_shreds);
for size in successors(init, |&k| (k > 2).then_some((k + 1) >> 1)) {
let offset = nodes.len() - size;
for index in (offset..offset + size).step_by(2) {
⋮----
let other = &nodes[(index + 1).min(offset + size - 1)];
let parent = join_nodes(node, other);
nodes.push(parent);
⋮----
debug_assert_eq!(nodes.len(), capacity);
Ok(MerkleTree { nodes })
⋮----
pub(crate) fn root(&self) -> &Hash {
self.nodes.last().unwrap()
⋮----
pub(crate) fn make_merkle_proof(
⋮----
(size, offset) = (0, self.nodes.len());
⋮----
let Some(node) = self.nodes.get(offset + (index ^ 1).min(size - 1)) else {
return Some(Err(Error::InvalidMerkleProof));
⋮----
let entry = &node.as_ref()[..SIZE_OF_MERKLE_PROOF_ENTRY];
let entry = <&MerkleProofEntry>::try_from(entry).unwrap();
Some(Ok(entry))
} else if offset + 1 == self.nodes.len() {
⋮----
Some(Err(Error::InvalidMerkleProof))
⋮----
fn join_nodes<S: AsRef<[u8]>, T: AsRef<[u8]>>(node: S, other: T) -> Hash {
let node = &node.as_ref()[..SIZE_OF_MERKLE_PROOF_ENTRY];
let other = &other.as_ref()[..SIZE_OF_MERKLE_PROOF_ENTRY];
hashv(&[MERKLE_HASH_PREFIX_NODE, node, other])
⋮----
pub fn get_merkle_root<'a, I>(index: usize, node: Hash, proof: I) -> Result<Hash, Error>
⋮----
.into_iter()
.fold((index, node), |(index, node), other| {
⋮----
join_nodes(node, other)
⋮----
join_nodes(other, node)
⋮----
.then_some(root)
.ok_or(Error::InvalidMerkleProof)
⋮----
pub fn get_merkle_tree_size(num_shreds: usize) -> usize {
successors(Some(num_shreds), |&k| (k > 1).then_some((k + 1) >> 1)).sum()
⋮----
pub(crate) const fn get_proof_size(num_shreds: usize) -> u8 {
let bits = usize::BITS - num_shreds.leading_zeros();
let proof_size = if num_shreds.is_power_of_two() {
bits.saturating_sub(1)
⋮----
mod tests {
⋮----
fn test_merkle_proof_entry_from_hash() {
⋮----
let bytes: [u8; 32] = rng.random();
⋮----
let entry = &hash.as_ref()[..SIZE_OF_MERKLE_PROOF_ENTRY];
let entry = MerkleProofEntry::try_from(entry).unwrap();
assert_eq!(entry, &bytes[..SIZE_OF_MERKLE_PROOF_ENTRY]);
⋮----
fn test_get_merkle_tree_size() {
⋮----
for (num_shreds, size) in TREE_SIZE.into_iter().enumerate() {
assert_eq!(get_merkle_tree_size(num_shreds), size);
⋮----
fn test_make_merkle_proof_error() {
⋮----
let nodes = repeat_with(|| rng.random::<[u8; 32]>()).map(Hash::from);
let nodes: Vec<_> = nodes.take(5).collect();
let size = nodes.len();
let tree = MerkleTree::try_new(nodes.into_iter().map(Ok)).unwrap();
⋮----
assert_matches!(
⋮----
fn run_merkle_tree_round_trip<R: Rng>(rng: &mut R, size: usize) {
⋮----
let nodes: Vec<_> = nodes.take(size).collect();
let tree = MerkleTree::try_new(nodes.iter().cloned().map(Ok)).unwrap();
let root = *tree.root();
⋮----
for (k, &node) in nodes.iter().enumerate() {
let proof = tree.make_merkle_proof(index, size).map(Result::unwrap);
⋮----
assert_eq!(root, get_merkle_root(k, node, proof).unwrap());
⋮----
assert_ne!(root, get_merkle_root(k, node, proof).unwrap());
⋮----
fn test_merkle_tree_round_trip_small() {
⋮----
run_merkle_tree_round_trip(&mut rng, size);
⋮----
fn test_merkle_tree_round_trip_big() {
⋮----
fn test_get_proof_size() {
assert_eq!(get_proof_size(0), 0);
assert_eq!(get_proof_size(1), 0);
assert_eq!(get_proof_size(2), 1);
assert_eq!(get_proof_size(3), 2);
assert_eq!(get_proof_size(4), 2);
assert_eq!(get_proof_size(5), 3);
assert_eq!(get_proof_size(63), 6);
assert_eq!(get_proof_size(64), 6);
assert_eq!(get_proof_size(65), 7);
assert_eq!(get_proof_size(usize::MAX - 1), 64);
assert_eq!(get_proof_size(usize::MAX), 64);
⋮----
assert_eq!(get_proof_size(num_shreds), proof_size);

================
File: ledger/src/shred/merkle.rs
================
use crate::shred::ShredType;
⋮----
const_assert_eq!(ShredData::SIZE_OF_PAYLOAD, 1203);
⋮----
pub struct ShredData {
⋮----
pub struct ShredCode {
⋮----
pub(crate) enum Shred {
⋮----
impl Shred {
dispatch!(fn erasure_shard_index(&self) -> Result<usize, Error>);
dispatch!(fn erasure_shard_mut(&mut self) -> Result<PayloadMutGuard<'_, Range<usize>>, Error>);
dispatch!(fn merkle_node(&self) -> Result<Hash, Error>);
dispatch!(fn sanitize(&self) -> Result<(), Error>);
dispatch!(fn set_chained_merkle_root(&mut self, chained_merkle_root: &Hash) -> Result<(), Error>);
dispatch!(fn set_signature(&mut self, signature: Signature));
dispatch!(fn signed_data(&self) -> Result<Hash, Error>);
dispatch!(pub(super) fn common_header(&self) -> &ShredCommonHeader);
dispatch!(pub(super) fn payload(&self) -> &Payload);
dispatch!(pub(super) fn set_retransmitter_signature(&mut self, signature: &Signature) -> Result<(), Error>);
⋮----
fn fec_set_index(&self) -> u32 {
self.common_header().fec_set_index
⋮----
fn merkle_proof(&self) -> Result<impl Iterator<Item = &MerkleProofEntry>, Error> {
⋮----
Self::ShredCode(shred) => shred.merkle_proof().map(Either::Left),
Self::ShredData(shred) => shred.merkle_proof().map(Either::Right),
⋮----
fn set_merkle_proof<'a, I>(&mut self, proof: I) -> Result<(), Error>
⋮----
Self::ShredCode(shred) => shred.set_merkle_proof(proof),
Self::ShredData(shred) => shred.set_merkle_proof(proof),
⋮----
fn verify(&self, pubkey: &Pubkey) -> bool {
match self.signed_data() {
Ok(data) => self.signature().verify(pubkey.as_ref(), data.as_ref()),
⋮----
fn signature(&self) -> &Signature {
&self.common_header().signature
⋮----
pub(super) fn from_payload<T: AsRef<[u8]>>(shred: T) -> Result<Self, Error>
⋮----
match shred::layout::get_shred_variant(shred.as_ref())? {
ShredVariant::MerkleCode { .. } => Ok(Self::ShredCode(ShredCode::from_payload(shred)?)),
ShredVariant::MerkleData { .. } => Ok(Self::ShredData(ShredData::from_payload(shred)?)),
⋮----
dispatch!(fn erasure_shard(&self) -> Result<&[u8], Error>);
dispatch!(fn proof_size(&self) -> Result<u8, Error>);
dispatch!(pub(super) fn chained_merkle_root(&self) -> Result<Hash, Error>);
dispatch!(pub(super) fn merkle_root(&self) -> Result<Hash, Error>);
dispatch!(pub(super) fn retransmitter_signature(&self) -> Result<Signature, Error>);
dispatch!(pub(super) fn retransmitter_signature_offset(&self) -> Result<usize, Error>);
fn index(&self) -> u32 {
self.common_header().index
⋮----
fn shred_type(&self) -> ShredType {
ShredType::from(self.common_header().shred_variant)
⋮----
impl ShredData {
impl_merkle_shred!(MerkleData);
// Offset into the payload where the erasure coded slice begins.
⋮----
// Given shred payload, ShredVariant{..} and DataShredHeader.size, returns
// the slice storing ledger entries in the shred.
pub(super) fn get_data(
⋮----
size: u16, // DataShredHeader.size
⋮----
.contains(&size)
.then(|| shred.get(Self::SIZE_OF_HEADERS..size))
.flatten()
.ok_or_else(|| Error::InvalidDataSize {
⋮----
payload: shred.len(),
⋮----
pub(super) fn get_merkle_root(shred: &[u8], proof_size: u8, resigned: bool) -> Option<Hash> {
debug_assert_eq!(
⋮----
// Shred index in the erasure batch.
⋮----
.checked_sub(fec_set_index)
.map(usize::try_from)?
.ok()?
⋮----
let proof_offset = Self::get_proof_offset(proof_size, resigned).ok()?;
let proof = get_merkle_proof(shred, proof_offset, proof_size).ok()?;
let node = get_merkle_node(shred, SIZE_OF_SIGNATURE..proof_offset).ok()?;
get_merkle_root(index, node, proof).ok()
⋮----
pub(crate) const fn const_capacity(proof_size: u8, resigned: bool) -> Result<usize, u8> {
// Merkle proof is generated and signed after coding shreds are
// generated. Coding shred headers cannot be erasure coded either.
match Self::SIZE_OF_PAYLOAD.checked_sub(
⋮----
Some(v) => Ok(v),
None => Err(proof_size),
⋮----
impl ShredCode {
impl_merkle_shred!(MerkleCode);
⋮----
let num_data_shreds = <[u8; 2]>::try_from(shred.get(83..85)?)
.map(u16::from_le_bytes)
.map(usize::from)
.ok()?;
let position = <[u8; 2]>::try_from(shred.get(87..89)?)
⋮----
num_data_shreds.checked_add(position)?
⋮----
macro_rules! impl_merkle_shred {
⋮----
// proof_size is the number of merkle proof entries.
⋮----
// For ShredCode, size of buffer embedding erasure codes.
// For ShredData, maximum size of ledger data that can be embedded in a
// data-shred, which is also equal to:
//   ShredCode::capacity(proof_size, chained, resigned).unwrap()
//       - ShredData::SIZE_OF_HEADERS
//       + SIZE_OF_SIGNATURE
⋮----
// Where the merkle proof starts in the shred binary.
⋮----
// Verify that exactly proof_size many entries are written.
⋮----
// Returns the offsets into the payload which are erasure coded.
⋮----
// Returns the erasure coded slice as an immutable reference.
⋮----
// Returns the erasure coded slice as a mutable reference.
⋮----
use impl_merkle_shred;
⋮----
type SignedData = Hash;
impl_shred_common!();
⋮----
fn from_payload<T>(payload: T) -> Result<Self, Error>
⋮----
if payload.len() < Self::SIZE_OF_PAYLOAD {
return Err(Error::InvalidPayloadSize(payload.len()));
⋮----
payload.truncate(Self::SIZE_OF_PAYLOAD);
⋮----
deserialize_from_with_limit(&payload[..])?;
if !matches!(common_header.shred_variant, ShredVariant::MerkleData { .. }) {
return Err(Error::InvalidShredVariant);
⋮----
shred.sanitize()?;
Ok(shred)
⋮----
fn erasure_shard_index(&self) -> Result<usize, Error> {
shred_data::erasure_shard_index(self).ok_or_else(|| {
⋮----
fn erasure_shard(&self) -> Result<&[u8], Error> {
⋮----
fn sanitize(&self) -> Result<(), Error> {
⋮----
if !matches!(shred_variant, ShredVariant::MerkleData { .. }) {
⋮----
let _ = self.merkle_proof()?;
⋮----
fn signed_data(&'a self) -> Result<Self::SignedData, Error> {
self.merkle_root()
⋮----
if !matches!(common_header.shred_variant, ShredVariant::MerkleCode { .. }) {
⋮----
// see: https://github.com/solana-labs/solana/pull/10109
⋮----
shred_code::erasure_shard_index(self).ok_or_else(|| {
⋮----
if !matches!(shred_variant, ShredVariant::MerkleCode { .. }) {
⋮----
impl ShredDataTrait for ShredData {
⋮----
fn data_header(&self) -> &DataShredHeader {
⋮----
fn data(&self) -> Result<&[u8], Error> {
⋮----
impl ShredCodeTrait for ShredCode {
⋮----
fn coding_header(&self) -> &CodingShredHeader {
⋮----
fn get_merkle_proof(
⋮----
Ok(shred
.get(proof_offset..proof_offset + proof_size)
.ok_or(Error::InvalidPayloadSize(shred.len()))?
.chunks(SIZE_OF_MERKLE_PROOF_ENTRY)
.map(<&MerkleProofEntry>::try_from)
.map(Result::unwrap))
⋮----
fn get_merkle_node(shred: &[u8], offsets: Range<usize>) -> Result<Hash, Error> {
⋮----
.get(offsets)
.ok_or(Error::InvalidPayloadSize(shred.len()))?;
Ok(hashv(&[MERKLE_HASH_PREFIX_LEAF, node]))
⋮----
pub(super) fn recover(
⋮----
let is_sorted = |(a, b)| cmp_shred_erasure_shard_index(a, b).is_le();
if !shreds.iter().tuple_windows().all(is_sorted) {
shreds.sort_unstable_by(cmp_shred_erasure_shard_index);
⋮----
let Some(Shred::ShredCode(shred)) = shreds.last() else {
return Err(Error::from(TooFewParityShards));
⋮----
let index = shred.common_header.index.checked_sub(position);
⋮----
index: index.ok_or(Error::from(InvalidIndex))?,
⋮----
shred.merkle_root()?,
shred.chained_merkle_root().ok(),
shred.retransmitter_signature().ok(),
⋮----
debug_assert_matches!(common_header.shred_variant, ShredVariant::MerkleCode { .. });
⋮----
debug_assert!(!resigned || retransmitter_signature.is_some());
debug_assert!(shreds.iter().all(|shred| {
⋮----
let mut mask = vec![false; num_shards];
⋮----
make_stub_shred(
⋮----
if shred.signature() != &common_header.signature {
return Err(Error::InvalidMerkleRoot);
⋮----
let erasure_shard_index = shred.erasure_shard_index()?;
if !(batch.len()..num_shards).contains(&erasure_shard_index) {
return Err(Error::from(InvalidIndex));
⋮----
while batch.len() < erasure_shard_index {
batch.push(make_stub_shred(batch.len())?);
⋮----
batch.push(shred);
⋮----
while batch.len() < num_shards {
⋮----
.iter_mut()
.zip(&mask)
.map(|(shred, &mask)| Ok((shred.erasure_shard_mut()?, mask)))
⋮----
.get(num_data_shreds, num_coding_shreds)?
.reconstruct(&mut shards)?;
drop(shards);
⋮----
.enumerate()
.map(|(index, (shred, mask))| {
⋮----
return Err(Error::InvalidRecoveredShred);
⋮----
deserialize_from_with_limit(&shred.payload[..])?;
⋮----
} else if !matches!(shred, Shred::ShredCode(_)) {
⋮----
shred.merkle_node()
⋮----
if tree.root() != &merkle_root {
⋮----
debug_assert!({
⋮----
Ok(None)
⋮----
let proof = tree.make_merkle_proof(index, num_shards);
shred.set_merkle_proof(proof)?;
debug_assert_matches!(shred.sanitize(), Ok(()));
debug_assert_eq!(shred, {
⋮----
Ok(Some(shred))
⋮----
Ok(shreds
.into_iter()
.zip(mask)
⋮----
.map(set_merkle_proof)
.filter_map(Result::transpose))
⋮----
fn cmp_shred_erasure_shard_index(a: &Shred, b: &Shred) -> Ordering {
⋮----
a.common_header.index.cmp(&b.common_header.index)
⋮----
fn make_stub_shred(
⋮----
let mut shred = if let Some(position) = erasure_shard_index.checked_sub(num_data_shreds) {
let position = u16::try_from(position).map_err(|_| Error::from(InvalidIndex))?;
⋮----
let mut payload = vec![0u8; ShredCode::SIZE_OF_PAYLOAD];
⋮----
resigned: retransmitter_signature.is_some(),
⋮----
+ u32::try_from(erasure_shard_index).map_err(|_| InvalidIndex)?;
⋮----
let mut payload = vec![0u8; ShredData::SIZE_OF_PAYLOAD];
payload[..SIZE_OF_SIGNATURE].copy_from_slice(common_header.signature.as_ref());
⋮----
shred.set_chained_merkle_root(chained_merkle_root)?;
⋮----
shred.set_retransmitter_signature(signature)?;
⋮----
fn make_shreds_data<'a>(
⋮----
debug_assert_matches!(common_header.shred_variant, ShredVariant::MerkleData { .. });
chunks.into_iter().map(move |chunk| {
debug_assert_matches!(common_header.shred_variant,
⋮----
let size = ShredData::SIZE_OF_HEADERS + chunk.len();
⋮----
payload[ShredData::SIZE_OF_HEADERS..size].copy_from_slice(chunk);
⋮----
// Generates coding shred blanks for the current erasure batch.
// Updates ShredCommonHeader.index for coding shreds of the next batch.
// These have the correct headers, but none of the payloads and signatures.
fn make_shreds_code_header_only(
⋮----
payload: Payload::from(vec![0u8; ShredCode::SIZE_OF_PAYLOAD]),
⋮----
.take(CODING_SHREDS_PER_FEC_BLOCK)
⋮----
pub(crate) fn make_shreds_from_data(
⋮----
.checked_sub(parent_slot)
.and_then(|offset| u16::try_from(offset).ok())
.ok_or(Error::InvalidParentSlot { slot, parent_slot })?;
⋮----
if data.len() > data_buffer_total_size_signed {
let split_at = data.len() - data_buffer_total_size_signed;
data.split_at(split_at)
⋮----
stats.data_bytes += unsigned_data.len() + signed_data.len();
let unsigned_sets = unsigned_data.len().div_ceil(data_buffer_total_size);
⋮----
while unsigned_data.len() >= data_buffer_total_size {
let (current_batch_data_chunk, rest) = unsigned_data.split_at(data_buffer_total_size);
⋮----
shreds.extend(
make_shreds_data(
⋮----
current_batch_data_chunk.chunks(data_buffer_per_shred_size),
⋮----
.map(Shred::ShredData),
⋮----
shreds.extend(make_shreds_code_header_only(&mut common_header_code).map(Shred::ShredCode));
⋮----
if !unsigned_data.is_empty() || (shreds.is_empty() && !is_last_in_slot) {
stats.padding_bytes += data_buffer_total_size - unsigned_data.len();
shred_leftover_data(
⋮----
if !signed_data.is_empty() || (shreds.is_empty() && is_last_in_slot) {
stats.padding_bytes += data_buffer_total_size_signed - signed_data.len();
⋮----
.rev()
.find(|shred| matches!(shred, Shred::ShredData(_)))
⋮----
stats.record_num_data_shreds(num_data_shreds as usize);
⋮----
stats.gen_data_elapsed += now.elapsed().as_micros() as u64;
⋮----
let mut batches = shreds.chunk_by_mut(|a, b| a.fec_set_index() == b.fec_set_index());
batches.try_fold(chained_merkle_root, |chained_merkle_root, batch| {
finish_erasure_batch(
Some(thread_pool),
⋮----
stats.gen_coding_elapsed += now.elapsed().as_micros() as u64;
Ok(shreds)
⋮----
fn shred_leftover_data(
⋮----
shreds.extend({
⋮----
.chunks(data_buffer_per_shred_size)
.chain(std::iter::repeat(&[][..]))
.take(DATA_SHREDS_PER_FEC_BLOCK);
make_shreds_data(common_header_data, data_header, chunks).map(Shred::ShredData)
⋮----
shreds.extend(make_shreds_code_header_only(common_header_code).map(Shred::ShredCode));
⋮----
fn finish_erasure_batch(
⋮----
debug_assert_eq!(shreds.iter().map(Shred::fec_set_index).dedup().count(), 1);
fn write_headers(shred: &mut Shred) -> Result<(), bincode::Error> {
⋮----
&mut shred.payload.as_mut()[..],
⋮----
None => shreds.iter_mut().try_for_each(write_headers),
⋮----
thread_pool.install(|| shreds.par_iter_mut().try_for_each(write_headers))
⋮----
.encode(
⋮----
.map(Shred::erasure_shard_mut)
⋮----
for shred in shreds.iter_mut() {
shred.set_chained_merkle_root(&chained_merkle_root)?;
⋮----
let nodes = shreds.iter().map(Shred::merkle_node);
⋮----
Some(thread_pool) => MerkleTree::try_new(thread_pool.install(|| {
⋮----
.par_iter()
.map(Shred::merkle_node)
⋮----
let signature = keypair.sign_message(tree.root().as_ref());
for (index, shred) in shreds.iter_mut().enumerate() {
let proof = tree.make_merkle_proof(index, erasure_batch_size);
⋮----
shred.set_signature(signature);
debug_assert!(shred.verify(&keypair.pubkey()));
⋮----
Ok(*tree.root())
⋮----
mod test {
⋮----
fn shred_data_size_of_payload(proof_size: u8, resigned: bool) -> usize {
⋮----
+ ShredData::capacity(proof_size, resigned).unwrap()
⋮----
fn shred_data_capacity(proof_size: u8, resigned: bool) -> usize {
⋮----
ShredCode::capacity(proof_size, resigned).unwrap() - SIZE_OF_ERASURE_ENCODED_HEADER
⋮----
fn shred_data_size_of_erasure_encoded_slice(proof_size: u8, resigned: bool) -> usize {
⋮----
fn test_shred_data_size_of_payload(resigned: bool) {
⋮----
assert_eq!(
⋮----
fn test_shred_data_capacity(resigned: bool) {
⋮----
fn test_shred_code_capacity(resigned: bool) {
⋮----
fn test_merkle_proof_entry_from_hash() {
⋮----
let bytes: [u8; 32] = rng.random();
⋮----
let entry = &hash.as_ref()[..SIZE_OF_MERKLE_PROOF_ENTRY];
let entry = MerkleProofEntry::try_from(entry).unwrap();
assert_eq!(entry, &bytes[..SIZE_OF_MERKLE_PROOF_ENTRY]);
⋮----
fn test_make_merkle_proof_error() {
⋮----
let nodes = repeat_with(|| rng.random::<[u8; 32]>()).map(Hash::from);
let nodes: Vec<_> = nodes.take(5).collect();
let size = nodes.len();
let tree = MerkleTree::try_new(nodes.into_iter().map(Ok)).unwrap();
⋮----
assert_matches!(
⋮----
fn test_recover_merkle_shreds(num_shreds: usize, resigned: bool) {
⋮----
run_recover_merkle_shreds(
⋮----
fn run_recover_merkle_shreds<R: Rng + CryptoRng>(
⋮----
let proof_size = get_proof_size(num_shreds);
let capacity = ShredData::capacity(proof_size, resigned).unwrap();
⋮----
version: rng.random(),
⋮----
let reference_tick = rng.random_range(0..0x40);
⋮----
parent_offset: rng.random::<u16>().max(1),
⋮----
let size = ShredData::SIZE_OF_HEADERS + rng.random_range(0..capacity);
⋮----
bincode::serialize_into(&mut payload[..], &(&common_header, &data_header)).unwrap();
rng.fill(&mut payload[ShredData::SIZE_OF_HEADERS..size]);
⋮----
shreds.push(Shred::ShredData(shred));
⋮----
.iter()
.map(Shred::erasure_shard)
⋮----
.unwrap();
let mut parity = vec![vec![0u8; data[0].len()]; num_coding_shreds];
⋮----
.get(num_data_shreds, num_coding_shreds)
.unwrap()
.encode_sep(&data, &mut parity[..])
⋮----
for (i, code) in parity.into_iter().enumerate() {
⋮----
bincode::serialize_into(&mut payload[..], &(&common_header, &coding_header)).unwrap();
payload[ShredCode::SIZE_OF_HEADERS..ShredCode::SIZE_OF_HEADERS + code.len()]
.copy_from_slice(&code);
⋮----
shreds.push(Shred::ShredCode(shred));
⋮----
let tree = MerkleTree::try_new(nodes).unwrap();
⋮----
let proof = tree.make_merkle_proof(index, num_shreds);
shred.set_merkle_proof(proof).unwrap();
let data = shred.signed_data().unwrap();
let signature = keypair.sign_message(data.as_ref());
⋮----
assert!(shred.verify(&keypair.pubkey()));
assert_matches!(shred.sanitize(), Ok(()));
⋮----
verify_erasure_recovery(rng, &shreds, reed_solomon_cache);
⋮----
fn verify_erasure_recovery<R: Rng>(
⋮----
assert_eq!(shreds.iter().map(Shred::signature).dedup().count(), 1);
assert_eq!(shreds.iter().map(Shred::fec_set_index).dedup().count(), 1);
let num_shreds = shreds.len();
⋮----
.filter(|shred| shred.shred_type() == ShredType::Data)
.count();
⋮----
shreds.shuffle(rng);
let mut removed_shreds = shreds.split_off(size);
if shreds.iter().all(|shred| {
matches!(
⋮----
if shreds.len() < num_data_shreds {
⋮----
let recovered_shreds: Vec<_> = recover(shreds, reed_solomon_cache)
⋮----
.map(Result::unwrap)
.collect();
assert_eq!(size + recovered_shreds.len(), num_shreds);
assert_eq!(recovered_shreds.len(), removed_shreds.len());
removed_shreds.sort_by(|a, b| {
if a.shred_type() == b.shred_type() {
a.index().cmp(&b.index())
} else if a.shred_type() == ShredType::Data {
⋮----
assert_eq!(recovered_shreds, removed_shreds);
⋮----
fn test_make_shreds_from_data(data_size: usize, is_last_in_slot: bool) {
⋮----
let data_size = data_size.saturating_sub(16);
⋮----
run_make_shreds_from_data(&mut rng, data_size, is_last_in_slot, &reed_solomon_cache);
⋮----
fn test_make_shreds_from_data_rand(is_last_in_slot: bool) {
⋮----
let data_size = rng.random_range(0..31200 * 7);
⋮----
fn test_make_shreds_from_data_paranoid(is_last_in_slot: bool) {
⋮----
fn run_make_shreds_from_data<R: Rng>(
⋮----
let thread_pool = ThreadPoolBuilder::new().num_threads(2).build().unwrap();
⋮----
let chained_merkle_root = Hash::new_from_array(rng.random());
⋮----
let parent_slot = slot - rng.random_range(1..65536);
let shred_version = rng.random();
let reference_tick = rng.random_range(1..64);
let next_shred_index = rng.random_range(0..671);
let next_code_index = rng.random_range(0..781);
let mut data = vec![0u8; data_size];
rng.fill(&mut data[..]);
let shreds = make_shreds_from_data(
⋮----
.filter_map(|shred| match shred {
⋮----
Shred::ShredData(shred) => Some(shred),
⋮----
.flat_map(|shred| shred.data().unwrap())
.copied()
⋮----
assert_eq!(data, data2);
let pubkey = keypair.pubkey();
⋮----
assert!(shred.verify(&pubkey));
⋮----
} = *shred.common_header();
⋮----
let merkle_root = shred.merkle_root().unwrap();
let chained_merkle_root = shred.chained_merkle_root().unwrap();
assert!(signature.verify(pubkey.as_ref(), merkle_root.as_ref()));
let shred = shred.payload();
assert_eq!(shred::layout::get_signature(shred), Some(signature));
⋮----
assert_eq!(shred::layout::get_shred_type(shred).unwrap(), shred_type);
assert_eq!(shred::layout::get_slot(shred), Some(slot));
assert_eq!(shred::layout::get_index(shred), Some(index));
assert_eq!(shred::layout::get_version(shred), Some(version));
assert_eq!(shred::layout::get_shred_id(shred), Some(key));
assert_eq!(shred::layout::get_merkle_root(shred), Some(merkle_root));
⋮----
let data = shred::layout::get_signed_data(shred).unwrap();
assert_eq!(data, merkle_root);
assert!(signature.verify(pubkey.as_ref(), data.as_ref()));
⋮----
for (index, shred) in shreds.iter().enumerate() {
let common_header = shred.common_header();
let resigned = is_last_in_slot && index >= shreds.len() - 64;
assert_eq!(common_header.slot, slot);
assert_eq!(common_header.version, shred_version);
let proof_size = shred.proof_size().unwrap();
⋮----
assert_eq!(common_header.index, next_code_index + num_coding_shreds);
⋮----
assert_matches!(shred::layout::get_data(shred), Err(Error::InvalidShredType));
⋮----
assert_eq!(common_header.index, next_shred_index + num_data_shreds);
⋮----
assert!(common_header.fec_set_index <= common_header.index);
⋮----
let data = shred.data().unwrap();
⋮----
assert_eq!(shred::layout::get_flags(shred).unwrap(), data_header.flags);
assert_eq!(shred::layout::get_data(shred).unwrap(), data);
⋮----
assert!(num_coding_shreds >= num_data_shreds);
⋮----
.chain(
⋮----
.sorted_unstable_by_key(|shred| shred.fec_set_index())
.dedup_by(|shred, other| shred.fec_set_index() == other.fec_set_index())
.map(|shred| (shred.fec_set_index(), shred.merkle_root().unwrap())),
⋮----
.tuple_windows()
.map(|((_, merkle_root), (fec_set_index, _))| (fec_set_index, merkle_root))
⋮----
let fec_set_index = shreds.iter().map(Shred::fec_set_index).max().unwrap();
assert!(
⋮----
Shred::ShredCode(_) => Some(shred.clone()),
⋮----
.chunk_by(|shred| shred.common_header().fec_set_index)
⋮----
.flat_map(|(_, shreds)| {
recover(shreds.collect(), reed_solomon_cache)
⋮----
assert_eq!(recovered_data_shreds.len(), data_shreds.len());
for (shred, other) in recovered_data_shreds.into_iter().zip(data_shreds) {
⋮----
Shred::ShredCode(_) => panic!("Invalid shred type!"),
Shred::ShredData(shred) => assert_eq!(shred, *other),
⋮----
.into_group_map_by(Shred::fec_set_index)
.values()
⋮----
verify_erasure_recovery(rng, shreds, reed_solomon_cache);

================
File: ledger/src/shred/payload.rs
================
pub struct Payload {
⋮----
impl Payload {
⋮----
pub fn into_bytes_mut(self) -> BytesMut {
self.bytes.into()
⋮----
pub fn as_mut(&mut self) -> PayloadMutGuard<'_, RangeFull> {
⋮----
/// Get a mutable reference via [`PayloadMutGuard`] to a subset of the payload's inner bytes.
    pub fn get_mut<I>(&mut self, index: I) -> Option<PayloadMutGuard<'_, I>>
⋮----
pub fn get_mut<I>(&mut self, index: I) -> Option<PayloadMutGuard<'_, I>>
⋮----
match index.end_bound() {
Bound::Included(&end) if end >= self.bytes.len() => None,
Bound::Excluded(&end) if end > self.bytes.len() => None,
_ => Some(PayloadMutGuard::new(self, index)),
⋮----
/// Shortens the buffer, keeping the first `len` bytes and dropping the rest.
    ///
⋮----
///
    /// See [`Bytes::truncate`].
⋮----
/// See [`Bytes::truncate`].
    #[inline]
pub fn truncate(&mut self, len: usize) {
self.bytes.truncate(len);
⋮----
pub fn copy_to_packet(&self, packet: &mut Packet) {
let size = self.len();
packet.buffer_mut()[..size].copy_from_slice(&self[..]);
packet.meta_mut().size = size;
⋮----
pub fn to_packet(&self, nonce: Option<Nonce>) -> Packet {
⋮----
packet.buffer_mut()[..size].copy_from_slice(self);
⋮----
packet.buffer_mut()[size..full_size].copy_from_slice(&nonce.to_le_bytes());
⋮----
pub fn to_bytes_packet(&self, nonce: Option<Nonce>) -> BytesPacket {
let cap = self.len() + nonce.map(|_| mem::size_of::<Nonce>()).unwrap_or(0);
⋮----
buffer.put_slice(&self[..]);
⋮----
buffer.put_u32_le(nonce);
⋮----
BytesPacket::new(buffer.freeze(), Meta::default())
⋮----
pub(crate) mod serde_bytes_payload {
⋮----
pub(crate) fn serialize<S: Serializer>(
⋮----
serializer.serialize_bytes(payload)
⋮----
pub(crate) fn deserialize<'de, D>(deserializer: D) -> Result<Payload, D::Error>
⋮----
.map(ByteBuf::into_vec)
.map(Payload::from)
⋮----
impl PartialEq for Payload {
⋮----
fn eq(&self, other: &Self) -> bool {
self.as_ref() == other.as_ref()
⋮----
fn from(bytes: Vec<u8>) -> Self {
⋮----
fn from(bytes: Bytes) -> Self {
⋮----
fn from(bytes: BytesMut) -> Self {
⋮----
bytes: bytes.freeze(),
⋮----
fn as_ref(&self) -> &[u8] {
self.bytes.as_ref()
⋮----
impl Deref for Payload {
type Target = [u8];
⋮----
fn deref(&self) -> &Self::Target {
self.bytes.deref()
⋮----
/// Convenience wrapper around [`Payload`] and a [`BytesMut`] into that payload's bytes.
pub struct PayloadMutGuard<'a, I> {
⋮----
pub struct PayloadMutGuard<'a, I> {
⋮----
pub fn new(payload: &'a mut Payload, slice_index: I) -> Self {
let bytes_mut: BytesMut = mem::take(&mut payload.bytes).into();
⋮----
impl<I> Drop for PayloadMutGuard<'_, I> {
⋮----
fn drop(&mut self) {
self.payload.bytes = mem::take(&mut self.bytes_mut).freeze();
⋮----
impl<I> Deref for PayloadMutGuard<'_, I>
⋮----
type Target = <I as SliceIndex<[u8]>>::Output;
⋮----
&self.bytes_mut[self.slice_index.clone()]
⋮----
impl<I> DerefMut for PayloadMutGuard<'_, I>
⋮----
fn deref_mut(&mut self) -> &mut Self::Target {
&mut self.bytes_mut[self.slice_index.clone()]
⋮----
fn as_mut(&mut self) -> &mut [u8] {
⋮----
mod test {
⋮----
fn test_guard_write_back() {
let mut payload = Payload::from(vec![1, 2, 3, 4, 5]);
⋮----
let mut guard = payload.get_mut(..).unwrap();
assert_eq!(guard[0], 1);
assert_eq!(guard[1], 2);
⋮----
assert_eq!(guard[0], 10);
assert_eq!(guard[1], 20);
⋮----
assert_eq!(payload.bytes[..], vec![10, 20, 3, 4, 5]);
⋮----
fn test_to_bytes_packet_nonce_endianness() {
⋮----
let shredder = Shredder::new(1, 0, 0, 0).unwrap();
let entries = vec![Entry::new(&Hash::default(), 0, vec![])];
⋮----
.make_merkle_shreds_from_entries(
⋮----
.collect();
⋮----
let mut bytes_packet = shred.payload().to_bytes_packet(Some(nonce));
bytes_packet.meta_mut().flags |= PacketFlags::REPAIR;
let (bytes, got) = wire::get_shred_and_repair_nonce(bytes_packet.as_ref())
.expect("valid packet and nonce");
assert_eq!(bytes, shred.payload().as_ref());
assert_eq!(got, Some(nonce));

================
File: ledger/src/shred/shred_code.rs
================
const_assert_eq!(ShredCode::SIZE_OF_PAYLOAD, 1228);
⋮----
pub enum ShredCode {
⋮----
impl ShredCode {
⋮----
dispatch!(fn coding_header(&self) -> &CodingShredHeader);
dispatch!(pub(super) fn common_header(&self) -> &ShredCommonHeader);
dispatch!(pub(super) fn first_coding_index(&self) -> Option<u32>);
dispatch!(pub(super) fn into_payload(self) -> Payload);
dispatch!(pub(super) fn payload(&self) -> &Payload);
dispatch!(pub(super) fn sanitize(&self) -> Result<(), Error>);
⋮----
dispatch!(pub(super) fn set_signature(&mut self, signature: Signature));
pub(super) fn signed_data(&self) -> Result<Hash, Error> {
⋮----
shred.signed_data()
⋮----
pub(super) fn chained_merkle_root(&self) -> Result<Hash, Error> {
⋮----
Self::Merkle(shred) => shred.chained_merkle_root(),
⋮----
pub(super) fn merkle_root(&self) -> Result<Hash, Error> {
⋮----
Self::Merkle(shred) => shred.merkle_root(),
⋮----
pub(super) fn num_data_shreds(&self) -> u16 {
self.coding_header().num_data_shreds
⋮----
pub(super) fn num_coding_shreds(&self) -> u16 {
self.coding_header().num_coding_shreds
⋮----
pub(super) fn erasure_mismatch(&self, other: &ShredCode) -> bool {
⋮----
erasure_mismatch(shred, other)
|| shred.common_header().signature != other.common_header().signature
⋮----
pub(super) fn retransmitter_signature(&self) -> Result<Signature, Error> {
⋮----
Self::Merkle(shred) => shred.retransmitter_signature(),
⋮----
fn from(shred: merkle::ShredCode) -> Self {
⋮----
pub(super) fn erasure_shard_index<T: ShredCodeTrait>(shred: &T) -> Option<usize> {
let common_header = shred.common_header();
let coding_header = shred.coding_header();
⋮----
.checked_add(u32::from(coding_header.num_data_shreds.checked_sub(1)?))? as usize
⋮----
.first_coding_index()?
.checked_add(u32::from(coding_header.num_coding_shreds.checked_sub(1)?))? as usize
⋮----
let fec_set_size = num_data_shreds.checked_add(num_coding_shreds)?;
let index = position.checked_add(num_data_shreds)?;
(index < fec_set_size).then_some(index)
⋮----
pub(super) fn sanitize<T: ShredCodeTrait>(shred: &T) -> Result<(), Error> {
if shred.payload().len() != T::SIZE_OF_PAYLOAD {
return Err(Error::InvalidPayloadSize(shred.payload().len()));
⋮----
return Err(Error::InvalidShredIndex(
⋮----
return Err(Error::InvalidNumCodingShreds(
⋮----
let _shard_index = shred.erasure_shard_index()?;
let _erasure_shard = shred.erasure_shard()?;
Ok(())
⋮----
pub(super) fn erasure_mismatch<T: ShredCodeTrait>(shred: &T, other: &T) -> bool {
⋮----
} = shred.coding_header();
*num_coding_shreds != other.coding_header().num_coding_shreds
|| *num_data_shreds != other.coding_header().num_data_shreds
|| shred.first_coding_index() != other.first_coding_index()

================
File: ledger/src/shred/shred_data.rs
================
pub enum ShredData {
⋮----
impl ShredData {
dispatch!(fn data_header(&self) -> &DataShredHeader);
dispatch!(pub(super) fn common_header(&self) -> &ShredCommonHeader);
dispatch!(pub(super) fn into_payload(self) -> Payload);
dispatch!(pub(super) fn parent(&self) -> Result<Slot, Error>);
dispatch!(pub(super) fn payload(&self) -> &Payload);
dispatch!(pub(super) fn sanitize(&self) -> Result<(), Error>);
⋮----
dispatch!(pub(super) fn set_signature(&mut self, signature: Signature));
pub(super) fn signed_data(&self) -> Result<Hash, Error> {
⋮----
shred.signed_data()
⋮----
pub(super) fn chained_merkle_root(&self) -> Result<Hash, Error> {
⋮----
Self::Merkle(shred) => shred.chained_merkle_root(),
⋮----
pub(super) fn merkle_root(&self) -> Result<Hash, Error> {
⋮----
Self::Merkle(shred) => shred.merkle_root(),
⋮----
pub(super) fn last_in_slot(&self) -> bool {
let flags = self.data_header().flags;
flags.contains(ShredFlags::LAST_SHRED_IN_SLOT)
⋮----
pub(super) fn data_complete(&self) -> bool {
⋮----
flags.contains(ShredFlags::DATA_COMPLETE_SHRED)
⋮----
pub(super) fn reference_tick(&self) -> u8 {
⋮----
(flags & ShredFlags::SHRED_TICK_REFERENCE_MASK).bits()
⋮----
pub(super) fn bytes_to_store(&self) -> &[u8] {
⋮----
Self::Merkle(shred) => shred.payload(),
⋮----
pub(crate) fn resize_stored_shred(shred: Vec<u8>) -> Result<Vec<u8>, Error> {
⋮----
ShredVariant::MerkleCode { .. } => Err(Error::InvalidShredType),
⋮----
if shred.len() != merkle::ShredData::SIZE_OF_PAYLOAD {
return Err(Error::InvalidPayloadSize(shred.len()));
⋮----
Ok(shred)
⋮----
pub fn capacity(proof_size: u8, resigned: bool) -> Result<usize, Error> {
⋮----
pub(super) fn retransmitter_signature(&self) -> Result<Signature, Error> {
⋮----
Self::Merkle(shred) => shred.retransmitter_signature(),
⋮----
fn from(shred: merkle::ShredData) -> Self {
⋮----
pub(super) fn erasure_shard_index<T: ShredDataTrait>(shred: &T) -> Option<usize> {
let fec_set_index = shred.common_header().fec_set_index;
let index = shred.common_header().index.checked_sub(fec_set_index)?;
usize::try_from(index).ok()
⋮----
pub(super) fn sanitize<T: ShredDataTrait>(shred: &T) -> Result<(), Error> {
if shred.payload().len() != T::SIZE_OF_PAYLOAD {
return Err(Error::InvalidPayloadSize(shred.payload().len()));
⋮----
let common_header = shred.common_header();
let data_header = shred.data_header();
⋮----
return Err(Error::InvalidShredIndex(
⋮----
if flags.intersects(ShredFlags::LAST_SHRED_IN_SLOT)
&& !flags.contains(ShredFlags::DATA_COMPLETE_SHRED)
⋮----
return Err(Error::InvalidShredFlags(data_header.flags.bits()));
⋮----
let _data = shred.data()?;
let _parent = shred.parent()?;
let _shard_index = shred.erasure_shard_index()?;
let _erasure_shard = shred.erasure_shard()?;
Ok(())

================
File: ledger/src/shred/stats.rs
================
pub struct ProcessShredsStats {
⋮----
pub struct ShredFetchStats {
⋮----
impl ProcessShredsStats {
pub fn submit(
⋮----
.map(|t| t.as_micros() as i64)
.unwrap_or(-1);
self.num_data_shreds_hist.iter_mut().fold(0, |acc, num| {
⋮----
datapoint_info!(
⋮----
pub(crate) fn record_num_data_shreds(&mut self, num_data_shreds: usize) {
let index = usize::BITS - num_data_shreds.leading_zeros();
let index = index.saturating_sub(3) as usize;
let index = index.min(self.num_data_shreds_hist.len() - 1);
⋮----
pub fn record_shred(&mut self, shred: &Shred) {
let num_shreds = match shred.shred_type() {
⋮----
impl ShredFetchStats {
pub fn maybe_submit(&mut self, name: &'static str, cadence: Duration) -> bool {
let elapsed = self.since.as_ref().map(Instant::elapsed);
if elapsed.unwrap_or(Duration::MAX) < cadence {
⋮----
since: Some(Instant::now()),
⋮----
fn add_assign(&mut self, rhs: Self) {
⋮----
for (i, bucket) in self.num_data_shreds_hist.iter_mut().enumerate() {

================
File: ledger/src/shred/traits.rs
================
pub(super) trait Shred<'a>: Sized {
// Total size of payload including headers, merkle
// branches (if any), zero paddings, etc.
⋮----
// Size of common and code/data headers.
⋮----
// Returns the shard index within the erasure coding set.
⋮----
// Returns the portion of the shred's payload which is erasure coded.
⋮----
pub(super) trait ShredData: for<'a> Shred<'a> {
⋮----
fn parent(&self) -> Result<Slot, Error> {
let slot = self.common_header().slot;
let parent_offset = self.data_header().parent_offset;
⋮----
return Err(Error::InvalidParentOffset {
⋮----
slot.checked_sub(Slot::from(parent_offset))
.ok_or(Error::InvalidParentOffset {
⋮----
pub(super) trait ShredCode: for<'a> Shred<'a> {
⋮----
fn first_coding_index(&self) -> Option<u32> {
let position = u32::from(self.coding_header().position);
self.common_header().index.checked_sub(position)

================
File: ledger/src/shred/wire.rs
================
fn get_shred_size(shred: &[u8]) -> Option<usize> {
match get_shred_variant(shred).ok()? {
ShredVariant::MerkleCode { .. } => Some(shred::merkle::ShredCode::SIZE_OF_PAYLOAD),
ShredVariant::MerkleData { .. } => Some(shred::merkle::ShredData::SIZE_OF_PAYLOAD),
⋮----
pub fn get_shred<'a, P>(packet: P) -> Option<&'a [u8]>
⋮----
let data = packet.into().data(..)?;
data.get(..get_shred_size(data)?)
⋮----
pub fn get_shred_mut(buffer: &mut [u8]) -> Option<&mut [u8]> {
buffer.get_mut(..get_shred_size(buffer)?)
⋮----
pub fn get_shred_and_repair_nonce(packet: PacketRef<'_>) -> Option<(&[u8], Option<Nonce>)> {
let data = packet.data(..)?;
let shred = data.get(..get_shred_size(data)?)?;
if !packet.meta().repair() {
return Some((shred, None));
⋮----
let offset = data.len().checked_sub(4)?;
let nonce = <[u8; 4]>::try_from(data.get(offset..)?).ok()?;
⋮----
Some((shred, Some(nonce)))
⋮----
pub fn get_common_header_bytes(shred: &[u8]) -> Option<&[u8]> {
shred.get(..SIZE_OF_COMMON_SHRED_HEADER)
⋮----
pub(crate) fn get_signature(shred: &[u8]) -> Option<Signature> {
let bytes = <[u8; 64]>::try_from(shred.get(..64)?).unwrap();
Some(Signature::from(bytes))
⋮----
pub(crate) const fn get_signature_range() -> Range<usize> {
⋮----
pub(super) fn get_shred_variant(shred: &[u8]) -> Result<ShredVariant, Error> {
let Some(&shred_variant) = shred.get(64) else {
return Err(Error::InvalidPayloadSize(shred.len()));
⋮----
ShredVariant::try_from(shred_variant).map_err(|_| Error::InvalidShredVariant)
⋮----
pub(super) fn get_shred_type(shred: &[u8]) -> Result<ShredType, Error> {
get_shred_variant(shred).map(ShredType::from)
⋮----
pub fn get_slot(shred: &[u8]) -> Option<Slot> {
let bytes = <[u8; 8]>::try_from(shred.get(65..65 + 8)?).unwrap();
Some(Slot::from_le_bytes(bytes))
⋮----
pub fn get_index(shred: &[u8]) -> Option<u32> {
let bytes = <[u8; 4]>::try_from(shred.get(73..73 + 4)?).unwrap();
Some(u32::from_le_bytes(bytes))
⋮----
pub(super) fn get_version(shred: &[u8]) -> Option<u16> {
let bytes = <[u8; 2]>::try_from(shred.get(77..77 + 2)?).unwrap();
Some(u16::from_le_bytes(bytes))
⋮----
pub fn get_fec_set_index(shred: &[u8]) -> Option<u32> {
let bytes = <[u8; 4]>::try_from(shred.get(79..79 + 4)?).unwrap();
⋮----
pub(super) fn get_parent_offset(shred: &[u8]) -> Option<u16> {
debug_assert_eq!(get_shred_type(shred).unwrap(), ShredType::Data);
let bytes = <[u8; 2]>::try_from(shred.get(83..83 + 2)?).unwrap();
⋮----
pub(crate) fn corrupt_and_set_parent_offset(shred: &mut [u8], parent_offset: u16) {
let bytes = parent_offset.to_le_bytes();
assert_eq!(get_shred_type(shred).unwrap(), ShredType::Data);
shred.get_mut(83..83 + 2).unwrap().copy_from_slice(&bytes);
⋮----
pub fn get_flags(shred: &[u8]) -> Result<ShredFlags, Error> {
match get_shred_type(shred)? {
ShredType::Code => Err(Error::InvalidShredType),
⋮----
let Some(flags) = shred.get(85).copied() else {
⋮----
ShredFlags::from_bits(flags).ok_or(Error::InvalidShredFlags(flags))
⋮----
fn get_data_size(shred: &[u8]) -> Result<u16, Error> {
⋮----
let Some(bytes) = shred.get(86..86 + 2) else {
⋮----
let bytes = <[u8; 2]>::try_from(bytes).unwrap();
Ok(u16::from_le_bytes(bytes))
⋮----
pub(crate) fn get_data(shred: &[u8]) -> Result<&[u8], Error> {
match get_shred_variant(shred)? {
ShredVariant::MerkleCode { .. } => Err(Error::InvalidShredType),
⋮----
} => shred::merkle::ShredData::get_data(shred, proof_size, resigned, get_data_size(shred)?),
⋮----
pub(crate) fn get_erasure_config(shred: &[u8]) -> Result<ErasureConfig, Error> {
if !matches!(get_shred_type(shred).unwrap(), ShredType::Code) {
return Err(Error::InvalidShredType);
⋮----
let Some(num_data_bytes) = shred.get(83..83 + 2) else {
⋮----
let Some(num_coding_bytes) = shred.get(85..85 + 2) else {
⋮----
.map(u16::from_le_bytes)
.map(usize::from)
.map_err(|_| Error::InvalidErasureConfig)?;
⋮----
Ok(ErasureConfig {
⋮----
pub fn get_shred_id(shred: &[u8]) -> Option<ShredId> {
Some(ShredId(
get_slot(shred)?,
get_index(shred)?,
get_shred_type(shred).ok()?,
⋮----
pub(crate) fn get_signed_data(shred: &[u8]) -> Option<Hash> {
let data = match get_shred_variant(shred).ok()? {
⋮----
Some(data)
⋮----
pub fn get_reference_tick(shred: &[u8]) -> Result<u8, Error> {
if get_shred_type(shred)? != ShredType::Data {
⋮----
let Some(flags) = shred.get(85) else {
⋮----
Ok(flags & ShredFlags::SHRED_TICK_REFERENCE_MASK.bits())
⋮----
pub fn get_merkle_root(shred: &[u8]) -> Option<Hash> {
⋮----
pub(crate) fn get_chained_merkle_root(shred: &[u8]) -> Option<Hash> {
let offset = match get_shred_variant(shred).ok()? {
⋮----
.ok()?;
let merkle_root = shred.get(offset..offset + SIZE_OF_MERKLE_ROOT)?;
Some(Hash::from(
<[u8; SIZE_OF_MERKLE_ROOT]>::try_from(merkle_root).unwrap(),
⋮----
fn get_retransmitter_signature_offset(shred: &[u8]) -> Result<usize, Error> {
⋮----
pub fn get_retransmitter_signature(shred: &[u8]) -> Result<Signature, Error> {
let offset = get_retransmitter_signature_offset(shred)?;
let Some(bytes) = shred.get(offset..offset + 64) else {
⋮----
Ok(Signature::from(<[u8; 64]>::try_from(bytes).unwrap()))
⋮----
pub fn is_retransmitter_signed_variant(shred: &[u8]) -> Result<bool, Error> {
⋮----
} => Ok(resigned),
⋮----
pub fn set_retransmitter_signature(shred: &mut [u8], signature: &Signature) -> Result<(), Error> {
⋮----
let Some(buffer) = shred.get_mut(offset..offset + SIGNATURE_BYTES) else {
⋮----
buffer.copy_from_slice(signature.as_ref());
Ok(())
⋮----
pub fn resign_packet(packet: &mut PacketRefMut, keypair: &Keypair) -> Result<(), Error> {
⋮----
let shred = get_shred_mut(packet.buffer_mut()).ok_or(Error::InvalidPacketSize)?;
resign_shred(shred, keypair)
⋮----
let mut buffer = packet.buffer().to_vec();
let shred = get_shred_mut(&mut buffer).ok_or(Error::InvalidPacketSize)?;
resign_shred(shred, keypair)?;
packet.set_buffer(buffer);
⋮----
pub fn resign_shred(shred: &mut [u8], keypair: &Keypair) -> Result<(), Error> {
let (offset, merkle_root) = match get_shred_variant(shred)? {
⋮----
.ok_or(Error::InvalidMerkleRoot)?,
⋮----
let signature = keypair.sign_message(merkle_root.as_ref());
⋮----
pub(crate) fn corrupt_packet<R: Rng>(
⋮----
fn modify_packet<R: Rng>(rng: &mut R, packet: &mut Packet, offsets: Range<usize>) {
let buffer = packet.buffer_mut();
let byte = buffer[offsets].choose_mut(rng).unwrap();
*byte = rng.random::<u8>().max(1u8).wrapping_add(*byte);
⋮----
let shred = get_shred(&*packet).unwrap();
let merkle_variant = match get_shred_variant(shred).unwrap() {
⋮----
} => Some((proof_size, resigned)),
⋮----
let coin_flip: bool = rng.random();
⋮----
modify_packet(rng, packet, 0..SIGNATURE_BYTES);
⋮----
.map(|(proof_size, resigned)| {
⋮----
let size = shred.len() - if resigned { SIGNATURE_BYTES } else { 0 };
⋮----
.expect("Only merkle shreds are possible");
modify_packet(rng, packet, offsets);
⋮----
let shred = get_shred(packet).unwrap();
let slot = get_slot(shred).unwrap();
let signature = get_signature(shred).unwrap();
⋮----
let pubkey = keypairs[&slot].pubkey();
let data = get_signed_data(shred).unwrap();
assert!(!signature.verify(pubkey.as_ref(), data.as_ref()));
⋮----
let pubkey = keypairs.get(&slot).map(Keypair::pubkey).unwrap_or_default();
if let Some(data) = get_signed_data(shred) {
⋮----
mod tests {
⋮----
fn make_dummy_signature<R: Rng>(rng: &mut R) -> Signature {
⋮----
rng.fill(&mut signature[..]);
⋮----
fn test_resign_packet(repaired: bool, is_last_in_slot: bool) {
⋮----
let slot = 318_230_963 + rng.random_range(0..318_230_963);
let data_size = 1200 * rng.random_range(32..64);
⋮----
make_merkle_shreds_for_tests(&mut rng, slot, data_size, is_last_in_slot).unwrap();
let shreds_len = shreds.len();
for (index, shred) in shreds.iter_mut().enumerate() {
⋮----
let signature = make_dummy_signature(&mut rng);
let nonce = repaired.then(|| rng.random::<Nonce>());
⋮----
shred.set_retransmitter_signature(&signature).unwrap();
let packet = &mut shred.payload().to_packet(nonce);
⋮----
packet.meta_mut().flags |= PacketFlags::REPAIR;
⋮----
resign_packet(&mut packet.into(), &keypair).unwrap();
let packet = &mut shred.payload().to_bytes_packet(nonce);
⋮----
resign_packet(&mut packet.as_mut(), &keypair).unwrap();
⋮----
assert_matches!(
⋮----
fn test_merkle_shred_wire_layout(repaired: bool, is_last_in_slot: bool) {
⋮----
for (index, shred) in shreds.iter().enumerate() {
⋮----
let mut packet = shred.payload().to_packet(nonce);
⋮----
assert_eq!(
⋮----
let bytes = get_shred(packet).unwrap();
assert_eq!(bytes, shred.payload().as_ref());
⋮----
let shred_common_header = shred.common_header();
⋮----
assert_eq!(get_signature(bytes).unwrap(), shred_common_header.signature,);
⋮----
assert_eq!(get_slot(bytes).unwrap(), shred_common_header.slot);
assert_eq!(get_index(bytes).unwrap(), shred_common_header.index);
assert_eq!(get_version(bytes).unwrap(), shred_common_header.version);
assert_eq!(get_shred_id(bytes).unwrap(), {
⋮----
let mut bytes = bytes.to_vec();
⋮----
assert_matches!(set_retransmitter_signature(&mut bytes, &signature), Ok(()));
assert_eq!(get_retransmitter_signature(&bytes).unwrap(), signature);
let shred = shred::merkle::Shred::from_payload(bytes).unwrap();
assert_eq!(shred.retransmitter_signature().unwrap(), signature);
⋮----
let signature = keypair.sign_message(shred.merkle_root().unwrap().as_ref());
assert_matches!(resign_shred(&mut bytes, &keypair), Ok(()));
⋮----
assert_matches!(get_flags(bytes), Err(Error::InvalidShredType));
assert_matches!(get_data(bytes), Err(Error::InvalidShredType));
assert_matches!(get_reference_tick(bytes), Err(Error::InvalidShredType));
⋮----
let shred_data_header = shred.data_header();
⋮----
assert_eq!(get_flags(bytes).unwrap(), shred_data_header.flags);
assert_eq!(get_data_size(bytes).unwrap(), shred_data_header.size);
assert_eq!(get_data(bytes).unwrap(), shred.data().unwrap());
assert_eq!(get_reference_tick(bytes).unwrap(), {

================
File: ledger/src/ancestor_iterator.rs
================
pub struct AncestorIterator<'a> {
⋮----
pub fn new(start_slot: Slot, blockstore: &'a Blockstore) -> Self {
let current = blockstore.meta(start_slot).unwrap().and_then(|slot_meta| {
⋮----
pub fn new_inclusive(start_slot: Slot, blockstore: &'a Blockstore) -> Self {
⋮----
current: blockstore.meta(start_slot).unwrap().map(|_| start_slot),
⋮----
impl Iterator for AncestorIterator<'_> {
type Item = Slot;
fn next(&mut self) -> Option<Self::Item> {
⋮----
current.inspect(|&slot| {
⋮----
.meta(slot)
.unwrap()
.and_then(|slot_meta| slot_meta.parent_slot);
⋮----
pub struct AncestorIteratorWithHash<'a> {
⋮----
fn from(ancestor_iterator: AncestorIterator<'a>) -> Self {
⋮----
impl Iterator for AncestorIteratorWithHash<'_> {
type Item = (Slot, Hash);
⋮----
.next()
.and_then(|next_ancestor_slot| {
⋮----
.get_bank_hash(next_ancestor_slot)
.map(|next_ancestor_hash| (next_ancestor_slot, next_ancestor_hash))
⋮----
mod tests {
⋮----
fn setup_forks(ledger_path: &Path) -> Blockstore {
let blockstore = Blockstore::open(ledger_path).unwrap();
let tree = tr(0) / (tr(1) / (tr(2) / (tr(3))) / (tr(4)));
blockstore.add_tree(tree, true, true, 2, Hash::default());
⋮----
fn test_ancestor_iterator() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = setup_forks(ledger_path.path());
assert!(AncestorIterator::new(0, &blockstore).next().is_none());
assert_eq!(
⋮----
fn test_ancestor_iterator_inclusive() {
⋮----
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
let (shreds, _) = make_slot_entries(0, 0, 42);
blockstore.insert_shreds(shreds, None, false).unwrap();
let (shreds, _) = make_slot_entries(1, 0, 42);
⋮----
let (shreds, _) = make_slot_entries(2, 1, 42);
⋮----
fn test_ancestor_iterator_with_hash() {
⋮----
slot_to_bank_hash.insert(slot, bank_hash);
blockstore.insert_bank_hash(slot, bank_hash, false);
⋮----
assert!(

================
File: ledger/src/bank_forks_utils.rs
================
pub enum BankForksUtilsError {
⋮----
pub type LoadResult = result::Result<
⋮----
pub fn load(
⋮----
let (bank_forks, leader_schedule_cache, starting_snapshot_hashes, ..) = load_bank_forks(
⋮----
.map_err(BankForksUtilsError::ProcessBlockstoreFromRoot)?;
Ok((bank_forks, leader_schedule_cache, starting_snapshot_hashes))
⋮----
pub fn load_bank_forks(
⋮----
fn get_snapshots_to_load(
⋮----
if !snapshot_config.should_load_snapshots() {
info!("Snapshots disabled; will load from genesis");
⋮----
warn!(
⋮----
full_snapshot_archive_info.slot(),
⋮----
Some((
⋮----
get_snapshots_to_load(snapshot_config)
⋮----
info!(
⋮----
.expect("create bank snapshots dir");
let (bank_forks, starting_snapshot_hashes) = bank_forks_from_snapshot(
⋮----
(bank_forks, Some(starting_snapshot_hashes))
⋮----
info!("Processing ledger from genesis");
⋮----
.map_err(BankForksUtilsError::ProcessBlockstoreFromGenesis)?;
⋮----
LeaderScheduleCache::new_from_bank(&bank_forks.read().unwrap().root_bank());
⋮----
leader_schedule_cache.set_max_schedules(usize::MAX);
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
⋮----
.iter()
.for_each(|hard_fork_slot| root_bank.register_hard_fork(*hard_fork_slot));
⋮----
pub fn bank_forks_from_snapshot(
⋮----
if account_paths.is_empty() {
return Err(BankForksUtilsError::AccountPathsNotPresent);
⋮----
.as_ref()
.map(SnapshotArchiveInfoGetter::slot)
.unwrap_or(0),
⋮----
return Err(BankForksUtilsError::NoBankSnapshotDirectory {
flag: use_snapshot_archives_at_startup::cli::LONG_ARG.to_string(),
value: UseSnapshotArchivesAtStartup::Never.to_string(),
⋮----
Some(bank_snapshot)
⋮----
.filter(|bank_snapshot| bank_snapshot.slot >= latest_snapshot_archive_slot)
⋮----
process_options.debug_keys.clone(),
⋮----
process_options.accounts_db_config.clone(),
⋮----
.map_err(|err| BankForksUtilsError::BankFromSnapshotsDirectory {
⋮----
path: fastboot_snapshot.snapshot_path(),
⋮----
incremental_snapshot_archive_info.as_ref(),
⋮----
.map_err(|err| BankForksUtilsError::BankFromSnapshotsArchive {
⋮----
full_snapshot_archive: full_snapshot_archive_info.path().display().to_string(),
⋮----
.map(|archive| archive.path().display().to_string())
.unwrap_or("none".to_string()),
⋮----
if snapshot_config.should_generate_snapshots() {
⋮----
.set_latest_full_snapshot_slot(full_snapshot_archive_info.slot());
⋮----
assert!(bank
⋮----
let full_snapshot_hash = FullSnapshotHash((
⋮----
*full_snapshot_archive_info.hash(),
⋮----
incremental_snapshot_archive_info.map(|incremental_snapshot_archive_info| {
IncrementalSnapshotHash((
incremental_snapshot_archive_info.slot(),
*incremental_snapshot_archive_info.hash(),
⋮----
Ok((BankForks::new_rw_arc(bank), starting_snapshot_hashes))

================
File: ledger/src/bigtable_delete.rs
================
pub async fn delete_confirmed_blocks(
⋮----
if blocks_to_delete.is_empty() {
info!("No blocks to be deleted");
return Ok(());
⋮----
info!("{} blocks to be deleted", blocks_to_delete.len());
⋮----
for blocks in blocks_to_delete.chunks(NUM_BLOCKS_TO_DELETE_IN_PARALLEL) {
⋮----
info!("Preparing the next {} blocks for deletion", blocks.len());
⋮----
.iter()
.map(|block| bigtable.delete_confirmed_block(*block, dry_run));
⋮----
.zip(futures::future::join_all(deletion_futures).await)
⋮----
if result.is_err() {
error!(
⋮----
measure_delete.stop();
info!("{} for {} blocks", measure_delete, blocks.len());
⋮----
measure.stop();
info!("{measure}");
⋮----
Err(format!("Incomplete deletion, {failures} operations failed").into())
⋮----
Ok(())

================
File: ledger/src/bigtable_upload_service.rs
================
pub struct BigTableUploadService {
⋮----
impl BigTableUploadService {
pub fn new(
⋮----
pub fn new_with_config(
⋮----
info!("Starting BigTable upload service");
⋮----
.name("solBigTUpload".to_string())
.spawn(move || {
⋮----
.unwrap();
⋮----
fn run(
⋮----
let mut start_slot = blockstore.get_first_available_block().unwrap_or_default();
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
max_complete_transaction_status_slot.load(Ordering::SeqCst),
block_commitment_cache.read().unwrap().root(),
⋮----
let end_slot = min(
⋮----
start_slot.saturating_add(config.max_num_slots_to_check as u64 * 2),
⋮----
let result = runtime.block_on(bigtable_upload::upload_confirmed_blocks(
blockstore.clone(),
bigtable_ledger_storage.clone(),
⋮----
config.clone(),
exit.clone(),
⋮----
Ok(last_slot_uploaded) => start_slot = last_slot_uploaded.saturating_add(1),
⋮----
warn!("bigtable: upload_confirmed_blocks: {err}");
⋮----
start_slot = blockstore.get_first_available_block().unwrap_or_default();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread.join()

================
File: ledger/src/bigtable_upload.rs
================
pub struct ConfirmedBlockUploadConfig {
⋮----
impl Default for ConfirmedBlockUploadConfig {
fn default() -> Self {
⋮----
struct BlockstoreLoadStats {
⋮----
pub async fn upload_confirmed_blocks(
⋮----
info!("Loading ledger slots from {starting_slot} to {ending_slot}");
⋮----
.rooted_slot_iterator(starting_slot)
.map_err(|err| {
format!("Failed to load entries starting from slot {starting_slot}: {err:?}")
⋮----
.take_while(|slot| *slot <= ending_slot)
.collect();
if blockstore_slots.is_empty() {
warn!("Ledger has no slots from {starting_slot} to {ending_slot:?}");
return Ok(ending_slot);
⋮----
let first_blockstore_slot = *blockstore_slots.first().unwrap();
let last_blockstore_slot = *blockstore_slots.last().unwrap();
info!(
⋮----
let mut bigtable_slots = vec![];
⋮----
let num_bigtable_blocks = min(1000, config.max_num_slots_to_check * 2);
⋮----
.get_confirmed_blocks(start_slot, num_bigtable_blocks)
⋮----
error!("get_confirmed_blocks for {start_slot} failed: {err:?}");
⋮----
if next_bigtable_slots.is_empty() {
⋮----
bigtable_slots.append(&mut next_bigtable_slots);
start_slot = bigtable_slots.last().unwrap() + 1;
⋮----
.into_iter()
.filter(|slot| *slot <= last_blockstore_slot)
⋮----
let blockstore_slots = blockstore_slots.into_iter().collect::<HashSet<_>>();
let bigtable_slots = bigtable_slots.into_iter().collect::<HashSet<_>>();
⋮----
.difference(&bigtable_slots)
.cloned()
⋮----
blocks_to_upload.sort_unstable();
blocks_to_upload.truncate(config.max_num_slots_to_check);
⋮----
if blocks_to_upload.is_empty() {
⋮----
let last_slot = *blocks_to_upload.last().unwrap();
⋮----
let exit = exit.clone();
let (sender, receiver) = bounded(config.block_read_ahead_depth);
let (slot_sender, slot_receiver) = unbounded();
⋮----
.for_each(|b| slot_sender.send(b).unwrap());
drop(slot_sender);
⋮----
.map(|i| {
let blockstore = blockstore.clone();
let sender = sender.clone();
let slot_receiver = slot_receiver.clone();
⋮----
.name(format!("solBigTGetBlk{i:02}"))
.spawn(move || {
⋮----
while let Ok(slot) = slot_receiver.recv() {
if exit.load(Ordering::Relaxed) {
⋮----
let _ = match blockstore.get_rooted_block_with_entries(slot, true) {
⋮----
sender.send((slot, Some(confirmed_block_with_entries)))
⋮----
warn!(
⋮----
sender.send((slot, None))
⋮----
elapsed: start.elapsed(),
⋮----
.unwrap()
⋮----
.collect(),
⋮----
use futures::stream::StreamExt;
⋮----
tokio_stream::iter(receiver.into_iter()).chunks(config.num_blocks_to_upload_in_parallel);
while let Some(blocks) = stream.next().await {
⋮----
let mut num_blocks = blocks.len();
info!("Preparing the next {num_blocks} blocks for upload");
let uploads = blocks.into_iter().filter_map(|(slot, block)| match block {
⋮----
let bt = bigtable.clone();
Some(tokio::spawn(async move {
bt.upload_confirmed_block_with_entries(slot, confirmed_block)
⋮----
error!("upload_confirmed_block() join failed: {err:?}");
⋮----
} else if let Err(err) = result.unwrap() {
error!("upload_confirmed_block() upload failed: {err:?}");
⋮----
measure_upload.stop();
info!("{measure_upload} for {num_blocks} blocks");
⋮----
measure.stop();
info!("{measure}");
let blockstore_results = loader_threads.into_iter().map(|t| t.join());
⋮----
blockstore_load_wallclock = max(stats.elapsed, blockstore_load_wallclock);
⋮----
error!("error joining blockstore thread: {e:?}");
⋮----
Err(format!("Incomplete upload, {failures} operations failed").into())
⋮----
Ok(last_slot)

================
File: ledger/src/bit_vec.rs
================
type Word = u8;
⋮----
pub struct BitVec<const NUM_BITS: usize> {
⋮----
impl<const NUM_BITS: usize> Default for BitVec<NUM_BITS> {
fn default() -> Self {
⋮----
words: vec![0; Self::NUM_WORDS],
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
words.extend_from_slice(bytes);
words.resize(Self::NUM_WORDS, 0);
Ok(Self { words })
⋮----
pub enum BitVecError {
⋮----
const NUM_WORDS: usize = NUM_BITS.div_ceil(BITS_PER_WORD);
/// Get the word and bit offset for the given index.
    ///
⋮----
///
    /// # Examples
⋮----
/// # Examples
    ///
⋮----
///
    /// ```
⋮----
/// ```
    /// # use solana_ledger::bit_vec::BitVec;
⋮----
/// # use solana_ledger::bit_vec::BitVec;
    /// let (word_idx, bit_idx) = BitVec::<64>::location_of(63);
⋮----
/// let (word_idx, bit_idx) = BitVec::<64>::location_of(63);
    /// assert_eq!(word_idx, 7);
⋮----
/// assert_eq!(word_idx, 7);
    /// assert_eq!(bit_idx, 7);
⋮----
/// assert_eq!(bit_idx, 7);
    /// ```
⋮----
/// ```
    pub fn location_of(idx: usize) -> (usize, usize) {
⋮----
pub fn location_of(idx: usize) -> (usize, usize) {
⋮----
fn check_bounds(&self, idx: usize) -> Result<(), BitVecError> {
⋮----
return Err(BitVecError::OutOfBounds {
⋮----
Ok(())
⋮----
/// Remove a bit at the given index.
    ///
⋮----
///
    /// Returns `true` if the bit was set, `false` otherwise.
⋮----
/// Returns `true` if the bit was set, `false` otherwise.
    ///
⋮----
///
    /// # Panics
⋮----
/// # Panics
    ///
⋮----
///
    /// Panics if the index is out of bounds.
⋮----
/// Panics if the index is out of bounds.
    ///
⋮----
///
    /// ```should_panic
⋮----
/// ```should_panic
    /// # use solana_ledger::bit_vec::BitVec;
⋮----
/// # use solana_ledger::bit_vec::BitVec;
    /// let mut bit_vec = BitVec::<64>::default();
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// bit_vec.remove_unchecked(64);
⋮----
/// bit_vec.remove_unchecked(64);
    /// ```
⋮----
/// ```
    ///
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// assert!(bit_vec.insert_unchecked(63));
⋮----
/// assert!(bit_vec.insert_unchecked(63));
    /// assert!(bit_vec.remove_unchecked(63));
⋮----
/// assert!(bit_vec.remove_unchecked(63));
    /// assert!(!bit_vec.remove_unchecked(63));
⋮----
/// assert!(!bit_vec.remove_unchecked(63));
    /// ```
⋮----
/// ```
    pub fn remove_unchecked(&mut self, idx: usize) -> bool {
⋮----
pub fn remove_unchecked(&mut self, idx: usize) -> bool {
⋮----
///
    /// # Errors
⋮----
/// # Errors
    ///
⋮----
///
    /// Returns an error if the index is out of bounds.
⋮----
/// Returns an error if the index is out of bounds.
    ///
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// assert!(bit_vec.remove(64).is_err());
⋮----
/// assert!(bit_vec.remove(64).is_err());
    /// ```
⋮----
/// assert!(bit_vec.insert_unchecked(63));
    /// assert!(bit_vec.remove(63).is_ok());
⋮----
/// assert!(bit_vec.remove(63).is_ok());
    /// assert!(!bit_vec.remove(63).unwrap());
⋮----
/// assert!(!bit_vec.remove(63).unwrap());
    /// ```
⋮----
/// ```
    pub fn remove(&mut self, idx: usize) -> Result<bool, BitVecError> {
⋮----
pub fn remove(&mut self, idx: usize) -> Result<bool, BitVecError> {
self.check_bounds(idx)?;
Ok(self.remove_unchecked(idx))
⋮----
/// Insert a bit at the given index.
    ///
⋮----
///
    /// Returns `true` if the bit was not already set, `false` otherwise.
⋮----
/// Returns `true` if the bit was not already set, `false` otherwise.
    ///
⋮----
/// # use solana_ledger::bit_vec::BitVec;
    /// let mut bit_vec = BitVec::<8>::default();
⋮----
/// let mut bit_vec = BitVec::<8>::default();
    /// bit_vec.insert_unchecked(64);
⋮----
/// bit_vec.insert_unchecked(64);
    /// ```
⋮----
/// assert!(bit_vec.insert_unchecked(63));
    /// assert!(!bit_vec.insert_unchecked(63));
⋮----
/// assert!(!bit_vec.insert_unchecked(63));
    /// ```
⋮----
/// ```
    pub fn insert_unchecked(&mut self, idx: usize) -> bool {
⋮----
pub fn insert_unchecked(&mut self, idx: usize) -> bool {
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// assert!(bit_vec.insert(64).is_err());
⋮----
/// assert!(bit_vec.insert(64).is_err());
    /// ```
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// assert!(bit_vec.insert(63).is_ok());
⋮----
/// assert!(bit_vec.insert(63).is_ok());
    /// assert!(!bit_vec.insert(63).unwrap());
⋮----
/// assert!(!bit_vec.insert(63).unwrap());
    /// ```
⋮----
/// ```
    pub fn insert(&mut self, idx: usize) -> Result<bool, BitVecError> {
⋮----
pub fn insert(&mut self, idx: usize) -> Result<bool, BitVecError> {
⋮----
Ok(self.insert_unchecked(idx))
⋮----
/// Check if a bit is set at the given index.
    ///
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// bit_vec.insert(63);
⋮----
/// bit_vec.insert(63);
    /// assert!(bit_vec.contains(63));
⋮----
/// assert!(bit_vec.contains(63));
    /// ```
⋮----
/// ```
    pub fn contains(&self, idx: usize) -> bool {
⋮----
pub fn contains(&self, idx: usize) -> bool {
if self.check_bounds(idx).is_err() {
⋮----
/// Get an iterator over the bits in the array within the given range.
    ///
⋮----
///
    /// See [`BitVecSlice::from_range_bounds`] for more information.
⋮----
/// See [`BitVecSlice::from_range_bounds`] for more information.
    ///
⋮----
/// # use solana_ledger::bit_vec::BitVec;
    ///
⋮----
///
    /// let mut bit_vec = BitVec::<64>::default();
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// bit_vec.insert(0);
⋮----
/// bit_vec.insert(0);
    /// bit_vec.insert(1);
⋮----
/// bit_vec.insert(1);
    /// assert_eq!(bit_vec.range(..2).iter_ones().collect::<Vec<_>>(), [0, 1]);
⋮----
/// assert_eq!(bit_vec.range(..2).iter_ones().collect::<Vec<_>>(), [0, 1]);
    /// assert_eq!(bit_vec.range(1..).count_ones(), 1);
⋮----
/// assert_eq!(bit_vec.range(1..).count_ones(), 1);
    /// ```
⋮----
/// ```
    pub fn range(&self, bounds: impl RangeBounds<usize>) -> BitVecSlice<'_, NUM_BITS> {
⋮----
pub fn range(&self, bounds: impl RangeBounds<usize>) -> BitVecSlice<'_, NUM_BITS> {
⋮----
pub fn iter_ones(&self) -> impl DoubleEndedIterator<Item = usize> + '_ {
self.range(..NUM_BITS).iter_ones()
⋮----
/// Count the number of set bits in the array.
    ///
⋮----
/// bit_vec.insert(1);
    /// assert_eq!(bit_vec.count_ones(), 2);
⋮----
/// assert_eq!(bit_vec.count_ones(), 2);
    /// ```
⋮----
/// ```
    pub fn count_ones(&self) -> usize {
⋮----
pub fn count_ones(&self) -> usize {
self.range(..NUM_BITS).count_ones()
⋮----
/// Shorthand for checking if there are no set bits in the array.
    ///
⋮----
/// let mut bit_vec = BitVec::<64>::default();
    /// assert!(bit_vec.is_empty());
⋮----
/// assert!(bit_vec.is_empty());
    /// ```
⋮----
/// ```
    pub fn is_empty(&self) -> bool {
⋮----
pub fn is_empty(&self) -> bool {
self.count_ones() == 0
⋮----
fn from_iter<T: IntoIterator<Item = usize>>(iter: T) -> Self {
⋮----
bit_vec.insert_unchecked(idx);
⋮----
/// A slice of a [`BitVec`] that provides efficient bit-level iteration.
pub struct BitVecSlice<'a, const NUM_BITS: usize> {
⋮----
pub struct BitVecSlice<'a, const NUM_BITS: usize> {
⋮----
/// Construct a new [`BitVecSlice`] from a [`BitVec`] and a range.
    ///
⋮----
///
    /// Internal function -- use [`BitVec::range`].
⋮----
/// Internal function -- use [`BitVec::range`].
    fn from_range_bounds(bit_vec: &'a BitVec<NUM_BITS>, bounds: impl RangeBounds<usize>) -> Self {
⋮----
fn from_range_bounds(bit_vec: &'a BitVec<NUM_BITS>, bounds: impl RangeBounds<usize>) -> Self {
let start = match bounds.start_bound() {
⋮----
.min(NUM_BITS);
let end = match bounds.end_bound() {
⋮----
let end_word: usize = end.div_ceil(BITS_PER_WORD);
let start_word = (start / BITS_PER_WORD).min(end_word);
⋮----
iter: bit_vec.words[start_word..end_word].iter().enumerate(),
⋮----
pub fn count_ones(self) -> usize {
⋮----
.map(|(_, mask)| mask.count_ones() as usize)
.sum()
⋮----
pub fn iter_ones(self) -> impl DoubleEndedIterator<Item = usize> + 'a {
⋮----
.flat_map(|(base_idx, mask)| IterOnes { base_idx, mask })
⋮----
/// An iterator over masked words of a [`BitVecSlice`].
struct BitVecMaskIter<'a, const NUM_BITS: usize> {
⋮----
struct BitVecMaskIter<'a, const NUM_BITS: usize> {
⋮----
fn compute_word_mask(&self, word_idx: usize, word: &Word) -> (usize, Word) {
⋮----
let lower_bound = self.start.saturating_sub(base_idx);
⋮----
impl<const NUM_BITS: usize> Iterator for BitVecMaskIter<'_, NUM_BITS> {
type Item = (usize, Word);
fn next(&mut self) -> Option<Self::Item> {
⋮----
.next()
.map(|(word_idx, word)| self.compute_word_mask(word_idx, word))
⋮----
impl<const NUM_BITS: usize> DoubleEndedIterator for BitVecMaskIter<'_, NUM_BITS> {
fn next_back(&mut self) -> Option<Self::Item> {
⋮----
.next_back()
⋮----
struct IterOnes {
⋮----
impl Iterator for IterOnes {
type Item = usize;
⋮----
let bit_idx = self.mask.trailing_zeros() as usize;
⋮----
Some(self.base_idx + bit_idx)
⋮----
impl DoubleEndedIterator for IterOnes {
⋮----
let bit_idx = BITS_PER_WORD - 1 - self.mask.leading_zeros() as usize;
⋮----
Some(index)
⋮----
impl ExactSizeIterator for IterOnes {
fn len(&self) -> usize {
self.mask.count_ones() as usize
⋮----
mod tests {
⋮----
fn rand_range(range: Range<usize>) -> impl Strategy<Value = Range<usize>> {
(range.clone(), range).prop_map(
⋮----
proptest! {
⋮----
fn test_bit_vec_range_bound_combinations() {
⋮----
bit_vec.insert_unchecked(10);
bit_vec.insert_unchecked(20);
bit_vec.insert_unchecked(30);
bit_vec.insert_unchecked(40);
⋮----
(Included(10), Included(30), vec![10, 20, 30]),
(Included(10), Excluded(30), vec![10, 20]),
(Excluded(10), Included(30), vec![20, 30]),
(Excluded(10), Excluded(30), vec![20]),
(Unbounded, Included(20), vec![10, 20]),
(Unbounded, Excluded(20), vec![10]),
(Included(30), Unbounded, vec![30, 40]),
(Excluded(30), Unbounded, vec![40]),
(Unbounded, Unbounded, vec![10, 20, 30, 40]),
⋮----
.range((start_bound, end_bound))
.iter_ones()
.collect();
assert_eq!(
⋮----
fn test_bit_vec_boundary_conditions() {
⋮----
bit_vec.insert_unchecked(0);
bit_vec.insert_unchecked(7);
bit_vec.insert_unchecked(8);
bit_vec.insert_unchecked(15);
bit_vec.insert_unchecked(MAX_BITS - 1);
assert!(bit_vec.insert(MAX_BITS).is_err());
assert!(bit_vec.contains(0));
assert!(bit_vec.contains(7));
assert!(bit_vec.contains(8));
assert!(bit_vec.contains(15));
assert!(bit_vec.contains(MAX_BITS - 1));
assert!(!bit_vec.contains(MAX_BITS));
⋮----
assert_eq!(bit_vec.range(0..0).iter_ones().count(), 0);
assert_eq!(bit_vec.range(1..1).iter_ones().count(), 0);
let oversized_range = bit_vec.range(0..MAX_BITS + 1);
assert_eq!(oversized_range.iter_ones().count(), 5);
bit_vec.remove_unchecked(0);
assert!(!bit_vec.contains(0));
bit_vec.remove_unchecked(7);
assert!(!bit_vec.contains(7));
bit_vec.remove_unchecked(8);
assert!(!bit_vec.contains(8));
bit_vec.remove_unchecked(15);
assert!(!bit_vec.contains(15));
bit_vec.remove_unchecked(MAX_BITS - 1);
assert!(!bit_vec.contains(MAX_BITS - 1));
assert_eq!(bit_vec.count_ones(), 0);

================
File: ledger/src/block_error.rs
================
use thiserror::Error;
⋮----
pub enum BlockError {

================
File: ledger/src/blockstore_cleanup_service.rs
================
pub struct BlockstoreCleanupService {
⋮----
impl BlockstoreCleanupService {
pub fn new(blockstore: Arc<Blockstore>, max_ledger_shreds: u64, exit: Arc<AtomicBool>) -> Self {
⋮----
.name("solBstoreClean".to_string())
.spawn(move || {
info!(
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
if last_check_time.elapsed() > LOOP_LIMITER {
⋮----
info!("BlockstoreCleanupService has stopped");
⋮----
.unwrap();
⋮----
fn find_slots_to_clean(
⋮----
.live_files_metadata()
.expect("Blockstore::live_files_metadata()");
⋮----
.iter()
.filter(|live_file| live_file.column_family_name == columns::ShredData::NAME)
.map(|file_meta| file_meta.num_entries)
.sum();
let lowest_slot = blockstore.lowest_slot();
⋮----
.highest_slot()
.expect("Blockstore::highest_slot()")
.unwrap_or(lowest_slot);
⋮----
error!(
⋮----
.checked_div(mean_shreds_per_slot);
⋮----
error!("Skipping Blockstore cleanup: calculated mean of 0 shreds per slot");
⋮----
pub fn cleanup_ledger(
⋮----
let root = blockstore.max_root();
⋮----
info!("Looking for Blockstore data to cleanup, latest root: {root}");
let disk_utilization_pre = blockstore.storage_size();
⋮----
*blockstore.lowest_cleanup_slot.write().unwrap() = lowest_cleanup_slot;
⋮----
blockstore.purge_slots(0, lowest_cleanup_slot, PurgeType::CompactionFilter);
blockstore.set_max_expired_slot(lowest_cleanup_slot);
purge_time.stop();
info!("Cleaned up Blockstore data older than slot {lowest_cleanup_slot}. {purge_time}");
⋮----
let disk_utilization_post = blockstore.storage_size();
⋮----
fn report_disk_metrics(
⋮----
datapoint_info!(
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_cleanup.join()
⋮----
mod tests {
⋮----
fn flush_blockstore_contents_to_disk(blockstore: Blockstore) -> Blockstore {
let ledger_path = blockstore.ledger_path().clone();
drop(blockstore);
Blockstore::open(&ledger_path).unwrap()
⋮----
fn test_find_slots_to_clean() {
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
let (shreds, _) = make_many_slot_entries(1, num_slots, num_entries);
let shreds_per_slot = (shreds.len() / num_slots as usize) as u64;
assert!(shreds_per_slot > 1);
blockstore.insert_shreds(shreds, None, false).unwrap();
let blockstore = Arc::new(flush_blockstore_contents_to_disk(blockstore));
⋮----
assert!(should_clean && lowest_purged == 0);
⋮----
assert!(!should_clean && lowest_purged == 0);
⋮----
assert!(should_clean && lowest_purged == slot);
⋮----
fn test_cleanup() {
⋮----
let (shreds, _) = make_many_slot_entries(0, 50, 5);
⋮----
blockstore.set_roots([50].iter()).unwrap();
⋮----
assert_eq!(last_purge_slot, 50);
⋮----
.slot_meta_iterator(0)
.unwrap()
.for_each(|(slot, _)| assert!(slot > 40));
⋮----
fn test_cleanup_speed() {
⋮----
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
let (shreds, _) = make_many_slot_entries(0, initial_slots, initial_entries);
⋮----
first_insert.stop();
info!("{first_insert}");
⋮----
let (shreds, _) = make_many_slot_entries(slot + i * batch_size, batch_size, 5);
⋮----
info!("inserting..{i} of {batches}");
⋮----
insert_time.stop();
⋮----
blockstore.set_roots([slot + num_slots].iter()).unwrap();
⋮----
time.stop();
info!("slot: {slot} size: {num_slots} {insert_time} {time}");

================
File: ledger/src/blockstore_db.rs
================
pub enum IteratorMode<Index> {
⋮----
struct OldestSlot {
⋮----
impl OldestSlot {
pub fn set(&self, oldest_slot: Slot) {
self.slot.store(oldest_slot, Ordering::Relaxed);
⋮----
pub fn get(&self) -> Slot {
self.slot.load(Ordering::Relaxed)
⋮----
pub(crate) fn set_clean_slot_0(&self, clean_slot_0: bool) {
self.clean_slot_0.store(clean_slot_0, Ordering::Relaxed);
⋮----
pub(crate) fn get_clean_slot_0(&self) -> bool {
self.clean_slot_0.load(Ordering::Relaxed)
⋮----
pub(crate) struct Rocks {
⋮----
impl Rocks {
pub(crate) fn open(path: PathBuf, options: BlockstoreOptions) -> Result<Rocks> {
let recovery_mode = options.recovery_mode.clone();
⋮----
let mut db_options = get_db_options(&options);
⋮----
db_options.set_wal_recovery_mode(recovery_mode.into());
⋮----
info!(
⋮----
if db.cf_handle(DEPRECATED_PROGRAM_COSTS_COLUMN_NAME).is_some() {
db.drop_cf(DEPRECATED_PROGRAM_COSTS_COLUMN_NAME)?;
⋮----
rocks.configure_compaction();
Ok(rocks)
⋮----
fn cf_descriptors(
⋮----
let mut cf_descriptors = vec![
⋮----
if !must_open_all_column_families(&options.access_type) {
⋮----
warn!("Unable to detect Rocks columns: {err:?}");
vec![]
⋮----
.iter()
.map(|cf_descriptor| cf_descriptor.name().to_string())
.chain(std::iter::once(DEFAULT_COLUMN_NAME.to_string()))
.collect();
detected_cfs.iter().for_each(|cf_name| {
if !known_cfs.contains(cf_name.as_str()) {
info!("Detected unknown column {cf_name}, opening column with basic options");
⋮----
options.set_write_buffer_size(1024 * 1024);
options.set_disable_auto_compactions(true);
cf_descriptors.push(ColumnFamilyDescriptor::new(cf_name, options));
⋮----
const fn columns() -> [&'static str; 20] {
⋮----
// Configure compaction on a per-column basis
fn configure_compaction(&self) {
// If compactions are disabled altogether, no need to tune values
if should_disable_auto_compactions(&self.access_type) {
⋮----
if should_enable_cf_compaction(cf_name) {
let cf_handle = self.cf_handle(cf_name);
⋮----
.set_options_cf(
⋮----
&PERIODIC_COMPACTION_SECONDS.to_string(),
⋮----
.unwrap();
⋮----
pub(crate) fn column<C>(self: &Arc<Self>) -> LedgerColumn<C>
⋮----
pub(crate) fn destroy(path: &Path) -> Result<()> {
⋮----
Ok(())
⋮----
pub(crate) fn cf_handle(&self, cf: &str) -> &ColumnFamily {
⋮----
.cf_handle(cf)
.expect("should never get an unknown column")
⋮----
fn get_cf<K: AsRef<[u8]>>(&self, cf: &ColumnFamily, key: K) -> Result<Option<Vec<u8>>> {
let opt = self.db.get_cf(cf, key)?;
Ok(opt)
⋮----
fn get_pinned_cf(
⋮----
let opt = self.db.get_pinned_cf(cf, key)?;
⋮----
fn put_cf<K: AsRef<[u8]>>(&self, cf: &ColumnFamily, key: K, value: &[u8]) -> Result<()> {
self.db.put_cf(cf, key, value)?;
⋮----
fn multi_get_cf<'a, K, I>(
⋮----
.batched_multi_get_cf(cf, keys,  false)
.into_iter()
.map(|out| out.map_err(BlockstoreError::RocksDb))
⋮----
fn delete_cf<K: AsRef<[u8]>>(&self, cf: &ColumnFamily, key: K) -> Result<()> {
self.db.delete_cf(cf, key)?;
⋮----
fn delete_file_in_range_cf<K: AsRef<[u8]>>(
⋮----
self.db.delete_file_in_range_cf(cf, from_key, to_key)?;
⋮----
pub(crate) fn iterator_cf(
⋮----
self.db.iterator_cf(cf, iterator_mode)
⋮----
pub(crate) fn raw_iterator_cf(&self, cf: &ColumnFamily) -> Result<DBRawIterator<'_>> {
Ok(self.db.raw_iterator_cf(cf))
⋮----
pub(crate) fn batch(&self) -> Result<WriteBatch> {
Ok(WriteBatch {
⋮----
pub(crate) fn write(&self, batch: WriteBatch) -> Result<()> {
let op_start_instant = maybe_enable_rocksdb_perf(
⋮----
let result = self.db.write(batch.write_batch);
⋮----
report_rocksdb_write_perf(
⋮----
&op_start_instant.elapsed(),
⋮----
Ok(_) => Ok(()),
Err(e) => Err(BlockstoreError::RocksDb(e)),
⋮----
pub(crate) fn is_primary_access(&self) -> bool {
⋮----
fn get_int_property_cf(&self, cf: &ColumnFamily, name: &'static std::ffi::CStr) -> Result<i64> {
match self.db.property_int_value_cf(cf, name) {
Ok(Some(value)) => Ok(value.try_into().unwrap()),
Ok(None) => Ok(0),
⋮----
pub(crate) fn live_files_metadata(&self) -> Result<Vec<LiveFile>> {
match self.db.live_files() {
Ok(live_files) => Ok(live_files),
⋮----
pub(crate) fn storage_size(&self) -> Result<u64> {
Ok(fs_extra::dir::get_size(&self.path)?)
⋮----
pub(crate) fn set_oldest_slot(&self, oldest_slot: Slot) {
self.oldest_slot.set(oldest_slot);
⋮----
self.oldest_slot.set_clean_slot_0(clean_slot_0);
⋮----
pub struct LedgerColumn<C: Column + ColumnName> {
⋮----
pub fn submit_rocksdb_cf_metrics(&self) {
⋮----
.get_int_property(RocksProperties::TOTAL_SST_FILES_SIZE)
.unwrap_or(BLOCKSTORE_METRICS_ERROR),
⋮----
.get_int_property(RocksProperties::SIZE_ALL_MEM_TABLES)
⋮----
.get_int_property(RocksProperties::NUM_SNAPSHOTS)
⋮----
.get_int_property(RocksProperties::OLDEST_SNAPSHOT_TIME)
⋮----
.get_int_property(RocksProperties::ACTUAL_DELAYED_WRITE_RATE)
⋮----
.get_int_property(RocksProperties::IS_WRITE_STOPPED)
⋮----
.get_int_property(RocksProperties::BLOCK_CACHE_CAPACITY)
⋮----
.get_int_property(RocksProperties::BLOCK_CACHE_USAGE)
⋮----
.get_int_property(RocksProperties::BLOCK_CACHE_PINNED_USAGE)
⋮----
.get_int_property(RocksProperties::ESTIMATE_TABLE_READERS_MEM)
⋮----
.get_int_property(RocksProperties::MEM_TABLE_FLUSH_PENDING)
⋮----
.get_int_property(RocksProperties::COMPACTION_PENDING)
⋮----
.get_int_property(RocksProperties::NUM_RUNNING_COMPACTIONS)
⋮----
.get_int_property(RocksProperties::NUM_RUNNING_FLUSHES)
⋮----
.get_int_property(RocksProperties::ESTIMATE_OLDEST_KEY_TIME)
⋮----
.get_int_property(RocksProperties::BACKGROUND_ERRORS)
⋮----
cf_rocksdb_metrics.report_metrics(C::NAME, &self.column_options);
⋮----
pub struct WriteBatch {
⋮----
pub(crate) struct BlockstoreByteReference<'a> {
⋮----
fn from(slice: DBPinnableSlice<'a>) -> Self {
⋮----
type Target = [u8];
⋮----
fn deref(&self) -> &[u8] {
⋮----
fn as_ref(&self) -> &[u8] {
⋮----
impl WriteBatch {
fn put_cf<K: AsRef<[u8]>>(&mut self, cf: &ColumnFamily, key: K, value: &[u8]) -> Result<()> {
self.write_batch.put_cf(cf, key, value);
⋮----
fn delete_cf<K: AsRef<[u8]>>(&mut self, cf: &ColumnFamily, key: K) -> Result<()> {
self.write_batch.delete_cf(cf, key);
⋮----
fn delete_range_cf<K: AsRef<[u8]>>(&mut self, cf: &ColumnFamily, from: K, to: K) -> Result<()> {
self.write_batch.delete_range_cf(cf, from, to);
⋮----
pub fn get_bytes(&self, index: C::Index) -> Result<Option<Vec<u8>>> {
let is_perf_enabled = maybe_enable_rocksdb_perf(
⋮----
let result = self.backend.get_cf(self.handle(), key);
⋮----
report_rocksdb_read_perf(
⋮----
/// Create a key type suitable for use with multi_get_bytes() and
    /// multi_get(). Those functions return iterators, so the keys must be
⋮----
/// multi_get(). Those functions return iterators, so the keys must be
    /// created with a separate function in order to live long enough
⋮----
/// created with a separate function in order to live long enough
    pub(crate) fn multi_get_keys<I>(&self, keys: I) -> Vec<<C as Column>::Key>
⋮----
pub(crate) fn multi_get_keys<I>(&self, keys: I) -> Vec<<C as Column>::Key>
⋮----
keys.into_iter().map(|index| C::key(&index)).collect()
⋮----
pub(crate) fn multi_get_bytes<'a, K>(
⋮----
.multi_get_cf(self.handle(), keys)
.map(|out| Ok(out?.map(BlockstoreByteReference::from)));
⋮----
pub fn iter(
⋮----
RocksIteratorMode::From(start_key.as_ref(), direction)
⋮----
let iter = self.backend.iterator_cf(self.handle(), iterator_mode);
Ok(iter.map(|pair| {
let (key, value) = pair.unwrap();
⋮----
// The validator performs compactions asynchronously, this method is
// provided to force a synchronous compaction to test our compaction filter
pub fn compact(&self) {
// compact_range_cf() optionally takes a start and end key to limit
// compaction. Providing values will result in a different method
// getting called in the rocksdb code, even if the specified keys span
// the entire key range of the column
//
// Internally, rocksdb will do some checks to figure out if it should
// run a compaction. Empirically, it has been found that passing the
// keys leads to more variability in whether rocksdb runs a compaction
// or not. For the sake of our unit tests, we want the compaction to
// run everytime. So, set the keys as None which will result in rocksdb
// using the heavier method to determine if a compaction should run
⋮----
self.backend.db.compact_range_cf(self.handle(), start, end);
⋮----
pub fn handle(&self) -> &ColumnFamily {
self.backend.cf_handle(C::NAME)
⋮----
pub fn is_empty(&self) -> Result<bool> {
let mut iter = self.backend.raw_iterator_cf(self.handle())?;
iter.seek_to_first();
Ok(!iter.valid())
⋮----
pub fn put_bytes(&self, index: C::Index, value: &[u8]) -> Result<()> {
⋮----
let result = self.backend.put_cf(self.handle(), key, value);
⋮----
pub fn put_bytes_in_batch(
⋮----
batch.put_cf(self.handle(), key, value)
⋮----
/// Retrieves the specified RocksDB integer property of the current
    /// column family.
⋮----
/// column family.
    ///
⋮----
///
    /// Full list of properties that return int values could be found
⋮----
/// Full list of properties that return int values could be found
    /// [here](https://github.com/facebook/rocksdb/blob/08809f5e6cd9cc4bc3958dd4d59457ae78c76660/include/rocksdb/db.h#L654-L689).
⋮----
/// [here](https://github.com/facebook/rocksdb/blob/08809f5e6cd9cc4bc3958dd4d59457ae78c76660/include/rocksdb/db.h#L654-L689).
    pub fn get_int_property(&self, name: &'static std::ffi::CStr) -> Result<i64> {
⋮----
pub fn get_int_property(&self, name: &'static std::ffi::CStr) -> Result<i64> {
self.backend.get_int_property_cf(self.handle(), name)
⋮----
pub fn delete(&self, index: C::Index) -> Result<()> {
⋮----
let result = self.backend.delete_cf(self.handle(), key);
⋮----
pub fn delete_in_batch(&self, batch: &mut WriteBatch, index: C::Index) -> Result<()> {
⋮----
batch.delete_cf(self.handle(), key)
⋮----
pub fn delete_range_in_batch(&self, batch: &mut WriteBatch, from: Slot, to: Slot) -> Result<()>
⋮----
let to_key = <C as Column>::key(&C::as_index(to.saturating_add(1)));
batch.delete_range_cf(self.handle(), from_key, to_key)
⋮----
pub fn delete_file_in_range(&self, from: Slot, to: Slot) -> Result<()>
⋮----
.delete_file_in_range_cf(self.handle(), from_key, to_key)
⋮----
pub(crate) fn multi_get<'a, K>(
⋮----
.map(|out| out?.as_deref().map(C::deserialize).transpose());
⋮----
pub fn get(&self, index: C::Index) -> Result<Option<C::Type>> {
⋮----
self.get_raw(key)
⋮----
pub fn get_raw<K: AsRef<[u8]>>(&self, key: K) -> Result<Option<C::Type>> {
let mut result = Ok(None);
⋮----
if let Some(pinnable_slice) = self.backend.get_pinned_cf(self.handle(), key)? {
let value = C::deserialize(pinnable_slice.as_ref())?;
result = Ok(Some(value))
⋮----
pub fn put(&self, index: C::Index, value: &C::Type) -> Result<()> {
⋮----
let result = self.backend.put_cf(self.handle(), key, &serialized_value);
⋮----
pub fn put_in_batch(
⋮----
batch.put_cf(self.handle(), key, &serialized_value)
⋮----
pub fn get_protobuf_or_bincode<T: DeserializeOwned + Into<C::Type>>(
⋮----
pub(crate) fn get_raw_protobuf_or_bincode<T: DeserializeOwned + Into<C::Type>>(
⋮----
let result = self.backend.get_pinned_cf(self.handle(), key);
⋮----
let value = match C::Type::decode(pinnable_slice.as_ref()) {
⋮----
Err(_) => deserialize::<T>(pinnable_slice.as_ref())?.into(),
⋮----
Ok(Some(value))
⋮----
Ok(None)
⋮----
pub fn get_protobuf(&self, index: C::Index) -> Result<Option<C::Type>> {
⋮----
Ok(Some(C::Type::decode(pinnable_slice.as_ref())?))
⋮----
pub fn put_protobuf(&self, index: C::Index, value: &C::Type) -> Result<()> {
let mut buf = Vec::with_capacity(value.encoded_len());
value.encode(&mut buf)?;
⋮----
let result = self.backend.put_cf(self.handle(), key, &buf);
⋮----
pub(crate) fn iter_current_index_filtered(
⋮----
Ok(iter.filter_map(|pair| {
⋮----
C::try_current_index(&key).ok().map(|index| (index, value))
⋮----
pub(crate) fn iter_deprecated_index_filtered(
⋮----
let iterator = self.backend.iterator_cf(self.handle(), iterator_mode);
Ok(iterator.filter_map(|pair| {
⋮----
.ok()
.map(|index| (index, value))
⋮----
pub(crate) fn delete_deprecated_in_batch(
⋮----
batch.delete_cf(self.handle(), &key)
⋮----
struct PurgedSlotFilter<C: Column + ColumnName> {
⋮----
impl<C: Column + ColumnName> CompactionFilter for PurgedSlotFilter<C> {
fn filter(&mut self, _level: u32, key: &[u8], _value: &[u8]) -> CompactionDecision {
⋮----
fn name(&self) -> &CStr {
⋮----
struct PurgedSlotFilterFactory<C: Column + ColumnName> {
⋮----
impl<C: Column + ColumnName> CompactionFilterFactory for PurgedSlotFilterFactory<C> {
type Filter = PurgedSlotFilter<C>;
fn create(&mut self, _context: CompactionFilterContext) -> Self::Filter {
let copied_oldest_slot = self.oldest_slot.get();
let copied_clean_slot_0 = self.oldest_slot.get_clean_slot_0();
⋮----
name: CString::new(format!(
⋮----
.unwrap(),
⋮----
fn new_cf_descriptor<C: 'static + Column + ColumnName>(
⋮----
fn get_cf_options<C: 'static + Column + ColumnName>(
⋮----
cf_options.set_max_write_buffer_number(8);
cf_options.set_write_buffer_size(MAX_WRITE_BUFFER_SIZE as usize);
⋮----
cf_options.set_level_zero_file_num_compaction_trigger(file_num_compaction_trigger as i32);
cf_options.set_max_bytes_for_level_base(total_size_base);
cf_options.set_target_file_size_base(file_size_base);
let disable_auto_compactions = should_disable_auto_compactions(&options.access_type);
⋮----
cf_options.set_disable_auto_compactions(true);
⋮----
if !disable_auto_compactions && should_enable_cf_compaction(C::NAME) {
cf_options.set_compaction_filter_factory(PurgedSlotFilterFactory::<C> {
oldest_slot: oldest_slot.clone(),
name: CString::new(format!("purged_slot_filter_factory({})", C::NAME)).unwrap(),
⋮----
fn process_cf_options_advanced<C: 'static + Column + ColumnName>(
⋮----
// Explicitly disable compression on all columns by default
// See https://docs.rs/rocksdb/0.21.0/rocksdb/struct.Options.html#method.set_compression_type
cf_options.set_compression_type(DBCompressionType::None);
⋮----
cf_options.set_compression_type(
⋮----
.to_rocksdb_compression_type(),
⋮----
fn get_db_options(blockstore_options: &BlockstoreOptions) -> Options {
⋮----
// Create missing items to support a clean start
options.create_if_missing(true);
options.create_missing_column_families(true);
// rocksdb builds two threadpools: low and high priority. The low priority
// pool is used for compactions whereas the high priority pool is used for
// memtable flushes. Separate pools are created so that compactions are
// unable to stall memtable flushes (which could stall memtable writes).
⋮----
// For now, use the deprecated methods to configure the exact amount of
// threads for each pool. The new method, set_max_background_jobs(N),
// configures N/4 low priority threads and 3N/4 high priority threads.
⋮----
options.set_max_background_compactions(
blockstore_options.num_rocksdb_compaction_threads.get() as i32,
⋮----
.set_max_background_flushes(blockstore_options.num_rocksdb_flush_threads.get() as i32);
⋮----
// Set max total wal size to 4G.
options.set_max_total_wal_size(4 * 1024 * 1024 * 1024);
if should_disable_auto_compactions(&blockstore_options.access_type) {
⋮----
// Limit to (10) 50 MB log files (500 MB total)
// Logs grow at < 5 MB / hour, so this provides several days of logs
options.set_max_log_file_size(50 * 1024 * 1024);
options.set_keep_log_file_num(10);
// Allow Rocks to open/keep open as many files as it needs for performance;
// it is not required for read-only access, but should be fine with our high ulimit.
options.set_max_open_files(-1);
⋮----
/// The default number of threads to use for rocksdb compaction in the rocksdb
/// low priority threadpool
⋮----
/// low priority threadpool
pub fn default_num_compaction_threads() -> NonZeroUsize {
⋮----
pub fn default_num_compaction_threads() -> NonZeroUsize {
NonZeroUsize::new(num_cpus::get()).expect("thread count is non-zero")
⋮----
/// The default number of threads to use for rocksdb memtable flushes in the
/// rocksdb high priority threadpool
⋮----
/// rocksdb high priority threadpool
pub fn default_num_flush_threads() -> NonZeroUsize {
⋮----
pub fn default_num_flush_threads() -> NonZeroUsize {
NonZeroUsize::new((num_cpus::get() / 4).max(1)).expect("thread count is non-zero")
⋮----
// Returns whether automatic compactions should be disabled for the entire
// database based upon the given access type.
fn should_disable_auto_compactions(access_type: &AccessType) -> bool {
// Leave automatic compactions enabled (do not disable) in Primary mode;
// disable in all other modes to prevent accidental cleaning
!matches!(access_type, AccessType::Primary)
⋮----
// Returns whether compactions should be enabled for the given column (name).
fn should_enable_cf_compaction(cf_name: &str) -> bool {
// In order to keep the ledger storage footprint within a desired size,
// LedgerCleanupService removes data in FIFO order by slot.
⋮----
// Several columns do not contain slot in their key. These columns must
// be manually managed to avoid unbounded storage growth.
⋮----
// Columns where slot is the primary index can be efficiently cleaned via
// Database::delete_range_cf() && Database::delete_file_in_range_cf().
⋮----
// Columns where a slot is part of the key but not the primary index can
// not be range deleted like above. Instead, the individual key/value pairs
// must be iterated over and a decision to keep or discard that pair is
// made. The comparison logic is implemented in PurgedSlotFilter which is
// configured to run as part of rocksdb's automatic compactions. Storage
matches!(
⋮----
fn should_enable_compression<C: 'static + Column + ColumnName>() -> bool {
⋮----
// If the access type is read-only, we don't need to open all of the columns
fn must_open_all_column_families(access_type: &AccessType) -> bool {
!matches!(access_type, AccessType::ReadOnly)
⋮----
pub mod tests {
⋮----
fn test_compaction_filter() {
⋮----
oldest_slot.set_clean_slot_0(true);
⋮----
name: CString::new("test compaction filter").unwrap(),
⋮----
let mut compaction_filter = factory.create(dummy_compaction_filter_context());
⋮----
let dummy_value = vec![];
assert!(matches!(
⋮----
oldest_slot.set(1);
⋮----
fn test_cf_names_and_descriptors_equal_length() {
⋮----
assert_eq!(
⋮----
fn test_should_disable_auto_compactions() {
assert!(!should_disable_auto_compactions(&AccessType::Primary));
assert!(should_disable_auto_compactions(
⋮----
assert!(should_disable_auto_compactions(&AccessType::ReadOnly));
⋮----
fn test_should_enable_cf_compaction() {
⋮----
columns_to_compact.iter().for_each(|cf_name| {
assert!(should_enable_cf_compaction(cf_name));
⋮----
assert!(!should_enable_cf_compaction("something else"));
⋮----
fn test_open_unknown_columns() {
⋮----
let temp_dir = tempdir().unwrap();
let db_path = temp_dir.path();
⋮----
let mut rocks = Rocks::open(db_path.to_path_buf(), options).unwrap();
⋮----
.create_cf("new_column", &Options::default())
⋮----
let _ = Rocks::open(db_path.to_path_buf(), options).unwrap();
⋮----
fn test_remove_deprecated_progam_costs_column_compat() {
⋮----
fn is_program_costs_column_present(path: &Path) -> bool {
⋮----
.unwrap()
⋮----
.any(|column_name| column_name == DEPRECATED_PROGRAM_COSTS_COLUMN_NAME)
⋮----
let _rocks = Rocks::open(db_path.to_path_buf(), options.clone()).unwrap();
⋮----
assert!(!is_program_costs_column_present(db_path));
⋮----
let mut rocks = Rocks::open(db_path.to_path_buf(), options.clone()).unwrap();
⋮----
.create_cf(DEPRECATED_PROGRAM_COSTS_COLUMN_NAME, &Options::default())
⋮----
assert!(is_program_costs_column_present(db_path));
⋮----
pub fn put_deprecated_protobuf(
⋮----
.put_cf(self.handle(), C::deprecated_key(index), &buf)
⋮----
pub fn put_deprecated(&self, index: C::DeprecatedIndex, value: &C::Type) -> Result<()> {
⋮----
.put_cf(self.handle(), C::deprecated_key(index), &serialized_value)

================
File: ledger/src/blockstore_meta.rs
================
bitflags! {
⋮----
impl Default for ConnectedFlags {
fn default() -> Self {
⋮----
pub type CompletedDataIndexesV1 = BTreeSet<u32>;
⋮----
pub struct CompletedDataIndexesV2 {
⋮----
impl CompletedDataIndexesV2 {
⋮----
pub fn iter(&self) -> impl DoubleEndedIterator<Item = u32> + '_ {
self.index.iter_ones().map(|i| i as u32)
⋮----
/// Only needed for V1 / V2 test compatibility.
    ///
⋮----
///
    /// TODO: Remove once the migration is complete.
⋮----
/// TODO: Remove once the migration is complete.
    #[cfg(test)]
⋮----
pub fn into_iter(&self) -> impl DoubleEndedIterator<Item = u32> + '_ {
self.iter()
⋮----
pub fn insert(&mut self, index: u32) {
self.index.insert_unchecked(index as usize);
⋮----
pub fn contains(&self, index: &u32) -> bool {
self.index.contains(*index as usize)
⋮----
pub fn is_empty(&self) -> bool {
self.index.is_empty()
⋮----
pub fn range<R>(&self, bounds: R) -> impl DoubleEndedIterator<Item = u32> + '_
⋮----
let start = bounds.start_bound().map(|&b| b as usize);
let end = bounds.end_bound().map(|&b| b as usize);
self.index.range((start, end)).iter_ones().map(|i| i as u32)
⋮----
fn from_iter<T: IntoIterator<Item = u32>>(iter: T) -> Self {
let index = iter.into_iter().map(|i| i as usize).collect();
⋮----
fn from(value: CompletedDataIndexesV2) -> Self {
value.iter().collect()
⋮----
fn from(value: CompletedDataIndexesV1) -> Self {
value.into_iter().collect()
⋮----
/// The Meta column family
pub struct SlotMetaBase<T> {
⋮----
pub struct SlotMetaBase<T> {
/// The number of slots above the root (the genesis block). The first
    /// slot has slot 0.
⋮----
/// slot has slot 0.
    pub slot: Slot,
/// The total number of consecutive shreds starting from index 0 we have received for this slot.
    /// At the same time, it is also an index of the first missing shred for this slot, while the
⋮----
/// At the same time, it is also an index of the first missing shred for this slot, while the
    /// slot is incomplete.
⋮----
/// slot is incomplete.
    pub consumed: u64,
/// The index *plus one* of the highest shred received for this slot.  Useful
    /// for checking if the slot has received any shreds yet, and to calculate the
⋮----
/// for checking if the slot has received any shreds yet, and to calculate the
    /// range where there is one or more holes: `(consumed..received)`.
⋮----
/// range where there is one or more holes: `(consumed..received)`.
    pub received: u64,
/// The timestamp of the first time a shred was added for this slot
    pub first_shred_timestamp: u64,
/// The index of the shred that is flagged as the last shred for this slot.
    /// None until the shred with LAST_SHRED_IN_SLOT flag is received.
⋮----
/// None until the shred with LAST_SHRED_IN_SLOT flag is received.
    #[serde(with = "serde_compat")]
⋮----
/// The slot height of the block this one derives from.
    /// The parent slot of the head of a detached chain of slots is None.
⋮----
/// The parent slot of the head of a detached chain of slots is None.
    #[serde(with = "serde_compat")]
⋮----
/// The list of slots, each of which contains a block that derives
    /// from this one.
⋮----
/// from this one.
    pub next_slots: Vec<Slot>,
/// Connected status flags of this slot
    pub connected_flags: ConnectedFlags,
/// Shreds indices which are marked data complete.  That is, those that have the
    /// [`ShredFlags::DATA_COMPLETE_SHRED`][`crate::shred::ShredFlags::DATA_COMPLETE_SHRED`] set.
⋮----
/// [`ShredFlags::DATA_COMPLETE_SHRED`][`crate::shred::ShredFlags::DATA_COMPLETE_SHRED`] set.
    pub completed_data_indexes: T,
⋮----
pub type SlotMetaV1 = SlotMetaBase<CompletedDataIndexesV1>;
pub type SlotMetaV2 = SlotMetaBase<CompletedDataIndexesV2>;
⋮----
fn from(value: SlotMetaV1) -> Self {
⋮----
completed_data_indexes: value.completed_data_indexes.into(),
⋮----
fn from(value: SlotMetaV2) -> Self {
⋮----
// We need to maintain both formats during migration,
// as both formats will need to be supported when reading
// from rocksdb until the migration is complete.
//
// Swap these types to migrate to the new format.
⋮----
// For example, to enable the new format,
⋮----
// ```
// pub type SlotMeta = SlotMetaV2;
// pub type CompletedDataIndexes = CompletedDataIndexesV2;
// pub type SlotMetaFallback = SlotMetaV1;
⋮----
// To enable the old format,
⋮----
// pub type SlotMeta = SlotMetaV1;
// pub type CompletedDataIndexes = CompletedDataIndexesV1;
// pub type SlotMetaFallback = SlotMetaV2;
⋮----
pub type SlotMeta = SlotMetaV2;
pub type CompletedDataIndexes = CompletedDataIndexesV2;
pub type SlotMetaFallback = SlotMetaV1;
// Serde implementation of serialize and deserialize for Option<u64>
// where None is represented as u64::MAX; for backward compatibility.
mod serde_compat {
⋮----
pub(super) fn serialize<S>(val: &Option<u64>, serializer: S) -> Result<S::Ok, S::Error>
⋮----
val.unwrap_or(u64::MAX).serialize(serializer)
⋮----
pub(super) fn deserialize<'de, D>(deserializer: D) -> Result<Option<u64>, D::Error>
⋮----
Ok((val != u64::MAX).then_some(val))
⋮----
pub type Index = IndexV2;
pub type ShredIndex = ShredIndexV2;
/// We currently support falling back to the previous format for migration purposes.
///
⋮----
///
/// See https://github.com/anza-xyz/agave/issues/3570.
⋮----
/// See https://github.com/anza-xyz/agave/issues/3570.
pub type IndexFallback = IndexV1;
⋮----
pub type IndexFallback = IndexV1;
pub type ShredIndexFallback = ShredIndexV1;
⋮----
/// Index recording presence/absence of shreds
pub struct IndexV1 {
⋮----
pub struct IndexV1 {
⋮----
pub struct IndexV2 {
⋮----
fn from(index: IndexV2) -> Self {
⋮----
data: index.data.into(),
coding: index.coding.into(),
⋮----
fn from(index: IndexV1) -> Self {
⋮----
pub struct ShredIndexV1 {
/// Map representing presence/absence of shreds
    index: BTreeSet<u64>,
⋮----
/// Erasure coding information
pub struct ErasureMeta {
⋮----
pub struct ErasureMeta {
/// Which erasure set in the slot this is
    #[serde(
⋮----
/// First coding index in the FEC set
    first_coding_index: u64,
/// Index of the first received coding shred in the FEC set
    first_received_coding_index: u64,
/// Erasure configuration for this erasure set
    config: ErasureConfig,
⋮----
// Helper module to serde values by type-casting to an intermediate
// type for backward compatibility.
mod serde_compat_cast {
⋮----
// Serializes a value of type T by first type-casting to type R.
pub(super) fn serialize<S: Serializer, R, T: Copy>(
⋮----
.map_err(serde::ser::Error::custom)?
.serialize(serializer)
⋮----
// Deserializes a value of type R and type-casts it to type T.
pub(super) fn deserialize<'de, D, R, T>(deserializer: D) -> Result<T, D::Error>
⋮----
.map(T::try_from)?
.map_err(serde::de::Error::custom)
⋮----
pub(crate) struct ErasureConfig {
⋮----
impl ErasureConfig {
pub(crate) fn is_fixed(&self) -> bool {
⋮----
pub struct MerkleRootMeta {
⋮----
pub struct DuplicateSlotProof {
⋮----
pub enum FrozenHashVersioned {
⋮----
impl FrozenHashVersioned {
pub fn frozen_hash(&self) -> Hash {
⋮----
pub fn is_duplicate_confirmed(&self) -> bool {
⋮----
pub struct FrozenHashStatus {
⋮----
impl Index {
pub(crate) fn new(slot: Slot) -> Self {
⋮----
pub fn data(&self) -> &ShredIndex {
⋮----
pub fn coding(&self) -> &ShredIndex {
⋮----
pub(crate) fn data_mut(&mut self) -> &mut ShredIndex {
⋮----
pub(crate) fn coding_mut(&mut self) -> &mut ShredIndex {
⋮----
impl IndexFallback {
⋮----
pub fn data(&self) -> &ShredIndexFallback {
⋮----
pub fn coding(&self) -> &ShredIndexFallback {
⋮----
pub(crate) fn data_mut(&mut self) -> &mut ShredIndexFallback {
⋮----
pub(crate) fn coding_mut(&mut self) -> &mut ShredIndexFallback {
⋮----
impl ShredIndexV1 {
pub fn num_shreds(&self) -> usize {
self.index.len()
⋮----
pub(crate) fn range<R>(&self, bounds: R) -> impl Iterator<Item = &u64>
⋮----
self.index.range(bounds)
⋮----
pub(crate) fn contains(&self, index: u64) -> bool {
self.index.contains(&index)
⋮----
pub(crate) fn insert(&mut self, index: u64) {
self.index.insert(index);
⋮----
fn remove(&mut self, index: u64) {
self.index.remove(&index);
⋮----
pub struct ShredIndexV2 {
⋮----
impl ShredIndexV2 {
⋮----
if self.index.remove_unchecked(index as usize) {
⋮----
pub(crate) fn contains(&self, idx: u64) -> bool {
self.index.contains(idx as usize)
⋮----
pub(crate) fn insert(&mut self, idx: u64) {
if let Ok(true) = self.index.insert(idx as usize) {
⋮----
pub(crate) fn range<R>(&self, bounds: R) -> impl Iterator<Item = u64> + '_
⋮----
.range((start, end))
.iter_ones()
.map(|idx| idx as u64)
⋮----
fn iter(&self) -> impl Iterator<Item = u64> + '_ {
self.range(0..MAX_DATA_SHREDS_PER_SLOT as u64)
⋮----
fn from_iter<T: IntoIterator<Item = u64>>(iter: T) -> Self {
⋮----
index.insert(idx);
⋮----
index: iter.into_iter().collect(),
⋮----
fn from(value: ShredIndexV1) -> Self {
value.index.into_iter().collect()
⋮----
fn from(value: ShredIndexV2) -> Self {
⋮----
index: value.iter().collect(),
⋮----
impl SlotMeta {
pub fn is_full(&self) -> bool {
⋮----
.map(|ix| self.consumed > ix + 1)
.unwrap_or_default()
⋮----
datapoint_error!(
⋮----
Some(self.consumed) == self.last_index.map(|ix| ix + 1)
⋮----
pub(crate) fn is_orphan(&self) -> bool {
self.parent_slot.is_none()
⋮----
pub fn is_connected(&self) -> bool {
self.connected_flags.contains(ConnectedFlags::CONNECTED)
⋮----
pub fn set_connected(&mut self) {
assert!(self.is_parent_connected());
self.connected_flags.set(ConnectedFlags::CONNECTED, true);
⋮----
pub fn is_parent_connected(&self) -> bool {
⋮----
.contains(ConnectedFlags::PARENT_CONNECTED)
⋮----
pub fn set_parent_connected(&mut self) -> bool {
if self.is_connected() {
⋮----
.set(ConnectedFlags::PARENT_CONNECTED, true);
if self.is_full() {
self.set_connected();
⋮----
self.is_connected()
⋮----
pub fn unset_parent(&mut self) {
⋮----
pub fn clear_unconfirmed_slot(&mut self) {
⋮----
pub(crate) fn new(slot: Slot, parent_slot: Option<Slot>) -> Self {
⋮----
pub(crate) fn new_orphan(slot: Slot) -> Self {
⋮----
impl ErasureMeta {
pub(crate) fn from_coding_shred(shred: &Shred) -> Option<Self> {
match shred.shred_type() {
⋮----
num_data: usize::from(shred.num_data_shreds().ok()?),
num_coding: usize::from(shred.num_coding_shreds().ok()?),
⋮----
let first_coding_index = u64::from(shred.first_coding_index()?);
let first_received_coding_index = u64::from(shred.index());
⋮----
fec_set_index: shred.fec_set_index(),
⋮----
Some(erasure_meta)
⋮----
pub(crate) fn check_coding_shred(&self, shred: &Shred) -> bool {
⋮----
pub fn check_erasure_consistency(shred1: &Shred, shred2: &Shred) -> bool {
⋮----
coding_shred.check_coding_shred(shred2)
⋮----
pub(crate) fn config(&self) -> ErasureConfig {
⋮----
pub(crate) fn data_shreds_indices(&self) -> Range<u64> {
⋮----
pub(crate) fn coding_shreds_indices(&self) -> Range<u64> {
⋮----
pub(crate) fn first_received_coding_shred_index(&self) -> Option<u32> {
u32::try_from(self.first_received_coding_index).ok()
⋮----
pub(crate) fn next_fec_set_index(&self) -> Option<u32> {
let num_data = u32::try_from(self.config.num_data).ok()?;
self.fec_set_index.checked_add(num_data)
⋮----
pub(crate) fn should_recover_shreds(&self, index: &Index) -> bool {
let num_data = index.data().range(self.data_shreds_indices()).count();
⋮----
let num_coding = index.coding().range(self.coding_shreds_indices()).count();
⋮----
pub(crate) fn clear_first_received_coding_shred_index(&mut self) {
⋮----
impl MerkleRootMeta {
pub(crate) fn from_shred(shred: &Shred) -> Self {
⋮----
merkle_root: shred.merkle_root().ok(),
first_received_shred_index: shred.index(),
first_received_shred_type: shred.shred_type(),
⋮----
pub(crate) fn merkle_root(&self) -> Option<Hash> {
⋮----
pub(crate) fn first_received_shred_index(&self) -> u32 {
⋮----
pub(crate) fn first_received_shred_type(&self) -> ShredType {
⋮----
impl DuplicateSlotProof {
pub(crate) fn new<S, T>(shred1: S, shred2: T) -> Self
⋮----
pub struct TransactionStatusIndexMeta {
⋮----
pub struct AddressSignatureMeta {
⋮----
pub enum PerfSample {
⋮----
fn from(value: PerfSampleV1) -> PerfSample {
⋮----
fn from(value: PerfSampleV2) -> PerfSample {
⋮----
pub struct PerfSampleV1 {
⋮----
pub struct PerfSampleV2 {
⋮----
pub struct OptimisticSlotMetaV0 {
⋮----
pub enum OptimisticSlotMetaVersioned {
⋮----
impl OptimisticSlotMetaVersioned {
pub fn new(hash: Hash, timestamp: UnixTimestamp) -> Self {
⋮----
pub fn hash(&self) -> Hash {
⋮----
pub fn timestamp(&self) -> UnixTimestamp {
⋮----
mod test {
⋮----
fn test_slot_meta_slot_zero_connected() {
⋮----
assert!(meta.is_parent_connected());
assert!(!meta.is_connected());
⋮----
fn test_should_recover_shreds() {
⋮----
let mut rng = rng();
⋮----
assert!(!e_meta.should_recover_shreds(&index));
for ix in data_indexes.clone() {
index.data_mut().insert(ix);
⋮----
for ix in coding_indexes.clone() {
index.coding_mut().insert(ix);
⋮----
.clone()
⋮----
.choose_multiple(&mut rng, erasure_config.num_data)
⋮----
index.data_mut().remove(idx);
assert!(e_meta.should_recover_shreds(&index));
⋮----
.choose_multiple(&mut rng, erasure_config.num_coding)
⋮----
index.coding_mut().remove(idx);
⋮----
fn rand_range(range: Range<u64>) -> impl Strategy<Value = Range<u64>> {
(range.clone(), range).prop_map(
⋮----
proptest! {
⋮----
fn test_shred_index_v2_range_bounds() {
⋮----
index.insert(10);
index.insert(20);
index.insert(30);
index.insert(40);
⋮----
(Included(10), Included(30), vec![10, 20, 30]),
(Included(10), Excluded(30), vec![10, 20]),
(Excluded(10), Included(30), vec![20, 30]),
(Excluded(10), Excluded(30), vec![20]),
(Unbounded, Included(20), vec![10, 20]),
(Unbounded, Excluded(20), vec![10]),
(Included(30), Unbounded, vec![30, 40]),
(Excluded(30), Unbounded, vec![40]),
(Unbounded, Unbounded, vec![10, 20, 30, 40]),
⋮----
let result: Vec<_> = index.range((start_bound, end_bound)).collect();
assert_eq!(
⋮----
fn test_shred_index_v2_boundary_conditions() {
⋮----
index.insert(0);
index.insert(7);
index.insert(8);
index.insert(15);
index.insert(MAX_DATA_SHREDS_PER_SLOT as u64 - 1);
index.insert(MAX_DATA_SHREDS_PER_SLOT as u64);
assert!(index.contains(0));
assert!(index.contains(7));
assert!(index.contains(8));
assert!(index.contains(15));
assert!(index.contains(MAX_DATA_SHREDS_PER_SLOT as u64 - 1));
assert!(!index.contains(MAX_DATA_SHREDS_PER_SLOT as u64));
assert_eq!(index.range(6..10).collect::<Vec<_>>(), vec![7, 8]);
assert_eq!(index.range(0..8).collect::<Vec<_>>(), vec![0, 7]);
assert_eq!(index.range(8..16).collect::<Vec<_>>(), vec![8, 15]);
assert_eq!(index.range(0..0).count(), 0);
assert_eq!(index.range(1..1).count(), 0);
let oversized_range = index.range(0..MAX_DATA_SHREDS_PER_SLOT as u64 + 1);
assert_eq!(oversized_range.count(), 5);
assert_eq!(index.num_shreds(), 5);
index.remove(0);
assert!(!index.contains(0));
index.remove(7);
assert!(!index.contains(7));
index.remove(8);
assert!(!index.contains(8));
index.remove(15);
assert!(!index.contains(15));
index.remove(MAX_DATA_SHREDS_PER_SLOT as u64 - 1);
assert!(!index.contains(MAX_DATA_SHREDS_PER_SLOT as u64 - 1));
assert_eq!(index.num_shreds(), 0);
⋮----
fn test_connected_flags_compatibility() {
⋮----
struct WithBool {
⋮----
struct WithFlags {
⋮----
assert_ne!(
⋮----
with_flags.connected.set(ConnectedFlags::CONNECTED, true);
⋮----
assert!(
⋮----
fn test_clear_unconfirmed_slot() {
⋮----
slot_meta.next_slots = vec![6, 7];
slot_meta.clear_unconfirmed_slot();
⋮----
expected.next_slots = vec![6, 7];
assert_eq!(slot_meta, expected);
⋮----
fn perf_sample_v1_is_prefix_of_perf_sample_v2() {
⋮----
let v2_bytes = bincode::serialize(&v2).expect("`PerfSampleV2` can be serialized");
⋮----
.expect("Bytes encoded as `PerfSampleV2` can be decoded as `PerfSampleV1`");
⋮----
assert_eq!(actual, expected);
⋮----
fn test_erasure_meta_transition() {
⋮----
struct OldErasureMeta {
⋮----
fec_set_index: u32::try_from(set_index).unwrap(),
⋮----
old_erasure_meta.__unused_size = usize::try_from(u32::MAX).unwrap();

================
File: ledger/src/blockstore_metric_report_service.rs
================
pub struct BlockstoreMetricReportService {
⋮----
impl BlockstoreMetricReportService {
pub fn new(blockstore: Arc<Blockstore>, exit: Arc<AtomicBool>) -> Self {
⋮----
.name("solRocksCfMtrcs".to_string())
.spawn(move || {
info!("BlockstoreMetricReportService has started");
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
if last_report_time.elapsed() > BLOCKSTORE_METRICS_REPORT_INTERVAL {
blockstore.submit_rocksdb_cf_metrics_for_all_cfs();
⋮----
info!("BlockstoreMetricReportService has stopped");
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_cf_metric.join()

================
File: ledger/src/blockstore_metrics.rs
================
pub struct BlockstoreInsertionMetrics {
⋮----
impl BlockstoreInsertionMetrics {
pub fn report_metrics(&self, metric_name: &'static str) {
datapoint_info!(
⋮----
/// A metrics struct that exposes RocksDB's column family properties.
#[derive(Default)]
pub struct BlockstoreRocksDbColumnFamilyMetrics {
⋮----
impl BlockstoreRocksDbColumnFamilyMetrics {
pub fn report_metrics(&self, cf_name: &'static str, column_options: &LedgerColumnOptions) {
⋮----
// tags that support group-by operations
⋮----
// Size related
⋮----
// Snapshot related
⋮----
// Write related
⋮----
// Memory / block cache related
⋮----
// Flush and compaction
⋮----
// FIFO Compaction related
⋮----
// Misc
⋮----
// Thread local instance of RocksDB's PerfContext.
thread_local! {static PER_THREAD_ROCKS_PERF_CONTEXT: RefCell<PerfContext> = RefCell::new(PerfContext::default());}
⋮----
pub(crate) fn maybe_enable_rocksdb_perf(
⋮----
if perf_status.should_sample(sample_interval) {
set_perf_stats(PerfStatsLevel::EnableTime);
PER_THREAD_ROCKS_PERF_CONTEXT.with(|perf_context| {
perf_context.borrow_mut().reset();
⋮----
return Some(Instant::now());
⋮----
pub(crate) fn report_rocksdb_read_perf(
⋮----
PER_THREAD_ROCKS_PERF_CONTEXT.with(|perf_context_cell| {
set_perf_stats(PerfStatsLevel::Disable);
let perf_context = perf_context_cell.borrow();
⋮----
pub(crate) fn report_rocksdb_write_perf(
⋮----
pub struct PerfSamplingStatus {
⋮----
impl PerfSamplingStatus {
fn should_sample(&self, sample_count_interval: usize) -> bool {
⋮----
if self.op_count.fetch_add(1, Ordering::Relaxed) < sample_count_interval {
⋮----
self.op_count.store(0, Ordering::Relaxed);
let current_time_ms = timestamp();
let old_time_ms = self.last_sample_time_ms.load(Ordering::Relaxed);
if old_time_ms + (PERF_SAMPLING_MIN_DURATION.as_millis() as u64) > current_time_ms {
⋮----
.compare_exchange_weak(
⋮----
.is_ok()

================
File: ledger/src/blockstore_options.rs
================
pub struct BlockstoreOptions {
⋮----
impl Default for BlockstoreOptions {
fn default() -> Self {
⋮----
num_rocksdb_compaction_threads: default_num_compaction_threads(),
num_rocksdb_flush_threads: default_num_flush_threads(),
⋮----
impl BlockstoreOptions {
pub fn default_for_tests() -> Self {
⋮----
pub enum AccessType {
⋮----
pub enum BlockstoreRecoveryMode {
⋮----
fn from(string: &str) -> Self {
⋮----
bad_mode => panic!("Invalid recovery mode: {bad_mode}"),
⋮----
fn from(brm: BlockstoreRecoveryMode) -> Self {
⋮----
pub struct LedgerColumnOptions {
⋮----
impl LedgerColumnOptions {
pub fn get_compression_type_string(&self) -> &'static str {
⋮----
pub enum BlockstoreCompressionType {
⋮----
impl BlockstoreCompressionType {
pub(crate) fn to_rocksdb_compression_type(&self) -> RocksCompressionType {

================
File: ledger/src/blockstore_processor.rs
================
pub struct TransactionBatchWithIndexes<'a, 'b, Tx: SVMMessage> {
⋮----
pub struct LockedTransactionsWithIndexes<Tx: SVMMessage> {
⋮----
struct ReplayEntry {
⋮----
fn first_err(results: &[Result<()>]) -> Result<()> {
⋮----
if r.is_err() {
return r.clone();
⋮----
Ok(())
⋮----
fn do_get_first_error<T, Tx: SVMTransaction>(
⋮----
for (result, transaction) in results.iter().zip(batch.sanitized_transactions()) {
⋮----
if first_err.is_none() {
first_err = Some((Err(err.clone()), *transaction.signature()));
⋮----
warn!("Unexpected validator error: {err:?}, transaction: {transaction:?}");
datapoint_error!(
⋮----
fn get_first_error<T, Tx: SVMTransaction>(
⋮----
do_get_first_error(batch, commit_results)
.map(|(error, _signature)| error)
.unwrap_or(Ok(()))
⋮----
fn create_thread_pool(num_threads: usize) -> ThreadPool {
⋮----
.num_threads(num_threads)
.thread_name(|i| format!("solReplayTx{i:02}"))
.build()
.expect("new rayon threadpool")
⋮----
pub fn execute_batch<'a>(
⋮----
// extra_pre_commit_callback allows for reuse of this function between the
// unified scheduler block production path and block verification path(s)
//   Some(_) => unified scheduler block production path
//   None    => block verification path(s)
let block_verification = extra_pre_commit_callback.is_none();
let record_transaction_meta = transaction_status_sender.is_some();
⋮----
// We're entering into one of the block-verification methods.
get_first_error(batch, processing_results)?;
Ok(None)
⋮----
panic!("unexpected result count: {}", processing_results.len());
⋮----
assert!(transaction_indexes.is_empty());
let freeze_lock = bank.freeze_lock();
let committed_index = extra_pre_commit_callback(result)?;
⋮----
let transaction_indexes = transaction_indexes.to_mut();
transaction_indexes.reserve_exact(1);
transaction_indexes.push(index);
⋮----
Ok(Some(freeze_lock))
⋮----
.bank()
.load_execute_and_commit_transactions_with_pre_commit_callback(
⋮----
ExecutionRecordingConfig::new_single_setting(transaction_status_sender.is_some()),
⋮----
let tx_costs = get_transaction_costs(bank, &commit_results, batch.sanitized_transactions());
check_block_cost_limits(bank, &tx_costs).map(|_| tx_costs)
⋮----
Ok(get_transaction_costs(
⋮----
batch.sanitized_transactions(),
⋮----
Ok(vec![])
⋮----
check_block_costs_elapsed.stop();
timings.saturating_add_in_place(
⋮----
check_block_costs_elapsed.as_us(),
⋮----
.iter()
.zip(batch.sanitized_transactions())
.filter_map(|(commit_result, tx)| commit_result.was_committed().then_some(tx));
prioritization_fee_cache.update(bank, committed_transactions);
⋮----
.sanitized_transactions()
⋮----
.map(|tx| tx.as_sanitized_transaction().into_owned())
.collect();
debug_assert!(balance_collector.is_some());
⋮----
compile_collected_balances(balance_collector.unwrap_or_default());
⋮----
.into_iter()
.map(|tx_cost_option| tx_cost_option.map(|tx_cost| tx_cost.sum()).or(Some(0)))
⋮----
transaction_status_sender.send_transaction_status_batch(
bank.slot(),
⋮----
transaction_indexes.into_owned(),
⋮----
fn get_transaction_costs<'a, Tx: TransactionWithMeta>(
⋮----
assert_eq!(sanitized_transactions.len(), commit_results.len());
⋮----
.zip(sanitized_transactions)
.map(|(commit_result, tx)| {
⋮----
Some(CostModel::calculate_cost_for_executed_transaction(
⋮----
.collect()
⋮----
fn check_block_cost_limits<Tx: TransactionWithMeta>(
⋮----
let mut cost_tracker = bank.write_cost_tracker().unwrap();
for tx_cost in tx_costs.iter().flatten() {
⋮----
.try_add(tx_cost)
.map_err(TransactionError::from)?;
⋮----
pub struct ExecuteBatchesInternalMetrics {
⋮----
impl ExecuteBatchesInternalMetrics {
pub fn new_with_timings_from_all_threads(execute_timings: ExecuteTimings) -> Self {
⋮----
new.execution_timings_per_thread.insert(
⋮----
fn execute_batches_internal(
⋮----
assert!(!batches.is_empty());
⋮----
let results: Vec<Result<()>> = replay_tx_thread_pool.install(|| {
⋮----
.into_par_iter()
.map(|transaction_batch| {
⋮----
transaction_batch.batch.sanitized_transactions().len() as u64;
⋮----
let (result, execute_batches_us) = measure_us!(execute_batch(
⋮----
let thread_index = replay_tx_thread_pool.current_thread_index().unwrap();
⋮----
.lock()
.unwrap()
.entry(thread_index)
.and_modify(|thread_execution_time| {
⋮----
.saturating_add_in_place(ExecuteTimingType::TotalBatchesLen, 1);
total_thread_execute_timings.accumulate(&timings);
⋮----
.or_insert(ThreadExecuteTimings {
total_thread_us: Saturating(execute_batches_us),
total_transactions_executed: Saturating(transaction_count),
⋮----
execute_batches_elapsed.stop();
first_err(&results)?;
Ok(ExecuteBatchesInternalMetrics {
execution_timings_per_thread: execution_timings_per_thread.into_inner().unwrap(),
total_batches_len: batches.len() as u64,
execute_batches_us: execute_batches_elapsed.as_us(),
⋮----
fn process_batches(
⋮----
if bank.has_installed_scheduler() {
debug!(
⋮----
schedule_batches_for_execution(bank, locked_entries)
⋮----
execute_batches(
⋮----
fn schedule_batches_for_execution(
⋮----
let mut first_err = Ok(());
⋮----
bank.unlock_accounts(transactions.iter().zip(lock_results.iter()));
let indexes = starting_index..starting_index + transactions.len();
let task_ids = indexes.map(|i| i.try_into().unwrap());
first_err = first_err.and_then(|()| {
bank.schedule_transaction_executions(transactions.into_iter().zip_eq(task_ids))
⋮----
fn execute_batches(
⋮----
if locked_entries.len() == 0 {
return Ok(());
⋮----
.map(
⋮----
let ending_index = starting_index + transactions.len();
⋮----
transaction_indexes: (starting_index..ending_index).collect(),
⋮----
let execute_batches_internal_metrics = execute_batches_internal(
⋮----
timing.accumulate(execute_batches_internal_metrics, false);
⋮----
pub fn process_entries_for_tests(
⋮----
let replay_tx_thread_pool = create_thread_pool(1);
⋮----
let bank = bank.clone_with_scheduler();
⋮----
bank.verify_transaction(versioned_tx, TransactionVerificationMode::FullVerification)
⋮----
let mut entry_starting_index: usize = bank.transaction_count().try_into().unwrap();
⋮----
.map(|entry| {
⋮----
entry_starting_index = entry_starting_index.saturating_add(transactions.len());
⋮----
let result = process_entries(
⋮----
debug!("process_entries: {batch_timing:?}");
⋮----
fn process_entries(
⋮----
let mut batches = vec![];
let mut tick_hashes = vec![];
⋮----
tick_hashes.push(hash);
if bank.is_block_boundary(bank.tick_height() + tick_hashes.len() as u64) {
process_batches(
⋮----
batches.drain(..),
⋮----
for hash in tick_hashes.drain(..) {
bank.register_tick(&hash);
⋮----
queue_batches_with_lock_retry(
⋮----
batches.into_iter(),
⋮----
fn queue_batches_with_lock_retry(
⋮----
let lock_results = bank.try_lock_accounts(&transactions, false);
let first_lock_err = first_err(&lock_results);
if first_lock_err.is_ok() {
batches.push(LockedTransactionsWithIndexes {
⋮----
process_batches(batches.drain(..))?;
⋮----
match first_err(&lock_results) {
⋮----
Err(err)
⋮----
pub enum BlockstoreProcessorError {
⋮----
pub type ProcessSlotCallback = Arc<dyn Fn(&Bank) + Sync + Send>;
⋮----
pub struct ProcessOptions {
⋮----
pub fn test_process_blockstore(
⋮----
exit.clone(),
⋮----
.unwrap();
process_blockstore_from_root(
⋮----
pub(crate) fn process_blockstore_for_bank_0(
⋮----
Arc::new(opts.runtime_config.clone()),
⋮----
opts.debug_keys.clone(),
opts.accounts_db_config.clone(),
⋮----
let bank0_slot = bank0.slot();
⋮----
info!("Processing ledger for slot 0...");
let replay_tx_thread_pool = create_thread_pool(num_cpus::get());
process_bank_0(
⋮----
.read()
⋮----
.get_with_scheduler(bank0_slot)
.unwrap(),
⋮----
Ok(bank_forks)
⋮----
pub fn process_blockstore_from_root(
⋮----
assert_eq!(bank_forks.read().unwrap().banks().len(), 1);
let bank = bank_forks.read().unwrap().root_bank();
⋮----
info!("Will override following slots' hashes: {hash_overrides:#?}");
bank.set_hash_overrides(hash_overrides.clone());
⋮----
warn!("setting block cost limits to MAX");
bank.write_cost_tracker()
⋮----
.set_limits(u64::MAX, u64::MAX, u64::MAX);
⋮----
assert!(bank.parent().is_none());
(bank.slot(), bank.hash())
⋮----
info!("Processing ledger from slot {start_slot}...");
⋮----
if blockstore.is_primary_access() {
⋮----
.mark_slots_as_if_rooted_normally_at_startup(
vec![(start_slot, Some(start_slot_hash))],
⋮----
.expect("Couldn't mark start_slot as root in startup");
⋮----
.set_and_chain_connected_on_root_and_next_slots(start_slot)
.expect("Couldn't mark start_slot as connected during startup")
⋮----
info!(
⋮----
if let Ok(Some(highest_slot)) = blockstore.highest_slot() {
info!("ledger holds data through slot {highest_slot}");
⋮----
.meta(start_slot)
.unwrap_or_else(|_| panic!("Failed to get meta for slot {start_slot}"))
⋮----
load_frozen_forks(
⋮----
warn!("Starting slot {start_slot} is not in Blockstore, unable to process");
⋮----
let processing_time = now.elapsed();
let num_frozen_banks = bank_forks.read().unwrap().frozen_banks().count();
datapoint_info!(
⋮----
info!("ledger processing timing: {timing:?}");
⋮----
let bank_forks = bank_forks.read().unwrap();
let mut bank_slots = bank_forks.banks().keys().copied().collect::<Vec<_>>();
bank_slots.sort_unstable();
⋮----
assert!(bank_forks.active_bank_slots().is_empty());
⋮----
/// Verify that a segment of entries has the correct number of ticks and hashes
fn verify_ticks(
⋮----
fn verify_ticks(
⋮----
let next_bank_tick_height = bank.tick_height() + entries.tick_count();
let max_bank_tick_height = bank.max_tick_height();
⋮----
warn!("Too many entry ticks found in slot: {}", bank.slot());
return Err(BlockError::TooManyTicks);
⋮----
info!("Too few entry ticks found in slot: {}", bank.slot());
return Err(BlockError::TooFewTicks);
⋮----
let has_trailing_entry = entries.last().map(|e| !e.is_tick()).unwrap_or_default();
⋮----
warn!("Slot: {} did not end with a tick entry", bank.slot());
return Err(BlockError::TrailingEntry);
⋮----
warn!("Slot: {} was not marked full", bank.slot());
return Err(BlockError::InvalidLastTick);
⋮----
.activated_slot(&agave_feature_set::alpenglow::id())
⋮----
if bank.parent_slot() >= first_alpenglow_slot {
⋮----
if bank.slot() >= first_alpenglow_slot && next_bank_tick_height == max_bank_tick_height {
if entries.is_empty() {
error!("Processing empty entries in verify_ticks()");
⋮----
entries = &entries[..entries.len() - 1];
⋮----
let hashes_per_tick = bank.hashes_per_tick().unwrap_or(0);
if !entries.verify_tick_hash_count(tick_hash_count, hashes_per_tick) {
warn!(
⋮----
return Err(BlockError::InvalidTickHashCount);
⋮----
fn confirm_full_slot(
⋮----
confirm_slot(
⋮----
timing.accumulate(&confirmation_timing.batch_execute.totals);
if !bank.is_complete() {
Err(BlockstoreProcessorError::InvalidBlock(
⋮----
pub struct ConfirmationTiming {
⋮----
impl Default for ConfirmationTiming {
fn default() -> Self {
⋮----
pub struct BatchExecutionTiming {
⋮----
impl BatchExecutionTiming {
pub fn accumulate(
⋮----
totals.saturating_add_in_place(TotalBatchesLen, new_batch.total_batches_len);
totals.saturating_add_in_place(NumExecuteBatches, 1);
⋮----
for thread_times in new_batch.execution_timings_per_thread.values() {
totals.accumulate(&thread_times.execute_timings);
⋮----
.values()
.max_by_key(|thread_times| thread_times.total_thread_us);
⋮----
slowest_thread.accumulate(slowest);
⋮----
.saturating_add_in_place(NumExecuteBatches, 1);
⋮----
pub struct ThreadExecuteTimings {
⋮----
impl ThreadExecuteTimings {
pub fn report_stats(&self, slot: Slot) {
lazy! {
⋮----
pub fn accumulate(&mut self, other: &ThreadExecuteTimings) {
self.execute_timings.accumulate(&other.execute_timings);
⋮----
pub struct ReplaySlotStats(ConfirmationTiming);
⋮----
type Target = ConfirmationTiming;
fn deref(&self) -> &Self::Target {
⋮----
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
impl ReplaySlotStats {
pub fn report_stats(
⋮----
Some(self.batch_execute.wall_clock_us.0 as i64)
⋮----
self.batch_execute.slowest_thread.report_stats(slot);
⋮----
per_pubkey_timings.sort_by(|a, b| b.1.accumulated_us.cmp(&a.1.accumulated_us));
⋮----
per_pubkey_timings.iter().fold(
⋮----
sum_errored_count + a.1.errored_txs_compute_consumed.len(),
⋮----
for (pubkey, time) in per_pubkey_timings.iter().take(5) {
datapoint_trace!(
⋮----
pub struct ConfirmationProgress {
⋮----
impl ConfirmationProgress {
pub fn new(last_entry: Hash) -> Self {
⋮----
pub fn confirm_slot(
⋮----
let slot = bank.slot();
⋮----
.get_slot_entries_with_shred_info(slot, progress.num_shreds, allow_dead_slots)
.map_err(BlockstoreProcessorError::FailedToLoadEntries);
load_elapsed.stop();
if load_result.is_err() {
timing.fetch_fail_elapsed += load_elapsed.as_us();
⋮----
timing.fetch_elapsed += load_elapsed.as_us();
⋮----
confirm_slot_entries(
⋮----
fn confirm_slot_entries(
⋮----
defer! {
⋮----
let num_entries = entries.len();
⋮----
.enumerate()
.map(|(i, entry)| {
⋮----
let entry_index = progress.num_entries.saturating_add(i);
if let Err(err) = entry_notification_sender.send(EntryNotification {
⋮----
entry: entry.into(),
⋮----
let num_txs = entry.transactions.len();
let next_tx_starting_index = entry_tx_starting_index.saturating_add(num_txs);
entry_tx_starting_indexes.push(entry_tx_starting_index);
⋮----
trace!(
⋮----
verify_ticks(bank, &entries, slot_full, tick_hash_count).map_err(|err| {
⋮----
let last_entry_hash = entries.last().map(|e| e.hash);
⋮----
datapoint_debug!("verify-batch-size", ("size", num_entries as i64, i64));
let entry_state = entries.verify(&progress.last_entry, replay_tx_thread_pool);
*poh_verify_elapsed += entry_state.poh_duration_us();
if !entry_state.status() {
warn!("Ledger proof of history failed at slot: {slot}");
return Err(BlockError::InvalidEntryHash.into());
⋮----
bank.verify_transaction(versioned_tx, verification_mode)
⋮----
let transaction_cpu_duration_us = transaction_verification_start.elapsed().as_micros() as u64;
⋮----
return Err(err.into());
⋮----
.entries()
.expect("Transaction verification generates entries");
⋮----
.zip(entry_tx_starting_indexes)
.map(|(entry, tx_starting_index)| ReplayEntry {
⋮----
let process_result = process_entries(
⋮----
.map_err(BlockstoreProcessorError::from);
replay_timer.stop();
*replay_elapsed += replay_timer.as_us();
⋮----
let valid = transaction_verification_result.status();
⋮----
return Err(TransactionError::SignatureFailure.into());
⋮----
fn process_bank_0(
⋮----
assert_eq!(bank0.slot(), 0);
let mut progress = ConfirmationProgress::new(bank0.last_blockhash());
confirm_full_slot(
⋮----
.map_err(|_| BlockstoreProcessorError::FailedToReplayBank0)?;
if let Some((result, _timings)) = bank0.wait_for_completed_scheduler() {
result.unwrap();
⋮----
bank0.freeze();
⋮----
blockstore.insert_bank_hash(bank0.slot(), bank0.hash(), false);
⋮----
transaction_status_sender.send_transaction_status_freeze_message(bank0);
⋮----
fn process_next_slots(
⋮----
if meta.next_slots.is_empty() {
⋮----
.is_some_and(|halt_at_slot| *next_slot > halt_at_slot)
⋮----
if !opts.allow_dead_slots && blockstore.is_dead(*next_slot) {
⋮----
.meta(*next_slot)
.map_err(|err| {
warn!("Failed to load meta for slot {next_slot}: {err:?}");
⋮----
if next_meta.is_full() {
⋮----
bank.clone(),
⋮----
.slot_leader_at(*next_slot, Some(bank))
⋮----
set_alpenglow_ticks(&next_bank);
⋮----
pending_slots.push((next_meta, next_bank, bank.last_blockhash()));
⋮----
pending_slots.sort_by(|a, b| b.1.slot().cmp(&a.1.slot()));
⋮----
pub fn set_alpenglow_ticks(bank: &Bank) {
⋮----
let Some(alpenglow_ticks) = calculate_alpenglow_ticks(
⋮----
bank.parent_slot(),
bank.ticks_per_slot(),
⋮----
bank.set_tick_height(bank.max_tick_height() - alpenglow_ticks);
⋮----
fn calculate_alpenglow_ticks(
⋮----
Some(alpenglow_ticks)
⋮----
fn load_frozen_forks(
⋮----
let blockstore_max_root = blockstore.max_root();
let mut root = bank_forks.read().unwrap().root();
⋮----
let mut pending_slots = vec![];
process_next_slots(
⋮----
.get(start_slot_meta.slot)
⋮----
if Some(bank_forks.read().unwrap().root()) != opts.halt_at_slot {
⋮----
while !pending_slots.is_empty() {
timing.details.per_program_timings.clear();
let (meta, bank, last_entry_hash) = pending_slots.pop().unwrap();
⋮----
if last_status_report.elapsed() > STATUS_REPORT_INTERVAL {
let secs = last_status_report.elapsed().as_secs() as f32;
⋮----
let bank = bank_forks.write().unwrap().insert_from_ledger(bank);
if let Err(error) = process_single_slot(
⋮----
assert!(bank_forks.write().unwrap().remove(bank.slot()).is_some());
⋮----
Err(error)?
⋮----
assert!(bank.is_frozen());
all_banks.insert(bank.slot(), bank.clone_with_scheduler());
m.stop();
process_single_slot_us += m.as_us();
⋮----
if bank_forks.read().unwrap().root() >= max_root {
supermajority_root_from_vote_accounts(
bank.total_epoch_stake(),
&bank.vote_accounts(),
).and_then(|supermajority_root| {
⋮----
let cluster_root_bank = all_banks.get(&supermajority_root).unwrap();
assert!(cluster_root_bank.ancestors.contains_key(&root));
⋮----
let mut rooted_slots = vec![];
let mut new_root_bank = cluster_root_bank.clone_without_scheduler();
⋮----
if new_root_bank.slot() == root { break; }
assert!(new_root_bank.slot() > root);
rooted_slots.push((new_root_bank.slot(), Some(new_root_bank.hash())));
new_root_bank = new_root_bank.parent().unwrap();
⋮----
total_rooted_slots += rooted_slots.len();
⋮----
.mark_slots_as_if_rooted_normally_at_startup(rooted_slots, true)
.expect("Blockstore::mark_slots_as_if_rooted_normally_at_startup() should succeed");
⋮----
Some(cluster_root_bank)
⋮----
} else if blockstore.is_root(slot) {
Some(&bank)
⋮----
voting_us += m.as_us();
⋮----
root = new_root_bank.slot();
leader_schedule_cache.set_root(new_root_bank);
new_root_bank.prune_program_cache(root, new_root_bank.epoch());
⋮----
.write()
⋮----
.set_root(root, snapshot_controller, None);
⋮----
set_root_us += m.as_us();
⋮----
.retain(|(_, pending_bank, _)| pending_bank.ancestors.contains_key(&root));
all_banks.retain(|_, bank| bank.ancestors.contains_key(&root));
⋮----
root_retain_us += m.as_us();
⋮----
.map(|halt_at_slot| slot >= halt_at_slot)
.unwrap_or(false);
⋮----
bank.run_final_hash_calc();
⋮----
bank_forks.read().unwrap().root_bank().run_final_hash_calc();
⋮----
Ok((total_slots_processed, total_rooted_slots))
⋮----
// `roots` is sorted largest to smallest by root slot
fn supermajority_root(roots: &[(Slot, u64)], total_epoch_stake: u64) -> Option<Slot> {
if roots.is_empty() {
⋮----
// Find latest root
⋮----
for (root, stake) in roots.iter() {
assert!(*root <= prev_root);
⋮----
return Some(*root);
⋮----
fn supermajority_root_from_vote_accounts(
⋮----
.filter_map(|(stake, account)| {
⋮----
Some((account.vote_state_view().root_slot()?, *stake))
⋮----
// Sort from greatest to smallest slot
roots_stakes.sort_unstable_by(|a, b| a.0.cmp(&b.0).reverse());
⋮----
supermajority_root(&roots_stakes, total_epoch_stake)
⋮----
// Processes and replays the contents of a single slot, returns Error
// if failed to play the slot
⋮----
pub fn process_single_slot(
⋮----
// Mark corrupt slots as dead so validators don't replay this slot and
// see AlreadyProcessed errors later in ReplayStage
⋮----
.and_then(|()| {
if let Some((result, completed_timings)) = bank.wait_for_completed_scheduler() {
timing.accumulate(&completed_timings);
⋮----
warn!("slot {slot} failed to verify: {err}");
⋮----
.set_dead_slot(slot)
.expect("Failed to mark slot as dead in blockstore");
⋮----
if let Some((result, _timings)) = bank.wait_for_completed_scheduler() {
⋮----
.check_last_fec_set_and_get_block_id(slot, bank.hash(), &bank.feature_set)
.inspect_err(|err| {
warn!("slot {slot} failed last fec set checks: {err}");
⋮----
bank.set_block_id(block_id);
bank.freeze();
⋮----
slot_callback(bank);
⋮----
blockstore.insert_bank_hash(bank.slot(), bank.hash(), false);
⋮----
transaction_status_sender.send_transaction_status_freeze_message(bank);
⋮----
type WorkSequence = u64;
⋮----
pub enum TransactionStatusMessage {
⋮----
pub struct TransactionStatusBatch {
⋮----
pub struct TransactionStatusSender {
⋮----
impl TransactionStatusSender {
pub fn send_transaction_status_batch(
⋮----
.as_ref()
.map(|dependency_tracker| dependency_tracker.declare_work());
if let Err(e) = self.sender.send(TransactionStatusMessage::Batch((
⋮----
trace!("Slot {slot} transaction_status send batch failed: {e:?}");
⋮----
pub fn send_transaction_status_freeze_message(&self, bank: &Arc<Bank>) {
⋮----
.send(TransactionStatusMessage::Freeze(bank.clone()))
⋮----
warn!("Slot {slot} transaction_status send freeze message failed: {e:?}");
⋮----
pub fn fill_blockstore_slot_with_ticks(
⋮----
assert!(slot.saturating_sub(1) >= parent_slot);
let num_slots = (slot - parent_slot).max(1);
let entries = create_ticks(num_slots * ticks_per_slot, 0, last_entry_hash);
let last_entry_hash = entries.last().unwrap().hash;
⋮----
.write_entries(
⋮----
Some(parent_slot),
⋮----
pub mod tests {
⋮----
fn test_process_blockstore_with_custom_options(
⋮----
test_process_blockstore(genesis_config, blockstore, opts, Arc::default())
⋮----
blockstore.ledger_path(),
⋮----
.expect("Unable to open access to blockstore");
test_process_blockstore(genesis_config, &read_only_blockstore, opts, Arc::default())
⋮----
fn process_entries_for_tests_without_scheduler(
⋮----
process_entries_for_tests(
&BankWithScheduler::new_without_scheduler(bank.clone()),
⋮----
fn test_process_blockstore_with_missing_hashes() {
do_test_process_blockstore_with_missing_hashes(AccessType::Primary);
⋮----
fn test_process_blockstore_with_missing_hashes_read_only_access() {
do_test_process_blockstore_with_missing_hashes(AccessType::ReadOnly);
⋮----
fn do_test_process_blockstore_with_missing_hashes(blockstore_access_type: AccessType) {
⋮----
} = create_genesis_config(10_000);
genesis_config.poh_config.hashes_per_tick = Some(hashes_per_tick);
⋮----
let (ledger_path, blockhash) = create_new_tmp_ledger_auto_delete!(&genesis_config);
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
let entries = create_ticks(ticks_per_slot, hashes_per_tick - 1, blockhash);
assert_matches!(
⋮----
let (bank_forks, ..) = test_process_blockstore_with_custom_options(
⋮----
blockstore_access_type.clone(),
⋮----
assert_eq!(frozen_bank_slots(&bank_forks.read().unwrap()), vec![0]);
let dead_slots: Vec<Slot> = blockstore.dead_slots_iterator(0).unwrap().collect();
⋮----
assert_eq!(dead_slots.len(), 0);
⋮----
assert_eq!(&dead_slots, &[1]);
⋮----
fn test_process_blockstore_with_invalid_slot_tick_count() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let entries = create_ticks(ticks_per_slot - 1, 0, blockhash);
⋮----
let (bank_forks, ..) = test_process_blockstore(
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 2, 0, blockhash);
⋮----
assert_eq!(frozen_bank_slots(&bank_forks.read().unwrap()), vec![0, 2]);
assert_eq!(bank_forks.read().unwrap().working_bank().slot(), 2);
assert_eq!(bank_forks.read().unwrap().root(), 0);
⋮----
fn test_process_blockstore_with_slot_with_trailing_entry() {
⋮----
let mut entries = create_ticks(ticks_per_slot, 0, blockhash);
⋮----
let tx = system_transaction::transfer(&mint_keypair, &keypair.pubkey(), 1, blockhash);
next_entry(&blockhash, 1, vec![tx])
⋮----
entries.push(trailing_entry);
⋮----
test_process_blockstore(&genesis_config, &blockstore, &opts, Arc::default());
⋮----
fn test_process_blockstore_with_incomplete_slot() {
⋮----
let (ledger_path, mut blockhash) = create_new_tmp_ledger_auto_delete!(&genesis_config);
debug!("ledger_path: {ledger_path:?}");
⋮----
blockhash = entries.last().unwrap().hash;
entries.pop();
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 2, 1, blockhash);
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 3, 0, blockhash);
⋮----
assert_eq!(frozen_bank_slots(&bank_forks.read().unwrap()), vec![0, 3]);
⋮----
fn test_process_blockstore_with_two_forks_and_squash() {
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 1, 0, last_entry_hash);
last_entry_hash = fill_blockstore_slot_with_ticks(
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 3, 2, last_entry_hash);
let last_fork2_entry_hash = fill_blockstore_slot_with_ticks(
⋮----
info!("last_fork1_entry.hash: {last_fork1_entry_hash:?}");
info!("last_fork2_entry.hash: {last_fork2_entry_hash:?}");
blockstore.set_roots([0, 1, 4].iter()).unwrap();
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![4]);
assert!(&bank_forks[4]
⋮----
verify_fork_infos(&bank_forks);
assert_eq!(bank_forks.root(), 4);
⋮----
fn test_process_blockstore_with_two_forks() {
⋮----
blockstore.set_roots([0, 1].iter()).unwrap();
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![1, 2, 3, 4]);
assert_eq!(bank_forks.working_bank().slot(), 4);
assert_eq!(bank_forks.root(), 1);
assert_eq!(
⋮----
fn test_process_blockstore_with_dead_slot() {
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 1, 0, blockhash);
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 2, 1, slot1_blockhash);
blockstore.set_dead_slot(2).unwrap();
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 3, 1, slot1_blockhash);
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![0, 1, 3]);
assert_eq!(bank_forks.working_bank().slot(), 3);
⋮----
fn test_process_blockstore_with_dead_child() {
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 4, 2, slot2_blockhash);
blockstore.set_dead_slot(4).unwrap();
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![0, 1, 2, 3]);
⋮----
fn test_root_with_all_dead_children() {
⋮----
blockstore.set_dead_slot(1).unwrap();
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![0]);
⋮----
fn test_process_blockstore_epoch_boundary_root() {
⋮----
let epoch_schedule = get_epoch_schedule(&genesis_config);
let last_slot = epoch_schedule.get_last_slot_in_epoch(1);
⋮----
let rooted_slots: Vec<Slot> = (0..=last_slot).collect();
blockstore.set_roots(rooted_slots.iter()).unwrap();
⋮----
.set_roots(std::iter::once(&(last_slot + 1)))
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![last_slot + 1]);
assert!(&bank_forks[last_slot + 1]
⋮----
fn test_first_err() {
assert_eq!(first_err(&[Ok(())]), Ok(()));
⋮----
fn test_process_empty_entry_is_registered() {
⋮----
} = create_genesis_config(2);
⋮----
let slot_entries = create_ticks(genesis_config.ticks_per_slot, 1, genesis_config.hash());
⋮----
&keypair.pubkey(),
⋮----
slot_entries.last().unwrap().hash,
⋮----
process_entries_for_tests_without_scheduler(&bank, slot_entries).unwrap();
assert_eq!(bank.process_transaction(&tx), Ok(()));
⋮----
fn test_process_ledger_simple() {
⋮----
} = create_genesis_config_with_leader(mint, &leader_pubkey, 50);
⋮----
create_new_tmp_ledger_auto_delete!(&genesis_config);
⋮----
let mut entries = vec![];
let blockhash = genesis_config.hash();
⋮----
let entry = next_entry_mut(&mut last_entry_hash, 1, vec![tx]);
entries.push(entry);
⋮----
system_transaction::transfer(&mint_keypair, &keypair2.pubkey(), 101, blockhash);
⋮----
let remaining_hashes = hashes_per_tick - entries.len() as u64;
let tick_entry = next_entry_mut(&mut last_entry_hash, remaining_hashes, vec![]);
entries.push(tick_entry);
entries.extend(create_ticks(
⋮----
genesis_config.poh_config.hashes_per_tick.unwrap(),
⋮----
let last_blockhash = entries.last().unwrap().hash;
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![0, 1]);
assert_eq!(bank_forks.root(), 0);
assert_eq!(bank_forks.working_bank().slot(), 1);
let bank = bank_forks[1].clone();
⋮----
assert_eq!(bank.tick_height(), 2 * genesis_config.ticks_per_slot);
assert_eq!(bank.last_blockhash(), last_blockhash);
⋮----
fn test_process_ledger_with_one_tick_per_slot() {
⋮----
} = create_genesis_config(123);
⋮----
let (ledger_path, _blockhash) = create_new_tmp_ledger_auto_delete!(&genesis_config);
⋮----
let bank = bank_forks[0].clone();
assert_eq!(bank.tick_height(), 1);
⋮----
fn test_process_ledger_options_full_leader_cache() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(123);
⋮----
assert_eq!(leader_schedule.max_schedules(), usize::MAX);
⋮----
fn test_process_entries_tick() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(1000);
⋮----
assert_eq!(bank.tick_height(), 0);
let tick = next_entry(&genesis_config.hash(), 1, vec![]);
⋮----
fn test_process_entries_2_entries_collision() {
⋮----
} = create_genesis_config(1000);
⋮----
let blockhash = bank.last_blockhash();
⋮----
&keypair1.pubkey(),
⋮----
bank.last_blockhash(),
⋮----
let entry_1 = next_entry(&blockhash, 1, vec![tx]);
⋮----
&keypair2.pubkey(),
⋮----
let entry_2 = next_entry(&entry_1.hash, 1, vec![tx]);
⋮----
assert_eq!(bank.get_balance(&keypair1.pubkey()), 2);
assert_eq!(bank.get_balance(&keypair2.pubkey()), 2);
assert_eq!(bank.last_blockhash(), blockhash);
⋮----
fn test_process_entries_2_txes_collision() {
⋮----
assert_matches!(bank.transfer(4, &mint_keypair, &keypair1.pubkey()), Ok(_));
assert_matches!(bank.transfer(4, &mint_keypair, &keypair2.pubkey()), Ok(_));
let entry_1_to_mint = next_entry(
&bank.last_blockhash(),
⋮----
vec![system_transaction::transfer(
⋮----
let entry_2_to_3_mint_to_1 = next_entry(
⋮----
vec![
⋮----
assert_eq!(bank.get_balance(&keypair1.pubkey()), 1);
⋮----
assert_eq!(bank.get_balance(&keypair3.pubkey()), 2);
⋮----
fn test_process_entries_2_txes_collision_and_error() {
⋮----
assert_matches!(bank.transfer(4, &mint_keypair, &keypair4.pubkey()), Ok(_));
⋮----
&mint_keypair.pubkey(),
⋮----
assert_eq!(bank.get_balance(&keypair1.pubkey()), 4);
assert_eq!(bank.get_balance(&keypair2.pubkey()), 4);
⋮----
let batch1 = bank.prepare_entry_batch(txs1).unwrap();
for result in batch1.lock_results() {
assert!(result.is_ok());
⋮----
drop(batch1);
let batch2 = bank.prepare_entry_batch(txs2).unwrap();
for result in batch2.lock_results() {
⋮----
drop(batch2);
let entry_3 = next_entry(&entry_2_to_3_mint_to_1.hash, 1, vec![good_tx]);
⋮----
assert_eq!(bank.get_balance(&keypair1.pubkey()), 3);
⋮----
fn test_transaction_result_does_not_affect_bankhash() {
⋮----
fn get_instruction_errors() -> Vec<InstructionError> {
⋮----
declare_process_instruction!(MockBuiltinOk, 1, |_invoke_context| {
⋮----
Some(&mint_keypair.pubkey()),
⋮----
let entry = next_entry(&bank.last_blockhash(), 1, vec![tx]);
let result = process_entries_for_tests_without_scheduler(&bank, vec![entry]);
⋮----
let ok_bank_details = SlotDetails::new_from_bank(&bank, true).unwrap();
⋮----
declare_process_instruction!(MockBuiltinErr, 1, |invoke_context| {
⋮----
(0..get_instruction_errors().len()).for_each(|err| {
⋮----
let bank_details = SlotDetails::new_from_bank(&bank, true).unwrap();
⋮----
assert_ne!(ok_bank_details, bank_details);
⋮----
err_bank_details = Some(bank_details);
⋮----
fn test_process_entries_2nd_entry_collision_with_self_and_error(
⋮----
bank.deactivate_feature(&agave_feature_set::relax_intrabatch_account_locks::id());
⋮----
let (bank, _bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
assert_matches!(bank.transfer(5, &mint_keypair, &keypair1.pubkey()), Ok(_));
⋮----
let entry_2_to_3_and_1_to_mint = next_entry(
⋮----
let entry_conflict_itself = next_entry(
⋮----
let result = process_entries_for_tests_without_scheduler(
⋮----
bank.get_balance(&keypair1.pubkey()),
bank.get_balance(&keypair2.pubkey()),
bank.get_balance(&keypair3.pubkey()),
⋮----
assert_eq!(balances, [0, 3, 3]);
⋮----
assert!(result.is_err());
assert_eq!(balances, [2, 2, 2]);
⋮----
fn test_process_entry_duplicate_transaction(relax_intrabatch_account_locks: bool) {
⋮----
assert_matches!(bank.transfer(5, &mint_keypair, &keypair2.pubkey()), Ok(_));
let entry_1_to_2_twice = next_entry(
⋮----
let result = process_entries_for_tests_without_scheduler(&bank, vec![entry_1_to_2_twice]);
⋮----
assert_eq!(balances, [5, 5]);
⋮----
assert_eq!(result, Err(TransactionError::AlreadyProcessed));
⋮----
assert_eq!(result, Err(TransactionError::AccountInUse));
⋮----
fn test_process_entries_2_entries_par() {
⋮----
system_transaction::transfer(&keypair1, &keypair3.pubkey(), 1, bank.last_blockhash());
⋮----
system_transaction::transfer(&keypair2, &keypair4.pubkey(), 1, bank.last_blockhash());
⋮----
assert_eq!(bank.get_balance(&keypair3.pubkey()), 1);
assert_eq!(bank.get_balance(&keypair4.pubkey()), 1);
⋮----
fn test_process_entry_tx_random_execution_with_error() {
⋮----
} = create_genesis_config(1_000_000_000);
⋮----
let keypairs: Vec<_> = (0..NUM_TRANSFERS * 2).map(|_| Keypair::new()).collect();
⋮----
bank.transfer(1, &mint_keypair, &keypair.pubkey())
.expect("funding failed");
⋮----
let mut hash = bank.last_blockhash();
⋮----
bank.store_account(&present_account_key.pubkey(), &present_account);
⋮----
.step_by(NUM_TRANSFERS_PER_ENTRY)
.map(|i| {
⋮----
.map(|j| {
⋮----
&keypairs[i + j + NUM_TRANSFERS].pubkey(),
⋮----
transactions.push(system_transaction::create_account(
⋮----
next_entry_mut(&mut hash, 0, transactions)
⋮----
fn test_process_entry_tx_random_execution_no_error() {
⋮----
} = create_genesis_config((num_accounts + 1) as u64 * initial_lamports);
⋮----
let mut keypairs: Vec<Keypair> = vec![];
⋮----
assert_eq!(bank.process_transaction(&create_account_tx), Ok(()));
⋮----
keypairs.push(keypair);
⋮----
let mut tx_vector: Vec<Transaction> = vec![];
for i in (0..num_accounts).step_by(4) {
tx_vector.append(&mut vec![
⋮----
let entry = next_entry(&bank.last_blockhash(), 1, tx_vector);
⋮----
bank.squash();
for (i, keypair) in keypairs.iter().enumerate() {
⋮----
assert_eq!(bank.get_balance(&keypair.pubkey()), 2 * initial_lamports);
⋮----
assert_eq!(bank.get_balance(&keypair.pubkey()), 0);
⋮----
fn test_process_entries_2_entries_tick() {
⋮----
while blockhash == bank.last_blockhash() {
bank.register_default_tick_for_test();
⋮----
let tx = system_transaction::transfer(&keypair2, &keypair3.pubkey(), 1, blockhash);
⋮----
let tick = next_entry(&entry_1.hash, 1, vec![]);
⋮----
system_transaction::transfer(&keypair1, &keypair4.pubkey(), 1, bank.last_blockhash());
let entry_2 = next_entry(&tick.hash, 1, vec![tx]);
⋮----
system_transaction::transfer(&keypair2, &keypair3.pubkey(), 1, bank.last_blockhash());
let entry_3 = next_entry(&entry_2.hash, 1, vec![tx]);
⋮----
fn test_update_transaction_statuses() {
⋮----
} = create_genesis_config(11_000);
⋮----
bank.transfer(1_000, &mint_keypair, &pubkey).unwrap();
assert_eq!(bank.transaction_count(), 1);
assert_eq!(bank.get_balance(&pubkey), 1_000);
⋮----
fn test_update_transaction_statuses_fail(relax_intrabatch_account_locks: bool) {
⋮----
fn test_halt_at_slot_starting_snapshot_root() {
⋮----
let forks = tr(0) / tr(1);
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
blockstore.add_tree(
⋮----
genesis_config.hash(),
⋮----
halt_at_slot: Some(0),
⋮----
assert!(bank_forks.get(0).is_some());
⋮----
fn test_process_blockstore_from_root() {
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, i + 1, i, last_hash);
⋮----
blockstore.set_roots([3, 5].iter()).unwrap();
⋮----
let bank0 = bank_forks.read().unwrap().get_with_scheduler(0).unwrap();
⋮----
let bank0_last_blockhash = bank0.last_blockhash();
let bank1 = bank_forks.write().unwrap().insert(Bank::new_from_parent(
bank0.clone_without_scheduler(),
⋮----
bank_forks.write().unwrap().set_root(1, None, None);
⋮----
assert_eq!(frozen_bank_slots(&bank_forks), vec![5, 6]);
assert_eq!(bank_forks.working_bank().slot(), 6);
assert_eq!(bank_forks.root(), 5);
⋮----
fn test_process_entries_stress() {
⋮----
next_entry_mut(&mut hash, 0, {
⋮----
&keypairs[i + NUM_TRANSFERS].pubkey(),
⋮----
info!("paying iteration {i}");
process_entries_for_tests_without_scheduler(&bank, entries).expect("paying failed");
⋮----
next_entry_mut(
⋮----
&keypairs[i].pubkey(),
⋮----
info!("refunding iteration {i}");
process_entries_for_tests_without_scheduler(&bank, entries).expect("refunding failed");
process_entries_for_tests_without_scheduler(
⋮----
(0..bank.ticks_per_slot())
.map(|_| next_entry_mut(&mut hash, 1, vec![]))
⋮----
.expect("process ticks failed");
⋮----
old_root.squash();
⋮----
root = Some(bank.clone());
⋮----
let slot = bank.slot() + rng().random_range(1..3);
⋮----
fn test_process_ledger_ticks_ordering() {
⋮----
} = create_genesis_config(100);
⋮----
let genesis_hash = genesis_config.hash();
⋮----
let mut entries = create_ticks(genesis_config.ticks_per_slot, 1, genesis_hash);
let new_blockhash = entries.last().unwrap().hash;
let tx = system_transaction::transfer(&mint_keypair, &keypair.pubkey(), 1, new_blockhash);
let entry = next_entry(&new_blockhash, 1, vec![tx]);
⋮----
process_entries_for_tests_without_scheduler(&bank0, entries).unwrap();
assert_eq!(bank0.get_balance(&keypair.pubkey()), 1)
⋮----
fn get_epoch_schedule(genesis_config: &GenesisConfig) -> EpochSchedule {
⋮----
bank.epoch_schedule().clone()
⋮----
fn frozen_bank_slots(bank_forks: &BankForks) -> Vec<Slot> {
⋮----
.frozen_banks()
.map(|(slot, _bank)| slot)
⋮----
slots.sort_unstable();
⋮----
fn verify_fork_infos(bank_forks: &BankForks) {
for slot in frozen_bank_slots(bank_forks) {
⋮----
let mut parents = head_bank.parents();
parents.push(head_bank.clone());
⋮----
let parent_bank = &bank_forks[parent.slot()];
assert_eq!(parent_bank.slot(), parent.slot());
assert!(parent_bank.is_frozen());
⋮----
fn test_get_first_error() {
⋮----
let txs = vec![account_not_found_tx, invalid_blockhash_tx];
let batch = bank.prepare_batch_for_tests(txs);
let (commit_results, _) = batch.bank().load_execute_and_commit_transactions(
⋮----
let (err, signature) = do_get_first_error(&batch, &commit_results).unwrap();
assert_eq!(err.unwrap_err(), TransactionError::AccountNotFound);
assert_eq!(signature, account_not_found_sig);
⋮----
fn test_replay_vote_sender() {
⋮----
(0..10).map(|_| ValidatorVoteKeypairs::new_rand()).collect();
⋮----
} = create_genesis_config_with_vote_accounts(
⋮----
vec![100; validator_keypairs.len()],
⋮----
.insert(Bank::new_from_parent(
bank0.clone(),
⋮----
.clone_without_scheduler();
let bank_1_blockhash = bank1.last_blockhash();
⋮----
.map(|(i, validator_keypairs)| {
let tower_sync = TowerSync::new_from_slots(vec![0], bank0.hash(), None);
⋮----
.insert(validator_keypairs.vote_keypair.pubkey());
⋮----
TowerSync::from(vec![(bank1.slot() + 1, 1)]),
⋮----
let entry = next_entry(&bank_1_blockhash, 1, vote_txs);
⋮----
let _ = process_entries_for_tests(
⋮----
vec![entry],
⋮----
Some(&replay_vote_sender),
⋮----
.try_iter()
.map(|(vote_pubkey, ..)| vote_pubkey)
⋮----
assert_eq!(successes, expected_successful_voter_pubkeys);
⋮----
fn make_slot_with_vote_tx(
⋮----
let vote_entry = next_entry(parent_blockhash, 1, vec![vote_tx]);
let mut entries = create_ticks(ticks_per_slot, 0, vote_entry.hash);
entries.insert(0, vote_entry);
⋮----
fn run_test_process_blockstore_with_supermajority_root(
⋮----
let mut main_fork = tr(starting_fork_slot);
let mut main_fork_ref = main_fork.root_mut().get_mut();
let expected_root_slot = starting_fork_slot + blockstore_root.unwrap_or(0);
⋮----
let minor_fork = tr(last_minor_fork_slot);
⋮----
main_fork_ref.push_front(minor_fork.clone());
⋮----
main_fork_ref.push_front(tr(slot));
main_fork_ref = main_fork_ref.front_mut().unwrap().get_mut();
⋮----
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / main_fork);
⋮----
vec![100],
⋮----
let ticks_per_slot = genesis_config.ticks_per_slot();
⋮----
blockstore.add_tree(forks, false, true, ticks_per_slot, genesis_config.hash());
⋮----
.set_roots(std::iter::once(&blockstore_root))
⋮----
let last_vote_bank_hash = bank_forks.get(last_main_fork_slot - 1).unwrap().hash();
⋮----
.get(last_main_fork_slot - 1)
⋮----
.last_blockhash();
⋮----
make_slot_with_vote_tx(
⋮----
assert_eq!(bank_forks.root(), expected_root_slot);
⋮----
let bank = bank_forks.get(slot).unwrap();
assert_eq!(bank.slot(), slot);
⋮----
assert!(bank_forks.get(slot).is_none());
⋮----
let last_vote_bank_hash = bank_forks.get(last_main_fork_slot).unwrap().hash();
⋮----
.get(last_main_fork_slot)
⋮----
assert_eq!(bank_forks.root(), really_expected_root_slot);
⋮----
fn test_process_blockstore_with_supermajority_root_without_blockstore_root() {
run_test_process_blockstore_with_supermajority_root(None, AccessType::Primary);
⋮----
fn test_process_blockstore_with_supermajority_root_without_blockstore_root_readonly_access() {
run_test_process_blockstore_with_supermajority_root(None, AccessType::ReadOnly);
⋮----
fn test_process_blockstore_with_supermajority_root_with_blockstore_root() {
run_test_process_blockstore_with_supermajority_root(Some(1), AccessType::Primary)
⋮----
fn test_supermajority_root_from_vote_accounts() {
⋮----
.map(|(root, stake)| {
⋮----
vote_state.root_slot = Some(root);
⋮----
VoteStateV4::serialize(&versioned, vote_account.data_as_mut_slice()).unwrap();
⋮----
(stake, VoteAccount::try_from(vote_account).unwrap()),
⋮----
assert!(supermajority_root_from_vote_accounts(total_stake, &HashMap::default()).is_none());
let roots_stakes = vec![(8, 1), (3, 1), (4, 1), (8, 1)];
let accounts = convert_to_vote_accounts(roots_stakes);
assert!(supermajority_root_from_vote_accounts(total_stake, &accounts).is_none());
let roots_stakes = vec![(8, 1), (3, 1), (4, 1), (8, 5)];
⋮----
let roots_stakes = vec![(8, 1), (3, 1), (4, 1), (8, 6)];
⋮----
fn confirm_slot_entries_for_tests(
⋮----
fn create_test_transactions(
⋮----
fn test_confirm_slot_entries_progress_num_txs_indexes() {
⋮----
} = create_genesis_config(100 * LAMPORTS_PER_SOL);
⋮----
let amount = genesis_config.rent.minimum_balance(0);
⋮----
bank.transfer(LAMPORTS_PER_SOL, &mint_keypair, &keypair1.pubkey())
⋮----
bank.transfer(LAMPORTS_PER_SOL, &mint_keypair, &keypair2.pubkey())
⋮----
&keypair3.pubkey(),
⋮----
&keypair4.pubkey(),
⋮----
let entry = next_entry(&blockhash, 1, vec![tx1, tx2]);
⋮----
(vec![entry], 0, false),
⋮----
Some(&transaction_status_sender),
⋮----
assert_eq!(progress.num_txs, 2);
let batch = transaction_status_receiver.recv().unwrap();
⋮----
assert_eq!(batch.transactions.len(), 2);
assert_eq!(batch.transaction_indexes.len(), 2);
assert_eq!(batch.transaction_indexes, [0, 1]);
⋮----
panic!("batch should have been sent");
⋮----
let entry = next_entry(&new_hash, 1, vec![tx1, tx2, tx3]);
⋮----
assert_eq!(progress.num_txs, 5);
⋮----
assert_eq!(batch.transactions.len(), 3);
assert_eq!(batch.transaction_indexes.len(), 3);
assert_eq!(batch.transaction_indexes, [2, 3, 4]);
⋮----
fn do_test_schedule_batches_for_execution(should_succeed: bool) {
⋮----
} = create_genesis_config_with_leader(500, &dummy_leader_pubkey, 100);
⋮----
let context = SchedulingContext::for_verification(bank.clone());
let txs = create_test_transactions(&mint_keypair, &genesis_config.hash());
⋮----
let seq_cloned = seq.clone();
⋮----
.expect_context()
.times(1)
.in_sequence(&mut seq.lock().unwrap())
.return_const(context);
⋮----
.expect_schedule_execution()
.times(txs.len())
.returning(|_, _| Ok(()));
⋮----
.returning(|_, _| Err(SchedulerAborted));
⋮----
.expect_recover_error_after_abort()
⋮----
.returning(|| TransactionError::InsufficientFundsForFee);
⋮----
.expect_wait_for_termination()
.with(mockall::predicate::eq(true))
⋮----
.returning(move |_| {
⋮----
.expect_return_to_pool()
⋮----
.in_sequence(&mut seq_cloned.lock().unwrap())
.returning(|| ());
⋮----
(Ok(()), ExecuteTimings::default()),
⋮----
let bank = BankWithScheduler::new(bank, Some(Box::new(mocked_scheduler)));
⋮----
lock_results: bank.try_lock_accounts(&txs, false),
⋮----
let result = process_batches(
⋮----
[locked_entry].into_iter(),
⋮----
assert_matches!(result, Ok(()));
⋮----
assert_matches!(result, Err(TransactionError::InsufficientFundsForFee));
⋮----
fn test_schedule_batches_for_execution_success() {
do_test_schedule_batches_for_execution(true);
⋮----
fn test_schedule_batches_for_execution_failure() {
do_test_schedule_batches_for_execution(false);
⋮----
enum TxResult {
⋮----
fn test_execute_batch_pre_commit_callback(
⋮----
Ok(()),
⋮----
Err(TransactionError::BlockhashNotFound),
⋮----
vec![Ok(()); 1],
⋮----
batch.set_needs_unlock(false);
let poh_with_index = matches!(&poh_result, Ok(Some(_)));
⋮----
transaction_indexes: vec![],
⋮----
assert_eq!(bank.transaction_count(), 0);
assert_eq!(bank.transaction_error_count(), 0);
let should_commit = poh_result.is_ok();
⋮----
let result = execute_batch(
⋮----
Some(&TransactionStatusSender {
⋮----
Some(|processing_result: &'_ Result<_>| {
⋮----
Err(error.clone())?;
⋮----
Ok(ok)
⋮----
// pre_commit_callback() should always be called regardless of tx_result
assert!(is_called);
⋮----
assert_eq!(result, expected_tx_result);
if expected_tx_result.is_ok() {
⋮----
if matches!(tx_result, TxResult::ExecutedWithFailure) {
assert_eq!(bank.transaction_error_count(), 1);
⋮----
assert_matches!(result, Err(TransactionError::CommitCancelled));
⋮----
if poh_with_index && expected_tx_result.is_ok() {
⋮----
} else if should_commit && expected_tx_result.is_ok() {
⋮----
assert_matches!(receiver.try_recv(), Err(_));
⋮----
fn test_confirm_slot_entries_with_fix() {
⋮----
genesis_config.poh_config.hashes_per_tick = Some(HASHES_PER_TICK);
⋮----
assert_eq!(slot_0_bank.slot(), 0);
assert_eq!(slot_0_bank.tick_height(), 0);
assert_eq!(slot_0_bank.max_tick_height(), 2);
assert_eq!(slot_0_bank.last_blockhash(), genesis_hash);
assert_eq!(slot_0_bank.get_hash_age(&genesis_hash), Some(0));
⋮----
let slot_0_hash = slot_0_entries.last().unwrap().hash;
confirm_slot_entries_for_tests(&slot_0_bank, slot_0_entries, true, genesis_hash).unwrap();
assert_eq!(slot_0_bank.tick_height(), slot_0_bank.max_tick_height());
assert_eq!(slot_0_bank.last_blockhash(), slot_0_hash);
assert_eq!(slot_0_bank.get_hash_age(&genesis_hash), Some(1));
assert_eq!(slot_0_bank.get_hash_age(&slot_0_hash), Some(0));
⋮----
.insert(new_bank)
⋮----
assert_eq!(slot_2_bank.slot(), 2);
assert_eq!(slot_2_bank.tick_height(), 2);
assert_eq!(slot_2_bank.max_tick_height(), 6);
assert_eq!(slot_2_bank.last_blockhash(), slot_0_hash);
⋮----
let slot_1_hash = slot_1_entries.last().unwrap().hash;
confirm_slot_entries_for_tests(&slot_2_bank, slot_1_entries, false, slot_0_hash).unwrap();
assert_eq!(slot_2_bank.tick_height(), 4);
⋮----
assert_eq!(slot_2_bank.get_hash_age(&genesis_hash), Some(1));
assert_eq!(slot_2_bank.get_hash_age(&slot_0_hash), Some(0));
struct TestCase {
⋮----
expected_result: Err(BlockstoreProcessorError::InvalidTransaction(
⋮----
expected_result: Ok(()),
⋮----
// Check that slot 2 transactions can only use hashes for completed blocks.
⋮----
remaining_entry_hashes = remaining_entry_hashes.checked_sub(1).unwrap();
let mut entries = vec![next_entry_mut(&mut prev_entry_hash, 1, vec![tx])];
entries.push(next_entry_mut(
⋮----
vec![],
⋮----
let slot_2_hash = slot_2_entries.last().unwrap().hash;
⋮----
confirm_slot_entries_for_tests(&slot_2_bank, slot_2_entries, true, slot_1_hash);
⋮----
assert_eq!(slot_2_bank.tick_height(), slot_2_bank.max_tick_height());
assert_eq!(slot_2_bank.last_blockhash(), slot_2_hash);
assert_eq!(slot_2_bank.get_hash_age(&genesis_hash), Some(2));
assert_eq!(slot_2_bank.get_hash_age(&slot_0_hash), Some(1));
assert_eq!(slot_2_bank.get_hash_age(&slot_2_hash), Some(0));
⋮----
assert_eq!(err, expected_err);
⋮----
panic!("actual result {result:?} != expected result {expected_result:?}");
⋮----
fn test_check_block_cost_limit() {
⋮----
unreachable!("test tx is non-vote tx");
⋮----
// set block-limit to be able to just have one transaction
let block_limit = tx_cost.sum();
⋮----
.set_limits(u64::MAX, block_limit, u64::MAX);
let tx_costs = vec![None, Some(tx_cost), None];
// The transaction will fit when added the first time
assert!(check_block_cost_limits(&bank, &tx_costs).is_ok());
// But adding a second time will exceed the block limit
⋮----
// Adding another None will noop (even though the block is already full)
assert!(check_block_cost_limits(&bank, &tx_costs[0..1]).is_ok());
⋮----
fn test_calculate_alpenglow_ticks() {
⋮----
// Slots before alpenglow don't have alpenglow ticks
⋮----
assert!(

================
File: ledger/src/blockstore.rs
================
pub mod blockstore_purge;
pub mod column;
pub mod error;
⋮----
use static_assertions::const_assert_eq;
⋮----
pub type CompletedSlotsSender = Sender<Vec<Slot>>;
pub type CompletedSlotsReceiver = Receiver<Vec<Slot>>;
type CompletedRanges = Vec<Range<u32>>;
⋮----
pub struct SignatureInfosForAddress {
⋮----
enum InsertDataShredError {
⋮----
pub enum PossibleDuplicateShred {
⋮----
impl PossibleDuplicateShred {
pub fn slot(&self) -> Slot {
⋮----
Self::Exists(shred) => shred.slot(),
Self::LastIndexConflict(shred, _) => shred.slot(),
Self::ErasureConflict(shred, _) => shred.slot(),
Self::MerkleRootConflict(shred, _) => shred.slot(),
Self::ChainedMerkleRootConflict(shred, _) => shred.slot(),
⋮----
enum WorkingEntry<T> {
⋮----
fn should_write(&self) -> bool {
matches!(self, Self::Dirty(_))
⋮----
fn as_ref(&self) -> &T {
⋮----
pub struct LastFECSetCheckResults {
⋮----
impl LastFECSetCheckResults {
fn get_last_fec_set_merkle_root(
⋮----
if self.last_fec_set_merkle_root.is_none() {
return Err(BlockstoreProcessorError::IncompleteFinalFecSet);
⋮----
.is_active(&agave_feature_set::vote_only_retransmitter_signed_fec_sets::id())
⋮----
return Err(BlockstoreProcessorError::InvalidRetransmitterSignatureFinalFecSet);
⋮----
Ok(self.last_fec_set_merkle_root)
⋮----
pub struct InsertResults {
⋮----
pub struct CompletedDataSetInfo {
⋮----
pub struct BlockstoreSignals {
⋮----
pub struct Blockstore {
⋮----
pub struct IndexMetaWorkingSetEntry {
⋮----
pub struct SlotMetaWorkingSetEntry {
⋮----
struct ShredInsertionTracker<'a> {
// Map which contains data shreds that have just been inserted.
⋮----
fn new(shred_num: usize, write_batch: WriteBatch) -> Self {
⋮----
duplicate_shreds: vec![],
⋮----
newly_completed_data_sets: vec![],
⋮----
impl SlotMetaWorkingSetEntry {
/// Construct a new SlotMetaWorkingSetEntry with the specified `new_slot_meta`
    /// and `old_slot_meta`.  `did_insert_occur` is set to false.
⋮----
/// and `old_slot_meta`.  `did_insert_occur` is set to false.
    fn new(new_slot_meta: Rc<RefCell<SlotMeta>>, old_slot_meta: Option<SlotMeta>) -> Self {
⋮----
fn new(new_slot_meta: Rc<RefCell<SlotMeta>>, old_slot_meta: Option<SlotMeta>) -> Self {
⋮----
pub fn banking_trace_path(path: &Path) -> PathBuf {
path.join("banking_trace")
⋮----
pub fn banking_retrace_path(path: &Path) -> PathBuf {
path.join("banking_retrace")
⋮----
impl Blockstore {
pub fn ledger_path(&self) -> &PathBuf {
⋮----
pub fn banking_trace_path(&self) -> PathBuf {
banking_trace_path(&self.ledger_path)
⋮----
pub fn banking_retracer_path(&self) -> PathBuf {
banking_retrace_path(&self.ledger_path)
⋮----
/// Opens a Ledger in directory, provides "infinite" window of shreds
    pub fn open(ledger_path: &Path) -> Result<Blockstore> {
⋮----
pub fn open(ledger_path: &Path) -> Result<Blockstore> {
⋮----
pub fn open_with_options(ledger_path: &Path, options: BlockstoreOptions) -> Result<Blockstore> {
⋮----
fn do_open(ledger_path: &Path, options: BlockstoreOptions) -> Result<Blockstore> {
⋮----
let blockstore_path = ledger_path.join(BLOCKSTORE_DIRECTORY_ROCKS_LEVEL);
// Open the database
⋮----
info!("Opening blockstore at {blockstore_path:?}");
⋮----
let address_signatures_cf = db.column();
let bank_hash_cf = db.column();
let block_height_cf = db.column();
let blocktime_cf = db.column();
let code_shred_cf = db.column();
let data_shred_cf = db.column();
let dead_slots_cf = db.column();
let duplicate_slots_cf = db.column();
let erasure_meta_cf = db.column();
let index_cf = db.column();
let merkle_root_meta_cf = db.column();
let meta_cf = db.column();
let optimistic_slots_cf = db.column();
let orphans_cf = db.column();
let perf_samples_cf = db.column();
let rewards_cf = db.column();
let roots_cf = db.column();
let transaction_memos_cf = db.column();
let transaction_status_cf = db.column();
let transaction_status_index_cf = db.column();
// Get max root or 0 if it doesn't exist
⋮----
.iter(IteratorMode::End)?
.next()
.map(|(slot, _)| slot)
.unwrap_or(0);
⋮----
measure.stop();
info!("Opening blockstore done; {measure}");
⋮----
ledger_path: ledger_path.to_path_buf(),
⋮----
blockstore.cleanup_old_entries()?;
blockstore.update_highest_primary_index_slot()?;
Ok(blockstore)
⋮----
pub fn open_with_signal(
⋮----
let (ledger_signal_sender, ledger_signal_receiver) = bounded(MAX_REPLAY_WAKE_UP_SIGNALS);
⋮----
bounded(MAX_COMPLETED_SLOTS_IN_CHANNEL);
blockstore.add_new_shred_signal(ledger_signal_sender);
blockstore.add_completed_slots_signal(completed_slots_sender);
Ok(BlockstoreSignals {
⋮----
pub fn add_tree(
⋮----
while let Some(visit) = walk.get() {
let slot = *visit.node().data();
if self.meta(slot).unwrap().is_some() && self.orphan(slot).unwrap().is_none() {
walk.forward();
⋮----
let parent = walk.get_parent().map(|n| *n.data());
if parent.is_some() || !is_orphan {
⋮----
.and_then(|parent| blockhashes.get(&parent))
.unwrap_or(&starting_hash);
let mut entries = create_ticks(
num_ticks * (std::cmp::max(1, slot - parent.unwrap_or(slot))),
⋮----
blockhashes.insert(slot, entries.last().unwrap().hash);
⋮----
entries.pop().unwrap();
⋮----
let shreds = entries_to_test_shreds(
⋮----
parent.unwrap_or(slot),
⋮----
self.insert_shreds(shreds, None, false).unwrap();
⋮----
pub fn destroy(ledger_path: &Path) -> Result<()> {
⋮----
Rocks::destroy(&Path::new(ledger_path).join(BLOCKSTORE_DIRECTORY_ROCKS_LEVEL))
⋮----
pub fn meta(&self, slot: Slot) -> Result<Option<SlotMeta>> {
self.meta_cf.get(slot)
⋮----
pub fn is_full(&self, slot: Slot) -> bool {
if let Ok(Some(meta)) = self.meta_cf.get(slot) {
return meta.is_full();
⋮----
fn erasure_meta(&self, erasure_set: ErasureSetId) -> Result<Option<ErasureMeta>> {
let (slot, fec_set_index) = erasure_set.store_key();
self.erasure_meta_cf.get((slot, u64::from(fec_set_index)))
⋮----
fn put_erasure_meta(
⋮----
self.erasure_meta_cf.put_bytes(
⋮----
&bincode::serialize(erasure_meta).unwrap(),
⋮----
fn previous_erasure_set<'a>(
⋮----
.range((
⋮----
.next_back();
⋮----
.filter(|(_, candidate_erasure_meta)| {
candidate_erasure_meta.as_ref().next_fec_set_index() == Some(fec_set_index)
⋮----
.map(|(erasure_set, erasure_meta)| {
(*erasure_set, Cow::Borrowed(erasure_meta.as_ref()))
⋮----
if candidate_erasure_set_and_meta.is_some() {
return Ok(candidate_erasure_set_and_meta);
⋮----
.iter(IteratorMode::From(
⋮----
.find(|((_, candidate_fec_set_index), _)| {
⋮----
.filter(|((candidate_slot, _), _)| *candidate_slot == slot)
⋮----
return Ok(None);
⋮----
.expect("fec_set_index from a previously inserted shred should fit in u32");
⋮----
let candidate_erasure_meta: ErasureMeta = deserialize(candidate_erasure_meta.as_ref())?;
let Some(next_fec_set_index) = candidate_erasure_meta.next_fec_set_index() else {
return Err(BlockstoreError::InvalidErasureConfig);
⋮----
return Ok(Some((
⋮----
Ok(None)
⋮----
fn merkle_root_meta(&self, erasure_set: ErasureSetId) -> Result<Option<MerkleRootMeta>> {
self.merkle_root_meta_cf.get(erasure_set.store_key())
⋮----
pub fn orphan(&self, slot: Slot) -> Result<Option<bool>> {
self.orphans_cf.get(slot)
⋮----
pub fn slot_meta_iterator(
⋮----
.iter(IteratorMode::From(slot, IteratorDirection::Forward))?;
Ok(meta_iter.map(|(slot, slot_meta_bytes)| {
⋮----
cf::SlotMeta::deserialize(&slot_meta_bytes).unwrap_or_else(|e| {
panic!("Could not deserialize SlotMeta for slot {slot}: {e:?}")
⋮----
pub fn live_slots_iterator(&self, root: Slot) -> impl Iterator<Item = (Slot, SlotMeta)> + '_ {
⋮----
let orphans_iter = self.orphans_iterator(root + 1).unwrap();
root_forks.chain(orphans_iter.flat_map(move |orphan| NextSlotsIterator::new(orphan, self)))
⋮----
pub fn live_files_metadata(&self) -> Result<Vec<LiveFile>> {
self.db.live_files_metadata()
⋮----
pub fn iterator_cf(
⋮----
let cf = self.db.cf_handle(cf_name);
let iterator = self.db.iterator_cf(cf, rocksdb::IteratorMode::Start);
Ok(iterator.map(|pair| pair.unwrap()))
⋮----
pub fn slot_data_iterator(
⋮----
let slot_iterator = self.data_shred_cf.iter(IteratorMode::From(
⋮----
Ok(slot_iterator.take_while(move |((shred_slot, _), _)| *shred_slot == slot))
⋮----
pub fn slot_coding_iterator(
⋮----
let slot_iterator = self.code_shred_cf.iter(IteratorMode::From(
⋮----
fn prepare_rooted_slot_iterator(
⋮----
let slot_iterator = self.roots_cf.iter(IteratorMode::From(slot, direction))?;
Ok(slot_iterator.map(move |(rooted_slot, _)| rooted_slot))
⋮----
pub fn rooted_slot_iterator(&self, slot: Slot) -> Result<impl Iterator<Item = Slot> + '_> {
self.prepare_rooted_slot_iterator(slot, IteratorDirection::Forward)
⋮----
pub fn reversed_rooted_slot_iterator(
⋮----
self.prepare_rooted_slot_iterator(slot, IteratorDirection::Reverse)
⋮----
pub fn reversed_optimistic_slots_iterator(
⋮----
let iter = self.optimistic_slots_cf.iter(IteratorMode::End)?;
Ok(iter.map(|(slot, bytes)| {
let meta: OptimisticSlotMetaVersioned = deserialize(&bytes).unwrap();
(slot, meta.hash(), meta.timestamp())
⋮----
pub fn slot_range_connected(&self, starting_slot: Slot, ending_slot: Slot) -> bool {
⋮----
let mut next_slots: VecDeque<_> = match self.meta(starting_slot) {
Ok(Some(starting_slot_meta)) => starting_slot_meta.next_slots.into(),
⋮----
while let Some(slot) = next_slots.pop_front() {
if let Ok(Some(slot_meta)) = self.meta(slot) {
if slot_meta.is_full() {
match slot.cmp(&ending_slot) {
cmp::Ordering::Less => next_slots.extend(slot_meta.next_slots),
⋮----
fn get_recovery_data_shreds<'a>(
⋮----
erasure_meta.data_shreds_indices().filter_map(move |i| {
let key = ShredId::new(slot, u32::try_from(i).unwrap(), ShredType::Data);
if let Some(shred) = prev_inserted_shreds.get(&key) {
return Some(shred.as_ref().clone());
⋮----
if !index.data().contains(i) {
⋮----
match self.data_shred_cf.get_bytes((slot, i)).unwrap() {
⋮----
error!(
⋮----
Some(data) => Shred::new_from_serialized_shred(data).ok(),
⋮----
fn get_recovery_coding_shreds<'a>(
⋮----
erasure_meta.coding_shreds_indices().filter_map(move |i| {
let key = ShredId::new(slot, u32::try_from(i).unwrap(), ShredType::Code);
⋮----
if !index.coding().contains(i) {
⋮----
match self.code_shred_cf.get_bytes((slot, i)).unwrap() {
⋮----
Some(code) => Shred::new_from_serialized_shred(code).ok(),
⋮----
fn recover_shreds<'a>(
⋮----
// Find shreds for this erasure set and try recovery
let data = self.get_recovery_data_shreds(index, erasure_meta, prev_inserted_shreds);
let code = self.get_recovery_coding_shreds(index, erasure_meta, prev_inserted_shreds);
let shreds = shred::recover(data.chain(code), reed_solomon_cache)?;
Ok(shreds.filter_map(std::result::Result::ok))
⋮----
/// Collects and reports [`BlockstoreRocksDbColumnFamilyMetrics`] for the
    /// all the column families.
⋮----
/// all the column families.
    ///
⋮----
///
    /// [`BlockstoreRocksDbColumnFamilyMetrics`]: crate::blockstore_metrics::BlockstoreRocksDbColumnFamilyMetrics
⋮----
/// [`BlockstoreRocksDbColumnFamilyMetrics`]: crate::blockstore_metrics::BlockstoreRocksDbColumnFamilyMetrics
    pub fn submit_rocksdb_cf_metrics_for_all_cfs(&self) {
⋮----
pub fn submit_rocksdb_cf_metrics_for_all_cfs(&self) {
self.meta_cf.submit_rocksdb_cf_metrics();
self.dead_slots_cf.submit_rocksdb_cf_metrics();
self.duplicate_slots_cf.submit_rocksdb_cf_metrics();
self.roots_cf.submit_rocksdb_cf_metrics();
self.erasure_meta_cf.submit_rocksdb_cf_metrics();
self.orphans_cf.submit_rocksdb_cf_metrics();
self.index_cf.submit_rocksdb_cf_metrics();
self.data_shred_cf.submit_rocksdb_cf_metrics();
self.code_shred_cf.submit_rocksdb_cf_metrics();
self.transaction_status_cf.submit_rocksdb_cf_metrics();
self.address_signatures_cf.submit_rocksdb_cf_metrics();
self.transaction_memos_cf.submit_rocksdb_cf_metrics();
self.transaction_status_index_cf.submit_rocksdb_cf_metrics();
self.rewards_cf.submit_rocksdb_cf_metrics();
self.blocktime_cf.submit_rocksdb_cf_metrics();
self.perf_samples_cf.submit_rocksdb_cf_metrics();
self.block_height_cf.submit_rocksdb_cf_metrics();
self.bank_hash_cf.submit_rocksdb_cf_metrics();
self.optimistic_slots_cf.submit_rocksdb_cf_metrics();
self.merkle_root_meta_cf.submit_rocksdb_cf_metrics();
⋮----
/// Attempts to insert shreds into blockstore and updates relevant metrics
    /// based on the results, split out by shred source (tubine vs. repair).
⋮----
/// based on the results, split out by shred source (tubine vs. repair).
    fn attempt_shred_insertion<'a>(
⋮----
fn attempt_shred_insertion<'a>(
⋮----
Item = (Cow<'a, Shred>, /*is_repaired:*/ bool),
⋮----
let shreds = shreds.into_iter();
metrics.num_shreds += shreds.len();
⋮----
match shred.shred_type() {
⋮----
match self.check_insert_data_shred(
⋮----
error!("blockstore error: {err}");
⋮----
self.check_insert_coding_shred(
⋮----
start.stop();
metrics.insert_shreds_elapsed_us += start.as_us();
⋮----
fn try_shred_recovery<'a>(
⋮----
.iter()
.filter_map(|(erasure_set, working_erasure_meta)| {
let erasure_meta = working_erasure_meta.as_ref();
let slot = erasure_set.slot();
let index_meta_entry = index_working_set.get(&slot).expect("Index");
⋮----
.should_recover_shreds(index)
.then(|| {
self.recover_shreds(
⋮----
.ok()
⋮----
.flatten()
⋮----
fn handle_shred_recovery(
⋮----
.try_shred_recovery(
⋮----
.filter_map(|shred| {
⋮----
recovered_shreds.push(shred.into_payload());
⋮----
recovered_shreds.push(shred.payload().clone());
Some(shred)
⋮----
.collect();
if !recovered_shreds.is_empty() {
let _ = retransmit_sender.try_send(recovered_shreds);
⋮----
metrics.num_recovered += recovered_data_shreds.len();
⋮----
*match self.check_insert_data_shred(
⋮----
metrics.shred_recovery_elapsed_us += start.as_us();
⋮----
fn check_chained_merkle_root_consistency(
⋮----
for (erasure_set, working_erasure_meta) in shred_insertion_tracker.erasure_metas.iter() {
if !working_erasure_meta.should_write() {
⋮----
let (slot, _) = erasure_set.store_key();
if self.has_duplicate_shreds_in_slot(slot) {
⋮----
.first_received_coding_shred_index()
.expect("First received coding index must fit in u32"),
⋮----
.get(&shred_id)
.expect("Erasure meta was just created, initial shred must exist");
self.check_forward_chained_merkle_root_consistency(
⋮----
shred_insertion_tracker.merkle_root_metas.iter()
⋮----
if !working_merkle_root_meta.should_write() {
⋮----
let merkle_root_meta = working_merkle_root_meta.as_ref();
⋮----
merkle_root_meta.first_received_shred_index(),
merkle_root_meta.first_received_shred_type(),
⋮----
.expect("Merkle root meta was just created, initial shred must exist");
self.check_backwards_chained_merkle_root_consistency(
⋮----
fn commit_updates_to_write_batch(
⋮----
let (should_signal, newly_completed_slots) = self.commit_slot_meta_working_set(
⋮----
self.erasure_meta_cf.put_in_batch(
⋮----
working_erasure_meta.as_ref(),
⋮----
self.merkle_root_meta_cf.put_in_batch(
⋮----
erasure_set.store_key(),
working_merkle_root_meta.as_ref(),
⋮----
for (&slot, index_working_set_entry) in shred_insertion_tracker.index_working_set.iter() {
⋮----
self.index_cf.put_in_batch(
⋮----
metrics.commit_working_sets_elapsed_us += start.as_us();
Ok((should_signal, newly_completed_slots))
⋮----
fn do_insert_shreds<'a>(
⋮----
let _lock = self.insert_shreds_lock.lock().unwrap();
⋮----
metrics.insert_lock_elapsed_us += start.as_us();
⋮----
ShredInsertionTracker::new(shreds.len(), self.get_write_batch()?);
self.attempt_shred_insertion(
⋮----
self.handle_shred_recovery(
⋮----
self.handle_chaining(
⋮----
self.check_chained_merkle_root_consistency(&mut shred_insertion_tracker);
⋮----
self.commit_updates_to_write_batch(&mut shred_insertion_tracker, metrics)?;
⋮----
self.write_batch(shred_insertion_tracker.write_batch)?;
⋮----
metrics.write_batch_elapsed_us += start.as_us();
send_signals(
&self.new_shreds_signals.lock().unwrap(),
&self.completed_slots_senders.lock().unwrap(),
⋮----
total_start.stop();
metrics.total_elapsed_us += total_start.as_us();
⋮----
Ok(InsertResults {
⋮----
pub fn insert_shreds_handle_duplicate<'a, F>(
⋮----
} = self.do_insert_shreds(
⋮----
Some((reed_solomon_cache, retransmit_sender)),
⋮----
handle_duplicate(shred);
⋮----
Ok(completed_data_set_infos)
⋮----
pub fn add_new_shred_signal(&self, s: Sender<bool>) {
self.new_shreds_signals.lock().unwrap().push(s);
⋮----
pub fn add_completed_slots_signal(&self, s: CompletedSlotsSender) {
self.completed_slots_senders.lock().unwrap().push(s);
⋮----
pub fn get_new_shred_signals_len(&self) -> usize {
self.new_shreds_signals.lock().unwrap().len()
⋮----
pub fn get_new_shred_signal(&self, index: usize) -> Option<Sender<bool>> {
self.new_shreds_signals.lock().unwrap().get(index).cloned()
⋮----
pub fn drop_signal(&self) {
self.new_shreds_signals.lock().unwrap().clear();
self.completed_slots_senders.lock().unwrap().clear();
⋮----
pub fn clear_unconfirmed_slot(&self, slot: Slot) {
⋮----
match self.purge_slot_cleanup_chaining(slot) {
⋮----
error!("clear_unconfirmed_slot() called on slot {slot} with no SlotMeta")
⋮----
Err(e) => panic!("Purge database operations failed {e}"),
⋮----
pub fn insert_cow_shreds<'a>(
⋮----
.into_iter()
.map(|shred| (shred,  false));
let insert_results = self.do_insert_shreds(
⋮----
Ok(insert_results.completed_data_set_infos)
⋮----
pub fn insert_shreds(
⋮----
let shreds = shreds.into_iter().map(Cow::Owned);
self.insert_cow_shreds(shreds, leader_schedule, is_trusted)
⋮----
fn insert_shred_return_duplicate(
⋮----
.do_insert_shreds(
⋮----
Some(leader_schedule),
⋮----
.unwrap();
⋮----
fn check_insert_coding_shred<'a>(
⋮----
let slot = shred.slot();
let shred_index = u64::from(shred.index());
⋮----
self.get_index_meta_entry(slot, index_working_set, index_meta_time_us);
⋮----
let erasure_set = shred.erasure_set();
if let HashMapEntry::Vacant(entry) = merkle_root_metas.entry(erasure_set) {
if let Some(meta) = self.merkle_root_meta(erasure_set).unwrap() {
entry.insert(WorkingEntry::Clean(meta));
⋮----
// This gives the index of first coding shred in this FEC block
// So, all coding shreds in a given FEC block will have the same set index
⋮----
if index_meta.coding().contains(shred_index) {
⋮----
duplicate_shreds.push(PossibleDuplicateShred::Exists(shred.into_owned()));
⋮----
if !Blockstore::should_insert_coding_shred(&shred, self.max_root()) {
⋮----
if let Some(merkle_root_meta) = merkle_root_metas.get(&erasure_set) {
// A previous shred has been inserted in this batch or in blockstore
// Compare our current shred against the previous shred for potential
// conflicts
if !self.check_merkle_root_consistency(
⋮----
merkle_root_meta.as_ref(),
⋮----
let erasure_meta_entry = erasure_metas.entry(erasure_set).or_insert_with(|| {
self.erasure_meta(erasure_set)
.expect("Expect database get to succeed")
.map(WorkingEntry::Clean)
.unwrap_or_else(|| {
WorkingEntry::Dirty(ErasureMeta::from_coding_shred(&shred).unwrap())
⋮----
let erasure_meta = erasure_meta_entry.as_ref();
if !erasure_meta.check_coding_shred(&shred) {
⋮----
if !self.has_duplicate_shreds_in_slot(slot) {
⋮----
.find_conflicting_coding_shred(&shred, slot, erasure_meta, just_inserted_shreds)
.map(Cow::into_owned)
⋮----
if let Err(e) = self.store_duplicate_slot(
⋮----
conflicting_shred.clone(),
shred.payload().clone(),
⋮----
warn!(
⋮----
duplicate_shreds.push(PossibleDuplicateShred::ErasureConflict(
shred.as_ref().clone(),
⋮----
// ToDo: This is a potential slashing condition
warn!("Received multiple erasure configs for the same erasure set!!!");
⋮----
.record_shred(shred.slot(), shred.fec_set_index(), shred_source, None);
// insert coding shred into rocks
⋮----
.insert_coding_shred(index_meta, &shred, write_batch)
.is_ok();
⋮----
.entry(erasure_set)
.or_insert(WorkingEntry::Dirty(MerkleRootMeta::from_shred(&shred)));
⋮----
if let HashMapEntry::Vacant(entry) = just_inserted_shreds.entry(shred.id()) {
⋮----
entry.insert(shred);
⋮----
fn find_conflicting_coding_shred<'a>(
⋮----
let index = erasure_meta.first_received_coding_shred_index()?;
⋮----
let maybe_shred = self.get_shred_from_just_inserted_or_db(just_received_shreds, shred_id);
if index != 0 || maybe_shred.is_some() {
⋮----
for coding_index in erasure_meta.coding_shreds_indices() {
let maybe_shred = self.get_coding_shred(slot, coding_index);
⋮----
let potential_shred = Shred::new_from_serialized_shred(shred_data).unwrap();
if shred.erasure_mismatch(&potential_shred).unwrap() {
return Some(Cow::Owned(potential_shred.into_payload()));
⋮----
let key = ShredId::new(slot, u32::try_from(coding_index).unwrap(), ShredType::Code);
just_received_shreds.get(&key)
⋮----
if shred.erasure_mismatch(potential_shred).unwrap() {
return Some(Cow::Borrowed(potential_shred.payload()));
⋮----
fn check_insert_data_shred<'a>(
⋮----
let slot_meta_entry = self.get_slot_meta_entry(
⋮----
.parent()
.map_err(|_| InsertDataShredError::InvalidShred)?,
⋮----
let slot_meta = &mut slot_meta_entry.new_slot_meta.borrow_mut();
⋮----
if Self::is_data_shred_present(&shred, slot_meta, index_meta.data()) {
⋮----
return Err(InsertDataShredError::Exists);
⋮----
if shred.last_in_slot() && shred_index < slot_meta.received && !slot_meta.is_full() {
// We got a last shred < slot_meta.received, which signals there's an alternative,
⋮----
.put_in_batch(write_batch, slot, &true)
⋮----
if !self.should_insert_data_shred(
⋮----
self.max_root(),
⋮----
return Err(InsertDataShredError::InvalidShred);
⋮----
let completed_data_sets = self.insert_data_shred(
⋮----
index_meta.data_mut(),
⋮----
newly_completed_data_sets.extend(completed_data_sets);
⋮----
just_inserted_shreds.insert(shred.id(), shred);
⋮----
if let BTreeMapEntry::Vacant(entry) = erasure_metas.entry(erasure_set) {
if let Some(meta) = self.erasure_meta(erasure_set).unwrap() {
⋮----
Ok(())
⋮----
fn should_insert_coding_shred(shred: &Shred, max_root: Slot) -> bool {
debug_assert_matches!(shred.sanitize(), Ok(()));
shred.is_code() && shred.slot() > max_root
⋮----
fn insert_coding_shred(
⋮----
assert!(shred.is_code());
⋮----
.put_bytes_in_batch(write_batch, (slot, shred_index), shred.payload())?;
index_meta.coding_mut().insert(shred_index);
⋮----
fn is_data_shred_present(shred: &Shred, slot_meta: &SlotMeta, data_index: &ShredIndex) -> bool {
⋮----
shred_index < slot_meta.consumed || data_index.contains(shred_index)
⋮----
fn get_shred_from_just_inserted_or_db<'a>(
⋮----
let (slot, index, shred_type) = shred_id.unpack();
match (just_inserted_shreds.get(&shred_id), shred_type) {
(Some(shred), _) => Some(Cow::Borrowed(shred.payload())),
// If it doesn't exist in the just inserted set, it must exist in
⋮----
.get_data_shred(slot, u64::from(index))
.unwrap()
.map(shred::Payload::from)
.map(Cow::Owned),
⋮----
.get_coding_shred(slot, u64::from(index))
⋮----
fn check_merkle_root_consistency(
⋮----
let new_merkle_root = shred.merkle_root().ok();
if merkle_root_meta.merkle_root() == new_merkle_root {
// No conflict, either both merkle shreds with same merkle root
// or both legacy shreds with merkle_root `None`
⋮----
.get_shred_from_just_inserted_or_db(just_inserted_shreds, shred_id)
⋮----
shred.clone().into_payload(),
⋮----
duplicate_shreds.push(PossibleDuplicateShred::MerkleRootConflict(
shred.clone(),
⋮----
/// Returns true if there is no chaining conflict between
    /// the `shred` and `merkle_root_meta` of the next FEC set,
⋮----
/// the `shred` and `merkle_root_meta` of the next FEC set,
    /// or if shreds from the next set are yet to be received.
⋮----
/// or if shreds from the next set are yet to be received.
    ///
⋮----
///
    /// Otherwise return false and add duplicate proof to
⋮----
/// Otherwise return false and add duplicate proof to
    /// `duplicate_shreds`.
⋮----
/// `duplicate_shreds`.
    ///
⋮----
///
    /// This is intended to be used right after `shred`'s `erasure_meta`
⋮----
/// This is intended to be used right after `shred`'s `erasure_meta`
    fn check_forward_chained_merkle_root_consistency(
⋮----
fn check_forward_chained_merkle_root_consistency(
⋮----
debug_assert!(erasure_meta.check_coding_shred(shred));
⋮----
// If a shred from the next fec set has already been inserted, check the chaining
let Some(next_fec_set_index) = erasure_meta.next_fec_set_index() else {
error!("Invalid erasure meta, unable to compute next fec set index {erasure_meta:?}");
⋮----
.get(&next_erasure_set)
.map(WorkingEntry::as_ref)
.map(Cow::Borrowed)
.or_else(|| {
self.merkle_root_meta(next_erasure_set)
⋮----
.map(Cow::Owned)
⋮----
// No shred from the next fec set has been received
⋮----
next_merkle_root_meta.first_received_shred_index(),
next_merkle_root_meta.first_received_shred_type(),
⋮----
let merkle_root = shred.merkle_root().ok();
⋮----
if !self.check_chaining(merkle_root, chained_merkle_root) {
⋮----
if !self.has_duplicate_shreds_in_slot(shred.slot()) {
duplicate_shreds.push(PossibleDuplicateShred::ChainedMerkleRootConflict(
⋮----
/// Returns true if there is no chaining conflict between
    /// the `shred` and `merkle_root_meta` of the previous FEC set,
⋮----
/// the `shred` and `merkle_root_meta` of the previous FEC set,
    /// or if shreds from the previous set are yet to be received.
⋮----
/// or if shreds from the previous set are yet to be received.
    ///
⋮----
///
    /// This is intended to be used right after `shred`'s `merkle_root_meta`
⋮----
/// This is intended to be used right after `shred`'s `merkle_root_meta`
    fn check_backwards_chained_merkle_root_consistency(
⋮----
fn check_backwards_chained_merkle_root_consistency(
⋮----
let fec_set_index = shred.fec_set_index();
⋮----
// Although the first fec set chains to the last fec set of the parent block,
// if this chain is incorrect we do not know which block is the duplicate until votes
// are received. We instead delay this check until the block reaches duplicate
// confirmation.
⋮----
// If a shred from the previous fec set has already been inserted, check the chaining.
// Since we cannot compute the previous fec set index, we check the in memory map, otherwise
// check the previous key from blockstore to see if it is consecutive with our current set.
⋮----
.previous_erasure_set(erasure_set, erasure_metas)
.expect("Expect database operations to succeed")
⋮----
// No shreds from the previous erasure batch have been received,
// so nothing to check. Once the previous erasure batch is received,
// we will verify this chain through the forward check above.
⋮----
let chained_merkle_root = shred.chained_merkle_root().ok();
⋮----
/// Checks if the chained merkle root == merkle root
    ///
⋮----
///
    /// Returns true if no conflict, or if chained merkle roots are not enabled
⋮----
/// Returns true if no conflict, or if chained merkle roots are not enabled
    fn check_chaining(&self, merkle_root: Option<Hash>, chained_merkle_root: Option<Hash>) -> bool {
⋮----
fn check_chaining(&self, merkle_root: Option<Hash>, chained_merkle_root: Option<Hash>) -> bool {
chained_merkle_root.is_none()  // Chained merkle roots have not been enabled yet
⋮----
fn should_insert_data_shred(
⋮----
let last_in_slot = if shred.last_in_slot() {
debug!("got last in slot");
⋮----
if last_index.map(|ix| shred_index >= ix).unwrap_or_default() {
⋮----
.and_then(|leader_schedule| leader_schedule.slot_leader_at(slot, None));
⋮----
u32::try_from(last_index.unwrap()).unwrap(),
⋮----
.store_duplicate_slot(slot, ending_shred.clone(), shred.payload().clone())
.is_err()
⋮----
warn!("store duplicate error");
⋮----
duplicate_shreds.push(PossibleDuplicateShred::LastIndexConflict(
⋮----
datapoint_error!(
⋮----
u32::try_from(slot_meta.received - 1).unwrap(),
⋮----
.map(|parent_slot| verify_shred_slots(slot, parent_slot, max_root))
.unwrap_or_default()
⋮----
fn insert_data_shred<'a>(
⋮----
let index = u64::from(shred.index());
⋮----
let last_in_data = if shred.data_complete() {
debug!("got last in data");
⋮----
assert!(!slot_meta.is_orphan());
⋮----
while data_index.contains(current_index) {
⋮----
self.data_shred_cf.put_bytes_in_batch(
⋮----
shred.bytes_to_store(),
⋮----
data_index.insert(index);
let newly_completed_data_sets = update_slot_meta(
⋮----
shred.reference_tick(),
⋮----
.map(move |indices| CompletedDataSetInfo { slot, indices });
self.slots_stats.record_shred(
shred.slot(),
shred.fec_set_index(),
⋮----
Some(slot_meta),
⋮----
trace!("inserted shred into slot {slot:?} and index {index:?}");
Ok(newly_completed_data_sets)
⋮----
pub fn get_data_shred(&self, slot: Slot, index: u64) -> Result<Option<Vec<u8>>> {
let shred = self.data_shred_cf.get_bytes((slot, index))?;
let shred = shred.map(ShredData::resize_stored_shred).transpose();
shred.map_err(|err| {
let err = format!("Invalid stored shred: {err}");
⋮----
pub fn get_data_shreds_for_slot(&self, slot: Slot, start_index: u64) -> Result<Vec<Shred>> {
self.slot_data_iterator(slot, start_index)
.expect("blockstore couldn't fetch iterator")
.map(|(_, bytes)| {
Shred::new_from_serialized_shred(Vec::from(bytes)).map_err(|err| {
⋮----
format!("Could not reconstruct shred from shred payload: {err:?}"),
⋮----
.collect()
⋮----
fn get_data_shreds(
⋮----
let _lock = self.check_lowest_cleanup_slot(slot)?;
⋮----
if let Some(meta) = self.meta_cf.get(slot)? {
if !meta.is_full() {
warn!("The slot is not yet full. Will not return any shreds");
return Ok((last_index, buffer_offset));
⋮----
if let Some(shred_data) = self.get_data_shred(slot, index)? {
let shred_len = shred_data.len();
if buffer.len().saturating_sub(buffer_offset) >= shred_len {
⋮----
.copy_from_slice(&shred_data[..shred_len]);
⋮----
if buffer.len().saturating_sub(buffer_offset) < shred_len {
⋮----
Ok((last_index, buffer_offset))
⋮----
pub fn get_coding_shred(&self, slot: Slot, index: u64) -> Result<Option<Vec<u8>>> {
self.code_shred_cf.get_bytes((slot, index))
⋮----
pub fn get_coding_shreds_for_slot(
⋮----
self.slot_coding_iterator(slot, start_index)
⋮----
.map(|(_, bytes)| Shred::new_from_serialized_shred(Vec::from(bytes)))
⋮----
pub(crate) fn write_entries(
⋮----
let mut parent_slot = parent.map_or(start_slot.saturating_sub(1), |v| v);
let num_slots = (start_slot - parent_slot).max(1);
assert!(num_ticks_in_start_slot < num_slots * ticks_per_slot);
⋮----
let mut shredder = Shredder::new(current_slot, parent_slot, 0, version).unwrap();
let mut all_shreds = vec![];
let mut slot_entries = vec![];
⋮----
let mut chained_merkle_root = Hash::new_from_array(rand::rng().random());
for entry in entries.into_iter() {
⋮----
if all_shreds.is_empty() {
⋮----
.entries_to_merkle_shreds_for_tests(
⋮----
all_shreds.append(&mut data_shreds);
all_shreds.append(&mut coding_shreds);
chained_merkle_root = coding_shreds.last().unwrap().merkle_root().unwrap();
⋮----
if entry.is_tick() {
⋮----
slot_entries.push(entry);
⋮----
if !slot_entries.is_empty() {
all_shreds.extend(shredder.make_merkle_shreds_from_entries(
⋮----
let num_data = all_shreds.iter().filter(|shred| shred.is_data()).count();
self.insert_shreds(all_shreds, None, false)?;
Ok(num_data)
⋮----
pub fn get_index(&self, slot: Slot) -> Result<Option<Index>> {
self.index_cf.get(slot)
⋮----
pub fn put_meta_bytes(&self, slot: Slot, bytes: &[u8]) -> Result<()> {
self.meta_cf.put_bytes(slot, bytes)
⋮----
pub fn put_meta(&self, slot: Slot, meta: &SlotMeta) -> Result<()> {
self.put_meta_bytes(slot, &cf::SlotMeta::serialize(meta)?)
⋮----
fn find_missing_indexes<C>(
⋮----
return vec![];
⋮----
let mut missing_indexes = vec![];
⋮----
DEFAULT_TICKS_PER_SECOND * timestamp().saturating_sub(first_timestamp) / 1000;
db_iterator.seek(C::key(&(slot, start_index)));
⋮----
if !db_iterator.valid() {
let num_to_take = max_missing - missing_indexes.len();
missing_indexes.extend((prev_index..end_index).take(num_to_take));
⋮----
let (current_slot, index) = C::index(db_iterator.key().expect("Expect a valid key"));
⋮----
let data = db_iterator.value().expect("couldn't read value");
let reference_tick = u64::from(shred::layout::get_reference_tick(data).unwrap());
⋮----
missing_indexes.extend((prev_index..upper_index).take(num_to_take));
if missing_indexes.len() == max_missing
⋮----
db_iterator.next();
⋮----
pub fn find_missing_data_indexes(
⋮----
let Ok(mut db_iterator) = self.db.raw_iterator_cf(self.data_shred_cf.handle()) else {
⋮----
fn get_block_time(&self, slot: Slot) -> Result<Option<UnixTimestamp>> {
⋮----
self.blocktime_cf.get(slot)
⋮----
pub fn get_rooted_block_time(&self, slot: Slot) -> Result<UnixTimestamp> {
⋮----
if self.is_root(slot) {
⋮----
.get(slot)?
.ok_or(BlockstoreError::SlotUnavailable);
⋮----
Err(BlockstoreError::SlotNotRooted)
⋮----
pub fn set_block_time(&self, slot: Slot, timestamp: UnixTimestamp) -> Result<()> {
self.blocktime_cf.put(slot, &timestamp)
⋮----
pub fn get_block_height(&self, slot: Slot) -> Result<Option<u64>> {
⋮----
self.block_height_cf.get(slot)
⋮----
pub fn set_block_height(&self, slot: Slot, block_height: u64) -> Result<()> {
self.block_height_cf.put(slot, &block_height)
⋮----
pub fn get_first_available_block(&self) -> Result<Slot> {
let mut root_iterator = self.rooted_slot_iterator(self.lowest_slot_with_genesis())?;
let first_root = root_iterator.next().unwrap_or_default();
⋮----
return Ok(first_root);
⋮----
Ok(root_iterator.next().unwrap_or_default())
⋮----
pub fn get_rooted_block(
⋮----
return self.get_complete_block(slot, require_previous_blockhash);
⋮----
pub fn get_complete_block(
⋮----
self.do_get_complete_block_with_entries(
⋮----
.map(|result| result.block)
⋮----
pub fn get_rooted_block_with_entries(
⋮----
return self.do_get_complete_block_with_entries(
⋮----
pub fn get_complete_block_with_entries(
⋮----
fn do_get_complete_block_with_entries(
⋮----
let Some(slot_meta) = self.meta_cf.get(slot)? else {
trace!("do_get_complete_block_with_entries() failed for {slot} (missing SlotMeta)");
return Err(BlockstoreError::SlotUnavailable);
⋮----
if !slot_meta.is_full() {
trace!("do_get_complete_block_with_entries() failed for {slot} (slot not full)");
⋮----
let (slot_entries, _, _) = self.get_slot_entries_with_shred_info(
⋮----
if slot_entries.is_empty() {
trace!("do_get_complete_block_with_entries() failed for {slot} (no entries found)");
⋮----
.last()
.map(|entry| entry.hash)
.unwrap_or_else(|| panic!("Rooted slot {slot:?} must have blockhash"));
⋮----
Vec::with_capacity(slot_entries.len())
⋮----
.flat_map(|entry| {
⋮----
entries.push(solana_transaction_status::EntrySummary {
⋮----
num_transactions: entry.transactions.len() as u64,
⋮----
starting_transaction_index += entry.transactions.len();
⋮----
.map(|transaction| {
if let Err(err) = transaction.sanitize() {
⋮----
.and_then(|parent_slot| {
self.get_slot_entries_with_shred_info(
⋮----
.map(|(entries, _, _)| entries)
⋮----
.unwrap_or_default();
if parent_slot_entries.is_empty() && require_previous_blockhash {
return Err(BlockstoreError::ParentEntriesUnavailable);
⋮----
let previous_blockhash = if !parent_slot_entries.is_empty() {
get_last_hash(parent_slot_entries.iter()).unwrap()
⋮----
.into();
let block_time = self.blocktime_cf.get(slot)?;
let block_height = self.block_height_cf.get(slot)?;
⋮----
previous_blockhash: previous_blockhash.to_string(),
blockhash: blockhash.to_string(),
parent_slot: slot_meta.parent_slot.unwrap(),
transactions: self.map_transactions_to_statuses(slot, slot_transaction_iterator)?,
⋮----
Ok(VersionedConfirmedBlockWithEntries { block, entries })
⋮----
pub fn map_transactions_to_statuses(
⋮----
Ok(VersionedTransactionWithStatusMeta {
⋮----
.read_transaction_status((signature, slot))?
.ok_or(BlockstoreError::MissingTransactionMetadata)?,
⋮----
fn cleanup_old_entries(&self) -> Result<()> {
if !self.is_primary_access() {
return Ok(());
⋮----
if self.transaction_status_index_cf.get(0)?.is_none() {
⋮----
.put(0, &TransactionStatusIndexMeta::default())?;
⋮----
if self.transaction_status_index_cf.get(1)?.is_none() {
⋮----
.put(1, &TransactionStatusIndexMeta::default())?;
⋮----
.is_some()
⋮----
.delete(transaction_status_dummy_key)?;
⋮----
.get(address_signatures_dummy_key)?
⋮----
.delete(address_signatures_dummy_key)?;
⋮----
fn get_highest_primary_index_slot(&self) -> Option<Slot> {
*self.highest_primary_index_slot.read().unwrap()
⋮----
fn set_highest_primary_index_slot(&self, slot: Option<Slot>) {
*self.highest_primary_index_slot.write().unwrap() = slot;
⋮----
fn update_highest_primary_index_slot(&self) -> Result<()> {
let iterator = self.transaction_status_index_cf.iter(IteratorMode::Start)?;
⋮----
let meta: TransactionStatusIndexMeta = deserialize(&data).unwrap();
if highest_primary_index_slot.is_none()
|| highest_primary_index_slot.is_some_and(|slot| slot < meta.max_slot)
⋮----
highest_primary_index_slot = Some(meta.max_slot);
⋮----
if highest_primary_index_slot.is_some_and(|slot| slot != 0) {
self.set_highest_primary_index_slot(highest_primary_index_slot);
⋮----
self.db.set_clean_slot_0(true);
⋮----
fn maybe_cleanup_highest_primary_index_slot(&self, oldest_slot: Slot) -> Result<()> {
let mut w_highest_primary_index_slot = self.highest_primary_index_slot.write().unwrap();
⋮----
fn read_deprecated_transaction_status(
⋮----
if result.is_none() {
Ok(self
⋮----
.and_then(|meta| meta.try_into().ok()))
⋮----
Ok(result.and_then(|meta| meta.try_into().ok()))
⋮----
pub fn read_transaction_status(
⋮----
let result = self.transaction_status_cf.get_protobuf(index)?;
if result.is_none()
⋮----
.get_highest_primary_index_slot()
.is_some_and(|highest_slot| highest_slot >= index.1)
⋮----
self.read_deprecated_transaction_status(index)
⋮----
fn write_transaction_status_helper<'a, F>(
⋮----
let status = status.into();
⋮----
.map_err(|_| BlockstoreError::TransactionIndexOverflow)?;
⋮----
.put_protobuf((signature, slot), &status)?;
⋮----
write_fn(address, slot, transaction_index, signature, writeable)?;
⋮----
pub fn write_transaction_status<'a>(
⋮----
self.write_transaction_status_helper(
⋮----
self.address_signatures_cf.put(
⋮----
pub fn add_transaction_status_to_batch<'a>(
⋮----
self.address_signatures_cf.put_in_batch(
⋮----
pub fn read_transaction_memos(
⋮----
let memos = self.transaction_memos_cf.get((signature, slot))?;
if memos.is_none()
⋮----
.is_some_and(|highest_slot| highest_slot >= slot)
⋮----
.get_raw(cf::TransactionMemos::deprecated_key(signature))
⋮----
Ok(memos)
⋮----
pub fn write_transaction_memos(
⋮----
self.transaction_memos_cf.put((*signature, slot), &memos)
⋮----
pub fn add_transaction_memos_to_batch(
⋮----
.put_in_batch(db_write_batch, (*signature, slot), &memos)
⋮----
fn check_lowest_cleanup_slot(
⋮----
// lowest_cleanup_slot is the last slot that was not cleaned up by LedgerCleanupService
let lowest_cleanup_slot = self.lowest_cleanup_slot.read().unwrap();
⋮----
return Err(BlockstoreError::SlotCleanedUp);
⋮----
// Make caller hold this lock properly; otherwise LedgerCleanupService can purge/compact
// needed slots here at any given moment
Ok(lowest_cleanup_slot)
⋮----
/// Acquires the lock of `lowest_cleanup_slot` and returns the tuple of
    /// the held lock and the lowest available slot.
⋮----
/// the held lock and the lowest available slot.
    ///
⋮----
///
    /// This function ensures a consistent result by using lowest_cleanup_slot
⋮----
/// This function ensures a consistent result by using lowest_cleanup_slot
    /// as the lower bound for reading columns that do not employ strong read
⋮----
/// as the lower bound for reading columns that do not employ strong read
    /// consistency with slot-based delete_range.
⋮----
/// consistency with slot-based delete_range.
    fn ensure_lowest_cleanup_slot(&self) -> (std::sync::RwLockReadGuard<'_, Slot>, Slot) {
⋮----
fn ensure_lowest_cleanup_slot(&self) -> (std::sync::RwLockReadGuard<'_, Slot>, Slot) {
⋮----
.checked_add(1)
.expect("overflow from trusted value");
⋮----
fn get_transaction_status_with_counter(
⋮----
let (lock, _) = self.ensure_lowest_cleanup_slot();
let first_available_block = self.get_first_available_block()?;
⋮----
.iter_current_index_filtered(IteratorMode::From(
⋮----
if !self.is_root(slot) && !confirmed_unrooted_slots.contains(&slot) {
⋮----
.get_protobuf((signature, slot))?
.and_then(|status| status.try_into().ok())
.map(|status| (slot, status));
return Ok((status, counter));
⋮----
if self.get_highest_primary_index_slot().is_none() {
return Ok((None, counter));
⋮----
.iter_deprecated_index_filtered(IteratorMode::From(
⋮----
drop(lock);
Ok((None, counter))
⋮----
pub fn get_rooted_transaction_status(
⋮----
self.get_transaction_status(signature, &HashSet::default())
⋮----
pub fn get_transaction_status(
⋮----
self.get_transaction_status_with_counter(signature, confirmed_unrooted_slots)
.map(|(status, _)| status)
⋮----
pub fn get_rooted_transaction(
⋮----
self.get_transaction_with_status(signature, &HashSet::default())
⋮----
pub fn get_complete_transaction(
⋮----
let max_root = self.max_root();
⋮----
.take_while(|&slot| slot > max_root)
⋮----
self.get_transaction_with_status(signature, &confirmed_unrooted_slots)
⋮----
fn get_transaction_with_status(
⋮----
self.get_transaction_status(signature, confirmed_unrooted_slots)?
⋮----
.find_transaction_in_slot(slot, signature)?
.ok_or(BlockstoreError::TransactionStatusSlotMismatch)?;
let block_time = self.get_block_time(slot)?;
Ok(Some(ConfirmedTransactionWithStatusMeta {
⋮----
fn find_transaction_in_slot(
⋮----
let slot_entries = self.get_slot_entries(slot, 0)?;
Ok(slot_entries
⋮----
.cloned()
.flat_map(|entry| entry.transactions)
⋮----
.find(|transaction| transaction.signatures[0] == signature))
⋮----
fn find_address_signatures(
⋮----
Ok(vec![])
⋮----
fn find_address_signatures_for_slot(
⋮----
let (lock, lowest_available_slot) = self.ensure_lowest_cleanup_slot();
let mut signatures: Vec<(Slot, Signature)> = vec![];
⋮----
return Ok(signatures);
⋮----
slot.max(lowest_available_slot),
⋮----
signatures.push((slot, signature));
⋮----
Ok(signatures)
⋮----
pub fn get_confirmed_signatures_for_address(
⋮----
self.find_address_signatures(pubkey, start_slot, end_slot)
.map(|signatures| signatures.iter().map(|(_, signature)| *signature).collect())
⋮----
fn get_block_signatures_rev(&self, slot: Slot) -> Result<Vec<Signature>> {
let block = self.get_complete_block(slot, false).map_err(|err| {
BlockstoreError::Io(IoError::other(format!("Unable to get block: {err}")))
⋮----
Ok(block
⋮----
.rev()
.filter_map(|transaction_with_meta| {
⋮----
.collect())
⋮----
pub fn get_confirmed_signatures_for_address2(
⋮----
self.get_transaction_status(before, &confirmed_unrooted_slots)?;
⋮----
None => return Ok(SignatureInfosForAddress::default()),
⋮----
let mut slot_signatures = self.get_block_signatures_rev(slot)?;
if let Some(pos) = slot_signatures.iter().position(|&x| x == before) {
slot_signatures.truncate(pos + 1);
⋮----
Some(slot_signatures.into_iter().collect::<HashSet<_>>()),
⋮----
get_before_slot_timer.stop();
⋮----
self.get_transaction_status(until, &confirmed_unrooted_slots)?;
⋮----
if let Some(pos) = slot_signatures.iter().position(|&x| x == until) {
slot_signatures = slot_signatures.split_off(pos);
⋮----
(slot, slot_signatures.into_iter().collect::<HashSet<_>>())
⋮----
get_until_slot_timer.stop();
let mut address_signatures = vec![];
⋮----
let mut signatures = self.find_address_signatures_for_slot(address, slot)?;
signatures.reverse();
if let Some(excluded_signatures) = before_excluded_signatures.take() {
address_signatures.extend(
⋮----
.filter(|(_, signature)| !excluded_signatures.contains(signature)),
⋮----
address_signatures.append(&mut signatures);
⋮----
get_initial_slot_timer.stop();
⋮----
while address_signatures.len() < limit {
if let Some(((key_address, slot, _transaction_index, signature), _)) = iterator.next() {
⋮----
if self.is_root(slot) || confirmed_unrooted_slots.contains(&slot) {
address_signatures.push((slot, signature));
⋮----
address_signatures_iter_timer.stop();
⋮----
.filter(|(_, signature)| !until_excluded_signatures.contains(signature))
.take(limit);
⋮----
let mut infos = vec![];
⋮----
self.get_transaction_status(signature, &confirmed_unrooted_slots)?;
let err = transaction_status.and_then(|(_slot, status)| status.status.err());
let memo = self.read_transaction_memos(signature, slot)?;
⋮----
infos.push(ConfirmedTransactionStatusWithSignature {
⋮----
get_status_info_timer.stop();
datapoint_info!(
⋮----
Ok(SignatureInfosForAddress {
⋮----
pub fn read_rewards(&self, index: Slot) -> Result<Option<Rewards>> {
⋮----
.map(|result| result.map(|option| option.into()))
⋮----
pub fn write_rewards(&self, index: Slot, rewards: RewardsAndNumPartitions) -> Result<()> {
let rewards = rewards.into();
self.rewards_cf.put_protobuf(index, &rewards)
⋮----
pub fn get_recent_perf_samples(&self, num: usize) -> Result<Vec<(Slot, PerfSample)>> {
⋮----
.take(num)
.map(|(slot, data)| {
⋮----
.map(|sample| (slot, sample.into()))
.or_else(|err| {
⋮----
if matches!(io_err.kind(), ErrorKind::UnexpectedEof) =>
⋮----
_ => return Err(err),
⋮----
deserialize::<PerfSampleV1>(&data).map(|sample| (slot, sample.into()))
⋮----
.map_err(Into::into)
⋮----
samples.collect()
⋮----
pub fn write_perf_sample(&self, index: Slot, perf_sample: &PerfSampleV2) -> Result<()> {
⋮----
serialize(&perf_sample).expect("`PerfSampleV2` can be serialized with `bincode`");
self.perf_samples_cf.put_bytes(index, &bytes)
⋮----
pub fn get_slot_entries(&self, slot: Slot, shred_start_index: u64) -> Result<Vec<Entry>> {
self.get_slot_entries_with_shred_info(slot, shred_start_index, false)
.map(|x| x.0)
⋮----
pub fn get_slot_entries_with_shred_info(
⋮----
let (completed_ranges, slot_meta) = self.get_completed_ranges(slot, start_index)?;
if self.is_dead(slot) && !allow_dead_slots {
return Err(BlockstoreError::DeadSlot);
} else if completed_ranges.is_empty() {
return Ok((vec![], 0, false));
⋮----
let slot_meta = slot_meta.unwrap();
⋮----
.map(|&Range { end, .. }| u64::from(end) - start_index)
⋮----
let entries = self.get_slot_entries_in_block(slot, completed_ranges, Some(&slot_meta))?;
Ok((entries, num_shreds, slot_meta.is_full()))
⋮----
pub fn get_accounts_used_in_range(
⋮----
fn add_to_set<'a>(set: &DashSet<Pubkey>, iter: impl IntoIterator<Item = &'a Pubkey>) {
iter.into_iter().for_each(|key| {
set.insert(*key);
⋮----
.into_par_iter()
.for_each(|slot| {
if let Ok(entries) = self.get_slot_entries(slot, 0) {
entries.into_par_iter().for_each(|entry| {
entry.transactions.into_iter().for_each(|tx| {
if let Some(lookups) = tx.message.address_table_lookups() {
add_to_set(
⋮----
lookups.iter().map(|lookup| &lookup.account_key),
⋮----
if let Ok(tx) = bank.fully_verify_transaction(tx.clone()) {
add_to_set(&result, tx.message().account_keys().iter());
⋮----
add_to_set(&result, tx.message.static_account_keys());
⋮----
.expect("transaction failed to sanitize");
let alt_scan_extensions = scan_transaction(&tx);
add_to_set(&result, &alt_scan_extensions.accounts);
⋮----
possible_cpi_alt_extend.store(true, Ordering::Relaxed);
⋮----
lookup_tables.into_par_iter().for_each(|lookup_table_key| {
bank.get_account(&lookup_table_key)
.map(|lookup_table_account| {
add_to_set(&result, &[lookup_table_key]);
AddressLookupTable::deserialize(lookup_table_account.data()).map(|t| {
add_to_set(&result, &t.addresses[..]);
⋮----
(result, possible_cpi_alt_extend.into_inner())
⋮----
fn get_completed_ranges(
⋮----
return Ok((vec![], None));
⋮----
Ok((completed_ranges, Some(slot_meta)))
⋮----
fn get_completed_data_ranges(
⋮----
assert!(!completed_data_indexes.contains(&consumed));
⋮----
.range(start_index..consumed)
.scan(start_index, |start, index| {
⋮----
Some(out)
⋮----
fn get_slot_entries_in_block(
⋮----
debug_assert!(completed_ranges
⋮----
if slot > self.lowest_cleanup_slot() {
panic!("Missing shred. slot: {slot}, index: {index}, slot meta: {slot_meta:?}");
⋮----
completed_ranges.first().zip(completed_ranges.last())
⋮----
return Ok(vec![]);
⋮----
let keys = indices.clone().map(|index| (slot, index));
let keys = self.data_shred_cf.multi_get_keys(keys);
⋮----
.multi_get_bytes(&keys)
.zip(indices)
.map(|(shred, index)| {
shred?.ok_or_else(|| {
maybe_panic(index);
⋮----
.map(|Range { start, end }| end - start)
.map(|num_shreds| {
⋮----
.by_ref()
.take(num_shreds as usize)
.process_results(|shreds| Shredder::deshred(shreds))?
.map_err(|e| {
⋮----
format!("could not reconstruct entries buffer from shreds: {e:?}"),
⋮----
.and_then(|payload| {
wincode::deserialize::<Vec<Entry>>(&payload).map_err(|e| {
⋮----
format!("could not reconstruct entries: {e:?}"),
⋮----
.flatten_ok()
⋮----
pub fn get_entries_in_data_block(
⋮----
self.get_slot_entries_in_block(slot, vec![range], slot_meta)
⋮----
pub fn check_last_fec_set_and_get_block_id(
⋮----
let results = self.check_last_fec_set(slot);
⋮----
if results.last_fec_set_merkle_root.is_none() {
datapoint_warn!("incomplete_final_fec_set", ("slot", slot, i64),);
⋮----
results.get_last_fec_set_merkle_root(feature_set)
⋮----
fn check_last_fec_set(&self, slot: Slot) -> Result<LastFECSetCheckResults> {
let slot_meta = self.meta(slot)?.ok_or(BlockstoreError::SlotUnavailable)?;
⋮----
.ok_or(BlockstoreError::UnknownLastIndex(slot))?;
⋮----
const_assert_eq!(MINIMUM_INDEX, 31);
let Some(start_index) = last_shred_index.checked_sub(MINIMUM_INDEX) else {
⋮----
return Ok(LastFECSetCheckResults {
⋮----
.multi_get_keys((start_index..=last_shred_index).map(|index| (slot, index)));
⋮----
.enumerate()
.map(|(offset, shred_bytes)| {
let shred_bytes = shred_bytes.ok().flatten().ok_or_else(|| {
let shred_index = start_index + u64::try_from(offset).unwrap();
warn!("Missing shred for {slot} index {shred_index}");
⋮----
shred::layout::is_retransmitter_signed_variant(&shred_bytes).map_err(|_| {
⋮----
warn!("Found legacy shred for {slot}, index {shred_index}");
⋮----
shred::layout::get_merkle_root(&shred_bytes).ok_or_else(|| {
⋮----
warn!("Unable to read merkle root for {slot}, index {shred_index}");
⋮----
Ok((merkle_root, is_retransmitter_signed))
⋮----
.dedup_by(|res1, res2| res1.as_ref().ok() == res2.as_ref().ok())
⋮----
let &[(block_id, is_retransmitter_signed)] = deduped_shred_checks.as_slice() else {
⋮----
Ok(LastFECSetCheckResults {
last_fec_set_merkle_root: Some(block_id),
⋮----
pub fn get_slots_since(&self, slots: &[Slot]) -> Result<HashMap<Slot, Vec<Slot>>> {
let keys = self.meta_cf.multi_get_keys(slots.iter().copied());
let slot_metas = self.meta_cf.multi_get(&keys);
let mut slots_since: HashMap<Slot, Vec<Slot>> = HashMap::with_capacity(slots.len());
for meta in slot_metas.into_iter() {
⋮----
slots_since.insert(meta.slot, meta.next_slots);
⋮----
Ok(slots_since)
⋮----
pub fn is_root(&self, slot: Slot) -> bool {
matches!(self.roots_cf.get(slot), Ok(Some(true)))
⋮----
pub fn is_skipped(&self, slot: Slot) -> bool {
⋮----
.rooted_slot_iterator(0)
⋮----
.and_then(|mut iter| iter.next())
⋮----
match self.roots_cf.get(slot).ok().flatten() {
⋮----
None => slot < self.max_root() && slot > lowest_root,
⋮----
pub fn insert_bank_hash(&self, slot: Slot, frozen_hash: Hash, is_duplicate_confirmed: bool) {
if let Some(prev_value) = self.bank_hash_cf.get(slot).unwrap() {
if prev_value.frozen_hash() == frozen_hash && prev_value.is_duplicate_confirmed() {
⋮----
self.bank_hash_cf.put(slot, &data).unwrap()
⋮----
pub fn get_bank_hash(&self, slot: Slot) -> Option<Hash> {
⋮----
.get(slot)
⋮----
.map(|versioned| versioned.frozen_hash())
⋮----
pub fn is_duplicate_confirmed(&self, slot: Slot) -> bool {
⋮----
.map(|versioned| versioned.is_duplicate_confirmed())
.unwrap_or(false)
⋮----
pub fn insert_optimistic_slot(
⋮----
self.optimistic_slots_cf.put(slot, &slot_data)
⋮----
pub fn get_optimistic_slot(&self, slot: Slot) -> Result<Option<(Hash, UnixTimestamp)>> {
⋮----
.map(|meta| (meta.hash(), meta.timestamp())))
⋮----
pub fn get_latest_optimistic_slots(
⋮----
let iter = self.reversed_optimistic_slots_iterator()?;
Ok(iter.take(num).collect())
⋮----
pub fn set_duplicate_confirmed_slots_and_hashes(
⋮----
let mut write_batch = self.get_write_batch()?;
⋮----
.put_in_batch(&mut write_batch, slot, &data)?;
⋮----
self.write_batch(write_batch)?;
⋮----
pub fn set_roots<'a>(&self, rooted_slots: impl Iterator<Item = &'a Slot>) -> Result<()> {
⋮----
self.roots_cf.put_in_batch(&mut write_batch, *slot, &true)?;
⋮----
.fetch_max(max_new_rooted_slot, Ordering::Relaxed);
⋮----
pub fn mark_slots_as_if_rooted_normally_at_startup(
⋮----
self.set_roots(slots.iter().map(|(slot, _hash)| slot))?;
⋮----
self.set_duplicate_confirmed_slots_and_hashes(
⋮----
.map(|(slot, maybe_hash)| (slot, maybe_hash.unwrap())),
⋮----
pub fn is_dead(&self, slot: Slot) -> bool {
matches!(
⋮----
pub fn set_dead_slot(&self, slot: Slot) -> Result<()> {
self.dead_slots_cf.put(slot, &true)
⋮----
pub fn remove_dead_slot(&self, slot: Slot) -> Result<()> {
self.dead_slots_cf.delete(slot)
⋮----
pub fn remove_slot_duplicate_proof(&self, slot: Slot) -> Result<()> {
self.duplicate_slots_cf.delete(slot)
⋮----
pub fn get_first_duplicate_proof(&self) -> Option<(Slot, DuplicateSlotProof)> {
⋮----
.iter(IteratorMode::From(0, IteratorDirection::Forward))
⋮----
iter.next()
.map(|(slot, proof_bytes)| (slot, deserialize(&proof_bytes).unwrap()))
⋮----
pub fn store_duplicate_slot<S, T>(&self, slot: Slot, shred1: S, shred2: T) -> Result<()>
⋮----
self.duplicate_slots_cf.put(slot, &duplicate_slot_proof)
⋮----
pub fn get_duplicate_slot(&self, slot: u64) -> Option<DuplicateSlotProof> {
⋮----
.expect("fetch from DuplicateSlots column family failed")
⋮----
pub fn is_shred_duplicate(&self, shred: &Shred) -> Option<Vec<u8>> {
let (slot, index, shred_type) = shred.id().unpack();
⋮----
ShredType::Data => self.get_data_shred(slot, u64::from(index)),
ShredType::Code => self.get_coding_shred(slot, u64::from(index)),
⋮----
.expect("fetch from DuplicateSlots column family failed")?;
if let Ok(signature) = shred.retransmitter_signature() {
⋮----
error!("set retransmitter signature failed: {err:?}");
⋮----
(other != **shred.payload()).then_some(other)
⋮----
pub fn has_duplicate_shreds_in_slot(&self, slot: Slot) -> bool {
⋮----
pub fn orphans_iterator(&self, slot: Slot) -> Result<impl Iterator<Item = u64> + '_> {
⋮----
Ok(orphans_iter.map(|(slot, _)| slot))
⋮----
pub fn dead_slots_iterator(&self, slot: Slot) -> Result<impl Iterator<Item = Slot> + '_> {
⋮----
Ok(dead_slots_iterator.map(|(slot, _)| slot))
⋮----
pub fn duplicate_slots_iterator(&self, slot: Slot) -> Result<impl Iterator<Item = Slot> + '_> {
⋮----
Ok(duplicate_slots_iterator.map(|(slot, _)| slot))
⋮----
pub fn has_existing_shreds_for_slot(&self, slot: Slot) -> bool {
match self.meta(slot).unwrap() {
⋮----
/// Returns the max root or 0 if it does not exist
    pub fn max_root(&self) -> Slot {
⋮----
pub fn max_root(&self) -> Slot {
self.max_root.load(Ordering::Relaxed)
⋮----
// find the first available slot in blockstore that has some data in it
pub fn lowest_slot(&self) -> Slot {
⋮----
.slot_meta_iterator(0)
.expect("unable to iterate over meta")
⋮----
// This means blockstore is empty, should never get here aside from right at boot.
self.max_root()
⋮----
fn lowest_slot_with_genesis(&self) -> Slot {
⋮----
/// Returns the highest available slot in the blockstore
    pub fn highest_slot(&self) -> Result<Option<Slot>> {
⋮----
pub fn highest_slot(&self) -> Result<Option<Slot>> {
⋮----
.map(|(slot, _)| slot);
Ok(highest_slot)
⋮----
pub fn lowest_cleanup_slot(&self) -> Slot {
*self.lowest_cleanup_slot.read().unwrap()
⋮----
pub fn storage_size(&self) -> Result<u64> {
self.db.storage_size()
⋮----
/// Returns the total physical storage size contributed by all data shreds.
    ///
⋮----
///
    /// Note that the reported size does not include those recently inserted
⋮----
/// Note that the reported size does not include those recently inserted
    /// shreds that are still in memory.
⋮----
/// shreds that are still in memory.
    pub fn total_data_shred_storage_size(&self) -> Result<i64> {
⋮----
pub fn total_data_shred_storage_size(&self) -> Result<i64> {
⋮----
.get_int_property(RocksProperties::TOTAL_SST_FILES_SIZE)
⋮----
/// Returns the total physical storage size contributed by all coding shreds.
    ///
⋮----
/// shreds that are still in memory.
    pub fn total_coding_shred_storage_size(&self) -> Result<i64> {
⋮----
pub fn total_coding_shred_storage_size(&self) -> Result<i64> {
⋮----
/// Returns whether the blockstore has primary (read and write) access
    pub fn is_primary_access(&self) -> bool {
⋮----
pub fn is_primary_access(&self) -> bool {
self.db.is_primary_access()
⋮----
/// Scan for any ancestors of the supplied `start_root` that are not
    /// marked as roots themselves. Mark any found slots as roots since
⋮----
/// marked as roots themselves. Mark any found slots as roots since
    /// the ancestor of a root is also inherently a root. Returns the
⋮----
/// the ancestor of a root is also inherently a root. Returns the
    /// number of slots that were actually updated.
⋮----
/// number of slots that were actually updated.
    ///
⋮----
///
    /// Arguments:
⋮----
/// Arguments:
    ///  - `start_root`: The root to start scan from, or the highest root in
⋮----
///  - `start_root`: The root to start scan from, or the highest root in
    ///    the blockstore if this value is `None`. This slot must be a root.
⋮----
///    the blockstore if this value is `None`. This slot must be a root.
    ///  - `end_slot``: The slot to stop the scan at; the scan will continue to
⋮----
///  - `end_slot``: The slot to stop the scan at; the scan will continue to
    ///    the earliest slot in the Blockstore if this value is `None`.
⋮----
///    the earliest slot in the Blockstore if this value is `None`.
    ///  - `exit`: Exit early if this flag is set to `true`.
⋮----
///  - `exit`: Exit early if this flag is set to `true`.
    pub fn scan_and_fix_roots(
⋮----
pub fn scan_and_fix_roots(
⋮----
// Hold the lowest_cleanup_slot read lock to prevent any cleaning of
// the blockstore from another thread. Doing so will prevent a
// possible inconsistency across column families where a slot is:
//  - Identified as needing root repair by this thread
//  - Cleaned from the blockstore by another thread (LedgerCleanupSerivce)
//  - Marked as root via Self::set_root() by this this thread
⋮----
if !self.is_root(slot) {
return Err(BlockstoreError::SlotNotRooted);
⋮----
let end_slot = end_slot.unwrap_or(*lowest_cleanup_slot);
⋮----
AncestorIterator::new(start_root, self).take_while(|&slot| slot >= end_slot);
⋮----
let mut roots_to_fix = vec![];
for slot in ancestor_iterator.filter(|slot| !self.is_root(*slot)) {
if exit.load(Ordering::Relaxed) {
return Ok(0);
⋮----
roots_to_fix.push(slot);
⋮----
find_missing_roots.stop();
⋮----
if !roots_to_fix.is_empty() {
info!("{} slots to be rooted", roots_to_fix.len());
⋮----
for (i, chunk) in roots_to_fix.chunks(chunk_size).enumerate() {
⋮----
return Ok(i * chunk_size);
⋮----
trace!("{chunk:?}");
self.set_roots(chunk.iter())?;
⋮----
debug!("No missing roots found in range {start_root} to {end_slot}");
⋮----
fix_roots.stop();
⋮----
Ok(roots_to_fix.len())
⋮----
/// Mark a root `slot` as connected, traverse `slot`'s children and update
    pub fn set_and_chain_connected_on_root_and_next_slots(&self, root: Slot) -> Result<()> {
⋮----
pub fn set_and_chain_connected_on_root_and_next_slots(&self, root: Slot) -> Result<()> {
⋮----
.meta(root)?
.unwrap_or_else(|| SlotMeta::new(root, None));
if root_meta.is_connected() {
⋮----
info!("Marking slot {root} and any full children slots as connected");
⋮----
root_meta.set_parent_connected();
root_meta.set_connected();
⋮----
.put_in_batch(&mut write_batch, root_meta.slot, &root_meta)?;
⋮----
while !next_slots.is_empty() {
let slot = next_slots.pop_front().unwrap();
let mut meta = self.meta(slot)?.unwrap_or_else(|| {
panic!("Slot {slot} is a child but has no SlotMeta in blockstore")
⋮----
if meta.set_parent_connected() {
next_slots.extend(meta.next_slots.iter());
⋮----
.put_in_batch(&mut write_batch, meta.slot, &meta)?;
⋮----
fn handle_chaining(
⋮----
working_set.retain(|_, entry| entry.did_insert_occur);
⋮----
let working_set_slots: Vec<_> = working_set.keys().collect();
⋮----
self.handle_chaining_for_slot(write_batch, working_set, &mut new_chained_slots, *slot)?;
⋮----
for (slot, meta) in new_chained_slots.iter() {
⋮----
self.meta_cf.put_in_batch(write_batch, *slot, meta)?;
⋮----
metrics.chaining_elapsed_us += start.as_us();
⋮----
fn handle_chaining_for_slot(
⋮----
.get(&slot)
.expect("Slot must exist in the working_set hashmap");
⋮----
let mut meta_mut = meta.borrow_mut();
⋮----
meta_backup.is_some() && meta_backup.as_ref().unwrap().is_orphan();
if slot != 0 && meta_mut.parent_slot.is_some() {
let prev_slot = meta_mut.parent_slot.unwrap();
if meta_backup.is_none() || was_orphan_slot {
⋮----
self.find_slot_meta_else_create(working_set, new_chained_slots, prev_slot)?;
chain_new_slot_to_prev_slot(
&mut prev_slot_meta.borrow_mut(),
⋮----
if RefCell::borrow(&*prev_slot_meta).is_orphan() {
⋮----
.put_in_batch(write_batch, prev_slot, &true)?;
⋮----
self.orphans_cf.delete_in_batch(write_batch, slot)?;
⋮----
is_newly_completed_slot(&RefCell::borrow(meta), meta_backup)
&& RefCell::borrow(meta).is_parent_connected();
⋮----
meta.borrow_mut().set_connected();
self.traverse_children_mut(
⋮----
fn traverse_children_mut<F>(
⋮----
let slot_meta = slot_meta.borrow();
let mut next_slots: VecDeque<u64> = slot_meta.next_slots.to_vec().into();
⋮----
self.find_slot_meta_else_create(working_set, passed_visisted_slots, slot)?;
let mut meta = meta_ref.borrow_mut();
if slot_function(&mut meta) {
⋮----
.for_each(|slot| next_slots.push_back(*slot));
⋮----
fn commit_slot_meta_working_set(
⋮----
let mut newly_completed_slots = vec![];
let completed_slots_senders = self.completed_slots_senders.lock().unwrap();
for (slot, slot_meta_entry) in slot_meta_working_set.iter() {
assert!(slot_meta_entry.did_insert_occur);
⋮----
if !completed_slots_senders.is_empty() && is_newly_completed_slot(meta, meta_backup) {
newly_completed_slots.push(*slot);
⋮----
if Some(meta) != meta_backup.as_ref() {
should_signal = should_signal || slot_has_updates(meta, meta_backup);
⋮----
fn get_slot_meta_entry<'a>(
⋮----
// Check if we've already inserted the slot metadata for this shred's slot
slot_meta_working_set.entry(slot).or_insert_with(|| {
⋮----
let backup = Some(meta.clone());
if meta.is_orphan() {
meta.parent_slot = Some(parent_slot);
⋮----
Rc::new(RefCell::new(SlotMeta::new(slot, Some(parent_slot)))),
⋮----
fn find_slot_meta_else_create<'a>(
⋮----
let result = find_slot_meta_in_cached_state(working_set, chained_slots, slot_index);
⋮----
Ok(slot)
⋮----
self.find_slot_meta_in_db_else_create(slot_index, chained_slots)
⋮----
/// A helper function to [`find_slot_meta_else_create`] that searches the
    /// `SlotMeta` based on the specified `slot` in `db` and updates `insert_map`.
⋮----
/// `SlotMeta` based on the specified `slot` in `db` and updates `insert_map`.
    ///
⋮----
///
    /// If the specified `db` does not contain a matched entry, then it will create
⋮----
/// If the specified `db` does not contain a matched entry, then it will create
    /// a dummy orphan slot in the database.
⋮----
/// a dummy orphan slot in the database.
    fn find_slot_meta_in_db_else_create(
⋮----
fn find_slot_meta_in_db_else_create(
⋮----
if let Some(slot_meta) = self.meta_cf.get(slot)? {
insert_map.insert(slot, Rc::new(RefCell::new(slot_meta)));
⋮----
// If this slot doesn't exist, make a orphan slot. This way we
insert_map.insert(slot, Rc::new(RefCell::new(SlotMeta::new_orphan(slot))));
⋮----
Ok(insert_map.get(&slot).unwrap().clone())
⋮----
fn get_index_meta_entry<'a>(
⋮----
let res = index_working_set.entry(slot).or_insert_with(|| {
⋮----
.unwrap_or_else(|| Index::new(slot));
⋮----
*index_meta_time_us += total_start.as_us();
⋮----
pub fn get_write_batch(&self) -> Result<WriteBatch> {
self.db.batch()
⋮----
pub fn write_batch(&self, write_batch: WriteBatch) -> Result<()> {
self.db.write(write_batch)
⋮----
// Updates the `completed_data_indexes` with a new shred `new_shred_index`.
// If a data set is complete, returns the range of shred indexes
//     start_index..end_index
// for that completed data set.
fn update_completed_data_indexes<'a>(
⋮----
// Shreds indices which are marked data complete.
⋮----
// new_shred_index is data complete, so need to insert here into
// the completed_data_indexes.
⋮----
completed_data_indexes.insert(new_shred_index);
⋮----
// Consecutive entries i, j, k in this array represent potential ranges
// [i, j), [j, k) that could be completed data ranges
⋮----
.range(..new_shred_index)
.next_back()
.map(|index| index + 1)
.or(Some(0u32)),
is_last_in_data.then(|| new_shred_index + 1),
⋮----
.range(new_shred_index + 1..)
⋮----
.map(|index| index + 1),
⋮----
.tuple_windows()
.filter(|&(start, end)| {
⋮----
received_data_shreds.range(bounds.clone()).eq(bounds)
⋮----
.map(|(start, end)| start..end)
⋮----
fn update_slot_meta<'a>(
⋮----
// Index is zero-indexed, while the "received" height starts from 1,
// so received = index + 1 for the same shred.
⋮----
// predict the timestamp of what would have been the first shred in this slot
⋮----
slot_meta.first_shred_timestamp = timestamp() - slot_time_elapsed;
⋮----
// If the last index in the slot hasn't been set before, then
if is_last_in_slot && slot_meta.last_index.is_none() {
slot_meta.last_index = Some(u64::from(index));
⋮----
update_completed_data_indexes(
⋮----
fn get_last_hash<'a>(iterator: impl Iterator<Item = &'a Entry> + 'a) -> Option<Hash> {
iterator.last().map(|entry| entry.hash)
⋮----
fn send_signals(
⋮----
match signal.try_send(true) {
⋮----
trace!("replay wake up signal channel is full.")
⋮----
trace!("replay wake up signal channel is disconnected.")
⋮----
if !completed_slots_senders.is_empty() && !newly_completed_slots.is_empty() {
let mut slots: Vec<_> = (0..completed_slots_senders.len() - 1)
.map(|_| newly_completed_slots.clone())
⋮----
slots.push(newly_completed_slots);
for (signal, slots) in completed_slots_senders.iter().zip(slots.into_iter()) {
let res = signal.try_send(slots);
⋮----
/// Returns the `SlotMeta` of the specified `slot` from the two cached states:
/// `working_set` and `chained_slots`.  If both contain the `SlotMeta`, then
⋮----
/// `working_set` and `chained_slots`.  If both contain the `SlotMeta`, then
/// the latest one from the `working_set` will be returned.
⋮----
/// the latest one from the `working_set` will be returned.
fn find_slot_meta_in_cached_state<'a>(
⋮----
fn find_slot_meta_in_cached_state<'a>(
⋮----
if let Some(entry) = working_set.get(&slot) {
Some(entry.new_slot_meta.clone())
⋮----
chained_slots.get(&slot).cloned()
⋮----
fn chain_new_slot_to_prev_slot(
⋮----
prev_slot_meta.next_slots.push(current_slot);
if prev_slot_meta.is_connected() {
current_slot_meta.set_parent_connected();
⋮----
fn is_newly_completed_slot(slot_meta: &SlotMeta, backup_slot_meta: &Option<SlotMeta>) -> bool {
slot_meta.is_full()
&& (backup_slot_meta.is_none()
|| slot_meta.consumed != backup_slot_meta.as_ref().unwrap().consumed)
⋮----
fn slot_has_updates(slot_meta: &SlotMeta, slot_meta_backup: &Option<SlotMeta>) -> bool {
slot_meta.is_parent_connected() &&
((slot_meta_backup.is_none() && slot_meta.consumed != 0) ||
(slot_meta_backup.is_some() && slot_meta_backup.as_ref().unwrap().consumed != slot_meta.consumed))
⋮----
pub fn create_new_ledger(
⋮----
genesis_config.write(ledger_path)?;
⋮----
column_options: column_options.clone(),
⋮----
let hashes_per_tick = genesis_config.poh_config.hashes_per_tick.unwrap_or(0);
let entries = create_ticks(ticks_per_slot, hashes_per_tick, genesis_config.hash());
let last_hash = entries.last().unwrap().hash;
⋮----
let chained_merkle_root = genesis_config.hash();
let shredder = Shredder::new(0, 0, 0, version).unwrap();
let (shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
assert!(shreds.last().unwrap().last_in_slot());
blockstore.insert_shreds(shreds, None, false)?;
blockstore.set_roots(std::iter::once(&0))?;
drop(blockstore);
let archive_path = ledger_path.join(DEFAULT_GENESIS_ARCHIVE);
⋮----
archive.append_path_with_name(ledger_path.join(DEFAULT_GENESIS_FILE), DEFAULT_GENESIS_FILE)?;
archive.append_dir_all(blockstore_dir, ledger_path.join(blockstore_dir))?;
archive.into_inner()?;
⋮----
let temp_dir = tempfile::tempdir_in(ledger_path).unwrap();
let unpack_check = unpack_genesis_archive(
⋮----
temp_dir.path(),
⋮----
ledger_path.join(DEFAULT_GENESIS_ARCHIVE),
ledger_path.join(format!("{DEFAULT_GENESIS_ARCHIVE}.failed")),
⋮----
.unwrap_or_else(|e| {
let _ = write!(
⋮----
ledger_path.join(DEFAULT_GENESIS_FILE),
ledger_path.join(format!("{DEFAULT_GENESIS_FILE}.failed")),
⋮----
ledger_path.join(blockstore_dir),
ledger_path.join(format!("{blockstore_dir}.failed")),
⋮----
return Err(BlockstoreError::Io(IoError::other(format!(
⋮----
Ok(last_hash)
⋮----
macro_rules! tmp_ledger_name {
⋮----
macro_rules! get_tmp_ledger_path {
⋮----
macro_rules! get_tmp_ledger_path_auto_delete {
⋮----
pub fn get_ledger_path_from_name_auto_delete(name: &str) -> TempDir {
let mut path = get_ledger_path_from_name(name);
let last = path.file_name().unwrap().to_str().unwrap().to_string();
path.pop();
fs::create_dir_all(&path).unwrap();
⋮----
.prefix(&last)
.rand_bytes(0)
.tempdir_in(path)
⋮----
pub fn get_ledger_path_from_name(name: &str) -> PathBuf {
use std::env;
let out_dir = env::var("FARF_DIR").unwrap_or_else(|_| "farf".to_string());
⋮----
"ledger".to_string(),
format!("{}-{}", name, keypair.pubkey()),
⋮----
macro_rules! create_new_tmp_ledger {
⋮----
macro_rules! create_new_tmp_ledger_with_size {
⋮----
macro_rules! create_new_tmp_ledger_auto_delete {
⋮----
pub(crate) fn verify_shred_slots(slot: Slot, parent: Slot, root: Slot) -> bool {
⋮----
pub fn create_new_ledger_from_name(
⋮----
let (ledger_path, blockhash) = create_new_ledger_from_name_auto_delete(
⋮----
(ledger_path.keep(), blockhash)
⋮----
pub fn create_new_ledger_from_name_auto_delete(
⋮----
let ledger_path = get_ledger_path_from_name_auto_delete(name);
let blockhash = create_new_ledger(
ledger_path.path(),
⋮----
pub fn entries_to_test_shreds(
⋮----
.make_merkle_shreds_from_entries(
⋮----
Hash::new_from_array(rand::rng().random()),
⋮----
.filter(Shred::is_data)
⋮----
pub fn make_slot_entries(
⋮----
let entries = create_ticks(num_entries, 1, Hash::new_unique());
let shreds = entries_to_test_shreds(&entries, slot, parent_slot, true, 0);
⋮----
pub fn make_many_slot_entries(
⋮----
let mut shreds = vec![];
let mut entries = vec![];
⋮----
let (slot_shreds, slot_entries) = make_slot_entries(slot, parent_slot, entries_per_slot);
shreds.extend(slot_shreds);
entries.extend(slot_entries);
⋮----
pub fn test_all_empty_or_min(blockstore: &Blockstore, min_slot: Slot) {
⋮----
.iter(IteratorMode::Start)
⋮----
.map(|(slot, _)| slot >= min_slot)
.unwrap_or(true)
⋮----
.map(|((slot, _), _)| slot >= min_slot)
⋮----
.map(|((_, slot), _)| slot >= min_slot || slot == 0)
⋮----
.map(|((_, slot, _, _), _)| slot >= min_slot || slot == 0)
⋮----
.unwrap_or(true);
assert!(condition_met);
⋮----
pub fn make_chaining_slot_entries(
⋮----
let mut slots_shreds_and_entries = vec![];
for (i, slot) in chain.iter().enumerate() {
⋮----
let result = make_slot_entries(*slot, parent_slot, entries_per_slot);
slots_shreds_and_entries.push(result);
⋮----
pub mod tests {
⋮----
pub(crate) fn make_slot_entries_with_transactions(num_entries: u64) -> Vec<Entry> {
⋮----
vec![solana_pubkey::new_rand()],
vec![CompiledInstruction::new(1, &(), vec![0])],
⋮----
entries.push(next_entry_mut(&mut Hash::default(), 0, vec![transaction]));
let mut tick = create_ticks(1, 0, hash(&serialize(&x).unwrap()));
entries.append(&mut tick);
⋮----
fn make_and_insert_slot(blockstore: &Blockstore, slot: Slot, parent_slot: Slot) {
let (shreds, _) = make_slot_entries(
⋮----
blockstore.insert_shreds(shreds, None, true).unwrap();
let meta = blockstore.meta(slot).unwrap().unwrap();
assert_eq!(slot, meta.slot);
assert!(meta.is_full());
assert!(meta.next_slots.is_empty());
⋮----
fn test_create_new_ledger() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(mint_total);
let (ledger_path, _blockhash) = create_new_tmp_ledger_auto_delete!(&genesis_config);
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
let ticks = create_ticks(genesis_config.ticks_per_slot, 0, genesis_config.hash());
let entries = blockstore.get_slot_entries(0, 0).unwrap();
assert_eq!(ticks, entries);
assert!(Path::new(ledger_path.path())
⋮----
assert_eq!(
⋮----
std::fs::remove_file(ledger_path.path().join(DEFAULT_GENESIS_FILE)).unwrap();
⋮----
fn test_insert_get_bytes() {
let num_entries = max_ticks_per_n_shreds(1, None) + 1;
assert!(num_entries > 1);
let (mut shreds, _) = make_slot_entries(
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
let last_shred = shreds.pop().unwrap();
assert!(last_shred.index() > 0);
⋮----
.insert_shreds(vec![last_shred.clone()], None, false)
⋮----
.get_bytes((0, last_shred.index() as u64))
⋮----
let deserialized_shred = Shred::new_from_serialized_shred(serialized_shred).unwrap();
assert_eq!(last_shred, deserialized_shred);
⋮----
fn test_write_entries() {
⋮----
let mut ticks = vec![];
let mut shreds_per_slot = vec![];
⋮----
let mut new_ticks = create_ticks(ticks_per_slot, 0, Hash::default());
⋮----
.write_entries(
⋮----
Some(i.saturating_sub(1)),
⋮----
new_ticks.clone(),
⋮----
.unwrap() as u64;
shreds_per_slot.push(num_shreds);
ticks.append(&mut new_ticks);
⋮----
let meta = blockstore.meta(i).unwrap().unwrap();
⋮----
assert_eq!(meta.consumed, num_shreds);
assert_eq!(meta.received, num_shreds);
assert_eq!(meta.last_index, Some(num_shreds - 1));
⋮----
assert_eq!(meta.next_slots, vec![i + 1]);
⋮----
assert_eq!(meta.parent_slot, Some(0));
⋮----
assert_eq!(meta.parent_slot, Some(i - 1));
⋮----
fn test_put_get_simple() {
⋮----
let meta = SlotMeta::new(0, Some(1));
blockstore.meta_cf.put(0, &meta).unwrap();
⋮----
.get(0)
⋮----
.expect("Expected meta object to exist");
assert_eq!(result, meta);
let erasure = vec![1u8; 16];
⋮----
.put_bytes(erasure_key, &erasure)
⋮----
.get_bytes(erasure_key)
⋮----
.expect("Expected erasure object to exist");
assert_eq!(result, erasure);
let data = vec![2u8; 16];
⋮----
blockstore.data_shred_cf.put_bytes(data_key, &data).unwrap();
⋮----
.get_bytes(data_key)
⋮----
.expect("Expected data object to exist");
assert_eq!(result, data);
⋮----
fn test_multi_get() {
⋮----
let k = u64::try_from(i).unwrap();
let meta = SlotMeta::new(k, Some(k + 1));
blockstore.meta_cf.put(k, &meta).unwrap();
⋮----
.get(k)
⋮----
.multi_get_keys(0..TEST_PUT_ENTRY_COUNT as Slot);
let values = blockstore.meta_cf.multi_get(&keys);
for (i, value) in values.enumerate().take(TEST_PUT_ENTRY_COUNT) {
⋮----
fn test_read_shred_bytes() {
⋮----
let (shreds, _) = make_slot_entries(slot, 0, 100);
let num_shreds = shreds.len() as u64;
let shred_bufs: Vec<_> = shreds.iter().map(Shred::payload).cloned().collect();
⋮----
blockstore.insert_shreds(shreds, None, false).unwrap();
⋮----
let (_, bytes) = blockstore.get_data_shreds(slot, 0, 1, &mut buf).unwrap();
assert_eq!(buf[..bytes], shred_bufs[0][..bytes]);
let (last_index, bytes2) = blockstore.get_data_shreds(slot, 0, 2, &mut buf).unwrap();
assert_eq!(last_index, 1);
assert!(bytes2 > bytes);
⋮----
assert_eq!(shred_data_1, &shred_bufs[0][..bytes]);
⋮----
assert_eq!(shred_data_2, &shred_bufs[1][..bytes2 - bytes]);
⋮----
let mut buf = vec![0; bytes + 1];
let (last_index, bytes3) = blockstore.get_data_shreds(slot, 0, 2, &mut buf).unwrap();
assert_eq!(last_index, 0);
assert_eq!(bytes3, bytes);
let mut buf = vec![0; bytes2 - 1];
let (last_index, bytes4) = blockstore.get_data_shreds(slot, 0, 2, &mut buf).unwrap();
⋮----
assert_eq!(bytes4, bytes);
let mut buf = vec![0; bytes * 2];
⋮----
.get_data_shreds(slot, num_shreds - 1, num_shreds, &mut buf)
⋮----
assert_eq!(last_index, num_shreds - 1);
⋮----
assert_eq!(shred_data, &shred_bufs[(num_shreds - 1) as usize][..bytes6]);
⋮----
.get_data_shreds(slot, num_shreds, num_shreds + 2, &mut buf)
⋮----
assert_eq!(bytes6, 0);
⋮----
fn test_shred_cleanup_check() {
⋮----
assert!(blockstore.get_data_shreds(slot, 0, 1, &mut buf).is_ok());
⋮----
.run_purge(0, max_purge_slot, PurgeType::Exact)
⋮----
*blockstore.lowest_cleanup_slot.write().unwrap() = max_purge_slot;
⋮----
assert!(blockstore.get_data_shreds(slot, 0, 1, &mut buf).is_err());
⋮----
fn test_insert_data_shreds_basic() {
⋮----
let (mut shreds, entries) = make_slot_entries(
⋮----
assert!(shreds.len() > 1);
⋮----
.insert_shreds(vec![last_shred], None, false)
⋮----
assert!(blockstore.get_slot_entries(0, 0).unwrap().is_empty());
⋮----
.meta(0)
⋮----
.expect("Expected new metadata object to be created");
assert!(meta.consumed == 0 && meta.received == num_shreds);
⋮----
let result = blockstore.get_slot_entries(0, 0).unwrap();
assert_eq!(result, entries);
⋮----
.expect("Expected new metadata object to exist");
⋮----
assert!(meta.is_connected());
⋮----
fn test_insert_data_shreds_reverse() {
⋮----
let num_entries = max_ticks_per_n_shreds(num_shreds, None);
⋮----
for i in (0..num_shreds).rev() {
let shred = shreds.pop().unwrap();
blockstore.insert_shreds(vec![shred], None, false).unwrap();
⋮----
.expect("Expected metadata object to exist");
⋮----
assert_eq!(result.len(), 0);
⋮----
assert!(meta.consumed == num_shreds && meta.received == num_shreds);
⋮----
fn test_insert_slots() {
test_insert_data_shreds_slots(false);
test_insert_data_shreds_slots(true);
⋮----
fn test_index_fallback_deserialize() {
⋮----
let slot = rng.random_range(0..100);
⋮----
.reject_trailing_bytes()
.with_fixint_encoding();
let data = 0..rng.random_range(100..MAX_DATA_SHREDS_PER_SLOT as u64);
let coding = 0..rng.random_range(100..MAX_DATA_SHREDS_PER_SLOT as u64);
⋮----
for (d, c) in data.clone().zip(coding.clone()) {
fallback.data_mut().insert(d);
fallback.coding_mut().insert(c);
⋮----
.put_bytes(slot, &bincode.serialize(&fallback).unwrap())
⋮----
let current = blockstore.index_cf.get(slot).unwrap().unwrap();
for (d, c) in data.zip(coding) {
assert!(current.data().contains(d));
assert!(current.coding().contains(c));
⋮----
fn test_get_slot_entries1() {
⋮----
let entries = create_ticks(8, 0, Hash::default());
let shreds = entries_to_test_shreds(&entries[0..4], 1, 0, false, 0);
⋮----
.insert_shreds(shreds, None, false)
.expect("Expected successful write of shreds");
⋮----
fn test_get_slot_entries3() {
⋮----
wincode::serialized_size(&create_ticks(1, 0, Hash::default())).unwrap();
⋮----
let entries = create_ticks(entries_per_slot, 0, Hash::default());
let shreds = entries_to_test_shreds(&entries, slot, slot.saturating_sub(1), false, 0);
assert!(shreds.len() as u64 >= shreds_per_slot);
⋮----
assert_eq!(blockstore.get_slot_entries(slot, 0).unwrap(), entries);
⋮----
fn test_insert_data_shreds_consecutive() {
⋮----
let min_entries = max_ticks_per_n_shreds(1, None) + 1;
⋮----
let (shreds, original_entries) = make_slot_entries(slot, parent_slot, num_entries);
⋮----
assert!(num_shreds > 1);
let mut even_shreds = vec![];
let mut odd_shreds = vec![];
for (i, shred) in shreds.into_iter().enumerate() {
⋮----
even_shreds.push(shred);
⋮----
odd_shreds.push(shred);
⋮----
blockstore.insert_shreds(odd_shreds, None, false).unwrap();
assert_eq!(blockstore.get_slot_entries(slot, 0).unwrap(), vec![]);
⋮----
if num_shreds.is_multiple_of(2) {
⋮----
trace!("got here");
assert_eq!(meta.received, num_shreds - 1);
⋮----
assert_eq!(meta.consumed, 0);
⋮----
assert_eq!(meta.last_index, None);
⋮----
blockstore.insert_shreds(even_shreds, None, false).unwrap();
⋮----
assert_eq!(meta.parent_slot, Some(parent_slot));
⋮----
fn test_data_set_completed_on_insert() {
⋮----
Blockstore::open_with_signal(ledger_path.path(), BlockstoreOptions::default()).unwrap();
⋮----
let entries = create_ticks(num_entries, slot, Hash::default());
let shreds = entries_to_test_shreds(&entries, slot, 0, true, 0);
let num_shreds = shreds.len();
⋮----
assert!(blockstore
⋮----
fn test_new_shreds_signal() {
⋮----
} = Blockstore::open_with_signal(ledger_path.path(), BlockstoreOptions::default()).unwrap();
⋮----
let shreds_per_slot = shreds.len() as u64;
⋮----
.insert_shreds(vec![shreds.remove(1)], None, false)
⋮----
assert!(recvr.recv_timeout(timer).is_err());
⋮----
.insert_shreds(vec![shreds.remove(0)], None, false)
⋮----
assert!(recvr.recv_timeout(timer).is_ok());
assert!(recvr.try_recv().is_err());
⋮----
let mut missing_shreds = vec![];
⋮----
let (mut slot_shreds, _) = make_slot_entries(
⋮----
let missing_shred = slot_shreds.remove(slot as usize - 1);
⋮----
missing_shreds.push(missing_shred);
⋮----
.drain((num_slots / 2) as usize..)
.collect_vec();
⋮----
.insert_shreds(missing_shreds, None, false)
⋮----
.insert_shreds(missing_shreds2, None, false)
⋮----
fn test_completed_shreds_signal() {
⋮----
let (mut shreds, _) = make_slot_entries(0, 0, entries_per_slot);
let shred0 = shreds.remove(0);
⋮----
blockstore.insert_shreds(vec![shred0], None, false).unwrap();
assert_eq!(recvr.try_recv().unwrap(), vec![0]);
⋮----
fn test_completed_shreds_signal_orphans() {
⋮----
let mut all_shreds = make_chaining_slot_entries(&slots[..], entries_per_slot, 0);
let (mut orphan_child, _) = all_shreds.remove(2);
let (mut orphan_shreds, _) = all_shreds.remove(1);
let orphan_child0 = orphan_child.remove(0);
blockstore.insert_shreds(orphan_child, None, false).unwrap();
⋮----
.insert_shreds(vec![orphan_child0], None, false)
⋮----
assert_eq!(recvr.try_recv().unwrap(), vec![slots[2]]);
let orphan_shred0 = orphan_shreds.remove(0);
⋮----
.insert_shreds(orphan_shreds, None, false)
⋮----
.insert_shreds(vec![orphan_shred0], None, false)
⋮----
assert_eq!(recvr.try_recv().unwrap(), vec![slots[1]]);
⋮----
fn test_completed_shreds_signal_many() {
⋮----
let mut slots = vec![2, 5, 10];
⋮----
let (shreds0, _) = all_shreds.remove(0);
let (shreds1, _) = all_shreds.remove(0);
let (shreds2, _) = all_shreds.remove(0);
let (shreds3, _) = make_slot_entries(
⋮----
let mut all_shreds: Vec<_> = vec![shreds0, shreds1, shreds2, shreds3]
⋮----
all_shreds.shuffle(&mut rng());
blockstore.insert_shreds(all_shreds, None, false).unwrap();
let mut result = recvr.try_recv().unwrap();
result.sort_unstable();
slots.push(disconnected_slot);
slots.sort_unstable();
assert_eq!(result, slots);
⋮----
fn test_handle_chaining_basic() {
⋮----
let (mut shreds, _) = make_many_slot_entries(0, num_slots, entries_per_slot);
let shreds_per_slot = shreds.len() / num_slots as usize;
⋮----
.drain(shreds_per_slot..2 * shreds_per_slot)
⋮----
blockstore.insert_shreds(shreds1, None, false).unwrap();
let meta1 = blockstore.meta(1).unwrap().unwrap();
assert!(meta1.next_slots.is_empty());
assert!(!meta1.is_connected());
assert_eq!(meta1.parent_slot, Some(0));
assert_eq!(meta1.last_index, Some(shreds_per_slot as u64 - 1));
⋮----
blockstore.insert_shreds(shreds2, None, false).unwrap();
let meta2 = blockstore.meta(2).unwrap().unwrap();
assert!(meta2.next_slots.is_empty());
assert!(!meta2.is_connected());
assert_eq!(meta2.parent_slot, Some(1));
assert_eq!(meta2.last_index, Some(shreds_per_slot as u64 - 1));
⋮----
assert_eq!(meta1.next_slots, vec![2]);
⋮----
assert_eq!(meta.next_slots, vec![slot + 1]);
⋮----
assert_eq!(meta.parent_slot, Some(slot - 1));
⋮----
assert_eq!(meta.last_index, Some(shreds_per_slot as u64 - 1));
⋮----
fn test_handle_chaining_missing_slots() {
⋮----
let (shreds, _) = make_many_slot_entries(0, num_slots, entries_per_slot);
let shreds_per_slot = shreds.len() as u64 / num_slots;
⋮----
shreds.into_iter().partition(|shred| shred.slot() % 2 == 0);
blockstore.insert_shreds(odd_slots, None, false).unwrap();
⋮----
assert_eq!(meta.parent_slot, None);
⋮----
assert!(!meta.is_connected());
assert!(!meta.is_parent_connected() || slot == 0);
⋮----
blockstore.insert_shreds(even_slots, None, false).unwrap();
⋮----
assert_eq!(meta.last_index, Some(shreds_per_slot - 1));
⋮----
pub fn test_forward_chaining_is_connected() {
⋮----
let entries_per_slot = max_ticks_per_n_shreds(1, None) + 1;
assert!(entries_per_slot > 1);
⋮----
assert!(shreds_per_slot > 1);
⋮----
let mut shreds_for_slot = shreds.drain(..shreds_per_slot).collect_vec();
⋮----
let shred0 = shreds_for_slot.remove(0);
missing_shreds.push(shred0);
⋮----
.insert_shreds(shreds_for_slot, None, false)
⋮----
let shred = missing_shreds.remove(0);
⋮----
fn test_scan_and_fix_roots() {
fn blockstore_roots(blockstore: &Blockstore) -> Vec<Slot> {
⋮----
let entries_per_slot = max_ticks_per_n_shreds(5, None);
⋮----
.flat_map(|slot| {
⋮----
slot.saturating_sub(2)
⋮----
slot.saturating_sub(1)
⋮----
let (shreds, _) = make_slot_entries(slot, parent_slot, entries_per_slot);
shreds.into_iter()
⋮----
let (start, end) = (Some(16), None);
assert_matches!(
⋮----
let new_roots = vec![6, 12];
blockstore.set_roots(new_roots.iter()).unwrap();
assert_eq!(&new_roots, &blockstore_roots(&blockstore));
let (start, end) = (Some(12), Some(8));
let roots = vec![6, 8, 10, 12];
⋮----
.scan_and_fix_roots(start, end, &AtomicBool::new(false))
⋮----
assert_eq!(&roots, &blockstore_roots(&blockstore));
let (start, end) = (None, Some(4));
let roots = vec![4, 6, 8, 10, 12];
⋮----
let (start, end) = (Some(12), None);
let roots = vec![0, 2, 4, 6, 8, 10, 12];
⋮----
let roots = vec![0, 2, 4, 6, 8, 10, 12, 16];
⋮----
let roots = vec![0, 2, 4, 6, 8, 10, 12, 14, 16];
⋮----
fn test_set_and_chain_connected_on_root_and_next_slots() {
⋮----
.set_and_chain_connected_on_root_and_next_slots(start_slot)
⋮----
let slot_meta5 = blockstore.meta(start_slot).unwrap().unwrap();
assert!(!slot_meta5.is_full());
assert!(slot_meta5.is_parent_connected());
assert!(slot_meta5.is_connected());
⋮----
let (shreds, _) = make_many_slot_entries(start_slot, num_slots, entries_per_slot);
⋮----
info!("Evaluating slot {slot}");
⋮----
assert!(meta.is_parent_connected());
⋮----
.partition(|shred| shred.slot() != non_full_slot || shred.index() == 0);
⋮----
assert!(!meta.is_parent_connected());
⋮----
match slot.cmp(&non_full_slot) {
⋮----
fn test_slot_range_connected_chain() {
⋮----
make_and_insert_slot(&blockstore, slot, slot.saturating_sub(1));
⋮----
assert!(blockstore.slot_range_connected(1, 3));
assert!(!blockstore.slot_range_connected(1, 4));
⋮----
fn test_slot_range_connected_disconnected() {
⋮----
make_and_insert_slot(&blockstore, 1, 0);
make_and_insert_slot(&blockstore, 2, 1);
make_and_insert_slot(&blockstore, 4, 2);
⋮----
assert!(blockstore.slot_range_connected(1, 4));
⋮----
fn test_slot_range_connected_same_slot() {
⋮----
assert!(blockstore.slot_range_connected(54, 54));
⋮----
fn test_slot_range_connected_starting_slot_not_full() {
⋮----
make_and_insert_slot(&blockstore, 5, 4);
make_and_insert_slot(&blockstore, 6, 5);
assert!(!blockstore.meta(4).unwrap().unwrap().is_full());
assert!(blockstore.slot_range_connected(4, 6));
⋮----
fn test_get_slots_since() {
⋮----
assert!(blockstore.get_slots_since(&[0]).unwrap().is_empty());
let mut meta0 = SlotMeta::new(0, Some(0));
blockstore.meta_cf.put(0, &meta0).unwrap();
let expected: HashMap<u64, Vec<u64>> = vec![(0, vec![])].into_iter().collect();
assert_eq!(blockstore.get_slots_since(&[0]).unwrap(), expected);
meta0.next_slots = vec![1, 2];
⋮----
let expected: HashMap<u64, Vec<u64>> = vec![(0, vec![1, 2])].into_iter().collect();
⋮----
assert_eq!(blockstore.get_slots_since(&[0, 1]).unwrap(), expected);
let mut meta3 = SlotMeta::new(3, Some(1));
meta3.next_slots = vec![10, 5];
blockstore.meta_cf.put(3, &meta3).unwrap();
let expected: HashMap<u64, Vec<u64>> = vec![(0, vec![1, 2]), (3, vec![10, 5])]
⋮----
assert_eq!(blockstore.get_slots_since(&[0, 1, 3]).unwrap(), expected);
⋮----
fn test_orphans() {
⋮----
let (mut shreds, _) = make_many_slot_entries(0, 3, entries_per_slot);
let shreds_per_slot = shreds.len() / 3;
let shreds_for_slot = shreds.drain((shreds_per_slot * 2)..).collect_vec();
⋮----
.meta(1)
⋮----
assert!(meta.is_orphan());
⋮----
let shreds_for_slot = shreds.drain(shreds_per_slot..).collect_vec();
⋮----
assert!(!meta.is_orphan());
⋮----
let (shred4, _) = make_slot_entries(4, 0, 1);
let (shred5, _) = make_slot_entries(5, 1, 1);
blockstore.insert_shreds(shred4, None, false).unwrap();
blockstore.insert_shreds(shred5, None, false).unwrap();
⋮----
.meta(i)
⋮----
assert!(blockstore.orphans_cf.is_empty().unwrap());
⋮----
fn test_insert_data_shreds_slots(should_bulk_write: bool) {
⋮----
let parent_slot = slot.saturating_sub(1);
let (slot_shreds, entry) = make_slot_entries(slot, parent_slot, 1);
⋮----
entries.extend(entry);
⋮----
let shred = shreds.remove(0);
⋮----
assert_eq!(meta.received, DATA_SHREDS_PER_FEC_BLOCK as u64);
assert_eq!(meta.last_index, Some(DATA_SHREDS_PER_FEC_BLOCK as u64 - 1));
assert_eq!(meta.parent_slot, Some(i.saturating_sub(1)));
assert_eq!(meta.consumed, DATA_SHREDS_PER_FEC_BLOCK as u64);
⋮----
fn test_find_missing_data_indexes() {
⋮----
assert!(gap > 3);
let entries = create_ticks(1, 0, Hash::default());
let mut shreds = entries_to_test_shreds(&entries, slot, 0, true, 0);
shreds.retain(|s| (s.index() % gap as u32) == 0);
⋮----
shreds.truncate(num_shreds);
⋮----
let expected: Vec<u64> = (1..gap).collect();
⋮----
let mut expected: Vec<u64> = (1..gap).collect();
expected.push(gap + 1);
⋮----
.flat_map(|k| {
⋮----
fn test_find_missing_data_indexes_timeout() {
⋮----
.map(|i| {
let shredder = Shredder::new(slot, slot - 1, i as u8, 42).unwrap();
⋮----
.make_shreds_from_data_slice(
⋮----
shreds.next().unwrap()
⋮----
let empty: Vec<u64> = vec![];
⋮----
let expected: Vec<_> = (1..=9).collect();
⋮----
fn test_find_missing_data_indexes_sanity() {
⋮----
let entries = create_ticks(100, 0, Hash::default());
⋮----
assert!(shreds.len() > OTHER as usize);
let shreds = vec![shreds.remove(OTHER as usize), shreds.remove(ONE as usize)];
⋮----
let result = blockstore.find_missing_data_indexes(
⋮----
let expected: Vec<u64> = (start..END).filter(|i| *i != ONE && *i != OTHER).collect();
assert_eq!(result, expected);
⋮----
fn test_no_missing_shred_indexes() {
⋮----
let entries = create_ticks(num_entries, 0, Hash::default());
⋮----
fn test_verify_shred_slots() {
assert!(verify_shred_slots(0, 0, 0));
assert!(verify_shred_slots(2, 1, 0));
assert!(verify_shred_slots(2, 1, 1));
assert!(!verify_shred_slots(2, 3, 0));
assert!(!verify_shred_slots(2, 2, 0));
assert!(!verify_shred_slots(2, 3, 3));
assert!(!verify_shred_slots(2, 2, 2));
assert!(!verify_shred_slots(2, 1, 3));
assert!(!verify_shred_slots(2, 3, 4));
assert!(!verify_shred_slots(2, 2, 3));
⋮----
fn test_should_insert_data_shred() {
⋮----
let entries = create_ticks(2000, 1, Hash::new_unique());
let shredder = Shredder::new(0, 0, 1, 0).unwrap();
⋮----
assert!(
⋮----
.insert_shreds(shreds[0..5].to_vec(), None, false)
⋮----
let slot_meta = blockstore.meta(0).unwrap().unwrap();
⋮----
let terminator_shred = terminator.last().unwrap().clone();
assert!(terminator_shred.last_in_slot());
assert!(blockstore.should_insert_data_shred(
⋮----
let term_last_idx = terminator.last().unwrap().index() as usize;
⋮----
.insert_shreds(
shreds[term_last_idx + 2..term_last_idx + 3].iter().cloned(),
⋮----
assert_eq!(slot_meta.received, term_last_idx as u64 + 3);
let mut duplicate_shreds = vec![];
⋮----
assert!(blockstore.has_duplicate_shreds_in_slot(0));
assert_eq!(duplicate_shreds.len(), 1);
⋮----
assert_eq!(duplicate_shreds[0].slot(), 0);
let last_idx = shreds.last().unwrap().index();
⋮----
duplicate_shreds.clear();
blockstore.duplicate_slots_cf.delete(0).unwrap();
assert!(!blockstore.has_duplicate_shreds_in_slot(0));
⋮----
fn test_is_data_shred_present() {
let (shreds, _) = make_slot_entries(0, 0, 200);
⋮----
let index = index_cf.get(0).unwrap().unwrap();
assert_eq!(slot_meta.consumed, 5);
assert!(Blockstore::is_data_shred_present(
⋮----
.insert_shreds(shreds[6..7].to_vec(), None, false)
⋮----
fn test_merkle_root_metas_coding() {
⋮----
let (_, coding_shreds, _) = setup_erasure_shreds(slot, parent_slot, 10);
let coding_shred = coding_shreds[index as usize].clone();
⋮----
ShredInsertionTracker::new(coding_shreds.len(), blockstore.get_write_batch().unwrap());
assert!(blockstore.check_insert_coding_shred(
⋮----
assert_eq!(merkle_root_metas.len(), 1);
⋮----
.put(erasure_set.store_key(), working_merkle_root_meta.as_ref())
⋮----
blockstore.write_batch(write_batch).unwrap();
⋮----
let new_coding_shred = coding_shreds[(index + 1) as usize].clone();
⋮----
assert!(!blockstore.check_insert_coding_shred(
⋮----
PossibleDuplicateShred::MerkleRootConflict(shred, _) if shred.slot() == slot => (),
_ => panic!("No merkle root conflict"),
⋮----
setup_erasure_shreds_with_index(slot, parent_slot, 10, new_index);
let new_coding_shred = coding_shreds[0].clone();
⋮----
assert_eq!(merkle_root_metas.len(), 2);
⋮----
fn test_merkle_root_metas_data() {
⋮----
setup_erasure_shreds_with_index(slot, parent_slot, 10, fec_set_index);
let data_shred = data_shreds[0].clone();
⋮----
ShredInsertionTracker::new(data_shreds.len(), blockstore.get_write_batch().unwrap());
⋮----
.check_insert_data_shred(
⋮----
let new_data_shred = data_shreds[1].clone();
⋮----
blockstore.db.write(write_batch).unwrap();
assert!(blockstore.is_dead(slot));
blockstore.remove_dead_slot(slot).unwrap();
⋮----
let shredder = Shredder::new(slot, slot.saturating_sub(1), 0, 0).unwrap();
⋮----
ShredInsertionTracker::new(data_shreds.len(), blockstore.db.batch().unwrap());
⋮----
fn test_check_insert_coding_shred() {
⋮----
setup_erasure_shreds_with_index_and_chained_merkle_and_last_in_slot(
⋮----
let coding_shred = code_shreds[0].clone();
⋮----
ShredInsertionTracker::new(1, blockstore.get_write_batch().unwrap());
⋮----
fn test_should_insert_coding_shred() {
⋮----
.insert_shreds(vec![coding_shred.clone()], None, false)
.expect("Insertion should succeed");
⋮----
fn test_insert_multiple_is_last() {
⋮----
let (shreds, _) = make_slot_entries(0, 0, 18);
⋮----
assert_eq!(slot_meta.consumed, num_shreds);
assert_eq!(slot_meta.received, num_shreds);
assert_eq!(slot_meta.last_index, Some(num_shreds - 1));
assert!(slot_meta.is_full());
let (shreds, _) = make_slot_entries(0, 0, 600);
assert!(shreds.len() > num_shreds as usize);
⋮----
fn test_slot_data_iterator() {
⋮----
let slots = vec![2, 4, 8, 12];
let all_shreds = make_chaining_slot_entries(&slots, shreds_per_slot, 0);
let slot_8_shreds = all_shreds[2].0.clone();
⋮----
blockstore.insert_shreds(slot_shreds, None, false).unwrap();
⋮----
let shred_iter = blockstore.slot_data_iterator(5, 0).unwrap();
let result: Vec<_> = shred_iter.collect();
assert_eq!(result, vec![]);
let shred_iter = blockstore.slot_data_iterator(8, 0).unwrap();
⋮----
.filter_map(|(_, bytes)| Shred::new_from_serialized_shred(bytes.to_vec()).ok())
⋮----
assert_eq!(result.len(), slot_8_shreds.len());
assert_eq!(result, slot_8_shreds);
⋮----
fn test_set_roots() {
⋮----
let chained_slots = vec![0, 2, 4, 7, 12, 15];
assert_eq!(blockstore.max_root(), 0);
blockstore.set_roots(chained_slots.iter()).unwrap();
assert_eq!(blockstore.max_root(), 15);
⋮----
assert!(blockstore.is_root(i));
⋮----
fn test_is_skipped() {
⋮----
blockstore.set_roots(roots.iter()).unwrap();
⋮----
if i < 2 || roots.contains(&i) || i > 15 {
assert!(!blockstore.is_skipped(i));
⋮----
assert!(blockstore.is_skipped(i));
⋮----
fn test_iter_bounds() {
⋮----
.slot_meta_iterator(5)
⋮----
.for_each(|_| panic!());
⋮----
fn test_get_completed_data_ranges() {
let completed_data_end_indexes = [2, 4, 9, 11].iter().copied().collect();
⋮----
let completed_data_end_indexes: Vec<_> = completed_data_end_indexes.into_iter().collect();
for i in 0..completed_data_end_indexes.len() {
for j in i..completed_data_end_indexes.len() {
⋮----
.chain(
⋮----
.windows(2)
.map(|end_indexes| end_indexes[0] + 1..end_indexes[1] + 1),
⋮----
completed_data_end_indexes.iter().copied().collect();
⋮----
fn test_get_slot_entries_with_shred_count_corruption() {
⋮----
let entries = create_ticks(num_ticks, 0, Hash::default());
⋮----
let shreds = entries_to_test_shreds(&entries, slot, 0, false, 0);
let next_shred_index = shreds.len();
⋮----
.take(DATA_SHREDS_PER_FEC_BLOCK)
⋮----
assert!(blockstore.get_slot_entries(slot, 0).is_err());
⋮----
fn test_no_insert_but_modify_slot_meta() {
let (shreds0, _) = make_slot_entries(0, 0, 200);
⋮----
.insert_shreds(shreds0[0..5].to_vec(), None, false)
⋮----
let (mut shreds2, _) = make_slot_entries(2, 0, 200);
let (mut shreds3, _) = make_slot_entries(3, 0, 200);
shreds2.push(shreds0[1].clone());
shreds3.insert(0, shreds0[1].clone());
⋮----
assert_eq!(slot_meta.next_slots, vec![2]);
blockstore.insert_shreds(shreds3, None, false).unwrap();
⋮----
assert_eq!(slot_meta.next_slots, vec![2, 3]);
⋮----
fn test_trusted_insert_shreds() {
⋮----
let (shreds1, _) = make_slot_entries(1, 0, 1);
⋮----
blockstore.set_roots(std::iter::once(&max_root)).unwrap();
⋮----
.insert_shreds(shreds1[..].to_vec(), None, false)
⋮----
assert!(blockstore.get_data_shred(1, 0).unwrap().is_none());
⋮----
.insert_shreds(shreds1[..].to_vec(), None, true)
⋮----
assert!(blockstore.get_data_shred(1, 0).unwrap().is_some());
⋮----
fn test_get_first_available_block() {
⋮----
assert_eq!(blockstore.get_first_available_block().unwrap(), 0);
assert_eq!(blockstore.lowest_slot_with_genesis(), 0);
assert_eq!(blockstore.lowest_slot(), 0);
⋮----
let entries = make_slot_entries_with_transactions(100);
⋮----
blockstore.set_roots([slot].iter()).unwrap();
⋮----
assert_eq!(blockstore.lowest_slot(), 1);
blockstore.purge_slots(0, 1, PurgeType::CompactionFilter);
assert_eq!(blockstore.get_first_available_block().unwrap(), 3);
assert_eq!(blockstore.lowest_slot_with_genesis(), 2);
assert_eq!(blockstore.lowest_slot(), 2);
⋮----
fn test_get_rooted_block() {
⋮----
let blockhash = get_last_hash(entries.iter()).unwrap();
⋮----
let more_shreds = entries_to_test_shreds(
⋮----
let unrooted_shreds = entries_to_test_shreds(
⋮----
blockstore.insert_shreds(more_shreds, None, false).unwrap();
⋮----
.insert_shreds(unrooted_shreds, None, false)
⋮----
.set_roots([slot - 1, slot, slot + 1].iter())
⋮----
blockstore.put_meta(slot - 1, &parent_meta).unwrap();
⋮----
.filter(|entry| !entry.is_tick())
⋮----
let mut pre_balances: Vec<u64> = vec![];
let mut post_balances: Vec<u64> = vec![];
for i in 0..transaction.message.static_account_keys().len() {
pre_balances.push(i as u64 * 10);
post_balances.push(i as u64 * 11);
⋮----
let compute_units_consumed = Some(12345);
let cost_units = Some(6789);
⋮----
status: Ok(()),
⋮----
pre_balances: pre_balances.clone(),
post_balances: post_balances.clone(),
inner_instructions: Some(vec![]),
log_messages: Some(vec![]),
pre_token_balances: Some(vec![]),
post_token_balances: Some(vec![]),
rewards: Some(vec![]),
⋮----
return_data: Some(TransactionReturnData::default()),
⋮----
.put_protobuf((signature, slot), &status)
⋮----
.put_protobuf((signature, slot + 1), &status)
⋮----
.put_protobuf((signature, slot + 2), &status)
⋮----
let confirmed_block = blockstore.get_rooted_block(slot, false).unwrap();
assert_eq!(confirmed_block.transactions.len(), 100);
⋮----
transactions: expected_transactions.clone(),
⋮----
previous_blockhash: Hash::default().to_string(),
rewards: vec![],
⋮----
assert_eq!(confirmed_block, expected_block);
let confirmed_block = blockstore.get_rooted_block(slot + 1, true).unwrap();
⋮----
previous_blockhash: blockhash.to_string(),
⋮----
let not_root = blockstore.get_rooted_block(slot + 2, true).unwrap_err();
assert_matches!(not_root, BlockstoreError::SlotNotRooted);
let complete_block = blockstore.get_complete_block(slot + 2, true).unwrap();
assert_eq!(complete_block.transactions.len(), 100);
⋮----
assert_eq!(complete_block, expected_complete_block);
⋮----
blockstore.blocktime_cf.put(slot + 1, &timestamp).unwrap();
expected_block.block_time = Some(timestamp);
⋮----
.put(slot + 1, &block_height)
⋮----
expected_block.block_height = Some(block_height);
⋮----
blockstore.blocktime_cf.put(slot + 2, &timestamp).unwrap();
expected_complete_block.block_time = Some(timestamp);
⋮----
.put(slot + 2, &block_height)
⋮----
expected_complete_block.block_height = Some(block_height);
⋮----
fn test_persist_transaction_status() {
⋮----
let pre_balances_vec = vec![1, 2, 3];
let post_balances_vec = vec![3, 2, 1];
let inner_instructions_vec = vec![InnerInstructions {
⋮----
let log_messages_vec = vec![String::from("Test message\n")];
let pre_token_balances_vec = vec![];
let post_token_balances_vec = vec![];
let rewards_vec = vec![];
⋮----
writable: vec![Pubkey::new_unique()],
readonly: vec![Pubkey::new_unique()],
⋮----
data: vec![1, 2, 3],
⋮----
let compute_units_consumed_1 = Some(3812649u64);
let cost_units_1 = Some(1234);
let compute_units_consumed_2 = Some(42u64);
let cost_units_2 = Some(5678);
assert!(transaction_status_cf
⋮----
pre_balances: pre_balances_vec.clone(),
post_balances: post_balances_vec.clone(),
inner_instructions: Some(inner_instructions_vec.clone()),
log_messages: Some(log_messages_vec.clone()),
pre_token_balances: Some(pre_token_balances_vec.clone()),
post_token_balances: Some(post_token_balances_vec.clone()),
rewards: Some(rewards_vec.clone()),
loaded_addresses: test_loaded_addresses.clone(),
return_data: Some(test_return_data.clone()),
⋮----
.get_protobuf((Signature::default(), 0))
⋮----
.try_into()
⋮----
assert_eq!(status, Err(TransactionError::AccountNotFound));
assert_eq!(fee, 5u64);
assert_eq!(pre_balances, pre_balances_vec);
assert_eq!(post_balances, post_balances_vec);
assert_eq!(inner_instructions.unwrap(), inner_instructions_vec);
assert_eq!(log_messages.unwrap(), log_messages_vec);
assert_eq!(pre_token_balances.unwrap(), pre_token_balances_vec);
assert_eq!(post_token_balances.unwrap(), post_token_balances_vec);
assert_eq!(rewards.unwrap(), rewards_vec);
assert_eq!(loaded_addresses, test_loaded_addresses);
assert_eq!(return_data.unwrap(), test_return_data);
assert_eq!(compute_units_consumed, compute_units_consumed_1);
assert_eq!(cost_units, cost_units_1);
⋮----
.get_protobuf((Signature::from([2u8; 64]), 9))
⋮----
assert_eq!(status, Ok(()));
assert_eq!(fee, 9u64);
⋮----
assert_eq!(compute_units_consumed, compute_units_consumed_2);
assert_eq!(cost_units, cost_units_2);
⋮----
fn test_read_transaction_status_with_old_data() {
⋮----
.write_deprecated_transaction_status(
⋮----
vec![&Pubkey::new_unique()],
⋮----
.write_transaction_status(
⋮----
vec![
⋮----
.into_iter(),
⋮----
.read_transaction_status((signature, slot))
⋮----
assert_eq!(meta.fee, slot * 1000);
⋮----
.read_transaction_status((signature, index0_slot))
⋮----
assert_eq!(meta.fee, index0_slot * 1000);
⋮----
.read_transaction_status((signature, index1_slot))
⋮----
assert_eq!(meta.fee, index1_slot * 1000);
⋮----
fn test_get_transaction_status() {
⋮----
compute_units_consumed: Some(42u64),
cost_units: Some(1234),
⋮----
let meta0 = SlotMeta::new(0, Some(0));
⋮----
let meta1 = SlotMeta::new(1, Some(0));
blockstore.meta_cf.put(1, &meta1).unwrap();
let meta2 = SlotMeta::new(2, Some(0));
blockstore.meta_cf.put(2, &meta2).unwrap();
let meta3 = SlotMeta::new(3, Some(2));
⋮----
blockstore.set_roots([0, 2].iter()).unwrap();
⋮----
.put_protobuf((signature2, 1), &status)
⋮----
.put_protobuf((signature2, 2), &status)
⋮----
.put_protobuf((signature4, 1), &status)
⋮----
.put_protobuf((signature5, 1), &status)
⋮----
.put_protobuf((signature5, 3), &status)
⋮----
.put_protobuf((signature6, 1), &status)
⋮----
.put_protobuf((signature5, 5), &status)
⋮----
.put_protobuf((signature6, 3), &status)
⋮----
.get_transaction_status_with_counter(signature2, &[].into())
⋮----
assert_eq!(slot, 2);
assert_eq!(counter, 2);
⋮----
.get_transaction_status_with_counter(signature2, &[3].into())
⋮----
.get_transaction_status_with_counter(signature4, &[].into())
⋮----
assert_eq!(status, None);
⋮----
.get_transaction_status_with_counter(signature4, &[3].into())
⋮----
.get_transaction_status_with_counter(signature5, &[].into())
⋮----
assert_eq!(counter, 4);
⋮----
.get_transaction_status_with_counter(signature5, &[3].into())
⋮----
assert_eq!(slot, 3);
⋮----
.get_transaction_status_with_counter(signature1, &[].into())
⋮----
assert_eq!(counter, 1);
⋮----
.get_transaction_status_with_counter(signature1, &[3].into())
⋮----
.get_transaction_status_with_counter(signature3, &[].into())
⋮----
.get_transaction_status_with_counter(signature3, &[3].into())
⋮----
.get_transaction_status_with_counter(signature7, &[].into())
⋮----
assert_eq!(counter, 0);
⋮----
.get_transaction_status_with_counter(signature7, &[3].into())
⋮----
fn test_get_transaction_status_with_old_data() {
⋮----
let meta4 = SlotMeta::new(4, Some(2));
blockstore.meta_cf.put(4, &meta4).unwrap();
let meta5 = SlotMeta::new(5, Some(4));
blockstore.meta_cf.put(5, &meta5).unwrap();
blockstore.set_roots([0, 2, 4].iter()).unwrap();
⋮----
.put_deprecated_protobuf((1, signature1, 1), &status)
⋮----
.put_deprecated_protobuf((1, signature1, 2), &status)
⋮----
.put_deprecated_protobuf((0, signature2, 3), &status)
⋮----
.put_deprecated_protobuf((0, signature2, 4), &status)
⋮----
blockstore.set_highest_primary_index_slot(Some(4));
⋮----
.put_protobuf((signature3, 4), &status)
⋮----
.put_protobuf((signature4, 5), &status)
⋮----
assert_eq!(slot, 4);
assert_eq!(counter, 3);
⋮----
.get_transaction_status_with_counter(signature6, &[].into())
⋮----
fn do_test_lowest_cleanup_slot_and_special_cfs(simulate_blockstore_cleanup_service: bool) {
⋮----
let meta2 = SlotMeta::new(2, Some(1));
⋮----
blockstore.set_roots([0, 1, 2, 3].iter()).unwrap();
⋮----
.put_protobuf((signature1, lowest_cleanup_slot), &status)
⋮----
.put_protobuf((signature2, lowest_available_slot), &status)
⋮----
vec![(&address0, true)].into_iter(),
⋮----
vec![(&address1, true)].into_iter(),
⋮----
.is_none(),
⋮----
.find_address_signatures_for_slot(address0, lowest_cleanup_slot)
⋮----
.is_empty(),
⋮----
.is_some(),
⋮----
.find_address_signatures_for_slot(address1, lowest_available_slot)
⋮----
assert_eq!(are_existing_always, (true, true));
⋮----
let are_missing = check_for_missing();
assert_eq!(are_missing, (false, false));
assert_existing_always();
⋮----
*blockstore.lowest_cleanup_slot.write().unwrap() = lowest_cleanup_slot;
blockstore.purge_slots(0, lowest_cleanup_slot, PurgeType::CompactionFilter);
⋮----
assert_eq!(are_missing, (true, true));
⋮----
fn test_lowest_cleanup_slot_and_special_cfs_with_blockstore_cleanup_service_simulation() {
do_test_lowest_cleanup_slot_and_special_cfs(true);
⋮----
fn test_lowest_cleanup_slot_and_special_cfs_without_blockstore_cleanup_service_simulation() {
do_test_lowest_cleanup_slot_and_special_cfs(false);
⋮----
fn test_get_rooted_transaction() {
⋮----
let entries = make_slot_entries_with_transactions(5);
⋮----
blockstore.set_roots([slot - 1, slot].iter()).unwrap();
⋮----
let inner_instructions = Some(vec![InnerInstructions {
⋮----
let log_messages = Some(vec![String::from("Test message\n")]);
let pre_token_balances = Some(vec![]);
let post_token_balances = Some(vec![]);
let rewards = Some(vec![]);
⋮----
let return_data = Some(TransactionReturnData {
⋮----
inner_instructions: inner_instructions.clone(),
log_messages: log_messages.clone(),
pre_token_balances: pre_token_balances.clone(),
post_token_balances: post_token_balances.clone(),
rewards: rewards.clone(),
⋮----
return_data: return_data.clone(),
compute_units_consumed: Some(42),
⋮----
for tx_with_meta in expected_transactions.clone() {
⋮----
.run_purge(0, slot, PurgeType::CompactionFilter)
⋮----
*blockstore.lowest_cleanup_slot.write().unwrap() = slot;
⋮----
assert_eq!(blockstore.get_rooted_transaction(signature).unwrap(), None);
⋮----
fn test_get_complete_transaction() {
⋮----
assert_eq!(blockstore.get_rooted_transaction(signature).unwrap(), None,);
⋮----
fn test_empty_transaction_status() {
⋮----
blockstore.set_roots(std::iter::once(&0)).unwrap();
⋮----
pub(crate) fn write_deprecated_transaction_status(
⋮----
.put_deprecated_protobuf((primary_index, signature, slot), &status)?;
⋮----
self.address_signatures_cf.put_deprecated(
⋮----
if w_highest_primary_index_slot.is_none()
|| w_highest_primary_index_slot.is_some_and(|highest_slot| highest_slot < slot)
⋮----
*w_highest_primary_index_slot = Some(slot);
⋮----
fn test_find_address_signatures_for_slot() {
⋮----
vec![(&address0, true), (&address1, false)].into_iter(),
⋮----
blockstore.set_roots(std::iter::once(&slot1)).unwrap();
⋮----
.find_address_signatures_for_slot(address0, 1)
⋮----
for (i, (slot, signature)) in slot1_signatures.iter().enumerate() {
assert_eq!(*slot, slot1);
assert_eq!(*signature, Signature::from([i as u8 + 1; 64]));
⋮----
.find_address_signatures_for_slot(address0, 2)
⋮----
for (i, (slot, signature)) in slot2_signatures.iter().enumerate() {
assert_eq!(*slot, slot2);
assert_eq!(*signature, Signature::from([i as u8 + 5; 64]));
⋮----
.find_address_signatures_for_slot(address0, 3)
⋮----
for (i, (slot, signature)) in slot3_signatures.iter().enumerate() {
assert_eq!(*slot, slot3);
assert_eq!(*signature, Signature::from([i as u8 + 9; 64]));
⋮----
fn test_get_confirmed_signatures_for_address2() {
⋮----
let (shreds, _) = make_slot_entries(1, 0, 4);
⋮----
fn make_slot_entries_with_transaction_addresses(addresses: &[Pubkey]) -> Vec<Entry> {
⋮----
let mut tick = create_ticks(1, 0, hash(&serialize(address).unwrap()));
⋮----
let entries = make_slot_entries_with_transaction_addresses(&[
⋮----
assert_eq!(transaction.signatures.len(), 1);
⋮----
.static_account_keys()
⋮----
.map(|key| (key, true)),
⋮----
let shreds = entries_to_test_shreds(&entries, slot, 8, true, 0);
⋮----
blockstore.set_roots([1, 2, 4, 5, 6, 7, 8].iter()).unwrap();
⋮----
.get_confirmed_signatures_for_address2(
⋮----
assert!(sig_infos.found_before);
⋮----
assert_eq!(all0.len(), 12);
⋮----
assert_eq!(all1.len(), 12);
for i in 0..all0.len() {
⋮----
Some(all0[i - 1].signature)
⋮----
assert_eq!(results.len(), 1);
assert_eq!(results[0], all0[i], "Unexpected result for {i}");
⋮----
if i == all0.len() - 1 || i == all0.len() {
⋮----
Some(all0[i + 1].signature)
⋮----
Some(all0[all0.len() - 1].signature),
⋮----
assert!(sig_infos.infos.is_empty());
⋮----
assert!(all0.len().is_multiple_of(3));
for i in (0..all0.len()).step_by(3) {
⋮----
assert_eq!(results.len(), 3);
assert_eq!(results[0], all0[i]);
assert_eq!(results[1], all0[i + 1]);
assert_eq!(results[2], all0[i + 2]);
⋮----
for i in (0..all1.len()).step_by(2) {
⋮----
Some(all1[i - 1].signature)
⋮----
assert_eq!(results.len(), 2);
assert_eq!(results[0].slot, results[1].slot);
assert_eq!(results[0], all1[i]);
assert_eq!(results[1], all1[i + 1]);
⋮----
Some(all1[0].signature),
⋮----
assert!(!results.is_empty());
⋮----
Some(all1[4].signature),
⋮----
assert!(results2.len() < results.len());
⋮----
assert_eq!(all0.len(), 14);
⋮----
assert_eq!(all1.len(), 14);
⋮----
assert!(all0.len() % 3 == 2);
⋮----
.delete((address0, 2, 0, all0[0].signature))
⋮----
Some(all0[0].signature),
⋮----
assert!(!sig_infos.found_before);
⋮----
fn test_get_last_hash() {
let entries: Vec<Entry> = vec![];
let empty_entries_iterator = entries.iter();
assert!(get_last_hash(empty_entries_iterator).is_none());
let entry = next_entry(&solana_sha256_hasher::hash(&[42u8]), 1, vec![]);
let entries: Vec<Entry> = std::iter::successors(Some(entry), |entry| {
Some(next_entry(&entry.hash, 1, vec![]))
⋮----
.take(10)
⋮----
let entries_iterator = entries.iter();
assert_eq!(get_last_hash(entries_iterator).unwrap(), entries[9].hash);
⋮----
fn test_map_transactions_to_statuses() {
⋮----
let mut transactions: Vec<VersionedTransaction> = vec![];
⋮----
pre_balances: vec![],
post_balances: vec![],
⋮----
.put_protobuf((transaction.signatures[0], slot), &status)
⋮----
transactions.push(transaction.into());
⋮----
blockstore.map_transactions_to_statuses(slot, transactions.clone().into_iter());
assert!(map_result.is_ok());
let map = map_result.unwrap();
assert_eq!(map.len(), 4);
for (x, m) in map.iter().enumerate() {
assert_eq!(m.meta.fee, x as u64);
⋮----
transactions.push(
⋮----
.into(),
⋮----
assert_matches!(map_result, Err(BlockstoreError::MissingTransactionMetadata));
⋮----
fn test_get_recent_perf_samples_v1_only() {
⋮----
let mut perf_samples: Vec<(Slot, PerfSample)> = vec![];
⋮----
let sample = slot_sample(i as u64);
let bytes = serialize(&sample).unwrap();
blockstore.perf_samples_cf.put_bytes(slot, &bytes).unwrap();
perf_samples.push((slot, sample.into()));
⋮----
let mut expected_samples = perf_samples[num_entries - 1 - i..].to_vec();
expected_samples.sort_by(|a, b| b.0.cmp(&a.0));
⋮----
fn test_get_recent_perf_samples_v2_only() {
⋮----
fn test_get_recent_perf_samples_v1_and_v2() {
⋮----
let sample = slot_sample_v1(i as u64);
⋮----
let sample = slot_sample_v2(i as u64);
⋮----
fn test_write_perf_samples() {
⋮----
blockstore.write_perf_sample(slot, &sample).unwrap();
perf_samples.push((slot, PerfSample::V2(sample)));
⋮----
let mut expected_samples = perf_samples[num_entries - 1 - x..].to_vec();
⋮----
fn test_lowest_slot() {
⋮----
let (shreds, _) = make_slot_entries(slot, 0, 1);
⋮----
blockstore.run_purge(0, 5, PurgeType::Exact).unwrap();
assert_eq!(blockstore.lowest_slot(), 6);
⋮----
fn test_highest_slot() {
⋮----
assert_eq!(blockstore.highest_slot().unwrap(), None);
⋮----
assert_eq!(blockstore.highest_slot().unwrap(), Some(slot));
⋮----
blockstore.run_purge(5, 10, PurgeType::Exact).unwrap();
assert_eq!(blockstore.highest_slot().unwrap(), Some(4));
blockstore.run_purge(0, 4, PurgeType::Exact).unwrap();
⋮----
fn test_recovery() {
⋮----
setup_erasure_shreds(slot, 0, 100);
⋮----
.map(|shred| (Cow::Owned(shred),  false));
⋮----
Some(&leader_schedule_cache),
⋮----
Some((&ReedSolomonCache::default(), &dummy_retransmit_sender)),
⋮----
let shred_bufs: Vec<_> = data_shreds.iter().map(Shred::payload).cloned().collect();
for (s, buf) in data_shreds.iter().zip(shred_bufs) {
⋮----
verify_index_integrity(&blockstore, slot);
⋮----
fn test_index_integrity() {
⋮----
setup_erasure_shreds(slot, 0, num_entries);
assert!(data_shreds.len() > 3);
assert!(coding_shreds.len() > 3);
⋮----
.chain(coding_shreds.iter().cloned())
⋮----
.insert_shreds(all_shreds, Some(&leader_schedule_cache), false)
⋮----
blockstore.purge_and_compact_slots(0, slot);
⋮----
.insert_shreds(coding_shreds.clone(), Some(&leader_schedule_cache), false)
⋮----
coding_shreds[..coding_shreds.len() - 1].to_vec(),
⋮----
let shreds: Vec<_> = data_shreds[..data_shreds.len() - 1]
⋮----
.chain(coding_shreds[..coding_shreds.len() - 1].iter().cloned())
⋮----
.insert_shreds(shreds, Some(&leader_schedule_cache), false)
⋮----
let shreds: Vec<_> = data_shreds[..data_shreds.len() / 2 - 1]
⋮----
.chain(coding_shreds[..coding_shreds.len() / 2 - 1].iter().cloned())
⋮----
let shreds1: Vec<_> = data_shreds[..data_shreds.len() / 2 - 1]
⋮----
let shreds2: Vec<_> = data_shreds[data_shreds.len() / 2 - 1..]
⋮----
.chain(coding_shreds[coding_shreds.len() / 2 - 1..].iter().cloned())
⋮----
.insert_shreds(shreds1, Some(&leader_schedule_cache), false)
⋮----
.insert_shreds(shreds2, Some(&leader_schedule_cache), false)
⋮----
let shreds2: Vec<_> = data_shreds[data_shreds.len() / 2 - 1..data_shreds.len() / 2]
⋮----
coding_shreds[coding_shreds.len() / 2 - 1..coding_shreds.len() / 2]
⋮----
.cloned(),
⋮----
let shreds1: Vec<_> = data_shreds[..data_shreds.len() / 2 - 2]
⋮----
.chain(coding_shreds[..coding_shreds.len() / 2 - 2].iter().cloned())
⋮----
let shreds2: Vec<_> = data_shreds[data_shreds.len() / 2 - 2..data_shreds.len() / 2 - 1]
⋮----
coding_shreds[coding_shreds.len() / 2 - 2..coding_shreds.len() / 2 - 1]
⋮----
fn setup_erasure_shreds(
⋮----
setup_erasure_shreds_with_index(slot, parent_slot, num_entries, 0)
⋮----
fn setup_erasure_shreds_with_index(
⋮----
setup_erasure_shreds_with_index_and_chained_merkle(
⋮----
fn setup_erasure_shreds_with_index_and_chained_merkle(
⋮----
fn setup_erasure_shreds_with_index_and_chained_merkle_and_last_in_slot(
⋮----
let entries = make_slot_entries_with_transactions(num_entries);
⋮----
let shredder = Shredder::new(slot, parent_slot, 0, 0).unwrap();
let (data_shreds, coding_shreds) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
let genesis_config = create_genesis_config(2).genesis_config;
⋮----
vec![leader_keypair.pubkey()],
⋮----
leader_schedule_cache.set_fixed_leader_schedule(Some(fixed_schedule));
⋮----
fn verify_index_integrity(blockstore: &Blockstore, slot: u64) {
let shred_index = blockstore.get_index(slot).unwrap().unwrap();
let data_iter = blockstore.slot_data_iterator(slot, 0).unwrap();
⋮----
assert!(blockstore.get_data_shred(slot, index).unwrap().is_some());
assert!(shred_index.data().contains(index));
⋮----
let num_data_in_index = shred_index.data().num_shreds();
assert_eq!(num_data_in_index, num_data);
let coding_iter = blockstore.slot_coding_iterator(slot, 0).unwrap();
⋮----
assert!(blockstore.get_coding_shred(slot, index).unwrap().is_some());
assert!(shred_index.coding().contains(index));
⋮----
let num_coding_in_index = shred_index.coding().num_shreds();
assert_eq!(num_coding_in_index, num_coding);
⋮----
fn test_duplicate_slot() {
⋮----
let entries1 = make_slot_entries_with_transactions(1);
let entries2 = make_slot_entries_with_transactions(1);
⋮----
let shredder = Shredder::new(slot, 0, 0, 0).unwrap();
let merkle_root = Hash::new_from_array(rand::rng().random());
⋮----
let (duplicate_shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
let shred = shreds[0].clone();
let duplicate_shred = duplicate_shreds[0].clone();
let non_duplicate_shred = shred.clone();
⋮----
.insert_shreds(vec![shred.clone()], None, false)
⋮----
assert!(!blockstore.has_duplicate_shreds_in_slot(slot));
⋮----
.store_duplicate_slot(
⋮----
duplicate_shred.payload().clone(),
⋮----
assert!(blockstore.has_duplicate_shreds_in_slot(slot));
let duplicate_proof = blockstore.get_duplicate_slot(slot).unwrap();
assert_eq!(duplicate_proof.shred1, *shred.payload());
assert_eq!(duplicate_proof.shred2, *duplicate_shred.payload());
⋮----
fn test_clear_unconfirmed_slot() {
⋮----
let slots = vec![2, unconfirmed_slot, unconfirmed_child_slot];
let shreds: Vec<_> = make_chaining_slot_entries(&slots, 1, 0)
⋮----
.flat_map(|x| x.0)
⋮----
blockstore.set_dead_slot(unconfirmed_slot).unwrap();
blockstore.clear_unconfirmed_slot(unconfirmed_slot);
assert!(!blockstore.is_dead(unconfirmed_slot));
⋮----
fn test_clear_unconfirmed_slot_and_insert_again() {
⋮----
let slots = vec![confirmed_slot, unconfirmed_slot];
⋮----
assert_eq!(shreds.len(), 2 * 32);
let unconfirmed_slot_shreds = vec![shreds[32].clone()];
assert_eq!(unconfirmed_slot_shreds[0].slot(), unconfirmed_slot);
⋮----
.insert_shreds(unconfirmed_slot_shreds, None, false)
⋮----
fn test_update_completed_data_indexes() {
⋮----
shred_index.insert(i as u64);
assert!(update_completed_data_indexes(
⋮----
assert!(completed_data_indexes.clone().into_iter().eq(0..=i));
⋮----
fn test_update_completed_data_indexes_out_of_order() {
⋮----
shred_index.insert(4);
⋮----
assert!(completed_data_indexes.is_empty());
shred_index.insert(2);
⋮----
shred_index.insert(3);
⋮----
assert!(completed_data_indexes.clone().into_iter().eq([3]));
shred_index.insert(1);
⋮----
assert!(completed_data_indexes.clone().into_iter().eq([1, 3]));
shred_index.insert(0);
⋮----
assert!(completed_data_indexes.clone().into_iter().eq([0, 1, 3]));
⋮----
fn test_rewards_protobuf_backward_compatibility() {
⋮----
.map(|i| Reward {
pubkey: solana_pubkey::new_rand().to_string(),
⋮----
reward_type: Some(RewardType::Fee),
⋮----
let protobuf_rewards: generated::Rewards = rewards.into();
let deprecated_rewards: StoredExtendedRewards = protobuf_rewards.clone().into();
⋮----
let data = serialize(&deprecated_rewards).unwrap();
blockstore.rewards_cf.put_bytes(slot, &data).unwrap();
⋮----
.put_protobuf(slot, &protobuf_rewards)
⋮----
fn test_transaction_status_protobuf_backward_compatibility() {
⋮----
pre_balances: vec![1, 2, 3],
post_balances: vec![1, 2, 3],
⋮----
pre_token_balances: Some(vec![TransactionTokenBalance {
⋮----
post_token_balances: Some(vec![TransactionTokenBalance {
⋮----
rewards: Some(vec![Reward {
⋮----
return_data: Some(TransactionReturnData {
⋮----
compute_units_consumed: Some(23456),
cost_units: Some(5678),
⋮----
let deprecated_status: StoredTransactionStatusMeta = status.clone().try_into().unwrap();
let protobuf_status: generated::TransactionStatusMeta = status.into();
⋮----
let data = serialize(&deprecated_status).unwrap();
⋮----
.put_bytes((Signature::default(), slot), &data)
⋮----
.put_protobuf((Signature::default(), slot), &protobuf_status)
⋮----
fn make_large_tx_entry(num_txs: usize) -> Entry {
⋮----
.map(|_| {
⋮----
fn erasure_multiple_config() {
⋮----
let entries = [make_large_tx_entry(num_txs)];
let entries2 = [make_large_tx_entry(num_txs)];
let version = version_from_hash(&entries[0].hash);
let shredder = Shredder::new(slot, 0, 0, version).unwrap();
⋮----
let (data1, coding1) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
let (_data2, coding2) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
info!("shred {:?}", shred.id());
⋮----
info!("coding1 {:?}", shred.id());
⋮----
info!("coding2 {:?}", shred.id());
⋮----
.insert_shreds(data1[..data1.len() - 2].to_vec(), None, false)
⋮----
.insert_shreds(vec![coding1[0].clone(), coding2[1].clone()], None, false)
⋮----
fn test_insert_data_shreds_same_slot_last_index() {
⋮----
let num_unique_entries = max_ticks_per_n_shreds(1, None) + 1;
let (mut original_shreds, original_entries) = make_slot_entries(0, 0, num_unique_entries);
let mut duplicate_shreds = original_shreds.clone();
⋮----
shred.sign(&Keypair::new());
⋮----
assert!(original_shreds.len() > 1);
let last_index = original_shreds.last().unwrap().index() as u64;
original_shreds.remove(0);
⋮----
.insert_shreds(original_shreds.clone(), None, false)
⋮----
let meta = blockstore.meta(0).unwrap().unwrap();
assert!(!blockstore.is_dead(0));
assert_eq!(blockstore.get_slot_entries(0, 0).unwrap(), vec![]);
⋮----
assert_eq!(meta.received, last_index + 1);
⋮----
assert_eq!(meta.last_index, Some(last_index));
assert!(!blockstore.is_full(0));
⋮----
let num_shreds = duplicate_shreds.len() as u64;
⋮----
.insert_shreds(duplicate_shreds, None, false)
⋮----
assert_eq!(blockstore.get_slot_entries(0, 0).unwrap(), original_entries);
⋮----
assert!(blockstore.is_full(0));
⋮----
fn setup_duplicate_last_in_slot(
⋮----
let entries = make_slot_entries_with_transactions(1);
⋮----
.partition(Shred::is_data);
let last_data1 = shreds1.last().unwrap();
let last_code1 = code1.last().unwrap();
⋮----
last_data1.chained_merkle_root().unwrap(),
last_data1.index() + 1,
last_code1.index() + 1,
⋮----
fn test_duplicate_last_index() {
⋮----
let ((shreds1, _code1), (shreds2, _code2)) = setup_duplicate_last_in_slot(slot);
⋮----
let last_data2 = shreds2.last().unwrap();
⋮----
.insert_shreds(vec![last_data1.clone(), last_data2.clone()], None, false)
⋮----
assert!(blockstore.get_duplicate_slot(slot).is_some());
⋮----
fn test_duplicate_last_index_mark_dead() {
⋮----
let ((mut shreds1, _code1), (mut shreds2, _code2)) = setup_duplicate_last_in_slot(slot);
shreds1.append(&mut shreds2);
⋮----
let slot = shreds[0].slot();
⋮----
.insert_shreds(shreds.clone(), None, false)
⋮----
assert_eq!(meta.consumed, shreds.len() as u64);
let shreds_index = blockstore.get_index(slot).unwrap().unwrap();
for i in 0..shreds.len() as u64 {
assert!(shreds_index.data().contains(i));
⋮----
.run_purge(slot, slot, PurgeType::Exact)
.expect("Purge database operations failed");
assert!(blockstore.meta(slot).unwrap().is_none());
⋮----
let shreds = setup_test_shreds(slot);
let (expected_slot_meta, expected_index) = get_expected_slot_meta_and_index_meta(
⋮----
shreds[..=smaller_last_shred_index].to_vec(),
⋮----
assert!(!blockstore.is_dead(slot));
⋮----
assert!(blockstore.get_data_shred(slot, i).unwrap().is_none());
⋮----
let mut meta = blockstore.meta(slot).unwrap().unwrap();
⋮----
assert_eq!(meta, expected_slot_meta);
assert_eq!(blockstore.get_index(slot).unwrap().unwrap(), expected_index);
⋮----
let mut shreds = setup_test_shreds(slot);
shreds.reverse();
⋮----
let shred_index = shred_to_check.index() as u64;
⋮----
for shred in shreds.clone() {
⋮----
fn test_get_slot_entries_dead_slot_race() {
⋮----
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
let (slot_sender, slot_receiver) = unbounded();
⋮----
let (signal_sender, signal_receiver) = unbounded();
⋮----
scope.spawn(|| {
while let Ok(slot) = slot_receiver.recv() {
match blockstore.get_slot_entries_with_shred_info(slot, 0, false) {
⋮----
.send(Err(IoError::other(
⋮----
assert_matches!(err, BlockstoreError::DeadSlot);
⋮----
signal_sender.send(Ok(())).unwrap();
⋮----
while let Ok(shreds) = shred_receiver.recv() {
⋮----
let _lowest_cleanup_slot = blockstore.lowest_cleanup_slot.write().unwrap();
⋮----
let ((mut shreds1, _), (mut shreds2, _)) = setup_duplicate_last_in_slot(slot);
shreds2.append(&mut shreds1);
slot_sender.send(slot).unwrap();
shred_sender.send(shreds2).unwrap();
⋮----
let res = signal_receiver.recv().unwrap();
assert!(res.is_ok(), "race condition: {res:?}");
⋮----
drop(slot_sender);
drop(shred_sender);
⋮----
fn test_previous_erasure_set() {
⋮----
setup_erasure_shreds_with_index(slot, parent_slot, 10, 0);
⋮----
ErasureMeta::from_coding_shred(coding_shreds_0.first().unwrap()).unwrap();
let prev_fec_set_index = data_shreds_0.len() as u32;
⋮----
setup_erasure_shreds_with_index(slot, parent_slot, 10, prev_fec_set_index);
⋮----
ErasureMeta::from_coding_shred(coding_shreds_prev.first().unwrap()).unwrap();
⋮----
setup_erasure_shreds_with_index(prev_slot, parent_slot, 10, prev_fec_set_index);
⋮----
ErasureMeta::from_coding_shred(coding_shreds_prev_slot.first().unwrap()).unwrap();
let fec_set_index = data_shreds_prev.len() as u32 + prev_fec_set_index;
⋮----
erasure_metas.insert(erasure_set_0, WorkingEntry::Dirty(erasure_meta_0));
⋮----
erasure_metas.insert(erasure_set_0, WorkingEntry::Clean(erasure_meta_0));
⋮----
.put_erasure_meta(erasure_set_0, &erasure_meta_0)
⋮----
erasure_metas.insert(erasure_set_prev, WorkingEntry::Dirty(erasure_meta_prev));
⋮----
erasure_metas.remove(&erasure_set_prev);
⋮----
.put_erasure_meta(erasure_set_prev, &erasure_meta_prev)
⋮----
erasure_metas.insert(erasure_set_prev, WorkingEntry::Clean(erasure_meta_prev));
⋮----
erasure_metas.remove(&erasure_set_0);
⋮----
erasure_metas.clear();
erasure_metas.insert(
⋮----
.put_erasure_meta(erasure_set_prev_slot, &erasure_meta_prev_slot)
⋮----
fn test_chained_merkle_root_consistency_backwards() {
⋮----
let coding_shred = coding_shreds[0].clone();
let next_fec_set_index = fec_set_index + data_shreds.len() as u32;
⋮----
let merkle_root = coding_shred.merkle_root().unwrap();
let (data_shreds, coding_shreds, _) = setup_erasure_shreds_with_index_and_chained_merkle(
⋮----
fn test_chained_merkle_root_consistency_forwards() {
⋮----
let (_, next_coding_shreds, _) = setup_erasure_shreds_with_index_and_chained_merkle(
⋮----
let next_coding_shred = next_coding_shreds[0].clone();
⋮----
fn test_chained_merkle_root_across_slots_backwards() {
⋮----
assert!(merkle_root != data_shred.merkle_root().unwrap());
⋮----
let next_slot_data_shred = next_slot_data_shreds[0].clone();
let next_slot_coding_shred = next_slot_coding_shreds[0].clone();
⋮----
fn test_chained_merkle_root_across_slots_forwards() {
⋮----
assert!(merkle_root != coding_shred.merkle_root().unwrap());
⋮----
fn test_chained_merkle_root_inconsistency_backwards_insert_code() {
⋮----
let coding_shred_previous = coding_shreds[0].clone();
⋮----
assert!(merkle_root != coding_shred_previous.merkle_root().unwrap());
⋮----
blockstore.insert_shred_return_duplicate(coding_shred.clone(), &leader_schedule);
⋮----
fn test_chained_merkle_root_inconsistency_backwards_insert_data() {
⋮----
blockstore.insert_shred_return_duplicate(data_shred.clone(), &leader_schedule);
⋮----
fn test_chained_merkle_root_inconsistency_forwards() {
⋮----
let next_data_shred = next_data_shreds[0].clone();
⋮----
fn test_chained_merkle_root_inconsistency_both() {
⋮----
let prev_coding_shred = prev_coding_shreds[0].clone();
let fec_set_index = prev_fec_set_index + prev_data_shreds.len() as u32;
⋮----
assert!(merkle_root != prev_coding_shred.merkle_root().unwrap());
⋮----
let next_fec_set_index = fec_set_index + prev_data_shreds.len() as u32;
⋮----
fn test_chained_merkle_root_upgrade_inconsistency_backwards() {
⋮----
let coding_shred_previous = coding_shreds[1].clone();
⋮----
.erasure_meta(coding_shred_previous.erasure_set())
⋮----
erasure_meta.clear_first_received_coding_shred_index();
⋮----
.put_erasure_meta(coding_shred_previous.erasure_set(), &erasure_meta)
⋮----
let mut write_batch = blockstore.get_write_batch().unwrap();
⋮----
.delete_range_in_batch(&mut write_batch, slot, slot)
⋮----
fn test_chained_merkle_root_upgrade_inconsistency_forwards() {
⋮----
fn test_check_last_fec_set() {
⋮----
let total_shreds = fec_set_index as u64 + data_shreds.len() as u64;
assert_eq!(data_shreds.len(), DATA_SHREDS_PER_FEC_BLOCK);
⋮----
data_shreds[0..DATA_SHREDS_PER_FEC_BLOCK - 1].to_vec(),
⋮----
assert!(meta.last_index.is_none());
⋮----
blockstore.run_purge(slot, slot, PurgeType::Exact).unwrap();
⋮----
.insert_shreds(data_shreds[1..].to_vec(), None, false)
⋮----
assert_eq!(meta.last_index, Some(total_shreds - 1));
⋮----
let block_id = data_shreds[0].merkle_root().unwrap();
blockstore.insert_shreds(data_shreds, None, false).unwrap();
let results = blockstore.check_last_fec_set(slot).unwrap();
assert_eq!(results.last_fec_set_merkle_root, Some(block_id));
assert!(results.is_retransmitter_signed);
⋮----
let merkle_root = first_data_shreds[0].merkle_root().unwrap();
fec_set_index += first_data_shreds.len() as u32;
⋮----
let last_index = last_data_shreds.last().unwrap().index();
let total_shreds = first_data_shreds.len() + last_data_shreds.len();
assert_eq!(total_shreds, 2 * DATA_SHREDS_PER_FEC_BLOCK);
let merkle_root = last_data_shreds[0].merkle_root().unwrap();
⋮----
.insert_shreds(first_data_shreds, None, false)
⋮----
.insert_shreds(last_data_shreds, None, false)
⋮----
let mut slot_meta = blockstore.meta(slot).unwrap().unwrap();
slot_meta.last_index = Some(last_index as u64);
blockstore.put_meta(slot, &slot_meta).unwrap();
⋮----
assert_eq!(results.last_fec_set_merkle_root, Some(merkle_root));
assert!(!results.is_retransmitter_signed);
⋮----
assert_eq!(last_data_shreds.len(), DATA_SHREDS_PER_FEC_BLOCK);
⋮----
fn test_last_fec_set_check_results() {
⋮----
fn test_write_transaction_memos() {
⋮----
let blockstore = Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger");
⋮----
.write_transaction_memos(&signature, 4, "test_write_transaction_memos".to_string())
⋮----
.read_transaction_memos(signature, 4)
.expect("Expected to find memo");
assert_eq!(memo, Some("test_write_transaction_memos".to_string()));
⋮----
fn test_add_transaction_memos_to_batch() {
⋮----
let signatures: Vec<Signature> = (0..2).map(|_| Signature::new_unique()).collect();
let mut memos_batch = blockstore.get_write_batch().unwrap();
⋮----
.add_transaction_memos_to_batch(
⋮----
"test_write_transaction_memos1".to_string(),
⋮----
"test_write_transaction_memos2".to_string(),
⋮----
blockstore.write_batch(memos_batch).unwrap();
⋮----
.read_transaction_memos(signatures[0], 4)
⋮----
assert_eq!(memo1, Some("test_write_transaction_memos1".to_string()));
⋮----
.read_transaction_memos(signatures[1], 5)
⋮----
assert_eq!(memo2, Some("test_write_transaction_memos2".to_string()));
⋮----
fn test_write_transaction_status() {
⋮----
vec![(Pubkey::new_unique(), true), (Pubkey::new_unique(), false)];
⋮----
.map(|&(ref pubkey, writable)| (pubkey, writable)),
⋮----
.read_transaction_status((signatures[0], slot))
⋮----
assert_eq!(tx_status.fee, 4200);
⋮----
fn test_add_transaction_status_to_batch() {
⋮----
.map(|_| vec![(Pubkey::new_unique(), true), (Pubkey::new_unique(), false)])
⋮----
let mut status_batch = blockstore.get_write_batch().unwrap();
for (tx_idx, signature) in signatures.iter().enumerate() {
⋮----
.add_transaction_status_to_batch(
⋮----
keys_with_writable[tx_idx].iter().map(|(k, v)| (k, *v)),
⋮----
Err(TransactionError::InsufficientFundsForFee)
⋮----
blockstore.write_batch(status_batch).unwrap();
⋮----
assert_eq!(tx_status1.fee, 5700);
assert_eq!(tx_status1.status, Ok(()));
⋮----
.read_transaction_status((signatures[1], slot))
⋮----
assert_eq!(tx_status2.fee, 5701);

================
File: ledger/src/entry_notifier_interface.rs
================
pub trait EntryNotifier {
⋮----
pub type EntryNotifierArc = Arc<dyn EntryNotifier + Sync + Send>;

================
File: ledger/src/entry_notifier_service.rs
================
pub struct EntryNotification {
⋮----
pub type EntryNotifierSender = Sender<EntryNotification>;
pub type EntryNotifierReceiver = Receiver<EntryNotification>;
pub struct EntryNotifierService {
⋮----
impl EntryNotifierService {
pub fn new(entry_notifier: EntryNotifierArc, exit: Arc<AtomicBool>) -> Self {
let (entry_notification_sender, entry_notification_receiver) = unbounded();
⋮----
.name("solEntryNotif".to_string())
.spawn(move || loop {
if exit.load(Ordering::Relaxed) {
⋮----
Self::notify_entry(&entry_notification_receiver, entry_notifier.clone())
⋮----
.unwrap();
⋮----
fn notify_entry(
⋮----
} = entry_notification_receiver.recv_timeout(Duration::from_secs(1))?;
entry_notifier.notify_entry(slot, index, &entry, starting_transaction_index);
Ok(())
⋮----
pub fn sender(&self) -> &EntryNotifierSender {
⋮----
pub fn sender_cloned(&self) -> EntryNotifierSender {
self.sender.clone()
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()

================
File: ledger/src/genesis_utils.rs
================
pub fn create_genesis_config(mint_lamports: u64) -> GenesisConfigInfo {
create_genesis_config_with_leader(
⋮----
bootstrap_validator_stake_lamports(),
⋮----
pub fn create_genesis_config_with_mint_keypair(
⋮----
create_genesis_config_with_leader_with_mint_keypair(

================
File: ledger/src/leader_schedule_cache.rs
================
type CachedSchedules = (HashMap<Epoch, Arc<LeaderSchedule>>, VecDeque<u64>);
⋮----
struct CacheCapacity(usize);
impl Default for CacheCapacity {
fn default() -> Self {
CacheCapacity(MAX_SCHEDULES)
⋮----
pub struct LeaderScheduleCache {
⋮----
impl LeaderScheduleCache {
pub fn new_from_bank(bank: &Bank) -> Self {
Self::new(bank.epoch_schedule().clone(), bank)
⋮----
pub fn new(epoch_schedule: EpochSchedule, root_bank: &Bank) -> Self {
⋮----
cache.set_root(root_bank);
⋮----
.get_leader_schedule_epoch(root_bank.slot());
⋮----
let first_slot_in_epoch = cache.epoch_schedule.get_first_slot_in_epoch(epoch);
cache.slot_leader_at(first_slot_in_epoch, Some(root_bank));
⋮----
pub fn set_max_schedules(&mut self, max_schedules: usize) {
⋮----
self.max_schedules = CacheCapacity(max_schedules);
⋮----
pub fn max_schedules(&self) -> usize {
⋮----
pub fn set_root(&self, root_bank: &Bank) {
⋮----
let old_max_epoch = self.max_epoch.swap(new_max_epoch, Ordering::AcqRel);
assert!(new_max_epoch >= old_max_epoch);
⋮----
self.compute_epoch_schedule(new_max_epoch, root_bank);
⋮----
pub fn slot_leader_at(&self, slot: Slot, bank: Option<&Bank>) -> Option<Pubkey> {
⋮----
self.slot_leader_at_else_compute(slot, bank)
⋮----
self.slot_leader_at_no_compute(slot)
⋮----
pub fn next_leader_slot(
⋮----
let (epoch, start_index) = bank.get_epoch_and_slot_index(current_slot + 1);
let max_epoch = self.max_epoch.load(Ordering::Acquire);
⋮----
debug!(
⋮----
.map(|epoch| self.get_epoch_schedule_else_compute(epoch, bank))
.while_some()
.zip(epoch..)
.collect();
⋮----
.iter()
.flat_map(|(leader_schedule, k)| {
⋮----
let num_slots = bank.get_slots_in_epoch(*k) as usize;
let first_slot = bank.epoch_schedule().get_first_slot_in_epoch(*k);
⋮----
.get_leader_upcoming_slots(pubkey, offset)
.take_while(move |i| *i < num_slots)
.map(move |i| i as Slot + first_slot)
⋮----
.skip_while(|slot| {
⋮----
.map(|bs| bs.has_existing_shreds_for_slot(*slot))
.unwrap_or(false)
⋮----
let first_slot = schedule.next()?;
let max_slot = first_slot.saturating_add(max_slot_range);
⋮----
.take_while(|slot| *slot < max_slot)
.zip(first_slot + 1..)
.take_while(|(a, b)| a == b)
.map(|(s, _)| s)
.last()
.unwrap_or(first_slot);
Some((first_slot, last_slot))
⋮----
pub fn set_fixed_leader_schedule(&mut self, fixed_schedule: Option<FixedSchedule>) {
self.fixed_schedule = fixed_schedule.map(Arc::new);
⋮----
fn slot_leader_at_no_compute(&self, slot: Slot) -> Option<Pubkey> {
let (epoch, slot_index) = self.epoch_schedule.get_epoch_and_slot_index(slot);
⋮----
return Some(fixed_schedule.leader_schedule[slot_index]);
⋮----
.read()
.unwrap()
⋮----
.get(&epoch)
.map(|schedule| schedule[slot_index])
⋮----
fn slot_leader_at_else_compute(&self, slot: Slot, bank: &Bank) -> Option<Pubkey> {
let cache_result = self.slot_leader_at_no_compute(slot);
let bank_epoch = self.epoch_schedule.get_epoch_and_slot_index(slot).0;
if bank_epoch > self.max_epoch.load(Ordering::Acquire) {
debug!("Requested leader in slot: {slot} of unconfirmed epoch: {bank_epoch}");
⋮----
if cache_result.is_some() {
⋮----
let (epoch, slot_index) = bank.get_epoch_and_slot_index(slot);
self.compute_epoch_schedule(epoch, bank)
.map(|epoch_schedule| epoch_schedule[slot_index])
⋮----
pub fn get_epoch_leader_schedule(&self, epoch: Epoch) -> Option<Arc<LeaderSchedule>> {
self.cached_schedules.read().unwrap().0.get(&epoch).cloned()
⋮----
fn get_epoch_schedule_else_compute(
⋮----
return Some(fixed_schedule.leader_schedule.clone());
⋮----
let epoch_schedule = self.get_epoch_leader_schedule(epoch);
if epoch_schedule.is_some() {
⋮----
fn compute_epoch_schedule(&self, epoch: Epoch, bank: &Bank) -> Option<Arc<LeaderSchedule>> {
⋮----
leader_schedule.map(|leader_schedule| {
⋮----
let (ref mut cached_schedules, ref mut order) = *self.cached_schedules.write().unwrap();
let entry = cached_schedules.entry(epoch);
⋮----
v.insert(leader_schedule.clone());
order.push_back(epoch);
Self::retain_latest(cached_schedules, order, self.max_schedules());
⋮----
fn retain_latest(
⋮----
while schedules.len() > max_schedules {
let first = order.pop_front().unwrap();
schedules.remove(&first);
⋮----
mod tests {
⋮----
fn test_new_cache() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(2);
⋮----
assert_eq!(bank.slot(), 0);
assert_eq!(cache.max_schedules(), MAX_SCHEDULES);
let epoch_schedule = bank.epoch_schedule();
let leader_schedule_epoch = bank.get_leader_schedule_epoch(bank.slot());
⋮----
let first_slot_in_leader_schedule_epoch = epoch_schedule.get_first_slot_in_epoch(epoch);
let last_slot_in_leader_schedule_epoch = epoch_schedule.get_last_slot_in_epoch(epoch);
assert!(cache
⋮----
assert_eq!(
⋮----
fn test_retain_latest() {
⋮----
cached_schedules.insert(
⋮----
order.push_back(i as u64);
⋮----
assert_eq!(cached_schedules.len(), MAX_SCHEDULES);
let mut keys: Vec<_> = cached_schedules.keys().cloned().collect();
keys.sort_unstable();
let expected: Vec<_> = (1..=MAX_SCHEDULES as u64).collect();
let expected_order: VecDeque<_> = (1..=MAX_SCHEDULES as u64).collect();
assert_eq!(expected, keys);
assert_eq!(expected_order, order);
⋮----
fn test_thread_race_leader_schedule_cache() {
⋮----
run_thread_race()
⋮----
fn run_thread_race() {
⋮----
.map(|_| {
let cache = cache.clone();
let bank = bank.clone();
let (sender, receiver) = unbounded();
⋮----
.name("test_thread_race_leader_schedule_cache".to_string())
.spawn(move || {
let _ = receiver.recv();
cache.slot_leader_at(bank.slot(), Some(&bank));
⋮----
.unwrap(),
⋮----
.unzip();
⋮----
sender.send(true).unwrap();
⋮----
for t in threads.into_iter() {
t.join().unwrap();
⋮----
let (ref cached_schedules, ref order) = *cache.cached_schedules.read().unwrap();
assert_eq!(cached_schedules.len(), 1);
assert_eq!(order.len(), 1);
⋮----
fn test_next_leader_slot() {
⋮----
create_genesis_config_with_leader(42, &pubkey, bootstrap_validator_stake_lamports())
⋮----
fn test_next_leader_slot_blockstore() {
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger");
⋮----
let (shreds, _) = make_slot_entries(2, 1, 1);
blockstore.insert_shreds(shreds, None, false).unwrap();
⋮----
let (shreds, _) = make_slot_entries(1, 0, 1);
⋮----
fn test_next_leader_slot_next_epoch() {
⋮----
} = create_genesis_config(10_000 * bootstrap_validator_stake_lamports());
⋮----
setup_vote_and_stake_accounts(
⋮----
bootstrap_validator_stake_lamports()
⋮----
bank.feature_set.is_active(
⋮----
let node_pubkey = validator_identity.pubkey();
⋮----
let epoch = bank.get_leader_schedule_epoch(0);
while bank.get_leader_schedule_epoch(target_slot) == epoch {
⋮----
.write()
⋮----
.insert(Bank::new_from_parent(bank, &Pubkey::default(), target_slot))
.clone_without_scheduler();
⋮----
let epoch = bank.get_leader_schedule_epoch(target_slot);
⋮----
expected_slot += bank.get_slots_in_epoch(i);
⋮----
let schedule = cache.compute_epoch_schedule(epoch, &bank).unwrap();
⋮----
assert_ne!(index, genesis_config.epoch_schedule.slots_per_epoch);
⋮----
cache.set_root(&bank);
⋮----
.next_leader_slot(&node_pubkey, 0, &bank, None, u64::MAX)
.unwrap();
assert_eq!(res.0, expected_slot);
assert!(res.1 >= expected_slot + NUM_CONSECUTIVE_LEADER_SLOTS - 1);
⋮----
.next_leader_slot(
⋮----
assert_eq!(res.1, expected_slot + NUM_CONSECUTIVE_LEADER_SLOTS - 2);
⋮----
fn test_schedule_for_unconfirmed_epoch() {
⋮----
assert_eq!(cache.max_epoch.load(Ordering::Acquire), 1);
assert_eq!(bank.get_epoch_and_slot_index(95).0, 1);
assert!(cache.slot_leader_at(95, Some(&bank)).is_some());
assert_eq!(bank.get_epoch_and_slot_index(96).0, 2);
assert!(cache.slot_leader_at(96, Some(&bank)).is_none());
⋮----
assert!(bank2.epoch_vote_accounts(2).is_some());
cache.set_root(&bank2);
assert_eq!(cache.max_epoch.load(Ordering::Acquire), 2);
assert!(cache.slot_leader_at(96, Some(&bank2)).is_some());
assert_eq!(bank2.get_epoch_and_slot_index(223).0, 2);
assert!(cache.slot_leader_at(223, Some(&bank2)).is_some());
assert_eq!(bank2.get_epoch_and_slot_index(224).0, 3);
assert!(cache.slot_leader_at(224, Some(&bank2)).is_none());
⋮----
fn test_set_max_schedules() {
⋮----
cache.set_max_schedules(0);
⋮----
cache.set_max_schedules(usize::MAX);
assert_eq!(cache.max_schedules(), usize::MAX);

================
File: ledger/src/leader_schedule_utils.rs
================
pub fn leader_schedule(epoch: Epoch, bank: &Bank) -> Option<LeaderSchedule> {
let use_new_leader_schedule = bank.should_use_vote_keyed_leader_schedule(epoch)?;
⋮----
bank.epoch_vote_accounts(epoch).map(|vote_accounts_map| {
⋮----
bank.get_slots_in_epoch(epoch),
⋮----
bank.epoch_staked_nodes(epoch).map(|stakes| {
⋮----
pub type LeaderScheduleByIdentity = HashMap<String, Vec<usize>>;
pub fn leader_schedule_by_identity<'a>(
⋮----
.entry(identity_pubkey)
.or_insert_with(Vec::new)
.push(slot_index);
⋮----
.into_iter()
.map(|(identity_pubkey, slot_indices)| (identity_pubkey.to_string(), slot_indices))
.collect()
⋮----
pub fn slot_leader_at(slot: Slot, bank: &Bank) -> Option<Pubkey> {
let (epoch, slot_index) = bank.get_epoch_and_slot_index(slot);
leader_schedule(epoch, bank).map(|leader_schedule| leader_schedule[slot_index])
⋮----
pub fn num_ticks_left_in_slot(bank: &Bank, tick_height: u64) -> u64 {
bank.ticks_per_slot() - tick_height % bank.ticks_per_slot()
⋮----
pub fn first_of_consecutive_leader_slots(slot: Slot) -> Slot {
⋮----
pub fn last_of_consecutive_leader_slots(slot: Slot) -> Slot {
first_of_consecutive_leader_slots(slot) + NUM_CONSECUTIVE_LEADER_SLOTS - 1
⋮----
pub fn leader_slot_index(slot: Slot) -> usize {
⋮----
pub fn remaining_slots_in_window(slot: Slot) -> u64 {
⋮----
.checked_sub(leader_slot_index(slot) as u64)
.unwrap()
⋮----
mod tests {
⋮----
fn test_leader_schedule_via_bank(use_vote_keyed_leader_schedule: bool) {
⋮----
create_genesis_config_with_leader(0, &pubkey, bootstrap_validator_stake_lamports())
⋮----
deactivate_features(
⋮----
&vec![agave_feature_set::enable_vote_address_leader_schedule::id()],
⋮----
let leader_schedule = leader_schedule(0, &bank).unwrap();
assert_eq!(
⋮----
assert_eq!(leader_schedule[0], pubkey);
assert_eq!(leader_schedule[1], pubkey);
assert_eq!(leader_schedule[2], pubkey);
⋮----
fn test_leader_scheduler1_basic() {
⋮----
create_genesis_config_with_leader(42, &pubkey, bootstrap_validator_stake_lamports())
⋮----
assert_eq!(slot_leader_at(bank.slot(), &bank).unwrap(), pubkey);
⋮----
fn test_leader_span_math() {
assert_eq!(NUM_CONSECUTIVE_LEADER_SLOTS, 4);
assert_eq!(first_of_consecutive_leader_slots(0), 0);
assert_eq!(first_of_consecutive_leader_slots(1), 0);
assert_eq!(first_of_consecutive_leader_slots(2), 0);
assert_eq!(first_of_consecutive_leader_slots(3), 0);
assert_eq!(first_of_consecutive_leader_slots(4), 4);
assert_eq!(last_of_consecutive_leader_slots(0), 3);
assert_eq!(last_of_consecutive_leader_slots(1), 3);
assert_eq!(last_of_consecutive_leader_slots(2), 3);
assert_eq!(last_of_consecutive_leader_slots(3), 3);
assert_eq!(last_of_consecutive_leader_slots(4), 7);
assert_eq!(leader_slot_index(0), 0);
assert_eq!(leader_slot_index(1), 1);
assert_eq!(leader_slot_index(2), 2);
assert_eq!(leader_slot_index(3), 3);
assert_eq!(leader_slot_index(4), 0);
assert_eq!(leader_slot_index(5), 1);
assert_eq!(leader_slot_index(6), 2);
assert_eq!(leader_slot_index(7), 3);
assert_eq!(remaining_slots_in_window(0), 4);
assert_eq!(remaining_slots_in_window(1), 3);
assert_eq!(remaining_slots_in_window(2), 2);
assert_eq!(remaining_slots_in_window(3), 1);
assert_eq!(remaining_slots_in_window(4), 4);

================
File: ledger/src/leader_schedule.rs
================
mod identity_keyed;
mod vote_keyed;
⋮----
pub struct FixedSchedule {
⋮----
pub type LeaderSchedule = Box<dyn LeaderScheduleVariant>;
pub trait LeaderScheduleVariant:
⋮----
fn get_vote_key_at_slot_index(&self, _epoch_slot_index: usize) -> Option<&Pubkey> {
⋮----
fn get_leader_upcoming_slots(
⋮----
let index = self.get_leader_slots_map().get(pubkey);
let num_slots = self.num_slots();
⋮----
Some(index) if !index.is_empty() => {
let size = index.len();
⋮----
.binary_search(&(offset % num_slots))
.unwrap_or_else(identity)
⋮----
// The modular arithmetic here and above replicate Index implementation
// for LeaderSchedule, where the schedule keeps repeating endlessly.
// The '%' returns where in a cycle we are and the '/' returns how many
// times the schedule is repeated.
⋮----
.map(move |k| index[k % size] + k / size * num_slots),
⋮----
// Empty iterator for pubkeys not in schedule
⋮----
Box::new((1..=0).map(|_| 0))
⋮----
fn num_slots(&self) -> usize {
self.get_slot_leaders().len()
⋮----
// Note: passing in zero keyed stakes will cause a panic.
fn stake_weighted_slot_leaders(
⋮----
sort_stakes(&mut keyed_stakes);
let (keys, stakes): (Vec<_>, Vec<_>) = keyed_stakes.into_iter().unzip();
let weighted_index = WeightedIndex::new(stakes).unwrap();
⋮----
seed[0..8].copy_from_slice(&epoch.to_le_bytes());
⋮----
.map(|i| {
⋮----
current_slot_leader = keys[weighted_index.sample(rng)];
⋮----
.collect()
⋮----
fn sort_stakes(stakes: &mut Vec<(&Pubkey, u64)>) {
// Sort first by stake. If stakes are the same, sort by pubkey to ensure a
// deterministic result.
// Note: Use unstable sort, because we dedup right after to remove the equal elements.
stakes.sort_unstable_by(|(l_pubkey, l_stake), (r_pubkey, r_stake)| {
⋮----
r_pubkey.cmp(l_pubkey)
⋮----
r_stake.cmp(l_stake)
⋮----
// Now that it's sorted, we can do an O(n) dedup.
stakes.dedup();
⋮----
mod tests {
⋮----
fn test_get_leader_upcoming_slots() {
⋮----
let pubkeys: Vec<_> = repeat_with(Pubkey::new_unique).take(4).collect();
let schedule: Vec<_> = repeat_with(|| pubkeys[rng.random_range(0..3)])
.take(19)
.collect();
⋮----
.map(|i| (schedule[i as u64], i))
.into_group_map();
⋮----
let index = leaders.get(pubkey).cloned().unwrap_or_default();
⋮----
.get_leader_upcoming_slots(pubkey, offset)
.take_while(|s| *s < NUM_SLOTS)
⋮----
let index: Vec<_> = index.iter().copied().skip_while(|s| *s < offset).collect();
assert_eq!(schedule, index);
⋮----
fn test_sort_stakes_basic() {
⋮----
let mut stakes = vec![(&pubkey0, 1), (&pubkey1, 2)];
sort_stakes(&mut stakes);
assert_eq!(stakes, vec![(&pubkey1, 2), (&pubkey0, 1)]);
⋮----
fn test_sort_stakes_with_dup() {
⋮----
let mut stakes = vec![(&pubkey0, 1), (&pubkey1, 2), (&pubkey0, 1)];
⋮----
fn test_sort_stakes_with_equal_stakes() {
⋮----
let mut stakes = vec![(&pubkey0, 1), (&pubkey1, 1)];
⋮----
assert_eq!(stakes, vec![(&pubkey1, 1), (&pubkey0, 1)]);
⋮----
fn pubkey_from_u16(n: u16) -> Pubkey {
⋮----
bytes[0..2].copy_from_slice(&n.to_le_bytes());
⋮----
fn test_stake_leader_schedule_exact_order(
⋮----
let pubkeys: Vec<_> = (0..stakes.len() as u16).map(pubkey_from_u16).collect();
let stakes = pubkeys.iter().zip(stakes.iter().copied()).collect();
let order: Vec<_> = stake_weighted_slot_leaders(stakes, epoch, len, repeat)
.into_iter()
.map(|pubkey| {
⋮----
.iter()
.find_position(|item| *item == &pubkey)
.unwrap()
⋮----
assert_eq!(order, expected_order);
⋮----
fn test_long_leader_schedule_hashed(
⋮----
fn hash_pubkeys(v: &[Pubkey]) -> String {
⋮----
let hasher = v.iter().fold(Sha256::new(), |hasher, pk| {
hasher.chain_update(pk.to_bytes())
⋮----
bs58::encode(hasher.finalize()).into_string()
⋮----
let pubkeys: Vec<_> = (0..=u16::MAX).map(pubkey_from_u16).collect();
⋮----
.enumerate()
.map(|(i, pk)| (pk, i.pow(stake_pow) as u64))
⋮----
let schedule = stake_weighted_slot_leaders(stakes, epoch, len, 1);
assert_eq!(hash_pubkeys(&schedule), expected_hash);
⋮----
fn test_zero_stake_panics() {
let _ = stake_weighted_slot_leaders(
vec![(&pubkey_from_u16(1), 0), (&pubkey_from_u16(2), 0)],

================
File: ledger/src/lib.rs
================
pub mod bank_forks_utils;
pub mod bigtable_delete;
pub mod bigtable_upload;
pub mod bigtable_upload_service;
pub mod block_error;
⋮----
pub mod blockstore;
pub mod ancestor_iterator;
pub mod bit_vec;
pub mod blockstore_cleanup_service;
pub mod blockstore_db;
pub mod blockstore_meta;
pub mod blockstore_metric_report_service;
pub mod blockstore_metrics;
pub mod blockstore_options;
pub mod blockstore_processor;
pub mod entry_notifier_interface;
pub mod entry_notifier_service;
pub mod genesis_utils;
pub mod leader_schedule;
pub mod leader_schedule_cache;
pub mod leader_schedule_utils;
pub mod next_slots_iterator;
pub mod rooted_slot_iterator;
⋮----
pub mod shred;
⋮----
pub(crate) mod shred;
mod shredder;
pub mod sigverify_shreds;
pub mod slot_stats;
mod staking_utils;
mod transaction_address_lookup_table_scanner;
pub mod transaction_balances;
pub mod use_snapshot_archives_at_startup;
⋮----
extern crate eager;
⋮----
extern crate solana_metrics;
⋮----
extern crate log;
⋮----
extern crate solana_frozen_abi_macro;
mod wire_format_tests;
⋮----
pub mod macro_reexports {
pub use solana_genesis_utils::MAX_GENESIS_ARCHIVE_UNPACKED_SIZE;

================
File: ledger/src/next_slots_iterator.rs
================
pub struct NextSlotsIterator<'a> {
⋮----
pub fn new(start_slot: Slot, blockstore: &'a Blockstore) -> Self {
⋮----
pending_slots: vec![start_slot],
⋮----
impl Iterator for NextSlotsIterator<'_> {
type Item = (Slot, SlotMeta);
fn next(&mut self) -> Option<Self::Item> {
if self.pending_slots.is_empty() {
⋮----
let slot = self.pending_slots.pop().unwrap();
if let Some(slot_meta) = self.blockstore.meta(slot).unwrap() {
self.pending_slots.extend(slot_meta.next_slots.iter());
Some((slot, slot_meta))
⋮----
mod tests {
⋮----
fn test_next_slots_iterator() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
blockstore.set_roots(std::iter::once(&0)).unwrap();
⋮----
let last_entry_hash = fill_blockstore_slot_with_ticks(
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 4, fork_point, fork_hash);
⋮----
.map(|(slot, _)| slot)
.collect();
let expected = vec![0, 1, 2, 3, 4].into_iter().collect();
assert_eq!(result, expected);
⋮----
let expected = vec![2, 3].into_iter().collect();
⋮----
let expected = vec![4].into_iter().collect();

================
File: ledger/src/rooted_slot_iterator.rs
================
pub struct RootedSlotIterator<'a> {
⋮----
pub fn new(start_slot: Slot, blockstore: &'a Blockstore) -> Result<Self> {
if blockstore.is_root(start_slot) {
Ok(Self {
next_slots: vec![start_slot],
⋮----
Err(BlockstoreError::SlotNotRooted)
⋮----
impl Iterator for RootedSlotIterator<'_> {
type Item = (Slot, Option<SlotMeta>);
fn next(&mut self) -> Option<Self::Item> {
⋮----
.iter()
.find(|x| self.blockstore.is_root(**x))
.map(|x| (Some(*x), false))
.unwrap_or_else(|| {
⋮----
.rooted_slot_iterator(
⋮----
.expect("Database failure, couldn't fetch rooted slots iterator");
iter.next();
(iter.next(), true)
⋮----
.map(|r| {
⋮----
.meta(r)
.expect("Database failure, couldn't fetch SlotMeta")
⋮----
.unwrap_or(None);
⋮----
self.next_slots.clone_from(&slot_meta.next_slots);
⋮----
if slot_meta.is_none() && slot_skipped {
warn!("Rooted SlotMeta was deleted in between checking is_root and fetch");
⋮----
rooted_slot.map(|r| {
⋮----
mod tests {
⋮----
fn test_rooted_slot_iterator() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
blockstore.set_roots(std::iter::once(&0)).unwrap();
⋮----
let last_entry_hash = fill_blockstore_slot_with_ticks(
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 4, fork_point, fork_hash);
blockstore.set_roots([1, 2, 3].iter()).unwrap();
assert!(RootedSlotIterator::new(4, &blockstore).is_err());
⋮----
.unwrap()
.map(|(slot, _)| slot)
.collect();
let expected = vec![3];
assert_eq!(result, expected);
⋮----
let expected = vec![0, 1, 2, 3];
⋮----
fn test_skipping_rooted_slot_iterator() {
⋮----
fill_blockstore_slot_with_ticks(
⋮----
blockstore.set_roots([0, 1, 2, 3].iter()).unwrap();
blockstore.set_roots(std::iter::once(&10)).unwrap();
⋮----
.map(|(slot, meta)| (slot, meta.is_some()))
⋮----
let expected = vec![(3, true), (10, false)];
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, 11, 10, Hash::default());
blockstore.set_roots(std::iter::once(&11)).unwrap();
⋮----
let expected = vec![

================
File: ledger/src/shred.rs
================
mod common;
pub(crate) mod merkle;
mod merkle_tree;
mod payload;
mod shred_code;
mod shred_data;
mod stats;
mod traits;
pub mod wire;
pub mod layout {
⋮----
pub type Nonce = u32;
const_assert_eq!(SIZE_OF_NONCE, 4);
⋮----
pub const fn get_data_shred_bytes_per_batch_typical() -> u64 {
⋮----
panic!("this is unreachable");
⋮----
bitflags! {
⋮----
impl ShredFlags {
pub(crate) fn from_reference_tick(reference_tick: u8) -> Self {
Self::from_bits_retain(Self::SHRED_TICK_REFERENCE_MASK.bits().min(reference_tick))
⋮----
pub enum Error {
⋮----
pub enum ShredType {
⋮----
enum ShredVariant {
⋮----
struct ShredCommonHeader {
⋮----
struct DataShredHeader {
⋮----
struct CodingShredHeader {
⋮----
pub enum Shred {
⋮----
pub struct ShredId(Slot,  u32, ShredType);
impl ShredId {
⋮----
pub fn new(slot: Slot, index: u32, shred_type: ShredType) -> ShredId {
ShredId(slot, index, shred_type)
⋮----
pub fn slot(&self) -> Slot {
⋮----
pub fn index(&self) -> u32 {
⋮----
pub fn shred_type(&self) -> ShredType {
⋮----
pub(crate) fn unpack(&self) -> (Slot,  u32, ShredType) {
⋮----
pub fn seed(&self, leader: &Pubkey) -> [u8; 32] {
⋮----
hashv(&[
&slot.to_le_bytes(),
&u8::from(*shred_type).to_le_bytes(),
&index.to_le_bytes(),
⋮----
.to_bytes()
⋮----
pub(crate) struct ErasureSetId(Slot,  u32);
impl ErasureSetId {
pub(crate) fn new(slot: Slot, fec_set_index: u32) -> Self {
Self(slot, fec_set_index)
⋮----
pub(crate) fn slot(&self) -> Slot {
⋮----
pub(crate) fn store_key(&self) -> (Slot,  u32) {
⋮----
macro_rules! dispatch {
⋮----
use dispatch;
impl Shred {
dispatch!(fn common_header(&self) -> &ShredCommonHeader);
⋮----
dispatch!(fn set_signature(&mut self, signature: Signature));
dispatch!(fn signed_data(&self) -> Result<Hash, Error>);
dispatch!(pub fn chained_merkle_root(&self) -> Result<Hash, Error>);
dispatch!(pub(crate) fn retransmitter_signature(&self) -> Result<Signature, Error>);
dispatch!(pub fn into_payload(self) -> Payload);
dispatch!(pub fn merkle_root(&self) -> Result<Hash, Error>);
dispatch!(pub fn payload(&self) -> &Payload);
dispatch!(pub fn sanitize(&self) -> Result<(), Error>);
⋮----
pub fn copy_to_packet(&self, packet: &mut Packet) {
let payload = self.payload();
let size = payload.len();
packet.buffer_mut()[..size].copy_from_slice(&payload[..]);
packet.meta_mut().size = size;
⋮----
pub fn new_from_serialized_shred<T>(shred: T) -> Result<Self, Error>
⋮----
Ok(match layout::get_shred_variant(shred.as_ref())? {
⋮----
pub fn id(&self) -> ShredId {
ShredId(self.slot(), self.index(), self.shred_type())
⋮----
self.common_header().slot
⋮----
pub fn parent(&self) -> Result<Slot, Error> {
⋮----
Self::ShredCode(_) => Err(Error::InvalidShredType),
Self::ShredData(shred) => shred.parent(),
⋮----
self.common_header().index
⋮----
pub(crate) fn bytes_to_store(&self) -> &[u8] {
⋮----
Self::ShredCode(shred) => shred.payload(),
Self::ShredData(shred) => shred.bytes_to_store(),
⋮----
pub fn fec_set_index(&self) -> u32 {
self.common_header().fec_set_index
⋮----
pub(crate) fn first_coding_index(&self) -> Option<u32> {
⋮----
Self::ShredCode(shred) => shred.first_coding_index(),
⋮----
pub fn version(&self) -> u16 {
self.common_header().version
⋮----
pub(crate) fn erasure_set(&self) -> ErasureSetId {
ErasureSetId(self.slot(), self.fec_set_index())
⋮----
pub fn signature(&self) -> &Signature {
&self.common_header().signature
⋮----
pub fn sign(&mut self, keypair: &Keypair) {
let data = self.signed_data().unwrap();
let signature = keypair.sign_message(data.as_ref());
self.set_signature(signature);
⋮----
ShredType::from(self.common_header().shred_variant)
⋮----
pub fn is_data(&self) -> bool {
self.shred_type() == ShredType::Data
⋮----
pub fn is_code(&self) -> bool {
self.shred_type() == ShredType::Code
⋮----
pub fn last_in_slot(&self) -> bool {
⋮----
Self::ShredData(shred) => shred.last_in_slot(),
⋮----
pub fn data_complete(&self) -> bool {
⋮----
Self::ShredData(shred) => shred.data_complete(),
⋮----
pub(crate) fn reference_tick(&self) -> u8 {
⋮----
Self::ShredCode(_) => ShredFlags::SHRED_TICK_REFERENCE_MASK.bits(),
Self::ShredData(shred) => shred.reference_tick(),
⋮----
pub fn verify(&self, pubkey: &Pubkey) -> bool {
match self.signed_data() {
Ok(data) => self.signature().verify(pubkey.as_ref(), data.as_ref()),
⋮----
pub(crate) fn erasure_mismatch(&self, other: &Self) -> Result<bool, Error> {
⋮----
(Self::ShredCode(shred), Self::ShredCode(other)) => Ok(shred.erasure_mismatch(other)),
_ => Err(Error::InvalidShredType),
⋮----
pub(crate) fn num_data_shreds(&self) -> Result<u16, Error> {
⋮----
Self::ShredCode(shred) => Ok(shred.num_data_shreds()),
Self::ShredData(_) => Err(Error::InvalidShredType),
⋮----
pub(crate) fn num_coding_shreds(&self) -> Result<u16, Error> {
⋮----
Self::ShredCode(shred) => Ok(shred.num_coding_shreds()),
⋮----
pub fn is_shred_duplicate(&self, other: &Shred) -> bool {
if self.id() != other.id() {
⋮----
fn get_payload(shred: &Shred) -> &[u8] {
let Ok(offset) = shred.retransmitter_signature_offset() else {
return shred.payload();
⋮----
debug_assert_eq!(offset + SIZE_OF_SIGNATURE, shred.payload().len());
⋮----
.payload()
.get(..offset)
.unwrap_or_else(|| shred.payload())
⋮----
get_payload(self) != get_payload(other)
⋮----
fn retransmitter_signature_offset(&self) -> Result<usize, Error> {
⋮----
Self::ShredCode(ShredCode::Merkle(shred)) => shred.retransmitter_signature_offset(),
Self::ShredData(ShredData::Merkle(shred)) => shred.retransmitter_signature_offset(),
⋮----
fn from(shred: ShredCode) -> Self {
⋮----
fn from(shred: ShredData) -> Self {
⋮----
fn from(shred: merkle::Shred) -> Self {
⋮----
type Error = Error;
fn try_from(shred: Shred) -> Result<Self, Self::Error> {
⋮----
Shred::ShredCode(ShredCode::Merkle(shred)) => Ok(Self::ShredCode(shred)),
Shred::ShredData(ShredData::Merkle(shred)) => Ok(Self::ShredData(shred)),
⋮----
fn from(shred_variant: ShredVariant) -> Self {
⋮----
fn from(shred_variant: ShredVariant) -> u8 {
⋮----
fn try_from(shred_variant: u8) -> Result<Self, Self::Error> {
⋮----
Err(Error::InvalidShredVariant)
⋮----
0x60 => Ok(ShredVariant::MerkleCode {
⋮----
0x70 => Ok(ShredVariant::MerkleCode {
⋮----
0x90 => Ok(ShredVariant::MerkleData {
⋮----
0xb0 => Ok(ShredVariant::MerkleData {
⋮----
_ => Err(Error::InvalidShredVariant),
⋮----
pub fn recover<T: IntoIterator<Item = Shred>>(
⋮----
.into_iter()
.map(|shred| {
debug_assert_matches!(
⋮----
Ok(shreds.map(|shred| shred.map(Shred::from)))
⋮----
pub fn should_discard_shred<'a, P>(
⋮----
debug_assert!(root < max_slot);
⋮----
if !erasure_config.is_fixed() {
⋮----
if enforce_fixed_fec_set(slot) {
⋮----
let Some(parent) = slot.checked_sub(Slot::from(parent_offset)) else {
⋮----
if shred_flags.contains(ShredFlags::DATA_COMPLETE_SHRED)
⋮----
if enforce_fixed_fec_set(slot) && discard_unexpected_data_complete_shreds(slot) {
⋮----
if shred_flags.contains(ShredFlags::LAST_SHRED_IN_SLOT)
&& !check_last_data_shred_index(index)
⋮----
if !check_fixed_fec_set(index, fec_set_index) {
⋮----
stats.num_shreds_merkle_code_chained.saturating_add(1);
⋮----
stats.num_shreds_merkle_data_chained.saturating_add(1);
⋮----
fn check_fixed_fec_set(index: u32, fec_set_index: u32) -> bool {
⋮----
&& fec_set_index.is_multiple_of(DATA_SHREDS_PER_FEC_BLOCK as u32)
⋮----
fn check_last_data_shred_index(index: u32) -> bool {
(index + 1).is_multiple_of(DATA_SHREDS_PER_FEC_BLOCK as u32)
⋮----
pub fn max_ticks_per_n_shreds(num_shreds: u64, shred_data_size: Option<usize>) -> u64 {
let ticks = create_ticks(1, 0, Hash::default());
max_entries_per_n_shred(&ticks[0], num_shreds, shred_data_size)
⋮----
pub fn max_entries_per_n_shred_last_or_not(
⋮----
let vec_size = wincode::serialized_size(&vec![entry]).unwrap();
let entry_size = wincode::serialized_size(entry).unwrap();
⋮----
ShredData::capacity( 6,  false).unwrap() as u64;
⋮----
ShredData::capacity( 6,  true).unwrap() as u64;
⋮----
pub fn max_entries_per_n_shred(
⋮----
let data_buffer_size = ShredData::capacity( 6,  true).unwrap();
let shred_data_size = shred_data_size.unwrap_or(data_buffer_size) as u64;
⋮----
pub fn verify_test_data_shred(
⋮----
shred.sanitize().unwrap();
assert!(shred.is_data());
assert_eq!(shred.index(), index);
assert_eq!(shred.slot(), slot);
assert_eq!(shred.parent().unwrap(), parent);
assert_eq!(verify, shred.verify(pk));
⋮----
assert!(shred.last_in_slot());
⋮----
assert!(!shred.last_in_slot());
⋮----
assert!(shred.data_complete());
⋮----
assert!(!shred.data_complete());
⋮----
mod tests {
⋮----
pub(super) fn make_merkle_shreds_for_tests<R: Rng>(
⋮----
let thread_pool = ThreadPoolBuilder::new().num_threads(2).build().unwrap();
let chained_merkle_root = Hash::new_from_array(rng.random());
let parent_offset = rng.random_range(1..=u16::try_from(slot).unwrap_or(u16::MAX));
let parent_slot = slot.checked_sub(u64::from(parent_offset)).unwrap();
let mut data = vec![0u8; data_size];
let fec_set_index = rng.random_range(0..21) * DATA_SHREDS_PER_FEC_BLOCK as u32;
rng.fill(&mut data[..]);
⋮----
rng.random(),
rng.random_range(1..64),
⋮----
fn test_shred_constants() {
⋮----
assert_eq!(
⋮----
fn test_shred_flags_reference_tick_saturates() {
const MAX_REFERENCE_TICK: u8 = ShredFlags::SHRED_TICK_REFERENCE_MASK.bits();
⋮----
assert_eq!(flags.bits(), tick.min(MAX_REFERENCE_TICK));
⋮----
fn test_invalid_parent_offset() {
⋮----
assert_matches!(shred.parent(), Ok(9));
⋮----
shred.copy_to_packet(&mut packet);
wire::corrupt_and_set_parent_offset(packet.buffer_mut(), 1000);
let shred_res = Shred::new_from_serialized_shred(packet.data(..).unwrap().to_vec());
assert_matches!(
⋮----
fn test_should_discard_shred(is_last_in_slot: bool) {
⋮----
let shreds = make_merkle_shreds_for_tests(
⋮----
.unwrap();
let shreds: Vec<_> = shreds.into_iter().map(Shred::from).collect();
assert_eq!(shreds.iter().map(Shred::fec_set_index).dedup().count(), 1);
assert_matches!(shreds[0].shred_type(), ShredType::Data);
let parent_slot = shreds[0].parent().unwrap();
let shred_version = shreds[0].common_header().version;
let root = rng.random_range(0..parent_slot);
let max_slot = slot + rng.random_range(1..65536);
⋮----
let shred = shreds.first().unwrap();
assert_eq!(shred.shred_type(), ShredType::Data);
⋮----
assert!(!should_discard_shred(
⋮----
let mut packet = packet.clone();
⋮----
packet.meta_mut().size = OFFSET_OF_SHRED_VARIANT;
assert!(should_discard_shred(
⋮----
assert_eq!(stats.index_overrun, 1);
packet.meta_mut().size = OFFSET_OF_SHRED_INDEX;
⋮----
assert_eq!(stats.index_overrun, 2);
packet.meta_mut().size = OFFSET_OF_SHRED_INDEX + 1;
⋮----
assert_eq!(stats.index_overrun, 3);
packet.meta_mut().size = OFFSET_OF_SHRED_INDEX + SIZE_OF_SHRED_INDEX - 1;
⋮----
assert_eq!(stats.index_overrun, 4);
packet.meta_mut().size = OFFSET_OF_SHRED_INDEX + SIZE_OF_SHRED_INDEX + 2;
⋮----
assert_eq!(stats.index_overrun, 5);
⋮----
assert_eq!(stats.shred_version_mismatch, 1);
⋮----
assert_eq!(stats.slot_out_of_range, 1);
⋮----
let mut cursor = Cursor::new(packet.buffer_mut());
cursor.seek(SeekFrom::Start(83)).unwrap();
cursor.write_all(&parent_offset.to_le_bytes()).unwrap();
⋮----
let parent_offset = u16::try_from(slot + 1).unwrap();
⋮----
assert_eq!(stats.bad_parent_offset, 1);
⋮----
.seek(SeekFrom::Start(OFFSET_OF_SHRED_INDEX as u64))
⋮----
cursor.write_all(&index.to_le_bytes()).unwrap();
⋮----
assert_eq!(layout::get_index(packet.data(..).unwrap()), Some(index));
⋮----
assert_eq!(stats.index_out_of_bounds, 1);
⋮----
let shred = shreds.last().unwrap();
assert_eq!(shred.shred_type(), ShredType::Code);
shreds.last().unwrap().copy_to_packet(&mut packet);
⋮----
let index = u32::try_from(MAX_CODE_SHREDS_PER_SLOT).unwrap();
⋮----
fn test_should_discard_shred_fec_set_checks(enforce_fixed_fec_set: bool) {
⋮----
shreds[0].copy_to_packet(&mut packet);
⋮----
.seek(SeekFrom::Start(OFFSET_OF_FEC_SET_INDEX as u64))
⋮----
cursor.write_all(&bad_fec_set_index.to_le_bytes()).unwrap();
⋮----
let should_discard = should_discard_shred(
⋮----
assert_eq!(should_discard, enforce_fixed_fec_set);
assert_eq!(stats.misaligned_fec_set, 1);
⋮----
cursor.write_all(&bad_index.to_le_bytes()).unwrap();
⋮----
cursor.write_all(&fec_set_index.to_le_bytes()).unwrap();
⋮----
.iter()
.find(|s| s.shred_type() == ShredType::Code)
⋮----
code_shred.copy_to_packet(&mut packet);
⋮----
.seek(SeekFrom::Start(OFFSET_OF_NUM_DATA as u64))
⋮----
cursor.write_all(&bad_num_data.to_le_bytes()).unwrap();
⋮----
assert_eq!(stats.misaligned_erasure_config, 1);
⋮----
.filter(|s| s.shred_type() == ShredType::Data)
.collect();
let last_data_shred = data_shreds.last().unwrap();
assert!(last_data_shred.last_in_slot());
⋮----
last_data_shred.copy_to_packet(&mut packet);
⋮----
cursor.write_all(&bad_last_index.to_le_bytes()).unwrap();
⋮----
assert_eq!(stats.misaligned_last_data_index, 1);
⋮----
fn test_shred_type_compat() {
assert_eq!(std::mem::size_of::<ShredType>(), std::mem::size_of::<u8>());
assert_matches!(ShredType::try_from(0u8), Err(_));
assert_matches!(ShredType::try_from(1u8), Err(_));
assert_matches!(bincode::deserialize::<ShredType>(&[0u8]), Err(_));
assert_matches!(bincode::deserialize::<ShredType>(&[1u8]), Err(_));
assert_eq!(ShredType::Data as u8, 0b1010_0101);
assert_eq!(u8::from(ShredType::Data), 0b1010_0101);
assert_eq!(ShredType::try_from(0b1010_0101), Ok(ShredType::Data));
let buf = bincode::serialize(&ShredType::Data).unwrap();
assert_eq!(buf, vec![0b1010_0101]);
⋮----
assert_eq!(ShredType::Code as u8, 0b0101_1010);
assert_eq!(u8::from(ShredType::Code), 0b0101_1010);
assert_eq!(ShredType::try_from(0b0101_1010), Ok(ShredType::Code));
let buf = bincode::serialize(&ShredType::Code).unwrap();
assert_eq!(buf, vec![0b0101_1010]);
⋮----
fn test_shred_variant_compat() {
assert_matches!(ShredVariant::try_from(0u8), Err(_));
assert_matches!(ShredVariant::try_from(1u8), Err(_));
assert_matches!(ShredVariant::try_from(0b0101_0000), Err(_));
assert_matches!(ShredVariant::try_from(0b1010_0000), Err(_));
assert_matches!(bincode::deserialize::<ShredVariant>(&[0b0101_0000]), Err(_));
assert_matches!(bincode::deserialize::<ShredVariant>(&[0b1010_0000]), Err(_));
assert_matches!(ShredVariant::try_from(0b0101_1010), Err(_));
assert_matches!(bincode::deserialize::<ShredVariant>(&[0b0101_1010]), Err(_));
assert_matches!(ShredVariant::try_from(0b1010_0101), Err(_));
assert_matches!(bincode::deserialize::<ShredVariant>(&[0b1010_0101]), Err(_));
⋮----
fn test_shred_variant_compat_merkle_code(resigned: bool, byte: u8) {
⋮----
assert_eq!(buf, vec![byte]);
⋮----
fn test_shred_variant_compat_merkle_data(resigned: bool, byte: u8) {
⋮----
fn test_shred_seed() {
⋮----
let leader = Pubkey::new_from_array(rng.random());
let key = ShredId(
⋮----
fn verify_shred_layout(shred: &Shred, packet: &Packet) {
let data = layout::get_shred(packet).unwrap();
assert_eq!(data, packet.data(..).unwrap());
assert_eq!(layout::get_slot(data), Some(shred.slot()));
assert_eq!(layout::get_index(data), Some(shred.index()));
assert_eq!(layout::get_version(data), Some(shred.version()));
assert_eq!(layout::get_shred_id(data), Some(shred.id()));
assert_eq!(layout::get_signature(data), Some(*shred.signature()));
assert_eq!(layout::get_shred_type(data).unwrap(), shred.shred_type());
match shred.shred_type() {
⋮----
let parent_offset = layout::get_parent_offset(data).unwrap();
let slot = layout::get_slot(data).unwrap();
let parent = slot.checked_sub(Slot::from(parent_offset)).unwrap();
assert_eq!(parent, shred.parent().unwrap());
⋮----
fn test_serde_compat_shred_data() {
⋮----
rng.fill(&mut seed[..]);
⋮----
let keypair = keypair_from_seed(&seed).unwrap();
⋮----
let shredder = Shredder::new(slot, slot.saturating_sub(1), 0, 42).unwrap();
⋮----
.make_shreds_from_data_slice(
⋮----
.unwrap()
.next()
⋮----
shred.sign(&keypair);
assert!(shred.verify(&keypair.pubkey()));
assert_matches!(shred.sanitize(), Ok(()));
let payload = bs58::decode(PAYLOAD).into_vec().unwrap();
⋮----
packet.buffer_mut()[..payload.len()].copy_from_slice(&payload);
packet.meta_mut().size = payload.len();
assert_eq!(shred.bytes_to_store(), payload);
⋮----
verify_shred_layout(&shred, &packet);
⋮----
fn test_serde_compat_shred_data_empty() {
⋮----
fn test_shred_flags_serde() {
let flags: ShredFlags = bincode::deserialize(&[0b0001_0101]).unwrap();
assert_eq!(flags, ShredFlags::from_bits(0b0001_0101).unwrap());
assert!(!flags.contains(ShredFlags::DATA_COMPLETE_SHRED));
assert!(!flags.contains(ShredFlags::LAST_SHRED_IN_SLOT));
assert_eq!((flags & ShredFlags::SHRED_TICK_REFERENCE_MASK).bits(), 21u8);
assert_eq!(bincode::serialize(&flags).unwrap(), [0b0001_0101]);
let flags: ShredFlags = bincode::deserialize(&[0b0111_0001]).unwrap();
assert_eq!(flags, ShredFlags::from_bits(0b0111_0001).unwrap());
assert!(flags.contains(ShredFlags::DATA_COMPLETE_SHRED));
⋮----
assert_eq!((flags & ShredFlags::SHRED_TICK_REFERENCE_MASK).bits(), 49u8);
assert_eq!(bincode::serialize(&flags).unwrap(), [0b0111_0001]);
let flags: ShredFlags = bincode::deserialize(&[0b1110_0101]).unwrap();
assert_eq!(flags, ShredFlags::from_bits(0b1110_0101).unwrap());
⋮----
assert!(flags.contains(ShredFlags::LAST_SHRED_IN_SLOT));
assert_eq!((flags & ShredFlags::SHRED_TICK_REFERENCE_MASK).bits(), 37u8);
assert_eq!(bincode::serialize(&flags).unwrap(), [0b1110_0101]);
let flags: ShredFlags = bincode::deserialize(&[0b1011_1101]).unwrap();
assert_eq!(flags, ShredFlags::from_bits(0b1011_1101).unwrap());
⋮----
assert_eq!((flags & ShredFlags::SHRED_TICK_REFERENCE_MASK).bits(), 61u8);
assert_eq!(bincode::serialize(&flags).unwrap(), [0b1011_1101]);
⋮----
fn test_shred_flags_data_complete() {
⋮----
flags.insert(ShredFlags::LAST_SHRED_IN_SLOT);
⋮----
let mut flags = ShredFlags::from_bits(0b0011_1111).unwrap();
⋮----
let mut flags: ShredFlags = bincode::deserialize(&[0b1011_1111]).unwrap();
⋮----
fn test_is_shred_duplicate(is_last_in_slot: bool) {
fn fill_retransmitter_signature<R: Rng>(
⋮----
let mut shred = shred.into_payload();
⋮----
rng.fill(&mut signature[..]);
⋮----
&mut shred.as_mut(),
⋮----
assert_matches!(out, Ok(()));
⋮----
assert_matches!(out, Err(Error::InvalidShredVariant));
⋮----
Shred::new_from_serialized_shred(shred).unwrap()
⋮----
let slot = 285_376_049 + rng.random_range(0..100_000);
let shreds: Vec<_> = make_merkle_shreds_for_tests(
⋮----
.map(Shred::from)
.map(|shred| fill_retransmitter_signature(&mut rng, shred, is_last_in_slot))
⋮----
let num_data_shreds = shreds.iter().filter(|shred| shred.is_data()).count();
let num_coding_shreds = shreds.iter().filter(|shred| shred.is_code()).count();
assert!(num_data_shreds > if is_last_in_slot { 31 } else { 5 });
assert!(num_coding_shreds > if is_last_in_slot { 31 } else { 20 });
⋮----
assert!(!shred.is_shred_duplicate(other));
⋮----
let other = fill_retransmitter_signature(&mut rng, shred.clone(), is_last_in_slot);
⋮----
assert_ne!(shred.payload(), other.payload());
⋮----
assert!(!shred.is_shred_duplicate(&other));
assert!(!other.is_shred_duplicate(shred));
⋮----
let mut other = shred.payload().clone();
other.as_mut()[90] = other[90].wrapping_add(1);
let other = Shred::new_from_serialized_shred(other).unwrap();
⋮----
assert!(shred.is_shred_duplicate(&other));
assert!(other.is_shred_duplicate(shred));
⋮----
fn test_data_complete_shred_index_validation() {
⋮----
.find(|s| s.shred_type() == ShredType::Data)
⋮----
let parent_slot = data_shred.parent().unwrap();
let shred_version = data_shred.common_header().version;
⋮----
data_shred.copy_to_packet(&mut packet);
⋮----
cursor.write_all(&wrong_index.to_le_bytes()).unwrap();
⋮----
.seek(SeekFrom::Start(OFFSET_OF_SHRED_FLAGS as u64))
⋮----
.write_all(&[ShredFlags::DATA_COMPLETE_SHRED.bits()])
⋮----
assert!(should_discard);
assert_eq!(stats.unexpected_data_complete_shred, 1);
⋮----
cursor.write_all(&correct_index.to_le_bytes()).unwrap();
⋮----
assert!(!should_discard);
assert_eq!(stats.unexpected_data_complete_shred, 0);

================
File: ledger/src/shredder.rs
================
.num_threads(get_thread_count())
.thread_name(|i| format!("solShredder{i:02}"))
.build()
.unwrap()
⋮----
type LruCacheOnce<K, V> = RwLock<LruCache<K, Arc<OnceLock<V>>>>;
pub struct ReedSolomonCache(
⋮----
pub struct Shredder {
⋮----
impl Shredder {
pub fn new(
⋮----
Err(Error::InvalidParentSlot { slot, parent_slot })
⋮----
Ok(Self {
⋮----
pub fn make_merkle_shreds_from_entries(
⋮----
let entries = wincode::serialize(entries).unwrap();
stats.serialize_elapsed += now.elapsed().as_micros() as u64;
⋮----
pub fn make_shreds_from_data_slice(
⋮----
Ok(shreds.into_iter().map(Shred::from))
⋮----
pub fn entries_to_merkle_shreds_for_tests(
⋮----
self.make_merkle_shreds_from_entries(
⋮----
.partition(Shred::is_data)
⋮----
pub fn deshred<I, T: AsRef<[u8]>>(shreds: I) -> Result<Vec<u8>, Error>
⋮----
let (data, _, data_complete) = shreds.into_iter().try_fold(
⋮----
return Err(Error::InvalidDeshredSet);
⋮----
let shred = shred.as_ref();
let index = Some(
⋮----
.ok_or_else(|| Error::InvalidPayloadSize(shred.len()))?,
⋮----
if prev.checked_add(1) != index {
return Err(Error::from(TooFewDataShards));
⋮----
data.extend_from_slice(shred::layout::get_data(shred)?);
⋮----
let data_complete = flags.contains(ShredFlags::DATA_COMPLETE_SHRED);
Ok((data, index, data_complete))
⋮----
if data.is_empty() {
⋮----
ShredData::capacity( 0,  false).unwrap();
Ok(vec![0u8; data_buffer_size])
⋮----
Ok(data)
⋮----
pub fn single_shred_for_tests(slot: Slot, keypair: &Keypair) -> Shred {
let shredder = Shredder::new(slot, slot.saturating_sub(1), 0, 42).unwrap();
⋮----
let (mut shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
shreds.pop().unwrap()
⋮----
impl ReedSolomonCache {
⋮----
pub(crate) fn get(
⋮----
let entry = self.0.read().unwrap().get(&key).cloned();
let entry: Arc<OnceLock<Result<_, _>>> = entry.unwrap_or_else(|| {
let mut cache = self.0.write().unwrap();
cache.get(&key).cloned().unwrap_or_else(|| {
⋮----
cache.put(key, Arc::clone(&entry));
⋮----
.get_or_init(|| ReedSolomon::new(data_shards, parity_shards).map(Arc::new))
.clone()
⋮----
impl Default for ReedSolomonCache {
fn default() -> Self {
Self(RwLock::new(LruCache::new(Self::CAPACITY)))
⋮----
mod tests {
⋮----
fn verify_test_code_shred(shred: &Shred, index: u32, slot: Slot, pk: &Pubkey, verify: bool) {
assert_matches!(shred.sanitize(), Ok(()));
assert!(!shred.is_data());
assert_eq!(shred.index(), index);
assert_eq!(shred.slot(), slot);
assert_eq!(verify, shred.verify(pk));
⋮----
fn run_test_data_shredder(slot: Slot, is_last_in_slot: bool) {
⋮----
assert_matches!(
⋮----
let shredder = Shredder::new(slot, parent_slot, 0, 0).unwrap();
⋮----
.map(|_| {
⋮----
system_transaction::transfer(&keypair0, &keypair1.pubkey(), 1, Hash::default());
Entry::new(&Hash::default(), 1, vec![tx0])
⋮----
.collect();
⋮----
let (data_shreds, coding_shreds) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
Hash::new_from_array(rand::rng().random()),
⋮----
let next_index = data_shreds.last().unwrap().index() + 1;
assert_eq!(next_index as usize, num_expected_data_shreds);
⋮----
for shred in data_shreds.iter() {
assert_eq!(shred.shred_type(), ShredType::Data);
let index = shred.index();
⋮----
verify_test_data_shred(
⋮----
&keypair.pubkey(),
⋮----
assert!(!data_shred_indexes.contains(&index));
data_shred_indexes.insert(index);
⋮----
for shred in coding_shreds.iter() {
⋮----
assert_eq!(shred.shred_type(), ShredType::Code);
verify_test_code_shred(shred, index, slot, &keypair.pubkey(), true);
assert!(!coding_shred_indexes.contains(&index));
coding_shred_indexes.insert(index);
⋮----
assert!(data_shred_indexes.contains(&i));
⋮----
assert!(coding_shred_indexes.contains(&i));
⋮----
assert_eq!(data_shred_indexes.len(), num_expected_data_shreds);
assert_eq!(coding_shred_indexes.len(), num_expected_coding_shreds);
⋮----
let shreds = data_shreds.iter().map(Shred::payload);
Shredder::deshred(shreds).unwrap()
⋮----
let deshred_entries: Vec<Entry> = wincode::deserialize(&deshred_payload).unwrap();
assert_eq!(entries, deshred_entries);
⋮----
fn test_data_shredder(is_last_in_slot: bool) {
run_test_data_shredder(0x1234_5678_9abc_def0, is_last_in_slot);
⋮----
fn test_deserialize_shred_payload(is_last_in_slot: bool) {
⋮----
.unwrap();
⋮----
for shred in [data_shreds, coding_shreds].into_iter().flatten() {
let other = Shred::new_from_serialized_shred(shred.payload().clone());
assert_eq!(shred, other.unwrap());
⋮----
fn test_shred_reference_tick(is_last_in_slot: bool) {
⋮----
let shredder = Shredder::new(slot, parent_slot, 5, 0).unwrap();
⋮----
let (data_shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
data_shreds.iter().for_each(|s| {
assert_eq!(s.reference_tick(), 5);
assert_eq!(shred::layout::get_reference_tick(s.payload()).unwrap(), 5);
⋮----
Shred::new_from_serialized_shred(data_shreds.last().unwrap().payload().clone())
⋮----
assert_eq!(deserialized_shred.reference_tick(), 5);
⋮----
fn test_shred_reference_tick_overflow(is_last_in_slot: bool) {
⋮----
let shredder = Shredder::new(slot, parent_slot, u8::MAX, 0).unwrap();
⋮----
assert_eq!(
⋮----
fn run_test_data_and_code_shredder(slot: Slot, is_last_in_slot: bool) {
⋮----
let shredder = Shredder::new(slot, slot - 5, 0, 0).unwrap();
⋮----
ShredData::capacity( 6,  false).unwrap();
let num_entries = max_ticks_per_n_shreds(1, Some(data_buffer_size)) + 1;
⋮----
for (i, s) in data_shreds.iter().enumerate() {
⋮----
s.index(),
⋮----
i == data_shreds.len() - 1 && is_last_in_slot,
i == data_shreds.len() - 1,
⋮----
verify_test_code_shred(&s, s.index(), slot, &keypair.pubkey(), true);
⋮----
fn test_data_and_code_shredder(is_last_in_slot: bool) {
run_test_data_and_code_shredder(0x1234_5678_9abc_def0, is_last_in_slot);
⋮----
fn test_shred_version(is_last_in_slot: bool) {
⋮----
let hash = hash(Hash::default().as_ref());
⋮----
assert_ne!(version, 0);
let shredder = Shredder::new(0, 0, 0, version).unwrap();
⋮----
assert!(!data_shreds
⋮----
fn test_shred_fec_set_index(is_last_in_slot: bool) {
⋮----
.iter()
.chunk_by(|shred| shred.fec_set_index())
.into_iter()
.map(|(fec_set_index, chunk)| (fec_set_index, chunk.count()))
⋮----
assert!(chunks
⋮----
assert_eq!(chunks[0].0, start_index);
assert!(chunks.iter().tuple_windows().all(
⋮----
assert!(coding_shreds.len() >= data_shreds.len());
assert!(coding_shreds

================
File: ledger/src/sigverify_shreds.rs
================
pub type LruCache = lazy_lru::LruCache<(Signature, Pubkey,  Hash), ()>;
pub type SlotPubkeys = HashMap<Slot, Pubkey, BuildNoHashHasher<Slot>>;
⋮----
pub fn verify_shred_cpu(
⋮----
if packet.meta().discard() {
⋮----
trace!("slot {slot}");
let Some(pubkey) = slot_leaders.get(&slot) else {
⋮----
trace!("signature {signature}");
⋮----
if cache.read().unwrap().get(&key).is_some() {
⋮----
} else if key.0.verify(key.1.as_ref(), key.2.as_ref()) {
cache.write().unwrap().put(key, ());
⋮----
pub fn verify_shreds(
⋮----
thread_pool.install(|| {
⋮----
.into_par_iter()
.map(|batch| {
⋮----
.par_iter()
.map(|packet| u8::from(verify_shred_cpu(packet, slot_leaders, cache)))
.collect()
⋮----
fn sign_shred_cpu(keypair: &Keypair, packet: &mut PacketRefMut) {
⋮----
let msg = shred::layout::get_shred(packet.as_ref())
.and_then(shred::layout::get_signed_data)
.unwrap();
assert!(
⋮----
let signature = keypair.sign_message(msg.as_ref());
trace!("signature {signature:?}");
⋮----
.data(..)
.expect("packet should not be discarded")
.to_vec();
buffer[sig].copy_from_slice(signature.as_ref());
packet.copy_from_slice(&buffer);
⋮----
mod tests {
⋮----
fn sign_shreds(thread_pool: &ThreadPool, keypair: &Keypair, batches: &mut [PacketBatch]) {
⋮----
batches.par_iter_mut().for_each(|batch| {
⋮----
.par_iter_mut()
.for_each(|mut p| sign_shred_cpu(keypair, &mut p));
⋮----
fn run_test_sigverify_shred_cpu(slot: Slot) {
⋮----
let shredder = Shredder::new(slot, slot.saturating_sub(1), 0, 0).unwrap();
⋮----
let (mut shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
let shred = shreds.pop().unwrap();
assert_eq!(shred.slot(), slot);
trace!("signature {}", shred.signature());
packet.buffer_mut()[..shred.payload().len()].copy_from_slice(shred.payload());
packet.meta_mut().size = shred.payload().len();
let leader_slots: SlotPubkeys = [(slot, keypair.pubkey())].into_iter().collect();
assert!(verify_shred_cpu((&packet).into(), &leader_slots, &cache));
⋮----
let leader_slots: SlotPubkeys = [(slot, wrong_keypair.pubkey())].into_iter().collect();
assert!(!verify_shred_cpu((&packet).into(), &leader_slots, &cache));
⋮----
fn test_sigverify_shred_cpu() {
run_test_sigverify_shred_cpu(0xdead_c0de);
⋮----
fn run_test_sigverify_shreds_cpu(thread_pool: &ThreadPool, slot: Slot) {
⋮----
let batch = make_packet_batch(&keypair, slot);
⋮----
let rv = verify_shreds(thread_pool, &batches, &leader_slots, &cache);
assert_eq!(rv.into_iter().flatten().all_equal_value().unwrap(), 1);
⋮----
assert_eq!(rv.into_iter().flatten().all_equal_value().unwrap(), 0);
⋮----
.iter_mut()
.for_each(|mut packet_ref| packet_ref.meta_mut().size = 0);
⋮----
fn test_sigverify_shreds_cpu() {
let thread_pool = ThreadPoolBuilder::new().num_threads(3).build().unwrap();
run_test_sigverify_shreds_cpu(&thread_pool, 0xdead_c0de);
⋮----
fn run_test_sigverify_shreds(thread_pool: &ThreadPool, slot: Slot) {
⋮----
let leader_slots: SlotPubkeys = [(u64::MAX, Pubkey::default()), (slot, keypair.pubkey())]
.into_iter()
.collect();
⋮----
(slot, wrong_keypair.pubkey()),
⋮----
let leader_slots: SlotPubkeys = [(u64::MAX, Pubkey::default())].into_iter().collect();
⋮----
.for_each(|mut pr| pr.meta_mut().size = 0);
⋮----
fn make_packet_batch(keypair: &Keypair, slot: u64) -> PacketBatch {
⋮----
let (shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
batch.resize(shreds.len(), Packet::default());
for i in 0..shreds.len() {
batch[i].buffer_mut()[..shreds[i].payload().len()].copy_from_slice(shreds[i].payload());
batch[i].meta_mut().size = shreds[i].payload().len();
⋮----
fn test_sigverify_shreds() {
⋮----
run_test_sigverify_shreds(&thread_pool, 0xdead_c0de);
⋮----
fn make_transaction<R: Rng>(rng: &mut R) -> Transaction {
⋮----
rng.random(),
⋮----
fn make_entry<R: Rng>(rng: &mut R, prev_hash: &Hash) -> Entry {
let size = rng.random_range(16..32);
let txs = repeat_with(|| make_transaction(rng)).take(size).collect();
⋮----
rng.random_range(1..64),
⋮----
fn make_entries<R: Rng>(rng: &mut R, num_entries: usize) -> Vec<Entry> {
⋮----
let entry = make_entry(rng, &prev_hash);
std::iter::successors(Some(entry), |entry| Some(make_entry(rng, &entry.hash)))
.take(num_entries)
⋮----
fn make_shreds<R: Rng>(
⋮----
.iter()
.flat_map(|(&slot, keypair)| {
let parent_slot = slot - rng.random::<u16>().max(1) as Slot;
let num_entries = rng.random_range(64..128);
⋮----
rng.random_range(0..0x40),
⋮----
.unwrap()
.make_merkle_shreds_from_entries(
⋮----
&make_entries(rng, num_entries),
⋮----
Hash::new_from_array(rng.random()),
rng.random_range(0..2671),
rng.random_range(0..2781),
⋮----
shreds.shuffle(rng);
⋮----
let pubkey = keypairs[&shred.slot()].pubkey();
assert!(shred.verify(&pubkey));
assert_matches!(shred.sanitize(), Ok(()));
⋮----
let shred = shred.payload();
let slot = shred::layout::get_slot(shred).unwrap();
let signature = shred::layout::get_signature(shred).unwrap();
let pubkey = keypairs[&slot].pubkey();
let data = shred::layout::get_signed_data(shred).unwrap();
assert!(signature.verify(pubkey.as_ref(), data.as_ref()));
⋮----
fn make_packets<R: Rng>(rng: &mut R, shreds: &[Shred]) -> Vec<PacketBatch> {
let mut packets = shreds.iter().map(|shred| {
⋮----
shred.copy_to_packet(&mut packet);
⋮----
let packets: Vec<PacketBatch> = repeat_with(|| {
let size = rng.random_range(0..16);
let packets = packets.by_ref().take(size).collect();
⋮----
(size == 0 || !batch.is_empty()).then_some(batch.into())
⋮----
.while_some()
⋮----
assert_eq!(
⋮----
fn test_verify_shreds_fuzz(is_last_in_slot: bool) {
⋮----
let keypairs = repeat_with(|| rng.random_range(169_367_809..169_906_789))
.map(|slot| (slot, Keypair::new()))
.take(3)
⋮----
let shreds = make_shreds(&mut rng, is_last_in_slot, &keypairs);
⋮----
.map(|(&slot, keypair)| (slot, keypair.pubkey()))
.chain(once((Slot::MAX, Pubkey::default())))
⋮----
let mut packets = make_packets(&mut rng, &shreds);
⋮----
.map(|packets| {
⋮----
unreachable!()
⋮----
.map(|packet| {
let coin_flip: bool = rng.random();
⋮----
assert_eq!(verify_shreds(&thread_pool, &packets, &pubkeys, &cache), out);
⋮----
fn test_sign_shreds(is_last_in_slot: bool) {
⋮----
make_shreds(&mut rng, is_last_in_slot, &keypairs)
⋮----
let pubkey = keypair.pubkey();
⋮----
.map(Shred::slot)
.map(|slot| (slot, pubkey))
⋮----
sign_shreds(&thread_pool, &keypair, &mut packets);

================
File: ledger/src/slot_stats.rs
================
pub(crate) enum ShredSource {
⋮----
bitflags! {
⋮----
pub struct SlotStats {
⋮----
impl SlotStats {
pub fn get_min_index_count(&self) -> usize {
⋮----
.values()
.min()
.copied()
.unwrap_or_default()
⋮----
fn report(&self, slot: Slot) {
let min_fec_set_count = self.get_min_index_count();
datapoint_info!(
⋮----
pub struct SlotsStats {
⋮----
impl Default for SlotsStats {
fn default() -> Self {
⋮----
impl SlotsStats {
fn get_or_default_with_eviction_check(
⋮----
let evicted = if stats.contains(&slot) {
⋮----
let evicted = stats.push(slot, SlotStats::default());
⋮----
assert_ne!(evicted_slot, slot);
⋮----
(stats.get_mut(&slot).unwrap(), evicted)
⋮----
pub(crate) fn record_shred(
⋮----
let mut stats = self.stats.lock().unwrap();
⋮----
.entry(fec_set_index)
.or_default() += 1
⋮----
if meta.is_full() {
slot_stats.last_index = meta.last_index.unwrap();
if !slot_stats.flags.contains(SlotFlags::FULL) {
⋮----
Some((slot_stats.num_repaired, slot_stats.num_recovered));
⋮----
let slot_meta = slot_meta.unwrap();
⋮----
solana_time_utils::timestamp().saturating_sub(slot_meta.first_shred_timestamp);
⋮----
.and_then(|ix| i64::try_from(ix).ok())
.unwrap_or(-1);
⋮----
evicted_stats.report(evicted_slot);
⋮----
fn add_flag(&self, slot: Slot, flag: SlotFlags) {
⋮----
pub fn mark_dead(&self, slot: Slot) {
self.add_flag(slot, SlotFlags::DEAD);
⋮----
pub fn mark_rooted(&self, slot: Slot) {
self.add_flag(slot, SlotFlags::ROOTED);

================
File: ledger/src/staking_utils.rs
================
pub(crate) mod tests {
⋮----
pub(crate) fn setup_vote_and_stake_accounts(
⋮----
let vote_pubkey = vote_account.pubkey();
fn process_instructions<T: Signers>(bank: &Bank, keypairs: &T, ixs: &[Instruction]) {
⋮----
Some(&keypairs.pubkeys()[0]),
⋮----
bank.last_blockhash(),
⋮----
bank.process_transaction(&tx).unwrap();
⋮----
process_instructions(
⋮----
&from_account.pubkey(),
⋮----
node_pubkey: validator_identity_account.pubkey(),
⋮----
let stake_account_pubkey = stake_account_keypair.pubkey();
⋮----
bincode::serialize(&stake_account).unwrap(),
⋮----
bank.store_account(&stake_account_pubkey, &account);
⋮----
fn test_to_staked_nodes() {
⋮----
stakes.push((
⋮----
let vote_accounts = stakes.into_iter().map(|(stake, vote_state)| {
⋮----
rng.random(),
⋮----
.unwrap();
⋮----
let vote_account = VoteAccount::try_from(account).unwrap();
⋮----
let result = vote_accounts.collect::<VoteAccounts>().staked_nodes();
assert_eq!(result.len(), 2);
assert_eq!(result[&node1], 3);
assert_eq!(result[&node2], 5);

================
File: ledger/src/token_balances.rs
================
fn get_mint_decimals(bank: &Bank, mint: &Pubkey) -> Option<u8> {
⋮----
Some(spl_token::native_mint::DECIMALS)
⋮----
let mint_account = bank.get_account(mint)?;
if !is_known_spl_token_id(mint_account.owner()) {
⋮----
let decimals = StateWithExtensions::<Mint>::unpack(mint_account.data())
.map(|mint| mint.base.decimals)
.ok()?;
Some(decimals)
⋮----
pub fn collect_token_balances(
⋮----
let mut balances: TransactionTokenBalances = vec![];
⋮----
for transaction in batch.sanitized_transactions() {
let account_keys = transaction.account_keys();
let has_token_program = account_keys.iter().any(is_known_spl_token_id);
let mut transaction_balances: Vec<TransactionTokenBalance> = vec![];
⋮----
for (index, account_id) in account_keys.iter().enumerate() {
if transaction.is_invoked(index) || is_known_spl_token_id(account_id) {
⋮----
}) = collect_token_balance_from_account(
⋮----
transaction_balances.push(TransactionTokenBalance {
⋮----
balances.push(transaction_balances);
⋮----
collect_time.stop();
datapoint_debug!(
⋮----
struct TokenBalanceData {
⋮----
fn collect_token_balance_from_account(
⋮----
account_overrides.and_then(|overrides| overrides.get(account_id))
⋮----
Some(account_override.clone())
⋮----
bank.get_account(account_id)
⋮----
if !is_known_spl_token_id(account.owner()) {
⋮----
let token_account = StateWithExtensions::<TokenAccount>::unpack(account.data()).ok()?;
⋮----
let decimals = mint_decimals.get(&mint).cloned().or_else(|| {
let decimals = get_mint_decimals(bank, &mint)?;
mint_decimals.insert(mint, decimals);
⋮----
Some(TokenBalanceData {
mint: token_account.base.mint.to_string(),
owner: token_account.base.owner.to_string(),
ui_token_amount: token_amount_to_ui_amount_v3(
⋮----
program_id: account.owner().to_string(),
⋮----
mod test {
⋮----
fn test_collect_token_balance_from_account() {
let (mut genesis_config, _mint_keypair) = create_genesis_config(500);
⋮----
Mint::pack(mint_data, &mut data).unwrap();
⋮----
data: data.to_vec(),
⋮----
TokenAccount::pack(token_data, &mut data).unwrap();
⋮----
TokenAccount::pack(other_mint_data, &mut data).unwrap();
⋮----
accounts.insert(account_pubkey, account);
accounts.insert(mint_pubkey, mint);
accounts.insert(other_mint_pubkey, other_mint);
⋮----
accounts.insert(spl_token_account_pubkey, spl_token_account);
⋮----
accounts.insert(other_account_pubkey, other_account);
⋮----
accounts.insert(other_mint_account_pubkey, other_mint_token_account);
⋮----
assert_eq!(
⋮----
fn test_collect_token_balance_from_spl_token_2022_account() {
⋮----
.unwrap();
⋮----
let mut mint_data = vec![0; mint_size];
⋮----
StateWithExtensionsMut::<Mint>::unpack_uninitialized(&mut mint_data).unwrap();
⋮----
mint_state.pack_base();
mint_state.init_account_type().unwrap();
⋮----
OptionalNonZeroPubkey::try_from(Some(mint_authority)).unwrap();
⋮----
data: mint_data.to_vec(),
⋮----
let mut account_data = vec![0; account_size];
⋮----
account_state.pack_base();
account_state.init_account_type().unwrap();
⋮----
let memo_transfer = account_state.init_extension::<MemoTransfer>(true).unwrap();
memo_transfer.require_incoming_transfer_memos = true.into();
⋮----
data: account_data.to_vec(),

================
File: ledger/src/transaction_address_lookup_table_scanner.rs
================
pub struct ScannedLookupTableExtensions {
⋮----
pub fn scan_transaction(
⋮----
for (program_id, instruction) in transaction.get_message().program_instructions_iter() {
⋮----
accounts.extend(new_addresses);
⋮----
no_user_programs &= RESERVED_IDS_SET.contains(program_id);

================
File: ledger/src/transaction_balances.rs
================
pub fn compile_collected_balances(
⋮----
let (native_pre, native_post, token_pre, token_post) = balance_collector.into_vecs();
⋮----
collected_token_infos_to_token_balances(token_pre),
collected_token_infos_to_token_balances(token_post),
⋮----
fn collected_token_infos_to_token_balances(
⋮----
.into_iter()
.map(|infos| {
⋮----
.map(svm_token_info_to_token_balance)
.collect()
⋮----
pub fn svm_token_info_to_token_balance(svm_info: SvmTokenInfo) -> TransactionTokenBalance {
⋮----
mint: mint.to_string(),
ui_token_amount: token_amount_to_ui_amount_v3(
⋮----
owner: owner.to_string(),
program_id: program_id.to_string(),
⋮----
mod tests {
⋮----
fn test_compile_collected_balances() {
let native_pre = vec![vec![1, 2, 3], vec![4, 5, 6]];
let native_post = vec![vec![7, 8, 9], vec![10, 11, 0]];
⋮----
let token_pre = vec![vec![token_info_before], vec![]];
let token_post = vec![vec![token_info_after], vec![]];
⋮----
mint: mint1.to_string(),
⋮----
ui_amount: Some(1.0),
⋮----
amount: amount1.to_string(),
ui_amount_string: "1".to_string(),
⋮----
owner: owner1.to_string(),
program_id: token::id().to_string(),
⋮----
mint: mint2.to_string(),
⋮----
ui_amount: Some(2.0),
⋮----
amount: amount2.to_string(),
ui_amount_string: "2".to_string(),
⋮----
owner: owner2.to_string(),
program_id: token_2022::id().to_string(),
⋮----
let expected_native = TransactionBalancesSet::new(native_pre.clone(), native_post.clone());
⋮----
vec![vec![token_balance_before], vec![]],
vec![vec![token_balance_after], vec![]],
⋮----
let (actual_native, actual_token) = compile_collected_balances(balance_collector);
assert_eq!(expected_native.pre_balances, actual_native.pre_balances);
assert_eq!(expected_native.post_balances, actual_native.post_balances);
assert_eq!(

================
File: ledger/src/use_snapshot_archives_at_startup.rs
================
pub enum UseSnapshotArchivesAtStartup {
⋮----
pub mod cli {
⋮----
pub fn default_value() -> &'static str {
UseSnapshotArchivesAtStartup::default().into()
⋮----
pub fn default_value_for_ledger_tool() -> &'static str {
UseSnapshotArchivesAtStartup::Always.into()

================
File: ledger/src/wire_format_tests.rs
================
mod tests {
⋮----
fn parse_turbine(bytes: &[u8]) -> anyhow::Result<Shred> {
let shred = Shred::new_from_serialized_shred(bytes.to_owned())
.map_err(|_e| anyhow::anyhow!("Can not deserialize"))?;
Ok(shred)
⋮----
fn serialize(pkt: Shred) -> Vec<u8> {
pkt.payload().to_vec()
⋮----
fn find_differences(a: &[u8], b: &[u8]) -> Option<usize> {
if a.len() != b.len() {
return Some(a.len());
⋮----
for (idx, (e1, e2)) in a.iter().zip(b).enumerate() {
⋮----
return Some(idx);
⋮----
fn show_packet(bytes: &[u8]) -> anyhow::Result<()> {
let shred = parse_turbine(bytes)?;
let merkle_root = shred.merkle_root();
let chained_merkle_root = shred.chained_merkle_root();
let rtx_sign = shred.retransmitter_signature();
println!("=== {} bytes ===", bytes.len());
println!(
⋮----
hexdump(bytes)?;
println!("===");
Ok(())
⋮----
fn test_turbine_wire_format() {
⋮----
eprintln!("Test requires TURBINE_WIRE_FORMAT_PACKETS env variable, skipping!");
⋮----
std::fs::read_dir(path_base).expect("Expecting env var to point to a directory")
⋮----
let entry = entry.expect("Expecting a readable file");
validate_packet_format(
&entry.path(),
⋮----
.unwrap();

================
File: ledger/tests/blockstore.rs
================
fn test_multiple_threads_insert_shred() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
.map(|i| {
⋮----
let blockstore_ = blockstore.clone();
⋮----
.name("blockstore-writer".to_string())
.spawn(move || {
blockstore_.insert_shreds(shreds, None, false).unwrap();
⋮----
.unwrap()
⋮----
.collect();
⋮----
t.join().unwrap()
⋮----
let mut meta0 = blockstore.meta(0).unwrap().unwrap();
meta0.next_slots.sort_unstable();
let expected_next_slots: Vec<_> = (1..num_threads + 1).collect();
assert_eq!(meta0.next_slots, expected_next_slots);
blockstore.purge_and_compact_slots(0, num_threads + 1);

================
File: ledger/tests/shred.rs
================
type IndexShredsMap = BTreeMap<u32, Vec<Shred>>;
⋮----
fn test_multi_fec_block_coding(is_last_in_slot: bool) {
⋮----
let shredder = Shredder::new(slot, slot - 5, 0, 0).unwrap();
⋮----
let tx0 = system_transaction::transfer(&keypair0, &keypair1.pubkey(), 1, Hash::default());
let entry = Entry::new(&Hash::default(), 1, vec![tx0]);
⋮----
max_entries_per_n_shred_last_or_not(&entry, num_data_shreds as u64, is_last_in_slot);
⋮----
.map(|_| {
⋮----
system_transaction::transfer(&keypair0, &keypair1.pubkey(), 1, Hash::default());
Entry::new(&Hash::default(), 1, vec![tx0])
⋮----
.collect();
⋮----
let serialized_entries = bincode::serialize(&entries).unwrap();
let (data_shreds, coding_shreds) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
let next_index = data_shreds.last().unwrap().index() + 1;
assert_eq!(next_index as usize, num_data_shreds);
assert_eq!(data_shreds.len(), num_data_shreds);
assert_eq!(coding_shreds.len(), num_data_shreds);
⋮----
assert!(!c.is_data());
⋮----
let mut all_shreds = vec![];
⋮----
.iter()
.cloned()
.chain(coding_shreds[shred_start_index..=end_index].iter().cloned())
⋮----
.enumerate()
.filter_map(|(i, b)| if i % 2 != 0 { Some(b.clone()) } else { None })
⋮----
let recovered_data = recover(shred_info.clone(), &reed_solomon_cache)
.unwrap()
.map(|result| result.unwrap())
.filter(|shred| shred.is_data());
for (i, recovered_shred) in recovered_data.enumerate() {
⋮----
verify_test_data_shred(
⋮----
index.try_into().unwrap(),
⋮----
&keypair.pubkey(),
⋮----
shred_info.insert(i * 2, recovered_shred);
⋮----
all_shreds.extend(shred_info.into_iter().take(DATA_SHREDS_PER_FEC_BLOCK));
⋮----
let shreds = all_shreds.iter().map(Shred::payload);
Shredder::deshred(shreds).unwrap()
⋮----
assert_eq!(serialized_entries[..], result[..serialized_entries.len()]);
⋮----
fn test_multi_fec_block_different_size_coding() {
⋮----
setup_different_sized_fec_blocks(slot, parent_slot, keypair.clone());
let total_num_data_shreds: usize = fec_data.values().map(|x| x.len()).sum();
⋮----
for (fec_data_shreds, fec_coding_shreds) in fec_data.values().zip(fec_coding.values()) {
let first_data_index = fec_data_shreds.first().unwrap().index() as usize;
⋮----
.step_by(2)
.chain(fec_coding_shreds.iter().step_by(2))
⋮----
.filter_map(|s| {
let s = s.unwrap();
s.is_data().then_some(s)
⋮----
assert_eq!(fec_data_shreds.len() % 2, 0);
for (i, recovered_shred) in recovered_data.into_iter().enumerate() {
⋮----
fn sort_data_coding_into_fec_sets(
⋮----
assert!(shred.is_data());
let key = (shred.slot(), shred.index());
assert!(!data_slot_and_index.contains(&key));
data_slot_and_index.insert(key);
let fec_entry = fec_data.entry(shred.fec_set_index()).or_default();
fec_entry.push(shred);
⋮----
assert!(!shred.is_data());
⋮----
assert!(!coding_slot_and_index.contains(&key));
coding_slot_and_index.insert(key);
let fec_entry = fec_coding.entry(shred.fec_set_index()).or_default();
⋮----
fn setup_different_sized_fec_blocks(
⋮----
let shredder = Shredder::new(slot, parent_slot, 0, 0).unwrap();
⋮----
let merkle_capacity = ShredData::capacity( 6,  true).unwrap();
⋮----
assert!(DATA_SHREDS_PER_FEC_BLOCK > 2);
⋮----
max_entries_per_n_shred(&entry, num_shreds_per_iter as u64, Some(merkle_capacity));
⋮----
if (shred.index() as usize) == total_num_data_shreds - 1 {
assert!(shred.data_complete());
assert!(shred.last_in_slot());
} else if (shred.index() as usize) % num_shreds_per_iter == num_shreds_per_iter - 1 {
⋮----
assert!(!shred.data_complete());
assert!(!shred.last_in_slot());
⋮----
assert_eq!(data_shreds.len(), num_shreds_per_iter);
next_shred_index = data_shreds.last().unwrap().index() + 1;
next_code_index = coding_shreds.last().unwrap().index() + 1;
sort_data_coding_into_fec_sets(
⋮----
assert_eq!(fec_data.len(), fec_coding.len());

================
File: ledger/.gitignore
================
/target/
/farf/

================
File: ledger/Cargo.toml
================
[package]
name = "solana-ledger"
description = "Solana ledger"
documentation = "https://docs.rs/solana-ledger"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_ledger"

[features]
dev-context-only-utils = ["solana-perf/dev-context-only-utils"]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-runtime/frozen-abi",
]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
agave-reserved-account-keys = { workspace = true }
agave-snapshots = { workspace = true }
anyhow = { workspace = true }
assert_matches = { workspace = true }
bincode = { workspace = true }
bitflags = { workspace = true, features = ["serde"] }
bytes = { workspace = true }
bzip2 = { workspace = true }
chrono = { workspace = true, features = ["default", "serde"] }
chrono-humanize = { workspace = true }
crossbeam-channel = { workspace = true }
dashmap = { workspace = true, features = ["rayon", "raw-api"] }
eager = { workspace = true }
fs_extra = { workspace = true }
futures = { workspace = true }
itertools = { workspace = true }
lazy-lru = { workspace = true }
log = { workspace = true }
lru = { workspace = true }
mockall = { workspace = true }
num_cpus = { workspace = true }
num_enum = { workspace = true }
prost = { workspace = true }
qualifier_attr = { workspace = true }
rand = { workspace = true }
rand0-8-5 = { package = "rand", version = "0.8.5" }
rand_chacha = { workspace = true }
rand_chacha0-3-1 = { package = "rand_chacha", version = "0.3.1" }
rayon = { workspace = true }
reed-solomon-erasure = { workspace = true, features = ["simd-accel"] }
scopeguard = { workspace = true }
serde = { workspace = true }
serde_bytes = { workspace = true }
sha2 = { workspace = true }
solana-account = { workspace = true }
solana-account-decoder = { workspace = true }
solana-accounts-db = { workspace = true }
solana-address-lookup-table-interface = { workspace = true }
solana-bpf-loader-program = { workspace = true }
solana-clock = { workspace = true }
solana-cost-model = { workspace = true }
solana-entry = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-genesis-config = { workspace = true }
solana-genesis-utils = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-native-token = { workspace = true }
solana-net-utils = { workspace = true }
solana-nohash-hasher = { workspace = true }
solana-packet = { workspace = true }
solana-perf = { workspace = true }
solana-program-runtime = { workspace = true, features = ["metrics"] }
solana-pubkey = { workspace = true }
solana-rayon-threadlimit = { workspace = true }
solana-runtime = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-seed-derivable = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-shred-version = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-stake-interface = { workspace = true }
solana-storage-bigtable = { workspace = true }
solana-storage-proto = { workspace = true }
solana-streamer = { workspace = true }
solana-svm = { workspace = true }
solana-svm-timings = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }
solana-time-utils = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status = { workspace = true }
solana-vote = { workspace = true }
solana-vote-program = { workspace = true }
static_assertions = { workspace = true }
strum = { workspace = true, features = ["derive"] }
strum_macros = { workspace = true }
tar = { workspace = true }
tempfile = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tokio-stream = { workspace = true }
trees = { workspace = true }
wincode = { workspace = true }

[dependencies.rocksdb]
# Avoid the vendored bzip2 within rocksdb-sys that can cause linker conflicts
# when also using the bzip2 crate
version = "0.24.0"
default-features = false
features = ["lz4"]

[dev-dependencies]
agave-logger = { workspace = true }
bs58 = { workspace = true }
criterion = { workspace = true }
proptest = { workspace = true }
solana-account-decoder = { workspace = true }
# See order-crates-for-publishing.py for using this unusual `path = "."`
solana-ledger = { path = ".", features = ["dev-context-only-utils", "agave-unstable-api"] }
solana-net-utils = { workspace = true, features = ["dev-context-only-utils"] }
solana-perf = { workspace = true, features = ["dev-context-only-utils"] }
solana-program-option = { workspace = true }
solana-program-pack = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-signature = { workspace = true, features = ["rand"] }
solana-vote = { workspace = true, features = ["dev-context-only-utils"] }
spl-generic-token = { workspace = true }
spl-pod = { workspace = true }
test-case = { workspace = true }

[[bench]]
name = "blockstore"

[[bench]]
name = "make_shreds_from_entries"
harness = false

[lints]
workspace = true

================
File: ledger-tool/src/args.rs
================
pub fn accounts_db_args<'a, 'b>() -> Box<[Arg<'a, 'b>]> {
vec![
⋮----
.into_boxed_slice()
⋮----
pub fn load_genesis_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long("max-genesis-archive-unpacked-size")
.value_name("NUMBER")
.takes_value(true)
.default_value(MAX_GENESIS_ARCHIVE_UNPACKED_SIZE_STR)
.help("maximum total uncompressed size of unpacked genesis archive")
⋮----
pub fn snapshot_args<'a, 'b>() -> Box<[Arg<'a, 'b>]> {
⋮----
pub fn parse_process_options(ledger_path: &Path, arg_matches: &ArgMatches<'_>) -> ProcessOptions {
let new_hard_forks = hardforks_of(arg_matches, "hard_forks");
let accounts_db_config = get_accounts_db_config(ledger_path, arg_matches);
let log_messages_bytes_limit = value_t!(arg_matches, "log_messages_bytes_limit", usize).ok();
⋮----
if arg_matches.is_present("skip_poh_verify") {
eprintln!("--skip-poh-verify is deprecated.  Replace with --skip-verification.");
⋮----
!(arg_matches.is_present("skip_poh_verify") || arg_matches.is_present("skip_verification"));
let halt_at_slot = value_t!(arg_matches, "halt_at_slot", Slot).ok();
let use_snapshot_archives_at_startup = value_t_or_exit!(
⋮----
let accounts_db_skip_shrink = arg_matches.is_present("accounts_db_skip_shrink");
let verify_index = arg_matches.is_present("verify_accounts_index");
⋮----
value_t!(arg_matches, "limit_load_slot_count_from_snapshot", usize).ok();
let run_final_accounts_hash_calc = arg_matches.is_present("run_final_hash_calc");
let debug_keys = pubkeys_of(arg_matches, "debug_key")
.map(|pubkeys| Arc::new(pubkeys.into_iter().collect::<HashSet<_>>()));
let allow_dead_slots = arg_matches.is_present("allow_dead_slots");
let abort_on_invalid_block = arg_matches.is_present("abort_on_invalid_block");
let no_block_cost_limits = arg_matches.is_present("no_block_cost_limits");
⋮----
// Build an `AccountsDbConfig` from subcommand arguments. All of the arguments
// matched by this functional are either optional or have a default value.
// Thus, a subcommand need not support all of the arguments that are matched
// by this function.
pub fn get_accounts_db_config(
⋮----
let ledger_tool_ledger_path = ledger_path.join(LEDGER_TOOL_DIRECTORY);
let accounts_index_bins = value_t!(arg_matches, "accounts_index_bins", usize).ok();
⋮----
value_t!(arg_matches, "accounts_index_initial_accounts_count", usize).ok();
let accounts_index_index_limit = if !arg_matches.is_present("enable_accounts_disk_index") {
⋮----
let accounts_index_drives = values_t!(arg_matches, "accounts_index_path", String)
.ok()
.map(|drives| drives.into_iter().map(PathBuf::from).collect())
.unwrap_or_else(|| vec![ledger_tool_ledger_path.join("accounts_index")]);
⋮----
drives: Some(accounts_index_drives),
⋮----
.value_of("accounts_db_access_storages_method")
.map(|method| match method {
⋮----
warn!("Using `mmap` for `--accounts-db-access-storages-method` is now deprecated.");
⋮----
unreachable!("invalid value given to accounts-db-access-storages-method")
⋮----
.unwrap_or_default();
⋮----
.value_of("accounts_db_scan_filter_for_shrinking")
.map(|filter| match filter {
⋮----
unreachable!("invalid value given to accounts_db_scan_filter_for_shrinking")
⋮----
index: Some(accounts_index_config),
base_working_path: Some(ledger_tool_ledger_path),
ancient_append_vec_offset: value_t!(arg_matches, "accounts_db_ancient_append_vecs", i64)
.ok(),
ancient_storage_ideal_size: value_t!(
⋮----
max_ancient_storages: value_t!(arg_matches, "accounts_db_max_ancient_storages", usize).ok(),
exhaustively_verify_refcounts: arg_matches.is_present("accounts_db_verify_refcounts"),
skip_initial_hash_calc: arg_matches.is_present("accounts_db_skip_initial_hash_calculation"),
⋮----
pub(crate) fn parse_encoding_format(matches: &ArgMatches<'_>) -> UiAccountEncoding {
match matches.value_of("encoding") {
⋮----
pub(crate) fn parse_account_output_config(matches: &ArgMatches<'_>) -> CliAccountNewConfig {
let data_encoding = parse_encoding_format(matches);
let output_account_data = !matches.is_present("no_account_data");
⋮----
Some(UiDataSliceConfig { offset, length })
⋮----
pub fn hardforks_of(matches: &ArgMatches<'_>, name: &str) -> Option<Vec<Slot>> {
if matches.is_present(name) {
Some(values_t_or_exit!(matches, name, Slot))
⋮----
mod tests {
⋮----
fn test_max_genesis_archive_unpacked_size_constant() {
assert_eq!(

================
File: ledger-tool/src/bigtable.rs
================
async fn upload(
⋮----
.map_err(|err| format!("Failed to connect to storage: {err:?}"))?;
⋮----
None => blockstore.get_first_available_block()?,
⋮----
let ending_slot = ending_slot.unwrap_or_else(|| blockstore.max_root());
⋮----
let current_ending_slot = min(
⋮----
starting_slot.saturating_add(config.max_num_slots_to_check as u64 * 2),
⋮----
blockstore.clone(),
bigtable.clone(),
⋮----
config.clone(),
⋮----
info!("last slot checked: {last_slot_checked}");
starting_slot = last_slot_checked.saturating_add(1);
⋮----
info!("No more blocks to upload.");
Ok(())
⋮----
async fn delete_slots(
⋮----
async fn first_available_block(
⋮----
match bigtable.get_first_available_block().await? {
Some(block) => println!("{block}"),
None => println!("No blocks available"),
⋮----
async fn block(
⋮----
let confirmed_block = bigtable.get_confirmed_block(slot).await?;
let encoded_block = encode_confirmed_block(confirmed_block)?;
⋮----
let entries = bigtable.get_entries(slot).await?;
⋮----
println!("{}", output_format.formatted_string(&cli_block));
⋮----
async fn entries(
⋮----
entries: entries.map(Into::into).collect(),
⋮----
println!("{}", output_format.formatted_string(&cli_entries));
⋮----
enum ShredPohGenerationMode {
⋮----
struct MockPohConfig {
⋮----
struct ShredConfig {
⋮----
fn get_shred_config_from_ledger(
⋮----
let process_options = parse_process_options(ledger_path, arg_matches);
let genesis_config = open_genesis_config_by(ledger_path, arg_matches);
let LoadAndProcessLedgerOutput { bank_forks, .. } = load_and_process_ledger_or_exit(
⋮----
let bank = bank_forks.read().unwrap().working_bank();
let shred_version = compute_shred_version(&genesis_config.hash(), Some(&bank.hard_forks()));
⋮----
let working_bank_epoch = bank.epoch();
let epoch_schedule = bank.epoch_schedule();
let starting_epoch = epoch_schedule.get_epoch(starting_slot);
let ending_epoch = epoch_schedule.get_epoch(ending_slot);
⋮----
eprintln!(
⋮----
exit(1);
⋮----
num_hashes_per_tick: bank.hashes_per_tick().unwrap_or(0),
num_ticks_per_slot: bank.ticks_per_slot(),
⋮----
async fn shreds(
⋮----
let limit = ending_slot.saturating_sub(starting_slot).saturating_add(1) as usize;
let mut slots = bigtable.get_confirmed_blocks(starting_slot, limit).await?;
slots.retain(|&slot| slot <= ending_slot);
let keypair = keypair_from_seed(&[0; 64])?;
for slot in slots.iter() {
let block = bigtable.get_confirmed_block(*slot).await?;
let entry_summaries = bigtable.get_entries(*slot).await;
⋮----
.enumerate()
.map(|(i, entry_summary)| {
⋮----
let Some(transactions) = block.transactions.get(
⋮----
let num_block_transactions = block.transactions.len();
return Err(format!(
⋮----
.iter()
.map(|tx_with_meta| tx_with_meta.get_transaction())
.collect();
Ok(Entry {
⋮----
let err_msg = format!("Failed to get PoH entries for {slot}: {err}");
⋮----
warn!("{err_msg}. Will create mock PoH entries instead.");
⋮----
let num_total_entries = num_total_ticks + block.transactions.len();
⋮----
create_ticks(num_virtual_ticks, num_hashes_per_tick, parent_blockhash);
entries.extend(virtual_ticks_entries.into_iter());
⋮----
let transaction_entries = block.transactions.iter().map(|tx_with_meta| Entry {
⋮----
transactions: vec![tx_with_meta.get_transaction()],
⋮----
entries.extend(transaction_entries.into_iter());
⋮----
let tick_entries = (0..num_ticks_per_slot).map(|idx| {
⋮----
transactions: vec![],
⋮----
entries.extend(tick_entries.into_iter());
⋮----
.make_merkle_shreds_from_entries(
⋮----
.filter(Shred::is_data)
⋮----
blockstore.insert_shreds(data_shreds, None, false)?;
⋮----
async fn blocks(
⋮----
let slots = bigtable.get_confirmed_blocks(starting_slot, limit).await?;
println!("{slots:?}");
println!("{} blocks found", slots.len());
⋮----
async fn compare_blocks(
⋮----
.map_err(|err| format!("failed to connect to reference bigtable: {err:?}"))?;
⋮----
.get_confirmed_blocks(starting_slot, limit)
⋮----
info!(
⋮----
if reference_bigtable_slots.is_empty() {
println!("Reference bigtable is empty after {starting_slot}. Aborting.");
return Ok(());
⋮----
.map_err(|err| format!("failed to connect to owned bigtable: {err:?}"))?;
⋮----
} = missing_blocks(&reference_bigtable_slots, &owned_bigtable_slots);
println!(
⋮----
async fn confirm(
⋮----
let transaction_status = bigtable.get_signature_status(signature).await?;
⋮----
match bigtable.get_confirmed_transaction(signature).await {
⋮----
let decoded_tx = confirmed_tx.get_transaction();
⋮----
.encode(UiTransactionEncoding::Json, Some(0), true)
.map_err(|_| "Failed to encode transaction in block".to_string())?;
transaction = Some(CliTransaction {
⋮----
slot: Some(confirmed_tx.slot),
⋮----
prefix: "  ".to_string(),
sigverify_status: vec![],
⋮----
get_transaction_error = Some(format!("{err:?}"));
⋮----
confirmation_status: Some(transaction_status.confirmation_status()),
⋮----
err: transaction_status.err.clone().map(Into::into),
⋮----
println!("{}", output_format.formatted_string(&cli_transaction));
⋮----
pub async fn transaction_history(
⋮----
.get_confirmed_signatures_for_address(
⋮----
before.as_ref(),
until.as_ref(),
limit.min(query_chunk_size),
⋮----
if results.is_empty() {
⋮----
before = Some(results.last().unwrap().0.signature);
assert!(limit >= results.len());
limit = limit.saturating_sub(results.len());
⋮----
println!("{}", result.signature);
⋮----
match block.transactions.get(index as usize).map(|tx_with_meta| {
⋮----
tx_with_meta.get_transaction(),
tx_with_meta.get_status_meta(),
⋮----
println_transaction(
⋮----
meta.map(|m| m.into()).as_ref(),
⋮----
match bigtable.get_confirmed_block(result.slot).await {
⋮----
println!("  Unable to get confirmed transaction details: {err}");
⋮----
loaded_block = Some((result.slot, confirmed_block));
⋮----
println!();
⋮----
struct CopyArgs {
⋮----
impl CopyArgs {
pub fn process(arg_matches: &ArgMatches) -> Self {
⋮----
from_slot: value_t!(arg_matches, "starting_slot", Slot).unwrap_or(0),
to_slot: value_t!(arg_matches, "ending_slot", Slot).ok(),
source_instance_name: value_t_or_exit!(arg_matches, "source_instance_name", String),
source_app_profile_id: value_t_or_exit!(arg_matches, "source_app_profile_id", String),
source_credential_path: value_t!(arg_matches, "source_credential_path", String).ok(),
emulated_source: value_t!(arg_matches, "emulated_source", String).ok(),
destination_instance_name: value_t_or_exit!(
⋮----
destination_app_profile_id: value_t_or_exit!(
⋮----
destination_credential_path: value_t!(
⋮----
.ok(),
emulated_destination: value_t!(arg_matches, "emulated_destination", String).ok(),
force: arg_matches.is_present("force"),
dry_run: arg_matches.is_present("dry_run"),
⋮----
async fn copy(args: CopyArgs) -> Result<(), Box<dyn std::error::Error>> {
⋮----
let to_slot = args.to_slot.unwrap_or(from_slot);
debug!("from_slot: {from_slot}, to_slot: {to_slot}");
⋮----
return Err("starting slot should be less than or equal to ending slot")?;
⋮----
let source_bigtable = get_bigtable(GetBigtableArgs {
⋮----
let destination_bigtable = get_bigtable(GetBigtableArgs {
⋮----
s.send(i).unwrap();
⋮----
let workers = min(to_slot - from_slot + 1, num_cpus::get().try_into().unwrap());
debug!("worker num: {workers}");
let success_slots = Arc::new(Mutex::new(vec![]));
let skip_slots = Arc::new(Mutex::new(vec![]));
let block_not_found_slots = Arc::new(Mutex::new(vec![]));
let failed_slots = Arc::new(Mutex::new(vec![]));
⋮----
.map(|i| {
let r = r.clone();
let source_bigtable_clone = source_bigtable.clone();
let destination_bigtable_clone = destination_bigtable.clone();
⋮----
while let Ok(slot) = r.try_recv() {
debug!("worker {i}: received slot {slot}");
⋮----
.confirmed_block_exists(slot)
⋮----
skip_slots_clone.lock().unwrap().push(slot);
⋮----
error!(
⋮----
failed_slots_clone.lock().unwrap().push(slot);
⋮----
match source_bigtable_clone.confirmed_block_exists(slot).await {
⋮----
debug!("will write block: {slot}");
success_slots_clone.lock().unwrap().push(slot);
⋮----
debug!("block not found, slot: {slot}");
block_not_found_slots_clone.lock().unwrap().push(slot);
⋮----
.get_confirmed_block(slot)
⋮----
error!("failed to get confirmed block, slot: {slot}, err: {err}");
⋮----
.upload_confirmed_block(slot, confirmed_block)
⋮----
debug!("wrote block: {slot}");
⋮----
error!("write failed, slot: {slot}, err: {err}");
⋮----
debug!("worker {i}: exit");
⋮----
let mut success_slots = success_slots.lock().unwrap();
success_slots.sort();
let mut skip_slots = skip_slots.lock().unwrap();
skip_slots.sort();
let mut block_not_found_slots = block_not_found_slots.lock().unwrap();
block_not_found_slots.sort();
let mut failed_slots = failed_slots.lock().unwrap();
failed_slots.sort();
debug!("success slots: {success_slots:?}");
debug!("skip slots: {skip_slots:?}");
debug!("blocks not found slots: {block_not_found_slots:?}");
debug!("failed slots: {failed_slots:?}");
⋮----
struct GetBigtableArgs {
⋮----
async fn get_bigtable(
⋮----
credential_type: CredentialType::Filepath(Some(args.crediential_path.unwrap())),
⋮----
pub trait BigTableSubCommand {
⋮----
impl BigTableSubCommand for App<'_, '_> {
fn bigtable_subcommand(self) -> Self {
self.subcommand(
⋮----
.about("Ledger data on a BigTable instance")
.setting(AppSettings::InferSubcommands)
.setting(AppSettings::SubcommandRequiredElseHelp)
.arg(
⋮----
.global(true)
.long("rpc-bigtable-instance-name")
.takes_value(true)
.value_name("INSTANCE_NAME")
.default_value(solana_storage_bigtable::DEFAULT_INSTANCE_NAME)
.help("Name of the target Bigtable instance"),
⋮----
.long("rpc-bigtable-app-profile-id")
⋮----
.value_name("APP_PROFILE_ID")
.default_value(solana_storage_bigtable::DEFAULT_APP_PROFILE_ID)
.help("Bigtable application profile id to use in requests"),
⋮----
.subcommand(
⋮----
.about("Upload the ledger to BigTable")
⋮----
.long("starting-slot")
.validator(is_slot)
.value_name("START_SLOT")
⋮----
.index(1)
.help(
⋮----
.long("ending-slot")
⋮----
.value_name("END_SLOT")
⋮----
.index(2)
.help("Stop uploading at this slot [default: last available slot]"),
⋮----
.long("force")
.takes_value(false)
⋮----
.about("Delete ledger information from BigTable")
⋮----
.value_name("SLOTS")
⋮----
.multiple(true)
.required(true)
.help("Slots to delete"),
⋮----
.about("Get the first available block in the storage"),
⋮----
.about("Get a list of slots with confirmed blocks for the given range")
⋮----
.value_name("SLOT")
⋮----
.default_value("0")
.help("Start listing at this slot"),
⋮----
.long("limit")
⋮----
.value_name("LIMIT")
⋮----
.default_value("1000")
.help("Maximum number of slots to return"),
⋮----
.about(
⋮----
.help("Maximum number of slots to check"),
⋮----
.long("reference-credential")
.short("c")
.value_name("REFERENCE_CREDENTIAL_FILEPATH")
⋮----
.help("File path for a credential to a reference bigtable"),
⋮----
.long("reference-instance-name")
⋮----
.help("Name of the reference Bigtable instance to compare to"),
⋮----
.long("reference-app-profile-id")
⋮----
.about("Get a confirmed block")
⋮----
.long("slot")
⋮----
.required(true),
⋮----
.long("show-entries")
.required(false)
.help("Display the transactions in their entries"),
⋮----
.about("Get the entry data for a block")
⋮----
.arg(load_genesis_arg())
.args(&snapshot_args())
⋮----
.help("Start shred creation at this slot (inclusive)"),
⋮----
.help("Stop shred creation at this slot (inclusive)"),
⋮----
.long("allow-mock-poh")
⋮----
.long("shred-version")
.validator(is_parsable::<u16>)
⋮----
.conflicts_with("allow_mock_poh")
⋮----
.about("Confirm transaction by signature")
⋮----
.long("signature")
.value_name("TRANSACTION_SIGNATURE")
⋮----
.help("The transaction signature to confirm"),
⋮----
.value_name("ADDRESS")
⋮----
.validator(is_valid_pubkey)
.help("Account address"),
⋮----
.default_value("18446744073709551615")
.help("Maximum number of transaction signatures to return"),
⋮----
.long("query-chunk-size")
⋮----
.value_name("AMOUNT")
⋮----
.long("before")
⋮----
.help("Start with the first signature older than this one"),
⋮----
.long("until")
⋮----
.help("End with the last signature newer than this one"),
⋮----
.long("show-transactions")
⋮----
.help("Display the full transactions"),
⋮----
.about("Copy blocks from a Bigtable to another Bigtable")
⋮----
.long("source-credential-path")
.value_name("SOURCE_CREDENTIAL_PATH")
⋮----
.conflicts_with("emulated_source")
⋮----
.long("emulated-source")
.value_name("EMULATED_SOURCE")
⋮----
.conflicts_with("source_credential_path")
.help("Source Bigtable emulated source"),
⋮----
.long("source-instance-name")
⋮----
.value_name("SOURCE_INSTANCE_NAME")
⋮----
.help("Source Bigtable instance name"),
⋮----
.long("source-app-profile-id")
⋮----
.value_name("SOURCE_APP_PROFILE_ID")
⋮----
.help("Source Bigtable app profile id"),
⋮----
.long("destination-credential-path")
.value_name("DESTINATION_CREDENTIAL_PATH")
⋮----
.conflicts_with("emulated_destination")
⋮----
.long("emulated-destination")
.value_name("EMULATED_DESTINATION")
⋮----
.conflicts_with("destination_credential_path")
.help("Destination Bigtable emulated destination"),
⋮----
.long("destination-instance-name")
⋮----
.value_name("DESTINATION_INSTANCE_NAME")
⋮----
.help("Destination Bigtable instance name"),
⋮----
.long("destination-app-profile-id")
⋮----
.value_name("DESTINATION_APP_PROFILE_ID")
⋮----
.help("Destination Bigtable app profile id"),
⋮----
.help("Start copying at this slot (inclusive)"),
⋮----
.help("Stop copying at this slot (inclusive)"),
⋮----
.value_name("FORCE")
⋮----
.long("dry-run")
.value_name("DRY_RUN")
⋮----
.help("Dry run. It won't upload any blocks"),
⋮----
fn get_global_subcommand_arg<T: FromStr>(
⋮----
// this is kinda stupid, but there seems to be a bug in clap when a subcommand
// arg is marked both `global(true)` and `default_value("default_value")`.
// despite the "global", when the arg is specified on the subcommand, its value
// is not propagated down to the (sub)subcommand args, resulting in the default
// value when queried there. similarly, if the arg is specified on the
// (sub)subcommand, the value is not propagated back up to the subcommand args,
// again resulting in the default value. the arg having declared a
// `default_value()` obviates `is_present(...)` tests since they will always
// return true. so we consede and compare against the expected default. :/
⋮----
.value_of(name)
.map(|v| v != default)
.unwrap_or(false);
⋮----
value_t_or_exit!(matches, name, T)
⋮----
let sub_matches = sub_matches.as_ref().unwrap();
value_t_or_exit!(sub_matches, name, T)
⋮----
pub fn bigtable_process_command(ledger_path: &Path, matches: &ArgMatches<'_>) {
let runtime = tokio::runtime::Runtime::new().unwrap();
let verbose = matches.is_present("verbose");
⋮----
let (subcommand, sub_matches) = matches.subcommand();
let instance_name = get_global_subcommand_arg(
⋮----
let app_profile_id = get_global_subcommand_arg(
⋮----
let starting_slot = value_t!(arg_matches, "starting_slot", Slot).ok();
let ending_slot = value_t!(arg_matches, "ending_slot", Slot).ok();
let force_reupload = arg_matches.is_present("force_reupload");
⋮----
&canonicalize_ledger_path(ledger_path),
⋮----
runtime.block_on(upload(
⋮----
let slots = values_t_or_exit!(arg_matches, "slots", Slot);
⋮----
read_only: !arg_matches.is_present("force"),
⋮----
runtime.block_on(delete_slots(slots, config))
⋮----
runtime.block_on(first_available_block(config))
⋮----
let slot = value_t_or_exit!(arg_matches, "slot", Slot);
let show_entries = arg_matches.is_present("show_entries");
⋮----
runtime.block_on(block(slot, output_format, show_entries, config))
⋮----
runtime.block_on(entries(slot, output_format, config))
⋮----
let starting_slot = value_t_or_exit!(arg_matches, "starting_slot", Slot);
let ending_slot = value_t_or_exit!(arg_matches, "ending_slot", Slot);
⋮----
let allow_mock_poh = arg_matches.is_present("allow_mock_poh");
let shred_version = value_t!(arg_matches, "shred_version", u16).ok();
let ledger_path = canonicalize_ledger_path(ledger_path);
⋮----
get_shred_config_from_ledger(
⋮----
runtime.block_on(shreds(
⋮----
let limit = value_t_or_exit!(arg_matches, "limit", usize);
⋮----
runtime.block_on(blocks(starting_slot, limit, config))
⋮----
let credential_path = Some(value_t_or_exit!(
⋮----
value_t_or_exit!(arg_matches, "reference_instance_name", String);
⋮----
value_t_or_exit!(arg_matches, "reference_app_profile_id", String);
⋮----
runtime.block_on(compare_blocks(starting_slot, limit, config, ref_config))
⋮----
.value_of("signature")
.unwrap()
.parse()
.expect("Invalid signature");
⋮----
runtime.block_on(confirm(&signature, verbose, output_format, config))
⋮----
let address = pubkey_of(arg_matches, "address").unwrap();
⋮----
let query_chunk_size = value_t_or_exit!(arg_matches, "query_chunk_size", usize);
⋮----
.value_of("before")
.map(|signature| signature.parse().expect("Invalid signature"));
⋮----
.value_of("until")
⋮----
let show_transactions = arg_matches.is_present("show_transactions");
⋮----
runtime.block_on(transaction_history(
⋮----
("copy", Some(arg_matches)) => runtime.block_on(copy(CopyArgs::process(arg_matches))),
_ => unreachable!(),
⋮----
future.unwrap_or_else(|err| {
eprintln!("{err:?}");
⋮----
struct MissingBlocksData {
⋮----
fn missing_blocks(reference: &[Slot], owned: &[Slot]) -> MissingBlocksData {
if reference.is_empty() {
⋮----
last_block_checked: owned.last().cloned().unwrap_or_default(),
missing_blocks: vec![],
superfluous_blocks: owned.to_owned(),
⋮----
num_owned_blocks: owned.len(),
⋮----
.last()
.expect("already returned if reference is empty");
⋮----
.map(|last_owned_block| min(last_owned_block, last_reference_block))
.unwrap_or(last_reference_block);
if owned.is_empty() && !reference.is_empty() {
⋮----
missing_blocks: reference.to_owned(),
superfluous_blocks: vec![],
num_reference_blocks: reference.len(),
⋮----
.take_while(|&slot| slot <= last_block_checked)
.cloned()
⋮----
.difference(&owned_hashset)
⋮----
missing_blocks.sort_unstable();
⋮----
.difference(&reference_hashset)
⋮----
superfluous_blocks.sort_unstable();
⋮----
num_reference_blocks: reference_hashset.len(),
num_owned_blocks: owned_hashset.len(),
⋮----
mod tests {
⋮----
fn test_missing_blocks() {
let reference_slots = vec![0, 37, 38, 39, 40, 41, 42, 43, 44, 45];
let owned_slots = vec![0, 38, 39, 40, 43, 44, 45, 46, 47];
let owned_slots_leftshift = vec![0, 25, 26, 27, 28, 29, 30, 31, 32];
let owned_slots_rightshift = vec![0, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54];
let missing_slots = vec![37, 41, 42];
let missing_slots_rightshift = vec![37, 38, 39, 40, 41, 42, 43, 45];
assert_eq!(

================
File: ledger-tool/src/blockstore.rs
================
fn analyze_column(blockstore: &Blockstore, column_name: &str) -> Result<()> {
⋮----
let column_iterator = blockstore.iterator_cf(column_name)?;
⋮----
key_len = key.len() as u64;
⋮----
let val_len = val.len() as u64;
⋮----
val_hist.increment(val_len).unwrap();
⋮----
row_hist.increment(key_len + val_len).unwrap();
⋮----
let json_result = if val_hist.entries() > 0 {
json!({
⋮----
println!("{}", serde_json::to_string_pretty(&json_result)?);
Ok(())
⋮----
fn analyze_storage(blockstore: &Blockstore) -> Result<()> {
⋮----
analyze_column(blockstore, SlotMeta::NAME)?;
analyze_column(blockstore, Orphans::NAME)?;
analyze_column(blockstore, DeadSlots::NAME)?;
analyze_column(blockstore, DuplicateSlots::NAME)?;
analyze_column(blockstore, ErasureMeta::NAME)?;
analyze_column(blockstore, BankHash::NAME)?;
analyze_column(blockstore, Root::NAME)?;
analyze_column(blockstore, Index::NAME)?;
analyze_column(blockstore, ShredData::NAME)?;
analyze_column(blockstore, ShredCode::NAME)?;
analyze_column(blockstore, TransactionStatus::NAME)?;
analyze_column(blockstore, AddressSignatures::NAME)?;
analyze_column(blockstore, TransactionMemos::NAME)?;
analyze_column(blockstore, TransactionStatusIndex::NAME)?;
analyze_column(blockstore, Rewards::NAME)?;
analyze_column(blockstore, Blocktime::NAME)?;
analyze_column(blockstore, PerfSamples::NAME)?;
analyze_column(blockstore, BlockHeight::NAME)?;
analyze_column(blockstore, OptimisticSlots::NAME)
⋮----
fn raw_key_to_slot(key: &[u8], column_name: &str) -> Option<Slot> {
⋮----
cf::SlotMeta::NAME => Some(cf::SlotMeta::slot(cf::SlotMeta::index(key))),
cf::Orphans::NAME => Some(cf::Orphans::slot(cf::Orphans::index(key))),
cf::DeadSlots::NAME => Some(cf::DeadSlots::slot(cf::DeadSlots::index(key))),
cf::DuplicateSlots::NAME => Some(cf::DuplicateSlots::slot(cf::DuplicateSlots::index(key))),
cf::ErasureMeta::NAME => Some(cf::ErasureMeta::slot(cf::ErasureMeta::index(key))),
cf::BankHash::NAME => Some(cf::BankHash::slot(cf::BankHash::index(key))),
cf::Root::NAME => Some(cf::Root::slot(cf::Root::index(key))),
cf::Index::NAME => Some(cf::Index::slot(cf::Index::index(key))),
cf::ShredData::NAME => Some(cf::ShredData::slot(cf::ShredData::index(key))),
cf::ShredCode::NAME => Some(cf::ShredCode::slot(cf::ShredCode::index(key))),
cf::TransactionStatus::NAME => Some(cf::TransactionStatus::slot(
⋮----
cf::AddressSignatures::NAME => Some(cf::AddressSignatures::slot(
⋮----
cf::Rewards::NAME => Some(cf::Rewards::slot(cf::Rewards::index(key))),
cf::Blocktime::NAME => Some(cf::Blocktime::slot(cf::Blocktime::index(key))),
cf::PerfSamples::NAME => Some(cf::PerfSamples::slot(cf::PerfSamples::index(key))),
cf::BlockHeight::NAME => Some(cf::BlockHeight::slot(cf::BlockHeight::index(key))),
⋮----
Some(cf::OptimisticSlots::slot(cf::OptimisticSlots::index(key)))
⋮----
fn slot_contains_nonvote_tx(blockstore: &Blockstore, slot: Slot) -> bool {
⋮----
.get_slot_entries_with_shred_info(slot, 0, false)
.expect("Failed to get slot entries");
⋮----
.iter()
.flat_map(|entry| entry.transactions.iter())
.flat_map(get_program_ids)
.any(|program_id| *program_id != solana_vote_program::id());
⋮----
type OptimisticSlotInfo = (Slot, Option<(Hash, UnixTimestamp)>, bool);
fn get_latest_optimistic_slots(
⋮----
.get_latest_optimistic_slots(1)
.expect("get_latest_optimistic_slots() failed")
.pop()
⋮----
eprintln!("Blockstore does not contain any optimistically confirmed slots");
return vec![];
⋮----
let slot_iter = AncestorIterator::new_inclusive(latest_slot, blockstore).map(|slot| {
let contains_nonvote_tx = slot_contains_nonvote_tx(blockstore, slot);
⋮----
.get_optimistic_slot(slot)
.expect("get_optimistic_slot() failed");
if hash_and_timestamp_opt.is_none() {
warn!(
⋮----
.filter(|(_, _, contains_nonvote)| *contains_nonvote)
.take(num_slots)
.collect()
⋮----
slot_iter.take(num_slots).collect()
⋮----
fn print_blockstore_file_metadata(blockstore: &Blockstore, file_name: &Option<&str>) -> Result<()> {
let live_files = blockstore.live_files_metadata()?;
let sst_file_name = file_name.as_ref().map(|name| format!("/{name}"));
⋮----
if sst_file_name.is_none() || file.name.eq(sst_file_name.as_ref().unwrap()) {
println!(
⋮----
if sst_file_name.is_some() {
return Ok(());
⋮----
return Err(LedgerToolError::BadArgument(format!(
⋮----
pub trait BlockstoreSubCommand {
⋮----
impl BlockstoreSubCommand for App<'_, '_> {
fn blockstore_subcommand(self) -> Self {
self.subcommand(
⋮----
.about("Commands to interact with a local Blockstore")
.setting(AppSettings::InferSubcommands)
.setting(AppSettings::SubcommandRequiredElseHelp)
.subcommands(blockstore_subcommands(false)),
⋮----
pub fn blockstore_subcommands<'a, 'b>(hidden: bool) -> Vec<App<'a, 'b>> {
⋮----
vec![AppSettings::Hidden]
⋮----
vec![]
⋮----
.long("starting-slot")
.value_name("SLOT")
.takes_value(true)
.default_value("0")
.help("Start at this slot");
⋮----
.long("ending-slot")
⋮----
.help("The last slot to iterate to");
⋮----
.long("allow-dead-slots")
.takes_value(false)
.help("Output dead slots as well");
vec![
⋮----
pub fn blockstore_process_command(ledger_path: &Path, matches: &ArgMatches<'_>) {
do_blockstore_process_command(ledger_path, matches).unwrap_or_else(|err| {
eprintln!("Failed to complete command: {err:?}");
⋮----
fn do_blockstore_process_command(ledger_path: &Path, matches: &ArgMatches<'_>) -> Result<()> {
let ledger_path = canonicalize_ledger_path(ledger_path);
let verbose_level = matches.occurrences_of("verbose");
match matches.subcommand() {
("analyze-storage", Some(arg_matches)) => analyze_storage(&crate::open_blockstore(
⋮----
let all = arg_matches.is_present("all");
⋮----
let slot_meta_iterator = blockstore.slot_meta_iterator(0)?;
let slots: Vec<_> = slot_meta_iterator.map(|(slot, _)| slot).collect();
let slot_bounds = if slots.is_empty() {
⋮----
total: slots.len(),
first: Some(*slots.first().unwrap()),
last: Some(*slots.last().unwrap()),
⋮----
bounds.all_slots = Some(&slots);
⋮----
let rooted_slot_iterator = blockstore.rooted_slot_iterator(0)?;
⋮----
for (i, slot) in rooted_slot_iterator.into_iter().enumerate() {
⋮----
first_rooted = Some(slot);
⋮----
last_rooted = Some(slot);
⋮----
let last_root_for_comparison = last_rooted.unwrap_or_default();
⋮----
.rev()
.take_while(|slot| *slot > &last_root_for_comparison)
.count();
⋮----
num_after_last_root: Some(count_past_root),
⋮----
println!("{}", output_format.formatted_string(&slot_bounds));
⋮----
let starting_slot = value_t_or_exit!(arg_matches, "starting_slot", Slot);
let ending_slot = value_t_or_exit!(arg_matches, "ending_slot", Slot);
⋮----
PathBuf::from(value_t_or_exit!(arg_matches, "target_ledger", String));
⋮----
for (slot, _meta) in source.slot_meta_iterator(starting_slot)? {
⋮----
let shreds = source.get_data_shreds_for_slot(slot, 0)?;
let shreds = shreds.into_iter().map(Cow::Owned);
if target.insert_cow_shreds(shreds, None, true).is_err() {
warn!("error inserting shreds for slot {slot}");
⋮----
for slot in blockstore.dead_slots_iterator(starting_slot)? {
println!("{slot}");
⋮----
for slot in blockstore.duplicate_slots_iterator(starting_slot)? {
⋮----
let proof = blockstore.get_duplicate_slot(slot).unwrap();
⋮----
println!("{}", output_format.formatted_string(&cli_duplicate_proof));
⋮----
let num_slots = value_t_or_exit!(arg_matches, "num_slots", usize);
let exclude_vote_only_slots = arg_matches.is_present("exclude_vote_only_slots");
⋮----
get_latest_optimistic_slots(&blockstore, num_slots, exclude_vote_only_slots);
⋮----
for (slot, hash_and_timestamp_opt, contains_nonvote) in slots.iter() {
⋮----
let datetime: DateTime<Utc> = t.into();
(datetime.to_rfc3339(), format!("{hash}"))
⋮----
let max_height = value_t!(arg_matches, "max_height", usize).unwrap_or(usize::MAX);
let start_root = value_t!(arg_matches, "start_root", Slot).unwrap_or(0);
let num_roots = value_t_or_exit!(arg_matches, "num_roots", usize);
let iter = blockstore.rooted_slot_iterator(start_root)?;
let mut output: Box<dyn Write> = if let Some(path) = arg_matches.value_of("slot_list") {
⋮----
_ => Box::new(stdout()),
⋮----
Box::new(stdout())
⋮----
.take(num_roots)
.take_while(|slot| *slot <= max_height as u64)
⋮----
.into_iter()
⋮----
let blockhash = blockstore.get_slot_entries(slot, 0)?.last().unwrap().hash;
writeln!(output, "{slot}: {blockhash:?}").expect("failed to write");
⋮----
assert!(
⋮----
ancestors.insert(a);
⋮----
println!("ancestors: {:?}", ancestors.iter());
⋮----
let frozen_regex = Regex::new(r"bank frozen: (\d*)").unwrap();
let full_regex = Regex::new(r"slot (\d*) is full").unwrap();
let log_file = PathBuf::from(value_t_or_exit!(arg_matches, "log_path", String));
⋮----
println!("Reading log file");
for line in f.lines().map_while(std::io::Result::ok) {
⋮----
if let Some(slot_string) = frozen_regex.captures_iter(&line).next() {
Some((slot_string, &mut frozen))
⋮----
.captures_iter(&line)
.next()
.map(|slot_string| (slot_string, &mut full))
⋮----
.get(1)
.expect("Only one match group")
.as_str()
⋮----
.unwrap();
if ancestors.contains(&slot) && !map.contains_key(&slot) {
map.insert(slot, line);
⋮----
if slot == ending_slot && frozen.contains_key(&slot) && full.contains_key(&slot)
⋮----
for ((slot1, frozen_log), (slot2, full_log)) in frozen.iter().zip(full.iter()) {
assert_eq!(slot1, slot2);
println!("Slot: {slot1}\n, full: {full_log}\n, frozen: {frozen_log}");
⋮----
let ending_slot = value_t!(arg_matches, "ending_slot", Slot).unwrap_or(Slot::MAX);
let num_slots = value_t!(arg_matches, "num_slots", Slot).ok();
let allow_dead_slots = arg_matches.is_present("allow_dead_slots");
let only_rooted = arg_matches.is_present("only_rooted");
⋮----
output_ledger(
⋮----
let sst_file_name = arg_matches.value_of("file_name");
print_blockstore_file_metadata(&blockstore, &sst_file_name)?;
⋮----
let start_slot = value_t_or_exit!(arg_matches, "start_slot", Slot);
let end_slot = value_t!(arg_matches, "end_slot", Slot).ok();
let perform_compaction = arg_matches.is_present("enable_compaction");
if arg_matches.is_present("no_compaction") {
warn!("--no-compaction is deprecated and is now the default behavior.");
⋮----
let dead_slots_only = arg_matches.is_present("dead_slots_only");
let batch_size = value_t_or_exit!(arg_matches, "batch_size", usize);
⋮----
let Some(highest_slot) = blockstore.highest_slot()? else {
return Err(LedgerToolError::BadArgument(
"blockstore is empty".to_string(),
⋮----
info!(
⋮----
blockstore.purge_from_next_slots(start_slot, end_slot);
⋮----
blockstore.purge_and_compact_slots(start_slot, end_slot);
⋮----
blockstore.purge_slots(start_slot, end_slot, PurgeType::Exact);
⋮----
let slots_iter = &(start_slot..=end_slot).chunks(batch_size);
⋮----
assert!(!slots.is_empty());
let start_slot = *slots.first().unwrap();
let end_slot = *slots.last().unwrap();
⋮----
purge_from_blockstore(start_slot, end_slot);
⋮----
.dead_slots_iterator(start_slot)?
.take_while(|s| *s <= end_slot);
⋮----
info!("Purging dead slot {dead_slot}");
purge_from_blockstore(dead_slot, dead_slot);
⋮----
let slots = values_t_or_exit!(arg_matches, "slots", Slot);
⋮----
.remove_dead_slot(slot)
.map(|_| println!("Slot {slot} not longer marked dead"))?;
⋮----
value_t!(arg_matches, "start_root", Slot).unwrap_or_else(|_| blockstore.max_root());
let max_slots = value_t_or_exit!(arg_matches, "max_slots", u64);
let end_root = value_t!(arg_matches, "end_root", Slot)
.unwrap_or_else(|_| start_root.saturating_sub(max_slots));
assert!(start_root > end_root);
⋮----
if arg_matches.is_present("end_root") && num_slots > max_slots {
⋮----
let num_repaired_roots = blockstore.scan_and_fix_roots(
Some(start_root),
Some(end_root),
⋮----
println!("Successfully repaired {num_repaired_roots} roots");
⋮----
.set_dead_slot(slot)
.map(|_| println!("Slot {slot} marked dead"))?;
⋮----
struct ShredMeta<'a> {
⋮----
.slot_meta_iterator(starting_slot)?
.take_while(|(slot, _)| *slot <= ending_slot)
⋮----
let full_slot = ledger.is_full(slot);
if let Ok(shreds) = ledger.get_data_shreds_for_slot(slot, 0) {
for (shred_index, shred) in shreds.iter().enumerate() {
⋮----
output_slot(
⋮----
_ => unreachable!(),
⋮----
pub mod tests {
⋮----
fn test_latest_optimistic_ancestors() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
let (shreds, _) = make_many_slot_entries(start_slot, num_slots, entries_per_shred);
blockstore.insert_shreds(shreds, None, false).unwrap();
(0..num_slots).step_by(2).for_each(|slot| {
⋮----
.insert_optimistic_slot(slot, &Hash::default(), UnixTimestamp::default())
⋮----
get_latest_optimistic_slots(&blockstore, num_slots as usize, exclude_vote_only_slots)
⋮----
.map(|(slot, _, _)| *slot)
.collect();
let expected: Vec<_> = (start_slot..num_slots).rev().collect();
assert_eq!(optimistic_slots, expected);

================
File: ledger-tool/src/error.rs
================
pub type Result<T> = std::result::Result<T, LedgerToolError>;
⋮----
pub enum LedgerToolError {

================
File: ledger-tool/src/ledger_path.rs
================
pub fn canonicalize_ledger_path(ledger_path: &Path) -> PathBuf {
fs::canonicalize(ledger_path).unwrap_or_else(|err| {
eprintln!(
⋮----
exit(1);

================
File: ledger-tool/src/ledger_utils.rs
================
pub struct LoadAndProcessLedgerOutput {
⋮----
pub(crate) enum LoadAndProcessLedgerError {
⋮----
pub fn load_and_process_ledger_or_exit(
⋮----
load_and_process_ledger(
⋮----
.unwrap_or_else(|err| {
eprintln!("Exiting. Failed to load and process ledger: {err}");
exit(1);
⋮----
pub fn load_and_process_ledger(
⋮----
.value_of("snapshots")
.map(PathBuf::from)
.unwrap_or_else(|| blockstore.ledger_path().to_path_buf());
let bank_snapshots_dir = if blockstore.is_primary_access() {
snapshots_dir.join(BANK_SNAPSHOTS_DIR)
⋮----
.ledger_path()
.join(LEDGER_TOOL_DIRECTORY)
.join(BANK_SNAPSHOTS_DIR)
⋮----
.value_of("full_snapshot_archive_path")
⋮----
.unwrap_or_else(|| snapshots_dir.clone());
⋮----
.value_of("incremental_snapshot_archive_path")
⋮----
.unwrap_or_default();
⋮----
let usage = if arg_matches.is_present("no_snapshot") {
⋮----
return Err(LoadAndProcessLedgerError::EndingSlotLessThanStartingSlot(
⋮----
PROCESS_SLOTS_HELP_STRING.to_string(),
⋮----
if !blockstore.slot_range_connected(starting_slot, halt_slot) {
return Err(
⋮----
let account_paths = if let Some(account_paths) = arg_matches.value_of("account_paths") {
if !blockstore.is_primary_access() {
info!(
⋮----
blockstore.ledger_path(),
⋮----
.map_err(LoadAndProcessLedgerError::CustomAccountsPathUnsupported)?;
⋮----
account_paths.split(',').map(PathBuf::from).collect()
} else if blockstore.is_primary_access() {
vec![blockstore.ledger_path().join("accounts")]
⋮----
.join("accounts");
⋮----
vec![non_primary_accounts_path]
⋮----
create_all_accounts_run_and_snapshot_dirs(&account_paths)
.map_err(LoadAndProcessLedgerError::CreateAllAccountsRunAndSnapshotDirectories)?;
⋮----
let (_, measure_clean_account_paths) = measure_time!(
⋮----
info!("{measure_clean_account_paths}");
⋮----
info!("Cleaning contents of account snapshot paths: {account_snapshot_paths:?}");
clean_orphaned_account_snapshot_dirs(
⋮----
.map_err(LoadAndProcessLedgerError::CleanOrphanedAccountSnapshotDirectories)?;
let geyser_plugin_active = arg_matches.is_present("geyser_plugin_config");
⋮----
let geyser_config_files = values_t_or_exit!(arg_matches, "geyser_plugin_config", String)
.into_iter()
⋮----
let (confirmed_bank_sender, confirmed_bank_receiver) = unbounded();
drop(confirmed_bank_sender);
⋮----
.map_err(LoadAndProcessLedgerError::GeyserServiceSetup)?;
⋮----
geyser_service.get_accounts_update_notifier(),
geyser_service.get_transaction_notifier(),
⋮----
let enable_rpc_transaction_history = arg_matches.is_present("enable_rpc_transaction_history");
⋮----
if enable_rpc_transaction_history && !blockstore.is_primary_access() {
Arc::new(open_blockstore(
⋮----
blockstore.clone()
⋮----
let (transaction_status_sender, transaction_status_receiver) = unbounded();
⋮----
write_blockstore.clone(),
arg_matches.is_present("enable_extended_tx_metadata_storage"),
⋮----
tss_exit.clone(),
⋮----
Some(TransactionStatusSender {
⋮----
Some(transaction_status_service),
⋮----
blockstore.as_ref(),
⋮----
transaction_status_sender.as_ref(),
⋮----
exit.clone(),
⋮----
.map_err(LoadAndProcessLedgerError::LoadBankForks)?;
let block_verification_method = value_t_or_exit!(
⋮----
info!("Using: block-verification-method: {block_verification_method}");
⋮----
value_t!(arg_matches, "unified_scheduler_handler_threads", usize).ok();
⋮----
info!("no scheduler pool is installed for block verification...");
⋮----
warn!(
⋮----
.write()
.unwrap()
.install_scheduler_pool(DefaultSchedulerPool::new_dyn(
⋮----
transaction_status_sender.clone(),
⋮----
bank_forks.read().unwrap().root(),
⋮----
snapshot_controller: snapshot_controller.clone(),
⋮----
AccountsBackgroundService::setup_bank_drop_callback(bank_forks.clone());
⋮----
AccountsBackgroundService::new(bank_forks.clone(), exit.clone(), abs_request_handler);
⋮----
Some(&snapshot_controller),
⋮----
.map(|_| LoadAndProcessLedgerOutput {
⋮----
.map_err(LoadAndProcessLedgerError::ProcessBlockstoreFromRoot);
exit.store(true, Ordering::Relaxed);
⋮----
service.quiesce_and_join_for_tests(tss_exit);
⋮----
pub fn open_blockstore(
⋮----
.value_of("wal_recovery_mode")
.map(BlockstoreRecoveryMode::from);
let force_update_to_open = matches.is_present("force_update_to_open");
⋮----
access_type: access_type.clone(),
recovery_mode: wal_recovery_mode.clone(),
⋮----
.to_string()
.starts_with("IO error: No such file or directory:");
⋮----
.starts_with("Invalid argument: Column family not found:");
⋮----
eprintln!(
⋮----
eprintln!("Failed to open blockstore at {ledger_path:?}: {err:?}");
⋮----
eprintln!("Use --force-update-to-open flag to attempt to update the blockstore");
⋮----
open_blockstore_with_temporary_primary_access(
⋮----
fn open_blockstore_with_temporary_primary_access(
⋮----
info!("Attempting to temporarily open blockstore with Primary access in order to update");
⋮----
pub fn open_genesis_config_by(ledger_path: &Path, matches: &ArgMatches<'_>) -> GenesisConfig {
⋮----
value_t_or_exit!(matches, "max_genesis_archive_unpacked_size", u64);
open_genesis_config(ledger_path, max_genesis_archive_unpacked_size).unwrap_or_else(|err| {
eprintln!("Exiting. Failed to open genesis config: {err}");
⋮----
pub fn get_program_ids(tx: &VersionedTransaction) -> impl Iterator<Item = &Pubkey> + '_ {
⋮----
let account_keys = message.static_account_keys();
⋮----
.instructions()
.iter()
.map(|ix| ix.program_id(account_keys))
⋮----
pub(crate) fn get_access_type(process_options: &ProcessOptions) -> AccessType {

================
File: ledger-tool/src/main.rs
================
mod args;
mod bigtable;
mod blockstore;
mod error;
mod ledger_path;
mod ledger_utils;
mod output;
mod program;
fn render_dot(dot: String, output_file: &str, output_format: &str) -> io::Result<()> {
⋮----
.arg(format!("-T{output_format}"))
.arg(format!("-o{output_file}"))
.stdin(Stdio::piped())
.spawn()
.map_err(|err| {
eprintln!("Failed to spawn dot: {err:?}");
⋮----
let stdin = child.stdin.as_mut().unwrap();
stdin.write_all(&dot.into_bytes())?;
let status = child.wait_with_output()?.status;
if !status.success() {
return Err(io::Error::other(format!(
⋮----
Ok(())
⋮----
enum GraphVoteAccountMode {
⋮----
impl GraphVoteAccountMode {
⋮----
fn is_enabled(&self) -> bool {
!matches!(self, Self::Disabled)
⋮----
fn as_ref(&self) -> &str {
⋮----
struct GraphVoteAccountModeError;
impl FromStr for GraphVoteAccountMode {
type Err = GraphVoteAccountModeError;
fn from_str(s: &str) -> Result<Self, Self::Err> {
⋮----
Self::DISABLED => Ok(Self::Disabled),
Self::LAST_ONLY => Ok(Self::LastOnly),
Self::WITH_HISTORY => Ok(Self::WithHistory),
_ => Err(GraphVoteAccountModeError),
⋮----
struct GraphConfig {
⋮----
fn graph_forks(bank_forks: &BankForks, config: &GraphConfig) -> String {
let frozen_banks = bank_forks.frozen_banks();
⋮----
.frozen_banks()
.map(|(slot, _bank)| slot)
.collect();
⋮----
for parent in bank.parents() {
fork_slots.remove(&parent.slot());
⋮----
// Search all forks and collect the last vote made by each validator
⋮----
.vote_accounts()
.iter()
.map(|(_, (stake, _))| stake)
.sum();
for (stake, vote_account) in bank.vote_accounts().values() {
let vote_state_view = vote_account.vote_state_view();
if let Some(last_vote) = vote_state_view.last_voted_slot() {
let entry = last_votes.entry(*vote_state_view.node_pubkey()).or_insert((
⋮----
vote_state_view.clone(),
⋮----
*entry = (last_vote, vote_state_view.clone(), *stake, total_stake);
⋮----
// Figure the stake distribution at all the nodes containing the last vote from each
// validator
⋮----
for (last_vote_slot, _, stake, total_stake) in last_votes.values() {
⋮----
.entry(last_vote_slot)
.or_insert((0, 0, *total_stake));
⋮----
assert_eq!(entry.2, *total_stake)
⋮----
let mut dot = vec!["digraph {".to_string()];
// Build a subgraph consisting of all banks and links to their parent banks
dot.push("  subgraph cluster_banks {".to_string());
dot.push("    style=invis".to_string());
⋮----
let mut bank = bank_forks[*fork_slot].clone();
⋮----
for (_, vote_account) in bank.vote_accounts().values() {
⋮----
all_votes.entry(*vote_state_view.node_pubkey()).or_default();
⋮----
.entry(last_vote)
.or_insert_with(|| vote_state_view.clone());
⋮----
if !styled_slots.contains(&bank.slot()) {
dot.push(format!(
⋮----
styled_slots.insert(bank.slot());
⋮----
match bank.parent() {
⋮----
if bank.slot() > 0 {
dot.push(format!(r#"    "{}" -> "..." [dir=back]"#, bank.slot(),));
⋮----
let slot_distance = bank.slot() - parent.slot();
let penwidth = if bank.epoch() > parent.epoch() {
⋮----
format!("label=\"{} slots\",color=red", slot_distance - 1)
⋮----
"color=blue".to_string()
⋮----
bank = parent.clone();
⋮----
dot.push("  }".to_string());
// Strafe the banks with links from validators to the bank they last voted on,
// while collecting information about the absent votes and stakes
⋮----
all_votes.entry(*node_pubkey).and_modify(|validator_votes| {
validator_votes.remove(last_vote_slot);
⋮----
let maybe_styled_last_vote_slot = styled_slots.get(last_vote_slot);
if maybe_styled_last_vote_slot.is_none() {
⋮----
if config.vote_account_mode.is_enabled() {
⋮----
if matches!(config.vote_account_mode, GraphVoteAccountMode::WithHistory) {
format!(
⋮----
// Annotate the final "..." node with absent vote and stake information
⋮----
// Add for vote information from all banks.
⋮----
dot.push("}".to_string());
dot.join("\n")
⋮----
fn compute_slot_cost(
⋮----
.get_slot_entries_with_shred_info(slot, 0, allow_dead_slots)
.map_err(|err| format!("Slot: {slot}, Failed to load entries, err {err:?}"))?;
let num_entries = entries.len();
⋮----
num_transactions += entry.transactions.len();
⋮----
.into_iter()
.filter_map(|transaction| {
⋮----
feature_set.is_active(&agave_feature_set::static_instruction_limit::id()),
⋮----
warn!("Failed to compute cost of transaction: {err:?}");
⋮----
.ok()
⋮----
.for_each(|transaction| {
num_programs += transaction.message().instructions().len();
⋮----
let result = cost_tracker.try_add(&tx_cost);
if result.is_err() {
println!(
⋮----
for (program_id, _instruction) in transaction.message().program_instructions_iter()
⋮----
*program_ids.entry(*program_id).or_insert(0) += 1;
⋮----
println!("  Programs: {program_ids:?}");
⋮----
/// Finds the accounts needed to replay slots `snapshot_slot` to `ending_slot`.
/// Removes all other accounts from accounts_db, and updates the accounts hash
⋮----
/// Removes all other accounts from accounts_db, and updates the accounts hash
/// and capitalization. This is used by the --minimize option in create-snapshot
⋮----
/// and capitalization. This is used by the --minimize option in create-snapshot
/// Returns true if the minimized snapshot may be incomplete.
⋮----
/// Returns true if the minimized snapshot may be incomplete.
fn minimize_bank_for_snapshot(
⋮----
fn minimize_bank_for_snapshot(
⋮----
let ((transaction_account_set, possibly_incomplete), transaction_accounts_measure) = measure_time!(
⋮----
let total_accounts_len = transaction_account_set.len();
info!("Added {total_accounts_len} accounts from transactions. {transaction_accounts_measure}");
⋮----
fn assert_capitalization(bank: &Bank) {
let calculated = bank.calculate_capitalization_for_tests();
let expected = bank.capitalization();
assert_eq!(
⋮----
fn load_banking_trace_events_or_exit(ledger_path: &Path) -> BankingTraceEvents {
let file_paths = read_banking_trace_event_file_paths_or_exit(banking_trace_path(ledger_path));
info!("Using: banking trace event files: {file_paths:?}");
⋮----
eprintln!("Failed to load banking trace events: {error:?}");
exit(1)
⋮----
fn read_banking_trace_event_file_paths_or_exit(banking_trace_path: PathBuf) -> Vec<PathBuf> {
info!("Using: banking trace events dir: {banking_trace_path:?}");
let entries = match read_dir(&banking_trace_path) {
⋮----
eprintln!("Error: failed to open banking_trace_path: {error:?}");
exit(1);
⋮----
.flat_map(|entry| entry.ok().map(|entry| entry.file_name()))
⋮----
let mut event_file_paths = vec![];
if entry_names.is_empty() {
warn!("banking_trace_path dir is empty.");
⋮----
let event_file_name: OsString = BankingSimulator::event_file_name(index).into();
if entry_names.remove(&event_file_name) {
event_file_paths.push(banking_trace_path.join(event_file_name));
⋮----
if event_file_paths.is_empty() {
warn!("Error: no event files found");
⋮----
if !entry_names.is_empty() {
⋮----
.map(|name| banking_trace_path.join(name))
⋮----
warn!(
⋮----
// Reverse to load in the chronicle order (note that this isn't strictly needed)
event_file_paths.reverse();
⋮----
struct SlotRecorderConfig {
⋮----
fn setup_slot_recording(
⋮----
let record_slots = arg_matches.occurrences_of("record_slots") > 0;
let verify_slots = arg_matches.occurrences_of("verify_slots") > 0;
⋮----
assert!(bank.is_frozen());
⋮----
let collector_fee_details = bank.get_collector_fee_details();
⋮----
collector_fee_details.total_transaction_fee(),
collector_fee_details.total_priority_fee(),
⋮----
let cost_tracker = bank.read_cost_tracker().unwrap();
let slot = bank.slot();
⋮----
cost_tracker.report_stats(
⋮----
(Some(slot_callback as ProcessSlotCallback), None)
⋮----
eprintln!(
⋮----
let filename = Path::new(arg_matches.value_of_os("record_slots").unwrap());
let file = File::create(filename).unwrap_or_else(|err| {
eprintln!("Unable to write to file: {}: {:#}", filename.display(), err);
⋮----
if let Some(args) = arg_matches.values_of("record_slots_config") {
⋮----
_ => unreachable!(),
⋮----
let transaction_recorder = Some(std::thread::spawn(move || {
record_transactions(receiver, slots);
⋮----
Some(TransactionStatusSender {
⋮----
.unwrap();
let mut slots = slots.lock().unwrap();
if let Some(recorded_slot) = slots.iter_mut().find(|f| f.slot == details.slot) {
swap(&mut recorded_slot.transactions, &mut details.transactions);
⋮----
slots.push(details);
⋮----
Some(slot_callback as ProcessSlotCallback),
Some(SlotRecorderConfig {
⋮----
let filename = Path::new(arg_matches.value_of_os("verify_slots").unwrap());
let file = File::open(filename).unwrap_or_else(|err| {
eprintln!("Unable to read file: {}: {err:#}", filename.display());
⋮----
.unwrap_or_else(|err| {
eprintln!("Error loading slots file: {err:#}");
⋮----
if slots.lock().unwrap().is_empty() {
error!(
⋮----
} = slots.lock().unwrap().remove(0);
if bank.slot() != expected_slot || bank.hash().to_string() != expected_hash {
⋮----
info!("Expected slot: {expected_slot} hash: {expected_hash} correct");
⋮----
fn record_transactions(
⋮----
assert_eq!(batch.transactions.len(), batch.commit_results.len());
⋮----
.zip(batch.commit_results)
.zip(batch.transaction_indexes)
.map(|((tx, commit_result), index)| {
let message = tx.message();
⋮----
.account_keys()
⋮----
.map(|acc| acc.to_string())
⋮----
.instructions()
⋮----
.map(|ix| {
parse_ui_instruction(
⋮----
&message.account_keys(),
Some(TRANSACTION_LEVEL_STACK_HEIGHT as u32),
⋮----
let is_simple_vote_tx = tx.is_simple_vote_transaction();
let commit_details = commit_result.ok().map(|committed_tx| committed_tx.into());
⋮----
signature: tx.signature().to_string(),
⋮----
if let Some(recorded_slot) = slots.iter_mut().find(|f| f.slot == batch.slot) {
recorded_slot.transactions.extend(transactions);
⋮----
slots.push(SlotDetails {
⋮----
for slot in slots.lock().unwrap().iter_mut() {
slot.transactions.sort_by(|a, b| a.index.cmp(&b.index));
⋮----
use jemallocator::Jemalloc;
⋮----
fn main() {
⋮----
unsafe { signal_hook::low_level::register(signal_hook::consts::SIGUSR1, || {}) }.unwrap();
⋮----
let load_genesis_config_arg = load_genesis_arg();
let accounts_db_config_args = accounts_db_args();
let snapshot_config_args = snapshot_args();
⋮----
.long("halt-at-slot")
.value_name("SLOT")
.validator(is_slot)
.takes_value(true)
.help("Halt processing at the given slot");
⋮----
.long("os-memory-stats-reporting")
.help("Enable reporting of OS memory statistics.");
⋮----
.long("verify-accounts-index")
.takes_value(false)
.help("For debugging and tests on accounts index.");
⋮----
.long("limit-load-slot-count-from-snapshot")
⋮----
.help(
⋮----
.long("hard-fork")
⋮----
.multiple(true)
⋮----
.help("Add a hard fork at this slot");
⋮----
.long("allow-dead-slots")
⋮----
.help("Output dead slots as well");
⋮----
.long("hashes-per-tick")
.value_name("NUM_HASHES|\"sleep\"")
⋮----
.long("snapshot-version")
.value_name("SNAPSHOT_VERSION")
.validator(is_parsable::<SnapshotVersion>)
⋮----
.default_value(SnapshotVersion::default().into())
.help("Output snapshot version");
⋮----
.long("debug-key")
.validator(is_pubkey)
.value_name("ADDRESS")
⋮----
.help("Log when transactions are processed that reference the given key(s).");
⋮----
.long("geyser-plugin-config")
.value_name("FILE")
⋮----
.help("Specify the configuration file for the Geyser plugin.");
⋮----
.long("log-messages-bytes-limit")
⋮----
.validator(is_parsable::<usize>)
.value_name("BYTES")
.help("Maximum number of bytes written to the program log before truncation");
⋮----
.long("encoding")
⋮----
.possible_values(&["base64", "base64+zstd", "jsonParsed"])
.default_value("base64")
.help("Print account data in specified format when printing account contents.");
⋮----
.max(rent.minimum_balance(VoteStateV4::size_of()))
.to_string();
⋮----
.max(rent.minimum_balance(StakeStateV2::size_of()))
⋮----
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.global_setting(AppSettings::ColoredHelp)
.global_setting(AppSettings::InferSubcommands)
.global_setting(AppSettings::UnifiedHelpMessage)
.global_setting(AppSettings::VersionlessSubcommands)
.setting(AppSettings::SubcommandRequiredElseHelp)
.arg(
⋮----
.short("l")
.long("ledger")
.value_name("DIR")
⋮----
.global(true)
.default_value("ledger")
.help("Use DIR as ledger location"),
⋮----
.long("log")
⋮----
.help("Redirect logging to the specified file, stderr is used if unset"),
⋮----
.long("wal-recovery-mode")
.value_name("MODE")
⋮----
.possible_values(&[
⋮----
.help("Mode to recovery the ledger db write ahead log"),
⋮----
.long("force-update-to-open")
⋮----
.long("ignore-ulimit-nofile-error")
⋮----
.long("block-verification-method")
.value_name("METHOD")
⋮----
.possible_values(BlockVerificationMethod::cli_names())
.default_value(BlockVerificationMethod::default().into())
⋮----
.help(BlockVerificationMethod::cli_message()),
⋮----
.long("unified-scheduler-handler-threads")
.value_name("COUNT")
⋮----
.validator(|s| is_within_range(s, 1..))
⋮----
.help(DefaultSchedulerPool::cli_message()),
⋮----
.long("output")
.value_name("FORMAT")
⋮----
.possible_values(&["json", "json-compact"])
⋮----
.short("v")
.long("verbose")
⋮----
.help("Show additional information where supported"),
⋮----
.bigtable_subcommand()
.blockstore_subcommand()
.subcommands(blockstore_subcommands(true))
.subcommand(
⋮----
.about("Prints the ledger's genesis config")
.arg(&load_genesis_config_arg)
⋮----
.long("accounts")
⋮----
.help("Print the ledger's genesis accounts"),
⋮----
.long("no-account-data")
⋮----
.requires("accounts")
.help("Do not print account data when printing account contents."),
⋮----
.arg(&accounts_data_encoding_arg),
⋮----
.about("Prints the ledger's genesis hash")
.arg(&load_genesis_config_arg),
⋮----
.about("Modifies genesis parameters")
⋮----
.arg(&hashes_per_tick)
⋮----
.long("cluster-type")
.possible_values(&ClusterType::STRINGS)
⋮----
.help("Selects the features that will be enabled for the cluster"),
⋮----
.index(1)
⋮----
.help("Output directory for the modified genesis config"),
⋮----
.about("Prints the ledger's shred hash")
⋮----
.args(&accounts_db_config_args)
.args(&snapshot_config_args)
.arg(&hard_forks_arg),
⋮----
.about("Prints the hash of the working bank after reading the ledger")
⋮----
.arg(&halt_at_slot_arg),
⋮----
.about("Verify the ledger")
⋮----
.arg(&halt_at_slot_arg)
.arg(&limit_load_slot_count_from_snapshot_arg)
.arg(&verify_index_arg)
.arg(&hard_forks_arg)
.arg(&os_memory_stats_reporting_arg)
.arg(&allow_dead_slots_arg)
.arg(&debug_key_arg)
.arg(&geyser_plugin_args)
.arg(&log_messages_bytes_limit_arg)
⋮----
.long("skip-poh-verify")
⋮----
.long("skip-verification")
⋮----
.help("Skip ledger PoH and transaction verification."),
⋮----
.long("enable-rpc-transaction-history")
⋮----
.help("Store transaction info for processed slots into local ledger"),
⋮----
.long("enable-extended-tx-metadata-storage")
.requires("enable_rpc_transaction_history")
⋮----
.long("run-final-accounts-hash-calculation")
⋮----
.long("print-accounts-stats")
⋮----
.long("print-bank-hash")
⋮----
.help("After verifying the ledger, print the working bank's hash"),
⋮----
.long("write-bank-file")
⋮----
.long("record-slots")
.default_value("slots.json")
.value_name("FILENAME")
.help("Record slots to a file"),
⋮----
.long("verify-slots")
⋮----
.help("Verify slots match contents of file"),
⋮----
.long("record-slots-config")
⋮----
.possible_values(&["accounts", "tx"])
.requires("record_slots")
.conflicts_with_all(&[
⋮----
.long("abort-on-invalid-block")
⋮----
.long("no-block-cost-limits")
⋮----
.help("Disable block cost limits effectively by setting them to the max"),
⋮----
.long("enable-hash-overrides")
⋮----
.about("Create a Graphviz rendering of the ledger")
⋮----
.long("include-all-votes")
.help("Include all votes in the graph"),
⋮----
.help("Output file"),
⋮----
.long("vote-account-mode")
⋮----
.default_value(default_graph_vote_account_mode.as_ref())
.possible_values(GraphVoteAccountMode::ALL_MODE_STRINGS)
⋮----
.about("Create a new ledger snapshot")
⋮----
.arg(&snapshot_version_arg)
⋮----
.validator(|value| {
if value.parse::<Slot>().is_ok() || value == "ROOT" {
⋮----
Err(format!(
⋮----
.index(2)
⋮----
.required(false)
.long("warp-slot")
⋮----
.value_name("WARP_SLOT")
⋮----
.short("t")
.long("faucet-lamports")
.value_name("LAMPORTS")
⋮----
.requires("faucet_pubkey")
.help("Number of lamports to assign to the faucet"),
⋮----
.short("m")
.long("faucet-pubkey")
.value_name("PUBKEY")
⋮----
.validator(is_pubkey_or_keypair)
.requires("faucet_lamports")
.help("Path to file containing the faucet's pubkey"),
⋮----
.short("b")
.long("bootstrap-validator")
.value_name("IDENTITY_PUBKEY VOTE_PUBKEY STAKE_PUBKEY")
⋮----
.number_of_values(3)
⋮----
.help("The bootstrap validator's identity, vote and stake pubkeys"),
⋮----
.long("bootstrap-stake-authorized-pubkey")
.value_name("BOOTSTRAP STAKE AUTHORIZED PUBKEY")
⋮----
.long("bootstrap-validator-lamports")
⋮----
.default_value(default_bootstrap_validator_lamports)
.help("Number of lamports to assign to the bootstrap validator"),
⋮----
.long("bootstrap-validator-stake-lamports")
⋮----
.default_value(default_bootstrap_validator_stake_lamports)
⋮----
.long("rent-burn-percentage")
.value_name("NUMBER")
⋮----
.help("Adjust percentage of collected rent to burn")
.validator(is_valid_percentage),
⋮----
.long("remove-account")
⋮----
.help("List of accounts to remove while creating the snapshot"),
⋮----
.long("deactivate-feature-gate")
⋮----
.help("List of feature gates to deactivate while creating the snapshot"),
⋮----
.long("destake-vote-account")
⋮----
.help("List of validator vote accounts to destake"),
⋮----
.long("remove-stake-accounts")
⋮----
.help("Remove all existing stake accounts from the new snapshot"),
⋮----
.long("incremental")
⋮----
.conflicts_with("no_snapshot"),
⋮----
.long("minimized")
⋮----
.conflicts_with("incremental")
.requires("ending_slot"),
⋮----
.long("ending-slot")
⋮----
.value_name("ENDING_SLOT")
.help("Ending slot for minimized snapshot creation"),
⋮----
.long("recalculate-accounts-lt-hash")
⋮----
.help("Recalculate the accounts lt hash for minimized snapshots")
.long_help(
⋮----
.requires("minimized"),
⋮----
.long("snapshot-archive-format")
.possible_values(SUPPORTED_ARCHIVE_COMPRESSION)
.default_value(DEFAULT_ARCHIVE_COMPRESSION)
.value_name("ARCHIVE_TYPE")
⋮----
.help("Snapshot archive format to use.")
⋮----
.long("snapshot-zstd-compression-level")
.default_value("0")
.value_name("LEVEL")
⋮----
.help("The compression level to use when archiving with zstd")
⋮----
.long("enable-capitalization-change")
⋮----
.help("If snapshot creation should succeed with a capitalization delta."),
⋮----
.about("Simulate producing blocks with banking trace event files in the ledger")
⋮----
.long("block-production-method")
⋮----
.possible_values(BlockProductionMethod::cli_names())
.default_value(BlockProductionMethod::default().into())
.help(BlockProductionMethod::cli_message()),
⋮----
.long("transaction-structure")
.value_name("STRUCT")
⋮----
.possible_values(TransactionStructure::cli_names())
.default_value(TransactionStructure::default().into())
.help(TransactionStructure::cli_message()),
⋮----
.long("first-simulated-slot")
⋮----
.required(true)
.help("Start simulation at the given slot"),
⋮----
.about("Print account stats and contents after processing the ledger")
⋮----
.arg(&accounts_data_encoding_arg)
⋮----
.long("include-sysvars")
⋮----
.help("Include sysvars too"),
⋮----
.long("no-account-contents")
⋮----
.long("account")
⋮----
.long("program-accounts")
⋮----
.conflicts_with("account")
.help("Limit output to accounts owned by the provided program pubkey"),
⋮----
.about("Print capitalization (aka, total supply) while checksumming it")
⋮----
.long("warp-epoch")
⋮----
.value_name("WARP_EPOCH")
⋮----
.long("inflation")
⋮----
.possible_values(&["pico", "full", "none"])
.help("Overwrite inflation when warping"),
⋮----
.long("enable-credits-auto-rewind")
⋮----
.help("Enable credits auto rewind"),
⋮----
.long("recalculate-capitalization")
⋮----
.long("csv-filename")
⋮----
.help("Output file in the csv format"),
⋮----
.about(
⋮----
.value_name("SLOTS")
⋮----
.arg(&allow_dead_slots_arg),
⋮----
.program_subcommand()
.get_matches();
let logfile = value_t!(matches, "logfile", PathBuf).ok();
⋮----
info!("{} {}", crate_name!(), solana_version::version!());
let ledger_path = PathBuf::from(value_t_or_exit!(matches, "ledger_path", String));
let verbose_level = matches.occurrences_of("verbose");
let enforce_nofile_limit = !matches.is_present("ignore_ulimit_nofile_error");
adjust_nofile_limit(enforce_nofile_limit).unwrap_or_else(|err| {
eprintln!("Error: {err:?}");
⋮----
.thread_name(|i| format!("solRayonGlob{i:02}"))
.build_global()
⋮----
match matches.subcommand() {
("bigtable", Some(arg_matches)) => bigtable_process_command(&ledger_path, arg_matches),
("blockstore", Some(arg_matches)) => blockstore_process_command(&ledger_path, arg_matches),
("program", Some(arg_matches)) => program(&ledger_path, arg_matches),
⋮----
| ("slot", Some(_)) => blockstore_process_command(&ledger_path, &matches),
⋮----
let ledger_path = canonicalize_ledger_path(&ledger_path);
⋮----
let output_accounts = arg_matches.is_present("accounts");
let genesis_config = open_genesis_config_by(&ledger_path, arg_matches);
⋮----
let output_config = parse_account_output_config(arg_matches);
⋮----
.map(|(pubkey, account)| {
⋮----
println!("{}", output_format.formatted_string(&accounts));
⋮----
println!("{genesis_config}");
⋮----
let mut genesis_config = open_genesis_config_by(&ledger_path, arg_matches);
⋮----
PathBuf::from(arg_matches.value_of("output_directory").unwrap());
if let Some(cluster_type) = cluster_type_of(arg_matches, "cluster_type") {
⋮----
if let Some(hashes_per_tick) = arg_matches.value_of("hashes_per_tick") {
⋮----
_ => Some(value_t_or_exit!(arg_matches, "hashes_per_tick", u64)),
⋮----
create_new_ledger(
⋮----
eprintln!("Failed to write genesis config: {err:?}");
⋮----
println!("{}", open_genesis_config_by(&output_directory, arg_matches));
⋮----
let mut process_options = parse_process_options(&ledger_path, arg_matches);
if process_options.halt_at_slot.is_none() {
process_options.halt_at_slot = Some(0);
⋮----
let blockstore = open_blockstore(
⋮----
get_access_type(&process_options),
⋮----
load_and_process_ledger_or_exit(
⋮----
arg_matches.is_present("os_memory_stats_reporting");
⋮----
if arg_matches.is_present("enable_hash_overrides") {
let banking_trace_events = load_banking_trace_events_or_exit(&ledger_path);
⋮----
Some(banking_trace_events.hash_overrides().clone());
⋮----
let (slot_callback, slot_recorder_config) = setup_slot_recording(arg_matches);
⋮----
.as_ref()
.and_then(|config| config.transaction_status_sender.clone());
⋮----
let print_accounts_stats = arg_matches.is_present("print_accounts_stats");
let print_bank_hash = arg_matches.is_present("print_bank_hash");
let write_bank_file = arg_matches.is_present("write_bank_file");
⋮----
info!("genesis hash: {}", genesis_config.hash());
⋮----
let working_bank = bank_forks.read().unwrap().working_bank();
⋮----
working_bank.print_accounts_stats();
⋮----
slot: working_bank.slot(),
hash: working_bank.hash().to_string(),
⋮----
println!("{}", output_format.formatted_string(&slot_bank_hash));
⋮----
warn!("Unable to write bank hash_details file: {err}");
⋮----
.ok();
⋮----
slot_recorder_config.transaction_status_sender.take();
drop(transaction_status_sender);
⋮----
transaction_recorder.join().unwrap();
⋮----
let slot_details = slot_recorder_config.slot_details.lock().unwrap();
⋮----
bank_hash_details::BankHashDetails::new(slot_details.to_vec());
⋮----
serde_json::to_writer_pretty(writer, &bank_hashes).unwrap();
⋮----
exit_signal.store(true, Ordering::Relaxed);
system_monitor_service.join().unwrap();
⋮----
let output_file = value_t_or_exit!(arg_matches, "graph_filename", String);
⋮----
include_all_votes: arg_matches.is_present("include_all_votes"),
vote_account_mode: value_t_or_exit!(
⋮----
let process_options = parse_process_options(&ledger_path, arg_matches);
⋮----
let dot = graph_forks(&bank_forks.read().unwrap(), &graph_config);
let extension = Path::new(&output_file).extension();
let result = if extension == Some(OsStr::new("pdf")) {
render_dot(dot, &output_file, "pdf")
} else if extension == Some(OsStr::new("png")) {
render_dot(dot, &output_file, "png")
⋮----
.and_then(|mut file| file.write_all(&dot.into_bytes()))
⋮----
Ok(_) => println!("Wrote {output_file}"),
Err(err) => eprintln!("Unable to write {output_file}: {err}"),
⋮----
.is_present("os_memory_stats_reporting")
.then(|| {
⋮----
let is_incremental = arg_matches.is_present("incremental");
let is_minimized = arg_matches.is_present("minimized");
let output_directory = value_t!(arg_matches, "output_directory", PathBuf)
.unwrap_or_else(|_| {
let snapshot_archive_path = value_t!(arg_matches, "snapshots", String)
⋮----
.map(PathBuf::from);
⋮----
value_t!(arg_matches, "incremental_snapshot_archive_path", String)
⋮----
incremental_snapshot_archive_path.clone()
⋮----
snapshot_archive_path.clone()
⋮----
(_, _, _) => ledger_path.clone(),
⋮----
let mut warp_slot = value_t!(arg_matches, "warp_slot", Slot).ok();
let remove_stake_accounts = arg_matches.is_present("remove_stake_accounts");
let faucet_pubkey = pubkey_of(arg_matches, "faucet_pubkey");
⋮----
value_t!(arg_matches, "faucet_lamports", u64).unwrap_or(0);
let rent_burn_percentage = value_t!(arg_matches, "rent_burn_percentage", u8);
let hashes_per_tick = arg_matches.value_of("hashes_per_tick");
⋮----
pubkey_of(arg_matches, "bootstrap_stake_authorized_pubkey");
⋮----
value_t_or_exit!(arg_matches, "bootstrap_validator_lamports", u64);
⋮----
value_t_or_exit!(arg_matches, "bootstrap_validator_stake_lamports", u64);
let minimum_stake_lamports = rent.minimum_balance(StakeStateV2::size_of());
⋮----
pubkeys_of(arg_matches, "bootstrap_validator");
⋮----
pubkeys_of(arg_matches, "accounts_to_remove").unwrap_or_default();
⋮----
pubkeys_of(arg_matches, "feature_gates_to_deactivate").unwrap_or_default();
⋮----
pubkeys_of(arg_matches, "vote_accounts_to_destake")
.unwrap_or_default()
⋮----
let snapshot_version = arg_matches.value_of("snapshot_version").map_or(
⋮----
s.parse::<SnapshotVersion>().unwrap_or_else(|e| {
eprintln!("Error: {e}");
⋮----
value_t_or_exit!(arg_matches, "snapshot_archive_format", String);
⋮----
.unwrap_or_else(|| {
panic!("Archive format not recognized: {archive_format_str}")
⋮----
config.compression_level = value_t_or_exit!(
⋮----
let blockstore = Arc::new(open_blockstore(
⋮----
let snapshot_slot = if Some("ROOT") == arg_matches.value_of("snapshot_slot") {
⋮----
.rooted_slot_iterator(0)
.expect("Failed to get rooted slot iterator")
.last()
.expect("Failed to get root")
⋮----
value_t_or_exit!(arg_matches, "snapshot_slot", Slot)
⋮----
.meta(snapshot_slot)
.unwrap()
.filter(|m| m.is_full())
.is_none()
⋮----
process_options.halt_at_slot = Some(snapshot_slot);
⋮----
let ending_slot = value_t_or_exit!(arg_matches, "ending_slot", Slot);
⋮----
Some(ending_slot)
⋮----
arg_matches.is_present("enable_capitalization_change");
⋮----
info!(
⋮----
} = load_and_process_ledger_or_exit(
⋮----
blockstore.clone(),
⋮----
.read()
⋮----
.get(snapshot_slot)
⋮----
eprintln!("Error: Slot {snapshot_slot} is not available");
⋮----
accounts_background_service.join().unwrap();
let child_bank_required = rent_burn_percentage.is_ok()
|| hashes_per_tick.is_some()
⋮----
|| !accounts_to_remove.is_empty()
|| !feature_gates_to_deactivate.is_empty()
|| !vote_accounts_to_destake.is_empty()
|| faucet_pubkey.is_some()
|| bootstrap_validator_pubkeys.is_some();
⋮----
bank.clone(),
bank.collector_id(),
bank.slot() + 1,
⋮----
child_bank.set_rent_burn_percentage(rent_burn_percentage);
⋮----
child_bank.set_hashes_per_tick(match hashes_per_tick {
⋮----
child_bank.get_account(&address).unwrap_or_else(|| {
⋮----
if feature.activated_at.is_none() {
warn!("Feature gate is not yet activated: {address}");
⋮----
child_bank.deactivate_feature(&address);
⋮----
eprintln!("Error: Account is not a `Feature`: {address}");
⋮----
account.set_lamports(0);
child_bank.store_account(&address, &account);
debug!("Feature gate deactivated: {address}");
⋮----
bank.store_account(
⋮----
.get_program_accounts(
⋮----
bank.store_account(&address, &account);
⋮----
let mut account = bank.get_account(&address).unwrap_or_else(|| {
⋮----
debug!("Account removed: {address}");
⋮----
if !vote_accounts_to_destake.is_empty() {
⋮----
if let Ok(StakeStateV2::Stake(meta, stake, _)) = account.state() {
if vote_accounts_to_destake.contains(&stake.delegation.voter_pubkey)
⋮----
account.set_state(&StakeStateV2::Initialized(meta)).unwrap();
⋮----
assert_eq!(bootstrap_validator_pubkeys.len() % 3, 0);
⋮----
let mut v = bootstrap_validator_pubkeys.clone();
v.sort();
v.dedup();
if v.len() != bootstrap_validator_pubkeys.len() {
⋮----
bootstrap_validator_pubkeys.iter();
⋮----
let Some(identity_pubkey) = bootstrap_validator_pubkeys_iter.next()
⋮----
let vote_pubkey = bootstrap_validator_pubkeys_iter.next().unwrap();
let stake_pubkey = bootstrap_validator_pubkeys_iter.next().unwrap();
⋮----
rent.minimum_balance(VoteStateV4::size_of()).max(1),
⋮----
.unwrap_or(identity_pubkey),
⋮----
bank.store_account(vote_pubkey, &vote_account);
⋮----
genesis_config.epoch_schedule.get_first_slot_in_epoch(
genesis_config.epoch_schedule.get_epoch(snapshot_slot) + 2,
⋮----
warn!("Warping to slot {minimum_warp_slot}");
warp_slot = Some(minimum_warp_slot);
⋮----
let num_ticks_per_slot = bank.ticks_per_slot();
let num_hashes_per_tick = bank.hashes_per_tick().unwrap_or(0);
let parent_blockhash = bank.last_blockhash();
⋮----
create_ticks(num_ticks_per_slot, num_hashes_per_tick, parent_blockhash);
⋮----
tick_entries.iter().for_each(|tick_entry| {
bank.register_tick(&tick_entry.hash, &scheduler);
⋮----
let pre_capitalization = bank.capitalization();
let post_capitalization = bank.calculate_capitalization_for_tests();
bank.set_capitalization_for_tests(post_capitalization);
⋮----
format!("-{}", pre_capitalization - post_capitalization)
⋮----
(post_capitalization - pre_capitalization).to_string()
⋮----
let msg = format!("Capitalization change: {amount} lamports");
warn!("{msg}");
⋮----
Some(msg)
⋮----
bank.squash();
bank.force_flush_accounts_cache();
⋮----
minimize_bank_for_snapshot(
⋮----
ending_slot.unwrap(),
arg_matches.is_present("recalculate_accounts_lt_hash"),
⋮----
if starting_snapshot_hashes.is_none() {
⋮----
let full_snapshot_slot = starting_snapshot_hashes.unwrap().full.0 .0;
if bank.slot() <= full_snapshot_slot {
⋮----
Some(snapshot_version),
output_directory.clone(),
⋮----
eprintln!("Unable to create incremental snapshot: {err}");
⋮----
eprintln!("Unable to create snapshot: {err}");
⋮----
let starting_epoch = bank.epoch_schedule().get_epoch(snapshot_slot);
⋮----
bank.epoch_schedule().get_epoch(ending_slot.unwrap());
⋮----
println!("{msg}");
⋮----
let slot = value_t!(arg_matches, "first_simulated_slot", Slot).unwrap();
⋮----
let Some(parent_slot) = simulator.parent_slot() else {
⋮----
process_options.halt_at_slot = Some(parent_slot);
⋮----
let block_production_method = value_t_or_exit!(
⋮----
info!("Using: block-production-method: {block_production_method}");
match simulator.start(
⋮----
Ok(()) => println!("Ok"),
⋮----
eprintln!("{error:?}");
⋮----
let bank = bank_forks.read().unwrap().working_bank();
let include_sysvars = arg_matches.is_present("include_sysvars");
let output_config = if arg_matches.is_present("no_account_contents") {
⋮----
Some(parse_account_output_config(arg_matches))
⋮----
let mode = if let Some(pubkeys) = pubkeys_of(arg_matches, "account") {
info!("Scanning individual accounts: {pubkeys:?}");
⋮----
} else if let Some(pubkey) = pubkey_of(arg_matches, "program_accounts") {
info!("Scanning program accounts for {pubkey}");
⋮----
info!("Scanning all accounts");
⋮----
let (_, scan_time) = measure_time!(
⋮----
info!("{scan_time}");
⋮----
let bank_forks = bank_forks.read().unwrap();
let slot = bank_forks.working_bank().slot();
let bank = bank_forks.get(slot).unwrap_or_else(|| {
eprintln!("Error: Slot {slot} is not available");
⋮----
if arg_matches.is_present("recalculate_capitalization") {
println!("Recalculating capitalization");
let old_capitalization = bank.capitalization();
let new_capitalization = bank.calculate_capitalization_for_tests();
bank.set_capitalization_for_tests(new_capitalization);
⋮----
eprintln!("Capitalization was identical: {}", Sol(old_capitalization));
⋮----
if arg_matches.is_present("warp_epoch") {
⋮----
let raw_warp_epoch = value_t!(arg_matches, "warp_epoch", String).unwrap();
let warp_epoch = if raw_warp_epoch.starts_with('+') {
base_bank.epoch() + value_t!(arg_matches, "warp_epoch", Epoch).unwrap()
⋮----
value_t!(arg_matches, "warp_epoch", Epoch).unwrap()
⋮----
if warp_epoch < base_bank.epoch() {
⋮----
if let Ok(raw_inflation) = value_t!(arg_matches, "inflation", String) {
let inflation = match raw_inflation.as_str() {
⋮----
base_bank.set_inflation(inflation);
⋮----
.epoch_schedule()
.get_first_slot_in_epoch(warp_epoch);
⋮----
genesis_config.rent.minimum_balance(Feature::size_of()),
⋮----
if arg_matches.is_present("enable_credits_auto_rewind") {
base_bank.unfreeze_for_ledger_tool();
⋮----
.get_account(&feature_set::credits_auto_rewind::id())
⋮----
base_bank.store_account(
⋮----
warn!("Already credits_auto_rewind is activated (or scheduled)");
⋮----
.get_account(&feature_set::deprecate_rewards_sysvar::id())
.is_some()
⋮----
assert_eq!(force_enabled_count, store_failed_count);
⋮----
let old_cap = base_bank.capitalization();
let new_cap = base_bank.calculate_capitalization_for_tests();
base_bank.set_capitalization_for_tests(new_cap);
⋮----
struct PointDetail {
⋮----
struct CalculationDetail {
⋮----
stake_calculation_details.entry(**pubkey).or_default();
⋮----
detail.points.push(PointDetail {
⋮----
detail.point_value = Some(point_value.clone());
let mut last_point_value = last_point_value.write().unwrap();
if let Some(last_point_value) = last_point_value.as_ref() {
assert_eq!(last_point_value, point_value);
⋮----
*last_point_value = Some(point_value.clone());
⋮----
detail.old_credits_observed = Some(*old_credits_observed);
⋮----
Some(delegation.deactivation_epoch);
⋮----
if detail.skipped_reasons.is_empty() {
detail.skipped_reasons = format!("{skipped_reason:?}");
⋮----
use std::fmt::Write;
let _ = write!(
⋮----
base_bank.clone(),
base_bank.collector_id(),
⋮----
warped_bank.freeze();
let mut csv_writer = if arg_matches.is_present("csv_filename") {
⋮----
value_t_or_exit!(arg_matches, "csv_filename", String);
let file = File::create(csv_filename).unwrap();
Some(csv::WriterBuilder::new().from_writer(file))
⋮----
println!("Slot: {} => {}", base_bank.slot(), warped_bank.slot());
println!("Epoch: {} => {}", base_bank.epoch(), warped_bank.epoch());
assert_capitalization(&base_bank);
assert_capitalization(&warped_bank);
let interest_per_epoch = ((warped_bank.capitalization() as f64)
/ (base_bank.capitalization() as f64)
⋮----
/ warped_bank.epoch_duration_in_years(base_bank.epoch());
⋮----
warped_bank.get_all_accounts_modified_since_parent();
⋮----
.get_account(pubkey)
.map(|a| a.lamports())
.unwrap_or_default(),
⋮----
rewarded_accounts.sort_unstable_by_key(
⋮----
*account.owner(),
⋮----
account.lamports() - base_lamports,
⋮----
.map(|entry| *entry.key())
⋮----
.difference(
⋮----
.map(|(pubkey, ..)| **pubkey)
.collect(),
⋮----
.map(|pubkey| (*pubkey, warped_bank.get_account(pubkey).unwrap()))
⋮----
unchanged_accounts.sort_unstable_by_key(|(pubkey, account)| {
(*account.owner(), account.lamports(), *pubkey)
⋮----
let unchanged_accounts = unchanged_accounts.into_iter();
⋮----
.map(|(pubkey, account, ..)| (*pubkey, account.clone()));
let all_accounts = unchanged_accounts.chain(rewarded_accounts);
⋮----
if solana_sdk_ids::sysvar::check_id(warped_account.owner()) {
⋮----
if let Some(base_account) = base_bank.get_account(&pubkey) {
let delta = warped_account.lamports() - base_account.lamports();
let detail_ref = stake_calculation_details.get(&pubkey);
⋮----
detail_ref.as_ref().map(|detail_ref| detail_ref.value());
⋮----
struct InflationRecord {
⋮----
fn format_or_na<T: std::fmt::Display>(
⋮----
data.map(|data| format!("{data}"))
.unwrap_or_else(|| "N/A".to_owned())
⋮----
.map(|d| d.points.iter().map(Some).collect::<Vec<_>>())
.unwrap_or_default();
if point_details.is_empty() {
point_details.push(None);
⋮----
.clone()
.map_or((None, None), |pv| {
(Some(pv.rewards), Some(pv.points))
⋮----
cluster_type: format!("{:?}", base_bank.cluster_type()),
rewarded_epoch: base_bank.epoch(),
account: format!("{pubkey}"),
owner: format!("{}", base_account.owner()),
old_balance: base_account.lamports(),
new_balance: warped_account.lamports(),
data_size: base_account.data().len(),
delegation: format_or_na(detail.map(|d| d.voter)),
delegation_owner: format_or_na(
detail.map(|d| d.voter_owner),
⋮----
effective_stake: format_or_na(
detail.map(|d| d.current_effective_stake),
⋮----
delegated_stake: format_or_na(
detail.map(|d| d.total_stake),
⋮----
rent_exempt_reserve: format_or_na(
detail.map(|d| d.rent_exempt_reserve),
⋮----
activation_epoch: format_or_na(detail.map(|d| {
⋮----
deactivation_epoch: format_or_na(
detail.and_then(|d| d.deactivation_epoch),
⋮----
earned_epochs: format_or_na(detail.map(|d| d.epochs)),
epoch: format_or_na(point_detail.map(|d| d.epoch)),
epoch_credits: format_or_na(
point_detail.map(|d| d.credits),
⋮----
epoch_points: format_or_na(
point_detail.map(|d| d.points),
⋮----
epoch_stake: format_or_na(
point_detail.map(|d| d.stake),
⋮----
old_credits_observed: format_or_na(
detail.and_then(|d| d.old_credits_observed),
⋮----
new_credits_observed: format_or_na(
detail.and_then(|d| d.new_credits_observed),
⋮----
base_rewards: format_or_na(
detail.map(|d| d.base_rewards),
⋮----
stake_rewards: format_or_na(
detail.map(|d| d.stake_rewards),
⋮----
vote_rewards: format_or_na(
detail.map(|d| d.vote_rewards),
⋮----
commission: format_or_na(detail.map(|d| d.commission)),
cluster_rewards: format_or_na(cluster_rewards),
cluster_points: format_or_na(cluster_points),
old_capitalization: base_bank.capitalization(),
new_capitalization: warped_bank.capitalization(),
⋮----
csv_writer.serialize(&record).unwrap();
⋮----
error!("new account!?: {pubkey}");
⋮----
println!("Sum of lamports changes: {}", Sol(overall_delta));
⋮----
eprintln!("Capitalization isn't verified because it's recalculated");
⋮----
if arg_matches.is_present("inflation") {
⋮----
assert_capitalization(&bank);
println!("Inflation: {:?}", bank.inflation());
println!("Capitalization: {}", Sol(bank.capitalization()));
⋮----
open_blockstore(&ledger_path, arg_matches, AccessType::ReadOnly);
let mut slots: Vec<u64> = vec![];
if !arg_matches.is_present("slots") {
if let Ok(metas) = blockstore.slot_meta_iterator(0) {
slots = metas.map(|(slot, _)| slot).collect();
⋮----
slots = values_t_or_exit!(arg_matches, "slots", Slot);
⋮----
let allow_dead_slots = arg_matches.is_present("allow_dead_slots");
⋮----
if let Err(err) = compute_slot_cost(&blockstore, slot, allow_dead_slots) {
eprintln!("{err}");
⋮----
eprintln!("{}", matches.usage());
⋮----
measure_total_execution_time.stop();
info!("{measure_total_execution_time}");

================
File: ledger-tool/src/output.rs
================
pub struct SlotInfo {
⋮----
pub struct SlotBounds<'a> {
⋮----
impl VerboseDisplay for SlotBounds<'_> {}
impl QuietDisplay for SlotBounds<'_> {}
impl Display for SlotBounds<'_> {
fn fmt(&self, f: &mut Formatter) -> fmt::Result {
⋮----
let first = self.slots.first.unwrap();
let last = self.slots.last.unwrap();
⋮----
writeln!(
⋮----
writeln!(f, "Non-empty slots: {all_slots:?}")?;
⋮----
writeln!(f, "Ledger has data for slot {first:?}")?;
⋮----
let first_rooted = self.roots.first.unwrap_or_default();
let last_rooted = self.roots.last.unwrap_or_default();
let num_after_last_root = self.roots.num_after_last_root.unwrap_or_default();
⋮----
writeln!(f, "  and {num_after_last_root:?} slots past the last root")?;
⋮----
writeln!(f, "  with no rooted slots")?;
⋮----
writeln!(f, "Ledger is empty")?;
⋮----
Ok(())
⋮----
pub struct SlotBankHash {
⋮----
impl VerboseDisplay for SlotBankHash {}
impl QuietDisplay for SlotBankHash {}
impl Display for SlotBankHash {
⋮----
writeln!(f, "Bank hash for slot {}: {}", self.slot, self.hash)
⋮----
fn writeln_entry(f: &mut dyn fmt::Write, i: usize, entry: &CliEntry, prefix: &str) -> fmt::Result {
⋮----
pub struct CliEntries {
⋮----
impl QuietDisplay for CliEntries {}
impl VerboseDisplay for CliEntries {}
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
writeln!(f, "Slot {}", self.slot)?;
for (i, entry) in self.entries.iter().enumerate() {
writeln_entry(f, i, entry, "  ")?;
⋮----
pub struct CliEntry {
⋮----
fn from(entry_summary: EntrySummary) -> Self {
⋮----
hash: entry_summary.hash.to_string(),
⋮----
fn from(populated_entry: &CliPopulatedEntry) -> Self {
⋮----
hash: populated_entry.hash.clone(),
⋮----
pub struct CliPopulatedEntry {
⋮----
pub struct CliBlockWithEntries {
⋮----
impl QuietDisplay for CliBlockWithEntries {}
impl VerboseDisplay for CliBlockWithEntries {}
⋮----
writeln!(f, "Slot: {}", self.slot)?;
⋮----
writeln!(f, "Blockhash: {}", self.encoded_confirmed_block.blockhash)?;
⋮----
writeln!(f, "Block Height: {block_height:?}")?;
⋮----
if !self.encoded_confirmed_block.rewards.is_empty() {
let mut rewards = self.encoded_confirmed_block.rewards.clone();
rewards.sort_by(|a, b| a.pubkey.cmp(&b.pubkey));
⋮----
writeln!(f, "Rewards:")?;
⋮----
for (index, entry) in self.encoded_confirmed_block.entries.iter().enumerate() {
writeln_entry(f, index, &entry.into(), "")?;
for (index, transaction_with_meta) in entry.transactions.iter().enumerate() {
writeln!(f, "  Transaction {index}:")?;
writeln_transaction(
⋮----
&transaction_with_meta.transaction.decode().unwrap(),
transaction_with_meta.meta.as_ref(),
⋮----
pub struct CliDuplicateSlotProof {
⋮----
impl QuietDisplay for CliDuplicateSlotProof {}
impl VerboseDisplay for CliDuplicateSlotProof {
fn write_str(&self, w: &mut dyn std::fmt::Write) -> std::fmt::Result {
write!(w, "    Shred1 ")?;
⋮----
write!(w, "    Shred2 ")?;
⋮----
writeln!(w, "    Erasure consistency {erasure_consistency}")?;
⋮----
fn fmt(&self, f: &mut Formatter<'_>) -> fmt::Result {
write!(f, "    Shred1 {}", self.shred1)?;
write!(f, "    Shred2 {}", self.shred2)?;
⋮----
writeln!(f, "    Erasure consistency {erasure_consistency}")?;
⋮----
fn from(proof: DuplicateSlotProof) -> Self {
let shred1 = Shred::new_from_serialized_shred(proof.shred1).unwrap();
let shred2 = Shred::new_from_serialized_shred(proof.shred2).unwrap();
let erasure_consistency = (shred1.shred_type() == ShredType::Code
&& shred2.shred_type() == ShredType::Code)
.then(|| ErasureMeta::check_erasure_consistency(&shred1, &shred2));
⋮----
pub struct CliDuplicateShred {
⋮----
impl CliDuplicateShred {
fn write_common(&self, w: &mut dyn std::fmt::Write) -> std::fmt::Result {
⋮----
impl QuietDisplay for CliDuplicateShred {}
impl VerboseDisplay for CliDuplicateShred {
⋮----
self.write_common(w)?;
writeln!(w, "       payload: {:?}", self.payload)
⋮----
self.write_common(f)
⋮----
fn from(shred: Shred) -> Self {
⋮----
fec_set_index: shred.fec_set_index(),
index: shred.index(),
shred_type: shred.shred_type(),
version: shred.version(),
merkle_root: shred.merkle_root().ok(),
chained_merkle_root: shred.chained_merkle_root().ok(),
last_in_slot: shred.last_in_slot(),
payload: Vec::from(shred.into_payload().bytes),
⋮----
pub struct EncodedConfirmedBlockWithEntries {
⋮----
impl EncodedConfirmedBlockWithEntries {
pub fn try_from(
⋮----
let mut entries = vec![];
for (i, entry) in entries_iterator.into_iter().enumerate() {
⋮----
.saturating_add(entry.num_transactions as usize);
⋮----
.get(entry.starting_transaction_index..ending_transaction_index)
.ok_or(LedgerToolError::Generic(format!(
⋮----
entries.push(CliPopulatedEntry {
⋮----
hash: entry.hash.to_string(),
⋮----
transactions: transactions.to_vec(),
⋮----
Ok(Self {
⋮----
pub(crate) fn encode_confirmed_block(
⋮----
let encoded_block = confirmed_block.encode_with_options(
⋮----
max_supported_transaction_version: Some(0),
⋮----
let encoded_block: EncodedConfirmedBlock = encoded_block.into();
Ok(encoded_block)
⋮----
fn encode_versioned_transactions(block: BlockWithoutMetadata) -> EncodedConfirmedBlock {
⋮----
.into_iter()
.map(|transaction| EncodedTransactionWithStatusMeta {
transaction: transaction.encode(UiTransactionEncoding::Base64),
⋮----
.collect();
⋮----
previous_blockhash: Hash::default().to_string(),
⋮----
pub enum BlockContents {
⋮----
// A VersionedConfirmedBlock analogue for use when the transaction metadata
// fields are unavailable. Also supports non-full blocks
pub struct BlockWithoutMetadata {
⋮----
impl BlockContents {
pub fn transactions(&self) -> impl Iterator<Item = &VersionedTransaction> {
⋮----
.iter()
.map(|VersionedTransactionWithStatusMeta { transaction, .. }| transaction),
⋮----
BlockContents::BlockWithoutMetadata(block) => Either::Right(block.transactions.iter()),
⋮----
type Error = LedgerToolError;
fn try_from(block_contents: BlockContents) -> Result<Self> {
⋮----
encode_confirmed_block(ConfirmedBlock::from(block))
⋮----
BlockContents::BlockWithoutMetadata(block) => Ok(encode_versioned_transactions(block)),
⋮----
pub fn output_slot(
⋮----
let is_root = blockstore.is_root(slot);
let is_dead = blockstore.is_dead(slot);
⋮----
eprintln!("Slot {slot} is marked as both a root and dead, this shouldn't be possible");
⋮----
println!(
⋮----
return Err(LedgerToolError::from(BlockstoreError::DeadSlot));
⋮----
let Some(meta) = blockstore.meta(slot)? else {
return Ok(());
⋮----
let (block_contents, entries) = match blockstore.get_complete_block_with_entries(
⋮----
/*require_previous_blockhash:*/ false,
/*populate_entries:*/ true,
⋮----
// Transaction metadata could be missing, try to fetch just the
// entries and leave the metadata fields empty
let (entries, _, _) = blockstore.get_slot_entries_with_shred_info(
⋮----
/*shred_start_index:*/ 0,
⋮----
.last()
.filter(|_| meta.is_full())
.map(|entry| entry.hash)
.unwrap_or(Hash::default());
let parent_slot = meta.parent_slot.unwrap_or(0);
let mut entry_summaries = Vec::with_capacity(entries.len());
⋮----
.flat_map(|entry| {
entry_summaries.push(EntrySummary {
⋮----
num_transactions: entry.transactions.len() as u64,
⋮----
starting_transaction_index += entry.transactions.len();
⋮----
blockhash: blockhash.to_string(),
⋮----
// Given that Blockstore::get_complete_block_with_entries() returned Ok(_), we know
// that we have a full block so meta.consumed is the number of shreds in the block
⋮----
println!("  {meta:?} is_full: {}", meta.is_full());
⋮----
for entry in entries.iter() {
⋮----
for transaction in block_contents.transactions() {
⋮----
for program_id in get_program_ids(transaction) {
*program_ids.entry(*program_id).or_insert(0) += 1;
⋮----
for (pubkey, count) in program_ids.iter() {
*all_program_ids.entry(*pubkey).or_insert(0) += count;
⋮----
println!("  Programs:");
output_sorted_program_ids(program_ids);
⋮----
println!("{}", output_format.formatted_string(&cli_block));
⋮----
pub fn output_ledger(
⋮----
let slot_iterator = blockstore.slot_meta_iterator(starting_slot)?;
⋮----
stdout().write_all(b"{\"ledger\":[\n")?;
⋮----
let num_slots = num_slots.unwrap_or(Slot::MAX);
⋮----
if only_rooted && !blockstore.is_root(slot) {
⋮----
if let Err(err) = output_slot(
⋮----
eprintln!("{err}");
⋮----
stdout().write_all(b"\n]}\n")?;
⋮----
println!("Summary of Programs:");
output_sorted_program_ids(all_program_ids);
⋮----
pub fn output_sorted_program_ids(program_ids: HashMap<Pubkey, u64>) {
let mut program_ids_array: Vec<_> = program_ids.into_iter().collect();
program_ids_array.sort_by(|a, b| b.1.cmp(&a.1));
for (program_id, count) in program_ids_array.iter() {
println!("{:<44}: {}", program_id.to_string(), count);
⋮----
pub struct AccountsOutputStreamer {
⋮----
pub enum AccountsOutputMode {
⋮----
pub struct AccountsOutputConfig {
⋮----
impl AccountsOutputStreamer {
pub fn new(bank: Arc<Bank>, output_format: OutputFormat, config: AccountsOutputConfig) -> Self {
⋮----
total_accounts_stats: total_accounts_stats.clone(),
⋮----
pub fn output(&self) -> std::result::Result<(), String> {
⋮----
let mut serializer = serde_json::Serializer::new(stdout());
⋮----
.serialize_struct("accountInfo", 2)
.map_err(|err| format!("unable to start serialization: {err}"))?;
⋮----
.serialize_field("accounts", &self.account_scanner)
.map_err(|err| format!("unable to serialize accounts scanner: {err}"))?;
⋮----
.serialize_field("summary", &*self.total_accounts_stats.borrow())
.map_err(|err| format!("unable to serialize accounts summary: {err}"))?;
⋮----
.map_err(|err| format!("unable to end serialization: {err}"))?;
println!();
⋮----
println!("\n{:#?}", self.total_accounts_stats.borrow());
⋮----
pub struct TotalAccountsStats {
⋮----
impl TotalAccountsStats {
pub fn accumulate_account(&mut self, account: &AccountSharedData) {
let data_len = account.data().len();
⋮----
if account.executable() {
⋮----
struct AccountsScanner {
⋮----
impl AccountsScanner {
fn should_process_account(&self, account: &AccountSharedData) -> bool {
account.is_loadable()
&& (self.config.include_sysvars || !solana_sdk_ids::sysvar::check_id(account.owner()))
⋮----
fn maybe_output_account<S>(
⋮----
serializer.serialize_element(&cli_account).unwrap();
⋮----
print!("{}", &cli_account);
let account_data = cli_account.keyed_account.account.data.decode();
⋮----
if !data.is_empty() {
println!("{:?}", data.hex_dump());
⋮----
pub fn output<S>(&self, seq_serializer: &mut Option<S>)
⋮----
let mut total_accounts_stats = self.total_accounts_stats.borrow_mut();
⋮----
account_tuple.filter(|(_, account, _)| self.should_process_account(account))
⋮----
total_accounts_stats.accumulate_account(&account);
self.maybe_output_account(seq_serializer, pubkey, &account);
⋮----
self.bank.scan_all_accounts(scan_func, true).unwrap();
⋮----
AccountsOutputMode::Individual(pubkeys) => pubkeys.iter().for_each(|pubkey| {
⋮----
.get_account_modified_slot_with_fixed_root(pubkey)
.filter(|(account, _)| self.should_process_account(account))
⋮----
.get_program_accounts(program_pubkey, &ScanConfig::new(ScanOrder::Sorted))
.unwrap()
⋮----
.filter(|(_, account)| self.should_process_account(account))
.for_each(|(pubkey, account)| {
total_accounts_stats.accumulate_account(account);
self.maybe_output_account(seq_serializer, pubkey, account);
⋮----
fn serialize<S>(&self, serializer: S) -> std::result::Result<S::Ok, S::Error>
⋮----
let mut seq_serializer = Some(serializer.serialize_seq(None)?);
self.output(&mut seq_serializer);
seq_serializer.unwrap().end()
⋮----
pub struct CliAccounts {
⋮----
write!(f, "{account}")?;
let account_data = account.keyed_account.account.data.decode();
⋮----
writeln!(f, "{:?}", data.hex_dump())?;
⋮----
impl QuietDisplay for CliAccounts {}
impl VerboseDisplay for CliAccounts {}

================
File: ledger-tool/src/program.rs
================
struct Account {
⋮----
struct Input {
⋮----
fn load_accounts(path: &Path) -> Result<Input> {
let file = File::open(path).unwrap();
⋮----
info!("Program input:");
info!("program_id: {}", &input.program_id);
info!("accounts {:?}", &input.accounts);
info!("instruction_data {:?}", &input.instruction_data);
info!("----------------------------------------");
Ok(input)
⋮----
fn load_blockstore(ledger_path: &Path, arg_matches: &ArgMatches<'_>) -> Arc<Bank> {
let process_options = parse_process_options(ledger_path, arg_matches);
let genesis_config = open_genesis_config_by(ledger_path, arg_matches);
info!("genesis hash: {}", genesis_config.hash());
let blockstore = open_blockstore(ledger_path, arg_matches, AccessType::ReadOnly);
let LoadAndProcessLedgerOutput { bank_forks, .. } = load_and_process_ledger_or_exit(
⋮----
let bank = bank_forks.read().unwrap().working_bank();
⋮----
pub trait ProgramSubCommand {
⋮----
impl ProgramSubCommand for App<'_, '_> {
fn program_subcommand(self) -> Self {
⋮----
.help(
⋮----
.required(true)
.index(1);
let load_genesis_config_arg = load_genesis_arg();
let snapshot_config_args = snapshot_args();
self.subcommand(
⋮----
.about("Run to test, debug, and analyze on-chain programs.")
.setting(AppSettings::InferSubcommands)
.setting(AppSettings::SubcommandRequiredElseHelp)
.subcommand(
⋮----
.about("generates Control Flow Graph of the program.")
.arg(&program_arg)
⋮----
.about("dumps disassembled code of the program.")
⋮----
.about(
⋮----
.arg(
⋮----
.short("i")
.long("input")
.value_name("FILE / BYTES")
.takes_value(true)
.default_value("0"),
⋮----
.arg(&load_genesis_config_arg)
.args(&snapshot_config_args)
⋮----
.short("e")
.long("mode")
⋮----
.value_name("VALUE")
.possible_values(&["interpreter", "debugger", "jit"])
.default_value("jit"),
⋮----
.help("Limit the number of instructions to execute")
.long("limit")
⋮----
.value_name("COUNT")
.default_value("9223372036854775807"),
⋮----
.help("Port to use for the connection with a remote debugger")
.long("port")
⋮----
.value_name("PORT")
.default_value("9001"),
⋮----
.help("Output instruction trace")
.short("t")
.long("trace")
⋮----
.value_name("FILE"),
⋮----
struct Output {
⋮----
fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
writeln!(f, "Program output:")?;
writeln!(f, "Result: {}", self.result)?;
writeln!(f, "Instruction Count: {}", self.instruction_count)?;
writeln!(f, "Execution time: {} us", self.execution_time.as_micros())?;
⋮----
writeln!(f, "{line}")?;
⋮----
Ok(())
⋮----
impl QuietDisplay for Output {}
impl VerboseDisplay for Output {}
// Replace with std::lazy::Lazy when stabilized.
// https://github.com/rust-lang/rust/issues/74465
struct LazyAnalysis<'a, 'b> {
⋮----
fn new(executable: &'a Executable<InvokeContext<'b, 'b>>) -> Self {
⋮----
fn analyze(&mut self) -> &Analysis<'_> {
⋮----
.insert(Analysis::from_executable(self.executable).unwrap())
⋮----
fn load_program<'a>(
⋮----
let mut file = File::open(filename).unwrap();
⋮----
file.read_exact(&mut magic).unwrap();
file.rewind().unwrap();
⋮----
file.read_to_end(&mut contents).unwrap();
⋮----
let log_collector = invoke_context.get_log_collector();
⋮----
program_id: program_id.to_string(),
⋮----
let account_size = contents.len();
let program_runtime_environment = create_program_runtime_environment_v1(
invoke_context.get_feature_set(),
invoke_context.get_compute_budget(),
⋮----
.unwrap();
⋮----
let result = load_program_from_bytes(
⋮----
ProgramCacheEntryType::Loaded(program) => Ok(program),
_ => unreachable!(),
⋮----
Err(err) => Err(format!("Loading executable failed: {err:?}")),
⋮----
std::str::from_utf8(contents.as_slice()).unwrap(),
⋮----
.map_err(|err| format!("Assembling executable failed: {err:?}"))
.and_then(|executable| {
⋮----
.map_err(|err| format!("Verifying executable failed: {err:?}"))?;
Ok(executable)
⋮----
verified_executable.jit_compile().unwrap();
⋮----
enum Action {
⋮----
fn process_static_action(action: Action, matches: &ArgMatches<'_>) {
⋮----
with_mock_invoke_context!(invoke_context, transaction_context, transaction_accounts);
let program = matches.value_of("PROGRAM").unwrap();
let verified_executable = load_program(Path::new(program), program_id, &invoke_context);
⋮----
let mut file = File::create("cfg.dot").unwrap();
⋮----
.analyze()
.visualize_graphically(&mut file, None)
⋮----
analysis.analyze().disassemble(&mut stdout.lock()).unwrap();
⋮----
pub fn program(ledger_path: &Path, matches: &ArgMatches<'_>) {
let matches = match matches.subcommand() {
⋮----
process_static_action(Action::Cfg, arg_matches);
⋮----
process_static_action(Action::Dis, arg_matches);
⋮----
let ledger_path = canonicalize_ledger_path(ledger_path);
let bank = load_blockstore(&ledger_path, matches);
⋮----
let mut cached_account_keys = vec![];
let instruction_data = match matches.value_of("input").unwrap().parse::<usize>() {
⋮----
transaction_accounts.push((
⋮----
instruction_accounts.push(InstructionAccount::new(0, false, true));
vec![]
⋮----
let input = load_accounts(Path::new(matches.value_of("input").unwrap())).unwrap();
program_id = input.program_id.parse::<Pubkey>().unwrap_or_else(|err| {
eprintln!(
⋮----
HashMap::<Pubkey, usize>::with_capacity(input.accounts.len());
⋮----
.into_iter()
.map(|account_info| {
let pubkey = account_info.key.parse::<Pubkey>().unwrap_or_else(|err| {
eprintln!("Invalid key in input {}, error {}", account_info.key, err);
exit(1);
⋮----
let data = account_info.data.unwrap_or_default();
let space = data.len();
let account = if let Some(account) = bank.get_account_with_fixed_root(&pubkey) {
let owner = *account.owner();
⋮----
}) = account.state()
⋮----
debug!("Program data address {programdata_address}");
⋮----
.get_account_with_fixed_root(&programdata_address)
.is_some()
⋮----
cached_account_keys.push(pubkey);
⋮----
let lamports = account_info.lamports.unwrap_or(account.lamports());
⋮----
account.set_data_from_slice(&data);
⋮----
.unwrap_or(Pubkey::new_unique().to_string());
let owner = owner.parse::<Pubkey>().unwrap_or_else(|err| {
eprintln!("Invalid owner key in input {owner}, error {err}");
⋮----
let lamports = account_info.lamports.unwrap_or(0);
⋮----
let txn_acct_index = if let Some(idx) = txn_acct_indices.get(&pubkey) {
⋮----
let idx = transaction_accounts.len();
txn_acct_indices.insert(pubkey, idx);
transaction_accounts.push((pubkey, account));
⋮----
account_info.is_signer.unwrap_or(false),
account_info.is_writable.unwrap_or(false),
⋮----
.collect();
⋮----
let program_index: u16 = instruction_accounts.len().try_into().unwrap();
⋮----
create_account_shared_data_for_test(bank.epoch_schedule()),
⋮----
let interpreted = matches.value_of("mode").unwrap() != "jit";
⋮----
.get_feature_set()
⋮----
ProgramCacheForTxBatch::new(bank.slot() + DELAY_VISIBILITY_SLOT_OFFSET);
⋮----
program_cache_for_tx_batch.replenish(
⋮----
bank.load_program(&key, false, bank.epoch())
.expect("Couldn't find program account"),
⋮----
debug!("Loaded program {key}");
⋮----
.configure_next_instruction_for_tests(
program_index.saturating_add(1),
⋮----
invoke_context.push().unwrap();
⋮----
serialize_parameters(
⋮----
.get_current_instruction_context()
.unwrap(),
⋮----
create_vm!(
⋮----
let (mut vm, _, _) = vm.unwrap();
⋮----
if matches.value_of("mode").unwrap() == "debugger" {
vm.debug_port = Some(matches.value_of("port").unwrap().parse::<u16>().unwrap());
⋮----
let (instruction_count, result) = vm.execute_program(&verified_executable, interpreted);
⋮----
if let Some(trace_option) = matches.value_of("trace") {
vm.context_object_pointer.iterate_vm_traces(
⋮----
writeln!(
⋮----
.disassemble_register_trace(&mut std::io::stdout(), register_trace)
⋮----
let filename = format!(
⋮----
let mut fd = File::create(filename).unwrap();
⋮----
.disassemble_register_trace(&mut fd, register_trace)
⋮----
drop(vm);
⋮----
result: format!("{result:?}"),
⋮----
.get_log_collector()
.unwrap()
.borrow()
.get_recorded_content()
.to_vec(),
⋮----
println!("{}", output_format.formatted_string(&output));

================
File: ledger-tool/tests/basic.rs
================
fn run_ledger_tool(args: &[&str]) -> Output {
Command::cargo_bin(env!("CARGO_PKG_NAME"))
.unwrap()
.args(args)
.output()
⋮----
fn bad_arguments() {
assert!(!run_ledger_tool(&[]).status.success());
assert!(!run_ledger_tool(&["-l", "invalid_ledger", "verify"])
⋮----
fn nominal_test_helper(ledger_path: &str) {
let output = run_ledger_tool(&["-l", ledger_path, "verify"]);
assert!(output.status.success());
let output = run_ledger_tool(&["-l", ledger_path, "print", "-vv"]);
⋮----
fn nominal_default() {
let genesis_config = create_genesis_config(100).genesis_config;
let (ledger_path, _blockhash) = create_new_tmp_ledger_auto_delete!(&genesis_config);
nominal_test_helper(ledger_path.path().to_str().unwrap());
⋮----
fn insert_test_shreds(ledger_path: &Path, ending_slot: u64) {
let blockstore = Blockstore::open(ledger_path).unwrap();
⋮----
blockstore.insert_shreds(shreds, None, false).unwrap();
⋮----
fn ledger_tool_copy_test() {
⋮----
insert_test_shreds(ledger_path.path(), LEDGER_TOOL_COPY_TEST_ENDING_SLOT);
let ledger_path = ledger_path.path().to_str().unwrap();
let target_ledger_path = get_tmp_ledger_path_auto_delete!();
let target_ledger_path = target_ledger_path.path().to_str().unwrap();
let output = run_ledger_tool(&[
⋮----
&(LEDGER_TOOL_COPY_TEST_ENDING_SLOT).to_string(),
⋮----
let src_slot_output = run_ledger_tool(&["-l", ledger_path, "slot", &slot_id.to_string()]);
⋮----
run_ledger_tool(&["-l", target_ledger_path, "slot", &slot_id.to_string()]);
assert!(src_slot_output.status.success());
assert!(dst_slot_output.status.success());
assert!(!src_slot_output.stdout.is_empty());

================
File: ledger-tool/.gitignore
================
/target/
/farf/

================
File: ledger-tool/Cargo.toml
================
[package]
name = "agave-ledger-tool"
documentation = "https://docs.rs/agave-ledger-tool"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
workspace = "../dev-bins"

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = []
dummy-for-ci-check = []
frozen-abi = []

[dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
agave-reserved-account-keys = { workspace = true }
agave-snapshots = { workspace = true }
agave-syscalls = { workspace = true }
bs58 = { workspace = true }
chrono = { workspace = true, features = ["default"] }
clap = { workspace = true }
crossbeam-channel = { workspace = true }
csv = { workspace = true }
dashmap = { workspace = true }
futures = { workspace = true }
histogram = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
num_cpus = { workspace = true }
pretty-hex = { workspace = true }
rayon = { workspace = true }
regex = { workspace = true }
serde = { workspace = true }
serde_bytes = { workspace = true }
serde_json = { workspace = true }
solana-account  = { workspace = true }
solana-account-decoder = { workspace = true }
solana-accounts-db = { workspace = true }
solana-bpf-loader-program = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-output = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-compute-budget = { workspace = true }
solana-core = { workspace = true, features = ["dev-context-only-utils"] }
solana-cost-model = { workspace = true }
solana-entry = { workspace = true }
solana-feature-gate-interface = { workspace = true }
solana-genesis-config = { workspace = true }
solana-genesis-utils = { workspace = true }
solana-geyser-plugin-manager = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-inflation = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true, features = ["dev-context-only-utils", "agave-unstable-api"] }
solana-loader-v3-interface = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-program-runtime = { workspace = true, features = ["metrics"] }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-rpc = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime-transaction = { workspace = true }
solana-sbpf = { workspace = true, features = ["debugger", "jit"] }
solana-sdk-ids = { workspace = true }
solana-shred-version = { workspace = true }
solana-signature = { workspace = true }
solana-stake-interface = { workspace = true }
solana-storage-bigtable = { workspace = true }
solana-svm-callback = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-status = { workspace = true }
solana-unified-scheduler-pool = { workspace = true }
solana-version = { workspace = true }
solana-vote = { workspace = true }
solana-vote-program = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dependencies]
jemallocator = { workspace = true }

[target."cfg(unix)".dependencies]
signal-hook = { workspace = true }

[dev-dependencies]
assert_cmd = { workspace = true }

================
File: legal.md
================
<u>**Terms of Use**</u>

Last Updated: October 9, 2025

1. **Introduction**

These Terms of Use provide the terms and conditions under which you, whether personally or on behalf of an entity (“you” or “your”), are permitted to use, interact with or otherwise access the website at www.bam.dev (the “Site”) and attendant tools or services (together with the Site, the “Services”) for the Block Assembly Marketplace (“BAM”), including those available at the GitHub repo [here](https://github.com/jito-foundation/jito-solana).  These Terms of Use, together with any documents and additional terms or policies that are appended hereto or that expressly incorporate these Terms of Use by reference as well as our Privacy Policy (collectively, the “Terms”), constitute a binding agreement between you and Jito Labs, Inc. (“Jito”, “we,” “us,” or “our”).  As referenced in these terms, “user” means someone who views, accesses or otherwise interacts with the Services.

By accessing any Services, you agree to these Terms as governing your use of the Services. These Terms apply to use of the Services, regardless of where or how they are accessed (including outside of or other than via the Site), and regardless of which device is used for access.

<u>NOTICE</u>: PLEASE REVIEW THE TERMS CAREFULLY.  BY ACCESSING, INTERACTING WITH OR USING ANY OF THE SERVICES, YOU AGREE THAT YOU ARE ABLE TO ENTER INTO A BINDING AGREEMENT AND, AS SUCH, HAVE READ, UNDERSTOOD, AND AGREE TO BE BOUND BY THE TERMS, INCLUDING THE BINDING, INDIVIDUAL ARBITRATION AGREEMENT AND CLASS ACTION WAIVER BELOW (SEE SECTION 8).  IF YOU DO NOT AGREE TO ALL OF THE TERMS, YOU ARE NOT AUTHORIZED TO INTERACT WITH, ACCESS OR USE ANY OF THE SERVICES.

2. **The Services & Third Party Technology**

a. <u>The Services</u>

The Site provides information about BAM, a network that allows for block building on the Solana blockchain network (“Solana”).  Your use, review or access of the Site does not authorize you to become a BAM Node Operator (as defined below), but does allow you to request to participate in BAM, as a node operator.   

The BAM validator client (available [here](https://github.com/jito-foundation/jito-solana)) (‘BAM Client”) is open source code that allows validator nodes to connect to BAM, if they so choose.  Your viewing, downloading, accessing or otherwise using the BAM Client is subject to these Terms, but also to the terms of use available on GitHub.  You acknowledge and agree that you are aware that 

As set out further below, nothing on this Site or with regards to the Services guarantees the safety, security, inclusion, or landing of any transaction on Solana via BAM.  BAM does not guarantee the elimination of negative implications of ordering of transactions by third parties, but it may reduce the implications of such conduct by third parties. 

The features of BAM apply only to its component parts and not to any other software that we developed, hosted or currently host, and not to any software that may work within or alongside BAM by third parties.

BAM includes BAM Node Operators (*i.e.*, the hardware layer running encrypted or trusted execution environments), the transaction execution layer of BAM Validators, and the software layer of Plugins that can be built in. 

The BAM Client does not give us access to your validator or node and you agree and acknowledge that we cannot control, provide guarantees for, or access your independent validator or node via the BAM Client or otherwise.  

We may be a BAM Node Operator at any time or at certain points in time, and independent third parties may also be BAM Node Operators at certain points in time.  We are not liable for the actions, activities or communications of any third party and/or independent BAM Node Operator and you expressly disclaim any liability against us and agree to indemnify us for actions arising out of the work of any third party BAM Node Operator. 

If you build a Plugin to work within BAM, there will be separate terms that apply to your work or use of BAM in that capacity, some of which may be available on GitHub.

For the avoidance of doubt, these Terms apply only to this Site and the forms submitted through it. Any information submitted to us via forms on the Site is subject to the Privacy Policy.

b. <u>Third Party Technology</u>

*Solana.*  As noted above, BAM is technology that allows for transparent, efficient, private and verifiable block building on Solana.  Solana is an open, decentralized and permissionless blockchain.  We do not have any control, management or administrative capabilities over Solana and are not responsible for your transactions thereon.  For the avoidance of doubt, Solana is not part of the Site or the Services made available by us under these Terms or otherwise.  We have no ability to monitor or control any use of Solana by you or any third party and expressly disclaim any liability for losses or damages arising from or relating to interaction with, or actions taken on, Solana through BAM or otherwise.  We make no representations or warranties about Solana’s functionality, and expressly disclaim any liability relating to any malfunction or failure of the network. 

*Your Wallet.* You may interact with Solana and third party applications thereon via your self-custodial, self-hosted wallet (“Wallet”).  You acknowledge and agree that we cannot control, provide guarantees for, or access your Wallet or its private keys and that you are solely responsible for your Wallet security.  You should consult the terms of service provided by your Wallet provider to understand your rights and responsibilities as they relate to your Wallet.   This also means that we are unable to assist with transactions: please be vigilant in interacting with Solana or any other immutable blockchain technology.

*Your Cryptoassets.*  Your transactions on Solana or applications thereon necessarily will involve cryptoassets that you hold in your Wallet or otherwise are in possession, custody or control of.  Nothing in BAM allows us, at any time, to take custody of your cryptoassets or otherwise interfere with any transaction or bundle you send to the BAM Network.  You are solely responsible for transactions involving your own cryptoassets and we expressly disclaim any liability for any losses, impairments or other changes – in value or otherwise – with your cryptoassets.

3. **Modifications**

We reserve the right, in our sole discretion, to modify the Terms at any time or from time to time.  The modified Terms will be posted on the Site, and will provide the last updated date at the top of the Terms. Any modified Terms will become effective upon posting.  By continuing to access, use or otherwise interact with any Services after the effective date of any modification to the Terms, you are providing your explicit agreement to be bound by the Terms as modified.  If you do not agree to be bound by any updated Terms, you are prohibited from using, accessing, or otherwise interacting with the Services. It is your responsibility to check the Site you use regularly for modifications to the Terms.

We also reserve the right, in our sole discretion, to modify the Services at any time and from time to time, with or without notice to you.  We may also eliminate any Service in full or in part, at our sole discretion, with or without notice, including deleting or otherwise materially modifying information.

4. **Your Responsibilities and Representations**

a. <u>Your Representations</u>

*General Disclaimer of Liability.*  You acknowledge and agree that:

* you – and only you – are responsible for properly configuring and using the Services and for taking appropriate action to secure your data when doing so;

* you have the financial and technical sophistication to properly use, access and interact with the Services and that you understand the inherent risks of blockchain technology, tokens and smart contracts, among other related technologies;

* you are using and accessing the Services on your own initiative and are responsible for compliance with all laws, including your local laws;

* all use and interaction of the Services is entirely at your own risk; and

* the Services may not be available without interruption or for any particular duration, and we shall not be responsible for any losses, damages, costs, expenses, lost opportunities or other harm suffered by you in connection with any interruption or termination of the Services.

*Of Age and Legal Authority.*  The Services are intended only for users who are 18 years of age or older.  If you are entering into the Terms on behalf of an entity, such as the company you work for, you represent to us that you have the legal authority to bind such an entity.  If you do not meet these requirements you are prohibited from accessing, using or otherwise interacting with the Services.

*Sanctions.*  You represent and warrant that you are not, and for the duration of time you use the Services will not be, (i) the subject of economic or trade sanctions administered or enforced by any governmental authority or otherwise designated on any list of prohibited or restricted parties; (ii) in contravention of any laws and regulations pertaining to anti-money laundering or terrorist financing; (iii) included on the List of Specially Designated Nationals and Blocked Persons maintained by the US Treasury Department’s Office of Foreign Assets Control (OFAC) or on any list pursuant to European Union (EU) and/or United Kingdom (UK) regulations; or (iv) operationally based on domiciled in a country or territory in which sanctions imposed by the United Nations (whether through the Security Council or otherwise), OFAC, the EU and/or the UK apply, or otherwise pursuant to sanctions imposed by the U.N., OFAC, EU or the UK.  If at any point the above is no longer true, then you must immediately cease using the Services.

*Wallet Configuration.*  You represent and warrant that you – and only you – are responsible for properly configuring, as applicable, and using the Services and for taking appropriate action to secure your data, including without limitation, financial or token information and private keys.  With respect to your Wallet (which is provided by a third party), you are solely responsible for familiarizing yourself with it and its safety and security features, including any private keys and passwords associated therewith.  We will not and cannot access your private key, password, or any cryptoassets held within your Wallet nor can it reverse any transactions you initiate with your Wallet (or otherwise). We cannot be responsible or liable in any way for how you use your Wallet.  
*Sophistication for the Services.* You acknowledge that you (A) are responsible for properly configuring and using the Services and for taking appropriate action to secure your data or the data of any third party for which you’re responsible when doing so, (B) have the technical and financial know-how, experience and sophistication to properly use, access and interact with the Services, (C) understand the inherent risks of blockchain technology, cryptoassets/tokens and smart contracts, among other related technologies, and (D) are facile, adept and able to use and configure the sophisticated hardware and software required for use, access and interaction with the Services.

*Sophistication Regarding Blockchain Technology.*  You further acknowledge that (I) blockchains and their attendant technologies involved in using any Services are novel, technologically complex, and involve inherent risk, (II) protocol upgrades and other technological mechanisms may contain bugs or security vulnerabilities that may result in loss of functionality and ultimately of funds, and (III) the Services involve technological innovations such that the results of usage of or interaction with them depend on factors beyond our control, including but not limited to network health, congestion, latency, incentives, user configuration settings, operation of third party software or hardware, wallet or account compatibility, protocol upgrades or forks, the activities of searchers, block builders and others.  You should also familiarize yourself with the risks associated with transacting on blockchain networks, including but not limited to smart contract vulnerabilities, front end vulnerabilities, hacks, phishing attacks, social engineering attacks, cryptoasset volatility and transaction irreversibility.  You understand that like any other software, the App, the Bot, and the Services could be at risk of third-party malware, hacks or cybersecurity breaches. You agree that it is your responsibility to monitor your assets and Wallet regularly and confirm their proper use and deployment consistent with your intentions. Ostium does not and cannot guarantee the security, performance, or reliability of the Protocol, or any associated blockchain networks, protocols or tools.  As part of your knowledge relating to the use of blockchain technology, you acknowledge and agree that you are solely responsible for any fees relating to transactions on any blockchain as it relates to the Protocol, including any transaction (or “gas”) fees relating to any blockchain.

*Applicable Law.*  Your access to the Services is not (i) prohibited by and does not otherwise violate or assist you to violate any domestic or foreign law, rule, statute, regulation, by-law, order, protocol, code, decree, or another directive, requirement, or guideline, published or in force that applies to or is otherwise intended to govern or regulate any person, property, transaction, activity, event or other matter, including any rule, order, judgment, directive or other requirement or guideline issued by any domestic or foreign federal, provincial or state, municipal, local or other governmental, regulatory, judicial or administrative authority having jurisdiction over us, you, the Services, or as otherwise duly enacted, enforceable by law, the common law or equity (collectively, “Applicable Laws”); or (ii) contribute to or facilitate any illegal activity.  You represent and warrant that you will comply with all Applicable Laws, and you will not use the Services if the laws of your country, or any Applicable Law, prohibit you from doing so.  ​​

*Financial Risks.*  Use of the Services may carry financial risk. You acknowledge and understand that blockchains and the applications built thereon are inherently risky and highly experimental by their nature and could result in the loss of the full amount supplied. Transactions entered into in connection with Solana are irreversible and final. You acknowledge and agree that you will access and use the Services at your own risk. The risk of loss in transacting in cryptoassets can be substantial. 

b. <u>Your Responsibilities & Prohibited Conduct</u>

You agree to access, use or otherwise interact with the Services only in an authorized, proper and appropriate manner and in accordance with these Terms and with all applicable laws.

You agree that you will not:

* violate any applicable laws or regulations through your access to or use of the Services;

* violate the Terms;

* exploit the Services for any unauthorized purpose;

* harvest or otherwise collect information from the Services for any unauthorized purpose;

* use the Services in any manner that could disable, damage or impair them or otherwise interfere with them in any way;

* sublicense, sell, or otherwise distribute the Services, or any portion thereof;

* use any data mining tools, robots, crawlers, or similar data gathering and extraction tools to scrape or otherwise remove data from the Services;

* use any manual process to monitor or copy any of the material on the Site or that is included in the Services or for any unauthorized purpose without our prior written consent;

* introduce any viruses, trojan horses, worms, logic bombs, or other material which is malicious or technologically harmful to the Services;

* attempt to gain unauthorized access to, interfere with, damage, or disrupt any parts of the Services; or

* attack the Services via a denial-of-service attack or a distributed denial-of-service attack or otherwise attempt to interfere with the proper working of the Services.

You acknowledge and agree that (i) we have the right to investigate violations of these Terms or conduct that affects the Services; (ii) in the event that you use a Service in a prohibited manner or for any other reason we determine is appropriate, in our sole discretion, we may investigate or take any other action we deem necessary, including but not limited to bringing legal action against you if your use or access of the Services results in harm or damage to us, to rectify the prohibited conduct or any consequences resulting therefrom; and (iii) we may consult and cooperate with law enforcement authorities where and when we deem appropriate or necessary, in our sole discretion.  

We are not obligated to monitor access to or use of the Services or to review or edit any content, although we reserve the right to do so in our sole discretion. We reserve the right, but are not obligated, to remove or disable access to any content, at any time and without notice, including, but not limited to, if we, at our sole discretion, consider it objectionable or in violation of these Terms. 

You also acknowledge and agree that we may terminate or suspend your access to all or part of the Services for any or no reason, including without limitation, any violation of these Terms.

c. <u>Your Feedback</u>

We appreciate feedback, comments, ideas, proposals, and suggestions for improvements to the Services as well as any information on forms you submit through the Site (collectively, “Feedback”). If you choose to submit Feedback, you agree that we are free to use it without any restriction or compensation to you, and you grant us and our affiliates and service providers, and each of their and our respective licensees, successors, and assigns the right to use, reproduce, modify, perform, display, distribute, and otherwise disclose to third parties any such material for any purpose. 

You acknowledge and agree that we will own all right, title, and interest in and to all Feedback you submit.  You represent and warrant that (i) you own all right, title, and interest in and to your Feedback; and (ii) you will not violate any intellectual property or other rights of third parties in providing Feedback to us. 

5. **Intellectual Property Rights**

a. <u>Ownership & License</u>

The Services and their entire contents, features, and functionality (including but not limited to all information, software, text, displays, images, video, and audio, and the design, selection, and arrangement thereof), other than third-party content, are owned by Jito, its licensors, or other providers of such material and are protected by United States and international copyright, trademark, patent, trade secret, and other intellectual property or proprietary rights laws.

Subject to these Terms, we hereby grant you a personal, limited, revocable, non-exclusive, non-sublicensable, non-transferable license in connection with the Services.  This license is solely intended to allow you to view, use or otherwise interact with the Services.

You acknowledge and agree that you do not receive any other rights to the Services other than those specified in the Terms.  Certain tools or services may be provided to you under a separate license; third-party features or applications integrated into the Services may be subject to other or additional intellectual property licenses and thus, you must review any terms relevant to those third party features or applications to determine the relevant licenses applicable thereto.  You agree you will not violate the terms of any such separate license.

b. <u>Reciprocal License</u>

By using the Services, you grant us a limited, non-exclusive, sublicensable, worldwide royalty free license to use, copy, modify and display any content or Feedback you provide to us or that you post on or through any of the Services solely for our business purposes, including but not limited to the purpose of providing the Services for so long as is necessary to do so.

6. **Third Party Information or Services**

The Services may be integrated with or otherwise give access to applications, services, sites, tools, technology, data, operations, features or resources that are provided or otherwise made available by third parties (“Third Party Services”).  

If the Services contain links to such Third Party Services, they are provided for your convenience only. We have no control over the content of those sites or resources, and accept no responsibility for them or for any loss or damage that may arise from your use of them.  If you decide to access a Third Party Service integrated with or linked to the Site or any Service, you do so entirely at your own risk and subject to the terms and conditions of use, privacy policies, or other agreements with those third parties that are applicable to those Third Party Services, which we do not control and otherwise may have no relationship with.  Please review any applicable terms, policies or agreements of Third Party Services prior to engaging with them.  We reserve the right to withdraw linking permission without notice.  We have no control over and are not responsible for such Third Party Services, including the accuracy, availability, reliability, verification or completeness of information or content shared by or available through Third Party Services, or the privacy practices of such services.

Your use of Third Party Services is directly between you and that third party, and you acknowledge and agree that we are not responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with your use of or reliance on any Third Party Service.

Separately, you may link to the Site, provided you do so in a way that is fair and legal and does not damage our reputation or take advantage of it, but you must not establish a link in such a way as to suggest any form of association, approval, or endorsement on our part without our express written consent.

7. **Indemnification**

a. <u>General</u>

You agree to defend, indemnify and hold harmless us and our licensors, and each of our and their respective employees, officers, directors, agents and representatives (collectively, the “Jito Parties”) from and against all liability for monetary damages, contractual claims of any nature, economic loss (including direct, incidental or consequential damages), loss of income or profits, fines, penalties, exemplary or punitive damages, and any other injury, damage, or harm, including reasonable attorney’s fees (collectively, “Damages”) that relate in any way to any demand, claim, regulatory action, proceeding or lawsuit, regardless of the cause or alleged cause, whether the allegations are groundless, fraudulent, false or lack merit and regardless of the theory of recovery (“Claims” and each, a “Claim”) arising out of or relating to: (i) your access to or use of the Services; (ii) violation or breach of the Terms or violation of applicable law by you, your customers, users, employees, agents and other associated persons; (iii) a dispute between you and any third party; (iv) your alleged or actual infringement or misappropriation of any third party’s intellectual property or other rights; and (v) your Feedback.  In the event we receive a third party subpoena or other compulsory legal order or process associated with Claims described in (i) through (v) above, then, in addition to the indemnification set forth above, you will reimburse us for the time, effort and expenditures we expended responding to such matters at our then-current hourly rates as well as our reasonable attorneys’ fees.

   b. <u>Process</u>

If you are obligated to indemnify us, then you agree that we will have the right in our sole discretion, to control any action or proceeding and to determine whether we wish to settle and if so, on what terms, and you agree to fully cooperate with us in the defense or settlement of such Claim.

8. **Disclaimers and Limitations of Liability**

a. <u>Services</u>

By accessing the Services, you hereby acknowledge and agree that we cannot and do not guarantee the functionality, security or availability of the Services.  The technologies on which the Services rely may be subject to sudden changes and we cannot and do not guarantee that your access to or use of the Services will be uninterrupted or error free.  You assume all risks related thereto.  We make no claims that the Services or any of its content is accessible or appropriate in your country. 

b. <u>No Representations or Warranties</u>

THE SERVICES ARE PROVIDED “AS IS,” EXCEPT TO THE EXTENT PROHIBITED BY LAW, OR TO THE EXTENT ANY STATUTORY RIGHTS APPLY THAT CANNOT BE EXCLUDED, LIMITED OR WAIVED, NEITHER WE NOR ANY OTHER JITO PARTY MAKES ANY REPRESENTATION OR WARRANTIES OF ANY KIND, WHETHER EXPRESS, IMPLIED, STATUTORY OR OTHERWISE REGARDING THE SERVICES, AND THE JITO PARTIES EXPRESSLY DISCLAIM ALL WARRANTIES, INCLUDING ANY IMPLIED OR EXPRESS WARRANTIES (i) OF MERCHANTABILITY, SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, OR QUIET ENJOYMENT; (ii) ARISING OUT OF ANY COURSE OF DEALING OR USAGE OR TRADE; (iii) THAT THE SERVICES WILL BE ACCURATE, UNINTERRUPTED, ERROR FREE OR FREE OF HARMFUL COMPONENTS; AND (iv) THAT ANY CONTENT OR FEEDBACK WILL BE SECURE OR NOT OTHERWISE LOST OR ALTERED.

c. <u>Limitations of Liability</u>

THE JITO PARTIES WILL NOT BE LIABLE TO YOU FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR EXEMPLARY DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, REVENUES, CUSTOMERS OR USERS, OPPORTUNITIES, GOODWILL, USE, DATA, CONTENT OR OTHER ASSETS), EVEN IF ANY OF THE JITO PARTIES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.  FURTHER, NONE OF THE JITO PARTIES WILL BE RESPONSIBLE FOR ANY COMPENSATION, REIMBURSEMENT, OR DAMAGES ARISING IN CONNECTION WITH (i) YOUR INABILITY TO USE, OR ANY DELAY IN THE USE OF, THE SERVICES, INCLUDING AS A RESULT OF ANY (A) TERMINATION OF THE TERMS OR YOUR USE OF OR ACCESS TO THE SERVICES, (B) OUR SUSPENSION OR DISCONTINUATION OF ANY OF THE SERVICES, OR (C) ANY UNANTICIPATED OR UNSCHEDULED DOWNTIME OF ALL OR A PORTION OF THE SERVICES FOR ANY REASON; (ii) COST OF PROCUREMENT OF SUBSTITUTE SERVICES; (iii) INVESTMENTS, EXPENDITURES, OR COMMITEMENTS BY YOU IN CONNECTION WITH THE TERMS OR YOUR USE OF OR ACCESS TO THE SERVICES; (iv) UNAUTHORIZED ACCESS TO, ALTERATION OF, OR THE DELETION, DESTRUCTION, DAMAGE, LOSS OR FAILURE TO STORE ANY OF YOUR DATA; OR (v) CHANGE IN VALUE OF ANY CRYPTOASSET. IN ANY CASE, THE JITO LABS PARTIES’ AGGREGATE LIABILITY UNDER THESE TERMS WILL NOT EXCEED $100. THE LIMITATIONS IN THIS SECTION APPLY ONLY TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW.

9. **Governing Law, Dispute Resolution & Class Action Waiver**

a. <u>Governing Law</u>

These Terms, and your use of the Services, are governed by the laws of the State of Delaware, without regard to conflict of laws rules. The exclusive jurisdiction for all Claims, to the extent applicable given the mandatory arbitration provisions in the Terms, will be the courts located in the state of Delaware, and you acknowledge and agree that any Claims are subject to the jurisdiction of such courts, and waive any objection to the laying of such venue.

b. <u>Dispute Resolution</u>

Prior to commencing any legal proceeding of any kind against us, including an arbitration, you and we agree that we will attempt to resolve any Claim by engaging in good faith negotiations.  Such negotiation requires that the aggrieved party provide written notice to the other party specifying the nature and details of the dispute (the “Initial Notice”). The party receiving such notice shall have 20 days to respond, and within 45 days after the Initial Notice was sent, the parties shall meet and confer in good faith to try and resolve the Claim. If the parties are unable to do so within 90 days of the Initial Notice, the parties may agree to mediate their dispute or either party may submit to arbitration according to these Terms.

c. <u>Mandatory Arbitration</u>

Any Claim arising out of or relating to these Terms or the Services, or the breach, termination, enforcement, interpretation or validity of the Terms, including the determination of the scope or applicability of this agreement to arbitrate, will be determined by arbitration in Delaware before a single arbitrator.  This Section will not preclude parties from seeking provisional remedies in aid of arbitration from a court of appropriate jurisdiction.

Any arbitration will be conducted by JAMS pursuant to its rules then in effect, except as modified by these Terms. Disputes involving claims and counterclaims under $250,000, not inclusive of attorneys’ fees and interest, shall be subject to JAMS’s most current version of the Streamlined Arbitration Rules. All other claims shall be subject to JAMS’s most current version of the Comprehensive Arbitration Rules and Procedures (collectively, the “JAMS Rules”). 

YOU UNDERSTAND THAT BY AGREEING TO THE TERMS, THE PARTIES ARE EACH WAIVING THE RIGHT TO TRIAL BY JURY OR TO PARTICIPATE IN A CLASS ACTION OR CLASS ARBITRATION.

d. <u>Class Action Waiver</u>

Any arbitration under the Terms shall take place on an individual basis – class arbitrations and class actions are not permitted.

To the fullest extent permitted by applicable law, you agree that any proceedings to resolve any Claim will be brought and conducted only in your individual capacity and not as a party (plaintiff or otherwise) or member of any class (or purported class), consolidated proceeding, multi-plaintiff proceeding or representative action or proceeding.

Any arbitration will not be permitted to be consolidated or aggregated with any other arbitration and the arbitrator will not have any authority to do so, and will not have the authority to make an award to any person or entity not a part of the individual arbitration in which you are a party. You further agree that any arbitrator may not preside over any form of class action involving you and us.

10. **General Provisions**

a. <u>No Relationship</u>

Nothing in the Terms shall be construed to create any relationship between you and us other than as defined herein.  Neither you nor we are an agent of each other under these Terms or otherwise, and you shall have no right to hold yourself out as in any way having a relationship with us other than as someone using, accessing or otherwise interfacing with the Services.

b. <u>Assignments</u>

You agree you are not permitted to assign or otherwise transfer any of your rights and obligations under the Terms, but Jito may assign or transfer the Terms, in whole or in part, without restriction.  Any assignment or transfer by you in violation of this Section will be void. Subject to the foregoing, the Terms will be binding upon, and inure to the benefit of, the parties and their respective permitted successors and assigns.

c. <u>Entire Agreement</u>

The Terms, including any policies that expressly or impliedly incorporate the Terms by reference, constitute the entire agreement between you and us regarding the subject matter herein.  The Terms supersede all prior or contemporaneous representations, understandings, agreements, or communications between you and us, if any, whether written or verbal, regarding the subject matter of the Terms.

d. <u>No Waiver</u>

The failure by us to enforce any provision of the Terms will not constitute a present or future waiver of such provision nor limit our right to enforce such provision at a later time. All waivers by us must be in writing to be effective.

e. <u>Severability</u>

If any portion of the Terms are held to be unenforceable or invalid, the remainder of the Terms will continue in full force and effect. Any invalid or unenforceable portions will be interpreted to effectuate the intent of the original portion. If such construction is not possible, the invalid or unenforceable portion will be severed from the Terms, but the rest of the Terms will remain in full force and effect.

f. <u>Notice</u>

Any notices or other communications provided by us under these Terms will be given: (i) via email; or (ii) by posting to the Services. For notices made by email, the date of receipt will be deemed the date on which such notice is transmitted.

11. **Contact Us**

If you have any questions about these Terms or the Services, you may contact us at support@bam.dev.

================
File: local-cluster/src/cluster_tests.rs
================
pub fn spend_and_verify_all_nodes<S: ::std::hash::BuildHasher + Sync + Send>(
⋮----
let cluster_nodes = discover_validators(
&entry_point_info.gossip().unwrap(),
⋮----
entry_point_info.shred_version(),
⋮----
.unwrap();
assert_eq!(cluster_nodes.len(), nodes);
⋮----
cluster_nodes.par_iter().for_each(|ingress_node| {
if ignore_nodes.contains(ingress_node.pubkey()) {
⋮----
let client = new_tpu_quic_client(ingress_node, connection_cache.clone()).unwrap();
⋮----
.rpc_client()
.poll_get_balance_with_commitment(
&funding_keypair.pubkey(),
⋮----
.expect("balance in source");
assert!(bal > 0);
⋮----
.get_latest_blockhash_with_commitment(CommitmentConfig::confirmed())
⋮----
system_transaction::transfer(funding_keypair, &random_keypair.pubkey(), 1, blockhash);
⋮----
if ignore_nodes.contains(validator.pubkey()) {
⋮----
.unwrap()
⋮----
pub fn verify_balances<S: ::std::hash::BuildHasher>(
⋮----
let client = new_tpu_quic_client(node, connection_cache.clone()).unwrap();
⋮----
.poll_get_balance_with_commitment(&pk, CommitmentConfig::processed())
⋮----
assert_eq!(bal, b);
⋮----
pub fn send_many_transactions(
⋮----
.get_latest_blockhash_with_commitment(CommitmentConfig::processed())
⋮----
let transfer_amount = rng().random_range(1..max_tokens_per_transfer);
⋮----
&random_keypair.pubkey(),
⋮----
expected_balances.insert(random_keypair.pubkey(), transfer_amount);
⋮----
pub fn verify_ledger_ticks(ledger_path: &Path, ticks_per_slot: usize) {
let ledger = Blockstore::open(ledger_path).unwrap();
⋮----
let zeroth_slot = ledger.get_slot_entries(0, 0).unwrap();
let last_id = zeroth_slot.last().unwrap().hash;
let next_slots = ledger.get_slots_since(&[0]).unwrap().remove(&0).unwrap();
⋮----
.into_iter()
.map(|slot| (slot, 0, last_id))
.collect();
while let Some((slot, parent_slot, last_id)) = pending_slots.pop() {
⋮----
.get_slots_since(&[slot])
⋮----
.remove(&slot)
⋮----
let should_verify_ticks = if !next_slots.is_empty() {
Some((slot - parent_slot) as usize * ticks_per_slot)
⋮----
let last_id = verify_slot_ticks(&ledger, &thread_pool, slot, &last_id, should_verify_ticks);
pending_slots.extend(
⋮----
.map(|child_slot| (child_slot, slot, last_id)),
⋮----
pub fn sleep_n_epochs(
⋮----
let num_ticks_per_second = config.target_tick_duration.as_secs_f64().recip();
⋮----
warn!("sleep_n_epochs: {secs} seconds");
sleep(Duration::from_secs(secs));
⋮----
pub fn kill_entry_and_spend_and_verify_rest(
⋮----
info!("kill_entry_and_spend_and_verify_rest...");
⋮----
assert!(cluster_nodes.len() >= nodes);
let client = new_tpu_quic_client(entry_point_info, connection_cache.clone()).unwrap();
⋮----
.poll_get_balance_with_commitment(ingress_node.pubkey(), CommitmentConfig::processed())
.unwrap_or_else(|err| panic!("Node {} has no balance: {}", ingress_node.pubkey(), err));
⋮----
info!("killing entry point: {}", entry_point_info.pubkey());
entry_point_validator_exit.write().unwrap().exit();
info!("sleeping for some time to let entry point exit and partitions to resolve...");
sleep(Duration::from_millis(slot_millis * MINIMUM_SLOTS_PER_EPOCH));
info!("done sleeping");
⋮----
if ingress_node.pubkey() == entry_point_info.pubkey() {
info!("ingress_node.id == entry_point_info.id, continuing...");
⋮----
assert_ne!(balance, 0);
let mut result = Ok(());
⋮----
result.unwrap();
⋮----
result = Err(err);
⋮----
info!("poll_all_nodes_for_signature()");
match poll_all_nodes_for_signature(
⋮----
info!("poll_all_nodes_for_signature() failed {e:?}");
result = Err(e);
⋮----
info!("poll_all_nodes_for_signature() succeeded, done.");
⋮----
pub fn apply_votes_to_tower(node_keypair: &Keypair, votes: Vec<(Slot, Hash)>, tower_path: PathBuf) {
⋮----
let mut tower = tower_storage.load(&node_keypair.pubkey()).unwrap();
⋮----
tower.record_vote(slot, hash);
⋮----
let saved_tower = SavedTowerVersions::from(SavedTower::new(&tower, node_keypair).unwrap());
tower_storage.store(&saved_tower).unwrap();
⋮----
pub fn check_min_slot_is_rooted(
⋮----
for ingress_node in contact_infos.iter() {
⋮----
.get_slot_with_commitment(CommitmentConfig::finalized())
.unwrap_or(0);
if root_slot >= min_slot || last_print.elapsed().as_secs() > 3 {
info!(
⋮----
sleep(Duration::from_millis(clock::DEFAULT_MS_PER_SLOT / 2));
assert!(loop_start.elapsed() < loop_timeout);
⋮----
pub fn check_for_new_roots(
⋮----
let mut roots = vec![HashSet::new(); contact_infos.len()];
⋮----
for (i, ingress_node) in contact_infos.iter().enumerate() {
⋮----
roots[i].insert(root_slot);
num_roots_map.insert(*ingress_node.pubkey(), roots[i].len());
let num_roots = roots.iter().map(|r| r.len()).min().unwrap();
⋮----
if done || last_print.elapsed().as_secs() > 3 {
⋮----
pub fn check_no_new_roots(
⋮----
assert!(!contact_infos.is_empty());
let mut roots = vec![0; contact_infos.len()];
⋮----
.iter()
.enumerate()
.map(|(i, ingress_node)| {
⋮----
.get_slot()
.unwrap_or_else(|_| panic!("get_slot for {} failed", ingress_node.pubkey()));
⋮----
.get_slot_with_commitment(CommitmentConfig::processed())
.unwrap_or_else(|_| panic!("get_slot for {} failed", ingress_node.pubkey()))
⋮----
.max()
⋮----
let client = new_tpu_quic_client(contact_info, connection_cache.clone()).unwrap();
⋮----
.unwrap_or_else(|_| panic!("get_slot for {} failed", contact_infos[0].pubkey()));
⋮----
if last_print.elapsed().as_secs() > 3 {
⋮----
assert_eq!(
⋮----
fn poll_all_nodes_for_signature(
⋮----
if validator.pubkey() == entry_point_info.pubkey() {
⋮----
let client = new_tpu_quic_client(validator, connection_cache.clone()).unwrap();
LocalCluster::poll_for_processed_transaction(&client, transaction)?.unwrap();
⋮----
Ok(())
⋮----
pub struct GossipVoter {
⋮----
impl GossipVoter {
pub fn close(self) {
self.exit.store(true, Ordering::Relaxed);
self.t_voter.join().unwrap();
self.gossip_service.join().unwrap();
⋮----
pub fn start_gossip_voter(
⋮----
node_keypair.insecure_clone(),
⋮----
exit.clone(),
⋮----
while cluster_info.gossip_peers().len() < num_expected_peers {
sleep(Duration::from_millis(sleep_ms));
⋮----
let exit = exit.clone();
let cluster_info = cluster_info.clone();
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
let (labels, votes) = cluster_info.get_votes_with_labels(&mut cursor);
if labels.is_empty() {
if latest_push_attempt.elapsed() > Duration::from_millis(refresh_ms) {
for (leader_vote_tx, parsed_vote) in refreshable_votes.iter().rev() {
let vote_slot = parsed_vote.last_voted_slot().unwrap();
info!("gossip voter refreshing vote {vote_slot}");
process_vote_tx(vote_slot, leader_vote_tx, parsed_vote, &cluster_info);
⋮----
.zip(votes)
.filter_map(&vote_filter)
⋮----
parsed_vote_iter.sort_by(|(vote, _), (vote2, _)| {
vote.last_voted_slot()
⋮----
.cmp(&vote2.last_voted_slot().unwrap())
⋮----
if let Some(vote_slot) = parsed_vote.last_voted_slot() {
info!("received vote for {vote_slot}");
⋮----
.push_front((leader_vote_tx.clone(), parsed_vote.clone()));
refreshable_votes.truncate(max_votes_to_refresh);
⋮----
fn get_and_verify_slot_entries(
⋮----
let entries = blockstore.get_slot_entries(slot, 0).unwrap();
assert!(entries.verify(last_entry, thread_pool).status());
⋮----
fn verify_slot_ticks(
⋮----
let entries = get_and_verify_slot_entries(blockstore, thread_pool, slot, last_entry);
let num_ticks: usize = entries.iter().map(|entry| entry.is_tick() as usize).sum();
⋮----
assert_eq!(num_ticks, expected_num_ticks);
⋮----
entries.last().unwrap().hash
⋮----
pub fn submit_vote_to_cluster_gossip(
⋮----
let tower_sync = TowerSync::new_from_slots(vec![vote_slot], vote_hash, None);
⋮----
vec![CrdsValue::new(
⋮----
node_keypair.pubkey(),
⋮----
pub fn new_tpu_quic_client(
⋮----
let rpc_pubsub_url = format!("ws://{}/", contact_info.rpc_pubsub().unwrap());
let rpc_url = format!("http://{}", contact_info.rpc().unwrap());
⋮----
ConnectionCache::Udp(_) => panic!("Expected a Quic ConnectionCache. Got UDP"),
⋮----
rpc_pubsub_url.as_str(),
⋮----
cache.clone(),

================
File: local-cluster/src/cluster.rs
================
pub type QuicTpuClient = TpuClient<QuicPool, QuicConnectionManager, QuicConfig>;
pub struct ValidatorInfo {
⋮----
pub struct ClusterValidatorInfo {
⋮----
impl ClusterValidatorInfo {
pub fn new(
⋮----
validator: Some(validator),
⋮----
pub trait Cluster {

================
File: local-cluster/src/integration_tests.rs
================
pub fn last_vote_in_tower(tower_path: &Path, node_pubkey: &Pubkey) -> Option<(Slot, Hash)> {
restore_tower(tower_path, node_pubkey).map(|tower| tower.last_voted_slot_hash().unwrap())
⋮----
pub fn last_root_in_tower(tower_path: &Path, node_pubkey: &Pubkey) -> Option<Slot> {
restore_tower(tower_path, node_pubkey).map(|tower| tower.root())
⋮----
pub fn restore_tower(tower_path: &Path, node_pubkey: &Pubkey) -> Option<Tower> {
let file_tower_storage = FileTowerStorage::new(tower_path.to_path_buf());
⋮----
if tower_err.is_file_missing() {
⋮----
panic!("tower restore failed...: {tower_err:?}");
⋮----
Tower::restore(&file_tower_storage, node_pubkey).ok()
⋮----
pub fn remove_tower_if_exists(tower_path: &Path, node_pubkey: &Pubkey) {
⋮----
let filename = file_tower_storage.filename(node_pubkey);
if filename.exists() {
fs::remove_file(file_tower_storage.filename(node_pubkey)).unwrap();
⋮----
pub fn remove_tower(tower_path: &Path, node_pubkey: &Pubkey) {
⋮----
pub fn open_blockstore(ledger_path: &Path) -> Blockstore {
⋮----
.unwrap_or_else(|_| {
⋮----
.unwrap_or_else(|e| {
panic!("Failed to open ledger at {ledger_path:?}, err: {e}");
⋮----
pub fn purge_slots_with_count(blockstore: &Blockstore, start_slot: Slot, slot_count: Slot) {
blockstore.purge_from_next_slots(start_slot, start_slot + slot_count - 1);
blockstore.purge_slots(start_slot, start_slot + slot_count - 1, PurgeType::Exact);
⋮----
pub fn wait_for_last_vote_in_tower_to_land_in_ledger(
⋮----
last_vote_in_tower(ledger_path, node_pubkey).map(|(last_vote, _)| {
⋮----
let blockstore = open_blockstore(ledger_path);
if blockstore.is_full(last_vote) {
⋮----
sleep(Duration::from_millis(100));
⋮----
pub fn wait_for_duplicate_proof(ledger_path: &Path, dup_slot: Slot) -> Option<DuplicateSlotProof> {
⋮----
let duplicate_fork_validator_blockstore = open_blockstore(ledger_path);
⋮----
duplicate_fork_validator_blockstore.get_first_duplicate_proof()
⋮----
return Some(found_duplicate_proof);
⋮----
sleep(Duration::from_millis(1000));
⋮----
pub fn copy_blocks(end_slot: Slot, source: &Blockstore, dest: &Blockstore, is_trusted: bool) {
for slot in std::iter::once(end_slot).chain(AncestorIterator::new(end_slot, source)) {
let source_meta = source.meta(slot).unwrap().unwrap();
assert!(source_meta.is_full());
let shreds = source.get_data_shreds_for_slot(slot, 0).unwrap();
dest.insert_shreds(shreds, None, is_trusted).unwrap();
let dest_meta = dest.meta(slot).unwrap().unwrap();
assert!(dest_meta.is_full());
assert_eq!(dest_meta.last_index, source_meta.last_index);
⋮----
pub fn ms_for_n_slots(num_blocks: u64, ticks_per_slot: u64) -> u64 {
(ticks_per_slot * DEFAULT_MS_PER_SLOT * num_blocks).div_ceil(DEFAULT_TICKS_PER_SLOT)
⋮----
pub fn run_kill_partition_switch_threshold<C>(
⋮----
info!("stakes_to_kill: {stakes_to_kill:?}, alive_stakes: {alive_stakes:?}");
⋮----
.iter()
.cloned()
.chain(alive_stakes.iter().cloned())
.collect();
let stake_partitions: Vec<usize> = partitions.iter().map(|(stake, _)| *stake).collect();
⋮----
partitions.iter().map(|(_, num_slots)| *num_slots).collect();
⋮----
create_custom_leader_schedule_with_random_keys(&num_slots_per_validator);
info!(
⋮----
let validator_pubkeys: Vec<Pubkey> = validator_keys.iter().map(|k| k.pubkey()).collect();
⋮----
[0..stakes_to_kill.len()]
⋮----
.map(|validator_to_kill| {
info!("Killing validator with id: {validator_to_kill}");
cluster.exit_node(validator_to_kill)
⋮----
on_partition_start(
⋮----
run_cluster_partition(
⋮----
Some((leader_schedule, validator_keys)),
⋮----
vec![],
⋮----
pub fn create_custom_leader_schedule(
⋮----
let mut leader_schedule = vec![];
⋮----
leader_schedule.push(k)
⋮----
info!("leader_schedule: {}", leader_schedule.len());
⋮----
pub fn create_custom_leader_schedule_with_random_keys(
⋮----
.take(validator_num_slots.len())
⋮----
let leader_schedule = create_custom_leader_schedule(
⋮----
.map(|k| k.pubkey())
.zip(validator_num_slots.iter().cloned()),
⋮----
pub fn run_cluster_partition<C>(
⋮----
info!("PARTITION_TEST!");
let num_nodes = partitions.len();
⋮----
.map(|stake_weight| 100 * *stake_weight as u64)
⋮----
assert_eq!(node_stakes.len(), num_nodes);
let mint_lamports = node_stakes.iter().sum::<u64>() * 2;
⋮----
Some(0)
⋮----
turbine_disabled: turbine_disabled.clone(),
⋮----
assert_eq!(validator_keys.len(), num_nodes);
let num_slots_per_rotation = leader_schedule.num_slots() as u64;
⋮----
validator_config.fixed_leader_schedule = Some(fixed_schedule);
⋮----
.take(partitions.len())
.collect(),
⋮----
let mut validator_configs = make_identical_validator_configs(&validator_config, num_nodes);
⋮----
.first_mut()
.unwrap()
⋮----
validator_keys: Some(
⋮----
.into_iter()
.zip(iter::repeat_with(|| true))
⋮----
ticks_per_slot: ticks_per_slot.unwrap_or(DEFAULT_TICKS_PER_SLOT),
⋮----
info!("PARTITION_TEST spend_and_verify_all_nodes(), ensure all nodes are caught up");
⋮----
let cluster_nodes = discover_validators(
&cluster.entry_point_info.gossip().unwrap(),
⋮----
cluster.entry_point_info.shred_version(),
⋮----
.unwrap();
info!("PARTITION_TEST sleeping until partition starting condition",);
⋮----
let node_client = RpcClient::new_socket(node.rpc().unwrap());
let epoch_info = node_client.get_epoch_info().unwrap();
assert_eq!(epoch_info.slots_in_epoch, slots_per_epoch);
⋮----
info!("PARTITION_TEST start partition");
on_partition_start(&mut cluster, &mut context);
turbine_disabled.store(true, Ordering::Relaxed);
sleep(partition_duration);
on_before_partition_resolved(&mut cluster, &mut context);
info!("PARTITION_TEST remove partition");
turbine_disabled.store(false, Ordering::Relaxed);
⋮----
sleep(timeout_duration);
⋮----
sleep(propagation_duration);
info!("PARTITION_TEST resuming normal operation");
on_partition_resolved(&mut cluster, &mut context);
⋮----
pub struct ValidatorTestConfig {
⋮----
pub fn test_faulty_node(
⋮----
let num_nodes = node_stakes.len();
⋮----
.as_ref()
.map(|configs| {
⋮----
.map(|config| (config.validator_keypair.clone(), config.in_genesis))
.collect()
⋮----
.unwrap_or_else(|| {
⋮----
validator_keys.resize_with(num_nodes, || (Arc::new(Keypair::new()), true));
⋮----
let fixed_leader_schedule = custom_leader_schedule.unwrap_or_else(|| {
let validator_to_slots = vec![(
⋮----
let leader_schedule = create_custom_leader_schedule(validator_to_slots.into_iter());
⋮----
.map(|config| config.validator_config)
⋮----
configs.resize_with(num_nodes, ValidatorConfig::default_for_test);
⋮----
config.fixed_leader_schedule = Some(fixed_leader_schedule.clone());
config.wait_for_supermajority = Some(0);
⋮----
validator_keys: Some(validator_keys.clone()),
⋮----
.map(|(keypair, _)| keypair)
⋮----
pub fn farf_dir() -> PathBuf {
⋮----
.unwrap_or_else(|_| "farf".to_string())
.into()
⋮----
pub fn generate_account_paths(num_account_paths: usize) -> (Vec<TempDir>, Vec<PathBuf>) {
⋮----
.map(|_| tempfile::tempdir_in(farf_dir()).unwrap())
⋮----
.map(|a| create_accounts_run_and_snapshot_dirs(a.path()).unwrap().0)
⋮----
pub struct SnapshotValidatorConfig {
⋮----
impl SnapshotValidatorConfig {
pub fn new(
⋮----
assert_ne!(full_snapshot_archive_interval, SnapshotInterval::Disabled);
let _ = fs::create_dir_all(farf_dir());
let bank_snapshots_dir = tempfile::tempdir_in(farf_dir()).unwrap();
let full_snapshot_archives_dir = tempfile::tempdir_in(farf_dir()).unwrap();
let incremental_snapshot_archives_dir = tempfile::tempdir_in(farf_dir()).unwrap();
⋮----
full_snapshot_archives_dir: full_snapshot_archives_dir.path().to_path_buf(),
⋮----
.path()
.to_path_buf(),
bank_snapshots_dir: bank_snapshots_dir.path().to_path_buf(),
maximum_full_snapshot_archives_to_retain: NonZeroUsize::new(usize::MAX).unwrap(),
maximum_incremental_snapshot_archives_to_retain: NonZeroUsize::new(usize::MAX).unwrap(),
⋮----
assert!(is_snapshot_config_valid(&snapshot_config));
⋮----
generate_account_paths(num_account_paths);
⋮----
SnapshotPackagerService::NAME.to_string(),
⋮----
.into(),
⋮----
pub fn setup_snapshot_validator_config(
⋮----
pub fn save_tower(tower_path: &Path, tower: &Tower, node_keypair: &Keypair) {
⋮----
tower.save(&file_tower_storage, node_keypair).unwrap();

================
File: local-cluster/src/lib.rs
================
pub mod cluster;
pub mod cluster_tests;
pub mod integration_tests;
pub mod local_cluster;
mod local_cluster_snapshot_utils;
pub mod validator_configs;

================
File: local-cluster/src/local_cluster_snapshot_utils.rs
================
impl LocalCluster {
pub fn wait_for_next_full_snapshot<T>(
⋮----
match self.wait_for_next_snapshot(
⋮----
_ => unreachable!(),
⋮----
pub fn wait_for_next_incremental_snapshot(
⋮----
Some(incremental_snapshot_archives_dir),
⋮----
fn wait_for_next_snapshot(
⋮----
full_snapshot_slot.and_then(|full_snapshot_slot| {
⋮----
incremental_snapshot_archives_dir.as_ref().unwrap(),
⋮----
.unwrap_or(0);
trace!(
⋮----
if full_snapshot_archive_info.slot() > last_slot {
⋮----
full_snapshot_archive_info.slot(),
⋮----
if incremental_snapshot_archive_info.slot() > last_slot {
⋮----
assert!(
⋮----
sleep(Duration::from_secs(1));
⋮----
pub enum NextSnapshotType {
⋮----
pub enum NextSnapshotResult {

================
File: local-cluster/src/local_cluster.rs
================
pub struct ClusterConfig {
⋮----
impl ClusterConfig {
pub fn new_with_equal_stakes(
⋮----
node_stakes: vec![lamports_per_node; num_nodes],
⋮----
validator_configs: make_identical_validator_configs(
⋮----
impl Default for ClusterConfig {
fn default() -> Self {
⋮----
validator_configs: vec![],
⋮----
node_stakes: vec![],
⋮----
additional_accounts: vec![],
⋮----
struct QuicConnectionCacheConfig {
⋮----
pub struct LocalCluster {
⋮----
impl LocalCluster {
⋮----
fn sync_ledger_path_across_nested_config_fields(
⋮----
config.account_paths = vec![
⋮----
config.tower_storage = Arc::new(FileTowerStorage::new(ledger_path.to_path_buf()));
⋮----
let dummy: PathBuf = DUMMY_SNAPSHOT_CONFIG_PATH_MARKER.into();
⋮----
snapshot_config.full_snapshot_archives_dir = ledger_path.to_path_buf();
⋮----
snapshot_config.bank_snapshots_dir = ledger_path.join(BANK_SNAPSHOTS_DIR);
⋮----
pub fn new(config: &mut ClusterConfig, socket_addr_space: SocketAddrSpace) -> Self {
assert_eq!(config.validator_configs.len(), config.node_stakes.len());
⋮----
for validator_config in config.validator_configs.iter_mut() {
⋮----
overrides.insert(client_keypair.pubkey(), stake);
⋮----
let total_stake = config.node_stakes.iter().sum::<u64>();
⋮----
(client_keypair.pubkey(), stake),
(Pubkey::new_unique(), total_stake.saturating_sub(stake)),
⋮----
let connection_cache = create_connection_cache(
⋮----
assert_eq!(config.validator_configs.len(), keys.len());
keys.clone()
⋮----
.take(config.validator_configs.len())
.collect()
⋮----
assert_eq!(config.validator_configs.len(), node_vote_keys.len());
node_vote_keys.clone()
⋮----
for core_program_account in &core_bpf_programs(&Rent::default(), |_| true) {
⋮----
.push(core_program_account.clone());
⋮----
.iter()
.zip(&config.node_stakes)
.zip(&vote_keys)
.filter_map(|(((node_keypair, in_genesis), stake), vote_keypair)| {
info!(
⋮----
Some((
⋮----
node_keypair: node_keypair.insecure_clone(),
vote_keypair: vote_keypair.insecure_clone(),
⋮----
.unzip();
assert!(
⋮----
let leader_pubkey = leader_keypair.pubkey();
⋮----
} = create_genesis_config_with_vote_accounts_and_cluster_type(
⋮----
genesis_config.accounts.extend(
⋮----
.drain(..)
.map(|(key, account)| (key, Account::from(account))),
⋮----
genesis_config.poh_config = config.poh_config.clone();
let mut leader_config = safe_clone_config(&config.validator_configs[0]);
let (leader_ledger_path, _blockhash) = create_new_tmp_ledger_with_size!(
⋮----
leader_config.rpc_addrs = Some((
leader_node.info.rpc().unwrap(),
leader_node.info.rpc_pubsub().unwrap(),
⋮----
let leader_keypair = Arc::new(leader_keypair.insecure_clone());
let leader_vote_keypair = Arc::new(leader_vote_keypair.insecure_clone());
⋮----
.into_iter()
.map(|keypairs| {
⋮----
keypairs.node_keypair.pubkey(),
Arc::new(keypairs.vote_keypair.insecure_clone()),
⋮----
.collect();
⋮----
.all(|cfg| cfg.wait_for_supermajority.is_some());
⋮----
entry_point_info: leader_node.info.clone(),
⋮----
shred_version: leader_node.info.shred_version(),
⋮----
cluster.start_all_validators_parallel(
⋮----
leader_keypair.clone(),
leader_vote_keypair.clone(),
⋮----
.get(&leader_pubkey)
.unwrap()
⋮----
.clone();
cluster.entry_point_info = leader_contact_info.clone();
cluster.shred_version = leader_contact_info.shred_version();
⋮----
&leader_vote_keypair.pubkey(),
Arc::new(RwLock::new(vec![leader_vote_keypair.clone()])),
vec![],
⋮----
.expect("assume successful validator start");
let leader_contact_info = leader_server.cluster_info.my_contact_info();
⋮----
contact_info: leader_contact_info.clone(),
⋮----
safe_clone_config(&config.validator_configs[0]),
⋮----
validators.insert(leader_pubkey, cluster_leader);
⋮----
entry_point_info: leader_contact_info.clone(),
⋮----
shred_version: leader_contact_info.shred_version(),
⋮----
for (stake, validator_config, (key, _)) in izip!(
⋮----
cluster.add_validator(
⋮----
key.clone(),
node_pubkey_to_vote_key.get(&key.pubkey()).cloned(),
⋮----
let mut listener_config = safe_clone_config(&config.validator_configs[0]);
⋮----
(0..config.num_listeners).for_each(|_| {
cluster.add_validator_listener(
⋮----
discover_peers(
⋮----
&vec![cluster.entry_point_info.gossip().unwrap()],
Some(config.node_stakes.len() + config.num_listeners as usize),
⋮----
leader_contact_info.shred_version(),
⋮----
.unwrap();
⋮----
pub fn shred_version(&self) -> u16 {
⋮----
pub fn set_shred_version(&mut self, shred_version: u16) {
⋮----
pub fn exit(&mut self) {
for node in self.validators.values_mut() {
⋮----
v.exit();
⋮----
pub fn close_preserve_ledgers(&mut self) {
self.exit();
for (_, node) in self.validators.iter_mut() {
if let Some(v) = node.validator.take() {
v.join();
⋮----
pub fn add_validator_listener(
⋮----
self.do_add_validator(
⋮----
pub fn add_validator(
⋮----
fn do_add_validator(
⋮----
.build_validator_tpu_quic_client(self.entry_point_info.pubkey())
.expect("tpu_client");
let should_create_vote_pubkey = voting_keypair.is_none();
if voting_keypair.is_none() {
voting_keypair = Some(Arc::new(Keypair::new()));
⋮----
let validator_pubkey = validator_keypair.pubkey();
let validator_node = Node::new_localhost_with_pubkey(&validator_keypair.pubkey());
let contact_info = validator_node.info.clone();
let (ledger_path, _blockhash) = create_new_tmp_ledger_with_size!(
⋮----
info!("listener {validator_pubkey} ",);
⋮----
.rpc_client()
.get_balance_with_commitment(&validator_pubkey, CommitmentConfig::processed())
.expect("received response")
⋮----
info!("validator {validator_pubkey} balance {validator_balance}");
⋮----
voting_keypair.as_ref().unwrap(),
⋮----
let mut config = safe_clone_config(validator_config);
config.rpc_addrs = Some((
validator_node.info.rpc().unwrap(),
validator_node.info.rpc_pubsub().unwrap(),
⋮----
let voting_keypair = voting_keypair.unwrap();
⋮----
validator_keypair.clone(),
⋮----
&voting_keypair.pubkey(),
Arc::new(RwLock::new(vec![voting_keypair.clone()])),
vec![self.entry_point_info.clone()],
⋮----
safe_clone_config(validator_config),
⋮----
self.validators.insert(validator_pubkey, validator_info);
⋮----
fn start_all_validators_parallel(
⋮----
let mut handles = vec![];
⋮----
info!("Starting boostrap {leader_pubkey}");
⋮----
.unwrap_or_else(|e| panic!("Cluster leader failed to start: {e:?}"));
⋮----
info!("Bootstrap {leader_pubkey} started successfully");
⋮----
handles.push(handle);
⋮----
.zip(validator_configs[1..].iter())
.enumerate()
⋮----
let validator_keypair = key.clone();
⋮----
.get(&validator_keypair.pubkey())
.expect("All vote accounts must be setup in genesis for WFSM")
⋮----
let validator_config = safe_clone_config(validator_config);
let genesis_config = self.genesis_config.clone();
let entry_points = vec![self.entry_point_info.clone()];
⋮----
info!("Starting validator {validator_pubkey}");
⋮----
.unwrap_or_else(|e| panic!("Validator {i} failed to start: {e:?}"));
⋮----
voting_keypair: voting_keypair.clone(),
⋮----
info!("Validator {validator_pubkey} started successfully");
⋮----
for handle in handles.into_iter() {
⋮----
handle.join().expect("Validator thread panicked");
⋮----
pub fn ledger_path(&self, validator_pubkey: &Pubkey) -> PathBuf {
⋮----
.get(validator_pubkey)
⋮----
.clone()
⋮----
fn close(&mut self) {
self.close_preserve_ledgers();
⋮----
pub fn transfer(&self, source_keypair: &Keypair, dest_pubkey: &Pubkey, lamports: u64) {
⋮----
.expect("new tpu quic client");
⋮----
fn discover_nodes(
⋮----
.values()
.map(|v| v.info.contact_info.clone())
⋮----
assert!(!alive_node_contact_infos.is_empty());
info!("{test_name} discovering nodes");
let cluster_nodes = discover_validators(
&alive_node_contact_infos[0].gossip().unwrap(),
alive_node_contact_infos.len(),
self.shred_version(),
⋮----
info!("{} discovered {} nodes", test_name, cluster_nodes.len());
⋮----
pub fn check_min_slot_is_rooted(
⋮----
let alive_node_contact_infos = self.discover_nodes(socket_addr_space, test_name);
info!("{test_name} looking minimum root {min_root} on all nodes");
⋮----
info!("{test_name} done waiting for roots");
⋮----
pub fn check_for_new_roots(
⋮----
info!("{test_name} looking for new roots on all nodes");
⋮----
pub fn check_no_new_roots(
⋮----
.map(|node| &node.info.contact_info)
⋮----
info!("{test_name} making sure no new roots on any nodes");
⋮----
pub fn poll_for_processed_transaction(
⋮----
let status = client.rpc_client().get_signature_status_with_commitment(
⋮----
if status.is_some() {
return Ok(Some(()));
⋮----
if !client.rpc_client().is_blockhash_valid(
⋮----
return Ok(None);
⋮----
pub fn send_transaction_with_retries<T: Signers + ?Sized>(
⋮----
client.send_transaction_to_upcoming_leaders(transaction)?;
if Self::poll_for_processed_transaction(client, transaction)?.is_some() {
return Ok(());
⋮----
.get_latest_blockhash_with_commitment(CommitmentConfig::processed())?;
transaction.sign(keypairs, blockhash);
warn!("Sending transaction with retries, attempt {attempt} failed");
⋮----
Err(std::io::Error::other("failed to confirm transaction").into())
⋮----
fn transfer_with_client(
⋮----
trace!("getting leader blockhash");
⋮----
.get_latest_blockhash_with_commitment(CommitmentConfig::processed())
⋮----
.expect("client transfer should succeed");
⋮----
fn setup_vote_and_stake_accounts(
⋮----
let vote_account_pubkey = vote_account.pubkey();
let node_pubkey = from_account.pubkey();
⋮----
let stake_account_pubkey = stake_account_keypair.pubkey();
⋮----
.poll_get_balance_with_commitment(&vote_account_pubkey, CommitmentConfig::processed())
.unwrap_or(0)
⋮----
&from_account.pubkey(),
⋮----
let message = Message::new(&instructions, Some(&from_account.pubkey()));
⋮----
&[from_account.as_ref(), vote_account],
⋮----
.expect("should fund vote");
⋮----
.wait_for_balance_with_commitment(
⋮----
Some(amount),
⋮----
.expect("get balance");
⋮----
&[from_account.as_ref(), &stake_account_keypair],
⋮----
.expect("should delegate stake");
⋮----
warn!("{vote_account_pubkey} vote_account already has a balance?!?");
⋮----
info!("Checking for vote account registration of {node_pubkey}");
⋮----
.get_account_with_commitment(&stake_account_pubkey, CommitmentConfig::processed()),
⋮----
.get_account_with_commitment(&vote_account_pubkey, CommitmentConfig::processed()),
⋮----
.ok()
.and_then(|state| state.stake()),
VoteStateV4::deserialize(vote_account.data(), &vote_account_pubkey)
.ok(),
⋮----
Err(Error::other("invalid stake account state"))
⋮----
Err(Error::other("invalid vote account state"))
⋮----
info!("node {node_pubkey} {stake_state:?} {vote_state:?}");
Ok(())
⋮----
(None, _) => Err(Error::other("invalid stake account data")),
(_, None) => Err(Error::other("invalid vote account data")),
⋮----
(None, _) => Err(Error::other("unable to retrieve stake account data")),
(_, None) => Err(Error::other("unable to retrieve vote account data")),
⋮----
(Err(_), _) => Err(Error::other("unable to retrieve stake account data")),
(_, Err(_)) => Err(Error::other("unable to retrieve vote account data")),
⋮----
pub fn create_dummy_load_only_snapshot_config() -> SnapshotConfig {
⋮----
full_snapshot_archives_dir: DUMMY_SNAPSHOT_CONFIG_PATH_MARKER.into(),
bank_snapshots_dir: DUMMY_SNAPSHOT_CONFIG_PATH_MARKER.into(),
⋮----
fn build_tpu_client(
⋮----
let rpc_pubsub_url = format!("ws://{rpc_pubsub_addr}/");
⋮----
return Err(Error::other("Expected a Quic ConnectionCache. Got UDP"))
⋮----
rpc_pubsub_url.as_str(),
⋮----
cache.clone(),
⋮----
.map_err(|err| Error::other(format!("TpuSenderError: {err}")))?;
Ok(tpu_client)
⋮----
fn required_validator_funding(stake: u64) -> u64 {
stake.saturating_mul(2).saturating_add(2)
⋮----
fn create_connection_cache(
⋮----
Some(solana_net_utils::sockets::bind_to_localhost_unique().unwrap()),
⋮----
Some((&config.staked_nodes, &config.client_keypair.pubkey())),
⋮----
impl Cluster for LocalCluster {
fn get_node_pubkeys(&self) -> Vec<Pubkey> {
self.validators.keys().cloned().collect()
⋮----
fn build_validator_tpu_quic_client(&self, pubkey: &Pubkey) -> Result<QuicTpuClient> {
let contact_info = self.get_contact_info(pubkey).unwrap();
let rpc_url: String = format!("http://{}", contact_info.rpc().unwrap());
⋮----
self.build_tpu_client(rpc_client, contact_info.rpc_pubsub().unwrap())
⋮----
fn build_validator_tpu_quic_client_with_commitment(
⋮----
let rpc_url = format!("http://{}", contact_info.rpc().unwrap());
⋮----
fn exit_node(&mut self, pubkey: &Pubkey) -> ClusterValidatorInfo {
let mut node = self.validators.remove(pubkey).unwrap();
let mut validator = node.validator.take().expect("Validator must be running");
validator.exit();
validator.join();
⋮----
fn create_restart_context(
⋮----
node.info.set_shred_version(self.shred_version());
cluster_validator_info.info.contact_info = node.info.clone();
⋮----
Some((node.info.rpc().unwrap(), node.info.rpc_pubsub().unwrap()));
if pubkey == self.entry_point_info.pubkey() {
self.entry_point_info = node.info.clone();
⋮----
.map(|validator| {
assert!(validator.info.contact_info.pubkey() != pubkey);
if validator.info.contact_info.pubkey() == self.entry_point_info.pubkey() {
⋮----
validator.info.contact_info.clone()
⋮----
entry_point_infos.push(self.entry_point_info.clone());
⋮----
fn set_entry_point(&mut self, entry_point_info: ContactInfo) {
⋮----
fn restart_node(
⋮----
let restart_context = self.create_restart_context(pubkey, &mut cluster_validator_info);
⋮----
self.add_node(pubkey, cluster_validator_info);
self.connection_cache = create_connection_cache(
⋮----
fn add_node(&mut self, pubkey: &Pubkey, cluster_validator_info: ClusterValidatorInfo) {
self.validators.insert(*pubkey, cluster_validator_info);
⋮----
fn restart_node_with_context(
⋮----
validator_info.keypair.clone(),
⋮----
&validator_info.voting_keypair.pubkey(),
Arc::new(RwLock::new(vec![validator_info.voting_keypair.clone()])),
⋮----
&safe_clone_config(&cluster_validator_info.config),
⋮----
cluster_validator_info.validator = Some(restarted_node);
⋮----
fn exit_restart_node(
⋮----
let mut cluster_validator_info = self.exit_node(pubkey);
⋮----
self.restart_node(pubkey, cluster_validator_info, socket_addr_space);
⋮----
fn get_contact_info(&self, pubkey: &Pubkey) -> Option<&ContactInfo> {
self.validators.get(pubkey).map(|v| &v.info.contact_info)
⋮----
fn send_shreds_to_validator(&self, dup_shreds: Vec<&Shred>, validator_key: &Pubkey) {
let send_socket = bind_to_localhost_unique().expect("should bind");
⋮----
.get_contact_info(validator_key)
⋮----
.tvu(Protocol::UDP)
⋮----
.send_to(shred.payload().as_ref(), validator_tvu)
⋮----
impl Drop for LocalCluster {
fn drop(&mut self) {
self.close();

================
File: local-cluster/src/validator_configs.rs
================
pub fn safe_clone_config(config: &ValidatorConfig) -> ValidatorConfig {
⋮----
logfile: config.logfile.clone(),
⋮----
account_paths: config.account_paths.clone(),
account_snapshot_paths: config.account_snapshot_paths.clone(),
rpc_config: config.rpc_config.clone(),
on_start_geyser_plugin_config_files: config.on_start_geyser_plugin_config_files.clone(),
⋮----
pubsub_config: config.pubsub_config.clone(),
snapshot_config: config.snapshot_config.clone(),
⋮----
blockstore_options: config.blockstore_options.clone(),
broadcast_stage_type: config.broadcast_stage_type.clone(),
turbine_disabled: config.turbine_disabled.clone(),
fixed_leader_schedule: config.fixed_leader_schedule.clone(),
⋮----
new_hard_forks: config.new_hard_forks.clone(),
known_validators: config.known_validators.clone(),
repair_validators: config.repair_validators.clone(),
repair_whitelist: config.repair_whitelist.clone(),
gossip_validators: config.gossip_validators.clone(),
⋮----
tower_storage: config.tower_storage.clone(),
debug_keys: config.debug_keys.clone(),
⋮----
send_transaction_service_config: config.send_transaction_service_config.clone(),
⋮----
staked_nodes_overrides: config.staked_nodes_overrides.clone(),
⋮----
.keys()
.map(|name| (name.clone(), Arc::new(AtomicBool::new(false))))
.collect(),
⋮----
accounts_db_config: config.accounts_db_config.clone(),
⋮----
runtime_config: config.runtime_config.clone(),
⋮----
block_verification_method: config.block_verification_method.clone(),
block_production_method: config.block_production_method.clone(),
⋮----
block_production_scheduler_config: config.block_production_scheduler_config.clone(),
⋮----
generator_config: config.generator_config.clone(),
⋮----
wen_restart_proto_path: config.wen_restart_proto_path.clone(),
⋮----
retransmit_xdp: config.retransmit_xdp.clone(),
repair_handler_type: config.repair_handler_type.clone(),
relayer_config: config.relayer_config.clone(),
block_engine_config: config.block_engine_config.clone(),
shred_receiver_address: config.shred_receiver_address.clone(),
shred_retransmit_receiver_address: config.shred_retransmit_receiver_address.clone(),
tip_manager_config: config.tip_manager_config.clone(),
bam_url: config.bam_url.clone(),
⋮----
pub fn make_identical_validator_configs(
⋮----
let mut configs = vec![];
⋮----
configs.push(safe_clone_config(config));

================
File: local-cluster/tests/local_cluster.rs
================
fn test_local_cluster_start_and_exit() {
⋮----
assert_eq!(cluster.validators.len(), num_nodes);
⋮----
fn test_local_cluster_start_and_exit_with_config() {
⋮----
validator_configs: make_identical_validator_configs(
⋮----
node_stakes: vec![DEFAULT_NODE_STAKE; NUM_NODES],
⋮----
assert_eq!(cluster.validators.len(), NUM_NODES);
⋮----
fn test_spend_and_verify_all_nodes_1() {
⋮----
error!("test_spend_and_verify_all_nodes_1");
⋮----
fn test_spend_and_verify_all_nodes_2() {
⋮----
error!("test_spend_and_verify_all_nodes_2");
⋮----
fn test_spend_and_verify_all_nodes_3() {
⋮----
error!("test_spend_and_verify_all_nodes_3");
⋮----
fn test_local_cluster_signature_subscribe() {
⋮----
let nodes = cluster.get_node_pubkeys();
⋮----
.into_iter()
.find(|id| id != cluster.entry_point_info.pubkey())
.unwrap();
let non_bootstrap_info = cluster.get_contact_info(&non_bootstrap_id).unwrap();
⋮----
.build_validator_tpu_quic_client(cluster.entry_point_info.pubkey())
⋮----
.rpc_client()
.get_latest_blockhash_with_commitment(CommitmentConfig::processed())
⋮----
format!("ws://{}", non_bootstrap_info.rpc_pubsub().unwrap()),
⋮----
Some(RpcSignatureSubscribeConfig {
commitment: Some(CommitmentConfig::processed()),
enable_received_notification: Some(true),
⋮----
let responses: Vec<_> = receiver.try_iter().collect();
⋮----
sleep(Duration::from_millis(100));
⋮----
drop(cluster);
sig_subscribe_client.shutdown().unwrap();
assert!(got_received_notification);
⋮----
fn test_two_unbalanced_stakes() {
⋮----
error!("test_two_unbalanced_stakes");
⋮----
node_stakes: vec![DEFAULT_NODE_STAKE * 100, DEFAULT_NODE_STAKE],
⋮----
validator_configs: make_identical_validator_configs(&validator_config, 2),
⋮----
cluster.close_preserve_ledgers();
let leader_pubkey = *cluster.entry_point_info.pubkey();
let leader_ledger = cluster.validators[&leader_pubkey].info.ledger_path.clone();
⋮----
fn test_forwarding() {
⋮----
let cluster_nodes = discover_validators(
&cluster.entry_point_info.gossip().unwrap(),
⋮----
cluster.entry_point_info.shred_version(),
⋮----
assert!(cluster_nodes.len() >= 2);
⋮----
.iter()
.find(|c| c.pubkey() != &leader_pubkey)
⋮----
fn test_restart_node() {
⋮----
error!("test_restart_node");
⋮----
node_stakes: vec![DEFAULT_NODE_STAKE],
validator_configs: vec![safe_clone_config(&validator_config)],
⋮----
cluster.exit_restart_node(&nodes[0], validator_config, SocketAddrSpace::Unspecified);
⋮----
fn test_mainnet_beta_cluster_type() {
⋮----
assert_eq!(cluster_nodes.len(), 1);
⋮----
assert_matches!(
⋮----
for program_id in [].iter() {
assert_eq!(
⋮----
fn test_snapshot_download() {
⋮----
let snapshot_interval_slots = NonZeroU64::new(50).unwrap();
⋮----
setup_snapshot_validator_config(snapshot_interval_slots, num_account_paths);
⋮----
node_stakes: vec![stake],
⋮----
trace!("Waiting for snapshot");
let full_snapshot_archive_info = cluster.wait_for_next_full_snapshot(
⋮----
Some(Duration::from_secs(5 * 60)),
⋮----
trace!("found: {}", full_snapshot_archive_info.path().display());
download_snapshot_archive(
&cluster.entry_point_info.rpc().unwrap(),
⋮----
full_snapshot_archive_info.slot(),
*full_snapshot_archive_info.hash(),
⋮----
cluster.add_validator(
⋮----
fn test_incremental_snapshot_download() {
⋮----
SnapshotInterval::Slots(NonZeroU64::new(full_snapshot_interval).unwrap()),
SnapshotInterval::Slots(NonZeroU64::new(incremental_snapshot_interval).unwrap()),
⋮----
debug!(
⋮----
trace!("Waiting for snapshots");
⋮----
.wait_for_next_incremental_snapshot(
⋮----
trace!(
⋮----
incremental_snapshot_archive_info.slot(),
*incremental_snapshot_archive_info.hash(),
⋮----
SnapshotArchiveKind::Incremental(incremental_snapshot_archive_info.base_slot()),
⋮----
fn test_incremental_snapshot_download_with_crossing_full_snapshot_interval_at_startup() {
⋮----
info!(
⋮----
info!("Waiting for leader to create the next incremental snapshot...");
⋮----
.path(),
⋮----
info!("Waiting for leader to create snapshots... DONE");
info!("Downloading full snapshot to validator...");
⋮----
(full_snapshot_archive.slot(), *full_snapshot_archive.hash()),
⋮----
info!("Downloading incremental snapshot to validator...");
⋮----
incremental_snapshot_archive.slot(),
*incremental_snapshot_archive.hash(),
⋮----
SnapshotArchiveKind::Incremental(incremental_snapshot_archive.base_slot()),
⋮----
full_snapshot_archive.slot(),
⋮----
for entry in fs::read_dir(from).unwrap() {
let entry = entry.unwrap();
if entry.file_type().unwrap().is_dir() {
⋮----
let from_file_path = entry.path();
let to_file_path = to.join(from_file_path.file_name().unwrap());
⋮----
fs::copy(from_file_path, to_file_path).unwrap();
⋮----
trace!("deleting files in dir {}", dir.display());
for entry in fs::read_dir(dir).unwrap() {
⋮----
let file_path = entry.path();
trace!("\t\tdeleting file {}...", file_path.display());
fs::remove_file(file_path).unwrap();
⋮----
copy_files(from, to);
⋮----
copy_files(&remote_from, &remote_to);
⋮----
delete_files(from);
⋮----
delete_files(&remote_dir);
⋮----
let backup_validator_full_snapshot_archives_dir = tempfile::tempdir_in(farf_dir()).unwrap();
⋮----
copy_files_with_remote(
⋮----
backup_validator_full_snapshot_archives_dir.path(),
⋮----
tempfile::tempdir_in(farf_dir()).unwrap();
⋮----
backup_validator_incremental_snapshot_archives_dir.path(),
⋮----
info!("Starting the validator...");
⋮----
validator_identity.clone(),
⋮----
info!("Starting the validator... DONE");
let starting_slot = incremental_snapshot_archive.slot();
⋮----
.build_validator_tpu_quic_client(&validator_identity.pubkey())
.unwrap()
⋮----
.get_slot_with_commitment(CommitmentConfig::finalized())
⋮----
trace!("validator current slot: {validator_current_slot}");
⋮----
assert!(
⋮----
leader_full_snapshots.retain(|full_snapshot| {
full_snapshot.slot() == validator_full_snapshot.slot()
&& full_snapshot.hash() == validator_full_snapshot.hash()
⋮----
let leader_full_snapshot = leader_full_snapshots.first().unwrap();
⋮----
leader_full_snapshot.clone()
⋮----
info!("Stopping the validator...");
let validator_info = cluster.exit_node(&validator_identity.pubkey());
info!("Stopping the validator... DONE");
info!("Delete all the snapshots on the validator and restore the originals from the backup...");
delete_files_with_remote(
⋮----
cluster.restart_node(
&validator_identity.pubkey(),
⋮----
info!("Restarting the validator... DONE");
info!("Waiting for the validator to make new snapshots...");
⋮----
info!("Waiting for validator next full snapshot slot: {validator_next_full_snapshot_slot}");
⋮----
info!("validator full snapshot archives: {validator_full_snapshot_archives:#?}");
⋮----
.find(|validator_full_snapshot_archive| {
validator_full_snapshot_archive.slot()
== leader_full_snapshot_archive_for_comparison.slot()
⋮----
.expect("validator created an unexpected full snapshot");
⋮----
copy_files(
⋮----
info!("Starting final validator...");
⋮----
info!("Starting final validator... DONE");
⋮----
fn test_snapshot_restart_tower() {
⋮----
let snapshot_interval_slots = NonZeroU64::new(10).unwrap();
⋮----
validator_configs: vec![
⋮----
sleep(Duration::from_millis(5000));
let all_pubkeys = cluster.get_node_pubkeys();
⋮----
.find(|x| x != cluster.entry_point_info.pubkey())
⋮----
let validator_info = cluster.exit_node(&validator_id);
⋮----
.keep(),
⋮----
full_snapshot_archive_info.hash(),
full_snapshot_archive_info.archive_format(),
⋮----
fs::hard_link(full_snapshot_archive_info.path(), validator_archive_path).unwrap();
cluster.restart_node(&validator_id, validator_info, SocketAddrSpace::Unspecified);
let restarted_node_info = cluster.get_contact_info(&validator_id).unwrap();
⋮----
fn test_snapshots_blockstore_floor() {
⋮----
let snapshot_interval_slots = NonZeroU64::new(100).unwrap();
⋮----
trace!("Waiting for snapshot tar to be generated with slot",);
⋮----
if archive.is_some() {
trace!("snapshot exists");
break archive.unwrap();
⋮----
archive_info.slot(),
archive_info.hash(),
⋮----
fs::hard_link(archive_info.path(), validator_archive_path).unwrap();
let slot_floor = archive_info.slot();
⋮----
known_validators.insert(*cluster_nodes[0].pubkey());
⋮----
.known_validators = Some(known_validators);
⋮----
.build_validator_tpu_quic_client(&validator_id)
⋮----
trace!("current_slot: {current_slot}");
⋮----
.get_slot_with_commitment(CommitmentConfig::processed())
⋮----
sleep(Duration::from_secs(1));
⋮----
let blockstore = Blockstore::open(&validator_ledger_path.info.ledger_path).unwrap();
let (first_slot, _) = blockstore.slot_meta_iterator(1).unwrap().next().unwrap();
assert_eq!(first_slot, slot_floor);
⋮----
fn test_snapshots_restart_validity() {
⋮----
let mut all_account_storage_dirs = vec![std::mem::take(
⋮----
info!("run {i}");
trace!("Sending transactions");
⋮----
expected_balances.extend(new_balances);
cluster.wait_for_next_full_snapshot(
⋮----
generate_account_paths(num_account_paths);
all_account_storage_dirs.push(new_account_storage_dirs);
⋮----
trace!("Restarting cluster from snapshot");
⋮----
cluster.exit_restart_node(
⋮----
safe_clone_config(&snapshot_test_config.validator_config),
⋮----
trace!("Verifying balances");
⋮----
expected_balances.clone(),
⋮----
cluster.connection_cache.clone(),
⋮----
trace!("Spending and verifying");
⋮----
fn test_fail_entry_verification_leader() {
⋮----
let (cluster, _) = test_faulty_node(
⋮----
vec![leader_stake, validator_stake1, validator_stake2],
⋮----
cluster.check_for_new_roots(
⋮----
fn test_fake_shreds_broadcast_leader() {
⋮----
let node_stakes = vec![300, 100];
⋮----
fn test_wait_for_max_stake() {
⋮----
let stakers_slot_offset = slots_per_epoch.saturating_mul(MAX_LEADER_SCHEDULE_EPOCH_OFFSET);
⋮----
node_stakes: vec![DEFAULT_NODE_STAKE; num_validators],
validator_configs: make_identical_validator_configs(&validator_config, num_validators),
⋮----
let client = RpcClient::new_socket(cluster.entry_point_info.rpc().unwrap());
⋮----
.log(1. + NEW_WARMUP_COOLDOWN_RATE)
.ceil() as u32
⋮----
if let Err(err) = client.wait_for_max_stake_below_threshold_with_timeout(
⋮----
panic!("wait_for_max_stake failed: {err:?}");
⋮----
assert!(client.get_slot().unwrap() > 10);
⋮----
fn test_no_voting() {
⋮----
validator_configs: vec![validator_config],
⋮----
.expect("Couldn't get slot");
⋮----
let ledger_path = cluster.validators[&leader_pubkey].info.ledger_path.clone();
let ledger = Blockstore::open(&ledger_path).unwrap();
⋮----
let meta = ledger.meta(i as u64).unwrap().unwrap();
⋮----
let expected_parent = i.saturating_sub(1);
assert_eq!(parent, Some(expected_parent as u64));
⋮----
fn test_optimistic_confirmation_violation_detection() {
⋮----
let node_stakes = vec![50 * DEFAULT_NODE_STAKE, 51 * DEFAULT_NODE_STAKE];
⋮----
.map(|s| (Arc::new(Keypair::from_base58_string(s)), true))
.take(node_stakes.len())
.collect();
let node_to_restart = validator_keys[1].0.pubkey();
⋮----
validator_config.wait_for_supermajority = Some(0);
⋮----
mint_lamports: DEFAULT_MINT_LAMPORTS + node_stakes.iter().sum::<u64>(),
node_stakes: node_stakes.clone(),
validator_configs: make_identical_validator_configs(&validator_config, node_stakes.len()),
validator_keys: Some(validator_keys),
⋮----
.build_validator_tpu_quic_client(&node_to_restart)
⋮----
.get_slot_with_commitment(CommitmentConfig::confirmed())
⋮----
if start.elapsed() > Duration::from_secs(max_wait_time_seconds) {
cluster.exit();
panic!(
⋮----
info!("exiting node");
drop(client);
let mut exited_validator_info = cluster.exit_node(&node_to_restart);
info!("exiting node success");
⋮----
let tower = restore_tower(
⋮----
&exited_validator_info.info.keypair.pubkey(),
⋮----
let last_voted_slot = tower.last_voted_slot().expect("vote must exist");
let blockstore = open_blockstore(&exited_validator_info.info.ledger_path);
⋮----
remove_tower(&exited_validator_info.info.ledger_path, &node_to_restart);
⋮----
.set_dead_slot(optimistically_confirmed_slot)
⋮----
.meta(optimistically_confirmed_slot)
⋮----
.err()
.map(|_| BufferRedirect::stderr().unwrap());
⋮----
sleep(Duration::from_millis(1000));
let ledger_path = cluster.ledger_path(&node_to_restart);
let blockstore = open_blockstore(&ledger_path);
⋮----
.meta(optimistically_confirmed_slot_parent)
⋮----
// Wait for a fork to be created that does not include the OC slot
// Now on restart the validator should only vote for this new`slot` which they have
// never voted on before and thus avoids the panic in gossip
if slot > optimistically_confirmed_slot && blockstore.is_full(slot) {
⋮----
panic!("Didn't get new fork within {max_wait_time_seconds} seconds");
⋮----
info!("looking for root > {optimistically_confirmed_slot} on new fork {new_fork_slot}");
⋮----
info!("Client connecting to: {}", client.rpc_client().url());
⋮----
info!("Found root: {last_root} > {new_fork_slot}");
⋮----
if AncestorIterator::new_inclusive(last_root, &blockstore).contains(&new_fork_slot)
⋮----
panic!("Didn't get root on new fork within {max_wait_time_seconds} seconds");
⋮----
while start.elapsed().as_secs() < 10 {
buf.read_to_string(&mut output).unwrap();
if output.contains(&expected_log) {
⋮----
sleep(Duration::from_millis(10));
⋮----
print!("{output}");
assert!(success);
⋮----
panic!("dumped log and disabled testing");
⋮----
&[cluster.get_contact_info(&node_to_restart).unwrap().clone()],
⋮----
fn test_validator_saves_tower() {
⋮----
let validator_id = validator_identity_keypair.pubkey();
⋮----
validator_keys: Some(vec![(validator_identity_keypair.clone(), true)]),
⋮----
.get(&validator_id)
⋮----
.clone();
let file_tower_storage = FileTowerStorage::new(ledger_path.clone());
⋮----
trace!("current slot: {slot}");
⋮----
let tower1 = Tower::restore(&file_tower_storage, &validator_id).unwrap();
trace!("tower1: {tower1:?}");
assert_eq!(tower1.root(), 0);
assert!(tower1.last_voted_slot().is_some());
⋮----
trace!("current root: {root}");
⋮----
sleep(Duration::from_millis(50));
⋮----
let tower2 = Tower::restore(&file_tower_storage, &validator_id).unwrap();
trace!("tower2: {tower2:?}");
assert_eq!(tower2.root(), last_replayed_root);
⋮----
.save(&file_tower_storage, &validator_identity_keypair)
⋮----
trace!("current root: {root}, last_replayed_root: {last_replayed_root}");
⋮----
let mut validator_info = cluster.exit_node(&validator_id);
let tower3 = Tower::restore(&file_tower_storage, &validator_id).unwrap();
trace!("tower3: {tower3:?}");
let tower3_root = tower3.root();
assert!(tower3_root >= new_root);
remove_tower(&ledger_path, &validator_id);
⋮----
trace!("current root: {root}, last tower root: {tower3_root}");
⋮----
let tower4 = Tower::restore(&file_tower_storage, &validator_id).unwrap();
trace!("tower4: {tower4:?}");
assert!(tower4.root() >= new_root);
⋮----
fn root_in_tower(tower_path: &Path, node_pubkey: &Pubkey) -> Option<Slot> {
restore_tower(tower_path, node_pubkey).map(|tower| tower.root())
⋮----
enum ClusterMode {
⋮----
fn do_test_future_tower(cluster_mode: ClusterMode) {
⋮----
ClusterMode::MasterOnly => vec![DEFAULT_NODE_STAKE],
ClusterMode::MasterSlave => vec![DEFAULT_NODE_STAKE * 100, DEFAULT_NODE_STAKE],
⋮----
.map(|(kp, _)| kp.pubkey())
⋮----
let val_a_ledger_path = cluster.ledger_path(&validator_a_pubkey);
⋮----
if let Some(root) = root_in_tower(&val_a_ledger_path, &validator_a_pubkey) {
⋮----
let validator_a_info = cluster.exit_node(&validator_a_pubkey);
⋮----
let blockstore = open_blockstore(&val_a_ledger_path);
purge_slots_with_count(&blockstore, purged_slot_before_restart, 100);
⋮----
let _validator_a_info = cluster.exit_node(&validator_a_pubkey);
⋮----
let (last_vote, _) = last_vote_in_tower(&val_a_ledger_path, &validator_a_pubkey).unwrap();
⋮----
.take_while(|a| *a >= some_root_after_restart)
⋮----
.rev()
⋮----
assert_eq!(actual_block_ancestors, expected_countinuous_no_fork_votes);
assert!(actual_block_ancestors.len() > MAX_LOCKOUT_HISTORY);
info!("validator managed to handle future tower!");
⋮----
panic!("no root detected");
⋮----
fn test_future_tower_master_only() {
do_test_future_tower(ClusterMode::MasterOnly);
⋮----
fn test_future_tower_master_slave() {
do_test_future_tower(ClusterMode::MasterSlave);
⋮----
fn restart_whole_cluster_after_hard_fork(
⋮----
let cluster_for_a = cluster.clone();
let val_a_ledger_path = validator_a_info.info.ledger_path.clone();
⋮----
.lock()
⋮----
.create_restart_context(&validator_a_pubkey, &mut validator_a_info);
⋮----
.add_node(&validator_a_pubkey, restarted_validator_info);
⋮----
last_vote_in_tower(&val_a_ledger_path, &validator_a_pubkey).unwrap();
⋮----
assert_eq!(last_vote, new_last_vote);
⋮----
last_vote = Some(new_last_vote);
⋮----
cluster.lock().unwrap().restart_node(
⋮----
thread.join().unwrap();
⋮----
fn test_hard_fork_invalidates_tower() {
⋮----
let node_stakes = vec![60 * DEFAULT_NODE_STAKE, 40 * DEFAULT_NODE_STAKE];
⋮----
node_stakes.len(),
⋮----
let val_a_ledger_path = cluster.lock().unwrap().ledger_path(&validator_a_pubkey);
⋮----
let mut validator_a_info = cluster.lock().unwrap().exit_node(&validator_a_pubkey);
let mut validator_b_info = cluster.lock().unwrap().exit_node(&validator_b_pubkey);
⋮----
let hard_fork_slots = Some(vec![hard_fork_slot]);
⋮----
hard_forks.register(hard_fork_slot);
⋮----
&cluster.lock().unwrap().genesis_config.hash(),
Some(&hard_forks),
⋮----
.set_shred_version(expected_shred_version);
⋮----
.clone_from(&hard_fork_slots);
validator_a_info.config.wait_for_supermajority = Some(hard_fork_slot);
validator_a_info.config.expected_shred_version = Some(expected_shred_version);
⋮----
validator_b_info.config.wait_for_supermajority = Some(hard_fork_slot);
validator_b_info.config.expected_shred_version = Some(expected_shred_version);
⋮----
let blockstore_a = open_blockstore(&validator_a_info.info.ledger_path);
let blockstore_b = open_blockstore(&validator_b_info.info.ledger_path);
purge_slots_with_count(&blockstore_a, hard_fork_slot + 1, 100);
purge_slots_with_count(&blockstore_b, hard_fork_slot + 1, 100);
⋮----
restart_whole_cluster_after_hard_fork(
⋮----
.check_for_new_roots(16, "hard fork", SocketAddrSpace::Unspecified);
⋮----
fn test_run_test_load_program_accounts_root() {
run_test_load_program_accounts(CommitmentConfig::finalized());
⋮----
fn create_simple_snapshot_config(ledger_path: &Path) -> SnapshotConfig {
⋮----
full_snapshot_archives_dir: ledger_path.to_path_buf(),
bank_snapshots_dir: ledger_path.join(snapshot_paths::BANK_SNAPSHOTS_DIR),
⋮----
fn create_snapshot_to_hard_fork(
⋮----
halt_at_slot: Some(snapshot_slot),
new_hard_forks: Some(hard_forks),
⋮----
let ledger_path = blockstore.ledger_path();
let genesis_config = open_genesis_config(ledger_path, u64::MAX).unwrap();
let snapshot_config = create_simple_snapshot_config(ledger_path);
⋮----
vec![
⋮----
let bank = bank_forks.read().unwrap().get(snapshot_slot).unwrap();
⋮----
Some(snapshot_config.snapshot_version),
⋮----
fn test_hard_fork_with_gap_in_roots() {
⋮----
let node_stakes = vec![60, 40];
⋮----
let val_b_ledger_path = cluster.lock().unwrap().ledger_path(&validator_b_pubkey);
⋮----
if let Some((last_vote, _)) = last_vote_in_tower(&val_a_ledger_path, &validator_a_pubkey) {
⋮----
&& root_in_tower(&val_a_ledger_path, &validator_a_pubkey) > Some(min_root)
⋮----
assert!(hard_fork_slot > root_in_tower(&val_a_ledger_path, &validator_a_pubkey).unwrap());
⋮----
let blockstore_a = Blockstore::open(&val_a_ledger_path).unwrap();
create_snapshot_to_hard_fork(&blockstore_a, hard_fork_slot, vec![hard_fork_slot]);
purge_slots_with_count(&blockstore_a, genesis_slot, 1);
⋮----
let mut meta = blockstore_a.meta(next_slot).unwrap().unwrap();
meta.unset_parent();
blockstore_a.put_meta(next_slot, &meta).unwrap();
⋮----
let (last_vote_a, _) = last_vote_in_tower(&val_a_ledger_path, &validator_a_pubkey).unwrap();
let (last_vote_b, _) = last_vote_in_tower(&val_b_ledger_path, &validator_b_pubkey).unwrap();
let root_a = root_in_tower(&val_a_ledger_path, &validator_a_pubkey).unwrap();
let root_b = root_in_tower(&val_b_ledger_path, &validator_b_pubkey).unwrap();
(last_vote_a.min(last_vote_b), root_a.min(root_b))
⋮----
let blockstore_b = Blockstore::open(&val_b_ledger_path).unwrap();
⋮----
.reversed_rooted_slot_iterator(common_root)
⋮----
slots_a.push(genesis_slot);
roots_a.push(genesis_slot);
⋮----
assert_eq!((&slots_a, &roots_a), (&slots_b, &roots_b));
assert_eq!(&slots_a[slots_a.len() - roots_a.len()..].to_vec(), &roots_a);
assert_eq!(&slots_b[slots_b.len() - roots_b.len()..].to_vec(), &roots_b);
⋮----
fn test_restart_tower_rollback() {
⋮----
let node_stakes = vec![DEFAULT_NODE_STAKE * 100, DEFAULT_NODE_STAKE];
⋮----
let b_pubkey = validator_keys[1].0.pubkey();
⋮----
let val_b_ledger_path = cluster.ledger_path(&b_pubkey);
⋮----
earlier_tower = restore_tower(&val_b_ledger_path, &b_pubkey).unwrap();
if earlier_tower.last_voted_slot().unwrap_or(0) > 1 {
⋮----
let tower = restore_tower(&val_b_ledger_path, &b_pubkey).unwrap();
if tower.root()
⋮----
.last_voted_slot()
.expect("Earlier tower must have at least one vote")
⋮----
exited_validator_info = cluster.exit_node(&b_pubkey);
last_voted_slot = tower.last_voted_slot().unwrap();
⋮----
save_tower(
⋮----
exited_validator_info.config.wait_to_vote_slot = Some(last_voted_slot + 10);
⋮----
fn test_run_test_load_program_accounts_partition_root() {
run_test_load_program_accounts_partition(CommitmentConfig::finalized());
⋮----
fn run_test_load_program_accounts_partition(scan_commitment: CommitmentConfig) {
⋮----
let (leader_schedule, validator_keys) = create_custom_leader_schedule_with_random_keys(&[
⋮----
let (update_client_sender, update_client_receiver) = unbounded();
let (scan_client_sender, scan_client_receiver) = unbounded();
⋮----
let (t_update, t_scan, additional_accounts) = setup_transfer_scan_threads(
⋮----
exit.clone(),
⋮----
update_client_sender.send(update_client).unwrap();
⋮----
scan_client_sender.send(scan_client).unwrap();
⋮----
exit.store(true, Ordering::Relaxed);
t_update.join().unwrap();
t_scan.join().unwrap();
⋮----
run_cluster_partition(
⋮----
Some((leader_schedule, validator_keys)),
⋮----
fn test_rpc_block_subscribe() {
⋮----
let node_stakes = vec![leader_stake, rpc_stake];
⋮----
validator_config.enable_default_rpc_block_subscribe();
⋮----
let rpc_node_pubkey = &validator_keys[1].0.pubkey();
⋮----
let rpc_node_contact_info = cluster.get_contact_info(rpc_node_pubkey).unwrap();
⋮----
format!(
⋮----
Some(RpcBlockSubscribeConfig {
commitment: Some(CommitmentConfig::confirmed()),
⋮----
if !responses.is_empty() {
⋮----
assert!(response.value.err.is_none());
assert!(response.value.block.is_some());
⋮----
block_subscribe_client.shutdown().unwrap();
⋮----
fn test_oc_bad_signatures() {
⋮----
let node_stakes = vec![leader_stake, our_node_stake];
⋮----
vec![validator_keys.first().unwrap().0.pubkey()],
⋮----
wait_for_supermajority: Some(0),
fixed_leader_schedule: Some(fixed_schedule),
⋮----
let our_id = validator_keys.last().unwrap().0.pubkey();
⋮----
let our_info = cluster.exit_node(&our_id);
⋮----
.map(|(_, vote, ..)| vote)
⋮----
if !vote.is_empty() {
Some((vote, leader_vote_tx))
⋮----
let node_keypair = node_keypair.insecure_clone();
let vote_keypair = vote_keypair.insecure_clone();
let num_votes_simulated = num_votes_simulated.clone();
⋮----
info!("received vote for {vote_slot}");
let vote_hash = parsed_vote.hash();
info!("Simulating vote from our node on slot {vote_slot}, hash {vote_hash}");
let tower_sync = TowerSync::new_from_slots(vec![vote_slot], vote_hash, None);
⋮----
num_votes_simulated.fetch_add(1, Ordering::Relaxed);
⋮----
cluster.validators.len().saturating_sub(1),
⋮----
assert!(voter_thread_sleep_ms * MAX_VOTES_TO_SIMULATE <= 1000);
⋮----
assert!(responses.is_empty());
if num_votes_simulated.load(Ordering::Relaxed) > MAX_VOTES_TO_SIMULATE {
⋮----
gossip_voter.close();
⋮----
fn test_votes_land_in_fork_during_long_partition() {
⋮----
struct PartitionContext {
⋮----
let lighter_validator_ledger_path = cluster.ledger_path(&context.lighter_validator_key);
⋮----
cluster.ledger_path(&context.heaviest_validator_key);
⋮----
let (heavier_validator_latest_vote_slot, _) = last_vote_in_tower(
⋮----
let lighter_validator_blockstore = open_blockstore(&lighter_validator_ledger_path);
⋮----
.meta(heavier_validator_latest_vote_slot)
⋮----
.is_none()
⋮----
let max_wait = ms_for_n_slots(MAX_PROCESSING_AGE as u64, DEFAULT_TICKS_PER_SLOT);
⋮----
if lighter_validator_blockstore.is_root(context.heavier_fork_slot) {
⋮----
run_kill_partition_switch_threshold(
⋮----
fn setup_transfer_scan_threads(
⋮----
let exit_ = exit.clone();
⋮----
.take(num_starting_accounts)
.collect(),
⋮----
.map(|k| {
⋮----
k.pubkey(),
⋮----
let starting_keypairs_ = starting_keypairs.clone();
let target_keypairs_ = target_keypairs.clone();
⋮----
.name("update".to_string())
.spawn(move || {
let client = update_client_receiver.recv().unwrap();
⋮----
if exit_.load(Ordering::Relaxed) {
⋮----
for i in 0..starting_keypairs_.len() {
let result = client.async_transfer(
⋮----
&target_keypairs_[i].pubkey(),
⋮----
if result.is_err() {
debug!("Failed in transfer for starting keypair: {result:?}");
⋮----
&starting_keypairs_[i].pubkey(),
⋮----
scan_commitment_config.account_config.commitment = Some(scan_commitment);
⋮----
.chain(target_keypairs.iter())
.map(|k| k.pubkey())
⋮----
.name("scan".to_string())
⋮----
let client = scan_client_receiver.recv().unwrap();
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
.get_program_ui_accounts_with_config(
⋮----
scan_commitment_config.clone(),
⋮----
.ok()
.map(|result| {
⋮----
.map(|(key, account)| {
if tracked_pubkeys.contains(&key) {
⋮----
assert_eq!(total_scan_balance, expected_total_balance);
⋮----
fn run_test_load_program_accounts(scan_commitment: CommitmentConfig) {
⋮----
let node_stakes = vec![51 * DEFAULT_NODE_STAKE, 50 * DEFAULT_NODE_STAKE];
⋮----
let (t_update, t_scan, starting_accounts) = setup_transfer_scan_threads(
⋮----
update_client_sender.send(client).unwrap();
⋮----
.build_validator_tpu_quic_client(&other_validator_id)
⋮----
fn test_no_lockout_violation_with_tower() {
do_test_lockout_violation_with_or_without_tower(true);
⋮----
fn test_lockout_violation_without_tower() {
do_test_lockout_violation_with_or_without_tower(false);
⋮----
fn do_test_lockout_violation_with_or_without_tower(with_tower: bool) {
⋮----
let node_stakes = vec![
⋮----
let validator_to_slots = vec![
⋮----
let c_validator_to_slots = vec![(validator_b_pubkey, DEFAULT_SLOTS_PER_EPOCH as usize)];
let c_leader_schedule = create_custom_leader_schedule(c_validator_to_slots.into_iter());
let leader_schedule = Arc::new(create_custom_leader_schedule(
validator_to_slots.into_iter(),
⋮----
assert_eq!(leader_schedule[slot], validator_b_pubkey);
⋮----
default_config.fixed_leader_schedule = Some(FixedSchedule {
leader_schedule: leader_schedule.clone(),
⋮----
default_config.wait_for_supermajority = Some(0);
⋮----
make_identical_validator_configs(&default_config, node_stakes.len());
⋮----
validator_configs[2].fixed_leader_schedule = Some(FixedSchedule {
⋮----
let val_b_ledger_path = cluster.ledger_path(&validator_b_pubkey);
let val_c_ledger_path = cluster.ledger_path(&validator_c_pubkey);
info!("val_a {validator_a_pubkey} ledger path {val_a_ledger_path:?}");
info!("val_b {validator_b_pubkey} ledger path {val_b_ledger_path:?}");
info!("val_c {validator_c_pubkey} ledger path {val_c_ledger_path:?}");
info!("Exiting validator C");
let mut validator_c_info = cluster.exit_node(&validator_c_pubkey);
info!("Waiting on validator A to vote");
⋮----
info!("Exiting validators A and B");
let _validator_b_info = cluster.exit_node(&validator_b_pubkey);
⋮----
let next_slot_on_a = last_vote_in_tower(&val_a_ledger_path, &validator_a_pubkey)
⋮----
info!("base slot: {base_slot}, next_slot_on_a: {next_slot_on_a}");
info!("Create validator C's ledger");
⋮----
std::fs::remove_dir_all(&validator_c_info.info.ledger_path).unwrap();
⋮----
fs_extra::dir::copy(&val_b_ledger_path, &val_c_ledger_path, &opt).unwrap();
remove_tower(&val_c_ledger_path, &validator_b_pubkey);
let blockstore = open_blockstore(&val_c_ledger_path);
purge_slots_with_count(&blockstore, next_slot_on_a, truncated_slots);
⋮----
info!("Create validator A's ledger");
⋮----
let b_blockstore = open_blockstore(&val_b_ledger_path);
let a_blockstore = open_blockstore(&val_a_ledger_path);
copy_blocks(next_slot_on_a, &b_blockstore, &a_blockstore, false);
purge_slots_with_count(&a_blockstore, next_slot_on_a + 1, truncated_slots);
⋮----
info!("Removing tower!");
remove_tower(&val_a_ledger_path, &validator_a_pubkey);
⋮----
info!("Not removing tower!");
⋮----
info!("Restart validator C again!!!");
⋮----
validator_c_info.config.fixed_leader_schedule = Some(FixedSchedule { leader_schedule });
⋮----
let elapsed = now.elapsed();
⋮----
if let Some((newest_vote, _)) = last_vote_in_tower(&val_c_ledger_path, &validator_c_pubkey)
⋮----
votes_on_c_fork.insert(last_vote);
if votes_on_c_fork.len() >= 4 {
⋮----
assert!(!votes_on_c_fork.is_empty());
info!("Collected validator C's votes: {votes_on_c_fork:?}");
info!("Restart validator A again!!!");
⋮----
let mut a_votes = vec![];
⋮----
a_votes.push(last_vote);
⋮----
if ancestors.any(|a| votes_on_c_fork.contains(&a)) {
⋮----
info!("Observed A's votes on: {a_votes:?}");
⋮----
panic!("No violation expected because of persisted tower!");
⋮----
panic!("Violation expected because of removed persisted tower!");
⋮----
fn test_fork_choice_refresh_old_votes() {
⋮----
assert!(alive_stake_1 < alive_stake_2);
assert!(alive_stake_1 + alive_stake_3 > alive_stake_2);
⋮----
let info = cluster.exit_node(smallest_validator_key);
context.smallest_validator_info = Some(info);
⋮----
let sleep_time_ms = ms_for_n_slots(
⋮----
info!("Wait for blockhashes to expire, {sleep_time_ms} ms");
sleep(Duration::from_millis(sleep_time_ms));
⋮----
.as_ref()
⋮----
.pubkey();
⋮----
let lighter_fork_ledger_path = cluster.ledger_path(&context.lighter_fork_validator_key);
let heaviest_ledger_path = cluster.ledger_path(&context.heaviest_validator_key);
⋮----
let lighter_fork_latest_vote = wait_for_last_vote_in_tower_to_land_in_ledger(
⋮----
let heaviest_fork_latest_vote = wait_for_last_vote_in_tower_to_land_in_ledger(
⋮----
let smallest_blockstore = open_blockstore(&smallest_ledger_path);
let lighter_fork_blockstore = open_blockstore(&lighter_fork_ledger_path);
let heaviest_blockstore = open_blockstore(&heaviest_ledger_path);
info!("Opened blockstores");
⋮----
.chain(AncestorIterator::new(
⋮----
.enumerate()
.zip(heavier_ancestors.iter())
.find(|((_index, lighter_fork_ancestor), heavier_fork_ancestor)| {
⋮----
assert!(heavier_ancestors.len() > last_common_ancestor_index + 4);
⋮----
distance_from_tip = lighter_ancestors.len() - different_ancestor_index - 1;
⋮----
assert!(context.first_slot_in_lighter_partition != 0);
⋮----
copy_blocks(
⋮----
.get_bank_hash(context.first_slot_in_lighter_partition)
⋮----
vec![(context.first_slot_in_lighter_partition, bank_hash)],
⋮----
drop(smallest_blockstore);
⋮----
sleep(Duration::from_millis(ms_for_n_slots(
⋮----
context.smallest_validator_info.take().unwrap(),
⋮----
cluster.check_min_slot_is_rooted(
⋮----
assert!(heaviest_blockstore.is_root(context.first_slot_in_lighter_partition));
⋮----
Some(ticks_per_slot),
⋮----
fn test_kill_heaviest_partition() {
⋮----
num_slots_per_validator * (partitions.len() - 1),
⋮----
let validator_to_kill = validator_keys[0].pubkey();
⋮----
info!("Killing validator with id: {validator_to_kill}");
cluster.exit_node(&validator_to_kill);
cluster.check_for_new_roots(16, "PARTITION_TEST", SocketAddrSpace::Unspecified);
⋮----
vec![],
⋮----
fn test_kill_partition_switch_threshold_no_progress() {
⋮----
cluster.check_no_new_roots(400, "PARTITION_TEST", SocketAddrSpace::Unspecified);
⋮----
fn test_kill_partition_switch_threshold_progress() {
⋮----
fn test_duplicate_shreds_broadcast_leader() {
run_duplicate_shreds_broadcast_leader(true);
⋮----
fn test_duplicate_shreds_broadcast_leader_ancestor_hashes() {
run_duplicate_shreds_broadcast_leader(false);
⋮----
fn run_duplicate_shreds_broadcast_leader(vote_on_duplicate: bool) {
⋮----
assert_eq!(*node_stakes.last().unwrap(), our_node_stake);
let total_stake: u64 = node_stakes.iter().sum();
⋮----
assert!((bad_leader_stake as f64 / total_stake as f64) >= 1.0 - DUPLICATE_THRESHOLD);
assert!(partition_node_stake < our_node_stake && partition_node_stake < good_node_stake);
let (duplicate_slot_sender, duplicate_slot_receiver) = unbounded();
let (mut cluster, validator_keys) = test_faulty_node(
⋮----
duplicate_slot_sender: Some(duplicate_slot_sender),
⋮----
let our_id = validator_keys.last().unwrap().pubkey();
⋮----
let bad_leader_id = *cluster.entry_point_info.pubkey();
let bad_leader_ledger_path = cluster.validators[&bad_leader_id].info.ledger_path.clone();
info!("our node id: {}", node_keypair.pubkey());
⋮----
if label.pubkey() == bad_leader_id {
⋮----
let mut duplicate_slots = vec![];
⋮----
info!("received vote for {latest_vote_slot}");
let new_epoch_slots: Vec<Slot> = (0..latest_vote_slot + 1).collect();
info!("Simulating epoch slots from our node: {new_epoch_slots:?}");
cluster_info.push_epoch_slots(&new_epoch_slots);
for slot in duplicate_slot_receiver.try_iter() {
duplicate_slots.push(slot);
⋮----
if vote_on_duplicate || !duplicate_slots.contains(&latest_vote_slot) {
⋮----
let leader_blockstore = open_blockstore(&bad_leader_ledger_path);
⋮----
.take(MAX_LOCKOUT_HISTORY)
.zip(1..)
⋮----
vote_slots.reverse();
⋮----
.nth(MAX_LOCKOUT_HISTORY);
⋮----
cluster_info.push_vote_at_index(vote_tx, gossip_vote_index, &node_keypair);
⋮----
fn test_switch_threshold_uses_gossip_votes() {
⋮----
assert_eq!(dead_validator_infos.len(), 1);
context.dead_validator_info = Some(dead_validator_infos.pop().unwrap());
⋮----
let heavier_validator_ledger_path = cluster.ledger_path(&context.heaviest_validator_key);
let (lighter_validator_latest_vote, _) = last_vote_in_tower(
⋮----
info!("Lighter validator's latest vote is for slot {lighter_validator_latest_vote}");
⋮----
let heavier_blockstore = open_blockstore(&heavier_validator_ledger_path);
⋮----
let (sanity_check_lighter_validator_latest_vote, _) = last_vote_in_tower(
⋮----
last_vote_in_tower(
⋮----
assert_ne!(lighter_validator_latest_vote, heavier_validator_latest_vote);
⋮----
assert!(!heavier_ancestors.contains(&lighter_validator_latest_vote));
⋮----
info!("Checking to make sure lighter validator doesn't switch");
⋮----
find_latest_replayed_slot_from_ledger(&lighter_validator_ledger_path, latest_slot);
⋮----
if latest_slot_ancestors.contains(&heavier_validator_latest_vote) {
⋮----
if !tower.is_locked_out(latest_slot, &latest_slot_ancestors) {
let new_lighter_validator_latest_vote = tower.last_voted_slot().unwrap();
⋮----
info!("Incrementing voting opportunities: {total_voting_opportunities}");
⋮----
info!("Tower still locked out, can't vote for slot: {latest_slot}");
⋮----
warn!(
⋮----
info!("Simulate vote for slot: {heavier_validator_latest_vote} from dead validator");
⋮----
.get_contact_info(&context.heaviest_validator_key)
⋮----
.gossip()
.unwrap(),
⋮----
let (new_lighter_validator_latest_vote, _) = last_vote_in_tower(
⋮----
let (heavier_validator_latest_vote, _) = last_vote_in_tower(
⋮----
.chain(std::iter::once(larger))
⋮----
assert!(larger_slot_ancestors.contains(&smaller));
⋮----
fn test_listener_startup() {
⋮----
assert_eq!(cluster_nodes.len(), 4);
⋮----
fn find_latest_replayed_slot_from_ledger(
⋮----
let mut blockstore = open_blockstore(ledger_path);
⋮----
.slot_meta_iterator(latest_slot)
⋮----
.filter_map(|(s, _)| if s > latest_slot { Some(s) } else { None })
⋮----
if let Some(new_latest_slot) = new_latest_slots.first() {
⋮----
info!("Checking latest_slot {latest_slot}");
⋮----
info!("Waiting for slot {latest_slot} to be full");
if blockstore.is_full(latest_slot) {
⋮----
blockstore = open_blockstore(ledger_path);
⋮----
info!("Waiting for slot {latest_slot} to be replayed");
if blockstore.get_bank_hash(latest_slot).is_some() {
⋮----
AncestorIterator::new(latest_slot, &blockstore).collect(),
⋮----
fn test_cluster_partition_1_1() {
⋮----
fn test_cluster_partition_1_1_1() {
⋮----
fn test_leader_failure_4() {
⋮----
error!("test_leader_failure_4");
⋮----
let validator_keys: Option<Vec<(Arc<Keypair>, bool)>> = Some(
⋮----
.map(|_| (Arc::new(Keypair::new()), true))
⋮----
node_stakes: vec![DEFAULT_NODE_STAKE; num_nodes],
validator_configs: make_identical_validator_configs(&validator_config, num_nodes),
⋮----
.get(local.entry_point_info.pubkey())
⋮----
config.ticks_per_slot * config.poh_config.target_tick_duration.as_millis() as u64,
⋮----
fn test_slot_hash_expiry() {
⋮----
.map(|s| Arc::new(Keypair::from_base58_string(s)))
⋮----
make_identical_validator_configs(&validator_config, node_stakes.len());
⋮----
node_vote_keys: Some(node_vote_keys),
⋮----
let a_ledger_path = cluster.ledger_path(&a_pubkey);
let b_ledger_path = cluster.ledger_path(&b_pubkey);
info!("Killing B");
let mut b_info = cluster.exit_node(&b_pubkey);
info!("Letting A run until common_ancestor_slot");
⋮----
if let Some((last_vote, _)) = last_vote_in_tower(&a_ledger_path, &a_pubkey) {
⋮----
info!("Copying A's ledger to B");
std::fs::remove_dir_all(&b_info.info.ledger_path).unwrap();
⋮----
fs_extra::dir::copy(&a_ledger_path, &b_ledger_path, &opt).unwrap();
info!("Removing A's tower in B's ledger dir");
remove_tower(&b_ledger_path, &a_pubkey);
info!("Loading A's tower");
if let Some(mut a_tower) = restore_tower(&a_ledger_path, &a_pubkey) {
⋮----
if let Some(s) = a_tower.last_voted_slot() {
⋮----
info!("New common_ancestor_slot {common_ancestor_slot}");
⋮----
panic!("A's tower has no votes");
⋮----
info!("Increase lockout by 6 confirmation levels and save as B's tower");
a_tower.increase_lockout(6);
save_tower(&b_ledger_path, &a_tower, &b_info.info.keypair);
info!("B's new tower: {:?}", a_tower.tower_slots());
⋮----
panic!("A's tower is missing");
⋮----
info!("Removing extra slots from B's blockstore");
let blockstore = open_blockstore(&b_ledger_path);
purge_slots_with_count(&blockstore, common_ancestor_slot + 1, 100);
⋮----
wait_for_last_vote_in_tower_to_land_in_ledger(&a_ledger_path, &a_pubkey).unwrap();
⋮----
let blockstore = open_blockstore(&a_ledger_path);
⋮----
info!("Killing A");
let a_info = cluster.exit_node(&a_pubkey);
info!("Restarting B");
⋮----
cluster.restart_node(&b_pubkey, b_info, SocketAddrSpace::Unspecified);
info!("Allowing B to fork");
⋮----
wait_for_last_vote_in_tower_to_land_in_ledger(&b_ledger_path, &b_pubkey).unwrap();
⋮----
if let Some(index) = ancestors.position(|x| x == common_ancestor_slot) {
⋮----
info!("Kill B");
b_info = cluster.exit_node(&b_pubkey);
info!("Resolve the partition");
⋮----
let a_blockstore = open_blockstore(&a_ledger_path);
let b_blockstore = open_blockstore(&b_ledger_path);
copy_blocks(last_vote_on_a, &a_blockstore, &b_blockstore, false);
⋮----
info!("Restarting A & B");
cluster.restart_node(&a_pubkey, a_info, SocketAddrSpace::Unspecified);
⋮----
info!("Waiting for B to switch to majority fork and make a root");
⋮----
&[cluster.get_contact_info(&a_pubkey).unwrap().clone()],
⋮----
fn test_duplicate_with_pruned_ancestor() {
⋮----
let mut node_stakes = vec![majority_leader_stake, minority_leader_stake, our_node];
node_stakes.append(&mut vec![observer_stake; 3]);
let num_nodes = node_stakes.len();
⋮----
.chain(std::iter::repeat_with(|| (Arc::new(Keypair::new()), true)))
⋮----
let leader_schedule = create_custom_leader_schedule(validator_to_slots.into_iter());
⋮----
let mut validator_configs = make_identical_validator_configs(&default_config, num_nodes);
validator_configs[0].fixed_leader_schedule = Some(FixedSchedule {
leader_schedule: Arc::new(create_custom_leader_schedule(
[(minority_pubkey, slots_per_epoch as usize)].into_iter(),
⋮----
let majority_ledger_path = cluster.ledger_path(&majority_pubkey);
let minority_ledger_path = cluster.ledger_path(&minority_pubkey);
let our_node_ledger_path = cluster.ledger_path(&our_node_pubkey);
info!("majority {majority_pubkey} ledger path {majority_ledger_path:?}");
info!("minority {minority_pubkey} ledger path {minority_ledger_path:?}");
info!("our_node {our_node_pubkey} ledger path {our_node_ledger_path:?}");
info!("Killing our node");
let our_node_info = cluster.exit_node(&our_node_pubkey);
info!("Waiting on majority validator to vote on at least {fork_slot}");
⋮----
if let Some((last_vote, _)) = last_vote_in_tower(&majority_ledger_path, &majority_pubkey) {
⋮----
info!("Killing majority validator, waiting for minority fork to reach a depth of at least 15");
let mut majority_validator_info = cluster.exit_node(&majority_pubkey);
⋮----
if let Some((last_vote, _)) = last_vote_in_tower(&minority_ledger_path, &minority_pubkey) {
⋮----
info!("Killing minority validator, fork created successfully: {last_minority_vote:?}");
⋮----
wait_for_last_vote_in_tower_to_land_in_ledger(&minority_ledger_path, &minority_pubkey)
⋮----
let minority_validator_info = cluster.exit_node(&minority_pubkey);
info!("Truncating majority validator ledger to {fork_slot}");
⋮----
remove_tower(&majority_ledger_path, &majority_pubkey);
let blockstore = open_blockstore(&majority_ledger_path);
purge_slots_with_count(&blockstore, fork_slot + 1, 100);
⋮----
info!("Restarting majority validator");
majority_validator_info.config.wait_to_vote_slot = Some(fork_slot + fork_length);
⋮----
.clone_from(&minority_validator_info.config.fixed_leader_schedule);
⋮----
if let Some(last_root) = last_root_in_tower(&majority_ledger_path, &majority_pubkey) {
⋮----
wait_for_last_vote_in_tower_to_land_in_ledger(&majority_ledger_path, &majority_pubkey)
⋮----
std::fs::remove_dir_all(&our_node_info.info.ledger_path).unwrap();
⋮----
fs_extra::dir::copy(&majority_ledger_path, &our_node_ledger_path, &opt).unwrap();
remove_tower(&our_node_ledger_path, &majority_pubkey);
⋮----
let minority_blockstore = open_blockstore(&minority_validator_info.info.ledger_path);
let our_blockstore = open_blockstore(&our_node_info.info.ledger_path);
⋮----
info!("For our node, changing parent of {last_majority_vote} to {last_minority_vote}");
our_blockstore.clear_unconfirmed_slot(last_majority_vote);
let entries = create_ticks(
⋮----
entries_to_test_shreds(&entries, last_majority_vote, last_minority_vote, true, 0);
our_blockstore.insert_shreds(shreds, None, false).unwrap();
⋮----
info!("Restarting our node, verifying that our node is making roots past the duplicate block");
⋮----
&[cluster.get_contact_info(&our_node_pubkey).unwrap().clone()],
⋮----
fn test_boot_from_local_state() {
⋮----
SnapshotInterval::Slots(NonZeroU64::new(100).unwrap());
⋮----
SnapshotInterval::Slots(NonZeroU64::new(10).unwrap());
⋮----
node_stakes: vec![100 * DEFAULT_NODE_STAKE],
validator_configs: make_identical_validator_configs(&validator1_config.validator_config, 1),
⋮----
info!("Waiting for validator1 to create snapshots...");
⋮----
info!("Waiting for validator1 to create snapshots... DONE");
info!("Copying snapshots to validator2...");
⋮----
full_snapshot_archive.path(),
⋮----
.path()
.join(full_snapshot_archive.path().file_name().unwrap()),
⋮----
incremental_snapshot_archive.path(),
⋮----
.join(incremental_snapshot_archive.path().file_name().unwrap()),
⋮----
info!("Copying snapshots to validator2... DONE");
info!("Starting validator2...");
⋮----
validator2_identity.clone(),
⋮----
info!("Starting validator2... DONE");
info!("Waiting for validator2 to create a new bank snapshot...");
⋮----
if bank_snapshot.slot > incremental_snapshot_archive.slot() {
⋮----
debug!("bank snapshot: {bank_snapshot:?}");
info!("Waiting for validator2 to create a new bank snapshot... DONE");
info!("Restarting validator2 from local state...");
let mut validator2_info = cluster.exit_node(&validator2_identity.pubkey());
⋮----
&validator2_identity.pubkey(),
⋮----
info!("Restarting validator2 from local state... DONE");
info!("Waiting for validator2 to create snapshots...");
⋮----
info!("Waiting for validator2 to create snapshots... DONE");
info!("Copying snapshots to validator3...");
⋮----
info!("Copying snapshots to validator3... DONE");
info!("Starting validator3...");
⋮----
info!("Starting validator3... DONE");
info!("Waiting for validator3 to create snapshots...");
⋮----
info!("Waiting for validator3 to create snapshots... DONE");
⋮----
struct SnapshotSlot(Slot);
⋮----
struct BaseSlot(Slot);
⋮----
info!("Checking if validator{i} has the same snapshots as validator1...");
⋮----
if other_full_snapshot_slot >= full_snapshot_archive.slot()
&& other_incremental_snapshot_slot >= Some(incremental_snapshot_archive.slot())
⋮----
debug!("validator{i} full snapshot archives: {other_full_snapshot_archives:?}");
⋮----
info!("Checking if validator{i} has the same snapshots as validator1... DONE");
⋮----
fn test_boot_from_local_state_missing_archive() {
⋮----
SnapshotInterval::Slots(NonZeroU64::new(20).unwrap());
⋮----
validator_configs: make_identical_validator_configs(&validator_config.validator_config, 1),
⋮----
info!("Waiting for validator to create snapshots...");
⋮----
info!("Waiting for validator to create snapshots... DONE");
info!("Stopping validator...");
let validator_pubkey = cluster.get_node_pubkeys()[0];
let mut validator_info = cluster.exit_node(&validator_pubkey);
info!("Stopping validator... DONE");
info!("Deleting latest full snapshot archive...");
⋮----
validator_config.full_snapshot_archives_dir.path(),
⋮----
fs::remove_file(highest_full_snapshot.path()).unwrap();
info!("Deleting latest full snapshot archive... DONE");
info!("Restarting validator...");
⋮----
info!("Restarting validator... DONE");
⋮----
fn test_duplicate_shreds_switch_failure() {
fn wait_for_duplicate_fork_frozen(ledger_path: &Path, dup_slot: Slot) -> Hash {
info!("Waiting to receive and replay entire duplicate fork with tip {dup_slot}");
⋮----
let duplicate_fork_validator_blockstore = open_blockstore(ledger_path);
if let Some(frozen_hash) = duplicate_fork_validator_blockstore.get_bank_hash(dup_slot) {
⋮----
fn clear_ledger_and_tower(ledger_path: &Path, pubkey: &Pubkey, start_slot: Slot) {
remove_tower_if_exists(ledger_path, pubkey);
let blockstore = open_blockstore(ledger_path);
purge_slots_with_count(&blockstore, start_slot, 1000);
⋮----
while let Some((proof_slot, _)) = blockstore.get_first_duplicate_proof() {
blockstore.remove_slot_duplicate_proof(proof_slot).unwrap();
⋮----
fn restart_dup_validator(
⋮----
duplicate_fork_validator_info.config.turbine_disabled = disable_turbine.clone();
info!("Restarting node: {pubkey}");
⋮----
let ledger_path = cluster.ledger_path(pubkey);
info!("Waiting on duplicate fork to vote on duplicate slot: {dup_slot}");
⋮----
let last_vote = last_vote_in_tower(&ledger_path, pubkey);
⋮----
info!("latest vote: {latest_vote_slot}");
⋮----
disable_turbine.store(false, Ordering::Relaxed);
info!("Resending duplicate shreds to duplicate fork validator");
cluster.send_shreds_to_validator(vec![dup_shred1, dup_shred2], pubkey);
info!("Waiting on duplicate fork validator to see duplicate shreds and make a proof",);
⋮----
let duplicate_fork_validator_blockstore = open_blockstore(&ledger_path);
if let Some(dup_proof) = duplicate_fork_validator_blockstore.get_first_duplicate_proof()
⋮----
assert_eq!(dup_proof.0, dup_slot);
⋮----
assert!(duplicate_fork_node1_stake <= (total_stake as f64 * DUPLICATE_THRESHOLD) as u64);
assert!(duplicate_fork_node2_stake <= (total_stake as f64 * DUPLICATE_THRESHOLD) as u64);
⋮----
.map(|(validator_keypair, in_genesis)| {
let pubkey = validator_keypair.pubkey();
⋮----
let (mut cluster, _validator_keypairs) = test_faulty_node(
⋮----
partition: ClusterPartition::Pubkey(vec![
⋮----
Some(validator_configs),
Some(FixedSchedule {
⋮----
info!("Killing unnecessary validators");
⋮----
cluster.ledger_path(&duplicate_fork_validator2_pubkey);
let duplicate_fork_validator2_info = cluster.exit_node(&duplicate_fork_validator2_pubkey);
⋮----
cluster.ledger_path(&target_switch_fork_validator_pubkey);
⋮----
cluster.exit_node(&target_switch_fork_validator_pubkey);
⋮----
.recv_timeout(Duration::from_millis(30_000))
.expect("Duplicate leader failed to make a duplicate slot in allotted time");
let dup_frozen_hash = wait_for_duplicate_fork_frozen(
&cluster.ledger_path(&duplicate_fork_validator1_pubkey),
⋮----
let original_frozen_hash = wait_for_duplicate_fork_frozen(
&cluster.ledger_path(&duplicate_leader_validator_pubkey),
⋮----
assert_ne!(
⋮----
info!("Waiting for duplicate proof for slot: {dup_slot}");
⋮----
let ledger_path = cluster.ledger_path(&duplicate_leader_validator_pubkey);
⋮----
.get_data_shreds_for_slot(dup_slot, 0)
⋮----
.pop()
⋮----
cluster.send_shreds_to_validator(vec![&dup_shred], &duplicate_fork_validator1_pubkey);
wait_for_duplicate_proof(
⋮----
.unwrap_or_else(|| panic!("Duplicate proof for slot {dup_slot} not found"))
⋮----
info!("Killing remaining validators");
⋮----
cluster.ledger_path(&duplicate_fork_validator1_pubkey);
let duplicate_fork_validator1_info = cluster.exit_node(&duplicate_fork_validator1_pubkey);
let duplicate_leader_ledger_path = cluster.ledger_path(&duplicate_leader_validator_pubkey);
cluster.exit_node(&duplicate_leader_validator_pubkey);
let dup_shred1 = Shred::new_from_serialized_shred(duplicate_proof.shred1.clone()).unwrap();
let dup_shred2 = Shred::new_from_serialized_shred(duplicate_proof.shred2).unwrap();
assert_eq!(dup_shred1.slot(), dup_shred2.slot());
assert_eq!(dup_shred1.slot(), dup_slot);
info!("Purging towers and ledgers for: {duplicate_leader_validator_pubkey:?}");
Blockstore::destroy(&target_switch_fork_validator_ledger_path).unwrap();
⋮----
let blockstore1 = open_blockstore(&duplicate_leader_ledger_path);
let blockstore2 = open_blockstore(&target_switch_fork_validator_ledger_path);
copy_blocks(dup_slot, &blockstore1, &blockstore2, false);
⋮----
clear_ledger_and_tower(
⋮----
info!("Purging towers and ledgers for: {duplicate_fork_validator1_pubkey:?}");
⋮----
info!("Purging towers and ledgers for: {duplicate_fork_validator2_pubkey:?}");
⋮----
Blockstore::destroy(&duplicate_fork_validator2_ledger_path).unwrap();
⋮----
let blockstore1 = open_blockstore(&duplicate_fork_validator1_ledger_path);
let blockstore2 = open_blockstore(&duplicate_fork_validator2_ledger_path);
⋮----
cluster.set_entry_point(target_switch_fork_validator_info.info.contact_info.clone());
info!("Restarting switch fork node");
⋮----
info!("Waiting for switch fork to make block past duplicate fork");
⋮----
let last_vote = wait_for_last_vote_in_tower_to_land_in_ledger(
⋮----
let blockstore = open_blockstore(&target_switch_fork_validator_ledger_path);
⋮----
AncestorIterator::new_inclusive(latest_vote_slot, &blockstore).collect();
assert!(ancestor_slots.contains(&latest_vote_slot));
assert!(ancestor_slots.contains(&0));
assert!(!ancestor_slots.contains(&dup_slot));
⋮----
info!("Restarting duplicate fork node");
restart_dup_validator(
⋮----
info!("Waiting on duplicate fork validator to generate block on top of duplicate fork",);
⋮----
open_blockstore(&cluster.ledger_path(&duplicate_fork_validator1_pubkey));
⋮----
.meta(dup_slot)
⋮----
if !meta.next_slots.is_empty() {
⋮----
fn test_randomly_mixed_block_verification_methods_between_bootstrap_and_not() {
⋮----
methods.shuffle(&mut rand::rng());
for (validator_config, method) in config.validator_configs.iter_mut().zip_eq(methods) {
⋮----
fn test_invalid_forks_persisted_on_restart() {
⋮----
let majority_keypair = validator_keypairs[1].0.clone();
⋮----
let node_stakes = vec![DEFAULT_NODE_STAKE, 100 * DEFAULT_NODE_STAKE];
⋮----
let mut validator_configs = make_identical_validator_configs(&default_config, 2);
⋮----
validator_keys: Some(validator_keypairs),
⋮----
let target_ledger_path = cluster.ledger_path(&target_pubkey);
⋮----
wait_for_last_vote_in_tower_to_land_in_ledger(&target_ledger_path, &target_pubkey)
⋮----
let blockstore = open_blockstore(&target_ledger_path);
⋮----
cluster.genesis_config.hash(),
⋮----
let last_hash = entries.last().unwrap().hash;
⋮----
.entries_to_merkle_shreds_for_tests(
⋮----
info!("Sending duplicate shreds for {dup_slot}");
cluster.send_shreds_to_validator(dup_shreds.iter().collect(), &target_pubkey);
wait_for_duplicate_proof(&target_ledger_path, dup_slot)
.expect("Duplicate proof for {dup_slot} not found");
⋮----
info!("Duplicate proof for {dup_slot} has landed, restarting node");
let info = cluster.exit_node(&target_pubkey);
⋮----
purge_slots_with_count(&blockstore, dup_slot + 5, 100);
⋮----
cluster.restart_node(&target_pubkey, info, SocketAddrSpace::Unspecified);
info!("Waiting for fork built off {parent}");
⋮----
let parent_meta = blockstore.meta(parent).unwrap().expect("Meta must exist");
⋮----
if checked_children.contains(&child) {
⋮----
if blockstore.is_full(child) {
⋮----
.get_data_shreds_for_slot(child, 0)
.expect("Child is full");
⋮----
is_our_block &= shred.verify(&target_pubkey);
⋮----
checked_children.insert(child);

================
File: local-cluster/.gitignore
================
/target/
/farf/

================
File: local-cluster/Cargo.toml
================
[package]
name = "solana-local-cluster"
documentation = "https://docs.rs/solana-local-cluster"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = ["solana-core/dev-context-only-utils"]

[dependencies]
agave-logger = { workspace = true }
agave-snapshots = { workspace = true }
crossbeam-channel = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
rand = { workspace = true }
rayon = { workspace = true }
solana-account = { workspace = true }
solana-accounts-db = { workspace = true }
solana-client = { workspace = true }
solana-client-traits = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-core = { workspace = true }
solana-entry = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-genesis-config = { workspace = true }
solana-gossip = { workspace = true }
solana-hard-forks = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-net-utils = { workspace = true }
solana-poh-config = { workspace = true }
solana-program-binaries = { workspace = true }
solana-pubkey = { workspace = true }
solana-pubsub-client = { workspace = true }
solana-quic-client = { workspace = true }
solana-rent = { workspace = true }
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-runtime = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-shred-version = { workspace = true }
solana-signer = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-stake-interface = { workspace = true }
solana-streamer = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }
solana-time-utils = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
solana-turbine = { workspace = true }
solana-validator-exit = { workspace = true }
solana-vote = { workspace = true }
solana-vote-interface = { workspace = true }
solana-vote-program = { workspace = true }
static_assertions = { workspace = true }
strum = { workspace = true, features = ["derive"] }
tempfile = { workspace = true }
trees = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
fs_extra = { workspace = true }
gag = { workspace = true }
serial_test = { workspace = true }
solana-core = { workspace = true, features = ["dev-context-only-utils"] }
solana-download-utils = { workspace = true }
solana-genesis-utils = { workspace = true }
solana-ledger = { workspace = true, features = ["dev-context-only-utils"] }
solana-local-cluster = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }

================
File: logger/src/lib.rs
================
struct LoggerShim {}
⋮----
fn enabled(&self, metadata: &log::Metadata) -> bool {
LOGGER.read().unwrap().enabled(metadata)
⋮----
fn log(&self, record: &log::Record) {
LOGGER.read().unwrap().log(record);
⋮----
fn flush(&self) {}
⋮----
fn replace_logger(logger: env_logger::Logger) {
log::set_max_level(logger.filter());
*LOGGER.write().unwrap() = logger;
⋮----
pub fn setup_with(filter: &str) {
⋮----
env_logger::Builder::from_env(env_logger::Env::new().filter_or("_RUST_LOG", filter))
.format_timestamp_nanos()
.build();
replace_logger(logger);
⋮----
pub fn setup_with_default(filter: &str) {
let logger = env_logger::Builder::from_env(env_logger::Env::new().default_filter_or(filter))
⋮----
pub fn setup_with_default_filter() {
setup_with_default(DEFAULT_FILTER);
⋮----
pub fn setup() {
setup_with_default("error");
⋮----
fn setup_file_with_default_filter(logfile: &Path) {
⋮----
.create(true)
.append(true)
.open(logfile)
.unwrap();
⋮----
env_logger::Builder::from_env(env_logger::Env::new().default_filter_or(DEFAULT_FILTER))
⋮----
.target(env_logger::Target::Pipe(Box::new(file)))
⋮----
pub fn redirect_stderr(filename: &Path) {
⋮----
match OpenOptions::new().create(true).append(true).open(filename) {
⋮----
libc::dup2(file.as_raw_fd(), libc::STDERR_FILENO);
⋮----
Err(err) => eprintln!("Unable to open {}: {err}", filename.display()),
⋮----
pub fn initialize_logging(logfile: Option<PathBuf>) {
if env::var_os("RUST_BACKTRACE").is_none() {
⋮----
setup_with_default_filter();
⋮----
redirect_stderr(&logfile);
⋮----
setup_file_with_default_filter(&logfile);
⋮----
pub fn redirect_stderr_to_file(logfile: Option<PathBuf>) -> Option<JoinHandle<()>> {
⋮----
use log::info;
⋮----
.unwrap_or_else(|err| {
eprintln!("Unable to register SIGUSR1 handler: {err:?}");
⋮----
Some(
⋮----
.name("solSigUsr1".into())
.spawn(move || {
for signal in signals.forever() {
info!(
⋮----
.unwrap(),
⋮----
println!("logrotate is not supported on this platform");

================
File: logger/Cargo.toml
================
[package]
name = "agave-logger"
description = "Agave Logger"
documentation = "https://docs.rs/agave-logger"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "agave_logger"

[features]
agave-unstable-api = []

[dependencies]
env_logger = { workspace = true }
log = { workspace = true }

[target."cfg(unix)".dependencies]
libc = { workspace = true }
signal-hook = { workspace = true }

================
File: measure/src/lib.rs
================
pub mod macros;
pub mod measure;

================
File: measure/src/macros.rs
================
macro_rules! measure_time {
⋮----
macro_rules! measure_us {
⋮----
/// Measures how long it takes to execute an expression, and returns a Duration
///
⋮----
///
/// # Examples
⋮----
/// # Examples
///
⋮----
///
/// ```
⋮----
/// ```
/// # use solana_measure::meas_dur;
⋮----
/// # use solana_measure::meas_dur;
/// # fn meow(x: i32, y: i32) -> i32 {x + y}
⋮----
/// # fn meow(x: i32, y: i32) -> i32 {x + y}
/// let (result, duration) = meas_dur!(meow(1, 2) + 3);
⋮----
/// let (result, duration) = meas_dur!(meow(1, 2) + 3);
/// # assert_eq!(result, 1 + 2 + 3);
⋮----
/// # assert_eq!(result, 1 + 2 + 3);
/// ```
⋮----
/// ```
//
⋮----
//
// The macro name, `meas_dur`, is "measure" + "duration".
⋮----
macro_rules! meas_dur {
⋮----
mod tests {
⋮----
fn my_multiply(x: i32, y: i32) -> i32 {
⋮----
fn square(x: i32) -> i32 {
my_multiply(x, x)
⋮----
struct SomeStruct {
⋮----
impl SomeStruct {
fn add_to(&self, x: i32) -> i32 {
⋮----
fn test_measure_macro() {
⋮----
let (_result, measure) = measure_time!(sleep(Duration::from_millis(1)), "test");
assert!(measure.as_s() > 0.0);
assert!(measure.as_ms() > 0);
assert!(measure.as_us() > 0);
⋮----
let (result, _measure) = measure_time!(my_multiply(3, 4), "test");
assert_eq!(result, 3 * 4);
let (result, _measure) = measure_time!(square(5), "test");
assert_eq!(result, 5 * 5)
⋮----
let (result, _measure) = measure_time!(some_struct.add_to(4), "test");
assert_eq!(result, 42 + 4);
⋮----
let (result, _measure) = measure_time!({ 1 + 2 }, "test");
assert_eq!(result, 3);
⋮----
let (result, _measure) = measure_time!(square(5), "test",);
⋮----
let (result, _measure) = measure_time!(square(5));
⋮----
fn test_measure_us_macro() {
⋮----
let (_result, measure) = measure_us!(sleep(Duration::from_millis(1)));
assert!(measure > 0);
⋮----
let (result, _measure) = measure_us!(my_multiply(3, 4));
⋮----
let (result, _measure) = measure_us!(square(5));
⋮----
let (result, _measure) = measure_us!(some_struct.add_to(4));
⋮----
let (result, _measure) = measure_us!({ 1 + 2 });
⋮----
fn test_meas_dur_macro() {
⋮----
let (result, _duration) = meas_dur!(my_multiply(3, 4));
⋮----
let (result, _duration) = meas_dur!(square(5));
⋮----
let (result, _duration) = meas_dur!(some_struct.add_to(4));
⋮----
let (result, _duration) = meas_dur!({ 1 + 2 });

================
File: measure/src/measure.rs
================
pub struct Measure {
⋮----
impl Measure {
pub fn start(name: &'static str) -> Self {
⋮----
pub fn stop(&mut self) {
self.duration = self.start.elapsed().as_nanos() as u64;
⋮----
pub fn as_ns(&self) -> u64 {
⋮----
pub fn as_us(&self) -> u64 {
⋮----
pub fn as_ms(&self) -> u64 {
⋮----
pub fn as_s(&self) -> f32 {
⋮----
pub fn as_duration(&self) -> Duration {
Duration::from_nanos(self.as_ns())
⋮----
pub fn end_as_ns(self) -> u64 {
self.start.elapsed().as_nanos() as u64
⋮----
pub fn end_as_us(self) -> u64 {
self.start.elapsed().as_micros() as u64
⋮----
pub fn end_as_ms(self) -> u64 {
self.start.elapsed().as_millis() as u64
⋮----
pub fn end_as_s(self) -> f32 {
self.start.elapsed().as_secs_f32()
⋮----
pub fn end_as_duration(self) -> Duration {
self.start.elapsed()
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
⋮----
write!(f, "{} running", self.name)
} else if self.as_us() < 1 {
write!(f, "{} took {}ns", self.name, self.duration)
} else if self.as_ms() < 1 {
write!(f, "{} took {}us", self.name, self.as_us())
} else if self.as_s() < 1. {
write!(f, "{} took {}ms", self.name, self.as_ms())
⋮----
write!(f, "{} took {:.1}s", self.name, self.as_s())
⋮----
mod tests {
⋮----
fn test_measure() {
⋮----
sleep(test_duration);
measure.stop();
assert!(measure.as_duration() >= test_duration);
⋮----
fn test_measure_as() {
⋮----
duration: test_duration.as_nanos() as u64,
⋮----
assert!(f32::abs(measure.as_s() - 0.1f32) <= f32::EPSILON);
assert_eq!(measure.as_ms(), 100);
assert_eq!(measure.as_us(), 100_000);
assert_eq!(measure.as_ns(), 100_000_000);
assert_eq!(measure.as_duration(), test_duration);
⋮----
fn test_measure_display() {
⋮----
assert_eq!(format!("{measure}"), "test_ns took 1ns");
⋮----
assert_eq!(format!("{measure}"), "test_us took 1us");
⋮----
assert_eq!(format!("{measure}"), "test_ms took 1ms");
⋮----
assert_eq!(format!("{measure}"), "test_s took 1.0s");
⋮----
assert_eq!(format!("{measure}"), "test_not_stopped running");

================
File: measure/.gitignore
================
/target/
/farf/

================
File: measure/Cargo.toml
================
[package]
name = "solana-measure"
documentation = "https://docs.rs/solana-measure"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

================
File: merkle-tree/src/lib.rs
================
pub mod merkle_tree;
pub use merkle_tree::MerkleTree;

================
File: merkle-tree/src/merkle_tree.rs
================
macro_rules! hash_leaf {
⋮----
macro_rules! hash_intermediate {
⋮----
pub struct MerkleTree {
⋮----
pub struct ProofEntry<'a>(&'a Hash, Option<&'a Hash>, Option<&'a Hash>);
⋮----
pub fn new(
⋮----
assert!(left_sibling.is_none() ^ right_sibling.is_none());
Self(target, left_sibling, right_sibling)
⋮----
pub fn get_left_sibling(&self) -> Option<&'a Hash> {
⋮----
pub fn get_right_sibling(&self) -> Option<&'a Hash> {
⋮----
pub struct Proof<'a>(Vec<ProofEntry<'a>>);
⋮----
pub fn push(&mut self, entry: ProofEntry<'a>) {
self.0.push(entry)
⋮----
pub fn verify(&self, candidate: Hash) -> bool {
let result = self.0.iter().try_fold(candidate, |candidate, pe| {
let lsib = pe.1.unwrap_or(&candidate);
let rsib = pe.2.unwrap_or(&candidate);
let hash = hash_intermediate!(lsib, rsib);
⋮----
Some(hash)
⋮----
result.is_some()
⋮----
pub fn get_proof_entries(self) -> Vec<ProofEntry<'a>> {
⋮----
impl MerkleTree {
⋮----
fn next_level_len(level_len: usize) -> usize {
⋮----
level_len.div_ceil(2)
⋮----
fn calculate_vec_capacity(leaf_count: usize) -> usize {
// the most nodes consuming case is when n-1 is full balanced binary tree
// then n will cause the previous tree add a left only path to the root
// this cause the total nodes number increased by tree height, we use this
// condition as the max nodes consuming case.
// n is current leaf nodes number
// assuming n-1 is a full balanced binary tree, n-1 tree nodes number will be
// 2(n-1) - 1, n tree height is closed to log2(n) + 1
// so the max nodes number is 2(n-1) - 1 + log2(n) + 1, finally we can use
// 2n + log2(n+1) as a safe capacity value.
// test results:
// 8192 leaf nodes(full balanced):
// computed cap is 16398, actually using is 16383
// 8193 leaf nodes:(full balanced plus 1 leaf):
// computed cap is 16400, actually using is 16398
// about performance: current used fast_math log2 code is constant algo time
⋮----
pub fn new<T: AsRef<[u8]>>(items: &[T], sorted_hashes: bool) -> Self {
let cap = MerkleTree::calculate_vec_capacity(items.len());
⋮----
leaf_count: items.len(),
⋮----
let item = item.as_ref();
let hash = hash_leaf!(item);
mt.nodes.push(hash);
⋮----
let mut level_len = MerkleTree::next_level_len(items.len());
let mut level_start = items.len();
let mut prev_level_len = items.len();
⋮----
// Duplicate last entry if the level length is odd
⋮----
// tip-distribution verification uses sorted hashing
⋮----
let hash = hash_intermediate!(rsib, lsib);
⋮----
// hashing for solana internals
⋮----
pub fn get_root(&self) -> Option<&Hash> {
self.nodes.iter().last()
⋮----
pub fn find_path(&self, index: usize) -> Option<Proof<'_>> {
⋮----
if lsib.is_some() || rsib.is_some() {
path.push(ProofEntry::new(target, lsib, rsib));
⋮----
if node_index.is_multiple_of(2) {
⋮----
rsib = if node_index + 1 < level.len() {
Some(&level[node_index + 1])
⋮----
Some(&level[node_index])
⋮----
lsib = Some(&level[node_index - 1]);
⋮----
Some(path)
⋮----
mod tests {
⋮----
fn test_tree_from_empty() {
⋮----
assert_eq!(mt.get_root(), None);
⋮----
fn test_tree_from_one() {
⋮----
let expected = hash_leaf!(input);
assert_eq!(mt.get_root(), Some(&expected));
⋮----
fn test_tree_from_many() {
⋮----
.unwrap();
⋮----
.map(Hash::new_from_array)
⋮----
fn test_path_creation() {
⋮----
for (i, _s) in TEST.iter().enumerate() {
let _path = mt.find_path(i).unwrap();
⋮----
fn test_path_creation_bad_index() {
⋮----
assert_eq!(mt.find_path(TEST.len()), None);
⋮----
fn test_path_verify_good() {
⋮----
for (i, s) in TEST.iter().enumerate() {
let hash = hash_leaf!(s);
let path = mt.find_path(i).unwrap();
assert!(path.verify(hash));
⋮----
fn test_path_verify_bad() {
⋮----
for (i, s) in BAD.iter().enumerate() {
⋮----
assert!(!path.verify(hash));
⋮----
fn test_proof_entry_instantiation_lsib_set() {
ProofEntry::new(&Hash::default(), Some(&Hash::default()), None);
⋮----
fn test_proof_entry_instantiation_rsib_set() {
ProofEntry::new(&Hash::default(), None, Some(&Hash::default()));
⋮----
fn test_nodes_capacity_compute() {
⋮----
let iter_count = iteration_count(leaf_count);
assert!(math_count >= iter_count);
⋮----
fn test_proof_entry_instantiation_both_clear() {
⋮----
fn test_proof_entry_instantiation_both_set() {
⋮----
Some(&Hash::default()),

================
File: merkle-tree/.gitignore
================
/target/
/farf/

================
File: merkle-tree/Cargo.toml
================
[package]
name = "solana-merkle-tree"
description = "Solana Merkle Tree"
documentation = "https://docs.rs/solana-merkle-tree"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_merkle_tree"

[features]
agave-unstable-api = []

[dependencies]
fast-math = { workspace = true }
solana-hash = { workspace = true }
solana-sha256-hasher = { workspace = true }

[dev-dependencies]
hex = { workspace = true }

================
File: metrics/benches/metrics.rs
================
fn bench_write_points(b: &mut Bencher) {
⋮----
.map(|_| {
⋮----
.add_field_i64("i", 0)
.add_field_i64("abc123", 2)
.add_field_i64("this-is-my-very-long-field-name", 3)
.clone()
⋮----
.collect();
⋮----
b.iter(|| {
⋮----
black_box(serialize_points(&points, host_id));
⋮----
fn bench_datapoint_submission(b: &mut Bencher) {
⋮----
agent.submit(
⋮----
.add_field_i64("i", i)
.to_owned(),
⋮----
agent.flush();
⋮----
fn bench_counter_submission(b: &mut Bencher) {
⋮----
agent.submit_counter(CounterPoint::new("counter 1"), Level::Info, i);
⋮----
fn bench_random_submission(b: &mut Bencher) {
⋮----
let die = Uniform::<i32>::try_from(1..7).expect("ok for non-empty range");
⋮----
let dice = die.sample(&mut rng);
⋮----
.add_field_i64("i", i as i64)
⋮----
benchmark_group!(
⋮----
benchmark_main!(benches);

================
File: metrics/scripts/grafana-provisioning/dashboards/cluster-monitor.json
================
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": "-- Grafana --",
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "gnetId": null,
  "graphTooltip": 0,
  "id": 2296,
  "iteration": 1616028833304,
  "links": [
    {
      "icon": "dashboard",
      "tags": [],
      "title": "Beta",
      "type": "link",
      "url": "https://internal-metrics.solana.com:3000/d/monitor-beta/cluster-telemetry-beta"
    },
    {
      "icon": "dashboard",
      "tags": [],
      "title": "Edge",
      "type": "link",
      "url": "https://internal-metrics.solana.com:3000/d/monitor-edge/cluster-telemetry-edge"
    }
  ],
  "panels": [
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "panels": [],
      "title": "Summary",
      "type": "row"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 12,
        "x": 0,
        "y": 1
      },
      "id": 2,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT last(\"leader\") FROM \"$testnet\".\"autogen\".\"replay_stage-new_leader\" WHERE $timeFilter ORDER BY \"time\" desc \n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Current Leader",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "first"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "decimals": null,
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 2,
        "x": 12,
        "y": 1
      },
      "id": 3,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT LAST(median) FROM ( SELECT median(num_nodes) - median(num_nodes_dead) as median FROM \"$testnet\".\"autogen\".\"cluster_nodes_broadcast\" WHERE $timeFilter AND live_count > 0 GROUP BY time(5s) )\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Node Count",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "current"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "decimals": null,
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 4,
        "x": 0,
        "y": 3
      },
      "id": 5,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": true
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT ROUND(MEAN(\"sum\")) FROM ( SELECT MEDIAN(tx_count) AS sum FROM (SELECT SUM(\"count\")  AS tx_count FROM \"$testnet\".\"autogen\".\"bank-process_transactions\" WHERE $timeFilter AND count > 0 GROUP BY time(1s), host_id) GROUP BY time(1s) )\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Mean Transactions per Second",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 4,
        "x": 4,
        "y": 3
      },
      "id": 6,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"median_sum\") FROM ( SELECT MEDIAN(tx_count) AS median_sum FROM (SELECT SUM(\"count\") AS tx_count FROM \"$testnet\".\"autogen\".\"bank-process_transactions\" WHERE $timeFilter AND count > 0 GROUP BY time(1s), host_id) GROUP BY time(1s) )\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Max Transactions per Second",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 3,
        "x": 8,
        "y": 3
      },
      "id": 7,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MEDIAN(tx_count) AS transactions FROM (SELECT SUM(\"count\")  AS tx_count FROM \"$testnet\".\"autogen\".\"bank-process_transactions\" WHERE $timeFilter GROUP BY host_id) WHERE tx_count > 0\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Total Transactions",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "ms",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 3,
        "x": 0,
        "y": 5
      },
      "id": 9,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"duration_ms\") FROM \"$testnet\".\"autogen\".\"validator-confirmation\" WHERE $timeFilter \n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Mean Confirmation",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "ms",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 3,
        "x": 3,
        "y": 5
      },
      "id": 10,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT median(\"duration_ms\") FROM \"$testnet\".\"autogen\".\"validator-confirmation\" WHERE $timeFilter \n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Median Confirmation",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "ms",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 3,
        "x": 6,
        "y": 5
      },
      "id": 11,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT min(\"duration_ms\") FROM \"$testnet\".\"autogen\".\"validator-confirmation\" WHERE $timeFilter \n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Min Confirmation",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "min"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "ms",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 2,
        "x": 9,
        "y": 5
      },
      "id": 12,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT max(\"duration_ms\") FROM \"$testnet\".\"autogen\".\"validator-confirmation\" WHERE $timeFilter \n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Max Confirmation",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "ms",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 3,
        "x": 11,
        "y": 5
      },
      "id": 13,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT percentile(\"duration_ms\", 99) FROM \"$testnet\".\"autogen\".\"validator-confirmation\" WHERE $timeFilter \n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Confirmation (99th percentile)",
      "type": "singlestat",
      "valueFontSize": "70%",
      "valueMaps": [
        {
          "op": "=",
          "text": "N/A",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "aliasColors": {},
      "bars": true,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "decimals": null,
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 10,
        "x": 14,
        "y": 6
      },
      "hiddenSeries": false,
      "id": 14,
      "legend": {
        "alignAsTable": true,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT median(\"broadcast_count\") AS \"broadcast_total\" FROM \"$testnet\".\"autogen\".\"cluster_info-num_nodes\" WHERE $timeFilter AND broadcast_count > 0 GROUP BY time(5s)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT median(\"live_count\") AS \"live_total\" FROM \"$testnet\".\"autogen\".\"cluster_info-num_nodes\" WHERE $timeFilter AND live_count > 0 GROUP BY time(5s)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Node Count",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": true,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 9,
        "w": 14,
        "x": 0,
        "y": 7
      },
      "hiddenSeries": false,
      "id": 15,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") / 2 AS \"packets\" FROM \"$testnet\".\"autogen\".\"packets-recv_count\" WHERE $timeFilter GROUP BY time(2s) FILL(0)\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") / 2 AS \"errors\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error_count\" WHERE $timeFilter GROUP BY time(2s) FILL(0)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "select median(\"tx_count\") as \"transactions\" from (select sum(\"count\") / 2  as \"tx_count\" from \"$testnet\".\"autogen\".\"bank-process_transactions\" where $timeFilter AND count > 0 GROUP BY time(2s), host_id) group by time(2s) fill(0)",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": true,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") / 2 AS \"transactions\" FROM \"$testnet\".\"autogen\".\"banking_stage-record_transactions\" WHERE $timeFilter GROUP BY time(2s) FILL(0)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": true,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "select median(\"tx_count\") as \"transactions\" from (select sum(\"count\") / 2  as \"tx_count\" from \"$testnet\".\"autogen\".\"replay_stage-replay_transactions\" where $timeFilter AND count > 0 GROUP BY time(2s), host_id) group by time(2s) fill(0)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Transactions",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": true,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 10,
        "x": 14,
        "y": 11
      },
      "hiddenSeries": false,
      "id": 16,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": false,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"duration_ms\") FROM \"$testnet\".\"autogen\".\"validator-confirmation\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Validator Confirmation Time ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 1,
          "format": "ms",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 16
      },
      "id": 17,
      "panels": [],
      "title": "Stability",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 3,
        "w": 8,
        "x": 0,
        "y": 17
      },
      "hiddenSeries": false,
      "id": 18,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": true,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MEAN(\"points_written\") as \"mean\" FROM \"$testnet\".\"autogen\".\"metrics\" WHERE $timeFilter GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"points_written\") as \"max\" FROM \"$testnet\".\"autogen\".\"metrics\" WHERE $timeFilter GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Metric Datapoints per node",
      "tooltip": {
        "shared": true,
        "sort": 1,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": "0.2",
          "show": true
        },
        {
          "decimals": null,
          "format": "short",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 7,
        "x": 8,
        "y": 17
      },
      "hiddenSeries": false,
      "id": 19,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"cpu_usage\") as \"cpu_usage\" FROM \"$testnet\".\"autogen\".\"system-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT max(\"ram_usage\") as \"ram_usage\" FROM \"$testnet\".\"autogen\".\"system-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"avg_gpu_usage\") as \"gpu_usage\" FROM \"$testnet\".\"autogen\".\"system-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"avg_gpu_mem_usage\") as \"gpu_memory_usage\" FROM \"$testnet\".\"autogen\".\"system-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Resource Usage",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "percent",
          "label": null,
          "logBase": 1,
          "max": "100",
          "min": "0",
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 4,
        "x": 15,
        "y": 17
      },
      "id": 20,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT SUM(\"points_written\") FROM \"$testnet\".\"autogen\".\"metrics\" WHERE $timeFilter\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Total Datapoints",
      "type": "singlestat",
      "valueFontSize": "80%",
      "valueMaps": [
        {
          "op": "=",
          "text": "None",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 3,
        "x": 19,
        "y": 17
      },
      "id": 21,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT SUM(\"points_lost\") FROM \"$testnet\".\"autogen\".\"metrics\" WHERE $timeFilter\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Lost Datapoints",
      "type": "singlestat",
      "valueFontSize": "80%",
      "valueMaps": [
        {
          "op": "=",
          "text": "None",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "cacheTimeout": null,
      "colorBackground": false,
      "colorValue": false,
      "colors": [
        "#299c46",
        "rgba(237, 129, 40, 0.89)",
        "#d44a3a"
      ],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "format": "none",
      "gauge": {
        "maxValue": 100,
        "minValue": 0,
        "show": false,
        "thresholdLabels": false,
        "thresholdMarkers": true
      },
      "gridPos": {
        "h": 2,
        "w": 2,
        "x": 22,
        "y": 17
      },
      "id": 22,
      "interval": null,
      "links": [],
      "mappingType": 1,
      "mappingTypes": [
        {
          "name": "value to text",
          "value": 1
        },
        {
          "name": "range to text",
          "value": 2
        }
      ],
      "maxDataPoints": 100,
      "nullPointMode": "connected",
      "nullText": null,
      "postfix": "",
      "postfixFontSize": "50%",
      "prefix": "",
      "prefixFontSize": "50%",
      "rangeMaps": [
        {
          "from": "null",
          "text": "N/A",
          "to": "null"
        }
      ],
      "sparkline": {
        "fillColor": "rgba(31, 118, 189, 0.18)",
        "full": false,
        "lineColor": "rgb(31, 120, 193)",
        "show": false
      },
      "tableColumn": "sum",
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"killed\") FROM \"$testnet\".\"autogen\".\"oom-killer\" WHERE $timeFilter",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": "",
      "title": "Total OOMs",
      "type": "singlestat",
      "valueFontSize": "80%",
      "valueMaps": [
        {
          "op": "=",
          "text": "None",
          "value": "null"
        }
      ],
      "valueName": "avg"
    },
    {
      "aliasColors": {},
      "bars": true,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 4,
        "w": 5,
        "x": 15,
        "y": 19
      },
      "hiddenSeries": false,
      "id": 23,
      "legend": {
        "alignAsTable": true,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"one\") as \".\" FROM \"$testnet\".\"autogen\".\"panic\" WHERE $timeFilter   GROUP BY time($__interval), \"program\" fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Panics",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": true,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 4,
        "w": 4,
        "x": 20,
        "y": 19
      },
      "hiddenSeries": false,
      "id": 24,
      "legend": {
        "alignAsTable": true,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"killed\") as \".\" FROM \"$testnet\".\"autogen\".\"oom-killer\" WHERE $timeFilter   GROUP BY time($__interval), \"victim\", \"hostname\" fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "OOMs",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 3,
        "w": 8,
        "x": 0,
        "y": 20
      },
      "hiddenSeries": false,
      "id": 25,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": true,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MEAN(\"count\") as \"mean\" FROM \"$testnet\".\"autogen\".\"open-files\" WHERE $timeFilter GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"count\") as \"max\" FROM \"$testnet\".\"autogen\".\"open-files\" WHERE $timeFilter GROUP BY time(5s) fill(null)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Open Files per node",
      "tooltip": {
        "shared": true,
        "sort": 1,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": "0.2",
          "show": true
        },
        {
          "decimals": null,
          "format": "short",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "columns": [],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fontSize": "100%",
      "gridPos": {
        "h": 6,
        "w": 15,
        "x": 0,
        "y": 23
      },
      "id": 26,
      "links": [],
      "pageSize": null,
      "scroll": true,
      "showHeader": true,
      "sort": {
        "col": 1,
        "desc": true
      },
      "styles": [
        {
          "alias": "Time",
          "align": "auto",
          "dateFormat": "YYYY-MM-DD HH:mm:ss",
          "pattern": "Time",
          "type": "date"
        },
        {
          "alias": "",
          "align": "auto",
          "colorMode": null,
          "colors": [
            "rgba(245, 54, 54, 0.9)",
            "rgba(237, 129, 40, 0.89)",
            "rgba(50, 172, 45, 0.97)"
          ],
          "decimals": 0,
          "pattern": "/.*/",
          "thresholds": [],
          "type": "number",
          "unit": "none"
        }
      ],
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT host_id, error, slot FROM \"$testnet\".\"autogen\".\"replay-stage-mark_dead_slot\"  WHERE $timeFilter ORDER BY time DESC ",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "title": "Dead Slots",
      "transform": "table",
      "type": "table-old"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 9,
        "x": 15,
        "y": 23
      },
      "hiddenSeries": false,
      "id": 27,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"total_errors\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error_count\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"account_in_use\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-account_in_use\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"account_loaded_twice\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-account_loaded_twice\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"account_not_found\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-account_not_found\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n\n\n",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"blockhash_not_found\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error-blockhash_not_found\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\")  AS \"blockhash_too_old\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error-blockhash_too_old\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"already_processed\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error-already_processed\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n\n\n\n",
          "rawQuery": true,
          "refId": "G",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"instruction_error\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error-instruction_error\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "H",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"insufficient_funds\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error-insufficient_funds\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "I",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"invalid_account_for_fee\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error-invalid_account_for_fee\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "J",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"invalid_account_index\" FROM \"$testnet\".\"autogen\".\"bank-process_transactions-error-invalid_account_index\" WHERE $timeFilter GROUP BY time(1s) FILL(0)\n",
          "rawQuery": true,
          "refId": "K",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Bank Errors",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "columns": [],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fontSize": "100%",
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 29
      },
      "id": 28,
      "links": [],
      "pageSize": null,
      "scroll": true,
      "showHeader": true,
      "sort": {
        "col": null,
        "desc": false
      },
      "styles": [
        {
          "alias": "Time",
          "align": "auto",
          "dateFormat": "YYYY-MM-DD HH:mm:ss",
          "pattern": "Time",
          "type": "date"
        },
        {
          "alias": "",
          "align": "auto",
          "colorMode": null,
          "colors": [
            "rgba(245, 54, 54, 0.9)",
            "rgba(237, 129, 40, 0.89)",
            "rgba(50, 172, 45, 0.97)"
          ],
          "decimals": 2,
          "pattern": "/.*/",
          "thresholds": [],
          "type": "number",
          "unit": "short"
        }
      ],
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT time, host_id,program,thread, message FROM \"$testnet\".\"autogen\".\"panic\"  WHERE $timeFilter ORDER BY time DESC ",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "title": "Panic Details",
      "transform": "table",
      "type": "table-old"
    },
    {
      "columns": [],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fontSize": "100%",
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 29
      },
      "id": 29,
      "links": [],
      "pageSize": null,
      "scroll": true,
      "showHeader": true,
      "sort": {
        "col": null,
        "desc": false
      },
      "styles": [
        {
          "alias": "Time",
          "align": "auto",
          "dateFormat": "YYYY-MM-DD HH:mm:ss",
          "pattern": "Time",
          "type": "date"
        },
        {
          "alias": "",
          "align": "auto",
          "colorMode": null,
          "colors": [
            "rgba(245, 54, 54, 0.9)",
            "rgba(237, 129, 40, 0.89)",
            "rgba(50, 172, 45, 0.97)"
          ],
          "decimals": 2,
          "pattern": "/.*/",
          "thresholds": [],
          "type": "number",
          "unit": "short"
        }
      ],
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT host_id,error FROM \"$testnet\".\"autogen\".\"validator_process_entry_error\"  WHERE $timeFilter ORDER BY time DESC ",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "title": "Unexpected Validator Process Entry Errors",
      "transform": "table",
      "type": "table-old"
    },
    {
      "columns": [],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fontSize": "100%",
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 29
      },
      "id": 30,
      "links": [],
      "pageSize": null,
      "scroll": true,
      "showHeader": true,
      "sort": {
        "col": null,
        "desc": false
      },
      "styles": [
        {
          "alias": "Time",
          "align": "auto",
          "dateFormat": "YYYY-MM-DD HH:mm:ss",
          "pattern": "Time",
          "type": "date"
        },
        {
          "alias": "",
          "align": "auto",
          "colorMode": null,
          "colors": [
            "rgba(245, 54, 54, 0.9)",
            "rgba(237, 129, 40, 0.89)",
            "rgba(50, 172, 45, 0.97)"
          ],
          "decimals": 2,
          "pattern": "/.*/",
          "thresholds": [],
          "type": "number",
          "unit": "short"
        }
      ],
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT host_id,error FROM \"$testnet\".\"autogen\".\"blockstore_error\"  WHERE $timeFilter ORDER BY time DESC ",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "title": "Unexpected Blockstore Errors",
      "transform": "table",
      "type": "table-old"
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 35
      },
      "id": 31,
      "panels": [],
      "title": "Validator Streamer",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 36
      },
      "hiddenSeries": false,
      "id": 32,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "broadcast-bank-stats.num_shreds",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"receive_time\") AS \"receive_time\" FROM \"$testnet\".\"autogen\".\"broadcast-process-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"shredding_time\") AS \"shredding_time\" FROM \"$testnet\".\"autogen\".\"broadcast-process-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"slot_broadcast_time\") AS \"slot_broadcast_time\" FROM \"$testnet\".\"autogen\".\"broadcast-process-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"num_data_shreds\") AS \"num_data_shreds\" FROM \"$testnet\".\"autogen\".\"broadcast-process-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Broadcast Receive/Shredding Stats ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 36
      },
      "hiddenSeries": false,
      "id": 33,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "broadcast-bank-stats.num_shreds",
          "yaxis": 2
        },
        {
          "alias": "broadcast-transmit-shreds-stats.num_data_and_coding_shreds",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"transmit_elapsed\") AS \"transmit_elapsed\" FROM \"$testnet\".\"autogen\".\"broadcast-transmit-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"send_mmsg_elapsed\") AS \"send_mmsg_elapsed\" FROM \"$testnet\".\"autogen\".\"broadcast-transmit-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"get_peers_elapsed\") AS \"get_peers_elapsed\" FROM \"$testnet\".\"autogen\".\"broadcast-transmit-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"num_shreds\") AS \"num_data_and_coding_shreds\" FROM \"$testnet\".\"autogen\".\"broadcast-transmit-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"end_to_end_elapsed\") AS \"end_to_end_elapsed\" FROM \"$testnet\".\"autogen\".\"broadcast-transmit-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Broadcast Transmit Stats ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 36
      },
      "hiddenSeries": false,
      "id": 34,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "broadcast-bank-stats.num_shreds",
          "yaxis": 2
        },
        {
          "alias": "broadcast-insert-shreds-stats.num_data_and_coding_shreds",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"insert_shreds_elapsed\") AS \"insert_shreds_elapsed\" FROM \"$testnet\".\"autogen\".\"broadcast-insert-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"num_shreds\") AS \"num_data_and_coding_shreds\" FROM \"$testnet\".\"autogen\".\"broadcast-insert-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"end_to_end_elapsed\") AS \"end_to_end_elapsed\" FROM \"$testnet\".\"autogen\".\"broadcast-insert-shreds-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Broadcast Insert Shreds Stats ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-observed.squash_account": "#0a437c",
        "tower-observed.squash_cache": "#ea6460",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 42
      },
      "hiddenSeries": false,
      "id": 35,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "replay-slot-stats.total_shreds",
          "yaxis": 2
        },
        {
          "alias": "replay-slot-stats.total_entries",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"fetch_entries_time\") AS \"fetch_entries_micros\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"fetch_entries_fail_time\") AS \"fetch_entries_fail_micros\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"entry_poh_verification_time\") AS \"entry_poh_verification_time\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"replay_time\") AS \"replay_time_micros\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"replay_total_elapsed\") AS \"replay_total_elapsed_micros\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"total_entries\") AS \"total_entries\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"total_shreds\") AS \"total_shreds\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"entry_transaction_verification_time\") AS \"entry_transaction_verification_time\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Time spent in ReplayStage ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-observed.squash_account": "#0a437c",
        "tower-observed.squash_cache": "#ea6460",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 42
      },
      "hiddenSeries": false,
      "id": 36,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "replay-slot-stats.total_shreds",
          "yaxis": 2
        },
        {
          "alias": "replay-slot-stats.total_entries",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"compute_bank_stats_elapsed\") AS \"compute_bank_stats_elapsed\" FROM \"$testnet\".\"autogen\".\"replay-loop-timing-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"select_vote_and_reset_forks_elapsed\") AS \"select_vote_and_reset_forks_elapsed\" FROM \"$testnet\".\"autogen\".\"replay-loop-timing-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Replay Loop Time Elapsed ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-observed.squash_account": "#0a437c",
        "tower-observed.squash_cache": "#ea6460",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 42
      },
      "hiddenSeries": false,
      "id": 37,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "replay-slot-stats.total_shreds",
          "yaxis": 2
        },
        {
          "alias": "replay-slot-stats.total_entries",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"repair-total\") AS \"repair-total\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"shred-count\") AS \"shred-count\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"highest-shred-count\") AS \"highest-shred-count\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"orphan-count\") AS \"orphan-count\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"repair-highest-slot\") AS \"repair-highest-slot\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"repair-orphan\") AS \"repair-orphan\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Repair Stats",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 48
      },
      "hiddenSeries": false,
      "id": 38,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"count\") AS \"sigverify-recv\" FROM \"$testnet\".\"autogen\".\"sigverify_stage-packets_received\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"banking\" FROM \"$testnet\".\"autogen\".\"banking_stage-entries_received\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"record\" FROM \"$testnet\".\"autogen\".\"record_stage-signals_received\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"lwrite\" FROM \"$testnet\".\"autogen\".\"ledger_writer_stage-entries_received\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"fetchstage\" FROM \"$testnet\".\"autogen\".\"packets-recv_count\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "G",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"broadcast\" FROM \"$testnet\".\"autogen\".\"broadcast_service-entries_received\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"banking\" FROM \"$testnet\".\"autogen\".\"banking_stage-process_packets\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"sigverify-send\" FROM \"$testnet\".\"autogen\".\"sigverify_stage-verified_packets_send\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "H",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"retransmit\" FROM \"$testnet\".\"autogen\".\"retransmit-stage\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval) FILL(0)",
          "rawQuery": true,
          "refId": "I",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Channel Pressure ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "banking_stage-buffered_packets.sum": "#3f6833",
        "banking_stage-consumed_buffered_packets.last": "#65c5db",
        "banking_stage-consumed_buffered_packets.sum": "#614d93",
        "banking_stage-forwarded_packets.last": "#f9ba8f",
        "banking_stage-forwarded_packets.sum": "#b7dbab",
        "banking_stage-rebuffered_packets.last": "#e5a8e2",
        "cluster-info.repair": "#ba43a9",
        "fetch_stage-discard_forwards.sum": "#00ffbb",
        "fetch_stage-honor_forwards.sum": "#bf1b00",
        "poh_recorder-record_lock_contention.sum": "#5195ce",
        "poh_recorder-tick_lock_contention.sum": "#962d82",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-vote.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 48
      },
      "hiddenSeries": false,
      "id": 39,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 2,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"count\") FROM \"$testnet\".\"autogen\".\"poh_recorder-tick_lock_contention\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"count\") FROM \"$testnet\".\"autogen\".\"poh_recorder-record_lock_contention\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"count\") FROM \"$testnet\".\"autogen\".\"poh_recorder-tick_overhead\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"count\") FROM \"$testnet\".\"autogen\".\"poh_recorder-record_ms\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "PoH Lock Contention ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-observed.squash_account": "#0a437c",
        "tower-observed.squash_cache": "#ea6460",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 48
      },
      "hiddenSeries": false,
      "id": 40,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "replay-slot-stats.total_shreds",
          "yaxis": 2
        },
        {
          "alias": "replay-slot-stats.total_entries",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"set-root-elapsed\") AS \"set-root-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"get-votes-elapsed\") AS \"get-votes-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"add-votes-elapsed\") AS \"add-votes-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"get-best-orphans-elapsed\") AS \"get-best-orphans-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"get-best-shreds-elapsed\") AS \"get-best-shreds-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"update-completed-slots-elapsed\") AS \"update-completed-slots-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"update-completed-slots-elapsed\") AS \"update-completed-slots-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"send-repairs-elapsed\") AS \"send-repairs-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"lowest-slot-elapsed\") AS \"lowest-slot-elapsed\" FROM \"$testnet\".\"autogen\".\"serve_repair-repair-timing\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "G",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Repair Timing",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "banking_stage-buffered_packets.sum": "#3f6833",
        "banking_stage-consumed_buffered_packets.last": "#65c5db",
        "banking_stage-consumed_buffered_packets.sum": "#614d93",
        "banking_stage-forwarded_packets.last": "#f9ba8f",
        "banking_stage-forwarded_packets.sum": "#b7dbab",
        "banking_stage-rebuffered_packets.last": "#e5a8e2",
        "cluster-info.repair": "#ba43a9",
        "fetch_stage-discard_forwards.sum": "#00ffbb",
        "fetch_stage-honor_forwards.sum": "#bf1b00",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-vote.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 0,
        "y": 54
      },
      "hiddenSeries": false,
      "id": 41,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 2,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"count\") FROM \"$testnet\".\"autogen\".\"banking_stage-buffered_packets\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") FROM \"$testnet\".\"autogen\".\"banking_stage-forwarded_packets\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") FROM \"$testnet\".\"autogen\".\"banking_stage-consumed_buffered_packets\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") FROM \"$testnet\".\"autogen\".\"banking_stage-rebuffered_packets\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") FROM \"$testnet\".\"autogen\".\"fetch_stage-honor_forwards\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") FROM \"$testnet\".\"autogen\".\"fetch_stage-discard_forwards\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Transaction Forwards ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 8,
        "y": 54
      },
      "hiddenSeries": false,
      "id": 42,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"count\") AS \"window receive\" FROM \"$testnet\".\"autogen\".\"streamer-recv_window-recv\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT sum(\"count\") AS \"window receive\" FROM \"$testnet\".\"autogen\".\"streamer-recv_window-consume\" WHERE $timeFilter GROUP BY time($__interval) FILL(0)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Window",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-observed.squash_account": "#0a437c",
        "tower-observed.squash_cache": "#ea6460",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 16,
        "y": 54
      },
      "hiddenSeries": false,
      "id": 43,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"count\") FROM \"$testnet\".\"autogen\".\"bank-forks_set_root_ms\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"squash_accounts_ms\")  AS \"squash_account\" FROM \"$testnet\".\"autogen\".\"tower-observed\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"count\")  AS \"serialize_bank\" FROM \"$testnet\".\"autogen\".\"bank-serialize-ms\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"count\")  AS \"add_snapshot_ms\" FROM \"$testnet\".\"autogen\".\"add-snapshot-ms\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"duration\")  AS \"serialize_account_storage\" FROM \"$testnet\".\"autogen\".\"serialize_account_storage_ms\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"squash_cache_ms\")  AS \"squash_cache\" FROM \"$testnet\".\"autogen\".\"tower-observed\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Time spent in squashing ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "ms",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 59
      },
      "hiddenSeries": false,
      "id": 44,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "hideEmpty": true,
        "hideZero": true,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 1,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT max(\"num_blocks_on_fork\") as \"num_blocks\" FROM \"$testnet\".\"autogen\".\"blocks_produced\" WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT max(\"num_dropped_blocks_on_fork\") as \"num_dropped_blocks\" FROM \"$testnet\".\"autogen\".\"blocks_produced\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Block Production ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "description": "(Must pick a host id for this to make sense)",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 59
      },
      "hiddenSeries": false,
      "id": 45,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT count(\"slot\") AS \"num_my_leader_slots\" FROM \"$testnet\".\"autogen\".\"replay_stage-my_leader_slot\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "My Leader Slots ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 59
      },
      "hiddenSeries": false,
      "id": 46,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"recovered\") AS \"recovered\" FROM \"$testnet\".\"autogen\".\"blockstore-erasure\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval) FILL(0)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"num_recovered\") AS \"num_recovered\" FROM \"$testnet\".\"autogen\".\"recv-window-insert-shreds\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval) FILL(0)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Erasure Recovery ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "poh-service.hashes": "#c15c17"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 65
      },
      "hiddenSeries": false,
      "id": 47,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "hideEmpty": true,
        "hideZero": true,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 1,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "$$hashKey": "object:373",
          "alias": "poh-service.elapsed_ms",
          "yaxis": 2
        },
        {
          "$$hashKey": "object:425",
          "alias": "poh-service.total_sleep_us",
          "yaxis": 2
        },
        {
          "$$hashKey": "object:426",
          "alias": "poh-service.total_tick_time_us",
          "yaxis": 2
        },
        {
          "$$hashKey": "object:448",
          "alias": "poh-service.total_hash_time_us",
          "yaxis": 2
        },
        {
          "$$hashKey": "object:449",
          "alias": "poh-service.total_lock_time_us",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"ticks\") as \"ticks\" FROM \"$testnet\".\"autogen\".\"poh-service\" WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"hashes\") as \"hashes\" FROM \"$testnet\".\"autogen\".\"poh-service\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"elapsed_us\") as \"elapsed_ms\" FROM \"$testnet\".\"autogen\".\"poh-service\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"total_sleep_us\") as \"total_sleep_us\" FROM \"$testnet\".\"autogen\".\"poh-service\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"total_tick_time_us\") as \"total_tick_time_us\" FROM \"$testnet\".\"autogen\".\"poh-service\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"total_lock_time_us\") as \"total_lock_time_us\" FROM \"$testnet\".\"autogen\".\"poh-service\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"total_hash_time_us\") as \"total_hash_time_us\" FROM \"$testnet\".\"autogen\".\"poh-service\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "G",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "PoH ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "$$hashKey": "object:380",
          "decimals": 0,
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "$$hashKey": "object:381",
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "poh-service.hashes": "#c15c17"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 65
      },
      "hiddenSeries": false,
      "id": 48,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "hideEmpty": true,
        "hideZero": true,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 1,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "poh-service.elapsed_ms",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"new_push_requests_num\") AS \"new_push_requests_num\" FROM \"$testnet\".\"autogen\".\"cluster_info_stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)\n\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"process_pull_resp_count\") AS \"process_pull_resp_count\" FROM \"$testnet\".\"autogen\".\"cluster_info_stats2\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)\n\n\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"process_pull_resp_len\") AS \"process_pull_resp_len\" FROM \"$testnet\".\"autogen\".\"cluster_info_stats3\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)\n\n\n\n\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"process_pull_response_success\") AS \"process_pull_response_success\" FROM \"$testnet\".\"autogen\".\"cluster_info_stats3\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)\n\n\n\n\n",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"process_pull_response_fail\") AS \"process_pull_response_fail\" FROM \"$testnet\".\"autogen\".\"cluster_info_stats3\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)\n\n\n\n\n",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"push_message_value_count\") AS \"push_message_value_count\" FROM \"$testnet\".\"autogen\".\"cluster_info_stats4\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"new_pull_requests_count\") AS \"new_pull_requests_count\" FROM \"$testnet\".\"autogen\".\"cluster_info_stats4\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "G",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"count\") as \"sent_pull_reqs\" FROM \"$testnet\".\"autogen\".\"gossip_pull_request-sent_requests\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "H",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"count\") as \"dropped_pull_reqs\" FROM \"$testnet\".\"autogen\".\"gossip_pull_request-dropped_requests\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "I",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Gossip ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "ms",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 65
      },
      "hiddenSeries": false,
      "id": 49,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT count(\"slot\") AS \"num_skipped\" FROM \"$testnet\".\"autogen\".\"replay_stage-skip_leader_slot\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Skipped Leader Slots ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 71
      },
      "hiddenSeries": false,
      "id": 50,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT sum(\"count\") AS \"dropped-vote\" FROM \"$testnet\".\"autogen\".\"dropped-vote-hash\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Vote dropped hash mismatch ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 71
      },
      "hiddenSeries": false,
      "id": 51,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "retransmit-stage.total_packets",
          "yaxis": 2
        },
        {
          "alias": "retransmit-stage.repair_total",
          "yaxis": 2
        },
        {
          "alias": "retransmit-stage.discard_total",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"total_time\") AS \"total_time\" FROM \"$testnet\".\"autogen\".\"retransmit-stage\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"total_packets\") AS \"total_packets\" FROM \"$testnet\".\"autogen\".\"retransmit-stage\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"retransmit_total\") AS \"retransmit_total\" FROM \"$testnet\".\"autogen\".\"retransmit-stage\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"compute_turbine\") AS \"compute_turbine\" FROM \"$testnet\".\"autogen\".\"retransmit-stage\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"repair_total\") AS \"repair_total\" FROM \"$testnet\".\"autogen\".\"retransmit-stage\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"discard_total\") AS \"discard_total\" FROM \"$testnet\".\"autogen\".\"retransmit-stage\"  WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "F",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Retransmit ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "poh-service.hashes": "#c15c17"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 71
      },
      "hiddenSeries": false,
      "id": 52,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "hideEmpty": true,
        "hideZero": true,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 1,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "poh-service.elapsed_ms",
          "yaxis": 2
        },
        {
          "alias": "recv-window-insert-shreds.num_shreds",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"num_shreds\") as \"num_shreds\" FROM \"$testnet\".\"autogen\".\"recv-window-insert-shreds\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "H",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"total_elapsed\") as \"total_elapsed\" FROM \"$testnet\".\"autogen\".\"recv-window-insert-shreds\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"insert_lock_elapsed\") as \"insert_lock_elapsed\" FROM \"$testnet\".\"autogen\".\"recv-window-insert-shreds\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"insert_shreds_elapsed\") as \"insert_shreds_elapsed\" FROM \"$testnet\".\"autogen\".\"recv-window-insert-shreds\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"shred_recovery_elapsed\") as \"shred_recovery_elapsed\" FROM \"$testnet\".\"autogen\".\"recv-window-insert-shreds\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Blockstore Insert ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "ms",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 77
      },
      "id": 53,
      "panels": [],
      "title": "Tower Consensus",
      "type": "row"
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 0,
        "y": 78
      },
      "hiddenSeries": false,
      "id": 54,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT last(\"latest\") - last(\"root\") FROM \"$testnet\".\"autogen\".\"tower-vote\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT last(\"slot\") - last(\"root\") FROM \"$testnet\".\"autogen\".\"tower-observed\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Tower Distance in Latest and Root Slot ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-vote.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 8,
        "y": 78
      },
      "hiddenSeries": false,
      "id": 55,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT last(\"root\") FROM \"$testnet\".\"autogen\".\"tower-vote\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT last(\"root\") FROM \"$testnet\".\"autogen\".\"tower-observed\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT max(\"root\") AS \"cluster-root\" FROM \"$testnet\".\"autogen\".\"tower-observed\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Tower Root Slot ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "bank-process_transactions-txs.transactions": "#e5a8e2",
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 16,
        "y": 78
      },
      "hiddenSeries": false,
      "id": 56,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT median(\"slot\") FROM \"$testnet\".\"autogen\".\"replay_stage-new_leader\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT max(\"slot\") FROM \"$testnet\".\"autogen\".\"replay_stage-new_leader\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT min(\"slot\") FROM \"$testnet\".\"autogen\".\"replay_stage-new_leader\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Leader Change",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": "slot",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-vote.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 0,
        "y": 83
      },
      "hiddenSeries": false,
      "id": 57,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT last(\"slot\") FROM \"$testnet\".\"autogen\".\"optimistic_slot\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Optimistic Slots ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-vote.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 8,
        "y": 83
      },
      "hiddenSeries": false,
      "id": 58,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT last(\"slot\") FROM \"$testnet\".\"autogen\".\"optimistic_slot_not_rooted\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Unrooted Optimistic Slots ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-vote.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 8,
        "x": 8,
        "y": 88
      },
      "hiddenSeries": false,
      "id": 60,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT max(\"slot_height\") FROM \"$testnet\".\"autogen\".\"bank-new_from_parent-heights\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"slot_height\") AS \"cluster-height\" FROM \"$testnet\".\"autogen\".\"bank-new_from_parent-heights\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT max(\"slot\") AS \"cluster-replay-slot\" FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT max(\"slot\") FROM \"$testnet\".\"autogen\".\"replay-slot-stats\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Slot Production ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 93
      },
      "id": 61,
      "panels": [],
      "repeat": null,
      "title": "IP Network",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 12,
        "x": 0,
        "y": 94
      },
      "hiddenSeries": false,
      "id": 62,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 1,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"packets_received\") as \"packets_received\" FROM \"$testnet\".\"autogen\".\"net-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"receive_errors\") as \"receive_errors\" FROM \"$testnet\".\"autogen\".\"net-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"rcvbuf_errors\") as \"rcvbuf_errors\" FROM \"$testnet\".\"autogen\".\"net-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"packets_sent\") as \"packets_sent\" FROM \"$testnet\".\"autogen\".\"net-stats\" WHERE  hostname =~ /$hostid/ AND $timeFilter   GROUP BY time(5s) fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "UDP Net Stats ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 12,
        "x": 12,
        "y": 94
      },
      "hiddenSeries": false,
      "id": 63,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "hideEmpty": true,
        "hideZero": true,
        "max": false,
        "min": false,
        "rightSide": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null as zero",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 1,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"in_datagrams_delta\") as \"recv\" FROM \"$testnet\".\"autogen\".\"net-stats-validator\" WHERE $timeFilter   GROUP BY time(1s) fill(null)\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"out_datagrams_delta\") as \"sent\" FROM \"$testnet\".\"autogen\".\"net-stats-validator\" WHERE $timeFilter   GROUP BY time(1s) fill(null)\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Total IP traffic (octets)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 99
      },
      "id": 64,
      "panels": [],
      "title": "Signature Verification",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 12,
        "x": 0,
        "y": 100
      },
      "hiddenSeries": false,
      "id": 65,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {
          "alias": "sigverify_stage-total_verify_time.num_packets",
          "yaxis": 2
        }
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"num_packets\") AS \"num_packets\" FROM \"$testnet\".\"autogen\".\"sigverify_stage-total_verify_time\" WHERE $timeFilter GROUP BY time(500ms) FILL(0)\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"verify_time_ms\") AS \"verify_time\" FROM \"$testnet\".\"autogen\".\"sigverify_stage-total_verify_time\" WHERE $timeFilter GROUP BY time(500ms) FILL(0)\n\n",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"recv_time\") AS \"recv_time\" FROM \"$testnet\".\"autogen\".\"sigverify_stage-total_verify_time\" WHERE $timeFilter GROUP BY time(500ms) FILL(0)\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "sigverify_stage - Batch Verification Time",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "ms",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {
        "cluster-info.repair": "#ba43a9",
        "replay_stage-new_leader.last": "#00ffbb",
        "tower-vote.last": "#00ffbb",
        "window-service.receive": "#b7dbab",
        "window-stage.consumed": "#5195ce"
      },
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 5,
        "w": 12,
        "x": 12,
        "y": 100
      },
      "hiddenSeries": false,
      "id": 66,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 2,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT last(\"slot_height\") as \"slot_height\" FROM \"$testnet\".\"autogen\".\"bank-new_from_parent-heights\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT last(\"block_height\") as \"block_height\" FROM \"$testnet\".\"autogen\".\"bank-new_from_parent-heights\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeRegions": [],
      "timeShift": null,
      "title": "Bank Height / Slot Distance ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "none",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 105
      },
      "id": 67,
      "panels": [],
      "title": "Snapshots",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 106
      },
      "id": 68,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 3,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MEAN(\"slot\")  FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": true,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"slot\") as \"Max\" FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": true,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MIN(\"slot\")  as \"Min\" FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Slot",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": null,
          "format": "none",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 8,
        "y": 106
      },
      "id": 69,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": true,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MIN(\"size\")  as \"Min\" FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"size\")  FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MEAN(\"size\")  FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"size\")  FROM \"$testnet\".\"autogen\".\"snapshot-bank-file\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"size\")  FROM \"$testnet\".\"autogen\".\"snapshot-status-cache-file\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Size",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": null,
          "format": "decbytes",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 16,
        "y": 106
      },
      "id": 70,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 4,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [
        {}
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": true,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MIN(\"duration_ms\")  as \"Min\" FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MAX(\"duration_ms\")  FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT MEAN(\"duration_ms\")  FROM \"$testnet\".\"autogen\".\"snapshot-package\" WHERE $timeFilter GROUP BY time($__interval)\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Duration",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": null,
          "format": "ms",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 6,
        "w": 8,
        "x": 0,
        "y": 112
      },
      "id": 71,
      "legend": {
        "avg": false,
        "current": false,
        "hideEmpty": true,
        "hideZero": true,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null as zero",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 1,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [
        {}
      ],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT max(\"compact_range_us\") as \"max compact range\" FROM \"$testnet\".\"autogen\".\"blockstore-compact\" WHERE host_id::tag =~ /$hostid/ AND  $timeFilter  GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT max(\"delete_range_us\") as \"max delete range\" FROM \"$testnet\".\"autogen\".\"blockstore-purge\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT max(\"write_batch_us\") as \"max write batch\" FROM \"$testnet\".\"autogen\".\"blockstore-purge\" WHERE host_id::tag =~ /$hostid/ AND $timeFilter  GROUP BY time(1s) fill(null)\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Ledger Pruning ($hostid)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "µs",
          "label": "",
          "logBase": 10,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 118
      },
      "id": 72,
      "panels": [],
      "title": "RPC Send Transaction Service",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": true,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 6,
        "w": 12,
        "x": 0,
        "y": 119
      },
      "id": 73,
      "legend": {
        "avg": false,
        "current": false,
        "hideEmpty": false,
        "hideZero": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "connected",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 3,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT SUM(\"len\" )  FROM \"$testnet\".\"autogen\".\"send_transaction_service-queue-size\" WHERE $timeFilter GROUP BY time(5s)\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Pending Transaction Queue Size",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": true,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 6,
        "w": 12,
        "x": 12,
        "y": 119
      },
      "id": 74,
      "legend": {
        "avg": false,
        "current": false,
        "hideEmpty": false,
        "hideZero": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": false,
      "linewidth": 0,
      "links": [],
      "nullPointMode": "connected",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 3,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": true,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT SUM(\"count\") as \"Transaction Retried\" FROM \"$testnet\".\"autogen\".\"send_transaction_service-retry\" WHERE $timeFilter GROUP BY time($__interval)\n\n\n",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT SUM(\"count\")  as \"Transaction Rooted\" FROM \"$testnet\".\"autogen\".\"send_transaction_service-rooted\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT SUM(\"count\")  as \"Transaction Expired\" FROM \"$testnet\".\"autogen\".\"send_transaction_service-expired\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT SUM(\"count\")  as \"Transaction Enqueued\" FROM \"$testnet\".\"autogen\".\"send_transaction_service-enqueue\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT COUNT(\"count\")  as \"Transaction Execution Failed\" FROM \"$testnet\".\"autogen\".\"send_transaction_service-failed\" WHERE $timeFilter GROUP BY time($__interval)",
          "rawQuery": true,
          "refId": "E",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Transaction Events",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "decimals": 0,
          "format": "short",
          "label": "",
          "logBase": 1,
          "max": null,
          "min": "0.1",
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    },
    {
      "collapsed": false,
      "datasource": null,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 125
      },
      "id": 75,
      "panels": [],
      "title": "Bench TPS",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 5,
        "w": 7,
        "x": 0,
        "y": 126
      },
      "id": 76,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": false,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT balance FROM  \"$testnet\".\"autogen\".\"bench-tps-lamport_balance\" WHERE $timeFilter fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Bench TPS Token Balance",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": "0",
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": true,
        "alignLevel": null
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 5,
        "w": 7,
        "x": 7,
        "y": 126
      },
      "id": 77,
      "legend": {
        "alignAsTable": false,
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT mean(\"duration\") as \"Generation Time\" FROM  \"$testnet\".\"autogen\".\"bench-tps-generate_txs\" WHERE $timeFilter GROUP BY time(1s)\n\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"duration\")  as \"Transmit Time\" FROM  \"$testnet\".\"autogen\".\"bench-tps-do_tx_transfers\" WHERE $timeFilter GROUP BY time(1s)",
          "rawQuery": true,
          "refId": "B",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"duration\") as \"Get Blockhash\" FROM  \"$testnet\".\"autogen\".\"bench-tps-get_blockhash\" WHERE $timeFilter GROUP BY time(1s)",
          "rawQuery": true,
          "refId": "C",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        },
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT mean(\"duration\") as \"Get Balance\" FROM  \"$testnet\".\"autogen\".\"bench-tps-get_balance\" WHERE $timeFilter GROUP BY time(1s)",
          "rawQuery": true,
          "refId": "D",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Bench TPS Transaction Activity",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "µs",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": "0",
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": false
        }
      ],
      "yaxis": {
        "align": true,
        "alignLevel": null
      }
    },
    {
      "columns": [],
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fontSize": "100%",
      "gridPos": {
        "h": 5,
        "w": 10,
        "x": 14,
        "y": 126
      },
      "id": 78,
      "links": [],
      "pageSize": null,
      "scroll": true,
      "showHeader": true,
      "sort": {
        "col": null,
        "desc": false
      },
      "styles": [
        {
          "alias": "Time",
          "align": "auto",
          "dateFormat": "YYYY-MM-DD HH:mm:ss",
          "pattern": "Time",
          "type": "date"
        },
        {
          "alias": "",
          "align": "auto",
          "colorMode": null,
          "colors": [
            "rgba(245, 54, 54, 0.9)",
            "rgba(237, 129, 40, 0.89)",
            "rgba(50, 172, 45, 0.97)"
          ],
          "decimals": 2,
          "pattern": "/.*/",
          "thresholds": [],
          "type": "number",
          "unit": "short"
        }
      ],
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "hide": false,
          "orderByTime": "ASC",
          "policy": "default",
          "query": "SELECT * FROM \"$testnet\".\"autogen\".\"ramp-tps\"  WHERE $timeFilter ORDER BY time DESC ",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "table",
          "select": [
            [
              {
                "params": [
                  "value"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "mean"
              }
            ]
          ],
          "tags": []
        }
      ],
      "title": "Ramp TPS Events",
      "transform": "table",
      "type": "table-old"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": "$datasource",
      "fieldConfig": {
        "defaults": {
          "custom": {}
        },
        "overrides": []
      },
      "fill": 1,
      "gridPos": {
        "h": 4,
        "w": 10,
        "x": 0,
        "y": 131
      },
      "id": 79,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": false,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "links": [],
      "nullPointMode": "null",
      "percentage": false,
      "pluginVersion": "7.4.3",
      "pointradius": 5,
      "points": true,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "groupBy": [
            {
              "params": [
                "$__interval"
              ],
              "type": "time"
            },
            {
              "params": [
                "null"
              ],
              "type": "fill"
            }
          ],
          "measurement": "cluster_info-vote-count",
          "orderByTime": "ASC",
          "policy": "autogen",
          "query": "SELECT \"request_amount\" FROM \"$testnet\".\"autogen\".\"faucet-airdrop\" WHERE $timeFilter fill(null)\n\n\n\n",
          "rawQuery": true,
          "refId": "A",
          "resultFormat": "time_series",
          "select": [
            [
              {
                "params": [
                  "count"
                ],
                "type": "field"
              },
              {
                "params": [],
                "type": "sum"
              }
            ]
          ],
          "tags": []
        }
      ],
      "thresholds": [],
      "timeFrom": null,
      "timeShift": null,
      "title": "Airdrops",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "buckets": null,
        "mode": "time",
        "name": null,
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        },
        {
          "format": "short",
          "label": null,
          "logBase": 1,
          "max": null,
          "min": null,
          "show": true
        }
      ],
      "yaxis": {
        "align": false,
        "alignLevel": null
      }
    }
  ],
  "refresh": "30s",
  "schemaVersion": 27,
  "style": "dark",
  "tags": [],
  "templating": {
    "list": [
      {
        "current": {
          "text": "$datasource",
          "value": "$datasource"
        },
        "description": null,
        "error": null,
        "hide": 1,
        "includeAll": false,
        "label": "Data Source",
        "multi": false,
        "name": "datasource",
        "options": [],
        "query": "influxdb",
        "refresh": 1,
        "regex": "",
        "skipUrlSync": false,
        "type": "datasource"
      },
      {
        "allValue": ".*",
        "current": {
          "selected": true,
          "text": "Mainnet Beta",
          "value": "mainnet-beta"
        },
        "datasource": "$datasource",
        "definition": "",
        "description": null,
        "error": null,
        "hide": 1,
        "includeAll": false,
        "label": "Testnet",
        "multi": false,
        "name": "testnet",
        "options": [],
        "query": "show databases",
        "refresh": 1,
        "regex": "(devnet|tds|mainnet-beta|testnet.*)",
        "skipUrlSync": false,
        "sort": 1,
        "tagValuesQuery": "",
        "tags": [],
        "tagsQuery": "",
        "type": "query",
        "useTags": false
      },
      {
        "allValue": ".*",
        "current": {
          "selected": false,
          "text": "All",
          "value": "$__all"
        },
        "datasource": "$datasource",
        "definition": "",
        "description": null,
        "error": null,
        "hide": 0,
        "includeAll": true,
        "label": "HostID",
        "multi": false,
        "name": "hostid",
        "options": [],
        "query": "SELECT DISTINCT(\"id\") FROM \"$testnet\".\"autogen\".\"validator-new\" ",
        "refresh": 2,
        "regex": "",
        "skipUrlSync": false,
        "sort": 1,
        "tagValuesQuery": "",
        "tags": [],
        "tagsQuery": "",
        "type": "query",
        "useTags": false
      }
    ]
  },
  "time": {
    "from": "now-15m",
    "to": "now"
  },
  "timepicker": {
    "refresh_intervals": [
      "10s",
      "30s",
      "1m",
      "5m",
      "15m",
      "1h",
      "1d"
    ],
    "time_options": [
      "5m",
      "15m",
      "1h",
      "6h",
      "12h",
      "24h",
      "2d"
    ]
  },
  "timezone": "",
  "title": "Cluster Telemetry",
  "uid": "monitor",
  "version": 2
}

================
File: metrics/scripts/grafana-provisioning/dashboards/dashboard.yml
================
apiVersion: 1
providers:
- name: 'InfluxDB'
  orgId: 1
  folder: ''
  type: file
  disableDeletion: false
  editable: true
  options:
    path: /etc/grafana/provisioning/dashboards

================
File: metrics/scripts/.gitignore
================
/lib/

================
File: metrics/scripts/adjust-dashboard-for-channel.py
================
dashboard_json = sys.argv[1]
channel = sys.argv[2]
⋮----
data = json.load(read_file)

================
File: metrics/scripts/enable.sh
================
echoSolanaMetricsConfig() {
  declare metrics_config_sh
  metrics_config_sh="$(dirname "${BASH_SOURCE[0]}")"/lib/config.sh
  if [[ ! -f "$metrics_config_sh" ]]; then
    echo "Run start.sh first" >&2
    return 1
  fi
  (
    source "$metrics_config_sh"
    echo "host=http://localhost:8086,db=testnet,u=$INFLUXDB_WRITE_USER,p=$INFLUXDB_WRITE_PASSWORD"
  )
}
SOLANA_METRICS_CONFIG=$(echoSolanaMetricsConfig)
export SOLANA_METRICS_CONFIG
unset -f echoSolanaMetricsConfig
__configure_metrics_sh="$(cd "$(dirname "${BASH_SOURCE[0]}")"/../.. || true; pwd)"/scripts/configure-metrics.sh
if [[ -f $__configure_metrics_sh ]]; then
  source "$__configure_metrics_sh"
fi
__configure_metrics_sh=

================
File: metrics/scripts/grafana.ini
================
[users]
auto_assign_org = true
auto_assign_org_role = Editor
viewers_can_edit = false

[auth.anonymous]
enabled = true

[alerting]
enabled = true
# Makes it possible to turn off alert rule execution but alerting UI is visible
;execute_alerts = true

[explore]
enabled = false

================
File: metrics/scripts/influxdb.conf
================
reporting-disabled = false
bind-address = "127.0.0.1:8088"

[meta]
  dir = "/var/lib/influxdb/meta"
  retention-autocreate = true
  logging-enabled = true

[data]
  dir = "/var/lib/influxdb/data"
  index-version = "inmem"
  wal-dir = "/var/lib/influxdb/wal"
  wal-fsync-delay = "0s"
  query-log-enabled = true
  cache-max-memory-size = 1073741824
  cache-snapshot-memory-size = 26214400
  cache-snapshot-write-cold-duration = "10m0s"
  compact-full-write-cold-duration = "4h0m0s"
  max-series-per-database = 1000000
  max-values-per-tag = 100000
  max-concurrent-compactions = 0
  max-index-log-file-size = 1048576
  trace-logging-enabled = false

[coordinator]
  write-timeout = "10s"
  max-concurrent-queries = 0
  query-timeout = "0s"
  log-queries-after = "0s"
  max-select-point = 0
  max-select-series = 0
  max-select-buckets = 0

[retention]
  enabled = true
  check-interval = "30m0s"

[shard-precreation]
  enabled = true
  check-interval = "10m0s"
  advance-period = "30m0s"

[monitor]
  store-enabled = true
  store-database = "_internal"
  store-interval = "10s"

[subscriber]
  enabled = true
  http-timeout = "30s"
  insecure-skip-verify = false
  ca-certs = ""
  write-concurrency = 40
  write-buffer-size = 1000

[http]
  enabled = true
  bind-address = ":8086"
  auth-enabled = false
  log-enabled = true
  write-tracing = false
  pprof-enabled = true
  debug-pprof-enabled = false
  https-enabled = false
  https-certificate = ""
  https-private-key = ""
  max-row-limit = 0
  max-connection-limit = 0
  shared-secret = ""
realm = "InfluxDB"
  unix-socket-enabled = false
  bind-socket = "/var/run/influxdb.sock"
  max-body-size = 25000000
  access-log-path = ""

[logging]
  format = "auto"
  level = "info"
  suppress-logo = false

[ifql]
  enabled = false
  log-enabled = true
  bind-address = ":8082"

[[graphite]]
  enabled = false
  bind-address = ":2003"
  database = "graphite"
  retention-policy = ""
  protocol = "tcp"
  batch-size = 5000
  batch-pending = 10
  batch-timeout = "1s"
  consistency-level = "one"
  separator = "."
  udp-read-buffer = 0

[[collectd]]
  enabled = false
  bind-address = ":25826"
  database = "collectd"
  retention-policy = ""
  batch-size = 5000
  batch-pending = 10
  batch-timeout = "10s"
  read-buffer = 0
  typesdb = "/usr/share/collectd/types.db"
  security-level = "none"
  auth-file = "/etc/collectd/auth_file"
  parse-multivalue-plugin = "split"

[[opentsdb]]
  enabled = false
  bind-address = ":4242"
  database = "opentsdb"
  retention-policy = ""
  consistency-level = "one"
  tls-enabled = false
  certificate = ""
  batch-size = 1000
  batch-pending = 5
  batch-timeout = "1s"
  log-point-errors = true

[[udp]]
  enabled = false
  bind-address = ":8089"
  database = "udp"
  retention-policy = ""
  batch-size = 5000
  batch-pending = 10
  read-buffer = 0
  batch-timeout = "1s"
  precision = ""

[continuous_queries]
  log-enabled = true
  enabled = true
  query-stats-enabled = false
  run-interval = "1s"

================
File: metrics/scripts/README.md
================
This directory contains scripts to manage a local instance of [InfluxDB OSS](https://docs.influxdata.com/influxdb/v1.6/) and [Grafana](https://grafana.com/docs/v5.2/)

### Setup

Start the local metric services:

`$ ./start.sh`

Metrics are enabled on a per-shell basis which means you must `source` the
following scripts in each shell in which you start an application you wish to
collect metrics from.  For example, if running a Solana validator you must run
`source ./enable.sh` before starting the node:

`$ source ./enable.sh`

Once metrics have been started and you have an application running you can view the metrics at:

http://localhost:3000/dashboards

To test that things are working correctly you can send a test airdrop data point and then check the
metrics dashboard:

`$ ./test.sh`

Stop metric services:

`$ ./stop.sh`

### InfluxDB CLI

You may find it useful to install the InfluxDB client for
adhoc metrics collection/viewing
* Linux - `sudo apt-get install influxdb-client`
* macOS - `brew install influxdb`

Simple example of pulling all airdrop measurements out of the `testnet` database:

```sh
$ influx -database testnet -username read -password read -execute 'SELECT * FROM "faucet-airdrop"'
```

Reference: https://docs.influxdata.com/influxdb/v1.5/query_language/

### Monitoring

To monitor activity, run one of:

```sh
$ docker logs -f influxdb
$ docker logs -f grafana
```

### Reference
* https://hub.docker.com/_/influxdata-influxdb
* https://hub.docker.com/r/grafana/grafana

================
File: metrics/scripts/start.sh
================
set -e
cd "$(dirname "$0")"
# Stop if already running
./stop.sh
randomPassword() {
  declare p=
  for _ in $(seq 0 16); do
    p+="$((RANDOM % 10))"
  done
  echo $p
}
mkdir -p lib
if [[ ! -f lib/config.sh ]]; then
  cat > lib/config.sh <<EOF
INFLUXDB_ADMIN_USER=admin
INFLUXDB_ADMIN_PASSWORD=$(randomPassword)
INFLUXDB_WRITE_USER=write
INFLUXDB_WRITE_PASSWORD=$(randomPassword)
INFLUXDB_READ_USER=read
INFLUXDB_READ_PASSWORD=read
EOF
fi
# shellcheck source=/dev/null
source lib/config.sh
if [[ ! -f lib/grafana-provisioning ]]; then
  cp -va grafana-provisioning lib
  ./adjust-dashboard-for-channel.py \
    lib/grafana-provisioning/dashboards/cluster-monitor.json local
  mkdir -p lib/grafana-provisioning/datasources
  cat > lib/grafana-provisioning/datasources/datasource.yml <<EOF
apiVersion: 1
datasources:
- name: local-influxdb
  type: influxdb
  isDefault: true
  access: proxy
  database: testnet
  user: $INFLUXDB_READ_USER
  password: $INFLUXDB_READ_PASSWORD
  url: http://influxdb:8086
  editable: true
EOF
fi
set -x
: "${INFLUXDB_IMAGE:=influxdb:1.7}"
: "${GRAFANA_IMAGE:=solanalabs/grafana:stable}"
: "${GRAFANA_IMAGE:=grafana/grafana:5.2.3}"
docker pull $INFLUXDB_IMAGE
docker pull $GRAFANA_IMAGE
docker network remove influxdb || true
docker network create influxdb
cat > "$PWD"/lib/influx-env-file <<EOF
INFLUXDB_ADMIN_USER=$INFLUXDB_ADMIN_USER
INFLUXDB_ADMIN_PASSWORD=$INFLUXDB_ADMIN_PASSWORD
INFLUXDB_READ_USER=$INFLUXDB_READ_USER
INFLUXDB_READ_PASSWORD=$INFLUXDB_READ_PASSWORD
INFLUXDB_WRITE_USER=$INFLUXDB_WRITE_USER
INFLUXDB_WRITE_PASSWORD=$INFLUXDB_WRITE_PASSWORD
INFLUXDB_DB=testnet
EOF
mkdir -p lib/influxdb
docker run \
  --detach \
  --name=influxdb \
  --net=influxdb \
  --publish 8086:8086 \
  --user "$(id -u):$(id -g)" \
  --volume "$PWD"/influxdb.conf:/etc/influxdb/influxdb.conf:ro \
  --volume "$PWD"/lib/influxdb:/var/lib/influxdb \
  --env-file "$PWD"/lib/influx-env-file \
  $INFLUXDB_IMAGE -config /etc/influxdb/influxdb.conf /init-influxdb.sh
cat > "$PWD"/lib/grafana-env-file <<EOF
GF_PATHS_CONFIG=/grafana.ini
GF_SECURITY_ADMIN_USER=$INFLUXDB_ADMIN_USER
GF_SECURITY_ADMIN_PASSWORD=$INFLUXDB_ADMIN_PASSWORD
EOF
mkdir -p lib/grafana
docker run \
  --detach \
  --name=grafana \
  --net=influxdb \
  --publish 3000:3000 \
  --user "$(id -u):$(id -g)" \
  --env-file "$PWD"/lib/grafana-env-file \
  --volume "$PWD"/grafana.ini:/grafana.ini:ro \
  --volume "$PWD"/lib/grafana:/var/lib/grafana \
  --volume "$PWD"/lib/grafana-provisioning/:/etc/grafana/provisioning:ro \
  $GRAFANA_IMAGE
sleep 5
./status.sh

================
File: metrics/scripts/status.sh
================
set -e
cd "$(dirname "$0")"
if [[ ! -f lib/config.sh ]]; then
  echo "Run start.sh first"
  exit 1
fi
source lib/config.sh
: "${INFLUXDB_ADMIN_USER:?}"
: "${INFLUXDB_ADMIN_PASSWORD:?}"
: "${INFLUXDB_WRITE_USER:?}"
: "${INFLUXDB_WRITE_PASSWORD:?}"
(
  set -x
  docker ps --no-trunc --size
)
curl_head() {
  curl --retry 5 --retry-delay 2 --retry-connrefused -v --head "$1"
}
if ! curl_head http://localhost:8086/ping; then
  echo Error: InfluxDB not running
  exit 1
fi
if ! curl_head http://localhost:3000; then
  echo Error: Grafana not running
  exit 1
fi
cat <<EOF
=========================================================================
* Grafana url: http://localhost:3000/dashboards
     username: $INFLUXDB_ADMIN_USER
     password: $INFLUXDB_ADMIN_PASSWORD
* Enable metric collection per shell by running:
     export SOLANA_METRICS_CONFIG="host=http://localhost:8086,db=testnet,u=$INFLUXDB_WRITE_USER,p=$INFLUXDB_WRITE_PASSWORD"
EOF

================
File: metrics/scripts/stop.sh
================
set -e
for container in influxdb grafana; do
  if [ "$(docker ps -q -a -f name=$container)" ]; then
    echo Stopping $container
    (
      set +e
      docker rm -f $container
      exit 0
    )
  fi
done

================
File: metrics/scripts/test.sh
================
set -e
cd "$(dirname "$0")"
# shellcheck source=metrics/scripts/enable.sh
source ./enable.sh
if [[ -z $INFLUX_DATABASE || -z $INFLUX_USERNAME || -z $INFLUX_PASSWORD ]]; then
  echo Influx user credentials not found
  exit 0
fi
host="https://localhost:8086"
if [[ -n $INFLUX_HOST ]]; then
  host="$INFLUX_HOST"
fi
set -x
point="faucet-airdrop,localmetrics=test request_amount=1i,request_current=1i"
echo "${host}/write?db=${INFLUX_DATABASE}&u=${INFLUX_USERNAME}&p={$INFLUX_PASSWORD}" \
  | xargs curl -XPOST --data-binary "$point"

================
File: metrics/src/counter.rs
================
pub struct Counter {
⋮----
/// total accumulated value
    pub counts: AtomicUsize,
⋮----
/// last accumulated value logged
    pub lastlog: AtomicUsize,
⋮----
pub struct CounterPoint {
⋮----
impl CounterPoint {
pub fn new(name: &'static str) -> Self {
⋮----
macro_rules! create_counter {
⋮----
macro_rules! inc_counter {
⋮----
macro_rules! inc_counter_info {
⋮----
macro_rules! inc_new_counter {
⋮----
macro_rules! inc_new_counter_error {
⋮----
macro_rules! inc_new_counter_warn {
⋮----
macro_rules! inc_new_counter_info {
⋮----
macro_rules! inc_new_counter_debug {
⋮----
impl Counter {
fn default_metrics_rate() -> u64 {
⋮----
.map(|x| x.parse().unwrap_or(0))
.unwrap_or(0);
⋮----
fn default_log_rate() -> usize {
⋮----
.map(|x| x.parse().unwrap_or(DEFAULT_LOG_RATE))
.unwrap_or(DEFAULT_LOG_RATE);
⋮----
pub fn init(&mut self) {
⋮----
.compare_and_swap(0, Self::default_log_rate(), Ordering::Relaxed);
⋮----
.compare_and_swap(0, Self::default_metrics_rate(), Ordering::Relaxed);
⋮----
pub fn inc(&self, level: log::Level, events: usize) {
⋮----
let counts = self.counts.fetch_add(events, Ordering::Relaxed);
let times = self.times.fetch_add(1, Ordering::Relaxed);
let lograte = self.lograte.load(Ordering::Relaxed);
let metricsrate = self.metricsrate.load(Ordering::Relaxed);
if times.is_multiple_of(lograte) && times > 0 && log_enabled!(level) {
log!(
⋮----
let lastlog = self.lastlog.load(Ordering::Relaxed);
⋮----
.compare_and_swap(lastlog, counts, Ordering::Relaxed);
⋮----
submit_counter(counter, level, bucket);
⋮----
mod tests {
⋮----
fn get_env_lock() -> &'static RwLock<()> {
⋮----
fn try_init_logger_at_level_info() -> Result<(), log::SetLoggerError> {
⋮----
.filter(module_limit, log::LevelFilter::Info)
.is_test(true)
.try_init()
⋮----
fn test_counter() {
try_init_logger_at_level_info().ok();
let _readlock = get_env_lock().read();
let mut counter = create_counter!("test", 1000, 1);
counter.init();
counter.inc(Level::Info, 1);
assert_eq!(counter.counts.load(Ordering::Relaxed), 1);
assert_eq!(counter.times.load(Ordering::Relaxed), 1);
assert_eq!(counter.lograte.load(Ordering::Relaxed), 1000);
assert_eq!(counter.lastlog.load(Ordering::Relaxed), 0);
assert_eq!(counter.name, "test");
⋮----
counter.inc(Level::Info, 2);
⋮----
assert_eq!(counter.lastlog.load(Ordering::Relaxed), 397);
⋮----
assert_eq!(counter.lastlog.load(Ordering::Relaxed), 399);
⋮----
fn test_metricsrate() {
⋮----
let mut counter = create_counter!("test", 1000, 0);
⋮----
assert_eq!(
⋮----
fn test_metricsrate_env() {
⋮----
let _writelock = get_env_lock().write();
⋮----
assert_eq!(counter.metricsrate.load(Ordering::Relaxed), 50);
⋮----
fn test_inc_new_counter() {
⋮----
inc_new_counter_info!("1", 1);
inc_new_counter_info!("2", 1, 3);
inc_new_counter_info!("3", 1, 2, 1);
⋮----
fn test_lograte() {
⋮----
let mut counter = create_counter!("test_lograte", 0, 1);
⋮----
assert_eq!(counter.lograte.load(Ordering::Relaxed), DEFAULT_LOG_RATE);
⋮----
fn test_lograte_env() {
⋮----
assert_ne!(DEFAULT_LOG_RATE, 0);
⋮----
let mut counter = create_counter!("test_lograte_env", 0, 1);
⋮----
assert_eq!(counter.lograte.load(Ordering::Relaxed), 50);
let mut counter2 = create_counter!("test_lograte_env", 0, 1);
⋮----
counter2.init();
assert_eq!(counter2.lograte.load(Ordering::Relaxed), DEFAULT_LOG_RATE);

================
File: metrics/src/datapoint.rs
================
pub struct DataPoint {
⋮----
/// tags are eligible for group-by operations.
    pub tags: Vec<(&'static str, String)>,
⋮----
impl DataPoint {
pub fn new(name: &'static str) -> Self {
⋮----
tags: vec![],
fields: vec![],
⋮----
pub fn add_tag(&mut self, name: &'static str, value: &str) -> &mut Self {
self.tags.push((name, value.to_string()));
⋮----
pub fn add_field_str(&mut self, name: &'static str, value: &str) -> &mut Self {
⋮----
.push((name, format!("\"{}\"", value.replace('\"', "\\\""))));
⋮----
pub fn add_field_bool(&mut self, name: &'static str, value: bool) -> &mut Self {
self.fields.push((name, value.to_string()));
⋮----
pub fn add_field_i64(&mut self, name: &'static str, value: i64) -> &mut Self {
self.fields.push((name, value.to_string() + "i"));
⋮----
pub fn add_field_f64(&mut self, name: &'static str, value: f64) -> &mut Self {
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
write!(f, "datapoint: {}", self.name)?;
⋮----
write!(f, ",{}={}", tag.0, tag.1)?;
⋮----
write!(f, " {}={}", field.0, field.1)?;
⋮----
Ok(())
⋮----
macro_rules! create_datapoint {
⋮----
macro_rules! datapoint {
⋮----
macro_rules! datapoint_error {
⋮----
macro_rules! datapoint_warn {
⋮----
macro_rules! datapoint_info {
⋮----
macro_rules! datapoint_debug {
⋮----
macro_rules! datapoint_trace {
⋮----
mod test {
⋮----
fn test_datapoint() {
datapoint_debug!("name", ("field name", "test", String));
datapoint_info!("name", ("field name", 12.34_f64, f64));
datapoint_trace!("name", ("field name", true, bool));
datapoint_warn!("name", ("field name", 1, i64));
datapoint_error!("name", ("field name", 1, i64),);
datapoint!(
⋮----
datapoint_info!("name", ("field1 name", 2, i64), ("field2 name", 2, i64),);
datapoint_trace!(
⋮----
let point = create_datapoint!(
⋮----
assert_eq!(point.name, "name");
assert_eq!(point.tags.len(), 0);
assert_eq!(point.fields[0], ("i64", "1i".to_string()));
assert_eq!(
⋮----
assert_eq!(point.fields[2], ("f64", "12.34".to_string()));
assert_eq!(point.fields[3], ("bool", "true".to_string()));
⋮----
fn test_optional_datapoint() {
datapoint_debug!("name", ("field name", Some("test"), Option<String>));
datapoint_info!("name", ("field name", Some(12.34_f64), Option<f64>));
datapoint_trace!("name", ("field name", Some(true), Option<bool>));
datapoint_warn!("name", ("field name", Some(1), Option<i64>));
datapoint_error!("name", ("field name", Some(1), Option<i64>),);
datapoint_debug!("name", ("field name", None::<String>, Option<String>));
datapoint_info!("name", ("field name", None::<f64>, Option<f64>));
datapoint_trace!("name", ("field name", None::<bool>, Option<bool>));
datapoint_warn!("name", ("field name", None::<i64>, Option<i64>));
datapoint_error!("name", ("field name", None::<i64>, Option<i64>),);
⋮----
assert_eq!(point.fields[0], ("some_i64", "1i".to_string()));
⋮----
assert_eq!(point.fields[2], ("some_f64", "12.34".to_string()));
assert_eq!(point.fields[3], ("some_bool", "true".to_string()));
assert_eq!(point.fields.len(), 4);
⋮----
fn test_datapoint_with_tags() {
datapoint_debug!("name", "tag" => "tag-value", ("field name", "test", String));
datapoint_info!(
⋮----
datapoint_warn!("name", "tag" => "tag-value");
datapoint_error!("name", "tag" => "tag-value", ("field name", 1, i64),);
⋮----
assert_eq!(point.tags[0], ("tag1", "tag-value-1".to_string()));
assert_eq!(point.tags[1], ("tag2", "tag-value-2".to_string()));
assert_eq!(point.tags[2], ("tag3", "tag-value-3".to_string()));

================
File: metrics/src/lib.rs
================
pub mod counter;
pub mod datapoint;
pub mod metrics;
⋮----
pub struct MovingStat {
⋮----
impl MovingStat {
pub fn update_stat(&self, old_value: &MovingStat, new_value: u64) {
let old = old_value.value.swap(new_value, Ordering::Acquire);
⋮----
.fetch_add(new_value.saturating_sub(old), Ordering::Release);
⋮----
pub fn load_and_reset(&self) -> u64 {
self.value.swap(0, Ordering::Acquire)
⋮----
pub struct TokenCounter(Arc<&'static str>);
impl TokenCounter {
/// Creates a new counter with the specified metrics `name`.
    pub fn new(name: &'static str) -> Self {
⋮----
pub fn new(name: &'static str) -> Self {
Self(Arc::new(name))
⋮----
pub fn create_token(&self) -> CounterToken {
datapoint_info!(*self.0, ("count", Arc::strong_count(&self.0), i64));
CounterToken(self.0.clone())
⋮----
pub struct CounterToken(Arc<&'static str>);
impl Clone for CounterToken {
fn clone(&self) -> Self {
// new_count = strong_count
//    - 1 (in TokenCounter)
//    + 1 (token that's being created)
⋮----
impl Drop for CounterToken {
fn drop(&mut self) {
datapoint_info!(
⋮----
impl Drop for TokenCounter {

================
File: metrics/src/metrics.rs
================
type CounterMap = HashMap<(&'static str, u64), CounterPoint>;
⋮----
pub enum MetricsError {
⋮----
fn from(error: MetricsError) -> Self {
error.to_string()
⋮----
fn from(counter_point: &CounterPoint) -> Self {
⋮----
point.add_field_i64("count", counter_point.count);
⋮----
enum MetricsCommand {
⋮----
pub struct MetricsAgent {
⋮----
pub trait MetricsWriter {
// Write the points and empty the vector.  Called on the internal
// MetricsAgent worker thread.
⋮----
struct InfluxDbMetricsWriter {
⋮----
impl InfluxDbMetricsWriter {
fn new() -> Self {
⋮----
write_url: Self::build_write_url().ok(),
⋮----
fn build_write_url() -> Result<String, MetricsError> {
let config = get_metrics_config().map_err(|err| {
info!("metrics disabled: {err}");
⋮----
info!(
⋮----
let write_url = format!(
⋮----
Ok(write_url)
⋮----
pub fn serialize_points(points: &Vec<DataPoint>, host_id: &str) -> String {
⋮----
const HOST_ID_LEN: usize = 8; // "host_id=".len()
const EXTRA_LEN: usize = 2; // "=,".len()
⋮----
len += name.len() + value.len() + EXTRA_LEN;
⋮----
len += point.name.len();
⋮----
len += host_id.len() + HOST_ID_LEN;
⋮----
let _ = write!(line, "{},host_id={}", &point.name, host_id);
for (name, value) in point.tags.iter() {
let _ = write!(line, ",{name}={value}");
⋮----
for (name, value) in point.fields.iter() {
let _ = write!(line, "{}{}={}", if first { ' ' } else { ',' }, name, value);
⋮----
let timestamp = point.timestamp.duration_since(UNIX_EPOCH);
let nanos = timestamp.unwrap().as_nanos();
let _ = writeln!(line, " {nanos}");
⋮----
impl MetricsWriter for InfluxDbMetricsWriter {
fn write(&self, points: Vec<DataPoint>) {
⋮----
debug!("submitting {} points", points.len());
let host_id = HOST_ID.read().unwrap();
let line = serialize_points(&points, &host_id);
⋮----
.timeout(Duration::from_secs(5))
.build();
⋮----
warn!("client instantiation failed: {err}");
⋮----
let response = client.post(write_url.as_str()).body(line).send();
⋮----
let status = resp.status();
if !status.is_success() {
⋮----
.text()
.unwrap_or_else(|_| "[text body empty]".to_string());
warn!("submit response unsuccessful: {status} {text}",);
⋮----
warn!("submit error: {}", response.unwrap_err());
⋮----
impl Default for MetricsAgent {
fn default() -> Self {
⋮----
.map(|x| {
x.parse()
.expect("Failed to parse SOLANA_METRICS_MAX_POINTS_PER_SECOND")
⋮----
.unwrap_or(4000);
⋮----
impl MetricsAgent {
pub fn new(
⋮----
.name("solMetricsAgent".into())
.spawn(move || Self::run(&receiver, &writer, write_frequency, max_points_per_sec))
.unwrap();
⋮----
// Combines `points` and `counters` into a single array of `DataPoint`s, appending a data point
// with the metrics stats at the end.
//
// Limits the number of produced points to the `max_points` value.  Takes `points` followed by
// `counters`, dropping `counters` first.
⋮----
// `max_points_per_sec` is only used in a warning message.
// `points_buffered` is used in the stats.
fn combine_points(
⋮----
// Reserve one slot for the stats point we will add at the end.
let max_points = max_points.saturating_sub(1);
let num_points = points.len().saturating_add(counters.len());
let fit_counters = max_points.saturating_sub(points.len());
⋮----
debug!("run: attempting to write {num_points} points");
⋮----
warn!(
⋮----
combined.truncate(points_written);
combined.extend(counters.values().take(fit_counters).map(|v| v.into()));
counters.clear();
combined.push(
⋮----
.add_field_i64("points_written", points_written as i64)
.add_field_i64("num_points", num_points as i64)
.add_field_i64("points_lost", (num_points - points_written) as i64)
.add_field_i64("points_buffered", points_buffered as i64)
.add_field_i64("secs_since_last_write", secs_since_last_write as i64)
.to_owned(),
⋮----
// Consumes provided `points`, sending up to `max_points` of them into the `writer`.
⋮----
// Returns an updated value for `last_write_time`.  Which is equal to `Instant::now()`, just
// before `write` in updated.
fn write(
⋮----
let secs_since_last_write = now.duration_since(last_write_time).as_secs();
writer.write(Self::combine_points(
⋮----
fn run(
⋮----
trace!("run: enter");
⋮----
let max_points = write_frequency.as_secs() as usize * max_points_per_sec;
// Bind common arguments in the `Self::write()` call.
⋮----
receiver.len(),
⋮----
match receiver.try_recv() {
⋮----
debug!("metrics_thread: flush");
last_write_time = write(last_write_time, &mut points, &mut counters);
barrier.wait();
⋮----
log!(level, "{point}");
points.push(point);
⋮----
debug!("{counter:?}");
⋮----
if let Some(value) = counters.get_mut(&key) {
⋮----
counters.insert(key, counter);
⋮----
debug!("run: sender disconnected");
⋮----
if now.duration_since(last_write_time) >= write_frequency {
⋮----
debug_assert!(
⋮----
trace!("run: exit");
⋮----
pub fn submit(&self, point: DataPoint, level: log::Level) {
⋮----
.send(MetricsCommand::Submit(point, level))
⋮----
pub fn submit_counter(&self, counter: CounterPoint, level: log::Level, bucket: u64) {
⋮----
.send(MetricsCommand::SubmitCounter(counter, level, bucket))
⋮----
pub fn flush(&self) {
debug!("Flush");
⋮----
.send(MetricsCommand::Flush(Arc::clone(&barrier)))
⋮----
impl Drop for MetricsAgent {
fn drop(&mut self) {
self.flush();
⋮----
fn get_singleton_agent() -> &'static MetricsAgent {
⋮----
let hostname: String = gethostname()
.into_string()
.unwrap_or_else(|_| "".to_string());
format!("{}", hash(hostname.as_bytes()))
⋮----
pub fn set_host_id(host_id: String) {
info!("host id: {host_id}");
*HOST_ID.write().unwrap() = host_id;
⋮----
pub fn get_host_id() -> String {
HOST_ID.read().unwrap().clone()
⋮----
/// Submits a new point from any thread.  Note that points are internally queued
/// and transmitted periodically in batches.
⋮----
/// and transmitted periodically in batches.
pub fn submit(point: DataPoint, level: log::Level) {
⋮----
pub fn submit(point: DataPoint, level: log::Level) {
let agent = get_singleton_agent();
agent.submit(point, level);
⋮----
/// Submits a new counter or updates an existing counter from any thread.  Note that points are
/// internally queued and transmitted periodically in batches.
⋮----
/// internally queued and transmitted periodically in batches.
pub(crate) fn submit_counter(point: CounterPoint, level: log::Level, bucket: u64) {
⋮----
pub(crate) fn submit_counter(point: CounterPoint, level: log::Level, bucket: u64) {
⋮----
agent.submit_counter(point, level, bucket);
⋮----
struct MetricsConfig {
⋮----
impl MetricsConfig {
fn complete(&self) -> bool {
!(self.host.is_empty()
|| self.db.is_empty()
|| self.username.is_empty()
|| self.password.is_empty())
⋮----
fn get_metrics_config() -> Result<MetricsConfig, MetricsError> {
⋮----
if config_var.is_empty() {
Err(env::VarError::NotPresent)?;
⋮----
for pair in config_var.split(',') {
let nv: Vec<_> = pair.split('=').collect();
if nv.len() != 2 {
return Err(MetricsError::ConfigInvalid(pair.to_string()));
⋮----
let v = nv[1].to_string();
⋮----
_ => return Err(MetricsError::ConfigInvalid(pair.to_string())),
⋮----
if !config.complete() {
return Err(MetricsError::ConfigIncomplete);
⋮----
Ok(config)
⋮----
pub fn metrics_config_sanity_check(cluster_type: ClusterType) -> Result<(), MetricsError> {
let config = match get_metrics_config() {
⋮----
Err(MetricsError::VarError(env::VarError::NotPresent)) => return Ok(()),
Err(e) => return Err(e),
⋮----
_ => return Ok(()),
⋮----
let msg = format!("cluster_type={cluster_type:?} host={host} database={db}");
Err(MetricsError::DbMismatch(msg))
⋮----
pub fn query(q: &str) -> Result<String, MetricsError> {
let config = get_metrics_config()?;
let query_url = format!(
⋮----
let response = reqwest::blocking::get(query_url.as_str())?.text()?;
Ok(response)
⋮----
pub fn flush() {
⋮----
agent.flush();
⋮----
pub fn set_panic_hook(program: &'static str, version: Option<String>) {
⋮----
SET_HOOK.call_once(|| {
⋮----
default_hook(ono);
let location = match ono.location() {
Some(location) => location.to_string(),
None => "?".to_string(),
⋮----
submit(
⋮----
.add_field_str("program", program)
.add_field_str("thread", thread::current().name().unwrap_or("?"))
// The 'one' field exists to give Kapacitor Alerts a numerical value
.add_field_i64("one", 1)
.add_field_str("message", &ono.to_string())
.add_field_str("location", &location)
.add_field_str("version", version.as_ref().unwrap_or(&"".to_string()))
⋮----
// Flush metrics immediately
flush();
// Exit cleanly so the process don't limp along in a half-dead state
⋮----
pub mod test_mocks {
⋮----
pub struct MockMetricsWriter {
⋮----
impl MockMetricsWriter {
pub fn new() -> Self {
⋮----
pub fn points_written(&self) -> usize {
self.points_written.lock().unwrap().len()
⋮----
impl Default for MockMetricsWriter {
⋮----
impl MetricsWriter for MockMetricsWriter {
⋮----
assert!(!points.is_empty());
let new_points = points.len();
self.points_written.lock().unwrap().extend(points);
⋮----
mod test {
⋮----
fn test_submit() {
⋮----
let agent = MetricsAgent::new(writer.clone(), Duration::from_secs(10), 1000);
⋮----
agent.submit(
⋮----
.add_field_i64("i", i)
⋮----
assert_eq!(writer.points_written(), 43);
⋮----
fn test_submit_counter() {
⋮----
agent.submit_counter(CounterPoint::new("counter 1"), Level::Info, i);
agent.submit_counter(CounterPoint::new("counter 2"), Level::Info, i);
⋮----
assert_eq!(writer.points_written(), 21);
⋮----
fn test_submit_counter_increment() {
⋮----
agent.submit_counter(
⋮----
assert_eq!(writer.points_written(), 2);
let submitted_point = writer.points_written.lock().unwrap()[0].clone();
assert_eq!(submitted_point.fields[0], ("count", "100i".to_string()));
⋮----
fn test_submit_bucketed_counter() {
⋮----
agent.submit_counter(CounterPoint::new("counter 1"), Level::Info, i / 10);
agent.submit_counter(CounterPoint::new("counter 2"), Level::Info, i / 10);
⋮----
assert_eq!(writer.points_written(), 11);
⋮----
fn test_submit_with_delay() {
⋮----
let agent = MetricsAgent::new(writer.clone(), Duration::from_secs(1), 1000);
agent.submit(DataPoint::new("point 1"), Level::Info);
⋮----
fn test_submit_exceed_max_rate() {
⋮----
let agent = MetricsAgent::new(writer.clone(), Duration::from_secs(1), max_points_per_sec);
⋮----
.add_field_i64("i", i.try_into().unwrap())
⋮----
assert_eq!(writer.points_written(), max_points_per_sec);
⋮----
fn test_multithread_submit() {
⋮----
writer.clone(),
⋮----
point.add_field_i64("i", i);
⋮----
threads.push(thread::spawn(move || {
agent.lock().unwrap().submit(point, Level::Info);
⋮----
thread.join().unwrap();
⋮----
agent.lock().unwrap().flush();
⋮----
fn test_flush_before_drop() {
⋮----
let agent = MetricsAgent::new(writer.clone(), Duration::from_secs(9_999_999), 1000);
⋮----
fn test_live_submit() {
⋮----
.add_field_bool("true", true)
.add_field_bool("random_bool", rand::random::<u8>() < 128)
.add_field_i64("random_int", rand::random::<u8>() as i64)
.to_owned();
agent.submit(point, Level::Info);
⋮----
fn test_host_id() {
let test_host_id = "test_host_123".to_string();
set_host_id(test_host_id.clone());
assert_eq!(get_host_id(), test_host_id);

================
File: metrics/.gitignore
================
/target/
/farf/

================
File: metrics/Cargo.toml
================
[package]
name = "solana-metrics"
description = "Solana Metrics"
documentation = "https://docs.rs/solana-metrics"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "solana_metrics"

[features]
agave-unstable-api = []

[dependencies]
crossbeam-channel = { workspace = true }
gethostname = { workspace = true }
log = { workspace = true }
reqwest = { workspace = true, features = ["blocking", "brotli", "deflate", "gzip", "rustls-tls", "json"] }
solana-cluster-type = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-time-utils = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
bencher = { workspace = true }
env_logger = { workspace = true }
rand = { workspace = true }
serial_test = { workspace = true }
solana-metrics = { path = ".", features = ["agave-unstable-api"] }
solana-sha256-hasher = { workspace = true, features = ["sha2"] }

[[bench]]
name = "metrics"
harness = false

================
File: metrics/README.md
================
![image](https://user-images.githubusercontent.com/110216567/184346286-94e0b45f-19e9-4fc9-a1a3-2e50c6f12bf8.png)

# Metrics

## InfluxDB

In order to explore validator specific metrics from mainnet-beta, testnet or devnet you can use Chronograf:

* https://metrics.solana.com:8888/ (production environment)
* https://metrics.solana.com:8889/ (testing environment)

For local cluster deployments you should use:

* https://internal-metrics.solana.com:8888/
* https://internal-metrics.solana.com:8889/

## Public Grafana Dashboards

There are three main public dashboards for cluster related metrics:

* https://metrics.solana.com/d/monitor-edge/cluster-telemetry
* https://metrics.solana.com/d/0n54roOVz/fee-market
* https://metrics.solana.com/d/UpIWbId4k/ping-result

For local cluster deployments you should use:

* https://internal-metrics.solana.com:3000/

### Cluster Telemetry

The cluster telemetry dashboard shows the current state of the cluster:

1. Cluster Stability
2. Validator Streamer
3. Tomer Consensus
4. IP Network
5. Snapshots
6. RPC Send Transaction Service

### Fee Market

The fee market dashboard shows:

1. Total Prioritization Fees
2. Block Min Prioritization Fees
3. Cost Tracker Stats

### Ping Results

The ping results dashboard displays relevant information about the Ping API

================
File: multinode-demo/bench-tps.sh
================
set -e
here=$(dirname "$0")
source "$here"/common.sh
usage() {
  if [[ -n $1 ]]; then
    echo "$*"
    echo
  fi
  echo "usage: $0 [extra args]"
  echo
  echo " Run bench-tps "
  echo
  echo "   extra args: additional arguments are passed along to solana-bench-tps"
  echo
  exit 1
}
args=("$@")
default_arg --url "http://127.0.0.1:8899"
default_arg --faucet "127.0.0.1:9900"
default_arg --duration 90
default_arg --tx-count 50000
default_arg --thread-batch-sleep-ms 0
default_arg --bind-address "127.0.0.1"
default_arg --client-node-id "${SOLANA_CONFIG_DIR}/bootstrap-validator/identity.json"
$solana_bench_tps "${args[@]}"

================
File: multinode-demo/bootstrap-validator.sh
================
set -e
here=$(dirname "$0")
source "$here"/common.sh
program=$agave_validator
no_restart=0
maybeRequireTower=true
args=()
while [[ -n $1 ]]; do
  if [[ ${1:0:1} = - ]]; then
    if [[ $1 = --init-complete-file ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --bind-address ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --gossip-port ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --dev-halt-at-slot ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --dynamic-port-range ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --limit-ledger-size ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --no-rocksdb-compaction ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --enable-rpc-transaction-history ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --rpc-pubsub-enable-block-subscription ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --enable-extended-tx-metadata-storage ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --enable-rpc-bigtable-ledger-storage ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --tpu-disable-quic ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --tpu-enable-udp ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --rpc-send-batch-ms ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --rpc-send-batch-size ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --skip-poh-verify ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --log ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --no-restart ]]; then
      no_restart=1
      shift
    elif [[ $1 == --wait-for-supermajority ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --expected-bank-hash ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --expected-shred-version ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --accounts ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --maximum-snapshots-to-retain ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --no-snapshot-fetch ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --accounts-db-skip-shrink ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --skip-require-tower ]]; then
      maybeRequireTower=false
      shift
    elif [[ $1 == --relayer-url ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --block-engine-url ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --disable-block-engine-autoconfig ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --tip-payment-program-pubkey ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --tip-distribution-program-pubkey ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --commission-bps ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --shred-receiver-address ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --shred-retransmit-receiver-address ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --log-messages-bytes-limit ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --block-production-method ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --enable-scheduler-bindings ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --transaction-structure ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --wen-restart ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --wen-restart-coordinator ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --geyser-plugin-config ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --trust-relayer-packets ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --rpc-threads ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --trust-block-engine-packets ]]; then
      args+=("$1")
      shift
    else
      echo "Unknown argument: $1"
      $program --help
      exit 1
    fi
  else
    echo "Unknown argument: $1"
    $program --help
    exit 1
  fi
done
identity=$SOLANA_CONFIG_DIR/bootstrap-validator/identity.json
vote_account="$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json
ledger_dir="$SOLANA_CONFIG_DIR"/bootstrap-validator
[[ -d "$ledger_dir" ]] || {
  echo "$ledger_dir does not exist"
  echo
  echo "Please run: $here/setup.sh"
  exit 1
}
if [[ $maybeRequireTower = true ]]; then
  args+=(--require-tower)
fi
args+=(
  --ledger "$ledger_dir"
  --rpc-port 8899
  --snapshot-interval-slots 200
  --no-incremental-snapshots
  --identity "$identity"
  --vote-account "$vote_account"
  --merkle-root-upload-authority "$identity"
  --rpc-faucet-address 127.0.0.1:9900
  --no-poh-speed-test
  --no-os-network-limits-test
  --no-wait-for-vote-to-start-leader
  --full-rpc-api
  --allow-private-addr
)
default_arg --gossip-port 8001
default_arg --log -
default_arg --tip-payment-program-pubkey "DThZmRNNXh7kvTQW9hXeGoWGPKktK8pgVAyoTLjH7UrT"
default_arg --tip-distribution-program-pubkey "FjrdANjvo76aCYQ4kf9FM1R8aESUcEE6F8V7qyoVUQcM"
default_arg --commission-bps 0
pid=
kill_node() {
  set +ex
  if [[ -n $pid ]]; then
    declare _pid=$pid
    pid=
    kill "$_pid" || true
    wait "$_pid" || true
  fi
}
kill_node_and_exit() {
  kill_node
  exit
}
trap 'kill_node_and_exit' INT TERM ERR
while true; do
  echo "$program ${args[*]}"
  $program "${args[@]}" &
  pid=$!
  echo "pid: $pid"
  if ((no_restart)); then
    wait "$pid"
    exit $?
  fi
  while true; do
    if [[ -z $pid ]] || ! kill -0 "$pid"; then
      echo "############## validator exited, restarting ##############"
      break
    fi
    sleep 1
  done
  kill_node
done

================
File: multinode-demo/common.sh
================
here="$(cd "$(dirname "${BASH_SOURCE[0]}")"/.. || exit 1; pwd)"
source "$here"/net/common.sh
prebuild=
if [[ $1 = "--prebuild" ]]; then
  prebuild=true
fi
if [[ -n $USE_INSTALL || ! -f "$SOLANA_ROOT"/Cargo.toml ]]; then
  solana_program() {
    declare program="$1"
    if [[ -z $program ]]; then
      printf "solana"
    else
      if [[ $program == "validator" || $program == "ledger-tool" || $program == "watchtower" || $program == "install" ]]; then
        printf "agave-%s" "$program"
      else
        printf "solana-%s" "$program"
      fi
    fi
  }
else
  solana_program() {
    declare program="$1"
    declare crate="$program"
    declare manifest_path
    if [[ $program == "bench-tps" || $program == "ledger-tool" ]]; then
      manifest_path="--manifest-path $here/dev-bins/Cargo.toml"
    fi
    if [[ -z $program ]]; then
      crate="cli"
      program="solana"
    elif [[ $program == "validator" || $program == "ledger-tool" || $program == "watchtower" || $program == "install" ]]; then
      program="agave-$program"
    else
      program="solana-$program"
    fi
    if [[ -n $CARGO_BUILD_PROFILE ]]; then
      profile_arg="--profile $CARGO_BUILD_PROFILE"
    fi
    if [[ $prebuild ]]; then
      (
        set -x
        cargo $CARGO_TOOLCHAIN build $manifest_path $profile_arg --bin $program
      )
    fi
    printf "cargo $CARGO_TOOLCHAIN run $manifest_path $profile_arg --bin %s %s -- " "$program"
  }
fi
solana_bench_tps=$(solana_program bench-tps)
solana_faucet=$(solana_program faucet)
agave_validator=$(solana_program validator)
solana_genesis=$(solana_program genesis)
solana_gossip=$(solana_program gossip)
solana_keygen=$(solana_program keygen)
solana_ledger_tool=$(solana_program ledger-tool)
solana_cli=$(solana_program)
export RUST_BACKTRACE=1
default_arg() {
  declare name=$1
  declare value=$2
  for arg in "${args[@]}"; do
    if [[ $arg = "$name" ]]; then
      return
    fi
  done
  if [[ -n $value ]]; then
    args+=("$name" "$value")
  else
    args+=("$name")
  fi
}
replace_arg() {
  declare name=$1
  declare value=$2
  default_arg "$name" "$value"
  declare index=0
  for arg in "${args[@]}"; do
    index=$((index + 1))
    if [[ $arg = "$name" ]]; then
      args[$index]="$value"
    fi
  done
}

================
File: multinode-demo/delegate-stake.sh
================
set -e
here=$(dirname "$0")
source "$here"/common.sh
stake_sol=10
url=http://127.0.0.1:8899
usage() {
  if [[ -n $1 ]]; then
    echo "$*"
    echo
  fi
  cat <<EOF
usage: $0 [OPTIONS] <SOL to stake ($stake_sol)>
Add stake to a validator
OPTIONS:
  --url   RPC_URL           - RPC URL to the cluster ($url)
  --label LABEL             - Append the given label to the configuration files, useful when running
                              multiple validators in the same workspace
  --no-airdrop              - Do not attempt to airdrop the stake
  --keypair FILE            - Keypair to fund the stake from
  --force                   - Override delegate-stake sanity checks
  --vote-account            - Path to vote-account keypair file
  --stake-account           - Path to stake-account keypair file
EOF
  exit 1
}
common_args=()
label=
airdrops_enabled=1
maybe_force=
keypair=
positional_args=()
while [[ -n $1 ]]; do
  if [[ ${1:0:1} = - ]]; then
    if [[ $1 = --label ]]; then
      label="-$2"
      shift 2
    elif [[ $1 = --keypair || $1 = -k ]]; then
      keypair="$2"
      shift 2
    elif [[ $1 = --force ]]; then
      maybe_force=--force
      shift 1
    elif [[ $1 = --url || $1 = -u ]]; then
      url=$2
      shift 2
    elif [[ $1 = --vote-account ]]; then
      vote_account=$2
      shift 2
    elif [[ $1 = --stake-account ]]; then
      stake_account=$2
      shift 2
    elif [[ $1 = --no-airdrop ]]; then
      airdrops_enabled=0
      shift
    elif [[ $1 = -h ]]; then
      usage "$@"
    else
      echo "Unknown argument: $1"
      usage
      exit 1
    fi
  else
    positional_args+=("$1")
    shift
  fi
done
common_args+=(--url "$url")
if [[ ${
  usage "$@"
fi
if [[ -n ${positional_args[0]} ]]; then
  stake_sol=${positional_args[0]}
fi
VALIDATOR_KEYS_DIR=$SOLANA_CONFIG_DIR/validator$label
vote_account="${vote_account:-$VALIDATOR_KEYS_DIR/vote-account.json}"
stake_account="${stake_account:-$VALIDATOR_KEYS_DIR/stake-account.json}"
if [[ ! -f $vote_account ]]; then
  echo "Error: $vote_account not found"
  exit 1
fi
if ((airdrops_enabled)); then
  if [[ -z $keypair ]]; then
    echo "--keypair argument must be provided"
    exit 1
  fi
  $solana_cli \
    "${common_args[@]}" --keypair "$SOLANA_CONFIG_DIR/faucet.json" \
    transfer --allow-unfunded-recipient "$keypair" "$stake_sol"
fi
if [[ -n $keypair ]]; then
  common_args+=(--keypair "$keypair")
fi
if ! [[ -f "$stake_account" ]]; then
  $solana_keygen new --no-passphrase -so "$stake_account"
else
  echo "$stake_account already exists! Using it"
fi
set -x
$solana_cli "${common_args[@]}" \
  vote-account "$vote_account"
$solana_cli "${common_args[@]}" \
  create-stake-account "$stake_account" "$stake_sol"
$solana_cli "${common_args[@]}" \
  delegate-stake $maybe_force "$stake_account" "$vote_account"
$solana_cli "${common_args[@]}" stake-account "$stake_account"

================
File: multinode-demo/faucet.sh
================
here=$(dirname "$0")
source "$here"/common.sh
[[ -f "$SOLANA_CONFIG_DIR"/faucet.json ]] || {
  echo "$SOLANA_CONFIG_DIR/faucet.json not found, create it by running:"
  echo
  echo "  ${here}/setup.sh"
  exit 1
}
set -x
exec $solana_faucet --keypair "$SOLANA_CONFIG_DIR"/faucet.json "$@"

================
File: multinode-demo/setup-from-mainnet-beta.sh
================
here=$(dirname "$0")
source "$here"/common.sh
set -e
rm -rf "$SOLANA_CONFIG_DIR"/latest-mainnet-beta-snapshot
mkdir -p "$SOLANA_CONFIG_DIR"/latest-mainnet-beta-snapshot
(
  cd "$SOLANA_CONFIG_DIR"/latest-mainnet-beta-snapshot || exit 1
  set -x
  wget http://api.mainnet-beta.solana.com/genesis.tar.bz2
  wget --trust-server-names http://api.mainnet-beta.solana.com/snapshot.tar.bz2
)
snapshot=$(ls "$SOLANA_CONFIG_DIR"/latest-mainnet-beta-snapshot/snapshot-[0-9]*-*.tar.zst)
if [[ -z $snapshot ]]; then
  echo Error: Unable to find latest snapshot
  exit 1
fi
if [[ ! $snapshot =~ snapshot-([0-9]*)-.*.tar.zst ]]; then
  echo Error: Unable to determine snapshot slot for "$snapshot"
  exit 1
fi
snapshot_slot="${BASH_REMATCH[1]}"
rm -rf "$SOLANA_CONFIG_DIR"/bootstrap-validator
mkdir -p "$SOLANA_CONFIG_DIR"/bootstrap-validator
if [[ -r $FAUCET_KEYPAIR ]]; then
  cp -f "$FAUCET_KEYPAIR" "$SOLANA_CONFIG_DIR"/faucet.json
else
  $solana_keygen new --no-passphrase -fso "$SOLANA_CONFIG_DIR"/faucet.json
fi
if [[ -f $BOOTSTRAP_VALIDATOR_IDENTITY_KEYPAIR ]]; then
  cp -f "$BOOTSTRAP_VALIDATOR_IDENTITY_KEYPAIR" "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json
else
  $solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json
fi
$solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json
$solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/stake-account.json
$solana_ledger_tool create-snapshot \
  --ledger "$SOLANA_CONFIG_DIR"/latest-mainnet-beta-snapshot \
  --faucet-pubkey "$SOLANA_CONFIG_DIR"/faucet.json \
  --faucet-lamports 500000000000000000 \
  --bootstrap-validator "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json \
                        "$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json \
                        "$SOLANA_CONFIG_DIR"/bootstrap-validator/stake-account.json \
  --hashes-per-tick sleep \
  "$snapshot_slot" "$SOLANA_CONFIG_DIR"/bootstrap-validator
$solana_ledger_tool modify-genesis \
  --ledger "$SOLANA_CONFIG_DIR"/latest-mainnet-beta-snapshot \
  --hashes-per-tick sleep \
  "$SOLANA_CONFIG_DIR"/bootstrap-validator

================
File: multinode-demo/setup-from-testnet.sh
================
here=$(dirname "$0")
source "$here"/common.sh
set -e
rm -rf "$SOLANA_CONFIG_DIR"/latest-testnet-snapshot
mkdir -p "$SOLANA_CONFIG_DIR"/latest-testnet-snapshot
(
  cd "$SOLANA_CONFIG_DIR"/latest-testnet-snapshot || exit 1
  set -x
  wget http://api.testnet.solana.com/genesis.tar.bz2
  wget --trust-server-names http://testnet.solana.com/snapshot.tar.bz2
)
snapshot=$(ls "$SOLANA_CONFIG_DIR"/latest-testnet-snapshot/snapshot-[0-9]*-*.tar.zst)
if [[ -z $snapshot ]]; then
  echo Error: Unable to find latest snapshot
  exit 1
fi
if [[ ! $snapshot =~ snapshot-([0-9]*)-.*.tar.zst ]]; then
  echo Error: Unable to determine snapshot slot for "$snapshot"
  exit 1
fi
snapshot_slot="${BASH_REMATCH[1]}"
rm -rf "$SOLANA_CONFIG_DIR"/bootstrap-validator
mkdir -p "$SOLANA_CONFIG_DIR"/bootstrap-validator
if [[ -r $FAUCET_KEYPAIR ]]; then
  cp -f "$FAUCET_KEYPAIR" "$SOLANA_CONFIG_DIR"/faucet.json
else
  $solana_keygen new --no-passphrase -fso "$SOLANA_CONFIG_DIR"/faucet.json
fi
if [[ -f $BOOTSTRAP_VALIDATOR_IDENTITY_KEYPAIR ]]; then
  cp -f "$BOOTSTRAP_VALIDATOR_IDENTITY_KEYPAIR" "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json
else
  $solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json
fi
$solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json
$solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/stake-account.json
$solana_ledger_tool create-snapshot \
  --ledger "$SOLANA_CONFIG_DIR"/latest-testnet-snapshot \
  --faucet-pubkey "$SOLANA_CONFIG_DIR"/faucet.json \
  --faucet-lamports 500000000000000000 \
  --bootstrap-validator "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json \
                        "$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json \
                        "$SOLANA_CONFIG_DIR"/bootstrap-validator/stake-account.json \
  --hashes-per-tick sleep \
  "$snapshot_slot" "$SOLANA_CONFIG_DIR"/bootstrap-validator
$solana_ledger_tool modify-genesis \
  --ledger "$SOLANA_CONFIG_DIR"/latest-testnet-snapshot \
  --hashes-per-tick sleep \
  "$SOLANA_CONFIG_DIR"/bootstrap-validator

================
File: multinode-demo/setup.sh
================
here=$(dirname "$0")
source "$here"/common.sh
set -e
rm -rf "$SOLANA_CONFIG_DIR"/bootstrap-validator
mkdir -p "$SOLANA_CONFIG_DIR"/bootstrap-validator
if [[ -r $FAUCET_KEYPAIR ]]; then
  cp -f "$FAUCET_KEYPAIR" "$SOLANA_CONFIG_DIR"/faucet.json
else
  $solana_keygen new --no-passphrase -fso "$SOLANA_CONFIG_DIR"/faucet.json
fi
if [[ -f $BOOTSTRAP_VALIDATOR_IDENTITY_KEYPAIR ]]; then
  cp -f "$BOOTSTRAP_VALIDATOR_IDENTITY_KEYPAIR" "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json
else
  $solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json
fi
if [[ -f $BOOTSTRAP_VALIDATOR_STAKE_KEYPAIR ]]; then
  cp -f "$BOOTSTRAP_VALIDATOR_STAKE_KEYPAIR" "$SOLANA_CONFIG_DIR"/bootstrap-validator/stake-account.json
else
  $solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/stake-account.json
fi
if [[ -f $BOOTSTRAP_VALIDATOR_VOTE_KEYPAIR ]]; then
  cp -f "$BOOTSTRAP_VALIDATOR_VOTE_KEYPAIR" "$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json
else
  $solana_keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json
fi
args=(
  "$@"
  --max-genesis-archive-unpacked-size 1073741824
  --enable-warmup-epochs
  --bootstrap-validator "$SOLANA_CONFIG_DIR"/bootstrap-validator/identity.json
                        "$SOLANA_CONFIG_DIR"/bootstrap-validator/vote-account.json
                        "$SOLANA_CONFIG_DIR"/bootstrap-validator/stake-account.json
)
"$SOLANA_ROOT"/fetch-core-bpf.sh
if [[ -r core-bpf-genesis-args.sh ]]; then
  CORE_BPF_GENESIS_ARGS=$(cat "$SOLANA_ROOT"/core-bpf-genesis-args.sh)
  args+=($CORE_BPF_GENESIS_ARGS)
fi
"$SOLANA_ROOT"/fetch-spl.sh
if [[ -r spl-genesis-args.sh ]]; then
  SPL_GENESIS_ARGS=$(cat "$SOLANA_ROOT"/spl-genesis-args.sh)
  args+=($SPL_GENESIS_ARGS)
fi
default_arg --ledger "$SOLANA_CONFIG_DIR"/bootstrap-validator
default_arg --faucet-pubkey "$SOLANA_CONFIG_DIR"/faucet.json
default_arg --faucet-lamports 500000000000000000
default_arg --hashes-per-tick auto
default_arg --cluster-type development
$solana_genesis "${args[@]}"

================
File: multinode-demo/validator-x.sh
================
here=$(dirname "$0")
exec "$here"/validator.sh --label x$$ "$@"

================
File: multinode-demo/validator.sh
================
here=$(dirname "$0")
source "$here"/common.sh
args=(
  --max-genesis-archive-unpacked-size 1073741824
  --no-poh-speed-test
  --no-os-network-limits-test
)
airdrops_enabled=1
node_sol=500
label=
identity=
vote_account=
no_restart=0
gossip_entrypoint=
ledger_dir=
usage() {
  if [[ -n $1 ]]; then
    echo "$*"
    echo
  fi
  cat <<EOF
usage: $0 [OPTIONS] [cluster entry point hostname]
Start a validator with no stake
OPTIONS:
  --ledger PATH             - store ledger under this PATH
  --init-complete-file FILE - create this file, if it doesn't already exist, once node initialization is complete
  --label LABEL             - Append the given label to the configuration files, useful when running
                              multiple validators in the same workspace
  --node-sol SOL            - Number of SOL this node has been funded from the genesis config (default: $node_sol)
  --no-voting               - start node without vote signer
  --rpc-port port           - custom RPC port for this node
  --no-restart              - do not restart the node if it exits
  --no-airdrop              - The genesis config has an account for the node. Airdrops are not required.
EOF
  exit 1
}
maybeRequireTower=true
positional_args=()
while [[ -n $1 ]]; do
  if [[ ${1:0:1} = - ]]; then
    if [[ $1 = --label ]]; then
      label="-$2"
      shift 2
    elif [[ $1 = --no-restart ]]; then
      no_restart=1
      shift
    elif [[ $1 = --node-sol ]]; then
      node_sol="$2"
      shift 2
    elif [[ $1 = --no-airdrop ]]; then
      airdrops_enabled=0
      shift
    elif [[ $1 = --expected-genesis-hash ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --expected-shred-version ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --identity ]]; then
      identity=$2
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --authorized-voter ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --authorized-withdrawer ]]; then
      authorized_withdrawer=$2
      shift 2
    elif [[ $1 = --vote-account ]]; then
      vote_account=$2
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --block-engine-url ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --relayer-url ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --merkle-root-upload-authority ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --tip-payment-program-pubkey ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --tip-distribution-program-pubkey ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --disable-block-engine-autoconfig ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --commission-bps ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --init-complete-file ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --ledger ]]; then
      ledger_dir=$2
      shift 2
    elif [[ $1 = --entrypoint ]]; then
      gossip_entrypoint=$2
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --no-snapshot-fetch ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --no-voting ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --dev-no-sigverify ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --dev-halt-at-slot ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --rpc-port ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --rpc-faucet-address ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --accounts ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --gossip-port ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --dynamic-port-range ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --snapshot-interval-slots ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --maximum-snapshots-to-retain ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --limit-ledger-size ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --no-rocksdb-compaction ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --enable-rpc-transaction-history ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --enable-extended-tx-metadata-storage ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --skip-poh-verify ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --tpu-disable-quic ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --tpu-enable-udp ]]; then
      args+=("$1")
      shift
    elif [[ $1 = --rpc-send-batch-ms ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --rpc-send-batch-size ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --log ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --known-validator ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 = --max-genesis-archive-unpacked-size ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --wait-for-supermajority ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --expected-bank-hash ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --accounts-db-skip-shrink ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --skip-require-tower ]]; then
      maybeRequireTower=false
      shift
    elif [[ $1 == --block-production-method ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --enable-scheduler-bindings ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --transaction-structure ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --wen-restart ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --wen-restart-coordinator ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --rpc-pubsub-enable-block-subscription ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --geyser-plugin-config ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --trust-relayer-packets ]]; then
      args+=("$1")
      shift
    elif [[ $1 == --rpc-threads ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --shred-receiver-address ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --shred-retransmit-receiver-address ]]; then
      args+=("$1" "$2")
      shift 2
    elif [[ $1 == --trust-block-engine-packets ]]; then
      args+=("$1")
      shift
    elif [[ $1 = -h ]]; then
      usage "$@"
    else
      echo "Unknown argument: $1"
      exit 1
    fi
  else
    positional_args+=("$1")
    shift
  fi
done
if [[ ${
  usage "$@"
fi
if [[ -n $REQUIRE_LEDGER_DIR ]]; then
  if [[ -z $ledger_dir ]]; then
    usage "Error: --ledger not specified"
  fi
  SOLANA_CONFIG_DIR="$ledger_dir"
fi
if [[ -n $REQUIRE_KEYPAIRS ]]; then
  if [[ -z $identity ]]; then
    usage "Error: --identity not specified"
  fi
  if [[ -z $vote_account ]]; then
    usage "Error: --vote-account not specified"
  fi
  if [[ -z $authorized_withdrawer ]]; then
    usage "Error: --authorized_withdrawer not specified"
  fi
fi
if [[ -z "$ledger_dir" ]]; then
  ledger_dir="$SOLANA_CONFIG_DIR/validator$label"
fi
mkdir -p "$ledger_dir"
if [[ -n $gossip_entrypoint ]]; then
  if [[ ${
    usage "$@"
  fi
else
  entrypoint_hostname=${positional_args[0]}
  if [[ -z $entrypoint_hostname ]]; then
    gossip_entrypoint=127.0.0.1:8001
  else
    gossip_entrypoint="$entrypoint_hostname":8001
  fi
fi
faucet_address="${gossip_entrypoint%:*}":9900
: "${identity:=$ledger_dir/identity.json}"
: "${vote_account:=$ledger_dir/vote-account.json}"
: "${authorized_withdrawer:=$ledger_dir/authorized-withdrawer.json}"
default_arg --entrypoint "$gossip_entrypoint"
if ((airdrops_enabled)); then
  default_arg --rpc-faucet-address "$faucet_address"
fi
default_arg --identity "$identity"
default_arg --vote-account "$vote_account"
default_arg --merkle-root-upload-authority "$identity"
default_arg --tip-payment-program-pubkey "DThZmRNNXh7kvTQW9hXeGoWGPKktK8pgVAyoTLjH7UrT"
default_arg --tip-distribution-program-pubkey "FjrdANjvo76aCYQ4kf9FM1R8aESUcEE6F8V7qyoVUQcM"
default_arg --commission-bps 0
default_arg --ledger "$ledger_dir"
default_arg --log -
default_arg --full-rpc-api
default_arg --no-incremental-snapshots
default_arg --allow-private-addr
if [[ $maybeRequireTower = true ]]; then
  default_arg --require-tower
fi
program=$agave_validator
set -e
PS4="$(basename "$0"): "
pid=
kill_node() {
  # Note: do not echo anything from this function to ensure $pid is actually
  # killed when stdout/stderr are redirected
  set +ex
  if [[ -n $pid ]]; then
    declare _pid=$pid
    pid=
    kill "$_pid" || true
    wait "$_pid" || true
  fi
}
kill_node_and_exit() {
  kill_node
  exit
}
trap 'kill_node_and_exit' INT TERM ERR
wallet() {
  (
    set -x
    $solana_cli --keypair "$identity" --url "$rpc_url" "$@"
  )
}
setup_validator_accounts() {
  declare node_sol=$1
  if [[ -n "$SKIP_ACCOUNTS_CREATION" ]]; then
    return 0
  fi
  if ! wallet vote-account "$vote_account"; then
    if ((airdrops_enabled)); then
      echo "Adding $node_sol to validator identity account:"
      (
        set -x
        $solana_cli \
          --keypair "$SOLANA_CONFIG_DIR/faucet.json" --url "$rpc_url" \
          transfer --allow-unfunded-recipient "$identity" "$node_sol"
      ) || return $?
    fi
    echo "Creating validator vote account"
    wallet create-vote-account "$vote_account" "$identity" "$authorized_withdrawer" || return $?
  fi
  echo "Validator vote account configured"
  echo "Validator identity account balance:"
  wallet balance || return $?
  return 0
}
rpc_url=$($solana_gossip --allow-private-addr rpc-url --timeout 180 --entrypoint "$gossip_entrypoint")
[[ -r "$identity" ]] || $solana_keygen new --no-passphrase -so "$identity"
[[ -r "$vote_account" ]] || $solana_keygen new --no-passphrase -so "$vote_account"
[[ -r "$authorized_withdrawer" ]] || $solana_keygen new --no-passphrase -so "$authorized_withdrawer"
setup_validator_accounts "$node_sol"
while true; do
  echo "$PS4$program ${args[*]}"
  $program "${args[@]}" &
  pid=$!
  echo "pid: $pid"
  if ((no_restart)); then
    wait "$pid"
    exit $?
  fi
  while true; do
    if [[ -z $pid ]] || ! kill -0 "$pid"; then
      echo "############## validator exited, restarting ##############"
      break
    fi
    sleep 1
  done
  kill_node
done

================
File: net/remote/cleanup.sh
================
set -x
! tmux list-sessions || tmux kill-session
declare sudo=
if sudo true; then
  sudo="sudo -n"
fi
echo "pwd: $(pwd)"
for pid in solana/*.pid; do
  pgid=$(ps opgid= "$(cat "$pid")" | tr -d '[:space:]')
  if [[ -n $pgid ]]; then
    $sudo kill -- -"$pgid"
  fi
done
for pattern in validator.sh boostrap-leader.sh solana- remote- iftop validator client node; do
  echo "killing $pattern"
  pkill -f $pattern
done

================
File: net/remote/README.md
================
Scripts that run on the remote testnet nodes

================
File: net/remote/remote-client.sh
================
set -e
cd "$(dirname "$0")"/../..
deployMethod="$1"
entrypointIp="$2"
clientToRun="$3"
if [[ -n $4 ]]; then
  export RUST_LOG="$4"
fi
benchTpsExtraArgs="$5"
clientIndex="$6"
clientType="${7:-tpu-client}"
maybeUseUnstakedConnection="$8"
missing() {
  echo "Error: $1 not specified"
  exit 1
}
[[ -n $deployMethod ]] || missing deployMethod
[[ -n $entrypointIp ]] || missing entrypointIp
source net/common.sh
loadConfigFile
threadCount=$(nproc)
if [[ $threadCount -gt 4 ]]; then
  threadCount=4
fi
case $deployMethod in
local|tar)
  PATH="$HOME"/.cargo/bin:"$PATH"
  export USE_INSTALL=1
  net/scripts/rsync-retry.sh -vPrc "$entrypointIp:~/.cargo/bin/*" ~/.cargo/bin/
  ;;
skip)
  ;;
*)
  echo "Unknown deployment method: $deployMethod"
  exit 1
esac
RPC_CLIENT=false
case "$clientType" in
  tpu-client)
    RPC_CLIENT=false
    ;;
  rpc-client)
    RPC_CLIENT=true
    ;;
  *)
    echo "Unexpected clientType: \"$clientType\""
    exit 1
    ;;
esac
case $clientToRun in
solana-bench-tps)
  net/scripts/rsync-retry.sh -vPrc \
    "$entrypointIp":~/solana/config/bench-tps"$clientIndex".yml ./client-accounts.yml
  net/scripts/rsync-retry.sh -vPrc \
    "$entrypointIp":~/solana/config/validator-identity-1.json ./validator-identity.json
  args=()
  if ${RPC_CLIENT}; then
    args+=(--use-rpc-client)
  fi
  if [[ -z "$maybeUseUnstakedConnection" ]]; then
    args+=(--bind-address "$entrypointIp")
    args+=(--client-node-id ./validator-identity.json)
  fi
  clientCommand="\
    solana-bench-tps \
      --duration 7500 \
      --sustained \
      --threads $threadCount \
      $benchTpsExtraArgs \
      --read-client-keys ./client-accounts.yml \
      --url "http://$entrypointIp:8899" \
      ${args[*]} \
  "
  ;;
idle)
  # Add the faucet keypair to idle clients for convenience
  net/scripts/rsync-retry.sh -vPrc \
    "$entrypointIp":~/solana/config/faucet.json ~/solana/
  exit 0
  ;;
*)
  echo "Unknown client name: $clientToRun"
  exit 1
esac
cat > ~/solana/on-reboot <<EOF
cd ~/solana
PATH="$HOME"/.cargo/bin:"$PATH"
export USE_INSTALL=1
echo "$(date) | $0 $*" >> client.log
(
  sudo SOLANA_METRICS_CONFIG="$SOLANA_METRICS_CONFIG" scripts/oom-monitor.sh
) > oom-monitor.log 2>&1 &
echo \$! > oom-monitor.pid
scripts/fd-monitor.sh > fd-monitor.log 2>&1 &
echo \$! > fd-monitor.pid
scripts/net-stats.sh  > net-stats.log 2>&1 &
echo \$! > net-stats.pid
! tmux list-sessions || tmux kill-session
tmux new -s "$clientToRun" -d "
  while true; do
    echo === Client start: \$(date) | tee -a client.log
    $metricsWriteDatapoint 'testnet-deploy client-begin=1'
    echo '$ $clientCommand' | tee -a client.log
    $clientCommand >> client.log 2>&1
    $metricsWriteDatapoint 'testnet-deploy client-complete=1'
  done
"
EOF
chmod +x ~/solana/on-reboot
echo "@reboot ~/solana/on-reboot" | crontab -
~/solana/on-reboot
sleep 1
tmux capture-pane -t "$clientToRun" -p -S -100

================
File: net/remote/remote-deploy-update.sh
================
set -e
cd "$(dirname "$0")"/../..
releaseChannel=$1
updatePlatform=$2
[[ -r deployConfig ]] || {
  echo deployConfig missing
  exit 1
}
# shellcheck source=/dev/null # deployConfig is written by remote-node.sh
source deployConfig
missing() {
  echo "Error: $1 not specified"
  exit 1
}
[[ -n $releaseChannel ]] || missing releaseChannel
[[ -n $updatePlatform ]] || missing updatePlatform
[[ -f update_manifest_keypair.json ]] || missing update_manifest_keypair.json
if [[ -n $2 ]]; then
  export RUST_LOG="$2"
fi
source net/common.sh
loadConfigFile
PATH="$HOME"/.cargo/bin:"$PATH"
set -x
scripts/agave-install-deploy.sh \
  --keypair config/faucet.json \
  localhost "$releaseChannel" "$updatePlatform"

================
File: net/remote/remote-node-wait-init.sh
================
set -e
set -x
initCompleteFile=init-complete-node.log
waitTime=${1:=600}
waitForNodeToInit() {
  declare hostname
  hostname=$(hostname)
  echo "--- waiting for $hostname to boot up"
  declare startTime=$SECONDS
  while [[ ! -r $initCompleteFile ]]; do
    declare timeWaited=$((SECONDS - startTime))
    if [[ $timeWaited -ge $waitTime ]]; then
      echo "^^^ +++"
      echo "Error: $initCompleteFile not found in $timeWaited seconds"
      exit 1
    fi
    echo "Waiting for $initCompleteFile ($timeWaited) on $hostname..."
    sleep 5
  done
  echo "$hostname booted up"
}
cd ~/solana
waitForNodeToInit

================
File: net/remote/remote-node.sh
================
set -e
cd "$(dirname "$0")"/../..
set -x
deployMethod="$1"
nodeType="$2"
entrypointIp="$3"
numNodes="$4"
if [[ -n $5 ]]; then
  export RUST_LOG="$5"
fi
skipSetup="$6"
failOnValidatorBootupFailure="$7"
externalPrimordialAccountsFile="$8"
maybeDisableAirdrops="$9"
internalNodesStakeLamports="${10}"
internalNodesLamports="${11}"
nodeIndex="${12}"
numBenchTpsClients="${13}"
benchTpsExtraArgs="${14}"
genesisOptions="${15}"
extraNodeArgs="${16}"
maybeWarpSlot="${17}"
maybeFullRpc="${18}"
waitForNodeInit="${19}"
extraPrimordialStakes="${20:=0}"
tmpfsAccounts="${21:false}"
disableQuic="${22}"
enableUdp="${23}"
maybeWenRestart="${24}"
set +x
missing() {
  echo "Error: $1 not specified"
  exit 1
}
[[ -n $deployMethod ]]  || missing deployMethod
[[ -n $nodeType ]]      || missing nodeType
[[ -n $entrypointIp ]]  || missing entrypointIp
[[ -n $numNodes ]]      || missing numNodes
[[ -n $skipSetup ]]     || missing skipSetup
[[ -n $failOnValidatorBootupFailure ]] || missing failOnValidatorBootupFailure
airdropsEnabled=true
if [[ -n $maybeDisableAirdrops ]]; then
  airdropsEnabled=false
fi
cat > deployConfig <<EOF
deployMethod="$deployMethod"
entrypointIp="$entrypointIp"
numNodes="$numNodes"
failOnValidatorBootupFailure=$failOnValidatorBootupFailure
genesisOptions="$genesisOptions"
airdropsEnabled=$airdropsEnabled
EOF
source net/common.sh
source multinode-demo/common.sh
loadConfigFile
initCompleteFile=init-complete-node.log
cat > ~/solana/on-reboot <<EOF
cd ~/solana
source scripts/oom-score-adj.sh
now=\$(date -u +"%Y-%m-%dT%H:%M:%SZ")
ln -sfT validator.log.\$now validator.log
EOF
chmod +x ~/solana/on-reboot
case $deployMethod in
local|tar|skip)
  PATH="$HOME"/.cargo/bin:"$PATH"
  export USE_INSTALL=1
  ./fetch-perf-libs.sh
cat >> ~/solana/on-reboot <<EOF
  PATH="$HOME"/.cargo/bin:"$PATH"
  export USE_INSTALL=1
  (
    sudo SOLANA_METRICS_CONFIG="$SOLANA_METRICS_CONFIG" scripts/oom-monitor.sh
  ) > oom-monitor.log 2>&1 &
  echo \$! > oom-monitor.pid
  scripts/fd-monitor.sh > fd-monitor.log 2>&1 &
  echo \$! > fd-monitor.pid
  scripts/net-stats.sh  > net-stats.log 2>&1 &
  echo \$! > net-stats.pid
  scripts/iftop.sh  > iftop.log 2>&1 &
  echo \$! > iftop.pid
  scripts/system-stats.sh  > system-stats.log 2>&1 &
  echo \$! > system-stats.pid
EOF
  case $nodeType in
  bootstrap-validator)
    set -x
    if [[ $skipSetup != true ]]; then
      clear_config_dir "$SOLANA_CONFIG_DIR"
      if [[ -n $internalNodesLamports ]]; then
        echo "---" >> config/validator-balances.yml
      fi
      setupValidatorKeypair() {
        declare name=$1
        if [[ -f net/keypairs/"$name".json ]]; then
          cp net/keypairs/"$name".json config/"$name".json
          if [[ "$name" =~ ^validator-identity- ]]; then
            name="${name//-identity-/-vote-}"
            cp net/keypairs/"$name".json config/"$name".json
            name="${name//-vote-/-stake-}"
            cp net/keypairs/"$name".json config/"$name".json
          fi
        else
          solana-keygen new --no-passphrase -so config/"$name".json
          if [[ "$name" =~ ^validator-identity- ]]; then
            name="${name//-identity-/-vote-}"
            solana-keygen new --no-passphrase -so config/"$name".json
            name="${name//-vote-/-stake-}"
            solana-keygen new --no-passphrase -so config/"$name".json
          fi
        fi
        if [[ -n $internalNodesLamports ]]; then
          declare pubkey
          pubkey="$(solana-keygen pubkey config/"$name".json)"
          cat >> config/validator-balances.yml <<EOF
$pubkey:
  balance: $internalNodesLamports
  owner: 11111111111111111111111111111111
  data:
  executable: false
EOF
        fi
      }
      for i in $(seq 1 "$numNodes"); do
        setupValidatorKeypair validator-identity-"$i"
      done
      setupValidatorKeypair blockstreamer-identity
      lamports_per_signature="42"
      genesis_args=($genesisOptions)
      for i in "${!genesis_args[@]}"; do
        if [[ "${genesis_args[$i]}" = --target-lamports-per-signature ]]; then
          lamports_per_signature="${genesis_args[$((i+1))]}"
          break
        fi
      done
      for i in $(seq 0 $((numBenchTpsClients-1))); do
        solana-bench-tps --write-client-keys config/bench-tps"$i".yml \
          --target-lamports-per-signature "$lamports_per_signature" $benchTpsExtraArgs
        tail -n +2 -q config/bench-tps"$i".yml >> config/client-accounts.yml
        echo "" >> config/client-accounts.yml
      done
      if [[ -f $externalPrimordialAccountsFile ]]; then
        cat "$externalPrimordialAccountsFile" >> config/validator-balances.yml
      fi
      if [[ -f config/validator-balances.yml ]]; then
        genesisOptions+=" --primordial-accounts-file config/validator-balances.yml"
      fi
      if [[ -f config/client-accounts.yml ]]; then
        genesisOptions+=" --primordial-accounts-file config/client-accounts.yml"
      fi
      if [[ -n $internalNodesStakeLamports ]]; then
        args+=(--bootstrap-validator-stake-lamports "$internalNodesStakeLamports")
      fi
      if [[ -n $internalNodesLamports ]]; then
        args+=(--bootstrap-validator-lamports "$internalNodesLamports")
      fi
      args+=($genesisOptions)
      if [[ -f net/keypairs/faucet.json ]]; then
        export FAUCET_KEYPAIR=net/keypairs/faucet.json
      fi
      if [[ -f net/keypairs/bootstrap-validator-identity.json ]]; then
        export BOOTSTRAP_VALIDATOR_IDENTITY_KEYPAIR=net/keypairs/bootstrap-validator-identity.json
      fi
      if [[ -f net/keypairs/bootstrap-validator-stake.json ]]; then
        export BOOTSTRAP_VALIDATOR_STAKE_KEYPAIR=net/keypairs/bootstrap-validator-stake.json
      fi
      if [[ -f net/keypairs/bootstrap-validator-vote.json ]]; then
        export BOOTSTRAP_VALIDATOR_VOTE_KEYPAIR=net/keypairs/bootstrap-validator-vote.json
      fi
      echo "remote-node.sh: Primordial stakes: $extraPrimordialStakes"
      if [[ "$extraPrimordialStakes" -gt 0 ]]; then
        if [[ "$extraPrimordialStakes" -gt "$numNodes" ]]; then
          echo "warning: extraPrimordialStakes($extraPrimordialStakes) clamped to numNodes($numNodes)"
          extraPrimordialStakes=$numNodes
        fi
        for i in $(seq "$extraPrimordialStakes"); do
          args+=(--bootstrap-validator "$(solana-keygen pubkey "config/validator-identity-$i.json")"
                                       "$(solana-keygen pubkey "config/validator-vote-$i.json")"
                                       "$(solana-keygen pubkey "config/validator-stake-$i.json")"
          )
        done
      fi
      multinode-demo/setup.sh "${args[@]}"
      maybeWaitForSupermajority=
      # shellcheck disable=SC2086 # Do not want to quote $extraNodeArgs
      set -- $extraNodeArgs
      while [[ -n $1 ]]; do
        if [[ $1 = "--wait-for-supermajority" ]]; then
          maybeWaitForSupermajority=$2
          break
        fi
        shift
      done
      if [[ -z "$maybeWarpSlot" && -n "$maybeWaitForSupermajority" ]]; then
        maybeWarpSlot="--warp-slot $maybeWaitForSupermajority"
      fi
      if [[ -n "$maybeWarpSlot" ]]; then
        agave-ledger-tool -l config/bootstrap-validator create-snapshot 0 config/bootstrap-validator $maybeWarpSlot
      fi
      agave-ledger-tool -l config/bootstrap-validator shred-version --max-genesis-archive-unpacked-size 1073741824 | tee config/shred-version
      if [[ -n "$maybeWaitForSupermajority" ]]; then
        bankHash=$(agave-ledger-tool -l config/bootstrap-validator verify --halt-at-slot 0 --print-bank-hash --output json | jq -r ".hash")
        shredVersion="$(cat "$SOLANA_CONFIG_DIR"/shred-version)"
        extraNodeArgs="$extraNodeArgs --expected-bank-hash $bankHash --expected-shred-version $shredVersion"
        echo "$bankHash" > config/bank-hash
      fi
    fi
    args=(
      --bind-address "$entrypointIp"
      --gossip-port 8001
      --init-complete-file "$initCompleteFile"
    )
    if [[ "$tmpfsAccounts" = "true" ]]; then
      args+=(--accounts /mnt/solana-accounts)
    fi
    if $maybeFullRpc; then
      args+=(--enable-rpc-transaction-history)
      args+=(--enable-extended-tx-metadata-storage)
    fi
    if $disableQuic; then
      args+=(--tpu-disable-quic)
    fi
    if $enableUdp; then
      args+=(--tpu-enable-udp)
    fi
    if [[ $airdropsEnabled = true ]]; then
cat >> ~/solana/on-reboot <<EOF
      ./multinode-demo/faucet.sh > faucet.log 2>&1 &
EOF
    fi
    if [[ -n "$maybeWenRestart" ]]; then
      args+=(--wen-restart "$maybeWenRestart")
    fi
    args+=($extraNodeArgs)
cat >> ~/solana/on-reboot <<EOF
    nohup ./multinode-demo/bootstrap-validator.sh ${args[@]} > validator.log.\$now 2>&1 &
    pid=\$!
    oom_score_adj "\$pid" 1000
    disown
EOF
    ~/solana/on-reboot
    if $waitForNodeInit; then
      net/remote/remote-node-wait-init.sh 600
    fi
    ;;
  validator|blockstreamer)
    if [[ $deployMethod != skip ]]; then
      net/scripts/rsync-retry.sh -vPrc "$entrypointIp":~/.cargo/bin/ ~/.cargo/bin/
      net/scripts/rsync-retry.sh -vPrc "$entrypointIp":~/version.yml ~/version.yml
    fi
    if [[ $skipSetup != true ]]; then
      clear_config_dir "$SOLANA_CONFIG_DIR"
      if [[ $nodeType = blockstreamer ]]; then
        net/scripts/rsync-retry.sh -vPrc \
          "$entrypointIp":~/solana/config/blockstreamer-identity.json "$SOLANA_CONFIG_DIR"/validator-identity.json
      else
        net/scripts/rsync-retry.sh -vPrc \
          "$entrypointIp":~/solana/config/validator-identity-"$nodeIndex".json "$SOLANA_CONFIG_DIR"/validator-identity.json
        net/scripts/rsync-retry.sh -vPrc \
          "$entrypointIp":~/solana/config/validator-stake-"$nodeIndex".json "$SOLANA_CONFIG_DIR"/stake-account.json
        net/scripts/rsync-retry.sh -vPrc \
          "$entrypointIp":~/solana/config/validator-vote-"$nodeIndex".json "$SOLANA_CONFIG_DIR"/vote-account.json
      fi
      net/scripts/rsync-retry.sh -vPrc \
        "$entrypointIp":~/solana/config/shred-version "$SOLANA_CONFIG_DIR"/shred-version
      net/scripts/rsync-retry.sh -vPrc \
        "$entrypointIp":~/solana/config/bank-hash "$SOLANA_CONFIG_DIR"/bank-hash || true
      net/scripts/rsync-retry.sh -vPrc \
        "$entrypointIp":~/solana/config/faucet.json "$SOLANA_CONFIG_DIR"/faucet.json
    fi
    args=(
      --entrypoint "$entrypointIp:8001"
      --gossip-port 8001
      --rpc-port 8899
      --expected-shred-version "$(cat "$SOLANA_CONFIG_DIR"/shred-version)"
    )
    if [[ $nodeType = blockstreamer ]]; then
      args+=(
        --blockstream /tmp/solana-blockstream.sock
        --no-voting
        --dev-no-sigverify
        --enable-rpc-transaction-history
      )
    else
      if [[ -n $internalNodesLamports ]]; then
        args+=(--node-lamports "$internalNodesLamports")
      fi
    fi
    if [[ ! -f "$SOLANA_CONFIG_DIR"/validator-identity.json ]]; then
      solana-keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/validator-identity.json
    fi
    args+=(--identity "$SOLANA_CONFIG_DIR"/validator-identity.json)
    if [[ ! -f "$SOLANA_CONFIG_DIR"/vote-account.json ]]; then
      solana-keygen new --no-passphrase -so "$SOLANA_CONFIG_DIR"/vote-account.json
    fi
    args+=(--vote-account "$SOLANA_CONFIG_DIR"/vote-account.json)
    if [[ $airdropsEnabled != true ]]; then
      args+=(--no-airdrop)
    else
      args+=(--rpc-faucet-address "$entrypointIp:9900")
    fi
    if [[ -r "$SOLANA_CONFIG_DIR"/bank-hash ]]; then
      args+=(--expected-bank-hash "$(cat "$SOLANA_CONFIG_DIR"/bank-hash)")
    fi
    set -x
    # Add the faucet keypair to validators for convenient access from tools
    # like bench-tps and add to blocktreamers to run a faucet
    scp "$entrypointIp":~/solana/config/faucet.json "$SOLANA_CONFIG_DIR"/
    if [[ $nodeType = blockstreamer ]]; then
      if [[ $airdropsEnabled = true ]]; then
cat >> ~/solana/on-reboot <<EOF
        multinode-demo/faucet.sh > faucet.log 2>&1 &
EOF
      fi
      if [[ -f /.cert.pem ]]; then
        sudo install -o $UID -m 400 /.cert.pem /.key.pem .
        ls -l .cert.pem .key.pem
      fi
    fi
    args+=(--init-complete-file "$initCompleteFile")
    args+=($extraNodeArgs)
    maybeSkipAccountsCreation=
    if [[ $nodeIndex -le $extraPrimordialStakes ]]; then
      maybeSkipAccountsCreation="export SKIP_ACCOUNTS_CREATION=1"
    fi
    if [[ "$tmpfsAccounts" = "true" ]]; then
      args+=(--accounts /mnt/solana-accounts)
    fi
    if $maybeFullRpc; then
      args+=(--enable-rpc-transaction-history)
      args+=(--enable-extended-tx-metadata-storage)
    fi
    if $disableQuic; then
      args+=(--tpu-disable-quic)
    fi
    if $enableUdp; then
      args+=(--tpu-enable-udp)
    fi
    if [[ -n "$maybeWenRestart" ]]; then
      args+=(--wen-restart wen_restart.proto3)
      args+=(--wen-restart-coordinator "$maybeWenRestart")
    fi
cat >> ~/solana/on-reboot <<EOF
    $maybeSkipAccountsCreation
    nohup multinode-demo/validator.sh ${args[@]} > validator.log.\$now 2>&1 &
    pid=\$!
    oom_score_adj "\$pid" 1000
    disown
EOF
    ~/solana/on-reboot
    if $waitForNodeInit; then
      net/remote/remote-node-wait-init.sh 600
    fi
    if [[ $skipSetup != true && $nodeType != blockstreamer && -z $maybeSkipAccountsCreation ]]; then
      solana --url http://"$entrypointIp":8899 catchup config/validator-identity.json
      args=(
        --url http://"$entrypointIp":8899
      )
      if [[ $airdropsEnabled != true ]]; then
        args+=(--no-airdrop)
      fi
      if [[ -f config/validator-identity.json ]]; then
        args+=(--keypair config/validator-identity.json)
      fi
      if [[ ${extraPrimordialStakes} -eq 0 ]]; then
        echo "0 Primordial stakes, staking with $internalNodesStakeLamports"
        multinode-demo/delegate-stake.sh --vote-account "$SOLANA_CONFIG_DIR"/vote-account.json \
                                         --stake-account "$SOLANA_CONFIG_DIR"/stake-account.json \
                                         --force \
                                         "${args[@]}" "$internalNodesStakeLamports"
      else
        echo "Skipping staking with extra stakes: ${extraPrimordialStakes}"
      fi
    fi
    ;;
  *)
    echo "Error: unknown node type: $nodeType"
    exit 1
    ;;
  esac
  ;;
*)
  echo "Unknown deployment method: $deployMethod"
  exit 1
esac

================
File: net/remote/remote-sanity.sh
================
set -e
cd "$(dirname "$0")"/../..
sanityTargetIp="$1"
shift
deployMethod=
entrypointIp=
numNodes=
failOnValidatorBootupFailure=
airdropsEnabled=true
[[ -r deployConfig ]] || {
  echo deployConfig missing
  exit 1
}
source deployConfig
missing() {
  echo "Error: $1 not specified"
  exit 1
}
[[ -n $sanityTargetIp ]] || missing sanityTargetIp
[[ -n $deployMethod ]]   || missing deployMethod
[[ -n $entrypointIp ]]   || missing entrypointIp
[[ -n $numNodes ]]       || missing numNodes
[[ -n $failOnValidatorBootupFailure ]] || missing failOnValidatorBootupFailure
installCheck=true
rejectExtraNodes=false
while [[ $1 = -o ]]; do
  opt="$2"
  shift 2
  case $opt in
  noInstallCheck)
    installCheck=false
    ;;
  rejectExtraNodes)
    rejectExtraNodes=true
    ;;
  *)
    echo "Error: unknown option: $opt"
    exit 1
    ;;
  esac
done
if [[ -n $1 ]]; then
  export RUST_LOG="$1"
fi
source net/common.sh
loadConfigFile
case $deployMethod in
local|tar|skip)
  PATH="$HOME"/.cargo/bin:"$PATH"
  export USE_INSTALL=1
  solana_cli=solana
  solana_gossip=solana-gossip
  solana_install=agave-install
  ;;
*)
  echo "Unknown deployment method: $deployMethod"
  exit 1
esac
if $failOnValidatorBootupFailure; then
  numSanityNodes="$numNodes"
else
  numSanityNodes=1
  if $rejectExtraNodes; then
    echo "rejectExtraNodes cannot be used with failOnValidatorBootupFailure"
    exit 1
  fi
fi
echo "--- $sanityTargetIp: validators"
(
  set -x
  $solana_cli --url http://"$sanityTargetIp":8899 validators
)
echo "--- $sanityTargetIp: node count ($numSanityNodes expected)"
(
  set -x
  nodeArg="num-nodes"
  if $rejectExtraNodes; then
    nodeArg="num-nodes-exactly"
  fi
  $solana_gossip --allow-private-addr spy --entrypoint "$sanityTargetIp:8001" \
    --$nodeArg "$numSanityNodes" --timeout 60 \
)
echo "--- $sanityTargetIp: RPC API: getTransactionCount"
(
  set -x
  curl --retry 5 --retry-delay 2 --retry-connrefused \
    -X POST -H 'Content-Type: application/json' \
    -d '{"jsonrpc":"2.0","id":1, "method":"getTransactionCount"}' \
    http://"$sanityTargetIp":8899
)
if [[ "$airdropsEnabled" = true ]]; then
  echo "--- $sanityTargetIp: wallet sanity"
  (
    set -x
    scripts/wallet-sanity.sh --url http://"$sanityTargetIp":8899
  )
else
  echo "^^^ +++"
  echo "Note: wallet sanity is disabled as airdrops are disabled"
fi
if $installCheck && [[ -r update_manifest_keypair.json ]]; then
  echo "--- $sanityTargetIp: agave-install test"
  (
    set -x
    rm -rf install-data-dir
    $solana_install init \
      --no-modify-path \
      --data-dir install-data-dir \
      --url http://"$sanityTargetIp":8899 \
    $solana_install info
  )
fi
echo --- Pass

================
File: net/scripts/azure-provider.sh
================
cloud_DefaultZone() {
  echo "westus"
}
cloud_DefaultCustomMemoryGB() {
  :
}
cloud_RestartPreemptedInstances() {
  :
}
cloud_GetConfigValueFromInstanceName() {
  query="[?name=='$1']"
  key="[$2]"
  config_value=$(az vm list -d -o tsv --query "$query.$key")
}
cloud_GetResourceGroupFromInstanceName() {
  resourceGroup=$(az vm list -o tsv --query "[?name=='$1'].[resourceGroup]")
}
cloud_GetIdFromInstanceName() {
  id=$(az vm list -o tsv --query "[?name=='$1'].[id]")
}
__cloud_FindInstances() {
  case $1 in
    prefix)
      query="[?starts_with(name,'$2')]"
      ;;
    name)
      query="[?name=='$2']"
      ;;
    *)
      echo "Unknown filter command: $1"
      ;;
  esac
  keys="[name,publicIps,privateIps,location]"
  instances=()
  while read -r name publicIp privateIp location; do
    instances+=("$name:$publicIp:$privateIp:$location")
  done < <(az vm list -d -o tsv --query "$query.$keys")
  echo "${instances[*]}"
}
cloud_FindInstances() {
  __cloud_FindInstances prefix "$1"
}
cloud_FindInstance() {
  __cloud_FindInstances name "$1"
}
cloud_Initialize() {
  declare resourceGroup="$1"
  declare location="$2"
  declare nsgName=${resourceGroup}-nsg
  (
    set -x
    numGroup=$(az group list --query "length([?name=='$resourceGroup'])")
    if [[ $numGroup -eq 0 ]]; then
      echo Resource Group "$resourceGroup" does not exist.  Creating it now.
      az group create --name "$resourceGroup" --location "$location"
    else
      echo Resource group "$resourceGroup" already exists.
      az group show --name "$resourceGroup"
    fi
    az network nsg create --name "$nsgName" --resource-group "$resourceGroup"
  )
  create_nsg_rule() {
    ruleName="$1"
    ports="$2"
    access="$3"
    protocol="$4"
    priority="$5"
    (
      set -x
      az network nsg rule create -g "${resourceGroup}" --nsg-name "${nsgName}" -n "${ruleName}" \
                                 --priority "${priority}" --source-address-prefixes "*" --source-port-ranges "*" \
                                 --destination-address-prefixes "*" --destination-port-ranges "${ports}" --access "${access}" \
                                 --protocol "${protocol}"
    )
  }
  create_nsg_rule "InboundTCP" "8000-10000" "Allow" "Tcp" 1000
  create_nsg_rule "InboundUDP" "8000-10000" "Allow" "Udp" 1001
  create_nsg_rule "InboundHTTP" "80" "Allow" "Tcp" 1002
  create_nsg_rule "InboundNetworkExplorerAPI" "3001" "Allow" "Tcp" 1003
  create_nsg_rule "InboundDrone" "9900" "Allow" "Tcp" 1004
  create_nsg_rule "InboundJsonRpc" "8899-8900" "Allow" "Tcp" 1005
  create_nsg_rule "InboundRsync" "873" "Allow" "Tcp" 1006
  create_nsg_rule "InboundStun" "3478" "Allow" "Udp" 1007
  create_nsg_rule "InboundSSH" "22" "Allow" "Tcp" 1008
}
cloud_CreateInstances() {
  declare networkName="$1"
  declare namePrefix="$2"
  declare numNodes="$3"
  declare machineType="$4"
  declare zone="$5"
  declare optionalBootDiskSize="$6"
  declare optionalStartupScript="$7"
  declare optionalAddress="$8"
  declare optionalBootDiskType="${9}"
  declare -a nodes
  if [[ $numNodes = 1 ]]; then
    nodes=("$namePrefix")
  else
    for node in $(seq -f "${namePrefix}%0${#numNodes}g" 1 "$numNodes"); do
      nodes+=("$node")
    done
  fi
  nsgName=${networkName}-nsg
  declare -a args
  args=(
    --resource-group "$networkName"
    --tags testnet
    --image UbuntuLTS
    --size "$machineType"
    --ssh-key-values "$(cat "${sshPrivateKey}".pub)"
    --location "$zone"
    --nsg "$nsgName"
  )
  if [[ -n $optionalBootDiskSize ]]; then
    args+=(
      --os-disk-size-gb "$optionalBootDiskSize"
    )
  fi
  if [[ -n $optionalStartupScript ]]; then
    args+=(
      --custom-data "$optionalStartupScript"
    )
  fi
  if [[ -n $optionalBootDiskType ]]; then
    args+=(
      --storage-sku "$optionalBootDiskType"
    )
  else
    args+=(
      --storage-sku StandardSSD_LRS
    )
  fi
  if [[ -n $optionalAddress ]]; then
    [[ $numNodes = 1 ]] || {
      echo "Error: address may not be supplied when provisioning multiple nodes: $optionalAddress"
      exit 1
    }
    args+=(
      --public-ip-address "$optionalAddress"
    )
  fi
  (
    set -x
    for nodeName in "${nodes[@]}"; do
      az vm create --name "$nodeName" "${args[@]}" --no-wait
    done
    for nodeName in "${nodes[@]}"; do
      az vm wait --created --name "$nodeName" --resource-group "$networkName" --verbose --timeout 600
    done
  )
}
cloud_DeleteInstances() {
  if [[ ${
    echo No instances to delete
    return
  fi
  declare names=("${instances[@]/:*/}")
  (
    set -x
    id_list=()
    for instance in "${names[@]}"; do
      cloud_GetIdFromInstanceName "$instance"
      id_list+=("$id")
    done
    az vm delete --ids "${id_list[@]}" --yes --verbose --no-wait
  )
}
cloud_WaitForInstanceReady() {
  declare instanceName="$1"
  declare timeout="$4"
  cloud_GetResourceGroupFromInstanceName "$instanceName"
  az vm wait -g "$resourceGroup" -n "$instanceName" --created  --interval 10 --timeout "$timeout"
}
cloud_FetchFile() {
  declare instanceName="$1"
  declare publicIp="$2"
  declare remoteFile="$3"
  declare localFile="$4"
  cloud_GetConfigValueFromInstanceName "$instanceName" osProfile.adminUsername
  scp "${config_value}@${publicIp}:${remoteFile}" "$localFile"
}
cloud_CreateAndAttachPersistentDisk() {
  echo "ERROR: cloud_CreateAndAttachPersistentDisk is not yet implemented for azure"
  exit 1
}
cloud_StatusAll() {
  echo "ERROR: cloud_StatusAll is not yet implemented for azure"
}

================
File: net/scripts/colo_nodes
================
billiards|216.24.140.153|10.1.1.24|16|64|SATA|2000|NVME|1000|2|Denver

================
File: net/scripts/colo-node-onacquire.sh
================
SOLANA_LOCK_FILE="${SOLANA_LOCK_FILE:?}"
INSTANCE_NAME="${INSTANCE_NAME:?}"
PREEMPTIBLE="${PREEMPTIBLE:?}"
SSH_AUTHORIZED_KEYS="${SSH_AUTHORIZED_KEYS:?}"
SSH_PRIVATE_KEY_TEXT="${SSH_PRIVATE_KEY_TEXT:?}"
SSH_PUBLIC_KEY_TEXT="${SSH_PUBLIC_KEY_TEXT:?}"
NETWORK_INFO="${NETWORK_INFO:-"Network info unavailable"}"
CREATION_INFO="${CREATION_INFO:-"Creation info unavailable"}"
if [[ ! -f "${SOLANA_LOCK_FILE}" ]]; then
  exec 9>>"${SOLANA_LOCK_FILE}"
  flock -x -n 9 || ( echo "Failed to acquire lock!" 1>&2 && exit 1 )
  SOLANA_USER="${SOLANA_USER:?"SOLANA_USER undefined"}"
  {
    echo "export SOLANA_LOCK_USER=${SOLANA_USER}"
    echo "export SOLANA_LOCK_INSTANCENAME=${INSTANCE_NAME}"
    echo "export PREEMPTIBLE=${PREEMPTIBLE}"
    echo "[[ -v SSH_TTY && -f \"${HOME}/.solana-motd\" ]] && cat \"${HOME}/.solana-motd\" 1>&2"
  } >&9
  exec 9>&-
  cat > /solana-scratch/id_ecdsa <<EOF
${SSH_PRIVATE_KEY_TEXT}
EOF
  cat > /solana-scratch/id_ecdsa.pub <<EOF
${SSH_PUBLIC_KEY_TEXT}
EOF
  chmod 0600 /solana-scratch/id_ecdsa
  cat > /solana-scratch/authorized_keys <<EOF
${SSH_AUTHORIZED_KEYS}
${SSH_PUBLIC_KEY_TEXT}
EOF
  cp /solana-scratch/id_ecdsa "${HOME}/.ssh/id_ecdsa"
  cp /solana-scratch/id_ecdsa.pub "${HOME}/.ssh/id_ecdsa.pub"
  cp /solana-scratch/authorized_keys "${HOME}/.ssh/authorized_keys"
  cat > "${HOME}/.solana-motd" <<EOF
${NETWORK_INFO}
${CREATION_INFO}
EOF
  touch /solana-scratch/.instance-startup-complete
else
  exec 9<"${SOLANA_LOCK_FILE}" && flock -s 9 && . "${SOLANA_LOCK_FILE}" && exec 9>&-
  echo "${INSTANCE_NAME} candidate is already ${SOLANA_LOCK_INSTANCENAME}" 1>&2
  false
fi

================
File: net/scripts/colo-node-onfree.sh
================
SOLANA_LOCK_FILE="${SOLANA_LOCK_FILE:?}"
SECONDARY_DISK_MOUNT_POINT="${SECONDARY_DISK_MOUNT_POINT:?}"
SSH_AUTHORIZED_KEYS="${SSH_AUTHORIZED_KEYS:?}"
FORCE_DELETE="${FORCE_DELETE:?}"
RC=false
if [[ -f "${SOLANA_LOCK_FILE}" ]]; then
  exec 9<>"${SOLANA_LOCK_FILE}"
  flock -x -n 9 || ( echo "Failed to acquire lock!" 1>&2 && exit 1 )
  . "${SOLANA_LOCK_FILE}"
  if [[ "${SOLANA_LOCK_USER}" = "${SOLANA_USER}"  || -n "${FORCE_DELETE}" ]]; then
    CLEANUP_PID=$$
    CLEANUP_PIDS=()
    CLEANUP_PPIDS=()
    get_pids() {
      CLEANUP_PIDS=()
      CLEANUP_PPIDS=()
      declare line maybe_ppid maybe_pid
      while read -r line; do
        read -r maybe_ppid maybe_pid _ _ _ _ _ _ _ _ <<<"${line}"
        CLEANUP_PIDS+=( "${maybe_pid}" )
        CLEANUP_PPIDS+=( "${maybe_ppid}" )
      done < <(ps jxh | sort -rn -k2,2)
    }
    CLEANUP_PROC_CHAINS=()
    resolve_chains() {
      CLEANUP_PROC_CHAINS=()
      declare i pid ppid handled n
      for i in "${!CLEANUP_PIDS[@]}"; do
        pid=${CLEANUP_PIDS[${i}]}
        ppid=${CLEANUP_PPIDS[${i}]}
        handled=false
        for j in "${!CLEANUP_PROC_CHAINS[@]}"; do
          if grep -q "^${ppid}\b" <<<"${CLEANUP_PROC_CHAINS[${j}]}"; then
            CLEANUP_PROC_CHAINS[${j}]="${pid} ${CLEANUP_PROC_CHAINS[${j}]}"
            handled=true
            break
          elif grep -q "\b${pid}\$" <<<"${CLEANUP_PROC_CHAINS[${j}]}"; then
            CLEANUP_PROC_CHAINS[${j}]+=" ${ppid}"
            handled=true
          fi
        done
        if ! ${handled}; then
          n=${
          CLEANUP_PROC_CHAINS[${n}]="${pid} ${ppid}"
        fi
      done
    }
    while read -r SID; do
      screen -S "${SID}" -X quit
    done < <(screen -wipe 2>&1 | sed -e 's/^\s\+\([^[:space:]]\+\)\s.*/\1/;t;d')
    tmux kill-server &> /dev/null
    for SIG in INT TERM KILL; do
      get_pids
      if [[ ${
        break
      else
        resolve_chains
        for p in "${CLEANUP_PROC_CHAINS[@]}"; do
          if ! grep -q "\b${CLEANUP_PID}\b" <<<"${p}"; then
            read -ra TO_KILL <<<"${p}"
            N=${
            ROOT_PPID="${TO_KILL[$((N-1))]}"
            if [[ 1 -ne ${ROOT_PPID} ]]; then
              LAST_PID_IDX=$((N-2))
              for I in $(seq 0 ${LAST_PID_IDX}); do
                pid="${TO_KILL[${I}]}"
                kill "-${SIG}" "${pid}" &>/dev/null
              done
            fi
          fi
        done
        get_pids
        if [[ ${
          sleep 5
        fi
      fi
    done
    git clean -qxdff
    rm -f /solana-scratch/* /solana-scratch/.[^.]*
    cat > "${HOME}/.ssh/authorized_keys" <<EOAK
${SSH_AUTHORIZED_KEYS}
EOAK
    EXTERNAL_CONFIG_DIR="${SECONDARY_DISK_MOUNT_POINT}/config/"
    if [[ -d "${EXTERNAL_CONFIG_DIR}" ]]; then
      rm -rf "${EXTERNAL_CONFIG_DIR}"
    fi
    RC=true
  else
    echo "Invalid user: expected \"${SOLANA_LOCK_USER}\" got \"${SOLANA_USER}\"" 1>&2
  fi
  exec 9>&-
fi
${RC}

================
File: net/scripts/colo-provider.sh
================
declare COLO_PARALLELIZE=false
__cloud_colo_here="$(dirname "${BASH_SOURCE[0]}")"
source "${__cloud_colo_here}/colo-utils.sh"
cloud_DefaultZone() {
  echo "Denver"
}
cloud_DefaultCustomMemoryGB() {
  :
}
cloud_RestartPreemptedInstances() {
  :
}
__cloud_FindInstances() {
  declare HOST_NAME IP PRIV_IP STATUS ZONE LOCK_USER INSTNAME INSTANCES_TEXT
  declare filter=${1}
  declare onlyPreemptible=${2}
  instances=()
  if ! ${COLO_PARALLELIZE}; then
    colo_load_resources
    colo_load_availability false
  fi
  INSTANCES_TEXT="$(
    for AVAIL in "${COLO_RES_AVAILABILITY[@]}"; do
      IFS=$'\x1f' read -r HOST_NAME IP PRIV_IP STATUS ZONE LOCK_USER INSTNAME PREEMPTIBLE <<<"${AVAIL}"
      if [[ ${INSTNAME} =~ ${filter} ]]; then
        if [[ -n $onlyPreemptible && $PREEMPTIBLE == "false" ]]; then
          continue
        else
          printf "%-40s | publicIp=%-16s privateIp=%s zone=%s preemptible=%s\n" "${INSTNAME}" "${IP}" "${PRIV_IP}" "${ZONE}" "${PREEMPTIBLE}" 1>&2
          echo -e "${INSTNAME}:${IP}:${PRIV_IP}:${ZONE}"
        fi
      fi
    done | sort -t $'\x1f' -k1
  )"
  if [[ -n "${INSTANCES_TEXT}" ]]; then
    while read -r LINE; do
      instances+=( "${LINE}" )
    done <<<"${INSTANCES_TEXT}"
  fi
}
#
# cloud_FindInstances [namePrefix]
#
# Find instances with names matching the specified prefix
#
# For each matching instance, an entry in the `instances` array will be added with the
# following information about the instance:
#   "name:public IP:private IP"
cloud_FindInstances() {
  declare filter="^${1}.*"
  declare onlyPreemptible="${2}"
  __cloud_FindInstances "${filter}" "${onlyPreemptible}"
}
cloud_FindInstance() {
  declare name="^${1}$"
  declare onlyPreemptible="${2}"
  __cloud_FindInstances "${name}" "${onlyPreemptible}"
}
cloud_Initialize() {
  colo_load_resources
  if ${COLO_PARALLELIZE}; then
    colo_load_availability
  fi
}
cloud_CreateInstances() {
  declare namePrefix="${2}"
  declare numNodes="${3}"
  declare machineType="${4}"
  declare optionalPreemptible="${11}"
  declare sshPrivateKey="${12}"
  declare -a nodes
  if [[ ${numNodes} = 1 ]]; then
    nodes=("${namePrefix}")
  else
    for node in $(seq -f "${namePrefix}%0${#numNodes}g" 1 "${numNodes}"); do
      nodes+=("${node}")
    done
  fi
  if ${COLO_PARALLELIZE}; then
    declare HOST_NAME IP PRIV_IP STATUS ZONE LOCK_USER INSTNAME INDEX RES LINE
    declare -a AVAILABLE
    declare AVAILABLE_TEXT
    AVAILABLE_TEXT="$(
      for RES in "${COLO_RES_AVAILABILITY[@]}"; do
        IFS=$'\x1f' read -r HOST_NAME IP PRIV_IP STATUS ZONE LOCK_USER INSTNAME <<<"${RES}"
        if [[ "FREE" = "${STATUS}" ]]; then
          INDEX=$(colo_res_index_from_ip "${IP}")
          RES_MACH="${COLO_RES_MACHINE[${INDEX}]}"
          if colo_machine_types_compatible "${RES_MACH}" "${machineType}"; then
            if ! colo_node_is_requisitioned "${INDEX}" "${COLO_RES_REQUISITIONED[*]}"; then
              echo -e "${RES_MACH}\x1f${IP}"
            fi
          fi
        fi
      done | sort -nt $'\x1f' -k1,1
    )"
    if [[ -n "${AVAILABLE_TEXT}" ]]; then
      while read -r LINE; do
        AVAILABLE+=("${LINE}")
      done <<<"${AVAILABLE_TEXT}"
    fi
    if [[ ${#AVAILABLE[@]} -lt ${numNodes} ]]; then
      echo "Insufficient resources available to allocate ${numNodes} ${namePrefix}" 1>&2
      exit 1
    fi
    declare node
    declare AI=0
    for node in "${nodes[@]}"; do
      IFS=$'\x1f' read -r _ IP <<<"${AVAILABLE[${AI}]}"
      colo_node_requisition "${IP}" "${node}" >/dev/null
      AI=$((AI+1))
    done
  else
    declare RES_MACH node
    declare RI=0
    declare NI=0
    while [[ ${NI} -lt ${numNodes} && ${RI} -lt ${COLO_RES_N} ]]; do
      node="${nodes[${NI}]}"
      RES_MACH="${COLO_RES_MACHINE[${RI}]}"
      IP="${COLO_RES_IP[${RI}]}"
      if colo_machine_types_compatible "${RES_MACH}" "${machineType}"; then
        if colo_node_requisition "${IP}" "${node}" "${sshPrivateKey}" "${optionalPreemptible}" >/dev/null; then
          NI=$((NI+1))
        fi
      fi
      RI=$((RI+1))
    done
  fi
}
#
# cloud_DeleteInstances
#
# Deletes all the instances listed in the `instances` array
#
cloud_DeleteInstances() {
  declare forceDelete="${1}"
  declare _ IP _ _
  for instance in "${instances[@]}"; do
    IFS=':' read -r _ IP _ _ <<< "${instance}"
    colo_node_free "${IP}" "${forceDelete}" >/dev/null
  done
}
#
# cloud_WaitForInstanceReady [instanceName] [instanceIp] [instanceZone] [timeout]
#
# Return once the newly created VM instance is responding.  This function is cloud-provider specific.
#
cloud_WaitForInstanceReady() {
  #declare instanceName="${1}" # unused
  #declare instanceIp="${2}" # unused
  #declare timeout="${4}" # unused
  true
}
#
# cloud_FetchFile [instanceName] [publicIp] [remoteFile] [localFile]
#
# Fetch a file from the given instance.  This function uses a cloud-specific
# mechanism to fetch the file
#
cloud_FetchFile() {
  #declare instanceName="${1}" # unused
  declare publicIp="${2}"
  declare remoteFile="${3}"
  declare localFile="${4}"
  #declare zone="${5}" # unused
  scp \
    -o "StrictHostKeyChecking=no" \
    -o "UserKnownHostsFile=/dev/null" \
    -o "User=solana" \
    -o "LogLevel=ERROR" \
    -F /dev/null \
    "solana@${publicIp}:${remoteFile}" "${localFile}"
}
cloud_StatusAll() {
  declare HOST_NAME IP PRIV_IP STATUS ZONE LOCK_USER INSTNAME PREEMPTIBLE
  if ! ${COLO_PARALLELIZE}; then
    colo_load_resources
    colo_load_availability false
  fi
  for AVAIL in "${COLO_RES_AVAILABILITY[@]}"; do
    IFS=$'\x1f' read -r HOST_NAME IP PRIV_IP STATUS ZONE LOCK_USER INSTNAME PREEMPTIBLE <<<"${AVAIL}"
    printf "%-30s | publicIp=%-16s privateIp=%s status=%s who=%s zone=%s inst=%s preemptible=%s\n" "${HOST_NAME}" "${IP}" "${PRIV_IP}" "${STATUS}" "${LOCK_USER}" "${ZONE}" "${INSTNAME}" "${PREEMPTIBLE}"
  done
}

================
File: net/scripts/colo-utils.sh
================
declare -r SOLANA_LOCK_FILE="/home/solana/.solana.lock"
__colo_here="$(dirname "${BASH_SOURCE[0]}")"
source "${__colo_here}"/../common.sh
export COLO_RES_N=0
export COLO_RES_HOSTNAME=()
export COLO_RES_IP=()
export COLO_RES_IP_PRIV=()
export COLO_RES_CPU_CORES=()
export COLO_RES_RAM_GB=()
export COLO_RES_STORAGE_TYPE=()
export COLO_RES_STORAGE_CAP_GB=()
export COLO_RES_ADD_STORAGE_TYPE=()
export COLO_RES_ADD_STORAGE_CAP_GB=()
export COLO_RES_MACHINE=()
export COLO_RESOURCES_LOADED=false
colo_load_resources() {
  if ! ${COLO_RESOURCES_LOADED}; then
    while read -r LINE; do
      IFS='|' read -r H I PI C M ST SC AST ASC G Z <<<"${LINE}"
      COLO_RES_HOSTNAME+=( "${H}" )
      COLO_RES_IP+=( "${I}" )
      COLO_RES_IP_PRIV+=( "${PI}" )
      COLO_RES_CPU_CORES+=( "${C}" )
      COLO_RES_RAM_GB+=( "${M}" )
      COLO_RES_STORAGE_TYPE+=( "${ST}" )
      COLO_RES_STORAGE_CAP_GB+=( "${SC}" )
      COLO_RES_ADD_STORAGE_TYPE+=( "$(tr ',' $'\x1f' <<<"${AST}")" )
      COLO_RES_ADD_STORAGE_CAP_GB+=( "$(tr ',' $'\x1f' <<<"${ASC}")" )
      COLO_RES_MACHINE+=( "${G}" )
      COLO_RES_ZONE+=( "${Z}" )
      COLO_RES_N=$((COLO_RES_N+1))
    done < <(sort -nt'|' -k10,10 "${__colo_here}"/colo_nodes)
    COLO_RESOURCES_LOADED=true
  fi
}
declare COLO_RES_AVAILABILITY_CACHED=false
declare -ax COLO_RES_AVAILABILITY
colo_load_availability() {
  declare USE_CACHE=${1:-${COLO_RES_AVAILABILITY_CACHED}}
  declare LINE PRIV_IP STATUS LOCK_USER I IP HOST_NAME ZONE INSTNAME PREEMPTIBLE
  if ! ${USE_CACHE}; then
    COLO_RES_AVAILABILITY=()
    COLO_RES_REQUISITIONED=()
    while read -r LINE; do
      IFS=$'\x1f' read -r IP STATUS LOCK_USER INSTNAME PREEMPTIBLE <<< "${LINE}"
      I=$(colo_res_index_from_ip "${IP}")
      PRIV_IP="${COLO_RES_IP_PRIV[${I}]}"
      HOST_NAME="${COLO_RES_HOSTNAME[${I}]}"
      ZONE="${COLO_RES_ZONE[${I}]}"
      COLO_RES_AVAILABILITY+=( "$(echo -e "${HOST_NAME}\x1f${IP}\x1f${PRIV_IP}\x1f${STATUS}\x1f${ZONE}\x1f${LOCK_USER}\x1f${INSTNAME}\x1f${PREEMPTIBLE}")" )
    done < <(colo_node_status_all | sort -t $'\x1f' -k1)
    COLO_RES_AVAILABILITY_CACHED=true
  fi
}
colo_res_index_from_ip() {
  declare IP="${1}"
  for i in "${!COLO_RES_IP_PRIV[@]}"; do
    if [[ "${IP}" = "${COLO_RES_IP[${i}]}" || "${IP}" = "${COLO_RES_IP_PRIV[${i}]}" ]]; then
      echo "${i}"
      return 0
    fi
  done
  return 1
}
colo_instance_run() {
  declare IP=${1}
  declare CMD="${2}"
  declare OUT
  set +e
  OUT=$(ssh -l solana -o "StrictHostKeyChecking=no" -o "ConnectTimeout=3" -n "${IP}" "${CMD}" 2>&1)
  declare RC=$?
  set -e
  while read -r LINE; do
    echo -e "${IP}\x1f${RC}\x1f${LINE}"
    if [[ "${RC}" -ne 0 ]]; then
      echo "IP(${IP}) Err(${RC}) LINE(${LINE})" 1>&2
    fi
  done < <(tr -d $'\r' <<<"${OUT}")
  return ${RC}
}
colo_instance_run_foreach() {
  declare CMD
  if test 1 -eq $
    CMD="${1}"
    declare IPS=()
    for I in $(seq 0 $((COLO_RES_N-1))); do
      IPS+=( "${COLO_RES_IP[${I}]}" )
    done
    set "${IPS[@]}" "${CMD}"
  fi
  CMD="${*: -1}"
  for I in $(seq 0 $(($
    declare IP="${1}"
    colo_instance_run "${IP}" "${CMD}" &
    shift
  done
  wait
}
colo_whoami() {
  declare ME LINE SOL_USER EOL
  while read -r LINE; do
    declare IP RC
    IFS=$'\x1f' read -r IP RC SOL_USER EOL <<< "${LINE}"
    if [ "${RC}" -eq 0 ]; then
      [[ "${EOL}" = "EOL" ]] || echo "${FUNCNAME[0]}: Unexpected input \"${LINE}\"" 1>&2
      if [ -z "${ME}" ] || [ "${ME}" = "${SOL_USER}" ]; then
        ME="${SOL_USER}"
      else
        echo "Found conflicting username \"${SOL_USER}\" on ${IP}, expected \"${ME}\"" 1>&2
      fi
    fi
  done < <(colo_instance_run_foreach "[ -n \"\${SOLANA_USER}\" ] && echo -e \"\${SOLANA_USER}\\x1fEOL\"")
  echo "${ME}"
}
COLO_SOLANA_USER=""
colo_get_solana_user() {
  if [ -z "${COLO_SOLANA_USER}" ]; then
    COLO_SOLANA_USER=$(colo_whoami)
  fi
  echo "${COLO_SOLANA_USER}"
}
__colo_node_status_script() {
  cat <<EOF
  exec 3>&2
  exec 2>/dev/null  # Suppress stderr as the next call to exec fails most of
                    # the time due to ${SOLANA_LOCK_FILE} not existing and is running from a
                    # subshell where normal redirection doesn't work
  exec 9<"${SOLANA_LOCK_FILE}" && flock -s 9 && . "${SOLANA_LOCK_FILE}" && exec 9>&-
  echo -e "\${SOLANA_LOCK_USER}\\x1f\${SOLANA_LOCK_INSTANCENAME}\\x1f\${PREEMPTIBLE}\\x1fEOL"
  exec 2>&3
EOF
}
__colo_node_status_result_normalize() {
  declare IP RC US BY INSTNAME PREEMPTIBLE EOL
  declare ST="DOWN"
  IFS=$'\x1f' read -r IP RC US INSTNAME PREEMPTIBLE EOL <<< "${1}"
  if [ "${RC}" -eq 0 ]; then
    [[ "${EOL}" = "EOL" ]] || echo "${FUNCNAME[0]}: Unexpected input \"${1}\"" 1>&2
    if [ -n "${US}" ]; then
      BY="${US}"
      ST="HELD"
      if [[ -z "${INSTNAME}" ]]; then
        return
      fi
    else
      ST="FREE"
    fi
  fi
  echo -e $"${IP}\x1f${ST}\x1f${BY}\x1f${INSTNAME}\x1f${PREEMPTIBLE}"
}
colo_node_status() {
  declare IP="${1}"
  __colo_node_status_result_normalize "$(colo_instance_run "${IP}" "$(__colo_node_status_script)")"
}
colo_node_status_all() {
  declare LINE
  while read -r LINE; do
    __colo_node_status_result_normalize "${LINE}"
  done < <(colo_instance_run_foreach "$(__colo_node_status_script)")
}
export COLO_RES_REQUISITIONED=()
colo_node_requisition() {
  declare IP=${1}
  declare INSTANCE_NAME=${2}
  declare SSH_PRIVATE_KEY="${3}"
  declare PREEMPTIBLE="${4}"
  declare INDEX
  INDEX=$(colo_res_index_from_ip "${IP}")
  declare RC=false
  colo_instance_run "${IP}" "$(cat <<EOF
SOLANA_LOCK_FILE="${SOLANA_LOCK_FILE}"
INSTANCE_NAME="${INSTANCE_NAME}"
PREEMPTIBLE="${PREEMPTIBLE}"
SSH_AUTHORIZED_KEYS=''
SSH_PRIVATE_KEY_TEXT="$(<"${SSH_PRIVATE_KEY}")"
SSH_PUBLIC_KEY_TEXT="$(<"${SSH_PRIVATE_KEY}.pub")"
NETWORK_INFO="$(printNetworkInfo 2>/dev/null)"
CREATION_INFO="$(creationInfo 2>/dev/null)"
$(<"${__colo_here}"/colo-node-onacquire.sh)
EOF
  )"
  if [[ 0 -eq $? ]]; then
    COLO_RES_REQUISITIONED+=("${INDEX}")
    RC=true
  fi
  ${RC}
}
colo_node_is_requisitioned() {
  declare INDEX="${1}"
  declare REQ
  declare RC=false
  for REQ in "${COLO_RES_REQUISITIONED[@]}"; do
    if [[ ${REQ} -eq ${INDEX} ]]; then
      RC=true
      break
    fi
  done
  ${RC}
}
colo_machine_types_compatible() {
  declare MAYBE_MACH="${1}"
  declare WANT_MACH="${2}"
  declare COMPATIBLE=false
  if [[ "${MAYBE_MACH}" -ge "${WANT_MACH}" ]]; then
    COMPATIBLE=true
  fi
  ${COMPATIBLE}
}
colo_node_free() {
  declare IP=${1}
  declare FORCE_DELETE=${2}
  colo_instance_run "${IP}" "$(cat <<EOF
SOLANA_LOCK_FILE="${SOLANA_LOCK_FILE}"
SECONDARY_DISK_MOUNT_POINT="${SECONDARY_DISK_MOUNT_POINT}"
SSH_AUTHORIZED_KEYS=''
FORCE_DELETE="${FORCE_DELETE}"
$(<"${__colo_here}"/colo-node-onfree.sh)
EOF
  )"
}

================
File: net/scripts/create-solana-user.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
if grep -q solana /etc/passwd ; then
  echo "User solana already exists"
else
  adduser solana --gecos "" --disabled-password --quiet
  adduser solana sudo
  adduser solana adm
  echo "solana ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers
  id solana
  [[ -r /solana-scratch/id_ecdsa ]] || exit 1
  [[ -r /solana-scratch/id_ecdsa.pub ]] || exit 1
  sudo -u solana bash -c "
    echo 'PATH=\"/home/solana/.cargo/bin:$PATH\"' > /home/solana/.profile
    mkdir -p /home/solana/.ssh/
    cd /home/solana/.ssh/
    cp /solana-scratch/id_ecdsa.pub authorized_keys
    umask 377
    cp /solana-scratch/id_ecdsa id_ecdsa
    echo \"
      Host *
      BatchMode yes
      IdentityFile ~/.ssh/id_ecdsa
      StrictHostKeyChecking no
    \" > config
  "
fi

================
File: net/scripts/disable-background-upgrades.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
rm -rf /usr/lib/apt/apt.systemd.daily
rm -rf /usr/bin/unattended-upgrade
killall apt.systemd.daily || true
killall unattended-upgrade || true
while fuser /var/lib/dpkg/lock; do
  echo Waiting for lock release...
  sleep 1
done

================
File: net/scripts/ec2-provider.sh
================
cloud_DefaultZone() {
  echo "us-east-1b"
}
cloud_DefaultCustomMemoryGB() {
  :
}
cloud_RestartPreemptedInstances() {
  :
}
__cloud_GetRegion() {
  declare zone="$1"
  declare region="${zone:0:$((${#zone} - 1))}"
  echo "$region"
}
__cloud_SshPrivateKeyCheck() {
  if [[ -z $sshPrivateKey ]]; then
    echo Error: sshPrivateKey not defined
    exit 1
  fi
  if [[ ! -r $sshPrivateKey ]]; then
    echo "Error: file is not readable: $sshPrivateKey"
    exit 1
  fi
}
__cloud_FindInstances() {
  declare filter="$1"
  instances=()
  declare -a regions=("us-east-1" "us-east-2" "us-west-1" "us-west-2" "sa-east-1" "ap-northeast-2" \
   "ap-northeast-1" "ap-southeast-2" "ap-southeast-1" "ap-south-1" "eu-west-1" "eu-west-2" "eu-central-1" "ca-central-1")
  for region in "${regions[@]}"
  do
    declare name publicIp privateIp
    while read -r name publicIp privateIp zone; do
      printf "%-30s | publicIp=%-16s privateIp=%s zone=%s\n" "$name" "$publicIp" "$privateIp" "$zone"
      instances+=("$name:$publicIp:$privateIp:$zone")
    done < <(aws ec2 describe-instances \
              --region "$region" \
              --filters \
                "Name=tag:name,Values=$filter" \
                "Name=instance-state-name,Values=pending,running" \
              --query "Reservations[].Instances[].[InstanceId,PublicIpAddress,PrivateIpAddress,Placement.AvailabilityZone]" \
              --output text \
      )
  done
}
cloud_FindInstances() {
  declare namePrefix="$1"
  __cloud_FindInstances "$namePrefix*"
}
cloud_FindInstance() {
  declare name="$1"
  __cloud_FindInstances "$name"
}
cloud_Initialize() {
  declare networkName="$1"
  declare zone="$2"
  declare region=
  region=$(__cloud_GetRegion "$zone")
  __cloud_SshPrivateKeyCheck
  aws ec2 delete-key-pair --region "$region" --key-name "$networkName"
  aws ec2 import-key-pair --region "$region" --key-name "$networkName" \
    --public-key-material file://"${sshPrivateKey}".pub
  declare rules
  rules=$(cat "$(dirname "${BASH_SOURCE[0]}")"/ec2-security-group-config.json)
  aws ec2 delete-security-group --region "$region" --group-name "$networkName" || true
  aws ec2 create-security-group --region "$region" --group-name "$networkName" --description "Created automatically by $0"
  aws ec2 authorize-security-group-ingress --output table --region "$region" --group-name "$networkName" --cli-input-json "$rules"
}
cloud_CreateInstances() {
  declare networkName="$1"
  declare namePrefix="$2"
  declare numNodes="$3"
  declare machineType="$4"
  declare zone="$5"
  declare optionalBootDiskSize="$6"
  declare optionalStartupScript="$7"
  declare optionalAddress="$8"
  declare region=
  region=$(__cloud_GetRegion "$zone")
  case $region in
  us-east-1)
    imageName="ami-0fba9b33b5304d8b4"
    ;;
  us-east-2)
    imageName="ami-0e04554247365d806"
    ;;
  us-west-1)
    imageName="ami-07390b6ff5934a238"
    ;;
  us-west-2)
    imageName="ami-03804ed633fe58109"
    ;;
  sa-east-1)
    imageName="ami-0f1678b6f63a0f923"
    ;;
  ap-northeast-2)
    imageName="ami-0695e34e31339c3ff"
    ;;
  ap-northeast-1)
    imageName="ami-003371bfa26192744"
    ;;
  ap-southeast-2)
    imageName="ami-0401c9e2f645b5557"
    ;;
  ap-southeast-1)
    imageName="ami-08050c889a630f1bd"
    ;;
  ap-south-1)
    imageName="ami-04184c12996409633"
    ;;
  eu-central-1)
    imageName="ami-054e21e355db24124"
    ;;
  eu-west-1)
    imageName="ami-0727f3c2d4b0226d5"
    ;;
  eu-west-2)
    imageName="ami-068f09e337d7da0c4"
    ;;
  ca-central-1)
    imageName="ami-06ed08059bdc08fc9"
    ;;
  *)
    usage "Unsupported region: $region"
    ;;
  esac
  declare -a args
  args=(
    --key-name "$networkName"
    --count "$numNodes"
    --region "$region"
    --placement "AvailabilityZone=$zone"
    --security-groups "$networkName"
    --image-id "$imageName"
    --instance-type "$machineType"
    --tag-specifications "ResourceType=instance,Tags=[{Key=name,Value=$namePrefix}]"
  )
  if [[ -n $optionalBootDiskSize ]]; then
    args+=(
      --block-device-mapping "[{\"DeviceName\": \"/dev/sda1\", \"Ebs\": { \"VolumeSize\": $optionalBootDiskSize }}]"
    )
  fi
  if [[ -n $optionalStartupScript ]]; then
    args+=(
      --user-data "file://$optionalStartupScript"
    )
  fi
  if [[ -n $optionalAddress ]]; then
    [[ $numNodes = 1 ]] || {
      echo "Error: address may not be supplied when provisioning multiple nodes: $optionalAddress"
      exit 1
    }
  fi
  (
    set -x
    aws ec2 run-instances --output table "${args[@]}"
  )
  if [[ -n $optionalAddress ]]; then
    cloud_FindInstance "$namePrefix"
    if [[ ${
      echo "Failed to find newly created instance: $namePrefix"
    fi
    declare instanceId
    IFS=: read -r instanceId publicIp privateIp zone < <(echo "${instances[0]}")
    (
      set -x
      sleep 30
      declare region=
      region=$(__cloud_GetRegion "$zone")
      aws ec2 associate-address \
        --instance-id "$instanceId" \
        --region "$region" \
        --allocation-id "$optionalAddress"
    )
  fi
}
cloud_DeleteInstances() {
  if [[ ${
    echo No instances to delete
    return
  fi
  for instance in "${instances[@]}"; do
    declare name="${instance/:*/}"
    declare zone="${instance/*:/}"
    declare region=
    region=$(__cloud_GetRegion "$zone")
    (
      set -x
      aws ec2 terminate-instances --output table --region "$region" --instance-ids "$name"
    )
  done
  for instance in "${instances[@]}"; do
    declare name="${instance/:*/}"
    declare zone="${instance/*:/}"
    declare region=
    region=$(__cloud_GetRegion "$zone")
    while true; do
      declare instanceState
      instanceState=$(\
        aws ec2 describe-instances \
          --region "$region" \
          --instance-ids "$name" \
          --query "Reservations[].Instances[].State.Name" \
          --output text \
      )
      echo "$name: $instanceState"
      if [[ $instanceState = terminated ]]; then
        break;
      fi
      sleep 2
    done
  done
}
cloud_WaitForInstanceReady() {
  declare instanceName="$1"
  declare instanceIp="$2"
  declare timeout="$4"
  timeout "${timeout}"s bash -c "set -o pipefail; until ping -c 3 $instanceIp | tr - _; do echo .; done"
}
cloud_FetchFile() {
  declare instanceName="$1"
  declare publicIp="$2"
  declare remoteFile="$3"
  declare localFile="$4"
  __cloud_SshPrivateKeyCheck
  (
    set -x
    scp \
      -o "StrictHostKeyChecking=no" \
      -o "UserKnownHostsFile=/dev/null" \
      -o "User=solana" \
      -o "IdentityFile=$sshPrivateKey" \
      -o "LogLevel=ERROR" \
      -F /dev/null \
      "solana@$publicIp:$remoteFile" "$localFile"
  )
}
cloud_CreateAndAttachPersistentDisk() {
  echo "ERROR: cloud_CreateAndAttachPersistentDisk is not yet implemented for ec2"
  exit 1
}
cloud_StatusAll() {
  echo "ERROR: cloud_StatusAll is not yet implemented for ec2"
}

================
File: net/scripts/ec2-security-group-config.json
================
{
    "IpPermissions": [
        {
            "PrefixListIds": [],
            "FromPort": 80,
            "IpRanges": [
                {
                    "CidrIp": "0.0.0.0/0",
                    "Description": "http for block explorer"
                }
            ],
            "ToPort": 80,
            "IpProtocol": "tcp",
            "UserIdGroupPairs": [],
            "Ipv6Ranges": [
                {
                    "CidrIpv6": "::/0",
                    "Description": "http for block explorer"
                }
            ]
        },
        {
            "PrefixListIds": [],
            "FromPort": 8000,
            "IpRanges": [
                {
                    "Description": "validator UDP range",
                    "CidrIp": "0.0.0.0/0"
                }
            ],
            "ToPort": 10000,
            "IpProtocol": "udp",
            "UserIdGroupPairs": [],
            "Ipv6Ranges": [
                {
                    "CidrIpv6": "::/0",
                    "Description": "validator UDP range"
                }
            ]
        },
        {
            "PrefixListIds": [],
            "FromPort": 22,
            "IpRanges": [
                {
                    "CidrIp": "0.0.0.0/0",
                    "Description": "ssh"
                }
            ],
            "ToPort": 22,
            "IpProtocol": "tcp",
            "UserIdGroupPairs": [],
            "Ipv6Ranges": [
                {
                    "CidrIpv6": "::/0",
                    "Description": "ssh"
                }
            ]
        },
        {
            "PrefixListIds": [],
            "FromPort": 3001,
            "IpRanges": [
                {
                    "Description": "blockexplorer http API port",
                    "CidrIp": "0.0.0.0/0"
                }
            ],
            "ToPort": 3001,
            "IpProtocol": "tcp",
            "UserIdGroupPairs": [],
            "Ipv6Ranges": [
                {
                    "CidrIpv6": "::/0",
                    "Description": "blockexplorer http API port"
                }
            ]
        },
        {
            "PrefixListIds": [],
            "FromPort": 3443,
            "IpRanges": [
                {
                    "Description": "blockexplorer https API port",
                    "CidrIp": "0.0.0.0/0"
                }
            ],
            "ToPort": 3443,
            "IpProtocol": "tcp",
            "UserIdGroupPairs": [],
            "Ipv6Ranges": [
                {
                    "CidrIpv6": "::/0",
                    "Description": "blockexplorer https API port"
                }
            ]
        },
        {
            "PrefixListIds": [],
            "FromPort": 8000,
            "IpRanges": [
                {
                    "Description": "validator TCP range",
                    "CidrIp": "0.0.0.0/0"
                }
            ],
            "ToPort": 10000,
            "IpProtocol": "tcp",
            "UserIdGroupPairs": [],
            "Ipv6Ranges": [
                {
                    "CidrIpv6": "::/0",
                    "Description": "validator TCP range"
                }
            ]
        },
        {
            "PrefixListIds": [],
            "FromPort": 8,
            "IpRanges": [
                {
                    "CidrIp": "0.0.0.0/0",
                    "Description": "allow ping"
                }
            ],
            "ToPort": -1,
            "IpProtocol": "icmp",
            "UserIdGroupPairs": [],
            "Ipv6Ranges": [
                {
                    "CidrIpv6": "::/0",
                    "Description": "allow ping"
                }
            ]
        }
    ]
}

================
File: net/scripts/gce-provider.sh
================
cloud_DefaultZone() {
  echo "us-west1-b"
}
cloud_DefaultCustomMemoryGB() {
  echo 64
}
cloud_RestartPreemptedInstances() {
  declare filter="$1"
  declare name status zone
  while read -r name status zone; do
    echo "Starting $status instance: $name"
    (
      set -x
      gcloud compute instances start --zone "$zone" "$name"
    )
  done < <(gcloud compute instances list \
             --filter "$filter" \
             --format 'value(name,status,zone)' \
           | grep TERMINATED)
}
__cloud_FindInstances() {
  declare filter="$1"
  instances=()
  declare name zone publicIp privateIp status
  while read -r name publicIp privateIp status zone; do
    printf "%-30s | publicIp=%-16s privateIp=%s status=%s zone=%s\n" "$name" "$publicIp" "$privateIp" "$status" "$zone"
    instances+=("$name:$publicIp:$privateIp:$zone")
  done < <(gcloud compute instances list \
             --filter "$filter" \
             --format 'value(name,networkInterfaces[0].accessConfigs[0].natIP,networkInterfaces[0].networkIP,status,zone)' \
           | grep RUNNING)
  while read -r name status zone; do
    privateIp=TERMINATED
    publicIp=TERMINATED
    printf "%-30s | publicIp=%-16s privateIp=%s status=%s zone=%s\n" "$name" "$publicIp" "$privateIp" "$status" "$zone"
    instances+=("$name:$publicIp:$privateIp:$zone")
  done < <(gcloud compute instances list \
             --filter "$filter" \
             --format 'value(name,status,zone)' \
           | grep TERMINATED)
}
cloud_FindInstances() {
  declare namePrefix="$1"
  __cloud_FindInstances "name~^$namePrefix"
}
cloud_FindInstance() {
  declare name="$1"
  __cloud_FindInstances "name=$name"
}
cloud_Initialize() {
  declare networkName="$1"
  echo "Note: one day create $networkName firewall rules programmatically instead of assuming the 'testnet' tag exists"
}
cloud_CreateInstances() {
  declare networkName="$1"
  declare namePrefix="$2"
  declare numNodes="$3"
  declare machineType="$4"
  declare zone="$5"
  declare optionalBootDiskSize="$6"
  declare optionalStartupScript="$7"
  declare optionalAddress="$8"
  declare optionalBootDiskType="${9:-pd-ssd}"
  declare optionalAdditionalDiskSize="${10}"
  declare optionalPreemptible="${11}"
  imageName="ubuntu-2404-noble-amd64-v20250709 --image-project ubuntu-os-cloud"
  declare -a nodes
  if [[ $numNodes = 1 ]]; then
    nodes=("$namePrefix")
  else
    for node in $(seq -f "${namePrefix}%0${#numNodes}g" 1 "$numNodes"); do
      nodes+=("$node")
    done
  fi
  declare -a args
  args=(
    --zone "$zone"
    --tags testnet
    --metadata "testnet=$networkName"
    --maintenance-policy TERMINATE
    --restart-on-failure
    --scopes compute-rw
  )
  args+=(--image $imageName)
  if [[ $optionalPreemptible = true ]]; then
    args+=(--preemptible)
  fi
  for word in $machineType; do
    args+=("${word//%20/ }")
  done
  if [[ -n $optionalBootDiskSize ]]; then
    args+=(
      --boot-disk-size "${optionalBootDiskSize}GB"
    )
  fi
  if [[ -n $optionalStartupScript ]]; then
    args+=(
      --metadata-from-file "startup-script=$optionalStartupScript"
    )
  fi
  if [[ -n $optionalBootDiskType ]]; then
    args+=(
        --boot-disk-type "${optionalBootDiskType}"
    )
  fi
  if [[ -n $optionalAddress ]]; then
    [[ $numNodes = 1 ]] || {
      echo "Error: address may not be supplied when provisioning multiple nodes: $optionalAddress"
      exit 1
    }
    args+=(
      --address "$optionalAddress"
    )
  fi
  (
    set -x
    gcloud beta compute instances create "${nodes[@]}" "${args[@]}"
  )
  if [[ -n $optionalAdditionalDiskSize ]]; then
    if [[ $numNodes = 1 ]]; then
      (
        set -x
        cloud_CreateAndAttachPersistentDisk "${namePrefix}" "$optionalAdditionalDiskSize" "pd-ssd" "$zone"
      )
    else
      for node in $(seq -f "${namePrefix}%0${#numNodes}g" 1 "$numNodes"); do
        (
          set -x
          cloud_CreateAndAttachPersistentDisk "${node}" "$optionalAdditionalDiskSize" "pd-ssd" "$zone"
        )
      done
    fi
  fi
}
cloud_DeleteInstances() {
  if [[ ${
    echo No instances to delete
    return
  fi
  declare names=("${instances[@]/:*/}")
  declare zones=("${instances[@]/*:/}")
  declare unique_zones=()
  read -r -a unique_zones <<< "$(echo "${zones[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ')"
  for zone in "${unique_zones[@]}"; do
    set -x
    gcloud beta compute instances delete --zone "$zone" --quiet "${names[@]}" || true
  done
}
cloud_WaitForInstanceReady() {
  declare instanceName="$1"
  declare instanceIp="$2"
  declare timeout="$4"
  if [[ $instanceIp = "TERMINATED" ]]; then
    return 1
  fi
  timeout "${timeout}"s bash -c "set -o pipefail; until ping -c 3 $instanceIp | tr - _; do echo .; done"
}
cloud_FetchFile() {
  declare instanceName="$1"
  declare publicIp="$2"
  declare remoteFile="$3"
  declare localFile="$4"
  declare zone="$5"
  if [[ $publicIp = "TERMINATED" ]]; then
    return 1
  fi
  (
    set -x
    gcloud compute scp --zone "$zone" "$instanceName:$remoteFile" "$localFile"
  )
}
cloud_CreateAndAttachPersistentDisk() {
  declare instanceName="$1"
  declare diskSize="$2"
  declare diskType="$3"
  declare zone="$4"
  diskName="${instanceName}-pd"
  gcloud beta compute disks create "$diskName" \
    --size "$diskSize" \
    --type "$diskType" \
    --zone "$zone"
  gcloud compute instances attach-disk "$instanceName" \
    --disk "$diskName" \
    --zone "$zone"
  gcloud compute instances set-disk-auto-delete "$instanceName" \
    --disk "$diskName" \
    --zone "$zone" \
    --auto-delete
}
cloud_StatusAll() {
  echo "ERROR: cloud_StatusAll is not yet implemented for GCE"
}

================
File: net/scripts/gce-self-destruct.sh
================
__gce_sd_here="$(dirname "${BASH_SOURCE[0]}")"
__gce_sd_conf="${__gce_sd_here}/gce-self-destruct.conf"
gce_metadata_req() {
  declare endpoint url
  endpoint="$1"
  url="http://metadata.google.internal/computeMetadata/v1/$endpoint"
  curl -sf -H Metadata-Flavor:Google "$url"
}
unix_to_at_time() {
  declare unix="$1"
  date --date="@$unix" "+%Y%m%d%H%M.%S"
}
timeout_to_destruct() {
  declare timeout_sec now_unix
  declare timeout_hrs=$1
  timeout_sec=$((timeout_hrs * 60 * 60))
  now_unix=$(date +%s)
  echo $((now_unix + timeout_sec))
}
relative_timespan()
{
  declare timeSpan="$1"
  declare -a units divs
  units+=( s ); divs+=( 60 )
  units+=( m ); divs+=( 60 )
  units+=( h ); divs+=( 24 )
  units+=( d ); divs+=( 7 )
  units+=( w ); divs+=( 52 )
  numUnits="${#units[@]}"
  units+=( y ); divs+=( 100 )
  declare -a outs
  declare i div remain
  for (( i=0; i < "$numUnits"; i++ )); do
    div="${divs[$i]}"
    [[ "$timeSpan" -lt "$div" ]] && break
    remain="$((timeSpan % div))"
    timeSpan="$((timeSpan / div))"
    outs+=( "$remain" )
  done
  outs+=( "$timeSpan" )
  numOut="${#outs[@]}"
  out1="$((numOut-1))"
  out2="$((numOut-2))"
  if [[ "$numOut" -eq 1 ]] || \
    [[ "$numOut" -ge "$numUnits" && \
      "${outs[out1]}" -ge "${divs[out1]}" ]]; then
    printf "%d%s" "${outs[out1]}" "${units[out1]}"
  else
    printf "%d%s%02d%s" "${outs[out1]}" \
      "${units[out1]}" "${outs[out2]}" "${units[out2]}"
  fi
}
gce_self_destruct_ttl() {
  declare colorize="${1:-true}"
  declare prefix="${2}"
  declare suffix="${3}"
  declare output=0
  if [[ -f "${__gce_sd_conf}" ]]; then
    source "${__gce_sd_conf}"
    declare ttl pttl color
    ttl="$((destruct - $(date +%s)))"
    if [[ "$ttl" -lt 0 ]]; then
      ttl=0
    fi
    pttl="$(relative_timespan "$ttl")"
    color=
    if [[ ttl -lt 3600 ]]; then
      color="\033[01;31m"
    fi
    output="${prefix}${pttl}${suffix}"
    if $colorize; then
      output="${color}${output}\033[01;00m"
    fi
  fi
  echo -e "$output"
}
gce_self_destruct_setup() {
  declare destruct at_time zone
  destruct="$(timeout_to_destruct "$1")"
  at_time="$(unix_to_at_time "$destruct")"
  zone=$(gce_metadata_req "instance/zone")
  zone=$(basename "$zone")
  cat >"${__gce_sd_conf}" <<EOF
export destruct=$destruct
export zone=$zone
EOF
  at -t "$at_time" <<EOF
bash -i <<'EOF2'
source /solana-scratch/gce-self-destruct.sh
gce_self_destruct_check
EOF2
EOF
}
gce_self_destruct_check() {
  if [[ -f "${__gce_sd_conf}" ]]; then
    source "${__gce_sd_conf}"
    declare now gcloudBin
    now=$(date +%s)
    if [[ "$now" -ge "$destruct" ]]; then
      gcloudBin="$(command -v gcloud)"
      gcloudBin="${gcloudBin:-/snap/bin/gcloud}"
      "$gcloudBin" compute instances delete --quiet "$(hostname)" --zone "$zone"
    else
      at -t "$(unix_to_at_time "$destruct")" <<EOF
bash -i <<'OEF2'
source /solana-scratch/gce-self-destruct.sh
gce_self_destruct_check
EOF2
EOF
    fi
  fi
}
gce_self_destruct_motd() {
  declare ttl
  ttl="$(gce_self_destruct_ttl)"
  echo -e '\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n'
  if [[ -n "${ttl}" ]]; then
    echo -e "\tThis instance will self-destruct in ${ttl}!"
  else
    echo -e "\tThis instance will NOT self-destruct. YOU are responsible for deleting it!"
  fi
  echo -e '\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n'
}
gce_self_destruct_ps1() {
  declare ttl
  ttl="$(gce_self_destruct_ttl true "[T-" "]")"
  ttl="${ttl:-"[T-~~~~~]"}"
  echo "!${ttl}"
}

================
File: net/scripts/install-ag.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get update
apt-get --assume-yes install silversearcher-ag

================
File: net/scripts/install-at.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get update
apt-get --assume-yes install at

================
File: net/scripts/install-certbot.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt install -y certbot
cat > /certbot-restore.sh <<'EOF'
set -e
domain=$1
email=$2
if [[ $USER != root ]]; then
  echo "Run as root"
  exit 1
fi
if [[ -f /.cert.pem ]]; then
  echo "Certificate already initialized"
  exit 0
fi
set -x
if [[ -r letsencrypt.tgz ]]; then
  tar -C / -zxf letsencrypt.tgz
fi
cd /
rm -f letsencrypt.tgz
maybeDryRun=
certbot certonly --standalone -d "$domain" --email "$email" --agree-tos -n $maybeDryRun
tar zcf letsencrypt.tgz /etc/letsencrypt
ls -l letsencrypt.tgz
rm -f /.key.pem /.cert.pem
cp /etc/letsencrypt/live/$domain/privkey.pem /.key.pem
cp /etc/letsencrypt/live/$domain/cert.pem /.cert.pem
EOF
chmod +x /certbot-restore.sh

================
File: net/scripts/install-docker.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get update
apt-get install -y \
  apt-transport-https \
  ca-certificates \
  curl \
  software-properties-common \
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository \
  "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
apt-get update
apt-get install -y docker-ce
cat > /lib/systemd/system/docker.service <<EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service
Wants=network-online.target
[Service]
Type=notify
ExecStart=/usr/bin/dockerd -H unix://
ExecReload=/bin/kill -s HUP '$MAINPID'
TimeoutSec=0
RestartSec=2
Restart=always
StartLimitBurst=3
StartLimitInterval=60s
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
LimitMEMLOCK=infinity
TasksMax=infinity
Delegate=yes
KillMode=process
[Install]
WantedBy=multi-user.target
EOF
cat > /etc/docker/daemon.json <<EOF
{
  "ipv6": true,
  "fixed-cidr-v6": "2001:db8:1::/64"
}
EOF
systemctl daemon-reload
systemctl enable --now /lib/systemd/system/docker.service
if id solana; then
  addgroup solana docker
fi
docker run hello-world

================
File: net/scripts/install-earlyoom.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
sysctl -w kernel.sysrq=1
echo kernel.sysrq=1 >> /etc/sysctl.conf
if command -v earlyoom; then
  systemctl status earlyoom
else
  apt-get install --quiet --yes earlyoom
  cat > earlyoom <<OOM
  EARLYOOM_ARGS="-m 20"
OOM
  cp earlyoom /etc/default/
  rm earlyoom
  systemctl stop earlyoom
  systemctl enable earlyoom
  systemctl start earlyoom
fi

================
File: net/scripts/install-iftop.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get --assume-yes install iftop

================
File: net/scripts/install-jq.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get --assume-yes install jq

================
File: net/scripts/install-libssl.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get update
apt-get --assume-yes install libssl-dev

================
File: net/scripts/install-perf.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get --assume-yes install linux-tools-common linux-tools-generic "linux-tools-$(uname -r)"
echo -1 | tee /proc/sys/kernel/perf_event_paranoid
echo 0 | tee /proc/sys/kernel/kptr_restrict

================
File: net/scripts/install-rsync.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
apt-get --assume-yes install rsync

================
File: net/scripts/localtime.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
ln -sf /usr/share/zoneinfo/America/Los_Angeles /etc/localtime

================
File: net/scripts/mount-additional-disk.sh
================
set -x
mount_point=/mnt/extra-disk
disk=sdb
if ! lsblk | grep -q ${disk} ; then
  echo "${disk} does not exist"
else
  sudo mount /dev/"$disk" "$mount_point" || true
  if mount | grep -q ${disk} ; then
    echo "${disk} is mounted"
  else
    sudo mkfs.ext4 -F /dev/"$disk"
    sudo mkdir -p "$mount_point"
    sudo mount /dev/"$disk" "$mount_point"
    sudo chmod a+w "$mount_point"
    if ! mount | grep -q ${mount_point} ; then
      echo "${disk} failed to mount!"
      exit 1
    fi
  fi
fi

================
File: net/scripts/network-config.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
sudo sysctl -w net.core.rmem_max=134217728
sudo sysctl -w net.core.wmem_max=134217728
sudo sysctl -w vm.max_map_count=1000000
echo "* - nofile 1000000" | sudo tee -a /etc/security/limits.conf
echo "MaxAuthTries 60" | sudo tee -a /etc/ssh/sshd_config
sudo systemctl restart ssh

================
File: net/scripts/remove-docker-interface.sh
================
set -ex
[[ $(uname) = Linux ]] || exit 1
[[ $USER = root ]] || exit 1
ip link delete docker0 || true

================
File: net/scripts/rsync-retry.sh
================
for i in $(seq 1 5); do
  (
    set -x
    rsync "$@"
  ) && exit 0
  echo Retry "$i"...
done

================
File: net/.gitignore
================
/config/
/log

================
File: net/common.sh
================
netDir=$(
  cd "$(dirname "${BASH_SOURCE[0]}")" || exit
  echo "$PWD"
)
netConfigDir="$netDir"/config
mkdir -p "$netConfigDir"
SOLANA_ROOT="$netDir"/..
source "$SOLANA_ROOT"/scripts/configure-metrics.sh
configFile="$netConfigDir/config"
geoipConfigFile="$netConfigDir/geoip.yml"
entrypointIp=
publicNetwork=
netBasename=
sshPrivateKey=
letsEncryptDomainName=
externalNodeSshKey=
sshOptions=()
validatorIpList=()
validatorIpListPrivate=()
validatorIpListZone=()
clientIpList=()
clientIpListPrivate=()
clientIpListZone=()
blockstreamerIpList=()
blockstreamerIpListPrivate=()
blockstreamerIpListZone=()
buildSshOptions() {
  sshOptions=(
    -o "ConnectTimeout=20"
    -o "BatchMode=yes"
    -o "StrictHostKeyChecking=no"
    -o "UserKnownHostsFile=/dev/null"
    -o "User=solana"
    -o "IdentitiesOnly=yes"
    -o "IdentityFile=$sshPrivateKey"
    -o "LogLevel=ERROR"
  )
  [[ -z $externalNodeSshKey ]] || sshOptions+=(-o "IdentityFile=$externalNodeSshKey")
}
loadConfigFile() {
  [[ -r $configFile ]] || usage "Config file unreadable: $configFile"
  source "$configFile"
  [[ -n "$publicNetwork" ]] || usage "Config file invalid, publicNetwork unspecified: $configFile"
  [[ -n "$netBasename" ]] || usage "Config file invalid, netBasename unspecified: $configFile"
  [[ -n $sshPrivateKey ]] || usage "Config file invalid, sshPrivateKey unspecified: $configFile"
  [[ ${
  [[ ${
  [[ ${
  if $publicNetwork; then
    entrypointIp=${validatorIpList[0]}
  else
    entrypointIp=${validatorIpListPrivate[0]}
  fi
  buildSshOptions
  configureMetrics
}
urlencode() {
  declare s="$1"
  declare l=$((${
  for i in $(seq 0 $l); do
    declare c="${s:$i:1}"
    case $c in
      [a-zA-Z0-9.~_-])
        echo -n "$c"
        ;;
      *)
        printf '%%%02X' "'$c"
        ;;
    esac
  done
}
SOLANA_CONFIG_DIR=$SOLANA_ROOT/config
clear_config_dir() {
  declare config_dir="$1"
  _setup_secondary_mount
  (
    set -x
    rm -rf "${config_dir:?}/"
    rm -rf "$config_dir"
    mkdir -p "$config_dir"
  )
  _setup_secondary_mount
}
SECONDARY_DISK_MOUNT_POINT=/mnt/extra-disk
_setup_secondary_mount() {
  (
    set -x
    if [[ -d $SECONDARY_DISK_MOUNT_POINT ]] && \
      [[ -w $SECONDARY_DISK_MOUNT_POINT ]]; then
      mkdir -p $SECONDARY_DISK_MOUNT_POINT/config
      rm -rf "$SOLANA_CONFIG_DIR"
      ln -sfT $SECONDARY_DISK_MOUNT_POINT/config "$SOLANA_CONFIG_DIR"
    fi
  )
}

================
File: net/gce.sh
================
set -e
here=$(dirname "$0")
source "$here"/common.sh
cloudProvider=$(basename "$0" .sh)
bootDiskType=""
case $cloudProvider in
gce)
  # shellcheck source=net/scripts/gce-provider.sh
  source "$here"/scripts/gce-provider.sh
  cpuBootstrapLeaderMachineType="--custom-cpu 24 --min-cpu-platform Intel%20Skylake --custom-vm-type n1"
  clientMachineType="--custom-cpu 16 --custom-memory 20GB"
  blockstreamerMachineType="--machine-type n1-standard-8"
  selfDestructHours=8
  ;;
ec2)
  source "$here"/scripts/ec2-provider.sh
  cpuBootstrapLeaderMachineType=m5.4xlarge
  clientMachineType=c5.2xlarge
  blockstreamerMachineType=m5.4xlarge
  selfDestructHours=0
  ;;
azure)
  source "$here"/scripts/azure-provider.sh
  cpuBootstrapLeaderMachineType=Standard_D16s_v3
  clientMachineType=Standard_D16s_v3
  blockstreamerMachineType=Standard_D16s_v3
  selfDestructHours=0
  ;;
colo)
  source "$here"/scripts/colo-provider.sh
  cpuBootstrapLeaderMachineType=0
  clientMachineType=0
  blockstreamerMachineType=0
  selfDestructHours=0
  ;;
*)
  echo "Error: Unknown cloud provider: $cloudProvider"
  ;;
esac
prefix=testnet-dev-${USER//[^A-Za-z0-9]/}
additionalValidatorCount=2
clientNodeCount=0
blockstreamer=false
validatorBootDiskSizeInGb=100
clientBootDiskSizeInGb=75
validatorAdditionalDiskSizeInGb=
externalNodes=false
failOnValidatorBootupFailure=true
preemptible=true
evalInfo=false
tmpfsAccounts=false
defaultCustomMemoryGB="$(cloud_DefaultCustomMemoryGB)"
customMemoryGB="$defaultCustomMemoryGB"
publicNetwork=false
letsEncryptDomainName=
customMachineType=
customAddress=
zones=()
containsZone() {
  local e match="$1"
  shift
  for e; do [[ "$e" == "$match" ]] && return 0; done
  return 1
}
usage() {
  exitcode=0
  if [[ -n "$1" ]]; then
    exitcode=1
    echo "Error: $*"
  fi
  cat <<EOF
usage: $0 [create|config|delete] [common options] [command-specific options]
Manage testnet instances
 create - create a new testnet (implies 'config')
 config - configure the testnet and write a config file describing it
 delete - delete the testnet
 info   - display information about the currently configured testnet
 status - display status information of all resources
 common options:
   -p [prefix]      - Optional common prefix for instance names to avoid
                      collisions (default: $prefix)
   -z [zone]        - Zone(s) for the nodes (default: $(cloud_DefaultZone))
                      If specified multiple times, the validators will be evenly
                      distributed over all specified zones and
                      client/blockstreamer nodes will be created in the first
                      zone
   -x               - append to the existing configuration instead of creating a
                      new configuration
   --allow-boot-failures
                    - Discard from config validator nodes that didn't bootup
                      successfully
 create-specific options:
   -n [number]      - Number of additional validators (default: $additionalValidatorCount)
   -c [number]      - Number of client nodes (default: $clientNodeCount)
   -u               - Include a Blockstreamer (default: $blockstreamer)
   -P               - Use public network IP addresses (default: $publicNetwork)
   -a [address]     - Address to be be assigned to the Blockstreamer if present,
                      otherwise the bootstrap validator.
                      * For GCE, [address] is the "name" of the desired External
                        IP Address.
                      * For EC2, [address] is the "allocation ID" of the desired
                        Elastic IP.
   -d [disk-type]   - Specify a boot disk type (default None) Use pd-ssd to get ssd on GCE.
   --letsencrypt [dns name]
                    - Attempt to generate a TLS certificate using this
                      DNS name (useful only when the -a and -P options
                      are also provided)
   --custom-machine-type [type]
                    - Set a custom machine type.
$(
  if [[ -n "$defaultCustomMemoryGB" ]]; then
    echo "   --custom-memory-gb"
    echo "                    - Set memory size for custom machine type in GB (default: $defaultCustomMemoryGB)"
  fi
)
   --validator-additional-disk-size-gb [number]
                    - Add an additional [number] GB SSD to all validators to store the config directory.
                      If not set, config will be written to the boot disk by default.
                      Only supported on GCE.
   --dedicated      - Use dedicated instances for additional validators
                      (by default preemptible instances are used to reduce
                      cost).  Note that the bootstrap validator,
                      blockstreamer and client nodes are always dedicated.
                      Set this flag on colo to prevent your testnet from being pre-empted by nightly test automation.
   --self-destruct-hours [number]
                    - Specify lifetime of the allocated instances in hours. 0 to
                      disable. Only supported on GCE. (default: $selfDestructHours)
   --validator-boot-disk-size-gb [number]
                    - Specify validator boot disk size in gb.
   --client-machine-type [type]
                    - custom client machine type
   --tmpfs-accounts - Put accounts directory on a swap-backed tmpfs volume
 config-specific options:
   -P               - Use public network IP addresses (default: $publicNetwork)
 delete-specific options:
   --reclaim-preemptible-reservations
                    - If set, reclaims all reservations on colo nodes that were not created with --dedicated.
                      This behavior does not filter by testnet name or owner.  Only implemented on colo.
   --reclaim-all-reservations
                    - If set, reclaims all reservations on all colo nodes, regardless of owner, pre-emptibility, or creator.
 info-specific options:
   --eval           - Output in a form that can be eval-ed by a shell: eval \$(gce.sh info --eval)
   none
EOF
  exit $exitcode
}
command=$1
[[ -n $command ]] || usage
shift
[[ $command = create || $command = config || $command = info || $command = delete || $command = status ]] ||
  usage "Invalid command: $command"
shortArgs=()
while [[ -n $1 ]]; do
  if [[ ${1:0:2} = -- ]]; then
    if [[ $1 = --letsencrypt ]]; then
      letsEncryptDomainName="$2"
      shift 2
    elif [[ $1 = --validator-additional-disk-size-gb ]]; then
      validatorAdditionalDiskSizeInGb="$2"
      shift 2
    elif [[ $1 == --machine-type* || $1 == --custom-cpu* ]]; then
      shortArgs+=("$1")
      shift
    elif [[ $1 == --allow-boot-failures ]]; then
      failOnValidatorBootupFailure=false
      shift
    elif [[ $1 == --dedicated ]]; then
      preemptible=false
      shift
    elif [[ $1 == --eval ]]; then
      evalInfo=true
      shift
    elif [[ $1 == --enable-gpu ]]; then
      echo "GPU support has been dropped, --enable-gpu is a noop"
      shift
    elif [[ $1 = --custom-machine-type ]]; then
      customMachineType="$2"
      shift 2
    elif [[ $1 = --client-machine-type ]]; then
      clientMachineType="$2"
      shift 2
    elif [[ $1 = --validator-boot-disk-size-gb ]]; then
      validatorBootDiskSizeInGb="$2"
      shift 2
    elif [[ $1 == --self-destruct-hours ]]; then
      maybeTimeout=$2
      if [[ $maybeTimeout =~ ^[0-9]+$ ]]; then
        selfDestructHours=$maybeTimeout
      else
        echo "  Invalid parameter ($maybeTimeout) to $1"
        usage 1
      fi
      shift 2
    elif [[ $1 == --reclaim-preemptible-reservations ]]; then
      reclaimOnlyPreemptibleReservations=true
      shift
    elif [[ $1 == --reclaim-all-reservations ]]; then
      reclaimAllReservations=true
      shift
    elif [[ $1 == --tmpfs-accounts ]]; then
      tmpfsAccounts=true
      shift
    elif [[ $1 == --custom-memory-gb ]]; then
      customMemoryGB=$2
      shift 2
    else
      usage "Unknown long option: $1"
    fi
  else
    shortArgs+=("$1")
    shift
  fi
done
while getopts "h?p:Pn:c:r:z:gG:a:d:uxf" opt "${shortArgs[@]}"; do
  case $opt in
  h | \?)
    usage
    ;;
  p)
    [[ ${OPTARG//[^A-Za-z0-9-]/} == "$OPTARG" ]] || usage "Invalid prefix: \"$OPTARG\", alphanumeric only"
    prefix=$OPTARG
    ;;
  P)
    publicNetwork=true
    ;;
  n)
    additionalValidatorCount=$OPTARG
    ;;
  c)
    clientNodeCount=$OPTARG
    ;;
  z)
    containsZone "$OPTARG" "${zones[@]}" || zones+=("$OPTARG")
    ;;
  g)
    echo "GPU support has been dropped, -g argument is a noop"
    ;;
  G)
    echo "GPU support has been dropped, -G argument is a noop"
    ;;
  a)
    customAddress=$OPTARG
    ;;
  d)
    bootDiskType=$OPTARG
    ;;
  u)
    blockstreamer=true
    ;;
  x)
    externalNodes=true
    ;;
  *)
    usage "unhandled option: $opt"
    ;;
  esac
done
[[ ${
[[ -z $1 ]] || usage "Unexpected argument: $1"
if [[ $cloudProvider = ec2 ]]; then
  sshPrivateKey="$HOME/.ssh/solana-net-id_$prefix"
else
  sshPrivateKey="$netConfigDir/id_$prefix"
fi
case $cloudProvider in
gce)
  if [[ "$tmpfsAccounts" = "true" ]]; then
    cpuBootstrapLeaderMachineType+=" --local-ssd interface=nvme"
    if [[ $customMemoryGB -lt 100 ]]; then
      echo -e '\nWarning: At least 100GB of system RAM is recommending with `--tmpfs-accounts` (see `--custom-memory-gb`)\n'
    fi
  fi
  cpuBootstrapLeaderMachineType+=" --custom-memory ${customMemoryGB}GB"
  ;;
ec2|azure|colo)
  if [[ -n $validatorAdditionalDiskSizeInGb ]] ; then
    usage "--validator-additional-disk-size-gb currently only supported with cloud provider: gce"
  fi
  if [[ "$tmpfsAccounts" = "true" ]]; then
    usage "--tmpfs-accounts only supported on cloud provider: gce"
  fi
  if [[ "$customMemoryGB" != "$defaultCustomMemoryGB" ]]; then
    usage "--custom-memory-gb only supported on cloud provider: gce"
  fi
  ;;
*)
  echo "Error: Unknown cloud provider: $cloudProvider"
  ;;
esac
case $cloudProvider in
  gce | ec2 | azure)
    maybePreemptible="never preemptible"
    ;;
  colo)
    maybePreemptible=$preemptible
    ;;
  *)
    echo "Error: Unknown cloud provider: $cloudProvider"
    ;;
esac
if [[ $reclaimOnlyPreemptibleReservations == "true" && $reclaimAllReservations == "true" ]]; then
  usage "Cannot set both --reclaim-preemptible-reservations and --reclaim-all-reservations.  Set one or none"
fi
if [[ -n $reclaimAllReservations || -n $reclaimOnlyPreemptibleReservations ]]; then
  forceDelete="true"
fi
if [[ -n "$customMachineType" ]] ; then
  bootstrapLeaderMachineType="$customMachineType"
else
  bootstrapLeaderMachineType="$cpuBootstrapLeaderMachineType"
fi
validatorMachineType=$bootstrapLeaderMachineType
blockstreamerMachineType=$bootstrapLeaderMachineType
cloud_ForEachInstance() {
  declare cmd="$1"
  shift
  [[ -n $cmd ]] || { echo cloud_ForEachInstance: cmd not specified; exit 1; }
  declare count=1
  for info in "${instances[@]}"; do
    declare name publicIp privateIp
    IFS=: read -r name publicIp privateIp zone < <(echo "$info")
    eval "$cmd" "$name" "$publicIp" "$privateIp" "$zone" "$count" "$@"
    count=$((count + 1))
  done
}
zoneLocation() {
  declare zone="$1"
  case "$zone" in
  us-west1*)
    echo "[45.5946, -121.1787]"
    ;;
  us-central1*)
    echo "[41.2619, -95.8608]"
    ;;
  us-east1*)
    echo "[33.1960, -80.0131]"
    ;;
  asia-east2*)
    echo "[22.3193, 114.1694]"
    ;;
  asia-northeast1*)
    echo "[35.6762, 139.6503]"
    ;;
  asia-northeast2*)
    echo "[34.6937, 135.5023]"
    ;;
  asia-south1*)
    echo "[19.0760, 72.8777]"
    ;;
  asia-southeast1*)
    echo "[1.3404, 103.7090]"
    ;;
  australia-southeast1*)
    echo "[-33.8688, 151.2093]"
    ;;
  europe-north1*)
    echo "[60.5693, 27.1878]"
    ;;
  europe-west2*)
    echo "[51.5074, -0.1278]"
    ;;
  europe-west3*)
    echo "[50.1109, 8.6821]"
    ;;
  europe-west4*)
    echo "[53.4386, 6.8355]"
    ;;
  europe-west6*)
    echo "[47.3769, 8.5417]"
    ;;
  northamerica-northeast1*)
    echo "[45.5017, -73.5673]"
    ;;
  southamerica-east1*)
    echo "[-23.5505, -46.6333]"
    ;;
  *)
    ;;
  esac
}
prepareInstancesAndWriteConfigFile() {
  $metricsWriteDatapoint "testnet-deploy net-config-begin=1"
  if $externalNodes; then
    echo "Appending to existing config file"
    echo "externalNodeSshKey=$sshPrivateKey" >> "$configFile"
  else
    rm -f "$geoipConfigFile"
    cat >> "$configFile" <<EOF
netBasename=$prefix
publicNetwork=$publicNetwork
sshPrivateKey=$sshPrivateKey
letsEncryptDomainName=$letsEncryptDomainName
export TMPFS_ACCOUNTS=$tmpfsAccounts
EOF
  fi
  touch "$geoipConfigFile"
  buildSshOptions
  cloud_RestartPreemptedInstances "$prefix"
  fetchPrivateKey() {
    declare nodeName
    declare nodeIp
    declare nodeZone
    IFS=: read -r nodeName nodeIp _ nodeZone < <(echo "${instances[0]}")
    timeout_sec=90
    cloud_WaitForInstanceReady "$nodeName" "$nodeIp" "$nodeZone" "$timeout_sec"
    if [[ ! -r $sshPrivateKey ]]; then
      echo "Fetching $sshPrivateKey from $nodeName"
      (
        set -o pipefail
        for i in $(seq 1 60); do
          set -x
          cloud_FetchFile "$nodeName" "$nodeIp" /solana-scratch/id_ecdsa "$sshPrivateKey" "$nodeZone" &&
            cloud_FetchFile "$nodeName" "$nodeIp" /solana-scratch/id_ecdsa.pub "$sshPrivateKey.pub" "$nodeZone" &&
              break
          set +x
          sleep 1
          echo "Retry $i..."
        done
      )
      chmod 400 "$sshPrivateKey"
      ls -l "$sshPrivateKey"
    fi
  }
  recordInstanceIp() {
    declare name="$1"
    declare publicIp="$2"
    declare privateIp="$3"
    declare zone="$4"
    declare failOnFailure="$6"
    declare arrayName="$7"
    if [ "$publicIp" = "TERMINATED" ] || [ "$privateIp" = "TERMINATED" ]; then
      if $failOnFailure; then
        exit 1
      else
        return 0
      fi
    fi
    ok=true
    echo "Waiting for $name to finish booting..."
    (
      set +e
      fetchPrivateKey || exit 1
      for i in $(seq 1 60); do
        (
          set -x
          timeout --preserve-status --foreground 20s ssh "${sshOptions[@]}" "$publicIp" "ls -l /solana-scratch/.instance-startup-complete"
        )
        ret=$?
        if [[ $ret -eq 0 ]]; then
          echo "$name has booted."
          exit 0
        fi
        sleep 5
        echo "Retry $i..."
      done
      echo "$name failed to boot."
      exit 1
    ) || ok=false
    if ! $ok; then
      if $failOnFailure; then
        exit 1
      fi
    else
      {
        echo "$arrayName+=($publicIp)  # $name"
        echo "${arrayName}Private+=($privateIp)  # $name"
        echo "${arrayName}Zone+=($zone)  # $name"
      } >> "$configFile"
      declare latlng=
      latlng=$(zoneLocation "$zone")
      if [[ -n $latlng ]]; then
        echo "$publicIp: $latlng" >> "$geoipConfigFile"
      fi
    fi
  }
  if $externalNodes; then
    echo "Bootstrap validator is already configured"
  else
    echo "Looking for bootstrap validator instance..."
    cloud_FindInstance "$prefix-bootstrap-validator"
    [[ ${
      echo "Unable to find bootstrap validator"
      exit 1
    }
    echo "validatorIpList=()" >> "$configFile"
    echo "validatorIpListPrivate=()" >> "$configFile"
    cloud_ForEachInstance recordInstanceIp true validatorIpList
  fi
  if [[ $additionalValidatorCount -gt 0 ]]; then
    numZones=${
    if [[ $additionalValidatorCount -gt $numZones ]]; then
      numNodesPerZone=$((additionalValidatorCount / numZones))
      numLeftOverNodes=$((additionalValidatorCount % numZones))
    else
      numNodesPerZone=1
      numLeftOverNodes=0
    fi
    for ((i=((numZones - 1)); i >= 0; i--)); do
      zone=${zones[i]}
      if [[ $i -eq 0 ]]; then
        numNodesPerZone=$((numNodesPerZone + numLeftOverNodes))
      fi
      echo "Looking for additional validator instances in $zone ..."
      cloud_FindInstances "$prefix-$zone-validator"
      declare numInstances=${
      if [[ $numInstances -ge $numNodesPerZone || ( ! $failOnValidatorBootupFailure && $numInstances -gt 0 ) ]]; then
        cloud_ForEachInstance recordInstanceIp "$failOnValidatorBootupFailure" validatorIpList
      else
        echo "Unable to find additional validators"
        if $failOnValidatorBootupFailure; then
          exit 1
        fi
      fi
    done
  fi
  if ! $externalNodes; then
    echo "clientIpList=()" >> "$configFile"
    echo "clientIpListPrivate=()" >> "$configFile"
  fi
  echo "Looking for client bencher instances..."
  cloud_FindInstances "$prefix-client"
  [[ ${
    cloud_ForEachInstance recordInstanceIp true clientIpList
  }
  if ! $externalNodes; then
    echo "blockstreamerIpList=()" >> "$configFile"
    echo "blockstreamerIpListPrivate=()" >> "$configFile"
  fi
  echo "Looking for blockstreamer instances..."
  cloud_FindInstances "$prefix-blockstreamer"
  [[ ${
    cloud_ForEachInstance recordInstanceIp true blockstreamerIpList
  }
  echo "Wrote $configFile"
  $metricsWriteDatapoint "testnet-deploy net-config-complete=1"
}
delete() {
  $metricsWriteDatapoint "testnet-deploy net-delete-begin=1"
  case $cloudProvider in
    gce | ec2 | azure)
      filter="$prefix-"
      ;;
    colo)
      if [[ -n $forceDelete ]]; then
        filter=".*-"
      else
        filter="$prefix-"
      fi
      ;;
    *)
      echo "Error: Unknown cloud provider: $cloudProvider"
      ;;
  esac
  echo "Searching for instances: $filter"
  cloud_FindInstances "$filter" "$reclaimOnlyPreemptibleReservations"
  if [[ ${
    echo "No instances found matching '$filter'"
  else
    cloud_DeleteInstances $forceDelete
  fi
  wait
  if $externalNodes; then
    echo "Let's not delete the current configuration file"
  else
    rm -f "$configFile"
  fi
  $metricsWriteDatapoint "testnet-deploy net-delete-complete=1"
}
create_error_cleanup() {
  declare RC=$?
  if [[ "$RC" -ne 0 ]]; then
    delete
  fi
  exit $RC
}
case $command in
delete)
  delete
  ;;
create)
  [[ -n $additionalValidatorCount ]] || usage "Need number of nodes"
  delete
  $metricsWriteDatapoint "testnet-deploy net-create-begin=1"
  if $failOnValidatorBootupFailure; then
    trap create_error_cleanup EXIT
  fi
  rm -rf "$sshPrivateKey"{,.pub}
  ssh-keygen -t rsa -N '' -f "$sshPrivateKey"
  printNetworkInfo() {
    cat <<EOF
==[ Network composition ]===============================================================
  Bootstrap validator = $bootstrapLeaderMachineType
  Additional validators = $additionalValidatorCount x $validatorMachineType
  Client(s) = $clientNodeCount x $clientMachineType
  Blockstreamer = $blockstreamer
========================================================================================
EOF
  }
  printNetworkInfo
  creationDate=$(date)
  creationInfo() {
    cat <<EOF
  Instance running since: $creationDate
========================================================================================
EOF
  }
  declare startupScript="$netConfigDir"/instance-startup-script.sh
  cat > "$startupScript" <<EOF
#!/usr/bin/env bash
# autogenerated at $(date)
set -ex
if [[ -f /solana-scratch/.instance-startup-complete ]]; then
  echo reboot
  $(
    cd "$here"/scripts/
    if [[ -n $validatorAdditionalDiskSizeInGb ]]; then
      cat mount-additional-disk.sh
    fi
    cat ../../scripts/ulimit-n.sh
  )
  if [[ -x ~solana/solana/on-reboot ]]; then
    sudo -u solana ~solana/solana/on-reboot
  fi
  # Skip most setup on instance reboot
  exit 0
fi
cat > /etc/motd <<EOM
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  This instance has not been fully configured.
  See startup script log messages in /var/log/syslog for status:
    $ sudo cat /var/log/syslog | egrep \\(startup-script\\|cloud-init\)
  To block until setup is complete, run:
    $ until [[ -f /solana-scratch/.instance-startup-complete ]]; do sleep 1; done
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
$(creationInfo)
EOM
# Place the generated private key at /solana-scratch/id_ecdsa so it's retrievable by anybody
mkdir -m 0777 /solana-scratch
cat > /solana-scratch/id_ecdsa <<EOK
$(cat "$sshPrivateKey")
EOK
cat > /solana-scratch/id_ecdsa.pub <<EOK
$(cat "$sshPrivateKey.pub")
EOK
chmod 444 /solana-scratch/id_ecdsa
USER=\$(id -un)
export DEBIAN_FRONTEND=noninteractive
$(
  cd "$here"/scripts/
  cat \
    disable-background-upgrades.sh \
    create-solana-user.sh \
    install-ag.sh \
    install-at.sh \
    install-certbot.sh \
    install-earlyoom.sh \
    install-iftop.sh \
    install-jq.sh \
    install-libssl.sh \
    install-rsync.sh \
    install-perf.sh \
    localtime.sh \
    network-config.sh \
    remove-docker-interface.sh \
  if [[ -n $validatorAdditionalDiskSizeInGb ]]; then
    cat mount-additional-disk.sh
  fi
  if [[ $selfDestructHours -gt 0 ]]; then
    cat <<EOSD
cat >/solana-scratch/gce-self-destruct.sh <<'EOS'
$(cat gce-self-destruct.sh)
EOS
EOSD
    cat <<'EOSD'
cat >/solana-scratch/gce-self-destruct-ps1.sh <<'EOS'
source "$(dirname "$0")/gce-self-destruct.sh"
gce_self_destruct_ps1
EOS
chmod +x /solana-scratch/gce-self-destruct-ps1.sh
cat >>~solana/.profile <<'EOS'
source "/solana-scratch/gce-self-destruct.sh"
gce_self_destruct_motd
export PS1='\[\e]0;\u@\h: \w\a\]${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]$(/solana-scratch/gce-self-destruct-ps1.sh):\[\033[01;34m\]\w\[\033[00m\]\$ '
EOS
EOSD
    cat <<EOSD
source /solana-scratch/gce-self-destruct.sh
gce_self_destruct_setup $selfDestructHours
EOSD
  fi
)
cat > /etc/motd <<EOM
See startup script log messages in /var/log/syslog for status:
  $ sudo cat /var/log/syslog | egrep \\(startup-script\\|cloud-init\)
$(printNetworkInfo)
$(creationInfo)
EOM
$(
  if [[ "$tmpfsAccounts" = "true" ]]; then
    cat <<'EOSWAP'
tmpfsMountPoint=/mnt/solana-accounts
swapDevice="/dev/nvme0n1"
swapUUID="43076c54-7840-4e59-a368-2d164f8984fb"
mkswap --uuid "$swapUUID" "$swapDevice"
echo "UUID=$swapUUID swap swap defaults 0 0" >> /etc/fstab
swapon "UUID=$swapUUID"
mkdir -p -m 0777 "$tmpfsMountPoint"
echo "tmpfs $tmpfsMountPoint tmpfs defaults,size=300G 0 0" >> /etc/fstab
mount "$tmpfsMountPoint"
EOSWAP
  fi
)
touch /solana-scratch/.instance-startup-complete
EOF
  if $blockstreamer; then
    blockstreamerAddress=$customAddress
  else
    bootstrapLeaderAddress=$customAddress
  fi
  for zone in "${zones[@]}"; do
    cloud_Initialize "$prefix" "$zone"
  done
  if $externalNodes; then
    echo "Bootstrap validator is already configured"
  else
    cloud_CreateInstances "$prefix" "$prefix-bootstrap-validator" 1 \
      "$bootstrapLeaderMachineType" "${zones[0]}" "$validatorBootDiskSizeInGb" \
      "$startupScript" "$bootstrapLeaderAddress" "$bootDiskType" "$validatorAdditionalDiskSizeInGb" \
      "$maybePreemptible" "$sshPrivateKey"
  fi
  if [[ $additionalValidatorCount -gt 0 ]]; then
    num_zones=${
    if [[ $additionalValidatorCount -gt $num_zones ]]; then
      numNodesPerZone=$((additionalValidatorCount / num_zones))
      numLeftOverNodes=$((additionalValidatorCount % num_zones))
    else
      numNodesPerZone=1
      numLeftOverNodes=0
    fi
    for ((i=((num_zones - 1)); i >= 0; i--)); do
      zone=${zones[i]}
      if [[ $i -eq 0 ]]; then
        numNodesPerZone=$((numNodesPerZone + numLeftOverNodes))
      fi
      cloud_CreateInstances "$prefix" "$prefix-$zone-validator" "$numNodesPerZone" \
        "$validatorMachineType" "$zone" "$validatorBootDiskSizeInGb" \
        "$startupScript" "" "$bootDiskType" "$validatorAdditionalDiskSizeInGb" \
        "$preemptible" "$sshPrivateKey" &
    done
    wait
  fi
  if [[ $clientNodeCount -gt 0 ]]; then
    cloud_CreateInstances "$prefix" "$prefix-client" "$clientNodeCount" \
      "$clientMachineType" "${zones[0]}" "$clientBootDiskSizeInGb" \
      "$startupScript" "" "$bootDiskType" "" "$maybePreemptible" "$sshPrivateKey"
  fi
  if $blockstreamer; then
    cloud_CreateInstances "$prefix" "$prefix-blockstreamer" "1" \
      "$blockstreamerMachineType" "${zones[0]}" "$validatorBootDiskSizeInGb" \
      "$startupScript" "$blockstreamerAddress" "$bootDiskType" "" "$maybePreemptible" "$sshPrivateKey"
  fi
  $metricsWriteDatapoint "testnet-deploy net-create-complete=1"
  prepareInstancesAndWriteConfigFile
  ;;
config)
  failOnValidatorBootupFailure=false
  prepareInstancesAndWriteConfigFile
  ;;
info)
  loadConfigFile
  printNode() {
    declare nodeType=$1
    declare ip=$2
    declare ipPrivate=$3
    declare zone=$4
    printf "  %-16s | %-15s | %-15s | %s\n" "$nodeType" "$ip" "$ipPrivate" "$zone"
  }
  if $evalInfo; then
    echo "NET_NUM_VALIDATORS=${#validatorIpList[@]}"
    echo "NET_NUM_CLIENTS=${#clientIpList[@]}"
    echo "NET_NUM_BLOCKSTREAMERS=${#blockstreamerIpList[@]}"
  else
    printNode "Node Type" "Public IP" "Private IP" "Zone"
    echo "-------------------+-----------------+-----------------+--------------"
  fi
  nodeType=bootstrap-validator
  if [[ ${
    for i in $(seq 0 $(( ${
      ipAddress=${validatorIpList[$i]}
      ipAddressPrivate=${validatorIpListPrivate[$i]}
      zone=${validatorIpListZone[$i]}
      if $evalInfo; then
        echo "NET_VALIDATOR${i}_IP=$ipAddress"
      else
        printNode $nodeType "$ipAddress" "$ipAddressPrivate" "$zone"
      fi
      nodeType=validator
    done
  fi
  if [[ ${
    for i in $(seq 0 $(( ${
      ipAddress=${clientIpList[$i]}
      ipAddressPrivate=${clientIpListPrivate[$i]}
      zone=${clientIpListZone[$i]}
      if $evalInfo; then
        echo "NET_CLIENT${i}_IP=$ipAddress"
      else
        printNode client "$ipAddress" "$ipAddressPrivate" "$zone"
      fi
    done
  fi
  if [[ ${
    for i in $(seq 0 $(( ${
      ipAddress=${blockstreamerIpList[$i]}
      ipAddressPrivate=${blockstreamerIpListPrivate[$i]}
      zone=${blockstreamerIpListZone[$i]}
      if $evalInfo; then
        echo "NET_BLOCKSTREAMER${i}_IP=$ipAddress"
      else
        printNode blockstreamer "$ipAddress" "$ipAddressPrivate" "$zone"
      fi
    done
  fi
  ;;
status)
  cloud_StatusAll
  ;;
*)
  usage "Unknown command: $command"
esac

================
File: net/init-metrics.sh
================
set -e
here=$(dirname "$0")
source "$here"/common.sh
usage() {
  exitcode=0
  if [[ -n "$1" ]]; then
    exitcode=1
    echo "Error: $*"
  fi
  cat <<EOF
usage: $0 [-e] [-d] [-c database_name] [username]
Creates a testnet dev metrics database
  username        InfluxDB user with access to create a new database
  -c              Manually specify a database to create, rather than read from config file
  -d              Delete the database instead of creating it
  -e              Assume database already exists and SOLANA_METRICS_CONFIG is
                  defined in the environment already
EOF
  exit $exitcode
}
useEnv=false
delete=false
createWithoutConfig=false
host="https://internal-metrics.solana.com:8086"
while getopts ":hdec:" opt; do
  case $opt in
  h)
    usage
    exit 0
    ;;
  c)
    createWithoutConfig=true
    netBasename=$OPTARG
    ;;
  d)
    delete=true
    ;;
  e)
    useEnv=true
    ;;
  *)
    usage "unhandled option: $OPTARG"
    ;;
  esac
done
shift $((OPTIND - 1))
if $useEnv; then
  [[ -n $SOLANA_METRICS_CONFIG ]] ||
    usage "SOLANA_METRICS_CONFIG is not defined in the environment"
else
  username=$1
  [[ -n "$username" ]] || usage "username not specified"
  read -rs -p "InfluxDB password for $username: " password
  [[ -n $password ]] || { echo "Password not specified"; exit 1; }
  echo
  password="$(urlencode "$password")"
  if ! $createWithoutConfig; then
    loadConfigFile
  fi
  query() {
    echo "$*"
    set -x
    curl -XPOST \
      "$host/query?u=${username}&p=${password}" \
      --data-urlencode "q=$*"
  }
  query "DROP DATABASE \"$netBasename\""
  ! $delete || exit 0
  query "CREATE DATABASE \"$netBasename\""
  query "ALTER RETENTION POLICY autogen ON \"$netBasename\" DURATION 7d"
  query "GRANT READ ON \"$netBasename\" TO \"ro\""
  query "GRANT WRITE ON \"$netBasename\" TO \"${username}\""
  SOLANA_METRICS_CONFIG="host=$host,db=$netBasename,u=${username},p=${password}"
  set +x
fi
set -x
echo "export SOLANA_METRICS_CONFIG=\"$SOLANA_METRICS_CONFIG\"" >> "$configFile"
exit 0

================
File: net/net.sh
================
set -e
here=$(dirname "$0")
SOLANA_ROOT="$(cd "$here"/..; pwd)"
# shellcheck source=net/common.sh
source "$here"/common.sh
usage() {
  exitcode=0
  if [[ -n "$1" ]]; then
    exitcode=1
    echo "Error: $*"
  fi
  CLIENT_OPTIONS=$(cat << EOM
-c clientType=numClients=extraArgs - Number of clientTypes to start.  This options can be specified
                                     more than once.  Defaults to bench-tps for all clients if not
                                     specified.
                                     Valid client types are:
                                         idle
                                         bench-tps
                                     User can optionally provide extraArgs that are transparently
                                     supplied to the client program as command line parameters.
                                     For example,
                                         -c bench-tps=2="--tx_count 25000"
                                     This will start 2 bench-tps clients, and supply "--tx_count 25000"
                                     to the bench-tps client.
--use-unstaked-connection          - Use unstaked connection. By default, staked connection with
                                     bootstrap node credendials is used.
EOM
)
  cat <<EOF
usage: $0 [start|stop|restart|sanity] [command-specific options]
Operate a configured testnet
 start        - Start the network
 sanity       - Sanity check the network
 stop         - Stop the network
 restart      - Shortcut for stop then start
 logs         - Fetch remote logs from each network node
 startnode    - Start an individual node (previously stopped with stopNode)
 stopnode     - Stop an individual node
 startclients - Start client nodes only
 prepare      - Prepare software deployment. (Build/download the software release)
 update       - Deploy a new software update to the cluster
 upgrade      - Upgrade software on bootstrap validator. (Restart bootstrap validator manually to run it)
 start-specific options:
   -T [tarFilename]                   - Deploy the specified release tarball
   -t edge|beta|stable|vX.Y.Z         - Deploy the latest tarball release for the
                                        specified release channel (edge|beta|stable) or release tag
                                        (vX.Y.Z)
   -r / --skip-setup                  - Reuse existing node/ledger configuration from a
                                        previous |start| (ie, don't run ./multinode-demo/setup.sh).
   -d / --debug                       - Build/deploy the testnet with debug binaries
   $CLIENT_OPTIONS
   --client-delay-start
                                      - Number of seconds to wait after validators have finished starting before starting client programs
                                        (default: $clientDelayStart)
   -n NUM_VALIDATORS                  - Number of validators to apply command to.
   --gpu-mode GPU_MODE                - Deprecated, this argument is ignored
   --hashes-per-tick NUM_HASHES|sleep|auto
                                      - Override the default --hashes-per-tick for the cluster
   --no-airdrop
                                      - If set, disables the faucet keypair.  Nodes must be funded in genesis config
   --faucet-lamports NUM_LAMPORTS_TO_MINT
                                      - Override the default 500000000000000000 lamports minted in genesis
   --extra-primordial-stakes NUM_EXTRA_PRIMORDIAL_STAKES
                                      - Number of nodes to be initially staked in genesis.
                                        Gives extra stake in genesis to NUM_EXTRA_PRIMORDIAL_STAKES many nodes.
                                        Implies --wait-for-supermajority 1 --async-node-init and the supermajority
                                        wait slot may be overridden with the corresponding flag
   --internal-nodes-stake-lamports NUM_LAMPORTS_PER_NODE
                                      - Amount to stake internal nodes.
   --internal-nodes-lamports NUM_LAMPORTS_PER_NODE
                                      - Amount to fund internal nodes in genesis config.
   --external-accounts-file FILE_PATH
                                      - A YML file with a list of account pubkeys and corresponding lamport balances
                                        in genesis config for external nodes
   --no-snapshot-fetch
                                      - If set, disables booting validators from a snapshot
   --copy-program URL_OR_MONIKER PUBKEY
                                      - Copies a program PUBKEY from URL_OR_MONIKER.
                                      For example, --copy-program t recr1L3PCGKLbckBqMNcJhuuyU1zgo8nBhfLVsJNwr5
   --skip-poh-verify
                                      - If set, validators will skip verifying
                                        the ledger they already have saved to disk at
                                        boot (results in a much faster boot)
   --no-deploy
                                      - Don't deploy new software, use the
                                        existing deployment
   --no-build
                                      - Don't build new software, deploy the
                                        existing binaries
   --deploy-if-newer                  - Only deploy if newer software is
                                        available (requires -t or -T)
   --cluster-type development|devnet|testnet|mainnet-beta
                                      - Specify whether or not to launch the cluster in "development" mode with all features enabled at epoch 0,
                                        or various other live clusters' feature set (default: development)
   --slots-per-epoch SLOTS
                                      - Override the number of slots in an epoch
   --warp-slot WARP_SLOT
                                      - Boot from a snapshot that has warped ahead to WARP_SLOT rather than a slot 0 genesis.
   --full-rpc
                                      - Support full RPC services on all nodes
   --tpu-disable-quic
                                      - Disable quic for tpu packet forwarding
   --tpu-enable-udp
                                      - Enable UDP for tpu transactions
   --client-type
                                      - Specify backend client type for bench-tps. Valid options are (rpc-client|tpu-client), tpu-client is default
 sanity/start-specific options:
   -F                   - Discard validator nodes that didn't bootup successfully
   -o noInstallCheck    - Skip agave-install sanity
   -o rejectExtraNodes  - Require the exact number of nodes
 stop-specific options:
   none
 logs-specific options:
   none
 update-specific options:
   --platform linux|osx|windows       - Deploy the tarball using 'agave-install deploy ...' for the
                                        given platform (multiple platforms may be specified)
                                        (-t option must be supplied as well)
 startnode/stopnode-specific options:
   -i [ip address]                    - IP Address of the node to start or stop
 startnode specific option:
   --wen-restart [coordinator_pubkey]      - Use given coordinator pubkey and apply wen_restat
 startclients-specific options:
   $CLIENT_OPTIONS
Note: if RUST_LOG is set in the environment it will be propagated into the
      network nodes.
EOF
  exit $exitcode
}
initLogDir() {
  [[ -z $netLogDir ]] || return 0
  netLogDir="$netDir"/log
  declare netLogDateDir
  netLogDateDir="$netDir"/log-$(date +"%Y-%m-%d_%H_%M_%S")
  if [[ -d $netLogDir && ! -L $netLogDir ]]; then
    echo "Warning: moving $netLogDir to make way for symlink."
    mv "$netLogDir" "$netDir"/log.old
  elif [[ -L $netLogDir ]]; then
    rm "$netLogDir"
  fi
  mkdir -p "$netConfigDir" "$netLogDateDir"
  ln -sf "$netLogDateDir" "$netLogDir"
  echo "Log directory: $netLogDateDir"
}
annotate() {
  [[ -z $BUILDKITE ]] || {
    buildkite-agent annotate "$@"
  }
}
annotateBlockexplorerUrl() {
  declare blockstreamer=${blockstreamerIpList[0]}
  if [[ -n $blockstreamer ]]; then
    annotate --style info --context blockexplorer-url "Block explorer: http://$blockstreamer/"
  fi
}
build() {
  declare MAYBE_DOCKER=
  if [[ $(uname) != Linux ]]; then
    source "$SOLANA_ROOT"/ci/docker/env.sh
    MAYBE_DOCKER="ci/docker-run.sh ${CI_DOCKER_IMAGE:?}"
  fi
  SECONDS=0
  (
    cd "$SOLANA_ROOT"
    echo "--- Build started at $(date)"
    set -x
    rm -rf farf
    buildVariant=
    if $debugBuild; then
      buildVariant=--debug
    fi
    if $profileBuild; then
      profilerFlags="RUSTFLAGS='-C force-frame-pointers=y -g ${RUSTFLAGS}'"
    fi
    $MAYBE_DOCKER bash -c "
      set -ex
      $profilerFlags scripts/cargo-install-all.sh farf $buildVariant --validator-only --no-spl-token
    "
  )
  (
    set +e
    COMMIT="$(git rev-parse HEAD)"
    BRANCH="$(git rev-parse --abbrev-ref HEAD)"
    TAG="$(git describe --exact-match --tags HEAD 2>/dev/null)"
    if [[ $TAG =~ ^v[0-9]+\.[0-9]+\.[0-9]+ ]]; then
      NOTE=$TAG
    else
      NOTE=$BRANCH
    fi
    (
      echo "channel: devbuild $NOTE"
      echo "commit: $COMMIT"
    ) > "$SOLANA_ROOT"/farf/version.yml
  )
  echo "Build took $SECONDS seconds"
}
remoteHomeDir() {
  declare ipAddress=$1
  declare remoteHome
  remoteHome="$(ssh "${sshOptions[@]}" "$ipAddress" "echo \$HOME")"
  echo "$remoteHome"
}
startCommon() {
  declare ipAddress=$1
  declare remoteHome
  remoteHome=$(remoteHomeDir "$ipAddress")
  local remoteSolanaHome="${remoteHome}/solana"
  local remoteCargoBin="${remoteHome}/.cargo/bin"
  test -d "$SOLANA_ROOT"
  if $skipSetup; then
    ssh "${sshOptions[@]}" "$ipAddress" "
      set -x;
      mkdir -p $remoteSolanaHome/config;
      rm -rf ~/config;
      mv $remoteSolanaHome/config ~;
      rm -rf $remoteSolanaHome;
      mkdir -p $remoteSolanaHome $remoteCargoBin;
      mv ~/config $remoteSolanaHome/
    "
  else
    ssh "${sshOptions[@]}" "$ipAddress" "
      set -x;
      rm -rf $remoteSolanaHome;
      mkdir -p $remoteCargoBin
    "
  fi
  [[ -z "$externalNodeSshKey" ]] || ssh-copy-id -f -i "$externalNodeSshKey" "${sshOptions[@]}" "solana@$ipAddress"
  syncScripts "$ipAddress"
}
syncScripts() {
  echo "rsyncing scripts... to $ipAddress"
  declare ipAddress=$1
  declare remoteHome
  remoteHome=$(remoteHomeDir "$ipAddress")
  local remoteSolanaHome="${remoteHome}/solana"
  rsync -vPrc -e "ssh ${sshOptions[*]}" \
    --exclude 'net/log*' \
    "$SOLANA_ROOT"/{fetch-perf-libs.sh,fetch-programs.sh,fetch-core-bpf.sh,fetch-spl.sh,scripts,net,multinode-demo} \
    "$ipAddress":"$remoteSolanaHome"/ > /dev/null
}
deployBootstrapValidator() {
  declare ipAddress=$1
  declare remoteHome
  remoteHome=$(remoteHomeDir "$ipAddress")
  local remoteCargoBin="${remoteHome}/.cargo/bin"
  echo "Deploying software to bootstrap validator ($ipAddress)"
  case $deployMethod in
  tar)
    rsync -vPrc -e "ssh ${sshOptions[*]}" "$SOLANA_ROOT"/solana-release/bin/* "$ipAddress:$remoteCargoBin/"
    rsync -vPrc -e "ssh ${sshOptions[*]}" "$SOLANA_ROOT"/solana-release/version.yml "$ipAddress:~/"
    ;;
  local)
    rsync -vPrc -e "ssh ${sshOptions[*]}" "$SOLANA_ROOT"/farf/bin/* "$ipAddress:$remoteCargoBin/"
    rsync -vPrc -e "ssh ${sshOptions[*]}" "$SOLANA_ROOT"/farf/version.yml "$ipAddress:~/"
    ;;
  skip)
    ;;
  *)
    usage "Internal error: invalid deployMethod: $deployMethod"
    ;;
  esac
}
startBootstrapLeader() {
  declare ipAddress=$1
  declare nodeIndex="$2"
  declare logFile="$3"
  echo "--- Starting bootstrap validator: $ipAddress"
  echo "start log: $logFile"
  (
    set -x
    startCommon "$ipAddress" || exit 1
    [[ -z "$externalPrimordialAccountsFile" ]] || rsync -vPrc -e "ssh ${sshOptions[*]}" "$externalPrimordialAccountsFile" \
      "$ipAddress:$remoteExternalPrimordialAccountsFile"
    deployBootstrapValidator "$ipAddress"
    ssh "${sshOptions[@]}" -n "$ipAddress" \
      "./solana/net/remote/remote-node.sh \
         $deployMethod \
         bootstrap-validator \
         $entrypointIp \
         $((${#validatorIpList[@]} + ${#blockstreamerIpList[@]})) \
         \"$RUST_LOG\" \
         $skipSetup \
         $failOnValidatorBootupFailure \
         \"$remoteExternalPrimordialAccountsFile\" \
         \"$maybeDisableAirdrops\" \
         \"$internalNodesStakeLamports\" \
         \"$internalNodesLamports\" \
         $nodeIndex \
         ${#clientIpList[@]} \"$benchTpsExtraArgs\" \
         \"$genesisOptions\" \
         \"$maybeNoSnapshot $maybeSkipLedgerVerify $maybeLimitLedgerSize $maybeWaitForSupermajority $maybeAccountsDbSkipShrink $maybeSkipRequireTower\" \
         \"$maybeWarpSlot\" \
         \"$maybeFullRpc\" \
         \"$waitForNodeInit\" \
         \"$extraPrimordialStakes\" \
         \"$TMPFS_ACCOUNTS\" \
         \"$disableQuic\" \
         \"$enableUdp\" \
         \"$maybeWenRestart\" \
      "
  ) >> "$logFile" 2>&1 || {
    cat "$logFile"
    echo "^^^ +++"
    exit 1
  }
}
startNode() {
  declare ipAddress=$1
  declare nodeType=$2
  declare nodeIndex="$3"
  initLogDir
  declare logFile="$netLogDir/validator-$ipAddress.log"
  if [[ -z $nodeType ]]; then
    echo nodeType not specified
    exit 1
  fi
  if [[ -z $nodeIndex ]]; then
    echo nodeIndex not specified
    exit 1
  fi
  echo "--- Starting $nodeType: $ipAddress"
  echo "start log: $logFile"
  (
    set -x
    startCommon "$ipAddress"
    if [[ $nodeType = blockstreamer ]] && [[ -n $letsEncryptDomainName ]]; then
      declare localArchive=~/letsencrypt-"$letsEncryptDomainName".tgz
      if [[ -r "$localArchive" ]]; then
        timeout 30s scp "${sshOptions[@]}" "$localArchive" "$ipAddress:letsencrypt.tgz"
      fi
      ssh "${sshOptions[@]}" -n "$ipAddress" \
        "sudo -H /certbot-restore.sh $letsEncryptDomainName maintainers@solanalabs.com"
      rm -f letsencrypt.tgz
      timeout 30s scp "${sshOptions[@]}" "$ipAddress:/letsencrypt.tgz" letsencrypt.tgz
      test -s letsencrypt.tgz
      cp letsencrypt.tgz "$localArchive"
    fi
    ssh "${sshOptions[@]}" -n "$ipAddress" \
      "./solana/net/remote/remote-node.sh \
         $deployMethod \
         $nodeType \
         $entrypointIp \
         $((${#validatorIpList[@]} + ${#blockstreamerIpList[@]})) \
         \"$RUST_LOG\" \
         $skipSetup \
         $failOnValidatorBootupFailure \
         \"$remoteExternalPrimordialAccountsFile\" \
         \"$maybeDisableAirdrops\" \
         \"$internalNodesStakeLamports\" \
         \"$internalNodesLamports\" \
         $nodeIndex \
         ${#clientIpList[@]} \"$benchTpsExtraArgs\" \
         \"$genesisOptions\" \
         \"$maybeNoSnapshot $maybeSkipLedgerVerify $maybeLimitLedgerSize $maybeWaitForSupermajority $maybeAccountsDbSkipShrink $maybeSkipRequireTower\" \
         \"$maybeWarpSlot\" \
         \"$maybeFullRpc\" \
         \"$waitForNodeInit\" \
         \"$extraPrimordialStakes\" \
         \"$TMPFS_ACCOUNTS\" \
         \"$disableQuic\" \
         \"$enableUdp\" \
         \"$maybeWenRestart\" \
      "
  ) >> "$logFile" 2>&1 &
  declare pid=$!
  ln -sf "validator-$ipAddress.log" "$netLogDir/validator-$pid.log"
  pids+=("$pid")
}
startClient() {
  declare ipAddress=$1
  declare clientToRun="$2"
  declare clientIndex="$3"
  initLogDir
  declare logFile="$netLogDir/client-$clientToRun-$ipAddress.log"
  echo "--- Starting client: $ipAddress - $clientToRun"
  echo "start log: $logFile"
  (
    set -x
    startCommon "$ipAddress"
    ssh "${sshOptions[@]}" -f "$ipAddress" \
      "./solana/net/remote/remote-client.sh $deployMethod $entrypointIp \
      $clientToRun \"$RUST_LOG\" \"$benchTpsExtraArgs\" $clientIndex $clientType \
      $maybeUseUnstakedConnection"
  ) >> "$logFile" 2>&1 || {
    cat "$logFile"
    echo "^^^ +++"
    exit 1
  }
}
startClients() {
  for ((i=0; i < "$numClients" && i < "$numClientsRequested"; i++)) do
    if [[ $i -lt "$numBenchTpsClients" ]]; then
      startClient "${clientIpList[$i]}" "solana-bench-tps" "$i"
    else
      startClient "${clientIpList[$i]}" "idle"
    fi
  done
}
sanity() {
  declare skipBlockstreamerSanity=$1
  $metricsWriteDatapoint "testnet-deploy net-sanity-begin=1"
  declare ok=true
  declare bootstrapLeader=${validatorIpList[0]}
  declare blockstreamer=${blockstreamerIpList[0]}
  annotateBlockexplorerUrl
  echo "--- Sanity: $bootstrapLeader"
  (
    set -x
    ssh "${sshOptions[@]}" "$bootstrapLeader" \
      "./solana/net/remote/remote-sanity.sh $bootstrapLeader $sanityExtraArgs \"$RUST_LOG\""
  ) || ok=false
  $ok || exit 1
  if [[ -z $skipBlockstreamerSanity && -n $blockstreamer ]]; then
    echo "--- Sanity: $blockstreamer"
    (
      set -x
      ssh "${sshOptions[@]}" "$blockstreamer" \
        "./solana/net/remote/remote-sanity.sh $blockstreamer $sanityExtraArgs \"$RUST_LOG\""
    ) || ok=false
    $ok || exit 1
  fi
  $metricsWriteDatapoint "testnet-deploy net-sanity-complete=1"
}
deployUpdate() {
  if [[ -z $updatePlatforms ]]; then
    echo "No update platforms"
    return
  fi
  if [[ -z $releaseChannel ]]; then
    echo "Release channel not specified (use -t option)"
    exit 1
  fi
  declare ok=true
  declare bootstrapLeader=${validatorIpList[0]}
  for updatePlatform in $updatePlatforms; do
    echo "--- Deploying agave-install update: $updatePlatform"
    (
      set -x
      scripts/agave-install-update-manifest-keypair.sh "$updatePlatform"
      timeout 30s scp "${sshOptions[@]}" \
        update_manifest_keypair.json "$bootstrapLeader:solana/update_manifest_keypair.json"
      ssh "${sshOptions[@]}" "$bootstrapLeader" \
        "./solana/net/remote/remote-deploy-update.sh $releaseChannel $updatePlatform"
    ) || ok=false
    $ok || exit 1
  done
}
getNodeType() {
  echo "getNodeType: $nodeAddress"
  [[ -n $nodeAddress ]] || {
    echo "Error: nodeAddress not set"
    exit 1
  }
  nodeIndex=0
  nodeType=validator
  for ipAddress in "${validatorIpList[@]}" b "${blockstreamerIpList[@]}"; do
    if [[ $ipAddress = b ]]; then
      nodeType=blockstreamer
      continue
    fi
    if [[ $ipAddress = "$nodeAddress" ]]; then
      echo "getNodeType: $nodeType ($nodeIndex)"
      return
    fi
    ((nodeIndex = nodeIndex + 1))
  done
  echo "Error: Unknown node: $nodeAddress"
  exit 1
}
prepareDeploy() {
  case $deployMethod in
  tar)
    if [[ -n $releaseChannel ]]; then
      echo "Downloading release from channel: $releaseChannel"
      rm -f "$SOLANA_ROOT"/solana-release.tar.bz2
      declare updateDownloadUrl=https://release.anza.xyz/"$releaseChannel"/solana-release-x86_64-unknown-linux-gnu.tar.bz2
      (
        set -x
        curl -L -I "$updateDownloadUrl"
        curl -L --retry 5 --retry-delay 2 --retry-connrefused \
          -o "$SOLANA_ROOT"/solana-release.tar.bz2 "$updateDownloadUrl"
      )
      tarballFilename="$SOLANA_ROOT"/solana-release.tar.bz2
    fi
    (
      set -x
      rm -rf "$SOLANA_ROOT"/solana-release
      cd "$SOLANA_ROOT"; tar jfxv "$tarballFilename"
      cat "$SOLANA_ROOT"/solana-release/version.yml
    )
    ;;
  local)
    if $doBuild; then
      build
    else
      echo "Build skipped due to --no-build"
    fi
    ;;
  skip)
    ;;
  *)
    usage "Internal error: invalid deployMethod: $deployMethod"
    ;;
  esac
  if [[ -n $deployIfNewer ]]; then
    if [[ $deployMethod != tar ]]; then
      echo "Error: --deploy-if-newer only supported for tar deployments"
      exit 1
    fi
    echo "Fetching current software version"
    (
      set -x
      rsync -vPrc -e "ssh ${sshOptions[*]}" "${validatorIpList[0]}":~/version.yml current-version.yml
    )
    cat current-version.yml
    if ! diff -q current-version.yml "$SOLANA_ROOT"/solana-release/version.yml; then
      echo "Cluster software version is old.  Update required"
    else
      echo "Cluster software version is current.  No update required"
      exit 0
    fi
  fi
}
deploy() {
  initLogDir
  echo "Deployment started at $(date)"
  $metricsWriteDatapoint "testnet-deploy net-start-begin=1"
  if [[ -n "$copyProgramPubkey" ]]; then
      echo "Copying program from ${copyProgramUrl}"
      solana -u "${copyProgramUrl}" program dump "${copyProgramPubkey}" "${copyProgramPubkey}".so || exit 1
      genesisOptions="${genesisOptions} --bpf-program ${copyProgramPubkey} BPFLoader2111111111111111111111111111111111 /home/solana/solana/net/${copyProgramPubkey}.so"
  fi
  declare bootstrapLeader=true
  for nodeAddress in "${validatorIpList[@]}" "${blockstreamerIpList[@]}"; do
    nodeType=
    nodeIndex=
    getNodeType
    if $bootstrapLeader; then
      SECONDS=0
      declare bootstrapNodeDeployTime=
      startBootstrapLeader "$nodeAddress" "$nodeIndex" "$netLogDir/bootstrap-validator-$ipAddress.log"
      bootstrapNodeDeployTime=$SECONDS
      $metricsWriteDatapoint "testnet-deploy net-bootnode-leader-started=1"
      bootstrapLeader=false
      SECONDS=0
      pids=()
    else
      startNode "$ipAddress" "$nodeType" "$nodeIndex"
      sleep 2
    fi
  done
  for pid in "${pids[@]}"; do
    declare ok=true
    wait "$pid" || ok=false
    if ! $ok; then
      echo "+++ validator failed to start"
      cat "$netLogDir/validator-$pid.log"
      if $failOnValidatorBootupFailure; then
        exit 1
      else
        echo "Failure is non-fatal"
      fi
    fi
  done
  if ! $waitForNodeInit; then
    declare startTime=$SECONDS
    for ipAddress in "${validatorIpList[@]}" "${blockstreamerIpList[@]}"; do
      declare timeWaited=$((SECONDS - startTime))
      if [[ $timeWaited -gt 600 ]]; then
        break
      fi
      ssh "${sshOptions[@]}" -n "$ipAddress" \
        "./solana/net/remote/remote-node-wait-init.sh $((600 - timeWaited))"
    done
  fi
  $metricsWriteDatapoint "testnet-deploy net-validators-started=1"
  additionalNodeDeployTime=$SECONDS
  annotateBlockexplorerUrl
  sanity skipBlockstreamerSanity
  echo "--- Sleeping $clientDelayStart seconds after validators are started before starting clients"
  sleep "$clientDelayStart"
  SECONDS=0
  startClients
  clientDeployTime=$SECONDS
  $metricsWriteDatapoint "testnet-deploy net-start-complete=1"
  declare networkVersion=unknown
  case $deployMethod in
  tar)
    networkVersion="$(
      (
        set -o pipefail
        grep "^commit: " "$SOLANA_ROOT"/solana-release/version.yml | head -n1 | cut -d\  -f2
      ) || echo "tar-unknown"
    )"
    ;;
  local)
    networkVersion="$(git rev-parse HEAD || echo local-unknown)"
    ;;
  skip)
    ;;
  *)
    usage "Internal error: invalid deployMethod: $deployMethod"
    ;;
  esac
  $metricsWriteDatapoint "testnet-deploy version=\"${networkVersion:0:9}\""
  echo
  echo "--- Deployment Successful"
  echo "Bootstrap validator deployment took $bootstrapNodeDeployTime seconds"
  echo "Additional validator deployment (${#validatorIpList[@]} validators, ${#blockstreamerIpList[@]} blockstreamer nodes) took $additionalNodeDeployTime seconds"
  echo "Client deployment (${#clientIpList[@]} instances) took $clientDeployTime seconds"
  echo "Network start logs in $netLogDir"
}
stopNode() {
  local ipAddress=$1
  local block=$2
  initLogDir
  declare logFile="$netLogDir/stop-validator-$ipAddress.log"
  echo "--- Stopping node: $ipAddress"
  echo "stop log: $logFile"
  syncScripts "$ipAddress"
  (
    set -x
    ssh "${sshOptions[@]}" "$ipAddress" "PS4=\"$PS4\" ./solana/net/remote/cleanup.sh"
  ) >> "$logFile" 2>&1 &
  declare pid=$!
  ln -sf "stop-validator-$ipAddress.log" "$netLogDir/stop-validator-$pid.log"
  if $block; then
    wait $pid || true
  else
    pids+=("$pid")
  fi
}
stop() {
  SECONDS=0
  $metricsWriteDatapoint "testnet-deploy net-stop-begin=1"
  declare loopCount=0
  pids=()
  for ipAddress in "${validatorIpList[@]}" "${blockstreamerIpList[@]}" "${clientIpList[@]}"; do
    stopNode "$ipAddress" false
    ((loopCount++ % 4 == 0)) && sleep 2
  done
  echo --- Waiting for nodes to finish stopping
  for pid in "${pids[@]}"; do
    echo -n "$pid "
    wait "$pid" || true
  done
  echo
  $metricsWriteDatapoint "testnet-deploy net-stop-complete=1"
  echo "Stopping nodes took $SECONDS seconds"
}
checkPremptibleInstances() {
  for ipAddress in "${validatorIpList[@]}"; do
    (
      timeout 5s ping -c 1 "$ipAddress" | tr - _ &>/dev/null
    ) || {
      cat <<EOF
Warning: $ipAddress may have been preempted.
Run |./gce.sh config| to restart it
EOF
      exit 1
    }
  done
}
releaseChannel=
deployMethod=local
deployIfNewer=
sanityExtraArgs=
skipSetup=false
updatePlatforms=
nodeAddress=
numIdleClients=0
numBenchTpsClients=0
benchTpsExtraArgs=
failOnValidatorBootupFailure=true
genesisOptions=
numValidatorsRequested=
externalPrimordialAccountsFile=
remoteExternalPrimordialAccountsFile=
internalNodesStakeLamports=
internalNodesLamports=
copyProgramUrl=""
copyProgramPubkey=""
maybeNoSnapshot=""
maybeLimitLedgerSize=""
maybeSkipLedgerVerify=""
maybeDisableAirdrops=""
maybeWaitForSupermajority=""
maybeAccountsDbSkipShrink=""
maybeSkipRequireTower=""
debugBuild=false
profileBuild=false
doBuild=true
clientDelayStart=0
netLogDir=
maybeWarpSlot=
maybeFullRpc=false
waitForNodeInit=true
extraPrimordialStakes=0
disableQuic=false
enableUdp=false
clientType=tpu-client
maybeUseUnstakedConnection=""
maybeWenRestart=""
command=$1
[[ -n $command ]] || usage
shift
shortArgs=()
while [[ -n $1 ]]; do
  if [[ ${1:0:2} = -- ]]; then
    if [[ $1 = --hashes-per-tick ]]; then
      genesisOptions="$genesisOptions $1 $2"
      shift 2
    elif [[ $1 = --slots-per-epoch ]]; then
      genesisOptions="$genesisOptions $1 $2"
      shift 2
    elif [[ $1 = --target-lamports-per-signature ]]; then
      genesisOptions="$genesisOptions $1 $2"
      shift 2
    elif [[ $1 = --faucet-lamports ]]; then
      genesisOptions="$genesisOptions $1 $2"
      shift 2
    elif [[ $1 = --cluster-type ]]; then
      case "$2" in
        development|devnet|testnet|mainnet-beta)
          ;;
        *)
          echo "Unexpected cluster type: \"$2\""
          exit 1
          ;;
      esac
      genesisOptions="$genesisOptions $1 $2"
      shift 2
    elif [[ $1 = --slots-per-epoch ]]; then
      genesisOptions="$genesisOptions $1 $2"
      shift 2
    elif [[ $1 = --no-snapshot-fetch ]]; then
      maybeNoSnapshot="$1"
      shift 1
    elif [[ $1 = --deploy-if-newer ]]; then
      deployIfNewer=1
      shift 1
    elif [[ $1 = --no-deploy ]]; then
      deployMethod=skip
      shift 1
    elif [[ $1 = --no-build ]]; then
      doBuild=false
      shift 1
    elif [[ $1 = --limit-ledger-size ]]; then
      maybeLimitLedgerSize="$1 $2"
      shift 2
    elif [[ $1 = --skip-poh-verify ]]; then
      maybeSkipLedgerVerify="$1"
      shift 1
    elif [[ $1 = --skip-setup ]]; then
      skipSetup=true
      shift 1
    elif [[ $1 = --platform ]]; then
      updatePlatforms="$updatePlatforms $2"
      shift 2
    elif [[ $1 = --internal-nodes-stake-lamports ]]; then
      internalNodesStakeLamports="$2"
      shift 2
    elif [[ $1 = --internal-nodes-lamports ]]; then
      internalNodesLamports="$2"
      shift 2
    elif [[ $1 = --copy-program ]]; then
      copyProgramUrl="$2"
      copyProgramPubkey="$3"
      shift 3
    elif [[ $1 = --external-accounts-file ]]; then
      externalPrimordialAccountsFile="$2"
      remoteExternalPrimordialAccountsFile=/tmp/external-primordial-accounts.yml
      shift 2
    elif [[ $1 = --no-airdrop ]]; then
      maybeDisableAirdrops="$1"
      shift 1
    elif [[ $1 = --debug ]]; then
      debugBuild=true
      shift 1
    elif [[ $1 = --profile ]]; then
      profileBuild=true
      shift 1
    elif [[ $1 = --gpu-mode ]]; then
      echo "'--gpu-mode' is deprecated, GPU support was removed from agave"
      shift 2
    elif [[ $1 == --client-delay-start ]]; then
      clientDelayStart=$2
      shift 2
    elif [[ $1 == --wait-for-supermajority ]]; then
      maybeWaitForSupermajority="$1 $2"
      shift 2
    elif [[ $1 == --warp-slot ]]; then
      maybeWarpSlot="$1 $2"
      shift 2
    elif [[ $1 == --full-rpc ]]; then
      maybeFullRpc=true
      shift 1
    elif [[ $1 == --tpu-disable-quic ]]; then
      disableQuic=true
      shift 1
    elif [[ $1 == --tpu-enable-udp ]]; then
      enableUdp=true
      shift 1
    elif [[ $1 == --async-node-init ]]; then
      waitForNodeInit=false
      shift 1
    elif [[ $1 == --extra-primordial-stakes ]]; then
      extraPrimordialStakes=$2
      shift 2
    elif [[ $1 = --allow-private-addr ]]; then
      echo "--allow-private-addr is a default value"
      shift 1
    elif [[ $1 = --accounts-db-skip-shrink ]]; then
      maybeAccountsDbSkipShrink="$1"
      shift 1
    elif [[ $1 = --skip-require-tower ]]; then
      maybeSkipRequireTower="$1"
      shift 1
    elif [[ $1 = --client-type ]]; then
      clientType=$2
      case "$clientType" in
        tpu-client|rpc-client)
          ;;
        *)
          echo "Unexpected client type: \"$clientType\""
          exit 1
          ;;
      esac
      shift 2
    elif [[ $1 = --use-unstaked-connection ]]; then
      maybeUseUnstakedConnection="$1"
      shift 1
    elif [[ $1 = --wen-restart ]]; then
      skipSetup=true
      maybeWenRestart="$2"
      shift 2
    else
      usage "Unknown long option: $1"
    fi
  else
    shortArgs+=("$1")
    shift
  fi
done
while getopts "h?T:t:o:f:rc:Fn:i:d" opt "${shortArgs[@]}"; do
  case $opt in
  h | \?)
    usage
    ;;
  T)
    tarballFilename=$OPTARG
    [[ -r $tarballFilename ]] || usage "File not readable: $tarballFilename"
    deployMethod=tar
    ;;
  t)
    case $OPTARG in
    edge|beta|stable|v*)
      releaseChannel=$OPTARG
      deployMethod=tar
      ;;
    *)
      usage "Invalid release channel: $OPTARG"
      ;;
    esac
    ;;
  n)
    numValidatorsRequested=$OPTARG
    ;;
  r)
    skipSetup=true
    ;;
  o)
    case $OPTARG in
    rejectExtraNodes|noInstallCheck)
      sanityExtraArgs="$sanityExtraArgs -o $OPTARG"
      ;;
    *)
      usage "Unknown option: $OPTARG"
      ;;
    esac
    ;;
  c)
    getClientTypeAndNum() {
      if ! [[ $OPTARG == *'='* ]]; then
        echo "Error: Expecting tuple \"clientType=numClientType=extraArgs\" but got \"$OPTARG\""
        exit 1
      fi
      local keyValue
      IFS='=' read -ra keyValue <<< "$OPTARG"
      local clientType=${keyValue[0]}
      local numClients=${keyValue[1]}
      local extraArgs=${keyValue[2]}
      re='^[0-9]+$'
      if ! [[ $numClients =~ $re ]] ; then
        echo "error: numClientType must be a number but got \"$numClients\""
        exit 1
      fi
      case $clientType in
        idle)
          numIdleClients=$numClients
        ;;
        bench-tps)
          numBenchTpsClients=$numClients
          benchTpsExtraArgs=$extraArgs
        ;;
        *)
          echo "Unknown client type: $clientType"
          exit 1
          ;;
      esac
    }
    getClientTypeAndNum
    ;;
  F)
    failOnValidatorBootupFailure=false
    ;;
  i)
    nodeAddress=$OPTARG
    ;;
  d)
    debugBuild=true
    ;;
  *)
    usage "Error: unhandled option: $opt"
    ;;
  esac
done
loadConfigFile
if [[ -n $numValidatorsRequested ]]; then
  truncatedNodeList=( "${validatorIpList[@]:0:$numValidatorsRequested}" )
  unset validatorIpList
  validatorIpList=( "${truncatedNodeList[@]}" )
fi
numClients=${
numClientsRequested=$((numBenchTpsClients + numIdleClients))
if [[ "$numClientsRequested" -eq 0 ]]; then
  numBenchTpsClients=$numClients
  numClientsRequested=$numClients
else
  if [[ "$numClientsRequested" -gt "$numClients" ]]; then
    echo "Error: More clients requested ($numClientsRequested) then available ($numClients)"
    exit 1
  fi
fi
if [[ -n "$maybeWaitForSupermajority" && -n "$maybeWarpSlot" ]]; then
  read -r _ waitSlot <<<"$maybeWaitForSupermajority"
  read -r _ warpSlot <<<"$maybeWarpSlot"
  if [[ $waitSlot -ne $warpSlot ]]; then
    echo "Error: When specifying both --wait-for-supermajority and --warp-slot,"
    echo "they must use the same slot. ($waitSlot != $warpSlot)"
    exit 1
  fi
fi
echo "net.sh: Primordial stakes: $extraPrimordialStakes"
if [[ $extraPrimordialStakes -gt 0 ]]; then
  waitForNodeInit=false
  if [[ -z "$maybeWaitForSupermajority" ]]; then
    waitSlot=
    if [[ -n "$maybeWarpSlot" ]]; then
      read -r _ waitSlot <<<"$maybeWarpSlot"
    else
      waitSlot=1
    fi
    maybeWaitForSupermajority="--wait-for-supermajority $waitSlot"
  fi
fi
checkPremptibleInstances
case $command in
restart)
  prepareDeploy
  stop
  deploy
  ;;
start)
  prepareDeploy
  deploy
  ;;
prepare)
  prepareDeploy
  ;;
sanity)
  sanity
  ;;
stop)
  stop
  ;;
update)
  deployUpdate
  ;;
upgrade)
  bootstrapValidatorIp="${validatorIpList[0]}"
  prepareDeploy
  deployBootstrapValidator "$bootstrapValidatorIp"
  ;;
stopnode)
  if [[ -z $nodeAddress ]]; then
    usage "node address (-i) not specified"
    exit 1
  fi
  stopNode "$nodeAddress" true
  ;;
startnode)
  if [[ -z $nodeAddress ]]; then
    usage "node address (-i) not specified"
    exit 1
  fi
  nodeType=
  nodeIndex=
  getNodeType
  startNode "$nodeAddress" "$nodeType" "$nodeIndex"
  ;;
startclients)
  startClients
  ;;
logs)
  initLogDir
  fetchRemoteLog() {
    declare ipAddress=$1
    declare log=$2
    echo "--- fetching $log from $ipAddress"
    (
      set -x
      timeout 30s scp "${sshOptions[@]}" \
        "$ipAddress":solana/"$log".log "$netLogDir"/remote-"$log"-"$ipAddress".log
    ) || echo "failed to fetch log"
  }
  fetchRemoteLog "${validatorIpList[0]}" faucet
  for ipAddress in "${validatorIpList[@]}"; do
    fetchRemoteLog "$ipAddress" validator
  done
  for ipAddress in "${clientIpList[@]}"; do
    fetchRemoteLog "$ipAddress" client
  done
  for ipAddress in "${blockstreamerIpList[@]}"; do
    fetchRemoteLog "$ipAddress" validator
  done
  ;;
*)
  echo "Internal error: Unknown command: $command"
  usage
  exit 1
esac

================
File: net/scp.sh
================
here=$(dirname "$0")
source "$here"/common.sh
usage() {
  exitcode=0
  if [[ -n "$1" ]]; then
    exitcode=1
    echo "Error: $*"
  fi
  cat <<EOF
usage: $0 source ... target
node scp - behaves like regular scp with the necessary options to
access network nodes added automatically
EOF
  exit $exitcode
}
while getopts "h?" opt; do
  case $opt in
  h | \?)
    usage
    ;;
  *)
    usage "Error: unhandled option: $opt"
    ;;
  esac
done
loadConfigFile
if [[ -n "$1" ]]; then
  set -x
  exec scp "${sshOptions[@]}" "$@"
fi
exec "$here"/ssh.sh
exit 0

================
File: net/ssh.sh
================
here=$(dirname "$0")
source "$here"/common.sh
usage() {
  exitcode=0
  if [[ -n "$1" ]]; then
    exitcode=1
    echo "Error: $*"
  fi
  cat <<EOF
usage: $0 [ipAddress] [extra ssh arguments]
ssh into a node
 ipAddress     - IP address of the desired node.
If ipAddress is unspecified, a list of available nodes will be displayed.
EOF
  exit $exitcode
}
while getopts "h?" opt; do
  case $opt in
  h | \?)
    usage
    ;;
  *)
    usage "Error: unhandled option: $opt"
    ;;
  esac
done
loadConfigFile
ipAddress=$1
shift
if [[ -n "$ipAddress" ]]; then
  set -x
  exec ssh "${sshOptions[@]}" "$ipAddress" "$@"
fi
printNode() {
  declare nodeType=$1
  declare ip=$2
  printf "  %-25s | For logs run: $0 $ip tail -f solana/$nodeType.log\n" "$0 $ip"
}
echo Validators:
for ipAddress in "${validatorIpList[@]}"; do
  printNode validator "$ipAddress"
done
echo
echo Clients:
if [[ ${
  echo "  None"
else
  for ipAddress in "${clientIpList[@]}"; do
    printNode client "$ipAddress"
  done
fi
echo
echo Blockstreamers:
if [[ ${
  echo "  None"
else
  for ipAddress in "${blockstreamerIpList[@]}"; do
    printNode validator "$ipAddress"
  done
fi
echo
echo "Use |scp.sh| to transfer files to and from nodes"
echo
exit 0

================
File: net-utils/benches/token_bucket.rs
================
fn bench_token_bucket() {
println!("Running bench_token_bucket...");
⋮----
scope.spawn(|| loop {
if start.elapsed() > run_duration {
⋮----
match tb.consume_tokens(request_size) {
Ok(_) => accepted.fetch_add(1, Ordering::Relaxed),
Err(_) => rejected.fetch_add(1, Ordering::Relaxed),
⋮----
let jh = scope.spawn(|| loop {
⋮----
let elapsed = start.elapsed();
⋮----
let acc = accepted.load(Ordering::Relaxed);
let rate = acc as f64 / elapsed.as_secs_f64();
assert!(
⋮----
jh.join().expect("Rate checks should pass");
⋮----
let rej = rejected.load(Ordering::Relaxed);
println!("Run complete over {:?} seconds", run_duration.as_secs());
println!("Accepted {acc}, Rejected: {rej}");
println!(
⋮----
fn bench_token_bucket_eviction() {
println!("Running bench_token_bucket_eviction...");
⋮----
limiter.set_shrink_interval(32);
⋮----
scope.spawn(|| {
⋮----
if limiter.consume_tokens(ip, 1).is_ok() {
accepted.fetch_add(1, Ordering::Relaxed);
⋮----
rejected.fetch_add(1, Ordering::Relaxed);
⋮----
let len_approx = limiter.len_approx();
max_size.fetch_max(len_approx, Ordering::Relaxed);
⋮----
eprintln!("Max observed size was {}", max_size.load(Ordering::Relaxed));
⋮----
println!("Rejected: {rej}");
⋮----
fn bench_keyed_rate_limiter() {
println!("Running bench_keyed_rate_limiter...");
⋮----
let expected_total_accepts = (run_duration.as_secs() * 100 * ip_pool) as i64;
⋮----
println!("Accepted: {acc} (target {expected_total_accepts})");
⋮----
assert!(((acc as i64) - expected_total_accepts).abs() < expected_total_accepts / 10);
⋮----
fn main() {
bench_token_bucket();
println!("==========");
bench_token_bucket_eviction();
⋮----
bench_keyed_rate_limiter();

================
File: net-utils/src/ip_echo_client.rs
================
pub(crate) async fn ip_echo_server_request_with_binding(
⋮----
socket.bind(SocketAddr::new(bind_address, 0))?;
⋮----
tokio::time::timeout(TIMEOUT, make_request(socket, ip_echo_server_addr, msg)).await??;
parse_response(response, ip_echo_server_addr)
⋮----
pub(crate) async fn ip_echo_server_request(
⋮----
async fn make_request(
⋮----
let mut stream = socket.connect(ip_echo_server_addr).await?;
⋮----
bytes.extend_from_slice(&[0u8; HEADER_LENGTH]);
bytes.extend_from_slice(&bincode::serialize(&msg)?);
bytes.put_u8(b'\n');
stream.write_all(&bytes).await?;
stream.flush().await?;
bytes.clear();
let _n = stream.read_buf(&mut bytes).await?;
stream.shutdown().await?;
Ok(bytes)
⋮----
fn parse_response(
⋮----
if response.len() < HEADER_LENGTH {
bail!("Response too short, received {} bytes", response.len());
⋮----
.ok_or(anyhow::anyhow!(
⋮----
Ok(r) => bail!(
⋮----
Err(_) => bail!(
⋮----
bail!(
⋮----
Ok(payload)
⋮----
pub(crate) async fn verify_all_reachable_tcp(
⋮----
if listeners.is_empty() {
warn!("No ports provided for verify_all_reachable_tcp to check");
⋮----
.local_addr()
.expect("Sockets should be bound")
.ip();
for listener in listeners.iter() {
let local_binding = listener.local_addr().expect("Sockets should be bound");
assert_eq!(
⋮----
for chunk in &listeners.into_iter().chunks(MAX_PORT_COUNT_PER_MESSAGE) {
let listeners = chunk.collect_vec();
⋮----
.iter()
.map(|l| l.local_addr().expect("Sockets should be bound").port())
.collect_vec();
info!(
⋮----
let _ = ip_echo_server_request_with_binding(
⋮----
.map_err(|err| warn!("ip_echo_server request failed: {err}"));
for (port, tcp_listener) in ports.into_iter().zip(listeners) {
let listening_addr = tcp_listener.local_addr().unwrap();
⋮----
debug!("Waiting for incoming connection on tcp/{port}");
match tcp_listener.incoming().next() {
⋮----
let _ = sender.send(());
⋮----
None => warn!("tcp incoming failed"),
⋮----
checkers.push((listening_addr, thread_handle, receiver));
⋮----
for (listening_addr, thread_handle, receiver) in checkers.drain(..) {
⋮----
info!("tcp/{} is reachable", listening_addr.port());
⋮----
unreachable!("The receive on oneshot channel should never fail");
⋮----
error!(
⋮----
TcpStream::connect_timeout(&listening_addr, timeout).unwrap();
⋮----
thread_handle.await.expect("Thread should exit cleanly");
⋮----
pub(crate) async fn verify_all_reachable_udp(
⋮----
if sockets.is_empty() {
warn!("No ports provided for verify_all_reachable_udp to check");
⋮----
for &socket in sockets.iter() {
let local_addr = socket.local_addr().expect("Socket must be bound");
⋮----
.entry(local_addr.ip())
.or_default()
.entry(local_addr.port())
⋮----
.push(socket);
⋮----
let ports: Vec<u16> = ports_to_socks_map.keys().copied().collect();
info!("Checking that udp ports {ports:?} are reachable from bind IP {bind_ip:?}");
'outer: for chunk_to_check in ports.chunks(MAX_PORT_COUNT_PER_MESSAGE) {
let ports_to_check = chunk_to_check.to_vec();
⋮----
error!("There are some udp ports with no response!! Retrying...");
⋮----
// clone off the sockets that use ports within our chunk
⋮----
.flat_map(|port| ports_to_socks_map.get(port).unwrap())
.map(|&s| s.try_clone().expect("Unable to clone UDP socket"))
.collect();
⋮----
// Spawn threads for each socket to check
⋮----
let port = socket.local_addr().expect("Socket should be bound").port();
let reachable_ports = reachable_ports.clone();
checkers.spawn_blocking(move || {
⋮----
let original_read_timeout = socket.read_timeout().unwrap();
⋮----
.set_read_timeout(Some(Duration::from_millis(250)))
.unwrap();
⋮----
if reachable_ports.read().unwrap().contains(&port)
|| Instant::now().duration_since(start) >= timeout
⋮----
let recv_result = socket.recv(&mut [0; 1]);
debug!("Waited for incoming datagram on udp/{port}: {recv_result:?}");
if recv_result.is_ok() {
reachable_ports.write().unwrap().insert(port);
⋮----
socket.set_read_timeout(original_read_timeout).unwrap();
⋮----
let next = checkers.join_next().await;
⋮----
r.expect("Threads should exit cleanly");
⋮----
// Might have lost a UDP packet, check that all ports were reached
⋮----
.expect("Single owner expected")
.into_inner()
.expect("No threads should hold the lock");
⋮----
if reachable_ports.len() == ports_to_check.len() {
⋮----
error!("Maximum retry count reached. Some ports for IP {bind_ip} unreachable.");

================
File: net-utils/src/ip_echo_server.rs
================
pub type IpEchoServer = Runtime;
pub const MINIMUM_IP_ECHO_SERVER_THREADS: NonZeroUsize = NonZeroUsize::new(2).unwrap();
⋮----
pub(crate) struct IpEchoServerMessage {
⋮----
pub struct IpEchoServerResponse {
⋮----
impl IpEchoServerMessage {
pub fn new(tcp_ports: &[u16], udp_ports: &[u16]) -> Self {
⋮----
assert!(tcp_ports.len() <= msg.tcp_ports.len());
assert!(udp_ports.len() <= msg.udp_ports.len());
msg.tcp_ports[..tcp_ports.len()].copy_from_slice(tcp_ports);
msg.udp_ports[..udp_ports.len()].copy_from_slice(udp_ports);
⋮----
pub(crate) fn ip_echo_server_request_length() -> usize {
⋮----
.wrapping_add(bincode::serialized_size(&IpEchoServerMessage::default()).unwrap() as usize)
⋮----
async fn process_connection(
⋮----
info!("connection from {peer_addr:?}");
let mut data = vec![0u8; ip_echo_server_request_length()];
⋮----
let (mut reader, writer) = socket.split();
let _ = timeout(IO_TIMEOUT, reader.read_exact(&mut data)).await??;
⋮----
let request_header: String = data[0..HEADER_LENGTH].iter().map(|b| *b as char).collect();
⋮----
timeout(
⋮----
writer.write_all(b"HTTP/1.1 400 Bad Request\nContent-length: 0\n\n"),
⋮----
return Ok(());
⋮----
return Err(io::Error::other(format!(
⋮----
bincode::deserialize::<IpEchoServerMessage>(&data[HEADER_LENGTH..]).map_err(|err| {
io::Error::other(format!(
⋮----
trace!("request: {msg:?}");
match bind_to_unspecified() {
⋮----
udp_socket.send_to(&[0], SocketAddr::from((peer_addr.ip(), *udp_port)));
⋮----
Ok(_) => debug!("Successful send_to udp/{udp_port}"),
Err(err) => info!("Failed to send_to udp/{udp_port}: {err}"),
⋮----
warn!("Failed to bind local udp socket: {err}");
⋮----
debug!("Connecting to tcp/{tcp_port}");
let mut tcp_stream = timeout(
⋮----
TcpStream::connect(&SocketAddr::new(peer_addr.ip(), *tcp_port)),
⋮----
debug!("Connection established to tcp/{}", *tcp_port);
tcp_stream.shutdown().await?;
⋮----
address: peer_addr.ip(),
⋮----
let mut bytes = vec![0u8; IP_ECHO_SERVER_RESPONSE_LENGTH];
bincode::serialize_into(&mut bytes[HEADER_LENGTH..], &response).unwrap();
trace!("response: {bytes:?}");
writer.write_all(&bytes).await
⋮----
async fn run_echo_server(tcp_listener: std::net::TcpListener, shred_version: Option<u16>) {
info!("bound to {:?}", tcp_listener.local_addr().unwrap());
⋮----
TcpListener::from_std(tcp_listener).expect("Failed to convert std::TcpListener");
⋮----
let connection = tcp_listener.accept().await;
⋮----
runtime::Handle::current().spawn(async move {
if let Err(err) = process_connection(socket, peer_addr, shred_version).await {
info!("session failed: {err:?}");
⋮----
Err(err) => warn!("listener accept failed: {err:?}"),
⋮----
pub fn ip_echo_server(
⋮----
tcp_listener.set_nonblocking(true).unwrap();
⋮----
.thread_name("solIpEchoSrvrRt")
.worker_threads(num_server_threads.get())
.enable_all()
.build()
.expect("new tokio runtime");
runtime.spawn(run_echo_server(tcp_listener, shred_version));

================
File: net-utils/src/lib.rs
================
mod ip_echo_client;
mod ip_echo_server;
pub mod multihomed_sockets;
pub mod socket_addr_space;
pub mod sockets;
pub mod token_bucket;
⋮----
pub mod tooling_for_tests;
⋮----
pub struct UdpSocketPair {
⋮----
pub type PortRange = (u16, u16);
⋮----
pub fn get_public_ip_addr_with_binding(
⋮----
let fut = ip_echo_server_request_with_binding(
⋮----
.enable_all()
.build()?;
let resp = rt.block_on(fut)?;
Ok(resp.address)
⋮----
pub fn get_cluster_shred_version(ip_echo_server_addr: &SocketAddr) -> Result<u16, String> {
let fut = ip_echo_server_request(*ip_echo_server_addr, IpEchoServerMessage::default());
⋮----
.build()
.map_err(|e| e.to_string())?;
let resp = rt.block_on(fut).map_err(|e| e.to_string())?;
⋮----
.ok_or_else(|| "IP echo server does not return a shred-version".to_owned())
⋮----
pub fn get_cluster_shred_version_with_binding(
⋮----
.ok_or_else(|| anyhow::anyhow!("IP echo server does not return a shred-version"))
⋮----
pub fn verify_all_reachable_udp(
⋮----
.max_blocking_threads(MAX_PORT_VERIFY_THREADS)
⋮----
.expect("Tokio builder should be able to reliably create a current thread runtime");
⋮----
rt.block_on(fut)
⋮----
pub fn verify_all_reachable_tcp(
⋮----
pub fn parse_port_or_addr(optstr: Option<&str>, default_addr: SocketAddr) -> SocketAddr {
⋮----
if let Ok(port) = addrstr.parse() {
⋮----
addr.set_port(port);
⋮----
} else if let Ok(addr) = addrstr.parse() {
⋮----
pub fn parse_port_range(port_range: &str) -> Option<PortRange> {
let ports: Vec<&str> = port_range.split('-').collect();
if ports.len() != 2 {
⋮----
let start_port = ports[0].parse();
let end_port = ports[1].parse();
if start_port.is_err() || end_port.is_err() {
⋮----
let start_port = start_port.unwrap();
let end_port = end_port.unwrap();
⋮----
Some((start_port, end_port))
⋮----
pub fn parse_host(host: &str) -> Result<IpAddr, String> {
let parsed_url = Url::parse(&format!("http://{host}")).map_err(|e| e.to_string())?;
if parsed_url.port().is_some() {
return Err(format!("Expected port in URL: {host}"));
⋮----
.to_socket_addrs()
.map_err(|err| err.to_string())?
.map(|socket_address| socket_address.ip())
.collect();
if ips.is_empty() {
Err(format!("Unable to resolve host: {host}"))
⋮----
Ok(ips[0])
⋮----
pub fn is_host(string: String) -> Result<(), String> {
parse_host(&string).map(|_| ())
⋮----
pub fn parse_host_port(host_port: &str) -> Result<SocketAddr, String> {
⋮----
.map_err(|err| format!("Unable to resolve host {host_port}: {err}"))?
⋮----
if addrs.is_empty() {
Err(format!("Unable to resolve host: {host_port}"))
⋮----
Ok(addrs[0])
⋮----
pub fn is_host_port(string: String) -> Result<(), String> {
parse_host_port(&string).map(|_| ())
⋮----
pub fn bind_in_range(ip_addr: IpAddr, range: PortRange) -> io::Result<(u16, UdpSocket)> {
⋮----
pub fn bind_to_localhost() -> io::Result<UdpSocket> {
⋮----
pub fn bind_to_unspecified() -> io::Result<UdpSocket> {
⋮----
pub fn find_available_port_in_range(ip_addr: IpAddr, range: PortRange) -> io::Result<u16> {
let [port] = find_available_ports_in_range(ip_addr, range)?;
Ok(port)
⋮----
pub fn find_available_ports_in_range<const N: usize>(
⋮----
.clone()
.cycle()
.skip(rng().random_range(range.clone()) as usize)
.take(range.len())
.peekable();
⋮----
let port_to_try = next_port_to_try.next().unwrap();
⋮----
num = num.saturating_add(1);
⋮----
if next_port_to_try.peek().is_none() {
return Err(err);
⋮----
Ok(result)
⋮----
mod tests {
⋮----
fn test_response_length() {
⋮----
shred_version: Some(u16::MAX),
⋮----
let resp_size = bincode::serialized_size(&resp).unwrap();
assert_eq!(
⋮----
fn test_backward_compat() {
⋮----
shred_version: Some(42),
⋮----
let mut data = vec![0u8; IP_ECHO_SERVER_RESPONSE_LENGTH];
bincode::serialize_into(&mut data[HEADER_LENGTH..], &response).unwrap();
data.truncate(HEADER_LENGTH + 20);
⋮----
fn test_forward_compat() {
⋮----
bincode::serialize_into(&mut data[HEADER_LENGTH..], &address).unwrap();
⋮----
fn test_parse_port_or_addr() {
let p1 = parse_port_or_addr(Some("9000"), SocketAddr::from(([1, 2, 3, 4], 1)));
assert_eq!(p1.port(), 9000);
let p2 = parse_port_or_addr(Some("127.0.0.1:7000"), SocketAddr::from(([1, 2, 3, 4], 1)));
assert_eq!(p2.port(), 7000);
let p2 = parse_port_or_addr(Some("hi there"), SocketAddr::from(([1, 2, 3, 4], 1)));
assert_eq!(p2.port(), 1);
let p3 = parse_port_or_addr(None, SocketAddr::from(([1, 2, 3, 4], 1)));
assert_eq!(p3.port(), 1);
⋮----
fn test_parse_port_range() {
assert_eq!(parse_port_range("garbage"), None);
assert_eq!(parse_port_range("1-"), None);
assert_eq!(parse_port_range("1-2"), Some((1, 2)));
assert_eq!(parse_port_range("1-2-3"), None);
assert_eq!(parse_port_range("2-1"), None);
⋮----
fn test_parse_host() {
parse_host("localhost:1234").unwrap_err();
parse_host("localhost").unwrap();
parse_host("127.0.0.0:1234").unwrap_err();
parse_host("127.0.0.0").unwrap();
⋮----
fn test_parse_host_port() {
parse_host_port("localhost:1234").unwrap();
parse_host_port("localhost").unwrap_err();
parse_host_port("127.0.0.0:1234").unwrap();
parse_host_port("127.0.0.0").unwrap_err();
⋮----
fn test_is_host_port() {
assert!(is_host_port("localhost:1234".to_string()).is_ok());
assert!(is_host_port("localhost".to_string()).is_err());
⋮----
fn test_find_available_port_in_range() {
⋮----
let port = find_available_port_in_range(ip_addr, (pr_s, pr_e)).unwrap();
assert!((pr_s..pr_e).contains(&port));
let _socket = sockets::bind_to(ip_addr, port).unwrap();
find_available_port_in_range(ip_addr, (port, port + 1)).unwrap_err();
⋮----
fn test_find_available_ports_in_range() {
⋮----
assert!(port_range.1 - port_range.0 > 16);
⋮----
.unwrap();
let ports: [u16; 15] = find_available_ports_in_range(ip_addr, port_range).unwrap();
⋮----
ports_vec.push(sock.local_addr().unwrap().port());
let res: Vec<_> = ports_vec.into_iter().unique().collect();
assert_eq!(res.len(), 16, "Should reserve 16 unique ports");

================
File: net-utils/src/multihomed_sockets.rs
================
pub enum CurrentSocket<'a> {
⋮----
pub trait SocketProvider {
⋮----
fn current_socket_ref(&self) -> &UdpSocket {
match self.current_socket() {
⋮----
pub struct FixedSocketProvider {
⋮----
impl FixedSocketProvider {
pub fn new(socket: Arc<UdpSocket>) -> Self {
⋮----
impl SocketProvider for FixedSocketProvider {
⋮----
fn current_socket(&self) -> CurrentSocket<'_> {
CurrentSocket::Same(self.socket.as_ref())
⋮----
pub struct MultihomedSocketProvider {
⋮----
impl MultihomedSocketProvider {
pub fn new(sockets: Arc<[UdpSocket]>, bind_ip_addrs: Arc<BindIpAddrs>) -> Self {
⋮----
impl SocketProvider for MultihomedSocketProvider {
⋮----
let idx = self.bind_ip_addrs.active_index();
let last = self.last_index.swap(idx, Ordering::AcqRel);
⋮----
pub struct BindIpAddrs {
⋮----
impl Default for BindIpAddrs {
fn default() -> Self {
Self::new(vec![IpAddr::V4(Ipv4Addr::LOCALHOST)]).unwrap()
⋮----
impl BindIpAddrs {
pub fn new(addrs: Vec<IpAddr>) -> Result<Self, String> {
if addrs.is_empty() {
return Err(
"BindIpAddrs requires at least one IP address (--bind-address)".to_string(),
⋮----
if addrs.len() > 1 {
⋮----
if ip.is_loopback() || ip.is_unspecified() || ip.is_multicast() {
return Err(format!(
⋮----
Ok(Self {
⋮----
pub fn active(&self) -> IpAddr {
self.addrs[self.active_index.load(Ordering::Acquire)]
⋮----
pub fn set_active(&self, index: usize) -> Result<IpAddr, String> {
if index >= self.addrs.len() {
⋮----
self.active_index.store(index, Ordering::Release);
Ok(self.addrs[index])
⋮----
pub fn active_index(&self) -> usize {
self.active_index.load(Ordering::Acquire)
⋮----
pub fn multihoming_enabled(&self) -> bool {
self.addrs.len() > 1
⋮----
impl Deref for BindIpAddrs {
type Target = [IpAddr];
fn deref(&self) -> &Self::Target {
⋮----
fn as_ref(&self) -> &[IpAddr] {

================
File: net-utils/src/socket_addr_space.rs
================
pub enum SocketAddrSpace {
⋮----
impl SocketAddrSpace {
pub fn new(allow_private_addr: bool) -> Self {
⋮----
pub fn check(&self, addr: &SocketAddr) -> bool {
if matches!(self, SocketAddrSpace::Unspecified) {
⋮----
match addr.ip() {
⋮----
!(addr.is_private() || addr.is_loopback())
⋮----
!addr.is_loopback()

================
File: net-utils/src/sockets.rs
================
pub fn unique_port_range_for_tests(size: u16) -> Range<u16> {
⋮----
let offset = SLICE.fetch_add(size, Ordering::SeqCst);
⋮----
let slot: u16 = slot.parse().unwrap();
assert!(
⋮----
assert!(start < u16::MAX - size, "Ran out of port numbers!");
⋮----
pub fn localhost_port_range_for_tests() -> (u16, u16) {
let pr = unique_port_range_for_tests(25);
⋮----
pub fn bind_to_localhost_unique() -> io::Result<UdpSocket> {
bind_to(
⋮----
unique_port_range_for_tests(1).start,
⋮----
pub fn bind_gossip_port_in_range(
⋮----
if gossip_addr.port() != 0 {
⋮----
gossip_addr.port(),
bind_common_with_config(bind_ip_addr, gossip_addr.port(), config).unwrap_or_else(|e| {
panic!("gossip_addr bind_to port {}: {}", gossip_addr.port(), e)
⋮----
bind_common_in_range_with_config(bind_ip_addr, port_range, config).expect("Failed to bind")
⋮----
cfg!(not(any(windows, target_os = "ios")));
⋮----
pub struct SocketConfiguration {
⋮----
impl SocketConfiguration {
pub fn recv_buffer_size(mut self, size: usize) -> Self {
self.recv_buffer_size = Some(size);
⋮----
pub fn send_buffer_size(mut self, size: usize) -> Self {
self.send_buffer_size = Some(size);
⋮----
pub fn set_non_blocking(mut self, non_blocking: bool) -> Self {
⋮----
fn set_reuse_port<T>(_socket: &T) -> io::Result<()> {
Ok(())
⋮----
fn set_reuse_port<T>(socket: &T) -> io::Result<()>
⋮----
setsockopt(socket, ReusePort, &true).map_err(io::Error::from)
⋮----
pub(crate) fn udp_socket_with_config(config: SocketConfiguration) -> io::Result<Socket> {
⋮----
sock.set_recv_buffer_size(recv_buffer_size)?;
⋮----
sock.set_send_buffer_size(send_buffer_size)?;
⋮----
set_reuse_port(&sock)?;
⋮----
sock.set_nonblocking(non_blocking)?;
Ok(sock)
⋮----
pub fn bind_common_in_range_with_config(
⋮----
if let Ok((sock, listener)) = bind_common_with_config(ip_addr, port, config) {
return Result::Ok((sock.local_addr().unwrap().port(), (sock, listener)));
⋮----
Err(io::Error::other(format!(
⋮----
pub fn bind_in_range_with_config(
⋮----
let socket = udp_socket_with_config(config)?;
⋮----
if socket.bind(&SockAddr::from(addr)).is_ok() {
let udp_socket: UdpSocket = socket.into();
return Result::Ok((udp_socket.local_addr().unwrap().port(), udp_socket));
⋮----
pub fn multi_bind_in_range_with_config(
⋮----
warn!(
⋮----
let (port, socket) = bind_in_range_with_config(ip_addr, range, config)?;
let sockets = bind_more_with_config(socket, num, config)?;
Ok((port, sockets))
⋮----
pub fn bind_to(ip_addr: IpAddr, port: u16) -> io::Result<UdpSocket> {
⋮----
bind_to_with_config(ip_addr, port, config)
⋮----
pub async fn bind_to_async(ip_addr: IpAddr, port: u16) -> io::Result<TokioUdpSocket> {
⋮----
let socket = bind_to_with_config(ip_addr, port, config)?;
⋮----
pub async fn bind_to_localhost_async() -> io::Result<TokioUdpSocket> {
let port = unique_port_range_for_tests(1).start;
bind_to_async(IpAddr::V4(Ipv4Addr::LOCALHOST), port).await
⋮----
pub async fn bind_to_unspecified_async() -> io::Result<TokioUdpSocket> {
⋮----
bind_to_async(IpAddr::V4(Ipv4Addr::UNSPECIFIED), port).await
⋮----
pub fn bind_to_with_config(
⋮----
let sock = udp_socket_with_config(config)?;
⋮----
sock.bind(&SockAddr::from(addr)).map(|_| sock.into())
⋮----
pub fn bind_common_with_config(
⋮----
sock.bind(&sock_addr)
.and_then(|_| TcpListener::bind(addr).map(|listener| (sock.into(), listener)))
⋮----
pub fn bind_two_in_range_with_offset_and_config(
⋮----
if range.1.saturating_sub(range.0) < offset {
return Err(io::Error::other(
"range too small to find two ports with the correct offset".to_string(),
⋮----
let max_start_port = range.1.saturating_sub(offset);
⋮----
let first_bind_result = bind_to_with_config(ip_addr, port, sock1_config);
⋮----
let second_port = port.saturating_add(offset);
let second_bind_result = bind_to_with_config(ip_addr, second_port, sock2_config);
⋮----
return Ok((
(first_bind.local_addr().unwrap().port(), first_bind),
(second_bind.local_addr().unwrap().port(), second_bind),
⋮----
pub fn bind_more_with_config(
⋮----
Ok(vec![socket])
⋮----
set_reuse_port(&socket)?;
⋮----
let addr = socket.local_addr().unwrap();
let ip = addr.ip();
let port = addr.port();
std::iter::once(Ok(socket))
.chain((1..num).map(|_| bind_to_with_config(ip, port, config)))
.collect()
⋮----
mod tests {
⋮----
fn runtime() -> Runtime {
⋮----
.enable_all()
.build()
.expect("Can not create a runtime")
⋮----
fn test_bind() {
let (pr_s, pr_e) = localhost_port_range_for_tests();
⋮----
let s = bind_in_range(ip_addr, (pr_s, pr_e)).unwrap();
assert_eq!(s.0, pr_s, "bind_in_range should use first available port");
⋮----
let x = bind_to_with_config(ip_addr, pr_s + 1, config).unwrap();
let y = bind_more_with_config(x, 2, config).unwrap();
assert_eq!(
⋮----
bind_to_with_config(ip_addr, pr_s, SocketConfiguration::default()).unwrap_err();
bind_in_range(ip_addr, (pr_s, pr_s + 2)).unwrap_err();
⋮----
multi_bind_in_range_with_config(ip_addr, (pr_s + 5, pr_e), config, 10).unwrap();
⋮----
assert_eq!(port, sock.local_addr().unwrap().port());
⋮----
fn test_bind_with_any_port() {
let x = bind_to_localhost_unique().unwrap();
let y = bind_to_localhost_unique().unwrap();
assert_ne!(
⋮----
fn test_bind_in_range_nil() {
⋮----
let range = unique_port_range_for_tests(2);
bind_in_range(ip_addr, (range.end, range.end)).unwrap_err();
bind_in_range(ip_addr, (range.end, range.start)).unwrap_err();
⋮----
fn test_bind_on_top() {
⋮----
let port_range = localhost_port_range_for_tests();
let (_p, s) = bind_in_range_with_config(localhost, port_range, config).unwrap();
let _socks = bind_more_with_config(s, 8, config).unwrap();
let _socks2 = multi_bind_in_range_with_config(localhost, port_range, config, 8).unwrap();
⋮----
fn test_bind_common_in_range() {
⋮----
let range = unique_port_range_for_tests(5);
⋮----
bind_common_in_range_with_config(ip_addr, (range.start, range.end), config).unwrap();
assert!(range.contains(&port));
bind_common_in_range_with_config(ip_addr, (port, port + 1), config).unwrap_err();
⋮----
fn test_bind_two_in_range_with_offset() {
⋮----
let port_range = unique_port_range_for_tests(10);
if let Ok(((port1, _), (port2, _))) = bind_two_in_range_with_offset_and_config(
⋮----
assert!(port2 == port1 + offset);
⋮----
assert!(bind_two_in_range_with_offset_and_config(
⋮----
fn test_get_public_ip_addr_none() {
⋮----
bind_common_in_range_with_config(ip_addr, (pr_s, pr_e), config).unwrap();
let _runtime = ip_echo_server(
⋮----
Some(42),
⋮----
let server_ip_echo_addr = server_udp_socket.local_addr().unwrap();
⋮----
assert_eq!(get_cluster_shred_version(&server_ip_echo_addr).unwrap(), 42);
assert!(verify_all_reachable_tcp(&server_ip_echo_addr, vec![],));
assert!(verify_all_reachable_udp(&server_ip_echo_addr, &[],));
⋮----
fn test_get_public_ip_addr_reachable() {
⋮----
bind_common_in_range_with_config(ip_addr, port_range, config).unwrap();
⋮----
Some(65535),
⋮----
let ip_echo_server_addr = server_udp_socket.local_addr().unwrap();
⋮----
assert!(verify_all_reachable_tcp(
⋮----
assert!(verify_all_reachable_udp(
⋮----
fn test_verify_ports_tcp_unreachable() {
⋮----
let rt = runtime();
assert!(!rt.block_on(ip_echo_client::verify_all_reachable_tcp(
⋮----
fn test_verify_ports_udp_unreachable() {
⋮----
let port_range = unique_port_range_for_tests(2);
⋮----
bind_common_in_range_with_config(ip_addr, (port_range.start, port_range.end), config)
.unwrap();
⋮----
assert!(!rt.block_on(ip_echo_client::verify_all_reachable_udp(
⋮----
fn test_verify_many_ports_reachable() {
⋮----
let mut tcp_listeners = vec![];
let mut udp_sockets = vec![];
let port_range = unique_port_range_for_tests(1);
⋮----
bind_common_in_range_with_config(
⋮----
tcp_listeners.push(client_tcp_listener);
udp_sockets.push(client_udp_socket);
⋮----
let ip_echo_server_addr = server_tcp_listener.local_addr().unwrap();
⋮----
let socket_refs = udp_sockets.iter().collect_vec();
⋮----
assert!(verify_all_reachable_udp(&ip_echo_server_addr, &socket_refs));
⋮----
fn test_verify_udp_multiple_ips_reachable() {
⋮----
bind_common_in_range_with_config(ip_a, port_range, config).unwrap();
let ip_echo_server_addr = srv_udp_sock.local_addr().unwrap();
⋮----
bind_common_in_range_with_config(ip_b, port_range, config).unwrap();
udp_sockets.push(sock_a);
udp_sockets.push(sock_b);
let socket_refs: Vec<&UdpSocket> = udp_sockets.iter().collect();

================
File: net-utils/src/token_bucket.rs
================
pub struct TokenBucket {
⋮----
impl TokenBucket {
pub fn new(initial_tokens: u64, max_tokens: u64, new_tokens_per_second: f64) -> Self {
assert!(
⋮----
pub fn current_tokens(&self) -> u64 {
let now = self.time_us();
self.update_state(now);
self.tokens.load(Ordering::Relaxed)
⋮----
pub fn consume_tokens(&self, request_size: u64) -> Result<u64, u64> {
⋮----
match self.tokens.fetch_update(
⋮----
Some(tokens.saturating_sub(request_size))
⋮----
Ok(prev) => Ok(prev.saturating_sub(request_size)),
Err(prev) => Err(request_size.saturating_sub(prev)),
⋮----
pub fn us_to_have_tokens(&self, num_tokens: u64) -> Option<u64> {
⋮----
match num_tokens.checked_sub(self.current_tokens()) {
Some(missing) => Some((missing as f64 / self.new_tokens_per_us) as u64),
None => Some(0),
⋮----
fn time_us(&self) -> u64 {
cfg_if! {
⋮----
fn update_state(&self, now: u64) {
let last = self.last_update.load(Ordering::SeqCst);
⋮----
match self.last_update.compare_exchange(
⋮----
let elapsed = now.saturating_sub(last);
⋮----
elapsed.saturating_add(self.credit_time_us.swap(0, Ordering::Relaxed));
⋮----
let new_tokens = new_tokens_f64.floor() as u64;
⋮----
let _ = self.tokens.fetch_update(
⋮----
|tokens| Some(tokens.saturating_add(new_tokens).min(self.max_tokens)),
⋮----
(new_tokens_f64.fract() / self.new_tokens_per_us) as u64
⋮----
.fetch_add(time_to_return, Ordering::Relaxed);
⋮----
impl Clone for TokenBucket {
fn clone(&self) -> Self {
⋮----
tokens: AtomicU64::new(self.tokens.load(Ordering::Relaxed)),
last_update: AtomicU64::new(self.last_update.load(Ordering::Relaxed)),
credit_time_us: AtomicU64::new(self.credit_time_us.load(Ordering::Relaxed)),
⋮----
pub struct KeyedRateLimiter<K>
⋮----
pub fn new(target_capacity: usize, prototype_bucket: TokenBucket, shard_amount: usize) -> Self {
⋮----
pub fn current_tokens(&self, key: impl Borrow<K>) -> Option<u64> {
let bucket = self.data.get(key.borrow())?;
Some(bucket.current_tokens())
⋮----
pub fn consume_tokens(&self, key: K, request_size: u64) -> Result<u64, u64> {
⋮----
let bucket = self.data.entry(key);
⋮----
Entry::Occupied(entry) => (false, entry.get().consume_tokens(request_size)),
⋮----
let bucket = self.prototype_bucket.clone();
let res = bucket.consume_tokens(request_size);
entry.insert(bucket);
⋮----
.fetch_update(Ordering::Relaxed, Ordering::Relaxed, |v| {
⋮----
Some(v.saturating_sub(1))
⋮----
self.maybe_shrink();
⋮----
.store(self.shrink_interval, Ordering::Relaxed);
⋮----
self.approx_len.fetch_add(1, Ordering::Relaxed);
⋮----
pub fn len_approx(&self) -> usize {
self.approx_len.load(Ordering::Relaxed)
⋮----
fn maybe_shrink(&self) {
⋮----
let target_shard_size = self.target_capacity / self.data.shards().len();
⋮----
for shardlock in self.data.shards() {
let mut shard = shardlock.write();
if shard.len() <= target_shard_size * 3 / 2 {
actual_len += shard.len();
⋮----
entries.clear();
entries.extend(
shard.drain().map(|(key, value)| {
(key, value.get().last_update.load(Ordering::SeqCst), value)
⋮----
entries.select_nth_unstable_by_key(target_shard_size, |(_, last_update, _)| {
Reverse(*last_update)
⋮----
shard.extend(
⋮----
.drain(..)
.take(target_shard_size)
.map(|(key, _last_update, value)| (key, value)),
⋮----
debug_assert!(shard.len() <= target_shard_size);
⋮----
self.approx_len.store(actual_len, Ordering::Relaxed);
⋮----
pub fn set_shrink_interval(&mut self, interval: usize) {
⋮----
pub fn shrink_interval(&self) -> usize {
⋮----
pub mod test {
⋮----
fn test_token_bucket_basics() {
⋮----
assert_eq!(tb.current_tokens(), 100);
tb.consume_tokens(50).expect("Bucket is initially full");
tb.consume_tokens(50)
.expect("We should still have >50 tokens left");
⋮----
.expect_err("There should not be enough tokens now");
⋮----
tb.consume_tokens(40)
.expect("Bucket should have enough for another request now");
⋮----
assert_eq!(tb.current_tokens(), 100, "Bucket should not overfill");
⋮----
fn test_token_bucket_us_to_have_tokens() {
⋮----
assert_eq!(tb.current_tokens(), 1000);
tb.consume_tokens(1000).expect("Bucket is initially full");
⋮----
.us_to_have_tokens(500)
.expect("500 < bucket capacity (1000)")
⋮----
assert!(t > 100, "time to fill should be ~ 500ms (got {t})");
assert!(t <= 500, "time to fill should be less than 500ms (got {t})");
⋮----
fn test_keyed_rate_limiter() {
⋮----
assert_eq!(rl.current_tokens(ip1), None, "Initially no buckets exist");
rl.consume_tokens(ip1, 50)
.expect("Bucket is initially full");
⋮----
rl.consume_tokens(ip2, 50)
⋮----
rl.consume_tokens(ip1, 40)
⋮----
assert_eq!(
⋮----
rl.consume_tokens(ip2, 100).expect("Bucket should be full");
⋮----
rl.consume_tokens(ip, 50).unwrap();
⋮----
rl.consume_tokens(ip2, 100)
.expect("New bucket should have been made for ip2");
⋮----
fn shuttle_test_token_bucket_race() {
use shuttle::sync::atomic::AtomicBool;
⋮----
TIME_US.store(0, Ordering::SeqCst);
⋮----
while current_time < test_duration_us && run.load(Ordering::SeqCst) {
⋮----
TIME_US.store(current_time, Ordering::SeqCst);
⋮----
run.store(false, Ordering::SeqCst);
⋮----
.map(|_| {
⋮----
while run.load(Ordering::SeqCst) {
if tb.consume_tokens(5).is_ok() {
⋮----
.collect();
time_advancer.join().unwrap();
let received = threads.into_iter().map(|t| t.join().unwrap()).sum();
assert_eq!(4, received);

================
File: net-utils/src/tooling_for_tests.rs
================
pub fn hexdump(bytes: &[u8]) -> anyhow::Result<()> {
⋮----
std::io::stderr().write_all(b"\n")?;
Ok(())
⋮----
pub struct PcapReader {
⋮----
impl PcapReader {
pub fn new(filename: &PathBuf) -> anyhow::Result<Self> {
let file_in = File::open(filename).with_context(|| format!("opening file {filename:?}"))?;
let reader = PcapNgReader::new(file_in).context("pcap reader creation")?;
Ok(PcapReader { reader })
⋮----
impl Iterator for PcapReader {
type Item = Vec<u8>;
fn next(&mut self) -> Option<Self::Item> {
⋮----
let block = match self.reader.next_block() {
Some(block) => block.ok()?,
⋮----
debug!("Skipping unknown block in pcap file");
⋮----
return Some(pkt_payload.to_vec());
⋮----
pub fn validate_packet_format<T>(
⋮----
info!(
⋮----
for data in reader.into_iter() {
⋮----
let packet = parse_packet(&data);
⋮----
let reconstructed_bytes = serialize_packet(pkt);
let diff = custom_compare(&reconstructed_bytes, &data);
⋮----
error!(
⋮----
error!("Differences start at byte {pos}");
error!("Original packet:");
show_packet(&data)?;
error!("Reserialized:");
show_packet(&reconstructed_bytes)?;
⋮----
error!("Found packet {number} that failed to parse with error {e}");
error!("Problematic packet:");
⋮----
error!("Packet format checks passed for {number} packets, failed for {errors} packets.");
Err(anyhow::anyhow!("Failed checks for {errors} packets"))
⋮----
info!("Packet format checks passed for {number} packets.");
Ok(number)

================
File: net-utils/.gitignore
================
/target/
/farf/

================
File: net-utils/Cargo.toml
================
[package]
name = "solana-net-utils"
description = "Solana Network Utilities"
documentation = "https://docs.rs/solana-net-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "solana_net_utils"

[features]
agave-unstable-api = []
dev-context-only-utils = ["dep:pcap-file", "dep:hxdmp"]
shuttle-test = ["dep:shuttle", "solana-svm-type-overrides/shuttle-test"]

[dependencies]
anyhow = { workspace = true }
bincode = { workspace = true }
bytes = { workspace = true }
cfg-if = { workspace = true }
dashmap = { workspace = true, features = ["raw-api"] }
hxdmp = { version = "0.2.1", optional = true }
itertools = { workspace = true }
log = { workspace = true }
nix = { workspace = true, features = ["socket"] }
pcap-file = { version = "2.0.0", optional = true }
rand = { workspace = true }
serde = { workspace = true }
shuttle = { workspace = true, optional = true }
socket2 = { workspace = true }
solana-serde = { workspace = true }
solana-svm-type-overrides = { workspace = true }
tokio = { workspace = true, features = ["full"] }
url = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
solana-net-utils = { path = ".", features = ["agave-unstable-api"] }

[lints]
workspace = true

[[bench]]
name = "token_bucket"
harness = false

================
File: notifier/src/lib.rs
================
struct TelegramWebHook {
⋮----
struct TwilioWebHook {
⋮----
impl TwilioWebHook {
fn complete(&self) -> bool {
!(self.account.is_empty()
|| self.token.is_empty()
|| self.to.is_empty()
|| self.from.is_empty())
⋮----
fn get_twilio_config() -> Result<Option<TwilioWebHook>, String> {
⋮----
if config_var.is_err() {
info!("Twilio notifications disabled");
return Ok(None);
⋮----
for pair in config_var.unwrap().split(',') {
let nv: Vec<_> = pair.split('=').collect();
if nv.len() != 2 {
return Err(format!("TWILIO_CONFIG is invalid: '{pair}'"));
⋮----
let v = nv[1].to_string();
⋮----
_ => return Err(format!("TWILIO_CONFIG is invalid: '{pair}'")),
⋮----
if !config.complete() {
return Err("TWILIO_CONFIG is incomplete".to_string());
⋮----
Ok(Some(config))
⋮----
enum NotificationChannel {
⋮----
pub enum NotificationType {
⋮----
pub struct Notifier {
⋮----
impl Default for Notifier {
fn default() -> Self {
⋮----
impl Notifier {
pub fn new(env_prefix: &str) -> Self {
info!("Initializing {env_prefix}Notifier");
let mut notifiers = vec![];
if let Ok(webhook) = env::var(format!("{env_prefix}DISCORD_WEBHOOK")) {
notifiers.push(NotificationChannel::Discord(webhook));
⋮----
if let Ok(webhook) = env::var(format!("{env_prefix}SLACK_WEBHOOK")) {
notifiers.push(NotificationChannel::Slack(webhook));
⋮----
if let Ok(routing_key) = env::var(format!("{env_prefix}PAGERDUTY_INTEGRATION_KEY")) {
notifiers.push(NotificationChannel::PagerDuty(routing_key));
⋮----
env::var(format!("{env_prefix}TELEGRAM_BOT_TOKEN")),
env::var(format!("{env_prefix}TELEGRAM_CHAT_ID")),
⋮----
notifiers.push(NotificationChannel::Telegram(TelegramWebHook {
⋮----
if let Ok(Some(webhook)) = get_twilio_config() {
notifiers.push(NotificationChannel::Twilio(webhook));
⋮----
if let Ok(log_level) = env::var(format!("{env_prefix}LOG_NOTIFIER_LEVEL")) {
⋮----
Ok(level) => notifiers.push(NotificationChannel::Log(level)),
⋮----
warn!("could not parse specified log notifier level string ({log_level}): {e}")
⋮----
info!("{} notifiers", notifiers.len());
⋮----
pub fn is_empty(&self) -> bool {
self.notifiers.is_empty()
⋮----
pub fn send(&self, msg: &str, notification_type: &NotificationType) {
⋮----
for line in msg.split('\n') {
sleep(Duration::from_millis(1000));
info!("Sending {line}");
let data = json!({ "content": line });
⋮----
let response = self.client.post(webhook).json(&data).send();
⋮----
warn!("Failed to send Discord message: \"{line}\": {err:?}");
⋮----
info!("response status: {}", response.status());
if response.status() == StatusCode::TOO_MANY_REQUESTS {
warn!("rate limited!...");
warn!("response text: {:?}", response.text());
sleep(Duration::from_secs(2));
⋮----
let data = json!({ "text": msg });
if let Err(err) = self.client.post(webhook).json(&data).send() {
warn!("Failed to send Slack message: {err:?}");
⋮----
NotificationType::Trigger { incident } => incident.clone().to_string(),
NotificationType::Resolve { incident } => incident.clone().to_string(),
⋮----
let data = json!({"payload":{"summary":msg,"source":"agave-watchtower","severity":"critical"},"routing_key":routing_key,"event_action":event_action,"dedup_key":dedup_key});
⋮----
if let Err(err) = self.client.post(url).json(&data).send() {
warn!("Failed to send PagerDuty alert: {err:?}");
⋮----
let data = json!({ "chat_id": chat_id, "text": msg });
let url = format!("https://api.telegram.org/bot{bot_token}/sendMessage");
⋮----
warn!("Failed to send Telegram message: {err:?}");
⋮----
let url = format!(
⋮----
let params = [("To", to), ("From", from), ("Body", &msg.to_string())];
if let Err(err) = self.client.post(url).form(&params).send() {
warn!("Failed to send Twilio message: {err:?}");
⋮----
log!(*level, "{msg}")

================
File: notifier/.gitignore
================
/target/
/farf/

================
File: notifier/Cargo.toml
================
[package]
name = "solana-notifier"
description = "Solana Notifier"
documentation = "https://docs.rs/solana-notifier"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "solana_notifier"

[features]
agave-unstable-api = []

[dependencies]
log = { workspace = true }
reqwest = { workspace = true, features = ["blocking", "brotli", "deflate", "gzip", "rustls-tls", "json"] }
serde_json = { workspace = true }
solana-hash = { workspace = true }

================
File: perf/benches/dedup.rs
================
fn test_packet_with_size(size: usize, rng: &mut ThreadRng) -> Vec<u8> {
(0..size.checked_sub(8).unwrap())
.map(|_| rng.gen())
.collect()
⋮----
fn do_bench_dedup_packets(b: &mut Bencher, mut batches: Vec<PacketBatch>) {
⋮----
b.iter(|| {
⋮----
deduper.maybe_reset(
⋮----
batches.iter_mut().for_each(|b| {
b.iter_mut()
.for_each(|mut p| p.meta_mut().set_discard(false))
⋮----
fn bench_dedup_same_small_packets(b: &mut Bencher) {
⋮----
let small_packet = test_packet_with_size(128, &mut rng);
let batches = to_packet_batches(
⋮----
do_bench_dedup_packets(b, batches);
⋮----
fn bench_dedup_same_big_packets(b: &mut Bencher) {
⋮----
let big_packet = test_packet_with_size(1024, &mut rng);
⋮----
fn bench_dedup_diff_small_packets(b: &mut Bencher) {
⋮----
.map(|_| test_packet_with_size(128, &mut rng))
⋮----
fn bench_dedup_diff_big_packets(b: &mut Bencher) {
⋮----
.map(|_| test_packet_with_size(1024, &mut rng))
⋮----
fn bench_dedup_baseline(b: &mut Bencher) {
⋮----
fn bench_dedup_reset(b: &mut Bencher) {
⋮----
benchmark_group!(
⋮----
benchmark_main!(benches);

================
File: perf/benches/discard.rs
================
fn bench_discard(b: &mut Bencher) {
⋮----
let tx = test_tx();
⋮----
let batches = to_packet_batches(
⋮----
b.iter(|| {
let mut discarded = batches.clone();
discard_batches_randomly(&mut discarded, 100, NUM);
assert_eq!(discarded.len(), 10);
⋮----
benchmark_group!(benches, bench_discard);
benchmark_main!(benches);

================
File: perf/benches/recycler.rs
================
fn bench_recycler(b: &mut Bencher) {
⋮----
let _packet = recycler.allocate("");
⋮----
b.iter(move || {
⋮----
benchmark_group!(benches, bench_recycler);
benchmark_main!(benches);

================
File: perf/benches/reset.rs
================
fn bench_reset1(b: &mut Bencher) {
⋮----
v.resize_with(N, AtomicU64::default);
b.iter(|| {
black_box({
⋮----
i.store(0, Ordering::Relaxed);
⋮----
fn bench_reset2(b: &mut Bencher) {
⋮----
v.clear();
⋮----
benchmark_group!(benches, bench_reset2, bench_reset1);
benchmark_main!(benches);

================
File: perf/benches/shrink.rs
================
fn test_packet_with_size(size: usize, rng: &mut ThreadRng) -> Vec<u8> {
(0..size.checked_sub(8).unwrap())
.map(|_| rng.gen())
.collect()
⋮----
fn do_bench_shrink_packets(b: &mut Bencher, mut batches: Vec<PacketBatch>) {
⋮----
batches.iter_mut().for_each(|b| {
b.iter_mut()
.for_each(|mut p| p.meta_mut().set_discard(thread_rng().gen()))
⋮----
batches.clone()
⋮----
.take(32)
⋮----
.into_iter()
.cycle();
b.iter(|| {
let batches = batches.next().unwrap();
⋮----
fn bench_shrink_diff_small_packets(b: &mut Bencher) {
⋮----
let batches = to_packet_batches(
⋮----
.map(|_| test_packet_with_size(128, &mut rng))
⋮----
do_bench_shrink_packets(b, batches);
⋮----
fn bench_shrink_diff_big_packets(b: &mut Bencher) {
⋮----
.map(|_| test_packet_with_size(1024, &mut rng))
⋮----
fn bench_shrink_count_packets(b: &mut Bencher) {
⋮----
let mut batches = to_packet_batches(
⋮----
benchmark_group!(
⋮----
benchmark_main!(benches);

================
File: perf/benches/sigverify.rs
================
fn bench_sigverify_simple(b: &mut Bencher) {
let tx = test_tx();
⋮----
let mut batches = to_packet_batches(
⋮----
b.iter(|| {
⋮----
fn gen_batches(
⋮----
to_packet_batches(&vec![tx; total_packets], packets_per_batch)
⋮----
.take(total_packets)
.collect();
to_packet_batches(&txs, packets_per_batch)
⋮----
fn bench_sigverify_low_packets_small_batch(b: &mut Bencher) {
⋮----
let mut batches = gen_batches(false, 1, num_packets);
⋮----
fn bench_sigverify_low_packets_large_batch(b: &mut Bencher) {
⋮----
let mut batches = gen_batches(false, LARGE_BATCH_PACKET_COUNT, num_packets);
⋮----
fn bench_sigverify_medium_packets_small_batch(b: &mut Bencher) {
⋮----
fn bench_sigverify_medium_packets_large_batch(b: &mut Bencher) {
⋮----
fn bench_sigverify_high_packets_small_batch(b: &mut Bencher) {
⋮----
fn bench_sigverify_high_packets_large_batch(b: &mut Bencher) {
⋮----
fn bench_sigverify_uneven(b: &mut Bencher) {
⋮----
let simple_tx = test_tx();
let multi_tx = test_multisig_tx();
⋮----
let mut batches = vec![];
⋮----
let mut len: usize = thread_rng().gen_range(1..128);
⋮----
if thread_rng().gen_ratio(1, 2) {
tx = simple_tx.clone();
⋮----
tx = multi_tx.clone();
⋮----
let mut packet = BytesPacket::from_data(None, &tx).expect("serialize request");
if thread_rng().gen_ratio((num_packets - NUM) as u32, num_packets as u32) {
packet.meta_mut().set_discard(true);
⋮----
batch.push(packet);
⋮----
batches.push(PacketBatch::from(batch));
⋮----
info!("num_packets: {num_packets} valid: {num_valid}");
⋮----
fn bench_get_offsets(b: &mut Bencher) {
⋮----
let mut batches = to_packet_batches(&std::iter::repeat_n(tx, 1024).collect::<Vec<_>>(), 1024);
⋮----
benchmark_group!(
⋮----
benchmark_main!(benches);

================
File: perf/src/data_budget.rs
================
pub struct DataBudget {
⋮----
impl DataBudget {
pub fn restricted() -> Self {
⋮----
pub fn take(&self, size: usize) -> bool {
let mut bytes = self.bytes.load(Ordering::Acquire);
⋮----
bytes = match self.bytes.compare_exchange_weak(
⋮----
match bytes.checked_sub(size) {
⋮----
fn can_update(&self, duration_millis: u64) -> bool {
⋮----
let mut asof = self.asof.load(Ordering::Acquire);
while asof.saturating_add(duration_millis) <= now {
asof = match self.asof.compare_exchange_weak(
⋮----
pub fn update<F>(&self, duration_millis: u64, updater: F) -> usize
⋮----
if self.can_update(duration_millis) {
⋮----
updater(bytes),
⋮----
self.bytes.load(Ordering::Acquire)
⋮----
pub fn check(&self, size: usize) -> bool {
size <= self.bytes.load(Ordering::Acquire)
⋮----
mod tests {
⋮----
fn test_data_budget() {
⋮----
assert!(!budget.take(1));
assert_eq!(budget.update(1000, |bytes| bytes + 5), 5);
assert!(budget.take(1));
assert!(budget.take(2));
assert!(!budget.take(3));
assert_eq!(budget.update(30, |_| 10), 2);
⋮----
assert_eq!(budget.update(30, |bytes| bytes * 2), 4);
assert!(budget.take(3));

================
File: perf/src/deduper.rs
================
pub struct Deduper<const K: usize, T: ?Sized> {
⋮----
pub fn new<R: Rng>(rng: &mut R, num_bits: u64) -> Self {
let size = num_bits.checked_add(63).unwrap() / 64;
let size = usize::try_from(size).unwrap();
⋮----
state: std::array::from_fn(|_| new_random_state(rng)),
⋮----
bits: repeat_with(AtomicU64::default).take(size).collect(),
⋮----
fn false_positive_rate(&self) -> f64 {
let popcount = self.popcount.load(Ordering::Relaxed);
let ones_ratio = popcount.min(self.num_bits) as f64 / self.num_bits as f64;
ones_ratio.powi(K as i32)
⋮----
pub fn maybe_reset<R: Rng>(
⋮----
assert!(0.0 < false_positive_rate && false_positive_rate < 1.0);
let saturated = self.false_positive_rate() >= false_positive_rate;
if saturated || self.clock.elapsed() >= reset_cycle {
self.state = std::array::from_fn(|_| new_random_state(rng));
⋮----
self.bits.fill_with(AtomicU64::default);
⋮----
pub fn dedup(&self, data: &T) -> bool {
⋮----
let hash: u64 = random_state.hash_one(data) % self.num_bits;
⋮----
let old = self.bits[index].fetch_or(mask, Ordering::Relaxed);
⋮----
self.popcount.fetch_add(1, Ordering::Relaxed);
⋮----
fn new_random_state<R: Rng>(rng: &mut R) -> RandomState {
RandomState::with_seeds(rng.gen(), rng.gen(), rng.gen(), rng.gen())
⋮----
pub fn dedup_packets_and_count_discards<const K: usize>(
⋮----
.iter_mut()
.flat_map(|batch| batch.iter_mut())
.map(|mut packet| {
if !packet.meta().discard()
⋮----
.data(..)
.map(|data| deduper.dedup(data))
.unwrap_or(true)
⋮----
packet.meta_mut().set_discard(true);
⋮----
u64::from(packet.meta().discard())
⋮----
.sum()
⋮----
mod tests {
⋮----
fn test_dedup_same() {
let tx = test_tx();
⋮----
to_packet_batches(&std::iter::repeat_n(tx, 1024).collect::<Vec<_>>(), 128);
⋮----
let discard = dedup_packets_and_count_discards(&filter, &mut batches) as usize;
assert_eq!(packet_count, discard + 1);
⋮----
fn test_dedup_diff() {
⋮----
let mut batches = to_packet_batches(&(0..1024).map(|_| test_tx()).collect::<Vec<_>>(), 128);
⋮----
assert_eq!(discard, 0);
assert!(!filter.maybe_reset(
⋮----
assert_eq!(i.load(Ordering::Relaxed), 0);
⋮----
fn get_capacity<const K: usize>(num_bits: u64, false_positive_rate: f64) -> u64 {
(num_bits as f64 * false_positive_rate.powf(1f64 / K as f64)) as u64
⋮----
fn test_dedup_saturated() {
⋮----
assert!(filter.popcount.load(Ordering::Relaxed) < capacity);
⋮----
to_packet_batches(&(0..1000).map(|_| test_tx()).collect::<Vec<_>>(), 128);
discard += dedup_packets_and_count_discards(&filter, &mut batches) as usize;
trace!("{i} {discard}");
if filter.popcount.load(Ordering::Relaxed) > capacity {
⋮----
assert!(filter.popcount.load(Ordering::Relaxed) > capacity);
assert!(filter.false_positive_rate() >= FALSE_POSITIVE_RATE);
assert!(filter.maybe_reset(
⋮----
fn test_dedup_false_positive() {
⋮----
to_packet_batches(&(0..1024).map(|_| test_tx()).collect::<Vec<_>>(), 128);
⋮----
debug!("false positive rate: {}/{}", discard, i * 1024);
⋮----
assert!(discard < 2);
⋮----
fn test_dedup_capacity(num_bits: u64, false_positive_rate: f64, capacity: u64) {
⋮----
assert_eq!(get_capacity::<2>(num_bits, false_positive_rate), capacity);
⋮----
assert_eq!(deduper.false_positive_rate(), 0.0);
deduper.popcount.store(capacity, Ordering::Relaxed);
assert!(deduper.false_positive_rate() < false_positive_rate);
deduper.popcount.store(capacity + 1, Ordering::Relaxed);
assert!(deduper.false_positive_rate() >= false_positive_rate);
assert!(deduper.maybe_reset(
⋮----
fn test_dedup_seeded(
⋮----
assert_eq!(get_capacity::<2>(num_bits, FALSE_POSITIVE_RATE), capacity);
⋮----
let size = rng.gen_range(0..PACKET_DATA_SIZE);
packet.meta_mut().size = size;
rng.fill(&mut packet.buffer_mut()[0..size]);
if deduper.dedup(packet.data(..).unwrap()) {
⋮----
assert!(deduper.dedup(packet.data(..).unwrap()));
⋮----
assert_eq!(dup_count, num_dups);
assert_eq!(deduper.popcount.load(Ordering::Relaxed), popcount);
assert!(deduper.false_positive_rate() < FALSE_POSITIVE_RATE);
assert!(!deduper.maybe_reset(

================
File: perf/src/discard.rs
================
pub fn discard_batches_randomly(
⋮----
let index = thread_rng().gen_range(0..batches.len());
let removed = batches.swap_remove(index);
total_packets = total_packets.saturating_sub(removed.len());
⋮----
mod tests {
⋮----
fn test_batch_discard_random() {
⋮----
batch.resize(1, BytesPacket::new(Bytes::new(), Meta::default()));
⋮----
let mut batches = vec![batch; num_batches];
⋮----
discard_batches_randomly(&mut batches, max, num_batches);
assert_eq!(batches.len(), max);

================
File: perf/src/lib.rs
================
pub mod data_budget;
pub mod deduper;
pub mod discard;
pub mod packet;
pub mod perf_libs;
pub mod recycled_vec;
pub mod recycler;
pub mod recycler_cache;
pub mod sigverify;
⋮----
pub mod test_tx;
pub mod thread;
⋮----
extern crate log;
⋮----
extern crate assert_matches;
⋮----
extern crate solana_metrics;
⋮----
extern crate solana_frozen_abi_macro;
fn is_rosetta_emulated() -> bool {
⋮----
use std::str::FromStr;
⋮----
.args(["-in", "sysctl.proc_translated"])
.output()
.map_err(|_| ())
.and_then(|output| String::from_utf8(output.stdout).map_err(|_| ()))
.and_then(|stdout| u8::from_str(stdout.trim()).map_err(|_| ()))
.map(|enabled| enabled == 1)
.unwrap_or(false)
⋮----
pub fn report_target_features() {
if !is_rosetta_emulated() {
⋮----
if is_x86_feature_detected!("avx") {
info!("AVX detected");
⋮----
error!(
⋮----
if is_x86_feature_detected!("avx2") {
info!("AVX2 detected");

================
File: perf/src/packet.rs
================
pub struct BytesPacket {
⋮----
impl BytesPacket {
pub fn new(buffer: Bytes, meta: Meta) -> Self {
⋮----
pub fn empty() -> Self {
⋮----
pub fn from_bytes(dest: Option<&SocketAddr>, buffer: impl Into<Bytes>) -> Self {
let buffer = buffer.into();
⋮----
meta.size = buffer.len();
⋮----
meta.set_socket_addr(dest);
⋮----
pub fn from_data<T>(dest: Option<&SocketAddr>, data: T) -> bincode::Result<Self>
⋮----
let mut writer = buffer.writer();
data.encode(&mut writer)?;
let buffer = writer.into_inner();
let buffer = buffer.freeze();
⋮----
Ok(Self { buffer, meta })
⋮----
pub fn data<I>(&self, index: I) -> Option<&<I as SliceIndex<[u8]>>::Output>
⋮----
if self.meta.discard() {
⋮----
self.buffer.get(index)
⋮----
pub fn meta(&self) -> &Meta {
⋮----
pub fn meta_mut(&mut self) -> &mut Meta {
⋮----
pub fn deserialize_slice<T, I>(&self, index: I) -> bincode::Result<T>
⋮----
let bytes = self.data(index).ok_or(bincode::ErrorKind::SizeLimit)?;
⋮----
.with_limit(self.meta().size as u64)
.with_fixint_encoding()
.reject_trailing_bytes()
.deserialize(bytes)
⋮----
pub fn copy_from_slice(&mut self, slice: &[u8]) {
self.buffer = Bytes::from(slice.to_vec());
⋮----
pub fn as_ref(&self) -> PacketRef<'_> {
⋮----
pub fn as_mut(&mut self) -> PacketRefMut<'_> {
⋮----
pub fn buffer(&self) -> &Bytes {
⋮----
pub fn set_buffer(&mut self, buffer: impl Into<Bytes>) {
⋮----
self.meta.size = buffer.len();
⋮----
pub enum PacketBatch {
⋮----
impl PacketBatch {
⋮----
pub fn first(&self) -> Option<PacketRef<'_>> {
⋮----
Self::Pinned(batch) => batch.first().map(PacketRef::from),
Self::Bytes(batch) => batch.first().map(PacketRef::from),
Self::Single(packet) => Some(PacketRef::from(packet)),
⋮----
pub fn first_mut(&mut self) -> Option<PacketRefMut<'_>> {
⋮----
Self::Pinned(batch) => batch.first_mut().map(PacketRefMut::from),
Self::Bytes(batch) => batch.first_mut().map(PacketRefMut::from),
Self::Single(packet) => Some(PacketRefMut::from(packet)),
⋮----
pub fn is_empty(&self) -> bool {
⋮----
Self::Pinned(batch) => batch.is_empty(),
Self::Bytes(batch) => batch.is_empty(),
⋮----
pub fn get(&self, index: usize) -> Option<PacketRef<'_>> {
⋮----
Self::Pinned(batch) => batch.get(index).map(PacketRef::from),
Self::Bytes(batch) => batch.get(index).map(PacketRef::from),
Self::Single(packet) => (index == 0).then_some(PacketRef::from(packet)),
⋮----
pub fn get_mut(&mut self, index: usize) -> Option<PacketRefMut<'_>> {
⋮----
Self::Pinned(batch) => batch.get_mut(index).map(PacketRefMut::from),
Self::Bytes(batch) => batch.get_mut(index).map(PacketRefMut::from),
Self::Single(packet) => (index == 0).then_some(PacketRefMut::from(packet)),
⋮----
pub fn iter(&self) -> PacketBatchIter<'_> {
⋮----
Self::Pinned(batch) => PacketBatchIter::Pinned(batch.iter()),
Self::Bytes(batch) => PacketBatchIter::Bytes(batch.iter()),
Self::Single(packet) => PacketBatchIter::Bytes(core::array::from_ref(packet).iter()),
⋮----
pub fn iter_mut(&mut self) -> PacketBatchIterMut<'_> {
⋮----
Self::Pinned(batch) => PacketBatchIterMut::Pinned(batch.iter_mut()),
Self::Bytes(batch) => PacketBatchIterMut::Bytes(batch.iter_mut()),
⋮----
PacketBatchIterMut::Bytes(core::array::from_mut(packet).iter_mut())
⋮----
pub fn par_iter(&self) -> PacketBatchParIter<'_> {
⋮----
PacketBatchParIter::Pinned(batch.par_iter().map(PacketRef::from))
⋮----
Self::Bytes(batch) => PacketBatchParIter::Bytes(batch.par_iter().map(PacketRef::from)),
⋮----
.par_iter()
.map(PacketRef::from),
⋮----
pub fn par_iter_mut(&mut self) -> PacketBatchParIterMut<'_> {
⋮----
PacketBatchParIterMut::Pinned(batch.par_iter_mut().map(PacketRefMut::from))
⋮----
PacketBatchParIterMut::Bytes(batch.par_iter_mut().map(PacketRefMut::from))
⋮----
.par_iter_mut()
.map(PacketRefMut::from),
⋮----
pub fn len(&self) -> usize {
⋮----
Self::Pinned(batch) => batch.len(),
Self::Bytes(batch) => batch.len(),
⋮----
fn from(batch: RecycledPacketBatch) -> Self {
⋮----
fn from(batch: BytesPacketBatch) -> Self {
⋮----
fn from(batch: Vec<BytesPacket>) -> Self {
⋮----
impl<'a> IntoIterator for &'a PacketBatch {
type Item = PacketRef<'a>;
type IntoIter = PacketBatchIter<'a>;
fn into_iter(self) -> Self::IntoIter {
self.iter()
⋮----
impl<'a> IntoIterator for &'a mut PacketBatch {
type Item = PacketRefMut<'a>;
type IntoIter = PacketBatchIterMut<'a>;
⋮----
self.iter_mut()
⋮----
impl<'a> IntoParallelIterator for &'a PacketBatch {
type Iter = PacketBatchParIter<'a>;
⋮----
fn into_par_iter(self) -> Self::Iter {
self.par_iter()
⋮----
impl<'a> IntoParallelIterator for &'a mut PacketBatch {
type Iter = PacketBatchParIterMut<'a>;
⋮----
self.par_iter_mut()
⋮----
pub enum PacketRef<'a> {
⋮----
impl PartialEq for PacketRef<'_> {
fn eq(&self, other: &PacketRef<'_>) -> bool {
self.meta().eq(other.meta()) && self.data(..).eq(&other.data(..))
⋮----
fn from(packet: &'a Packet) -> Self {
⋮----
fn from(packet: &'a mut Packet) -> Self {
⋮----
fn from(packet: &'a BytesPacket) -> Self {
⋮----
fn from(packet: &'a mut BytesPacket) -> Self {
⋮----
pub fn data<I>(&self, index: I) -> Option<&'a <I as SliceIndex<[u8]>>::Output>
⋮----
Self::Packet(packet) => packet.data(index),
Self::Bytes(packet) => packet.data(index),
⋮----
Self::Packet(packet) => packet.meta(),
Self::Bytes(packet) => packet.meta(),
⋮----
Self::Packet(packet) => packet.deserialize_slice(index),
Self::Bytes(packet) => packet.deserialize_slice(index),
⋮----
pub fn to_bytes_packet(&self) -> BytesPacket {
⋮----
.data(..)
.map(|data| Bytes::from(data.to_vec()))
.unwrap_or_else(Bytes::new);
BytesPacket::new(buffer, self.meta().clone())
⋮----
Self::Bytes(packet) => packet.to_owned().to_owned(),
⋮----
pub enum PacketRefMut<'a> {
⋮----
impl<'a> PartialEq for PacketRefMut<'a> {
fn eq(&self, other: &PacketRefMut<'a>) -> bool {
self.data(..).eq(&other.data(..)) && self.meta().eq(other.meta())
⋮----
Self::Packet(packet) => packet.meta_mut(),
Self::Bytes(packet) => packet.meta_mut(),
⋮----
pub fn copy_from_slice(&mut self, src: &[u8]) {
⋮----
let size = src.len();
packet.buffer_mut()[..size].copy_from_slice(src);
⋮----
Self::Bytes(packet) => packet.copy_from_slice(src),
⋮----
pub enum PacketBatchIter<'a> {
⋮----
impl DoubleEndedIterator for PacketBatchIter<'_> {
fn next_back(&mut self) -> Option<Self::Item> {
⋮----
Self::Pinned(iter) => iter.next_back().map(PacketRef::Packet),
Self::Bytes(iter) => iter.next_back().map(PacketRef::Bytes),
⋮----
impl<'a> Iterator for PacketBatchIter<'a> {
⋮----
fn next(&mut self) -> Option<Self::Item> {
⋮----
Self::Pinned(iter) => iter.next().map(PacketRef::Packet),
Self::Bytes(iter) => iter.next().map(PacketRef::Bytes),
⋮----
pub enum PacketBatchIterMut<'a> {
⋮----
impl DoubleEndedIterator for PacketBatchIterMut<'_> {
⋮----
Self::Pinned(iter) => iter.next_back().map(PacketRefMut::Packet),
Self::Bytes(iter) => iter.next_back().map(PacketRefMut::Bytes),
⋮----
impl<'a> Iterator for PacketBatchIterMut<'a> {
⋮----
Self::Pinned(iter) => iter.next().map(PacketRefMut::Packet),
Self::Bytes(iter) => iter.next().map(PacketRefMut::Bytes),
⋮----
type PacketParIter<'a> = rayon::slice::Iter<'a, Packet>;
type BytesPacketParIter<'a> = rayon::slice::Iter<'a, BytesPacket>;
pub enum PacketBatchParIter<'a> {
⋮----
impl<'a> ParallelIterator for PacketBatchParIter<'a> {
⋮----
fn drive_unindexed<C>(self, consumer: C) -> C::Result
⋮----
Self::Pinned(iter) => iter.drive_unindexed(consumer),
Self::Bytes(iter) => iter.drive_unindexed(consumer),
⋮----
impl IndexedParallelIterator for PacketBatchParIter<'_> {
fn len(&self) -> usize {
⋮----
Self::Pinned(iter) => iter.len(),
Self::Bytes(iter) => iter.len(),
⋮----
fn drive<C: rayon::iter::plumbing::Consumer<Self::Item>>(self, consumer: C) -> C::Result {
⋮----
Self::Pinned(iter) => iter.drive(consumer),
Self::Bytes(iter) => iter.drive(consumer),
⋮----
fn with_producer<CB: rayon::iter::plumbing::ProducerCallback<Self::Item>>(
⋮----
Self::Pinned(iter) => iter.with_producer(callback),
Self::Bytes(iter) => iter.with_producer(callback),
⋮----
type PacketParIterMut<'a> = rayon::slice::IterMut<'a, Packet>;
type BytesPacketParIterMut<'a> = rayon::slice::IterMut<'a, BytesPacket>;
pub enum PacketBatchParIterMut<'a> {
⋮----
impl<'a> ParallelIterator for PacketBatchParIterMut<'a> {
⋮----
impl IndexedParallelIterator for PacketBatchParIterMut<'_> {
⋮----
pub struct RecycledPacketBatch {
⋮----
pub type PacketBatchRecycler = Recycler<RecycledVec<Packet>>;
impl RecycledPacketBatch {
pub fn new(packets: Vec<Packet>) -> Self {
⋮----
pub fn with_capacity(capacity: usize) -> Self {
⋮----
pub fn new_with_recycler(
⋮----
let mut packets = recycler.allocate(name);
packets.preallocate(capacity);
⋮----
pub fn new_with_recycler_data(
⋮----
let mut batch = Self::new_with_recycler(recycler, packets.len(), name);
batch.packets.append(&mut packets);
⋮----
pub fn new_with_recycler_data_and_dests<S, T>(
⋮----
let dests_and_data = dests_and_data.into_iter();
let mut batch = Self::new_with_recycler(recycler, dests_and_data.len(), name);
⋮----
.resize(dests_and_data.len(), Packet::default());
for ((addr, data), packet) in dests_and_data.zip(batch.packets.iter_mut()) {
let addr = addr.borrow();
if !addr.ip().is_unspecified() && addr.port() != 0 {
if let Err(e) = Packet::populate_packet(packet, Some(addr), &data) {
// TODO: This should never happen. Instead the caller should
// break the payload into smaller messages, and here any errors
// should be propagated.
error!("Couldn't write to packet {e:?}. Data skipped.");
packet.meta_mut().set_discard(true);
⋮----
trace!("Dropping packet, as destination is unknown");
⋮----
pub fn set_addr(&mut self, addr: &SocketAddr) {
for p in self.iter_mut() {
p.meta_mut().set_socket_addr(addr);
⋮----
pub fn push(&mut self, packet: Packet) {
self.packets.push(packet)
⋮----
pub fn truncate(&mut self, len: usize) {
self.packets.truncate(len)
⋮----
pub fn resize(&mut self, packets_per_batch: usize, value: Packet) {
self.packets.resize(packets_per_batch, value)
⋮----
pub fn capacity(&self) -> usize {
self.packets.capacity()
⋮----
impl Deref for RecycledPacketBatch {
type Target = [Packet];
fn deref(&self) -> &Self::Target {
⋮----
impl DerefMut for RecycledPacketBatch {
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
type Output = I::Output;
⋮----
fn index(&self, index: I) -> &Self::Output {
⋮----
fn index_mut(&mut self, index: I) -> &mut Self::Output {
⋮----
impl<'a> IntoIterator for &'a RecycledPacketBatch {
type Item = &'a Packet;
type IntoIter = Iter<'a, Packet>;
⋮----
self.packets.iter()
⋮----
impl<'a> IntoParallelIterator for &'a RecycledPacketBatch {
type Iter = rayon::slice::Iter<'a, Packet>;
⋮----
self.packets.par_iter()
⋮----
impl<'a> IntoParallelIterator for &'a mut RecycledPacketBatch {
type Iter = rayon::slice::IterMut<'a, Packet>;
type Item = &'a mut Packet;
⋮----
self.packets.par_iter_mut()
⋮----
batch.packets.into()
⋮----
pub fn to_packet_batches<T: Serialize>(items: &[T], chunk_size: usize) -> Vec<PacketBatch> {
⋮----
.chunks(chunk_size)
.map(|batch_items| {
let mut batch = RecycledPacketBatch::with_capacity(batch_items.len());
batch.packets.resize(batch_items.len(), Packet::default());
for (item, packet) in batch_items.iter().zip(batch.packets.iter_mut()) {
Packet::populate_packet(packet, None, item).expect("serialize request");
⋮----
batch.into()
⋮----
.collect()
⋮----
fn to_packet_batches_for_tests<T: Serialize>(items: &[T]) -> Vec<PacketBatch> {
to_packet_batches(items, NUM_PACKETS)
⋮----
pub struct BytesPacketBatch {
⋮----
impl BytesPacketBatch {
pub fn new() -> Self {
⋮----
impl Deref for BytesPacketBatch {
type Target = Vec<BytesPacket>;
⋮----
impl DerefMut for BytesPacketBatch {
⋮----
fn from(packets: Vec<BytesPacket>) -> Self {
⋮----
fn from_iter<T: IntoIterator<Item = BytesPacket>>(iter: T) -> Self {
⋮----
impl<'a> IntoIterator for &'a BytesPacketBatch {
type Item = &'a BytesPacket;
type IntoIter = Iter<'a, BytesPacket>;
⋮----
impl<'a> IntoParallelIterator for &'a BytesPacketBatch {
type Iter = rayon::slice::Iter<'a, BytesPacket>;
⋮----
impl<'a> IntoParallelIterator for &'a mut BytesPacketBatch {
type Iter = rayon::slice::IterMut<'a, BytesPacket>;
type Item = &'a mut BytesPacket;
⋮----
pub fn deserialize_from_with_limit<R, T>(reader: R) -> bincode::Result<T>
⋮----
.with_limit(PACKET_DATA_SIZE as u64)
⋮----
.allow_trailing_bytes()
.deserialize_from(reader)
⋮----
mod tests {
⋮----
fn test_to_packet_batches() {
⋮----
let tx = transfer(&keypair, &keypair.pubkey(), 1, hash);
let rv = to_packet_batches_for_tests(&[tx.clone(); 1]);
assert_eq!(rv.len(), 1);
assert_eq!(rv[0].len(), 1);
⋮----
let rv = to_packet_batches_for_tests(&vec![tx.clone(); NUM_PACKETS]);
⋮----
assert_eq!(rv[0].len(), NUM_PACKETS);
⋮----
let rv = to_packet_batches_for_tests(&vec![tx; NUM_PACKETS + 1]);
assert_eq!(rv.len(), 2);
⋮----
assert_eq!(rv[1].len(), 1);
⋮----
fn test_to_packets_pinning() {

================
File: perf/src/perf_libs.rs
================
pub fn locate_perf_libs() -> Option<PathBuf> {
let exe = env::current_exe().expect("Unable to get executable path");
let perf_libs = exe.parent().unwrap().join("perf-libs");
if perf_libs.is_dir() {
info!("perf-libs found at {perf_libs:?}");
return Some(perf_libs);
⋮----
warn!("{perf_libs:?} does not exist");
⋮----
pub fn append_to_ld_library_path(mut ld_library_path: String) {
⋮----
ld_library_path.push(':');
ld_library_path.push_str(&env_value);
⋮----
info!("setting ld_library_path to: {ld_library_path:?}");

================
File: perf/src/recycled_vec.rs
================
pub struct RecycledVec<T: Default + Clone + Sized> {
⋮----
impl<T: Default + Clone + Sized> Reset for RecycledVec<T> {
fn reset(&mut self) {
self.x.clear();
⋮----
fn warm(&mut self, size_hint: usize) {
self.resize(size_hint, T::default());
⋮----
fn set_recycler(&mut self, recycler: Weak<RecyclerX<Self>>) {
⋮----
fn from(mut recycled_vec: RecycledVec<T>) -> Self {
⋮----
impl<'a, T: Clone + Default + Sized> IntoIterator for &'a RecycledVec<T> {
type Item = &'a T;
type IntoIter = Iter<'a, T>;
fn into_iter(self) -> Self::IntoIter {
self.x.iter()
⋮----
type Output = I::Output;
⋮----
fn index(&self, index: I) -> &Self::Output {
⋮----
fn index_mut(&mut self, index: I) -> &mut Self::Output {
⋮----
impl<'a, T: Clone + Send + Sync + Default + Sized> IntoParallelIterator for &'a RecycledVec<T> {
type Iter = rayon::slice::Iter<'a, T>;
⋮----
fn into_par_iter(self) -> Self::Iter {
self.x.par_iter()
⋮----
impl<'a, T: Clone + Send + Sync + Default + Sized> IntoParallelIterator for &'a mut RecycledVec<T> {
type Iter = rayon::slice::IterMut<'a, T>;
type Item = &'a mut T;
⋮----
self.x.par_iter_mut()
⋮----
pub fn preallocate(&mut self, size: usize) {
let capacity_to_reserve = size.saturating_sub(self.x.capacity());
self.x.reserve(capacity_to_reserve);
⋮----
pub fn from_vec(source: Vec<T>) -> Self {
⋮----
pub fn with_capacity(capacity: usize) -> Self {
⋮----
pub fn push(&mut self, x: T) {
self.x.push(x);
⋮----
pub fn resize(&mut self, size: usize, elem: T) {
self.x.resize(size, elem);
⋮----
pub fn append(&mut self, other: &mut Vec<T>) {
self.x.append(other);
⋮----
pub fn shuffle<R: Rng>(&mut self, rng: &mut R) {
self.x.shuffle(rng)
⋮----
pub fn truncate(&mut self, len: usize) {
self.x.truncate(len);
⋮----
pub(crate) fn capacity(&self) -> usize {
self.x.capacity()
⋮----
impl<T: Clone + Default + Sized> Clone for RecycledVec<T> {
fn clone(&self) -> Self {
let x = self.x.clone();
debug!("clone PreallocatedVec: size: {}", self.x.capacity());
⋮----
recycler: self.recycler.clone(),
⋮----
impl<T: Sized + Default + Clone> Deref for RecycledVec<T> {
type Target = [T];
fn deref(&self) -> &Self::Target {
⋮----
impl<T: Sized + Default + Clone> DerefMut for RecycledVec<T> {
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
impl<T: Sized + Default + Clone> Drop for RecycledVec<T> {
fn drop(&mut self) {
if let Some(recycler) = self.recycler.upgrade() {
recycler.recycle(std::mem::take(self));
⋮----
impl<T: Sized + Default + Clone + PartialEq> PartialEq for RecycledVec<T> {
fn eq(&self, other: &Self) -> bool {
self.x.eq(&other.x)
⋮----
impl<T: Sized + Default + Clone + PartialEq + Eq> Eq for RecycledVec<T> {}
⋮----
mod tests {
⋮----
fn test_recycled_vec() {
⋮----
mem.push(50);
mem.resize(2, 10);
assert_eq!(mem[0], 50);
assert_eq!(mem[1], 10);
assert_eq!(mem.len(), 2);
assert!(!mem.is_empty());
let mut iter = mem.iter();
assert_eq!(*iter.next().unwrap(), 50);
assert_eq!(*iter.next().unwrap(), 10);
assert_eq!(iter.next(), None);

================
File: perf/src/recycler_cache.rs
================
pub struct RecyclerCache {
⋮----
impl RecyclerCache {
pub fn warmed() -> Self {
⋮----
pub fn offsets(&self) -> &Recycler<TxOffset> {
⋮----
pub fn buffer(&self) -> &Recycler<RecycledVec<u8>> {

================
File: perf/src/recycler.rs
================
struct RecyclerStats {
⋮----
pub struct Recycler<T> {
⋮----
pub struct RecyclerX<T> {
⋮----
impl<T: Default> Default for RecyclerX<T> {
fn default() -> RecyclerX<T> {
let id = thread_rng().gen_range(0..1000);
trace!("new recycler..{id}");
⋮----
fn example() -> Self {
⋮----
pub trait Reset {
⋮----
pub fn enable_recycler_warming() {
WARM_RECYCLERS.store(true, Ordering::Relaxed);
⋮----
fn warm_recyclers() -> bool {
WARM_RECYCLERS.load(Ordering::Relaxed)
⋮----
pub fn warmed(num: usize, size_hint: usize) -> Self {
⋮----
if warm_recyclers() {
⋮----
.map(|_| {
let mut item = new.allocate("warming");
item.warm(size_hint);
⋮----
.collect();
⋮----
.into_iter()
.for_each(|i| new.recycler.recycle(i));
⋮----
pub fn allocate(&self, name: &'static str) -> T {
⋮----
let mut gc = self.recycler.gc.lock().unwrap();
self.recycler.size_factor.store(
⋮----
.load(Ordering::Acquire)
.saturating_mul(RECYCLER_SHRINK_WINDOW_SUB_ONE)
.saturating_add(RECYCLER_SHRINK_WINDOW_HALF)
.checked_div(RECYCLER_SHRINK_WINDOW)
.unwrap()
.saturating_add(gc.len()),
⋮----
if let Some(mut x) = gc.pop() {
self.recycler.stats.reuse.fetch_add(1, Ordering::Relaxed);
x.reset();
⋮----
let total = self.recycler.stats.total.fetch_add(1, Ordering::Relaxed);
trace!(
⋮----
t.set_recycler(Arc::downgrade(&self.recycler));
⋮----
pub fn recycle(&self, x: T) {
⋮----
let mut gc = self.gc.lock().expect("recycler lock in pub fn recycle");
gc.push(x);
⋮----
if gc.len() > RECYCLER_SHRINK_SIZE
&& self.size_factor.load(Ordering::Acquire) >= SIZE_FACTOR_AFTER_SHRINK
⋮----
self.stats.freed.fetch_add(
gc.len().saturating_sub(RECYCLER_SHRINK_SIZE),
⋮----
for mut x in gc.drain(RECYCLER_SHRINK_SIZE..) {
x.set_recycler(Weak::default());
⋮----
.store(SIZE_FACTOR_AFTER_SHRINK, Ordering::Release);
⋮----
gc.len()
⋮----
let max_gc = self.stats.max_gc.load(Ordering::Relaxed);
⋮----
let _ = self.stats.max_gc.compare_exchange(
⋮----
let total = self.stats.total.load(Ordering::Relaxed);
let reuse = self.stats.reuse.load(Ordering::Relaxed);
let freed = self.stats.freed.load(Ordering::Relaxed);
datapoint_debug!(
⋮----
mod tests {
⋮----
impl Reset for u64 {
fn reset(&mut self) {
⋮----
fn warm(&mut self, _size_hint: usize) {}
fn set_recycler(&mut self, _recycler: Weak<RecyclerX<Self>>) {}
⋮----
fn test_recycler() {
⋮----
let mut y: u64 = recycler.allocate("test_recycler1");
assert_eq!(y, 0);
⋮----
let recycler2 = recycler.clone();
recycler2.recycler.recycle(y);
assert_eq!(recycler.recycler.gc.lock().unwrap().len(), 1);
let z = recycler.allocate("test_recycler2");
assert_eq!(z, 10);
assert_eq!(recycler.recycler.gc.lock().unwrap().len(), 0);
⋮----
fn test_recycler_shrink() {
⋮----
let _packets: Vec<_> = repeat_with(|| recycler.allocate(""))
.take(NUM_PACKETS)
⋮----
assert_eq!(recycler.recycler.gc.lock().unwrap().len(), NUM_PACKETS);
// Process a normal load of packets for a while.
⋮----
let count = rng.gen_range(1..128);
let _packets: Vec<_> = repeat_with(|| recycler.allocate("")).take(count).collect();
⋮----
assert_eq!(

================
File: perf/src/sigverify.rs
================
.num_threads(get_thread_count())
.thread_name(|i| format!("solSigVerify{i:02}"))
.build()
.unwrap()
⋮----
pub type TxOffset = RecycledVec<u32>;
type TxOffsets = (TxOffset, TxOffset, TxOffset, TxOffset, Vec<Vec<u32>>);
⋮----
struct PacketOffsets {
⋮----
impl PacketOffsets {
pub fn new(
⋮----
pub enum PacketError {
⋮----
fn from(_e: std::boxed::Box<bincode::ErrorKind>) -> PacketError {
⋮----
fn from(_e: std::num::TryFromIntError) -> Self {
⋮----
pub fn verify_packet(packet: &mut PacketRefMut, reject_non_vote: bool) -> bool {
if packet.meta().discard() {
⋮----
let packet_offsets = get_packet_offsets(packet, 0, reject_non_vote);
⋮----
if packet.meta().size <= msg_start {
⋮----
let pubkey_end = pubkey_start.saturating_add(size_of::<Pubkey>());
let Some(sig_end) = sig_start.checked_add(size_of::<Signature>()) else {
⋮----
let Some(Ok(signature)) = packet.data(sig_start..sig_end).map(Signature::try_from) else {
⋮----
let Some(pubkey) = packet.data(pubkey_start..pubkey_end) else {
⋮----
let Some(message) = packet.data(msg_start..) else {
⋮----
if !signature.verify(pubkey, message) {
⋮----
pub fn count_packets_in_batches(batches: &[PacketBatch]) -> usize {
batches.iter().map(|batch| batch.len()).sum()
⋮----
pub fn count_valid_packets<'a>(batches: impl IntoIterator<Item = &'a PacketBatch>) -> usize {
⋮----
.into_iter()
.map(|batch| batch.into_iter().filter(|p| !p.meta().discard()).count())
.sum()
⋮----
pub fn count_discarded_packets(batches: &[PacketBatch]) -> usize {
⋮----
.iter()
.map(|batch| batch.iter().filter(|p| p.meta().discard()).count())
⋮----
fn do_get_packet_offsets(
⋮----
.checked_add(size_of::<Signature>())
.filter(|v| *v <= packet.meta().size)
.ok_or(PacketError::InvalidLen)?;
⋮----
.data(..)
.and_then(|bytes| decode_shortu16_len(bytes).ok())
.ok_or(PacketError::InvalidShortVec)?;
⋮----
.checked_mul(size_of::<Signature>())
.and_then(|v| v.checked_add(sig_size))
⋮----
if msg_start_offset >= packet.meta().size {
return Err(PacketError::InvalidSignatureLen);
⋮----
.data(msg_start_offset)
.ok_or(PacketError::InvalidSignatureLen)?;
⋮----
.checked_add(1)
.ok_or(PacketError::InvalidLen)?
⋮----
_ => return Err(PacketError::UnsupportedVersion),
⋮----
.checked_add(MESSAGE_HEADER_LENGTH)
⋮----
.data(msg_header_offset)
⋮----
.data(readonly_signer_offset)
.ok_or(PacketError::InvalidSignatureLen)?
⋮----
return Err(PacketError::PayerNotWritable);
⋮----
return Err(PacketError::MismatchSignatureLen);
⋮----
.data(message_account_keys_len_offset..)
⋮----
.checked_add(pubkey_len_size)
.ok_or(PacketError::InvalidPubkeyLen)?;
⋮----
.checked_mul(size_of::<Pubkey>())
.and_then(|v| v.checked_add(pubkey_start))
⋮----
return Err(PacketError::InvalidPubkeyLen);
⋮----
.checked_add(size_of::<Hash>())
⋮----
.checked_add(1usize)
⋮----
.data(instructions_len_offset..)
⋮----
return Err(PacketError::InvalidNumberOfInstructions);
⋮----
.checked_add(instruction_len_size)
⋮----
.checked_add(sig_size)
⋮----
.checked_add(msg_start_offset)
⋮----
.checked_add(pubkey_start)
⋮----
.checked_add(instruction_start)
⋮----
Ok(PacketOffsets::new(
⋮----
fn get_packet_offsets(
⋮----
let unsanitized_packet_offsets = do_get_packet_offsets(packet.as_ref(), current_offset);
⋮----
check_for_simple_vote_transaction(packet, &offsets, current_offset).ok();
if !reject_non_vote || packet.meta().is_simple_vote_tx() {
⋮----
fn check_for_simple_vote_transaction(
⋮----
.checked_sub(current_offset)
⋮----
let message_prefix = *packet.data(msg_start).ok_or(PacketError::InvalidLen)?;
⋮----
return Ok(());
⋮----
.data(instruction_start)
.ok_or(PacketError::InvalidLen)?,
⋮----
return Err(PacketError::InvalidProgramIdIndex);
⋮----
.checked_add(size_of::<Pubkey>())
⋮----
.data(instruction_program_id_start..instruction_program_id_end)
⋮----
== solana_sdk_ids::vote::id().as_ref()
⋮----
packet.meta_mut().flags |= PacketFlags::SIMPLE_VOTE_TX;
⋮----
Ok(())
⋮----
pub fn generate_offsets(
⋮----
debug!("allocating..");
let mut signature_offsets: RecycledVec<_> = recycler.allocate("sig_offsets");
let mut pubkey_offsets: RecycledVec<_> = recycler.allocate("pubkey_offsets");
let mut msg_start_offsets: RecycledVec<_> = recycler.allocate("msg_start_offsets");
let mut msg_sizes: RecycledVec<_> = recycler.allocate("msg_size_offsets");
⋮----
.iter_mut()
.map(|batch| {
⋮----
.map(|mut packet| {
⋮----
get_packet_offsets(&mut packet, current_offset, reject_non_vote);
trace!("pubkey_offset: {}", packet_offsets.pubkey_start);
⋮----
let msg_size = current_offset.saturating_add(packet.meta().size) as u32;
⋮----
signature_offsets.push(sig_offset);
sig_offset = sig_offset.saturating_add(size_of::<Signature>() as u32);
pubkey_offsets.push(pubkey_offset);
pubkey_offset = pubkey_offset.saturating_add(size_of::<Pubkey>() as u32);
msg_start_offsets.push(packet_offsets.msg_start);
let msg_size = msg_size.saturating_sub(packet_offsets.msg_start);
msg_sizes.push(msg_size);
⋮----
current_offset = current_offset.saturating_add(size_of::<Packet>());
⋮----
.collect()
⋮----
.collect();
⋮----
fn split_batches(batches: Vec<PacketBatch>) -> (Vec<BytesPacketBatch>, Vec<RecycledPacketBatch>) {
⋮----
PacketBatch::Bytes(batch) => bytes_batches.push(batch),
PacketBatch::Pinned(batch) => pinned_batches.push(batch),
⋮----
batch.push(packet);
bytes_batches.push(batch);
⋮----
macro_rules! shrink_batches_fn {
⋮----
shrink_batches_fn!(shrink_bytes_batches, BytesPacketBatch);
shrink_batches_fn!(shrink_pinned_batches, RecycledPacketBatch);
pub fn shrink_batches(batches: Vec<PacketBatch>) -> Vec<PacketBatch> {
let (mut bytes_batches, mut pinned_batches) = split_batches(batches);
shrink_bytes_batches(&mut bytes_batches);
shrink_pinned_batches(&mut pinned_batches);
⋮----
.map(PacketBatch::Bytes)
.chain(pinned_batches.into_iter().map(PacketBatch::Pinned))
⋮----
pub fn ed25519_verify(batches: &mut [PacketBatch], reject_non_vote: bool, packet_count: usize) {
debug!("CPU ECDSA for {packet_count}");
PAR_THREAD_POOL.install(|| {
batches.par_iter_mut().flatten().for_each(|mut packet| {
if !packet.meta().discard() && !verify_packet(&mut packet, reject_non_vote) {
packet.meta_mut().set_discard(true);
⋮----
pub fn ed25519_verify_disabled(batches: &mut [PacketBatch]) {
let packet_count = count_packets_in_batches(batches);
debug!("disabled ECDSA for {packet_count}");
⋮----
packet.meta_mut().set_discard(false);
⋮----
pub fn copy_return_values<I, T>(sig_lens: I, out: &RecycledVec<u8>, rvs: &mut [Vec<u8>])
⋮----
debug_assert!(rvs.iter().flatten().all(|&rv| rv == 0u8));
⋮----
let rvs = rvs.iter_mut().flatten();
for (k, rv) in sig_lens.into_iter().flatten().zip(rvs) {
let out = out[offset..].iter().take(k as usize).all(|&x| x == 1u8);
⋮----
offset = offset.saturating_add(k as usize);
⋮----
pub fn mark_disabled(batches: &mut [PacketBatch], r: &[Vec<u8>]) {
for (batch, v) in batches.iter_mut().zip(r) {
for (mut pkt, f) in batch.iter_mut().zip(v) {
if !pkt.meta().discard() {
pkt.meta_mut().set_discard(*f == 0);
⋮----
mod tests {
⋮----
pub fn memfind<A: Eq>(a: &[A], b: &[A]) -> Option<usize> {
assert!(a.len() >= b.len());
let end = a.len() - b.len() + 1;
(0..end).find(|&i| a[i..i + b.len()] == b[..])
⋮----
fn test_copy_return_values() {
⋮----
let size = rng.gen_range(0..64);
repeat_with(|| {
let size = rng.gen_range(0..16);
repeat_with(|| rng.gen_range(0..5)).take(size).collect()
⋮----
.take(size)
⋮----
.map(|sig_lens| {
⋮----
.map(|&size| repeat_with(|| rng.gen()).take(size as usize).collect())
⋮----
.map(|out| {
out.iter()
.map(|out| u8::from(!out.is_empty() && out.iter().all(|&k| k)))
⋮----
out.into_iter().flatten().flatten().map(u8::from).collect(),
⋮----
.map(|sig_lens| vec![0u8; sig_lens.len()])
⋮----
copy_return_values(sig_lens, &out, &mut rvs);
assert_eq!(rvs, expected);
⋮----
fn test_mark_disabled() {
⋮----
batch.resize(batch_size, BytesPacket::empty());
let mut batches: Vec<PacketBatch> = vec![batch.into()];
mark_disabled(&mut batches, &[vec![0]]);
assert!(batches[0].get(0).unwrap().meta().discard());
batches[0].get_mut(0).unwrap().meta_mut().set_discard(false);
mark_disabled(&mut batches, &[vec![1]]);
assert!(!batches[0].get(0).unwrap().meta().discard());
⋮----
fn test_layout() {
let tx = test_tx();
let tx_bytes = serialize(&tx).unwrap();
let packet = serialize(&tx).unwrap();
assert_matches!(memfind(&packet, &tx_bytes), Some(0));
assert_matches!(memfind(&packet, &[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), None);
⋮----
fn test_system_transaction_layout() {
⋮----
let message_data = tx.message_data();
let mut packet = BytesPacket::from_data(None, tx.clone()).unwrap();
let packet_offsets = sigverify::get_packet_offsets(&mut packet.as_mut(), 0, false);
assert_eq!(
⋮----
assert_eq!(packet_offsets.sig_len, 1);
⋮----
fn packet_from_num_sigs(required_num_sigs: u8, actual_num_sigs: usize) -> BytesPacket {
⋮----
account_keys: vec![],
⋮----
instructions: vec![],
⋮----
tx.signatures = vec![Signature::default(); actual_num_sigs];
BytesPacket::from_data(None, tx).unwrap()
⋮----
fn test_untrustworthy_sigs() {
⋮----
let packet = packet_from_num_sigs(required_num_sigs, actual_num_sigs);
let unsanitized_packet_offsets = sigverify::do_get_packet_offsets(packet.as_ref(), 0);
⋮----
fn test_small_packet() {
⋮----
let mut data = bincode::serialize(&tx).unwrap();
⋮----
data.truncate(2);
⋮----
let res = sigverify::do_get_packet_offsets(packet.as_ref(), 0);
assert_eq!(res, Err(PacketError::InvalidLen));
⋮----
fn test_pubkey_too_small() {
⋮----
let mut tx = test_tx();
⋮----
tx.signatures = vec![sig; NUM_SIG];
tx.message.account_keys = vec![];
⋮----
let mut packet = BytesPacket::from_data(None, tx).unwrap();
⋮----
assert_eq!(res, Err(PacketError::InvalidPubkeyLen));
assert!(!verify_packet(&mut packet.as_mut(), false));
⋮----
let mut batches = generate_packet_batches(&packet, 1, 1);
ed25519_verify(&mut batches);
⋮----
fn test_pubkey_len() {
⋮----
let pubkey1 = keypair1.pubkey();
let mut message = Message::new(&[], Some(&pubkey1));
message.account_keys.push(pubkey1);
⋮----
message.recent_blockhash = Hash::new_from_array(pubkey1.to_bytes());
⋮----
info!("message: {:?}", tx.message_data());
info!("tx: {tx:?}");
let sig = keypair1.try_sign_message(&tx.message_data()).unwrap();
⋮----
fn test_large_sig_len() {
⋮----
assert_eq!(res, Err(PacketError::InvalidSignatureLen));
⋮----
fn test_really_large_sig_len() {
⋮----
assert_eq!(res, Err(PacketError::InvalidShortVec));
⋮----
fn test_invalid_pubkey_len() {
⋮----
let packet = BytesPacket::from_bytes(None, Bytes::from(data.clone()));
let offsets = sigverify::do_get_packet_offsets(packet.as_ref(), 0).unwrap();
⋮----
fn test_fee_payer_is_debitable() {
⋮----
tx.signatures = vec![Signature::default()];
let packet = BytesPacket::from_data(None, tx).unwrap();
⋮----
assert_eq!(res, Err(PacketError::PayerNotWritable));
⋮----
fn test_unsupported_version() {
⋮----
data[res.unwrap().msg_start as usize] = MESSAGE_VERSION_PREFIX + 1;
⋮----
assert_eq!(res, Err(PacketError::UnsupportedVersion));
⋮----
fn test_versioned_message() {
⋮----
let mut legacy_offsets = sigverify::do_get_packet_offsets(packet.as_ref(), 0).unwrap();
⋮----
let msg_bytes = packet.data(msg_start..).unwrap();
let mut buf = BytesMut::with_capacity(packet.meta().size + 1);
buf.put_slice(packet.data(..msg_start).unwrap());
buf.put_u8(MESSAGE_VERSION_PREFIX);
buf.put_slice(msg_bytes);
let packet = BytesPacket::from_bytes(None, buf.freeze());
⋮----
assert_eq!(expected_offsets, offsets);
⋮----
fn test_system_transaction_data_layout() {
let mut tx0 = test_tx();
tx0.message.instructions[0].data = vec![1, 2, 3];
let message0a = tx0.message_data();
let tx_bytes = serialize(&tx0).unwrap();
assert!(tx_bytes.len() <= PACKET_DATA_SIZE);
⋮----
let tx1 = deserialize(&tx_bytes).unwrap();
assert_eq!(tx0, tx1);
assert_eq!(tx1.message().instructions[0].data, vec![1, 2, 3]);
tx0.message.instructions[0].data = vec![1, 2, 4];
let message0b = tx0.message_data();
assert_ne!(message0a, message0b);
⋮----
fn get_packet_offsets_from_tx(tx: Transaction, current_offset: u32) -> PacketOffsets {
⋮----
sigverify::get_packet_offsets(&mut packet.as_mut(), current_offset as usize, false);
⋮----
fn test_get_packet_offsets() {
⋮----
fn generate_bytes_packet_batches(
⋮----
.map(|_| {
⋮----
packet_batch.push(packet.clone());
⋮----
assert_eq!(packet_batch.len(), num_packets_per_batch);
⋮----
assert_eq!(batches.len(), num_batches);
⋮----
fn generate_packet_batches(
⋮----
packet_batch.into()
⋮----
fn test_verify_n(n: usize, modify_data: bool) {
⋮----
data[20] = data[20].wrapping_add(10);
⋮----
let mut batches = generate_packet_batches(&packet, n, 2);
⋮----
assert!(batches
⋮----
fn ed25519_verify(batches: &mut [PacketBatch]) {
⋮----
fn test_verify_tampered_sig_len() {
⋮----
tx.signatures.pop();
⋮----
fn test_verify_zero() {
test_verify_n(0, false);
⋮----
fn test_verify_one() {
test_verify_n(1, false);
⋮----
fn test_verify_seventy_one() {
test_verify_n(71, false);
⋮----
fn test_verify_medium_pass() {
test_verify_n(VERIFY_PACKET_CHUNK_SIZE, false);
⋮----
fn test_verify_large_pass() {
test_verify_n(VERIFY_PACKET_CHUNK_SIZE * get_thread_count(), false);
⋮----
fn test_verify_medium_fail() {
test_verify_n(VERIFY_PACKET_CHUNK_SIZE, true);
⋮----
fn test_verify_large_fail() {
test_verify_n(VERIFY_PACKET_CHUNK_SIZE * get_thread_count(), true);
⋮----
fn test_verify_multisig() {
⋮----
let tx = test_multisig_tx();
⋮----
let mut batches = generate_bytes_packet_batches(&packet, n, num_batches);
data[40] = data[40].wrapping_add(8);
⋮----
batches[0].push(packet);
let mut batches: Vec<PacketBatch> = batches.into_iter().map(PacketBatch::from).collect();
⋮----
let mut ref_vec = vec![vec![ref_ans; n]; num_batches];
ref_vec[0].push(0u8);
⋮----
fn test_verify_fail() {
test_verify_n(5, true);
⋮----
fn test_is_simple_vote_transaction() {
⋮----
tx.message.instructions[0].data = vec![1, 2, 3];
⋮----
let packet_offsets = do_get_packet_offsets(packet.as_ref(), 0).unwrap();
check_for_simple_vote_transaction(&mut packet.as_mut(), &packet_offsets, 0).ok();
assert!(!packet.meta().is_simple_vote_tx());
⋮----
let mut tx = new_test_vote_tx(&mut rng);
⋮----
assert!(packet.meta().is_simple_vote_tx());
⋮----
let mut packet_offsets = do_get_packet_offsets(packet.as_ref(), 0).unwrap();
⋮----
let mut packet = BytesPacket::from_bytes(None, buf.freeze());
packet_offsets = do_get_packet_offsets(packet.as_ref(), 0).unwrap();
⋮----
vec![solana_vote_program::id(), Pubkey::new_unique()],
vec![
⋮----
tx.signatures.push(Signature::default());
⋮----
fn test_is_simple_vote_transaction_with_offsets() {
⋮----
batch.push(BytesPacket::from_data(None, test_tx()).unwrap());
let tx = new_test_vote_tx(&mut rng);
batch.push(BytesPacket::from_data(None, tx).unwrap());
batch.iter_mut().enumerate().for_each(|(index, packet)| {
⋮----
do_get_packet_offsets(packet.as_ref(), current_offset).unwrap();
check_for_simple_vote_transaction(
&mut packet.as_mut(),
⋮----
.ok();
⋮----
batch.iter_mut().for_each(|packet| {
⋮----
fn test_shrink_fuzz() {
⋮----
if rng.gen_bool(0.5) {
⋮----
BytesPacket::from_data(None, test_tx()).expect("serialize request")
⋮----
.map(|_| Packet::from_data(None, test_tx()).expect("serialize request"))
⋮----
batches.iter_mut().for_each(|b| {
b.iter_mut()
.for_each(|mut p| p.meta_mut().set_discard(thread_rng().gen()))
⋮----
let mut start = vec![];
⋮----
.filter(|p| !p.meta().discard())
.for_each(|p| start.push(p.data(..).unwrap().to_vec()))
⋮----
start.sort();
let packet_count = count_valid_packets(&batches);
let mut batches = shrink_batches(batches);
let mut end = vec![];
⋮----
.for_each(|p| end.push(p.data(..).unwrap().to_vec()))
⋮----
end.sort();
let packet_count2 = count_valid_packets(&batches);
assert_eq!(packet_count, packet_count2);
assert_eq!(start, end);
⋮----
fn test_shrink_empty() {
⋮----
shrink_batches(Vec::new());
⋮----
let batches = vec![RecycledPacketBatch::with_capacity(0).into()];
let batches = shrink_batches(batches);
assert_eq!(batches.len(), 0);
⋮----
.map(|_| RecycledPacketBatch::with_capacity(0).into())
⋮----
fn test_shrink_vectors() {
⋮----
|b: usize, p: usize| ((b * PACKETS_PER_BATCH) + p).is_multiple_of(2),
|b: usize, p: usize| !((b * PACKETS_PER_BATCH) + p).is_multiple_of(2),
⋮----
let test_cases = set_discards.iter().zip(&expect_valids).enumerate();
⋮----
debug!("test_shrink case: {i}");
let mut batches = to_packet_batches(
&(0..PACKET_COUNT).map(|_| test_tx()).collect::<Vec<_>>(),
⋮----
assert_eq!(batches.len(), BATCH_COUNT);
assert_eq!(count_valid_packets(&batches), PACKET_COUNT);
batches.iter_mut().enumerate().for_each(|(i, b)| {
⋮----
.enumerate()
.for_each(|(j, mut p)| p.meta_mut().set_discard(set_discard(i, j)))
⋮----
assert_eq!(count_valid_packets(&batches), *expect_valid_packets);
debug!("show valid packets for case {i}");
⋮----
b.iter_mut().enumerate().for_each(|(j, p)| {
if !p.meta().discard() {
trace!("{i} {j}")
⋮----
debug!("done show valid packets for case {i}");
⋮----
let shrunken_batch_count = batches.len();
debug!("shrunk batch test {i} count: {shrunken_batch_count}");
assert_eq!(shrunken_batch_count, *expect_batch_count);
⋮----
fn test_split_batches() {
⋮----
let batches = vec![];
let (bytes_batches, pinned_batches) = split_batches(batches);
assert!(bytes_batches.is_empty());
assert!(pinned_batches.is_empty());
let pinned_packet = Packet::from_data(None, tx.clone()).unwrap();
let bytes_packet = BytesPacket::from_data(None, tx).unwrap();
let batches = vec![
⋮----
fn test_number_of_instructions(too_many_ixs: bool, is_versioned_tx: bool) {
⋮----
let tx: VersionedTransaction = new_test_tx_with_number_of_ixs(number_of_ixs);
BytesPacket::from_data(None, tx.clone()).unwrap()
⋮----
let tx: Transaction = new_test_tx_with_number_of_ixs(number_of_ixs);
⋮----
assert!(do_get_packet_offsets(packet.as_ref(), 0).is_ok());

================
File: perf/src/test_tx.rs
================
pub fn test_tx() -> Transaction {
⋮----
let pubkey1 = keypair1.pubkey();
⋮----
pub fn test_invalid_tx() -> Transaction {
let mut tx = test_tx();
tx.signatures = vec![Transaction::get_invalid_signature()];
⋮----
pub fn test_multisig_tx() -> Transaction {
⋮----
let keypairs = vec![&keypair0, &keypair1];
⋮----
let program_ids = vec![system_program::id(), stake::id()];
let instructions = vec![CompiledInstruction::new(
⋮----
pub fn new_test_vote_tx<R>(rng: &mut R) -> Transaction
⋮----
let mut slots: Vec<Slot> = std::iter::repeat_with(|| rng.gen()).take(5).collect();
slots.sort_unstable();
slots.dedup();
let switch_proof_hash = rng.gen_bool(0.5).then(Hash::new_unique);
⋮----
pub fn new_test_tx_with_number_of_ixs<T>(number_of_ixs: usize) -> T
⋮----
instructions.push(Instruction {
⋮----
accounts: vec![AccountMeta::new(account, false)],
data: vec![i as u8],
⋮----
pub trait FromTestTx: Sized {
⋮----
impl FromTestTx for Transaction {
fn from_test_tx(payer: &Keypair, blockhash: Hash, ixs: Vec<Instruction>) -> Self {
let msg = Message::new(&ixs, Some(&payer.pubkey()));
⋮----
impl FromTestTx for VersionedTransaction {
fn from_test_tx(payer: &Keypair, _blockhash: Hash, ixs: Vec<Instruction>) -> Self {
⋮----
MessageV0::try_compile(&payer.pubkey(), &ixs, &[], Hash::new_unique()).unwrap();
⋮----
VersionedTransaction::try_new(versioned, &[payer]).unwrap()

================
File: perf/src/thread.rs
================
use std::fmt::Display;
⋮----
fn nice(adjustment: i8) -> Result<i8, nix::errno::Errno> {
⋮----
Err(errno)
⋮----
Ok(niceness)
⋮----
.map(|niceness| i8::try_from(niceness).expect("Unexpected niceness value"))
.map_err(nix::errno::Errno::from_raw)
⋮----
pub fn renice_this_thread(adjustment: i8) -> Result<(), String> {
nice(adjustment)
.map(|_| ())
.map_err(|err| format!("Failed to change thread's nice value: {err}"))
⋮----
Ok(())
⋮----
Err(String::from(
⋮----
pub fn is_renice_allowed(adjustment: i8) -> bool {
⋮----
nix::unistd::geteuid().is_root()
⋮----
.map_err(|err| warn!("Failed to get thread's capabilities: {err}"))
.unwrap_or(false)
⋮----
pub fn is_niceness_adjustment_valid<T>(value: T) -> Result<(), String>
⋮----
.as_ref()
⋮----
.map_err(|err| format!("error parsing niceness adjustment value '{value}': {err}"))?;
if is_renice_allowed(adjustment) {
⋮----
mod tests {
⋮----
fn test_nice() {
let niceness = nice(0).unwrap();
let result = std::thread::spawn(|| nice(1)).join().unwrap();
assert_eq!(result, Ok(niceness + 1));
assert_eq!(nice(0), Ok(niceness));
⋮----
nice(1).unwrap();
std::thread::spawn(|| nice(0).unwrap()).join().unwrap()
⋮----
.join()
.unwrap();
assert_eq!(inherited_niceness, niceness + 1);
if !is_renice_allowed(-1) {
let result = std::thread::spawn(|| nice(-1)).join().unwrap();
assert!(result.is_err());
⋮----
fn test_is_niceness_adjustment_valid() {
use super::is_niceness_adjustment_valid;
assert_eq!(is_niceness_adjustment_valid("0"), Ok(()));
assert!(is_niceness_adjustment_valid("128").is_err());
assert!(is_niceness_adjustment_valid("-129").is_err());

================
File: perf/build.rs
================
fn main() {
⋮----
if is_x86_feature_detected!("avx") {
println!("cargo:rustc-cfg=build_target_feature_avx");
⋮----
if is_x86_feature_detected!("avx2") {
println!("cargo:rustc-cfg=build_target_feature_avx2");

================
File: perf/Cargo.toml
================
[package]
name = "solana-perf"
description = "Solana Performance APIs"
documentation = "https://docs.rs/solana-perf"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "solana_perf"

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "dep:solana-clock",
    "dep:solana-keypair",
    "dep:solana-signer",
    "dep:solana-system-interface",
    "dep:solana-system-transaction",
    "dep:solana-transaction",
    "dep:solana-vote-program",
    "dep:solana-vote",
]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-short-vec/frozen-abi",
    "solana-vote-program/frozen-abi",
]

[dependencies]
ahash = { workspace = true }
bincode = { workspace = true }
bv = { workspace = true, features = ["serde"] }
bytes = { workspace = true, features = ["serde"] }
curve25519-dalek = { workspace = true }
dlopen2 = { workspace = true }
fnv = { workspace = true }
log = { workspace = true }
rand = "0.8.5"
rayon = { workspace = true }
serde = { workspace = true }
solana-clock = { workspace = true, optional = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-hash = { workspace = true }
solana-keypair = { workspace = true, optional = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-packet = { workspace = true, features = ["bincode"] }
solana-pubkey = { workspace = true, default-features = false }
solana-rayon-threadlimit = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-short-vec = { workspace = true }
solana-signature = { workspace = true, features = ["verify"] }
solana-signer = { workspace = true, optional = true }
solana-system-interface = { workspace = true, optional = true }
solana-system-transaction = { workspace = true, optional = true }
solana-time-utils = { workspace = true }
solana-transaction = { workspace = true, optional = true }
solana-transaction-context = { workspace = true }
solana-vote = { workspace = true, optional = true }
solana-vote-program = { workspace = true, optional = true }

[target."cfg(target_os = \"linux\")".dependencies]
caps = { workspace = true }
libc = { workspace = true }
nix = { workspace = true, features = ["user"] }

[dev-dependencies]
agave-logger = { workspace = true }
assert_matches = { workspace = true }
bencher = { workspace = true }
rand_chacha = "0.3.1"
solana-perf = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
test-case = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dev-dependencies]
jemallocator = { workspace = true }

[[bench]]
name = "dedup"
harness = false

[[bench]]
name = "recycler"
harness = false

[[bench]]
name = "reset"
harness = false

[[bench]]
name = "shrink"
harness = false

[[bench]]
name = "sigverify"
harness = false

[[bench]]
name = "discard"
harness = false

[lints.rust.unexpected_cfgs]
level = "warn"
check-cfg = ['cfg(build_target_feature_avx)', 'cfg(build_target_feature_avx2)']

================
File: platform-tools-sdk/cargo-build-sbf/src/main.rs
================
mod post_processing;
mod toolchain;
mod utils;
⋮----
pub struct Config<'a> {
⋮----
impl Default for Config<'_> {
fn default() -> Self {
⋮----
cargo_args: vec![],
⋮----
.expect("Unable to get current executable")
.parent()
.expect("Unable to get parent directory")
.to_path_buf()
.join("platform-tools-sdk")
.join("sbf"),
⋮----
features: vec![],
⋮----
pub fn is_version_string(arg: &str) -> Result<(), String> {
let semver_re = Regex::new(r"^v?[0-9]+\.[0-9]+(\.[0-9]+)?$").unwrap();
if semver_re.is_match(arg) {
return Ok(());
⋮----
Err(
⋮----
.to_string(),
⋮----
fn home_dir() -> PathBuf {
⋮----
.or_else(|| {
⋮----
debug!("Could not read env variable 'HOME', falling back to 'USERPROFILE'");
⋮----
.unwrap_or_else(|| {
error!("Can't get home directory path");
exit(1);
⋮----
fn prepare_environment(
⋮----
&package.manifest_path.parent().unwrap_or_else(|| {
error!("Unable to get directory of {}", package.manifest_path);
⋮----
env::set_current_dir(root_dir).unwrap_or_else(|err| {
error!("Unable to set current directory to {root_dir}: {err}");
⋮----
install_and_link_tools(config, package, metadata)
⋮----
fn invoke_cargo(config: &Config, validated_toolchain_version: String) {
let target_triple = rust_target_triple(config);
info!("Solana SDK: {}", config.sbf_sdk.display());
⋮----
info!("No default features");
⋮----
if !config.features.is_empty() {
info!("Features: {}", config.features.join(" "));
⋮----
if corrupted_toolchain(config) {
error!(
⋮----
.join("dependencies")
.join("platform-tools")
.join("llvm")
.join("bin");
env::set_var("CC", llvm_bin.join("clang"));
env::set_var("AR", llvm_bin.join("llvm-ar"));
env::set_var("OBJDUMP", llvm_bin.join("llvm-objdump"));
env::set_var("OBJCOPY", llvm_bin.join("llvm-objcopy"));
let cargo_target = format!(
⋮----
let rustflags = env::var("RUSTFLAGS").ok().unwrap_or_default();
if env::var("RUSTFLAGS").is_ok() {
warn!("Removed RUSTFLAGS from cargo environment, because it overrides {cargo_target}.");
⋮----
let target_rustflags = env::var(&cargo_target).ok();
let mut target_rustflags = Cow::Borrowed(target_rustflags.as_deref().unwrap_or_default());
target_rustflags = Cow::Owned(format!("{} {}", &rustflags, &target_rustflags));
⋮----
target_rustflags = Cow::Owned(format!("{} -Zremap-cwd-prefix=", &target_rustflags));
⋮----
target_rustflags = Cow::Owned(format!("{} -C opt-level=s", &target_rustflags));
⋮----
target_rustflags = Cow::Owned(format!(
⋮----
target_rustflags = Cow::Owned(format!("{} -g", &target_rustflags));
⋮----
debug!(
⋮----
let mut cargo_build_args = vec![];
⋮----
toolchain_name = generate_toolchain_name(validated_toolchain_version.as_str());
toolchain_name = format!("+{toolchain_name}");
cargo_build_args.push(toolchain_name.as_str());
⋮----
cargo_build_args.append(&mut vec!["build", "--release", "--target", &target_triple]);
⋮----
cargo_build_args.push("--no-default-features");
⋮----
cargo_build_args.push("--features");
cargo_build_args.push(feature);
⋮----
cargo_build_args.push("--verbose");
⋮----
cargo_build_args.push("--quiet");
⋮----
cargo_build_args.push("--jobs");
cargo_build_args.push(jobs);
⋮----
cargo_build_args.push("--workspace");
⋮----
cargo_build_args.append(&mut config.cargo_args.clone());
let output = spawn(
⋮----
debug!("{output}");
⋮----
fn generate_program_name(package: &cargo_metadata::Package) -> Option<String> {
⋮----
.iter()
.filter_map(|target| {
if target.crate_types.contains(&"cdylib".to_string()) {
let other_crate_type = if target.crate_types.contains(&"rlib".to_string()) {
Some("rlib")
} else if target.crate_types.contains(&"lib".to_string()) {
Some("lib")
⋮----
warn!(
⋮----
Some(&target.name)
⋮----
match cdylib_targets.len() {
⋮----
1 => Some(cdylib_targets[0].replace('-', "_")),
⋮----
fn build_solana(config: Config, manifest_path: Option<PathBuf>) {
⋮----
metadata_command.manifest_path(manifest_path);
⋮----
metadata_command.other_options(vec!["--offline".to_string()]);
⋮----
let metadata = metadata_command.exec().unwrap_or_else(|err| {
error!("Failed to obtain package metadata: {err}");
⋮----
.clone()
.unwrap_or(metadata.target_directory.clone());
if let Some(root_package) = metadata.root_package() {
⋮----
let program_name = generate_program_name(root_package);
⋮----
prepare_environment(&config, Some(root_package), &metadata);
invoke_cargo(&config, validated_toolchain_version);
post_process(&config, target_dir.as_ref(), program_name);
⋮----
let validated_toolchain_version = prepare_environment(&config, None, &metadata);
⋮----
.filter(|package| {
if metadata.workspace_members.contains(&package.id) {
for target in package.targets.iter() {
if target.kind.contains(&"cdylib".to_string()) {
⋮----
let program_name = generate_program_name(package);
⋮----
fn main() {
⋮----
let default_sbf_sdk = format!("{}", default_config.sbf_sdk.display());
⋮----
if let Some(arg1) = args.get(1) {
⋮----
args.remove(1);
⋮----
let rust_base_version = get_base_rust_version(DEFAULT_PLATFORM_TOOLS_VERSION);
let version = format!(
⋮----
let matches = clap::Command::new(crate_name!())
.about(crate_description!())
.version(version.as_str())
.arg(
⋮----
.env("SBF_OUT_PATH")
.long("sbf-out-dir")
.value_name("DIRECTORY")
.takes_value(true)
.help("Place final SBF build artifacts in this directory"),
⋮----
.env("SBF_SDK_PATH")
.long("sbf-sdk")
.value_name("PATH")
⋮----
.default_value(&default_sbf_sdk)
.help("Path to the Solana SBF SDK"),
⋮----
.help("Arguments passed directly to `cargo build`")
.multiple_occurrences(true)
.multiple_values(true)
.last(true),
⋮----
.long("disable-remap-cwd")
.takes_value(false)
.help("Disable remap of cwd prefix and preserve full path strings in binaries"),
⋮----
.long("debug")
⋮----
.help("Enable debug symbols"),
⋮----
.long("dump")
⋮----
.help("Dump ELF information to a text file on success"),
⋮----
.long("features")
.value_name("FEATURES")
⋮----
.help("Space-separated list of features to activate"),
⋮----
.long("force-tools-install")
⋮----
.conflicts_with("skip_tools_install")
.help("Download and install platform-tools even when existing tools are located"),
⋮----
.long("install-only")
⋮----
.help(
⋮----
.long("skip-tools-install")
⋮----
.conflicts_with("force_tools_install")
⋮----
.long("no-rustup-override")
⋮----
.long("generate-child-script-on-failure")
⋮----
.help("Generate a shell script to rerun a failed subcommand"),
⋮----
.long("manifest-path")
⋮----
.help("Path to Cargo.toml"),
⋮----
.long("no-default-features")
⋮----
.help("Do not activate the `default` feature"),
⋮----
.long("offline")
⋮----
.help("Run without accessing the network"),
⋮----
.long("tools-version")
.value_name("STRING")
⋮----
.validator(is_version_string)
⋮----
.short('v')
.long("verbose")
⋮----
.help("Use verbose output"),
⋮----
.short('q')
.long("quiet")
⋮----
.help("Do not print cargo log messages"),
⋮----
.long("workspace")
⋮----
.alias("all")
.help("Build all Solana packages in the workspace"),
⋮----
.short('j')
.long("jobs")
⋮----
.value_name("N")
.validator(|val| val.parse::<usize>().map_err(|e| e.to_string()))
.help("Number of parallel jobs, defaults to # of CPUs"),
⋮----
.long("arch")
.possible_values(["v0", "v1", "v2", "v3", "v4"])
.default_value("v0")
.help("Build for the given target architecture"),
⋮----
.long("optimize-size")
⋮----
.arg(Arg::new("lto").long("lto").takes_value(false).help(
⋮----
.get_matches_from(args);
let sbf_sdk: PathBuf = matches.value_of_t_or_exit("sbf_sdk");
let sbf_out_dir: Option<PathBuf> = matches.value_of_t("sbf_out_dir").ok();
⋮----
.values_of("cargo_args")
.map(|vals| vals.collect::<Vec<_>>())
.unwrap_or_default();
⋮----
.iter_mut()
.skip_while(|x| x != &&"--target-dir")
.nth(1)
⋮----
fs::create_dir_all(&target_path).unwrap_or_else(|err| {
error!("Unable to create target-dir directory {target_dir}: {err}");
⋮----
let canonicalized = target_path.canonicalize_utf8().unwrap_or_else(|err| {
error!("Unable to canonicalize provided target-dir directory {target_path}: {err}");
⋮----
target_dir_string = canonicalized.to_string();
⋮----
Some(canonicalized)
⋮----
sbf_sdk: fs::canonicalize(&sbf_sdk).unwrap_or_else(|err| {
⋮----
sbf_out_dir: sbf_out_dir.map(|sbf_out_dir| {
if sbf_out_dir.is_absolute() {
⋮----
.expect("Unable to get current working directory")
.join(sbf_out_dir)
⋮----
platform_tools_version: matches.value_of("tools_version"),
dump: matches.is_present("dump"),
features: matches.values_of_t("features").ok().unwrap_or_default(),
force_tools_install: matches.is_present("force_tools_install"),
skip_tools_install: matches.is_present("skip_tools_install"),
no_rustup_override: matches.is_present("no_rustup_override"),
generate_child_script_on_failure: matches.is_present("generate_child_script_on_failure"),
no_default_features: matches.is_present("no_default_features"),
remap_cwd: !matches.is_present("remap_cwd"),
debug: matches.is_present("debug"),
offline: matches.is_present("offline"),
verbose: matches.is_present("verbose"),
quiet: matches.is_present("quiet"),
workspace: matches.is_present("workspace"),
jobs: matches.value_of_t("jobs").ok(),
arch: matches.value_of("arch").unwrap(),
optimize_size: matches.is_present("optimize_size"),
lto: matches.is_present("lto"),
install_only: matches.is_present("install_only"),
⋮----
let manifest_path: Option<PathBuf> = matches.value_of_t("manifest_path").ok();
⋮----
debug!("{config:?}");
debug!("manifest_path: {manifest_path:?}");
⋮----
let platform_tools_version = validate_platform_tools_version(
⋮----
.unwrap_or(DEFAULT_PLATFORM_TOOLS_VERSION),
⋮----
install_tools(&config, &platform_tools_version, true);
⋮----
build_solana(config, manifest_path);
⋮----
mod tests {
⋮----
fn test_is_version_string_valid_versions() {
assert!(is_version_string("1.2.3").is_ok());
assert!(is_version_string("v2.1.0").is_ok());
assert!(is_version_string("1.32").is_ok());
assert!(is_version_string("v1.32").is_ok());
assert!(is_version_string("0.1").is_ok());
assert!(is_version_string("v0.1").is_ok());
assert!(is_version_string("10.20.30").is_ok());
assert!(is_version_string("v10.20.30").is_ok());
⋮----
fn test_is_version_string_invalid_versions() {
assert!(is_version_string("1.2.3abc").is_err());
assert!(is_version_string("v2.1.0-extra").is_err());
assert!(is_version_string("abc1.2.3").is_err());
assert!(is_version_string("1").is_err());
assert!(is_version_string("v1").is_err());
assert!(is_version_string("1.2.3.4.5").is_err());
assert!(is_version_string("").is_err());
assert!(is_version_string("v").is_err());
assert!(is_version_string("1.").is_err());
assert!(is_version_string("v1.").is_err());
assert!(is_version_string(".1.2").is_err());
assert!(is_version_string("1.2.3-beta").is_err());
assert!(is_version_string("v1.2.3+build").is_err());
⋮----
fn test_is_version_string_error_message() {
let result = is_version_string("invalid");
assert!(result.is_err());
let error_msg = result.unwrap_err();
assert!(error_msg.contains("version string may start with 'v'"));
assert!(error_msg.contains("major and minor version numbers"));
assert!(error_msg.contains("separated by a dot"));

================
File: platform-tools-sdk/cargo-build-sbf/src/post_processing.rs
================
pub(crate) fn post_process(config: &Config, target_directory: &Path, program_name: Option<String>) {
⋮----
.as_ref()
.cloned()
.unwrap_or_else(|| target_directory.join("deploy"));
let target_triple = rust_target_triple(config);
let target_build_directory = target_directory.join(&target_triple).join("release");
⋮----
let program_unstripped_so = target_build_directory.join(format!("{program_name}.so"));
let program_dump = sbf_out_dir.join(format!("{program_name}-dump.txt"));
let program_so = sbf_out_dir.join(format!("{program_name}.so"));
let program_debug = sbf_out_dir.join(format!("{program_name}.debug"));
let program_keypair = sbf_out_dir.join(format!("{program_name}-keypair.json"));
fn file_older_or_missing(prerequisite_file: &Path, target_file: &Path) -> bool {
let prerequisite_metadata = fs::metadata(prerequisite_file).unwrap_or_else(|err| {
error!(
⋮----
exit(1);
⋮----
use std::time::UNIX_EPOCH;
prerequisite_metadata.modified().unwrap_or(UNIX_EPOCH)
> target_metadata.modified().unwrap_or(UNIX_EPOCH)
⋮----
if !program_keypair.exists() {
write_keypair_file(&Keypair::new(), &program_keypair).unwrap_or_else(|err| {
⋮----
.join("dependencies")
.join("platform-tools")
.join("llvm")
.join("bin");
if file_older_or_missing(&program_unstripped_so, &program_so) {
⋮----
let output = spawn(
&llvm_bin.join("llvm-objcopy"),
⋮----
"--strip-all".as_ref(),
program_unstripped_so.as_os_str(),
program_so.as_os_str(),
⋮----
&config.sbf_sdk.join("scripts").join("strip.sh"),
⋮----
debug!("{output}");
⋮----
if config.dump && file_older_or_missing(&program_unstripped_so, &program_dump) {
let dump_script = config.sbf_sdk.join("scripts").join("dump.sh");
⋮----
postprocess_dump(&program_dump);
⋮----
if config.debug && file_older_or_missing(&program_unstripped_so, &program_debug) {
⋮----
let llvm_objcopy = &llvm_bin.join("llvm-objcopy");
⋮----
let llvm_objcopy = &config.sbf_sdk.join("scripts").join("objcopy.sh");
⋮----
"--only-keep-debug".as_ref(),
⋮----
program_debug.as_os_str(),
⋮----
check_undefined_symbols(config, &program_so);
⋮----
info!("To deploy this program:");
info!("  $ solana program deploy {}", program_so.display());
info!("The program address will default to this keypair (override with --program-id):");
info!("  {}", program_keypair.display());
⋮----
warn!("Note: --dump is only available for crates with a cdylib target");
⋮----
fn check_undefined_symbols(config: &Config, program: &Path) {
let syscalls_txt = config.sbf_sdk.join("syscalls.txt");
⋮----
for line_result in BufReader::new(file).lines() {
let line = line_result.unwrap();
let line = line.trim_end();
syscalls.insert(line.to_string());
⋮----
.unwrap();
⋮----
.join("bin")
.join("llvm-readelf");
let mut readelf_args = vec!["--dyn-symbols"];
readelf_args.push(program.to_str().unwrap());
⋮----
for line in output.lines() {
⋮----
if entry.is_match(line) {
let captures = entry.captures(line).unwrap();
let symbol = captures[1].to_string();
if !syscalls.contains(&symbol) {
unresolved_symbols.push(symbol);
⋮----
if !unresolved_symbols.is_empty() {
warn!(
⋮----
warn!("         Calling them will trigger a run-time error.");
⋮----
fn postprocess_dump(program_dump: &Path) {
if !program_dump.exists() {
⋮----
let postprocessed_dump = program_dump.with_extension("postprocessed");
let head_re = Regex::new(r"^([0-9a-f]{16}) (.+)").unwrap();
let insn_re = Regex::new(r"^ +([0-9]+)((\s[0-9a-f]{2})+)\s.+").unwrap();
let call_re = Regex::new(r"^ +([0-9]+)(\s[0-9a-f]{2})+\scall (-?)0x([0-9a-f]+)").unwrap();
let relo_re = Regex::new(r"^([0-9a-f]{16})  [0-9a-f]{16} R_BPF_64_32 +0{16} (.+)").unwrap();
⋮----
if relo_re.is_match(line) {
let captures = relo_re.captures(line).unwrap();
let address = u64::from_str_radix(&captures[1], 16).unwrap();
let symbol = captures[2].to_string();
rel.insert(address, symbol);
⋮----
if head_re.is_match(line) {
⋮----
let captures = head_re.captures(line).unwrap();
name = captures[2].to_string();
⋮----
if insn_re.is_match(line) {
let captures = insn_re.captures(line).unwrap();
let address = i64::from_str(&captures[1]).unwrap();
a2n.insert(address, name.clone());
⋮----
pc = u64::from_str_radix(&captures[1], 16).unwrap();
writeln!(out, "{line}").unwrap();
⋮----
step = if captures[2].len() > 24 { 16 } else { 8 };
⋮----
if call_re.is_match(line) {
if rel.contains_key(&pc) {
writeln!(out, "{} ; {}", line, rel[&pc]).unwrap();
⋮----
let captures = call_re.captures(line).unwrap();
let pc = i64::from_str(&captures[1]).unwrap().checked_add(1).unwrap();
let offset = i64::from_str_radix(&captures[4], 16).unwrap();
⋮----
offset.checked_neg().unwrap()
⋮----
let address = pc.checked_add(offset).unwrap();
if a2n.contains_key(&address) {
writeln!(out, "{} ; {}", line, a2n[&address]).unwrap();
⋮----
pc = pc.checked_add(step).unwrap();
⋮----
fs::rename(postprocessed_dump, program_dump).unwrap();

================
File: platform-tools-sdk/cargo-build-sbf/src/toolchain.rs
================
fn find_installed_platform_tools() -> Vec<String> {
let solana = home_dir().join(".cache").join("solana");
⋮----
dir.filter_map(|e| match e {
⋮----
if e.path().join(package).is_dir() {
Some(e.path().file_name().unwrap().to_string_lossy().to_string())
⋮----
fn get_latest_platform_tools_version() -> Result<String, String> {
⋮----
let resp = reqwest::blocking::get(url).map_err(|err| format!("Failed to GET {url}: {err}"))?;
let path = std::path::Path::new(resp.url().path());
let version = path.file_name().unwrap().to_string_lossy().to_string();
Ok(version)
⋮----
fn downloadable_version(version: &str) -> String {
if version.starts_with('v') {
version.to_string()
⋮----
format!("v{version}")
⋮----
fn semver_version(version: &str) -> String {
let starts_with_v = version.starts_with('v');
let dots = version.as_bytes().iter().fold(
⋮----
|n: u32, c| if *c == b'.' { n.saturating_add(1) } else { n },
⋮----
(0, false) => format!("{version}.0.0"),
(0, true) => format!("{}.0.0", &version[1..]),
(1, false) => format!("{version}.0"),
(1, true) => format!("{}.0", &version[1..]),
(_, false) => version.to_string(),
(_, true) => version[1..].to_string(),
⋮----
pub(crate) fn validate_platform_tools_version(
⋮----
// Early return here in case it's the first time we're running `cargo build-sbf`
⋮----
return builtin_version.to_string();
⋮----
let normalized_requested = semver_version(requested_version);
let requested_semver = semver::Version::parse(&normalized_requested).unwrap();
let installed_versions = find_installed_platform_tools();
⋮----
if requested_semver <= semver::Version::parse(&semver_version(&v)).unwrap() {
return downloadable_version(requested_version);
⋮----
let latest_version = get_latest_platform_tools_version().unwrap_or_else(|err| {
debug!(
⋮----
builtin_version.to_string()
⋮----
let normalized_latest = semver_version(&latest_version);
let latest_semver = semver::Version::parse(&normalized_latest).unwrap();
⋮----
downloadable_version(requested_version)
⋮----
warn!(
⋮----
fn make_platform_tools_path_for_version(package: &str, version: &str) -> PathBuf {
home_dir()
.join(".cache")
.join("solana")
.join(version)
.join(package)
⋮----
pub(crate) fn get_base_rust_version(platform_tools_version: &str) -> String {
⋮----
make_platform_tools_path_for_version("platform-tools", platform_tools_version);
let rustc = target_path.join("rust").join("bin").join("rustc");
if !rustc.exists() {
⋮----
let args = vec!["--version"];
let output = spawn(&rustc, args, false);
let rustc_re = Regex::new(r"(rustc [0-9]+\.[0-9]+\.[0-9]+).*").unwrap();
if rustc_re.is_match(output.as_str()) {
let captures = rustc_re.captures(output.as_str()).unwrap();
captures[1].to_string()
⋮----
struct Items {
⋮----
struct GithubResponse {
⋮----
let url = format!("https://github.com/anza-xyz/platform-tools/releases/download/{platform_tools_version}/{download_file_name}");

================
File: platform-tools-sdk/cargo-build-sbf/src/utils.rs
================
pub(crate) fn spawn<I, S>(program: &Path, args: I, generate_child_script_on_failure: bool) -> String
⋮----
.iter()
.map(|arg| arg.as_ref().to_str().unwrap_or("?"))
.join(" ");
info!("spawn: {program:?} {msg}");
⋮----
.args(args)
.stdout(Stdio::piped())
.spawn()
.unwrap_or_else(|err| {
error!("Failed to execute {}: {}", program.display(), err);
exit(1);
⋮----
let output = child.wait_with_output().expect("failed to wait on child");
if !output.status.success() {
⋮----
error!("cargo-build-sbf exited on command execution failure");
let script_name = format!(
⋮----
let file = File::create(&script_name).unwrap();
⋮----
writeln!(out, "{key}=\"{value}\" \\").unwrap();
⋮----
write!(out, "{}", program.display()).unwrap();
writeln!(out, "{msg}").unwrap();
out.flush().unwrap();
error!("To rerun the failed command for debugging use {script_name}");
⋮----
.as_slice()
⋮----
.map(|&c| c as char)

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/fail/src/lib.rs
================
fn process_instruction(

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/fail/Cargo.toml
================
[package]
name = "fail"
version = "4.0.0-alpha.0"
description = "Solana SBF test program written in Rust"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
repository = "https://github.com/anza-xyz/agave"
license = "Apache-2.0"
homepage = "https://anza.xyz"
edition = "2021"
publish = false

[dependencies]
solana-account-info = "=2.3.0"
solana-program-entrypoint = "=2.3.0"
solana-program-error = "=2.2.2"
solana-pubkey = "=2.4.0"

[lib]
crate-type = ["cdylib"]

[workspace]

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/noop/src/lib.rs
================
fn process_instruction(
⋮----
Ok(())

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/noop/Cargo.toml
================
[package]
name = "noop"
version = "4.0.0-alpha.0"
description = "Solana SBF test program written in Rust"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
repository = "https://github.com/anza-xyz/agave"
license = "Apache-2.0"
homepage = "https://anza.xyz"
edition = "2021"
publish = false

[dependencies]
solana-account-info = "=2.3.0"
solana-program-entrypoint = "=2.3.0"
solana-program-error = "=2.2.2"
solana-pubkey = "=2.4.0"

[lib]
crate-type = ["cdylib"]

[lints.rust.unexpected_cfgs]
level = "warn"
check-cfg = [
    'cfg(feature, values("custom-panic", "custom-heap"))'
]

[workspace]

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/package-metadata/src/lib.rs
================
fn process_instruction(
⋮----
Ok(())

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/package-metadata/Cargo.toml
================
[package]
name = "package-metadata"
version = "4.0.0-alpha.0"
description = "Solana SBF test program with tools version in package metadata"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
repository = "https://github.com/anza-xyz/agave"
license = "Apache-2.0"
homepage = "https://anza.xyz"
edition = "2021"
publish = false

[package.metadata.solana]
tools-version = "v1.52"
program-id = "MyProgram1111111111111111111111111111111111"

[dependencies]
solana-account-info = "=2.3.0"
solana-package-metadata = "=2.2.1"
solana-program-entrypoint = "=2.3.0"
solana-program-error = "=2.2.2"
solana-pubkey = "=2.4.0"

[lib]
crate-type = ["cdylib"]

[lints.rust.unexpected_cfgs]
level = "warn"
check-cfg = [
    'cfg(feature, values("custom-panic", "custom-heap"))'
]

[workspace]

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/workspace-metadata/src/lib.rs
================
fn process_instruction(
⋮----
Ok(())

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates/workspace-metadata/Cargo.toml
================
[package]
name = "workspace-metadata"
version = "4.0.0-alpha.0"
description = "Solana SBF test program with tools version in workspace metadata"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
repository = "https://github.com/anza-xyz/agave"
license = "Apache-2.0"
homepage = "https://anza.xyz"
edition = "2021"
publish = false

[dependencies]
solana-account-info = "=2.3.0"
solana-program-entrypoint = "=2.3.0"
solana-program-error = "=2.2.2"
solana-pubkey = "=2.4.0"

[lib]
crate-type = ["cdylib"]

[lints.rust.unexpected_cfgs]
level = "warn"
check-cfg = [
    'cfg(feature, values("custom-panic", "custom-heap"))'
]

[workspace]

[workspace.metadata.solana]
tools-version = "v1.52"

================
File: platform-tools-sdk/cargo-build-sbf/tests/crates.rs
================
extern crate serial_test;
fn should_install_tools() -> bool {
let tools_path = env::var("HOME").unwrap();
⋮----
.join(".cache")
.join("solana")
.join("v1.52")
.join("platform-tools");
let rust_path = toolchain_path.join("rust");
let llvm_path = toolchain_path.join("llvm");
let binaries = rust_path.join("bin");
let rustc = binaries.join(if cfg!(windows) { "rustc.exe" } else { "rustc" });
let cargo = binaries.join(if cfg!(windows) { "cargo.exe" } else { "cargo" });
if !toolchain_path.try_exists().unwrap_or(false)
|| !rust_path.try_exists().unwrap_or(false)
|| !llvm_path.try_exists().unwrap_or(false)
|| !binaries.try_exists().unwrap_or(false)
|| !rustc.try_exists().unwrap_or(false)
|| !cargo.try_exists().unwrap_or(false)
⋮----
let Ok(creation_time) = folder_metadata.created() else {
⋮----
let Ok(elapsed_time) = now.duration_since(creation_time) else {
⋮----
if elapsed_time.as_secs() > 300 {
⋮----
fn run_cargo_build(crate_name: &str, extra_args: &[&str], fail: bool) {
let cwd = env::current_dir().expect("Unable to get current working directory");
⋮----
.join("tests")
.join("crates")
.join(crate_name)
.join("Cargo.toml");
let toml = format!("{}", toml.display());
let mut args = vec!["-v", "--sbf-sdk", "../sbf", "--manifest-path", &toml];
if should_install_tools() {
args.push("--force-tools-install");
⋮----
args.push(arg);
⋮----
if !extra_args.contains(&"--") {
args.push("--");
⋮----
args.push("-vv");
let mut cmd = assert_cmd::Command::cargo_bin("cargo-build-sbf").unwrap();
let assert = cmd.env("RUST_LOG", "debug").args(&args).assert();
let output = assert.get_output();
eprintln!("Test stdout\n{}\n", String::from_utf8_lossy(&output.stdout));
eprintln!("Test stderr\n{}\n", String::from_utf8_lossy(&output.stderr));
⋮----
assert.failure();
⋮----
assert.success();
⋮----
fn clean_target(crate_name: &str) {
⋮----
.join("target");
fs::remove_dir_all(target).expect("Failed to remove target dir");
⋮----
fn test_build() {
run_cargo_build("noop", &[], false);
clean_target("noop");
⋮----
fn test_dump() {
⋮----
.args(["install", "rustfilt"])
.assert()
.success();
run_cargo_build("noop", &["--dump"], false);
⋮----
.join("noop")
.join("target")
.join("deploy")
.join("noop-dump.txt");
assert!(dump.exists());
⋮----
fn test_out_dir() {
run_cargo_build("noop", &["--sbf-out-dir", "tmp_out"], false);
⋮----
let dir = cwd.join("tmp_out");
assert!(dir.exists());
fs::remove_dir_all("tmp_out").expect("Failed to remove tmp_out dir");
⋮----
fn test_target_dir() {
⋮----
run_cargo_build("noop", &["--lto", "--", "--target-dir", target_dir], false);
⋮----
let normal_target_dir = cwd.join("tests").join("crates").join("noop").join("target");
assert!(!normal_target_dir.exists());
⋮----
.unwrap()
⋮----
.join("noop.so");
assert!(so_file.exists());
fs::remove_dir_all(target_dir).expect("Failed to remove custom target dir");
⋮----
fn test_target_and_out_dir() {
⋮----
run_cargo_build(
⋮----
fn test_generate_child_script_on_failure() {
run_cargo_build("fail", &["--generate-child-script-on-failure"], true);
⋮----
.join("fail")
.join("cargo-build-sbf-child-script-cargo.sh");
assert!(scr.exists());
fs::remove_file(scr).expect("Failed to remove script");
clean_target("fail");
⋮----
fn build_noop_and_readelf(arch: &str) -> Assert {
run_cargo_build("noop", &["--arch", arch], false);
⋮----
let bin = bin.to_str().unwrap();
⋮----
.parent()
.expect("Unable to get parent directory of current working dir")
⋮----
.expect("Unable to get ../.. of current working dir");
⋮----
.join("platform-tools-sdk")
.join("sbf")
.join("dependencies")
.join("platform-tools")
.join("llvm")
.join("bin")
.join("llvm-readelf");
assert_cmd::Command::new(readelf).args(["-h", bin]).assert()
⋮----
fn test_sbpfv0() {
let assert_v0 = build_noop_and_readelf("v0");
⋮----
.stdout(predicate::str::contains(
⋮----
fn test_sbpfv1() {
let assert_v1 = build_noop_and_readelf("v1");
⋮----
fn test_sbpfv2() {
let assert_v1 = build_noop_and_readelf("v2");
⋮----
fn test_sbpfv3() {
let assert_v1 = build_noop_and_readelf("v3");
⋮----
fn test_sbpfv4() {
let assert_v1 = build_noop_and_readelf("v4");
⋮----
fn test_package_metadata_tools_version() {
run_cargo_build("package-metadata", &[], false);
clean_target("package-metadata");
⋮----
fn test_workspace_metadata_tools_version() {
run_cargo_build("workspace-metadata", &[], false);
clean_target("workspace-metadata");
⋮----
fn test_corrupted_toolchain() {
⋮----
fn assert_failed_command() {
⋮----
let args = vec!["--sbf-sdk", "../sbf", "--manifest-path", &toml];
⋮----
assert!(
⋮----
let sdk_path = cwd.parent().unwrap().join("sbf");
⋮----
.join("rust")
.join("bin");
fs::rename(bin_folder.join("cargo"), bin_folder.join("cargo_2"))
.expect("Failed to rename file");
assert_failed_command();
fs::rename(bin_folder.join("cargo_2"), bin_folder.join("cargo"))
⋮----
fs::rename(bin_folder.join("rustc"), bin_folder.join("rustc_2"))
⋮----
fs::rename(bin_folder.join("rustc_2"), bin_folder.join("rustc"))
⋮----
fs::rename(&bin_folder, bin_folder.parent().unwrap().join("bin2"))
⋮----
fs::rename(bin_folder.parent().unwrap().join("bin2"), &bin_folder)
⋮----
let right_rust_folder = bin_folder.parent().unwrap();
let wrong_rust_folder = right_rust_folder.parent().unwrap().join("rust_2");
fs::rename(right_rust_folder, &wrong_rust_folder).expect("Failed to rename file");
⋮----
fs::rename(wrong_rust_folder, right_rust_folder).expect("Failed to rename file");
⋮----
fn test_alternate_download() {
⋮----
.env("RUST_LOG", "debug")
.args(args)
.assert();
⋮----
build_noop_and_readelf("v0");

================
File: platform-tools-sdk/cargo-build-sbf/.gitignore
================
/tests/crates/*/target/
Cargo.lock

================
File: platform-tools-sdk/cargo-build-sbf/Cargo.toml
================
[package]
name = "solana-cargo-build-sbf"
description = "Compile a local package and all of its dependencies using the Solana SBF SDK"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[[bin]]
name = "cargo-build-sbf"
path = "src/main.rs"

[features]
agave-unstable-api = []
program = []

[dependencies]
agave-logger = { workspace = true }
bzip2 = { workspace = true }
cargo_metadata = { workspace = true }
clap = { version = "3.1.5", features = ["cargo", "env"] }
itertools = { workspace = true }
log = { workspace = true, features = ["std"] }
regex = { workspace = true }
reqwest = { workspace = true, features = ["blocking", "rustls-tls", "rustls-tls-native-roots", "json" ] }
semver = { workspace = true }
serde = { workspace = true }
solana-file-download = "=3.1.0"
solana-keypair = "=3.0.1"
tar = { workspace = true }

[dev-dependencies]
assert_cmd = { workspace = true }
predicates = { workspace = true }
serial_test = { workspace = true }

================
File: platform-tools-sdk/cargo-test-sbf/src/main.rs
================
struct Config<'a> {
⋮----
impl Default for Config<'_> {
fn default() -> Self {
⋮----
extra_cargo_test_args: vec![],
features: vec![],
packages: vec![],
⋮----
fn spawn<I, S>(program: &Path, args: I, generate_child_script_on_failure: bool)
⋮----
.iter()
.map(|arg| arg.as_ref().to_str().unwrap_or("?"))
.join(" ");
info!("spawn: {msg}");
⋮----
.args(args)
.spawn()
.unwrap_or_else(|err| {
error!("Failed to execute {}: {}", program.display(), err);
exit(1);
⋮----
let exit_status = child.wait().expect("failed to wait on child");
if !exit_status.success() {
⋮----
error!("cargo-test-sbf exited on command execution failure");
let script_name = format!(
⋮----
let file = File::create(&script_name).unwrap();
⋮----
writeln!(out, "{key}=\"{value}\" \\").unwrap();
⋮----
write!(out, "{}", program.display()).unwrap();
writeln!(out, "{msg}").unwrap();
out.flush().unwrap();
error!("To rerun the failed command for debugging use {script_name}");
⋮----
pub fn is_version_string(arg: &str) -> Result<(), String> {
let semver_re = Regex::new(r"^v?[0-9]+\.[0-9]+(\.[0-9]+)?").unwrap();
if semver_re.is_match(arg) {
return Ok(());
⋮----
Err(
⋮----
.to_string(),
⋮----
fn test_solana_package(
⋮----
.as_ref()
.cloned()
.unwrap_or_else(|| format!("{}", target_directory.join("deploy").display()));
let manifest_path = format!("{}", package.manifest_path);
let mut cargo_args = vec!["--manifest-path", &manifest_path];
⋮----
cargo_args.push("--no-default-features");
⋮----
cargo_args.push("--features");
cargo_args.push(feature);
⋮----
cargo_args.push("--verbose");
⋮----
cargo_args.push("--jobs");
cargo_args.push(jobs);
⋮----
let mut build_sbf_args = cargo_args.clone();
if let Some(sbf_sdk) = config.sbf_sdk.as_ref() {
build_sbf_args.push("--sbf-sdk");
build_sbf_args.push(sbf_sdk);
⋮----
build_sbf_args.push("--sbf-out-dir");
build_sbf_args.push(&sbf_out_dir);
build_sbf_args.push("--arch");
build_sbf_args.push(config.arch);
if let Some(tools_version) = config.platform_tools_version.as_ref() {
build_sbf_args.push("--tools-version");
build_sbf_args.push(tools_version);
⋮----
if !config.packages.is_empty() {
build_sbf_args.push("--");
⋮----
build_sbf_args.push("-p");
build_sbf_args.push(package);
⋮----
spawn(
⋮----
cargo_args.insert(0, "test");
⋮----
cargo_args.push("-p");
cargo_args.push(package);
⋮----
cargo_args.push("--test");
cargo_args.push(test_name);
⋮----
cargo_args.push("--no-run");
⋮----
if package.features.contains_key("test-sbf") {
⋮----
cargo_args.push("test-sbf");
⋮----
if package.features.contains_key("test-bpf") {
⋮----
cargo_args.push("test-bpf");
⋮----
cargo_args.push(extra_cargo_test_arg);
⋮----
fn test_solana(config: Config, manifest_path: Option<PathBuf>) {
⋮----
if let Some(manifest_path) = manifest_path.as_ref() {
metadata_command.manifest_path(manifest_path);
⋮----
metadata_command.other_options(vec!["--offline".to_string()]);
⋮----
let metadata = metadata_command.exec().unwrap_or_else(|err| {
error!("Failed to obtain package metadata: {err}");
⋮----
if let Some(root_package) = metadata.root_package() {
⋮----
&& (config.packages.is_empty()
⋮----
.any(|p| root_package.id.repr.contains(p)))
⋮----
debug!("test root package {:?}", root_package.id);
test_solana_package(&config, metadata.target_directory.as_ref(), root_package);
⋮----
.filter(|package| {
if metadata.workspace_members.contains(&package.id) {
for target in package.targets.iter() {
if target.kind.contains(&"cdylib".to_string()) {
⋮----
if config.packages.is_empty() || config.packages.iter().any(|p| package.id.repr.contains(p))
⋮----
debug!("test package {:?}", package.id);
test_solana_package(&config, metadata.target_directory.as_ref(), package);
⋮----
fn main() {
⋮----
if let Some(arg1) = args.get(1) {
⋮----
args.remove(1);
⋮----
let em_dash = "--".to_string();
let args_contain_dashash = args.contains(&em_dash);
let matches = clap::Command::new(crate_name!())
.about(crate_description!())
.version(crate_version!())
.trailing_var_arg(true)
.arg(
⋮----
.long("sbf-sdk")
.value_name("PATH")
.takes_value(true)
.help("Path to the Solana SBF SDK"),
⋮----
.long("features")
.value_name("FEATURES")
⋮----
.multiple_occurrences(true)
.multiple_values(true)
.help("Space-separated list of features to activate"),
⋮----
.long("no-default-features")
.takes_value(false)
.help("Do not activate the `default` feature"),
⋮----
.long("test")
.value_name("NAME")
⋮----
.help("Test only the specified test target"),
⋮----
.long("manifest-path")
⋮----
.help("Path to Cargo.toml"),
⋮----
.long("package")
.short('p')
.value_name("SPEC")
⋮----
.help("Package to run tests for"),
⋮----
.long("sbf-out-dir")
.value_name("DIRECTORY")
⋮----
.help("Place final SBF build artifacts in this directory"),
⋮----
.long("no-run")
⋮----
.help("Compile, but don't run tests"),
⋮----
.long("offline")
⋮----
.help("Run without accessing the network"),
⋮----
.long("generate-child-script-on-failure")
⋮----
.help("Generate a shell script to rerun a failed subcommand"),
⋮----
.short('v')
.long("verbose")
⋮----
.help("Use verbose output"),
⋮----
.long("workspace")
⋮----
.alias("all")
.help("Test all Solana packages in the workspace"),
⋮----
.short('j')
.long("jobs")
⋮----
.value_name("N")
.validator(|val| val.parse::<usize>().map_err(|e| e.to_string()))
.help("Number of parallel jobs, defaults to # of CPUs"),
⋮----
.long("arch")
.possible_values(["v0", "v1", "v2", "v3"])
.default_value("v0")
.help("Build for the given target architecture"),
⋮----
.value_name("extra args for cargo test and the test binary")
.index(1)
⋮----
.help("All extra arguments are passed through to cargo test"),
⋮----
.long("tools-version")
.value_name("STRING")
⋮----
.validator(is_version_string)
.help(
⋮----
.get_matches_from(args);
⋮----
sbf_sdk: matches.value_of_t("sbf_sdk").ok(),
sbf_out_dir: matches.value_of_t("sbf_out_dir").ok(),
⋮----
.values_of_t("extra_cargo_test_args")
.ok()
.unwrap_or_default(),
platform_tools_version: matches.value_of_t("tools_version").ok(),
features: matches.values_of_t("features").ok().unwrap_or_default(),
packages: matches.values_of_t("packages").ok().unwrap_or_default(),
generate_child_script_on_failure: matches.is_present("generate_child_script_on_failure"),
test_name: matches.value_of_t("test").ok(),
no_default_features: matches.is_present("no_default_features"),
no_run: matches.is_present("no_run"),
offline: matches.is_present("offline"),
verbose: matches.is_present("verbose"),
workspace: matches.is_present("workspace"),
jobs: matches.value_of_t("jobs").ok(),
arch: matches.value_of("arch").unwrap(),
⋮----
if args_contain_dashash && !config.extra_cargo_test_args.contains(&em_dash) {
config.extra_cargo_test_args.insert(0, em_dash);
⋮----
let manifest_path: Option<PathBuf> = matches.value_of_t("manifest_path").ok();
test_solana(config, manifest_path);

================
File: platform-tools-sdk/cargo-test-sbf/Cargo.toml
================
[package]
name = "solana-cargo-test-sbf"
description = "Execute all unit and integration tests after building with the Solana SBF SDK"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[[bin]]
name = "cargo-test-sbf"
path = "src/main.rs"

[features]
agave-unstable-api = []

[dependencies]
agave-logger = { workspace = true }
cargo_metadata = { workspace = true }
clap = { version = "3.1.5", features = ["cargo"] }
itertools = { workspace = true }
log = { workspace = true, features = ["std"] }
regex = { workspace = true }

================
File: platform-tools-sdk/gen-headers/src/main.rs
================
fn main() {
⋮----
if syscalls_inc_path.is_dir() {
for entry in fs::read_dir(syscalls_inc_path).expect("Can't open headers dir") {
let entry = entry.expect("Can't open header file");
let path = entry.path();
if !path.is_dir() {
let extension = path.extension();
if extension == Some(OsStr::new("inc")) {
transform(&path);
⋮----
fn transform(inc: &PathBuf) {
⋮----
let Some(filename) = inc_path.file_name() else {
⋮----
let Some(parent) = inc_path.parent() else {
⋮----
let Some(parent) = parent.parent() else {
⋮----
filename.set_extension("h");
header_path.push(filename);
info!(
⋮----
Err(err) => panic!("Failed to open {}: {}", inc.display(), err),
⋮----
let mut input_content = vec![];
input.read_to_end(&mut input_content).unwrap();
let input_content = str::from_utf8(&input_content).unwrap();
⋮----
Err(err) => panic!("Failed to create {}: {}", header_path.display(), err),
⋮----
.unwrap();
let comm_re = Regex::new(r",").unwrap();
let output_content = decl_re.replace_all(input_content, |caps: &Captures| {
let ty = &caps[1].to_string();
let func = &caps[2].to_string();
let args = &caps[3].to_string();
let warn = format!(
⋮----
let ifndef = format!("#ifndef SOL_SBPFV3\n{ty} {func}({args});");
let hash = sys_hash(func);
let typedef_statement = format!("typedef {ty}(*{func}_pointer_type)({args});");
⋮----
let mut arg_list = "".to_string();
if !args.is_empty() {
⋮----
.replace_all(args, |_caps: &Captures| {
⋮----
format!(" arg{arg},")
⋮----
.to_string();
⋮----
arg_list = format!("{arg_list} arg{arg}");
⋮----
let function_signature = format!("static {ty} {func}({arg_list})");
⋮----
format!("{func}_pointer_type {func}_pointer = ({func}_pointer_type) {hash};");
⋮----
arg_list = "arg1".to_string();
⋮----
arg_list = format!("{arg_list}, arg{a}");
⋮----
format!("{func}_pointer({arg_list});")
⋮----
format!("return {func}_pointer({arg_list});")
⋮----
format!(
⋮----
write!(output_writer, "{output_content}").unwrap();
⋮----
const fn sys_hash(name: &str) -> usize {
murmur3_32(name.as_bytes(), 0) as usize
⋮----
const fn murmur3_32(buf: &[u8], seed: u32) -> u32 {
⋮----
while i < buf.len() / 4 {
⋮----
hash ^= pre_mix(buf);
hash = hash.rotate_left(13);
hash = hash.wrapping_mul(5).wrapping_add(0xe6546b64);
⋮----
match buf.len() % 4 {
⋮----
hash ^= pre_mix([buf[i * 4], 0, 0, 0]);
⋮----
hash ^= pre_mix([buf[i * 4], buf[i * 4 + 1], 0, 0]);
⋮----
hash ^= pre_mix([buf[i * 4], buf[i * 4 + 1], buf[i * 4 + 2], 0]);
⋮----
hash ^= buf.len() as u32;
hash ^= hash.wrapping_shr(16);
hash = hash.wrapping_mul(0x85ebca6b);
hash ^= hash.wrapping_shr(13);
hash = hash.wrapping_mul(0xc2b2ae35);
⋮----
const fn pre_mix(buf: [u8; 4]) -> u32 {
⋮----
.wrapping_mul(0xcc9e2d51)
.rotate_left(15)
.wrapping_mul(0x1b873593)

================
File: platform-tools-sdk/gen-headers/Cargo.toml
================
[package]
name = "gen-headers"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[[bin]]
name = "gen-headers"
path = "src/main.rs"

[features]
agave-unstable-api = []

[dependencies]
log = { workspace = true, features = ["std"] }
regex = { workspace = true }

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/alt_bn128_compression.inc
================
#pragma once
/**
 * @brief Solana bn128 elliptic curve compression and decompression
**/

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Output length for the g1 compress operation.
 */
#define ALT_BN128_COMPRESSION_G1_COMPRESS_OUTPUT_LEN 32

/**
 * Output length for the g1 decompress operation.
 */
#define ALT_BN128_COMPRESSION_G1_DECOMPRESS_OUTPUT_LEN 64

/**
 * Output length for the g1 compress operation.
 */
#define ALT_BN128_COMPRESSION_G2_COMPRESS_OUTPUT_LEN 64

/**
 * Output length for the g2 decompress operation.
 */
#define ALT_BN128_COMPRESSION_G2_DECOMPRESS_OUTPUT_LEN 128

/**
 * G1 compression operation.
 */
#define ALT_BN128_G1_COMPRESS 0

/**
 * G1 decompression operation.
 */
#define ALT_BN128_G1_DECOMPRESS 1

/**
 * G2 compression operation.
 */
#define ALT_BN128_G2_COMPRESS 2

/**
 * G2 decompression operation.
 */
#define ALT_BN128_G2_DECOMPRESS 3

/**
 * Compression of alt_bn128 g1 and g2 points
 *
 * @param op ...
 * @param input ...
 * @param input_size ...
 * @param result 64 byte array to hold the result. ...
 * @return 0 if executed successfully
 */
@SYSCALL uint64_t sol_alt_bn128_compression(
        const uint64_t op,
        const uint8_t *input,
        const uint64_t input_size,
        uint8_t *result
);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/alt_bn128.inc
================
#pragma once
/**
 * @brief Solana bn128 elliptic curve addition, multiplication, and pairing
**/

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Output length for the add operation.
 */
#define ALT_BN128_ADDITION_OUTPUT_LEN 64

/**
 * Output length for the multiplication operation.
 */
#define ALT_BN128_MULTIPLICATION_OUTPUT_LEN 64

/**
 * Output length for pairing operation.
 */
#define ALT_BN128_PAIRING_OUTPUT_LEN 32

/**
 * Add operation.
 */
#define ALT_BN128_ADD 0

/**
 * Subtraction operation.
 */
#define ALT_BN128_SUB 1

/**
 * Multiplication operation.
 */
#define ALT_BN128_MUL 2

/**
 * Pairing operation.
 */
#define ALT_BN128_PAIRING 3

/**
 * Addition on elliptic curves alt_bn128
 *
 * @param group_op ...
 * @param input ...
 * @param input_size ...
 * @param result 64 byte array to hold the result. ...
 * @return 0 if executed successfully
 */
@SYSCALL uint64_t sol_alt_bn128_group_op(const uint64_t, const uint8_t *, const uint64_t, uint8_t *);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/assert.inc
================
#pragma once
/**
 * @brief Solana assert and panic utilities
 */

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif


/**
 * Panics
 *
 * Prints the line number where the panic occurred and then causes
 * the SBF VM to immediately halt execution. No accounts' data are updated
 */
@SYSCALL void sol_panic_(const char *, uint64_t, uint64_t, uint64_t);
#define sol_panic() sol_panic_(__FILE__, sizeof(__FILE__), __LINE__, 0)

/**
 * Asserts
 */
#define sol_assert(expr)  \
if (!(expr)) {          \
  sol_panic(); \
}

#ifdef SOL_TEST
/**
 * Stub functions when building tests
 */
#include <stdio.h>
#include <stdlib.h>

void sol_panic_(const char *file, uint64_t len, uint64_t line, uint64_t column) {
  printf("Panic in %s at %d:%d\n", file, line, column);
  abort();
}
#endif

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/big_mod_exp.inc
================
#pragma once
/**
 * @brief Solana big_mod_exp system call
**/

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Big integer modular exponentiation
 *
 * @param bytes Pointer to BigModExpParam struct
 * @param result 32 byte array to hold the result
 * @return 0 if executed successfully
 */
@SYSCALL uint64_t sol_big_mod_exp(const uint8_t *, uint8_t *);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/blake3.inc
================
#pragma once
/**
 * @brief Solana Blake3 system call
 */

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Length of a Blake3 hash result
 */
#define BLAKE3_RESULT_LENGTH 32

/**
 * Blake3
 *
 * @param bytes Array of byte arrays
 * @param bytes_len Number of byte arrays
 * @param result 32 byte array to hold the result
 */
@SYSCALL uint64_t sol_blake3(const SolBytes *, int, const uint8_t *);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/compute_units.inc
================
#pragma once
/**
 * @brief Solana logging utilities
 */

#include <sol/types.h>
#include <sol/string.h>
#include <sol/entrypoint.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Prints a string to stdout
 */
@SYSCALL uint64_t sol_remaining_compute_units();

#ifdef SOL_TEST
/**
 * Stub functions when building tests
 */

uint64_t sol_remaining_compute_units() {
  return UINT64_MAX;
}
#endif

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/cpi.inc
================
#pragma once
/**
 * @brief Solana Cross-Program Invocation
 */

#include <sol/types.h>
#include <sol/pubkey.h>
#include <sol/entrypoint.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Maximum CPI instruction data size. 10 KiB was chosen to ensure that CPI
 * instructions are not more limited than transaction instructions if the size
 * of transactions is doubled in the future.
 */
static const uint64_t MAX_CPI_INSTRUCTION_DATA_LEN = 10240;

/**
 * Maximum CPI instruction accounts. 255 was chosen to ensure that instruction
 * accounts are always within the maximum instruction account limit for SBF
 * program instructions.
 */
static const uint8_t MAX_CPI_INSTRUCTION_ACCOUNTS = 255;

/**
 * Maximum number of account info structs that can be used in a single CPI
 * invocation. A limit on account info structs is effectively the same as
 * limiting the number of unique accounts. 128 was chosen to match the max
 * number of locked accounts per transaction (MAX_TX_ACCOUNT_LOCKS).
 */
static const uint16_t MAX_CPI_ACCOUNT_INFOS = 128;

/**
 * Account Meta
 */
typedef struct {
  SolPubkey *pubkey; /** An account's public key */
  bool is_writable; /** True if the `pubkey` can be loaded as a read-write account */
  bool is_signer; /** True if an Instruction requires a Transaction signature matching `pubkey` */
} SolAccountMeta;

/**
 * Instruction
 */
typedef struct {
  SolPubkey *program_id; /** Pubkey of the instruction processor that executes this instruction */
  SolAccountMeta *accounts; /** Metadata for what accounts should be passed to the instruction processor */
  uint64_t account_len; /** Number of SolAccountMetas */
  uint8_t *data; /** Opaque data passed to the instruction processor */
  uint64_t data_len; /** Length of the data in bytes */
} SolInstruction;

/**
 * Internal cross-program invocation function
 */
@SYSCALL uint64_t sol_invoke_signed_c(
  const SolInstruction *,
  const SolAccountInfo *,
  int,
  const SolSignerSeeds *,
  int
);

/**
 * Invoke another program and sign for some of the keys
 *
 * @param instruction Instruction to process
 * @param account_infos Accounts used by instruction
 * @param account_infos_len Length of account_infos array
 * @param seeds Seed bytes used to sign program accounts
 * @param seeds_len Length of the seeds array
 */
static uint64_t sol_invoke_signed(
    const SolInstruction *instruction,
    const SolAccountInfo *account_infos,
    int account_infos_len,
    const SolSignerSeeds *signers_seeds,
    int signers_seeds_len
) {
  return sol_invoke_signed_c(
    instruction,
    account_infos,
    account_infos_len,
    signers_seeds,
    signers_seeds_len
  );
}
/**
 * Invoke another program
 *
 * @param instruction Instruction to process
 * @param account_infos Accounts used by instruction
 * @param account_infos_len Length of account_infos array
*/
static uint64_t sol_invoke(
    const SolInstruction *instruction,
    const SolAccountInfo *account_infos,
    int account_infos_len
) {
  const SolSignerSeeds signers_seeds[] = {{}};
  return sol_invoke_signed(
    instruction,
    account_infos,
    account_infos_len,
    signers_seeds,
    0
  );
}

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/keccak.inc
================
#pragma once
/**
 * @brief Solana keccak system call
**/

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Length of a Keccak hash result
 */
#define KECCAK_RESULT_LENGTH 32

/**
 * Keccak
 *
 * @param bytes Array of byte arrays
 * @param bytes_len Number of byte arrays
 * @param result 32 byte array to hold the result
 */
@SYSCALL uint64_t sol_keccak256(const SolBytes *, int, uint8_t *);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/last_restart_slot.inc
================
#pragma once
/**
 * @brief Solana Last Restart Slot system call
 */

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Get Last Restart Slot
 */
@SYSCALL u64 sol_get_last_restart_slot(uint8_t *result);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/log.inc
================
#pragma once
/**
 * @brief Solana logging utilities
 */

#include <sol/types.h>
#include <sol/string.h>
#include <sol/entrypoint.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Prints a string to stdout
 */
@SYSCALL void sol_log_(const char *, uint64_t);
#define sol_log(message) sol_log_(message, sol_strlen(message))

/**
 * Prints a 64 bit values represented in hexadecimal to stdout
 */
@SYSCALL void sol_log_64_(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
#define sol_log_64 sol_log_64_

/**
 * Prints the current compute unit consumption to stdout
 */
@SYSCALL void sol_log_compute_units_();
#define sol_log_compute_units() sol_log_compute_units_()

/**
 * Prints the hexadecimal representation of an array
 *
 * @param array The array to print
 */
static void sol_log_array(const uint8_t *array, int len) {
  for (int j = 0; j < len; j++) {
    sol_log_64(0, 0, 0, j, array[j]);
  }
}

/**
 * Print the base64 representation of some arrays.
 */
@SYSCALL void sol_log_data(SolBytes *, uint64_t);

/**
 * Prints the program's input parameters
 *
 * @param params Pointer to a SolParameters structure
 */
static void sol_log_params(const SolParameters *params) {
  sol_log("- Program identifier:");
  sol_log_pubkey(params->program_id);

  sol_log("- Number of KeyedAccounts");
  sol_log_64(0, 0, 0, 0, params->ka_num);
  for (int i = 0; i < params->ka_num; i++) {
    sol_log("  - Is signer");
    sol_log_64(0, 0, 0, 0, params->ka[i].is_signer);
    sol_log("  - Is writable");
    sol_log_64(0, 0, 0, 0, params->ka[i].is_writable);
    sol_log("  - Key");
    sol_log_pubkey(params->ka[i].key);
    sol_log("  - Lamports");
    sol_log_64(0, 0, 0, 0, *params->ka[i].lamports);
    sol_log("  - data");
    sol_log_array(params->ka[i].data, params->ka[i].data_len);
    sol_log("  - Owner");
    sol_log_pubkey(params->ka[i].owner);
    sol_log("  - Executable");
    sol_log_64(0, 0, 0, 0, params->ka[i].executable);
    sol_log("  - Rent Epoch");
    sol_log_64(0, 0, 0, 0, params->ka[i].rent_epoch);
  }
  sol_log("- Instruction data\0");
  sol_log_array(params->data, params->data_len);
}

#ifdef SOL_TEST
/**
 * Stub functions when building tests
 */
#include <stdio.h>

void sol_log_(const char *s, uint64_t len) {
  printf("Program log: %s\n", s);
}
void sol_log_64(uint64_t arg1, uint64_t arg2, uint64_t arg3, uint64_t arg4, uint64_t arg5) {
  printf("Program log: %llu, %llu, %llu, %llu, %llu\n", arg1, arg2, arg3, arg4, arg5);
}

void sol_log_compute_units_() {
  printf("Program consumption: __ units remaining\n");
}
#endif

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/poseidon.inc
================
#pragma once
/**
 * @brief Solana poseidon system call
**/

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Length of a Poseidon hash result
 */
#define POSEIDON_RESULT_LENGTH 32

/**
 * Configuration using the Barreto–Naehrig curve with an embedding degree of
 * 12, defined over a 254-bit prime field.
 *
 * Configuration Details:
 * - S-Box: x^5
 * - Width: 2 <= t <= 13
 * - Inputs: 1 <= n <= 12
 * - Full rounds: 8
 * - Partial rounds: Depending on width: [56, 57, 56, 60, 60, 63, 64, 63,
 *   60, 66, 60, 65]
 */
#define POSEIDON_PARAMETERS_BN254_X5 0

/**
 * Big-endian inputs and output
 */
#define POSEIDON_ENDIANNESS_BIG_ENDIAN 0

/**
 * Little-endian inputs and output
 */
#define POSEIDON_ENDIANNESS_LITTLE_ENDIAN 1

/**
 * Poseidon
 *
 * @param parameters Configuration parameters for the hash function
 * @param endianness Endianness of inputs and result
 * @param bytes Array of byte arrays
 * @param bytes_len Number of byte arrays
 * @param result 32 byte array to hold the result
 */
@SYSCALL uint64_t sol_poseidon(
  const uint64_t parameters,
  const uint64_t endianness,
  const SolBytes *bytes,
  const uint64_t bytes_len,
  uint8_t *result
);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/pubkey.inc
================
#pragma once
/**
 * @brief Solana Public key
 */

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Size of Public key in bytes
 */
#define SIZE_PUBKEY 32

/**
 * Public key
 */
typedef struct {
  uint8_t x[SIZE_PUBKEY];
} SolPubkey;

/**
 * Prints the hexadecimal representation of a public key
 *
 * @param key The public key to print
 */
@SYSCALL void sol_log_pubkey(const SolPubkey *);

/**
 * Compares two public keys
 *
 * @param one First public key
 * @param two Second public key
 * @return true if the same
 */
static bool SolPubkey_same(const SolPubkey *one, const SolPubkey *two) {
  for (int i = 0; i < sizeof(*one); i++) {
    if (one->x[i] != two->x[i]) {
      return false;
    }
  }
  return true;
}

/**
 * Seed used to create a program address or passed to sol_invoke_signed
 */
typedef struct {
  const uint8_t *addr; /** Seed bytes */
  uint64_t len; /** Length of the seed bytes */
} SolSignerSeed;

/**
 * Seeds used by a signer to create a program address or passed to
 * sol_invoke_signed
 */
typedef struct {
  const SolSignerSeed *addr; /** An array of a signer's seeds */
  uint64_t len; /** Number of seeds */
} SolSignerSeeds;

/**
 * Create a program address
 *
 * @param seeds Seed bytes used to sign program accounts
 * @param seeds_len Length of the seeds array
 * @param program_id Program id of the signer
 * @param program_address Program address created, filled on return
 */
@SYSCALL uint64_t sol_create_program_address(const SolSignerSeed *, int, const SolPubkey *, SolPubkey *);

/**
 * Try to find a program address and return corresponding bump seed
 *
 * @param seeds Seed bytes used to sign program accounts
 * @param seeds_len Length of the seeds array
 * @param program_id Program id of the signer
 * @param program_address Program address created, filled on return
 * @param bump_seed Bump seed required to create a valid program address
 */
@SYSCALL uint64_t sol_try_find_program_address(const SolSignerSeed *, int, const SolPubkey *, SolPubkey *, uint8_t *);

#ifdef SOL_TEST
/**
 * Stub functions when building tests
 */
#include <stdio.h>

void sol_log_pubkey(
  const SolPubkey *pubkey
) {
  printf("Program log: ");
  for (int i = 0; i < SIZE_PUBKEY; i++) {
    printf("%02 ", pubkey->x[i]);
  }
  printf("\n");
}

#endif

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/return_data.inc
================
#pragma once
/**
 * @brief Solana return data system calls
**/

#include <sol/types.h>
#include <sol/pubkey.h>

#ifdef __cplusplus
extern "C"
{
#endif

/**
 * Maximum size of return data
 */
#define MAX_RETURN_DATA 1024

/**
 * Set the return data
 *
 * @param bytes byte array to set
 * @param bytes_len length of byte array. This may not exceed MAX_RETURN_DATA.
 */
@SYSCALL void sol_set_return_data(const uint8_t *, uint64_t);

/**
 * Get the return data
 *
 * @param bytes byte buffer
 * @param bytes_len maximum length of buffer
 * @param program_id the program_id which set the return data. Only set if there was some return data (the function returns non-zero).
 * @param result length of return data (may exceed bytes_len if the return data is longer)
 */
@SYSCALL uint64_t sol_get_return_data(uint8_t *, uint64_t, SolPubkey *);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/secp256k1.inc
================
#pragma once
/**
 * @brief Solana secp256k1 system call
 */

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/** Length of a secp256k1 recover input hash */
#define SECP256K1_RECOVER_HASH_LENGTH 32
/** Length of a secp256k1 input signature */
#define SECP256K1_RECOVER_SIGNATURE_LENGTH 64
/** Length of a secp256k1 recover result */
#define SECP256K1_RECOVER_RESULT_LENGTH 64

/** The hash provided to a sol_secp256k1_recover is invalid */
#define SECP256K1_RECOVER_ERROR_INVALID_HASH 1
/** The recovery_id provided to a sol_secp256k1_recover is invalid */
#define SECP256K1_RECOVER_ERROR_INVALID_RECOVERY_ID 2
/** The signature provided to a sol_secp256k1_recover is invalid */
#define SECP256K1_RECOVER_ERROR_INVALID_SIGNATURE 3

/**
 * Recover public key from a signed message.
 *
 * @param hash Hashed message
 * @param recovery_id Tag used for public key recovery from signatures. Can be 0 or 1
 * @param signature An ECDSA signature
 * @param result 64 byte array to hold the result. A recovered public key
 * @return 0 if executed successfully
 */
@SYSCALL uint64_t sol_secp256k1_recover(const uint8_t *, uint64_t, const uint8_t *, uint8_t *);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/inc/sha.inc
================
#pragma once
/**
 * @brief Solana sha system call
 */

#include <sol/types.h>

#ifdef __cplusplus
extern "C" {
#endif

/**
 * Length of a sha256 hash result
 */
#define SHA256_RESULT_LENGTH 32

/**
 * Sha256
 *
 * @param bytes Array of byte arrays
 * @param bytes_len Number of byte arrays
 * @param result 32 byte array to hold the result
 */
@SYSCALL uint64_t sol_sha256(const SolBytes *, int, uint8_t *);

#ifdef __cplusplus
}
#endif

/**@}*/

================
File: platform-tools-sdk/sbf/c/inc/sol/alt_bn128_compression.h
================
uint64_t sol_alt_bn128_compression(
⋮----
static uint64_t sol_alt_bn128_compression(

================
File: platform-tools-sdk/sbf/c/inc/sol/alt_bn128.h
================
uint64_t sol_alt_bn128_group_op(const uint64_t, const uint8_t *, const uint64_t, uint8_t *);
⋮----
static uint64_t sol_alt_bn128_group_op(const uint64_t arg1, const uint8_t * arg2, const uint64_t arg3, uint8_t * arg4) {

================
File: platform-tools-sdk/sbf/c/inc/sol/assert.h
================
void sol_panic_(const char *, uint64_t, uint64_t, uint64_t);
⋮----
static void sol_panic_(const char * arg1, uint64_t arg2, uint64_t arg3, uint64_t arg4) {
⋮----
void sol_panic_(const char *file, uint64_t len, uint64_t line, uint64_t column) {

================
File: platform-tools-sdk/sbf/c/inc/sol/big_mod_exp.h
================
uint64_t sol_big_mod_exp(const uint8_t *, uint8_t *);
⋮----
static uint64_t sol_big_mod_exp(const uint8_t * arg1, uint8_t * arg2) {

================
File: platform-tools-sdk/sbf/c/inc/sol/blake3.h
================
uint64_t sol_blake3(const SolBytes *, int, const uint8_t *);
⋮----
static uint64_t sol_blake3(const SolBytes * arg1, int arg2, const uint8_t * arg3) {

================
File: platform-tools-sdk/sbf/c/inc/sol/compute_units.h
================
uint64_t sol_remaining_compute_units();
⋮----
static uint64_t sol_remaining_compute_units() {
⋮----
uint64_t sol_remaining_compute_units() {

================
File: platform-tools-sdk/sbf/c/inc/sol/constants.h
================


================
File: platform-tools-sdk/sbf/c/inc/sol/cpi.h
================
} SolAccountMeta;
⋮----
} SolInstruction;
⋮----
uint64_t sol_invoke_signed_c(
⋮----
static uint64_t sol_invoke_signed_c(
⋮----
static uint64_t sol_invoke_signed(
⋮----
static uint64_t sol_invoke(

================
File: platform-tools-sdk/sbf/c/inc/sol/deserialize_deprecated.h
================
static bool sol_deserialize_deprecated(

================
File: platform-tools-sdk/sbf/c/inc/sol/deserialize.h
================
static bool sol_deserialize(

================
File: platform-tools-sdk/sbf/c/inc/sol/entrypoint.h
================
} SolAccountInfo;
⋮----
} SolParameters;
uint64_t entrypoint(const uint8_t *input);

================
File: platform-tools-sdk/sbf/c/inc/sol/keccak.h
================
uint64_t sol_keccak256(const SolBytes *, int, uint8_t *);
⋮----
static uint64_t sol_keccak256(const SolBytes * arg1, int arg2, uint8_t * arg3) {

================
File: platform-tools-sdk/sbf/c/inc/sol/last_restart_slot.h
================
u64 sol_get_last_restart_slot(uint8_t *result);
⋮----
static u64 sol_get_last_restart_slot(uint8_t *result arg1) {

================
File: platform-tools-sdk/sbf/c/inc/sol/log.h
================
void sol_log_(const char *, uint64_t);
⋮----
static void sol_log_(const char * arg1, uint64_t arg2) {
⋮----
void sol_log_64_(uint64_t, uint64_t, uint64_t, uint64_t, uint64_t);
⋮----
static void sol_log_64_(uint64_t arg1, uint64_t arg2, uint64_t arg3, uint64_t arg4, uint64_t arg5) {
⋮----
void sol_log_compute_units_();
⋮----
static void sol_log_compute_units_() {
⋮----
static void sol_log_array(const uint8_t *array, int len) {
⋮----
void sol_log_data(SolBytes *, uint64_t);
⋮----
static void sol_log_data(SolBytes * arg1, uint64_t arg2) {
⋮----
static void sol_log_params(const SolParameters *params) {
⋮----
void sol_log_(const char *s, uint64_t len) {
⋮----
void sol_log_64(uint64_t arg1, uint64_t arg2, uint64_t arg3, uint64_t arg4, uint64_t arg5) {
⋮----
void sol_log_compute_units_() {

================
File: platform-tools-sdk/sbf/c/inc/sol/poseidon.h
================
uint64_t sol_poseidon(
⋮----
static uint64_t sol_poseidon(

================
File: platform-tools-sdk/sbf/c/inc/sol/pubkey.h
================
} SolPubkey;
⋮----
void sol_log_pubkey(const SolPubkey *);
⋮----
static void sol_log_pubkey(const SolPubkey * arg1) {
⋮----
static bool SolPubkey_same(const SolPubkey *one, const SolPubkey *two) {
⋮----
} SolSignerSeed;
⋮----
} SolSignerSeeds;
⋮----
uint64_t sol_create_program_address(const SolSignerSeed *, int, const SolPubkey *, SolPubkey *);
⋮----
static uint64_t sol_create_program_address(const SolSignerSeed * arg1, int arg2, const SolPubkey * arg3, SolPubkey * arg4) {
⋮----
uint64_t sol_try_find_program_address(const SolSignerSeed *, int, const SolPubkey *, SolPubkey *, uint8_t *);
⋮----
static uint64_t sol_try_find_program_address(const SolSignerSeed * arg1, int arg2, const SolPubkey * arg3, SolPubkey * arg4, uint8_t * arg5) {
⋮----
void sol_log_pubkey(

================
File: platform-tools-sdk/sbf/c/inc/sol/return_data.h
================
void sol_set_return_data(const uint8_t *, uint64_t);
⋮----
static void sol_set_return_data(const uint8_t * arg1, uint64_t arg2) {
⋮----
uint64_t sol_get_return_data(uint8_t *, uint64_t, SolPubkey *);
⋮----
static uint64_t sol_get_return_data(uint8_t * arg1, uint64_t arg2, SolPubkey * arg3) {

================
File: platform-tools-sdk/sbf/c/inc/sol/secp256k1.h
================
uint64_t sol_secp256k1_recover(const uint8_t *, uint64_t, const uint8_t *, uint8_t *);
⋮----
static uint64_t sol_secp256k1_recover(const uint8_t * arg1, uint64_t arg2, const uint8_t * arg3, uint8_t * arg4) {

================
File: platform-tools-sdk/sbf/c/inc/sol/sha.h
================
uint64_t sol_sha256(const SolBytes *, int, uint8_t *);
⋮----
static uint64_t sol_sha256(const SolBytes * arg1, int arg2, uint8_t * arg3) {

================
File: platform-tools-sdk/sbf/c/inc/sol/string.h
================
static void *sol_memcpy(void *dst, const void *src, int len) {
⋮----
static int sol_memcmp(const void *s1, const void *s2, int n) {
⋮----
static void *sol_memset(void *b, int c, size_t len) {
⋮----
static size_t sol_strlen(const char *s) {
⋮----
static void *sol_calloc(size_t nitems, size_t size) {
⋮----
static void sol_free(void *ptr) {

================
File: platform-tools-sdk/sbf/c/inc/sol/types.h
================
} SolBytes;

================
File: platform-tools-sdk/sbf/c/inc/sys/param.h
================


================
File: platform-tools-sdk/sbf/c/inc/deserialize_deprecated.h
================


================
File: platform-tools-sdk/sbf/c/inc/solana_sdk.h
================


================
File: platform-tools-sdk/sbf/c/inc/stdio.h
================
int printf(const char * restrictformat, ... );

================
File: platform-tools-sdk/sbf/c/inc/stdlib.h
================


================
File: platform-tools-sdk/sbf/c/inc/string.h
================


================
File: platform-tools-sdk/sbf/c/inc/wchar.h
================


================
File: platform-tools-sdk/sbf/c/README.md
================
## Development

### Quick start
To get started create a `makefile` containing:
```make
include path/to/sbf.mk
```
and `src/program.c` containing:
```c
#include <solana_sdk.h>

extern uint64_t entrypoint(const uint8_t *input) {
  SolAccountInfo ka[1];
  SolParameters params = (SolParameters) { .ka = ka };

  if (!sol_deserialize(input, &params, SOL_ARRAY_SIZE(ka))) {
    return ERROR_INVALID_ARGUMENT;
  }
  return SUCCESS;
}
```

Then run `make` to build `out/program.o`.
Run `make help` for more details.

### Unit tests
Built-in support for unit testing is provided by the
[Criterion](https://criterion.readthedocs.io/en/master/index.html) test framework.
To get started create the file `test/example.c` containing:
```c
#include <criterion/criterion.h>
#include "../src/program.c"

Test(test_suite_name, test_case_name) {
  cr_assert(true);
}
```
Then run `make test`.

### Limitations
* Programs must be fully contained within a single .c file
* No libc is available but `solana_sdk.h` provides a minimal set of primitives

================
File: platform-tools-sdk/sbf/c/sbf.ld
================
PHDRS
{
  text PT_LOAD  ;
  rodata PT_LOAD ;
  data PT_LOAD ;
  dynamic PT_DYNAMIC ;
}

SECTIONS
{
  . = SIZEOF_HEADERS;
  .text : { *(.text*) } :text
  .rodata : { *(.rodata*) } :rodata
  .data.rel.ro : { *(.data.rel.ro*) } :rodata
  .dynamic : { *(.dynamic) } :dynamic
  .dynsym : { *(.dynsym) } :data
  .dynstr : { *(.dynstr) } :data
  .rel.dyn : { *(.rel.dyn) } :data
  /DISCARD/ : {
      *(.eh_frame*)
      *(.gnu.hash*)
      *(.hash*)
    }
}

================
File: platform-tools-sdk/sbf/c/sbf.mk
================
LOCAL_PATH := $(dir $(lastword $(MAKEFILE_LIST)))
INSTALL_SH := $(abspath $(LOCAL_PATH)/../scripts/install.sh)

all:
.PHONY: help all clean

ifneq ($(V),1)
_@ :=@
endif

INC_DIRS ?=
SRC_DIR ?= ./src
TEST_PREFIX ?= test_
OUT_DIR ?= ./out
SBPF_CPU ?= v0
OS := $(shell uname)

LLVM_DIR = $(LOCAL_PATH)../dependencies/platform-tools/llvm
LLVM_SYSTEM_INC_DIRS := $(LLVM_DIR)/lib/clang/18/include

ifeq "$(SBPF_CPU)" "v1"
TARGET_NAME := sbpfv1
else ifeq "$(SBPF_CPU)" "v2"
TARGET_NAME := sbpfv2
else
TARGET_NAME := sbpf
endif

LLVM_TARGET := $(TARGET_NAME)-solana-solana

COMPILER_RT_DIR = $(LOCAL_PATH)../dependencies/platform-tools/rust/lib/rustlib/$(LLVM_TARGET)/lib
STD_INC_DIRS := $(LLVM_DIR)/$(TARGET_NAME)/include
STD_LIB_DIRS := $(LLVM_DIR)/lib/$(TARGET_NAME)
ifdef LLVM_DIR
CC := $(LLVM_DIR)/bin/clang
CXX := $(LLVM_DIR)/bin/clang++
LLD := $(LLVM_DIR)/bin/ld.lld
OBJ_DUMP := $(LLVM_DIR)/bin/llvm-objdump
READ_ELF := $(LLVM_DIR)/bin/llvm-readelf
endif

SYSTEM_INC_DIRS := \
  $(LOCAL_PATH)inc \
  $(LLVM_SYSTEM_INC_DIRS) \

C_FLAGS := \
  -Werror \
  -O2 \
  -fno-builtin \
  -std=c17 \
  $(addprefix -isystem,$(SYSTEM_INC_DIRS)) \
  $(addprefix -I,$(STD_INC_DIRS)) \
  $(addprefix -I,$(INC_DIRS)) \

ifeq ($(SOL_SBPFV3),1)
C_FLAGS := \
  $(C_FLAGS) \
  -DSOL_SBPFV3=1
endif

CXX_FLAGS := \
  $(C_FLAGS) \
  -std=c++17 \

SBF_C_FLAGS := \
  $(C_FLAGS) \
  -target sbf \
  -fPIC

SBF_CXX_FLAGS := \
  $(CXX_FLAGS) \
  -target sbf \
  -fPIC \
  -fomit-frame-pointer \
  -fno-exceptions \
  -fno-asynchronous-unwind-tables \
  -fno-unwind-tables

ifeq "$(SBPF_CPU)" "v1"
SBF_C_FLAGS := \
  $(SBF_C_FLAGS) \
  -mcpu=v1

SBF_CXX_FLAGS := \
  $(SBF_CXX_FLAGS) \
  -mcpu=v1
else ifeq "$(SBPF_CPU)" "v2"
SBF_C_FLAGS := \
  $(SBF_C_FLAGS) \
  -mcpu=v2

SBF_CXX_FLAGS := \
  $(SBF_CXX_FLAGS) \
  -mcpu=v2
endif

SBF_LLD_FLAGS := \
  -z notext \
  -shared \
  --Bdynamic \
  $(LOCAL_PATH)sbf.ld \
  --entry entrypoint \
  -L $(STD_LIB_DIRS) \
  -z max-page-size=4096 \
  -lc \

OBJ_DUMP_FLAGS := \
  --source \
  --disassemble \

READ_ELF_FLAGS := \
  --all \

TESTFRAMEWORK_RPATH := $(abspath $(LOCAL_PATH)../dependencies/criterion/lib)
TESTFRAMEWORK_FLAGS := \
  -DSOL_TEST \
  -isystem $(LOCAL_PATH)../dependencies/criterion/include \
  -L $(LOCAL_PATH)../dependencies/criterion/lib \
  -rpath $(TESTFRAMEWORK_RPATH) \
  -lcriterion \

MACOS_ADJUST_TEST_DYLIB := \
$(if $(filter $(OS),Darwin),\
 $(_@)install_name_tool -change libcriterion.3.dylib $(TESTFRAMEWORK_RPATH)/libcriterion.3.dylib, \
 : \
)

TEST_C_FLAGS := \
  $(C_FLAGS) \
  $(TESTFRAMEWORK_FLAGS) \

TEST_CXX_FLAGS := \
  $(CXX_FLAGS) \
  $(TESTFRAMEWORK_FLAGS) \

help:
	@echo ''
	@echo 'Solana VM Program makefile'
	@echo ''
	@echo 'This makefile will build Solana Programs from C or C++ source files into ELFs'
	@echo ''
	@echo 'Assumptions:'
	@echo '  - Programs are located in the source directory: $(SRC_DIR)/<program name>'
	@echo '  - Programs are named by their directory name (eg. directory name:src/foo/ -> program name:foo)'
	@echo '  - Tests are located in their corresponding program directory and must being with "test_"'
	@echo '  - Output files will be placed in the directory: $(OUT_DIR)'
	@echo ''
	@echo 'User settings'
	@echo '  - The following setting are overridable on the command line, default values shown:'
	@echo '    - Show commands while building: V=1'
	@echo '      V=$(V)'
	@echo '    - List of include directories:'
	@echo '      INC_DIRS=$(INC_DIRS)'
	@echo '    - List of system include directories:'
	@echo '      SYSTEM_INC_DIRS=$(SYSTEM_INC_DIRS)'
	@echo '    - List of standard library include directories:'
	@echo '      STD_INC_DIRS=$(STD_INC_DIRS)'
	@echo '    - List of standard library archive directories:'
	@echo '      STD_LIB_DIRS=$(STD_LIB_DIRS)'
	@echo '    - Location of source directories:'
	@echo '      SRC_DIR=$(SRC_DIR)'
	@echo '    - Location to place output files:'
	@echo '      OUT_DIR=$(OUT_DIR)'
	@echo '    - Location of LLVM:'
	@echo '      LLVM_DIR=$(LLVM_DIR)'
	@echo '    - Version of SBPF (v0, v1 or v2):'
	@echo '      SBPF_CPU=$(SBPF_CPU)'
	@echo ''
	@echo 'Usage:'
	@echo '  - make help - This help message'
	@echo '  - make all - Build all the programs and tests, run the tests'
	@echo '  - make programs - Build all the programs'
	@echo '  - make tests - Build and run all tests'
	@echo '  - make dump_<program name> - Dump the contents of the program to stdout'
	@echo '  - make readelf_<program name> - Display information about the ELF binary'
	@echo '  - make <program name> - Build a single program by name'
	@echo '  - make <test name> - Build and run a single test by name'
	@echo ''
	@echo 'Available programs:'
	$(foreach name, $(PROGRAM_NAMES), @echo '  - $(name)'$(\n))
	@echo ''
	@echo 'Available tests:'
	$(foreach name, $(TEST_NAMES), @echo '  - $(name)'$(\n))
	@echo ''
	@echo 'Example:'
	@echo '  - Assuming a program named foo (src/foo/foo.c)'
	@echo '    - make foo'
	@echo '    - make dump_foo'
	@echo ''

define C_RULE
$1: $2
	@echo "[cc] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CC) $(SBF_C_FLAGS) -o $1 -c $2
endef

define CC_RULE
$1: $2
	@echo "[cxx] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CXX) $(SBF_CXX_FLAGS) -o $1 -c $2
endef

define D_RULE
$1: $2 $(LOCAL_PATH)/sbf.mk
	@echo "[GEN] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CC) -M -MT '$(basename $1).o' $(SBF_C_FLAGS) $2 | sed 's,\($(basename $1)\)\.o[ :]*,\1.o $1 : ,g' > $1
endef

define DXX_RULE
$1: $2 $(LOCAL_PATH)/sbf.mk
	@echo "[GEN] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CXX) -M -MT '$(basename $1).o' $(SBF_CXX_FLAGS) $2 | sed 's,\($(basename $1)\)\.o[ :]*,\1.o $1 : ,g' > $1
endef

define O_RULE
$1: $2
	@echo "[llc] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(LLC) $(SBF_LLC_FLAGS) -o $1 $2
endef

define SO_RULE
$1: $2
	@echo "[lld] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(LLD) $(SBF_LLD_FLAGS) -o $1 $2 $(COMPILER_RT_DIR)/libcompiler_builtins-*.rlib
ifeq (,$(wildcard $(subst .so,-keypair.json,$1)))
	$(_@)solana-keygen new --no-passphrase --silent -o $(subst .so,-keypair.json,$1)
endif
	@echo To deploy this program:
	@echo $$$$ solana program deploy $(abspath $1)
endef

define TEST_C_RULE
$1: $2
	@echo "[test cc] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CC) $(TEST_C_FLAGS) -o $1 $2
	$(_@)$(MACOS_ADJUST_TEST_DYLIB) $1
endef

define TEST_CC_RULE
$1: $2
	@echo "[test cxx] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CXX) $(TEST_CXX_FLAGS) -o $1 $2
	$(_@)$(MACOS_ADJUST_TEST_DYLIB) $1
endef

define TEST_D_RULE
$1: $2 $(LOCAL_PATH)/sbf.mk
	@echo "[GEN] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CC) -M -MT '$(basename $1)' $(TEST_C_FLAGS) $2 | sed 's,\($(basename $1)\)[ :]*,\1 $1 : ,g' > $1
endef

define TEST_DXX_RULE
$1: $2 $(LOCAL_PATH)/sbf.mk
	@echo "[GEN] $1 ($2)"
	$(_@)mkdir -p $(dir $1)
	$(_@)$(CXX) -M -MT '$(basename $1)' $(TEST_CXX_FLAGS) $2 | sed 's,\($(basename $1)\)[ :]*,\1 $1 : ,g' > $1
endef

define TEST_EXEC_RULE
$1: $2
	LD_LIBRARY_PATH=$(TESTFRAMEWORK_RPATH) \
	$2$(\n)
endef

.PHONY: $(INSTALL_SH)
$(INSTALL_SH):
	$(_@)$(INSTALL_SH)

PROGRAM_NAMES := $(notdir $(basename $(wildcard $(SRC_DIR)/*)))

define \n


endef

all: programs tests

$(foreach PROGRAM, $(PROGRAM_NAMES), \
  $(eval -include $(wildcard $(OUT_DIR)/$(PROGRAM)/*.d)) \
  \
  $(eval $(PROGRAM): %: $(addprefix $(OUT_DIR)/, %.so)) \
  $(eval $(PROGRAM)_SRCS := \
    $(addprefix $(SRC_DIR)/$(PROGRAM)/, \
    $(filter-out $(TEST_PREFIX)%,$(notdir $(wildcard $(SRC_DIR)/$(PROGRAM)/*.c $(SRC_DIR)/$(PROGRAM)/*.cc))))) \
  $(eval $(PROGRAM)_OBJS := $(subst $(SRC_DIR), $(OUT_DIR), \
    $(patsubst %.c,%.o, \
    $(patsubst %.cc,%.o,$($(PROGRAM)_SRCS))))) \
	$(eval $($(PROGRAM)_SRCS): $(INSTALL_SH)) \
  $(eval $(call SO_RULE,$(OUT_DIR)/$(PROGRAM).so,$($(PROGRAM)_OBJS))) \
  $(foreach _,$(filter %.c,$($(PROGRAM)_SRCS)), \
    $(eval $(call D_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.c=%.d)),$_)) \
    $(eval $(call C_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.c=%.o)),$_))) \
  $(foreach _,$(filter %.cc,$($(PROGRAM)_SRCS)), \
    $(eval $(call DXX_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.cc=%.d)),$_)) \
    $(eval $(call CC_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.cc=%.o)),$_))) \
  \
  $(eval TESTS := $(notdir $(basename $(wildcard $(SRC_DIR)/$(PROGRAM)/$(TEST_PREFIX)*.c)))) \
  $(eval $(TESTS) : %: $(addprefix $(OUT_DIR)/$(PROGRAM)/, %)) \
  $(eval TEST_NAMES := $(TEST_NAMES) $(TESTS)) \
  $(foreach TEST, $(TESTS), \
    $(eval $(TEST)_SRCS := \
      $(addprefix $(SRC_DIR)/$(PROGRAM)/, \
      $(notdir $(wildcard $(SRC_DIR)/$(PROGRAM)/$(TEST).c $(SRC_DIR)/$(PROGRAM)/$(TEST).cc)))) \
		$(eval $($(TEST)_SRCS): $(INSTALL_SH)) \
    $(foreach _,$(filter %.c,$($(TEST)_SRCS)), \
      $(eval $(call TEST_D_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.c=%.d)),$_)) \
      $(eval $(call TEST_C_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.c=%)),$_))) \
    $(foreach _,$(filter %.cc, $($(TEST)_SRCS)), \
      $(eval $(call TEST_DXX_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.cc=%.d)),$_)) \
      $(eval $(call TEST_CC_RULE,$(subst $(SRC_DIR),$(OUT_DIR),$(_:%.cc=%)),$_))) \
    $(eval $(call TEST_EXEC_RULE,$(TEST),$(addprefix $(OUT_DIR)/$(PROGRAM)/, $(TEST)))) \
   ) \
)

.PHONY: $(PROGRAM_NAMES)
programs: $(PROGRAM_NAMES)

.PHONY: $(TEST_NAMES)
tests: $(TEST_NAMES)

dump_%: %
	$(_@)$(OBJ_DUMP) $(OBJ_DUMP_FLAGS) $(addprefix $(OUT_DIR)/, $(addsuffix .so, $<))

readelf_%: %
	$(_@)$(READ_ELF) $(READ_ELF_FLAGS) $(addprefix $(OUT_DIR)/, $(addsuffix .so, $<))

clean:
	rm -rf $(OUT_DIR)

================
File: platform-tools-sdk/sbf/scripts/dump.sh
================
sbf_sdk=$(cd "$(dirname "$0")/.." && pwd)
# shellcheck source=platform-tools-sdk/sbf/env.sh
source "$sbf_sdk"/env.sh
so=$1
dump=$2
if [[ -z $so ]] || [[ -z $dump ]]; then
  echo "Usage: $0 sbf-program.so dump.txt" >&2
  exit 1
fi
if [[ ! -r $so ]]; then
  echo "Error: File not found or readable: $so" >&2
  exit 1
fi
if ! command -v rustfilt > /dev/null; then
  echo "Error: rustfilt not found.  It can be installed by running: cargo install rustfilt" >&2
  exit 1
fi
set -e
out_dir=$(dirname "$dump")
if [[ ! -d $out_dir ]]; then
  mkdir -p "$out_dir"
fi
dump_mangled=$dump.mangled
(
  set -ex
  ls -la "$so" > "$dump_mangled"
  "$sbf_sdk"/dependencies/platform-tools/llvm/bin/llvm-readelf -aW "$so" >>"$dump_mangled"
  "$OBJDUMP" --print-imm-hex --source --disassemble "$so" >> "$dump_mangled"
  sed s/://g < "$dump_mangled" | rustfilt > "$dump"
)
rm -f "$dump_mangled"
if [[ ! -f "$dump" ]]; then
  echo "Error: Failed to create $dump" >&2
  exit 1
fi
echo >&2
echo "Wrote $dump" >&2

================
File: platform-tools-sdk/sbf/scripts/install.sh
================
mkdir -p "$(dirname "$0")"/../dependencies
cd "$(dirname "$0")"/../dependencies
unameOut="$(uname -s)"
case "${unameOut}" in
  Linux*)
    criterion_suffix=
    machine=linux;;
  Darwin*)
    criterion_suffix=
    machine=osx;;
  MINGW*)
    criterion_suffix=-mingw
    machine=windows;;
  *)
    criterion_suffix=
    machine=linux
esac
unameOut="$(uname -m)"
case "${unameOut}" in
  arm64*)
    arch=aarch64;;
  *)
    arch=x86_64
esac
download() {
  declare url="$1/$2/$3"
  declare filename=$3
  declare wget_args=(
    "$url" -O "$filename"
    "--progress=dot:giga"
    "--retry-connrefused"
    "--read-timeout=30"
  )
  declare curl_args=(
    -L "$url" -o "$filename"
  )
  if hash wget 2>/dev/null; then
    wget_or_curl="wget ${wget_args[*]}"
  elif hash curl 2>/dev/null; then
    wget_or_curl="curl ${curl_args[*]}"
  else
    echo "Error: Neither curl nor wget were found" >&2
    return 1
  fi
  set -x
  if $wget_or_curl; then
    tar --strip-components 1 -jxf "$filename" || return 1
    { set +x; } 2>/dev/null
    rm -rf "$filename"
    return 0
  fi
  return 1
}
get() {
  declare version=$1
  declare dirname=$2
  declare job=$3
  declare cache_root=~/.cache/solana
  declare cache_dirname="$cache_root/$version/$dirname"
  declare cache_partial_dirname="$cache_dirname"_partial
  if [[ -r $cache_dirname ]]; then
    ln -sf "$cache_dirname" "$dirname" || return 1
    return 0
  fi
  rm -rf "$cache_partial_dirname" || return 1
  mkdir -p "$cache_partial_dirname" || return 1
  pushd "$cache_partial_dirname"
  if $job; then
    popd
    mv "$cache_partial_dirname" "$cache_dirname" || return 1
    ln -sf "$cache_dirname" "$dirname" || return 1
    return 0
  fi
  popd
  return 1
}
if [[ $machine == "linux" ]]; then
  version=v2.3.3
else
  version=v2.3.2
fi
if [[ ! -e criterion-$version.md || ! -e criterion ]]; then
  (
    set -e
    rm -rf criterion*
    job="download \
           https://github.com/Snaipe/Criterion/releases/download \
           $version \
           criterion-$version-$machine$criterion_suffix-x86_64.tar.bz2 \
           criterion"
    get $version criterion "$job"
  )
  exitcode=$?
  if [[ $exitcode -ne 0 ]]; then
    exit 1
  fi
  touch criterion-$version.md
fi
tools_version=v1.52
rust_version=1.89.0
if [[ ! -e platform-tools-$tools_version.md || ! -e platform-tools ]]; then
  (
    set -e
    rm -rf platform-tools*
    job="download \
           https://github.com/anza-xyz/platform-tools/releases/download \
           $tools_version \
           platform-tools-${machine}-${arch}.tar.bz2 \
           platform-tools"
    get $tools_version platform-tools "$job"
  )
  exitcode=$?
  if [[ $exitcode -ne 0 ]]; then
    exit 1
  fi
  touch platform-tools-$tools_version.md
  set -ex
  ./platform-tools/rust/bin/rustc --version
  ./platform-tools/rust/bin/rustc --print sysroot
  if [[ "${BASH_VERSINFO[0]}" -lt 4 ]]; then
    toolchains=()
    while IFS='' read -r line; do toolchains+=("$line"); done < <(rustup toolchain list)
  else
    mapfile -t toolchains < <(rustup toolchain list)
  fi
  set +e
  for item in "${toolchains[@]}"
  do
    if [[ $item == *"solana"* ]]; then
      rustup toolchain uninstall "$item"
    fi
  done
  set -e
  rustup toolchain link "$rust_version-sbpf-solana-$tools_version" platform-tools/rust
fi
exit 0

================
File: platform-tools-sdk/sbf/scripts/objcopy.sh
================
sbf_sdk=$(cd "$(dirname "$0")/.." && pwd)
# shellcheck source=platform-tools-sdk/sbf/env.sh
source "$sbf_sdk"/env.sh
exec "$sbf_sdk"/dependencies/platform-tools/llvm/bin/llvm-objcopy "$@"

================
File: platform-tools-sdk/sbf/scripts/package.sh
================
set -ex
cd "$(dirname "$0")"/../../..
echo --- Creating sbf-sdk tarball
rm -rf sbf-sdk.tar.bz2 sbf-sdk/
mkdir sbf-sdk/
cp LICENSE sbf-sdk/
(
  ci/crate-version.sh platform-tools-sdk/cargo-build-sbf/Cargo.toml
  git rev-parse HEAD
) > sbf-sdk/version.txt
cp -a platform-tools-sdk/sbf/* sbf-sdk/
tar jvcf sbf-sdk.tar.bz2 sbf-sdk/

================
File: platform-tools-sdk/sbf/scripts/strip.sh
================
so=$1
if [[ ! -r $so ]]; then
  echo "Error: file not found: $so"
  exit 1
fi
so_stripped=$2
if [[ -z $so_stripped ]]; then
  echo "Usage: $0 unstripped.so stripped.so"
  exit 1
fi
sbf_sdk=$(cd "$(dirname "$0")/.." && pwd)
# shellcheck source=platform-tools-sdk/sbf/env.sh
source "$sbf_sdk"/env.sh
set -e
out_dir=$(dirname "$so_stripped")
if [[ ! -d $out_dir ]]; then
  mkdir -p "$out_dir"
fi
"$sbf_sdk"/dependencies/platform-tools/llvm/bin/llvm-objcopy --strip-all "$so" "$so_stripped"

================
File: platform-tools-sdk/sbf/.gitignore
================
/dependencies/criterion*
/dependencies/hashbrown*
/dependencies/llvm-native*
/dependencies/rust-bpf-sysroot*
/dependencies/bpf-tools*
/dependencies/platform-tools*
/dependencies/sbf-tools*
/dependencies/xargo*
/dependencies/bin*
/dependencies/.crates.toml
/dependencies/.crates2.json
/syscalls.txt

================
File: platform-tools-sdk/sbf/env.sh
================
if [ -z "$sbf_sdk" ]; then
  sbf_sdk=.
fi
"$sbf_sdk"/scripts/install.sh
export CC="$sbf_sdk/dependencies/platform-tools/llvm/bin/clang"
export AR="$sbf_sdk/dependencies/platform-tools/llvm/bin/llvm-ar"
export OBJDUMP="$sbf_sdk/dependencies/platform-tools/llvm/bin/llvm-objdump"
export OBJCOPY="$sbf_sdk/dependencies/platform-tools/llvm/bin/llvm-objcopy"

================
File: poh/benches/poh_verify.rs
================
extern crate test;
⋮----
fn bench_poh_verify_ticks(bencher: &mut Bencher) {
⋮----
let start_hash = hash(zero.as_ref());
⋮----
ticks.push(next_entry_mut(&mut cur_hash, NUM_HASHES, vec![]));
⋮----
bencher.iter(|| {
assert!(ticks.verify(&start_hash, &thread_pool).status());
⋮----
fn bench_poh_verify_transaction_entries(bencher: &mut Bencher) {
⋮----
let pubkey1 = keypair1.pubkey();
⋮----
let tx = transfer(&keypair1, &pubkey1, 42, cur_hash);
ticks.push(next_entry_mut(&mut cur_hash, NUM_HASHES, vec![tx]));

================
File: poh/benches/poh.rs
================
extern crate test;
⋮----
fn bench_poh_hash(bencher: &mut Bencher) {
⋮----
bencher.iter(|| {
poh.hash(NUM_HASHES);
⋮----
fn bench_arc_mutex_poh_hash(bencher: &mut Bencher) {
⋮----
poh.lock().unwrap().hash(1);
⋮----
fn bench_arc_mutex_poh_batched_hash(bencher: &mut Bencher) {
let poh = Arc::new(Mutex::new(Poh::new(Hash::default(), Some(NUM_HASHES))));
⋮----
if poh.lock().unwrap().hash(DEFAULT_HASHES_PER_BATCH) {
poh.lock().unwrap().tick().unwrap();
if exit.load(Ordering::Relaxed) {
⋮----
fn bench_poh_lock_time_per_batch(bencher: &mut Bencher) {
⋮----
poh.hash(DEFAULT_HASHES_PER_BATCH);
⋮----
fn bench_poh_recorder_record(bencher: &mut Bencher) {
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path()).expect("Expected to be able to open database ledger");
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(2);
⋮----
let prev_hash = bank.last_blockhash();
⋮----
bank.clone(),
Some((4, 4)),
bank.ticks_per_slot(),
⋮----
let h1 = hash(b"hello Agave, hello Anza!");
poh_recorder.set_bank_for_test(bank.clone());
poh_recorder.tick();
⋮----
SanitizedTransaction::from_transaction_for_tests(test_tx()),
⋮----
let txs: Vec<_> = txs.iter().map(|tx| tx.to_versioned_transaction()).collect();
⋮----
.record(
bank.slot(),
vec![test::black_box(h1)],
vec![test::black_box(txs.clone())],
⋮----
.unwrap();
⋮----
fn bench_poh_recorder_set_bank(bencher: &mut Bencher) {
⋮----
poh_recorder.clear_bank_for_test();

================
File: poh/benches/transaction_recorder.rs
================
fn bench_record_transactions(c: &mut Criterion) {
⋮----
let mut genesis_config_info = create_genesis_config(2);
⋮----
hashes_per_tick: Some(solana_clock::DEFAULT_HASHES_PER_TICK),
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path()).expect("Expected to be able to open database ledger"),
⋮----
bank.tick_height(),
bank.last_blockhash(),
bank.clone(),
⋮----
bank.ticks_per_slot(),
⋮----
exit.clone(),
⋮----
let (record_sender, record_receiver) = record_channels(false);
⋮----
.map(|_| {
⋮----
.collect();
⋮----
poh_recorder.clone(),
⋮----
.set_bank_sync(BankWithScheduler::new_without_scheduler(bank.clone()))
.unwrap();
let mut group = c.benchmark_group("record_transactions");
group.throughput(criterion::Throughput::Elements(
⋮----
group.bench_function("record_transactions", |b| {
b.iter_custom(|iters| {
⋮----
let tx_batches: Vec<_> = (0..NUM_BATCHES).map(|_| txs.clone()).collect();
let next_slot = bank.slot().wrapping_add(1);
⋮----
.reset_sync(bank.clone(), Some((next_slot, next_slot)))
⋮----
let summary = transaction_recorder.record_transactions(bank.bank_id(), txs);
assert!(summary.result.is_ok());
⋮----
let elapsed = start.elapsed();
total = total.saturating_add(elapsed);
⋮----
exit.store(true, std::sync::atomic::Ordering::Relaxed);
poh_service.join().unwrap();
⋮----
criterion_group!(benches, bench_record_transactions,);
criterion_main!(benches);

================
File: poh/src/lib.rs
================
pub mod poh_controller;
pub mod poh_recorder;
pub mod poh_service;
pub mod record_channels;
pub mod transaction_recorder;
⋮----
extern crate solana_metrics;
⋮----
extern crate assert_matches;

================
File: poh/src/poh_controller.rs
================
pub enum PohServiceMessage {
⋮----
pub struct PohController {
⋮----
impl PohController {
pub fn new() -> (Self, PohServiceMessageReceiver) {
⋮----
pending_message: pending_message.clone(),
⋮----
pub fn has_pending_message(&self) -> bool {
self.pending_message.load(Ordering::Acquire) > 0
⋮----
pub fn set_bank_sync(
⋮----
self.send_and_wait_on_pending_message(PohServiceMessage::SetBank { bank })
⋮----
pub fn set_bank(
⋮----
self.send_message(PohServiceMessage::SetBank { bank })
⋮----
pub fn reset_sync(
⋮----
self.send_and_wait_on_pending_message(PohServiceMessage::Reset {
⋮----
pub fn reset(
⋮----
self.send_message(PohServiceMessage::Reset {
⋮----
fn send_and_wait_on_pending_message(
⋮----
self.send_message(message)?;
while self.has_pending_message() {
⋮----
Ok(())
⋮----
fn send_message(&self, message: PohServiceMessage) -> Result<(), SendError<PohServiceMessage>> {
self.pending_message.fetch_add(1, Ordering::AcqRel);
self.sender.send(message)?;
⋮----
pub struct PohServiceMessageReceiver {
⋮----
impl PohServiceMessageReceiver {
pub(crate) fn try_recv(&self) -> Result<PohServiceMessageGuard<'_>, TryRecvError> {
⋮----
.try_recv()
.map(|message| PohServiceMessageGuard {
⋮----
message: Some(message),
⋮----
pub(crate) struct PohServiceMessageGuard<'a> {
⋮----
pub(crate) fn take(&mut self) -> PohServiceMessage {
self.message.take().unwrap()
⋮----
impl Drop for PohServiceMessageGuard<'_> {
fn drop(&mut self) {
if self.message.is_none() {
⋮----
.fetch_sub(1, Ordering::AcqRel);
⋮----
panic!("PohServiceMessageGuard dropped without processing the message");

================
File: poh/src/poh_recorder.rs
================
use qualifier_attr::qualifiers;
⋮----
pub enum PohRecorderError {
⋮----
pub(crate) type Result<T> = std::result::Result<T, PohRecorderError>;
pub type WorkingBankEntry = (Arc<Bank>, (Entry, u64));
⋮----
pub struct RecordSummary {
⋮----
pub struct Record {
⋮----
impl Record {
pub fn new(
⋮----
pub struct WorkingBank {
⋮----
pub enum PohLeaderStatus {
⋮----
struct PohRecorderMetrics {
⋮----
impl Default for PohRecorderMetrics {
fn default() -> Self {
⋮----
impl PohRecorderMetrics {
fn report(&mut self, bank_slot: Slot) {
if self.last_metric.elapsed().as_millis() > 1000 {
datapoint_info!(
⋮----
pub struct PohRecorder {
⋮----
impl PohRecorder {
⋮----
pub fn new_with_clear_signal(
⋮----
let (working_bank_sender, working_bank_receiver) = unbounded();
⋮----
tick_cache: vec![],
⋮----
start_bank_active_descendants: vec![],
⋮----
leader_schedule_cache: leader_schedule_cache.clone(),
⋮----
pub fn reset(&mut self, reset_bank: Arc<Bank>, next_leader_slot: Option<(Slot, Slot)>) {
self.clear_bank(false);
let tick_height = self.reset_poh(reset_bank, true);
⋮----
self.shared_leader_state.store(Arc::new(LeaderState::new(
⋮----
pub fn record(
⋮----
assert!(
⋮----
if let Some(working_bank) = self.working_bank.as_ref() {
⋮----
measure_us!(self.metrics.report(working_bank.bank.slot()));
⋮----
let (flush_cache_res, flush_cache_us) = measure_us!(self.flush_cache(false));
⋮----
let tick_height = self.tick_height();
⋮----
.as_mut()
.ok_or(PohRecorderError::MaxHeightReached)?;
if bank_id != working_bank.bank.bank_id() {
return Err(PohRecorderError::MaxHeightReached);
⋮----
let (mut poh_lock, poh_lock_us) = measure_us!(self.poh.lock().unwrap());
⋮----
measure_us!(poh_lock.record_batches(&mixins, &mut self.entries));
⋮----
poh_lock.remaining_hashes_in_slot(working_bank.bank.ticks_per_slot());
drop(poh_lock);
⋮----
debug_assert_eq!(self.entries.len(), mixins.len());
for (entry, transactions) in self.entries.drain(..).zip(transaction_batches) {
⋮----
measure_us!(self.working_bank_sender.send((
⋮----
return Ok(RecordSummary {
⋮----
self.tick();
⋮----
pub(crate) fn tick(&mut self) {
let (poh_entry, tick_lock_contention_us) = measure_us!({
⋮----
self.shared_leader_state.increment_tick_height();
trace!("tick_height {}", self.tick_height());
⋮----
.load()
⋮----
.is_none()
⋮----
self.tick_cache.push((
⋮----
transactions: vec![],
⋮----
self.tick_height(),
⋮----
let (_flush_res, flush_cache_and_tick_us) = measure_us!(self.flush_cache(true));
⋮----
pub fn set_bank(&mut self, bank: BankWithScheduler) {
assert!(self.working_bank.is_none());
⋮----
min_tick_height: bank.tick_height(),
max_tick_height: bank.max_tick_height(),
⋮----
trace!("new working bank");
assert_eq!(working_bank.bank.ticks_per_slot(), self.ticks_per_slot());
let mut tick_height = self.tick_height();
if let Some(hashes_per_tick) = *working_bank.bank.hashes_per_tick() {
if self.poh.lock().unwrap().hashes_per_tick() != hashes_per_tick {
info!(
⋮----
tick_height = self.reset_poh(working_bank.bank.clone(), false);
⋮----
let leader_state = self.shared_leader_state.load();
let leader_first_tick_height = leader_state.leader_first_tick_height();
let next_leader_slot = leader_state.next_leader_slot_range();
drop(leader_state);
⋮----
Some(working_bank.bank.clone_without_scheduler()),
⋮----
self.working_bank = Some(working_bank);
let _ = self.flush_cache(false);
⋮----
fn clear_bank(&mut self, set_shared_state: bool) {
if let Some(WorkingBank { bank, start, .. }) = self.working_bank.take() {
let next_leader_slot = self.leader_schedule_cache.next_leader_slot(
bank.collector_id(),
bank.slot(),
⋮----
Some(&self.blockstore),
⋮----
assert_eq!(self.ticks_per_slot, bank.ticks_per_slot());
⋮----
match signal.try_send(true) {
⋮----
trace!("replay wake up signal channel is full.")
⋮----
trace!("replay wake up signal channel is disconnected.")
⋮----
fn reset_poh(&mut self, reset_bank: Arc<Bank>, reset_start_bank: bool) -> u64 {
let blockhash = reset_bank.last_blockhash();
⋮----
*reset_bank.hashes_per_tick()
⋮----
let mut poh = self.poh.lock().unwrap();
poh.reset(blockhash, hashes_per_tick);
⋮----
self.tick_cache = vec![];
⋮----
self.start_bank_active_descendants = vec![];
⋮----
let tick_height = (self.start_slot() + 1) * self.ticks_per_slot;
⋮----
fn flush_cache(&mut self, tick: bool) -> Result<()> {
⋮----
.as_ref()
⋮----
if self.tick_height() < working_bank.min_tick_height {
return Err(PohRecorderError::MinHeightNotReached);
⋮----
if tick && self.tick_height() == working_bank.min_tick_height {
⋮----
.iter()
.take_while(|x| x.1 <= working_bank.max_tick_height)
.count();
let mut send_result: std::result::Result<(), SendError<WorkingBankEntry>> = Ok(());
⋮----
trace!(
⋮----
working_bank.bank.register_tick(&tick.0.hash);
⋮----
.send((working_bank.bank.clone(), tick.clone()));
if send_result.is_err() {
⋮----
if self.tick_height() >= working_bank.max_tick_height {
⋮----
self.start_bank = working_bank.bank.clone();
let working_slot = self.start_slot();
⋮----
self.clear_bank(true);
⋮----
info!("WorkingBank::sender disconnected {send_result:?}");
⋮----
let _ = self.tick_cache.drain(..entry_count);
⋮----
Ok(())
⋮----
pub fn would_be_leader(&self, within_next_n_ticks: u64) -> bool {
self.has_bank()
⋮----
.leader_first_tick_height()
.is_some_and(|leader_first_tick_height| {
⋮----
fn slot_for_tick_height(&self, tick_height: u64) -> Slot {
tick_height.saturating_sub(1) / self.ticks_per_slot
⋮----
pub fn current_poh_slot(&self) -> Slot {
let next_tick_height = self.tick_height().saturating_add(1);
self.slot_for_tick_height(next_tick_height)
⋮----
pub fn leader_after_n_slots(&self, slots: u64) -> Option<Pubkey> {
⋮----
.slot_leader_at(self.current_poh_slot() + slots, None)
⋮----
pub fn leader_and_slot_after_n_slots(
⋮----
let target_slot = self.current_poh_slot().checked_add(slots_in_the_future)?;
⋮----
.slot_leader_at(target_slot, None)
.map(|leader| (leader, target_slot))
⋮----
pub fn shared_leader_state(&self) -> SharedLeaderState {
self.shared_leader_state.clone()
⋮----
pub fn bank(&self) -> Option<Arc<Bank>> {
self.working_bank.as_ref().map(|w| w.bank.clone())
⋮----
pub fn has_bank(&self) -> bool {
self.working_bank.is_some()
⋮----
pub fn tick_height(&self) -> u64 {
self.shared_leader_state.load().tick_height()
⋮----
fn leader_first_tick_height(&self) -> Option<u64> {
self.shared_leader_state.load().leader_first_tick_height()
⋮----
pub fn ticks_per_slot(&self) -> u64 {
⋮----
pub fn start_slot(&self) -> Slot {
self.start_bank.slot()
⋮----
pub fn reached_leader_slot(&self, my_pubkey: &Pubkey) -> PohLeaderStatus {
⋮----
let current_poh_slot = self.current_poh_slot();
let Some(leader_first_tick_height) = self.leader_first_tick_height() else {
⋮----
if !self.reached_leader_tick(my_pubkey, leader_first_tick_height) {
⋮----
.has_existing_shreds_for_slot(current_poh_slot)
⋮----
let parent_slot = self.start_slot();
⋮----
fn reached_leader_tick(&self, my_pubkey: &Pubkey, leader_first_tick_height: u64) -> bool {
⋮----
let ideal_target_tick_height = leader_first_tick_height.saturating_sub(1);
if self.tick_height() < ideal_target_tick_height {
⋮----
if self.tick_height() >= ideal_target_tick_height.saturating_add(self.grace_ticks) {
⋮----
let next_leader_slot = self.current_poh_slot();
self.can_skip_grace_ticks(my_pubkey, next_leader_slot)
⋮----
fn can_skip_grace_ticks(&self, my_pubkey: &Pubkey, next_leader_slot: Slot) -> bool {
if self.start_slot_was_mine(my_pubkey) {
⋮----
if self.start_slot_was_mine_or_previous_leader(next_leader_slot) {
return self.building_off_previous_leader_last_block(my_pubkey, next_leader_slot);
⋮----
if !self.is_new_reset_bank_pending(next_leader_slot) {
⋮----
self.report_pending_fork_was_detected(next_leader_slot);
⋮----
fn start_slot_was_mine_or_previous_leader(&self, next_leader_slot: Slot) -> bool {
(next_leader_slot.saturating_sub(NUM_CONSECUTIVE_LEADER_SLOTS)..next_leader_slot).any(
⋮----
slot == self.start_slot()
⋮----
fn building_off_previous_leader_last_block(
⋮----
(next_leader_slot.saturating_sub(NUM_CONSECUTIVE_LEADER_SLOTS)..next_leader_slot).rev()
⋮----
let leader_for_slot = self.leader_schedule_cache.slot_leader_at(slot, None);
⋮----
return slot == self.start_slot();
⋮----
fn start_slot_was_mine(&self, my_pubkey: &Pubkey) -> bool {
self.start_bank.collector_id() == my_pubkey
⋮----
fn is_new_reset_bank_pending(&self, next_leader_slot: Slot) -> bool {
⋮----
.any(|pending_slot| *pending_slot < next_leader_slot)
⋮----
fn report_pending_fork_was_detected(&self, next_leader_slot: Slot) {
let mut last_slot = self.last_reported_slot_for_pending_fork.lock().unwrap();
⋮----
fn compute_leader_slot_tick_heights(
⋮----
.map(|(first_slot, last_slot)| {
⋮----
Some(leader_first_tick_height),
⋮----
.unwrap_or((
⋮----
pub fn update_start_bank_active_descendants(&mut self, active_descendants: &[Slot]) {
self.start_bank_active_descendants = active_descendants.to_vec();
⋮----
pub fn set_bank_for_test(&mut self, bank: Arc<Bank>) {
self.set_bank(BankWithScheduler::new_without_scheduler(bank))
⋮----
pub fn clear_bank_for_test(&mut self) {
⋮----
pub fn tick_alpenglow(&mut self, slot_max_tick_height: u64) {
⋮----
.store(slot_max_tick_height, Ordering::Release);
⋮----
.load(Ordering::Acquire),
⋮----
pub fn enable_alpenglow(&mut self) {
info!("Enabling Alpenglow, migrating poh to low power mode");
⋮----
poh.reset(current_hash, hashes_per_tick);
⋮----
pub fn get_blockstore(&self) -> Arc<Blockstore> {
self.blockstore.clone()
⋮----
fn do_create_test_recorder(
⋮----
let poh_config = poh_config.unwrap_or_default();
⋮----
bank.tick_height(),
bank.last_blockhash(),
bank.clone(),
Some((4, 4)),
bank.ticks_per_slot(),
⋮----
exit.clone(),
⋮----
let ticks_per_slot = bank.ticks_per_slot();
let (record_sender, record_receiver) = record_channels(track_transaction_indexes);
⋮----
poh_recorder.clone(),
⋮----
.set_bank_sync(BankWithScheduler::new_without_scheduler(bank))
.unwrap();
⋮----
pub fn create_test_recorder(
⋮----
do_create_test_recorder(bank, blockstore, poh_config, leader_schedule_cache, false)
⋮----
pub struct SharedLeaderState(Arc<ArcSwap<LeaderState>>);
impl SharedLeaderState {
⋮----
fn new(
⋮----
Self(Arc::new(ArcSwap::from_pointee(inner)))
⋮----
pub fn load(&self) -> arc_swap::Guard<Arc<LeaderState>> {
self.0.load()
⋮----
fn store(&mut self, state: Arc<LeaderState>) {
self.0.store(state)
⋮----
fn increment_tick_height(&self) {
let inner = self.0.load();
inner.tick_height.fetch_add(1, Ordering::Release);
⋮----
pub struct LeaderState {
⋮----
impl LeaderState {
⋮----
pub fn working_bank(&self) -> Option<&Arc<Bank>> {
self.working_bank.as_ref()
⋮----
self.tick_height.load(Ordering::Acquire)
⋮----
pub fn leader_first_tick_height(&self) -> Option<u64> {
⋮----
pub fn next_leader_slot_range(&self) -> Option<(Slot, Slot)> {
⋮----
mod tests {
⋮----
fn test_poh_recorder_no_zero_tick() {
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger");
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(2);
⋮----
poh_recorder.tick();
assert_eq!(poh_recorder.tick_cache.len(), 1);
assert_eq!(poh_recorder.tick_cache[0].1, 1);
assert_eq!(poh_recorder.tick_height(), 1);
⋮----
fn test_poh_recorder_tick_height_is_last_tick() {
⋮----
assert_eq!(poh_recorder.tick_cache.len(), 2);
assert_eq!(poh_recorder.tick_cache[1].1, 2);
assert_eq!(poh_recorder.tick_height(), 2);
⋮----
fn test_poh_recorder_reset_clears_cache() {
⋮----
bank0.clone(),
⋮----
poh_recorder.reset(bank0, Some((4, 4)));
assert_eq!(poh_recorder.tick_cache.len(), 0);
⋮----
fn test_poh_recorder_clear() {
⋮----
let prev_hash = bank.last_blockhash();
⋮----
poh_recorder.set_bank_for_test(bank);
assert!(poh_recorder.working_bank.is_some());
poh_recorder.clear_bank(true);
assert!(poh_recorder.working_bank.is_none());
⋮----
fn test_poh_recorder_tick_sent_after_min() {
⋮----
let prev_hash = bank0.last_blockhash();
⋮----
bank0.ticks_per_slot(),
⋮----
bank0.fill_bank_with_ticks_for_tests();
⋮----
poh_recorder.set_bank_for_test(bank1.clone());
let num_new_ticks = bank1.tick_height() - poh_recorder.tick_height();
println!("{} {}", bank1.tick_height(), poh_recorder.tick_height());
assert!(num_new_ticks > 0);
⋮----
let min_tick_height = poh_recorder.working_bank.as_ref().unwrap().min_tick_height;
assert_eq!(min_tick_height, bank1.tick_height());
assert_eq!(poh_recorder.tick_height(), min_tick_height);
assert_eq!(poh_recorder.tick_cache.last().unwrap().1, num_new_ticks);
assert!(entry_receiver.try_recv().is_err());
let tick_height_before = poh_recorder.tick_height();
⋮----
assert_eq!(poh_recorder.tick_height(), tick_height_before + 1);
⋮----
while let Ok((wbank, (_entry, _tick_height))) = entry_receiver.try_recv() {
assert_eq!(wbank.slot(), bank1.slot());
⋮----
assert_eq!(num_entries, num_new_ticks + 1);
⋮----
fn test_poh_recorder_tick_sent_upto_and_including_max() {
⋮----
for _ in 0..bank.max_tick_height() + 1 {
⋮----
assert_eq!(
⋮----
assert_eq!(poh_recorder.tick_height(), bank.max_tick_height() + 1);
poh_recorder.set_bank_for_test(bank.clone());
⋮----
assert_eq!(poh_recorder.tick_height(), bank.max_tick_height() + 2);
⋮----
while entry_receiver.try_recv().is_ok() {
⋮----
assert_eq!(num_entries, bank.max_tick_height());
⋮----
fn test_poh_recorder_record_to_early() {
⋮----
for _ in 0..bank1.tick_height() - 1 {
poh_recorder.tick()
⋮----
let tx = test_tx();
let h1 = hash(b"hello world!");
// We haven't yet reached the minimum tick height for the working bank,
// so record should fail
assert_matches!(
⋮----
fn test_poh_recorder_record_bad_slot() {
⋮----
// Fulfills min height criteria for a successful record
⋮----
// However we hand over a bad slot so record fails
let bad_slot = bank.slot() + 1;
⋮----
fn test_poh_recorder_record_at_min_passes() {
⋮----
while poh_recorder.tick_height() < min_tick_height {
⋮----
assert_eq!(poh_recorder.tick_cache.len() as u64, min_tick_height);
⋮----
assert!(poh_recorder
⋮----
//tick in the cache + entry
⋮----
let (_bank, (e, _tick_height)) = entry_receiver.recv().unwrap();
assert!(e.is_tick());
⋮----
assert!(!e.is_tick());
⋮----
fn test_poh_recorder_record_at_max_fails() {
⋮----
let num_ticks_to_max = bank.max_tick_height() - poh_recorder.tick_height();
⋮----
let (_bank, (entry, _tick_height)) = entry_receiver.recv().unwrap();
assert!(entry.is_tick());
⋮----
fn test_poh_cache_on_disconnect() {
⋮----
poh_recorder.set_bank_for_test(bank1);
let remaining_ticks_to_min = poh_recorder.working_bank.as_ref().unwrap().min_tick_height
- poh_recorder.tick_height();
⋮----
assert_eq!(poh_recorder.tick_height(), remaining_ticks_to_min);
⋮----
drop(entry_receiver);
⋮----
fn test_reset_current() {
⋮----
poh_recorder.reset(bank, Some((4, 4)));
⋮----
fn test_reset_with_cached() {
⋮----
fn test_reset_to_new_value() {
⋮----
assert_eq!(poh_recorder.tick_cache.len(), 4);
assert_eq!(poh_recorder.tick_height(), 4);
⋮----
assert_eq!(poh_recorder.tick_height(), DEFAULT_TICKS_PER_SLOT + 1);
⋮----
fn test_reset_clear_bank() {
⋮----
assert_eq!(bank.slot(), 0);
⋮----
pub fn test_clear_signal() {
⋮----
let (sender, receiver) = bounded(1);
⋮----
Some(sender),
⋮----
assert!(receiver.try_recv().is_ok());
⋮----
fn test_poh_recorder_record_sets_start_slot() {
⋮----
} = create_genesis_config(2);
⋮----
let max_tick_height = poh_recorder.working_bank.as_ref().unwrap().max_tick_height;
⋮----
// Even thought we ticked much further than working_bank.max_tick_height,
// the `start_slot` is still the slot of the last working bank set by
// the earlier call to `poh_recorder.set_bank()`
assert_eq!(poh_recorder.start_slot(), bank.slot());
⋮----
fn test_current_poh_slot() {
let genesis_config = create_genesis_config(2).genesis_config;
⋮----
let last_entry_hash = bank.last_blockhash();
⋮----
assert_eq!(0, poh_recorder.current_poh_slot());
poh_recorder.reset(bank.clone(), None);
assert_eq!(bank.slot() + 1, poh_recorder.current_poh_slot());
for _ in 0..bank.ticks_per_slot() - 1 {
⋮----
assert_eq!(bank.slot() + 2, poh_recorder.current_poh_slot());
⋮----
fn test_reached_leader_tick() {
⋮----
slot_leaders.extend(std::iter::repeat_n(
⋮----
leader_schedule_cache.set_fixed_leader_schedule(Some(fixed_schedule));
⋮----
poh_recorder.reset(
⋮----
Some((leader_a_start_slot + 1, leader_a_end_slot)),
⋮----
assert!(poh_recorder.reached_leader_tick(&leader_a_pubkey, leader_a_start_tick));
⋮----
poh_recorder.reset(bank.clone(), Some((leader_b_start_slot, leader_b_end_slot)));
assert!(!poh_recorder.reached_leader_tick(&leader_b_pubkey, leader_b_start_tick));
for _ in poh_recorder.tick_height()..ticks_in_leader_slot_set {
⋮----
let child_slot = bank.slot() + 1;
⋮----
assert!(poh_recorder.reached_leader_tick(&leader_b_pubkey, leader_b_start_tick));
⋮----
Some((leader_b_start_slot + 1, leader_b_end_slot)),
⋮----
poh_recorder.reset(bank, Some((leader_d_start_slot, leader_d_end_slot)));
⋮----
assert!(poh_recorder.reached_leader_tick(&leader_d_pubkey, leader_d_start_tick));
let active_descendants = vec![NUM_CONSECUTIVE_LEADER_SLOTS];
poh_recorder.update_start_bank_active_descendants(&active_descendants);
⋮----
assert!(!poh_recorder.reached_leader_tick(&leader_d_pubkey, leader_d_start_tick));
⋮----
fn test_reached_leader_slot() {
⋮----
assert_eq!(bank0.slot(), 0);
poh_recorder.reset(bank0.clone(), None);
⋮----
poh_recorder.reset(bank0.clone(), Some((2, 2)));
let init_ticks = poh_recorder.tick_height();
for _ in 0..bank0.ticks_per_slot() {
⋮----
poh_recorder.blockstore.put_meta(0, &parent_meta).unwrap();
⋮----
assert_eq!(bank1.slot(), 1);
poh_recorder.reset(bank1.clone(), Some((2, 2)));
⋮----
poh_recorder.reset(bank1.clone(), Some((3, 3)));
for _ in 0..bank1.ticks_per_slot() {
⋮----
for _ in 0..bank1.ticks_per_slot() / GRACE_TICKS_FACTOR {
⋮----
let bank2 = Arc::new(Bank::new_from_parent(bank1.clone(), &Pubkey::default(), 2));
poh_recorder.reset(bank2.clone(), Some((4, 4)));
⋮----
assert_eq!(bank3.slot(), 3);
poh_recorder.reset(bank3.clone(), Some((4, 4)));
⋮----
poh_recorder.reset(bank4.clone(), Some((5, 5)));
⋮----
for _ in 0..overshoot_factor * bank4.ticks_per_slot() {
⋮----
poh_recorder.reset(bank4.clone(), Some((9, 9)));
for _ in 0..4 * bank4.ticks_per_slot() {
⋮----
poh_recorder.update_start_bank_active_descendants(&[5]);
assert!(poh_recorder.is_new_reset_bank_pending(8));
⋮----
fn test_would_be_leader_soon() {
⋮----
assert!(!poh_recorder.would_be_leader(2 * bank.ticks_per_slot()));
⋮----
let bank_slot = bank.slot() + 3;
poh_recorder.reset(bank.clone(), Some((bank_slot, bank_slot)));
⋮----
assert!(poh_recorder.would_be_leader(3 * bank.ticks_per_slot()));
⋮----
assert!(poh_recorder.would_be_leader(2 * bank.ticks_per_slot()));
⋮----
fn test_flush_virtual_ticks() {
⋮----
let genesis_hash = bank.last_blockhash();
⋮----
Some((2, 2)),
⋮----
for _ in 0..(bank.ticks_per_slot() * 3) {
⋮----
assert!(!bank.is_hash_valid_for_age(&genesis_hash, 0));
assert!(bank.is_hash_valid_for_age(&genesis_hash, 1));
⋮----
fn test_compute_leader_slot_tick_heights() {

================
File: poh/src/poh_service.rs
================
pub struct PohService {
⋮----
struct PohTiming {
⋮----
impl PohTiming {
fn new() -> Self {
⋮----
fn report(&mut self, ticks_per_slot: u64) {
if self.last_metric.elapsed().as_millis() > 1000 {
let elapsed_us = self.last_metric.elapsed().as_micros() as u64;
⋮----
datapoint_info!(
⋮----
impl PohService {
pub fn new(
⋮----
let poh_config = poh_config.clone();
⋮----
.name("solPohTickProd".to_string())
.spawn(move || {
if poh_config.hashes_per_tick.is_none() {
if poh_config.target_tick_count.is_none() {
⋮----
poh_config.target_tick_duration.as_nanos() as u64,
⋮----
poh_exit.store(true, Ordering::Relaxed);
⋮----
.unwrap();
⋮----
pub fn target_ns_per_tick(ticks_per_slot: u64, target_tick_duration_ns: u64) -> u64 {
⋮----
target_tick_duration_ns.saturating_sub(adjustment_per_tick)
⋮----
fn low_power_tick_producer(
⋮----
let poh = poh_recorder.read().unwrap().poh.clone();
⋮----
record_receiver.shutdown();
⋮----
while !poh_exit.load(Ordering::Relaxed) {
⋮----
.saturating_sub(last_tick.elapsed());
⋮----
debug_assert!(
⋮----
if remaining_tick_time.is_zero()
&& (!should_shutdown_for_test_producers || record_receiver.is_safe_to_restart())
⋮----
poh_recorder.write().unwrap().tick();
⋮----
|| record_receiver.should_shutdown(
poh.lock().unwrap().remaining_hashes_in_slot(ticks_per_slot),
⋮----
while !record_receiver.is_safe_to_restart() {
⋮----
pub fn read_record_receiver_and_process(
⋮----
let record = record_receiver.recv_timeout(timeout);
⋮----
match poh_recorder.write().unwrap().record(
⋮----
.should_shutdown(record_summary.remaining_hashes_in_slot, ticks_per_slot)
⋮----
panic!("PohRecorder::record failed: {err:?}");
⋮----
fn short_lived_low_power_tick_producer(
⋮----
let num_ticks = poh_config.target_tick_count.unwrap();
⋮----
if poh_exit.load(Ordering::Relaxed) && !warned {
⋮----
warn!("exit signal is ignored because PohService is scheduled to exit soon");
⋮----
fn should_shutdown_for_test_producers(poh_recorder: &RwLock<PohRecorder>) -> bool {
let poh_recorder = poh_recorder.read().unwrap();
⋮----
.bank()
.map(|bank| bank.max_tick_height().wrapping_sub(1) <= poh_recorder.tick_height())
.unwrap_or(false)
⋮----
fn record_or_hash(
⋮----
match next_record.take() {
⋮----
let mut poh_recorder_l = poh_recorder.write().unwrap();
lock_time.stop();
timing.total_lock_time_ns += lock_time.as_ns();
⋮----
match poh_recorder_l.record(
⋮----
if record_receiver.should_shutdown(
⋮----
if let Ok(new_record) = record_receiver.try_recv() {
⋮----
record_time.stop();
timing.total_record_time_us += record_time.as_us();
⋮----
let mut poh_l = poh.lock().unwrap();
⋮----
let should_tick = poh_l.hash(hashes_per_batch);
let ideal_time = poh_l.target_poh_time(target_ns_per_tick);
hash_time.stop();
let remaining_hashes_in_slot = poh_l.remaining_hashes_in_slot(ticks_per_slot);
⋮----
remaining_hashes_in_slot.saturating_sub(hashes_per_batch);
⋮----
.should_shutdown(remaining_hashes_after_next_batch, ticks_per_slot)
⋮----
timing.total_hash_time_ns += hash_time.as_ns();
⋮----
if let Ok(record) = record_receiver.try_recv() {
*next_record = Some(record);
⋮----
drop(poh_l);
⋮----
timing.total_sleep_us += wait_start.elapsed().as_micros() as u64;
⋮----
fn tick_producer(
⋮----
let mut should_exit = poh_exit.load(Ordering::Relaxed);
⋮----
should_exit |= poh_exit.load(Ordering::Relaxed);
⋮----
poh_recorder_l.tick();
tick_time.stop();
timing.total_tick_time_ns += tick_time.as_ns();
⋮----
timing.report(ticks_per_slot);
⋮----
if next_record.is_none()
⋮----
if should_exit && record_receiver.is_safe_to_restart() {
⋮----
fn check_for_service_message<'a>(
⋮----
match service_message_receiver.try_recv() {
⋮----
Some(bank_message)
⋮----
fn handle_service_message(
⋮----
let mut recorder = poh_recorder.write().unwrap();
match service_message.take() {
⋮----
recorder.reset(reset_bank, next_leader_slot);
⋮----
let bank_id = bank.bank_id();
let bank_max_tick_height = bank.max_tick_height();
recorder.set_bank(bank);
⋮----
recorder.tick_height() < bank_max_tick_height.saturating_sub(1);
⋮----
record_receiver.restart(bank_id);
⋮----
/// If we have a service message and there are no more records to process,
    /// we can break inner recording loops and handle the service message.
⋮----
/// we can break inner recording loops and handle the service message.
    /// However, if there are still records to process, we must continue processing
⋮----
/// However, if there are still records to process, we must continue processing
    /// records before handling the service message, to ensure we do not lose
⋮----
/// records before handling the service message, to ensure we do not lose
    /// any records.
⋮----
/// any records.
    fn can_process_service_message(
⋮----
fn can_process_service_message(
⋮----
service_message.is_none() || record_receiver.is_safe_to_restart()
⋮----
pub fn join(self) -> thread::Result<()> {
self.tick_producer.join()
⋮----
mod tests {
⋮----
fn test_poh_service() {
⋮----
} = create_genesis_config(2);
let hashes_per_tick = Some(DEFAULT_HASHES_PER_TICK);
⋮----
let prev_hash = bank.last_blockhash();
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger");
⋮----
PohConfig::default().target_tick_duration.as_micros() as u64;
⋮----
let ticks_per_slot = bank.ticks_per_slot();
⋮----
// Just set something very far in the future that we won't reach.
let next_leader_slot = Some((1_000_000, 1_000_000));
⋮----
bank.tick_height(),
⋮----
bank.clone(),
⋮----
exit.clone(),
⋮----
.map(|x| x.parse().unwrap())
.unwrap_or(0);
⋮----
let poh_recorder = poh_recorder.clone();
let exit = exit.clone();
let mut bank = bank.clone();
⋮----
.name("solPohEntryProd".to_string())
⋮----
let h1 = hash(b"hello world!");
let tx = VersionedTransaction::from(test_tx());
⋮----
// send some data
⋮----
let res = poh_recorder.write().unwrap().record(
bank.slot(),
vec![h1],
vec![vec![tx.clone()]],
⋮----
.write()
.unwrap()
.reset(bank.clone(), next_leader_slot);
⋮----
bank.slot() + 1,
⋮----
.set_bank_for_test(bank.clone());
⋮----
time.stop();
total_us += time.as_us();
⋮----
if is_test_run && rng().random_ratio(1, 4) {
sleep(Duration::from_millis(200));
⋮----
if exit.load(Ordering::Relaxed) {
info!(
⋮----
.unwrap_or(DEFAULT_HASHES_PER_BATCH);
let (_record_sender, record_receiver) = record_channels(false);
⋮----
poh_recorder.clone(),
⋮----
poh_recorder.write().unwrap().set_bank_for_test(bank);
⋮----
.recv_timeout(Duration::from_millis(DEFAULT_MS_PER_SLOT))
.expect("Expected to receive an entry");
if entry.is_tick() {
⋮----
assert!(
⋮----
if entry.num_hashes == poh_config.hashes_per_tick.unwrap() {
⋮----
assert_eq!(hashes, poh_config.hashes_per_tick.unwrap());
⋮----
assert!(entry.num_hashes >= 1);
⋮----
if time.elapsed().as_millis() > run_time {
⋮----
let elapsed = time.elapsed();
⋮----
exit.store(true, Ordering::Relaxed);
poh_service.join().unwrap();
entry_producer.join().unwrap();
⋮----
fn test_poh_service_record_race() {
⋮----
let (record_sender, mut record_receiver) = record_channels(false);
record_receiver.restart(bank.bank_id());
⋮----
poh_controller.reset(bank.clone(), None).unwrap();
⋮----
.try_send(Record {
mixins: vec![Hash::new_unique()],
transaction_batches: vec![vec![VersionedTransaction::from(test_tx())]],
bank_id: bank.bank_id(),
⋮----
while !record_sender.is_empty() {
assert!(start.elapsed() < Duration::from_secs(1));

================
File: poh/src/record_channels.rs
================
pub fn record_channels(track_transaction_indexes: bool) -> (RecordSender, RecordReceiver) {
⋮----
let (sender, receiver) = bounded(CAPACITY);
⋮----
Some(Arc::new(Mutex::new(0)))
⋮----
active_senders: active_senders.clone(),
bank_id_allowed_insertions: bank_id_allowed_insertions.clone(),
⋮----
transaction_indexes: transaction_indexes.clone(),
⋮----
pub enum RecordSenderError {
⋮----
pub struct RecordSender {
⋮----
impl RecordSender {
⋮----
pub(crate) fn is_empty(&self) -> bool {
self.sender.is_empty()
⋮----
pub fn try_send(&self, record: Record) -> Result<Option<usize>, RecordSenderError> {
⋮----
.iter()
.map(|batch| batch.len())
.sum();
assert!(num_transactions > 0);
⋮----
.as_ref()
.map(|transaction_indexes| transaction_indexes.lock().unwrap());
⋮----
self.bank_id_allowed_insertions.0.load(Ordering::Acquire);
⋮----
return Err(RecordSenderError::Shutdown);
⋮----
return Err(RecordSenderError::InactiveBankId);
⋮----
if allowed_insertions < record.transaction_batches.len() as u64 {
return Err(RecordSenderError::Full);
⋮----
allowed_insertions.wrapping_sub(record.transaction_batches.len() as u64),
⋮----
self.active_senders.fetch_add(1, Ordering::AcqRel);
⋮----
.compare_exchange(
⋮----
.is_err()
⋮----
self.active_senders.fetch_sub(1, Ordering::AcqRel);
⋮----
match self.sender.try_send(record) {
⋮----
return Ok(transaction_indexes.map(|mut transaction_indexes| {
⋮----
assert!(err.is_disconnected());
⋮----
return Err(RecordSenderError::Disconnected);
⋮----
pub struct RecordReceiver {
⋮----
impl RecordReceiver {
pub fn should_shutdown(&self, remaining_hashes_in_slot: u64, ticks_per_slot: u64) -> bool {
remaining_hashes_in_slot.saturating_sub(ticks_per_slot) <= self.capacity
⋮----
pub fn shutdown(&mut self) {
self.bank_id_allowed_insertions.shutdown();
⋮----
pub fn is_shutdown(&self) -> bool {
BankIdAllowedInsertions::bank_id(self.bank_id_allowed_insertions.0.load(Ordering::Acquire))
⋮----
pub fn restart(&mut self, bank_id: BankId) {
assert!(bank_id <= BankIdAllowedInsertions::MAX_BANK_ID);
assert!(self.receiver.is_empty());
⋮----
.map(|transaction_indexes| {
let mut lock = transaction_indexes.lock().unwrap();
⋮----
self.bank_id_allowed_insertions.0.store(
⋮----
drop(transaction_indexes_lock);
⋮----
pub fn drain(&self) -> impl Iterator<Item = Record> + '_ {
core::iter::from_fn(|| self.try_recv().ok())
⋮----
pub fn is_safe_to_restart(&self) -> bool {
self.active_senders.load(Ordering::Acquire) == 0 && self.receiver.is_empty()
⋮----
pub fn try_recv(&self) -> Result<Record, TryRecvError> {
let mut sender_active = self.active_senders.load(Ordering::Acquire) > 0;
⋮----
match self.receiver.try_recv() {
⋮----
self.on_received_record(record.transaction_batches.len() as u64);
return Ok(record);
⋮----
sender_active = self.active_senders.load(Ordering::Acquire) > 0;
⋮----
return Err(TryRecvError::Empty);
⋮----
Err(e) => return Err(e),
⋮----
pub fn recv_timeout(&self, duration: Duration) -> Result<Record, RecvTimeoutError> {
let record = self.receiver.recv_timeout(duration)?;
⋮----
Ok(record)
⋮----
fn on_received_record(&self, num_batches: u64) {
⋮----
.fetch_add(num_batches, Ordering::AcqRel);
⋮----
struct BankIdAllowedInsertions(Arc<AtomicU64>);
impl BankIdAllowedInsertions {
⋮----
fn new_shutdown() -> Self {
Self(Arc::new(AtomicU64::new(Self::SHUTDOWN)))
⋮----
fn shutdown(&self) {
self.0.store(Self::SHUTDOWN, Ordering::Release);
⋮----
const fn encoded_value(bank_id: BankId, allowed_insertions: u64) -> u64 {
assert!(bank_id <= Self::DISABLED_BANK_ID);
assert!(allowed_insertions <= Self::MAX_ALLOWED_INSERTIONS);
⋮----
fn bank_id(value: u64) -> BankId {
⋮----
fn allowed_insertions(value: u64) -> u64 {
⋮----
mod tests {
⋮----
pub(super) fn test_record(bank_id: BankId, num_batches: usize) -> Record {
⋮----
.map(|_| vec![VersionedTransaction::default()])
.collect(),
mixins: (0..num_batches).map(|_| Hash::default()).collect(),
⋮----
fn test_record_channels() {
let (sender, mut receiver) = record_channels(false);
assert!(matches!(
⋮----
receiver.restart(1);
⋮----
assert!(matches!(sender.try_send(test_record(1, 1)), Ok(None)));
⋮----
assert!(matches!(sender.try_send(test_record(1, 1022)), Ok(None)));
⋮----
assert!(receiver.try_recv().is_ok());
assert!(!receiver.is_safe_to_restart());
⋮----
assert!(receiver.is_safe_to_restart());
⋮----
fn test_record_channels_track_indexes() {
let (sender, mut receiver) = record_channels(true);
⋮----
assert!(matches!(sender.try_send(test_record(1, 1)), Ok(Some(0))));
let mut record = test_record(1, 2);
⋮----
.last_mut()
.unwrap()
.push(VersionedTransaction::default());
assert!(matches!(sender.try_send(record), Ok(Some(1))));
assert!(*sender.transaction_indexes.as_ref().unwrap().lock().unwrap() == 4);
⋮----
mod shuttle_tests {
⋮----
fn test_sender_shutdown_safety_race() {
⋮----
if sender.try_send(test_record(bank_id, 1)).is_ok() {
⋮----
receiver.restart(current_bank_id);
⋮----
if receiver.is_shutdown() && receiver.is_safe_to_restart() {
⋮----
if let Ok(record) = receiver.try_recv() {
assert!(record.bank_id == current_bank_id, "bank_id mismatch!");
⋮----
receiver.shutdown();
⋮----
fn test_try_recv_not_sent_on_inner_channel_yet() {
⋮----
receiver.restart(0);
⋮----
let sender = sender.clone();
⋮----
let _ = sender.try_send(test_record(0, 1));
⋮----
let active_at_start = sender.active_senders.load(Ordering::Acquire);
let result = receiver.try_recv();
if result.is_err() && active_at_start > 0 {
panic!(

================
File: poh/src/transaction_recorder.rs
================
pub struct RecordTransactionsTimings {
⋮----
impl RecordTransactionsTimings {
pub fn accumulate(&mut self, other: &RecordTransactionsTimings) {
⋮----
pub struct RecordTransactionsSummary {
⋮----
pub struct TransactionRecorder {
⋮----
impl TransactionRecorder {
pub fn new(record_sender: RecordSender) -> Self {
⋮----
pub fn record_transactions(
⋮----
if !transactions.is_empty() {
let (hash, hash_us) = measure_us!(hash_transactions(&transactions));
record_transactions_timings.hash_us = Saturating(hash_us);
⋮----
measure_us!(self.record(bank_id, vec![hash], vec![transactions]));
record_transactions_timings.poh_record_us = Saturating(poh_record_us);
⋮----
result: Err(PohRecorderError::MaxHeightReached),
⋮----
result: Err(PohRecorderError::ChannelFull),
⋮----
result: Err(PohRecorderError::ChannelDisconnected),
⋮----
result: Ok(()),
⋮----
pub fn record_batch(
⋮----
if !transaction_batches.is_empty() {
⋮----
measure_us!(self.record(bank_id, mixins, transaction_batches));
⋮----
pub fn record(
⋮----
.try_send(Record::new(mixins, transaction_batches, bank_id))
⋮----
pub fn record_bundle(
⋮----
let mut hashes = Vec::with_capacity(transactions.len());
let mut batches = Vec::with_capacity(transactions.len());
⋮----
let batch = vec![tx];
let (hash, hash_us) = measure_us!(hash_transactions(&batch));
⋮----
hashes.push(hash);
batches.push(batch);
⋮----
let (res, poh_record_us) = measure_us!(self.record(bank_id, hashes, batches));

================
File: poh/.gitignore
================
/target/
/farf/

================
File: poh/Cargo.toml
================
[package]
name = "solana-poh"
description = "Solana PoH"
documentation = "https://docs.rs/solana-poh"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_poh"

[features]
agave-unstable-api = []
dev-context-only-utils = []
shuttle-test = ["dep:shuttle"]

[dependencies]
arc-swap = { workspace = true }
core_affinity = { workspace = true }
crossbeam-channel = { workspace = true }
log = { workspace = true }
qualifier_attr = { workspace = true }
shuttle = { workspace = true, optional = true }
solana-clock = { workspace = true }
solana-entry = { workspace = true }
solana-hash = { workspace = true }
solana-ledger = { workspace = true }
solana-measure = { workspace = true }
solana-metrics = { workspace = true }
solana-poh-config = { workspace = true }
solana-pubkey = { workspace = true }
solana-runtime = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-time-utils = { workspace = true }
solana-transaction = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
assert_matches = { workspace = true }
bincode = { workspace = true }
criterion = { workspace = true }
rand = { workspace = true }
solana-entry = { workspace = true, features = ["dev-context-only-utils"] }
solana-keypair = { workspace = true }
solana-perf = { workspace = true, features = ["dev-context-only-utils"] }
solana-poh = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-sha256-hasher = { workspace = true }
solana-signer = { workspace = true }
solana-system-transaction = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dev-dependencies]
jemallocator = { workspace = true }

[[bench]]
name = "poh"

[[bench]]
name = "transaction_recorder"
harness = false

================
File: poh-bench/src/main.rs
================
fn main() {
⋮----
let matches = Command::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.long("max-num-entries")
.takes_value(true)
.value_name("SIZE")
.help("Number of entries."),
⋮----
.long("start-num-entries")
⋮----
.help("Packets per chunk"),
⋮----
.long("hashes-per-tick")
⋮----
.help("hashes per tick"),
⋮----
.long("num-transactions-per-entry")
⋮----
.value_name("NUM")
.help("Skip transaction sanity execution"),
⋮----
.long("iterations")
⋮----
.help("Number of iterations"),
⋮----
.long("num-threads")
⋮----
.help("Number of threads"),
⋮----
.get_matches();
let max_num_entries: u64 = matches.value_of_t("max_num_entries").unwrap_or(64);
⋮----
.value_of_t("start_num_entries")
.unwrap_or(max_num_entries);
let iterations: usize = matches.value_of_t("iterations").unwrap_or(10);
let hashes_per_tick: u64 = matches.value_of_t("hashes_per_tick").unwrap_or(10_000);
let start_hash = hash(&[1, 2, 3, 4]);
let ticks = create_ticks(max_num_entries, hashes_per_tick, start_hash);
⋮----
let num_threads = matches.value_of_t("num_threads").unwrap_or(num_cpus::get());
⋮----
.num_threads(num_threads)
.thread_name(|i| format!("solPohBench{i:02}"))
.build()
.expect("new rayon threadpool");
init_poh();
⋮----
assert!(ticks[..num_entries]
⋮----
time.stop();
println!(
⋮----
if is_x86_feature_detected!("avx2") && entry::api().is_some() {
⋮----
if is_x86_feature_detected!("avx512f") && entry::api().is_some() {
⋮----
println!();

================
File: poh-bench/Cargo.toml
================
[package]
name = "solana-poh-bench"
documentation = "https://docs.rs/solana-poh-bench"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
agave-logger = { workspace = true }
clap = { version = "3.1.5", features = ["cargo"] }
log = { workspace = true }
num_cpus = { workspace = true }
rayon = { workspace = true }
solana-entry = { workspace = true }
solana-measure = { workspace = true }
solana-perf = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-version = { workspace = true }

================
File: poseidon/src/legacy.rs
================
pub fn hashv(
⋮----
fn from(error: PoseidonError) -> Self {
⋮----
Poseidon::<Fr>::new_circom(vals.len()).map_err(PoseidonSyscallError::from)?;
⋮----
Endianness::BigEndian => hasher.hash_bytes_be(vals),
Endianness::LittleEndian => hasher.hash_bytes_le(vals),
⋮----
.map_err(PoseidonSyscallError::from)?;
Ok(PoseidonHash(res))
⋮----
sol_poseidon(
parameters.into(),
endianness.into(),
⋮----
vals.len() as u64,
⋮----
0 => Ok(PoseidonHash::new(hash_result)),
_ => Err(PoseidonSyscallError::Unexpected),
⋮----
pub fn hash(
⋮----
hashv(parameters, endianness, &[val])
⋮----
mod tests {
⋮----
fn test_poseidon_input_ones_be() {
⋮----
let hash = hash(Parameters::Bn254X5, Endianness::BigEndian, &input).unwrap();
assert_eq!(
⋮----
fn test_poseidon_input_ones_le() {
⋮----
let hash = hash(Parameters::Bn254X5, Endianness::LittleEndian, &input).unwrap();
⋮----
fn test_poseidon_input_ones_twos_be() {
⋮----
let hash = hashv(
⋮----
.unwrap();
⋮----
fn test_poseidon_input_ones_twos_le() {
⋮----
fn test_poseidon_input_one() {
⋮----
for (i, expected_hash) in expected_hashes.into_iter().enumerate() {
let inputs = vec![&input[..]; i + 1];
let hash = hashv(Parameters::Bn254X5, Endianness::BigEndian, &inputs).unwrap();
assert_eq!(hash.to_bytes(), expected_hash);
⋮----
fn test_poseidon_input_without_padding_be() {
⋮----
fn test_poseidon_input_without_padding_le() {
⋮----
let hash = hashv(Parameters::Bn254X5, Endianness::LittleEndian, &inputs).unwrap();

================
File: poseidon/src/lib.rs
================
use thiserror::Error;
⋮----
pub mod legacy;
⋮----
pub enum PoseidonSyscallError {
⋮----
fn from(error: u64) -> Self {
⋮----
fn from(error: PoseidonSyscallError) -> Self {
⋮----
pub enum Parameters {
⋮----
type Error = PoseidonSyscallError;
fn try_from(value: u64) -> Result<Self, Self::Error> {
⋮----
x if x == Parameters::Bn254X5 as u64 => Ok(Parameters::Bn254X5),
_ => Err(PoseidonSyscallError::InvalidParameters),
⋮----
fn from(value: Parameters) -> Self {
⋮----
pub enum Endianness {
⋮----
x if x == Endianness::BigEndian as u64 => Ok(Endianness::BigEndian),
x if x == Endianness::LittleEndian as u64 => Ok(Endianness::LittleEndian),
_ => Err(PoseidonSyscallError::InvalidEndianness),
⋮----
fn from(value: Endianness) -> Self {
⋮----
pub struct PoseidonHash(pub [u8; HASH_BYTES]);
impl PoseidonHash {
pub fn new(hash_array: [u8; HASH_BYTES]) -> Self {
Self(hash_array)
⋮----
pub fn to_bytes(&self) -> [u8; HASH_BYTES] {
⋮----
pub use solana_define_syscall::definitions::sol_poseidon;
⋮----
pub fn hashv(
⋮----
fn from(error: PoseidonError) -> Self {
⋮----
Poseidon::<Fr>::new_circom(vals.len()).map_err(PoseidonSyscallError::from)?;
⋮----
Endianness::BigEndian => hasher.hash_bytes_be(vals),
Endianness::LittleEndian => hasher.hash_bytes_le(vals),
⋮----
.map_err(PoseidonSyscallError::from)?;
Ok(PoseidonHash(res))
⋮----
sol_poseidon(
parameters.into(),
endianness.into(),
⋮----
vals.len() as u64,
⋮----
0 => Ok(PoseidonHash::new(hash_result)),
_ => Err(PoseidonSyscallError::Unexpected),
⋮----
pub fn hash(
⋮----
hashv(parameters, endianness, &[val])
⋮----
mod tests {
⋮----
fn test_poseidon_input_ones_be() {
⋮----
let hash = hash(Parameters::Bn254X5, Endianness::BigEndian, &input).unwrap();
assert_eq!(
⋮----
fn test_poseidon_input_ones_le() {
⋮----
let hash = hash(Parameters::Bn254X5, Endianness::LittleEndian, &input).unwrap();
⋮----
fn test_poseidon_input_ones_twos_be() {
⋮----
let hash = hashv(
⋮----
.unwrap();
⋮----
fn test_poseidon_input_ones_twos_le() {
⋮----
fn test_poseidon_input_one() {
⋮----
for (i, expected_hash) in expected_hashes.into_iter().enumerate() {
let inputs = vec![&input[..]; i + 1];
let hash = hashv(Parameters::Bn254X5, Endianness::BigEndian, &inputs).unwrap();
assert_eq!(hash.to_bytes(), expected_hash);
⋮----
fn test_poseidon_input_without_padding() {
⋮----
let res = hashv(Parameters::Bn254X5, Endianness::BigEndian, &inputs);
assert!(res.is_err());
let res = hashv(Parameters::Bn254X5, Endianness::LittleEndian, &inputs);

================
File: poseidon/Cargo.toml
================
[package]
name = "solana-poseidon"
description = "Solana Poseidon hashing"
documentation = "https://docs.rs/solana-poseidon"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
thiserror = { workspace = true }

[target.'cfg(not(target_os = "solana"))'.dependencies]
ark-bn254 = { workspace = true }
ark-bn254-0-4 = { workspace = true }
light-poseidon = { workspace = true }
light-poseidon-0-2 = { workspace = true }

[target.'cfg(target_os = "solana")'.dependencies]
solana-define-syscall = { workspace = true }

[lints]
workspace = true

================
File: precompiles/benches/ed25519_instructions.rs
================
extern crate test;
⋮----
fn create_test_instructions(message_length: u16) -> Vec<Instruction> {
⋮----
.map(|_| {
let mut rng = thread_rng();
⋮----
let message: Vec<u8> = (0..message_length).map(|_| rng.gen_range(0, 255)).collect();
let signature = privkey.sign(&message).to_bytes();
let pubkey = privkey.public.to_bytes();
new_ed25519_instruction_with_signature(&message, &signature, &pubkey)
⋮----
.collect()
⋮----
fn bench_ed25519_len_032(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(32);
let mut ix_iter = ixs.iter().cycle();
b.iter(|| {
let instruction = ix_iter.next().unwrap();
verify(&instruction.data, &[&instruction.data], &feature_set).unwrap();
⋮----
fn bench_ed25519_len_128(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(128);
⋮----
fn bench_ed25519_len_32k(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(32 * 1024);
⋮----
fn bench_ed25519_len_max(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(u16::MAX - required_extra_space);

================
File: precompiles/benches/secp256k1_instructions.rs
================
extern crate test;
⋮----
fn create_test_instructions(message_length: u16) -> Vec<Instruction> {
⋮----
.map(|_| {
let mut rng = thread_rng();
let secp_privkey = libsecp256k1::SecretKey::random(&mut thread_rng());
let message: Vec<u8> = (0..message_length).map(|_| rng.gen_range(0, 255)).collect();
⋮----
eth_address_from_pubkey(&secp_pubkey.serialize()[1..].try_into().unwrap());
⋮----
sign_message(&secp_privkey.serialize(), &message).unwrap();
new_secp256k1_instruction_with_signature(
⋮----
.collect()
⋮----
fn bench_secp256k1_len_032(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(32);
let mut ix_iter = ixs.iter().cycle();
b.iter(|| {
let instruction = ix_iter.next().unwrap();
verify(&instruction.data, &[&instruction.data], &feature_set).unwrap();
⋮----
fn bench_secp256k1_len_256(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(256);
⋮----
fn bench_secp256k1_len_32k(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(32 * 1024);
⋮----
fn bench_secp256k1_len_max(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(u16::MAX - required_extra_space);

================
File: precompiles/benches/secp256r1_instructions.rs
================
extern crate test;
⋮----
fn create_test_instructions(message_length: u16) -> Vec<Instruction> {
⋮----
.map(|_| {
let mut rng = thread_rng();
let group = EcGroup::from_curve_name(Nid::X9_62_PRIME256V1).unwrap();
let secp_privkey = EcKey::generate(&group).unwrap();
let message: Vec<u8> = (0..message_length).map(|_| rng.gen_range(0, 255)).collect();
⋮----
sign_message(&message, &secp_privkey.private_key_to_der().unwrap()).unwrap();
let mut ctx = BigNumContext::new().unwrap();
⋮----
.public_key()
.to_bytes(
⋮----
.unwrap();
new_secp256r1_instruction_with_signature(
⋮----
&pubkey.try_into().unwrap(),
⋮----
.collect()
⋮----
fn bench_secp256r1_len_032(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(32);
let mut ix_iter = ixs.iter().cycle();
b.iter(|| {
let instruction = ix_iter.next().unwrap();
verify(&instruction.data, &[&instruction.data], &feature_set).unwrap();
⋮----
fn bench_secp256r1_len_256(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(256);
⋮----
fn bench_secp256r1_len_32k(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(32 * 1024);
⋮----
fn bench_secp256r1_len_max(b: &mut Bencher) {
⋮----
let ixs = create_test_instructions(u16::MAX - required_extra_space);

================
File: precompiles/src/ed25519.rs
================
pub fn verify(
⋮----
if data.len() < SIGNATURE_OFFSETS_START {
return Err(PrecompileError::InvalidInstructionDataSize);
⋮----
if num_signatures == 0 && data.len() > SIGNATURE_OFFSETS_START {
⋮----
.saturating_mul(SIGNATURE_OFFSETS_SERIALIZED_SIZE)
.saturating_add(SIGNATURE_OFFSETS_START);
if data.len() < expected_data_size {
⋮----
core::ptr::read_unaligned(data.as_ptr().add(start) as *const Ed25519SignatureOffsets)
⋮----
let signature = get_data_slice(
⋮----
Signature::from_bytes(signature).map_err(|_| PrecompileError::InvalidSignature)?;
let pubkey = get_data_slice(
⋮----
.map_err(|_| PrecompileError::InvalidPublicKey)?;
let message = get_data_slice(
⋮----
if feature_set.is_active(&ed25519_precompile_verify_strict::id()) {
⋮----
.verify_strict(message, &signature)
.map_err(|_| PrecompileError::InvalidSignature)?;
⋮----
.verify(message, &signature)
⋮----
Ok(())
⋮----
fn get_data_slice<'a>(
⋮----
if signature_index >= instruction_datas.len() {
return Err(PrecompileError::InvalidDataOffsets);
⋮----
let end = start.saturating_add(size);
if end > instruction.len() {
⋮----
Ok(&instruction[start..end])
⋮----
pub mod tests {
⋮----
pub fn new_ed25519_instruction_raw(
⋮----
assert_eq!(pubkey.len(), PUBKEY_SERIALIZED_SIZE);
assert_eq!(signature.len(), SIGNATURE_SERIALIZED_SIZE);
⋮----
.saturating_add(SIGNATURE_SERIALIZED_SIZE)
.saturating_add(PUBKEY_SERIALIZED_SIZE)
.saturating_add(message.len()),
⋮----
let signature_offset = public_key_offset.saturating_add(PUBKEY_SERIALIZED_SIZE);
let message_data_offset = signature_offset.saturating_add(SIGNATURE_SERIALIZED_SIZE);
instruction_data.extend_from_slice(bytes_of(&[num_signatures, 0]));
⋮----
message_data_size: message.len() as u16,
⋮----
instruction_data.extend_from_slice(bytes_of(&offsets));
debug_assert_eq!(instruction_data.len(), public_key_offset);
instruction_data.extend_from_slice(pubkey);
debug_assert_eq!(instruction_data.len(), signature_offset);
instruction_data.extend_from_slice(signature);
debug_assert_eq!(instruction_data.len(), message_data_offset);
instruction_data.extend_from_slice(message);
⋮----
accounts: vec![],
⋮----
fn test_case(
⋮----
assert_eq!(
⋮----
let mut instruction_data = vec![0u8; DATA_START];
instruction_data[0..SIGNATURE_OFFSETS_START].copy_from_slice(bytes_of(&num_signatures));
instruction_data[SIGNATURE_OFFSETS_START..DATA_START].copy_from_slice(bytes_of(offsets));
test_verify_with_alignment(
⋮----
fn test_invalid_offsets() {
⋮----
instruction_data[0..SIGNATURE_OFFSETS_START].copy_from_slice(bytes_of(&1u16));
instruction_data[SIGNATURE_OFFSETS_START..DATA_START].copy_from_slice(bytes_of(&offsets));
instruction_data.truncate(instruction_data.len() - 1);
⋮----
fn test_message_data_offsets() {
⋮----
fn test_pubkey_offset() {
⋮----
fn test_signature_offset() {
⋮----
fn test_ed25519() {
⋮----
let privkey = ed25519_dalek::Keypair::generate(&mut thread_rng());
⋮----
let signature = privkey.sign(message_arr).to_bytes();
let pubkey = privkey.public.to_bytes();
⋮----
new_ed25519_instruction_with_signature(message_arr, &signature, &pubkey);
⋮----
assert!(test_verify_with_alignment(
⋮----
let index = thread_rng().gen_range(0, instruction.data.len());
⋮----
instruction.data[index] = instruction.data[index].wrapping_add(12);
⋮----
fn test_offsets_to_ed25519_instruction() {
⋮----
messages.len() * SIGNATURE_OFFSETS_SERIALIZED_SIZE + SIGNATURE_OFFSETS_START;
⋮----
.into_iter()
.map(|message| {
⋮----
data_offset += SIGNATURE_SERIALIZED_SIZE + message.len();
⋮----
.unzip();
let mut instruction = offsets_to_ed25519_instruction(&offsets);
let pubkey = privkey.public.as_ref();
instruction.data.extend_from_slice(pubkey);
⋮----
let signature = privkey.sign(message).to_bytes();
instruction.data.extend_from_slice(&signature);
instruction.data.extend_from_slice(message);
⋮----
fn test_ed25519_malleability() {
⋮----
let instruction = new_ed25519_instruction_with_signature(message_arr, &signature, &pubkey);
⋮----
.unwrap();
let signature = &hex::decode("00000000000000000000000000000000000000000000000000000000000000009472a69cd9a701a50d130ed52189e2455b23767db52cacb8716fb896ffeeac09").unwrap();
⋮----
let instruction = new_ed25519_instruction_raw(pubkey, signature, message);

================
File: precompiles/src/lib.rs
================
pub mod ed25519;
pub mod secp256k1;
pub mod secp256r1;
pub type Verify = fn(&[u8], &[&[u8]], &FeatureSet) -> std::result::Result<(), PrecompileError>;
pub struct Precompile {
⋮----
impl Precompile {
pub fn new(program_id: Pubkey, feature: Option<Pubkey>, verify_fn: Verify) -> Self {
⋮----
pub fn check_id<F>(&self, program_id: &Pubkey, is_enabled: F) -> bool
⋮----
.is_none_or(|ref feature_id| is_enabled(feature_id))
⋮----
pub fn verify(
⋮----
vec![
⋮----
pub fn is_precompile<F>(program_id: &Pubkey, is_enabled: F) -> bool
⋮----
.iter()
.any(|precompile| precompile.check_id(program_id, |feature_id| is_enabled(feature_id)))
⋮----
pub fn get_precompile<F>(program_id: &Pubkey, is_enabled: F) -> Option<&Precompile>
⋮----
.find(|precompile| precompile.check_id(program_id, |feature_id| is_enabled(feature_id)))
⋮----
pub fn get_precompiles<'a>() -> &'a [Precompile] {
⋮----
pub fn verify_if_precompile(
⋮----
for precompile in PRECOMPILES.iter() {
if precompile.check_id(program_id, |feature_id| feature_set.is_active(feature_id)) {
⋮----
.map(|instruction| instruction.data.as_ref())
.collect();
return precompile.verify(
⋮----
Ok(())
⋮----
pub(crate) fn test_verify_with_alignment(
⋮----
let mut instruction_data_copy = vec![0u8; instruction_data.len().checked_add(1).unwrap()];
instruction_data_copy[0..instruction_data.len()].copy_from_slice(instruction_data);
let result = verify(
&instruction_data_copy[..instruction_data.len()],
⋮----
instruction_data_copy[1..].copy_from_slice(instruction_data);
let result_shifted = verify(&instruction_data_copy[1..], instruction_datas, feature_set);
assert_eq!(result, result_shifted);

================
File: precompiles/src/secp256k1.rs
================
pub fn verify(
⋮----
if data.is_empty() {
return Err(PrecompileError::InvalidInstructionDataSize);
⋮----
if count == 0 && data.len() > 1 {
⋮----
.saturating_mul(SIGNATURE_OFFSETS_SERIALIZED_SIZE)
.saturating_add(1);
if data.len() < expected_data_size {
⋮----
let end = start.saturating_add(SIGNATURE_OFFSETS_SERIALIZED_SIZE);
⋮----
.map_err(|_| PrecompileError::InvalidSignature)?;
⋮----
if signature_index >= instruction_datas.len() {
⋮----
let sig_end = sig_start.saturating_add(SIGNATURE_SERIALIZED_SIZE);
if sig_end >= signature_instruction.len() {
return Err(PrecompileError::InvalidSignature);
⋮----
.map_err(|_| PrecompileError::InvalidRecoveryId)?;
let eth_address_slice = get_data_slice(
⋮----
let message_slice = get_data_slice(
⋮----
hasher.update(message_slice);
let message_hash = hasher.finalize();
⋮----
&libsecp256k1::Message::parse_slice(&message_hash).unwrap(),
⋮----
let eth_address = eth_address_from_pubkey(&pubkey.serialize()[1..].try_into().unwrap());
⋮----
Ok(())
⋮----
fn get_data_slice<'a>(
⋮----
return Err(PrecompileError::InvalidDataOffsets);
⋮----
let end = start.saturating_add(size);
if end > signature_instruction.len() {
⋮----
Ok(&instruction_datas[signature_index][start..end])
⋮----
pub mod tests {
⋮----
fn test_case(
⋮----
let mut instruction_data = vec![0u8; DATA_START];
⋮----
bincode::serialize_into(writer, &offsets).unwrap();
⋮----
test_verify_with_alignment(verify, &instruction_data, &[&[0u8; 100]], &feature_set)
⋮----
fn test_invalid_offsets() {
⋮----
instruction_data.truncate(instruction_data.len() - 1);
⋮----
assert_eq!(
⋮----
fn test_message_data_offsets() {
⋮----
fn test_eth_offset() {
⋮----
fn test_signature_offset() {
⋮----
fn test_count_is_zero_but_sig_data_exists() {
⋮----
fn test_secp256k1() {
⋮----
let secp_privkey = libsecp256k1::SecretKey::random(&mut thread_rng());
⋮----
eth_address_from_pubkey(&secp_pubkey.serialize()[1..].try_into().unwrap());
⋮----
sign_message(&secp_privkey.serialize(), message_arr).unwrap();
let mut instruction = new_secp256k1_instruction_with_signature(
⋮----
assert!(test_verify_with_alignment(
⋮----
let index = thread_rng().gen_range(0, instruction.data.len());
instruction.data[index] = instruction.data[index].wrapping_add(12);
⋮----
fn test_malleability() {
⋮----
let secret_key = libsecp256k1::SecretKey::random(&mut thread_rng());
⋮----
let eth_address = eth_address_from_pubkey(&public_key.serialize()[1..].try_into().unwrap());
⋮----
hasher.hash(message);
hasher.result()
⋮----
let secp_message = libsecp256k1::Message::parse(message_hash.as_bytes());
⋮----
let alt_recovery_id = libsecp256k1::RecoveryId::parse(recovery_id.serialize() ^ 1).unwrap();
let mut data: Vec<u8> = vec![];
let mut both_offsets = vec![];
⋮----
for (signature, recovery_id) in sigs.iter() {
let signature_offset = data.len();
data.extend(signature.serialize());
data.push(recovery_id.serialize());
let eth_address_offset = data.len();
data.extend(eth_address);
let message_data_offset = data.len();
data.extend(message);
⋮----
message_data_size: message.len() as u16,
⋮----
both_offsets.push(offsets);
⋮----
let mut instruction_data: Vec<u8> = vec![2];
⋮----
let offsets = bincode::serialize(&offsets).unwrap();
instruction_data.extend(offsets);
⋮----
instruction_data.extend(data);
test_verify_with_alignment(
⋮----
.unwrap();

================
File: precompiles/src/secp256r1.rs
================
pub fn verify(
⋮----
if data.len() < SIGNATURE_OFFSETS_START {
return Err(PrecompileError::InvalidInstructionDataSize);
⋮----
.saturating_mul(SIGNATURE_OFFSETS_SERIALIZED_SIZE)
.saturating_add(SIGNATURE_OFFSETS_START);
if data.len() < expected_data_size {
⋮----
BigNum::from_slice(&SECP256R1_HALF_ORDER).map_err(|_| PrecompileError::InvalidSignature)?;
⋮----
.map_err(|_| PrecompileError::InvalidSignature)?;
let one = BigNum::from_u32(1).map_err(|_| PrecompileError::InvalidSignature)?;
⋮----
let mut ctx = BigNumContext::new().map_err(|_| PrecompileError::InvalidSignature)?;
⋮----
core::ptr::read_unaligned(data.as_ptr().add(start) as *const Secp256r1SignatureOffsets)
⋮----
let signature = get_data_slice(
⋮----
let pubkey = get_data_slice(
⋮----
let message = get_data_slice(
⋮----
return Err(PrecompileError::InvalidSignature);
⋮----
.and_then(|sig| sig.to_der())
⋮----
.map_err(|_| PrecompileError::InvalidPublicKey)?;
⋮----
PKey::from_ec_key(public_key).map_err(|_| PrecompileError::InvalidPublicKey)?;
⋮----
.update(message)
⋮----
.verify(&ecdsa_sig)
.map_err(|_| PrecompileError::InvalidSignature)?
⋮----
Ok(())
⋮----
fn get_data_slice<'a>(
⋮----
if signature_index >= instruction_datas.len() {
return Err(PrecompileError::InvalidDataOffsets);
⋮----
let end = start.saturating_add(size);
if end > instruction.len() {
⋮----
Ok(&instruction[start..end])
⋮----
mod tests {
⋮----
fn test_case(
⋮----
assert_eq!(
⋮----
let mut instruction_data = vec![0u8; DATA_START];
instruction_data[0..SIGNATURE_OFFSETS_START].copy_from_slice(bytes_of(&num_signatures));
instruction_data[SIGNATURE_OFFSETS_START..DATA_START].copy_from_slice(bytes_of(offsets));
test_verify_with_alignment(
⋮----
fn test_invalid_offsets() {
⋮----
instruction_data[0..SIGNATURE_OFFSETS_START].copy_from_slice(bytes_of(&1u16));
instruction_data[SIGNATURE_OFFSETS_START..DATA_START].copy_from_slice(bytes_of(&offsets));
instruction_data.truncate(instruction_data.len() - 1);
⋮----
fn test_invalid_signature_data_size() {
⋮----
let small_data = vec![0u8; SIGNATURE_OFFSETS_START - 1];
⋮----
let mut zero_sigs_data = vec![0u8; DATA_START];
⋮----
let mut too_many_sigs = vec![0u8; DATA_START];
⋮----
fn test_message_data_offsets() {
⋮----
fn test_pubkey_offset() {
⋮----
fn test_signature_offset() {
⋮----
fn test_secp256r1() {
⋮----
let group = EcGroup::from_curve_name(Nid::X9_62_PRIME256V1).unwrap();
let signing_key = EcKey::generate(&group).unwrap();
⋮----
sign_message(message_arr, &signing_key.private_key_to_der().unwrap()).unwrap();
let mut ctx = BigNumContext::new().unwrap();
⋮----
.public_key()
.to_bytes(
⋮----
.unwrap();
let mut instruction = new_secp256r1_instruction_with_signature(
⋮----
&pubkey.try_into().unwrap(),
⋮----
assert!(test_verify_with_alignment(
⋮----
let message_byte_index = instruction.data.len() - 1;
⋮----
instruction.data[message_byte_index].wrapping_add(12);
⋮----
fn test_secp256r1_high_s() {
⋮----
let tx_pass = test_verify_with_alignment(
⋮----
instruction.data.as_slice(),
&[instruction.data.as_slice()],
⋮----
assert!(tx_pass.is_ok());
⋮----
let order = BigNum::from_slice(&SECP256R1_ORDER).unwrap();
⋮----
BigNum::from_slice(&instruction.data[s_offset..s_offset + FIELD_SIZE]).unwrap();
let mut high_s = BigNum::new().unwrap();
high_s.checked_sub(&order, &current_s).unwrap();
instruction.data[s_offset..s_offset + FIELD_SIZE].copy_from_slice(&high_s.to_vec());
let tx_fail = test_verify_with_alignment(
⋮----
assert!(tx_fail.unwrap_err() == PrecompileError::InvalidSignature);
⋮----
fn test_new_secp256r1_instruction_31byte_components() {
⋮----
let instruction = new_secp256r1_instruction_with_signature(
⋮----
let r_bn = BigNum::from_slice(r).unwrap();
let s_bn = BigNum::from_slice(s).unwrap();
let r_bytes = r_bn.to_vec();
let s_bytes = s_bn.to_vec();
if r_bytes.len() == 31 || s_bytes.len() == 31 {
⋮----
fn test_secp256r1_order() {
⋮----
let mut openssl_order = BigNum::new().unwrap();
group.order(&mut openssl_order, &mut ctx).unwrap();
let our_order = BigNum::from_slice(&SECP256R1_ORDER).unwrap();
assert_eq!(our_order, openssl_order);
⋮----
fn test_secp256r1_order_minus_one() {
⋮----
let mut expected_order_minus_one = BigNum::new().unwrap();
⋮----
.checked_sub(&openssl_order, &BigNum::from_u32(1).unwrap())
⋮----
let our_order_minus_one = BigNum::from_slice(&SECP256R1_ORDER_MINUS_ONE).unwrap();
assert_eq!(our_order_minus_one, expected_order_minus_one);
⋮----
fn test_secp256r1_half_order() {
⋮----
let mut calculated_half_order = BigNum::new().unwrap();
let two = BigNum::from_u32(2).unwrap();
⋮----
.checked_div(&openssl_order, &two, &mut ctx)
⋮----
let our_half_order = BigNum::from_slice(&SECP256R1_HALF_ORDER).unwrap();
assert_eq!(calculated_half_order, our_half_order);
⋮----
fn test_secp256r1_order_relationships() {
⋮----
let mut expected_half_order = BigNum::new().unwrap();
⋮----
.checked_div(&openssl_order, &BigNum::from_u32(2).unwrap(), &mut ctx)
⋮----
assert_eq!(our_half_order, expected_half_order);
let mut double_half_order = BigNum::new().unwrap();
⋮----
.checked_mul(&our_half_order, &BigNum::from_u32(2).unwrap(), &mut ctx)
⋮----
assert_eq!(double_half_order, expected_order_minus_one);

================
File: precompiles/Cargo.toml
================
[package]
name = "agave-precompiles"
description = "Solana precompiled programs."
documentation = "https://docs.rs/agave-precompiles"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]
all-features = true
rustdoc-args = ["--cfg=docsrs"]

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
bincode = { workspace = true }
digest = { workspace = true }
ed25519-dalek = { workspace = true }
libsecp256k1 = { workspace = true, features = ["hmac"] }
openssl = { workspace = true }
sha3 = { workspace = true }
solana-ed25519-program = { workspace = true }
solana-message = { workspace = true }
solana-precompile-error = { workspace = true }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-secp256k1-program = { workspace = true, features = ["serde"] }
solana-secp256r1-program = { workspace = true, features = ["openssl-vendored"] }

[dev-dependencies]
agave-logger = { workspace = true }
agave-precompiles = { path = ".", features = ["agave-unstable-api"] }
bytemuck = { workspace = true }
hex = { workspace = true }
rand0-7 = { workspace = true }
solana-instruction = { workspace = true }
solana-keccak-hasher = { workspace = true, features = ["sha3"] }
solana-secp256k1-program = { workspace = true, features = ["bincode"] }

[lints]
workspace = true

================
File: privacy.md
================
Last Updated: October 9, 2025

**Privacy Policy**

This Privacy Policy (the “Policy”) sets out how your information may be collected, used, processed or disclosed by Jito Labs, Inc. (“Jito Labs”, “we”, “us,” or “our”) in connection with your access and/or use of the website available at www.bam.dev (the “Site”) and attendant tools or services (together with the Site, the “Services”) for the Block Assembly Marketplace (“BAM”), including those available at the GitHub repo [here](https://github.com/jito-foundation/jito-solana).  

By accessing any Services, you agree to this Policy as governing your use of the Services. This Policy applies to use of the Services, regardless of where or how they are accessed (including outside of or other than via the Site), and regardless of which device is used for access.  

This Policy does not apply to any products, services, websites, or content offered or provided by third parties and we are not responsible in any way for those third party offerings; please review any separate privacy policies or terms made available by those third parties.

We may update this Policy from time to time.  If we make any changes, we will change the “Last Updated” date above. Any modifications to this Policy will be effective upon our posting of the updated Policy.  In all cases, your continued use of the Site following the posting of any updated Policy indicates your acceptance of the updated Policy.

Please read this Policy carefully to ensure you understand it.

1. **Information We Collect**

This provision applies only to Services hosted or operated by Jito, including the Site.  Information is not generally collected via open source software developed by Jito and downloaded and/or run by third parties, including third party validators. 

We may collect automatically certain limited information when you access, use, or interact with the Services, such as:

* Device type, browser type and version

* Operating system and version

* Internet Protocol (“IP”) address and other location information

* Internet service provider information

* API keys

* Unique Universal Identifier (“UUID”) keys

* Publicly available blockchain data, including time stamps and number of packets sent

* Blockchain account information or wallet addresses, including signer and auth pubkeys as well as transaction signatures

* Device user agent, client library version

* Analytics about aggregate numbers of users and usage types (*e.g.*, number of page views, event counts, and aggregate acquisition metrics), and information such as how many users in the aggregate are using certain features; and aggregate location data across users including countries and regions from which users access the Site

When you access the Site, we may collect the following information:

* Contact information, such as name, email address, profile picture, user name or social media information; or your preferences for receiving marketing information and communication from us when you opt in to various programs, sign up to receive information from us, submit forms via the Site or forms that are available on the Site, request information about the Site, send in support requests/provide feedback or otherwise voluntarily interact with the Site

* Personal information from individuals when we attend or host conferences, trade shows, and other events

* Personal information from individuals and third parties to assess and pursue potential business opportunities

* Personal information, such as your application, CV, cover letter, and/or any other information you provide to us,  if you respond to a job opening or opportunities on our Site

* Any information you voluntarily provide to us

* Any information about you provided to us by third parties with whom we work (including contractors, service providers and analytics providers)

For information about how we use tracking technology, please see our Cookie Policy.

We receive additional information about you from third parties we work with (including contractors, service providers, and analytics providers).  This information may be combined with other information you provide to us or that we automatically collect.  We may also obtain information about you through our analysis of blockchain information.

2. **How We Use Information**

We use your information for various business and administrative purposes, such as to:

* Provide, maintain, support and improve the Site, respond to inquiries regarding the Site and respond to your feedback about the Site;

* Detect, investigate and prevent fraudulent, abusive or harmful activities, including detecting potential security incidents; 

* Analyze usage trends to the Site;

* Ensure internal quality control and safety;

* Enforce this Policy and/or other policies;

* Take certain actions that you have asked us to and provided consent for us to take; 

* Communicate with you about the Site or the activities thereon; 

* Processing applications if you apply for a job we post on our Site;

* Comply with any legal or regulation obligations, including in the event we have a legal obligation to collect, use or retain information about you or comply with legal requests from government authorities or in private suits.

You agree and acknowledge that we may use information that does not identify you (including information that has been aggregated or de-identified) for any purpose except as prohibited by applicable law.

We may use personal information for other purposes that are clearly disclosed to you at the time you provide it or with your consent.

We may use personal information to create de-identified and/or aggregated information, such as demographic information, information about the device from which you access our Site, or other analyses we create.

3. **How We Share Information**

We may disclose your personal information to third parties for a variety of business purposes, including to provide our Site, to protect us or others, or in other events, as described below:

* As described above in “Information We Collect,” our Site may allow you to share personal information or interact with third parties (including individuals and third parties who do not use our Site and the general public), including those third-party service providers and vendors that assist us with the provision of our Site.  This includes service providers and vendors that provide us with IT support, hosting, payment processing, customer service, and related services.

* We may use third-party application program interfaces (“APIs”) and software development kits (“SDKs”) as part of the functionality of our Site.  SDKs typically collect your mobile device identifier and information related to your use of the Site.  SDKs may allow third parties including analytics and advertising partners to collect your information for various purposes including to provide analytics services and personalized advertising.  For more information about our use of APIs and SDKs, please contact us as set forth in “Contact Us” below. 

* We may share information about you when we believe providing such information is necessary to comply with law, and other agreements with you, or protect the rights, property, or security of the Company, our agents and employees or users.  This includes sharing information with other organizations for fraud prevention and detection purposes.

* We may share your personal information with business partners if we are involved in a merger, acquisition, financing, reorganization, bankruptcy, receivership, purchase or sale of assets, or transition of service to another provider.

4. **Location of Information** 

Please be aware that information collected through the Site may be processed, stored, and used in the United States as well as other jurisdictions. Data protection laws in the United States may be different from those of your country of residence. Your use of the Site or provision of any information therefore constitutes your consent to the transfer to and from, processing, usage, sharing, and storage of information about you in the United States and other jurisdictions as set out in this Policy.

5. **Access, Deletion, Choice**

If you need to update or delete certain information about you or your interactions with the Site, you can contact us at privacy@bam.dev.

You may have choices about the collection and use of information about you.  You can choose not to provide certain information, but then you might not be able to use the Site.

If you do not want to receive messages from us about the Site or other matters related to the Company and its technology, please notify us.

The help feature on most browsers and devices will tell you how to prevent your browser or device from accepting new cookies, how to have the browser notify you when you receive a new cookie, or how to disable cookies altogether.  Please see our Cookie Policy for more information.

In accordance with applicable law, you may have the right to:

* Confirm whether we are processing your personal information;

* Access and port your personal information, including: (i) obtaining access to or a copy of your personal information; and (ii) receiving an electronic copy of personal information that you have provided to us, or asking us to send that information to another company in a structured, commonly used, and machine readable format (also known as the “right of data portability”);

* Request:

  * Correction of your personal information where it is inaccurate or incomplete;

  * Deletion of your personal information;

  * To opt-out of certain processing activities, including, as applicable, if we process your personal information for “targeted advertising” (as “targeted advertising” is defined by applicable privacy laws), if we “sell” your personal information (as “sell” is defined by applicable privacy laws), or if we engage in “profiling” in furtherance of certain “decisions that produce legal or similarly significant effects” concerning you (as such terms are defined by applicable privacy laws);

  * Restriction of or object to our processing of your personal information;

* Withdraw your consent to our processing of your personal information. Please note that your withdrawal will only take effect for future processing, and will not affect the lawfulness of processing before the withdrawal; and

* Appeal any decision to decline to process your request.

For additional information related to choices on data privacy in certain jurisdictions, please see Section 9 below.

If you would like to exercise any of these rights, please contact us privacy@bam.dev. We will process such requests in accordance with applicable laws.

6. **Children’s Information**

The Site is intended for general users who are 18 years of age or older, and are in no way directed at children.  We do not knowingly collect personal information from children.  If you believe such information has been collected in error, please email [privacy@bam.dev](mailto:privacy@bam.dev) to notify us of this.

7. **Retention of Information**

We store the personal information we collect as described in this Privacy Policy for as long as you use our Site, or as necessary to fulfill the purpose(s) for which it was collected, provide our Site, resolve disputes, establish legal defenses, conduct audits, pursue legitimate business purposes, enforce our agreements, and comply with applicable laws.  

To determine the appropriate retention period for personal information, we may consider applicable legal requirements, the amount, nature, and sensitivity of the personal information, certain risk factors, the purposes for which we process your personal information, and whether we can achieve those purposes through other means.

8. **Notices**

If you have any questions about this Privacy Policy, please Contact Us at privacy@bam.dev.

If you interact with the Site on behalf of or through your organization, then your information may also be subject to your organization’s privacy practices and you should direct privacy inquiries to your organization.

9. **Additional Information for Certain Jurisdictions**

<u>*California*</u>

If you are a California resident, you have certain additional rights with respect to personal information about you under the California Consumer Privacy Act of 2018 (“CCPA”).

We are required to inform you of:

* What categories of information we may collect about you, including during the preceding 12 months;

* The purposes for which we may use your personal information, including during the preceding 12 months;

* The purposes of which we may share your personal information, including during the preceding 12 months;

* In the preceding 12 months, we have not sold any personal information of consumers.

You have the right to request to know: (i) the categories of personal information we have collected about you in the last 12 months; (ii) the specific pieces of personal information we have about you; (iii) the categories of sources from which that personal information was collected; (iv) the categories of your personal information that we sold or disclosed in the last 12 months, if applicable; (v) the categories of third parties to whom your personal information was sold or disclosed in the last 12 months; and (vi) the purpose for collecting and selling your personal information, if applicable. These rights are subject to limitations as described in the relevant law. We may deny your request if we need to do so to comply with our legal rights or obligations.

We will not discriminate against any user for exercising their CCPA rights.

You may exercise these rights yourself or you may designate an authorized agent to make these requests on your behalf. To protect your information, we may need to verify your identity before processing your request, including by collecting additional information to verify your identity, such as government issued identification documents. We will not fulfill your request unless you have provided sufficient information for us to reasonably verify you are the individual about whom we collected personal information. We will only use the personal information provided in the verification process to verify your identity or authority to make a request and to track and document request responses, unless you initially provided the information for another purpose. When we verify your agent’s request, we may verify your identity and request a signed document from your agent that authorizes your agent to make the request on your behalf. To protect your personal information, we reserve the right to deny a request from an agent who does not submit proof that they have been authorized by you to act on their behalf.

If you would like to exercise any of these rights, please contact us at privacy@bam.dev..

<u>*European Economic Area, the United Kingdom, and Switzerland*</u>

If you are a data subject in the European Economic Area, the United Kingdom, or Switzerland, you have certain rights with respect to your personal data pursuant to the General Data Protection Regulation of the European Union (“GDPR”) and similar laws. This section applies to you.

References to “personal information” in this Policy are equivalent to “personal data” as defined under GDPR.

We process your personal data as necessary and in reliance on the legal bases below, in order to:

* Provide access to and improve the Site;

* Comply with applicable laws and our legal obligations, and prevent fraud;

* Ensure safety and security of your data, the Site and the Company;

* Send communications, or for research or analytics; and

* Comply with your consent, which may be withdrawn at any time by communicating your withdrawal to us.

You may: (i) ask whether we have any personal data about you and request a copy of such personal data; (ii) request that we update or correct inaccuracies in your personal data; (iii) request that we delete your personal data; (iv) request a portable copy of your personal data; (v) request that we restrict the processing of your personal data if such processing is inappropriate; and (vi) object to our processing of your personal data. These rights are subject to applicable law.

If you would like to exercise any of these rights, please contact us.  To protect your information, we may need to verify your identity before processing your request, including by collecting additional information about your identity, such as government issued identification documents. 

If your personal information is subject to the applicable data protection laws of the European Economic Area, Switzerland, or the United Kingdom, you have the right to lodge a complaint with the competent supervisory authority or attorney general if you believe our processing of your personal information violates applicable law. 

All information processed by us may be transferred, processed, and stored anywhere in the world, including, but not limited to, the United States or other countries, which may have data protection laws that are different from the laws where you live.  We endeavor to safeguard your information consistent with the requirements of applicable laws.  To the extent that we transfer your personal data outside the European Economic Area, the United Kingdom or Switzerland, as applicable, we will do so in accordance with the terms of this Policy and applicable data protection law.

================
File: program-binaries/src/lib.rs
================
mod spl_memo_1_0 {
⋮----
mod spl_memo_3_0 {
⋮----
pub mod jito_tip_payment {
⋮----
pub mod jito_tip_distribution {
⋮----
include_bytes!("programs/spl_token-3.5.0.so"),
⋮----
include_bytes!("programs/spl_token_2022-8.0.0.so"),
⋮----
include_bytes!("programs/spl_memo-1.0.0.so"),
⋮----
include_bytes!("programs/spl_memo-3.0.0.so"),
⋮----
include_bytes!("programs/spl_associated_token_account-1.1.1.so"),
⋮----
include_bytes!("programs/spl-jito_tip_distribution-0.1.10.so"),
⋮----
include_bytes!("programs/spl-jito_tip_payment-0.1.10.so"),
⋮----
include_bytes!("programs/core_bpf_address_lookup_table-3.0.0.so"),
⋮----
include_bytes!("programs/core_bpf_config-3.0.0.so"),
⋮----
include_bytes!("programs/core_bpf_feature_gate-0.0.1.so"),
⋮----
include_bytes!("programs/core_bpf_stake-1.0.1.so"),
⋮----
fn bpf_loader_program_account(program_id: &Pubkey, elf: &[u8], rent: &Rent) -> (Pubkey, Account) {
⋮----
lamports: rent.minimum_balance(elf.len()).max(1),
data: elf.to_vec(),
⋮----
pub fn bpf_loader_upgradeable_program_accounts(
⋮----
let programdata_address = get_program_data_address(program_id);
⋮----
let lamports = rent.minimum_balance(space);
⋮----
.unwrap();
⋮----
let space = UpgradeableLoaderState::size_of_programdata_metadata() + elf.len();
⋮----
upgrade_authority_address: Some(Pubkey::default()),
⋮----
data.extend_from_slice(elf);
⋮----
pub fn spl_programs(rent: &Rent) -> Vec<(Pubkey, AccountSharedData)> {
⋮----
.iter()
.flat_map(|(program_id, loader_id, elf)| {
let mut accounts = vec![];
if loader_id.eq(&solana_sdk_ids::bpf_loader_upgradeable::ID) {
for (key, account) in bpf_loader_upgradeable_program_accounts(program_id, elf, rent)
⋮----
accounts.push((key, AccountSharedData::from(account)));
⋮----
let (key, account) = bpf_loader_program_account(program_id, elf, rent);
⋮----
.collect()
⋮----
pub fn core_bpf_programs<F>(rent: &Rent, is_feature_active: F) -> Vec<(Pubkey, AccountSharedData)>
⋮----
.flat_map(|(program_id, feature_id, elf)| {
⋮----
if feature_id.is_none() || feature_id.is_some_and(|f| is_feature_active(&f)) {
⋮----
pub fn by_id(program_id: &Pubkey, rent: &Rent) -> Option<Vec<(Pubkey, AccountSharedData)>> {
let programs = spl_programs(rent);
if let Some(i) = programs.iter().position(|(key, _)| key == program_id) {
let n = num_accounts(programs[i].1.owner());
return Some(programs.into_iter().skip(i).take(n).collect());
⋮----
let programs = core_bpf_programs(rent, |_| true);
⋮----
fn num_accounts(owner_id: &Pubkey) -> usize {

================
File: program-binaries/Cargo.toml
================
[package]
name = "solana-program-binaries"
description = "Prebuilt SPL and Core BPF programs"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
bincode = { workspace = true }
serde = { workspace = true }
solana-account = { workspace = true }
solana-loader-v3-interface = { workspace = true, features = ["bincode", "serde"] }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-sdk-ids = { workspace = true }
spl-generic-token = { workspace = true }

================
File: program-runtime/src/cpi.rs
================
pub enum CpiError {
⋮----
type Error = Box<dyn std::error::Error>;
⋮----
struct SolInstruction {
⋮----
struct SolAccountMeta {
⋮----
struct SolAccountInfo {
⋮----
struct SolSignerSeedC {
⋮----
struct SolSignerSeedsC {
⋮----
fn check_account_info_pointer(
⋮----
ic_msg!(
⋮----
return Err(Box::new(CpiError::InvalidPointer));
⋮----
Ok(())
⋮----
fn check_instruction_size(num_accounts: usize, data_len: usize) -> Result<(), Error> {
⋮----
return Err(Box::new(CpiError::MaxInstructionAccountsExceeded {
⋮----
return Err(Box::new(CpiError::MaxInstructionDataLenExceeded {
⋮----
fn check_account_infos(
⋮----
.get_feature_set()
⋮----
return Err(Box::new(CpiError::MaxInstructionAccountInfosExceeded {
⋮----
fn check_authorized_program(
⋮----
|| invoke_context.is_precompile(program_id)
⋮----
return Err(Box::new(CpiError::ProgramNotSupported(*program_id)));
⋮----
pub struct CallerAccount<'a> {
⋮----
// The original data length of the account at the start of the current
// instruction. We use this to determine whether an account was shrunk or
// grown before or after CPI, and to derive the vm address of the realloc
// region.
⋮----
// This points to the data section for this account, as serialized and
// mapped inside the vm (see serialize_parameters() in
// BpfExecutor::execute).
//
// This is only set when account_data_direct_mapping is off.
⋮----
pub fn get_serialized_data(
⋮----
use crate::memory::translate_slice_mut_for_cpi;
⋮----
Ok(&mut [])
⋮----
// Workaround the memory permissions (as these are from the PoV of being inside the VM)
⋮----
false, // Don't care since it is byte aligned
⋮----
.as_mut_ptr();
⋮----
Ok(std::slice::from_raw_parts_mut(
⋮----
.add(vm_addr.saturating_sub(solana_sbpf::ebpf::MM_INPUT_START) as usize),
⋮----
pub fn from_account_info(
⋮----
invoke_context.get_feature_set().account_data_direct_mapping;
⋮----
check_account_info_pointer(
⋮----
account_info.lamports.as_ptr() as u64,
⋮----
if account_info.lamports.as_ptr() as u64 >= solana_sbpf::ebpf::MM_INPUT_START {
⋮----
&& account_info.data.as_ptr() as u64 >= solana_sbpf::ebpf::MM_INPUT_START
⋮----
account_info.data.as_ptr() as *const _ as u64,
⋮----
data.as_ptr() as u64,
⋮----
invoke_context.consume_checked(
(data.len() as u64)
.checked_div(invoke_context.get_execution_cost().cpi_bytes_per_unit)
.unwrap_or(u64::MAX),
⋮----
let vm_len_addr = (account_info.data.as_ptr() as *const u64 as u64)
.saturating_add(std::mem::size_of::<u64>() as u64);
⋮----
let vm_data_addr = data.as_ptr() as u64;
⋮----
data.len() as u64,
⋮----
Ok(CallerAccount {
⋮----
fn from_sol_account_info(
⋮----
use crate::memory::translate_type_mut_for_cpi;
⋮----
.saturating_add(&account_info.data_len as *const u64 as u64)
.saturating_sub(account_info as *const _ as *const u64 as u64);
⋮----
pub trait SyscallInvokeSigned {
⋮----
pub fn translate_instruction_rust(
⋮----
ix.accounts.as_vaddr(),
ix.accounts.len(),
⋮----
ix.data.as_vaddr(),
ix.data.len(),
⋮----
check_instruction_size(account_metas.len(), data.len())?;
let mut total_cu_translation_cost: u64 = (data.len() as u64)
⋮----
.unwrap_or(u64::MAX);
⋮----
// Each account meta is 34 bytes (32 for pubkey, 1 for is_signer, 1 for is_writable)
⋮----
(account_metas.len().saturating_mul(size_of::<AccountMeta>()) as u64)
⋮----
total_cu_translation_cost.saturating_add(account_meta_translation_cost);
⋮----
consume_compute_meter(invoke_context, total_cu_translation_cost)?;
let mut accounts = Vec::with_capacity(account_metas.len());
⋮----
for account_index in 0..account_metas.len() {
⋮----
return Err(Box::new(InstructionError::InvalidArgument));
⋮----
accounts.push(account_meta.clone());
⋮----
Ok(Instruction {
⋮----
data: data.to_vec(),
⋮----
pub fn translate_accounts_rust<'a>(
⋮----
let (account_infos, account_info_keys) = translate_account_infos(
⋮----
translate_accounts_common(
⋮----
pub fn translate_signers_rust(
⋮----
if signers_seeds.len() > MAX_SIGNERS {
return Err(Box::new(CpiError::TooManySigners));
⋮----
for signer_seeds in signers_seeds.iter() {
⋮----
signer_seeds.ptr(),
signer_seeds.len(),
⋮----
if untranslated_seeds.len() > MAX_SEEDS {
return Err(Box::new(InstructionError::MaxSeedLengthExceeded));
⋮----
.iter()
.map(|untranslated_seed| {
translate_vm_slice(untranslated_seed, memory_mapping, check_aligned)
⋮----
Pubkey::create_program_address(&seeds, program_id).map_err(CpiError::BadSeeds)?;
signers.push(signer);
⋮----
Ok(signers)
⋮----
Ok(vec![])
⋮----
pub fn translate_instruction_c(
⋮----
check_instruction_size(ix_c.accounts_len as usize, data.len())?;
⋮----
.saturating_mul(size_of::<AccountMeta>() as u64))
⋮----
accounts.push(AccountMeta {
⋮----
pub fn translate_accounts_c<'a>(
⋮----
pub fn translate_signers_c(
⋮----
Ok(signers_seeds
⋮----
.map(|signer_seeds| {
⋮----
if seeds.len() > MAX_SEEDS {
return Err(Box::new(InstructionError::MaxSeedLengthExceeded) as Error);
⋮----
.map(|seed| {
⋮----
.map_err(|err| Box::new(CpiError::BadSeeds(err)) as Error)
⋮----
/// Call process instruction, common to both Rust and C
pub fn cpi_common<S: SyscallInvokeSigned>(
⋮----
pub fn cpi_common<S: SyscallInvokeSigned>(
⋮----
// CPI entry.
⋮----
// Translate the inputs to the syscall and synchronize the caller's account
consume_compute_meter(
⋮----
invoke_context.get_execution_cost().invoke_units,
⋮----
if let Some(execute_time) = invoke_context.execute_time.as_mut() {
execute_time.stop();
invoke_context.timings.execute_us += execute_time.as_us();
⋮----
let account_data_direct_mapping = invoke_context.get_feature_set().account_data_direct_mapping;
let check_aligned = invoke_context.get_check_aligned();
⋮----
let instruction_context = transaction_context.get_current_instruction_context()?;
let caller_program_id = instruction_context.get_program_key()?;
⋮----
check_authorized_program(&instruction.program_id, &instruction.data, invoke_context)?;
invoke_context.prepare_next_instruction(instruction, &signers)?;
⋮----
for translated_account in accounts.iter_mut() {
⋮----
.try_borrow_instruction_account(translated_account.index_in_caller)?;
let update_caller = update_callee_account(
⋮----
.process_instruction(&mut compute_units_consumed, &mut ExecuteTimings::default())?;
⋮----
update_caller_account(
⋮----
for translated_account in accounts.iter() {
⋮----
update_caller_account_region(
⋮----
invoke_context.execute_time = Some(Measure::start("execute"));
Ok(SUCCESS)
⋮----
pub struct TranslatedAccount<'a> {
⋮----
fn translate_account_infos<'a, T, F>(
⋮----
.saturating_add(account_infos_len.saturating_mul(std::mem::size_of::<T>() as u64))
⋮----
return Err(CpiError::InvalidPointer.into());
⋮----
check_account_infos(account_infos.len(), invoke_context)?;
⋮----
let account_infos_bytes = account_infos.len().saturating_mul(ACCOUNT_INFO_BYTE_SIZE);
⋮----
account_info_keys.push(translate_type::<Pubkey>(
⋮----
key_addr(account_info),
⋮----
Ok((account_infos, account_info_keys))
⋮----
fn translate_accounts_common<'a, T, F>(
⋮----
let next_instruction_context = transaction_context.get_next_instruction_context()?;
let next_instruction_accounts = next_instruction_context.instruction_accounts();
⋮----
let mut accounts = Vec::with_capacity(next_instruction_accounts.len());
// unwrapping here is fine: we're in a syscall and the method below fails
⋮----
.get_syscall_context()
.unwrap()
⋮----
next_instruction_accounts.iter().enumerate()
⋮----
.is_instruction_account_duplicate(instruction_account_index as IndexOfAccount)?
.is_some()
⋮----
.get_index_of_account_in_instruction(instruction_account.index_in_transaction)?;
let callee_account = instruction_context.try_borrow_instruction_account(index_in_caller)?;
⋮----
.get_key_of_account_at_index(instruction_account.index_in_transaction)?;
⋮----
if callee_account.is_executable() {
⋮----
(callee_account.get_data().len() as u64)
⋮----
account_info_keys.iter().position(|key| *key == account_key)
⋮----
.get(index_in_caller as usize)
.ok_or_else(|| {
⋮----
if caller_account_index >= account_infos.len() {
return Err(Box::new(CpiError::InvalidLength));
⋮----
do_translate(
⋮----
account_infos_addr.saturating_add(
caller_account_index.saturating_mul(mem::size_of::<T>()) as u64,
⋮----
update_callee_account(
⋮----
accounts.push(TranslatedAccount {
⋮----
update_caller_account_region: instruction_account.is_writable() || update_caller,
update_caller_account_info: instruction_account.is_writable(),
⋮----
return Err(Box::new(InstructionError::MissingAccount));
⋮----
Ok(accounts)
⋮----
fn consume_compute_meter(invoke_context: &InvokeContext, amount: u64) -> Result<(), Error> {
invoke_context.consume_checked(amount)?;
⋮----
fn update_callee_account(
⋮----
if callee_account.get_lamports() != *caller_account.lamports {
callee_account.set_lamports(*caller_account.lamports)?;
⋮----
let prev_len = callee_account.get_data().len();
⋮----
.saturating_add(MAX_PERMITTED_DATA_INCREASE)
⋮----
return Err(InstructionError::InvalidRealloc.into());
⋮----
.get_mut(post_len..)
.ok_or_else(|| Box::new(InstructionError::AccountDataTooSmall))?
.fill(0);
⋮----
callee_account.set_data_length(post_len)?;
⋮----
if !account_data_direct_mapping && callee_account.can_data_be_changed().is_ok() {
callee_account.set_data_from_slice(caller_account.serialized_data)?;
⋮----
match callee_account.can_data_be_resized(caller_account.serialized_data.len()) {
Ok(()) => callee_account.set_data_from_slice(caller_account.serialized_data)?,
Err(err) if callee_account.get_data() != caller_account.serialized_data => {
return Err(Box::new(err));
⋮----
if callee_account.get_owner() != caller_account.owner {
callee_account.set_owner(caller_account.owner.as_ref())?;
⋮----
Ok(must_update_caller)
⋮----
fn update_caller_account_region(
⋮----
.find_region(caller_account.vm_data_addr)
.ok_or_else(|| Box::new(InstructionError::MissingAccount))?;
debug_assert_eq!(region.vm_addr, caller_account.vm_data_addr);
⋮----
new_region = region.clone();
modify_memory_region_of_account(callee_account, &mut new_region);
⋮----
new_region = create_memory_region_of_account(callee_account, region.vm_addr)?;
⋮----
memory_mapping.replace_region(region_index, new_region)?;
⋮----
fn update_caller_account(
⋮----
*caller_account.lamports = callee_account.get_lamports();
*caller_account.owner = *callee_account.get_owner();
⋮----
let post_len = callee_account.get_data().len();
⋮----
address_space_reserved_for_account.saturating_sub(caller_account.original_data_len);
⋮----
return Err(Box::new(InstructionError::InvalidRealloc));
⋮----
.saturating_sub(std::mem::size_of::<u64>() as u64),
⋮----
.get_data()
.get(0..post_len)
.ok_or(CpiError::InvalidLength)?;
if to_slice.len() != from_slice.len() {
return Err(Box::new(InstructionError::AccountDataTooSmall));
⋮----
to_slice.copy_from_slice(from_slice);
⋮----
mod tests {
⋮----
macro_rules! mock_invoke_context {
⋮----
macro_rules! borrow_instruction_account {
⋮----
fn is_zeroed(data: &[u8]) -> bool {
data.iter().all(|b| *b == 0)
⋮----
struct MockCallerAccount {
⋮----
impl MockCallerAccount {
fn new(
⋮----
data.len() + MAX_PERMITTED_DATA_INCREASE
⋮----
let mut d = vec![0; region_len];
let mut regions = vec![];
unsafe { ptr::write_unaligned::<u64>(d.as_mut_ptr().cast(), data.len() as u64) };
⋮----
d[mem::size_of::<u64>()..][..data.len()].copy_from_slice(data);
⋮----
regions.push(MemoryRegion::new_writable(&mut d[..region_len], vm_addr));
⋮----
regions.push(MemoryRegion::new_readonly(data, region_addr));
region_addr += data.len() as u64;
regions.push(MemoryRegion::new_writable(
⋮----
d.truncate(mem::size_of::<u64>() + data.len());
⋮----
len: data.len() as u64,
⋮----
fn data_slice<'a>(&self) -> &'a [u8] {
⋮----
self.data[mem::size_of::<u64>()..].as_ptr(),
self.data.capacity() - mem::size_of::<u64>(),
⋮----
fn caller_account(&mut self) -> CallerAccount<'_> {
⋮----
struct MockAccountInfo<'a> {
⋮----
fn new(key: Pubkey, account: &AccountSharedData) -> MockAccountInfo<'_> {
⋮----
lamports: account.lamports(),
data: account.data(),
owner: *account.owner(),
executable: account.executable(),
_unused: account.rent_epoch(),
⋮----
fn into_region(self, vm_addr: u64) -> (Vec<u8>, MemoryRegion, SerializedAccountMetadata) {
⋮----
+ self.data.len();
let mut data = vec![0; size];
⋮----
key: unsafe { (key_addr as *const Pubkey).as_ref() }.unwrap(),
⋮----
owner: unsafe { (owner_addr as *const Pubkey).as_ref() }.unwrap(),
⋮----
ptr::write_unaligned(data.as_mut_ptr().cast(), info);
⋮----
(data.as_mut_ptr() as usize + key_addr - vm_addr) as *mut _,
⋮----
(data.as_mut_ptr() as usize + lamports_cell_addr - vm_addr) as *mut _,
RcBox::new(RefCell::new((lamports_addr as *mut u64).as_mut().unwrap())),
⋮----
(data.as_mut_ptr() as usize + lamports_addr - vm_addr) as *mut _,
⋮----
(data.as_mut_ptr() as usize + owner_addr - vm_addr) as *mut _,
⋮----
(data.as_mut_ptr() as usize + data_cell_addr - vm_addr) as *mut _,
⋮----
self.data.len(),
⋮----
data[data_addr - vm_addr..].copy_from_slice(self.data);
⋮----
let region = MemoryRegion::new_writable(data.as_mut_slice(), vm_addr as u64);
⋮----
original_data_len: self.data.len(),
⋮----
struct MockInstruction {
⋮----
impl MockInstruction {
fn into_region(self, vm_addr: u64) -> (Vec<u8>, MemoryRegion) {
let accounts_len = mem::size_of::<AccountMeta>() * self.accounts.len();
let size = mem::size_of::<StableInstruction>() + accounts_len + self.data.len();
⋮----
self.accounts.len(),
⋮----
Vec::from_raw_parts(data_addr as *mut _, self.data.len(), self.data.len())
⋮----
ptr::write_unaligned(data.as_mut_ptr().cast(), ins);
data[accounts_addr - vm_addr..][..accounts_len].copy_from_slice(
slice::from_raw_parts(self.accounts.as_ptr().cast(), accounts_len),
⋮----
data[data_addr - vm_addr..].copy_from_slice(&self.data);
⋮----
struct RcBox<T> {
⋮----
fn new(value: T) -> RcBox<T> {
⋮----
type TestTransactionAccount = (Pubkey, AccountSharedData, bool);
fn transaction_with_one_writable_instruction_account(
⋮----
vec![
⋮----
fn transaction_with_one_readonly_instruction_account(
⋮----
fn mock_signers(signers: &[&[u8]], vm_addr: u64) -> (Vec<u8>, MemoryRegion) {
⋮----
// calculate size
let fat_ptr_size_of_slice = mem::size_of::<&[()]>(); // pointer size + length size
let singers_length = signers.len();
let sum_signers_data_length: usize = signers.iter().map(|s| s.len()).sum();
// init data vec
⋮----
let mut data = vec![0; total_size];
// data is composed by 3 parts
// A.
// [ singers address, singers length, ...,
// B.                                      |
//                                         signer1 address, signer1 length, signer2 address ...,
//                                         ^ p1 --->
// C.                                                                                           |
//                                                                                              signer1 data, signer2 data, ... ]
//                                                                                              ^ p2 --->
⋮----
.clone_from_slice(&(fat_ptr_size_of_slice + vm_addr).to_le_bytes());
⋮----
.clone_from_slice(&(singers_length).to_le_bytes());
// B. + C.
⋮----
for signer in signers.iter() {
let signer_length = signer.len();
// B.
⋮----
.clone_from_slice(&(p2 + vm_addr).to_le_bytes());
⋮----
.clone_from_slice(&(signer_length).to_le_bytes());
⋮----
// C.
data[p2..p2 + signer_length].clone_from_slice(signer);
⋮----
fn test_translate_instruction() {
⋮----
transaction_with_one_writable_instruction_account(b"foo".to_vec());
mock_invoke_context!(
⋮----
let accounts = vec![AccountMeta {
⋮----
let data = b"ins data".to_vec();
⋮----
accounts: accounts.clone(),
data: data.clone(),
⋮----
.into_region(vm_addr);
⋮----
let memory_mapping = MemoryMapping::new(vec![region], &config, SBPFVersion::V3).unwrap();
let ins = translate_instruction_rust(
⋮----
true, // check_aligned
⋮----
.unwrap();
assert_eq!(ins.program_id, program_id);
assert_eq!(ins.accounts, accounts);
assert_eq!(ins.data, data);
⋮----
fn test_translate_signers() {
⋮----
let (_mem, region) = mock_signers(&[b"foo", &[bump_seed]], vm_addr);
⋮----
let signers = translate_signers_rust(
⋮----
assert_eq!(signers[0], derived_key);
⋮----
fn test_translate_accounts_rust() {
⋮----
transaction_with_one_writable_instruction_account(b"foobar".to_vec());
let account = transaction_accounts[1].1.clone();
⋮----
let original_data_len = account.data().len();
⋮----
MockAccountInfo::new(key, &account).into_region(vm_addr);
⋮----
.set_syscall_context(SyscallContext {
⋮----
accounts_metadata: vec![account_metadata],
⋮----
.configure_next_instruction_for_tests(
⋮----
vec![],
⋮----
let accounts = translate_accounts_rust(
⋮----
assert_eq!(accounts.len(), 1);
⋮----
assert_eq!(caller_account.serialized_data, account.data());
assert_eq!(caller_account.original_data_len, original_data_len);
⋮----
fn test_caller_account_from_account_info() {
⋮----
let account_info = translate_type::<AccountInfo>(&memory_mapping, vm_addr, false).unwrap();
⋮----
assert_eq!(*caller_account.lamports, account.lamports());
assert_eq!(caller_account.owner, account.owner());
assert_eq!(caller_account.original_data_len, account.data().len());
assert_eq!(
⋮----
fn test_update_caller_account_lamports_owner(
⋮----
let transaction_accounts = transaction_with_one_writable_instruction_account(vec![]);
⋮----
MockCallerAccount::new(1234, *account.owner(), account.data(), false);
⋮----
mock_caller_account.regions.split_off(0),
⋮----
let mut caller_account = mock_caller_account.caller_account();
⋮----
.get_current_instruction_context()
⋮----
.try_borrow_instruction_account(0)
⋮----
callee_account.set_lamports(42).unwrap();
⋮----
.set_owner(Pubkey::new_unique().as_ref())
⋮----
assert_eq!(*caller_account.lamports, 42);
assert_eq!(caller_account.owner, callee_account.get_owner());
⋮----
fn test_update_caller_account_data() {
⋮----
MockCallerAccount::new(account.lamports(), *account.owner(), account.data(), false);
⋮----
mock_caller_account.regions.clone(),
⋮----
let data_slice = mock_caller_account.data_slice();
⋮----
.as_ptr()
.offset(-(mem::size_of::<u64>() as isize))
⋮----
(b"foo".to_vec(), MAX_PERMITTED_DATA_INCREASE + 3),
(b"foobaz".to_vec(), MAX_PERMITTED_DATA_INCREASE),
(b"foobazbad".to_vec(), MAX_PERMITTED_DATA_INCREASE - 3),
⋮----
assert_eq!(caller_account.serialized_data, callee_account.get_data());
callee_account.set_data_from_slice(&new_value).unwrap();
⋮----
let data_len = callee_account.get_data().len();
assert_eq!(data_len, *caller_account.ref_to_len_in_vm as usize);
assert_eq!(data_len, serialized_len());
assert_eq!(data_len, caller_account.serialized_data.len());
⋮----
assert_eq!(data_slice[data_len..].len(), expected_realloc_size);
assert!(is_zeroed(&data_slice[data_len..]));
⋮----
.set_data_length(original_data_len + MAX_PERMITTED_DATA_INCREASE)
⋮----
assert_eq!(data_slice[data_len..].len(), 0);
⋮----
.set_data_length(original_data_len + MAX_PERMITTED_DATA_INCREASE + 1)
⋮----
assert_matches!(
⋮----
// close the account
callee_account.set_data_length(0).unwrap();
⋮----
.set_owner(system_program::id().as_ref())
⋮----
assert_eq!(data_len, 0);
⋮----
fn test_update_callee_account_lamports_owner(
⋮----
let caller_account = mock_caller_account.caller_account();
borrow_instruction_account!(callee_account, invoke_context, 0);
⋮----
assert_eq!(callee_account.get_lamports(), 42);
⋮----
fn test_update_callee_account_data_writable(
⋮----
// stricter_abi_and_runtime_constraints does not copy data in update_callee_account()
⋮----
assert_eq!(callee_account.get_data(), b"boobar");
let mut data = b"foobarbaz".to_vec();
*caller_account.ref_to_len_in_vm = data.len() as u64;
⋮----
let mut data = b"baz".to_vec();
⋮----
assert_eq!(callee_account.get_data(), b"");
// growing beyond address_space_reserved_for_account
⋮----
let result = update_callee_account(
⋮----
result.unwrap();
⋮----
fn test_update_callee_account_data_readonly(
⋮----
transaction_with_one_readonly_instruction_account(b"foobar".to_vec());

================
File: program-runtime/src/execution_budget.rs
================
fn get_max_instruction_stack_depth(simd_0268_active: bool) -> usize {
⋮----
fn get_invoke_unit_cost(simd_0339_active: bool) -> u64 {
⋮----
NonZeroU32::new(64 * 1024 * 1024).unwrap();
⋮----
pub struct SVMTransactionExecutionBudget {
⋮----
impl Default for SVMTransactionExecutionBudget {
fn default() -> Self {
⋮----
impl SVMTransactionExecutionBudget {
pub fn new_with_defaults(simd_0268_active: bool) -> Self {
⋮----
max_instruction_stack_depth: get_max_instruction_stack_depth(simd_0268_active),
⋮----
heap_size: u32::try_from(solana_program_entrypoint::HEAP_LENGTH).unwrap(),
⋮----
pub struct SVMTransactionExecutionCost {
⋮----
impl Default for SVMTransactionExecutionCost {
⋮----
impl SVMTransactionExecutionCost {
pub fn new_with_defaults(simd_0339_active: bool) -> Self {
⋮----
invoke_units: get_invoke_unit_cost(simd_0339_active),
⋮----
pub fn poseidon_cost(&self, nr_inputs: u64) -> Option<u64> {
let squared_inputs = nr_inputs.checked_pow(2)?;
⋮----
.checked_mul(squared_inputs)?;
let final_result = mul_result.checked_add(self.poseidon_cost_coefficient_c)?;
Some(final_result)
⋮----
pub struct SVMTransactionExecutionAndFeeBudgetLimits {
⋮----
impl Default for SVMTransactionExecutionAndFeeBudgetLimits {
⋮----
impl SVMTransactionExecutionAndFeeBudgetLimits {
pub fn with_fee(fee_details: FeeDetails) -> Self {

================
File: program-runtime/src/invoke_context.rs
================
pub type BuiltinFunctionWithContext = BuiltinFunction<InvokeContext<'static, 'static>>;
pub type Executable = GenericExecutable<InvokeContext<'static, 'static>>;
pub type RegisterTrace<'a> = &'a [[u64; 12]];
⋮----
macro_rules! declare_process_instruction {
⋮----
impl ContextObject for InvokeContext<'_, '_> {
fn consume(&mut self, amount: u64) {
let mut compute_meter = self.compute_meter.borrow_mut();
*compute_meter = compute_meter.saturating_sub(amount);
⋮----
fn get_remaining(&self) -> u64 {
*self.compute_meter.borrow()
⋮----
pub struct AllocErr;
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
f.write_str("Error: Memory allocation failed")
⋮----
pub struct BpfAllocator {
⋮----
impl BpfAllocator {
pub fn new(len: u64) -> Self {
⋮----
pub fn alloc(&mut self, layout: Layout) -> Result<u64, AllocErr> {
let bytes_to_align = (self.pos as *const u8).align_offset(layout.align()) as u64;
⋮----
.saturating_add(bytes_to_align)
.saturating_add(layout.size() as u64)
⋮----
self.pos = self.pos.saturating_add(bytes_to_align);
let addr = MM_HEAP_START.saturating_add(self.pos);
self.pos = self.pos.saturating_add(layout.size() as u64);
Ok(addr)
⋮----
Err(AllocErr)
⋮----
pub struct EnvironmentConfig<'a> {
⋮----
pub fn new(
⋮----
pub struct SyscallContext {
⋮----
pub struct SerializedAccountMetadata {
⋮----
/// Main pipeline from runtime to program execution.
pub struct InvokeContext<'a, 'ix_data> {
⋮----
pub struct InvokeContext<'a, 'ix_data> {
/// Information about the currently executing transaction.
    pub transaction_context: &'a mut TransactionContext<'ix_data>,
/// The local program cache for the transaction batch.
    pub program_cache_for_tx_batch: &'a mut ProgramCacheForTxBatch,
⋮----
/// The compute budget for the current invocation.
    compute_budget: SVMTransactionExecutionBudget,
/// The compute cost for the current invocation.
    execution_cost: SVMTransactionExecutionCost,
/// Instruction compute meter, for tracking compute units consumed against
    /// the designated compute budget during program execution.
⋮----
/// the designated compute budget during program execution.
    compute_meter: RefCell<u64>,
⋮----
/// Latest measurement not yet accumulated in [ExecuteDetailsTimings::execute_us]
    pub execute_time: Option<Measure>,
⋮----
/// Pairs of index in TX instruction trace and VM register trace
    register_traces: Vec<(usize, Vec<[u64; 12]>)>,
⋮----
/// Push a stack frame onto the invocation stack
    pub fn push(&mut self) -> Result<(), InstructionError> {
⋮----
pub fn push(&mut self) -> Result<(), InstructionError> {
⋮----
.get_instruction_context_at_index_in_trace(
self.transaction_context.get_instruction_trace_length(),
⋮----
.get_program_key()
.map_err(|_| InstructionError::UnsupportedProgramId)?;
if self.transaction_context.get_instruction_stack_height() != 0 {
⋮----
(0..self.transaction_context.get_instruction_stack_height()).any(|level| {
⋮----
.get_instruction_context_at_nesting_level(level)
.and_then(|instruction_context| instruction_context.get_program_key())
.map(|program_key| program_key == program_id)
.unwrap_or(false)
⋮----
.get_current_instruction_context()
⋮----
.unwrap_or(false);
⋮----
// Reentrancy not allowed unless caller is calling itself
return Err(InstructionError::ReentrancyNotAllowed);
⋮----
self.syscall_context.push(None);
self.transaction_context.push()
⋮----
/// Pop a stack frame from the invocation stack
    fn pop(&mut self) -> Result<(), InstructionError> {
⋮----
fn pop(&mut self) -> Result<(), InstructionError> {
self.syscall_context.pop();
self.transaction_context.pop()
⋮----
/// Current height of the invocation stack, top level instructions are height
    /// `solana_instruction::TRANSACTION_LEVEL_STACK_HEIGHT`
⋮----
/// `solana_instruction::TRANSACTION_LEVEL_STACK_HEIGHT`
    pub fn get_stack_height(&self) -> usize {
⋮----
pub fn get_stack_height(&self) -> usize {
self.transaction_context.get_instruction_stack_height()
⋮----
/// Entrypoint for a cross-program invocation from a builtin program
    pub fn native_invoke(
⋮----
pub fn native_invoke(
⋮----
self.prepare_next_instruction(instruction, signers)?;
⋮----
self.process_instruction(&mut compute_units_consumed, &mut ExecuteTimings::default())?;
Ok(())
⋮----
/// Helper to prepare for process_instruction() when the instruction is not a top level one,
    /// and depends on `AccountMeta`s
⋮----
/// and depends on `AccountMeta`s
    pub fn prepare_next_instruction(
⋮----
pub fn prepare_next_instruction(
⋮----
// We reference accounts by an u8 index, so we have a total of 256 accounts.
let mut transaction_callee_map: Vec<u16> = vec![u16::MAX; MAX_ACCOUNTS_PER_TRANSACTION];
⋮----
Vec::with_capacity(instruction.accounts.len());
// This code block is necessary to restrict the scope of the immutable borrow of
// transaction context (the `instruction_context` variable). At the end of this
// function, we must borrow it again as mutable.
⋮----
let instruction_context = self.transaction_context.get_current_instruction_context()?;
for account_meta in instruction.accounts.iter() {
⋮----
.find_index_of_account(&account_meta.pubkey)
.ok_or_else(|| {
ic_msg!(
⋮----
debug_assert!((index_in_transaction as usize) < transaction_callee_map.len());
⋮----
.get_mut(index_in_transaction as usize)
.unwrap();
if (*index_in_callee as usize) < instruction_accounts.len() {
⋮----
.get_mut(*index_in_callee as usize)
.ok_or(InstructionError::MissingAccount)?;
instruction_account.set_is_signer(
instruction_account.is_signer() || account_meta.is_signer,
⋮----
instruction_account.set_is_writable(
instruction_account.is_writable() || account_meta.is_writable,
⋮----
instruction_accounts.push(cloned_account);
⋮----
*index_in_callee = instruction_accounts.len() as u16;
instruction_accounts.push(InstructionAccount::new(
⋮----
for current_index in 0..instruction_accounts.len() {
let instruction_account = instruction_accounts.get(current_index).unwrap();
⋮----
.get(instruction_account.index_in_transaction as usize)
.unwrap() as usize;
⋮----
.get(index_in_callee)
⋮----
reference_account.is_signer(),
reference_account.is_writable(),
⋮----
let current_account = instruction_accounts.get_mut(current_index).unwrap();
current_account.set_is_signer(current_account.is_signer() || is_signer);
current_account.set_is_writable(current_account.is_writable() || is_writable);
// This account is repeated, so there is no need to check for permissions
⋮----
let index_in_caller = instruction_context.get_index_of_account_in_instruction(
⋮----
// This unwrap is safe because instruction.accounts.len() == instruction_accounts.len()
let account_key = &instruction.accounts.get(current_index).unwrap().pubkey;
// get_index_of_account_in_instruction has already checked if the index is valid.
⋮----
.instruction_accounts()
.get(index_in_caller as usize)
⋮----
// Readonly in caller cannot become writable in callee
if instruction_account.is_writable() && !caller_instruction_account.is_writable() {
ic_msg!(self, "{}'s writable privilege escalated", account_key,);
return Err(InstructionError::PrivilegeEscalation);
⋮----
if instruction_account.is_signer()
&& !(caller_instruction_account.is_signer() || signers.contains(account_key))
⋮----
ic_msg!(self, "{}'s signer privilege escalated", account_key,);
⋮----
.find_index_of_account(callee_program_id);
⋮----
.map(|index| instruction_context.get_index_of_account_in_instruction(index));
if program_account_index_in_instruction.is_none()
|| program_account_index_in_instruction.unwrap().is_err()
⋮----
ic_msg!(self, "Unknown program {}", callee_program_id);
return Err(InstructionError::MissingAccount);
⋮----
program_account_index_in_transaction.unwrap()
⋮----
self.transaction_context.configure_next_instruction(
⋮----
pub fn prepare_next_top_level_instruction(
⋮----
for index_in_transaction in instruction.accounts.iter() {
debug_assert!((*index_in_transaction as usize) < transaction_callee_map.len());
⋮----
.get_mut(*index_in_transaction as usize)
⋮----
if (*index_in_callee as usize) > instruction_accounts.len() {
⋮----
message.is_signer(index_in_transaction),
message.is_writable(index_in_transaction),
⋮----
/// Processes an instruction and returns how many compute units were used
    pub fn process_instruction(
⋮----
pub fn process_instruction(
⋮----
self.push()?;
self.process_executable_chain(compute_units_consumed, timings)
// MUST pop if and only if `push` succeeded, independent of `result`.
// Thus, the `.and()` instead of an `.and_then()`.
.and(self.pop())
⋮----
/// Processes a precompile instruction
    pub fn process_precompile(
⋮----
pub fn process_precompile(
⋮----
let instruction_datas: Vec<_> = message_instruction_datas_iter.collect();
⋮----
.process_precompile(program_id, instruction_data, instruction_datas)
.map_err(InstructionError::from)
⋮----
fn process_executable_chain(
⋮----
let owner_id = instruction_context.get_program_owner()?;
⋮----
*instruction_context.get_program_key()?
⋮----
return Err(InstructionError::UnsupportedProgramId);
⋮----
.find(&builtin_id)
.ok_or(InstructionError::UnsupportedProgramId)?;
⋮----
.get_function_registry()
.lookup_by_key(ENTRYPOINT_KEY)
.map(|(_name, function)| function),
⋮----
let program_id = *instruction_context.get_program_key()?;
⋮----
.set_return_data(program_id, Vec::new())?;
let logger = self.get_log_collector();
stable_log::program_invoke(&logger, &program_id, self.get_stack_height());
let pre_remaining_units = self.get_remaining();
⋮----
MemoryMapping::new(Vec::new(), &mock_config, SBPFVersion::V0).unwrap();
⋮----
.clone(),
⋮----
vm.invoke_function(function);
⋮----
Err(instruction_err.clone())
⋮----
Err(InstructionError::ProgramFailedToComplete)
⋮----
let post_remaining_units = self.get_remaining();
*compute_units_consumed = pre_remaining_units.saturating_sub(post_remaining_units);
if builtin_id == program_id && result.is_ok() && *compute_units_consumed == 0 {
return Err(InstructionError::BuiltinProgramsMustConsumeComputeUnits);
⋮----
.process_executable_chain_us += process_executable_chain_time.end_as_us();
⋮----
pub fn get_log_collector(&self) -> Option<Rc<RefCell<LogCollector>>> {
self.log_collector.clone()
⋮----
pub fn consume_checked(&self, amount: u64) -> Result<(), Box<dyn std::error::Error>> {
⋮----
return Err(Box::new(InstructionError::ComputationalBudgetExceeded));
⋮----
pub fn mock_set_remaining(&self, remaining: u64) {
*self.compute_meter.borrow_mut() = remaining;
⋮----
pub fn get_compute_budget(&self) -> &SVMTransactionExecutionBudget {
⋮----
pub fn get_execution_cost(&self) -> &SVMTransactionExecutionCost {
⋮----
pub fn get_feature_set(&self) -> &SVMFeatureSet {
⋮----
pub fn get_program_runtime_environments_for_deployment(&self) -> &ProgramRuntimeEnvironments {
⋮----
pub fn is_stake_raise_minimum_delegation_to_1_sol_active(&self) -> bool {
⋮----
pub fn is_deprecate_legacy_vote_ixs_active(&self) -> bool {
⋮----
pub fn get_sysvar_cache(&self) -> &SysvarCache {
⋮----
pub fn get_epoch_stake(&self) -> u64 {
⋮----
.get_epoch_stake()
⋮----
pub fn get_epoch_stake_for_vote_account(&self, pubkey: &'a Pubkey) -> u64 {
⋮----
.get_epoch_stake_for_vote_account(pubkey)
⋮----
pub fn is_precompile(&self, pubkey: &Pubkey) -> bool {
⋮----
.is_precompile(pubkey)
⋮----
// Should alignment be enforced during user pointer translation
pub fn get_check_aligned(&self) -> bool {
⋮----
.and_then(|instruction_context| {
let owner_id = instruction_context.get_program_owner();
debug_assert!(owner_id.is_ok());
⋮----
.map(|owner_key| owner_key != bpf_loader_deprecated::id())
.unwrap_or(true)
⋮----
// Set this instruction syscall context
pub fn set_syscall_context(
⋮----
.last_mut()
.ok_or(InstructionError::CallDepth)? = Some(syscall_context);
⋮----
// Get this instruction's SyscallContext
pub fn get_syscall_context(&self) -> Result<&SyscallContext, InstructionError> {
⋮----
.last()
.and_then(std::option::Option::as_ref)
.ok_or(InstructionError::CallDepth)
⋮----
pub fn get_syscall_context_mut(&mut self) -> Result<&mut SyscallContext, InstructionError> {
⋮----
.and_then(|syscall_context| syscall_context.as_mut())
⋮----
pub fn insert_register_trace(&mut self, register_trace: Vec<[u64; 12]>) {
if register_trace.is_empty() {
⋮----
let Ok(instruction_context) = self.transaction_context.get_current_instruction_context()
⋮----
.push((instruction_context.get_index_in_trace(), register_trace));
⋮----
pub fn iterate_vm_traces(
⋮----
.get_instruction_context_at_index_in_trace(*index_in_trace)
⋮----
let Ok(program_id) = instruction_context.get_program_key() else {
⋮----
let Some(entry) = self.program_cache_for_tx_batch.find(program_id) else {
⋮----
callback(instruction_context, executable, register_trace.as_slice());
⋮----
macro_rules! with_mock_invoke_context_with_feature_set {
⋮----
macro_rules! with_mock_invoke_context {
⋮----
pub fn mock_process_instruction_with_feature_set<
⋮----
Vec::with_capacity(instruction_account_metas.len());
for account_meta in instruction_account_metas.iter() {
⋮----
.iter()
.position(|(key, _account)| *key == account_meta.pubkey)
.unwrap_or(transaction_accounts.len())
⋮----
transaction_accounts.push((*loader_id, processor_account));
transaction_accounts.len().saturating_sub(1) as IndexOfAccount
⋮----
.any(|(key, _)| *key == sysvar::epoch_schedule::id())
⋮----
transaction_accounts.push((
⋮----
create_account_shared_data_for_test(&EpochSchedule::default()),
⋮----
with_mock_invoke_context_with_feature_set!(
⋮----
program_cache_for_tx_batch.replenish(
⋮----
program_cache_for_tx_batch.set_slot_for_tests(
⋮----
.get_sysvar_cache()
.get_clock()
.map(|clock| clock.slot)
.unwrap_or(1),
⋮----
pre_adjustments(&mut invoke_context);
⋮----
.configure_next_instruction_for_tests(
⋮----
instruction_data.to_vec(),
⋮----
let result = invoke_context.process_instruction(&mut 0, &mut ExecuteTimings::default());
assert_eq!(result, expected_result);
post_adjustments(&mut invoke_context);
let mut transaction_accounts = transaction_context.deconstruct_without_keys().unwrap();
⋮----
transaction_accounts.pop();
⋮----
pub fn mock_process_instruction<F: FnMut(&mut InvokeContext), G: FnMut(&mut InvokeContext)>(
⋮----
mock_process_instruction_with_feature_set(
⋮----
mod tests {
⋮----
enum MockInstruction {
⋮----
declare_process_instruction!(
⋮----
fn test_instruction_stack_height(simd_0268_active: bool) {
⋮----
.saturating_add(1);
let mut invoke_stack = vec![];
let mut transaction_accounts = vec![];
let mut instruction_accounts = vec![];
⋮----
invoke_stack.push(solana_pubkey::new_rand());
⋮----
AccountSharedData::new(index as u64, 1, invoke_stack.get(index).unwrap()),
⋮----
for (index, program_id) in invoke_stack.iter().enumerate() {
⋮----
with_mock_invoke_context!(invoke_context, transaction_context, transaction_accounts);
⋮----
for _ in 0..invoke_stack.len() {
⋮----
one_more_than_max_depth.saturating_add(depth_reached) as IndexOfAccount,
instruction_accounts.clone(),
vec![],
⋮----
if Err(InstructionError::CallDepth) == invoke_context.push() {
⋮----
depth_reached = depth_reached.saturating_add(1);
⋮----
assert_ne!(depth_reached, 0);
assert!(depth_reached < one_more_than_max_depth);
⋮----
fn test_max_instruction_trace_length() {
⋮----
vec![(
⋮----
transaction_context.push().unwrap();
⋮----
vec![InstructionAccount::new(0, false, false)],
⋮----
transaction_context.pop().unwrap();
⋮----
assert_eq!(
⋮----
fn test_process_instruction_account_modifications(
⋮----
program_account.set_executable(true);
let transaction_accounts = vec![
⋮----
let metas = vec![
⋮----
.map(|instruction_account_index| {
⋮----
.configure_next_instruction_for_tests(4, instruction_accounts, vec![])
⋮----
invoke_context.push().unwrap();
⋮----
Instruction::new_with_bincode(callee_program_id, &instruction, metas.clone());
⋮----
.native_invoke(inner_instruction, &[])
.and(invoke_context.pop());
⋮----
fn test_process_instruction_compute_unit_consumption(
⋮----
desired_result: expected_result.clone(),
⋮----
metas.clone(),
⋮----
.prepare_next_instruction(inner_instruction, &[])
⋮----
.process_instruction(&mut compute_units_consumed, &mut ExecuteTimings::default());
assert!(compute_units_consumed > 0);
⋮----
invoke_context.pop().unwrap();
⋮----
fn test_invoke_context_compute_budget() {
let transaction_accounts = vec![(solana_pubkey::new_rand(), AccountSharedData::default())];
⋮----
.configure_next_instruction_for_tests(0, vec![], vec![])
⋮----
assert_eq!(*invoke_context.get_compute_budget(), execution_budget);
⋮----
fn test_process_instruction_accounts_resize_delta(resize_delta: i64) {
⋮----
let instruction_accounts = vec![
⋮----
let new_len = (user_account_data_len as i64).saturating_add(resize_delta) as u64;
let instruction_data = bincode::serialize(&MockInstruction::Resize { new_len }).unwrap();
⋮----
.configure_next_instruction_for_tests(2, instruction_accounts, instruction_data)
⋮----
assert!(result.is_ok());
⋮----
fn test_prepare_instruction_maximum_accounts() {
⋮----
fee_payer.pubkey(),
⋮----
account_metas.push(AccountMeta::new(fee_payer.pubkey(), true));
⋮----
transaction_accounts.push((program_id, program_account));
account_metas.push(AccountMeta::new_readonly(program_id, false));
⋮----
.push((key, AccountSharedData::new(1, 1, &Pubkey::new_unique())));
account_metas.push(AccountMeta::new_readonly(key, false));
⋮----
.get(i % MAX_ACCOUNTS_PER_TRANSACTION)
.unwrap()
⋮----
account_metas.push(AccountMeta::new_readonly(repeated_key, false));
⋮----
let instruction_1 = Instruction::new_with_bytes(program_id, &[20], account_metas.clone());
⋮----
account_metas.iter().rev().cloned().collect(),
⋮----
&[instruction_1.clone(), instruction_2.clone()],
Some(&fee_payer.pubkey()),
⋮----
fn test_case_1(invoke_context: &InvokeContext) {
⋮----
.get_next_instruction_context()
⋮----
.get_index_of_instruction_account_in_transaction(index_in_instruction)
⋮----
.get_index_of_account_in_instruction(index_in_transaction)
⋮----
assert_eq!(index_in_instruction, index_in_transaction);
assert_eq!(index_in_instruction, other_ix_index);
⋮----
fn test_case_2(invoke_context: &InvokeContext) {
⋮----
SVMInstruction::from(sanitized.message().instructions().first().unwrap());
⋮----
.prepare_next_top_level_instruction(
⋮----
test_case_1(&invoke_context);
invoke_context.transaction_context.push().unwrap();
⋮----
SVMInstruction::from(sanitized.message().instructions().get(1).unwrap());
⋮----
test_case_2(&invoke_context);
⋮----
.prepare_next_instruction(instruction_1, &[fee_payer.pubkey()])
⋮----
.prepare_next_instruction(instruction_2, &[fee_payer.pubkey()])
⋮----
fn test_duplicated_accounts() {
⋮----
Vec::with_capacity(MAX_ACCOUNTS_PER_INSTRUCTION.saturating_sub(1));
⋮----
for i in 2..account_metas.capacity() {
⋮----
let last_key = transaction_accounts.last().unwrap().0;
account_metas.push(AccountMeta::new_readonly(last_key, false));
⋮----
let instruction = Instruction::new_with_bytes(program_id, &[20], account_metas.clone());
let transaction = Transaction::new_with_payer(&[instruction], Some(&fee_payer.pubkey()));
⋮----
for index_in_instruction in 2..account_metas.len() as IndexOfAccount {
⋮----
.is_instruction_account_duplicate(index_in_instruction)
⋮----
assert!(is_duplicate.is_none());
⋮----
assert_eq!(is_duplicate, Some(index_in_instruction.saturating_sub(1)));
⋮----
account_metas.iter().cloned().rev().collect(),
⋮----
.prepare_next_instruction(instruction, &[fee_payer.pubkey()])
⋮----
for index_in_instruction in 2..account_metas.len().saturating_sub(1) as u16 {

================
File: program-runtime/src/lib.rs
================
pub use solana_sbpf;
pub mod cpi;
pub mod execution_budget;
pub mod invoke_context;
pub mod loaded_programs;
pub mod mem_pool;
pub mod memory;
pub mod serialization;
pub mod stable_log;
pub mod sysvar_cache;
pub mod __private {

================
File: program-runtime/src/loaded_programs.rs
================
pub type ProgramRuntimeEnvironment = Arc<BuiltinProgram<InvokeContext<'static, 'static>>>;
⋮----
pub enum BlockRelation {
⋮----
pub trait ForkGraph {
⋮----
pub enum ProgramCacheEntryOwner {
⋮----
type Error = ();
fn try_from(loader_key: &Pubkey) -> Result<Self, ()> {
⋮----
Ok(ProgramCacheEntryOwner::NativeLoader)
⋮----
Ok(ProgramCacheEntryOwner::LoaderV1)
⋮----
Ok(ProgramCacheEntryOwner::LoaderV2)
⋮----
Ok(ProgramCacheEntryOwner::LoaderV3)
⋮----
Ok(ProgramCacheEntryOwner::LoaderV4)
⋮----
Err(())
⋮----
fn from(program_cache_entry_owner: ProgramCacheEntryOwner) -> Self {
⋮----
pub enum ProgramCacheEntryType {
⋮----
impl Debug for ProgramCacheEntryType {
fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
⋮----
write!(f, "ProgramCacheEntryType::FailedVerification")
⋮----
ProgramCacheEntryType::Closed => write!(f, "ProgramCacheEntryType::Closed"),
⋮----
write!(f, "ProgramCacheEntryType::DelayVisibility")
⋮----
ProgramCacheEntryType::Unloaded(_) => write!(f, "ProgramCacheEntryType::Unloaded"),
ProgramCacheEntryType::Loaded(_) => write!(f, "ProgramCacheEntryType::Loaded"),
ProgramCacheEntryType::Builtin(_) => write!(f, "ProgramCacheEntryType::Builtin"),
⋮----
impl ProgramCacheEntryType {
/// Returns a reference to its environment if it has one
    pub fn get_environment(&self) -> Option<&ProgramRuntimeEnvironment> {
⋮----
pub fn get_environment(&self) -> Option<&ProgramRuntimeEnvironment> {
⋮----
ProgramCacheEntryType::Loaded(program) => Some(program.get_loader()),
⋮----
| ProgramCacheEntryType::Unloaded(env) => Some(env),
⋮----
/// Holds a program version at a specific address and on a specific slot / fork.
///
⋮----
///
/// It contains the actual program in [ProgramCacheEntryType] and a bunch of meta-data.
⋮----
/// It contains the actual program in [ProgramCacheEntryType] and a bunch of meta-data.
#[derive(Debug, Default)]
pub struct ProgramCacheEntry {
/// The program of this entry
    pub program: ProgramCacheEntryType,
/// The loader of this entry
    pub account_owner: ProgramCacheEntryOwner,
/// Size of account that stores the program and program data
    pub account_size: usize,
/// Slot in which the program was (re)deployed
    pub deployment_slot: Slot,
/// Slot in which this entry will become active (can be in the future)
    pub effective_slot: Slot,
/// How often this entry was used by a transaction
    pub tx_usage_counter: Arc<AtomicU64>,
/// Latest slot in which the entry was used
    pub latest_access_slot: AtomicU64,
⋮----
/// Global cache statistics for [ProgramCache].
#[derive(Debug, Default)]
pub struct ProgramCacheStats {
/// a program was already in the cache
    pub hits: AtomicU64,
/// a program was not found and loaded instead
    pub misses: AtomicU64,
/// a compiled executable was unloaded
    pub evictions: HashMap<Pubkey, u64>,
/// an unloaded program was loaded again (opposite of eviction)
    pub reloads: AtomicU64,
/// a program was loaded or un/re/deployed
    pub insertions: AtomicU64,
/// a program was loaded but can not be extracted on its own fork anymore
    pub lost_insertions: AtomicU64,
/// a program which was already in the cache was reloaded by mistake
    pub replacements: AtomicU64,
/// a program was only used once before being unloaded
    pub one_hit_wonders: AtomicU64,
/// a program became unreachable in the fork graph because of rerooting
    pub prunes_orphan: AtomicU64,
/// a program got pruned because it was not recompiled for the next epoch
    pub prunes_environment: AtomicU64,
/// a program had no entries because all slot versions got pruned
    pub empty_entries: AtomicU64,
/// water level of loaded entries currently cached
    pub water_level: AtomicU64,
⋮----
impl ProgramCacheStats {
pub fn reset(&mut self) {
⋮----
pub fn log(&self) {
let hits = self.hits.load(Ordering::Relaxed);
let misses = self.misses.load(Ordering::Relaxed);
let evictions: u64 = self.evictions.values().sum();
let reloads = self.reloads.load(Ordering::Relaxed);
let insertions = self.insertions.load(Ordering::Relaxed);
let lost_insertions = self.lost_insertions.load(Ordering::Relaxed);
let replacements = self.replacements.load(Ordering::Relaxed);
let one_hit_wonders = self.one_hit_wonders.load(Ordering::Relaxed);
let prunes_orphan = self.prunes_orphan.load(Ordering::Relaxed);
let prunes_environment = self.prunes_environment.load(Ordering::Relaxed);
let empty_entries = self.empty_entries.load(Ordering::Relaxed);
let water_level = self.water_level.load(Ordering::Relaxed);
debug!(
⋮----
if log_enabled!(log::Level::Trace) && !self.evictions.is_empty() {
let mut evictions = self.evictions.iter().collect::<Vec<_>>();
evictions.sort_by_key(|e| e.1);
⋮----
.into_iter()
.rev()
.map(|(program_id, evictions)| {
format!("  {:<44}  {}", program_id.to_string(), evictions)
⋮----
let evictions = evictions.join("\n");
trace!(
⋮----
/// Time measurements for loading a single [ProgramCacheEntry].
#[derive(Debug, Default)]
pub struct LoadProgramMetrics {
/// Program address, but as text
    pub program_id: String,
/// Microseconds it took to `create_program_runtime_environment`
    pub register_syscalls_us: u64,
/// Microseconds it took to `Executable::<InvokeContext>::load`
    pub load_elf_us: u64,
/// Microseconds it took to `executable.verify::<RequisiteVerifier>`
    pub verify_code_us: u64,
/// Microseconds it took to `executable.jit_compile`
    pub jit_compile_us: u64,
⋮----
impl LoadProgramMetrics {
pub fn submit_datapoint(&self, timings: &mut ExecuteDetailsTimings) {
⋮----
impl PartialEq for ProgramCacheEntry {
fn eq(&self, other: &Self) -> bool {
⋮----
&& self.is_tombstone() == other.is_tombstone()
⋮----
impl ProgramCacheEntry {
/// Creates a new user program
    pub fn new(
⋮----
pub fn new(
⋮----
false, /* reloading */
⋮----
/// Reloads a user program, *without* running the verifier.
    ///
⋮----
///
    /// # Safety
⋮----
/// # Safety
    ///
⋮----
///
    /// This method is unsafe since it assumes that the program has already been verified. Should
⋮----
/// This method is unsafe since it assumes that the program has already been verified. Should
    /// only be called when the program was previously verified and loaded in the cache, but was
⋮----
/// only be called when the program was previously verified and loaded in the cache, but was
    /// unloaded due to inactivity. It should also be checked that the `program_runtime_environment`
⋮----
/// unloaded due to inactivity. It should also be checked that the `program_runtime_environment`
    /// hasn't changed since it was unloaded.
⋮----
/// hasn't changed since it was unloaded.
    pub unsafe fn reload(
⋮----
pub unsafe fn reload(
⋮----
fn new_internal(
⋮----
let mut executable = Executable::load(elf_bytes, program_runtime_environment.clone())?;
⋮----
metrics.load_elf_us = load_elf_time.end_as_us();
⋮----
metrics.verify_code_us = verify_code_time.end_as_us();
⋮----
executable.jit_compile()?;
⋮----
metrics.jit_compile_us = jit_compile_time.end_as_us();
⋮----
Ok(Self {
⋮----
account_owner: ProgramCacheEntryOwner::try_from(loader_key).unwrap(),
⋮----
pub fn to_unloaded(&self) -> Option<Self> {
⋮----
Some(Self {
program: ProgramCacheEntryType::Unloaded(self.program.get_environment()?.clone()),
⋮----
tx_usage_counter: self.tx_usage_counter.clone(),
latest_access_slot: AtomicU64::new(self.latest_access_slot.load(Ordering::Relaxed)),
⋮----
pub fn new_builtin(
⋮----
.register_function("entrypoint", builtin_function)
.unwrap();
⋮----
pub fn new_tombstone(
⋮----
pub fn new_tombstone_with_usage_counter(
⋮----
debug_assert!(tombstone.is_tombstone());
⋮----
pub fn is_tombstone(&self) -> bool {
matches!(
⋮----
fn is_implicit_delay_visibility_tombstone(&self, slot: Slot) -> bool {
!matches!(self.program, ProgramCacheEntryType::Builtin(_))
&& self.effective_slot.saturating_sub(self.deployment_slot)
⋮----
pub fn update_access_slot(&self, slot: Slot) {
let _ = self.latest_access_slot.fetch_max(slot, Ordering::Relaxed);
⋮----
pub fn decayed_usage_counter(&self, now: Slot) -> u64 {
let last_access = self.latest_access_slot.load(Ordering::Relaxed);
let decaying_for = std::cmp::min(63, now.saturating_sub(last_access));
self.tx_usage_counter.load(Ordering::Relaxed) >> decaying_for
⋮----
pub fn account_owner(&self) -> Pubkey {
self.account_owner.into()
⋮----
pub struct ProgramRuntimeEnvironments {
⋮----
impl Default for ProgramRuntimeEnvironments {
fn default() -> Self {
⋮----
program_runtime_v1: empty_loader.clone(),
⋮----
pub struct EpochBoundaryPreparation {
⋮----
impl EpochBoundaryPreparation {
pub fn new(epoch: Epoch) -> Self {
⋮----
pub fn get_upcoming_environments_for_epoch(
⋮----
return self.upcoming_environments.clone();
⋮----
pub fn reroot(&mut self, epoch: Epoch) -> Option<ProgramRuntimeEnvironments> {
⋮----
if let Some(upcoming_environments) = self.upcoming_environments.take() {
self.programs_to_recompile.clear();
return Some(upcoming_environments);
⋮----
pub struct LoadingTaskCookie(u64);
impl LoadingTaskCookie {
fn new() -> Self {
Self(0)
⋮----
fn update(&mut self) {
⋮----
*cookie = cookie.wrapping_add(1);
⋮----
pub struct LoadingTaskWaiter {
⋮----
impl LoadingTaskWaiter {
pub fn new() -> Self {
⋮----
pub fn cookie(&self) -> LoadingTaskCookie {
*self.cookie.lock().unwrap()
⋮----
pub fn notify(&self) {
let mut cookie = self.cookie.lock().unwrap();
cookie.update();
self.cond.notify_all();
⋮----
pub fn wait(&self, cookie: LoadingTaskCookie) -> LoadingTaskCookie {
let cookie_guard = self.cookie.lock().unwrap();
⋮----
.wait_while(cookie_guard, |current_cookie| *current_cookie == cookie)
.unwrap()
⋮----
enum IndexImplementation {
⋮----
pub struct ProgramCache<FG: ForkGraph> {
⋮----
impl<FG: ForkGraph> Debug for ProgramCache<FG> {
⋮----
f.debug_struct("ProgramCache")
.field("root slot", &self.latest_root_slot)
.field("stats", &self.stats)
.field("index", &self.index)
.finish()
⋮----
/// Local view into [ProgramCache] which was extracted for a specific TX batch.
///
⋮----
///
/// This isolation enables the global [ProgramCache] to continue to evolve (e.g. evictions),
⋮----
/// This isolation enables the global [ProgramCache] to continue to evolve (e.g. evictions),
/// while the TX batch is guaranteed it will continue to find all the programs it requires.
⋮----
/// while the TX batch is guaranteed it will continue to find all the programs it requires.
/// For program management instructions this also buffers them before they are merged back into the global [ProgramCache].
⋮----
/// For program management instructions this also buffers them before they are merged back into the global [ProgramCache].
#[derive(Clone, Debug, Default)]
pub struct ProgramCacheForTxBatch {
/// Pubkey is the address of a program.
    /// ProgramCacheEntry is the corresponding program entry valid for the slot in which a transaction is being executed.
⋮----
/// ProgramCacheEntry is the corresponding program entry valid for the slot in which a transaction is being executed.
    entries: HashMap<Pubkey, Arc<ProgramCacheEntry>>,
/// Program entries modified during the transaction batch.
    modified_entries: HashMap<Pubkey, Arc<ProgramCacheEntry>>,
⋮----
impl ProgramCacheForTxBatch {
pub fn new(slot: Slot) -> Self {
⋮----
/// Refill the cache with a single entry. It's typically called during transaction loading, and
    pub fn replenish(
⋮----
pub fn replenish(
⋮----
(self.entries.insert(key, entry.clone()).is_some(), entry)
⋮----
pub fn store_modified_entry(&mut self, key: Pubkey, entry: Arc<ProgramCacheEntry>) {
self.modified_entries.insert(key, entry);
⋮----
pub fn drain_modified_entries(&mut self) -> HashMap<Pubkey, Arc<ProgramCacheEntry>> {
⋮----
pub fn find(&self, key: &Pubkey) -> Option<Arc<ProgramCacheEntry>> {
⋮----
.get(key)
.or_else(|| self.entries.get(key))
.map(|entry| {
if entry.is_implicit_delay_visibility_tombstone(self.slot) {
⋮----
entry.tx_usage_counter.clone(),
⋮----
entry.clone()
⋮----
pub fn slot(&self) -> Slot {
⋮----
pub fn set_slot_for_tests(&mut self, slot: Slot) {
⋮----
pub fn merge(&mut self, modified_entries: &HashMap<Pubkey, Arc<ProgramCacheEntry>>) {
modified_entries.iter().for_each(|(key, entry)| {
⋮----
self.replenish(*key, entry.clone());
⋮----
pub fn is_empty(&self) -> bool {
self.entries.is_empty()
⋮----
pub enum ProgramCacheMatchCriteria {
⋮----
pub fn new(root_slot: Slot) -> Self {
⋮----
pub fn set_fork_graph(&mut self, fork_graph: Weak<RwLock<FG>>) {
self.fork_graph = Some(fork_graph);
⋮----
pub fn assign_program(
⋮----
debug_assert!(!matches!(
⋮----
fn is_current_env(
⋮----
.map(|env| {
⋮----
.unwrap_or(true)
⋮----
let slot_versions = &mut entries.entry(key).or_default();
match slot_versions.binary_search_by(|at| {
⋮----
.cmp(&entry.effective_slot)
.then(at.deployment_slot.cmp(&entry.deployment_slot))
.then(
is_current_env(
⋮----
at.program.get_environment(),
⋮----
.cmp(&is_current_env(
⋮----
entry.program.get_environment(),
⋮----
let existing = slot_versions.get_mut(index).unwrap();
⋮----
error!(
⋮----
debug_assert!(false, "Unexpected replacement of an entry");
self.stats.replacements.fetch_add(1, Ordering::Relaxed);
⋮----
entry.tx_usage_counter.fetch_add(
existing.tx_usage_counter.load(Ordering::Relaxed),
⋮----
self.stats.reloads.fetch_add(1, Ordering::Relaxed);
⋮----
self.stats.insertions.fetch_add(1, Ordering::Relaxed);
slot_versions.insert(index, Arc::clone(&entry));
⋮----
slot_versions.retain(|existing| {
⋮----
.get_environment()
.zip(entry.program.get_environment())
.map(|(a, b)| !Arc::ptr_eq(a, b))
.unwrap_or(false)
⋮----
pub fn prune_by_deployment_slot(&mut self, slot: Slot) {
⋮----
for second_level in entries.values_mut() {
second_level.retain(|entry| entry.deployment_slot != slot);
⋮----
self.remove_programs_with_no_entries();
⋮----
pub fn prune(
⋮----
let Some(fork_graph) = self.fork_graph.clone() else {
error!("Program cache doesn't have fork graph.");
⋮----
let fork_graph = fork_graph.upgrade().unwrap();
let Ok(fork_graph) = fork_graph.read() else {
error!("Failed to lock fork graph for reading.");
⋮----
.iter()
⋮----
.filter(|entry| {
⋮----
fork_graph.relationship(entry.deployment_slot, new_root_slot);
⋮----
matches!(relation, BlockRelation::Equal | BlockRelation::Descendant)
} else if matches!(relation, BlockRelation::Ancestor)
⋮----
first_ancestor_env = entry.program.get_environment();
⋮----
if let Some(entry_env) = entry.program.get_environment() {
⋮----
self.stats.prunes_orphan.fetch_add(1, Ordering::Relaxed);
⋮----
if let Some(upcoming_environments) = upcoming_environments.as_ref() {
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
.cloned()
.collect();
second_level.reverse();
⋮----
debug_assert!(self.latest_root_slot <= new_root_slot);
⋮----
fn matches_environment(
⋮----
let Some(environment) = entry.program.get_environment() else {
⋮----
fn matches_criteria(
⋮----
ProgramCacheMatchCriteria::Tombstone => program.is_tombstone(),
⋮----
pub fn extract(
⋮----
debug_assert!(self.fork_graph.is_some());
let fork_graph = self.fork_graph.as_ref().unwrap().upgrade().unwrap();
let locked_fork_graph = fork_graph.read().unwrap();
⋮----
search_for.retain(|(key, match_criteria)| {
if let Some(second_level) = entries.get(key) {
⋮----
for entry in second_level.iter().rev() {
⋮----
.map(|slot| slot != entry.deployment_slot)
⋮----
|| matches!(
⋮----
.or(Some(entry.deployment_slot));
⋮----
} else if entry.is_implicit_delay_visibility_tombstone(
⋮----
.update_access_slot(loaded_programs_for_tx_batch.slot);
⋮----
.insert(*key, entry_to_return);
⋮----
if cooperative_loading_task.is_none() {
let mut loading_entries = loading_entries.lock().unwrap();
let entry = loading_entries.entry(*key);
⋮----
entry.insert((
⋮----
thread::current().id(),
⋮----
cooperative_loading_task = Some(*key);
⋮----
drop(locked_fork_graph);
⋮----
.fetch_add(search_for.len() as u64, Ordering::Relaxed);
self.stats.hits.fetch_add(
loaded_programs_for_tx_batch.entries.len() as u64,
⋮----
pub fn finish_cooperative_loading_task(
⋮----
let loading_thread = loading_entries.get_mut().unwrap().remove(&key);
debug_assert_eq!(loading_thread, Some((slot, thread::current().id())));
⋮----
&& !matches!(
⋮----
self.stats.lost_insertions.fetch_add(1, Ordering::Relaxed);
⋮----
self.assign_program(program_runtime_environments, key, loaded_program);
self.loading_task_waiter.notify();
⋮----
pub fn merge(
⋮----
self.assign_program(program_runtime_environments, *key, entry.clone());
⋮----
pub fn get_flattened_entries(
⋮----
.flat_map(|(id, second_level)| {
⋮----
.filter_map(move |program| match program.program {
⋮----
Some((*id, program.clone()))
⋮----
.collect(),
⋮----
pub fn get_flattened_entries_for_tests(&self) -> Vec<(Pubkey, Arc<ProgramCacheEntry>)> {
⋮----
second_level.iter().map(|program| (*id, program.clone()))
⋮----
pub fn get_slot_versions_for_tests(&self, key: &Pubkey) -> &[Arc<ProgramCacheEntry>] {
⋮----
.map(|second_level| second_level.as_ref())
.unwrap_or(&[]),
⋮----
pub fn sort_and_unload(&mut self, shrink_to: PercentageInteger) {
let mut sorted_candidates = self.get_flattened_entries(true, true);
⋮----
.sort_by_cached_key(|(_id, program)| program.tx_usage_counter.load(Ordering::Relaxed));
⋮----
.len()
.saturating_sub(shrink_to.apply_to(MAX_LOADED_ENTRY_COUNT));
self.unload_program_entries(sorted_candidates.iter().take(num_to_unload));
⋮----
pub fn evict_using_2s_random_selection(&mut self, shrink_to: PercentageInteger, now: Slot) {
let mut candidates = self.get_flattened_entries(true, true);
⋮----
.store(candidates.len() as u64, Ordering::Relaxed);
⋮----
fn random_index_and_usage_counter(
⋮----
let mut rng = rng();
⋮----
let index = rng.gen_range(0..candidates.len());
⋮----
.get(index)
.expect("Failed to get cached entry")
⋮----
.decayed_usage_counter(now);
⋮----
let (index1, usage_counter1) = random_index_and_usage_counter(&candidates, now);
let (index2, usage_counter2) = random_index_and_usage_counter(&candidates, now);
⋮----
candidates.swap_remove(index1)
⋮----
candidates.swap_remove(index2)
⋮----
self.unload_program_entry(&program, &entry);
⋮----
pub fn remove_programs(&mut self, keys: impl Iterator<Item = Pubkey>) {
⋮----
entries.remove(&k);
⋮----
fn unload_program_entry(&mut self, program: &Pubkey, remove_entry: &Arc<ProgramCacheEntry>) {
⋮----
let second_level = entries.get_mut(program).expect("Cache lookup failed");
⋮----
.iter_mut()
.find(|entry| entry == &remove_entry)
.expect("Program entry not found");
if let Some(unloaded) = candidate.to_unloaded() {
if candidate.tx_usage_counter.load(Ordering::Relaxed) == 1 {
self.stats.one_hit_wonders.fetch_add(1, Ordering::Relaxed);
⋮----
.entry(*program)
.and_modify(|c| *c = c.saturating_add(1))
.or_insert(1);
⋮----
fn unload_program_entries<'a>(
⋮----
self.unload_program_entry(program, entry);
⋮----
fn remove_programs_with_no_entries(&mut self) {
⋮----
let num_programs_before_removal = entries.len();
entries.retain(|_key, second_level| !second_level.is_empty());
if entries.len() < num_programs_before_removal {
self.stats.empty_entries.fetch_add(
num_programs_before_removal.saturating_sub(entries.len()) as u64,
⋮----
fn example() -> Self {
⋮----
mod tests {
⋮----
fn get_mock_env() -> ProgramRuntimeEnvironment {
⋮----
.get_or_init(|| Arc::new(BuiltinProgram::new_mock()))
.clone()
⋮----
fn get_mock_envs() -> ProgramRuntimeEnvironments {
⋮----
program_runtime_v1: get_mock_env(),
program_runtime_v2: get_mock_env(),
⋮----
fn new_test_entry(deployment_slot: Slot, effective_slot: Slot) -> Arc<ProgramCacheEntry> {
new_test_entry_with_usage(deployment_slot, effective_slot, AtomicU64::default())
⋮----
fn new_loaded_entry(env: ProgramRuntimeEnvironment) -> ProgramCacheEntryType {
⋮----
.read_to_end(&mut elf)
⋮----
let executable = Executable::load(&elf, env).unwrap();
⋮----
fn new_test_entry_with_usage(
⋮----
program: new_loaded_entry(get_mock_env()),
⋮----
fn new_test_builtin_entry(
⋮----
fn set_tombstone<FG: ForkGraph>(
⋮----
let envs = get_mock_envs();
⋮----
cache.assign_program(&envs, key, program.clone());
⋮----
fn insert_unloaded_entry<FG: ForkGraph>(
⋮----
let loaded = new_test_entry_with_usage(slot, slot.saturating_add(1), AtomicU64::default());
let unloaded = Arc::new(loaded.to_unloaded().expect("Failed to unload the program"));
cache.assign_program(&envs, key, unloaded.clone());
⋮----
fn num_matching_entries<P, FG>(cache: &ProgramCache<FG>, predicate: P) -> usize
⋮----
.get_flattened_entries_for_tests()
⋮----
.filter(|(_key, program)| predicate(&program.program))
.count()
⋮----
fn test_usage_counter_decay() {
let program = new_test_entry_with_usage(10, 11, AtomicU64::new(32));
program.update_access_slot(15);
assert_eq!(program.decayed_usage_counter(15), 32);
assert_eq!(program.decayed_usage_counter(16), 16);
assert_eq!(program.decayed_usage_counter(17), 8);
assert_eq!(program.decayed_usage_counter(18), 4);
assert_eq!(program.decayed_usage_counter(19), 2);
assert_eq!(program.decayed_usage_counter(20), 1);
assert_eq!(program.decayed_usage_counter(21), 0);
⋮----
assert_eq!(program.decayed_usage_counter(14), 32);
program.update_access_slot(18);
⋮----
assert_eq!(program.decayed_usage_counter(16), 32);
assert_eq!(program.decayed_usage_counter(17), 32);
assert_eq!(program.decayed_usage_counter(18), 32);
assert_eq!(program.decayed_usage_counter(19), 16);
assert_eq!(program.decayed_usage_counter(20), 8);
assert_eq!(program.decayed_usage_counter(21), 4);
assert_eq!(program.decayed_usage_counter(18 + 63), 0);
assert_eq!(program.decayed_usage_counter(100), 0);
⋮----
fn program_deploy_test_helper(
⋮----
.enumerate()
.for_each(|(i, deployment_slot)| {
let usage_counter = *usage_counters.get(i).unwrap_or(&0);
cache.assign_program(
⋮----
new_test_entry_with_usage(
⋮----
(*deployment_slot).saturating_add(2),
⋮----
programs.push((program, *deployment_slot, usage_counter));
⋮----
set_tombstone(
⋮----
ProgramCacheEntryType::FailedVerification(env.clone()),
⋮----
insert_unloaded_entry(cache, program, slot);
⋮----
fn test_random_eviction() {
let mut programs = vec![];
⋮----
program_deploy_test_helper(
⋮----
vec![0, 10, 20],
vec![4, 5, 25],
⋮----
vec![5, 11],
vec![0, 2],
⋮----
vec![0, 5, 15],
vec![100, 3, 20],
⋮----
programs.sort_by_key(|(_id, _slot, usage_count)| *usage_count);
let num_loaded = num_matching_entries(&cache, |program_type| {
matches!(program_type, ProgramCacheEntryType::Loaded(_))
⋮----
let num_unloaded = num_matching_entries(&cache, |program_type| {
matches!(program_type, ProgramCacheEntryType::Unloaded(_))
⋮----
let num_tombstones = num_matching_entries(&cache, |program_type| {
⋮----
assert_eq!(num_loaded, num_loaded_expected);
assert_eq!(num_unloaded, num_unloaded_expected);
assert_eq!(num_tombstones, num_tombstones_expected);
⋮----
Percentage::from(eviction_pct).apply_to(crate::loaded_programs::MAX_LOADED_ENTRY_COUNT);
⋮----
cache.evict_using_2s_random_selection(Percentage::from(eviction_pct), 21);
⋮----
matches!(program_type, ProgramCacheEntryType::FailedVerification(_))
⋮----
fn test_eviction() {
⋮----
cache.sort_and_unload(Percentage::from(eviction_pct));
let entries = cache.get_flattened_entries_for_tests();
programs.iter().for_each(|entry| {
assert!(entries.iter().any(|(key, _entry)| key == &entry.0));
⋮----
.filter_map(|(key, program)| {
matches!(program.program, ProgramCacheEntryType::Unloaded(_))
.then_some((*key, program.tx_usage_counter.load(Ordering::Relaxed)))
⋮----
let expected = programs.get(index).expect("Missing program");
assert!(unloaded.contains(&(expected.0, expected.2)));
⋮----
fn test_usage_count_of_unloaded_program() {
⋮----
Percentage::from(evict_to_pct).apply_to(crate::loaded_programs::MAX_LOADED_ENTRY_COUNT);
⋮----
(0..num_total_programs).for_each(|i| {
⋮----
new_test_entry_with_usage(i, i + 2, AtomicU64::new(i + 10)),
⋮----
cache.sort_and_unload(Percentage::from(evict_to_pct));
⋮----
assert_eq!(num_unloaded, 1);
⋮----
.for_each(|(_key, program)| {
if matches!(program.program, ProgramCacheEntryType::Unloaded(_)) {
assert_eq!(program.tx_usage_counter.load(Ordering::Relaxed), 10);
assert_eq!(program.deployment_slot, 0);
assert_eq!(program.effective_slot, 2);
⋮----
new_test_entry_with_usage(0, 2, AtomicU64::new(0)),
⋮----
if matches!(program.program, ProgramCacheEntryType::Unloaded(_))
⋮----
fn test_fuzz_assign_program_order() {
use rand::prelude::SliceRandom;
⋮----
let mut entries = EXPECTED_ENTRIES.to_vec();
entries.shuffle(&mut rng);
⋮----
program: new_loaded_entry(Arc::new(BuiltinProgram::new_mock())),
⋮----
assert!(!cache.assign_program(&envs, program_id, entry));
⋮----
.zip(cache.get_slot_versions_for_tests(&program_id).iter())
⋮----
assert_eq!(entry.deployment_slot, *deployment_slot);
assert_eq!(entry.effective_slot, *effective_slot);
⋮----
fn test_assign_program_failure(old: ProgramCacheEntryType, new: ProgramCacheEntryType) {
⋮----
assert!(!cache.assign_program(
⋮----
fn test_assign_program_success(old: ProgramCacheEntryType, new: ProgramCacheEntryType) {
⋮----
fn test_assign_program_removes_entries_in_same_slot() {
⋮----
program: ProgramCacheEntryType::Unloaded(get_mock_env()),
⋮----
assert!(!cache.assign_program(&envs, program_id, closed_other_slot.clone()));
assert!(!cache.assign_program(&envs, program_id, closed_current_slot));
assert!(!cache.assign_program(&envs, program_id, loaded_entry_upcoming_env.clone()));
assert!(!cache.assign_program(&envs, program_id, loaded_entry_current_env.clone()));
assert_eq!(
⋮----
fn test_tombstone() {
⋮----
assert_matches!(
⋮----
assert!(tombstone.is_tombstone());
assert_eq!(tombstone.deployment_slot, 0);
assert_eq!(tombstone.effective_slot, 0);
⋮----
assert_matches!(tombstone.program, ProgramCacheEntryType::Closed);
⋮----
assert_eq!(tombstone.deployment_slot, 100);
assert_eq!(tombstone.effective_slot, 100);
⋮----
let tombstone = set_tombstone(
⋮----
let slot_versions = cache.get_slot_versions_for_tests(&program1);
assert_eq!(slot_versions.len(), 1);
assert!(slot_versions.first().unwrap().is_tombstone());
assert_eq!(tombstone.deployment_slot, 10);
assert_eq!(tombstone.effective_slot, 10);
⋮----
cache.assign_program(&envs, program2, new_test_builtin_entry(50, 51));
let slot_versions = cache.get_slot_versions_for_tests(&program2);
⋮----
assert!(!slot_versions.first().unwrap().is_tombstone());
⋮----
assert_eq!(slot_versions.len(), 2);
⋮----
assert!(slot_versions.get(1).unwrap().is_tombstone());
⋮----
assert_eq!(tombstone.deployment_slot, 60);
assert_eq!(tombstone.effective_slot, 60);
⋮----
struct TestForkGraph {
⋮----
impl ForkGraph for TestForkGraph {
fn relationship(&self, _a: Slot, _b: Slot) -> BlockRelation {
⋮----
fn test_prune_empty() {
⋮----
cache.set_fork_graph(Arc::downgrade(&fork_graph));
cache.prune(0, None);
assert!(cache.get_flattened_entries_for_tests().is_empty());
cache.prune(10, None);
⋮----
fn test_prune_different_env() {
⋮----
cache.assign_program(&envs, program1, new_test_entry(10, 10));
⋮----
let upcoming_environments = Some(ProgramRuntimeEnvironments {
program_runtime_v1: new_env.clone(),
program_runtime_v2: new_env.clone(),
⋮----
program: new_loaded_entry(new_env.clone()),
⋮----
cache.assign_program(&envs, program1, updated_program.clone());
assert_eq!(cache.get_slot_versions_for_tests(&program1).len(), 2);
cache.prune(21, None);
⋮----
cache.prune(22, upcoming_environments);
assert_eq!(cache.get_slot_versions_for_tests(&program1).len(), 1);
⋮----
.get_slot_versions_for_tests(&program1)
.first()
.expect("Failed to get the program")
.clone();
assert_eq!(entry, updated_program);
⋮----
struct TestForkGraphSpecific {
⋮----
impl TestForkGraphSpecific {
fn insert_fork(&mut self, fork: &[Slot]) {
let mut fork = fork.to_vec();
fork.sort();
self.forks.push(fork)
⋮----
impl ForkGraph for TestForkGraphSpecific {
fn relationship(&self, a: Slot, b: Slot) -> BlockRelation {
match self.forks.iter().try_for_each(|fork| {
⋮----
.position(|x| *x == a)
.and_then(|a_pos| {
fork.iter().position(|x| *x == b).and_then(|b_pos| {
⋮----
.then_some(BlockRelation::Equal)
.or_else(|| (a_pos < b_pos).then_some(BlockRelation::Ancestor))
.or(Some(BlockRelation::Descendant))
⋮----
.unwrap_or(BlockRelation::Unrelated);
⋮----
fn get_entries_to_load(
⋮----
let fork_graph = cache.fork_graph.as_ref().unwrap().upgrade().unwrap();
⋮----
keys.iter()
.filter_map(|key| {
⋮----
.find(|(program_id, entry)| {
⋮----
&& matches!(
⋮----
.map(|(program_id, _entry)| {
⋮----
.collect()
⋮----
fn match_slot(
⋮----
assert_eq!(extracted.slot, working_slot);
⋮----
.get(program)
.map(|entry| entry.deployment_slot == deployment_slot)
⋮----
fn match_missing(
⋮----
missing.iter().any(|(key, _)| key == program) == expected_result
⋮----
fn test_fork_extract_and_prune() {
⋮----
fork_graph.insert_fork(&[0, 10, 20, 22]);
fork_graph.insert_fork(&[0, 5, 11, 15, 16, 18, 19, 21, 23]);
fork_graph.insert_fork(&[0, 5, 11, 25, 27]);
⋮----
cache.assign_program(&envs, program1, new_test_entry(0, 1));
cache.assign_program(&envs, program1, new_test_entry(10, 11));
cache.assign_program(&envs, program1, new_test_entry(20, 21));
⋮----
cache.assign_program(&envs, program2, new_test_entry(5, 6));
⋮----
new_test_entry(11, 11 + DELAY_VISIBILITY_SLOT_OFFSET),
⋮----
cache.assign_program(&envs, program3, new_test_entry(25, 26));
⋮----
cache.assign_program(&envs, program4, new_test_entry(0, 1));
cache.assign_program(&envs, program4, new_test_entry(5, 6));
⋮----
new_test_entry(15, 15 + DELAY_VISIBILITY_SLOT_OFFSET),
⋮----
get_entries_to_load(&cache, 22, &[program1, program2, program3, program4]);
assert!(match_missing(&missing, &program2, false));
assert!(match_missing(&missing, &program3, false));
⋮----
cache.extract(&mut missing, &mut extracted, &envs, true, true);
assert!(match_slot(&extracted, &program1, 20, 22));
assert!(match_slot(&extracted, &program4, 0, 22));
⋮----
get_entries_to_load(&cache, 15, &[program1, program2, program3, program4]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 15));
assert!(match_slot(&extracted, &program2, 11, 15));
⋮----
.find(&program4)
.expect("Failed to find the tombstone");
assert_matches!(tombstone.program, ProgramCacheEntryType::DelayVisibility);
assert_eq!(tombstone.deployment_slot, 15);
⋮----
get_entries_to_load(&cache, 18, &[program1, program2, program3, program4]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 18));
assert!(match_slot(&extracted, &program2, 11, 18));
assert!(match_slot(&extracted, &program4, 15, 18));
⋮----
get_entries_to_load(&cache, 23, &[program1, program2, program3, program4]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 23));
assert!(match_slot(&extracted, &program2, 11, 23));
assert!(match_slot(&extracted, &program4, 15, 23));
⋮----
get_entries_to_load(&cache, 11, &[program1, program2, program3, program4]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 11));
⋮----
.find(&program2)
⋮----
assert_eq!(tombstone.deployment_slot, 11);
assert!(match_slot(&extracted, &program4, 5, 11));
cache.prune(5, None);
⋮----
get_entries_to_load(&cache, 21, &[program1, program2, program3, program4]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 21));
assert!(match_slot(&extracted, &program2, 11, 21));
assert!(match_slot(&extracted, &program4, 15, 21));
⋮----
get_entries_to_load(&cache, 27, &[program1, program2, program3, program4]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 27));
assert!(match_slot(&extracted, &program2, 11, 27));
assert!(match_slot(&extracted, &program3, 25, 27));
assert!(match_slot(&extracted, &program4, 5, 27));
cache.prune(15, None);
⋮----
fn test_extract_using_deployment_slot() {
⋮----
fork_graph.insert_fork(&[0, 5, 11, 12, 15, 16, 18, 19, 21, 23]);
⋮----
cache.assign_program(&envs, program2, new_test_entry(11, 12));
⋮----
let mut missing = get_entries_to_load(&cache, 12, &[program1, program2, program3]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 12));
assert!(match_slot(&extracted, &program2, 11, 12));
⋮----
missing.get_mut(0).unwrap().1 = ProgramCacheMatchCriteria::DeployedOnOrAfterSlot(5);
missing.get_mut(1).unwrap().1 = ProgramCacheMatchCriteria::DeployedOnOrAfterSlot(5);
⋮----
assert!(match_missing(&missing, &program1, true));
⋮----
fn test_extract_unloaded() {
⋮----
fork_graph.insert_fork(&[0, 5, 11, 15, 16, 19, 21, 23]);
⋮----
let _ = insert_unloaded_entry(&mut cache, program3, 25);
⋮----
new_test_entry(20, 21)
.to_unloaded()
.expect("Failed to create unloaded program"),
⋮----
let mut missing = get_entries_to_load(&cache, 19, &[program1, program2, program3]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 19));
assert!(match_slot(&extracted, &program2, 11, 19));
let mut missing = get_entries_to_load(&cache, 27, &[program1, program2, program3]);
⋮----
assert!(match_missing(&missing, &program3, true));
let mut missing = get_entries_to_load(&cache, 22, &[program1, program2, program3]);
⋮----
fn test_extract_different_environment() {
⋮----
let mut missing = get_entries_to_load(&cache, 22, &[program1]);
⋮----
cache.extract(&mut missing, &mut extracted, &other_envs, true, true);
⋮----
fn test_extract_nonexistent() {
⋮----
let mut missing = vec![(program1, ProgramCacheMatchCriteria::NoCriteria)];
⋮----
fn test_unloaded() {
⋮----
ProgramCacheEntryType::FailedVerification(get_mock_env()),
⋮----
ProgramCacheEntryType::Unloaded(get_mock_env()),
⋮----
assert!(entry.to_unloaded().is_none());
⋮----
cache.assign_program(&envs, program_id, entry.clone());
cache.unload_program_entry(&program_id, &entry);
assert_eq!(cache.get_slot_versions_for_tests(&program_id).len(), 1);
assert!(cache.stats.evictions.is_empty());
⋮----
let entry = new_test_entry_with_usage(1, 2, AtomicU64::new(3));
let unloaded_entry = entry.to_unloaded().unwrap();
assert_eq!(unloaded_entry.deployment_slot, 1);
assert_eq!(unloaded_entry.effective_slot, 2);
assert_eq!(unloaded_entry.latest_access_slot.load(Ordering::Relaxed), 1);
assert_eq!(unloaded_entry.tx_usage_counter.load(Ordering::Relaxed), 3);
⋮----
assert!(cache.stats.evictions.contains_key(&program_id));
⋮----
fn test_fork_prune_find_first_ancestor() {
⋮----
fork_graph.insert_fork(&[0, 10, 20]);
fork_graph.insert_fork(&[0, 5]);
⋮----
cache.assign_program(&envs, program1, new_test_entry(5, 6));
⋮----
let mut missing = get_entries_to_load(&cache, 20, &[program1]);
⋮----
fn test_prune_by_deployment_slot() {
⋮----
fork_graph.insert_fork(&[0, 5, 6]);
⋮----
cache.assign_program(&envs, program2, new_test_entry(10, 11));
let mut missing = get_entries_to_load(&cache, 20, &[program1, program2]);
⋮----
assert!(match_slot(&extracted, &program1, 0, 20));
assert!(match_slot(&extracted, &program2, 10, 20));
let mut missing = get_entries_to_load(&cache, 6, &[program1, program2]);
⋮----
assert!(match_slot(&extracted, &program1, 5, 6));
cache.prune_by_deployment_slot(5);
⋮----
assert!(match_slot(&extracted, &program1, 0, 6));
cache.prune_by_deployment_slot(10);
⋮----
fn test_usable_entries_for_slot() {
⋮----
assert!(ProgramCache::<TestForkGraph>::matches_criteria(
⋮----
assert!(!ProgramCache::<TestForkGraph>::matches_criteria(
⋮----
let program = new_test_entry(0, 1);
⋮----
let program = Arc::new(new_test_entry_with_usage(0, 1, AtomicU64::default()));

================
File: program-runtime/src/mem_pool.rs
================
trait Reset {
⋮----
struct Pool<T: Reset, const SIZE: usize> {
⋮----
fn new(items: [T; SIZE]) -> Self {
⋮----
items: items.map(|i| Some(i)),
⋮----
fn len(&self) -> usize {
⋮----
fn get(&mut self) -> Option<T> {
⋮----
self.next_empty = self.next_empty.saturating_sub(1);
⋮----
.get_mut(self.next_empty)
.and_then(|item| item.take())
⋮----
fn put(&mut self, mut value: T) -> bool {
⋮----
.map(|item| {
value.reset();
item.replace(value);
self.next_empty = self.next_empty.saturating_add(1);
⋮----
.unwrap_or(false)
⋮----
impl Reset for AlignedMemory<{ HOST_ALIGN }> {
fn reset(&mut self) {
self.as_slice_mut().fill(0)
⋮----
pub struct VmMemoryPool {
⋮----
impl VmMemoryPool {
pub fn new() -> Self {
⋮----
pub fn stack_len(&self) -> usize {
self.stack.len()
⋮----
pub fn heap_len(&self) -> usize {
self.heap.len()
⋮----
pub fn get_stack(&mut self, size: usize) -> AlignedMemory<{ HOST_ALIGN }> {
debug_assert!(size == STACK_FRAME_SIZE * MAX_CALL_DEPTH);
⋮----
.get()
.unwrap_or_else(|| AlignedMemory::zero_filled(size))
⋮----
pub fn put_stack(&mut self, stack: AlignedMemory<{ HOST_ALIGN }>) -> bool {
self.stack.put(stack)
⋮----
pub fn get_heap(&mut self, heap_size: u32) -> AlignedMemory<{ HOST_ALIGN }> {
debug_assert!((MIN_HEAP_FRAME_BYTES..=MAX_HEAP_FRAME_BYTES).contains(&heap_size));
⋮----
.unwrap_or_else(|| AlignedMemory::zero_filled(MAX_HEAP_FRAME_BYTES as usize))
⋮----
pub fn put_heap(&mut self, heap: AlignedMemory<{ HOST_ALIGN }>) -> bool {
let heap_size = heap.len();
debug_assert!(
⋮----
self.heap.put(heap)
⋮----
impl Default for VmMemoryPool {
fn default() -> Self {
⋮----
mod test {
⋮----
struct Item(u8, u8);
impl Reset for Item {
⋮----
fn test_pool() {
let mut pool = Pool::<Item, 2>::new([Item(0, 1), Item(1, 1)]);
assert_eq!(pool.get(), Some(Item(1, 1)));
assert_eq!(pool.get(), Some(Item(0, 1)));
assert_eq!(pool.get(), None);
pool.put(Item(1, 1));
assert_eq!(pool.get(), Some(Item(1, 0)));
pool.put(Item(2, 2));
pool.put(Item(3, 3));
assert!(!pool.put(Item(4, 4)));
assert_eq!(pool.get(), Some(Item(3, 0)));
assert_eq!(pool.get(), Some(Item(2, 0)));

================
File: program-runtime/src/memory.rs
================
pub enum MemoryTranslationError {
⋮----
pub fn address_is_aligned<T>(address: u64) -> bool {
⋮----
.checked_rem(align_of::<T>())
.map(|rem| rem == 0)
.expect("T to be non-zero aligned")
⋮----
macro_rules! translate_inner {
⋮----
macro_rules! translate_type_inner {
⋮----
macro_rules! translate_slice_inner {
⋮----
pub fn translate_type<'a, T>(
⋮----
translate_type_inner!(memory_mapping, AccessType::Load, vm_addr, T, check_aligned)
.map(|value| &*value)
⋮----
pub fn translate_slice<'a, T>(
⋮----
translate_slice_inner!(
⋮----
pub fn translate_type_mut_for_cpi<'a, T>(
⋮----
translate_type_inner!(memory_mapping, AccessType::Store, vm_addr, T, check_aligned)
⋮----
pub fn translate_slice_mut_for_cpi<'a, T>(
⋮----
pub fn translate_vm_slice<'a, T>(
⋮----
translate_slice::<T>(memory_mapping, slice.ptr(), slice.len(), check_aligned)

================
File: program-runtime/src/serialization.rs
================
pub fn modify_memory_region_of_account(
⋮----
region.len = account.get_data().len() as u64;
if account.can_data_be_changed().is_ok() {
⋮----
region.access_violation_handler_payload = Some(account.get_index_in_transaction());
⋮----
pub fn create_memory_region_of_account(
⋮----
let can_data_be_changed = account.can_data_be_changed().is_ok();
let mut memory_region = if can_data_be_changed && !account.is_shared() {
MemoryRegion::new_writable(account.get_data_mut()?, vaddr)
⋮----
MemoryRegion::new_readonly(account.get_data(), vaddr)
⋮----
memory_region.access_violation_handler_payload = Some(account.get_index_in_transaction());
⋮----
Ok(memory_region)
⋮----
enum SerializeAccount<'a, 'ix_data> {
⋮----
struct Serializer {
⋮----
impl Serializer {
fn new(
⋮----
fn fill_write(&mut self, num: usize, value: u8) -> std::io::Result<()> {
self.buffer.fill_write(num, value)
⋮----
fn write<T: Pod>(&mut self, value: T) -> u64 {
⋮----
.saturating_add(self.buffer.len() as u64)
.saturating_sub(self.region_start as u64);
⋮----
self.buffer.write_unchecked(value);
⋮----
fn write_all(&mut self, value: &[u8]) -> u64 {
⋮----
self.buffer.write_all_unchecked(value);
⋮----
fn write_account(
⋮----
let vm_data_addr = self.vaddr.saturating_add(self.buffer.len() as u64);
self.write_all(account.get_data());
⋮----
(account.get_data().len() as *const u8).align_offset(BPF_ALIGN_OF_U128);
self.fill_write(MAX_PERMITTED_DATA_INCREASE + align_offset, 0)
.map_err(|_| InstructionError::InvalidArgument)?;
⋮----
Ok(vm_data_addr)
⋮----
self.push_region();
⋮----
self.fill_write(MAX_PERMITTED_DATA_INCREASE, 0)
⋮----
.get_data()
.len()
.saturating_add(MAX_PERMITTED_DATA_INCREASE)
⋮----
account.get_data().len()
⋮----
let region = self.regions.last_mut().unwrap();
modify_memory_region_of_account(account, region);
⋮----
let new_region = create_memory_region_of_account(account, self.vaddr)?;
⋮----
self.regions.push(new_region);
⋮----
self.fill_write(align_offset, 0)
⋮----
self.fill_write(BPF_ALIGN_OF_U128, 0)
⋮----
self.region_start += BPF_ALIGN_OF_U128.saturating_sub(align_offset);
⋮----
fn push_region(&mut self) {
let range = self.region_start..self.buffer.len();
self.regions.push(MemoryRegion::new_writable(
self.buffer.as_slice_mut().get_mut(range.clone()).unwrap(),
⋮----
self.vaddr += range.len() as u64;
⋮----
fn finish(mut self) -> (AlignedMemory<HOST_ALIGN>, Vec<MemoryRegion>) {
⋮----
debug_assert_eq!(self.region_start, self.buffer.len());
⋮----
fn debug_assert_alignment<T>(&self) {
debug_assert!(
⋮----
pub fn serialize_parameters(
⋮----
let num_ix_accounts = instruction_context.get_number_of_instruction_accounts();
⋮----
return Err(InstructionError::MaxAccountsExceeded);
⋮----
let program_id = *instruction_context.get_program_key()?;
⋮----
instruction_context.get_program_owner()? == bpf_loader_deprecated::id();
let accounts = (0..instruction_context.get_number_of_instruction_accounts())
.map(|instruction_account_index| {
⋮----
.is_instruction_account_duplicate(instruction_account_index)
.unwrap()
⋮----
.try_borrow_instruction_account(instruction_account_index)
.unwrap();
⋮----
serialize_parameters_unaligned(
⋮----
instruction_context.get_instruction_data(),
⋮----
serialize_parameters_aligned(
⋮----
pub fn deserialize_parameters(
⋮----
let account_lengths = accounts_metadata.iter().map(|a| a.original_data_len);
⋮----
deserialize_parameters_unaligned(
⋮----
deserialize_parameters_aligned(
⋮----
fn serialize_parameters_unaligned(
⋮----
size += account.get_data().len();
⋮----
+ instruction_data.len()
⋮----
let mut accounts_metadata: Vec<SerializedAccountMetadata> = Vec::with_capacity(accounts.len());
s.write::<u64>((accounts.len() as u64).to_le());
⋮----
accounts_metadata.push(accounts_metadata.get(position as usize).unwrap().clone());
s.write(position as u8);
⋮----
s.write::<u8>(account.is_signer() as u8);
s.write::<u8>(account.is_writable() as u8);
let vm_key_addr = s.write_all(account.get_key().as_ref());
let vm_lamports_addr = s.write::<u64>(account.get_lamports().to_le());
s.write::<u64>((account.get_data().len() as u64).to_le());
let vm_data_addr = s.write_account(&mut account)?;
let vm_owner_addr = s.write_all(account.get_owner().as_ref());
⋮----
s.write::<u8>(account.is_executable() as u8);
⋮----
account.get_rent_epoch()
⋮----
s.write::<u64>(rent_epoch.to_le());
accounts_metadata.push(SerializedAccountMetadata {
original_data_len: account.get_data().len(),
⋮----
s.write::<u64>((instruction_data.len() as u64).to_le());
let instruction_data_offset = s.write_all(instruction_data);
s.write_all(program_id.as_ref());
let (mem, regions) = s.finish();
Ok((
⋮----
fn deserialize_parameters_unaligned<I: IntoIterator<Item = usize>>(
⋮----
.get_number_of_instruction_accounts())
.zip(account_lengths.into_iter())
⋮----
instruction_context.is_instruction_account_duplicate(instruction_account_index)?;
⋮----
if duplicate.is_none() {
⋮----
instruction_context.try_borrow_instruction_account(instruction_account_index)?;
⋮----
.get(start..start.saturating_add(8))
.map(<[u8; 8]>::try_from)
.and_then(Result::ok)
.map(u64::from_le_bytes)
.ok_or(InstructionError::InvalidArgument)?;
if borrowed_account.get_lamports() != lamports {
borrowed_account.set_lamports(lamports)?;
⋮----
.get(start..start + pre_len)
⋮----
match borrowed_account.can_data_be_resized(pre_len) {
Ok(()) => borrowed_account.set_data_from_slice(data)?,
Err(err) if borrowed_account.get_data() != data => return Err(err),
⋮----
} else if !account_data_direct_mapping && borrowed_account.can_data_be_changed().is_ok()
⋮----
borrowed_account.set_data_from_slice(data)?;
} else if borrowed_account.get_data().len() != pre_len {
borrowed_account.set_data_length(pre_len)?;
⋮----
Ok(())
⋮----
fn serialize_parameters_aligned(
⋮----
let mut accounts_metadata = Vec::with_capacity(accounts.len());
⋮----
let data_len = account.get_data().len();
⋮----
+ (data_len as *const u8).align_offset(BPF_ALIGN_OF_U128);
⋮----
s.write::<u8>(borrowed_account.is_signer() as u8);
s.write::<u8>(borrowed_account.is_writable() as u8);
⋮----
s.write::<u8>(borrowed_account.is_executable() as u8);
s.write_all(&[0u8, 0, 0, 0]);
let vm_key_addr = s.write_all(borrowed_account.get_key().as_ref());
let vm_owner_addr = s.write_all(borrowed_account.get_owner().as_ref());
let vm_lamports_addr = s.write::<u64>(borrowed_account.get_lamports().to_le());
s.write::<u64>((borrowed_account.get_data().len() as u64).to_le());
let vm_data_addr = s.write_account(&mut borrowed_account)?;
⋮----
borrowed_account.get_rent_epoch()
⋮----
original_data_len: borrowed_account.get_data().len(),
⋮----
s.write_all(&[0u8, 0, 0, 0, 0, 0, 0]);
⋮----
fn deserialize_parameters_aligned<I: IntoIterator<Item = usize>>(
⋮----
if duplicate.is_some() {
⋮----
.get(start..start + size_of::<Pubkey>())
⋮----
.ok_or(InstructionError::InvalidArgument)? as usize;
⋮----
if post_len.saturating_sub(pre_len) > MAX_PERMITTED_DATA_INCREASE
⋮----
return Err(InstructionError::InvalidRealloc);
⋮----
.get(start..start + post_len)
⋮----
match borrowed_account.can_data_be_resized(post_len) {
⋮----
} else if borrowed_account.get_data().len() != post_len {
borrowed_account.set_data_length(post_len)?;
⋮----
let alignment_offset = (pre_len as *const u8).align_offset(BPF_ALIGN_OF_U128);
⋮----
.saturating_add(alignment_offset)
⋮----
if borrowed_account.get_owner().to_bytes() != owner {
borrowed_account.set_owner(owner)?;
⋮----
mod tests {
⋮----
fn deduplicated_instruction_accounts(
⋮----
.iter()
.enumerate()
.map(|(index_in_instruction, index_in_transaction)| {
⋮----
is_writable(index_in_instruction),
⋮----
.collect()
⋮----
fn test_serialize_parameters_with_many_accounts() {
struct TestCase {
⋮----
expected_err: Some(InstructionError::MaxAccountsExceeded),
⋮----
let mut transaction_accounts = vec![(
⋮----
transaction_accounts.push((
⋮----
data: vec![],
⋮----
(0..num_ix_accounts as u16).collect();
⋮----
deduplicated_instruction_accounts(&transaction_accounts_indexes, |_| false);
⋮----
instruction_accounts.push(instruction_accounts.last().cloned().unwrap());
⋮----
let instruction_data = vec![];
with_mock_invoke_context!(
⋮----
if instruction_accounts.len() > MAX_ACCOUNTS_PER_INSTRUCTION {
// Special case implementation of configure_next_instruction_for_tests()
// which avoids the overflow when constructing the dedup_map
// by simply not filling it.
let dedup_map = vec![u16::MAX; MAX_ACCOUNTS_PER_TRANSACTION];
⋮----
.configure_next_instruction(
⋮----
Cow::Owned(instruction_data.clone()),
⋮----
.configure_next_instruction_for_tests(
⋮----
instruction_data.clone(),
⋮----
invoke_context.push().unwrap();
⋮----
.get_current_instruction_context()
⋮----
let serialization_result = serialize_parameters(
⋮----
false, // account_data_direct_mapping
true,  // mask_out_rent_epoch_in_vm_serialization
⋮----
assert_eq!(
⋮----
if expected_err.is_some() {
⋮----
serialization_result.unwrap();
let mut serialized_regions = concat_regions(&regions);
⋮----
deserialize(
⋮----
serialized.as_slice_mut()
⋮----
serialized_regions.as_slice_mut()
⋮----
.first_mut()
.unwrap() as *mut u8,
⋮----
assert_eq!(de_program_id, &program_id);
assert_eq!(de_instruction_data, &instruction_data);
⋮----
.find_index_of_account(account_info.key)
⋮----
.accounts()
.try_borrow(index_in_transaction)
⋮----
assert_eq!(account.lamports(), account_info.lamports());
assert_eq!(account.data(), &account_info.data.borrow()[..]);
assert_eq!(account.owner(), account_info.owner);
assert_eq!(account.executable(), account_info.executable);
⋮----
// Using the sdk entrypoint, the rent-epoch is skipped
assert_eq!(0, account_info._unused);
⋮----
fn test_serialize_parameters() {
⋮----
let transaction_accounts = vec![
⋮----
deduplicated_instruction_accounts(&[1, 1, 2, 3, 4, 4, 5, 6], |index| index >= 4);
let instruction_data = vec![1u8, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11];
let original_accounts = transaction_accounts.clone();
with_mock_invoke_context!(invoke_context, transaction_context, transaction_accounts);
⋮----
instruction_accounts.clone(),
⋮----
// check serialize_parameters_aligned
⋮----
serialize_parameters(
⋮----
assert_eq!(serialized.as_slice(), serialized_regions.as_slice());
⋮----
assert_eq!(&program_id, de_program_id);
assert_eq!(instruction_data, de_instruction_data);
⋮----
deserialize_parameters(
⋮----
serialized.as_slice(),
⋮----
original_accounts.iter().enumerate()
⋮----
.try_borrow(index_in_transaction as IndexOfAccount)
⋮----
assert_eq!(&*account, original_account);
⋮----
// check serialize_parameters_unaligned
⋮----
deserialize_unaligned(
⋮----
assert_eq!(u64::MAX, account_info._unused);
⋮----
fn test_serialize_parameters_mask_out_rent_epoch_in_vm_serialization() {
⋮----
.configure_next_instruction_for_tests(0, instruction_accounts.clone(), vec![])
⋮----
deserialize(serialized_regions.as_slice_mut().first_mut().unwrap() as *mut u8)
⋮----
// Using program-entrypoint, the rent-epoch will always be 0
⋮----
.configure_next_instruction_for_tests(7, instruction_accounts, vec![])
⋮----
serialized_regions.as_slice_mut().first_mut().unwrap() as *mut u8
⋮----
account.rent_epoch()
⋮----
assert_eq!(expected_rent_epoch, account_info._unused);
⋮----
// the old bpf_loader in-program deserializer bpf_loader::id()
⋮----
unsafe fn deserialize_unaligned<'a>(
⋮----
// this boring boilerplate struct is needed until inline const...
struct Ptr<T>(std::marker::PhantomData<T>);
⋮----
fn read_possibly_unaligned(input: *mut u8, offset: usize) -> T {
⋮----
let src = input.add(offset) as *const T;
⋮----
src.read_unaligned()
⋮----
src.read()
⋮----
// rustc inserts debug_assert! for misaligned pointer dereferences when
// deserializing, starting from [1]. so, use std::mem::transmute as the last resort
// while preventing clippy from complaining to suggest not to use it.
// [1]: https://github.com/rust-lang/rust/commit/22a7a19f9333bc1fcba97ce444a3515cb5fb33e6
// as for the ub nature of the misaligned pointer dereference, this is
// acceptable in this code, given that this is cfg(test) and it's cared only with
⋮----
fn ref_possibly_unaligned<'a>(input: *mut u8, offset: usize) -> &'a T {
⋮----
transmute(input.add(offset) as *const T)
⋮----
fn mut_possibly_unaligned<'a>(input: *mut u8, offset: usize) -> &'a mut T {
⋮----
transmute(input.add(offset) as *mut T)
⋮----
from_raw_parts_mut(input.add(offset), data_len)
⋮----
accounts.push(AccountInfo {
⋮----
accounts.push(accounts.get(dup_info as usize).unwrap().clone());
⋮----
let instruction_data = unsafe { from_raw_parts(input.add(offset), instruction_data_len) };
⋮----
fn concat_regions(regions: &[MemoryRegion]) -> AlignedMemory<HOST_ALIGN> {
let last_region = regions.last().unwrap();
⋮----
mem.as_slice_mut()[(region.vm_addr - MM_INPUT_START) as usize..][..region.len as usize]
.copy_from_slice(host_slice)
⋮----
fn test_access_violation_handler() {
⋮----
vec![
⋮----
deduplicated_instruction_accounts(&transaction_accounts_indexes, |index| index > 0);
⋮----
.configure_next_instruction_for_tests(6, instruction_accounts, vec![])
⋮----
transaction_context.push().unwrap();
⋮----
.map(|(index_in_instruction, account_start_offset)| {
create_memory_region_of_account(
⋮----
.try_borrow_instruction_account(index_in_instruction as IndexOfAccount)
.unwrap(),
⋮----
transaction_context.access_violation_handler(true, true),
⋮----
.unwrap_err();
⋮----
assert!(transaction_context
⋮----
assert!(!transaction_context
⋮----
assert!(
⋮----
.try_borrow_instruction_account(index_in_instruction)
⋮----
.set_data_from_slice(&vec![0u8; MAX_PERMITTED_DATA_LENGTH as usize])

================
File: program-runtime/src/stable_log.rs
================
pub fn program_invoke(
⋮----
ic_logger_msg!(
⋮----
pub fn program_log(log_collector: &Option<Rc<RefCell<LogCollector>>>, message: &str) {
ic_logger_msg!(log_collector, "Program log: {}", message);
⋮----
pub fn program_data(log_collector: &Option<Rc<RefCell<LogCollector>>>, data: &[&[u8]]) {
⋮----
pub fn program_return(
⋮----
pub fn program_success(log_collector: &Option<Rc<RefCell<LogCollector>>>, program_id: &Pubkey) {
ic_logger_msg!(log_collector, "Program {} success", program_id);
⋮----
pub fn program_failure<E: std::fmt::Display>(
⋮----
ic_logger_msg!(log_collector, "Program {} failed: {}", program_id, err);

================
File: program-runtime/src/sysvar_cache.rs
================
fn example() -> Self {
⋮----
pub struct SysvarCache {
⋮----
impl SysvarCache {
⋮----
pub fn set_sysvar_for_tests<T: SysvarSerialize + SysvarId>(&mut self, sysvar: &T) {
let data = bincode::serialize(sysvar).expect("Failed to serialize sysvar.");
⋮----
self.clock = Some(data);
⋮----
self.epoch_rewards = Some(data);
⋮----
self.epoch_schedule = Some(data);
⋮----
bincode::deserialize(&data).expect("Failed to deserialize Fees sysvar.");
self.fees = Some(fees);
⋮----
self.last_restart_slot = Some(data);
⋮----
.expect("Failed to deserialize RecentBlockhashes sysvar.");
self.recent_blockhashes = Some(recent_blockhashes);
⋮----
self.rent = Some(data);
⋮----
bincode::deserialize(&data).expect("Failed to deserialize SlotHashes sysvar.");
self.slot_hashes = Some(data);
self.slot_hashes_obj = Some(Arc::new(slot_hashes));
⋮----
.expect("Failed to deserialize StakeHistory sysvar.");
self.stake_history = Some(data);
self.stake_history_obj = Some(Arc::new(stake_history));
⋮----
_ => panic!("Unrecognized Sysvar ID: {sysvar_id}"),
⋮----
pub fn sysvar_id_to_buffer(&self, sysvar_id: &Pubkey) -> &Option<Vec<u8>> {
⋮----
fn get_sysvar_obj<T: DeserializeOwned>(
⋮----
if let Some(sysvar_buf) = self.sysvar_id_to_buffer(sysvar_id) {
⋮----
.map(Arc::new)
.map_err(|_| InstructionError::UnsupportedSysvar)
⋮----
Err(InstructionError::UnsupportedSysvar)
⋮----
pub fn get_clock(&self) -> Result<Arc<Clock>, InstructionError> {
self.get_sysvar_obj(&Clock::id())
⋮----
pub fn get_epoch_schedule(&self) -> Result<Arc<EpochSchedule>, InstructionError> {
self.get_sysvar_obj(&EpochSchedule::id())
⋮----
pub fn get_epoch_rewards(&self) -> Result<Arc<EpochRewards>, InstructionError> {
self.get_sysvar_obj(&EpochRewards::id())
⋮----
pub fn get_rent(&self) -> Result<Arc<Rent>, InstructionError> {
self.get_sysvar_obj(&Rent::id())
⋮----
pub fn get_last_restart_slot(&self) -> Result<Arc<LastRestartSlot>, InstructionError> {
self.get_sysvar_obj(&LastRestartSlot::id())
⋮----
pub fn get_stake_history(&self) -> Result<Arc<StakeHistory>, InstructionError> {
⋮----
.clone()
.ok_or(InstructionError::UnsupportedSysvar)
⋮----
pub fn get_slot_hashes(&self) -> Result<Arc<SlotHashes>, InstructionError> {
⋮----
pub fn get_fees(&self) -> Result<Arc<Fees>, InstructionError> {
⋮----
pub fn get_recent_blockhashes(&self) -> Result<Arc<RecentBlockhashes>, InstructionError> {
⋮----
pub fn fill_missing_entries<F: FnMut(&Pubkey, &mut dyn FnMut(&[u8]))>(
⋮----
if self.clock.is_none() {
get_account_data(&Clock::id(), &mut |data: &[u8]| {
if bincode::deserialize::<Clock>(data).is_ok() {
self.clock = Some(data.to_vec());
⋮----
if self.epoch_schedule.is_none() {
get_account_data(&EpochSchedule::id(), &mut |data: &[u8]| {
if bincode::deserialize::<EpochSchedule>(data).is_ok() {
self.epoch_schedule = Some(data.to_vec());
⋮----
if self.epoch_rewards.is_none() {
get_account_data(&EpochRewards::id(), &mut |data: &[u8]| {
if bincode::deserialize::<EpochRewards>(data).is_ok() {
self.epoch_rewards = Some(data.to_vec());
⋮----
if self.rent.is_none() {
get_account_data(&Rent::id(), &mut |data: &[u8]| {
if bincode::deserialize::<Rent>(data).is_ok() {
self.rent = Some(data.to_vec());
⋮----
if self.slot_hashes.is_none() {
get_account_data(&SlotHashes::id(), &mut |data: &[u8]| {
⋮----
self.slot_hashes = Some(data.to_vec());
self.slot_hashes_obj = Some(Arc::new(obj));
⋮----
if self.stake_history.is_none() {
get_account_data(&StakeHistory::id(), &mut |data: &[u8]| {
⋮----
self.stake_history = Some(data.to_vec());
self.stake_history_obj = Some(Arc::new(obj));
⋮----
if self.last_restart_slot.is_none() {
get_account_data(&LastRestartSlot::id(), &mut |data: &[u8]| {
if bincode::deserialize::<LastRestartSlot>(data).is_ok() {
self.last_restart_slot = Some(data.to_vec());
⋮----
if self.fees.is_none() {
get_account_data(&Fees::id(), &mut |data: &[u8]| {
⋮----
if self.recent_blockhashes.is_none() {
get_account_data(&RecentBlockhashes::id(), &mut |data: &[u8]| {
⋮----
pub fn reset(&mut self) {
⋮----
pub mod get_sysvar_with_account_check {
⋮----
fn check_sysvar_account<S: SysvarId>(
⋮----
instruction_context.get_key_of_instruction_account(instruction_account_index)?,
⋮----
return Err(InstructionError::InvalidArgument);
⋮----
Ok(())
⋮----
pub fn clock(
⋮----
invoke_context.get_sysvar_cache().get_clock()
⋮----
pub fn rent(
⋮----
invoke_context.get_sysvar_cache().get_rent()
⋮----
pub fn slot_hashes(
⋮----
invoke_context.get_sysvar_cache().get_slot_hashes()
⋮----
pub fn recent_blockhashes(
⋮----
invoke_context.get_sysvar_cache().get_recent_blockhashes()
⋮----
pub fn stake_history(
⋮----
invoke_context.get_sysvar_cache().get_stake_history()
⋮----
pub fn last_restart_slot(
⋮----
invoke_context.get_sysvar_cache().get_last_restart_slot()
⋮----
mod tests {
⋮----
fn test_sysvar_cache_preserves_bytes<T: SysvarSerialize>(_: T) {
⋮----
let size = T::size_of().saturating_mul(2);
let in_buf = vec![0; size];
⋮----
sysvar_cache.fill_missing_entries(|pubkey, callback| {
⋮----
callback(&in_buf)
⋮----
let out_buf = sysvar_cache.sysvar_id_to_buffer(&id).clone().unwrap();
assert_eq!(out_buf, in_buf);

================
File: program-runtime/Cargo.toml
================
[package]
name = "solana-program-runtime"
description = "Solana program runtime"
documentation = "https://docs.rs/solana-program-runtime"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_program_runtime"

[features]
agave-unstable-api = []
dev-context-only-utils = []
dummy-for-ci-check = ["metrics"]
frozen-abi = ["dep:solana-frozen-abi", "dep:solana-frozen-abi-macro"]
metrics = []
shuttle-test = ["solana-sbpf/shuttle-test", "solana-svm-type-overrides/shuttle-test"]

[dependencies]
base64 = { workspace = true }
bincode = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
percentage = { workspace = true }
rand = { workspace = true }
serde = { workspace = true }
solana-account = { workspace = true, features = ["bincode"] }
solana-account-info = { workspace = true }
solana-clock = { workspace = true }
solana-epoch-rewards = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-fee-structure = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-last-restart-slot = { workspace = true }
solana-loader-v3-interface = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-sbpf = { workspace = true, features = ["jit"] }
solana-sdk-ids = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-stable-layout = { workspace = true }
solana-stake-interface = { workspace = true, features = ["bincode", "sysvar"] }
solana-svm-callback = { workspace = true }
solana-svm-feature-set = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-measure = { workspace = true }
solana-svm-timings = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-svm-type-overrides = { workspace = true }
solana-system-interface = { workspace = true }
solana-sysvar = { workspace = true, features = ["bincode"] }
solana-sysvar-id = { workspace = true }
solana-transaction-context = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
solana-account-info = { workspace = true }
solana-instruction = { workspace = true, features = ["bincode"] }
solana-instruction-error = { workspace = true, features = ["serde"] }
solana-keypair = { workspace = true }
solana-program-runtime = { path = ".", features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-signer = { workspace = true }
solana-transaction = { workspace = true, features = ["dev-context-only-utils"] }
solana-transaction-context = { workspace = true, features = [
    "dev-context-only-utils",
] }
test-case = { workspace = true }

[lints]
workspace = true

================
File: program-test/src/lib.rs
================
pub enum ProgramTestError {
⋮----
thread_local! {
⋮----
fn set_invoke_context(new: &mut InvokeContext) {
INVOKE_CONTEXT.with(|invoke_context| unsafe {
invoke_context.replace(Some(transmute::<&mut InvokeContext, usize>(new)))
⋮----
fn get_invoke_context<'a, 'b>() -> &'a mut InvokeContext<'b, 'b> {
let ptr = INVOKE_CONTEXT.with(|invoke_context| match *invoke_context.borrow() {
⋮----
None => panic!("Invoke context not set!"),
⋮----
pub fn invoke_builtin_function(
⋮----
set_invoke_context(invoke_context);
⋮----
let instruction_context = transaction_context.get_current_instruction_context()?;
let instruction_account_indices = 0..instruction_context.get_number_of_instruction_accounts();
// mock builtin program must consume units
invoke_context.consume_checked(1)?;
let log_collector = invoke_context.get_log_collector();
let program_id = instruction_context.get_program_key()?;
⋮----
invoke_context.get_stack_height(),
⋮----
// Copy indices_in_instruction into a HashSet to ensure there are no duplicates
let deduplicated_indices: HashSet<IndexOfAccount> = instruction_account_indices.collect();
// Serialize entrypoint parameters with SBF ABI
⋮----
.get_feature_set()
⋮----
serialize_parameters(
⋮----
false, // There is no VM so stricter_abi_and_runtime_constraints can not be implemented here
false, // There is no VM so account_data_direct_mapping can not be implemented here
⋮----
// Deserialize data back into instruction params
⋮----
unsafe { deserialize(&mut parameter_bytes.as_slice_mut()[0] as *mut u8) };
// Execute the program
match std::panic::catch_unwind(AssertUnwindSafe(|| {
builtin_function(program_id, &account_infos, input)
⋮----
program_result.map_err(|program_error| {
⋮----
Err(err)?;
⋮----
// Lookup table for AccountInfo
let account_info_map: HashMap<_, _> = account_infos.into_iter().map(|a| (a.key, a)).collect();
// Re-fetch the instruction context. The previous reference may have been
// invalidated due to the `set_invoke_context` in a CPI.
⋮----
// Commit AccountInfo changes back into KeyedAccounts
for i in deduplicated_indices.into_iter() {
let mut borrowed_account = instruction_context.try_borrow_instruction_account(i)?;
if borrowed_account.is_writable() {
if let Some(account_info) = account_info_map.get(borrowed_account.get_key()) {
if borrowed_account.get_lamports() != account_info.lamports() {
borrowed_account.set_lamports(account_info.lamports())?;
⋮----
.can_data_be_resized(account_info.data_len())
.is_ok()
⋮----
borrowed_account.set_data_from_slice(&account_info.data.borrow())?;
⋮----
if borrowed_account.get_owner() != account_info.owner {
borrowed_account.set_owner(account_info.owner.as_ref())?;
⋮----
Ok(0)
⋮----
/// Converts a `solana-program`-style entrypoint into the runtime's entrypoint style, for
#[macro_export]
macro_rules! processor {
⋮----
fn get_sysvar<T: Default + SysvarSerialize + Sized + serde::de::DeserializeOwned + Clone>(
⋮----
let invoke_context = get_invoke_context();
⋮----
.consume_checked(invoke_context.get_execution_cost().sysvar_base_cost + T::size_of() as u64)
.is_err()
⋮----
panic!("Exceeded compute budget");
⋮----
struct SyscallStubs {}
⋮----
fn sol_log(&self, message: &str) {
⋮----
ic_msg!(invoke_context, "Program log: {}", message);
⋮----
fn sol_invoke_signed(
⋮----
.get_current_instruction_context()
.unwrap();
let caller = instruction_context.get_program_key().unwrap();
⋮----
.iter()
.map(|seeds| Pubkey::create_program_address(seeds, caller).unwrap())
⋮----
.prepare_next_instruction(instruction.clone(), &signers)
⋮----
let next_instruction_context = transaction_context.get_next_instruction_context().unwrap();
let next_instruction_accounts = next_instruction_context.instruction_accounts();
let mut account_indices = Vec::with_capacity(next_instruction_accounts.len());
for instruction_account in next_instruction_accounts.iter() {
⋮----
.get_key_of_account_at_index(instruction_account.index_in_transaction)
⋮----
.position(|account_info| account_info.unsigned_key() == account_key)
.ok_or(InstructionError::MissingAccount)
⋮----
.get_index_of_account_in_instruction(instruction_account.index_in_transaction)
⋮----
.try_borrow_instruction_account(index_in_caller)
⋮----
.set_lamports(account_info.lamports())
⋮----
let account_info_data = account_info.try_borrow_data().unwrap();
match borrowed_account.can_data_be_resized(account_info_data.len()) {
⋮----
.set_data_from_slice(&account_info_data)
.unwrap(),
Err(err) if borrowed_account.get_data() != *account_info_data => {
panic!("{err:?}");
⋮----
.set_owner(account_info.owner.as_ref())
⋮----
if instruction_account.is_writable() {
⋮----
.push((instruction_account.index_in_transaction, account_info_index));
⋮----
.process_instruction(&mut compute_units_consumed, &mut ExecuteTimings::default())
.map_err(|err| ProgramError::try_from(err).unwrap_or_else(|err| panic!("{}", err)))?;
⋮----
for (index_in_transaction, account_info_index) in account_indices.into_iter() {
⋮----
.get_index_of_account_in_instruction(index_in_transaction)
⋮----
**account_info.try_borrow_mut_lamports().unwrap() = borrowed_account.get_lamports();
if account_info.owner != borrowed_account.get_owner() {
⋮----
*account_info_mut = *borrowed_account.get_owner();
⋮----
let new_data = borrowed_account.get_data();
let new_len = new_data.len();
if account_info.data_len() != new_len {
account_info.resize(new_len)?;
⋮----
let mut data = account_info.try_borrow_mut_data()?;
data.clone_from_slice(new_data);
⋮----
Ok(())
⋮----
fn sol_get_clock_sysvar(&self, var_addr: *mut u8) -> u64 {
get_sysvar(
get_invoke_context().get_sysvar_cache().get_clock(),
⋮----
fn sol_get_epoch_schedule_sysvar(&self, var_addr: *mut u8) -> u64 {
⋮----
get_invoke_context().get_sysvar_cache().get_epoch_schedule(),
⋮----
fn sol_get_epoch_rewards_sysvar(&self, var_addr: *mut u8) -> u64 {
⋮----
get_invoke_context().get_sysvar_cache().get_epoch_rewards(),
⋮----
fn sol_get_fees_sysvar(&self, var_addr: *mut u8) -> u64 {
get_sysvar(get_invoke_context().get_sysvar_cache().get_fees(), var_addr)
⋮----
fn sol_get_rent_sysvar(&self, var_addr: *mut u8) -> u64 {
get_sysvar(get_invoke_context().get_sysvar_cache().get_rent(), var_addr)
⋮----
fn sol_get_last_restart_slot(&self, var_addr: *mut u8) -> u64 {
⋮----
get_invoke_context()
.get_sysvar_cache()
.get_last_restart_slot(),
⋮----
fn sol_get_return_data(&self) -> Option<(Pubkey, Vec<u8>)> {
let (program_id, data) = get_invoke_context().transaction_context.get_return_data();
Some((*program_id, data.to_vec()))
⋮----
fn sol_set_return_data(&self, data: &[u8]) {
⋮----
let caller = *instruction_context.get_program_key().unwrap();
⋮----
.set_return_data(caller, data.to_vec())
⋮----
fn sol_get_stack_height(&self) -> u64 {
⋮----
invoke_context.get_stack_height().try_into().unwrap()
⋮----
pub fn find_file(filename: &str) -> Option<PathBuf> {
for dir in default_shared_object_dirs() {
let candidate = dir.join(filename);
if candidate.exists() {
return Some(candidate);
⋮----
fn default_shared_object_dirs() -> Vec<PathBuf> {
let mut search_path = vec![];
⋮----
search_path.push(PathBuf::from(bpf_out_dir));
⋮----
search_path.push(PathBuf::from("tests/fixtures"));
⋮----
search_path.push(dir);
⋮----
trace!("SBF .so search path: {search_path:?}");
⋮----
pub fn read_file<P: AsRef<Path>>(path: P) -> Vec<u8> {
let path = path.as_ref();
⋮----
.unwrap_or_else(|err| panic!("Failed to open \"{}\": {}", path.display(), err));
⋮----
file.read_to_end(&mut file_data)
.unwrap_or_else(|err| panic!("Failed to read \"{}\": {}", path.display(), err));
⋮----
pub struct ProgramTest {
⋮----
impl Default for ProgramTest {
/// Initialize a new ProgramTest
    ///
⋮----
///
    /// If the `BPF_OUT_DIR` environment variable is defined, BPF programs will be preferred over
⋮----
/// If the `BPF_OUT_DIR` environment variable is defined, BPF programs will be preferred over
    /// over a native instruction processor.  The `ProgramTest::prefer_bpf()` method may be
⋮----
/// over a native instruction processor.  The `ProgramTest::prefer_bpf()` method may be
    /// used to override this preference at runtime.  `cargo test-bpf` will set `BPF_OUT_DIR`
⋮----
/// used to override this preference at runtime.  `cargo test-bpf` will set `BPF_OUT_DIR`
    /// automatically.
⋮----
/// automatically.
    ///
⋮----
///
    /// SBF program shared objects and account data files are searched for in
⋮----
/// SBF program shared objects and account data files are searched for in
    /// * the value of the `BPF_OUT_DIR` environment variable
⋮----
/// * the value of the `BPF_OUT_DIR` environment variable
    /// * the `tests/fixtures` sub-directory
⋮----
/// * the `tests/fixtures` sub-directory
    /// * the current working directory
⋮----
/// * the current working directory
    ///
⋮----
///
    fn default() -> Self {
⋮----
fn default() -> Self {
⋮----
std::env::var("BPF_OUT_DIR").is_ok() || std::env::var("SBF_OUT_DIR").is_ok();
⋮----
accounts: vec![],
genesis_accounts: vec![],
builtin_programs: vec![],
⋮----
impl ProgramTest {
/// Create a `ProgramTest`.
    ///
⋮----
///
    /// This is a wrapper around [`default`] and [`add_program`]. See their documentation for more
⋮----
/// This is a wrapper around [`default`] and [`add_program`]. See their documentation for more
    /// details.
⋮----
/// details.
    ///
⋮----
///
    /// [`default`]: #method.default
⋮----
/// [`default`]: #method.default
    /// [`add_program`]: #method.add_program
⋮----
/// [`add_program`]: #method.add_program
    pub fn new(
⋮----
pub fn new(
⋮----
me.add_program(program_name, program_id, builtin_function);
⋮----
pub fn prefer_bpf(&mut self, prefer_bpf: bool) {
⋮----
pub fn set_compute_max_units(&mut self, compute_max_units: u64) {
debug_assert!(
⋮----
self.compute_max_units = Some(compute_max_units);
⋮----
pub fn set_transaction_account_lock_limit(&mut self, transaction_account_lock_limit: usize) {
self.transaction_account_lock_limit = Some(transaction_account_lock_limit);
⋮----
pub fn add_genesis_account(&mut self, address: Pubkey, account: Account) {
⋮----
.push((address, AccountSharedData::from(account)));
⋮----
pub fn add_account(&mut self, address: Pubkey, account: Account) {
⋮----
pub fn add_account_with_file_data(
⋮----
self.add_account(
⋮----
data: read_file(find_file(filename).unwrap_or_else(|| {
panic!("Unable to locate {filename}");
⋮----
pub fn add_account_with_base64_data(
⋮----
.decode(data_base64)
.unwrap_or_else(|err| panic!("Failed to base64 decode: {err}")),
⋮----
pub fn add_sysvar_account<S: SysvarSerialize>(&mut self, address: Pubkey, sysvar: &S) {
let account = create_account_shared_data_for_test(sysvar);
self.add_account(address, account.into());
⋮----
pub fn add_upgradeable_program_to_genesis(
⋮----
let program_file = find_file(&format!("{program_name}.so")).unwrap_or_else(|| {
panic!("Program file data not available for {program_name} ({program_id})")
⋮----
let elf = read_file(program_file);
⋮----
self.add_genesis_account(address, account);
⋮----
/// Add a SBF program to the test environment.
    ///
⋮----
///
    /// `program_name` will also be used to locate the SBF shared object in the current or fixtures
⋮----
/// `program_name` will also be used to locate the SBF shared object in the current or fixtures
    /// directory.
⋮----
/// directory.
    ///
⋮----
///
    /// If `builtin_function` is provided, the natively built-program may be used instead of the
⋮----
/// If `builtin_function` is provided, the natively built-program may be used instead of the
    /// SBF shared object depending on the `BPF_OUT_DIR` environment variable.
⋮----
/// SBF shared object depending on the `BPF_OUT_DIR` environment variable.
    pub fn add_program(
⋮----
pub fn add_program(
⋮----
let data = read_file(&program_file);
info!(
⋮----
this.add_account(
⋮----
lamports: Rent::default().minimum_balance(data.len()).max(1),
⋮----
let valid_program_names = default_shared_object_dirs()
⋮----
.filter_map(|dir| dir.read_dir().ok())
.flat_map(|read_dir| {
read_dir.filter_map(|entry| {
let path = entry.ok()?.path();
if !path.is_file() {
⋮----
match path.extension()?.to_str()? {
"so" => Some(path.file_stem()?.to_os_string()),
⋮----
if valid_program_names.is_empty() {
warn!("No SBF shared objects found.");
⋮----
warn!(
⋮----
warn!(" - {}", name.to_str().unwrap());
⋮----
let program_file = find_file(&format!("{program_name}.so"));
⋮----
(true, Some(file), _) => add_bpf(self, file),
⋮----
self.add_builtin_program(program_name, program_id, builtin_function)
⋮----
warn_invalid_program_name();
panic!("Program file data not available for {program_name} ({program_id})");
⋮----
panic!("Program processor not available for {program_name} ({program_id})");
⋮----
pub fn add_builtin_program(
⋮----
info!("\"{program_name}\" builtin program");
self.builtin_programs.push((
⋮----
ProgramCacheEntry::new_builtin(0, program_name.len(), builtin_function),
⋮----
/// Deactivate a runtime feature.
    ///
⋮----
///
    /// Note that all features are activated by default.
⋮----
/// Note that all features are activated by default.
    pub fn deactivate_feature(&mut self, feature_id: Pubkey) {
⋮----
pub fn deactivate_feature(&mut self, feature_id: Pubkey) {
self.deactivate_feature_set.insert(feature_id);
⋮----
fn setup_bank(
⋮----
use std::sync::Once;
⋮----
ONCE.call_once(|| {
⋮----
// Initialize with a non-zero fee
⋮----
rent.minimum_balance(VoteStateV4::size_of()) + 1_000_000 * LAMPORTS_PER_SOL;
⋮----
let mut genesis_config = create_genesis_config_with_leader_ex(
⋮----
&mint_keypair.pubkey(),
⋮----
&voting_keypair.pubkey(),
⋮----
rent.clone(),
⋮----
// Remove features tagged to deactivate
⋮----
if FEATURE_NAMES.contains_key(deactivate_feature_pk) {
match genesis_config.accounts.remove(deactivate_feature_pk) {
Some(_) => debug!("Feature for {deactivate_feature_pk:?} deactivated"),
None => warn!(
⋮----
debug!("Payer address: {}", mint_keypair.pubkey());
debug!("Genesis config: {genesis_config}");
⋮----
compute_budget: self.compute_max_units.map(|max_units| ComputeBudget {
⋮----
.contains_key(&raise_cpi_nesting_limit_to_8::id()),
⋮----
.contains_key(&increase_cpi_account_info_limit::id()),
⋮----
// Add commonly-used SPL programs as a convenience to the user
for (program_id, account) in programs::spl_programs(&rent).iter() {
bank.store_account(program_id, account);
⋮----
// Add migrated Core BPF programs.
⋮----
genesis_config.accounts.contains_key(feature_id)
⋮----
// User-supplied additional builtins
⋮----
for (program_id, name, builtin) in builtin_programs.into_iter() {
bank.add_builtin(program_id, name, builtin);
⋮----
for (address, account) in self.accounts.iter() {
if bank.get_account(address).is_some() {
info!("Overriding account at {address}");
⋮----
bank.store_account(address, account);
⋮----
bank.set_capitalization_for_tests(bank.calculate_capitalization_for_tests());
// Advance beyond slot 0 for a slightly more realistic test environment
⋮----
bank.fill_bank_with_ticks_for_tests();
let bank = Bank::new_from_parent(bank.clone(), bank.collector_id(), bank.slot() + 1);
debug!("Bank slot: {}", bank.slot());
⋮----
let slot = bank.slot();
let last_blockhash = bank.last_blockhash();
⋮----
pub async fn start(mut self) -> (BanksClient, Keypair, Hash) {
let (bank_forks, block_commitment_cache, last_blockhash, gci) = self.setup_bank();
⋮----
let transport = start_local_server(
bank_forks.clone(),
block_commitment_cache.clone(),
⋮----
let banks_client = start_client(transport)
⋮----
.unwrap_or_else(|err| panic!("Failed to start banks client: {err}"));
// Run a simulated PohService to provide the client with new blockhashes.  New blockhashes
// are required when sending multiple otherwise identical transactions in series from a
// test
⋮----
.read()
.unwrap()
.working_bank()
.register_unique_recent_blockhash_for_test();
⋮----
/// Start the test client
    ///
⋮----
///
    /// Returns a `BanksClient` interface into the test environment as well as a payer `Keypair`
⋮----
/// Returns a `BanksClient` interface into the test environment as well as a payer `Keypair`
    /// with SOL for sending transactions
⋮----
/// with SOL for sending transactions
    pub async fn start_with_context(mut self) -> ProgramTestContext {
⋮----
pub async fn start_with_context(mut self) -> ProgramTestContext {
⋮----
pub trait ProgramTestBanksClientExt {
/// Get a new latest blockhash, similar in spirit to RpcClient::get_latest_blockhash()
    async fn get_new_latest_blockhash(&mut self, blockhash: &Hash) -> io::Result<Hash>;
⋮----
impl ProgramTestBanksClientExt for BanksClient {
async fn get_new_latest_blockhash(&mut self, blockhash: &Hash) -> io::Result<Hash> {
⋮----
while start.elapsed().as_secs() < 5 {
let new_blockhash = self.get_latest_blockhash().await?;
⋮----
return Ok(new_blockhash);
⋮----
debug!("Got same blockhash ({blockhash:?}), will retry...");
⋮----
Err(io::Error::other(format!(
⋮----
struct DroppableTask<T>(Arc<AtomicBool>, JoinHandle<T>);
impl<T> Drop for DroppableTask<T> {
fn drop(&mut self) {
self.0.store(true, Ordering::Relaxed);
trace!(
⋮----
pub struct ProgramTestContext {
⋮----
impl ProgramTestContext {
fn new(
⋮----
let running_bank_forks = bank_forks.clone();
⋮----
let bank_task = DroppableTask(
exit.clone(),
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
pub fn genesis_config(&self) -> &GenesisConfig {
⋮----
/// Manually increment vote credits for the current epoch in the specified vote account to simulate validator voting activity
    pub fn increment_vote_account_credits(
⋮----
pub fn increment_vote_account_credits(
⋮----
let bank_forks = self.bank_forks.read().unwrap();
let bank = bank_forks.working_bank();
// generate some vote activity for rewards
let mut vote_account = bank.get_account(vote_account_address).unwrap();
⋮----
VoteStateV4::deserialize(vote_account.data(), vote_account_address).unwrap();
let epoch = bank.epoch();
// Inlined from vote program - maximum number of epoch credits to keep in history
⋮----
// Inline increment_credits logic from vote program.
⋮----
// never seen a credit
if vote_state.epoch_credits.is_empty() {
vote_state.epoch_credits.push((epoch, 0, 0));
} else if epoch != vote_state.epoch_credits.last().unwrap().0 {
let (_, credits_val, prev_credits) = *vote_state.epoch_credits.last().unwrap();
⋮----
// if credits were earned previous epoch
// append entry at end of list for the new epoch
⋮----
.push((epoch, credits_val, credits_val));
⋮----
// else just move the current epoch
vote_state.epoch_credits.last_mut().unwrap().0 = epoch;
⋮----
// Remove too old epoch_credits
if vote_state.epoch_credits.len() > MAX_EPOCH_CREDITS_HISTORY {
vote_state.epoch_credits.remove(0);
⋮----
vote_state.epoch_credits.last_mut().unwrap().1 = vote_state
⋮----
.last()
⋮----
.saturating_add(credits);
⋮----
vote_account.set_state(&versioned).unwrap();
bank.store_account(vote_account_address, &vote_account);
⋮----
/// Create or overwrite an account, subverting normal runtime checks.
    ///
⋮----
///
    /// This method exists to make it easier to set up artificial situations
⋮----
/// This method exists to make it easier to set up artificial situations
    /// that would be difficult to replicate by sending individual transactions.
⋮----
/// that would be difficult to replicate by sending individual transactions.
    /// Beware that it can be used to create states that would not be reachable
⋮----
/// Beware that it can be used to create states that would not be reachable
    /// by sending transactions!
⋮----
/// by sending transactions!
    pub fn set_account(&mut self, address: &Pubkey, account: &AccountSharedData) {
⋮----
pub fn set_account(&mut self, address: &Pubkey, account: &AccountSharedData) {
⋮----
/// Create or overwrite a sysvar, subverting normal runtime checks.
    ///
/// This method exists to make it easier to set up artificial situations
    /// that would be difficult to replicate on a new test cluster. Beware
⋮----
/// that would be difficult to replicate on a new test cluster. Beware
    /// that it can be used to create states that would not be reachable
⋮----
/// that it can be used to create states that would not be reachable
    /// under normal conditions!
⋮----
/// under normal conditions!
    pub fn set_sysvar<T: SysvarId + SysvarSerialize>(&self, sysvar: &T) {
⋮----
pub fn set_sysvar<T: SysvarId + SysvarSerialize>(&self, sysvar: &T) {
⋮----
bank.set_sysvar_for_tests(sysvar);
⋮----
/// Force the working bank ahead to a new slot
    pub fn warp_to_slot(&mut self, warp_slot: Slot) -> Result<(), ProgramTestError> {
⋮----
pub fn warp_to_slot(&mut self, warp_slot: Slot) -> Result<(), ProgramTestError> {
let mut bank_forks = self.bank_forks.write().unwrap();
⋮----
// Fill ticks until a new blockhash is recorded, otherwise retried transactions will have
// the same signature
⋮----
// Ensure that we are actually progressing forward
let working_slot = bank.slot();
⋮----
return Err(ProgramTestError::InvalidWarpSlot);
⋮----
// Warp ahead to one slot *before* the desired slot because the bank
// from Bank::warp_from_parent() is frozen. If the desired slot is one
// slot *after* the working_slot, no need to warp at all.
⋮----
bank.freeze();
⋮----
.insert(Bank::warp_from_parent(
⋮----
.clone_without_scheduler()
⋮----
bank_forks.set_root(
⋮----
None, // snapshots are disabled
Some(pre_warp_slot),
⋮----
// warp_bank is frozen so go forward to get unfrozen bank at warp_slot
bank_forks.insert(Bank::new_from_parent(
⋮----
// Update block commitment cache, otherwise banks server will poll at
// the wrong slot
let mut w_block_commitment_cache = self.block_commitment_cache.write().unwrap();
// HACK: The root set here should be `pre_warp_slot`, but since we're
w_block_commitment_cache.set_all_slots(warp_slot, warp_slot);
⋮----
self.last_blockhash = bank.last_blockhash();
⋮----
pub fn warp_to_epoch(&mut self, warp_epoch: Epoch) -> Result<(), ProgramTestError> {
⋮----
.get_first_slot_in_epoch(warp_epoch);
self.warp_to_slot(warp_slot)
⋮----
pub fn warp_forward_force_reward_interval_end(&mut self) -> Result<(), ProgramTestError> {
⋮----
let pre_warp_slot = bank.slot();
⋮----
warp_bank.force_reward_interval_end_for_tests();
bank_forks.insert(warp_bank);
⋮----
pub async fn get_new_latest_blockhash(&mut self) -> io::Result<Hash> {
⋮----
.get_new_latest_blockhash(&self.last_blockhash)
⋮----
Ok(blockhash)
⋮----
pub fn register_hard_fork(&mut self, hard_fork_slot: Slot) {
⋮----
.register_hard_fork(hard_fork_slot)

================
File: program-test/tests/bpf.rs
================
async fn test_add_bpf_program() {
⋮----
program_test.prefer_bpf(true);
program_test.add_program("noop_program", program_id, None);
let context = program_test.start_with_context().await;
⋮----
.get_account(program_id)
⋮----
.unwrap()
.unwrap();
assert_eq!(program_account.owner, bpf_loader::id());
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
async fn test_max_accounts(num_accounts: u8, deactivate_feature: bool, expect_success: bool) {
⋮----
program_test.deactivate_feature(feature_set::increase_tx_account_lock_limit::id());
⋮----
let num_extra_accounts = num_accounts.checked_sub(2).unwrap();
⋮----
.map(|_| AccountMeta::new_readonly(Pubkey::new_unique(), false))
⋮----
.unwrap_err();

================
File: program-test/tests/builtins.rs
================
async fn test_bpf_loader_upgradeable_present() {
let (banks_client, payer, recent_blockhash) = ProgramTest::default().start().await;
⋮----
let rent = banks_client.get_rent().await.unwrap();
let buffer_rent = rent.minimum_balance(UpgradeableLoaderState::size_of_programdata(1));
⋮----
&payer.pubkey(),
&buffer_keypair.pubkey(),
&upgrade_authority_keypair.pubkey(),
⋮----
.unwrap();
⋮----
Transaction::new_with_payer(&create_buffer_instructions[..], Some(&payer.pubkey()));
transaction.sign(&[&payer, &buffer_keypair], recent_blockhash);
banks_client.process_transaction(transaction).await.unwrap();
⋮----
.get_account(buffer_keypair.pubkey())
⋮----
.unwrap()
⋮----
assert_eq!(buffer_account.owner, bpf_loader_upgradeable::id());
⋮----
async fn versioned_transaction() {
⋮----
let context = program_test.start_with_context().await;
⋮----
let rent = context.banks_client.get_rent().await.unwrap();
⋮----
&context.payer.pubkey(),
⋮----
&account.pubkey(),
rent.minimum_balance(space),
⋮----
.unwrap(),
⋮----
.process_transaction(transaction)

================
File: program-test/tests/compute_units.rs
================
fn overflow_compute_units() {
⋮----
program_test.set_compute_max_units(i64::MAX as u64 + 1);
⋮----
async fn max_compute_units() {
⋮----
program_test.set_compute_max_units(i64::MAX as u64);
let context = program_test.start_with_context().await;
⋮----
let rent = context.banks_client.get_rent().await.unwrap();
⋮----
&context.payer.pubkey(),
&mint.pubkey(),
rent.minimum_balance(space),
⋮----
vec![
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
.unwrap();

================
File: program-test/tests/core_bpf.rs
================
async fn assert_bpf_program(context: &ProgramTestContext, program_id: &Pubkey) {
⋮----
.get_account(*program_id)
⋮----
.unwrap()
.unwrap();
assert_eq!(program_account.owner, bpf_loader_upgradeable::id());
assert!(program_account.executable);
⋮----
async fn test_vended_core_bpf_programs() {
⋮----
let context = program_test.start_with_context().await;
assert_bpf_program(&context, &solana_sdk_ids::address_lookup_table::id()).await;
assert_bpf_program(&context, &solana_sdk_ids::config::id()).await;
assert_bpf_program(&context, &solana_sdk_ids::feature::id()).await;
assert_bpf_program(&context, &solana_sdk_ids::stake::id()).await;
⋮----
async fn test_add_core_bpf_program_manually() {
⋮----
program_test.add_upgradeable_program_to_genesis("noop_program", &program_id);
⋮----
assert_bpf_program(&context, &program_id).await;
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)

================
File: program-test/tests/cpi.rs
================
fn invoker_process_instruction(
⋮----
msg!("Processing invoker instruction before CPI");
let account_info_iter = &mut accounts.iter();
let invoked_program_info = next_account_info(account_info_iter)?;
invoke(
⋮----
vec![AccountMeta::new_readonly(*invoked_program_info.key, false)],
⋮----
msg!("Processing invoker instruction after CPI");
Ok(())
⋮----
fn invoker_dupes_process_instruction(
⋮----
vec![
⋮----
invoked_program_info.clone(),
⋮----
fn invoked_process_instruction(
⋮----
msg!("Processing invoked instruction");
⋮----
fn invoke_create_account(
⋮----
msg!("Processing instruction before system program CPI instruction");
⋮----
let payer_info = next_account_info(account_info_iter)?;
let create_account_info = next_account_info(account_info_iter)?;
let system_program_info = next_account_info(account_info_iter)?;
⋮----
let minimum_balance = rent.minimum_balance(MAX_PERMITTED_DATA_INCREASE);
⋮----
payer_info.clone(),
create_account_info.clone(),
system_program_info.clone(),
⋮----
msg!("Processing instruction after system program CPI");
⋮----
async fn cpi() {
⋮----
processor!(invoker_process_instruction),
⋮----
program_test.add_program(
⋮----
processor!(invoked_process_instruction),
⋮----
let context = program_test.start_with_context().await;
let instructions = vec![Instruction::new_with_bincode(
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
.unwrap();
⋮----
async fn cpi_dupes() {
⋮----
processor!(invoker_dupes_process_instruction),
⋮----
async fn cpi_create_account() {
⋮----
processor!(invoke_create_account),
⋮----
fn invoker_stack_height(
⋮----
let stack_height = get_stack_height();
assert_eq!(stack_height, 1);
⋮----
&Instruction::new_with_bytes(*invoked_program_info.key, &[], vec![]),
⋮----
fn invoked_stack_height(
⋮----
assert_eq!(stack_height, 2);
⋮----
async fn stack_height() {
⋮----
processor!(invoker_stack_height),
⋮----
processor!(invoked_stack_height),
⋮----
let instructions = vec![Instruction::new_with_bytes(

================
File: program-test/tests/fuzz.rs
================
fn process_instruction(
⋮----
msg!("Processing instruction");
Ok(())
⋮----
fn simulate_fuzz() {
let rt = tokio::runtime::Runtime::new().unwrap();
⋮----
processor!(process_instruction),
⋮----
let (mut banks_client, payer, last_blockhash) = rt.block_on(program_test.start());
rt.block_on(run_fuzz_instructions(
⋮----
fn simulate_fuzz_with_context() {
⋮----
let mut context = rt.block_on(program_test.start_with_context());
⋮----
async fn run_fuzz_instructions(
⋮----
let mut instructions = vec![];
let mut signer_keypairs = vec![];
⋮----
&payer.pubkey(),
&keypair.pubkey(),
Rent::default().minimum_balance(i as usize),
⋮----
instructions.push(instruction);
instructions.push(Instruction::new_with_bincode(*program_id, &[0], vec![]));
signer_keypairs.push(keypair);
⋮----
let mut transaction = Transaction::new_with_payer(&instructions, Some(&payer.pubkey()));
⋮----
.iter()
.copied()
.chain(signer_keypairs.iter())
⋮----
transaction.partial_sign(&signers, last_blockhash);
banks_client.process_transaction(transaction).await.unwrap();
⋮----
.get_account(keypair.pubkey())
⋮----
.expect("account exists")
.unwrap();
assert!(account.lamports > 0);
assert!(!account.data.is_empty());

================
File: program-test/tests/genesis_accounts.rs
================
async fn genesis_accounts() {
⋮----
for (pubkey, account) in my_genesis_accounts.iter() {
program_test.add_genesis_account(*pubkey, account.clone());
⋮----
let context = program_test.start_with_context().await;
⋮----
.get_account(*pubkey)
⋮----
.unwrap()
.unwrap();
assert_eq!(fetched_account, *account);

================
File: program-test/tests/lamports.rs
================
fn move_lamports_process_instruction(
⋮----
msg!("Processing lamports instruction");
let account_info_iter = &mut accounts.iter();
let source_info = next_account_info(account_info_iter)?;
let _other_info = next_account_info(account_info_iter)?;
let destination_info = next_account_info(account_info_iter)?;
let destination_lamports = destination_info.lamports();
**destination_info.lamports.borrow_mut() = destination_lamports
.checked_add(source_info.lamports())
.unwrap();
**source_info.lamports.borrow_mut() = 0;
Ok(())
⋮----
async fn move_lamports() {
⋮----
processor!(move_lamports_process_instruction),
⋮----
let context = program_test.start_with_context().await;
let instructions = vec![
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
assert_eq!(

================
File: program-test/tests/panic.rs
================
fn panic(_program_id: &Pubkey, _accounts: &[AccountInfo], _input: &[u8]) -> ProgramResult {
panic!("I panicked");
⋮----
async fn panic_test() {
⋮----
let program_test = ProgramTest::new("panic", program_id, processor!(panic));
let context = program_test.start_with_context().await;
let instruction = Instruction::new_with_bytes(program_id, &[], vec![]);
⋮----
Some(&context.payer.pubkey()),
⋮----
assert_eq!(

================
File: program-test/tests/realloc.rs
================
fn process_instruction(
⋮----
let account_info_iter = &mut accounts.iter();
let account_info = next_account_info(account_info_iter)?;
let destination_info = next_account_info(account_info_iter)?;
let owner_info = next_account_info(account_info_iter)?;
let token_program_info = next_account_info(account_info_iter)?;
invoke(
⋮----
vec![
⋮----
account_info.clone(),
destination_info.clone(),
owner_info.clone(),
⋮----
Ok(())
⋮----
async fn realloc_smaller_in_cpi() {
⋮----
processor!(process_instruction),
⋮----
let context = program_test.start_with_context().await;
⋮----
let rent = context.banks_client.get_rent().await.unwrap();
⋮----
&context.payer.pubkey(),
&mint.pubkey(),
rent.minimum_balance(mint_space),
⋮----
&account.pubkey(),
rent.minimum_balance(account_space),
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
.unwrap();

================
File: program-test/tests/return_data.rs
================
fn get_return_data_process_instruction(
⋮----
msg!("Processing get_return_data instruction before CPI");
let account_info_iter = &mut accounts.iter();
let invoked_program_info = next_account_info(account_info_iter)?;
invoke(
⋮----
accounts: vec![],
data: input.to_vec(),
⋮----
let return_data = get_return_data().unwrap();
msg!("Processing get_return_data instruction after CPI");
msg!("{}", from_utf8(&return_data.1).unwrap());
assert_eq!(return_data.1, input.to_vec());
Ok(())
⋮----
fn set_return_data_process_instruction(
⋮----
msg!("Processing invoked instruction before set_return_data");
set_return_data(input);
msg!("Processing invoked instruction after set_return_data");
⋮----
async fn return_data() {
⋮----
processor!(get_return_data_process_instruction),
⋮----
program_test.add_program(
⋮----
processor!(set_return_data_process_instruction),
⋮----
let context = program_test.start_with_context().await;
let instructions = vec![Instruction {
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
.unwrap();
⋮----
fn error_set_return_data_process_instruction(
⋮----
Err(ProgramError::InvalidInstructionData)
⋮----
async fn simulation_return_data() {
⋮----
processor!(error_set_return_data_process_instruction),
⋮----
let expected_data = vec![240, 159, 166, 150];
⋮----
.process_transaction_with_preflight_and_commitment(transaction, CommitmentLevel::Confirmed)
⋮----
.unwrap_err();
assert_matches!(

================
File: program-test/tests/setup.rs
================
pub async fn setup_stake(
⋮----
&context.payer.pubkey(),
&stake_keypair.pubkey(),
⋮----
&Authorized::auto(&user.pubkey()),
⋮----
Some(&context.payer.pubkey()),
&vec![&context.payer, &stake_keypair, user],
⋮----
.process_transaction(transaction)
⋮----
.unwrap();
stake_keypair.pubkey()
⋮----
pub async fn setup_vote(context: &mut ProgramTestContext) -> Pubkey {
let mut instructions = vec![];
⋮----
instructions.push(system_instruction::create_account(
⋮----
&validator_keypair.pubkey(),
Rent::default().minimum_balance(0),
⋮----
let vote_lamports = Rent::default().minimum_balance(VoteStateV4::size_of());
⋮----
instructions.append(&mut vote_instruction::create_account_with_config(
⋮----
&vote_keypair.pubkey(),
⋮----
node_pubkey: validator_keypair.pubkey(),
authorized_voter: user_keypair.pubkey(),
⋮----
&vec![&context.payer, &validator_keypair, &vote_keypair],
⋮----
vote_keypair.pubkey()

================
File: program-test/tests/spl.rs
================
async fn programs_present() {
let (banks_client, _, _) = ProgramTest::default().start().await;
let rent = banks_client.get_rent().await.unwrap();
⋮----
Pubkey::find_program_address(&[token_2022_id.as_ref()], &bpf_loader_upgradeable::id());
for (program_id, _) in spl_programs(&rent) {
let program_account = banks_client.get_account(program_id).await.unwrap().unwrap();
⋮----
assert_eq!(program_account.owner, bpf_loader_upgradeable::id());
⋮----
assert_eq!(program_account.owner, bpf_loader::id());
⋮----
async fn token_2022() {
let (banks_client, payer, recent_blockhash) = ProgramTest::default().start().await;
⋮----
&payer.pubkey(),
&mint.pubkey(),
rent.minimum_balance(space),
⋮----
vec![
⋮----
Some(&payer.pubkey()),
⋮----
banks_client.process_transaction(transaction).await.unwrap();

================
File: program-test/tests/sysvar_last_restart_slot.rs
================
fn sysvar_last_restart_slot_process_instruction(
⋮----
msg!("sysvar_last_restart_slot");
assert_eq!(input.len(), 8);
let expected_last_hardfork_slot = u64::from_le_bytes(input[0..8].try_into().unwrap());
⋮----
msg!("last restart slot: {:?}", last_restart_slot);
assert_eq!(
⋮----
msg!("slot via account: {:?}", slot_via_account);
⋮----
Ok(())
⋮----
async fn check_with_program(
⋮----
let instructions = vec![Instruction::new_with_bincode(
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
.unwrap();
⋮----
async fn get_sysvar_last_restart_slot() {
⋮----
processor!(sysvar_last_restart_slot_process_instruction),
⋮----
let mut context = program_test.start_with_context().await;
check_with_program(&mut context, program_id, 0).await;
context.warp_to_slot(40).unwrap();
context.register_hard_fork(41 as Slot);
⋮----
context.warp_to_slot(41).unwrap();
check_with_program(&mut context, program_id, 41).await;
context.register_hard_fork(40 as Slot);
context.warp_to_slot(45).unwrap();
⋮----
context.register_hard_fork(47 as Slot);
context.register_hard_fork(48 as Slot);
context.warp_to_slot(46).unwrap();
⋮----
context.register_hard_fork(50 as Slot);
context.warp_to_slot(48).unwrap();
check_with_program(&mut context, program_id, 48).await;
context.warp_to_slot(50).unwrap();
check_with_program(&mut context, program_id, 50).await;

================
File: program-test/tests/sysvar.rs
================
fn sysvar_getter_process_instruction(
⋮----
msg!("sysvar_getter");
⋮----
assert_eq!(42, clock.slot);
⋮----
assert_eq!(epoch_schedule, EpochSchedule::default());
⋮----
assert_eq!(rent, Rent::default());
Ok(())
⋮----
async fn get_sysvar() {
⋮----
processor!(sysvar_getter_process_instruction),
⋮----
let mut context = program_test.start_with_context().await;
context.warp_to_slot(42).unwrap();
let instructions = vec![Instruction::new_with_bincode(program_id, &(), vec![])];
⋮----
Some(&context.payer.pubkey()),
⋮----
.process_transaction(transaction)
⋮----
.unwrap();
⋮----
fn epoch_reward_sysvar_getter_process_instruction(
⋮----
msg!("epoch_reward_sysvar_getter");
⋮----
assert!(!epoch_rewards.active);
⋮----
assert!(epoch_rewards.active);
⋮----
async fn get_epoch_rewards_sysvar() {
⋮----
processor!(epoch_reward_sysvar_getter_process_instruction),
⋮----
let first_normal_slot = context.genesis_config().epoch_schedule.first_normal_slot;
let slots_per_epoch = context.genesis_config().epoch_schedule.slots_per_epoch;
⋮----
.saturating_add(slots_per_epoch)
.saturating_sub(1);
context.warp_to_slot(last_slot_before_new_epoch).unwrap();
let instructions = vec![Instruction::new_with_bincode(program_id, &[0u8], vec![])];
⋮----
let first_slot_in_new_epoch = first_normal_slot.saturating_add(slots_per_epoch);
context.warp_to_slot(first_slot_in_new_epoch).unwrap();
let instructions = vec![Instruction::new_with_bincode(program_id, &[1u8], vec![])];

================
File: program-test/tests/warp.rs
================
mod setup;
⋮----
fn process_instruction(
⋮----
let account_info_iter = &mut accounts.iter();
let clock_info = next_account_info(account_info_iter)?;
⋮----
let expected_slot = u64::from_le_bytes(input.try_into().unwrap());
⋮----
Ok(())
⋮----
Err(ProgramError::Custom(WRONG_SLOT_ERROR))
⋮----
async fn clock_sysvar_updated_from_warp() {
⋮----
processor!(process_instruction),
⋮----
let mut context = program_test.start_with_context().await;
⋮----
vec![AccountMeta::new_readonly(clock::id(), false)],
⋮----
Some(&context.payer.pubkey()),
⋮----
assert_eq!(
⋮----
context.warp_to_slot(expected_slot).unwrap();
⋮----
.process_transaction(transaction)
⋮----
.unwrap();
⋮----
assert!(context.warp_to_slot(expected_slot).is_ok());
⋮----
async fn stake_rewards_from_warp() {
⋮----
context.warp_to_slot(100).unwrap();
let vote_address = setup_vote(&mut context).await;
⋮----
setup_stake(&mut context, &user_keypair, &vote_address, stake_lamports).await;
⋮----
.get_account(stake_address)
⋮----
.expect("account exists")
⋮----
assert_eq!(account.lamports, stake_lamports);
let first_normal_slot = context.genesis_config().epoch_schedule.first_normal_slot;
context.warp_to_slot(first_normal_slot).unwrap();
⋮----
context.increment_vote_account_credits(&vote_address, 100);
let slots_per_epoch = context.genesis_config().epoch_schedule.slots_per_epoch;
⋮----
.warp_to_slot(first_normal_slot + slots_per_epoch + 1)
⋮----
assert!(account.lamports > stake_lamports);
⋮----
.get_account(stake_history::id())
⋮----
.get_account(clock::id())
⋮----
let stake_state: StakeStateV2 = deserialize(&account.data).unwrap();
let stake_history: StakeHistory = deserialize(&stake_history_account.data).unwrap();
let clock: Clock = deserialize(&clock_account.data).unwrap();
let stake = stake_state.stake().unwrap();
⋮----
async fn stake_rewards_filter_bench_100() {
stake_rewards_filter_bench_core(100).await;
⋮----
async fn stake_rewards_filter_bench_core(num_stake_accounts: u64) {
⋮----
program_test.add_account(vote_address, vote_account.clone().into());
⋮----
let mut to_filter = vec![];
⋮----
program_test.add_account(stake_pubkey, stake_account);
to_filter.push(stake_pubkey);
⋮----
debug!("create stake account {i} {stake_pubkey}");
⋮----
assert_eq!(account.lamports, TEST_FILTER_STAKE);
⋮----
async fn check_credits_observed(
⋮----
.unwrap()
⋮----
let stake_state: StakeStateV2 = deserialize(&stake_account.data).unwrap();
⋮----
async fn stake_merge_immediately_after_activation() {
⋮----
context.warp_to_slot(current_slot).unwrap();
context.warp_forward_force_reward_interval_end().unwrap();
⋮----
check_credits_observed(&mut context.banks_client, base_stake_address, 100).await;
⋮----
context.warp_to_epoch(clock.epoch + 1).unwrap();
⋮----
check_credits_observed(&mut context.banks_client, absorbed_stake_address, 200).await;
⋮----
.get_account(base_stake_address)
⋮----
assert_eq!(stake_state.stake().unwrap().credits_observed, 300);
assert!(stake_account.lamports > stake_lamports);
⋮----
.get_account(absorbed_stake_address)
⋮----
assert_eq!(stake_account.lamports, stake_lamports);
⋮----
&user_keypair.pubkey(),
⋮----
&vec![&context.payer, &user_keypair],
⋮----
async fn get_blockhash_post_warp() {
⋮----
.get_new_latest_blockhash(&context.last_blockhash)
⋮----
let mut tx = Transaction::new_with_payer(&[], Some(&context.payer.pubkey()));
tx.sign(&[&context.payer], new_blockhash);
context.banks_client.process_transaction(tx).await.unwrap();
context.warp_to_slot(10).unwrap();

================
File: program-test/Cargo.toml
================
[package]
description = "Solana Program Test Framework"
name = "solana-program-test"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
assert_matches = { workspace = true }
async-trait = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
chrono-humanize = { workspace = true }
crossbeam-channel = { workspace = true }
log = { workspace = true }
serde = { workspace = true }
solana-account = { workspace = true }
solana-account-info = { workspace = true }
solana-accounts-db = { workspace = true }
solana-banks-client = { workspace = true }
solana-banks-interface = { workspace = true }
solana-banks-server = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-compute-budget = { workspace = true }
solana-epoch-rewards = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-genesis-config = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-loader-v3-interface = { workspace = true }
solana-message = { workspace = true }
solana-msg = { workspace = true }
solana-native-token = { workspace = true }
solana-poh-config = { workspace = true }
solana-program-binaries = { workspace = true }
solana-program-entrypoint = { workspace = true }
solana-program-error = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-runtime = { workspace = true }
solana-sbpf = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signer = { workspace = true }
solana-stable-layout = { workspace = true }
solana-stake-interface = { workspace = true }
solana-svm = { workspace = true }
solana-svm-log-collector = { workspace = true }
solana-svm-timings = { workspace = true }
solana-system-interface = { workspace = true }
solana-sysvar = { workspace = true }
solana-sysvar-id = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
solana-vote-program = { workspace = true }
spl-generic-token = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[dev-dependencies]
solana-cpi = { workspace = true }
solana-program = { workspace = true }
solana-program-test = { path = ".", features = ["agave-unstable-api"] }
test-case = { workspace = true }

================
File: LICENSE
================
Apache License
Version 2.0, January 2004
http://www.apache.org/licenses/

TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

1. Definitions.

"License" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.

"Licensor" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.

"Legal Entity" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, "control" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.

"You" (or "Your") shall mean an individual or Legal Entity exercising permissions granted by this License.

"Source" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.

"Object" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.

"Work" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).

"Derivative Works" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.

"Contribution" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, "submitted" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as "Not a Contribution."

"Contributor" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.

2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.

3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.

4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:

You must give any other recipients of the Work or Derivative Works a copy of this License; and
You must cause any modified files to carry prominent notices stating that You changed the files; and
You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and
If the Work includes a "NOTICE" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.

You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.
5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.

6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.

7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.

8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.

9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.

END OF TERMS AND CONDITIONS





================================================================
End of Codebase
================================================================
