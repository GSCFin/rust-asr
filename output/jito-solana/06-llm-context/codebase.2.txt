This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
.buildkite/
  hooks/
    post-checkout
    post-command
    pre-command
  scripts/
    build-stable.sh
    build-stable.test.sh
    common.sh
    common.test.sh
    func-assert-eq.sh
    test-all.sh
    trigger-github-actions-windows-build.sh
  pipeline-upload.sh
  solana-private.sh
.config/
  nextest.toml
.github/
  ISSUE_TEMPLATE/
    0-community.md
    1-core-contributor.md
    2-feature-gate.yml
  scripts/
    add-team-to-ghsa.sh
    check-changelog.sh
    downstream-project-spl-common.sh
    downstream-project-spl-install-deps.sh
    install-all-deps.sh
    install-openssl.sh
    install-proto.sh
    purge-ubuntu-runner.sh
  workflows/
    add-team-to-ghsa.yml
    benchmark.yml
    cargo.yml
    changelog-label.yml
    client-targets.yml
    crate-check.yml
    dependabot-pr.yml
    docs.yml
    downstream-project-anchor.yml
    downstream-project-spl-nightly.yml
    downstream-project-spl.yml
    error-reporting.yml
    label-actions.yml
    publish-windows-tarball.yml
    rebase.yaml
    release.yml
    verify-packets.yml
  CODEOWNERS
  dependabot.yml
  label-actions.yml
  PULL_REQUEST_TEMPLATE.md
  RELEASE_TEMPLATE.md
account-decoder/
  src/
    lib.rs
    parse_account_data.rs
    parse_address_lookup_table.rs
    parse_bpf_loader.rs
    parse_config.rs
    parse_nonce.rs
    parse_stake.rs
    parse_sysvar.rs
    parse_token_extension.rs
    parse_token.rs
    parse_vote.rs
    validator_info.rs
  Cargo.toml
account-decoder-client-types/
  src/
    lib.rs
    token.rs
  Cargo.toml
accounts-cluster-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
accounts-db/
  benches/
    accounts_index.rs
    accounts.rs
    bench_accounts_file.rs
    bench_hashing.rs
    bench_lock_accounts.rs
    bench_serde.rs
    read_only_accounts_cache.rs
    utils.rs
  src/
    account_storage/
      stored_account_info.rs
    accounts_db/
      accounts_db_config.rs
      geyser_plugin_utils.rs
      stats.rs
      tests.rs
    accounts_index/
      account_map_entry.rs
      accounts_index_storage.rs
      bucket_map_holder.rs
      in_mem_accounts_index.rs
      iter.rs
      roots_tracker.rs
      secondary.rs
      stats.rs
    append_vec/
      meta.rs
      test_utils.rs
    rolling_bit_field/
      iterators.rs
    tiered_storage/
      byte_block.rs
      error.rs
      file.rs
      footer.rs
      hot.rs
      index.rs
      meta.rs
      mmap_utils.rs
      owners.rs
      readable.rs
      test_utils.rs
    account_info.rs
    account_locks.rs
    account_storage_reader.rs
    account_storage.rs
    accounts_cache.rs
    accounts_db.rs
    accounts_file.rs
    accounts_hash.rs
    accounts_index.rs
    accounts_update_notifier_interface.rs
    accounts.rs
    active_stats.rs
    ancestors.rs
    ancient_append_vecs.rs
    append_vec.rs
    blockhash_queue.rs
    contains.rs
    is_loadable.rs
    is_zero_lamport.rs
    lib.rs
    obsolete_accounts.rs
    partitioned_rewards.rs
    pubkey_bins.rs
    read_only_accounts_cache.rs
    rolling_bit_field.rs
    sorted_storages.rs
    stake_rewards.rs
    storable_accounts.rs
    tiered_storage.rs
    utils.rs
    waitable_condvar.rs
  store-histogram/
    src/
      main.rs
    Cargo.toml
  store-tool/
    src/
      main.rs
    Cargo.toml
  tests/
    read_only_accounts_cache.rs
  Cargo.toml
bam-banking-bench/
  src/
    main.rs
    mock_bam_server.rs
  .gitignore
  Cargo.toml
bam-local-cluster/
  examples/
    example_config.toml
  src/
    cluster_manager.rs
    config.rs
    lib.rs
    main.rs
  Cargo.toml
  README.md
banking-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
banking-stage-ingress-types/
  src/
    lib.rs
  Cargo.toml
banks-client/
  src/
    error.rs
    lib.rs
  Cargo.toml
banks-interface/
  src/
    lib.rs
  Cargo.toml
banks-server/
  src/
    banks_server.rs
    lib.rs
  Cargo.toml
bench-streamer/
  src/
    main.rs
  .gitignore
  Cargo.toml
bench-tps/
  src/
    bench.rs
    cli.rs
    keypairs.rs
    lib.rs
    log_transaction_service.rs
    main.rs
    perf_utils.rs
    rpc_with_retry_utils.rs
    send_batch.rs
  tests/
    fixtures/
      spl_instruction_padding.so
    bench_tps.rs
  .gitignore
  Cargo.toml
bench-vote/
  src/
    main.rs
  Cargo.toml
bloom/
  benches/
    bloom.rs
  src/
    bloom.rs
    lib.rs
  Cargo.toml
bucket_map/
  src/
    bucket_api.rs
    bucket_item.rs
    bucket_map.rs
    bucket_stats.rs
    bucket_storage.rs
    bucket.rs
    index_entry.rs
    lib.rs
    restart.rs
  tests/
    bucket_map.rs
  Cargo.toml
builtins/
  src/
    core_bpf_migration.rs
    lib.rs
    prototype.rs
  Cargo.toml
builtins-default-costs/
  src/
    lib.rs
  Cargo.toml
bundle/
  src/
    lib.rs
  Cargo.toml
cargo-registry/
  src/
    client.rs
    crate_handler.rs
    main.rs
    response_builder.rs
    sparse_index.rs
  Cargo.toml
ci/
  bench/
    common.sh
    part1.sh
    part2.sh
  common/
    limit-threads.sh
    shared-functions.sh
  coverage/
    common.sh
    part-1.sh
    part-2.sh
    part-3.sh
  docker/
    build.sh
    Dockerfile
    env.sh
    README.md
  downstream-projects/
    common.sh
    func-openbook-dex.sh
    func-spl.sh
    run-all.sh
    run-openbook-dex.sh
    run-spl.sh
  semver_bash/
    LICENSE
    README.md
    semver_test.sh
    semver.sh
  stable/
    common.sh
    run-all.sh
    run-local-cluster-partially.sh
    run-localnet.sh
    run-partition.sh
  xtask/
    src/
      commands/
        bump_version.rs
        hello.rs
      commands.rs
      common.rs
      main.rs
    Cargo.toml
  _
  .gitignore
  buildkite-pipeline.sh
  buildkite-secondary.yml
  buildkite-solana-private.sh
  channel_restriction.sh
  channel-info.sh
  check-channel-version.sh
  check-crates.sh
  check-install-all.sh
  crate-version.sh
  do-audit.sh
  docker-run-default-image.sh
  docker-run.sh
  env.sh
  format-url.sh
  hoover.sh
  intercept.sh
  localnet-sanity.sh
  nits.sh
  order-crates-for-publishing.py
  platform-tools-info.sh
  publish-crate.sh
  publish-installer.sh
  publish-metrics-dashboard.sh
  publish-tarball.sh
  run-local.sh
  run-sanity.sh
  rust-version.sh
  shellcheck.sh
  test-checks.sh
  test-coverage.sh
  test-dev-context-only-utils.sh
  test-downstream-builds.sh
  test-frozen-abi.sh
  test-miri.sh
  test-sanity.sh
  test-shuttle.sh
  test-stable.sh
  test-verify-packets-gossip.sh
  test.sh
  upload-benchmark.sh
  upload-ci-artifact.sh
  upload-github-release-asset.sh
clap-utils/
  src/
    compute_budget.rs
    compute_unit_price.rs
    fee_payer.rs
    input_parsers.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
clap-v3-utils/
  src/
    input_parsers/
      mod.rs
      signer.rs
    keygen/
      derivation_path.rs
      mnemonic.rs
      mod.rs
    compute_budget.rs
    fee_payer.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
cli/
  src/
    address_lookup_table.rs
    checks.rs
    clap_app.rs
    cli.rs
    cluster_query.rs
    compute_budget.rs
    feature.rs
    inflation.rs
    lib.rs
    main.rs
    memo.rs
    nonce.rs
    program_v4.rs
    program.rs
    spend_utils.rs
    stake.rs
    test_utils.rs
    validator_info.rs
    vote.rs
    wallet.rs
  tests/
    fixtures/
      alt_bn128.so
      build.sh
      noop_large.so
      noop.so
    address_lookup_table.rs
    cluster_query.rs
    nonce.rs
    program.rs
    request_airdrop.rs
    stake.rs
    transfer.rs
    validator_info.rs
    vote.rs
  .gitignore
  Cargo.toml
cli-config/
  src/
    config_input.rs
    config.rs
    lib.rs
  Cargo.toml
cli-output/
  src/
    cli_output.rs
    cli_version.rs
    display.rs
    lib.rs
  Cargo.toml
client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    connection_cache.rs
    lib.rs
    send_and_confirm_transactions_in_parallel.rs
    tpu_client.rs
    transaction_executor.rs
  .gitignore
  Cargo.toml
client-test/
  tests/
    client.rs
    send_and_confirm_transactions_in_parallel.rs
  .gitignore
  Cargo.toml
compute-budget/
  src/
    compute_budget_limits.rs
    compute_budget.rs
    lib.rs
  Cargo.toml
compute-budget-instruction/
  benches/
    process_compute_budget_instructions.rs
  src/
    builtin_programs_filter.rs
    compute_budget_instruction_details.rs
    compute_budget_program_id_filter.rs
    instructions_processor.rs
    lib.rs
  Cargo.toml
connection-cache/
  src/
    nonblocking/
      client_connection.rs
      mod.rs
    client_connection.rs
    connection_cache_stats.rs
    connection_cache.rs
    lib.rs
  Cargo.toml
core/
  benches/
    banking_stage.rs
    banking_trace.rs
    consensus.rs
    consumer.rs
    gen_keys.rs
    proto_to_packet.rs
    receive_and_buffer_utils.rs
    receive_and_buffer.rs
    scheduler.rs
    shredder.rs
    sigverify_stage.rs
  src/
    banking_stage/
      transaction_scheduler/
        bam_receive_and_buffer.rs
        bam_scheduler.rs
        bam_utils.rs
        batch_id_generator.rs
        greedy_scheduler.rs
        in_flight_tracker.rs
        mod.rs
        prio_graph_scheduler.rs
        receive_and_buffer.rs
        scheduler_common.rs
        scheduler_controller.rs
        scheduler_error.rs
        scheduler_metrics.rs
        scheduler.rs
        transaction_priority_id.rs
        transaction_state_container.rs
        transaction_state.rs
      committer.rs
      consume_worker.rs
      consumer.rs
      decision_maker.rs
      latest_validator_vote_packet.rs
      leader_slot_metrics.rs
      leader_slot_timing_metrics.rs
      progress_tracker.rs
      qos_service.rs
      read_write_account_set.rs
      scheduler_messages.rs
      tpu_to_pack.rs
      unified_scheduler.rs
      vote_packet_receiver.rs
      vote_storage.rs
      vote_worker.rs
    block_creation_loop/
      stats.rs
    bundle_stage/
      bundle_account_locker.rs
      bundle_consumer.rs
      bundle_packet_deserializer.rs
      bundle_stage_leader_metrics.rs
      bundle_storage.rs
    cluster_slots_service/
      cluster_slots.rs
      slot_supporters.rs
    consensus/
      fork_choice.rs
      heaviest_subtree_fork_choice.rs
      latest_validator_votes_for_frozen_banks.rs
      progress_map.rs
      tower_storage.rs
      tower_vote_state.rs
      tower1_14_11.rs
      tower1_7_14.rs
      tree_diff.rs
      vote_stake_tracker.rs
    forwarding_stage/
      packet_container.rs
    proxy/
      auth.rs
      block_engine_stage.rs
      fetch_stage_manager.rs
      mod.rs
      relayer_stage.rs
    repair/
      ancestor_hashes_service.rs
      cluster_slot_state_verifier.rs
      duplicate_repair_status.rs
      malicious_repair_handler.rs
      mod.rs
      outstanding_requests.rs
      packet_threshold.rs
      quic_endpoint.rs
      repair_generic_traversal.rs
      repair_handler.rs
      repair_response.rs
      repair_service.rs
      repair_weight.rs
      repair_weighted_traversal.rs
      request_response.rs
      result.rs
      serve_repair_service.rs
      serve_repair.rs
      standard_repair_handler.rs
    snapshot_packager_service/
      snapshot_gossip_manager.rs
    tip_manager/
      tip_distribution.rs
      tip_payment.rs
    admin_rpc_post_init.rs
    bam_connection.rs
    bam_dependencies.rs
    bam_manager.rs
    banking_simulation.rs
    banking_stage.rs
    banking_trace.rs
    block_creation_loop.rs
    bundle_sigverify_stage.rs
    bundle_stage.rs
    bundle.rs
    cluster_info_vote_listener.rs
    cluster_slots_service.rs
    commitment_service.rs
    completed_data_sets_service.rs
    consensus.rs
    cost_update_service.rs
    drop_bank_service.rs
    fetch_stage.rs
    forwarding_stage.rs
    gen_keys.rs
    lib.rs
    mock_alpenglow_consensus.rs
    next_leader.rs
    optimistic_confirmation_verifier.rs
    packet_bundle.rs
    replay_stage.rs
    resource_limits.rs
    result.rs
    sample_performance_service.rs
    scheduler_bindings_server.rs
    shred_fetch_stage.rs
    sigverify_stage.rs
    sigverify.rs
    snapshot_packager_service.rs
    staked_nodes_updater_service.rs
    stats_reporter_service.rs
    system_monitor_service.rs
    tip_manager.rs
    tpu_entry_notifier.rs
    tpu.rs
    tvu.rs
    unfrozen_gossip_verified_vote_hashes.rs
    validator.rs
    vortexor_receiver_adapter.rs
    vote_simulator.rs
    voting_service.rs
    warm_quic_cache_service.rs
    window_service.rs
  tests/
    bam_connection.rs
    fork-selection.rs
    scheduler_cost_adjustment.rs
    snapshots.rs
    unified_scheduler.rs
  .gitignore
  Cargo.toml
cost-model/
  benches/
    cost_model.rs
    cost_tracker.rs
  src/
    block_cost_limits.rs
    cost_model.rs
    cost_tracker_post_analysis.rs
    cost_tracker.rs
    lib.rs
    transaction_cost.rs
  Cargo.toml
curves/
  curve25519/
    src/
      curve_syscall_traits.rs
      edwards.rs
      errors.rs
      lib.rs
      ristretto.rs
      scalar.rs
    .gitignore
    Cargo.toml
dev/
  Dockerfile
dev-bins/
  .config/
    nextest.toml
  Cargo.toml
docker-solana/
  .gitignore
  build.sh
  Dockerfile
  README.md
docs/
  art/
    fork-generation.bob
    forks-pruned.bob
    forks-pruned2.bob
    forks.bob
    passive-staking-callflow.msc
    retransmit_stage.bob
    runtime.bob
    sdk-tools.bob
    spv-bank-hash.bob
    spv-block-merkle.bob
    tpu.bob
    transaction.bob
    tvu.bob
    validator-proposal.bob
    validator.bob
  components/
    Card.jsx
    HomeCtaLinks.jsx
  src/
    cli/
      examples/
        _category_.json
        choose-a-cluster.md
        delegate-stake.md
        deploy-a-program.md
        durable-nonce.md
        offline-signing.md
        sign-offchain-message.md
        test-validator.md
        transfer-tokens.md
      wallets/
        hardware/
          _category_.json
          index.md
          ledger.md
        _category_.json
        file-system.md
        index.md
        paper.md
      .usage.md.header
      index.md
      install.md
      intro.md
    clusters/
      available.md
      benchmark.md
      index.md
      metrics.md
      testnet.md
    consensus/
      commitments.md
      fork-generation.md
      leader-rotation.md
      managing-forks.md
      stake-delegation-and-rewards.md
      synchronization.md
      turbine-block-propagation.md
      vote-signing.md
    css/
      custom.css
    implemented-proposals/
      ed_overview/
        ed_validation_client_economics/
          ed_vce_overview.md
          ed_vce_state_validation_protocol_based_rewards.md
          ed_vce_state_validation_transaction_fees.md
          ed_vce_validation_stake_delegation.md
        ed_economic_sustainability.md
        ed_mvp.md
        ed_overview.md
        ed_references.md
        ed_storage_rent_economics.md
      abi-management.md
      bank-timestamp-correction.md
      commitment.md
      durable-tx-nonces.md
      epoch_accounts_hash.md
      index.md
      installer.md
      instruction_introspection.md
      leader-leader-transition.md
      leader-validator-transition.md
      persistent-account-storage.md
      readonly-accounts.md
      reliable-vote-transmission.md
      rent.md
      repair-service.md
      rpc-transaction-history.md
      snapshot-verification.md
      staking-rewards.md
      testing-programs.md
      tower-bft.md
      transaction-fees.md
      validator-timestamp-oracle.md
    operations/
      best-practices/
        _category_.json
        general.md
        monitoring.md
        security.md
      guides/
        _category_.json
        restart-cluster.md
        validator-failover.md
        validator-info.md
        validator-monitor.md
        validator-stake.md
        validator-start.md
        validator-troubleshoot.md
        vote-accounts.md
      _category_.json
      index.md
      prerequisites.md
      requirements.md
      setup-a-validator.md
      setup-an-rpc-node.md
      validator-or-rpc-node.md
    pages/
      styles.module.css
    proposals/
      accepted-design-proposals.md
      accounts-db-replication.md
      bankless-leader.md
      block-confirmation.md
      cluster-test-framework.md
      comprehensive-compute-fees.md
      embedding-move.md
      fee_transaction_priority.md
      handle-duplicate-block.md
      interchain-transaction-verification.md
      ledger-replication-to-implement.md
      log_data.md
      off-chain-message-signing.md
      optimistic_confirmation.md
      optimistic-confirmation-and-slashing.md
      optimistic-transaction-propagation-signal.md
      partitioned-inflationary-rewards-distribution.md
      program-instruction-macro.md
      return-data.md
      rip-curl.md
      simple-payment-and-state-verification.md
      slashing.md
      snapshot-verification.md
      tick-verification.md
      timely-vote-credits.md
      validator-proposal.md
      versioned-transactions.md
      vote-signing-to-implement.md
    runtime/
      zk-docs/
        ciphertext_ciphertext_equality.pdf
        ciphertext_commitment_equality.pdf
        ciphertext_validity.pdf
        percentage_with_cap.pdf
        pubkey_proof.pdf
        twisted_elgamal.pdf
        zero_proof.pdf
      programs.md
      sysvars.md
      zk-elgamal-proof.md
    theme/
      Footer/
        index.js
        styles.module.css
    validator/
      anatomy.md
      blockstore.md
      geyser.md
      gossip.md
      runtime.md
      tpu.md
      tvu.md
    architecture.md
    backwards-compatibility.md
    faq.md
    index.mdx
    proposals.md
    what-is-a-validator.md
    what-is-an-rpc-node.md
  static/
    img/
      favicon.ico
    katex/
      contrib/
        auto-render.js
        auto-render.min.js
        auto-render.mjs
        copy-tex.css
        copy-tex.js
        copy-tex.min.css
        copy-tex.min.js
        copy-tex.mjs
        mathtex-script-type.js
        mathtex-script-type.min.js
        mathtex-script-type.mjs
        mhchem.js
        mhchem.min.js
        mhchem.mjs
        render-a11y-string.js
        render-a11y-string.min.js
        render-a11y-string.mjs
      fonts/
        KaTeX_AMS-Regular.ttf
        KaTeX_AMS-Regular.woff
        KaTeX_AMS-Regular.woff2
        KaTeX_Caligraphic-Bold.ttf
        KaTeX_Caligraphic-Bold.woff
        KaTeX_Caligraphic-Bold.woff2
        KaTeX_Caligraphic-Regular.ttf
        KaTeX_Caligraphic-Regular.woff
        KaTeX_Caligraphic-Regular.woff2
        KaTeX_Fraktur-Bold.ttf
        KaTeX_Fraktur-Bold.woff
        KaTeX_Fraktur-Bold.woff2
        KaTeX_Fraktur-Regular.ttf
        KaTeX_Fraktur-Regular.woff
        KaTeX_Fraktur-Regular.woff2
        KaTeX_Main-Bold.ttf
        KaTeX_Main-Bold.woff
        KaTeX_Main-Bold.woff2
        KaTeX_Main-BoldItalic.ttf
        KaTeX_Main-BoldItalic.woff
        KaTeX_Main-BoldItalic.woff2
        KaTeX_Main-Italic.ttf
        KaTeX_Main-Italic.woff
        KaTeX_Main-Italic.woff2
        KaTeX_Main-Regular.ttf
        KaTeX_Main-Regular.woff
        KaTeX_Main-Regular.woff2
        KaTeX_Math-BoldItalic.ttf
        KaTeX_Math-BoldItalic.woff
        KaTeX_Math-BoldItalic.woff2
        KaTeX_Math-Italic.ttf
        KaTeX_Math-Italic.woff
        KaTeX_Math-Italic.woff2
        KaTeX_SansSerif-Bold.ttf
        KaTeX_SansSerif-Bold.woff
        KaTeX_SansSerif-Bold.woff2
        KaTeX_SansSerif-Italic.ttf
        KaTeX_SansSerif-Italic.woff
        KaTeX_SansSerif-Italic.woff2
        KaTeX_SansSerif-Regular.ttf
        KaTeX_SansSerif-Regular.woff
        KaTeX_SansSerif-Regular.woff2
        KaTeX_Script-Regular.ttf
        KaTeX_Script-Regular.woff
        KaTeX_Script-Regular.woff2
        KaTeX_Size1-Regular.ttf
        KaTeX_Size1-Regular.woff
        KaTeX_Size1-Regular.woff2
        KaTeX_Size2-Regular.ttf
        KaTeX_Size2-Regular.woff
        KaTeX_Size2-Regular.woff2
        KaTeX_Size3-Regular.ttf
        KaTeX_Size3-Regular.woff
        KaTeX_Size3-Regular.woff2
        KaTeX_Size4-Regular.ttf
        KaTeX_Size4-Regular.woff
        KaTeX_Size4-Regular.woff2
        KaTeX_Typewriter-Regular.ttf
        KaTeX_Typewriter-Regular.woff
        KaTeX_Typewriter-Regular.woff2
      katex.css
      katex.js
      katex.min.css
      katex.min.js
      katex.mjs
      README.md
    .nojekyll
  .eslintignore
  .eslintrc
  .gitignore
  .prettierignore
  .prettierrc.json
  babel.config.js
  build-cli-usage.sh
  build.sh
  convert-ascii-to-svg.sh
  deploy.sh
  docusaurus.config.js
  offline-cmd-md-links.sh
  package.json
  README.md
  set-solana-release-tag.sh
  sidebars.js
dos/
  src/
    cli.rs
    lib.rs
    main.rs
  Cargo.toml
download-utils/
  src/
    lib.rs
  Cargo.toml
entry/
  benches/
    entry_sigverify.rs
  src/
    entry.rs
    lib.rs
    poh.rs
    wincode.rs
  Cargo.toml
faucet/
  src/
    bin/
      faucet.rs
    faucet_mock.rs
    faucet.rs
    lib.rs
  tests/
    local-faucet.rs
  .gitignore
  Cargo.toml
feature-set/
  src/
    lib.rs
  Cargo.toml
fee/
  src/
    lib.rs
  Cargo.toml
fs/
  src/
    io_uring/
      dir_remover.rs
      file_creator.rs
      memory.rs
      mod.rs
      sequential_file_reader.rs
    buffered_reader.rs
    dirs.rs
    file_io.rs
    lib.rs
  Cargo.toml
genesis/
  src/
    address_generator.rs
    genesis_accounts.rs
    lib.rs
    main.rs
    stakes.rs
    unlocks.rs
  .gitignore
  Cargo.toml
  README.md
genesis-utils/
  src/
    lib.rs
    open.rs
  Cargo.toml
geyser-plugin-interface/
  src/
    geyser_plugin_interface.rs
    lib.rs
  Cargo.toml
  README.md
geyser-plugin-manager/
  src/
    accounts_update_notifier.rs
    block_metadata_notifier_interface.rs
    block_metadata_notifier.rs
    entry_notifier.rs
    geyser_plugin_manager.rs
    geyser_plugin_service.rs
    lib.rs
    slot_status_notifier.rs
    slot_status_observer.rs
    transaction_notifier.rs
  Cargo.toml
gossip/
  benches/
    crds_gossip_pull.rs
    crds_shards.rs
    crds.rs
    weighted_shuffle.rs
  src/
    cluster_info_metrics.rs
    cluster_info.rs
    contact_info.rs
    crds_data.rs
    crds_entry.rs
    crds_filter.rs
    crds_gossip_error.rs
    crds_gossip_pull.rs
    crds_gossip_push.rs
    crds_gossip.rs
    crds_shards.rs
    crds_value.rs
    crds.rs
    deprecated.rs
    duplicate_shred_handler.rs
    duplicate_shred_listener.rs
    duplicate_shred.rs
    epoch_slots.rs
    epoch_specs.rs
    gossip_error.rs
    gossip_service.rs
    legacy_contact_info.rs
    lib.rs
    node.rs
    ping_pong.rs
    protocol.rs
    push_active_set.rs
    received_cache.rs
    restart_crds_values.rs
    tlv.rs
    weighted_shuffle.rs
    wire_format_tests.rs
  tests/
    crds_gossip.rs
    gossip.rs
  .gitignore
  Cargo.toml
gossip-bin/
  src/
    main.rs
  Cargo.toml
install/
  src/
    bin/
      agave-install-init.rs
    build_env.rs
    command.rs
    config.rs
    defaults.rs
    lib.rs
    main.rs
    stop_process.rs
    update_manifest.rs
  .gitignore
  agave-install-init.sh
  build.rs
  Cargo.toml
  install-help.sh
io-uring/
  src/
    lib.rs
    ring.rs
    slab.rs
  Cargo.toml
jito-protos/
  src/
    lib.rs
  build.rs
  Cargo.toml
keygen/
  src/
    keygen.rs
  .gitignore
  Cargo.toml
lattice-hash/
  benches/
    bench_lt_hash.rs
  src/
    lib.rs
    lt_hash.rs
  Cargo.toml
ledger/
  benches/
    blockstore.rs
    make_shreds_from_entries.rs
    protobuf.rs
  proptest-regressions/
    blockstore_meta.txt
  src/
    blockstore/
      blockstore_purge.rs
      column.rs
      error.rs
    leader_schedule/
      identity_keyed.rs
      vote_keyed.rs
    shred/
      common.rs
      merkle_tree.rs
      merkle.rs
      payload.rs
      shred_code.rs
      shred_data.rs
      stats.rs
      traits.rs
      wire.rs
    ancestor_iterator.rs
    bank_forks_utils.rs
    bigtable_delete.rs
    bigtable_upload_service.rs
    bigtable_upload.rs
    bit_vec.rs
    block_error.rs
    blockstore_cleanup_service.rs
    blockstore_db.rs
    blockstore_meta.rs
    blockstore_metric_report_service.rs
    blockstore_metrics.rs
    blockstore_options.rs
    blockstore_processor.rs
    blockstore.rs
    entry_notifier_interface.rs
    entry_notifier_service.rs
    genesis_utils.rs
    leader_schedule_cache.rs
    leader_schedule_utils.rs
    leader_schedule.rs
    lib.rs
    next_slots_iterator.rs
    rooted_slot_iterator.rs
    shred.rs
    shredder.rs
    sigverify_shreds.rs
    slot_stats.rs
    staking_utils.rs
    token_balances.rs
    transaction_address_lookup_table_scanner.rs
    transaction_balances.rs
    use_snapshot_archives_at_startup.rs
    wire_format_tests.rs
  tests/
    blockstore.rs
    shred.rs
  .gitignore
  Cargo.toml
ledger-tool/
  src/
    args.rs
    bigtable.rs
    blockstore.rs
    error.rs
    ledger_path.rs
    ledger_utils.rs
    main.rs
    output.rs
    program.rs
  tests/
    basic.rs
  .gitignore
  Cargo.toml
local-cluster/
  src/
    cluster_tests.rs
    cluster.rs
    integration_tests.rs
    lib.rs
    local_cluster_snapshot_utils.rs
    local_cluster.rs
    validator_configs.rs
  tests/
    local_cluster.rs
  .gitignore
  Cargo.toml
logger/
  src/
    lib.rs
  Cargo.toml
measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  .gitignore
  Cargo.toml
merkle-tree/
  src/
    lib.rs
    merkle_tree.rs
  .gitignore
  Cargo.toml
metrics/
  benches/
    metrics.rs
  scripts/
    grafana-provisioning/
      dashboards/
        cluster-monitor.json
        dashboard.yml
    .gitignore
    adjust-dashboard-for-channel.py
    enable.sh
    grafana.ini
    influxdb.conf
    README.md
    start.sh
    status.sh
    stop.sh
    test.sh
  src/
    counter.rs
    datapoint.rs
    lib.rs
    metrics.rs
  .gitignore
  Cargo.toml
  README.md
multinode-demo/
  bench-tps.sh
  bootstrap-validator.sh
  common.sh
  delegate-stake.sh
  faucet.sh
  setup-from-mainnet-beta.sh
  setup-from-testnet.sh
  setup.sh
  validator-x.sh
  validator.sh
net/
  remote/
    cleanup.sh
    README.md
    remote-client.sh
    remote-deploy-update.sh
    remote-node-wait-init.sh
    remote-node.sh
    remote-sanity.sh
  scripts/
    azure-provider.sh
    colo_nodes
    colo-node-onacquire.sh
    colo-node-onfree.sh
    colo-provider.sh
    colo-utils.sh
    create-solana-user.sh
    disable-background-upgrades.sh
    ec2-provider.sh
    ec2-security-group-config.json
    gce-provider.sh
    gce-self-destruct.sh
    install-ag.sh
    install-at.sh
    install-certbot.sh
    install-docker.sh
    install-earlyoom.sh
    install-iftop.sh
    install-jq.sh
    install-libssl.sh
    install-perf.sh
    install-rsync.sh
    localtime.sh
    mount-additional-disk.sh
    network-config.sh
    remove-docker-interface.sh
    rsync-retry.sh
  .gitignore
  common.sh
  gce.sh
  init-metrics.sh
  net.sh
  scp.sh
  ssh.sh
net-utils/
  benches/
    token_bucket.rs
  src/
    ip_echo_client.rs
    ip_echo_server.rs
    lib.rs
    multihomed_sockets.rs
    socket_addr_space.rs
    sockets.rs
    token_bucket.rs
    tooling_for_tests.rs
  .gitignore
  Cargo.toml
notifier/
  src/
    lib.rs
  .gitignore
  Cargo.toml
perf/
  benches/
    dedup.rs
    discard.rs
    recycler.rs
    reset.rs
    shrink.rs
    sigverify.rs
  src/
    data_budget.rs
    deduper.rs
    discard.rs
    lib.rs
    packet.rs
    perf_libs.rs
    recycled_vec.rs
    recycler_cache.rs
    recycler.rs
    sigverify.rs
    test_tx.rs
    thread.rs
  build.rs
  Cargo.toml
platform-tools-sdk/
  cargo-build-sbf/
    src/
      main.rs
      post_processing.rs
      toolchain.rs
      utils.rs
    tests/
      crates/
        fail/
          src/
            lib.rs
          Cargo.toml
        noop/
          src/
            lib.rs
          Cargo.toml
        package-metadata/
          src/
            lib.rs
          Cargo.toml
        workspace-metadata/
          src/
            lib.rs
          Cargo.toml
      crates.rs
    .gitignore
    Cargo.toml
  cargo-test-sbf/
    src/
      main.rs
    Cargo.toml
  gen-headers/
    src/
      main.rs
    Cargo.toml
  sbf/
    c/
      inc/
        sol/
          inc/
            alt_bn128_compression.inc
            alt_bn128.inc
            assert.inc
            big_mod_exp.inc
            blake3.inc
            compute_units.inc
            cpi.inc
            keccak.inc
            last_restart_slot.inc
            log.inc
            poseidon.inc
            pubkey.inc
            return_data.inc
            secp256k1.inc
            sha.inc
          alt_bn128_compression.h
          alt_bn128.h
          assert.h
          big_mod_exp.h
          blake3.h
          compute_units.h
          constants.h
          cpi.h
          deserialize_deprecated.h
          deserialize.h
          entrypoint.h
          keccak.h
          last_restart_slot.h
          log.h
          poseidon.h
          pubkey.h
          return_data.h
          secp256k1.h
          sha.h
          string.h
          types.h
        sys/
          param.h
        deserialize_deprecated.h
        solana_sdk.h
        stdio.h
        stdlib.h
        string.h
        wchar.h
      README.md
      sbf.ld
      sbf.mk
    scripts/
      dump.sh
      install.sh
      objcopy.sh
      package.sh
      strip.sh
    .gitignore
    env.sh
poh/
  benches/
    poh_verify.rs
    poh.rs
    transaction_recorder.rs
  src/
    lib.rs
    poh_controller.rs
    poh_recorder.rs
    poh_service.rs
    record_channels.rs
    transaction_recorder.rs
  .gitignore
  Cargo.toml
poh-bench/
  src/
    main.rs
  Cargo.toml
poseidon/
  src/
    legacy.rs
    lib.rs
  Cargo.toml
precompiles/
  benches/
    ed25519_instructions.rs
    secp256k1_instructions.rs
    secp256r1_instructions.rs
  src/
    ed25519.rs
    lib.rs
    secp256k1.rs
    secp256r1.rs
  Cargo.toml
program-binaries/
  src/
    programs/
      core_bpf_address_lookup_table-3.0.0.so
      core_bpf_config-3.0.0.so
      core_bpf_feature_gate-0.0.1.so
      core_bpf_stake-1.0.1.so
      spl_associated_token_account-1.1.1.so
      spl_memo-1.0.0.so
      spl_memo-3.0.0.so
      spl_token_2022-8.0.0.so
      spl_token-3.5.0.so
      spl-jito_tip_distribution-0.1.10.so
      spl-jito_tip_distribution-0.1.4.so
      spl-jito_tip_distribution-0.1.7.so
      spl-jito_tip_payment-0.1.10.so
      spl-jito_tip_payment-0.1.4.so
      spl-jito_tip_payment-0.1.7.so
    lib.rs
  Cargo.toml
program-runtime/
  src/
    cpi.rs
    execution_budget.rs
    invoke_context.rs
    lib.rs
    loaded_programs.rs
    mem_pool.rs
    memory.rs
    serialization.rs
    stable_log.rs
    sysvar_cache.rs
  Cargo.toml
program-test/
  src/
    lib.rs
  tests/
    fixtures/
      noop_program.so
    bpf.rs
    builtins.rs
    compute_units.rs
    core_bpf.rs
    cpi.rs
    fuzz.rs
    genesis_accounts.rs
    lamports.rs
    panic.rs
    realloc.rs
    return_data.rs
    setup.rs
    spl.rs
    sysvar_last_restart_slot.rs
    sysvar.rs
    warp.rs
  Cargo.toml
programs/
  bpf_loader/
    benches/
      bpf_loader_upgradeable.rs
      serialization.rs
    src/
      lib.rs
    test_elfs/
      out/
        noop_aligned.so
        noop_unaligned.so
        sbpfv0_verifier_err.so
        sbpfv3_return_err.so
        sbpfv3_return_ok.so
      src/
        noop_aligned/
          noop_aligned.c
        noop_unaligned/
          noop_unaligned.c
      makefile
    Cargo.toml
  bpf-loader-tests/
    tests/
      common.rs
      extend_program_ix.rs
    Cargo.toml
    noop.so
  compute-budget/
    src/
      lib.rs
    Cargo.toml
  compute-budget-bench/
    benches/
      compute_budget.rs
    Cargo.toml
  ed25519-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  loader-v4/
    src/
      lib.rs
    Cargo.toml
  sbf/
    benches/
      bpf_loader.rs
    c/
      src/
        alloc/
          alloc.c
        alt_bn128/
          alt_bn128.c
        alt_bn128_compression/
          alt_bn128.c
        bench_alu/
          bench_alu.c
          test_bench_alu.c
        big_mod_exp/
          big_mod_exp.c
        deprecated_loader/
          deprecated_loader.c
        dup_accounts/
          dup_accounts.c
        error_handling/
          error_handling.c
        float/
          float.c
        invoke/
          invoke.c
        invoked/
          instruction.h
          invoked.c
        log_data/
          log_data.c
        move_funds/
          move_funds.c
        multiple_static/
          multiple_static.c
        noop/
          noop.c
        noop++/
          noop++.cc
        panic/
          panic.c
        poseidon/
          poseidon.c
        read_program/
          read_program.c
        relative_call/
          relative_call.c
        remaining_compute_units/
          remaining_compute_units.c
        return_data/
          return_data.c
        sanity/
          sanity.c
        sanity++/
          sanity++.cc
        sbf_to_sbf/
          entrypoint.c
          helper.c
          helper.h
        secp256k1_recover/
          secp256k1_recover.c
        ser/
          ser.c
        sha/
          sha.c
        stdlib/
          stdlib.c
        struct_pass/
          struct_pass.c
        struct_ret/
          struct_ret.c
        tuner/
          tuner.c
        tuner-variable-iterations/
          tuner-variable-iterations.c
      .gitignore
    rust/
      128bit/
        src/
          lib.rs
        Cargo.toml
      128bit_dep/
        src/
          lib.rs
        Cargo.toml
      account_mem/
        src/
          lib.rs
        Cargo.toml
      account_mem_deprecated/
        src/
          lib.rs
        Cargo.toml
      alloc/
        src/
          lib.rs
        Cargo.toml
      alt_bn128/
        src/
          lib.rs
        Cargo.toml
      alt_bn128_compression/
        src/
          lib.rs
        Cargo.toml
      big_mod_exp/
        src/
          lib.rs
        Cargo.toml
      call_args/
        src/
          lib.rs
        Cargo.toml
      call_depth/
        src/
          lib.rs
        Cargo.toml
      caller_access/
        src/
          lib.rs
        Cargo.toml
      curve25519/
        src/
          lib.rs
        Cargo.toml
      custom_heap/
        src/
          lib.rs
        Cargo.toml
      dep_crate/
        src/
          lib.rs
        Cargo.toml
      deprecated_loader/
        src/
          lib.rs
        Cargo.toml
      divide_by_zero/
        src/
          lib.rs
        Cargo.toml
      dup_accounts/
        src/
          lib.rs
        Cargo.toml
      error_handling/
        src/
          lib.rs
        Cargo.toml
      external_spend/
        src/
          lib.rs
        Cargo.toml
      get_minimum_delegation/
        src/
          lib.rs
        Cargo.toml
      inner_instruction_alignment_check/
        src/
          lib.rs
        Cargo.toml
      instruction_introspection/
        src/
          lib.rs
        Cargo.toml
      invoke/
        src/
          lib.rs
        Cargo.toml
      invoke_and_error/
        src/
          lib.rs
        Cargo.toml
      invoke_and_ok/
        src/
          lib.rs
        Cargo.toml
      invoke_and_return/
        src/
          lib.rs
        Cargo.toml
      invoke_dep/
        src/
          lib.rs
        Cargo.toml
      invoked/
        src/
          lib.rs
        Cargo.toml
      invoked_dep/
        src/
          lib.rs
        Cargo.toml
      iter/
        src/
          lib.rs
        Cargo.toml
      log_data/
        src/
          lib.rs
        Cargo.toml
      many_args/
        src/
          helper.rs
          lib.rs
        Cargo.toml
      many_args_dep/
        src/
          lib.rs
        Cargo.toml
      mem/
        src/
          lib.rs
        Cargo.toml
      mem_dep/
        src/
          lib.rs
        Cargo.toml
      membuiltins/
        src/
          lib.rs
        Cargo.toml
      noop/
        src/
          lib.rs
        Cargo.toml
      panic/
        src/
          lib.rs
        Cargo.toml
      param_passing/
        src/
          lib.rs
        Cargo.toml
      param_passing_dep/
        src/
          lib.rs
        Cargo.toml
      poseidon/
        src/
          lib.rs
        Cargo.toml
      r2_instruction_data_pointer/
        src/
          lib.rs
        Cargo.toml
      rand/
        src/
          lib.rs
        Cargo.toml
      realloc/
        src/
          lib.rs
        Cargo.toml
      realloc_dep/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke_dep/
        src/
          lib.rs
        Cargo.toml
      remaining_compute_units/
        src/
          lib.rs
        Cargo.toml
      ro_account_modify/
        src/
          lib.rs
        Cargo.toml
      ro_modify/
        src/
          lib.rs
        Cargo.toml
      sanity/
        src/
          lib.rs
        Cargo.toml
      secp256k1_recover/
        src/
          lib.rs
        Cargo.toml
      sha/
        src/
          lib.rs
        Cargo.toml
      sibling_inner_instructions/
        src/
          lib.rs
        Cargo.toml
      sibling_instructions/
        src/
          lib.rs
        Cargo.toml
      simulation/
        src/
          lib.rs
        Cargo.toml
      spoof1/
        src/
          lib.rs
        Cargo.toml
      spoof1_system/
        src/
          lib.rs
        Cargo.toml
      syscall-get-epoch-stake/
        src/
          lib.rs
        Cargo.toml
      sysvar/
        src/
          lib.rs
        Cargo.toml
      upgradeable/
        src/
          lib.rs
        Cargo.toml
      upgraded/
        src/
          lib.rs
        Cargo.toml
    tests/
      programs.rs
      simulation.rs
      syscall_get_epoch_stake.rs
      sysvar.rs
    .gitignore
    Cargo.toml
    Makefile
  system/
    benches/
      system.rs
    src/
      lib.rs
      system_instruction.rs
      system_processor.rs
    Cargo.toml
  vote/
    benches/
      process_vote.rs
      vote_instructions.rs
    src/
      vote_state/
        handler.rs
        mod.rs
      lib.rs
      vote_processor.rs
    Cargo.toml
  zk-elgamal-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
  zk-elgamal-proof-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  zk-token-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
pubsub-client/
  src/
    nonblocking/
      mod.rs
      pubsub_client.rs
    lib.rs
    pubsub_client.rs
  Cargo.toml
quic-client/
  src/
    nonblocking/
      mod.rs
      quic_client.rs
    lib.rs
    quic_client.rs
  tests/
    quic_client.rs
  Cargo.toml
rayon-threadlimit/
  src/
    lib.rs
  .gitignore
  Cargo.toml
rbpf-cli/
  src/
    main.rs
  Cargo.toml
remote-wallet/
  src/
    ledger_error.rs
    ledger.rs
    lib.rs
    locator.rs
    remote_keypair.rs
    remote_wallet.rs
  Cargo.toml
  README.md
reserved-account-keys/
  src/
    lib.rs
  Cargo.toml
rpc/
  src/
    rpc/
      account_resolver.rs
    cluster_tpu_info.rs
    filter.rs
    lib.rs
    max_slots.rs
    optimistically_confirmed_bank_tracker.rs
    parsed_token_accounts.rs
    rpc_cache.rs
    rpc_completed_slots_service.rs
    rpc_health.rs
    rpc_pubsub_service.rs
    rpc_pubsub.rs
    rpc_service.rs
    rpc_subscription_tracker.rs
    rpc_subscriptions.rs
    rpc.rs
    slot_status_notifier.rs
    transaction_notifier_interface.rs
    transaction_status_service.rs
  .gitignore
  Cargo.toml
rpc-client/
  src/
    nonblocking/
      mod.rs
      rpc_client.rs
    http_sender.rs
    lib.rs
    mock_sender.rs
    rpc_client.rs
    rpc_sender.rs
    spinner.rs
  Cargo.toml
rpc-client-api/
  src/
    bundles.rs
    client_error.rs
    custom_error.rs
    lib.rs
    response.rs
  Cargo.toml
rpc-client-nonce-utils/
  src/
    nonblocking/
      blockhash_query.rs
      mod.rs
    blockhash_query.rs
    lib.rs
  Cargo.toml
rpc-client-types/
  src/
    config.rs
    error_object.rs
    filter.rs
    lib.rs
    request.rs
    response.rs
  Cargo.toml
rpc-test/
  tests/
    nonblocking.rs
    rpc.rs
  .gitignore
  Cargo.toml
runtime/
  benches/
    accounts.rs
    prioritization_fee_cache.rs
    status_cache.rs
  src/
    accounts_background_service/
      pending_snapshot_packages.rs
      stats.rs
    bank/
      builtins/
        core_bpf_migration/
          error.rs
          mod.rs
          source_buffer.rs
          target_bpf_v2.rs
          target_builtin.rs
          target_core_bpf.rs
        mod.rs
      partitioned_epoch_rewards/
        calculation.rs
        distribution.rs
        epoch_rewards_hasher.rs
        mod.rs
        sysvar.rs
      accounts_lt_hash.rs
      address_lookup_table.rs
      bank_hash_details.rs
      check_transactions.rs
      fee_distribution.rs
      metrics.rs
      recent_blockhashes_account.rs
      serde_snapshot.rs
      sysvar_cache.rs
      tests.rs
    inflation_rewards/
      mod.rs
      points.rs
    serde_snapshot/
      obsolete_accounts.rs
      status_cache.rs
      storage.rs
      tests.rs
      types.rs
      utils.rs
    snapshot_package/
      compare.rs
    snapshot_utils/
      snapshot_storage_rebuilder.rs
    stakes/
      serde_stakes.rs
    account_saver.rs
    accounts_background_service.rs
    bank_client.rs
    bank_forks.rs
    bank_hash_cache.rs
    bank_utils.rs
    bank.rs
    commitment.rs
    dependency_tracker.rs
    epoch_stakes.rs
    genesis_utils.rs
    installed_scheduler_pool.rs
    lib.rs
    loader_utils.rs
    non_circulating_supply.rs
    prioritization_fee_cache.rs
    prioritization_fee.rs
    read_optimized_dashmap.rs
    rent_collector.rs
    runtime_config.rs
    serde_snapshot.rs
    snapshot_bank_utils.rs
    snapshot_controller.rs
    snapshot_minimizer.rs
    snapshot_package.rs
    snapshot_utils.rs
    stake_account.rs
    stake_history.rs
    stake_utils.rs
    stake_weighted_timestamp.rs
    stakes.rs
    static_ids.rs
    status_cache.rs
    transaction_batch.rs
    vote_sender_types.rs
  .gitignore
  Cargo.toml
runtime-transaction/
  benches/
    get_signature_details.rs
  src/
    runtime_transaction/
      sdk_transactions.rs
      transaction_view.rs
    instruction_data_len.rs
    instruction_meta.rs
    lib.rs
    runtime_transaction.rs
    signature_details.rs
    transaction_meta.rs
    transaction_with_meta.rs
  Cargo.toml
scheduler-bindings/
  src/
    lib.rs
  Cargo.toml
scheduling-utils/
  src/
    handshake/
      client.rs
      mod.rs
      server.rs
      shared.rs
      tests.rs
    error.rs
    lib.rs
    pubkeys_ptr.rs
    responses_region.rs
    thread_aware_account_locks.rs
    transaction_ptr.rs
  Cargo.toml
scripts/
  agave-build-lists.sh
  agave-install-deploy.sh
  agave-install-update-manifest-keypair.sh
  build-agave-xdp-ebpf.sh
  build-downstream-anchor-projects.sh
  cargo-clippy-nightly.sh
  cargo-clippy.sh
  cargo-for-all-lock-files.sh
  cargo-install-all.sh
  check-dev-context-only-utils.sh
  configure-metrics.sh
  confirm-cargo-version-numbers-before-bump.sh
  coverage.sh
  create-release-tarball.sh
  elf-hash-symbol.sh
  fd-monitor.sh
  generate-target-triple.sh
  iftop.sh
  increment-cargo-version.sh
  metrics-write-datapoint.sh
  net-shaper.sh
  net-stats.sh
  oom-monitor.sh
  oom-score-adj.sh
  patch-crates.sh
  patch-spl-crates-for-anchor.sh
  perf-plot.py
  perf-stats.py
  read-cargo-variable.sh
  reserve-cratesio-package-name.sh
  run.sh
  sed-i-all-rs-files-for-rust-analyzer.sh
  spl-token-cli-version.sh
  system-stats.sh
  ulimit-n.sh
  wallet-sanity.sh
sdk/
  README.md
send-transaction-service/
  src/
    lib.rs
    send_transaction_service_stats.rs
    send_transaction_service.rs
    test_utils.rs
    tpu_info.rs
    transaction_client.rs
  Cargo.toml
snapshots/
  src/
    archive_format.rs
    archive.rs
    error.rs
    hardened_unpack.rs
    kind.rs
    lib.rs
    paths.rs
    snapshot_archive_info.rs
    snapshot_config.rs
    snapshot_hash.rs
    snapshot_interval.rs
    snapshot_version.rs
    unarchive.rs
  Cargo.toml
stake-accounts/
  src/
    arg_parser.rs
    args.rs
    main.rs
    stake_accounts.rs
  Cargo.toml
storage-bigtable/
  build-proto/
    src/
      main.rs
    .gitignore
    build.sh
    Cargo.toml
    README.md
  proto/
    google.api.rs
    google.bigtable.v2.rs
    google.protobuf.rs
    google.rpc.rs
  src/
    access_token.rs
    bigtable.rs
    compression.rs
    lib.rs
    pki-goog-roots.pem
    root_ca_certificate.rs
  Cargo.toml
  init-bigtable.sh
  README.md
storage-proto/
  proto/
    confirmed_block.proto
    entries.proto
    transaction_by_addr.proto
  src/
    convert.rs
    lib.rs
  build.rs
  Cargo.toml
  README.md
streamer/
  examples/
    swqos.rs
  src/
    nonblocking/
      connection_rate_limiter.rs
      mod.rs
      qos.rs
      quic.rs
      recvmmsg.rs
      sendmmsg.rs
      simple_qos.rs
      stream_throttle.rs
      swqos.rs
      testing_utilities.rs
    evicting_sender.rs
    lib.rs
    msghdr.rs
    packet.rs
    quic.rs
    recvmmsg.rs
    sendmmsg.rs
    streamer.rs
  tests/
    recvmmsg.rs
  Cargo.toml
svm/
  doc/
    diagrams/
      context.svg
      context.tex
    spec.md
  src/
    account_loader.rs
    account_overrides.rs
    lib.rs
    message_processor.rs
    nonce_info.rs
    program_loader.rs
    rent_calculator.rs
    rollback_accounts.rs
    transaction_account_state_info.rs
    transaction_balances.rs
    transaction_commit_result.rs
    transaction_error_metrics.rs
    transaction_execution_result.rs
    transaction_processing_callback.rs
    transaction_processing_result.rs
    transaction_processor.rs
  tests/
    example-programs/
      clock-sysvar/
        src/
          lib.rs
        Cargo.toml
        clock_sysvar_program.so
      hello-solana/
        src/
          lib.rs
        Cargo.toml
        hello_solana_program.so
      simple-transfer/
        src/
          lib.rs
        Cargo.toml
        simple_transfer_program.so
      transfer-from-account/
        src/
          lib.rs
        Cargo.toml
        transfer_from_account_program.so
      write-to-account/
        src/
          lib.rs
        Cargo.toml
        write_to_account_program.so
    concurrent_tests.rs
    integration_test.rs
    mock_bank.rs
  Cargo.toml
svm-callback/
  src/
    lib.rs
  Cargo.toml
svm-feature-set/
  src/
    lib.rs
  Cargo.toml
svm-log-collector/
  src/
    lib.rs
  Cargo.toml
svm-measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  Cargo.toml
svm-rent-calculator/
  src/
    lib.rs
    rent_state.rs
    svm_rent_calculator.rs
  Cargo.toml
svm-test-harness/
  bin/
    test_exec_instr.rs
  src/
    fixture/
      account_state.rs
      error.rs
      feature_set.rs
      instr_context.rs
      instr_effects.rs
      mod.rs
    file.rs
    fuzz.rs
    instr.rs
    lib.rs
    program_cache.rs
    sysvar_cache.rs
  .gitignore
  build.rs
  Cargo.toml
  Makefile
svm-timings/
  src/
    lib.rs
  Cargo.toml
svm-transaction/
  src/
    svm_message/
      sanitized_message.rs
      sanitized_transaction.rs
    svm_transaction/
      sanitized_transaction.rs
    instruction.rs
    lib.rs
    message_address_table_lookup.rs
    svm_message.rs
    svm_transaction.rs
    tests.rs
  Cargo.toml
svm-type-overrides/
  src/
    lib.rs
  Cargo.toml
syscalls/
  gen-syscall-list/
    src/
      main.rs
    build.rs
    Cargo.toml
  src/
    cpi.rs
    lib.rs
    logging.rs
    mem_ops.rs
    sysvar.rs
  Cargo.toml
test-validator/
  src/
    lib.rs
  Cargo.toml
thread-manager/
  examples/
    common/
      mod.rs
    core_contention_basics.rs
    core_contention_contending_set.toml
    core_contention_dedicated_set.toml
    core_contention_sweep.rs
  src/
    lib.rs
    native_thread_runtime.rs
    policy.rs
    rayon_runtime.rs
    tokio_runtime.rs
  Cargo.toml
  README.md
tls-utils/
  src/
    config.rs
    crypto_provider.rs
    lib.rs
    quic_client_certificate.rs
    skip_client_verification.rs
    skip_server_verification.rs
    tls_certificates.rs
  Cargo.toml
  README
tokens/
  src/
    arg_parser.rs
    args.rs
    commands.rs
    db.rs
    lib.rs
    main.rs
    spl_token.rs
    stake.rs
    token_display.rs
  tests/
    commands.rs
  .gitignore
  Cargo.toml
  README.md
tps-client/
  src/
    bank_client.rs
    lib.rs
    rpc_client.rs
    tpu_client.rs
    utils.rs
  Cargo.toml
tpu-client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    lib.rs
    tpu_client.rs
  .gitignore
  Cargo.toml
tpu-client-next/
  src/
    node_address_service/
      leader_tpu_cache_service.rs
      recent_leader_slots.rs
      slot_event.rs
      slot_receiver.rs
      slot_update_service.rs
    quic_networking/
      error.rs
    client_builder.rs
    connection_worker.rs
    connection_workers_scheduler.rs
    leader_updater.rs
    lib.rs
    logging.rs
    metrics.rs
    node_address_service.rs
    quic_networking.rs
    send_transaction_stats.rs
    transaction_batch.rs
    websocket_node_address_service.rs
    workers_cache.rs
  tests/
    connection_workers_scheduler_test.rs
  Cargo.toml
transaction-context/
  src/
    instruction_accounts.rs
    instruction.rs
    lib.rs
    transaction_accounts.rs
    vm_slice.rs
  Cargo.toml
transaction-dos/
  src/
    main.rs
  .gitignore
  Cargo.toml
transaction-metrics-tracker/
  src/
    lib.rs
  Cargo.toml
transaction-status/
  benches/
    extract_memos.rs
  src/
    parse_token/
      extension/
        confidential_mint_burn.rs
        confidential_transfer_fee.rs
        confidential_transfer.rs
        cpi_guard.rs
        default_account_state.rs
        group_member_pointer.rs
        group_pointer.rs
        interest_bearing_mint.rs
        memo_transfer.rs
        metadata_pointer.rs
        mint_close_authority.rs
        mod.rs
        pausable.rs
        permanent_delegate.rs
        reallocate.rs
        scaled_ui_amount.rs
        token_group.rs
        token_metadata.rs
        transfer_fee.rs
        transfer_hook.rs
    extract_memos.rs
    lib.rs
    parse_accounts.rs
    parse_address_lookup_table.rs
    parse_associated_token.rs
    parse_bpf_loader.rs
    parse_instruction.rs
    parse_stake.rs
    parse_system.rs
    parse_token.rs
    parse_vote.rs
    token_balances.rs
  Cargo.toml
transaction-status-client-types/
  src/
    lib.rs
    option_serializer.rs
  Cargo.toml
transaction-view/
  benches/
    bytes.rs
    transaction_view.rs
  src/
    address_table_lookup_frame.rs
    bytes.rs
    instructions_frame.rs
    lib.rs
    message_header_frame.rs
    resolved_transaction_view.rs
    result.rs
    sanitize.rs
    signature_frame.rs
    static_account_keys_frame.rs
    transaction_data.rs
    transaction_frame.rs
    transaction_version.rs
    transaction_view.rs
  Cargo.toml
turbine/
  benches/
    cluster_info.rs
    cluster_nodes.rs
  src/
    broadcast_stage/
      broadcast_duplicates_run.rs
      broadcast_fake_shreds_run.rs
      broadcast_metrics.rs
      broadcast_utils.rs
      fail_entry_verification_broadcast_run.rs
      standard_broadcast_run.rs
    addr_cache.rs
    broadcast_stage.rs
    cluster_nodes.rs
    lib.rs
    quic_endpoint.rs
    retransmit_stage.rs
    sigverify_shreds.rs
    xdp.rs
  Cargo.toml
udp-client/
  src/
    nonblocking/
      mod.rs
      udp_client.rs
    lib.rs
    udp_client.rs
  Cargo.toml
unified-scheduler-logic/
  src/
    lib.rs
  Cargo.toml
unified-scheduler-pool/
  src/
    lib.rs
    sleepless_testing.rs
  Cargo.toml
validator/
  src/
    bin/
      solana-test-validator.rs
    cli/
      thread_args.rs
    commands/
      authorized_voter/
        mod.rs
      bam/
        mod.rs
      block_engine/
        mod.rs
      contact_info/
        mod.rs
      exit/
        mod.rs
      manage_block_production/
        mod.rs
      monitor/
        mod.rs
      plugin/
        mod.rs
      relayer/
        mod.rs
      repair_shred_from_peer/
        mod.rs
      repair_whitelist/
        mod.rs
      run/
        args/
          account_secondary_indexes.rs
          blockstore_options.rs
          json_rpc_config.rs
          pub_sub_config.rs
          rpc_bigtable_config.rs
          rpc_bootstrap_config.rs
          send_transaction_config.rs
        args.rs
        execute.rs
        mod.rs
      set_identity/
        mod.rs
      set_log_filter/
        mod.rs
      set_public_address/
        mod.rs
      shred/
        mod.rs
      staked_nodes_overrides/
        mod.rs
      wait_for_restart_window/
        mod.rs
      mod.rs
    admin_rpc_service.rs
    bootstrap.rs
    cli.rs
    dashboard.rs
    lib.rs
    main.rs
  tests/
    cli.rs
  .gitignore
  Cargo.toml
  solana-test-validator
verified-packet-receiver/
  src/
    lib.rs
    receiver.rs
  Cargo.toml
  Readme.md
version/
  src/
    legacy.rs
    lib.rs
  .gitignore
  build.rs
  Cargo.toml
vortexor/
  src/
    cli.rs
    lib.rs
    main.rs
    rpc_load_balancer.rs
    sender.rs
    stake_updater.rs
    vortexor.rs
  tests/
    vortexor.rs
  Cargo.toml
  README.md
vote/
  benches/
    vote_account.rs
  src/
    vote_state_view/
      field_frames.rs
      frame_v1_14_11.rs
      frame_v3.rs
      frame_v4.rs
      list_view.rs
    lib.rs
    vote_account.rs
    vote_parser.rs
    vote_state_view.rs
    vote_transaction.rs
  Cargo.toml
votor/
  src/
    consensus_pool/
      certificate_builder.rs
      parent_ready_tracker.rs
      slot_stake_counters.rs
      stats.rs
      vote_pool.rs
    consensus_pool_service/
      stats.rs
    event_handler/
      stats.rs
    timer_manager/
      stats.rs
      timers.rs
    commitment.rs
    common.rs
    consensus_metrics.rs
    consensus_pool_service.rs
    consensus_pool.rs
    event_handler.rs
    event.rs
    lib.rs
    root_utils.rs
    staked_validators_cache.rs
    timer_manager.rs
    vote_history_storage.rs
    vote_history.rs
    voting_service.rs
    voting_utils.rs
    votor.rs
  Cargo.toml
votor-messages/
  src/
    consensus_message.rs
    lib.rs
    vote.rs
  Cargo.toml
watchtower/
  src/
    main.rs
  .gitignore
  Cargo.toml
  README.md
web3.js/
  README.md
wen-restart/
  proto/
    wen_restart.proto
  src/
    heaviest_fork_aggregate.rs
    last_voted_fork_slots_aggregate.rs
    lib.rs
    wen_restart.rs
  build.rs
  Cargo.toml
xdp/
  src/
    device.rs
    lib.rs
    netlink.rs
    packet.rs
    program.rs
    route.rs
    socket.rs
    tx_loop.rs
    umem.rs
  Cargo.toml
xdp-ebpf/
  src/
    bin/
      agave-xdp-prog.rs
    lib.rs
  agave-xdp-prog
  Cargo.toml
  README
zk-keygen/
  README.md
zk-sdk/
  README.md
zk-token-sdk/
  src/
    encryption/
      auth_encryption.rs
      decode_u32_precomputation_for_G.bincode
      discrete_log.rs
      elgamal.rs
      grouped_elgamal.rs
      mod.rs
      pedersen.rs
    instruction/
      batched_grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      batched_range_proof/
        batched_range_proof_u128.rs
        batched_range_proof_u256.rs
        batched_range_proof_u64.rs
        mod.rs
      grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      transfer/
        encryption.rs
        mod.rs
        with_fee.rs
        without_fee.rs
      ciphertext_ciphertext_equality.rs
      ciphertext_commitment_equality.rs
      errors.rs
      fee_sigma.rs
      mod.rs
      pubkey_validity.rs
      range_proof.rs
      withdraw.rs
      zero_balance.rs
    range_proof/
      errors.rs
      generators.rs
      inner_product.rs
      mod.rs
      util.rs
    sigma_proofs/
      batched_grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      ciphertext_ciphertext_equality_proof.rs
      ciphertext_commitment_equality_proof.rs
      errors.rs
      fee_proof.rs
      mod.rs
      pubkey_proof.rs
      zero_balance_proof.rs
    zk_token_elgamal/
      pod/
        auth_encryption.rs
        elgamal.rs
        grouped_elgamal.rs
        instruction.rs
        mod.rs
        pedersen.rs
        range_proof.rs
        sigma_proofs.rs
      convert.rs
      decryption.rs
      mod.rs
      ops.rs
    errors.rs
    lib.rs
    macros.rs
    transcript.rs
    zk_token_proof_instruction.rs
    zk_token_proof_program.rs
    zk_token_proof_state.rs
  .gitignore
  Cargo.toml
  README.md
.codecov.yml
.dockerignore
.gitignore
.gitmodules
.mergify.yml
bootstrap
cargo
cargo-build-sbf
cargo-test-sbf
Cargo.toml
CHANGELOG.md
clippy.toml
CONTRIBUTING.md
deploy_programs
f
fetch-core-bpf.sh
fetch-perf-libs.sh
fetch-programs.sh
fetch-spl.sh
legal.md
LICENSE
privacy.md
README.md
RELEASE.md
rust-toolchain.toml
rustfmt.toml
s
SECURITY.md
start
start_multi
vercel.json

================================================================
Files
================================================================

================
File: core/benches/banking_stage.rs
================
extern crate test;
⋮----
fn check_txs(receiver: &Arc<Receiver<WorkingBankEntry>>, ref_tx_count: usize) {
⋮----
if let Ok((_bank, (entry, _tick_height))) = receiver.recv_timeout(Duration::new(1, 0)) {
total += entry.transactions.len();
⋮----
if now.elapsed().as_secs() > 60 {
⋮----
assert_eq!(total, ref_tx_count);
⋮----
fn make_accounts_txs(txes: usize, mint_keypair: &Keypair, hash: Hash) -> Vec<Transaction> {
⋮----
.into_par_iter()
.map(|_| {
let mut new = dummy.clone();
let sig: [u8; 64] = std::array::from_fn(|_| thread_rng().gen::<u8>());
⋮----
new.signatures = vec![Signature::from(sig)];
⋮----
.collect()
⋮----
fn make_programs_txs(txes: usize, hash: Hash) -> Vec<Transaction> {
⋮----
let instructions: Vec<_> = repeat_with(|| {
⋮----
system_instruction::transfer(&from_key.pubkey(), &to_key, 1)
⋮----
.take(progs)
.collect();
let message = Message::new(&instructions, Some(&from_key.pubkey()));
⋮----
fn make_vote_txs(txes: usize) -> Vec<Transaction> {
⋮----
.map(|_| (Keypair::new(), Keypair::new()))
.unzip();
⋮----
.map(|i| {
⋮----
TowerSync::from(vec![(2, 1)])
⋮----
TowerSync::from(vec![(i as u64, 1)])
⋮----
new_tower_sync_transaction(
⋮----
enum TransactionType {
⋮----
fn bench_banking(
⋮----
let txes = PACKETS_PER_BATCH * num_threads.get() * CHUNKS;
⋮----
} = create_genesis_config(mint_total);
⋮----
} = banking_tracer.create_channels(false);
⋮----
let bank = bank_forks.read().unwrap().get(0).unwrap();
bank.write_cost_tracker()
.unwrap()
.set_limits(u64::MAX, u64::MAX, u64::MAX);
debug!("threads: {num_threads} txs: {txes}");
⋮----
make_accounts_txs(txes, &mint_keypair, genesis_config.hash())
⋮----
make_programs_txs(txes, genesis_config.hash())
⋮----
Some(make_vote_txs(txes))
⋮----
transactions.iter().for_each(|tx| {
⋮----
genesis_config.hash(),
⋮----
let x = bank.process_transaction(&fund);
x.unwrap();
⋮----
let res = bank.process_transaction(tx);
assert!(res.is_ok(), "sanity test transactions");
⋮----
bank.clear_signatures();
let res = bank.process_transactions(transactions.iter());
⋮----
assert!(r.is_ok(), "sanity parallel execution");
⋮----
let verified: Vec<_> = to_packet_batches(&transactions, PACKETS_PER_BATCH);
let vote_packets = vote_txs.map(|vote_txs| {
let mut packet_batches = to_packet_batches(&vote_txs, PACKETS_PER_BATCH);
for batch in packet_batches.iter_mut() {
for mut packet in batch.iter_mut() {
packet.meta_mut().set_simple_vote(true);
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path()).expect("Expected to be able to open database ledger"),
⋮----
create_test_recorder(bank.clone(), blockstore, None, None);
let (s, _r) = unbounded();
⋮----
poh_recorder.clone(),
⋮----
let chunk_len = verified.len() / CHUNKS;
⋮----
bencher.iter(move || {
⋮----
.send(BankingPacketBatch::new(
vote_packets[start..start + chunk_len].to_vec(),
⋮----
.unwrap();
⋮----
for v in verified[start..start + chunk_len].chunks(chunk_len / num_threads) {
debug!(
⋮----
sent += xv.len();
⋮----
.send(BankingPacketBatch::new(v.to_vec()))
⋮----
check_txs(&signal_receiver2, txes / CHUNKS);
⋮----
trace!(
⋮----
start %= verified.len();
⋮----
exit.store(true, Ordering::Relaxed);
poh_service.join().unwrap();
⋮----
fn bench_banking_stage_multi_accounts(bencher: &mut Bencher) {
bench_banking(
⋮----
fn bench_banking_stage_multi_programs(bencher: &mut Bencher) {
⋮----
fn bench_banking_stage_multi_accounts_with_voting(bencher: &mut Bencher) {
⋮----
fn bench_banking_stage_multi_programs_with_voting(bencher: &mut Bencher) {
⋮----
fn simulate_process_entries(
⋮----
let slot = bank.slot();
⋮----
let bank = bank_fork.read().unwrap().get_with_scheduler(slot).unwrap();
bank.clone_without_scheduler()
.set_fork_graph_in_program_cache(Arc::downgrade(&bank_fork));
⋮----
bank.transfer(initial_lamports, mint_keypair, &keypairs[i * 2].pubkey())
⋮----
for i in (0..num_accounts).step_by(2) {
tx_vector.push(
⋮----
&keypairs[i + 1].pubkey(),
⋮----
bank.last_blockhash(),
⋮----
.into(),
⋮----
hash: next_hash(&bank.last_blockhash(), 1, &tx_vector),
⋮----
process_entries_for_tests(&bank, vec![entry], None, None).unwrap();
⋮----
fn bench_process_entries(bencher: &mut Bencher) {
⋮----
} = create_genesis_config((num_accounts + 1) as u64 * initial_lamports);
let keypairs: Vec<Keypair> = repeat_with(Keypair::new).take(num_accounts).collect();
⋮----
bencher.iter(|| {
simulate_process_entries(
⋮----
tx_vector.clone(),

================
File: core/benches/banking_trace.rs
================
extern crate test;
⋮----
fn ensure_fresh_setup_to_benchmark(path: &PathBuf) {
BankingTracer::ensure_cleanup_path(path).unwrap();
⋮----
fn black_box_packet_batch(packet_batch: BankingPacketBatch) -> TracerThreadResult {
⋮----
Ok(())
⋮----
fn bench_banking_tracer_main_thread_overhead_noop_baseline(bencher: &mut Bencher) {
⋮----
} = tracer.create_channels(false);
let exit_for_dummy_thread = exit.clone();
⋮----
let packet_batch = sample_packet_batch();
bencher.iter(|| {
non_vote_sender.send(packet_batch.clone()).unwrap();
⋮----
terminate_tracer(tracer, None, dummy_main_thread, non_vote_sender, Some(exit));
⋮----
fn bench_banking_tracer_main_thread_overhead_under_peak_write(bencher: &mut Bencher) {
let temp_dir = TempDir::new().unwrap();
⋮----
let (tracer, tracer_thread) = BankingTracer::new(Some((
&temp_dir.path().join("banking-trace"),
exit.clone(),
⋮----
.unwrap();
⋮----
terminate_tracer(
⋮----
Some(exit),
⋮----
drop_and_clean_temp_dir_unless_suppressed(temp_dir);
⋮----
fn bench_banking_tracer_main_thread_overhead_under_sustained_write(bencher: &mut Bencher) {
⋮----
fn bench_banking_tracer_background_thread_throughput(bencher: &mut Bencher) {
⋮----
let base_path = temp_dir.path();
⋮----
bencher.iter(move || {
let path = base_path.join("banking-trace");
ensure_fresh_setup_to_benchmark(&path);
⋮----
BankingTracer::new(Some((&path, exit.clone(), 50 * 1024 * 1024))).unwrap();

================
File: core/benches/consensus.rs
================
extern crate solana_core;
extern crate test;
⋮----
fn bench_save_tower(bench: &mut Bencher) {
let dir = TempDir::new().unwrap();
⋮----
.read()
.unwrap()
.working_bank();
let tower_storage = FileTowerStorage::new(dir.path().to_path_buf());
⋮----
&node_keypair.pubkey(),
⋮----
bench.iter(move || {
tower.save(&tower_storage, &node_keypair).unwrap();
⋮----
fn bench_generate_ancestors_descendants(bench: &mut Bencher) {
⋮----
let forks = tr(0);
⋮----
vote_simulator.fill_bank_forks(forks, &HashMap::new(), true);
vote_simulator.create_and_vote_new_branch(
⋮----
let _ancestors = vote_simulator.bank_forks.read().unwrap().ancestors();
let _descendants = vote_simulator.bank_forks.read().unwrap().descendants();

================
File: core/benches/consumer.rs
================
extern crate test;
fn create_accounts(num: usize) -> Vec<Keypair> {
(0..num).into_par_iter().map(|_| Keypair::new()).collect()
⋮----
fn create_funded_accounts(bank: &Bank, num: usize) -> Vec<Keypair> {
assert!(
⋮----
let accounts = create_accounts(num);
accounts.par_iter().for_each(|account| {
bank.store_account(
&account.pubkey(),
⋮----
data: vec![],
⋮----
.to_account_shared_data(),
⋮----
fn create_transactions(bank: &Bank, num: usize) -> Vec<RuntimeTransaction<SanitizedTransaction>> {
let funded_accounts = create_funded_accounts(bank, 2 * num);
⋮----
.into_par_iter()
.chunks(2)
.map(|chunk| {
⋮----
system_transaction::transfer(from, &to.pubkey(), 1, bank.last_blockhash())
⋮----
.map(RuntimeTransaction::from_transaction_for_tests)
.collect()
⋮----
fn create_consumer(transaction_recorder: TransactionRecorder) -> Consumer {
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
struct BenchFrame {
⋮----
fn setup() -> BenchFrame {
⋮----
} = create_genesis_config(mint_total);
⋮----
bank.write_cost_tracker()
.unwrap()
.set_limits(u64::MAX, u64::MAX, u64::MAX);
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
let ledger_path = TempDir::new().unwrap();
⋮----
Blockstore::open(ledger_path.path()).expect("Expected to be able to open database ledger"),
⋮----
create_test_recorder(bank.clone(), blockstore, None, None);
⋮----
fn bench_process_and_record_transactions(bencher: &mut Bencher, batch_size: usize) {
⋮----
assert_eq!(
⋮----
} = setup();
let consumer = create_consumer(transaction_recorder);
let transactions = create_transactions(&bank, 2_usize.pow(20));
let mut transaction_iter = transactions.chunks(batch_size);
bencher.iter(move || {
⋮----
let summary = consumer.process_and_record_transactions(
⋮----
transaction_iter.next().unwrap(),
⋮----
assert!(summary
⋮----
exit.store(true, Ordering::Relaxed);
poh_service.join().unwrap();
⋮----
fn bench_process_and_record_transactions_unbatched(bencher: &mut Bencher) {
bench_process_and_record_transactions(bencher, 1);
⋮----
fn bench_process_and_record_transactions_half_batch(bencher: &mut Bencher) {
bench_process_and_record_transactions(bencher, 32);
⋮----
fn bench_process_and_record_transactions_full_batch(bencher: &mut Bencher) {
bench_process_and_record_transactions(bencher, 64);

================
File: core/benches/gen_keys.rs
================
extern crate test;
⋮----
fn bench_gen_keys(b: &mut Bencher) {
⋮----
b.iter(|| rnd.gen_n_keypairs(1000));

================
File: core/benches/proto_to_packet.rs
================
extern crate test;
⋮----
fn get_proto_packet(i: u8) -> PbPacket {
⋮----
data: repeat_n(i, PACKET_DATA_SIZE).collect(),
meta: Some(PbMeta {
⋮----
addr: "255.255.255.255:65535".to_string(),
⋮----
flags: Some(PbFlags {
⋮----
fn bench_proto_to_packet(bencher: &mut Bencher) {
bencher.iter(|| {
black_box(proto_packet_to_packet(get_proto_packet(1)));
⋮----
fn bench_batch_list_to_packets(bencher: &mut Bencher) {
⋮----
packets: (0..128).map(get_proto_packet).collect(),
⋮----
black_box(
⋮----
.iter()
.map(|p| proto_packet_to_packet(p.clone()))

================
File: core/benches/receive_and_buffer_utils.rs
================
fn create_accounts(num_accounts: usize, genesis_config: &mut GenesisConfig) -> Vec<Keypair> {
⋮----
let account_keypairs: Vec<Keypair> = (0..num_accounts).map(|_| Keypair::new()).collect();
for keypair in account_keypairs.iter() {
genesis_config.add_account(keypair.pubkey(), AccountSharedData::new(10000, 0, owner));
⋮----
pub struct FaultyBlockhash {
⋮----
impl FaultyBlockhash {
pub fn new(blockhash: Hash, probability_invalid_blockhash: f64) -> Self {
⋮----
pub fn get<R: Rng>(&self, rng: &mut R) -> Hash {
⋮----
fn generate_transactions(
⋮----
assert!(num_instructions_per_tx <= MAX_INSTRUCTIONS_PER_TRANSACTION);
⋮----
assert!(
⋮----
let blockhash = FaultyBlockhash::new(bank.last_blockhash(), probability_invalid_blockhash);
⋮----
let mut fee_payers = fee_payers.iter().cycle();
⋮----
.map(|_| {
let fee_payer = fee_payers.next().unwrap();
⋮----
let compute_unit_price = rng.gen_range(0..1000);
instructions.push(ComputeBudgetInstruction::set_compute_unit_price(
⋮----
for _ in 0..num_instructions_per_tx.saturating_sub(1) {
instructions.push(Instruction::new_with_bytes(
⋮----
vec![AccountMeta {
⋮----
Some(&fee_payer.pubkey()),
&blockhash.get(&mut rng),
⋮----
.unwrap()
⋮----
.collect();
BankingPacketBatch::new(to_packet_batches(&txs, NUM_PACKETS))
⋮----
pub trait ReceiveAndBufferCreator {
⋮----
impl ReceiveAndBufferCreator for TransactionViewReceiveAndBuffer {
fn create(
⋮----
pub struct ReceiveAndBufferSetup<T: ReceiveAndBuffer> {
⋮----
pub fn setup_receive_and_buffer<T: ReceiveAndBuffer + ReceiveAndBufferCreator>(
⋮----
} = create_genesis_config(100_000);
⋮----
let fee_payers = create_accounts(num_fee_payers, &mut genesis_config);
⋮----
Bank::new_for_benches(&genesis_config).wrap_with_bank_forks_for_tests();
let (sender, receiver) = unbounded();
⋮----
let decision = BufferedPacketsDecision::Consume(bank.clone());
let txs = generate_transactions(
⋮----
bank.clone(),

================
File: core/benches/receive_and_buffer.rs
================
mod utils;
⋮----
fn bench_receive_and_buffer<T: ReceiveAndBuffer + utils::ReceiveAndBufferCreator>(
⋮----
let mut group = c.benchmark_group("receive_and_buffer");
group.throughput(Throughput::Elements(num_txs as u64));
group.bench_function(bench_name, |bencher| {
bencher.iter_custom(|iters| {
⋮----
if sender.send(txs.clone()).is_err() {
panic!("Unexpectedly dropped receiver!");
⋮----
container.clear();
⋮----
receive_and_buffer.receive_and_buffer_packets(&mut container, &decision);
assert!(res.unwrap().num_received == num_txs && !container.is_empty());
black_box(&container);
⋮----
total = total.saturating_add(start.elapsed());
⋮----
group.finish();
⋮----
fn bench_transaction_view_receive_and_buffer(c: &mut Criterion) {
⋮----
criterion_group!(benches, bench_transaction_view_receive_and_buffer);
criterion_main!(benches);

================
File: core/benches/scheduler.rs
================
use jemallocator::Jemalloc;
use solana_core::bundle_stage::bundle_account_locker::BundleAccountLocker;
⋮----
mod utils;
⋮----
struct PingPong {
⋮----
impl PingPong {
fn new<Tx: TransactionWithMeta + Send + Sync + 'static>(
⋮----
let mut threads = Vec::with_capacity(work_receivers.len());
⋮----
let completed_work_sender_clone = completed_work_sender.clone();
⋮----
threads.push(handle);
⋮----
fn service_loop<Tx: TransactionWithMeta + Send + Sync + 'static>(
⋮----
while let Ok(work) = work_receiver.recv() {
⋮----
.send(FinishedConsumeWork {
⋮----
retryable_indexes: vec![],
⋮----
.is_err()
⋮----
struct BenchEnv<Tx: TransactionWithMeta + Send + Sync + 'static> {
⋮----
fn new() -> Self {
⋮----
(0..num_workers).map(|_| unbounded()).unzip();
let (finished_consume_work_sender, finished_consume_work_receiver) = unbounded();
⋮----
fn test_pre_graph_filter(_txs: &[&Tx], results: &mut [bool]) {
results.fill(true);
⋮----
fn test_pre_lock_filter(_tx: &TransactionState<Tx>) -> PreLockFilterAction {
⋮----
fn bench_scheduler_impl<T: ReceiveAndBuffer + utils::ReceiveAndBufferCreator>(
⋮----
let mut group = c.benchmark_group("bench_scheduler");
group.sample_size(10);
⋮----
vec![(true, "greedy_scheduler"), (false, "prio_graph_scheduler")];
let tx_counts: Vec<(usize, &str)> = vec![(16 * 1024, "16K_txs")];
let ix_counts: Vec<(usize, &str)> = vec![
⋮----
let conflict_types: Vec<(bool, &str)> = vec![(true, "single-payer"), (false, "unique_payer")];
⋮----
let bench_name = format!(
⋮----
group.throughput(Throughput::Elements(*tx_count as u64));
group.bench_function(&bench_name, |bencher| {
bencher.iter_custom(|iters| {
⋮----
timing_scheduler(
⋮----
bench_env.consume_work_senders.clone(),
bench_env.finished_consume_work_receiver.clone(),
⋮----
fn timing_scheduler<T: ReceiveAndBuffer, S: Scheduler<T::Transaction>>(
⋮----
let num_txs: usize = txs.iter().map(|txs| txs.len()).sum();
⋮----
if sender.send(txs.clone()).is_err() {
panic!("Unexpectedly dropped receiver!");
⋮----
.receive_and_buffer_packets(&mut container, &decision)
.unwrap();
assert_eq!(res.num_received, num_txs);
assert!(!container.is_empty());
⋮----
while !container.is_empty() {
⋮----
.receive_completed(black_box(&mut container), &ignored_decision)
⋮----
.schedule(
black_box(&mut container),
⋮----
start.elapsed()
⋮----
execute_time = execute_time.saturating_add(elapsed);
⋮----
fn bench_scheduler(c: &mut Criterion) {
⋮----
criterion_group!(benches, bench_scheduler,);
criterion_main!(benches);

================
File: core/benches/shredder.rs
================
fn make_test_entry(txs_per_entry: u64) -> Entry {
⋮----
transactions: vec![test_tx::test_tx().into(); txs_per_entry as usize],
⋮----
fn make_large_unchained_entries(txs_per_entry: u64, num_entries: u64) -> Vec<Entry> {
⋮----
.map(|_| make_test_entry(txs_per_entry))
.collect()
⋮----
let batch_payload = get_data_shred_bytes_per_batch_typical() as usize;
⋮----
fn bench_shredder_ticks(bencher: &mut Bencher) {
⋮----
let num_shreds = 1_000_000_usize.div_ceil(SHRED_SIZE_TYPICAL);
let num_ticks = max_ticks_per_n_shreds(1, Some(SHRED_SIZE_TYPICAL)) * num_shreds as u64;
let entries = create_ticks(num_ticks, 0, Hash::default());
⋮----
let chained_merkle_root = Hash::new_from_array(rand::thread_rng().gen());
bencher.iter(|| {
let shredder = Shredder::new(1, 0, 0, 0).unwrap();
shredder.entries_to_merkle_shreds_for_tests(
⋮----
fn bench_shredder_large_entries(bencher: &mut Bencher) {
⋮----
let num_shreds = 1_000_000_usize.div_ceil(shred_size);
⋮----
let num_entries = max_entries_per_n_shred(
&make_test_entry(txs_per_entry),
⋮----
Some(shred_size),
⋮----
let entries = make_large_unchained_entries(txs_per_entry, num_entries);
⋮----
fn bench_deshredder(bencher: &mut Bencher) {
⋮----
let num_shreds = 10_000_000_usize.div_ceil(shred_size);
let num_ticks = max_ticks_per_n_shreds(1, Some(shred_size)) * num_shreds as u64;
⋮----
let (data_shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
let data_shreds = data_shreds.iter().map(Shred::payload);
let raw = &mut Shredder::deshred(data_shreds).unwrap();
assert_ne!(raw.len(), 0);
⋮----
fn bench_deserialize_hdr(bencher: &mut Bencher) {
⋮----
let shredder = Shredder::new(2, 1, 0, 0).unwrap();
let merkle_root = Hash::new_from_array(rand::thread_rng().gen());
⋮----
.make_merkle_shreds_from_entries(
⋮----
.filter(Shred::is_data)
⋮----
let shred = shreds.remove(0);
⋮----
let payload = shred.payload().clone();
let _ = Shred::new_from_serialized_shred(payload).unwrap();
⋮----
fn make_entries() -> Vec<Entry> {
⋮----
let num_entries = max_entries_per_n_shred(&make_test_entry(txs_per_entry), 200, Some(1000));
make_large_unchained_entries(txs_per_entry, num_entries)
⋮----
fn bench_shredder_coding(bencher: &mut Bencher) {
let entries = make_entries();
⋮----
.collect();
black_box(result);
⋮----
fn bench_shredder_decoding(bencher: &mut Bencher) {
⋮----
.partition(Shred::is_data);
coding_shreds.truncate(CODING_SHREDS_PER_FEC_BLOCK);
⋮----
for shred in recover(coding_shreds.clone(), &reed_solomon_cache).unwrap() {
black_box(shred.unwrap());
⋮----
benchmark_group!(
⋮----
benchmark_main!(benches);

================
File: core/benches/sigverify_stage.rs
================
extern crate solana_core;
⋮----
struct Bench<T>(T);
impl<T> TDynBenchFn for Bench<T>
⋮----
fn run(&self, harness: &mut Bencher) {
⋮----
fn run_bench_packet_discard(num_ips: usize, bencher: &mut Bencher) {
⋮----
let tx = test_tx();
let mut batches = to_packet_batches(&vec![tx; len], chunk_size);
⋮----
.map(|_| {
⋮----
thread_rng().fill(&mut addr);
⋮----
.collect();
for batch in batches.iter_mut() {
total += batch.len();
for mut p in batch.iter_mut() {
let ip_index = thread_rng().gen_range(0..ips.len());
p.meta_mut().addr = ips[ip_index];
⋮----
info!("total packets: {total}");
bencher.iter(move || {
⋮----
if !p.meta().discard() {
⋮----
p.meta_mut().set_discard(false);
⋮----
assert_eq!(num_packets, 10_000);
⋮----
fn bench_packet_discard_many_senders(bencher: &mut Bencher) {
run_bench_packet_discard(1000, bencher);
⋮----
fn bench_packet_discard_single_sender(bencher: &mut Bencher) {
run_bench_packet_discard(1, bencher);
⋮----
fn bench_packet_discard_mixed_senders(bencher: &mut Bencher) {
⋮----
fn new_rand_addr<R: Rng>(rng: &mut R) -> std::net::IpAddr {
⋮----
rng.fill(&mut addr);
⋮----
let mut rng = thread_rng();
let mut batches = to_packet_batches(&vec![test_tx(); SIZE], CHUNK_SIZE);
let spam_addr = new_rand_addr(&mut rng);
⋮----
for mut packet in batch.iter_mut() {
packet.meta_mut().addr = if rng.gen_ratio(1, 30) {
new_rand_addr(&mut rng)
⋮----
if !packet.meta().discard() {
⋮----
packet.meta_mut().set_discard(false);
⋮----
fn gen_batches(use_same_tx: bool) -> Vec<PacketBatch> {
⋮----
to_packet_batches(&vec![tx; len], chunk_size)
⋮----
let amount = thread_rng().gen();
⋮----
&to_keypair.pubkey(),
⋮----
to_packet_batches(&txs, chunk_size)
⋮----
fn bench_sigverify_stage_with_same_tx(bencher: &mut Bencher) {
bench_sigverify_stage(bencher, true)
⋮----
fn bench_sigverify_stage_without_same_tx(bencher: &mut Bencher) {
bench_sigverify_stage(bencher, false)
⋮----
fn bench_sigverify_stage(bencher: &mut Bencher, use_same_tx: bool) {
⋮----
trace!("start");
let (packet_s, packet_r) = unbounded();
⋮----
let batches = gen_batches(use_same_tx);
trace!(
⋮----
for batch in batches.into_iter() {
sent_len += batch.len();
packet_s.send(batch).unwrap();
⋮----
trace!("sent: {sent_len}, expected: {expected}");
⋮----
if let Ok(verifieds) = verified_r.recv_timeout(Duration::from_millis(10)) {
received += verifieds.iter().map(|batch| batch.len()).sum::<usize>();
black_box(verifieds);
⋮----
trace!("received: {received}");
⋮----
stage.join().unwrap();
⋮----
fn prepare_batches(discard_factor: i32) -> (Vec<PacketBatch>, usize) {
⋮----
let mut batches = to_packet_batches(&txs, chunk_size);
⋮----
batches.iter_mut().for_each(|batch| {
batch.iter_mut().for_each(|mut p| {
let throw = die.sample(&mut rng);
⋮----
p.meta_mut().set_discard(true);
⋮----
fn bench_shrink_sigverify_stage_core(bencher: &mut Bencher, discard_factor: i32) {
let (batches0, num_valid_packets) = prepare_batches(discard_factor);
⋮----
bencher.iter(|| {
let batches = batches0.clone();
⋮----
let _batches = verifier.verify_batches(batches, num_valid_packets);
verify_time.stop();
⋮----
total_verify_time += verify_time.as_us();
⋮----
error!(
⋮----
fn benches() -> Vec<TestDescAndFn> {
let mut benches = vec![
⋮----
.iter()
.enumerate()
.for_each(|(i, &discard_factor)| {
let name = format!(
⋮----
benches.push(TestDescAndFn {
⋮----
testfn: TestFn::DynBenchFn(Box::new(Bench(move |b: &mut Bencher| {
bench_shrink_sigverify_stage_core(b, discard_factor);
⋮----
benchmark_main!(benches);

================
File: core/src/banking_stage/transaction_scheduler/bam_receive_and_buffer.rs
================
type PrevalidationResult = Result<(usize, bool, u32, u64), (Reason, u32)>;
type VerifyResult = Result<(Vec<SharedBytes>, bool, u32, u64), (Reason, u32)>;
pub struct BamReceiveAndBuffer {
⋮----
struct ParsedBatch {
⋮----
impl BamReceiveAndBuffer {
pub fn new(
⋮----
let response_sender_clone = response_sender.clone();
⋮----
parsing_thread: Some(parsing_thread),
⋮----
fn run_parsing(
⋮----
while !exit.load(Ordering::Relaxed) {
if last_metrics_report.elapsed() > METRICS_REPORT_INTERVAL {
metrics.report();
⋮----
let _ = recv_stats_sender.try_send(stats);
⋮----
let (recv_info, receive_time_us) = measure_us!(Self::batch_receive_until(
⋮----
.as_ref()
.and_then(|leader_state| leader_state.load().working_bank().map(|b| b.slot()))
.unwrap_or_else(|| bank_forks.read().unwrap().working_bank().slot());
let (deserialize_stats, duration_us) = measure_us!(Self::batch_verify(
⋮----
stats.accumulate(deserialize_stats);
metrics.increment_total_us(duration_us);
recv_buffer.clear();
for result in verify_results.drain(..) {
⋮----
.increment_total_batches_verified(1);
⋮----
measure_us!(Self::parse_batch(
⋮----
stats.accumulate(parse_stats);
⋮----
.try_send(BamOutboundMessage::AtomicTxnBatchResult(
⋮----
result: Some(
⋮----
reason: Some(reason),
⋮----
.saturating_add(parsed_batch.txns_max_age.len());
let _ = parsed_batch_sender.try_send(parsed_batch);
⋮----
let _ = response_sender.try_send(BamOutboundMessage::AtomicTxnBatchResult(
⋮----
result: Some(atomic_txn_batch_result::Result::NotCommitted(
⋮----
fn send_no_leader_slot_txn_batch_result(&self, seq_id: u32) {
⋮----
reason: Some(Reason::SchedulingError(
⋮----
fn send_container_full_txn_batch_result(&self, seq_id: u32) {
⋮----
fn parse_batch(
⋮----
let bank_forks = bank_forks.read().unwrap();
let root_bank = bank_forks.root_bank();
let working_bank = bank_forks.working_bank();
⋮----
let alt_resolved_slot = root_bank.slot();
let sanitized_epoch = root_bank.epoch();
let transaction_account_lock_limit = working_bank.get_transaction_account_lock_limit();
let vote_only = working_bank.vote_only_bank();
⋮----
.is_active(&agave_feature_set::static_instruction_limit::ID);
⋮----
let mut txns_max_age = Vec::with_capacity(verified_batch.len());
for (index, verified_packet) in verified_batch.into_iter().enumerate() {
⋮----
Err(Reason::DeserializationError(
⋮----
metrics.increment_sanitization_us(
⋮----
.duration_since(sanitization_start)
.as_micros() as u64,
⋮----
if view.is_simple_vote_transaction() {
⋮----
let load_addresses_result = match view.version() {
TransactionVersion::Legacy => Ok((None, u64::MAX)),
⋮----
.load_addresses_from_ref(view.address_table_lookup_iter())
.map(|(loaded_addresses, deactivation_slot)| {
(Some(loaded_addresses), deactivation_slot)
⋮----
root_bank.get_reserved_account_keys(),
⋮----
metrics.increment_resolution_us(
resolution_end.duration_since(resolution_start).as_micros() as u64,
⋮----
validate_account_locks(view.account_keys(), transaction_account_lock_limit)
⋮----
let reason = convert_txn_error_to_proto(err);
⋮----
Err(Reason::TransactionError(
⋮----
metrics.increment_lock_validation_us(start.elapsed().as_micros() as u64);
let (result, duration_us) = measure_us!(view
⋮----
metrics.increment_fee_budget_extraction_us(duration_us);
⋮----
let lock_results: [_; 1] = core::array::from_fn(|_| Ok(()));
let (check_results, duration_us) = measure_us!(working_bank.check_transactions(
⋮----
metrics.increment_check_transactions_us(duration_us);
if let Some(Err(err)) = check_results.first() {
let reason = convert_txn_error_to_proto(err.clone());
⋮----
let (result, duration_us) = measure_us!(Consumer::check_fee_payer_unlocked(
⋮----
metrics.increment_fee_payer_check_us(duration_us);
⋮----
let (contains_blacklisted_account, duration_us) = measure_us!(view
⋮----
metrics.increment_blacklist_check_us(duration_us);
⋮----
let max_age = calculate_max_age(sanitized_epoch, deactivation_slot, alt_resolved_slot);
⋮----
calculate_priority_and_cost(&view, &fee_budget_limits.into(), &working_bank);
cost = cost.saturating_add(txn_cost);
txns_max_age.push((view, max_age));
⋮----
let priority = seq_id_to_priority(seq_id);
⋮----
Ok(ParsedBatch {
⋮----
fn batch_receive_until(
⋮----
let batch = bundle_receiver.recv_timeout(recv_timeout)?;
let mut num_packets_received = batch.packets.len();
⋮----
recv_buffer.push(batch);
while let Ok(batch) = bundle_receiver.try_recv() {
trace!("got more packet batches in bam receive and buffer");
num_packets_received += batch.packets.len();
⋮----
if start.elapsed() > recv_timeout || recv_buffer.len() >= batch_count_upperbound {
⋮----
Ok((num_packets_received, num_atomic_txn_batches_received))
⋮----
fn prevalidate_batches(
⋮----
prevalidated.clear();
prevalidated.extend(atomic_txn_batches.iter().enumerate().map(
⋮----
return Err((
⋮----
if atomic_txn_batch.packets.is_empty() {
⋮----
if atomic_txn_batch.packets.len() > 5 {
⋮----
.iter()
.map(|p| {
⋮----
.and_then(|meta| meta.flags.as_ref())
.is_some_and(|flags| flags.revert_on_error)
⋮----
.all_equal_value()
⋮----
Ok((
⋮----
fn batch_verify(
⋮----
fn proto_packet_to_packet(from_packet: &Packet) -> BytesPacket {
let copy_len = min(PACKET_DATA_SIZE, from_packet.data.len());
⋮----
to_packet.meta_mut().size = from_packet.data.len();
to_packet.meta_mut().set_discard(false);
⋮----
to_packet.meta_mut().size = meta.size as usize;
⋮----
.meta_mut()
⋮----
.insert(PacketFlags::SIMPLE_VOTE_TX);
⋮----
fn pkt_to_shared_bytes(
⋮----
if solana_packet_ref.meta().discard() {
⋮----
.increment_total_packets_verified(1);
let Some(data) = solana_packet_ref.data(..) else {
⋮----
Ok(SharedBytes::new(data.to_vec()))
⋮----
stats.accumulate(preverify_stats);
packet_batches.clear();
packet_batches.reserve(prevalidated.len());
⋮----
prevalidated.iter().flatten().for_each(|result| {
⋮----
.map(proto_packet_to_packet)
.collect();
packet_count += solana_packet_batch.len();
packet_batches.push(solana_perf::packet::PacketBatch::Bytes(
⋮----
ed25519_verify(packet_batches, false, packet_count);
verify_packet_batch_time_us.stop();
⋮----
.increment_verify_batches_pp_us(verify_packet_batch_time_us.as_us(), packet_count);
⋮----
.increment_batch_packets_len(packet_count);
⋮----
.increment_total_verify_time(verify_packet_batch_time_us.as_us());
results.clear();
results.reserve(prevalidated.len());
let mut packet_batch_iter = packet_batches.iter();
for pre_result in prevalidated.drain(..) {
let result = pre_result.and_then(|(_, revert_on_error, seq_id, max_schedule_slot)| {
let batch = packet_batch_iter.next().unwrap();
⋮----
.enumerate()
.map(|(i, pkt)| pkt_to_shared_bytes(&pkt, i, seq_id, metrics))
⋮----
Ok((deserialized, revert_on_error, seq_id, max_schedule_slot))
⋮----
results.push(result);
⋮----
impl ReceiveAndBuffer for BamReceiveAndBuffer {
type Transaction = RuntimeTransaction<ResolvedTransactionView<SharedBytes>>;
type Container = TransactionStateContainer<Self::Transaction>;
fn receive_and_buffer_packets(
⋮----
let is_bam_enabled = BamConnectionState::from_u8(self.bam_enabled.load(Ordering::Relaxed))
⋮----
while let Ok(batch_stats) = self.recv_stats_receiver.try_recv() {
stats.accumulate(batch_stats);
⋮----
let batch = match self.parsed_batch_receiver.try_recv() {
⋮----
Err(TryRecvError::Disconnected) => return Err(DisconnectedError),
⋮----
stats.num_dropped_without_parsing += batch.txns_max_age.len();
⋮----
.insert_new_batch(
⋮----
.is_none()
⋮----
self.send_container_full_txn_batch_result(seq_id);
⋮----
while let Some(next_batch_id) = container.pop() {
let seq_id = priority_to_seq_id(next_batch_id.priority);
self.send_no_leader_slot_txn_batch_result(seq_id);
container.remove_by_id(next_batch_id.id);
⋮----
measure_us!(self.parsed_batch_receiver.recv_deadline(deadline));
⋮----
Err(RecvTimeoutError::Disconnected) => return Err(DisconnectedError),
⋮----
self.send_no_leader_slot_txn_batch_result(batch.seq_id);
⋮----
Ok(stats)
⋮----
impl Drop for BamReceiveAndBuffer {
fn drop(&mut self) {
if let Some(parsing_thread) = self.parsing_thread.take() {
parsing_thread.join().unwrap();
⋮----
pub fn seq_id_to_priority(seq_id: u32) -> u64 {
u64::MAX.saturating_sub(seq_id as u64)
⋮----
pub fn priority_to_seq_id(priority: u64) -> u32 {
u32::try_from(u64::MAX.saturating_sub(priority)).unwrap_or(u32::MAX)
⋮----
struct BamReceiveAndBufferMetrics {
⋮----
impl BamReceiveAndBufferMetrics {
fn has_data(&self) -> bool {
⋮----
fn report(&mut self) {
if !self.has_data() {
⋮----
datapoint_info!(
⋮----
self.sigverify_metrics.report();
⋮----
fn increment_total_us(&mut self, us: u64) {
self.total_us = self.total_us.saturating_add(us);
⋮----
fn increment_sanitization_us(&mut self, us: u64) {
self.sanitization_us = self.sanitization_us.saturating_add(us);
⋮----
fn increment_resolution_us(&mut self, us: u64) {
self.resolution_us = self.resolution_us.saturating_add(us);
⋮----
fn increment_lock_validation_us(&mut self, us: u64) {
self.lock_validation_us = self.lock_validation_us.saturating_add(us);
⋮----
fn increment_fee_budget_extraction_us(&mut self, us: u64) {
self.fee_budget_extraction_us = self.fee_budget_extraction_us.saturating_add(us);
⋮----
fn increment_check_transactions_us(&mut self, us: u64) {
self.check_transactions_us = self.check_transactions_us.saturating_add(us);
⋮----
fn increment_fee_payer_check_us(&mut self, us: u64) {
self.fee_payer_check_us = self.fee_payer_check_us.saturating_add(us);
⋮----
fn increment_blacklist_check_us(&mut self, us: u64) {
self.blacklist_check_us = self.blacklist_check_us.saturating_add(us);
⋮----
struct SigverifyMetrics {
⋮----
impl Default for SigverifyMetrics {
fn default() -> Self {
⋮----
impl SigverifyMetrics {
pub fn report(&self) {
⋮----
pub fn increment_verify_batches_pp_us(&mut self, us: u64, packet_count: usize) {
⋮----
let per_packet_us = (us as f64 / packet_count as f64).round() as u64;
⋮----
.increment(per_packet_us)
.unwrap();
⋮----
pub fn increment_batch_packets_len(&mut self, packet_count: usize) {
⋮----
.increment(packet_count as u64)
⋮----
pub fn increment_total_verify_time(&mut self, us: u64) {
⋮----
pub fn increment_total_packets_verified(&mut self, count: usize) {
⋮----
pub fn increment_total_batches_verified(&mut self, count: usize) {
⋮----
mod tests {
⋮----
fn test_seq_id_to_priority() {
assert_eq!(seq_id_to_priority(0), u64::MAX);
assert_eq!(seq_id_to_priority(1), u64::MAX - 1);
⋮----
fn test_priority_to_seq_id() {
assert_eq!(priority_to_seq_id(u64::MAX), 0);
assert_eq!(priority_to_seq_id(u64::MAX - 1), 1);
⋮----
fn test_bank_forks() -> (Arc<RwLock<BankForks>>, Keypair) {
⋮----
} = create_slow_genesis_config(u64::MAX);
⋮----
fn setup_bam_receive_and_buffer(
⋮----
exit.clone(),
⋮----
fn verify_container<Tx: TransactionWithMeta>(
⋮----
while let Some(id) = container.pop() {
let Some((ids, _, _)) = container.get_batch(id.id) else {
panic!(
⋮----
assert!(
⋮----
assert_eq!(actual_length, expected_length);
⋮----
fn run_batch_verify(
⋮----
fn test_receive_and_buffer_simple_transfer<R: ReceiveAndBuffer>(
⋮----
let (sender, receiver) = unbounded();
let (bank_forks, mint_keypair) = test_bank_forks();
⋮----
setup_receive_and_buffer(receiver, bank_forks.clone(), HashSet::new());
let transaction = transfer(
⋮----
bank_forks.read().unwrap().root_bank().last_blockhash(),
⋮----
let data = bincode::serialize(&transaction).expect("serializes");
⋮----
packets: vec![Packet { data, meta: None }],
⋮----
sender.send(bundle).unwrap();
⋮----
while start.elapsed() < Duration::from_secs(2) {
⋮----
.receive_and_buffer_packets(&mut container, &BufferedPacketsDecision::Hold)
⋮----
verify_container(&mut container, 1);
exit.store(true, Ordering::Relaxed);
⋮----
fn test_receive_and_buffer_invalid_packet() {
let (bank_forks, _mint_keypair) = test_bank_forks();
⋮----
setup_bam_receive_and_buffer(receiver, bank_forks.clone(), HashSet::new());
⋮----
packets: vec![Packet {
⋮----
assert_eq!(num_received, 0);
verify_container(&mut container, 0);
let response = response_receiver.recv().unwrap();
assert!(matches!(
⋮----
fn test_batch_deserialize_success() {
⋮----
let (results, _batch_stats) = run_batch_verify(&[bundle], Slot::MAX, &mut stats);
assert_eq!(results.len(), 1);
assert!(results[0].is_ok());
⋮----
assert_eq!(deserialized_packets.len(), 1);
assert_eq!(*seq_id, 1);
⋮----
fn test_batch_deserialize_empty() {
let (_bank_forks, _mint_keypair) = test_bank_forks();
⋮----
packets: vec![],
⋮----
let (results, batch_stats) = run_batch_verify(&[batch], Slot::MAX, &mut stats);
⋮----
assert!(results[0].is_err());
assert_eq!(batch_stats.num_dropped_without_parsing, 1);
⋮----
assert!(matches!(reason, Reason::DeserializationError(_)));
⋮----
fn test_batch_deserialize_invalid_packet() {
⋮----
let (results, _batch_stats) = run_batch_verify(&[batch], Slot::MAX, &mut stats);
⋮----
fn test_batch_deserialize_fee_payer_doesnt_exist() {
let (bank_forks, _) = test_bank_forks();
⋮----
deserialized_packets.clone(),
⋮----
assert!(result.is_err());
assert_eq!(stats.num_dropped_on_fee_payer, 1);
assert!(matches!(result.err().unwrap(), Reason::TransactionError(_)));
⋮----
fn test_batch_deserialize_inconsistent() {
⋮----
packets: vec![
⋮----
let (results, batch_stats) = run_batch_verify(&[bundle], Slot::MAX, &mut stats);
⋮----
fn test_batch_deserialize_blacklisted_account() {
⋮----
let blacklisted_accounts = HashSet::from_iter(std::iter::once(keypair.pubkey()));
⋮----
assert_eq!(stats.num_dropped_on_blacklisted_account, 1);
⋮----
fn test_batch_deserialize_rejects_vote_transactions() {
⋮----
let recent_blockhash = bank_forks.read().unwrap().root_bank().last_blockhash();
⋮----
&vote_keypair.pubkey(),
&authorized_voter.pubkey(),
solana_vote_program::vote_state::Vote::new(vec![1], recent_blockhash),
⋮----
Some(&node_keypair.pubkey()),
⋮----
let vote_data = bincode::serialize(&VersionedTransaction::from(vote_tx)).unwrap();
⋮----
flags: Some(jito_protos::proto::bam_types::PacketFlags {
⋮----
size: vote_data.len() as u64,
⋮----
assert_eq!(stats.num_dropped_on_parsing_and_sanitization, 1);
⋮----
fn test_batch_deserialize_reject_wrong_slot() {

================
File: core/src/banking_stage/transaction_scheduler/bam_scheduler.rs
================
use crate::banking_stage::transaction_scheduler::scheduler::PreLockFilterAction;
⋮----
type SchedulerPrioGraph = PrioGraph<
⋮----
fn passthrough_priority(
⋮----
pub struct BamScheduler<Tx: TransactionWithMeta> {
⋮----
struct InflightBatchInfo {
⋮----
pub fn new(
⋮----
fn get_transactions_account_access<'a>(
⋮----
transactions.flat_map(|txn| {
txn.account_keys().iter().enumerate().map(|(index, key)| {
if txn.is_writable(index) {
⋮----
/// Insert all incoming transactions into the `PrioGraph`.
    fn pull_into_prio_graph<S: StateContainer<Tx>>(&mut self, container: &mut S) {
⋮----
fn pull_into_prio_graph<S: StateContainer<Tx>>(&mut self, container: &mut S) {
⋮----
warn!("Slot is not set, cannot pull transactions into prio-graph");
⋮----
let working_bank = self.bank_forks.read().unwrap().working_bank();
while let Some(next_batch_id) = container.pop() {
let Some((batch_ids, _, max_schedule_slot)) = container.get_batch(next_batch_id.id)
⋮----
error!("Batch {} not found in container", next_batch_id.id);
⋮----
// If the slot has changed, we cannot schedule this batch
let seq_id = priority_to_seq_id(next_batch_id.priority);
self.send_no_leader_slot_bundle_result(seq_id);
container.remove_by_id(next_batch_id.id);
⋮----
.iter()
.filter_map(|txn_id| container.get_transaction(*txn_id))
.collect_vec();
⋮----
let lock_results = (0..txns.len())
.map(|_| Ok(()))
⋮----
lock_results.as_slice(),
⋮----
.find_position(|res| res.is_err())
.map(|(i, res)| (i, res.as_ref().err().unwrap().clone()))
⋮----
reason: Some(Self::convert_reason_to_proto(
⋮----
self.send_back_result(seq_id, result);
⋮----
.insert(priority_to_seq_id(next_batch_id.priority), Instant::now());
self.prio_graph.insert_transaction(
⋮----
Self::get_transactions_account_access(txns.into_iter()),
⋮----
fn send_to_workers(
⋮----
warn!("Slot is not set, cannot schedule transactions");
⋮----
while let Some(id) = self.prio_graph.pop() {
⋮----
container.get_batch(id.id).unwrap();
// Update time in prio-graph metric
⋮----
.remove(&priority_to_seq_id(id.priority))
⋮----
.increment(now.duration_since(insertion_time).as_micros() as u64);
⋮----
// Filter on slot
⋮----
self.prio_graph.unblock(&id);
let seq_id = priority_to_seq_id(id.priority);
⋮----
container.remove_by_id(id.id);
⋮----
// Filter on check_transactions
⋮----
.map(|txn| txn.borrow())
⋮----
let lock_results = (0..sanitized_txs.len())
⋮----
// Schedule it
let mut work = self.get_or_create_work_object();
let batch_id = self.get_next_schedule_id();
*num_scheduled += batch_ids.len();
⋮----
self.send_to_worker(vec![id], work, slot);
⋮----
fn send_to_worker(
⋮----
let _ = self.consume_work_sender.send(work);
self.inflight_batch_info.insert(
⋮----
fn get_next_schedule_id(&mut self) -> TransactionBatchId {
⋮----
fn get_or_create_work_object(&mut self) -> ConsumeWork<Tx> {
if let Some(work) = self.reusable_consume_work.pop() {
⋮----
// These values will be overwritten by `generate_work`
⋮----
fn recycle_work_object(&mut self, mut work: ConsumeWork<Tx>) {
// Just in case, clear the work object
work.ids.clear();
work.transactions.clear();
work.max_ages.clear();
self.reusable_consume_work.push(work);
⋮----
fn recycle_priority_ids(&mut self, mut priority_ids: Vec<TransactionPriorityId>) {
priority_ids.clear();
self.reusable_priority_ids.push(priority_ids);
⋮----
fn generate_work(
⋮----
output.ids.clear();
output.ids.extend(
⋮----
.filter_map(|priority_id| container.get_batch(priority_id.id))
.flat_map(|(batch_ids, _, _)| batch_ids.into_iter())
.copied(),
⋮----
output.transactions.clear();
output.max_ages.clear();
for (txn, max_age) in output.ids.iter().filter_map(|txn_id| {
let result = container.get_mut_transaction_state(*txn_id)?;
let result = result.take_transaction_for_scheduling();
Some(result)
⋮----
output.transactions.push(txn);
output.max_ages.push(max_age);
⋮----
output.max_schedule_slot = Some(slot);
⋮----
fn send_no_leader_slot_bundle_result(&self, seq_id: u32) {
⋮----
.try_send(BamOutboundMessage::AtomicTxnBatchResult(
⋮----
result: Some(atomic_txn_batch_result::Result::NotCommitted(
⋮----
reason: Some(Reason::SchedulingError(
⋮----
fn send_back_result(&self, seq_id: u32, result: atomic_txn_batch_result::Result) {
⋮----
result: Some(result),
⋮----
/// Generates a `bundle_result::Result` based on the processed results for 'revert_on_error' batches.
    fn generate_revert_on_error_bundle_result(
⋮----
fn generate_revert_on_error_bundle_result(
⋮----
.all(|result| matches!(result, TransactionResult::Committed(_)))
⋮----
.filter_map(|result| {
⋮----
Some(processed.clone())
⋮----
.collect();
⋮----
for (i, result) in processed_results.iter().enumerate() {
⋮----
not_commit_reason = NotCommittedReason::Error(err.clone());
⋮----
reason: Some(Self::convert_reason_to_proto(index, not_commit_reason)),
⋮----
fn generate_bundle_result(processed: &TransactionResult) -> atomic_txn_batch_result::Result {
⋮----
transaction_results: vec![result.clone()],
⋮----
NotCommittedReason::Error(err) => (0, NotCommittedReason::Error(err.clone())),
⋮----
fn convert_reason_to_proto(
⋮----
reason: convert_txn_error_to_proto(err) as i32,
⋮----
fn maybe_bank_boundary_actions(
⋮----
let maybe_bank = decision.bank();
if maybe_bank.map(|bank| bank.slot()) == self.slot {
⋮----
info!(
⋮----
self.slot = Some(bank.slot());
⋮----
info!("Bank boundary detected: slot changed to None");
⋮----
if self.slot.is_none() {
⋮----
for (_, inflight_info) in self.inflight_batch_info.iter() {
⋮----
if prev_slot == Some(inflight_info.slot) {
self.prio_graph.unblock(priority_id);
⋮----
while let Some((next_batch_id, _)) = self.prio_graph.pop_and_unblock() {
⋮----
.remove(&priority_to_seq_id(next_batch_id.priority))
⋮----
self.prio_graph.clear();
self.insertion_to_prio_graph_time.clear();
⋮----
self.report_histogram_metrics();
⋮----
fn report_histogram_metrics(&mut self) {
datapoint_info!(
⋮----
self.time_in_priograph_us.clear();
⋮----
self.time_in_worker_us.clear();
⋮----
self.time_between_schedule_us.clear();
⋮----
fn schedule<S: StateContainer<Tx>>(
⋮----
let starting_queue_size = container.queue_size();
let starting_buffer_size = container.buffer_size();
⋮----
let time_since_last_schedule = start_time.duration_since(self.last_schedule_time);
⋮----
.increment(time_since_last_schedule.as_micros() as u64);
⋮----
self.pull_into_prio_graph(container);
self.send_to_workers(container, &mut num_scheduled);
Ok(SchedulingSummary {
⋮----
filter_time_us: start_time.elapsed().as_micros() as u64,
⋮----
fn receive_completed(
⋮----
self.maybe_bank_boundary_actions(decision, container);
⋮----
while let Ok(result) = self.finished_consume_work_receiver.try_recv() {
num_transactions += result.work.ids.len();
⋮----
self.recycle_work_object(result.work);
let Some(inflight_batch_info) = self.inflight_batch_info.remove(&batch_id) else {
⋮----
let _ = self.time_in_worker_us.increment(
now.duration_since(inflight_batch_info.schedule_time)
.as_micros() as u64,
⋮----
inflight_batch_info.batch_priority_ids.len()
⋮----
.enumerate()
.take(len)
⋮----
if let Some(extra_info) = result.extra_info.as_ref() {
⋮----
let Some(txn_result) = extra_info.processed_results.get(i) else {
warn!(
⋮----
self.send_back_result(priority_to_seq_id(priority_id.priority), bundle_result);
⋮----
if Some(inflight_batch_info.slot) == self.slot {
⋮----
container.remove_by_id(priority_id.id);
⋮----
self.recycle_priority_ids(inflight_batch_info.batch_priority_ids);
⋮----
Ok((num_transactions, 0))
⋮----
fn scheduling_common_mut(&mut self) -> &mut SchedulingCommon<Tx> {
todo!()
⋮----
mod tests {
⋮----
struct TestScheduler {
⋮----
fn create_test_scheduler(
⋮----
let (consume_work_sender, consume_work_receiver) = unbounded();
let (finished_consume_work_sender, finished_consume_work_receiver) = unbounded();
let (response_sender, response_receiver) = unbounded();
test_bank_forks();
⋮----
bank_forks.clone(),
⋮----
.map(|_| consume_work_receiver.clone())
.collect(),
⋮----
fn prioritized_tranfers(
⋮----
.into_iter()
.map(|pubkey| *pubkey.borrow())
.zip(std::iter::repeat(lamports))
⋮----
let mut ixs = transfer_many(&from_keypair.pubkey(), &to_pubkeys_lamports);
⋮----
ixs.push(prioritization);
let message = Message::new(&ixs, Some(&from_keypair.pubkey()));
⋮----
fn create_container(
⋮----
tx_infos.into_iter()
⋮----
let transaction = prioritized_tranfers(
from_keypair.borrow(),
⋮----
container.insert_new_batch(
vec![(transaction, MaxAge::MAX)],
⋮----
fn test_bank_forks() -> (Arc<RwLock<BankForks>>, Keypair) {
⋮----
} = create_slow_genesis_config(u64::MAX);
⋮----
fn test_scheduler_empty() {
let (bank_forks, _) = test_bank_forks();
⋮----
} = create_test_scheduler(4, &bank_forks);
⋮----
.schedule(
⋮----
.unwrap();
assert_eq!(result.num_scheduled, 0);
⋮----
fn test_scheduler_basic() {
⋮----
let mut container = create_container(vec![
⋮----
assert!(
⋮----
let decision = BufferedPacketsDecision::Consume(bank_forks.read().unwrap().working_bank());
⋮----
.receive_completed(&mut container, &decision)
⋮----
assert_eq!(result.num_scheduled, 2);
let work_1 = consume_work_receivers[0].try_recv().unwrap();
assert_eq!(work_1.ids.len(), 1);
let work_2 = consume_work_receivers[0].try_recv().unwrap();
assert_eq!(work_2.ids.len(), 1);
assert_eq!(
⋮----
assert_ne!(
⋮----
for (work, response) in responses.into_iter() {
⋮----
retryable_indexes: vec![],
extra_info: Some(
⋮----
processed_results: vec![response],
⋮----
let _ = finished_consume_work_sender.send(finished_work);
⋮----
assert_eq!(num_transactions, 2);
let response = response_receiver.try_recv().unwrap();
⋮----
panic!("Expected AtomicTxnBatchResult message");
⋮----
assert_eq!(bundle_result.seq_id, 0);
⋮----
let result = bundle_result.result.unwrap();
⋮----
assert_eq!(committed.transaction_results.len(), 1);
assert_eq!(committed.transaction_results[0].cus_consumed, 100);
⋮----
panic!("Expected Committed result, got NotCommitted: {not_committed:?}");
⋮----
assert_eq!(bundle_result.seq_id, 3);
⋮----
panic!("Expected NotCommitted result, got Committed");
⋮----
let reason = not_committed.reason.unwrap();
⋮----
assert_eq!(result.num_scheduled, 1);
⋮----
processed_results: vec![TransactionResult::Committed(
⋮----
assert_eq!(num_transactions, 1);
⋮----
assert_eq!(bundle_result.seq_id, 1);
⋮----
assert_eq!(committed.transaction_results[0].cus_consumed, 1500);
⋮----
.receive_completed(&mut container, &BufferedPacketsDecision::Forward)
⋮----
assert_eq!(num_transactions, 0);
⋮----
assert_eq!(bundle_result.seq_id, 2);
⋮----
fn test_prio_graph_clears_on_slot_boundary() {
⋮----
let bank = bank_forks.read().unwrap().working_bank();
let mut container = create_container(vec![(
⋮----
let decision = BufferedPacketsDecision::Consume(bank.clone());
⋮----
assert_eq!(scheduler.slot, Some(bank.slot()));
⋮----
scheduler.pull_into_prio_graph(&mut container);
⋮----
while let Some(txn_id) = scheduler.prio_graph.pop() {
stored_txn_ids.push(txn_id);
scheduler.prio_graph.unblock(&txn_id);
⋮----
if let Some((batch_ids, _, _)) = container.get_batch(txn_id.id) {
⋮----
.filter_map(|id| container.get_transaction(*id));
scheduler.prio_graph.insert_transaction(
⋮----
BamScheduler::<RuntimeTransaction<SanitizedTransaction>>::get_transactions_account_access(txns.into_iter()),
⋮----
.receive_completed(&mut container, &decision_no_bank)
⋮----
assert_eq!(scheduler.slot, None);
if let Some(first_id) = stored_txn_ids.first() {
scheduler.prio_graph.unblock(first_id);

================
File: core/src/banking_stage/transaction_scheduler/bam_utils.rs
================
pub fn convert_txn_error_to_proto(err: TransactionError) -> TransactionErrorReason {

================
File: core/src/banking_stage/transaction_scheduler/batch_id_generator.rs
================
use crate::banking_stage::scheduler_messages::TransactionBatchId;
⋮----
pub struct BatchIdGenerator {
⋮----
impl BatchIdGenerator {
pub fn next(&mut self) -> TransactionBatchId {
⋮----
self.next_id = self.next_id.wrapping_sub(1);

================
File: core/src/banking_stage/transaction_scheduler/greedy_scheduler.rs
================
use qualifier_attr::qualifiers;
⋮----
pub(crate) struct GreedySchedulerConfig {
⋮----
impl Default for GreedySchedulerConfig {
fn default() -> Self {
⋮----
pub struct GreedyScheduler<Tx: TransactionWithMeta> {
⋮----
pub(crate) fn new(
⋮----
fn schedule<S: StateContainer<Tx>>(
⋮----
let mut budget = budget.saturating_sub(
⋮----
.cus_in_flight_per_thread()
.iter()
.sum(),
⋮----
let starting_queue_size = container.queue_size();
let starting_buffer_size = container.buffer_size();
let num_threads = self.common.consume_work_senders.len();
⋮----
if self.common.in_flight_tracker.cus_in_flight_per_thread()[thread_id]
⋮----
schedulable_threads.remove(thread_id);
⋮----
if schedulable_threads.is_empty() {
return Ok(SchedulingSummary {
⋮----
debug_assert!(
⋮----
&& !schedulable_threads.is_empty()
&& !container.is_empty()
⋮----
let Some(id) = container.pop() else {
unreachable!("container is not empty")
⋮----
let Some(transaction_state) = container.get_mut_transaction_state(id.id) else {
panic!("transaction state must exist")
⋮----
.check_locks(transaction_state.transaction())
⋮----
self.working_account_set.clear();
num_sent += self.common.send_batches()?;
⋮----
match try_schedule_transaction(
⋮----
select_thread(
⋮----
self.common.batches.total_cus(),
self.common.in_flight_tracker.cus_in_flight_per_thread(),
self.common.batches.transactions(),
self.common.in_flight_tracker.num_in_flight_per_thread(),
⋮----
self.unschedulables.push(id);
⋮----
assert!(
⋮----
self.common.batches.add_transaction_to_batch(
⋮----
budget = budget.saturating_sub(cost);
if self.common.batches.transactions()[thread_id].len()
⋮----
+ self.common.batches.total_cus()[thread_id]
⋮----
assert_eq!(
⋮----
container.push_ids_into_queue(self.unschedulables.drain(..));
Ok(SchedulingSummary {
⋮----
fn scheduling_common_mut(&mut self) -> &mut SchedulingCommon<Tx> {
⋮----
fn try_schedule_transaction<Tx: TransactionWithMeta>(
⋮----
match pre_lock_filter(transaction_state) {
⋮----
let transaction = transaction_state.transaction();
let account_keys = transaction.account_keys();
⋮----
.enumerate()
.filter_map(|(index, key)| transaction.is_writable(index).then_some(key));
⋮----
.filter_map(|(index, key)| (!transaction.is_writable(index)).then_some(key));
let l_account_locks = bundle_account_locker.account_locks();
for lock in read_account_locks.clone() {
if l_account_locks.write_locks().contains_key(lock) {
return Err(TransactionSchedulingError::UnschedulableConflicts);
⋮----
for lock in write_account_locks.clone() {
if l_account_locks.write_locks().contains_key(lock)
|| l_account_locks.read_locks().contains_key(lock)
⋮----
let thread_id = match account_locks.try_lock_accounts(
⋮----
return Err(TransactionSchedulingError::UnschedulableThread);
⋮----
drop(l_account_locks);
let (transaction, max_age) = transaction_state.take_transaction_for_scheduling();
let cost = transaction_state.cost();
Ok(TransactionSchedulingInfo {
⋮----
mod test {
⋮----
fn create_test_frame(
⋮----
(0..num_threads).map(|_| unbounded()).unzip();
let (finished_consume_work_sender, finished_consume_work_receiver) = unbounded();
⋮----
fn prioritized_tranfers(
⋮----
.into_iter()
.map(|pubkey| *pubkey.borrow())
.zip(std::iter::repeat(lamports))
.collect_vec();
⋮----
system_instruction::transfer_many(&from_keypair.pubkey(), &to_pubkeys_lamports);
⋮----
ixs.push(prioritization);
let message = Message::new(&ixs, Some(&from_keypair.pubkey()));
⋮----
fn create_container(
⋮----
for (from_keypair, to_pubkeys, lamports, compute_unit_price) in tx_infos.into_iter() {
let transaction = prioritized_tranfers(
from_keypair.borrow(),
⋮----
container.insert_new_transaction(
⋮----
fn collect_work(
⋮----
.try_iter()
.map(|work| {
let ids = work.ids.clone();
⋮----
.unzip()
⋮----
fn test_pre_graph_filter(
⋮----
results.fill(true);
⋮----
fn test_pre_lock_filter(
⋮----
fn test_schedule_disconnected_channel() {
let (mut scheduler, work_receivers, _finished_work_sender) = create_test_frame(
⋮----
let mut container = create_container([(&Keypair::new(), &[Pubkey::new_unique()], 1, 1)]);
drop(work_receivers);
assert_matches!(
⋮----
fn test_schedule_single_threaded_no_conflicts() {
⋮----
let mut container = create_container([
⋮----
.schedule(
⋮----
.unwrap();
assert_eq!(scheduling_summary.num_scheduled, 2);
assert_eq!(scheduling_summary.num_unschedulable_conflicts, 0);
assert_eq!(collect_work(&work_receivers[0]).1, vec![vec![1, 0]]);
⋮----
fn test_schedule_budget() {
let (mut scheduler, _work_receivers, _finished_work_sender) = create_test_frame(
⋮----
assert_eq!(scheduling_summary.num_scheduled, 0);
⋮----
fn test_schedule_single_threaded_scheduling_cu_limit() {
⋮----
assert_eq!(scheduling_summary.num_scheduled, 1);
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, vec![vec![1]]);
⋮----
fn test_schedule_single_threaded_scheduling_scan_limit() {
⋮----
fn test_schedule_single_threaded_scheduling_batch_size() {
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, vec![vec![1], vec![0]]);
⋮----
fn test_schedule_single_threaded_conflict(relax_intrabatch_account_locks: bool) {
⋮----
fn test_schedule_simple_thread_selection() {
⋮----
create_container((0..4).map(|i| (Keypair::new(), [Pubkey::new_unique()], 1, i)));
⋮----
assert_eq!(scheduling_summary.num_scheduled, 4);
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, [vec![3, 1]]);
assert_eq!(collect_work(&work_receivers[1]).1, [vec![2, 0]]);
⋮----
fn test_schedule_scan_past_highest_priority() {
⋮----
assert_eq!(scheduling_summary.num_scheduled, 3);
assert_eq!(scheduling_summary.num_unschedulable_conflicts, 1);
assert_eq!(collect_work(&work_receivers[0]).1, [vec![3], vec![0]]);
assert_eq!(collect_work(&work_receivers[1]).1, [vec![2]]);
⋮----
fn test_schedule_local_fee_markets() {
⋮----
assert_eq!(scheduling_summary.num_unschedulable_threads, 3);
assert_eq!(collect_work(&work_receivers[0]).1, [vec![5], vec![4]]);
assert_eq!(collect_work(&work_receivers[1]).1, [vec![0]]);
⋮----
fn test_schedule_bundle_account_locker() {
⋮----
let tx_1_a = transfer(&keypair_1, &keypair_1.pubkey(), 1, Hash::default());
let tx_1_b = transfer(&keypair_1, &keypair_1.pubkey(), 2, Hash::default());
let tx_2_a = transfer(&keypair_2, &keypair_2.pubkey(), 1, Hash::default());
let tx_2_b = transfer(&keypair_2, &keypair_2.pubkey(), 2, Hash::default());
⋮----
let runtime_tx_1_b = vec![runtime_tx_1_b];
⋮----
let runtime_tx_2_b = vec![runtime_tx_2_b];
⋮----
container.insert_new_transaction(runtime_tx_1_a, MaxAge::MAX, 1, 5000);
container.insert_new_transaction(runtime_tx_2_a, MaxAge::MAX, 1, 5000);
⋮----
bundle_account_locker.clone(),
⋮----
.lock_bundle(&runtime_tx_1_b, &bank)
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, [vec![1]]);
⋮----
.unlock_bundle(&runtime_tx_1_b, &bank)
⋮----
.lock_bundle(&runtime_tx_2_b, &bank)
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, [vec![0]]);
⋮----
.unlock_bundle(&runtime_tx_2_b, &bank)

================
File: core/src/banking_stage/transaction_scheduler/in_flight_tracker.rs
================
pub struct InFlightTracker {
⋮----
struct BatchEntry {
⋮----
impl InFlightTracker {
pub fn new(num_threads: usize) -> Self {
⋮----
num_in_flight_per_thread: vec![0; num_threads],
cus_in_flight_per_thread: vec![0; num_threads],
⋮----
pub fn num_in_flight_per_thread(&self) -> &[usize] {
⋮----
pub fn cus_in_flight_per_thread(&self) -> &[u64] {
⋮----
pub fn track_batch(
⋮----
let batch_id = self.batch_id_generator.next();
⋮----
self.batches.insert(
⋮----
pub fn complete_batch(&mut self, batch_id: TransactionBatchId) -> ThreadId {
⋮----
}) = self.batches.remove(&batch_id)
⋮----
panic!("batch id {batch_id} is not being tracked");
⋮----
mod tests {
⋮----
fn test_in_flight_tracker_untracked_batch() {
⋮----
in_flight_tracker.complete_batch(TransactionBatchId::new(5));
⋮----
fn test_in_flight_tracker() {
⋮----
let batch_id_0 = in_flight_tracker.track_batch(2, 10_000, 0);
assert_eq!(in_flight_tracker.num_in_flight_per_thread(), &[2, 0]);
assert_eq!(in_flight_tracker.cus_in_flight_per_thread(), &[10_000, 0]);
let batch_id_1 = in_flight_tracker.track_batch(1, 15_000, 1);
assert_eq!(in_flight_tracker.num_in_flight_per_thread(), &[2, 1]);
assert_eq!(
⋮----
in_flight_tracker.complete_batch(batch_id_0);
assert_eq!(in_flight_tracker.num_in_flight_per_thread(), &[0, 1]);
assert_eq!(in_flight_tracker.cus_in_flight_per_thread(), &[0, 15_000]);
in_flight_tracker.complete_batch(batch_id_1);
assert_eq!(in_flight_tracker.num_in_flight_per_thread(), &[0, 0]);
assert_eq!(in_flight_tracker.cus_in_flight_per_thread(), &[0, 0]);

================
File: core/src/banking_stage/transaction_scheduler/mod.rs
================
mod batch_id_generator;
⋮----
pub mod greedy_scheduler;
⋮----
pub(crate) mod greedy_scheduler;
mod in_flight_tracker;
⋮----
pub mod prio_graph_scheduler;
⋮----
pub(crate) mod prio_graph_scheduler;
⋮----
pub mod receive_and_buffer;
⋮----
pub(crate) mod receive_and_buffer;
⋮----
pub mod scheduler;
⋮----
pub(crate) mod scheduler;
pub(crate) mod scheduler_common;
pub mod scheduler_controller;
pub(crate) mod scheduler_error;
⋮----
pub mod scheduler_metrics;
⋮----
mod scheduler_metrics;
mod transaction_priority_id;
⋮----
pub mod transaction_state;
⋮----
pub(crate) mod transaction_state;
⋮----
pub mod transaction_state_container;
⋮----
pub(crate) mod transaction_state_container;
pub(crate) mod bam_receive_and_buffer;
pub(crate) mod bam_scheduler;
pub(crate) mod bam_utils;

================
File: core/src/banking_stage/transaction_scheduler/prio_graph_scheduler.rs
================
use qualifier_attr::qualifiers;
⋮----
fn passthrough_priority(
⋮----
type SchedulerPrioGraph = PrioGraph<
⋮----
pub(crate) struct PrioGraphSchedulerConfig {
⋮----
impl Default for PrioGraphSchedulerConfig {
fn default() -> Self {
⋮----
pub(crate) struct PrioGraphScheduler<Tx> {
⋮----
pub(crate) fn new(
⋮----
fn schedule<S: StateContainer<Tx>>(
⋮----
let mut budget = budget.saturating_sub(
⋮----
.cus_in_flight_per_thread()
.iter()
.sum(),
⋮----
let starting_queue_size = container.queue_size();
let starting_buffer_size = container.buffer_size();
let num_threads = self.common.consume_work_senders.len();
⋮----
if self.common.in_flight_tracker.cus_in_flight_per_thread()[thread_id]
⋮----
schedulable_threads.remove(thread_id);
⋮----
if schedulable_threads.is_empty() {
return Ok(SchedulingSummary {
⋮----
let chunk_size = (*window_budget).min(MAX_FILTER_CHUNK_SIZE);
⋮----
if let Some(id) = container.pop() {
ids.push(id);
⋮----
*window_budget = window_budget.saturating_sub(chunk_size);
ids.iter().for_each(|id| {
let transaction = container.get_transaction(id.id).unwrap();
txs.push(transaction);
⋮----
measure_us!(pre_graph_filter(&txs, &mut filter_array[..chunk_size]));
⋮----
for (id, filter_result) in ids.iter().zip(&filter_array[..chunk_size]) {
⋮----
prio_graph.insert_transaction(
⋮----
container.remove_by_id(id.id);
⋮----
if ids.len() != chunk_size {
⋮----
chunked_pops(container, &mut self.prio_graph, &mut window_budget);
⋮----
debug_assert!(
⋮----
self.common.consume_work_senders.len() * self.config.target_transactions_per_batch,
⋮----
if self.prio_graph.is_empty() {
⋮----
while let Some(id) = self.prio_graph.pop() {
⋮----
unblock_this_batch.push(id);
let Some(transaction_state) = container.get_mut_transaction_state(id.id) else {
panic!("transaction state must exist")
⋮----
let maybe_schedule_info = try_schedule_transaction(
⋮----
select_thread(
⋮----
self.common.batches.total_cus(),
self.common.in_flight_tracker.cus_in_flight_per_thread(),
self.common.batches.transactions(),
self.common.in_flight_tracker.num_in_flight_per_thread(),
⋮----
unschedulable_ids.push(id);
⋮----
self.common.batches.add_transaction_to_batch(
⋮----
budget = budget.saturating_sub(cost);
if self.common.batches.transactions()[thread_id].len()
⋮----
num_sent += self.common.send_batch(thread_id)?;
⋮----
+ self.common.batches.total_cus()[thread_id]
⋮----
num_sent += self.common.send_batches()?;
window_budget += unblock_this_batch.len();
⋮----
for id in unblock_this_batch.drain(..) {
self.prio_graph.unblock(&id);
⋮----
container.push_ids_into_queue(unschedulable_ids.into_iter());
container.push_ids_into_queue(std::iter::from_fn(|| {
self.prio_graph.pop_and_unblock().map(|(id, _)| id)
⋮----
self.prio_graph.clear();
assert_eq!(
⋮----
Ok(SchedulingSummary {
⋮----
fn scheduling_common_mut(&mut self) -> &mut SchedulingCommon<Tx> {
⋮----
fn get_transaction_account_access(
⋮----
.account_keys()
⋮----
.enumerate()
.map(|(index, key)| {
if message.is_writable(index) {
⋮----
fn try_schedule_transaction<Tx: TransactionWithMeta>(
⋮----
match pre_lock_filter(transaction_state) {
⋮----
// Check if this transaction conflicts with any blocked transactions
let transaction = transaction_state.transaction();
if !blocking_locks.check_locks(transaction) {
blocking_locks.take_locks(transaction);
return Err(TransactionSchedulingError::UnschedulableConflicts);
⋮----
// Schedule the transaction if it can be.
let account_keys = transaction.account_keys();
⋮----
.filter_map(|(index, key)| transaction.is_writable(index).then_some(key));
⋮----
.filter_map(|(index, key)| (!transaction.is_writable(index)).then_some(key));
// Check bundle account locks doesn't have it yet
let l_account_locks = bundle_account_locker.account_locks();
for lock in read_account_locks.clone() {
if l_account_locks.write_locks().contains_key(lock) {
⋮----
for lock in write_account_locks.clone() {
if l_account_locks.write_locks().contains_key(lock)
|| l_account_locks.read_locks().contains_key(lock)
⋮----
let thread_id = match account_locks.try_lock_accounts(
⋮----
return Err(TransactionSchedulingError::UnschedulableThread);
⋮----
drop(l_account_locks);
let (transaction, max_age) = transaction_state.take_transaction_for_scheduling();
let cost = transaction_state.cost();
Ok(TransactionSchedulingInfo {
⋮----
mod tests {
⋮----
fn create_test_frame(
⋮----
(0..num_threads).map(|_| unbounded()).unzip();
let (finished_consume_work_sender, finished_consume_work_receiver) = unbounded();
⋮----
fn prioritized_tranfers(
⋮----
.into_iter()
.map(|pubkey| *pubkey.borrow())
.zip(std::iter::repeat(lamports))
.collect_vec();
⋮----
system_instruction::transfer_many(&from_keypair.pubkey(), &to_pubkeys_lamports);
⋮----
ixs.push(prioritization);
let message = Message::new(&ixs, Some(&from_keypair.pubkey()));
⋮----
fn create_container(
⋮----
create_container_with_capacity(100 * 1024, tx_infos)
⋮----
fn create_container_with_capacity(
⋮----
for (from_keypair, to_pubkeys, lamports, compute_unit_price) in tx_infos.into_iter() {
let transaction = prioritized_tranfers(
from_keypair.borrow(),
⋮----
container.insert_new_transaction(
⋮----
fn collect_work(
⋮----
.try_iter()
.map(|work| {
let ids = work.ids.clone();
⋮----
.unzip()
⋮----
fn test_pre_graph_filter(
⋮----
results.fill(true);
⋮----
fn test_pre_lock_filter(
⋮----
fn test_schedule_disconnected_channel() {
⋮----
create_test_frame(1, BundleAccountLocker::default());
let mut container = create_container([(&Keypair::new(), &[Pubkey::new_unique()], 1, 1)]);
drop(work_receivers);
assert_matches!(
⋮----
fn test_schedule_single_threaded_no_conflicts() {
⋮----
let mut container = create_container([
⋮----
.schedule(
⋮----
.unwrap();
assert_eq!(scheduling_summary.num_scheduled, 2);
assert_eq!(scheduling_summary.num_unschedulable_conflicts, 0);
assert_eq!(collect_work(&work_receivers[0]).1, vec![vec![1, 0]]);
⋮----
fn test_schedule_budget() {
⋮----
assert_eq!(scheduling_summary.num_scheduled, 0);
⋮----
fn test_schedule_single_threaded_conflict() {
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, vec![vec![1], vec![0]]);
⋮----
fn test_schedule_consume_single_threaded_multi_batch() {
⋮----
let mut container = create_container(
⋮----
.map(|i| (Keypair::new(), [Pubkey::new_unique()], i as u64, 1)),
⋮----
.map(|work| work.ids.len())
.collect();
assert_eq!(thread0_work_counts, [TARGET_NUM_TRANSACTIONS_PER_BATCH; 4]);
⋮----
fn test_schedule_simple_thread_selection() {
⋮----
create_test_frame(2, BundleAccountLocker::default());
⋮----
create_container((0..4).map(|i| (Keypair::new(), [Pubkey::new_unique()], 1, i)));
⋮----
assert_eq!(scheduling_summary.num_scheduled, 4);
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, [vec![3, 1]]);
assert_eq!(collect_work(&work_receivers[1]).1, [vec![2, 0]]);
⋮----
fn test_schedule_priority_guard() {
⋮----
let accounts = (0..8).map(|_| Keypair::new()).collect_vec();
⋮----
(&accounts[0], &[accounts[1].pubkey()], 1, 6),
(&accounts[2], &[accounts[3].pubkey()], 1, 5),
(&accounts[4], &[accounts[5].pubkey()], 1, 4),
(&accounts[6], &[accounts[7].pubkey()], 1, 3),
(&accounts[1], &[accounts[2].pubkey()], 1, 2),
(&accounts[2], &[accounts[3].pubkey()], 1, 1),
⋮----
assert_eq!(scheduling_summary.num_unschedulable_conflicts, 2);
let (thread_0_work, thread_0_ids) = collect_work(&work_receivers[0]);
assert_eq!(thread_0_ids, [vec![0], vec![2]]);
assert_eq!(collect_work(&work_receivers[1]).1, [vec![1], vec![3]]);
⋮----
.send(FinishedConsumeWork {
work: thread_0_work.into_iter().next().unwrap(),
retryable_indexes: vec![],
⋮----
.receive_completed(&mut container, &BufferedPacketsDecision::Hold)
⋮----
assert_eq!(collect_work(&work_receivers[1]).1, [vec![4], vec![5]]);
⋮----
fn test_schedule_over_full_container() {
⋮----
.map(|_| (Keypair::new(), [Pubkey::new_unique()], 1, 1))
⋮----
let mut container = create_container_with_capacity(capacity, txs);
⋮----
assert_eq!(scheduling_summary.num_scheduled, expected_num_scheduled);
⋮----
while let Some(_p) = container.pop() {
⋮----
fn test_schedule_with_bundle_account_locker() {
⋮----
let tx_1_a = transfer(&keypair_1, &keypair_1.pubkey(), 1, Hash::default());
let tx_1_b = transfer(&keypair_1, &keypair_1.pubkey(), 2, Hash::default());
let tx_2_a = transfer(&keypair_2, &keypair_2.pubkey(), 1, Hash::default());
let tx_2_b = transfer(&keypair_2, &keypair_2.pubkey(), 2, Hash::default());
⋮----
let runtime_tx_1_b = vec![runtime_tx_1_b];
⋮----
let runtime_tx_2_b = vec![runtime_tx_2_b];
⋮----
create_test_frame(1, bundle_account_locker.clone());
⋮----
container.insert_new_transaction(runtime_tx_1_a, MaxAge::MAX, 1, 5000);
container.insert_new_transaction(runtime_tx_2_a, MaxAge::MAX, 1, 5000);
⋮----
.lock_bundle(&runtime_tx_1_b, &bank)
⋮----
assert_eq!(scheduling_summary.num_scheduled, 1);
assert_eq!(collect_work(&work_receivers[0]).1, [vec![1]]);
⋮----
.unlock_bundle(&runtime_tx_1_b, &bank)
⋮----
.lock_bundle(&runtime_tx_2_b, &bank)
⋮----
assert_eq!(collect_work(&work_receivers[0]).1, [vec![0]]);
⋮----
.unlock_bundle(&runtime_tx_2_b, &bank)

================
File: core/src/banking_stage/transaction_scheduler/receive_and_buffer.rs
================
use qualifier_attr::qualifiers;
⋮----
pub(crate) struct DisconnectedError;
⋮----
pub(crate) struct ReceivingStats {
⋮----
impl ReceivingStats {
pub(crate) fn accumulate(&mut self, other: ReceivingStats) {
⋮----
pub(crate) trait ReceiveAndBuffer {
⋮----
pub(crate) struct TransactionViewReceiveAndBuffer {
⋮----
impl ReceiveAndBuffer for TransactionViewReceiveAndBuffer {
type Transaction = RuntimeTransaction<ResolvedTransactionView<SharedBytes>>;
type Container = TransactionViewStateContainer;
fn receive_and_buffer_packets(
⋮----
let bank_forks = self.bank_forks.read().unwrap();
let root_bank = bank_forks.root_bank();
let working_bank = bank_forks.working_bank();
⋮----
if container.is_empty()
&& matches!(
⋮----
match self.receiver.recv_timeout(TIMEOUT) {
⋮----
stats.accumulate(self.handle_packet_batch_message(
⋮----
return Err(DisconnectedError);
⋮----
while start.elapsed() < TIMEOUT && stats.num_received < PACKET_BURST_LIMIT {
match self.receiver.try_recv() {
⋮----
stats.receive_time_us += start.elapsed().as_micros() as u64;
⋮----
let batch_stats = self.handle_packet_batch_message(
⋮----
stats.accumulate(batch_stats);
⋮----
Ok(ReceivingStats {
⋮----
pub enum PacketHandlingError {
⋮----
impl TransactionViewReceiveAndBuffer {
fn handle_packet_batch_message(
⋮----
let should_parse = !matches!(decision, BufferedPacketsDecision::Forward);
⋮----
.is_active(&agave_feature_set::static_instruction_limit::ID);
let transaction_account_lock_limit = working_bank.get_transaction_account_lock_limit();
⋮----
let lock_results: [_; EXTRA_CAPACITY] = core::array::from_fn(|_| Ok(()));
⋮----
transactions.extend(transaction_priority_ids.iter().map(|priority_id| {
⋮----
.get_transaction(priority_id.id)
.expect("transaction must exist")
⋮----
&lock_results[..transactions.len()],
⋮----
.iter_mut()
.zip(transaction_priority_ids.iter())
⋮----
container.remove_by_id(priority_id.id);
⋮----
.expect("transaction must exist");
⋮----
*result = Err(err);
⋮----
num_dropped_on_capacity += container.push_ids_into_queue(
⋮----
.into_iter()
.zip(transaction_priority_ids.drain(..))
.filter(|(r, _)| r.is_ok())
.map(|(_, id)| id),
⋮----
for packet_batch in packet_batch_message.iter() {
for packet in packet_batch.iter() {
let Some(packet_data) = packet.data(..) else {
⋮----
container.try_insert_map_only_with_data(packet_data, |bytes| {
⋮----
Ok(state) => Ok(state),
⋮----
Err(())
⋮----
.get_mut_transaction_state(transaction_id)
⋮----
.priority();
⋮----
.push(TransactionPriorityId::new(priority, transaction_id));
if transaction_priority_ids.len() == EXTRA_CAPACITY {
check_and_push_to_queue(container, &mut transaction_priority_ids);
⋮----
buffer_time_us: start.elapsed().as_micros() as u64,
⋮----
fn try_handle_packet(
⋮----
let (view, deactivation_slot) = translate_to_runtime_view(
⋮----
if validate_account_locks(
view.account_keys(),
root_bank.get_transaction_account_lock_limit(),
⋮----
.is_err()
⋮----
return Err(PacketHandlingError::LockValidation);
⋮----
.account_keys()
.iter()
.any(|account| blacklisted_accounts.contains(account))
⋮----
return Err(PacketHandlingError::BlacklistedAccount);
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(&working_bank.feature_set)
⋮----
return Err(PacketHandlingError::ComputeBudget);
⋮----
let max_age = calculate_max_age(root_bank.epoch(), deactivation_slot, root_bank.slot());
⋮----
let (priority, cost) = calculate_priority_and_cost(&view, &fee_budget_limits, working_bank);
Ok(TransactionState::new(view, max_age, priority, cost))
⋮----
pub(crate) fn translate_to_runtime_view<D: TransactionData>(
⋮----
return Err(PacketHandlingError::Sanitization);
⋮----
if bank.vote_only_bank() && !view.is_simple_vote_transaction() {
⋮----
if usize::from(view.total_num_accounts()) > transaction_account_lock_limit {
⋮----
let (loaded_addresses, deactivation_slot) = load_addresses_for_view(&view, bank)?;
⋮----
bank.get_reserved_account_keys(),
⋮----
Ok((view, deactivation_slot))
⋮----
pub(crate) fn load_addresses_for_view<D: TransactionData>(
⋮----
match view.version() {
TransactionVersion::Legacy => Ok((None, u64::MAX)),
⋮----
.load_addresses_from_ref(view.address_table_lookup_iter())
.map(|(loaded_addresses, deactivation_slot)| {
(Some(loaded_addresses), deactivation_slot)
⋮----
.map_err(|_| PacketHandlingError::ALTResolution),
⋮----
pub(crate) fn calculate_priority_and_cost(
⋮----
let cost = CostModel::calculate_cost(transaction, &bank.feature_set).sum();
let reward = bank.calculate_reward_for_transaction(transaction, fee_budget_limits);
⋮----
.saturating_mul(MULTIPLIER)
.saturating_div(cost.saturating_add(1)),
⋮----
pub(crate) fn calculate_max_age(
⋮----
let alt_min_expire_slot = estimate_last_valid_slot(deactivation_slot.min(current_slot));
⋮----
mod tests {
⋮----
fn test_bank_forks() -> (Arc<RwLock<BankForks>>, Keypair) {
⋮----
} = create_slow_genesis_config(u64::MAX);
⋮----
fn setup_transaction_view_receive_and_buffer(
⋮----
fn verify_container<Tx: TransactionWithMeta>(
⋮----
while let Some(id) = container.pop() {
let Some(_) = container.get_transaction(id.id) else {
panic!(
⋮----
assert_eq!(actual_length, expected_length);
⋮----
fn test_calculate_max_age() {
⋮----
assert_eq!(
⋮----
fn test_receive_and_buffer_disconnected_channel() {
let (sender, receiver) = unbounded();
let (bank_forks, _mint_keypair) = test_bank_forks();
⋮----
setup_transaction_view_receive_and_buffer(receiver, bank_forks, HashSet::new());
drop(sender);
⋮----
.receive_and_buffer_packets(&mut container, &BufferedPacketsDecision::Hold);
assert!(r.is_err());
⋮----
fn test_receive_and_buffer_no_hold() {
⋮----
let (bank_forks, mint_keypair) = test_bank_forks();
⋮----
setup_transaction_view_receive_and_buffer(receiver, bank_forks.clone(), HashSet::new());
let transaction = transfer(
⋮----
bank_forks.read().unwrap().root_bank().last_blockhash(),
⋮----
let packet_batches = Arc::new(to_packet_batches(&[transaction], 1));
sender.send(packet_batches).unwrap();
⋮----
.receive_and_buffer_packets(
⋮----
.unwrap();
assert_eq!(num_received, 1);
assert_eq!(num_dropped_without_parsing, 1);
assert_eq!(num_dropped_on_parsing_and_sanitization, 0);
assert_eq!(num_dropped_on_lock_validation, 0);
assert_eq!(num_dropped_on_compute_budget, 0);
assert_eq!(num_dropped_on_age, 0);
assert_eq!(num_dropped_on_already_processed, 0);
assert_eq!(num_dropped_on_fee_payer, 0);
assert_eq!(num_dropped_on_capacity, 0);
assert_eq!(num_buffered, 0);
verify_container(&mut container, 0);
⋮----
fn test_receive_and_buffer_discard() {
⋮----
let mut packet_batches = Arc::new(to_packet_batches(&[transaction], 1));
⋮----
.first_mut()
.unwrap()
.meta_mut()
.set_discard(true);
⋮----
.receive_and_buffer_packets(&mut container, &BufferedPacketsDecision::Hold)
⋮----
assert_eq!(num_received, 0);
assert_eq!(num_dropped_without_parsing, 0);
⋮----
fn test_receive_and_buffer_invalid_transaction_format() {
⋮----
let packet_batches = Arc::new(vec![PacketBatch::from(RecycledPacketBatch::new(vec![
⋮----
assert_eq!(num_dropped_on_parsing_and_sanitization, 1);
⋮----
fn test_receive_and_buffer_invalid_blockhash() {
⋮----
let transaction = transfer(&mint_keypair, &Pubkey::new_unique(), 1, Hash::new_unique());
⋮----
assert_eq!(num_dropped_on_age, 1);
⋮----
fn test_receive_and_buffer_simple_transfer_unfunded_fee_payer() {
⋮----
assert_eq!(num_dropped_on_fee_payer, 1);
⋮----
fn test_receive_and_buffer_failed_alt_resolve() {
⋮----
&mint_keypair.pubkey(),
⋮----
addresses: vec![to_pubkey],
⋮----
.unwrap(),
⋮----
fn test_receive_and_buffer_simple_transfer() {
⋮----
assert_eq!(num_buffered, 1);
verify_container(&mut container, 1);
⋮----
fn test_receive_and_buffer_overfull() {
⋮----
let transactions = Vec::from_iter((0..num_transactions).map(|_| {
transfer(
⋮----
let packet_batches = Arc::new(to_packet_batches(&transactions, 17));
⋮----
assert_eq!(num_received, num_transactions);
⋮----
assert!(num_dropped_on_capacity > 0);
assert_eq!(num_buffered, num_transactions);
verify_container(&mut container, TEST_CONTAINER_CAPACITY);
⋮----
fn test_receive_and_buffer_too_many_keys() {
fn create_tx_with_n_keys(payer: &Keypair, n: usize) -> VersionedTransaction {
let alt_keys = (0..n - 2).map(|_| Pubkey::new_unique()).collect::<Vec<_>>();
⋮----
&payer.pubkey(),
⋮----
.map(|k| AccountMeta::new(*k, false))
⋮----
.read()
⋮----
.root_bank()
.get_transaction_account_lock_limit();
let bad_tx = create_tx_with_n_keys(&mint_keypair, transaction_account_lock_limit + 1);
⋮----
assert_eq!(num_dropped_on_lock_validation, 1);
⋮----
fn test_receive_blacklisted_account() {
⋮----
let blacklisted_accounts = HashSet::from_iter([keypair.pubkey()]);
⋮----
let (mut receive_and_buffer, mut container) = setup_transaction_view_receive_and_buffer(
⋮----
bank_forks.clone(),
⋮----
assert_eq!(r.num_dropped_on_blacklisted_account, 1);

================
File: core/src/banking_stage/transaction_scheduler/scheduler_common.rs
================
use qualifier_attr::qualifiers;
⋮----
pub struct Batches<Tx> {
⋮----
pub fn new(num_threads: usize, target_num_transactions_per_batch: usize) -> Self {
fn make_vecs<T>(
⋮----
.map(|_| Vec::with_capacity(target_num_transactions_per_batch))
.collect()
⋮----
ids: make_vecs(num_threads, target_num_transactions_per_batch),
transactions: make_vecs(num_threads, target_num_transactions_per_batch),
max_ages: make_vecs(num_threads, target_num_transactions_per_batch),
total_cus: vec![0; num_threads],
⋮----
pub fn is_empty(&self) -> bool {
self.ids.iter().all(|ids| ids.is_empty())
&& self.transactions.iter().all(|txs| txs.is_empty())
&& self.max_ages.iter().all(|max_ages| max_ages.is_empty())
&& self.total_cus.iter().all(|&cus| cus == 0)
⋮----
pub fn total_cus(&self) -> &[u64] {
⋮----
pub fn transactions(&self) -> &[Vec<Tx>] {
⋮----
pub fn add_transaction_to_batch(
⋮----
self.ids[thread_id].push(transaction_id);
self.transactions[thread_id].push(transaction);
self.max_ages[thread_id].push(max_age);
⋮----
pub fn take_batch(
⋮----
pub struct TransactionSchedulingInfo<Tx> {
⋮----
pub enum TransactionSchedulingError {
⋮----
pub fn select_thread<Tx>(
⋮----
.contained_threads_iter()
.map(|thread_id| {
⋮----
batches_per_thread[thread_id].len() + in_flight_per_thread[thread_id],
⋮----
.min_by(|a, b| a.1.cmp(&b.1).then_with(|| a.2.cmp(&b.2)))
.map(|(thread_id, _, _)| thread_id)
.unwrap()
⋮----
pub(crate) struct SchedulingCommon<Tx> {
⋮----
pub fn new(
⋮----
let num_threads = consume_work_senders.len();
assert!(num_threads > 0, "must have at least one worker");
assert!(
⋮----
consume_work_senders.len(),
⋮----
pub fn send_batch(&mut self, thread_index: usize) -> Result<usize, SchedulerError> {
if self.batches.ids[thread_index].is_empty() {
return Ok(0);
⋮----
let (ids, transactions, max_ages, total_cus) = self.batches.take_batch(thread_index);
⋮----
.track_batch(ids.len(), total_cus, thread_index);
let num_scheduled = ids.len();
⋮----
.send(work)
.map_err(|_| SchedulerError::DisconnectedSendChannel("consume work sender"))?;
Ok(num_scheduled)
⋮----
pub fn send_batches(&mut self) -> Result<usize, SchedulerError> {
(0..self.consume_work_senders.len())
.map(|thread_index| self.send_batch(thread_index))
.sum()
⋮----
pub fn try_receive_completed(
⋮----
match self.finished_consume_work_receiver.try_recv() {
⋮----
let num_transactions = ids.len();
let num_retryable = retryable_indexes.len();
self.complete_batch(batch_id, &transactions);
let mut retryable_iter = retryable_indexes.iter().peekable();
for (index, (id, transaction)) in izip!(ids, transactions).enumerate() {
if let Some(&retryable_index) = retryable_iter.peek() {
⋮----
container.retry_transaction(
⋮----
retryable_iter.next();
⋮----
container.remove_by_id(id);
⋮----
debug_assert!(
⋮----
Ok((num_transactions, num_retryable))
⋮----
Err(TryRecvError::Empty) => Ok((0, 0)),
Err(TryRecvError::Disconnected) => Err(SchedulerError::DisconnectedRecvChannel(
⋮----
fn complete_batch(&mut self, batch_id: TransactionBatchId, transactions: &[Tx]) {
let thread_id = self.in_flight_tracker.complete_batch(batch_id);
⋮----
let account_keys = transaction.account_keys();
⋮----
.iter()
.enumerate()
.filter_map(|(index, key)| transaction.is_writable(index).then_some(key));
⋮----
.filter_map(|(index, key)| (!transaction.is_writable(index)).then_some(key));
⋮----
.unlock_accounts(write_account_locks, read_account_locks, thread_id);
⋮----
mod tests {
⋮----
fn simple_transaction() -> RuntimeTransaction<SanitizedTransaction> {
⋮----
fn add_transactions_to_container(
⋮----
container.insert_new_transaction(
simple_transaction(),
⋮----
fn pop_and_add_transaction<Tx: TransactionWithMeta>(
⋮----
let tx_id = container.pop().unwrap();
⋮----
.get_mut_transaction_state(tx_id.id)
⋮----
.take_transaction_for_scheduling();
⋮----
.try_lock_accounts(
⋮----
.unwrap();
common.batches.add_transaction_to_batch(
⋮----
fn test_select_thread(
⋮----
let selected_thread = select_thread(
⋮----
assert_eq!(selected_thread, expected_thread);
⋮----
fn test_send_batches() {
⋮----
add_transactions_to_container(&mut container, 3);
⋮----
(0..NUM_WORKERS).map(|_| unbounded()).unzip();
let (_finished_work_sender, finished_work_receiver) = unbounded();
⋮----
pop_and_add_transaction(&mut container, &mut common, 0);
let num_scheduled = common.send_batch(0).unwrap();
assert_eq!(num_scheduled, 1);
assert_eq!(work_receivers[0].len(), 1);
assert_eq!(
⋮----
let num_scheduled = common.send_batch(1).unwrap();
assert_eq!(num_scheduled, 0);
assert_eq!(work_receivers[1].len(), 0);
work_receivers[0].recv().unwrap();
⋮----
pop_and_add_transaction(&mut container, &mut common, 2);
common.send_batches().unwrap();
⋮----
assert_eq!(work_receivers[2].len(), 1);
assert_eq!(work_receivers[3].len(), 0);
⋮----
fn test_receive_completed() {
⋮----
add_transactions_to_container(&mut container, 1);
⋮----
let (finished_work_sender, finished_work_receiver) = unbounded();
⋮----
let work = work_receivers[0].try_recv().unwrap();
assert_eq!(work.ids.len(), num_scheduled);
let retryable_indexes = vec![];
⋮----
finished_work_sender.send(finished_work).unwrap();
⋮----
common.try_receive_completed(&mut container).unwrap();
assert_eq!(num_transactions, num_scheduled);
assert_eq!(num_retryable, 0);
assert_eq!(container.buffer_size(), 0);
⋮----
let retryable_indexes = vec![
⋮----
let expected_num_retryable = retryable_indexes.len();
⋮----
assert_eq!(num_retryable, expected_num_retryable);
assert_eq!(container.buffer_size(), expected_num_retryable);
assert_eq!(container.queue_size(), expected_num_retryable - 1);
⋮----
fn test_receive_completed_out_of_order() {
⋮----
add_transactions_to_container(&mut container, 2);
⋮----
let retryable_indexes = vec![RetryableIndex::new(1, true), RetryableIndex::new(0, true)];
⋮----
let _ = common.try_receive_completed(&mut container);

================
File: core/src/banking_stage/transaction_scheduler/scheduler_controller.rs
================
pub struct SchedulerConfig {
⋮----
impl Default for SchedulerConfig {
fn default() -> Self {
⋮----
NonZeroU64::new(350).unwrap();
pub(crate) struct SchedulerController<R, S>
⋮----
pub fn new(
⋮----
pub fn run(mut self) -> Result<(), SchedulerError> {
⋮----
while !self.exit.load(Ordering::Relaxed) {
⋮----
measure_us!(self.decision_maker.make_consume_or_forward_decision());
self.timing_metrics.update(|timing_metrics| {
⋮----
let new_leader_slot = decision.bank().map(|b| b.slot());
⋮----
.maybe_report_and_reset_slot(new_leader_slot);
⋮----
self.container.flush_held_transactions();
⋮----
cost_pacer = decision.bank().map(|b| {
let cost_tracker = b.read_cost_tracker().unwrap();
let block_limit = cost_tracker.get_block_limit();
let shared_block_cost = cost_tracker.shared_block_cost();
drop(cost_tracker);
let fill_time = self.config.scheduler_pacing.fill_time();
if let Some(pacing_fill_time) = fill_time.as_ref() {
if pacing_fill_time.as_nanos() > b.ns_per_slot {
warn!(
⋮----
.unwrap_or(NonZeroU64::new(1).unwrap()),
⋮----
self.receive_completed(&decision)?;
self.process_transactions(&decision, cost_pacer.as_ref(), &now)?;
if self.receive_and_buffer_packets(&decision).is_err() {
⋮----
let should_report = self.count_metrics.interval_has_data() && self.scheduling_enabled();
let priority_min_max = self.container.get_min_max_priority();
self.count_metrics.update(|count_metrics| {
count_metrics.update_priority_stats(priority_min_max);
⋮----
.maybe_report_and_reset_interval(should_report);
⋮----
.iter()
.for_each(|metrics| metrics.maybe_report_and_reset());
self.scheduling_details.maybe_report();
⋮----
Ok(())
⋮----
fn process_transactions(
⋮----
if !self.scheduling_enabled() {
return Ok(());
⋮----
cost_pacer.scheduling_budget(now)
⋮----
let (scheduling_summary, schedule_time_us) = measure_us!(self.scheduler.schedule(
⋮----
self.scheduling_details.update(&scheduling_summary);
⋮----
let (_, clear_time_us) = measure_us!(self.clear_container());
⋮----
let (_, clean_time_us) = measure_us!(self.clean_queue());
⋮----
fn pre_graph_filter(
⋮----
let lock_results = vec![Ok(()); transactions.len()];
⋮----
.into_iter()
.zip(transactions)
.zip(results.iter_mut())
⋮----
.and_then(|_| Consumer::check_fee_payer_unlocked(bank, *tx, &mut error_counters))
.is_ok();
⋮----
fn clear_container(&mut self) {
⋮----
while let Some(id) = self.container.pop() {
self.container.remove_by_id(id.id);
⋮----
fn clean_queue(&mut self) {
⋮----
while transaction_ids.len() < MAX_TRANSACTION_CHECKS {
let Some(id) = self.container.pop() else {
⋮----
transaction_ids.push(id);
⋮----
let bank = self.bank_forks.read().unwrap().working_bank();
⋮----
for chunk in transaction_ids.chunks(CHUNK_SIZE) {
let lock_results = vec![Ok(()); chunk.len()];
⋮----
.map(|id| {
⋮----
.get_transaction(id.id)
.expect("transaction must exist")
⋮----
.collect();
⋮----
for (result, id) in check_results.iter().zip(chunk.iter()) {
if result.is_err() {
⋮----
self.container.push_ids_into_queue(
⋮----
.zip(chunk.iter())
.filter(|(r, _)| r.is_ok())
.map(|(_, id)| *id),
⋮----
fn receive_completed(
⋮----
let ((num_transactions, num_retryable), receive_completed_time_us) = measure_us!(self
⋮----
fn receive_and_buffer_packets(
⋮----
.receive_and_buffer_packets(&mut self.container, decision)?;
⋮----
Ok(receiving_stats)
⋮----
fn scheduling_enabled(&self) -> bool {
let bam_connected = BamConnectionState::from_u8(self.bam_enabled.load(Ordering::Relaxed))
⋮----
struct CostPacer {
⋮----
impl CostPacer {
fn scheduling_budget(&self, current_time: &Instant) -> u64 {
⋮----
let time_since = current_time.saturating_duration_since(self.detection_time);
⋮----
let allocation_per_milli = self.block_limit / fill_time.as_millis() as u64;
let millis_since_detection = time_since.as_millis() as u64;
⋮----
target.saturating_sub(self.shared_block_cost.load())
⋮----
mod tests {
⋮----
fn create_channels<T>(num: usize) -> (Vec<Sender<T>>, Vec<Receiver<T>>) {
(0..num).map(|_| unbounded()).unzip()
⋮----
struct TestFrame<Tx> {
⋮----
fn test_create_transaction_view_receive_and_buffer(
⋮----
fn create_test_frame<R: ReceiveAndBuffer>(
⋮----
} = create_slow_genesis_config(u64::MAX);
⋮----
let decision_maker = DecisionMaker::new(shared_leader_state.clone());
let (banking_packet_sender, banking_packet_receiver) = unbounded();
let receive_and_buffer = create_receive_and_buffer(
⋮----
bank_forks.clone(),
⋮----
let (consume_work_senders, consume_work_receivers) = create_channels(num_threads);
let (finished_consume_work_sender, finished_consume_work_receiver) = unbounded();
⋮----
vec![],
⋮----
fn create_and_fund_prioritized_transfer(
⋮----
&from_keypair.pubkey(),
⋮----
bank.last_blockhash(),
⋮----
bank.process_transaction(&transfer).unwrap();
⋮----
let transfer = system_instruction::transfer(&from_keypair.pubkey(), to_pubkey, lamports);
⋮----
let message = Message::new(&[transfer, prioritization], Some(&from_keypair.pubkey()));
Transaction::new(&vec![from_keypair], message, recent_blockhash)
⋮----
fn to_banking_packet_batch(txs: &[Transaction]) -> BankingPacketBatch {
BankingPacketBatch::new(to_packet_batches(txs, NUM_PACKETS))
⋮----
fn test_receive_then_schedule<R: ReceiveAndBuffer>(
⋮----
.make_consume_or_forward_decision();
assert!(matches!(decision, BufferedPacketsDecision::Consume(_)));
assert!(scheduler_controller.receive_completed(&decision).is_ok());
⋮----
.receive_and_buffer_packets(&decision)
.map(|n| n.num_received > 0)
.unwrap_or_default()
⋮----
assert!(scheduler_controller
⋮----
fn test_unexpected_batch_id() {
let (test_frame, scheduler_controller) = create_test_frame(
⋮----
.send(FinishedConsumeWork {
⋮----
ids: vec![],
transactions: vec![],
max_ages: vec![],
⋮----
retryable_indexes: vec![],
⋮----
.unwrap();
scheduler_controller.run().unwrap();
⋮----
fn test_schedule_consume_single_threaded_no_conflicts() {
let (mut test_frame, mut scheduler_controller) = create_test_frame(
⋮----
shared_leader_state.store(Arc::new(LeaderState::new(
Some(bank.clone()),
bank.tick_height(),
⋮----
let tx1 = create_and_fund_prioritized_transfer(
⋮----
let tx2 = create_and_fund_prioritized_transfer(
⋮----
let tx1_hash = tx1.message().hash();
let tx2_hash = tx2.message().hash();
let txs = vec![tx1, tx2];
⋮----
.send(to_banking_packet_batch(&txs))
⋮----
test_receive_then_schedule(&mut scheduler_controller);
let consume_work = consume_work_receivers[0].try_recv().unwrap();
assert_eq!(consume_work.ids.len(), 2);
assert_eq!(consume_work.transactions.len(), 2);
⋮----
.map(|tx| tx.message_hash())
.collect_vec();
assert_eq!(message_hashes, vec![&tx2_hash, &tx1_hash]);
⋮----
fn test_schedule_consume_single_threaded_conflict() {
⋮----
.map(|_| consume_work_receivers[0].try_recv().unwrap())
⋮----
let num_txs_per_batch = consume_works.iter().map(|cw| cw.ids.len()).collect_vec();
⋮----
.flat_map(|cw| cw.transactions.iter().map(|tx| tx.message_hash()))
⋮----
assert_eq!(num_txs_per_batch, vec![1; 2]);
⋮----
fn test_schedule_consume_single_threaded_multi_batch() {
⋮----
.map(|i| {
create_and_fund_prioritized_transfer(
⋮----
.send(to_banking_packet_batch(&txs1))
⋮----
.send(to_banking_packet_batch(&txs2))
⋮----
assert_eq!(
⋮----
fn test_schedule_consume_simple_thread_selection() {
⋮----
.map(|i| txs[i].message().hash())
⋮----
.try_recv()
.unwrap()
⋮----
.map(|tx| *tx.message_hash())
⋮----
assert_eq!(t0_actual, t0_expected);
assert_eq!(t1_actual, t1_expected);
⋮----
fn test_schedule_consume_retryable() {
⋮----
retryable_indexes: vec![RetryableIndex::new(1, true)],
⋮----
assert_eq!(consume_work.ids.len(), 1);
assert_eq!(consume_work.transactions.len(), 1);
⋮----
assert_eq!(message_hashes, vec![&tx1_hash]);

================
File: core/src/banking_stage/transaction_scheduler/scheduler_error.rs
================
use thiserror::Error;
⋮----
pub enum SchedulerError {

================
File: core/src/banking_stage/transaction_scheduler/scheduler_metrics.rs
================
pub struct SchedulerCountMetrics {
⋮----
impl SchedulerCountMetrics {
pub fn update(&mut self, update: impl Fn(&mut SchedulerCountMetricsInner)) {
update(&mut self.interval.metrics);
update(&mut self.slot.metrics);
⋮----
pub fn maybe_report_and_reset_slot(&mut self, slot: Option<Slot>) {
self.slot.maybe_report_and_reset(slot);
⋮----
pub fn maybe_report_and_reset_interval(&mut self, should_report: bool) {
self.interval.maybe_report_and_reset(should_report);
⋮----
pub fn interval_has_data(&self) -> bool {
self.interval.metrics.has_data()
⋮----
struct IntervalSchedulerCountMetrics {
⋮----
struct SlotSchedulerCountMetrics {
⋮----
pub struct SchedulerCountMetricsInner {
⋮----
impl IntervalSchedulerCountMetrics {
fn maybe_report_and_reset(&mut self, should_report: bool) {
⋮----
if self.interval.should_update(REPORT_INTERVAL_MS) {
⋮----
self.metrics.report("banking_stage_scheduler_counts", None);
⋮----
self.metrics.reset();
⋮----
impl SlotSchedulerCountMetrics {
fn maybe_report_and_reset(&mut self, slot: Option<Slot>) {
⋮----
if self.slot.is_some() {
⋮----
.report("banking_stage_scheduler_slot_counts", self.slot);
⋮----
impl SchedulerCountMetricsInner {
fn report(&self, name: &'static str, slot: Option<Slot>) {
⋮----
let mut datapoint = create_datapoint!(
⋮----
datapoint.add_field_i64("slot", slot as i64);
⋮----
fn has_data(&self) -> bool {
self.num_received != Saturating(0)
|| self.num_buffered != Saturating(0)
|| self.num_scheduled != Saturating(0)
|| self.num_unschedulable_conflicts != Saturating(0)
|| self.num_unschedulable_threads != Saturating(0)
|| self.num_schedule_filtered_out != Saturating(0)
|| self.num_finished != Saturating(0)
|| self.num_retryable != Saturating(0)
|| self.num_dropped_on_blacklisted_account != Saturating(0)
⋮----
fn reset(&mut self) {
self.num_received = Saturating(0);
self.num_buffered = Saturating(0);
self.num_scheduled = Saturating(0);
self.num_unschedulable_conflicts = Saturating(0);
self.num_unschedulable_threads = Saturating(0);
self.num_schedule_filtered_out = Saturating(0);
self.num_finished = Saturating(0);
self.num_retryable = Saturating(0);
self.num_dropped_on_receive = Saturating(0);
self.num_dropped_on_parsing_and_sanitization = Saturating(0);
self.num_dropped_on_validate_locks = Saturating(0);
self.num_dropped_on_receive_compute_budget = Saturating(0);
self.num_dropped_on_receive_age = Saturating(0);
self.num_dropped_on_receive_already_processed = Saturating(0);
self.num_dropped_on_receive_fee_payer = Saturating(0);
self.num_dropped_on_clear = Saturating(0);
self.num_dropped_on_clean = Saturating(0);
self.num_dropped_on_capacity = Saturating(0);
⋮----
self.num_dropped_on_blacklisted_account = Saturating(0);
⋮----
pub fn update_priority_stats(&mut self, min_max_fees: MinMaxResult<u64>) {
// update min/max priority
⋮----
// do nothing
⋮----
fn get_min_priority(&self) -> u64 {
// to avoid getting u64::max recorded by metrics / in case of edge cases
⋮----
fn get_max_priority(&self) -> u64 {
⋮----
pub struct SchedulerTimingMetrics {
⋮----
impl SchedulerTimingMetrics {
pub fn update(&mut self, update: impl Fn(&mut SchedulerTimingMetricsInner)) {
⋮----
struct IntervalSchedulerTimingMetrics {
⋮----
struct SlotSchedulerTimingMetrics {
⋮----
pub struct SchedulerTimingMetricsInner {
/// Time spent making processing decisions.
    pub decision_time_us: Saturating<u64>,
/// Time spent receiving packets.
    pub receive_time_us: Saturating<u64>,
/// Time spent buffering packets.
    pub buffer_time_us: Saturating<u64>,
/// Time spent filtering transactions during scheduling.
    pub schedule_filter_time_us: Saturating<u64>,
/// Time spent scheduling transactions.
    pub schedule_time_us: Saturating<u64>,
/// Time spent clearing transactions from the container.
    pub clear_time_us: Saturating<u64>,
/// Time spent cleaning expired or processed transactions from the container.
    pub clean_time_us: Saturating<u64>,
/// Time spent receiving completed transactions.
    pub receive_completed_time_us: Saturating<u64>,
⋮----
impl IntervalSchedulerTimingMetrics {
⋮----
self.metrics.report("banking_stage_scheduler_timing", None);
⋮----
impl SlotSchedulerTimingMetrics {
⋮----
// Only report if there was an assigned slot.
⋮----
.report("banking_stage_scheduler_slot_timing", self.slot);
⋮----
impl SchedulerTimingMetricsInner {
⋮----
self.decision_time_us = Saturating(0);
self.receive_time_us = Saturating(0);
self.buffer_time_us = Saturating(0);
self.schedule_filter_time_us = Saturating(0);
self.schedule_time_us = Saturating(0);
self.clear_time_us = Saturating(0);
self.clean_time_us = Saturating(0);
self.receive_completed_time_us = Saturating(0);
⋮----
pub struct SchedulingDetails {
⋮----
impl Default for SchedulingDetails {
fn default() -> Self {
⋮----
impl SchedulingDetails {
pub fn update(&mut self, scheduling_summary: &SchedulingSummary) {
⋮----
.min(scheduling_summary.starting_queue_size);
⋮----
.max(scheduling_summary.starting_queue_size);
⋮----
.min(scheduling_summary.starting_buffer_size);
⋮----
.max(scheduling_summary.starting_buffer_size);
⋮----
pub fn maybe_report(&mut self) {
⋮----
if now.duration_since(self.last_report) > REPORT_INTERVAL {
⋮----
datapoint_info!(

================
File: core/src/banking_stage/transaction_scheduler/scheduler.rs
================
use qualifier_attr::qualifiers;
⋮----
pub(crate) trait Scheduler<Tx: TransactionWithMeta> {
⋮----
fn receive_completed(
⋮----
.scheduling_common_mut()
.try_receive_completed(container)?;
⋮----
Ok((total_num_transactions, total_num_retryable))
⋮----
pub(crate) enum PreLockFilterAction {
⋮----
pub(crate) struct SchedulingSummary {

================
File: core/src/banking_stage/transaction_scheduler/transaction_priority_id.rs
================
use qualifier_attr::qualifiers;
⋮----
pub(crate) struct TransactionPriorityId {
⋮----
impl TransactionPriorityId {
pub(crate) fn new(priority: u64, id: TransactionId) -> Self {
⋮----
impl Hash for TransactionPriorityId {
fn hash<H: Hasher>(&self, state: &mut H) {
self.id.hash(state)
⋮----
fn id(&self) -> Self {
⋮----
mod tests {
⋮----
fn test_transaction_priority_id_ordering() {
⋮----
assert!(id1 < id2);
assert!(id1 <= id2);
assert!(id2 > id1);
assert!(id2 >= id1);
⋮----
assert_eq!(id1, id2);
assert!(id1 >= id2);
⋮----
assert!(id2 <= id1);

================
File: core/src/banking_stage/transaction_scheduler/transaction_state_container.rs
================
use qualifier_attr::qualifiers;
⋮----
pub(crate) struct TransactionStateContainer<Tx: TransactionWithMeta> {
⋮----
struct BatchInfo {
⋮----
enum BatchIdOrTransactionState<Tx: TransactionWithMeta> {
⋮----
pub(crate) trait StateContainer<Tx: TransactionWithMeta> {
⋮----
fn retry_transaction(
⋮----
.get_mut_transaction_state(transaction_id)
.expect("transaction must exist");
let priority_id = TransactionPriorityId::new(transaction_state.priority(), transaction_id);
transaction_state.retry_transaction(transaction);
⋮----
self.push_ids_into_queue(std::iter::once(priority_id));
⋮----
self.hold_transaction(priority_id);
⋮----
fn with_capacity(capacity: usize) -> Self {
⋮----
fn queue_size(&self) -> usize {
self.priority_queue.len()
⋮----
fn buffer_size(&self) -> usize {
self.id_to_transaction_state.len()
⋮----
fn is_empty(&self) -> bool {
self.priority_queue.is_empty()
⋮----
fn pop(&mut self) -> Option<TransactionPriorityId> {
self.priority_queue.pop_max()
⋮----
fn get_mut_transaction_state(
⋮----
match self.id_to_transaction_state.get_mut(id) {
⋮----
Some(BatchIdOrTransactionState::TransactionState(state)) => Some(state),
⋮----
fn get_transaction(&self, id: TransactionId) -> Option<&Tx> {
let batch_or_txn = self.id_to_transaction_state.get(id)?;
⋮----
BatchIdOrTransactionState::TransactionState(state) => Some(state.transaction()),
⋮----
fn get_batch(&self, id: TransactionId) -> Option<(&SmallVec<[TransactionId; 5]>, bool, u64)> {
⋮----
self.id_to_transaction_state.get(id)
⋮----
Some((
self.batch_id_to_transaction_ids.get(&batch_info.batch_id)?,
⋮----
fn push_ids_into_queue(
⋮----
self.priority_queue.push(id);
⋮----
.len()
.saturating_sub(self.capacity);
⋮----
let priority_id = self.priority_queue.pop_min().expect("queue is not empty");
self.id_to_transaction_state.remove(priority_id.id);
⋮----
fn hold_transaction(&mut self, priority_id: TransactionPriorityId) {
self.held_transactions.push(priority_id);
⋮----
fn remove_by_id(&mut self, id: TransactionId) {
let BatchIdOrTransactionState::Batch(batch_info) = self.id_to_transaction_state.remove(id)
⋮----
.remove(&batch_info.batch_id)
⋮----
self.id_to_transaction_state.remove(transaction_id);
⋮----
fn flush_held_transactions(&mut self) {
⋮----
self.push_ids_into_queue(held_transactions.drain(..));
⋮----
fn get_min_max_priority(&self) -> MinMaxResult<u64> {
match self.priority_queue.peek_min() {
Some(min) => match self.priority_queue.peek_max() {
⋮----
fn clear(&mut self) {
self.priority_queue.clear();
self.id_to_transaction_state.clear();
⋮----
pub(crate) fn insert_new_transaction(
⋮----
let entry: VacantEntry<'_, BatchIdOrTransactionState<Tx>> = self.get_vacant_map_entry();
let transaction_id = entry.key();
entry.insert(BatchIdOrTransactionState::TransactionState(
⋮----
self.push_ids_into_queue(std::iter::once(priority_id)) > 0
⋮----
/// Will try to insert a new batch of transactions if there is enough
    /// capacity in the container. If successful, returns the batch id.
⋮----
/// capacity in the container. If successful, returns the batch id.
    /// If there is not enough capacity, returns `None`.
⋮----
/// If there is not enough capacity, returns `None`.
    /// Note: will not evict existing transactions to make room for the batch (unlike `insert_new_transaction`).
⋮----
/// Note: will not evict existing transactions to make room for the batch (unlike `insert_new_transaction`).
    pub(crate) fn insert_new_batch(
⋮----
pub(crate) fn insert_new_batch(
⋮----
let capacity_required = self.id_to_transaction_state.len() + txns_max_age.len() + 1;
if capacity_required >= self.id_to_transaction_state.capacity() {
⋮----
let entry = self.get_vacant_map_entry();
let batch_id = entry.key();
entry.insert(BatchIdOrTransactionState::Batch(BatchInfo {
⋮----
let mut transaction_ids = SmallVec::with_capacity(txns_max_age.len());
⋮----
let transaction_id: usize = entry.key();
⋮----
transaction_ids.push(transaction_id);
⋮----
.insert(batch_id, transaction_ids);
⋮----
.push(TransactionPriorityId::new(priority, batch_id));
Some(batch_id)
⋮----
fn get_vacant_map_entry(&mut self) -> VacantEntry<'_, BatchIdOrTransactionState<Tx>> {
assert!(self.id_to_transaction_state.len() < self.id_to_transaction_state.capacity());
self.id_to_transaction_state.vacant_entry()
⋮----
pub type SharedBytes = Arc<Vec<u8>>;
pub(crate) type RuntimeTransactionView = RuntimeTransaction<ResolvedTransactionView<SharedBytes>>;
pub(crate) type TransactionViewState = TransactionState<RuntimeTransactionView>;
pub struct TransactionViewStateContainer {
⋮----
impl TransactionViewStateContainer {
pub(crate) fn try_insert_map_only_with_data(
⋮----
let vacant_entry = self.inner.get_vacant_map_entry();
let transaction_id = vacant_entry.key();
⋮----
assert_eq!(Arc::strong_count(bytes_entry), 1, "entry must be unique");
⋮----
bytes.clear();
bytes.extend_from_slice(data);
⋮----
if let Ok(state) = f(Arc::clone(bytes_entry)) {
vacant_entry.insert(BatchIdOrTransactionState::TransactionState(state));
Some(transaction_id)
⋮----
let bytes_buffer = (0..inner.id_to_transaction_state.capacity())
.map(|_| Arc::new(Vec::with_capacity(PACKET_DATA_SIZE)))
⋮----
.into_boxed_slice();
⋮----
self.inner.queue_size()
⋮----
self.inner.buffer_size()
⋮----
self.inner.is_empty()
⋮----
self.inner.pop()
⋮----
self.inner.get_mut_transaction_state(id)
⋮----
fn get_transaction(&self, id: TransactionId) -> Option<&RuntimeTransactionView> {
self.inner.get_transaction(id)
⋮----
self.inner.push_ids_into_queue(priority_ids)
⋮----
self.inner.hold_transaction(priority_id);
⋮----
fn get_batch(&self, _: TransactionId) -> Option<(&SmallVec<[TransactionId; 5]>, bool, u64)> {
unimplemented!("get_batch not implemented for TransactionViewStateContainer");
⋮----
self.inner.remove_by_id(id);
⋮----
self.inner.flush_held_transactions();
⋮----
self.inner.get_min_max_priority()
⋮----
self.inner.clear();
⋮----
mod tests {
⋮----
fn test_transaction(
⋮----
let ixs = vec![
⋮----
let message = Message::new(&ixs, Some(&from_keypair.pubkey()));
⋮----
fn push_to_container(
⋮----
let (transaction, max_age, priority, cost) = test_transaction(priority);
container.insert_new_transaction(transaction, max_age, priority, cost);
⋮----
fn test_is_empty() {
⋮----
assert!(container.is_empty());
push_to_container(&mut container, 1);
assert!(!container.is_empty());
⋮----
fn test_priority_queue_capacity() {
⋮----
push_to_container(&mut container, 5);
assert_eq!(container.priority_queue.len(), 1);
assert_eq!(container.id_to_transaction_state.len(), 1);
assert_eq!(
⋮----
fn test_get_mut_transaction_state() {
⋮----
assert!(container.get_mut_transaction_state(existing_id).is_some());
⋮----
assert!(container
⋮----
fn test_view_push_ids_to_queue() {
⋮----
let view = SanitizedTransactionView::try_new_sanitized(data, true).unwrap();
⋮----
.unwrap();
⋮----
Ok(TransactionState::new(view, MaxAge::MAX, priority, cost))
⋮----
let (transaction, _max_age, priority, cost) = test_transaction(priority);
let packet = Packet::from_data(None, transaction.to_versioned_transaction()).unwrap();
⋮----
.try_insert_map_only_with_data(packet.data(..).unwrap(), |data| {
packet_parser(data, priority, cost)
⋮----
priority_ids.push(priority_id);
⋮----
assert_eq!(container.push_ids_into_queue(priority_ids.into_iter()), 5);
assert_eq!(container.pop().unwrap().priority, 12);
assert_eq!(container.pop().unwrap().priority, 11);
assert!(container.pop().is_none());
⋮----
fn test_batch() {
⋮----
let (transaction, max_age, _, _) = test_transaction(priority);
transaction_max_ages.push((transaction, max_age));
⋮----
let batch_id = container.insert_new_batch(transaction_max_ages, 10, 100, true, 0);
assert!(batch_id.is_some());
⋮----
assert_eq!(container.id_to_transaction_state.len(), 6);
assert_eq!(container.batch_id_to_transaction_ids.len(), 1);
let batch_id = batch_id.unwrap();
let (batch, revert_on_error, slot) = container.get_batch(batch_id).unwrap();
assert_eq!(batch.len(), 5);
assert!(revert_on_error);
assert_eq!(slot, 0);
let batch_id = container.pop().unwrap();
container.remove_by_id(batch_id.id);
assert_eq!(container.priority_queue.len(), 0);
assert_eq!(container.id_to_transaction_state.len(), 0);
assert!(container.batch_id_to_transaction_ids.is_empty());

================
File: core/src/banking_stage/transaction_scheduler/transaction_state.rs
================
use crate::banking_stage::scheduler_messages::MaxAge;
⋮----
use qualifier_attr::qualifiers;
⋮----
pub(crate) struct TransactionState<Tx> {
⋮----
pub(crate) fn new(transaction: Tx, max_age: MaxAge, priority: u64, cost: u64) -> Self {
⋮----
transaction: Some(transaction),
⋮----
pub(crate) fn priority(&self) -> u64 {
⋮----
pub(crate) fn cost(&self) -> u64 {
⋮----
pub(crate) fn take_transaction_for_scheduling(&mut self) -> (Tx, MaxAge) {
⋮----
.take()
.expect("transaction not already pending");
⋮----
pub(crate) fn retry_transaction(&mut self, transaction: Tx) {
assert!(
⋮----
pub(crate) fn transaction(&self) -> &Tx {
⋮----
.as_ref()
.expect("transaction is not pending")
⋮----
mod tests {
⋮----
fn create_transaction_state(
⋮----
let ixs = vec![
⋮----
let message = Message::new(&ixs, Some(&from_keypair.pubkey()));
⋮----
fn test_take_transaction_for_scheduling_panic() {
let mut transaction_state = create_transaction_state(0);
transaction_state.take_transaction_for_scheduling();
⋮----
fn test_take_transaction_for_scheduling() {
⋮----
assert!(transaction_state.transaction.is_some());
let _ = transaction_state.take_transaction_for_scheduling();
assert!(transaction_state.transaction.is_none());
⋮----
fn test_retry_transaction_panic() {
⋮----
let transaction_clone = transaction_state.transaction.as_ref().unwrap().clone();
⋮----
transaction_state.retry_transaction(transaction_clone);
⋮----
fn test_retry_transaction() {
⋮----
let (transaction, _max_age) = transaction_state.take_transaction_for_scheduling();
⋮----
transaction_state.retry_transaction(transaction);
⋮----
fn test_priority() {
⋮----
let mut transaction_state = create_transaction_state(priority);
assert_eq!(transaction_state.priority(), priority);
⋮----
fn test_transaction_panic() {
⋮----
let _transaction = transaction_state.transaction();
⋮----
let _ = transaction_state.transaction();
⋮----
fn test_transaction() {

================
File: core/src/banking_stage/committer.rs
================
pub enum CommitTransactionDetails {
⋮----
pub struct Committer {
⋮----
impl Committer {
pub fn new(
⋮----
pub fn transaction_status_sender_enabled(&self) -> bool {
self.transaction_status_sender.is_some()
⋮----
pub fn commit_transactions(
⋮----
let (commit_results, commit_time_us) = measure_us!(bank.commit_transactions(
⋮----
.iter()
.map(|commit_result| match commit_result {
⋮----
result: committed_tx.status.clone(),
⋮----
Err(err) => CommitTransactionDetails::NotCommitted(err.clone()),
⋮----
.collect();
let ((), find_and_send_votes_us) = measure_us!({
⋮----
fn collect_balances_and_send_status_batch(
⋮----
let sanitized_transactions = batch.sanitized_transactions();
⋮----
.map(|tx| tx.as_sanitized_transaction().into_owned())
.collect_vec();
let mut transaction_index = Saturating(starting_transaction_index.unwrap_or_default());
⋮----
.zip(sanitized_transactions.iter())
.map(|(commit_result, tx)| {
⋮----
let tx_cost = Some(
⋮----
.sum(),
⋮----
(0, Some(0))
⋮----
.unzip();
debug_assert!(balance_collector.is_some());
⋮----
compile_collected_balances(balance_collector.unwrap_or_default());
transaction_status_sender.send_transaction_status_batch(
bank.slot(),

================
File: core/src/banking_stage/consume_worker.rs
================
pub enum ConsumeWorkerError<Tx> {
⋮----
enum ProcessingStatus<Tx> {
⋮----
pub(crate) struct ConsumeWorker<Tx> {
⋮----
pub fn new(
⋮----
pub fn new_with_tip_processing_deps(
⋮----
pub fn metrics_handle(&self) -> Arc<ConsumeWorkerMetrics> {
self.metrics.clone()
⋮----
pub fn run(self) -> Result<(), ConsumeWorkerError<Tx>> {
⋮----
while !self.exit.load(Ordering::Relaxed) {
match self.consume_receiver.try_recv() {
⋮----
match self.consume(work)? {
⋮----
self.retry_drain(work)?;
⋮----
let idle_duration = now.duration_since(last_empty_time);
sleep_duration = backoff(idle_duration, &sleep_duration);
⋮----
return Err(ConsumeWorkerError::Recv(TryRecvError::Disconnected))
⋮----
Ok(())
⋮----
fn consume(
⋮----
let Some(leader_state) = active_leader_state_with_timeout(&self.shared_leader_state) else {
return Ok(ProcessingStatus::CouldNotProcess(work));
⋮----
.working_bank()
.expect("active_leader_state_with_timeout should only return an active bank");
⋮----
if max_schedule_slot < bank.slot() {
return self.retry(work);
⋮----
if !self.run_tip_programs_if_needed(bank, &work.transactions) {
error!(
⋮----
datapoint_error!(
⋮----
.fetch_add(1, Ordering::Relaxed);
let output = self.consumer.process_and_record_aged_transactions(
⋮----
self.metrics.update_for_consume(&output);
self.metrics.has_data.store(true, Ordering::Relaxed);
⋮----
Some(Self::generate_extra_info(&output, &work))
⋮----
self.consumed_sender.send(FinishedConsumeWork {
⋮----
Ok(ProcessingStatus::Processed)
⋮----
fn run_tip_programs_if_needed(
⋮----
let tip_accounts = tip_manager.get_tip_accounts();
if !txs.iter().any(|tx| {
tx.account_keys()
.iter()
.any(|key| tip_accounts.contains(key))
⋮----
let mut last_tip_updated_slot_guard = last_tip_updated_slot.lock().unwrap();
if bank.slot() == *last_tip_updated_slot_guard {
⋮----
let keypair = cluster_info.keypair().clone();
⋮----
tip_manager.get_initialize_tip_programs_bundle(bank, &keypair);
⋮----
let result = self.consumer.process_and_record_transactions(
⋮----
.map_or(true, |results| {
⋮----
.any(|r| matches!(r, CommitTransactionDetails::NotCommitted(_)))
⋮----
let block_builder_fee_info = (*block_builder_fee_info.lock().unwrap()).clone();
⋮----
match tip_manager.get_tip_programs_crank_bundle(bank, &keypair, &block_builder_fee_info) {
⋮----
error!("error getting tip programs crank bundle: {e:?}");
⋮----
*last_tip_updated_slot_guard = bank.slot();
⋮----
fn generate_extra_info(
⋮----
.as_ref()
⋮----
processed_results: vec![
⋮----
let mut processed_results = vec![];
for commit_info in commit_transactions_result.iter() {
⋮----
processed_results.push(TransactionResult::Committed(
⋮----
execution_success: result.is_ok(),
⋮----
processed_results.push(TransactionResult::NotCommitted(
NotCommittedReason::Error(err.clone()),
⋮----
fn retry_drain(&self, work: ConsumeWork<Tx>) -> Result<(), ConsumeWorkerError<Tx>> {
for work in try_drain_iter(work, &self.consume_receiver) {
if self.exit.load(Ordering::Relaxed) {
return Ok(());
⋮----
self.retry(work)?;
⋮----
fn retry(&self, work: ConsumeWork<Tx>) -> Result<ProcessingStatus<Tx>, ConsumeWorkerError<Tx>> {
let retryable_indexes: Vec<_> = (0..work.transactions.len())
.map(|index| RetryableIndex {
⋮----
.collect();
let num_retryable = retryable_indexes.len();
⋮----
.fetch_add(num_retryable, Ordering::Relaxed);
⋮----
Some(FinishedConsumeWorkExtraInfo {
processed_results: (0..work.transactions.len())
.map(|_| TransactionResult::NotCommitted(NotCommittedReason::PohTimeout))
.collect(),
⋮----
pub(crate) mod external {
⋮----
pub enum ExternalConsumeWorkerError {
⋮----
pub(crate) struct ExternalWorker {
⋮----
type Tx = RuntimeTransaction<ResolvedTransactionView<TransactionPtr>>;
type TxView = SanitizedTransactionView<TransactionPtr>;
impl ExternalWorker {
⋮----
pub fn run(
⋮----
self.allocator.clean_remote_free_lists();
if receiver.is_empty() {
receiver.sync();
⋮----
match receiver.try_read() {
⋮----
self.sender.sync();
⋮----
self.process_message(message, should_drain_executes)?;
self.sender.commit();
receiver.finalize();
⋮----
fn process_message(
⋮----
.return_unprocessed_message(
⋮----
.map(|()| false);
⋮----
self.execute_batch(message, should_drain_executes)
⋮----
self.check_batch(message).map(|()| false)
⋮----
fn execute_batch(
⋮----
.return_not_included_with_reason(
⋮----
.map(|()| true);
⋮----
active_leader_state_with_timeout(&self.shared_leader_state)
⋮----
if bank.slot() > message.max_working_slot {
⋮----
if execution_flags.all_or_nothing && translation_results.len() != transactions.len()
⋮----
self.send_execution_response(
⋮----
return Ok(false);
⋮----
Some(&self.bundle_account_locker),
⋮----
if bank.slot() == message.max_working_slot {
⋮----
self.return_not_included_with_reason(message, not_included_reasons::BANK_NOT_AVAILABLE)
.map(|()| false)
⋮----
fn check_batch(
⋮----
} = self.sharable_banks.load();
if working_bank.slot() > message.max_working_slot {
return self.return_unprocessed_message(
⋮----
let (responses_ptr, responses) = allocate_check_response_region(
⋮----
.ok_or(ExternalConsumeWorkerError::AllocationFailure)?;
⋮----
self.check_resolve_pubkeys(
⋮----
root_bank.slot(),
⋮----
.try_write(response)
.map_err(|_| ExternalConsumeWorkerError::SenderDisconnected)?;
⋮----
fn send_execution_response(
⋮----
let responses = execution_responses_from_iter(&self.allocator, iter)
⋮----
fn all_or_nothing_translate_iterator(
⋮----
translation_results.iter().map(|res| ExecutionResponse {
⋮----
fn consume_response_iterator<'a>(
⋮----
assert_eq!(transactions.len(), commit_results.len());
let mut transactions_iterator = transactions.iter();
let mut commit_result_iterator = commit_results.iter();
⋮----
.map(move |translation_result| match translation_result {
⋮----
let tx = transactions_iterator.next().expect(
⋮----
let commit_details = commit_result_iterator.next().expect(
⋮----
/// Return all transactions in the batch as not included with the provided
        /// reason.
⋮----
/// reason.
        fn return_not_included_with_reason(
⋮----
fn return_not_included_with_reason(
⋮----
let response_region = execution_responses_from_iter(
⋮----
(0..message.batch.num_transactions).map(|_| ExecutionResponse {
⋮----
// Should de-allocate the memory, but this is a non-recoverable
// error and so it's not needed.
⋮----
.try_write(response_message)
⋮----
fn check_resolve_pubkeys(
⋮----
assert_eq!(parsing_results.len(), parsing_and_resolve_results.len());
assert_eq!(parsing_results.len(), responses.len());
let mut resolved_transaction_iter = txs.iter();
let mut max_age_iter = max_ages.iter();
⋮----
.zip(parsing_and_resolve_results.iter())
.enumerate()
⋮----
if parsing_result.is_err() {
⋮----
if parsing_and_resolve_results.is_err() {
⋮----
let transaction = resolved_transaction_iter.next().expect(
⋮----
let max_age = max_age_iter.next().expect(
⋮----
let (sharable_keys, alt_invalidation_slot) = match transaction.loaded_addresses() {
Some(loaded_addresses) if !loaded_addresses.is_empty() => {
let num_pubkeys = loaded_addresses.len();
⋮----
.allocate(
num_pubkeys.wrapping_mul(core::mem::size_of::<Pubkey>()) as u32
⋮----
.ok_or(ExternalConsumeWorkerError::AllocationFailure)?
.cast();
⋮----
let offset = unsafe { self.allocator.offset(pubkeys_allocation.cast()) };
⋮----
fn return_unprocessed_message(
⋮----
assert_ne!(
⋮----
unsafe fn parse_transactions_and_populate_initial_check_responses<'a>(
⋮----
.is_active(&agave_feature_set::static_instruction_limit::ID);
⋮----
for (tx_ptr, _) in batch.iter() {
⋮----
parsing_results.push(Ok(()));
parsed_transactions.push(view);
⋮----
parsing_results.push(Err(err));
⋮----
responses_ptr.as_ptr(),
⋮----
unsafe fn check_populate_initial_messages(
⋮----
assert_eq!(
⋮----
for (transaction_index, parsing_result) in parsing_results.iter().enumerate() {
let parsing_and_sanitization_flags = if parsing_result.is_err() {
⋮----
responses_ptr.add(transaction_index).write(CheckResponse {
⋮----
fn check_load_fee_payer_balance<D: TransactionData>(
⋮----
assert_eq!(responses.len(), parsing_results.len());
let mut parsed_transaction_iter = parsed_transactions.iter();
⋮----
let transaction = parsed_transaction_iter.next().expect(
⋮----
.load_with_fixed_root(
⋮----
&transaction.static_account_keys()[0],
⋮----
.map(|(account, _slot)| account.lamports())
.unwrap_or(0);
⋮----
response.balance_slot = working_bank.slot();
⋮----
fn check_status_checks<D: TransactionData>(
⋮----
assert_eq!(parsing_and_resolve_results.len(), responses.len());
⋮----
.check_transactions_with_processed_slots(
⋮----
&[const { Ok(()) }; MAX_TRANSACTIONS_PER_MESSAGE],
⋮----
let included_slots = included_slots.expect("requested to collect processed slots");
⋮----
status_check_results.iter().zip(included_slots.iter());
⋮----
parsing_and_resolve_results.iter().enumerate()
⋮----
if parsing_and_resolve_result.is_err() {
⋮----
.next()
.expect("status check results must have element for each sent transaction");
⋮----
included_slot.expect("included_slot must be set for already processed");
⋮----
fn translate_transaction_batch(
⋮----
let transaction_account_lock_limit = bank.get_transaction_account_lock_limit();
⋮----
for (transaction_ptr, _) in batch.iter() {
⋮----
transactions.push(tx);
max_ages.push(max_age);
translation_results.push(Ok(()));
⋮----
Err(err) => translation_results.push(Err(err)),
⋮----
fn translate_transaction(
⋮----
translate_to_runtime_view(
⋮----
.map(|(view, deactivation_slot)| {
⋮----
sanitized_epoch: bank.epoch(),
⋮----
unsafe fn copy_loaded_addresses(loaded_addresses: &LoadedAddresses, dest: NonNull<Pubkey>) {
⋮----
loaded_addresses.writable.as_ptr(),
dest.as_ptr(),
loaded_addresses.writable.len(),
⋮----
loaded_addresses.readonly.as_ptr(),
dest.add(loaded_addresses.writable.len()).as_ptr(),
loaded_addresses.readonly.len(),
⋮----
fn validate_message(message: &PackToWorkerMessage) -> bool {
⋮----
fn validate_message_flags(flags: u16) -> bool {
⋮----
fn response_from_commit_details(
⋮----
.sum(),
⋮----
not_included_reason: transaction_error_to_not_included_reason(
⋮----
fn reason_from_packet_handling_error(err: &PacketHandlingError) -> u8 {
⋮----
mod tests {
⋮----
fn test_validate_message() {
⋮----
assert!(!ExternalWorker::validate_message(&message));
⋮----
assert!(ExternalWorker::validate_message(&message));
⋮----
fn test_validate_message_flags() {
assert!(ExternalWorker::validate_message_flags(
⋮----
assert!(!ExternalWorker::validate_message_flags(
⋮----
fn test_consume_response_iterator() {
let simple_tx = bincode::serialize(&transfer(
⋮----
.unwrap();
⋮----
.map(|_| {
⋮----
bank.get_transaction_account_lock_limit(),
⋮----
.ok()
.unwrap()
⋮----
Err(PacketHandlingError::Sanitization),
Ok(()),
⋮----
result: Err(TransactionError::InstructionError(
⋮----
result: Ok(()),
⋮----
fn test_all_or_nothing_translate_iterator() {
let translation_results = vec![Ok(()), Err(PacketHandlingError::Sanitization), Ok(())];
⋮----
fn empty_check_responses(count: u8) -> Vec<CheckResponse> {
⋮----
.map(|_| CheckResponse {
⋮----
.collect()
⋮----
fn test_check_populate_initial_messages() {
⋮----
let mut responses = empty_check_responses(message.batch.num_transactions);
let responses_ptr = NonNull::new(responses.as_mut_ptr()).unwrap();
⋮----
Err(TransactionViewError::ParseError),
Err(TransactionViewError::SanitizeError),
⋮----
assert_eq!(responses[0].parsing_and_sanitization_flags, 0);
⋮----
for response in responses.iter() {
⋮----
fn test_serialized_transaction(recent_blockhash: solana_hash::Hash) -> Vec<u8> {
let tx = transfer(
⋮----
bincode::serialize(&tx).unwrap()
⋮----
fn test_check_load_fee_payer_balance() {
⋮----
let tx1 = test_serialized_transaction(bank.confirmed_last_blockhash());
let tx2 = test_serialized_transaction(bank.confirmed_last_blockhash());
let parsing_results = [Ok(()), Err(TransactionViewError::ParseError), Ok(())];
⋮----
SanitizedTransactionView::try_new_sanitized(&tx1[..], true).unwrap(),
SanitizedTransactionView::try_new_sanitized(&tx2[..], true).unwrap(),
⋮----
bank.store_account(
&parsed_transactions[1].static_account_keys()[0],
⋮----
let mut responses = empty_check_responses(parsing_results.len() as u8);
⋮----
assert_eq!(responses[0].balance_slot, bank.slot());
assert_eq!(responses[0].fee_payer_balance, 0);
assert_eq!(responses[1].fee_payer_balance_flags, 0);
assert_eq!(responses[1].balance_slot, 0);
assert_eq!(responses[1].fee_payer_balance, 0);
⋮----
assert_eq!(responses[2].balance_slot, bank.slot());
assert_eq!(responses[2].fee_payer_balance, 1_000_000_000);
⋮----
fn test_check_status_checks() {
⋮----
Bank::new_for_tests(&genesis.genesis_config).wrap_with_bank_forks_for_tests();
⋮----
let tx2 = test_serialized_transaction(solana_hash::Hash::new_unique());
let tx3 = test_serialized_transaction(bank.confirmed_last_blockhash());
fn to_resolved_view(
⋮----
SanitizedTransactionView::try_new_sanitized(tx, true).unwrap(),
⋮----
Some(false),
⋮----
.unwrap(),
⋮----
to_resolved_view(&tx1[..]),
to_resolved_view(&tx2[..]),
to_resolved_view(&tx3[..]),
⋮----
&parsed_transactions[2].static_account_keys()[0],
⋮----
bank.process_transaction(&bincode::deserialize(&tx3).unwrap())
⋮----
let mut responses = empty_check_responses(parsing_and_resolve_results.len() as u8);
⋮----
assert_eq!(responses[1].status_check_flags, 0);
⋮----
assert_eq!(responses[3].included_slot, bank.slot());
⋮----
fn test_reason_from_packet_handling_error() {
⋮----
fn test_copy_loaded_addresses() {
⋮----
writable: (0..5).map(|_| Pubkey::new_unique()).collect(),
readonly: (0..2).map(|_| Pubkey::new_unique()).collect(),
⋮----
let mut buffer = vec![Pubkey::default(); 7];
⋮----
NonNull::new(buffer.as_mut_ptr()).unwrap(),
⋮----
assert_eq!(&loaded_addresses.writable, &buffer[0..5]);
assert_eq!(&loaded_addresses.readonly, &buffer[5..7]);
⋮----
fn try_drain_iter<T>(work: T, receiver: &Receiver<T>) -> impl Iterator<Item = T> + '_ {
std::iter::once(work).chain(receiver.try_iter())
⋮----
/// Get active bank with timeout.
fn active_leader_state_with_timeout(
⋮----
fn active_leader_state_with_timeout(
⋮----
// Do an initial bank load without sampling time. If we're in a hot loop
if let Some(guard) = active_leader_state(shared_leader_state) {
return Some(guard);
⋮----
while now.elapsed() < TIMEOUT {
⋮----
fn active_leader_state(
⋮----
let guard = shared_leader_state.load();
⋮----
.map(|bank| bank.is_complete())
.unwrap_or(true)
⋮----
Some(guard)
⋮----
fn backoff(idle_duration: Duration, sleep_duration: &Duration) -> Duration {
⋮----
sleep_duration.saturating_mul(2).min(MAX_SLEEP_DURATION)
⋮----
pub struct ConsumeWorkerMetrics {
⋮----
impl ConsumeWorkerMetrics {
pub fn maybe_report_and_reset(&self) {
⋮----
if self.interval.should_update(REPORT_INTERVAL_MS)
&& self.has_data.swap(false, Ordering::Relaxed)
⋮----
self.count_metrics.report_and_reset(&self.id);
self.timing_metrics.report_and_reset(&self.id);
self.error_metrics.report_and_reset(&self.id);
⋮----
pub(crate) fn set_has_data(&self, has_data: bool) {
self.has_data.store(has_data, Ordering::Relaxed);
⋮----
pub(crate) fn new(id: u32) -> Self {
⋮----
id: id.to_string(),
⋮----
pub(crate) fn update_for_consume(
⋮----
.fetch_add(*cost_model_throttled_transactions_count, Ordering::Relaxed);
⋮----
.fetch_add(*cost_model_us, Ordering::Relaxed);
self.update_on_execute_and_commit_transactions_output(
⋮----
fn update_on_execute_and_commit_transactions_output(
⋮----
.fetch_add(
⋮----
.fetch_add(transaction_counts.processed_count, Ordering::Relaxed);
⋮----
.fetch_add(retryable_transaction_indexes.len(), Ordering::Relaxed);
⋮----
.fetch_min(*min_prioritization_fees, Ordering::Relaxed);
⋮----
.fetch_max(*max_prioritization_fees, Ordering::Relaxed);
⋮----
.swap(min_prioritization_fees, Ordering::Relaxed);
⋮----
.swap(max_prioritization_fees, Ordering::Relaxed);
self.update_on_execute_and_commit_timings(execute_and_commit_timings);
self.update_on_error_counters(error_counters);
⋮----
fn update_on_execute_and_commit_timings(
⋮----
.fetch_min(*load_execute_us, Ordering::Relaxed);
⋮----
.fetch_max(*load_execute_us, Ordering::Relaxed);
⋮----
.fetch_add(*load_execute_us, Ordering::Relaxed);
⋮----
.fetch_add(*freeze_lock_us, Ordering::Relaxed);
⋮----
.fetch_add(*record_us, Ordering::Relaxed);
⋮----
.fetch_add(*commit_us, Ordering::Relaxed);
⋮----
.fetch_add(*find_and_send_votes_us, Ordering::Relaxed);
⋮----
fn update_on_error_counters(
⋮----
.fetch_add(total.0, Ordering::Relaxed);
⋮----
.fetch_add(account_in_use.0, Ordering::Relaxed);
⋮----
.fetch_add(too_many_account_locks.0, Ordering::Relaxed);
⋮----
.fetch_add(account_loaded_twice.0, Ordering::Relaxed);
⋮----
.fetch_add(account_not_found.0, Ordering::Relaxed);
⋮----
.fetch_add(blockhash_not_found.0, Ordering::Relaxed);
⋮----
.fetch_add(blockhash_too_old.0, Ordering::Relaxed);
⋮----
.fetch_add(call_chain_too_deep.0, Ordering::Relaxed);
⋮----
.fetch_add(already_processed.0, Ordering::Relaxed);
⋮----
.fetch_add(instruction_error.0, Ordering::Relaxed);
⋮----
.fetch_add(insufficient_funds.0, Ordering::Relaxed);
⋮----
.fetch_add(invalid_account_for_fee.0, Ordering::Relaxed);
⋮----
.fetch_add(invalid_account_index.0, Ordering::Relaxed);
⋮----
.fetch_add(invalid_program_for_execution.0, Ordering::Relaxed);
⋮----
.fetch_add(invalid_compute_budget.0, Ordering::Relaxed);
⋮----
.fetch_add(not_allowed_during_cluster_maintenance.0, Ordering::Relaxed);
⋮----
.fetch_add(invalid_writable_account.0, Ordering::Relaxed);
⋮----
.fetch_add(invalid_rent_paying_account.0, Ordering::Relaxed);
⋮----
.fetch_add(would_exceed_max_block_cost_limit.0, Ordering::Relaxed);
⋮----
.fetch_add(would_exceed_max_account_cost_limit.0, Ordering::Relaxed);
⋮----
.fetch_add(would_exceed_max_vote_cost_limit.0, Ordering::Relaxed);
⋮----
.fetch_add(would_exceed_account_data_block_limit.0, Ordering::Relaxed);
⋮----
.fetch_add(max_loaded_accounts_data_size_exceeded.0, Ordering::Relaxed);
⋮----
struct ConsumeWorkerCountMetrics {
⋮----
impl Default for ConsumeWorkerCountMetrics {
fn default() -> Self {
⋮----
impl ConsumeWorkerCountMetrics {
fn report_and_reset(&self, id: &str) {
datapoint_info!(
⋮----
struct ConsumeWorkerTimingMetrics {
⋮----
impl ConsumeWorkerTimingMetrics {
⋮----
struct ConsumeWorkerTransactionErrorMetrics {
⋮----
impl ConsumeWorkerTransactionErrorMetrics {
⋮----
struct TestFrame {
⋮----
fn setup_test_frame(
⋮----
} = create_slow_genesis_config(LAMPORTS_PER_SOL);
⋮----
genesis_config.accounts.extend(
spl_programs(&genesis_config.rent)
.into_iter()
.map(|(a, b)| (a, Account::from(b))),
⋮----
bank.clone(),
⋮----
bank.get_epoch_info().slots_in_epoch,
⋮----
bank.deactivate_feature(&agave_feature_set::relax_intrabatch_account_locks::id());
⋮----
let (record_sender, record_receiver) = record_channels(false);
⋮----
let (replay_vote_sender, replay_vote_receiver) = unbounded();
⋮----
let node = Node::new_localhost_with_pubkey(&mint_keypair.pubkey());
⋮----
node.info.clone(),
Arc::new(mint_keypair.insecure_clone()),
⋮----
let (consume_sender, consume_receiver) = unbounded();
let (consumed_sender, consumed_receiver) = unbounded();
⋮----
shared_leader_state.clone(),
Some(TipProcessingDependencies {
⋮----
*jito_tip_payment::id().as_array(),
⋮----
*jito_tip_distribution::id().as_array(),
⋮----
merkle_root_upload_authority: mint_keypair.pubkey(),
vote_account: voting_keypair.pubkey(),
⋮----
block_builder: mint_keypair.pubkey(),
⋮----
fn test_worker_consume_no_bank() {
let (test_frame, worker) = setup_test_frame(true, false);
⋮----
let worker_thread = std::thread::spawn(move || worker.run());
⋮----
let transactions = sanitize_transactions(vec![system_transaction::transfer(
⋮----
alt_invalidation_slot: bank.slot(),
⋮----
ids: vec![id],
⋮----
max_ages: vec![max_age],
⋮----
consume_sender.send(work).unwrap();
let consumed = consumed_receiver.recv().unwrap();
assert_eq!(consumed.work.batch_id, bid);
assert_eq!(consumed.work.ids, vec![id]);
assert_eq!(consumed.work.max_ages, vec![max_age]);
⋮----
drop(test_frame);
let _ = worker_thread.join().unwrap();
⋮----
fn test_worker_consume_simple() {
let (mut test_frame, worker) = setup_test_frame(true, false);
⋮----
shared_leader_state.store(Arc::new(LeaderState::new(
Some(bank.clone()),
bank.tick_height(),
⋮----
record_receiver.restart(bank.bank_id());
⋮----
assert_eq!(consumed.retryable_indexes, Vec::new());
⋮----
fn test_worker_consume_self_conflicting(relax_intrabatch_account_locks: bool) {
let (mut test_frame, worker) = setup_test_frame(relax_intrabatch_account_locks, false);
⋮----
let txs = sanitize_transactions(vec![
⋮----
.send(ConsumeWork {
⋮----
ids: vec![id1, id2],
⋮----
max_ages: vec![max_age, max_age],
⋮----
assert_eq!(consumed.work.ids, vec![id1, id2]);
assert_eq!(consumed.work.max_ages, vec![max_age, max_age]);
⋮----
fn test_worker_consume_multiple_messages() {
⋮----
let txs1 = sanitize_transactions(vec![system_transaction::transfer(
⋮----
let txs2 = sanitize_transactions(vec![system_transaction::transfer(
⋮----
ids: vec![id1],
⋮----
ids: vec![id2],
⋮----
assert_eq!(consumed.work.batch_id, bid1);
assert_eq!(consumed.work.ids, vec![id1]);
⋮----
assert_eq!(consumed.work.batch_id, bid2);
assert_eq!(consumed.work.ids, vec![id2]);
⋮----
fn test_worker_ttl() {
⋮----
assert!(bank.slot() > 0);
assert!(bank.epoch() > 0);
⋮----
genesis_config.hash(),
⋮----
writable: vec![to_pubkey],
readonly: vec![],
⋮----
&payer.pubkey(),
&[system_instruction::transfer(&payer.pubkey(), &to_pubkey, 1)],
⋮----
addresses: vec![to_pubkey],
⋮----
.is_active(&agave_feature_set::static_instruction_limit::id()),
⋮----
let mut txs = sanitize_transactions(vec![
⋮----
txs.push(simple_v0_transfer());
⋮----
let sanitized_txs = txs.clone();
⋮----
bank.process_transaction(&system_transaction::transfer(
⋮----
&tx.account_keys()[0],
⋮----
ids: vec![0, 1, 2, 3, 4, 5],
⋮----
max_ages: vec![
⋮----
assert_eq!(bank.transaction_count(), 6 + 5);
⋮----
.check_transactions(
⋮----
&vec![Ok(()); sanitized_txs.len()],
⋮----
.map(|r| match r {
Ok(_) => Ok(()),
Err(err) => Err(err),
⋮----
fn test_backoff() {
⋮----
let sleep_duration = backoff(Duration::ZERO, &sleep_duration);
assert_eq!(sleep_duration, STARTING_SLEEP_DURATION);
let sleep_duration = backoff(IDLE_SLEEP_THRESHOLD, &sleep_duration);
assert_eq!(sleep_duration, STARTING_SLEEP_DURATION.saturating_mul(2));
⋮----
assert_eq!(sleep_duration, MAX_SLEEP_DURATION);
⋮----
fn test_handle_tip_programs() {
let (mut test_frame, worker) = setup_test_frame(true, true);
⋮----
&Pubkey::new_from_array(*jito_tip_payment::id().as_array()),
⋮----
bank.last_blockhash(),
⋮----
ids: vec![0],
transactions: vec![runtime_tx.clone()],
max_ages: vec![MaxAge::MAX],
⋮----
.recv_timeout(Duration::from_secs(1))
⋮----
assert_eq!(consumed.retryable_indexes.len(), 0);
assert_eq!(consumed.work.transactions.len(), 1);
⋮----
.get_account(&JitoTipPaymentConfig::find_program_address(&jito_tip_payment::id()).0)
⋮----
assert_eq!(tip_payment_config.block_builder(), mint_keypair.pubkey(),);

================
File: core/src/banking_stage/consumer.rs
================
pub struct ExecutionFlags {
⋮----
impl Default for ExecutionFlags {
fn default() -> Self {
⋮----
pub struct RetryableIndex {
⋮----
impl RetryableIndex {
pub fn new(index: usize, immediately_retryable: bool) -> Self {
⋮----
pub struct ProcessTransactionBatchOutput {
⋮----
pub struct ExecuteAndCommitTransactionsOutput {
⋮----
pub struct LeaderProcessedTransactionCounts {
⋮----
pub struct TipProcessingDependencies {
⋮----
pub struct Consumer {
⋮----
struct SeqNotConflictBatchReusables {
⋮----
impl SeqNotConflictBatchReusables {
pub fn clear(&mut self) {
self.aggregate_write_locks.clear();
self.aggregate_read_locks.clear();
self.transaction_write_locks.clear();
self.transaction_read_locks.clear();
⋮----
impl Consumer {
pub fn new(
⋮----
pub fn process_and_record_transactions(
⋮----
let pre_results = vec![Ok(()); txs.len()];
⋮----
bank.check_transactions(txs, &pre_results, MAX_PROCESSING_AGE, &mut error_counters);
⋮----
.into_iter()
.map(|result| match result {
Ok(_) => Ok(()),
Err(err) => Err(err),
⋮----
.collect();
let mut output = self.process_and_record_transactions_with_pre_results(
⋮----
check_results.into_iter(),
⋮----
Some(bundle_account_locker),
⋮----
.accumulate(&error_counters);
⋮----
pub fn process_and_record_aged_transactions(
⋮----
let pre_results = txs.iter().zip(max_ages).map(|(tx, max_age)| {
bank.resanitize_transaction_minimally(
⋮----
self.process_and_record_transactions_with_pre_results(
⋮----
fn process_and_record_transactions_with_pre_results(
⋮----
) = measure_us!(self.qos_service.select_and_accumulate_transaction_costs(
⋮----
let l_bundle_account_locker = bundle_account_locker.map(|locker| locker.account_locks());
let (batch, lock_us) = measure_us!(bank.prepare_sanitized_batch_with_results(
⋮----
drop(l_bundle_account_locker);
⋮----
self.execute_and_commit_transactions_locked(bank, &batch, flags, revert_on_error);
let (_, unlock_us) = measure_us!(drop(batch));
⋮----
transaction_qos_cost_results.iter(),
commit_transactions_result.as_ref().ok(),
⋮----
self.qos_service.report_metrics(bank.slot());
debug!(
⋮----
fn execute_and_commit_transactions_locked(
⋮----
let transaction_status_sender_enabled = self.committer.transaction_status_sender_enabled();
⋮----
.sanitized_transactions()
.iter()
.filter_map(|transaction| {
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(&bank.feature_set)
.ok()
.map(|limits| limits.compute_unit_price)
⋮----
.minmax();
⋮----
min_max.into_option().unwrap_or_default();
⋮----
.lock_results()
⋮----
.enumerate()
.filter_map(|(index, res)| match res {
⋮----
Some(RetryableIndex {
⋮----
if revert_on_error && batch.lock_results().iter().any(|res| res.is_err()) {
⋮----
attempted_processing_count: batch.sanitized_transactions().len() as u64,
⋮----
commit_transactions_result: Ok(batch
⋮----
.map(|res| match res {
⋮----
Err(err) => CommitTransactionDetails::NotCommitted(err.clone()),
⋮----
.collect()),
⋮----
let (load_and_execute_transactions_output, load_execute_us) = measure_us!(bank
⋮----
.map(|result| result.flattened_result().err())
.collect_vec();
if revert_on_error && successful_count != batch.sanitized_transactions().len() {
⋮----
commit_transactions_result: Ok((0..batch.sanitized_transactions().len())
.map(|index| {
⋮----
.get(index)
.map(|error| {
error.clone().unwrap_or(TransactionError::CommitCancelled)
⋮----
.unwrap_or(TransactionError::CommitCancelled),
⋮----
.map(|processing_result| {
⋮----
.as_ref()
.map_or(0, |pr| pr.executed_units())
⋮----
.sum();
⋮----
.accumulate_actual_execute_cu(actual_executed_cu);
⋮----
.accumulate_actual_execute_time(actual_execute_time);
⋮----
attempted_processing_count: processing_results.len() as u64,
⋮----
measure_us!(processing_results
⋮----
let (freeze_lock, freeze_lock_us) = measure_us!(bank.freeze_lock());
⋮----
let mut reusables = self.seq_not_conflict_batch_reusables.take();
⋮----
processed_transactions.into_iter(),
⋮----
self.seq_not_conflict_batch_reusables.set(reusables);
⋮----
.map(|batch| hash_transactions(batch))
⋮----
let (record_transactions_summary, record_us) = measure_us!(self
⋮----
processing_results_to_transactions_us: Saturating(
⋮----
retryable_transaction_indexes.extend(processing_results.iter().enumerate().filter_map(
⋮----
processing_result.was_processed().then_some(RetryableIndex {
⋮----
retryable_transaction_indexes.sort_unstable();
⋮----
commit_transactions_result: Err(recorder_err),
⋮----
self.committer.commit_transactions(
⋮----
.map(|processing_result| match processing_result {
Ok(_) => unreachable!("processed transaction count is 0"),
⋮----
.collect(),
⋮----
drop(freeze_lock);
⋮----
debug_assert_eq!(
⋮----
commit_transactions_result: Ok(commit_transaction_statuses),
⋮----
fn create_sequential_non_conflicting_batches<'a, T: TransactionWithMeta + 'a>(
⋮----
let mut result = vec![];
let mut current_batch = vec![];
reusables.clear();
⋮----
transaction_write_locks.clear();
transaction_read_locks.clear();
⋮----
TransactionAccountLocksIterator::new(transaction_info).accounts_with_is_writable()
⋮----
transaction_write_locks.push(*key);
⋮----
transaction_read_locks.push(*key);
⋮----
&& (aggregate_write_locks.contains(key)
|| (writable && aggregate_read_locks.contains(key)))
⋮----
result.push(std::mem::take(&mut current_batch));
aggregate_write_locks.clear();
aggregate_read_locks.clear();
⋮----
current_batch.push(transaction);
aggregate_write_locks.extend(transaction_write_locks.drain(..));
aggregate_read_locks.extend(transaction_read_locks.drain(..));
⋮----
if !current_batch.is_empty() {
result.push(current_batch);
⋮----
pub fn check_fee_payer_unlocked(
⋮----
let fee_payer = transaction.fee_payer();
⋮----
.sanitize_and_convert_to_compute_budget_limits(&bank.feature_set)?,
⋮----
bank.get_lamports_per_signature() == 0,
bank.fee_structure().lamports_per_signature,
⋮----
FeeFeatures::from(bank.feature_set.as_ref()),
⋮----
.load_with_fixed_root(&bank.ancestors, fee_payer)
.ok_or(TransactionError::AccountNotFound)?;
validate_fee_payer(
⋮----
&bank.rent_collector().rent,
⋮----
mod tests {
⋮----
struct TestFrame {
⋮----
fn setup_test(
⋮----
} = create_genesis_config_with_leader(
⋮----
bootstrap_validator_stake_lamports(),
⋮----
bank.deactivate_feature(&agave_feature_set::relax_intrabatch_account_locks::id());
⋮----
let (bank, bank_forks) = bank.wrap_with_bank_forks_for_tests();
let (record_sender, mut record_receiver) = record_channels(false);
⋮----
record_receiver.restart(bank.bank_id());
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
fn execute_transactions_for_test(
⋮----
let transactions = sanitize_transactions(transactions);
⋮----
consumer.process_and_record_transactions(
⋮----
fn generate_new_address_lookup_table(
⋮----
addresses.resize_with(num_addresses, Pubkey::new_unique);
⋮----
fn store_nonce_account(
⋮----
.set_state(&nonce::versions::Versions::new(nonce_state))
.unwrap();
bank.store_account(&account_address, &account);
⋮----
fn store_address_lookup_table(
⋮----
let data = address_lookup_table.serialize_for_tests().unwrap();
⋮----
AccountSharedData::new(1, data.len(), &address_lookup_table::program::id());
account.set_data(data);
⋮----
fn test_bank_process_and_record_transactions() {
⋮----
} = setup_test(true, None);
⋮----
let transactions = sanitize_transactions(vec![system_transaction::transfer(
⋮----
let process_transactions_batch_output = consumer.process_and_record_transactions(
⋮----
assert_eq!(
⋮----
assert!(commit_transactions_result.is_ok());
// When poh is near end of slot, it will be shutdown.
record_receiver.shutdown();
let record = record_receiver.drain().next().unwrap();
assert_eq!(record.bank_id, bank.bank_id());
assert_eq!(record.transaction_batches.len(), 1);
let transaction_batch = record.transaction_batches[0].clone();
assert_eq!(transaction_batch.len(), 1);
⋮----
// Transaction was still processed, just wasn't committed, so should be counted here.
⋮----
assert_matches!(
⋮----
assert_eq!(bank.get_balance(&pubkey), 1);
⋮----
fn test_bank_nonce_update_blockhash_queried_before_transaction_record() {
⋮----
let nonce_hash = *durable_nonce.as_hash();
⋮----
authority: mint_keypair.pubkey(),
⋮----
store_nonce_account(&bank, nonce_pubkey, nonce_state);
let transactions = sanitize_transactions(vec![system_transaction::nonced_transfer(
⋮----
let bank_hash = bank.last_blockhash();
while bank.tick_height() != bank.max_tick_height() - 1 {
bank.register_default_tick_for_test();
⋮----
let expected_nonce_hash = expected_nonce.as_hash();
let nonce_account = bank.get_account(&nonce_pubkey).unwrap();
assert!(verify_nonce_account(&nonce_account, expected_nonce_hash).is_some());
⋮----
fn test_bank_process_and_record_transactions_all_unexecuted() {
⋮----
system_transaction::transfer(&mint_keypair, &pubkey, 1, bank.last_blockhash());
tx.message.account_keys.push(pubkey);
sanitize_transactions(vec![tx])
⋮----
assert!(retryable_transaction_indexes.is_empty());
⋮----
fn test_bank_process_and_record_transactions_cost_tracker(
⋮----
} = setup_test(relax_intrabatch_account_locks, None);
⋮----
let get_block_cost = || bank.read_cost_tracker().unwrap().block_cost();
let get_tx_count = || bank.read_cost_tracker().unwrap().transaction_count();
assert_eq!(get_block_cost(), 0);
assert_eq!(get_tx_count(), 0);
⋮----
assert_eq!(transaction_counts.processed_with_successful_result_count, 1);
⋮----
let block_cost = get_block_cost();
assert_ne!(block_cost, 0);
assert_eq!(get_tx_count(), 1);
⋮----
let transactions = sanitize_transactions(vec![
⋮----
let conflicting_transaction = sanitize_transactions(vec![system_transaction::transfer(
⋮----
bank.try_lock_accounts(&conflicting_transaction, false);
⋮----
let commit_transactions_result = commit_transactions_result.unwrap();
assert_eq!(commit_transactions_result.len(), 2);
⋮----
match commit_transactions_result.first().unwrap() {
⋮----
unreachable!()
⋮----
block_cost + cost.sum()
⋮----
assert_eq!(get_block_cost(), expected_block_cost);
assert_eq!(get_tx_count(), 2);
⋮----
fn test_bank_process_and_record_transactions_account_in_use(
⋮----
sanitize_transactions(vec![system_transaction::transfer(
⋮----
assert_eq!(retryable_transaction_indexes, Vec::<_>::new());
⋮----
fn test_process_transactions_instruction_error() {
⋮----
} = create_slow_genesis_config(lamports);
⋮----
bank.write_cost_tracker()
.unwrap()
.set_limits(u64::MAX, u64::MAX, u64::MAX);
let transactions = vec![system_transaction::transfer(
⋮----
let transactions_len = transactions.len();
⋮----
} = execute_transactions_for_test(
⋮----
fn test_process_transactions_account_in_use(
⋮----
} = create_slow_genesis_config(10_000);
⋮----
let (bank, _bank_forks) = bank.wrap_with_bank_forks_for_tests();
⋮----
let mut transactions = vec![];
⋮----
transactions.push(system_transaction::transfer(
⋮----
genesis_config.hash(),
⋮----
fn test_process_transactions_returns_unprocessed_txs() {
⋮----
let process_transactions_summary = consumer.process_and_record_transactions(
⋮----
assert!(execute_and_commit_transactions_output
⋮----
.sort_unstable();
let expected: Vec<_> = (0..transactions.len())
.map(|index| RetryableIndex::new(index, true))
⋮----
fn test_write_persist_transaction_status() {
let (transaction_status_sender, transaction_status_receiver) = unbounded();
let tss = Some(TransactionStatusSender {
⋮----
} = setup_test(true, tss);
⋮----
let rent_exempt_amount = bank.get_minimum_balance_for_rent_exemption(0);
assert!(rent_exempt_amount > 0);
⋮----
bank.last_blockhash(),
⋮----
let transactions = sanitize_transactions(vec![success_tx, ix_error_tx]);
⋮----
.map(|tx| tx.clone().into_inner_transaction())
⋮----
bank.transfer(rent_exempt_amount, &mint_keypair, &keypair1.pubkey())
⋮----
let _ = consumer.process_and_record_transactions(
⋮----
drop(consumer);
let status_messages = transaction_status_receiver.into_iter().collect::<Vec<_>>();
assert_eq!(status_messages.len(), 1);
⋮----
status_messages.into_iter().next().unwrap()
⋮----
panic!("not a batch");
⋮----
assert_eq!(status_batch.transactions, batch_transactions_inner);
⋮----
.map(|r| r.unwrap().status.clone())
⋮----
fn test_write_persist_loaded_addresses() {
⋮----
let address_table_state = generate_new_address_lookup_table(None, 2);
store_address_lookup_table(&bank, address_table_key, address_table_state);
⋮----
.write()
⋮----
.insert(new_bank)
.clone_without_scheduler();
⋮----
recent_blockhash: bank.last_blockhash(),
account_keys: vec![keypair.pubkey()],
address_table_lookups: vec![MessageAddressTableLookup {
⋮----
instructions: vec![],
⋮----
let tx = VersionedTransaction::try_new(message, &[&keypair]).unwrap();
⋮----
tx.clone(),
⋮----
Some(false),
bank.as_ref(),
⋮----
.is_active(&agave_feature_set::static_instruction_limit::id()),
⋮----
bank.transfer(1, &mint_keypair, &keypair.pubkey()).unwrap();
⋮----
assert_eq!(status_batch.commit_results.len(), 1);
⋮----
.next()
⋮----
assert!(committed_transaction.status.is_ok());
⋮----
fn test_process_and_record_transactions_with_pre_results_with_bundle_account_locks() {
⋮----
.lock_bundle(&transactions, &bank)
⋮----
let (record_sender, _record_receiver) = record_channels(false);
⋮----
let consumer = Consumer::new(committer, recorder.clone(), QosService::new(1), None);
⋮----
.unlock_bundle(&transactions, &bank)
⋮----
assert_eq!(result.len(), 1);
⋮----
assert_eq!(bank.read_cost_tracker().unwrap().block_cost(), 0);
⋮----
fn test_process_transactions_instruction_error_revert_on_error() {
⋮----
let transactions = vec![
⋮----
} = execute_transactions_for_test(bank, transactions, BundleAccountLocker::default(), true);
⋮----
assert_eq!(results.len(), transactions_len);
⋮----
fn test_create_sequential_non_conflicting_batches() {
⋮----
let txns = vec![
⋮----
.map(|tx| RuntimeTransaction::from_transaction_for_tests(tx.clone()))
⋮----
.map(VersionedTransaction::from)
⋮----
txns.into_iter().zip(txn_infos.iter()),
⋮----
assert_eq!(batches.len(), 2);
assert_eq!(batches[0].len(), 2);
assert_eq!(batches[1].len(), 1);

================
File: core/src/banking_stage/decision_maker.rs
================
pub enum BufferedPacketsDecision {
⋮----
impl BufferedPacketsDecision {
pub fn bank(&self) -> Option<&Arc<Bank>> {
⋮----
Self::Consume(bank) => Some(bank),
⋮----
pub struct DecisionMaker {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
f.debug_struct("DecisionMaker").finish()
⋮----
impl DecisionMaker {
pub fn new(shared_leader_state: SharedLeaderState) -> Self {
⋮----
pub(crate) fn make_consume_or_forward_decision(&self) -> BufferedPacketsDecision {
let state = self.shared_leader_state.load();
if let Some(working_bank) = state.working_bank() {
BufferedPacketsDecision::Consume(working_bank.clone())
} else if let Some(leader_first_tick_height) = state.leader_first_tick_height() {
let current_tick_height = state.tick_height();
let ticks_until_leader = leader_first_tick_height.saturating_sub(current_tick_height);
⋮----
fn from(poh_recorder: &PohRecorder) -> Self {
Self::new(poh_recorder.shared_leader_state())
⋮----
pub(crate) struct DecisionMakerWrapper {
⋮----
impl DecisionMakerWrapper {
pub(crate) fn new(is_exited: Arc<AtomicBool>, decision_maker: DecisionMaker) -> Self {
⋮----
impl BankingStageMonitor for DecisionMakerWrapper {
fn status(&mut self) -> BankingStageStatus {
if self.is_exited.load(Relaxed) {
⋮----
} else if matches!(
⋮----
mod tests {
⋮----
fn test_buffered_packet_decision_bank() {
⋮----
assert!(BufferedPacketsDecision::Consume(bank).bank().is_some());
assert!(BufferedPacketsDecision::Forward.bank().is_none());
assert!(BufferedPacketsDecision::ForwardAndHold.bank().is_none());
assert!(BufferedPacketsDecision::Hold.bank().is_none());
⋮----
fn test_make_consume_or_forward_decision() {
let genesis_config = create_genesis_config(2).genesis_config;
⋮----
let decision_maker = DecisionMaker::new(shared_leader_state.clone());
assert_matches!(
⋮----
shared_leader_state.store(Arc::new(LeaderState::new(
Some(bank.clone()),
⋮----
shared_leader_state.store(Arc::new(LeaderState::new(None, 0, None, None)));
for next_leader_slot_offset in [0, 1].into_iter() {
let next_leader_slot = bank.slot() + next_leader_slot_offset;
⋮----
Some(next_leader_slot * DEFAULT_TICKS_PER_SLOT),
Some((next_leader_slot, next_leader_slot + 4)),
⋮----
let decision = decision_maker.make_consume_or_forward_decision();
assert!(
⋮----
for next_leader_slot_offset in [2, 19].into_iter() {
⋮----
let next_leader_slot = 20 + bank.slot();

================
File: core/src/banking_stage/latest_validator_vote_packet.rs
================
use solana_perf::packet::PacketRef;
⋮----
pub enum VoteSource {
⋮----
pub struct LatestValidatorVote {
⋮----
impl LatestValidatorVote {
pub fn new_from_view(
⋮----
.program_instructions_iter()
.next()
.ok_or(DeserializedPacketError::VoteTransaction)?;
⋮----
matches!(
⋮----
ix.is_single_vote_state_update()
⋮----
if instruction_filter(&vote_state_update_instruction) =>
⋮----
.first()
.copied()
⋮----
.static_account_keys()
.get(vote_account_index as usize)
⋮----
let slot = vote_state_update_instruction.last_voted_slot().unwrap_or(0);
let hash = vote_state_update_instruction.hash();
let timestamp = vote_state_update_instruction.timestamp();
Ok(Self {
vote: Some(vote),
⋮----
_ => Err(DeserializedPacketError::VoteTransaction),
⋮----
pub fn new(
⋮----
if !packet.meta().is_simple_vote_tx() {
return Err(DeserializedPacketError::VoteTransaction);
⋮----
std::sync::Arc::new(packet.data(..).unwrap().to_vec()),
⋮----
.unwrap();
⋮----
pub fn vote_pubkey(&self) -> Pubkey {
⋮----
pub fn slot(&self) -> Slot {
⋮----
pub fn source(&self) -> VoteSource {
⋮----
pub(crate) fn hash(&self) -> Hash {
⋮----
pub fn timestamp(&self) -> Option<UnixTimestamp> {
⋮----
pub fn is_vote_taken(&self) -> bool {
self.vote.is_none()
⋮----
pub fn take_vote(&mut self) -> Option<SanitizedTransactionView<SharedBytes>> {
self.vote.take()
⋮----
pub enum DeserializedPacketError {
⋮----
mod tests {
⋮----
fn deserialize_packets(
⋮----
.iter()
.filter_map(move |packet| LatestValidatorVote::new(packet, vote_source, true).ok())
⋮----
fn test_deserialize_vote_packets() {
⋮----
new_tower_sync_transaction(
TowerSync::from(vec![(0, 3), (1, 2), (2, 1)]),
⋮----
.meta_mut()
⋮----
.set(PacketFlags::SIMPLE_VOTE_TX, true);
⋮----
TowerSync::from(vec![(0, 3), (1, 2), (3, 1)]),
⋮----
Some(switch_proof),
⋮----
transfer(
⋮----
PacketBatch::from(vec![tower_sync, tower_sync_switch, random_transaction]);
⋮----
deserialize_packets(&packet_batch, VoteSource::Gossip).collect_vec();
assert_eq!(2, deserialized_packets.len());
assert_eq!(VoteSource::Gossip, deserialized_packets[0].vote_source);
assert_eq!(VoteSource::Gossip, deserialized_packets[1].vote_source);
assert_eq!(
⋮----
assert!(deserialized_packets[0].vote.is_some());
assert!(deserialized_packets[1].vote.is_some());

================
File: core/src/banking_stage/leader_slot_metrics.rs
================
pub(crate) struct ProcessTransactionsSummary {
⋮----
pub struct CommittedTransactionsCounts {
⋮----
impl CommittedTransactionsCounts {
pub fn accumulate(
⋮----
struct LeaderSlotPacketCountMetrics {
⋮----
impl LeaderSlotPacketCountMetrics {
fn new() -> Self {
⋮----
fn report(&self, slot: Slot) {
⋮----
datapoint_info!(
⋮----
fn report_transaction_error_metrics(errors: &TransactionErrorMetrics, slot: Slot) {
⋮----
pub(crate) struct LeaderSlotMetrics {
⋮----
impl LeaderSlotMetrics {
pub(crate) fn new(slot: Slot) -> Self {
⋮----
pub(crate) fn report(&mut self) {
⋮----
self.timing_metrics.report(self.slot);
report_transaction_error_metrics(&self.transaction_error_metrics, self.slot);
self.packet_count_metrics.report(self.slot);
self.vote_packet_count_metrics.report(self.slot);
⋮----
fn reported_slot(&self) -> Option<Slot> {
⋮----
Some(self.slot)
⋮----
fn mark_slot_end_detected(&mut self) {
self.timing_metrics.mark_slot_end_detected();
⋮----
pub(crate) struct VotePacketCountMetrics {
⋮----
impl VotePacketCountMetrics {
⋮----
pub(crate) enum MetricsTrackerAction {
⋮----
pub struct LeaderSlotMetricsTracker {
⋮----
impl LeaderSlotMetricsTracker {
pub(crate) fn check_leader_slot_boundary(
⋮----
match (self.leader_slot_metrics.as_mut(), bank) {
⋮----
leader_slot_metrics.mark_slot_end_detected();
⋮----
MetricsTrackerAction::NewTracker(Some(LeaderSlotMetrics::new(bank.slot())))
⋮----
if leader_slot_metrics.slot != bank.slot() {
⋮----
MetricsTrackerAction::ReportAndNewTracker(Some(LeaderSlotMetrics::new(
bank.slot(),
⋮----
pub(crate) fn apply_action(&mut self, action: MetricsTrackerAction) -> Option<Slot> {
⋮----
if let Some(leader_slot_metrics) = self.leader_slot_metrics.as_mut() {
leader_slot_metrics.report();
reported_slot = leader_slot_metrics.reported_slot();
⋮----
self.leader_slot_metrics.as_ref().unwrap().reported_slot()
⋮----
pub(crate) fn accumulate_process_transactions_summary(
⋮----
.retryable_errored_transaction_count += retryable_transaction_indexes.len() as u64;
⋮----
.saturating_sub(committed_transactions_count)
.saturating_sub(retryable_transaction_indexes.len() as u64);
⋮----
.accumulate(execute_and_commit_timings);
⋮----
pub(crate) fn accumulate_vote_batch_insertion_metrics(
⋮----
self.increment_exceeded_buffer_limit_dropped_packets_count(
vote_batch_insertion_metrics.total_dropped_packets() as u64,
⋮----
self.increment_dropped_gossip_vote_count(
vote_batch_insertion_metrics.dropped_gossip_packets() as u64,
⋮----
self.increment_dropped_tpu_vote_count(
vote_batch_insertion_metrics.dropped_tpu_packets() as u64
⋮----
pub(crate) fn accumulate_transaction_errors(
⋮----
.accumulate(error_metrics);
⋮----
pub(crate) fn increment_received_packet_counts(&mut self, stats: PacketReceiverStats) {
⋮----
pub(crate) fn increment_exceeded_buffer_limit_dropped_packets_count(&mut self, count: u64) {
⋮----
pub(crate) fn increment_newly_buffered_packets_count(&mut self, count: u64) {
⋮----
pub(crate) fn increment_retryable_packets_filtered_count(&mut self, count: u64) {
⋮----
pub(crate) fn increment_retryable_packets_count(&mut self, count: u64) {
⋮----
pub(crate) fn set_end_of_slot_unprocessed_buffer_len(&mut self, len: u64) {
⋮----
pub(crate) fn increment_process_buffered_packets_us(&mut self, us: u64) {
⋮----
pub(crate) fn increment_receive_and_buffer_packets_us(&mut self, us: u64) {
⋮----
pub(crate) fn increment_make_decision_us(&mut self, us: u64) {
⋮----
pub(crate) fn increment_consume_buffered_packets_us(&mut self, us: u64) {
⋮----
pub(crate) fn increment_process_packets_transactions_us(&mut self, us: u64) {
⋮----
pub(crate) fn increment_process_transactions_us(&mut self, us: u64) {
⋮----
pub(crate) fn increment_filter_retryable_packets_us(&mut self, us: u64) {
⋮----
pub(crate) fn increment_dropped_gossip_vote_count(&mut self, count: u64) {
⋮----
pub(crate) fn increment_dropped_tpu_vote_count(&mut self, count: u64) {
⋮----
mod tests {
⋮----
struct TestSlotBoundaryComponents {
⋮----
fn setup_test_slot_boundary_banks() -> TestSlotBoundaryComponents {
let genesis = create_genesis_config(10);
⋮----
first_bank.clone(),
⋮----
first_bank.slot() + 1,
⋮----
pub fn test_update_on_leader_slot_boundary_not_leader_to_not_leader() {
⋮----
} = setup_test_slot_boundary_banks();
let action = leader_slot_metrics_tracker.check_leader_slot_boundary(None);
assert_eq!(
⋮----
assert!(leader_slot_metrics_tracker.apply_action(action).is_none());
assert!(leader_slot_metrics_tracker.leader_slot_metrics.is_none());
⋮----
pub fn test_update_on_leader_slot_boundary_not_leader_to_leader() {
⋮----
let action = leader_slot_metrics_tracker.check_leader_slot_boundary(Some(&first_bank));
⋮----
assert!(leader_slot_metrics_tracker.leader_slot_metrics.is_some());
⋮----
pub fn test_update_on_leader_slot_boundary_leader_to_not_leader() {
⋮----
pub fn test_update_on_leader_slot_boundary_leader_to_leader_same_slot() {
⋮----
pub fn test_update_on_leader_slot_boundary_leader_to_leader_bigger_slot() {
⋮----
let action = leader_slot_metrics_tracker.check_leader_slot_boundary(Some(&next_bank));
⋮----
pub fn test_update_on_leader_slot_boundary_leader_to_leader_smaller_slot() {

================
File: core/src/banking_stage/leader_slot_timing_metrics.rs
================
pub struct LeaderExecuteAndCommitTimings {
⋮----
impl LeaderExecuteAndCommitTimings {
pub fn accumulate(&mut self, other: &LeaderExecuteAndCommitTimings) {
⋮----
.accumulate(&other.record_transactions_timings);
self.execute_timings.accumulate(&other.execute_timings);
⋮----
pub fn report(&self, slot: Slot) {
datapoint_info!(
⋮----
pub(crate) struct LeaderSlotTimingMetrics {
⋮----
impl LeaderSlotTimingMetrics {
pub(crate) fn new() -> Self {
⋮----
pub(crate) fn report(&self, slot: Slot) {
self.outer_loop_timings.report(slot);
self.process_buffered_packets_timings.report(slot);
self.consume_buffered_packets_timings.report(slot);
self.process_packets_timings.report(slot);
self.execute_and_commit_timings.report(slot);
⋮----
pub(crate) fn mark_slot_end_detected(&mut self) {
self.outer_loop_timings.mark_slot_end_detected();
⋮----
pub(crate) struct OuterLoopTimings {
⋮----
impl OuterLoopTimings {
fn new() -> Self {
⋮----
fn mark_slot_end_detected(&mut self) {
⋮----
self.bank_detected_time.elapsed().as_micros() as u64;
⋮----
fn report(&self, slot: Slot) {
⋮----
pub(crate) struct ProcessBufferedPacketsTimings {
⋮----
impl ProcessBufferedPacketsTimings {
⋮----
pub(crate) struct ConsumeBufferedPacketsTimings {
⋮----
impl ConsumeBufferedPacketsTimings {
⋮----
pub(crate) struct ProcessPacketsTimings {
⋮----
impl ProcessPacketsTimings {

================
File: core/src/banking_stage/progress_tracker.rs
================
pub fn spawn(
⋮----
.name("solProgTrker".to_string())
.spawn(move || {
⋮----
.run(&mut producer);
⋮----
.unwrap()
⋮----
struct ProgressTracker {
⋮----
impl ProgressTracker {
fn new(
⋮----
fn run(mut self, producer: &mut shaq::Producer<ProgressMessage>) {
⋮----
while !self.exit.load(Ordering::Relaxed) {
let (message, tick_height) = self.produce_progress_message();
⋮----
if !self.publish(producer, message) {
⋮----
.iter()
.for_each(|metrics| metrics.maybe_report_and_reset());
⋮----
fn publish(
⋮----
producer.sync();
if producer.try_write(message).is_ok() {
producer.commit();
⋮----
fn produce_progress_message(&mut self) -> (ProgressMessage, u64) {
let leader_state = self.shared_leader_state.load();
let tick_height = leader_state.tick_height();
⋮----
.next_leader_slot_range()
.unwrap_or((u64::MAX, u64::MAX));
let progress_message = if let Some(working_bank) = leader_state.working_bank() {
if self.last_observed_leader_slot != Some(working_bank.slot()) {
let cost_tracker = working_bank.read_cost_tracker().unwrap();
self.limit_and_shared_block_cost = Some((
cost_tracker.get_block_limit(),
cost_tracker.shared_block_cost(),
⋮----
self.last_observed_leader_slot = Some(working_bank.slot());
⋮----
current_slot: working_bank.slot(),
⋮----
remaining_cost_units: self.remaining_block_cost(),
current_slot_progress: progress(
working_bank.slot(),
⋮----
let current_slot = slot_from_tick_height(tick_height, self.ticks_per_slot);
⋮----
current_slot_progress: progress(current_slot, tick_height, self.ticks_per_slot),
⋮----
fn remaining_block_cost(&self) -> u64 {
⋮----
.as_ref()
.map(|(limit, shared_block_cost)| limit.saturating_sub(shared_block_cost.load()))
.unwrap_or(0)
⋮----
fn progress(slot: Slot, tick_height: u64, ticks_per_slot: u64) -> u8 {
debug_assert!(ticks_per_slot < u8::MAX as u64 && ticks_per_slot > 0);
((100 * tick_height.saturating_sub(slot * ticks_per_slot)) / ticks_per_slot) as u8
⋮----
fn slot_from_tick_height(tick_height: u64, ticks_per_slot: u64) -> u64 {
⋮----
mod tests {
⋮----
fn test_progress_tracker_produce_progress_message() {
⋮----
shared_leader_state.clone(),
vec![],
⋮----
let (message, tick_height) = progress_tracker.produce_progress_message();
assert_eq!(tick_height, 0);
assert_eq!(
⋮----
assert_eq!(message.current_slot, 0);
assert_eq!(message.current_slot_progress, 0);
assert_eq!(message.next_leader_slot, u64::MAX);
assert_eq!(message.leader_range_end, u64::MAX);
⋮----
shared_leader_state.store(Arc::new(LeaderState::new(
⋮----
assert_eq!(tick_height, expected_tick_height);
⋮----
assert_eq!(message.current_slot, 2);
⋮----
Some(4 * ticks_per_slot),
Some((4, 7)),
⋮----
assert_eq!(message.next_leader_slot, 4);
assert_eq!(message.leader_range_end, 7);
⋮----
Some(bank.clone()),
bank.tick_height(),
⋮----
assert!(!bank.is_complete());
⋮----
assert_eq!(tick_height, bank.tick_height());
assert_eq!(message.leader_state, agave_scheduler_bindings::IS_LEADER);
assert_eq!(message.current_slot, bank.slot());
⋮----
bank.fill_bank_with_ticks_for_tests();
assert!(bank.is_complete());
⋮----
assert_eq!(message.current_slot_progress, 100);
⋮----
fn test_progress_tracker_remaining_block_cost() {
⋮----
assert_eq!(0, progress_tracker.remaining_block_cost());
⋮----
progress_tracker.limit_and_shared_block_cost = Some((block_limit, SharedBlockCost::new(0)));
assert_eq!(block_limit, progress_tracker.remaining_block_cost());
⋮----
Some((block_limit, SharedBlockCost::new(block_limit / 2)));
assert_eq!(block_limit / 2, progress_tracker.remaining_block_cost());
⋮----
fn test_progress() {
⋮----
assert_eq!(0, progress(0, 0, ticks_per_slot));
assert_eq!(1, progress(0, 1, ticks_per_slot));
assert_eq!(3, progress(0, 2, ticks_per_slot));
assert_eq!(98, progress(0, ticks_per_slot - 1, ticks_per_slot));
assert_eq!(100, progress(0, ticks_per_slot, ticks_per_slot));
assert_eq!(0, progress(1, ticks_per_slot, ticks_per_slot));
assert_eq!(3, progress(1, ticks_per_slot + 2, ticks_per_slot));
⋮----
fn test_slot_from_tick_height() {
⋮----
assert_eq!(0, slot_from_tick_height(0, ticks_per_slot));
assert_eq!(0, slot_from_tick_height(ticks_per_slot - 1, ticks_per_slot));
assert_eq!(1, slot_from_tick_height(ticks_per_slot, ticks_per_slot));
assert_eq!(1, slot_from_tick_height(ticks_per_slot + 1, ticks_per_slot));
⋮----
assert_eq!(2, slot_from_tick_height(2 * ticks_per_slot, ticks_per_slot));

================
File: core/src/banking_stage/qos_service.rs
================
mod transaction {
⋮----
pub struct QosService {
⋮----
impl QosService {
pub fn new(id: u32) -> Self {
⋮----
pub fn select_and_accumulate_transaction_costs<'a, Tx: TransactionWithMeta>(
⋮----
self.compute_transaction_costs(&bank.feature_set, transactions.iter(), pre_results);
let (transactions_qos_cost_results, num_included) = self.select_transactions_per_cost(
transactions.iter(),
transaction_costs.into_iter(),
⋮----
self.accumulate_estimated_transaction_costs(&Self::accumulate_batched_transaction_costs(
transactions_qos_cost_results.iter(),
⋮----
transactions.len().saturating_sub(num_included) as u64;
⋮----
// invoke cost_model to calculate cost for the given list of transactions that have not
// been filtered out already.
fn compute_transaction_costs<'a, Tx: TransactionWithMeta>(
⋮----
.zip(pre_results)
.map(|(tx, pre_result)| pre_result.map(|()| CostModel::calculate_cost(tx, feature_set)))
.collect();
compute_cost_time.stop();
⋮----
.fetch_add(compute_cost_time.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(txs_costs.len() as u64, Ordering::Relaxed);
⋮----
fn select_transactions_per_cost<'a, Tx: TransactionWithMeta>(
⋮----
let mut cost_tracker = bank.write_cost_tracker().unwrap();
⋮----
.zip(transactions_costs)
.map(|(tx, cost)| match cost {
Ok(cost) => match cost_tracker.try_add(&cost) {
⋮----
debug!(
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
Ok(cost)
⋮----
Err(TransactionError::from(e))
⋮----
Err(e) => Err(e),
⋮----
cost_tracker.add_transactions_in_flight(num_included);
cost_tracking_time.stop();
⋮----
.fetch_add(cost_tracking_time.as_us(), Ordering::Relaxed);
⋮----
pub fn remove_or_update_costs<'a, Tx: TransactionWithMeta + 'a>(
⋮----
fn remove_or_update_recorded_transaction_costs<'a, Tx: TransactionWithMeta + 'a>(
⋮----
.zip(transaction_committed_status)
.for_each(|(tx_cost, transaction_committed_details)| {
⋮----
cost_tracker.update_execution_cost(
⋮----
cost_tracker.remove(tx_cost);
⋮----
cost_tracker.sub_transactions_in_flight(num_included);
⋮----
fn remove_unrecorded_transaction_costs<'a, Tx: TransactionWithMeta + 'a>(
⋮----
transaction_cost_results.for_each(|tx_cost| {
⋮----
pub fn report_metrics(&self, slot: Slot) {
self.metrics.report(slot);
⋮----
fn accumulate_estimated_transaction_costs(
⋮----
.fetch_add(batched_signature_cost, Ordering::Relaxed);
⋮----
.fetch_add(batched_write_lock_cost, Ordering::Relaxed);
⋮----
.fetch_add(batched_data_bytes_cost, Ordering::Relaxed);
⋮----
.fetch_add(batched_loaded_accounts_data_size_cost, Ordering::Relaxed);
⋮----
.fetch_add(batched_programs_execute_cost, Ordering::Relaxed);
⋮----
.fetch_add(batched_retried_txs_per_block_limit_count, Ordering::Relaxed);
⋮----
.fetch_add(batched_retried_txs_per_vote_limit_count, Ordering::Relaxed);
⋮----
.fetch_add(
⋮----
pub fn accumulate_actual_execute_cu(&self, units: u64) {
⋮----
.fetch_add(units, Ordering::Relaxed);
⋮----
pub fn accumulate_actual_execute_time(&self, micro_sec: u64) {
⋮----
.fetch_add(micro_sec, Ordering::Relaxed);
⋮----
fn accumulate_batched_transaction_costs<'a, Tx: TransactionWithMeta + 'a>(
⋮----
transactions_costs.for_each(|cost| match cost {
⋮----
batched_transaction_details.costs.batched_signature_cost += cost.signature_cost();
batched_transaction_details.costs.batched_write_lock_cost += cost.write_lock_cost();
⋮----
u64::from(cost.data_bytes_cost());
⋮----
cost.loaded_accounts_data_size_cost();
⋮----
.batched_programs_execute_cost += cost.programs_execution_cost();
⋮----
struct QosServiceMetrics {
⋮----
struct QosServiceMetricsStats {
⋮----
struct QosServiceMetricsErrors {
⋮----
impl QosServiceMetrics {
⋮----
id: id.to_string(),
⋮----
pub fn report(&self, bank_slot: Slot) {
if bank_slot != self.slot.load(Ordering::Relaxed) {
datapoint_info!(
⋮----
self.slot.store(bank_slot, Ordering::Relaxed);
⋮----
mod tests {
⋮----
fn test_compute_transaction_costs() {
⋮----
system_transaction::transfer(&keypair, &keypair.pubkey(), 1, Hash::default()),
⋮----
TowerSync::from(vec![(42, 1)]),
⋮----
let txs = [transfer_tx.clone(), vote_tx.clone(), vote_tx, transfer_tx];
⋮----
let txs_costs = qos_service.compute_transaction_costs(
⋮----
txs.iter(),
std::iter::repeat(Ok(())),
⋮----
assert_eq!(txs_costs.len(), txs.len());
⋮----
.iter()
.enumerate()
.map(|(index, cost)| {
assert_eq!(
⋮----
.collect_vec();
⋮----
fn test_select_transactions_per_cost() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10);
⋮----
CostModel::calculate_cost(&transfer_tx, &FeatureSet::all_enabled()).sum();
let vote_tx_cost = CostModel::calculate_cost(&vote_tx, &FeatureSet::all_enabled()).sum();
let txs = [transfer_tx.clone(), vote_tx.clone(), transfer_tx, vote_tx];
⋮----
bank.write_cost_tracker()
.unwrap()
.set_limits(cost_limit, cost_limit, cost_limit);
⋮----
qos_service.select_transactions_per_cost(txs.iter(), txs_costs.into_iter(), &bank);
assert_eq!(num_selected, 2);
assert_eq!(results.len(), txs.len());
assert!(results[0].is_ok());
assert!(results[1].is_ok());
assert!(results[2].is_err());
assert!(results[3].is_err());
⋮----
fn test_update_and_remove_transaction_costs_committed() {
⋮----
solana_system_interface::instruction::transfer(&keypair.pubkey(), &solana_pubkey::Pubkey::new_unique(), 1),
⋮----
Some(&keypair.pubkey()),
⋮----
let transfer_tx = RuntimeTransaction::from_transaction_for_tests(transaction.clone());
⋮----
.map(|_| transfer_tx.clone())
⋮----
.map(|cost| cost.as_ref().unwrap().sum())
.sum();
⋮----
.map(|tx_cost| CommitTransactionDetails::Committed {
compute_units: tx_cost.as_ref().unwrap().programs_execution_cost()
⋮----
result: Ok(()),
⋮----
qos_cost_results.iter(),
Some(&committed_status),
⋮----
fn test_update_and_remove_transaction_costs_not_committed() {
⋮----
.map(|_| {
⋮----
&keypair.pubkey(),
⋮----
QosService::remove_or_update_costs(qos_cost_results.iter(), None, &bank);
assert_eq!(0, bank.read_cost_tracker().unwrap().block_cost());
assert_eq!(0, bank.read_cost_tracker().unwrap().transaction_count());
⋮----
fn test_update_and_remove_transaction_costs_mixed_execution() {
⋮----
.map(|_| RuntimeTransaction::from_transaction_for_tests(transaction.clone()))
⋮----
.map(|(n, tx_cost)| {
⋮----
qos_cost_results.iter().enumerate().for_each(|(n, cost)| {
⋮----
expected_final_block_cost += cost.as_ref().unwrap().sum()
⋮----
fn test_accumulate_batched_transaction_costs() {
⋮----
let dummy_transaction = WritableKeysTransaction(vec![]);
⋮----
.map(|n| {
⋮----
Ok(TransactionCost::Transaction(UsageCostDetails {
⋮----
Err(TransactionError::WouldExceedMaxBlockCostLimit)
⋮----
QosService::accumulate_batched_transaction_costs(tx_cost_results.iter());

================
File: core/src/banking_stage/read_write_account_set.rs
================
pub struct ReadWriteAccountSet {
⋮----
impl ReadWriteAccountSet {
pub fn check_locks(&self, message: &impl SVMMessage) -> bool {
⋮----
.account_keys()
.iter()
.enumerate()
.all(|(index, pubkey)| {
if message.is_writable(index) {
self.can_write(pubkey)
⋮----
self.can_read(pubkey)
⋮----
pub fn take_locks(&mut self, message: &impl SVMMessage) -> bool {
⋮----
.fold(true, |all_available, (index, pubkey)| {
⋮----
all_available & self.add_write(pubkey)
⋮----
all_available & self.add_read(pubkey)
⋮----
pub fn clear(&mut self) {
self.read_set.clear();
self.write_set.clear();
⋮----
fn can_read(&self, pubkey: &Pubkey) -> bool {
!self.write_set.contains(pubkey)
⋮----
fn can_write(&self, pubkey: &Pubkey) -> bool {
!self.write_set.contains(pubkey) && !self.read_set.contains(pubkey)
⋮----
fn add_read(&mut self, pubkey: &Pubkey) -> bool {
let can_read = self.can_read(pubkey);
self.read_set.insert(*pubkey);
⋮----
fn add_write(&mut self, pubkey: &Pubkey) -> bool {
let can_write = self.can_write(pubkey);
self.write_set.insert(*pubkey);
⋮----
mod tests {
⋮----
fn create_test_versioned_message(
⋮----
num_required_signatures: write_keys.len() as u8,
⋮----
num_readonly_unsigned_accounts: read_keys.len() as u8,
⋮----
account_keys: write_keys.iter().chain(read_keys.iter()).copied().collect(),
⋮----
instructions: vec![],
⋮----
fn create_test_sanitized_transaction(
⋮----
let message = create_test_versioned_message(
&[write_keypair.pubkey()],
⋮----
VersionedTransaction::try_new(message, &[write_keypair]).unwrap(),
⋮----
Some(false),
⋮----
bank.get_reserved_account_keys(),
⋮----
.unwrap()
⋮----
fn create_test_address_lookup_table(
⋮----
addresses.resize_with(num_addresses, Pubkey::new_unique);
⋮----
let data = address_lookup_table.serialize_for_tests().unwrap();
⋮----
AccountSharedData::new(1, data.len(), &address_lookup_table::program::id());
account.set_data(data);
bank.store_account(&address_table_key, &account);
let slot = bank.slot() + 1;
⋮----
fn create_test_bank() -> (Arc<Bank>, Arc<RwLock<BankForks>>) {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
fn test_check_and_take_locks(conflict_index: usize, add_write: bool, expectation: bool) {
let (bank, _bank_forks) = create_test_bank();
let (bank, table_address) = create_test_address_lookup_table(bank, 2);
let tx = create_test_sanitized_transaction(
⋮----
vec![MessageAddressTableLookup {
⋮----
let message = tx.message();
⋮----
let conflict_key = message.account_keys().get(conflict_index).unwrap();
⋮----
account_locks.add_write(conflict_key);
⋮----
account_locks.add_read(conflict_key);
⋮----
assert_eq!(expectation, account_locks.check_locks(message));
assert_eq!(expectation, account_locks.take_locks(message));
⋮----
fn test_check_and_take_locks_write_write_conflict() {
test_check_and_take_locks(0, true, false);
test_check_and_take_locks(2, true, false);
⋮----
fn test_check_and_take_locks_read_write_conflict() {
test_check_and_take_locks(0, false, false);
test_check_and_take_locks(2, false, false);
⋮----
fn test_check_and_take_locks_write_read_conflict() {
test_check_and_take_locks(1, true, false);
test_check_and_take_locks(3, true, false);
⋮----
fn test_check_and_take_locks_read_read_non_conflict() {
test_check_and_take_locks(1, false, true);
test_check_and_take_locks(3, false, true);
⋮----
pub fn test_write_write_conflict() {
⋮----
assert!(account_locks.can_write(&account));
account_locks.add_write(&account);
assert!(!account_locks.can_write(&account));
⋮----
pub fn test_read_write_conflict() {
⋮----
assert!(account_locks.can_read(&account));
account_locks.add_read(&account);
⋮----
pub fn test_write_read_conflict() {
⋮----
assert!(!account_locks.can_read(&account));
⋮----
pub fn test_read_read_non_conflict() {
⋮----
pub fn test_write_write_different_keys() {
⋮----
assert!(account_locks.can_write(&account1));
account_locks.add_write(&account1);
assert!(account_locks.can_write(&account2));
assert!(account_locks.can_read(&account2));

================
File: core/src/banking_stage/scheduler_messages.rs
================
pub struct TransactionBatchId(pub u64);
impl TransactionBatchId {
pub fn new(index: u64) -> Self {
Self(index)
⋮----
impl Display for TransactionBatchId {
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
write!(f, "{}", self.0)
⋮----
pub type TransactionId = usize;
⋮----
pub struct MaxAge {
⋮----
impl MaxAge {
⋮----
pub struct ConsumeWork<Tx> {
⋮----
pub struct FinishedConsumeWork<Tx> {
⋮----
pub struct FinishedConsumeWorkExtraInfo {
⋮----
pub enum TransactionResult {
⋮----
pub enum NotCommittedReason {

================
File: core/src/banking_stage/tpu_to_pack.rs
================
pub struct BankingPacketReceivers {
⋮----
pub fn spawn(
⋮----
.name("solTpu2Pack".to_string())
.spawn(move || {
tpu_to_pack(exit, receivers, allocator, producer);
⋮----
.unwrap()
⋮----
fn tpu_to_pack(
⋮----
.unwrap_or_else(crossbeam_channel::never);
⋮----
while !exit.load(Ordering::Relaxed) {
⋮----
handle_packet_batches(&allocator, &mut producer, packet_batches);
⋮----
fn handle_packet_batches(
⋮----
allocator.clean_remote_free_lists();
producer.sync();
'batch_loop: for batch in packet_batches.iter() {
for packet in batch.iter() {
// Check if the packet is valid and get the bytes.
let Some(packet_bytes) = packet.data(..) else {
⋮----
let packet_size = packet_bytes.len();
// Allocate space for the packet to be copied into.
let Some(allocated_ptr) = allocator.allocate(packet_size as u32) else {
warn!("Failed to allocate. Dropping the rest of the batch.");
⋮----
let allocated_ptr_offset_in_allocator = unsafe { allocator.offset(allocated_ptr) };
⋮----
copy_packet_and_populate_message(
⋮----
packet.meta(),
⋮----
if producer.try_write(message).is_err() {
unsafe { allocator.free(allocated_ptr) };
⋮----
producer.commit();
⋮----
unsafe fn copy_packet_and_populate_message(
⋮----
allocated_ptr.copy_from_nonoverlapping(
NonNull::new(packet_bytes.as_ptr().cast_mut()).expect("packet bytes must be non-null"),
packet_bytes.len(),
⋮----
length: packet_bytes.len() as u32,
⋮----
let tpu_message_flags = flags_from_meta(packet_meta.flags);
let src_addr = map_src_addr(packet_meta.addr);
⋮----
fn flags_from_meta(flags: PacketFlags) -> u8 {
⋮----
if flags.contains(PacketFlags::SIMPLE_VOTE_TX) {
⋮----
if flags.contains(PacketFlags::FORWARDED) {
⋮----
if flags.contains(PacketFlags::FROM_STAKED_NODE) {
⋮----
fn map_src_addr(addr: IpAddr) -> [u8; 16] {
⋮----
IpAddr::V4(ipv4) => ipv4.to_ipv6_mapped().octets(),
IpAddr::V6(ipv6) => ipv6.octets(),
⋮----
mod tests {
⋮----
fn test_copy_packet_and_populate_message() {
let packet_bytes = vec![1, 2, 3, 4, 5];
⋮----
packet_meta.size = packet_bytes.len();
⋮----
packet_bytes.as_slice(),
⋮----
NonNull::new(buffer.as_mut_ptr()).unwrap(),
⋮----
assert_eq!(&buffer[..packet_bytes.len()], packet_bytes.as_slice());
assert_eq!(tpu_to_pack_message.transaction.offset, DUMMY_OFFSET);
assert_eq!(
⋮----
fn test_flags_from_meta() {

================
File: core/src/banking_stage/unified_scheduler.rs
================
use qualifier_attr::qualifiers;
⋮----
pub(crate) fn ensure_banking_stage_setup(
⋮----
let sharable_banks = bank_forks.read().unwrap().sharable_banks();
let unified_receiver = channels.unified_receiver().clone();
⋮----
let poh_recorder = poh_recorder.read().unwrap();
⋮----
poh_recorder.is_exited.clone(),
DecisionMaker::from(poh_recorder.deref()),
⋮----
Box::new(DecisionMakerWrapper::new(is_exited, decision_maker.clone()));
⋮----
let decision = decision_maker.make_consume_or_forward_decision();
if matches!(decision, BufferedPacketsDecision::Forward) {
⋮----
let bank = sharable_banks.root();
for batch in batches.iter() {
let task_id_base = helper.generate_task_ids(batch.len());
let tasks = batch.iter().enumerate().filter_map(|(i, packet)| {
let tx: VersionedTransaction = packet.deserialize_slice(..).ok()?;
let tx = SanitizedVersionedTransaction::try_from(tx).ok()?;
⋮----
Some(packet.meta().is_simple_vote_tx()),
⋮----
.ok()?;
⋮----
resolve_addresses_with_deactivation(&tx, &bank).ok()?;
⋮----
bank.get_reserved_account_keys(),
⋮----
validate_account_locks(
tx.account_keys(),
bank.get_transaction_account_lock_limit(),
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(&bank.feature_set)
⋮----
calculate_priority_and_cost(&tx, &compute_budget_limits.into(), &bank);
⋮----
Some(helper.create_new_task(
⋮----
packet.meta().size,
bank.epoch(),
estimate_last_valid_slot(bank.slot().min(deactivation_slot)),
⋮----
helper.send_new_task(task);
⋮----
pool.register_banking_stage(
Some(num_threads.get()),
⋮----
fn resolve_addresses_with_deactivation(
⋮----
let Some(address_table_lookups) = transaction.get_message().message.address_table_lookups()
⋮----
return Ok((LoadedAddresses::default(), Slot::MAX));
⋮----
bank.load_addresses_from_ref(
⋮----
.iter()
.map(SVMMessageAddressTableLookup::from),

================
File: core/src/banking_stage/vote_packet_receiver.rs
================
pub struct VotePacketReceiver {
⋮----
impl VotePacketReceiver {
pub fn new(banking_packet_receiver: BankingPacketReceiver) -> Self {
⋮----
pub fn receive_and_buffer_packets(
⋮----
let (result, recv_time_us) = measure_us!({
⋮----
slot_metrics_tracker.increment_receive_and_buffer_packets_us(recv_time_us);
⋮----
fn receive_until(
⋮----
let packet_batches = self.banking_packet_receiver.recv_timeout(recv_timeout)?;
⋮----
.iter()
.map(|batch| batch.len())
⋮----
let mut messages = vec![packet_batches];
while let Ok(packet_batches) = self.banking_packet_receiver.try_recv() {
⋮----
messages.push(packet_batches);
if start.elapsed() >= recv_timeout || num_packets_received >= packet_count_upperbound {
⋮----
.flat_map(|batches| batches.iter())
.flat_map(|batch| batch.iter())
.filter_map(|pkt| {
⋮----
Arc::new(pkt.data(..)?.to_vec()),
⋮----
Ok(pkt) => Some(pkt),
⋮----
.collect();
⋮----
packet_stats.passed_sigverify_count += errors.saturating_add(parsed_packets.len()) as u64;
⋮----
.saturating_sub(parsed_packets.len())
.saturating_sub(errors) as u64;
Ok((parsed_packets, packet_stats))
⋮----
fn get_receive_timeout(vote_storage: &VoteStorage) -> Duration {
if !vote_storage.is_empty() {
⋮----
fn buffer_packets(
⋮----
let packet_count = deserialized_packets.len();
slot_metrics_tracker.increment_received_packet_counts(packet_stats);
let mut dropped_packets_count = Saturating(0);
⋮----
.fetch_add(packet_count, Ordering::Relaxed);
⋮----
.fetch_add(dropped_packets_count, Ordering::Relaxed);
⋮----
.fetch_add(newly_buffered_packets_count, Ordering::Relaxed);
⋮----
.swap(vote_storage.len(), Ordering::Relaxed);
⋮----
fn push_unprocessed(
⋮----
if !deserialized_packets.is_empty() {
⋮----
.increment(deserialized_packets.len() as u64);
*newly_buffered_packets_count += deserialized_packets.len();
⋮----
.increment_newly_buffered_packets_count(deserialized_packets.len() as u64);
⋮----
vote_storage.insert_batch(vote_source, deserialized_packets.into_iter());
⋮----
.accumulate_vote_batch_insertion_metrics(&vote_batch_insertion_metrics);
*dropped_packets_count += vote_batch_insertion_metrics.total_dropped_packets();
⋮----
pub struct PacketReceiverStats {

================
File: core/src/banking_stage/vote_storage.rs
================
pub(crate) struct VoteBatchInsertionMetrics {
⋮----
impl VoteBatchInsertionMetrics {
pub fn total_dropped_packets(&self) -> usize {
⋮----
pub fn dropped_gossip_packets(&self) -> usize {
⋮----
pub fn dropped_tpu_packets(&self) -> usize {
⋮----
pub struct VoteStorage {
⋮----
impl VoteStorage {
pub fn new(bank: &Bank) -> Self {
⋮----
cached_epoch_stakes: bank.current_epoch_stakes().clone(),
current_epoch: bank.epoch(),
⋮----
.is_active(&feature_set::deprecate_legacy_vote_ixs::id()),
⋮----
pub fn new_for_tests(vote_pubkeys_to_stake: &[Pubkey]) -> Self {
use solana_vote::vote_account::VoteAccount;
⋮----
.iter()
.map(|pubkey| (*pubkey, (1u64, VoteAccount::new_random())))
.collect();
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub fn len(&self) -> usize {
⋮----
pub fn max_receive_size(&self) -> usize {
⋮----
pub(crate) fn insert_batch(
⋮----
self.insert_batch_with_replenish(
votes.filter_map(|vote| {
⋮----
.ok()
⋮----
pub(crate) fn reinsert_packets(
⋮----
packets.filter_map(|packet| {
⋮----
pub fn drain_unprocessed(&mut self, bank: &Bank) -> Vec<SanitizedTransactionView<SharedBytes>> {
⋮----
.get_account(&sysvar::slot_hashes::id())
.and_then(|account| from_account::<SlotHashes, _>(&account));
if slot_hashes.is_none() {
error!(
⋮----
self.weighted_random_order_by_stake()
.filter_map(|pubkey| {
⋮----
.get_mut(&pubkey)
.and_then(|latest_vote| {
⋮----
latest_vote.take_vote().inspect(|_vote| {
⋮----
.collect_vec()
⋮----
pub fn clear(&mut self) {
⋮----
.values_mut()
.for_each(|vote| {
if vote.take_vote().is_some() {
⋮----
pub fn cache_epoch_boundary_info(&mut self, bank: &Bank) {
if bank.epoch() <= self.current_epoch {
⋮----
self.cached_epoch_stakes = bank.current_epoch_stakes().clone();
self.current_epoch = bank.epoch();
⋮----
.is_active(&feature_set::deprecate_legacy_vote_ixs::id());
⋮----
.retain(|vote_pubkey, vote| {
let is_present = !vote.is_vote_taken();
let should_evict = self.cached_epoch_stakes.vote_account_stake(vote_pubkey) == 0;
⋮----
datapoint_info!(
⋮----
fn insert_batch_with_replenish(
⋮----
.vote_account_stake(&vote.vote_pubkey())
⋮----
if let Some(vote) = self.update_latest_vote(vote, should_replenish_taken_votes) {
match vote.source() {
⋮----
fn update_latest_vote(
⋮----
let vote_pubkey = vote.vote_pubkey();
match self.latest_vote_per_vote_pubkey.entry(vote_pubkey) {
⋮----
let latest_vote = entry.get_mut();
⋮----
if old_vote.is_vote_taken() {
⋮----
return Some(old_vote);
⋮----
Some(vote)
⋮----
entry.insert(vote);
⋮----
fn allow_update(
⋮----
let slot = vote.slot();
match slot.cmp(&latest_vote.slot()) {
⋮----
match vote.timestamp().cmp(&latest_vote.timestamp()) {
⋮----
should_replenish_taken_votes && latest_vote.is_vote_taken()
⋮----
fn weighted_random_order_by_stake(&self) -> impl Iterator<Item = Pubkey> + use<> {
⋮----
.keys()
.filter_map(|&pubkey| {
let stake = self.cached_epoch_stakes.vote_account_stake(&pubkey);
⋮----
Some((thread_rng().gen::<f64>().powf(1.0 / (stake as f64)), pubkey))
⋮----
pubkey_with_weight.sort_by(|(w1, _), (w2, _)| w2.partial_cmp(w1).unwrap());
pubkey_with_weight.into_iter().map(|(_, pubkey)| pubkey)
⋮----
fn is_valid_for_our_fork(vote: &LatestValidatorVote, slot_hashes: &Option<SlotHashes>) -> bool {
⋮----
.get(&vote.slot())
.map(|found_hash| *found_hash == vote.hash())
.unwrap_or(false)
⋮----
pub fn get_latest_vote_slot(&self, pubkey: Pubkey) -> Option<solana_clock::Slot> {
⋮----
.get(&pubkey)
.map(|l| l.slot())
⋮----
fn get_latest_timestamp(&self, pubkey: Pubkey) -> Option<solana_clock::UnixTimestamp> {
⋮----
.and_then(|l| l.timestamp())
⋮----
pub(crate) mod tests {
⋮----
pub(crate) fn packet_from_slots(
⋮----
let vote_tx = new_tower_sync_transaction(
⋮----
let mut packet = BytesPacket::from_data(None, vote_tx).unwrap();
⋮----
.meta_mut()
⋮----
.set(PacketFlags::SIMPLE_VOTE_TX, true);
⋮----
fn from_slots(
⋮----
let packet = packet_from_slots(slots, keypairs, timestamp);
LatestValidatorVote::new(packet.as_ref(), vote_source, true).unwrap()
⋮----
fn to_sanitized_view(packet: BytesPacket) -> SanitizedTransactionView<SharedBytes> {
SanitizedTransactionView::try_new_sanitized(Arc::new(packet.buffer().to_vec()), false)
.unwrap()
⋮----
fn test_reinsert_packets() -> Result<(), Box<dyn Error>> {
⋮----
genesis_utils::create_genesis_config_with_leader(100, &node_keypair.pubkey(), 200)
⋮----
new_tower_sync_transaction(
⋮----
vote.meta_mut().flags.set(PacketFlags::SIMPLE_VOTE_TX, true);
let mut vote_storage = VoteStorage::new_for_tests(&[vote_keypair.pubkey()]);
vote_storage.insert_batch(VoteSource::Tpu, std::iter::once(to_sanitized_view(vote)));
assert_eq!(1, vote_storage.len());
let packets = vote_storage.drain_unprocessed(&bank);
vote_storage.reinsert_packets(packets.into_iter());
⋮----
Ok(())
⋮----
fn test_update_latest_vote() {
⋮----
keypair_a.vote_keypair.pubkey(),
keypair_b.vote_keypair.pubkey(),
⋮----
let vote_a = from_slots(vec![(0, 2), (1, 1)], VoteSource::Gossip, &keypair_a, None);
let vote_b = from_slots(
vec![(0, 5), (4, 2), (9, 1)],
⋮----
assert!(vote_storage
⋮----
assert_eq!(2, vote_storage.len());
assert_eq!(
⋮----
let vote_a = from_slots(
vec![(0, 5), (1, 4), (3, 3), (10, 1)],
⋮----
vec![(0, 5), (4, 2), (6, 1)],
⋮----
vote_storage.update_latest_vote(vote_a, false );
vote_storage.update_latest_vote(vote_b, false );
⋮----
Some(1),
⋮----
Some(2),
⋮----
Some(5),
⋮----
Some(6),
⋮----
from_slots(
⋮----
Some(3),
⋮----
vote_storage.update_latest_vote(vote_a(), false );
vote_storage.update_latest_vote(vote_b(), false );
⋮----
for packet in vote_storage.latest_vote_per_vote_pubkey.values_mut() {
packet.take_vote().inspect(|_vote| {
⋮----
assert_eq!(0, vote_storage.len());
⋮----
vote_storage.update_latest_vote(vote_a(), true );
vote_storage.update_latest_vote(vote_b(), true );
⋮----
fn test_clear() {
⋮----
keypair_c.vote_keypair.pubkey(),
keypair_d.vote_keypair.pubkey(),
⋮----
let vote_a = from_slots(vec![(1, 1)], VoteSource::Gossip, &keypair_a, None);
let vote_b = from_slots(vec![(2, 1)], VoteSource::Tpu, &keypair_b, None);
let vote_c = from_slots(vec![(3, 1)], VoteSource::Tpu, &keypair_c, None);
let vote_d = from_slots(vec![(4, 1)], VoteSource::Gossip, &keypair_d, None);
⋮----
vote_storage.update_latest_vote(vote_c, false );
vote_storage.update_latest_vote(vote_d, false );
assert_eq!(4, vote_storage.len());
vote_storage.clear();
⋮----
fn test_insert_batch_unstaked() {
⋮----
let vote_a = packet_from_slots(vec![(1, 1)], &keypair_a, None);
let vote_b = packet_from_slots(vec![(vote_b_slot, 1)], &keypair_b, None);
let vote_c = packet_from_slots(vec![(vote_c_slot, 1)], &keypair_c, None);
let vote_d = packet_from_slots(vec![(4, 1)], &keypair_d, None);
⋮----
vec![
⋮----
vote_storage.insert_batch(VoteSource::Tpu, votes().into_iter());
assert!(vote_storage.is_empty());
⋮----
genesis_utils::create_genesis_config_with_vote_accounts(100, &[&keypair_a], vec![200])
⋮----
assert_eq!(bank.epoch(), 0);
vote_storage.cache_epoch_boundary_info(&bank);
⋮----
genesis_utils::create_genesis_config_with_vote_accounts(100, &[&keypair_b], vec![200])
⋮----
assert_eq!(bank.epoch(), 1);
⋮----
vote_storage.insert_batch(VoteSource::Gossip, votes().into_iter());
assert_eq!(vote_storage.len(), 1);
⋮----
genesis_utils::create_genesis_config_with_vote_accounts(100, &[&keypair_c], vec![200])
⋮----
assert_eq!(bank.epoch(), 2);
⋮----
assert_eq!(vote_storage.len(), 0);

================
File: core/src/banking_stage/vote_worker.rs
================
mod transaction {
⋮----
pub struct VoteWorker {
⋮----
impl VoteWorker {
pub fn new(
⋮----
pub fn run(mut self) {
⋮----
while !self.exit.load(Ordering::Relaxed) {
if !self.storage.is_empty()
|| last_metrics_update.elapsed() >= SLOT_BOUNDARY_CHECK_PERIOD
⋮----
measure_us!(self.process_buffered_packets(
⋮----
.increment_process_buffered_packets_us(process_buffered_packets_us);
⋮----
match self.tpu_receiver.receive_and_buffer_packets(
⋮----
match self.gossip_receiver.receive_and_buffer_packets(
⋮----
banking_stage_stats.report(1000);
⋮----
fn process_buffered_packets(
⋮----
measure_us!(self.decision_maker.make_consume_or_forward_decision());
let metrics_action = slot_metrics_tracker.check_leader_slot_boundary(decision.bank());
slot_metrics_tracker.increment_make_decision_us(make_decision_us);
slot_metrics_tracker.apply_action(metrics_action);
⋮----
let (_, consume_buffered_packets_us) = measure_us!(self.consume_buffered_packets(
⋮----
.increment_consume_buffered_packets_us(consume_buffered_packets_us);
⋮----
let current_bank = self.bank_forks.read().unwrap().working_bank();
self.storage.cache_epoch_boundary_info(&current_bank);
self.storage.clear();
⋮----
fn consume_buffered_packets(
⋮----
if self.storage.is_empty() {
⋮----
let num_packets_to_process = self.storage.len();
let reached_end_of_slot = self.process_packets(
⋮----
slot_metrics_tracker.set_end_of_slot_unprocessed_buffer_len(self.storage.len() as u64);
⋮----
proc_start.stop();
debug!(
⋮----
.fetch_add(proc_start.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(rebuffered_packet_count, Ordering::Relaxed);
⋮----
.fetch_add(consumed_buffered_packets_count, Ordering::Relaxed);
⋮----
fn process_packets(
⋮----
let all_vote_packets = self.storage.drain_unprocessed(bank);
⋮----
for chunk in Itertools::chunks(all_vote_packets.into_iter(), UNPROCESSED_BUFFER_STEP_SIZE)
.into_iter()
⋮----
debug_assert!(resolved_txs.is_empty());
⋮----
self.storage.reinsert_packets(chunk.into_iter());
⋮----
for packet in chunk.into_iter() {
⋮----
consume_scan_should_process_packet(bank, packet, &mut error_counters)
⋮----
resolved_txs.push(tx);
⋮----
if let Some(retryable_vote_indices) = self.do_process_packets(
⋮----
self.storage.reinsert_packets(
⋮----
.map(|tx| tx.into_inner_transaction().into_view()),
⋮----
.drain(..)
⋮----
fn do_process_packets(
⋮----
let (process_transactions_summary, process_packets_transactions_us) = measure_us!(self
⋮----
.increment_process_packets_transactions_us(process_packets_transactions_us);
⋮----
*reached_end_of_slot = has_reached_end_of_slot(reached_max_poh_height, bank);
⋮----
.len()
.saturating_sub(retryable_transaction_indexes.len());
*rebuffered_packet_count += retryable_transaction_indexes.len();
⋮----
.increment_retryable_packets_count(retryable_transaction_indexes.len() as u64);
Some(retryable_transaction_indexes)
⋮----
fn process_packets_transactions(
⋮----
measure_us!(self.process_transactions(bank, sanitized_transactions));
slot_metrics_tracker.increment_process_transactions_us(process_transactions_us);
⋮----
.fetch_add(process_transactions_us, Ordering::Relaxed);
⋮----
slot_metrics_tracker.accumulate_process_transactions_summary(&process_transactions_summary);
slot_metrics_tracker.accumulate_transaction_errors(error_counters);
⋮----
measure_us!(Self::filter_pending_packets_from_pending_txs(
⋮----
slot_metrics_tracker.increment_filter_retryable_packets_us(filter_retryable_packets_us);
⋮----
.fetch_add(filter_retryable_packets_us, Ordering::Relaxed);
⋮----
.saturating_sub(filtered_retryable_transaction_indexes.len());
⋮----
.increment_retryable_packets_filtered_count(retryable_packets_filtered_count as u64);
⋮----
.fetch_add(retryable_packets_filtered_count, Ordering::Relaxed);
⋮----
fn process_transactions(
⋮----
let process_transaction_batch_output = self.consumer.process_and_record_transactions(
⋮----
.accumulate(&transaction_counts, commit_transactions_result.is_ok());
let should_bank_still_be_processing_txs = bank.is_complete();
⋮----
info!(
⋮----
.map(|retryable_index| retryable_index.index)
.collect(),
⋮----
fn filter_pending_packets_from_pending_txs(
⋮----
Self::prepare_filter_for_pending_transactions(transactions.len(), pending_indexes);
let results = bank.check_transactions_with_forwarding_delay(
⋮----
fn prepare_filter_for_pending_transactions(
⋮----
let mut mask = vec![Err(TransactionError::BlockhashNotFound); transactions_len];
pending_tx_indexes.iter().for_each(|x| mask[*x] = Ok(()));
⋮----
fn filter_valid_transaction_indexes(valid_txs: &[TransactionCheckResult]) -> Vec<usize> {
⋮----
.iter()
.enumerate()
.filter_map(|(index, res)| res.as_ref().ok().map(|_| index))
.collect()
⋮----
fn extract_retryable(
⋮----
debug_assert!(retryable_vote_indices.is_sorted());
let mut retryable_vote_indices = retryable_vote_indices.into_iter().peekable();
⋮----
.filter_map(move |(i, packet)| {
(Some(&i) == retryable_vote_indices.peek()).then(|| {
retryable_vote_indices.next();
⋮----
fn consume_scan_should_process_packet(
⋮----
if !view.is_simple_vote_transaction() {
⋮----
debug_assert!(!matches!(view.version(), TransactionVersion::V0));
let Ok(view) = RuntimeTransactionView::try_new(view, None, bank.get_reserved_account_keys())
⋮----
if validate_account_locks(
view.account_keys(),
bank.get_transaction_account_lock_limit(),
⋮----
.is_err()
⋮----
if Consumer::check_fee_payer_unlocked(bank, &view, error_counters).is_err() {
⋮----
Some(view)
⋮----
fn has_reached_end_of_slot(reached_max_poh_height: bool, bank: &Bank) -> bool {
reached_max_poh_height || bank.is_complete()
⋮----
mod tests {
⋮----
fn to_runtime_transaction_view(packet: BytesPacket) -> RuntimeTransactionView {
⋮----
SanitizedTransactionView::try_new_sanitized(Arc::new(packet.buffer().to_vec()), false)
.unwrap();
⋮----
RuntimeTransactionView::try_new(tx, None, &HashSet::default()).unwrap()
⋮----
fn test_bank_prepare_filter_for_pending_transaction() {
assert_eq!(
⋮----
fn test_bank_filter_valid_transaction_indexes() {
⋮----
fn extract_retryable_one_all_retryable() {
⋮----
let mut packets = ArrayVec::from_iter([to_runtime_transaction_view(packet_from_slots(
vec![(1, 1)],
⋮----
let retryable_indices = vec![0];
let expected = *packets[0].message_hash();
⋮----
assert_eq!(extracted.next().unwrap().message_hash(), &expected);
assert!(extracted.next().is_none());
⋮----
fn extract_retryable_one_none_retryable() {
⋮----
let retryable_indices = vec![];
⋮----
fn extract_retryable_three_last_retryable() {
⋮----
packet_from_slots(vec![(5, 3)], &keypair_a, None),
packet_from_slots(vec![(6, 2)], &keypair_a, None),
packet_from_slots(vec![(7, 1)], &keypair_a, None),
⋮----
.map(to_runtime_transaction_view),
⋮----
let retryable_indices = vec![2];
let expected = *packets[2].message_hash();
⋮----
fn extract_retryable_three_first_last_retryable() {
⋮----
let retryable_indices = vec![0, 2];
let expected0 = *packets[0].message_hash();
let expected1 = *packets[2].message_hash();
⋮----
assert_eq!(extracted.next().unwrap().message_hash(), &expected0);
assert_eq!(extracted.next().unwrap().message_hash(), &expected1);
⋮----
fn test_has_reached_end_of_slot() {
let GenesisConfigInfo { genesis_config, .. } = create_slow_genesis_config(10_000);
⋮----
assert!(!has_reached_end_of_slot(false, &bank));
assert!(has_reached_end_of_slot(true, &bank));
bank.fill_bank_with_ticks_for_tests();
assert!(bank.is_complete());
assert!(has_reached_end_of_slot(false, &bank));

================
File: core/src/block_creation_loop/stats.rs
================
pub(crate) struct LoopMetrics {
⋮----
impl Default for LoopMetrics {
fn default() -> Self {
⋮----
impl LoopMetrics {
fn is_empty(&self) -> bool {
⋮----
+ self.bank_timeout_completion_elapsed_hist.entries()
⋮----
pub(crate) fn report(&mut self, report_interval: Duration) {
if self.is_empty() {
⋮----
if self.last_report.elapsed() > report_interval {
datapoint_info!(
⋮----
self.reset();
⋮----
fn reset(&mut self) {
⋮----
pub(crate) struct SlotMetrics {
⋮----
impl SlotMetrics {
pub(crate) fn report(&mut self) {
⋮----
pub(crate) fn reset(&mut self, slot: Slot) {

================
File: core/src/bundle_stage/bundle_account_locker.rs
================
pub enum BundleAccountLockerError {
⋮----
pub type BundleAccountLockerResult<T> = Result<T, BundleAccountLockerError>;
⋮----
pub struct BundleAccountLocks {
⋮----
impl BundleAccountLocks {
pub fn read_locks(&self) -> &HashMap<Pubkey, u64> {
⋮----
pub fn write_locks(&self) -> &HashMap<Pubkey, u64> {
⋮----
pub fn lock_accounts<'a>(
⋮----
*self.write_locks.entry(*acc).or_insert(0) += 1;
⋮----
*self.read_locks.entry(*acc).or_insert(0) += 1;
⋮----
pub fn unlock_accounts<'a>(
⋮----
let entry = self.write_locks.entry(*acc);
⋮----
*entry.get_mut() = entry.get().saturating_sub(1);
if *entry.get() == 0 {
entry.remove();
⋮----
let entry = self.read_locks.entry(*acc);
⋮----
pub struct BundleAccountLocker {
⋮----
impl BundleAccountLocker {
pub fn account_locks(&self) -> MutexGuard<'_, BundleAccountLocks> {
self.account_locks.lock().unwrap()
⋮----
/// Prepares a locked bundle and returns a LockedBundle containing locked accounts.
    /// When a LockedBundle is dropped, the accounts are automatically unlocked
⋮----
/// When a LockedBundle is dropped, the accounts are automatically unlocked
    pub fn lock_bundle<Tx: TransactionWithMeta>(
⋮----
pub fn lock_bundle<Tx: TransactionWithMeta>(
⋮----
.lock()
.unwrap()
.lock_accounts(transaction_locks);
Ok(())
⋮----
/// Unlocks bundle accounts. Note that LockedBundle::drop will auto-drop the bundle account locks
    pub fn unlock_bundle<Tx: TransactionWithMeta>(
⋮----
pub fn unlock_bundle<Tx: TransactionWithMeta>(
⋮----
.unlock_accounts(transaction_locks);
⋮----
/// Returns the read and write locks for this bundle
    /// Each lock type contains a HashMap which maps Pubkey to number of locks held
⋮----
/// Each lock type contains a HashMap which maps Pubkey to number of locks held
    fn get_transaction_locks<'a, Tx: TransactionWithMeta>(
⋮----
fn get_transaction_locks<'a, Tx: TransactionWithMeta>(
⋮----
.iter()
.filter_map(|tx| {
if validate_account_locks(
tx.account_keys(),
bank.get_transaction_account_lock_limit(),
⋮----
.is_ok()
⋮----
Some(TransactionAccountLocksIterator::new(tx).accounts_with_is_writable())
⋮----
if transaction_locks.len() != transactions.len() {
return Err(BundleAccountLockerError::LockingError);
⋮----
Ok(transaction_locks)
⋮----
mod tests {
⋮----
fn test_simple_lock_bundles() {
⋮----
} = create_genesis_config(2);
⋮----
let tx0 = VersionedTransaction::from(transfer(
⋮----
&kp0.pubkey(),
⋮----
genesis_config.hash(),
⋮----
let tx1 = VersionedTransaction::from(transfer(
⋮----
&kp1.pubkey(),
⋮----
.data(..)
⋮----
.to_vec(),
⋮----
.unwrap();
let tx0 = vec![tx0.take_transaction_for_scheduling().0];
⋮----
let tx1 = vec![tx1.take_transaction_for_scheduling().0];
bundle_account_locker.lock_bundle(&tx0, &bank).unwrap();
assert_eq!(
⋮----
bundle_account_locker.lock_bundle(&tx1, &bank).unwrap();
⋮----
bundle_account_locker.unlock_bundle(&tx0, &bank).unwrap();
⋮----
bundle_account_locker.unlock_bundle(&tx1, &bank).unwrap();
assert!(bundle_account_locker
⋮----
fn test_lock_bundles_duplicate_accounts() {

================
File: core/src/bundle_stage/bundle_consumer.rs
================
pub struct BundleConsumer {
⋮----
impl BundleConsumer {
⋮----
pub fn new(
⋮----
pub fn process_and_record_aged_transactions(
⋮----
.iter()
.zip(max_ages)
.map(|(tx, max_age)| {
if bank.epoch() != max_age.sanitized_epoch {
bank.check_reserved_keys(tx)?;
⋮----
if bank.slot() > max_age.alt_invalidation_slot {
⋮----
bank.load_addresses_from_ref(tx.message_address_table_lookups())?;
⋮----
Ok(())
⋮----
.collect_vec();
⋮----
bank.check_transactions(txs, &pre_results, MAX_PROCESSING_AGE, &mut error_counters);
⋮----
.enumerate()
.find(|(_, result)| result.is_err())
⋮----
let err = err.clone().unwrap_err();
⋮----
vec![
⋮----
commit_transactions_result[index] = CommitTransactionDetails::NotCommitted(err.clone());
⋮----
retryable_transaction_indexes: vec![],
commit_transactions_result: Ok(commit_transactions_result),
⋮----
self.process_and_record_transactions_with_pre_results(bank, txs, max_bundle_duration)
⋮----
fn process_and_record_transactions_with_pre_results(
⋮----
) = measure_us!(self.qos_service.select_and_accumulate_transaction_costs(
⋮----
.find(|(_, r)| r.is_err())
⋮----
let err = err.as_ref().unwrap_err().clone();
⋮----
QosService::remove_or_update_costs(transaction_qos_cost_results.iter(), None, bank);
⋮----
retryable_transaction_indexes: (0..txs.len())
.map(|index| RetryableIndex {
⋮----
.collect(),
commit_transactions_result: Ok(commit_transactions_results),
⋮----
let (batch, lock_us) = measure_us!(Self::try_lock_batch(bank, txs, max_bundle_duration));
if let Some(err) = batch.lock_results().iter().find(|x| x.is_err()) {
⋮----
(0..txs.len())
⋮----
.collect()
⋮----
vec![]
⋮----
commit_transactions_result: Ok(vec![
⋮----
let execute_and_commit_transactions_output = self.execute_and_commit_transactions_locked(
⋮----
let (_, unlock_us) = measure_us!(drop(batch));
⋮----
transaction_qos_cost_results.iter(),
commit_transactions_result.as_ref().ok(),
⋮----
self.qos_service.report_metrics(bank.slot());
debug!(
⋮----
fn try_lock_batch<'a, 'b, Tx>(
⋮----
while start.elapsed() < max_bundle_duration {
let batch = bank.prepare_sanitized_batch_relax_intrabatch_account_locks(txs);
⋮----
if err.as_ref().unwrap_err() == &TransactionError::AccountInUse {
sleep(Duration::from_millis(1));
⋮----
bank.prepare_sanitized_batch_relax_intrabatch_account_locks(txs)
⋮----
fn execute_and_commit_transactions_locked(
⋮----
let transaction_status_sender_enabled = self.committer.transaction_status_sender_enabled();
⋮----
.sanitized_transactions()
⋮----
.filter_map(|transaction| {
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(&bank.feature_set)
.ok()
.map(|limits| limits.compute_unit_price)
⋮----
.minmax();
⋮----
min_max.into_option().unwrap_or_default();
⋮----
let (load_and_execute_transactions_output, load_execute_us) = measure_us!(bank
⋮----
if processing_results.iter().any(|result| result.is_err()) {
⋮----
.map(|r| match r {
⋮----
Err(err) => CommitTransactionDetails::NotCommitted(err.clone()),
⋮----
.collect();
⋮----
attempted_processing_count: batch.sanitized_transactions().len() as u64,
⋮----
.map(|processing_result| {
⋮----
.as_ref()
.map_or(0, |pr| pr.executed_units())
⋮----
.sum();
⋮----
.accumulate_actual_execute_cu(actual_executed_cu);
⋮----
.accumulate_actual_execute_time(actual_execute_time);
⋮----
attempted_processing_count: processing_results.len() as u64,
⋮----
measure_us!(processing_results
⋮----
let (freeze_lock, freeze_lock_us) = measure_us!(bank.freeze_lock());
⋮----
let (record_transactions_summary, record_us) = measure_us!(self
⋮----
processing_results_to_transactions_us: Saturating(
⋮----
retryable_transaction_indexes: (0..batch.sanitized_transactions().len())
⋮----
commit_transactions_result: Err(recorder_err),
⋮----
self.committer.commit_transactions(
⋮----
.into_iter()
.map(|processing_result| match processing_result {
Ok(_) => unreachable!("processed transaction count is 0"),
⋮----
drop(freeze_lock);
⋮----
debug_assert_eq!(
⋮----
commit_transactions_result: Ok(commit_transaction_statuses),
⋮----
mod tests {
⋮----
fn sanitize_transactions(
⋮----
txs.into_iter()
.map(RuntimeTransaction::from_transaction_for_tests)
⋮----
fn test_try_lock_timeout() {
let bank = Bank::new_for_tests(&create_genesis_config(100).0);
⋮----
let tx = transfer(&kp1, &new_rand(), 1, bank.last_blockhash());
let tx = vec![RuntimeTransaction::from_transaction_for_tests(tx)];
let batch_1 = bank.prepare_sanitized_batch(&tx);
assert!(batch_1.lock_results().iter().all(|x| x.is_ok()));
let tx2 = transfer(&kp1, &new_rand(), 1, bank.last_blockhash());
let tx2 = vec![RuntimeTransaction::from_transaction_for_tests(tx2)];
⋮----
assert_eq!(
⋮----
drop(batch_1);
⋮----
assert!(batch_2.lock_results().iter().all(|x| x.is_ok()));
⋮----
fn test_single_tx_ok_bundle_committed() {
⋮----
} = create_genesis_config_with_leader(
⋮----
bootstrap_validator_stake_lamports(),
⋮----
let transactions = sanitize_transactions(vec![transfer(
⋮----
let (record_sender, mut record_receiver) = record_channels(false);
⋮----
record_receiver.restart(bank.bank_id());
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
} = consumer.process_and_record_aged_transactions(
⋮----
assert_eq!(cost_model_throttled_transactions_count, 0);
assert!(commit_transactions_result.is_ok());
let commit_transactions_result = commit_transactions_result.unwrap();
assert_eq!(commit_transactions_result.len(), 1);
assert_matches!(
⋮----
fn test_single_tx_bad_not_committed() {
⋮----
assert_eq!(bank.read_cost_tracker().unwrap().block_cost(), 0);
⋮----
fn test_multi_tx_bundle_last_tx_bad_not_committed() {
⋮----
let transactions = sanitize_transactions(vec![
⋮----
assert_eq!(commit_transactions_result.len(), 4);
⋮----
fn test_multi_bundle_seed_fee_payer_ok() {
⋮----
assert_eq!(commit_transactions_result.len(), 3);
⋮----
fn test_tx_compute_reservation_exceeds_drops_bundle() {
⋮----
.map(|tx| CostModel::calculate_cost(tx, &bank.feature_set))
⋮----
let total_cost: u64 = txs_costs.iter().map(|cost| cost.sum()).sum();
bank.write_cost_tracker()
.unwrap()
.set_limits(u64::MAX, total_cost - 1, u64::MAX);
⋮----
assert_eq!(cost_model_throttled_transactions_count, 1);
⋮----
fn test_transaction_already_processed_fails() {
⋮----
fn test_bundle_account_in_use_rolls_back_qos() {
⋮----
let tx = sanitize_transactions(vec![transfer(
⋮----
let batch = bank.prepare_sanitized_batch(&tx);
⋮----
drop(batch);
⋮----
fn test_non_fee_payer_failure_tx_reverts() {
⋮----
assert_eq!(commit_transactions_result.len(), 2);

================
File: core/src/bundle_stage/bundle_packet_deserializer.rs
================
pub struct BundlePacketDeserializer;
impl BundlePacketDeserializer {
pub fn try_handle_packet(
⋮----
let (view, deactivation_slot) = translate_to_runtime_view(
⋮----
if validate_account_locks(
view.account_keys(),
root_bank.get_transaction_account_lock_limit(),
⋮----
.is_err()
⋮----
return Err(PacketHandlingError::LockValidation);
⋮----
.account_keys()
.iter()
.any(|account| blacklisted_accounts.contains(account))
⋮----
return Err(PacketHandlingError::BlacklistedAccount);
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(&working_bank.feature_set)
⋮----
return Err(PacketHandlingError::ComputeBudget);
⋮----
let max_age = calculate_max_age(root_bank.epoch(), deactivation_slot, root_bank.slot());
⋮----
let (priority, cost) = calculate_priority_and_cost(&view, &fee_budget_limits, working_bank);
Ok(TransactionState::new(view, max_age, priority, cost))

================
File: core/src/bundle_stage/bundle_stage_leader_metrics.rs
================
pub struct BundleStageLeaderMetrics {
⋮----
pub(crate) enum MetricsTrackerAction {
⋮----
impl BundleStageLeaderMetrics {
pub fn new(id: u32) -> Self {
⋮----
pub(crate) fn check_leader_slot_boundary(
⋮----
.check_leader_slot_boundary(bank);
⋮----
pub(crate) fn apply_action(
⋮----
.apply_action(banking_stage_metrics_action);
⋮----
.apply_action(bundle_stage_metrics_action)
⋮----
pub fn leader_slot_metrics_tracker(&mut self) -> &mut LeaderSlotMetricsTracker {
⋮----
pub fn bundle_stage_metrics_tracker(&mut self) -> &mut BundleStageStatsMetricsTracker {
⋮----
pub struct BundleStageStatsMetricsTracker {
⋮----
impl BundleStageStatsMetricsTracker {
⋮----
match (self.bundle_stage_metrics.as_mut(), bank) {
⋮----
MetricsTrackerAction::NewTracker(Some(BundleStageStats::new(self.id, bank.slot())))
⋮----
if bundle_stage_metrics.slot != bank.slot() {
MetricsTrackerAction::ReportAndNewTracker(Some(BundleStageStats::new(
⋮----
bank.slot(),
⋮----
pub(crate) fn apply_action(&mut self, action: MetricsTrackerAction) -> Option<Slot> {
⋮----
if let Some(bundle_stage_metrics) = self.bundle_stage_metrics.as_mut() {
bundle_stage_metrics.report();
reported_slot = bundle_stage_metrics.reported_slot();
⋮----
self.bundle_stage_metrics.as_ref().unwrap().reported_slot()
⋮----
pub(crate) fn increment_sanitize_transaction_result(
⋮----
.add_assign(Saturating(1));
⋮----
pub fn increment_bundle_execution_result(&mut self, result: &Result<(), BundleExecutionError>) {
⋮----
bundle_stage_metrics.bad_argument.add_assign(Saturating(1));
⋮----
pub(crate) fn increment_sanitize_bundle_elapsed_us(&mut self, count: u64) {
⋮----
.add_assign(Saturating(count));
⋮----
pub(crate) fn increment_locked_bundle_elapsed_us(&mut self, count: u64) {
⋮----
pub(crate) fn increment_num_init_tip_account_errors(&mut self, count: u64) {
⋮----
pub(crate) fn increment_num_init_tip_account_ok(&mut self, count: u64) {
⋮----
pub(crate) fn increment_num_change_tip_receiver_errors(&mut self, count: u64) {
⋮----
pub(crate) fn increment_num_change_tip_receiver_ok(&mut self, count: u64) {
⋮----
pub(crate) fn increment_change_tip_receiver_elapsed_us(&mut self, count: u64) {
⋮----
pub(crate) fn increment_num_execution_retries(&mut self, count: Saturating<u64>) {
⋮----
bundle_stage_metrics.num_execution_retries.add_assign(count);
⋮----
pub(crate) fn increment_execute_locked_bundles_elapsed_us(&mut self, count: u64) {
⋮----
pub struct BundleStageStats {
⋮----
impl BundleStageStats {
pub fn new(id: u32, slot: Slot) -> BundleStageStats {
⋮----
fn reported_slot(&self) -> Option<Slot> {
⋮----
Some(self.slot)
⋮----
pub fn report(&mut self) {
⋮----
datapoint_info!(

================
File: core/src/bundle_stage/bundle_storage.rs
================
pub enum BundleStorageError {
⋮----
struct BundleTransactionId {
⋮----
pub struct BundleStorageEntry {
⋮----
pub struct BundleStorage {
⋮----
impl BundleStorage {
⋮----
pub fn with_capacity(transaction_capacity: usize) -> Self {
⋮----
pub fn unprocessed_bundles_len(&self) -> usize {
self.unprocessed_bundles.len()
⋮----
pub fn cost_model_buffered_bundles_len(&self) -> usize {
self.cost_model_buffered_bundles.len()
⋮----
pub fn num_packets_buffered(&self) -> usize {
self.transaction_view_state_container.buffer_size()
⋮----
pub fn retry_bundle(&mut self, bundle: BundleStorageEntry) {
⋮----
.iter()
.zip(bundle.transactions.into_iter())
⋮----
.get_mut_transaction_state(*container_id)
.unwrap()
.retry_transaction(transaction);
⋮----
.push_back(BundleTransactionId {
⋮----
pub fn destroy_bundle(&mut self, bundle: BundleStorageEntry) {
for container_id in bundle.container_ids.into_iter() {
⋮----
.remove_by_id(container_id);
⋮----
pub fn pop_bundle(&mut self, slot: Slot) -> Option<BundleStorageEntry> {
⋮----
while let Some(bundle) = self.cost_model_buffered_bundles.pop_back() {
self.unprocessed_bundles.push_front(bundle);
⋮----
let bundle = self.unprocessed_bundles.pop_front()?;
⋮----
.map(|id| {
⋮----
.get_mut_transaction_state(*id)
⋮----
.take_transaction_for_scheduling()
⋮----
.collect();
Some(BundleStorageEntry {
⋮----
pub fn insert_bundle(
⋮----
let batch = bundle.take();
if batch.is_empty() {
return Err(BundleStorageError::EmptyBatch);
⋮----
if batch.len() > Self::MAX_PACKETS_PER_BUNDLE {
return Err(BundleStorageError::BundleTooLarge);
⋮----
.enumerate()
.find_map(|(idx, packet)| packet.meta().discard().then_some(idx))
⋮----
return Err(BundleStorageError::PacketMarkedDiscard(idx));
⋮----
.buffer_size()
.saturating_add(batch.len())
⋮----
return Err(BundleStorageError::ContainerFull);
⋮----
let mut container_ids: Vec<usize> = Vec::with_capacity(batch.len());
let mut maybe_error = Ok(());
for (idx, packet) in batch.iter().enumerate() {
let packet_data = packet.data(..).unwrap();
⋮----
.try_insert_map_only_with_data(packet_data, |bytes| {
⋮----
.is_active(&agave_feature_set::static_instruction_limit::id()),
working_bank.get_transaction_account_lock_limit(),
⋮----
Ok(state) => Ok(state),
⋮----
maybe_error = Err(e);
Err(())
⋮----
container_ids.push(container_id);
⋮----
for container_id in container_ids.iter() {
⋮----
.remove_by_id(*container_id);
⋮----
return Err(BundleStorageError::PacketFilterError((
maybe_error.unwrap_err(),
⋮----
let is_duplicate_hashes = self.does_contain_duplicate_hashes(&container_ids);
⋮----
return Err(BundleStorageError::DuplicateTransaction);
⋮----
.push_back(BundleTransactionId { container_ids });
Ok(())
⋮----
fn does_contain_duplicate_hashes(&self, container_ids: &[usize]) -> bool {
⋮----
.get_transaction(*container_id)
⋮----
.message_hash();
if transaction_hashes.contains(&transaction_hash) {
⋮----
transaction_hashes.push(transaction_hash);
⋮----
pub fn clear(&mut self) {
for bundle in self.unprocessed_bundles.drain(..) {
for id in bundle.container_ids.iter() {
self.transaction_view_state_container.remove_by_id(*id);
⋮----
for bundle in self.cost_model_buffered_bundles.drain(..) {
⋮----
mod tests {
⋮----
pub fn test_tx() -> Transaction {
⋮----
let pubkey1 = keypair1.pubkey();
⋮----
fn test_bundle_too_large() {
⋮----
.map(|_| BytesPacket::from_data(None, test_tx()).unwrap())
⋮----
let result = bundle_storage.insert_bundle(bundle, &bank, &bank, &HashSet::new());
assert_matches!(result, Err(BundleStorageError::BundleTooLarge));
assert_eq!(bundle_storage.unprocessed_bundles.len(), 0);
assert_eq!(bundle_storage.cost_model_buffered_bundles.len(), 0);
assert!(bundle_storage.transaction_view_state_container.is_empty());
⋮----
fn test_bundle_marked_discard() {
⋮----
let packet_1 = BytesPacket::from_data(None, test_tx()).unwrap();
let mut packet_2 = BytesPacket::from_data(None, test_tx()).unwrap();
packet_2.meta_mut().set_discard(true);
let bundle = VerifiedPacketBundle::new(PacketBatch::from(vec![packet_1, packet_2]));
⋮----
assert_matches!(result, Err(BundleStorageError::PacketMarkedDiscard(1)));
⋮----
fn test_bundle_storage_exceeds_capacity() {
⋮----
let packet = BytesPacket::from_data(None, test_tx()).unwrap();
let bundle = VerifiedPacketBundle::new(PacketBatch::from(vec![packet]));
⋮----
.insert_bundle(bundle, &bank, &bank, &HashSet::new())
.unwrap();
assert_eq!(bundle_storage.unprocessed_bundles.len(), i + 1);
assert_eq!(
⋮----
assert_eq!(result, Err(BundleStorageError::ContainerFull));
assert_eq!(bundle_storage.unprocessed_bundles.len(), 10);
⋮----
fn test_bundle_empty() {
⋮----
let bundle = VerifiedPacketBundle::new(PacketBatch::from(vec![]));
⋮----
assert_matches!(result, Err(BundleStorageError::EmptyBatch));
⋮----
fn test_bundle_duplicate_hashes() {
⋮----
let packet_2 = packet_1.clone();
⋮----
assert_matches!(result, Err(BundleStorageError::DuplicateTransaction));
assert!(
⋮----
assert!(bundle_storage.unprocessed_bundles.is_empty());
assert!(bundle_storage.cost_model_buffered_bundles.is_empty());
⋮----
fn test_retry_bundle() {
⋮----
let packet_2 = BytesPacket::from_data(None, test_tx()).unwrap();
⋮----
assert!(result.is_ok());
let bundle_storage_entry = bundle_storage.pop_bundle(bank.slot()).unwrap();
bundle_storage.retry_bundle(bundle_storage_entry);
assert!(bundle_storage.pop_bundle(bank.slot()).is_none());
⋮----
assert_eq!(bundle_storage.cost_model_buffered_bundles.len(), 1);
⋮----
bundle_storage.pop_bundle(bank.slot() + 1).unwrap();
⋮----
fn test_bundle_blacklisted_account() {
⋮----
let tx = test_tx();
let pubkey = tx.message().account_keys[0];
⋮----
let packet = BytesPacket::from_data(None, tx).unwrap();
⋮----
let result = bundle_storage.insert_bundle(bundle, &bank, &bank, &blacklisted_accounts);
assert_matches!(
⋮----
fn test_retry_bundle_ordering_preserved() {
⋮----
let tx_1 = test_tx();
let tx_2 = test_tx();
let tx_3 = test_tx();
let tx_4 = test_tx();
let packet_batch_1 = VerifiedPacketBundle::new(PacketBatch::from(vec![
⋮----
let packet_batch_2 = VerifiedPacketBundle::new(PacketBatch::from(vec![
⋮----
let packet_batch_3 = VerifiedPacketBundle::new(PacketBatch::from(vec![
⋮----
let packet_batch_4 = VerifiedPacketBundle::new(PacketBatch::from(vec![
⋮----
.insert_bundle(packet_batch_1, &bank, &bank, &HashSet::new())
⋮----
.insert_bundle(packet_batch_2, &bank, &bank, &HashSet::new())
⋮----
.insert_bundle(packet_batch_3, &bank, &bank, &HashSet::new())
⋮----
.insert_bundle(packet_batch_4, &bank, &bank, &HashSet::new())
⋮----
let bundle_storage_entry_1 = bundle_storage.pop_bundle(bank.slot()).unwrap();
⋮----
let bundle_storage_entry_2 = bundle_storage.pop_bundle(bank.slot()).unwrap();
⋮----
bundle_storage.retry_bundle(bundle_storage_entry_1);
bundle_storage.destroy_bundle(bundle_storage_entry_2);
let bundle_storage_entry_1 = bundle_storage.pop_bundle(bank.slot() + 1).unwrap();
⋮----
let bundle_storage_entry_3 = bundle_storage.pop_bundle(bank.slot() + 1).unwrap();
⋮----
let bundle_storage_entry_4 = bundle_storage.pop_bundle(bank.slot() + 1).unwrap();
⋮----
fn test_destroy_bundle() {
⋮----
bundle_storage.destroy_bundle(bundle_storage_entry_1);
⋮----
fn test_clear() {
⋮----
bundle_storage.clear();

================
File: core/src/cluster_slots_service/cluster_slots.rs
================
type PubkeyHasherBuilder = RandomState;
pub(crate) type ValidatorStakesMap = HashMap<Pubkey, Stake, PubkeyHasherBuilder>;
struct EpochStakeInfo {
⋮----
fn from(stakes: &VersionedEpochStakes) -> Self {
⋮----
.node_id_to_vote_accounts()
.iter()
.map(|(k, v)| (*k, v.total_stake)),
⋮----
Self::new(validator_stakes, stakes.total_stake())
⋮----
impl EpochStakeInfo {
fn new(validator_stakes: HashMap<Pubkey, Stake>, total_stake: Stake) -> Self {
⋮----
.keys()
.enumerate()
.map(|(v, &k)| (k, v))
.collect();
⋮----
struct RootEpoch {
⋮----
pub struct ClusterSlots {
⋮----
struct RowContent {
⋮----
impl ClusterSlots {
pub fn new(root_bank: &Bank, cluster_info: &ClusterInfo) -> Self {
⋮----
cluster_slots.update(root_bank, cluster_info);
⋮----
fn default() -> Self {
⋮----
pub fn default_for_tests() -> Self {
⋮----
pub(crate) fn lookup(&self, slot: Slot) -> Option<Arc<SlotSupporters>> {
let cluster_slots = self.cluster_slots.read().unwrap();
⋮----
if row.supporters.is_blank() {
⋮----
Some(row.supporters.clone())
⋮----
fn get_row_for_slot(slot: Slot, cluster_slots: &VecDeque<RowContent>) -> Option<&RowContent> {
let start = cluster_slots.front()?.slot;
⋮----
cluster_slots.get(idx as usize)
⋮----
fn get_row_for_slot_mut(
⋮----
cluster_slots.get_mut(idx as usize)
⋮----
pub(crate) fn update(&self, root_bank: &Bank, cluster_info: &ClusterInfo) {
let root_slot = root_bank.slot();
let current_slot = self.get_current_slot();
⋮----
error!("Invalid update call to ClusterSlots, can not roll time backwards!");
⋮----
let root_epoch = root_bank.epoch();
if self.need_to_update_epoch(root_epoch) {
self.update_epoch_info(root_bank);
⋮----
let mut cursor = self.cursor.lock().unwrap();
cluster_info.get_epoch_slots(&mut cursor)
⋮----
self.update_internal(root_slot, epoch_slots);
self.maybe_report_cluster_slots_perf_stats();
⋮----
fn need_to_update_epoch(&self, root_epoch: Epoch) -> bool {
let rg = self.root_epoch.read().unwrap();
let my_epoch = rg.as_ref().map(|v| v.number);
Some(root_epoch) != my_epoch
⋮----
fn update_epoch_info(&self, root_bank: &Bank) {
⋮----
info!("Updating epoch_metadata for epoch {root_epoch}");
let epoch_stakes_map = root_bank.epoch_stakes_map();
⋮----
let mut epoch_metadata = self.epoch_metadata.write().unwrap();
⋮----
if let Some(my_epoch) = self.get_epoch_for_slot(self.get_current_slot()) {
info!("Evicting epoch_metadata for epoch {my_epoch}");
epoch_metadata.remove(&my_epoch);
⋮----
epoch_metadata.insert(
⋮----
*self.root_epoch.write().unwrap() = Some(RootEpoch {
schedule: root_bank.epoch_schedule().clone(),
⋮----
let next_epoch = root_epoch.wrapping_add(1);
⋮----
.epoch_schedule()
.get_first_slot_in_epoch(next_epoch);
let mut cluster_slots = self.cluster_slots.write().unwrap();
⋮----
next_epoch_info.pubkey_to_index.clone(),
⋮----
first_slot = first_slot.wrapping_add(1);
⋮----
warn!("Finalized init for {patched} slots in epoch {next_epoch}");
⋮----
epoch_metadata.insert(next_epoch, next_epoch_info);
⋮----
pub(crate) fn fake_epoch_info_for_tests(&self, validator_stakes: ValidatorStakesMap) {
assert!(
⋮----
let total_stake = validator_stakes.values().sum();
⋮----
epoch_metadata.insert(0, EpochStakeInfo::new(validator_stakes, total_stake));
⋮----
fn roll_cluster_slots(&self, root: Slot) -> Range<Slot> {
let slot_range = (root + 1)..root.saturating_add(CLUSTER_SLOTS_TRIM_SIZE as u64 + 1);
let current_slot = self.current_slot.load(Ordering::Relaxed);
⋮----
self.metric_write_locks.fetch_add(1, Ordering::Relaxed);
let epoch_metadata = self.epoch_metadata.read().unwrap();
if cluster_slots.is_empty() {
info!("Init cluster_slots at range {slot_range:?}");
for slot in slot_range.clone() {
⋮----
.get_epoch_for_slot(slot)
.expect("Epoch should be defined for all slots");
let supporters = if let Some(epoch_data) = epoch_metadata.get(&epoch) {
SlotSupporters::new(epoch_data.total_stake, epoch_data.pubkey_to_index.clone())
⋮----
.get_epoch_for_slot(current_slot)
.expect("Epochs should be defined")
⋮----
panic!(
⋮----
self.metric_allocations.fetch_add(1, Ordering::Relaxed);
cluster_slots.push_back(RowContent {
⋮----
.front()
.expect("After initialization the ring buffer can not be empty");
⋮----
let RowContent { supporters, .. } = cluster_slots.pop_front().unwrap();
let slot = cluster_slots.back().unwrap().slot + 1;
⋮----
.expect("Epoch should be defined for all slots in the window");
let Some(stake_info) = epoch_metadata.get(&epoch) else {
warn!(
⋮----
supporters.recycle(stake_info.total_stake, &stake_info.pubkey_to_index)
⋮----
SlotSupporters::new(stake_info.total_stake, stake_info.pubkey_to_index.clone())
⋮----
debug_assert!(
⋮----
self.current_slot.store(slot_range.start, Ordering::Relaxed);
⋮----
fn update_internal(&self, root: Slot, epoch_slots_list: Vec<EpochSlots>) {
let slot_range = self.roll_cluster_slots(root);
⋮----
let Some(first_slot) = epoch_slots.first_slot() else {
⋮----
let Some(epoch) = self.get_epoch_for_slot(first_slot) else {
⋮----
let Some(epoch_meta) = epoch_metadata.get(&epoch) else {
⋮----
let Some(&sender_stake) = epoch_meta.validator_stakes.get(&epoch_slots.from) else {
⋮----
.to_slots(root)
.filter(|slot| slot_range.contains(slot));
⋮----
} = Self::get_row_for_slot(slot, &cluster_slots).unwrap();
debug_assert_eq!(*s, slot, "Fetched slot does not match expected value!");
if map.is_frozen() {
⋮----
.set_support_by_pubkey(&epoch_slots.from, sender_stake)
.is_err()
⋮----
error!("Unexpected pubkey {} for slot {}!", &epoch_slots.from, slot);
⋮----
fn maybe_report_cluster_slots_perf_stats(&self) {
if self.metrics_last_report.should_update(10_000) {
let write_locks = self.metric_write_locks.swap(0, Ordering::Relaxed);
let allocations = self.metric_allocations.swap(0, Ordering::Relaxed);
⋮----
.fold((0, 0, 0), |(s, f, b), RowContent { supporters, .. }| {
⋮----
s + supporters.memory_usage(),
f + supporters.is_frozen() as usize,
b + supporters.is_blank() as usize,
⋮----
datapoint_info!(
⋮----
fn get_current_slot(&self) -> Slot {
self.current_slot.load(Ordering::Relaxed)
⋮----
fn with_root_epoch<T>(&self, closure: impl FnOnce(&RootEpoch) -> T) -> Option<T> {
⋮----
rg.as_ref().map(closure)
⋮----
fn get_epoch_for_slot(&self, slot: Slot) -> Option<u64> {
self.with_root_epoch(|b| b.schedule.get_epoch_and_slot_index(slot).0)
⋮----
pub(crate) fn insert_node_id(&self, slot: Slot, node_id: Pubkey) {
⋮----
epoch_slot.fill(&[slot], 0);
let current_root = self.current_slot.load(Ordering::Relaxed);
self.update_internal(current_root, vec![epoch_slot]);
⋮----
pub(crate) fn compute_weights(&self, slot: Slot, repair_peers: &[ContactInfo]) -> Vec<u64> {
if repair_peers.is_empty() {
return vec![];
⋮----
let failsafe = std::iter::repeat_n(1, repair_peers.len());
let Some(epoch) = self.get_epoch_for_slot(slot) else {
error!("No epoch info for slot {slot}");
⋮----
let Some(stakeinfo) = epoch_metadata.get(&epoch) else {
error!("No epoch_metadata record for epoch {epoch}");
⋮----
let validator_stakes = stakeinfo.validator_stakes.as_ref();
⋮----
.map(|peer| {
⋮----
.get(peer.pubkey())
.cloned()
.unwrap_or(1)
.max(1)
⋮----
.collect()
⋮----
let Some(slot_peers) = self.lookup(slot) else {
⋮----
.map(|peer| slot_peers.get_support_by_pubkey(peer.pubkey()).unwrap_or(0))
.zip(stakes)
.map(|(a, b)| (a / 2 + b / 2).max(1u64))
⋮----
pub(crate) fn compute_weights_exclude_nonfrozen(
⋮----
return (vec![], vec![]);
⋮----
let mut weights = Vec::with_capacity(repair_peers.len());
let mut indices = Vec::with_capacity(repair_peers.len());
for (index, peer) in repair_peers.iter().enumerate() {
if let Some(stake) = slot_peers.get_support_by_pubkey(peer.pubkey()) {
⋮----
weights.push(stake.max(1));
indices.push(index);
⋮----
mod tests {
⋮----
fn test_default() {
⋮----
assert!(cs.cluster_slots.read().unwrap().is_empty());
⋮----
fn test_roll_cluster_slots() {
⋮----
assert_eq!(
⋮----
cs.fake_epoch_info_for_tests(validator_stakes);
cs.roll_cluster_slots(0);
⋮----
let rg = cs.cluster_slots.read().unwrap();
⋮----
assert_eq!(rg.front().unwrap().slot, 1, "first slot should be root + 1");
⋮----
cs.roll_cluster_slots(1);
⋮----
assert_eq!(rg.front().unwrap().slot, 2, "first slot should be root + 1");
⋮----
let allocs = cs.metric_allocations.load(Ordering::Relaxed);
cs.roll_cluster_slots(trimsize);
⋮----
let allocs = cs.metric_allocations.load(Ordering::Relaxed) - allocs;
assert_eq!(allocs, 0, "No need to allocate when rolling ringbuf");
⋮----
fn fake_stakes() -> (Pubkey, Pubkey, ValidatorStakesMap) {
⋮----
fn test_roll_cluster_slots_backwards() {
⋮----
let (_, _, validator_stakes) = fake_stakes();
⋮----
cs.roll_cluster_slots(10);
cs.roll_cluster_slots(5);
⋮----
fn test_update_empty() {
⋮----
let (pk1, _, validator_stakes) = fake_stakes();
⋮----
cs.update_internal(0, vec![epoch_slot]);
assert!(cs.lookup(0).is_none());
⋮----
fn test_update_rooted() {
⋮----
epoch_slot.fill(&[0], 0);
⋮----
fn test_update_multiple_slots() {
⋮----
let (pk1, pk2, validator_stakes) = fake_stakes();
⋮----
epoch_slot1.fill(&[2, 4, 5], 0);
⋮----
epoch_slot2.fill(&[1, 3, 5], 1);
cs.update_internal(0, vec![epoch_slot1, epoch_slot2]);
⋮----
assert!(cs.lookup(1).is_some(), "slot 1 should be supported");
⋮----
let map = cs.lookup(5).unwrap();
⋮----
fn test_compute_weights_failsafes() {
⋮----
assert_eq!(cs.compute_weights(0, &[ci]), vec![1]);
⋮----
fn test_best_peer_2() {
⋮----
map.insert(pk1, 1000);
map.insert(pk2, 10);
map.insert(Pubkey::new_unique(), u64::MAX / 2);
cs.fake_epoch_info_for_tests(map);
⋮----
epoch_slot1.fill(&[1, 2, 3, 4], 0);
⋮----
epoch_slot2.fill(&[1, 2, 3, 4], 0);
cs.update_internal(1, vec![epoch_slot1, epoch_slot2]);
⋮----
fn test_best_peer_3() {
⋮----
[(pk1, 42), (pk_other, u64::MAX / 2)].into_iter().collect();
⋮----
epoch_slot.fill(&[1, 2, 3, 4], 0);
⋮----
fn test_best_completed_slot_peer() {
⋮----
.take(2)
⋮----
let (w, i) = cs.compute_weights_exclude_nonfrozen(slot, &contact_infos);
assert!(w.is_empty());
assert!(i.is_empty());
⋮----
(*contact_infos[0].pubkey(), 42),
(*contact_infos[1].pubkey(), u64::MAX / 2),
⋮----
.into_iter()
⋮----
cs.insert_node_id(slot, *contact_infos[0].pubkey());
⋮----
assert_eq!(w, [42]);
assert_eq!(i, [0]);
⋮----
fn test_update_new_staked_slot() {
⋮----
epoch_slot.fill(&[1], 0);
⋮----
assert!(cs.lookup(1).is_some(), "slot 1 should have records");

================
File: core/src/cluster_slots_service/slot_supporters.rs
================
type PubkeyHasherBuilder = RandomState;
pub(crate) type IndexMap =
⋮----
pub struct SlotSupporters {
⋮----
fn repeat_atomic_u64(count: usize) -> impl Iterator<Item = AtomicU64> {
std::iter::repeat_with(|| AtomicU64::new(0)).take(count)
⋮----
impl SlotSupporters {
pub(crate) fn memory_usage(&self) -> usize {
self.supporting_stakes.capacity() * std::mem::size_of::<AtomicU64>()
⋮----
pub(crate) fn set_support_by_index(&self, index: usize, stake: Stake) {
let old = self.supporting_stakes[index].swap(stake, Ordering::Relaxed);
⋮----
self.total_support.fetch_add(stake - old, Ordering::Relaxed);
⋮----
pub(crate) fn total_support(&self) -> Stake {
⋮----
.load(std::sync::atomic::Ordering::Relaxed)
⋮----
pub(crate) fn total_stake(&self) -> Stake {
⋮----
pub(crate) fn is_frozen(&self) -> bool {
let slot_weight_f64 = self.total_support() as f64;
⋮----
pub(crate) fn set_support_by_pubkey(&self, pubkey: &Pubkey, stake: Stake) -> Result<(), ()> {
let Some(idx) = self.pubkey_to_index_map.get(pubkey) else {
return Err(());
⋮----
self.set_support_by_index(*idx, stake);
Ok(())
⋮----
pub(crate) fn get_support_by_index(&self, index: usize) -> Option<Stake> {
Some(self.supporting_stakes.get(index)?.load(Ordering::Relaxed))
⋮----
pub(crate) fn get_support_by_pubkey(&self, key: &Pubkey) -> Option<Stake> {
let index = self.pubkey_to_index_map.get(key)?;
self.get_support_by_index(*index)
⋮----
pub(crate) fn new(total_stake: Stake, index_map: Arc<IndexMap>) -> Self {
⋮----
supporting_stakes: Vec::from_iter(repeat_atomic_u64(index_map.len())),
⋮----
pub(crate) fn new_blank() -> Self {
⋮----
supporting_stakes: vec![],
⋮----
pub(crate) fn is_blank(&self) -> bool {
⋮----
pub(crate) fn recycle(mut self, total_stake: Stake, index_map: &Arc<IndexMap>) -> Self {
⋮----
self.total_support.store(0, Ordering::Relaxed);
⋮----
let old_len = self.supporting_stakes.len();
let new_len = index_map.len();
⋮----
self.supporting_stakes = Vec::from_iter(repeat_atomic_u64(new_len));
⋮----
self.supporting_stakes.truncate(new_len);
⋮----
.iter_mut()
.for_each(|v| v.store(0, Ordering::Relaxed));
if self.supporting_stakes.len() < new_len {
let num_missing = new_len - self.supporting_stakes.len();
⋮----
.extend(repeat_atomic_u64(num_missing));
⋮----
self.pubkey_to_index_map = index_map.clone();
⋮----
pub fn iter(&self) -> impl Iterator<Item = (&Pubkey, Stake)> {
⋮----
.iter()
.map(|(pk, &idx)| (pk, self.get_support_by_index(idx).unwrap()))
⋮----
pub fn keys(&self) -> impl Iterator<Item = &Pubkey> {
self.pubkey_to_index_map.iter().filter_map(|(pk, &idx)| {
if self.get_support_by_index(idx).unwrap() > 0 {
Some(pk)

================
File: core/src/consensus/fork_choice.rs
================
pub struct SelectVoteAndResetForkResult {
⋮----
struct CandidateVoteAndResetBanks<'a> {
// A bank that the validator will vote on given it passes all
// remaining vote checks
⋮----
pub trait ForkChoice {
⋮----
// Returns:
// 1) The heaviest overall bank
// 2) The heaviest bank on the same fork as the last vote (doesn't require a
⋮----
fn last_vote_able_to_land(
⋮----
let Some(last_voted_slot) = tower.last_voted_slot() else {
⋮----
progress.my_latest_landed_vote(heaviest_bank_on_same_voted_fork.slot())
⋮----
|| last_voted_slot >= heaviest_bank_on_same_voted_fork.slot()
⋮----
.is_in_slot_hashes_history(&last_voted_slot)
⋮----
fn recheck_fork_decision_failed_switch_threshold(
⋮----
if !last_vote_able_to_land(reset_bank, progress, tower) {
⋮----
info!(
⋮----
failure_reasons.push(HeaviestForkFailures::FailedSwitchThreshold(
⋮----
fn select_candidates_failed_switch<'a>(
⋮----
// If our last vote is unable to land (even through normal refresh), then we
// temporarily "super" refresh our vote to the tip of our last voted fork.
let final_switch_fork_decision = recheck_fork_decision_failed_switch_threshold(
heaviest_bank_on_same_voted_fork.map(|bank| bank.as_ref()),
⋮----
heaviest_bank.slot(),
⋮----
let candidate_vote_bank = if final_switch_fork_decision.can_vote() {
// We need to "super" refresh our vote to the tip of our last voted fork
// because our last vote is unable to land. This is inferred by
// initially determining we can't vote but then determining we can vote
⋮----
Some(heaviest_bank)
⋮----
fn select_candidates_failed_switch_duplicate_rollback<'a>(
⋮----
// If we can't switch and our last vote was on an unconfirmed, duplicate
let reset_bank = Some(heaviest_bank);
⋮----
fn select_candidate_vote_and_reset_banks<'a>(
⋮----
select_candidates_failed_switch(
⋮----
select_candidates_failed_switch_duplicate_rollback(
⋮----
candidate_vote_bank: Some(heaviest_bank),
reset_bank: Some(heaviest_bank),
⋮----
fn can_vote_on_candidate_bank(
⋮----
let fork_stats = progress.get_fork_stats(candidate_vote_bank_slot).unwrap();
⋮----
.get_propagated_stats(candidate_vote_bank_slot)
.unwrap();
⋮----
fork_stats.fork_weight(),
⋮----
failure_reasons.push(HeaviestForkFailures::LockedOut(candidate_vote_bank_slot));
⋮----
failure_reasons.push(HeaviestForkFailures::FailedThreshold(
⋮----
.get_leader_propagation_slot_must_exist(candidate_vote_bank_slot)
⋮----
failure_reasons.push(HeaviestForkFailures::NoPropagatedConfirmation(
⋮----
&& switch_fork_decision.can_vote()
⋮----
pub fn select_vote_and_reset_forks(
⋮----
let initial_switch_fork_decision: SwitchForkDecision = tower.check_switch_threshold(
⋮----
heaviest_bank.total_epoch_stake(),
⋮----
.epoch_vote_accounts(heaviest_bank.epoch())
.expect("Bank epoch vote accounts must contain entry for the bank's own epoch"),
⋮----
let mut failure_reasons = vec![];
⋮----
} = select_candidate_vote_and_reset_banks(
⋮----
reset_bank: reset_bank.cloned(),
⋮----
if can_vote_on_candidate_bank(
candidate_vote_bank.slot(),
⋮----
vote_bank: Some((candidate_vote_bank.clone(), switch_fork_decision)),
reset_bank: Some(candidate_vote_bank.clone()),

================
File: core/src/consensus/heaviest_subtree_fork_choice.rs
================
pub type ForkWeight = u64;
pub type SlotHashKey = (Slot, Hash);
type UpdateOperations = BTreeMap<(SlotHashKey, UpdateLabel), UpdateOperation>;
⋮----
enum UpdateLabel {
⋮----
pub trait GetSlotHash {
⋮----
impl GetSlotHash for SlotHashKey {
fn slot_hash(&self) -> SlotHashKey {
⋮----
impl GetSlotHash for Slot {
⋮----
enum UpdateOperation {
⋮----
impl UpdateOperation {
fn update_stake(&mut self, new_stake: u64) {
⋮----
Self::Aggregate => panic!("Should not get here"),
⋮----
Self::MarkValid(_slot) => panic!("Should not get here"),
Self::MarkInvalid(_slot) => panic!("Should not get here"),
⋮----
struct ForkInfo {
⋮----
impl ForkInfo {
fn is_unconfirmed_duplicate(&self, my_slot: Slot) -> bool {
⋮----
.map(|ancestor| ancestor == my_slot)
.unwrap_or(false)
⋮----
fn is_candidate(&self) -> bool {
self.latest_invalid_ancestor.is_none()
⋮----
fn is_duplicate_confirmed(&self) -> bool {
⋮----
fn set_duplicate_confirmed(&mut self) {
⋮----
fn update_with_newly_valid_ancestor(
⋮----
info!(
⋮----
fn update_with_newly_invalid_ancestor(
⋮----
assert!(!self.is_duplicate_confirmed);
⋮----
.map(|latest_invalid_ancestor| newly_invalid_ancestor > latest_invalid_ancestor)
.unwrap_or(true)
⋮----
self.latest_invalid_ancestor = Some(newly_invalid_ancestor);
⋮----
impl PartialEq for ForkInfo {
fn eq(&self, other: &Self) -> bool {
⋮----
pub struct HeaviestSubtreeForkChoice {
⋮----
impl PartialEq for HeaviestSubtreeForkChoice {
⋮----
impl PartialOrd for HeaviestSubtreeForkChoice {
fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
Some(self.cmp(other))
⋮----
impl Eq for HeaviestSubtreeForkChoice {}
⋮----
impl Ord for HeaviestSubtreeForkChoice {
fn cmp(&self, other: &Self) -> std::cmp::Ordering {
self.tree_root.cmp(&other.tree_root)
⋮----
impl HeaviestSubtreeForkChoice {
pub fn new(tree_root: SlotHashKey) -> Self {
⋮----
heaviest_subtree_fork_choice.add_new_leaf_slot(tree_root, None);
⋮----
pub fn new_from_frozen_banks(root: SlotHashKey, frozen_banks: &[Arc<Bank>]) -> Self {
⋮----
for bank in frozen_banks.iter() {
assert!(bank.is_frozen());
if bank.slot() > root.0 {
assert!(bank.slot() > prev_slot);
prev_slot = bank.slot();
let bank_hash = bank.hash();
assert_ne!(bank_hash, Hash::default());
let parent_bank_hash = bank.parent_hash();
assert_ne!(parent_bank_hash, Hash::default());
heaviest_subtree_fork_choice.add_new_leaf_slot(
(bank.slot(), bank_hash),
Some((bank.parent_slot(), parent_bank_hash)),
⋮----
pub fn new_from_bank_forks(bank_forks: Arc<RwLock<BankForks>>) -> Self {
⋮----
let bank_forks = bank_forks.read().unwrap();
⋮----
.frozen_banks()
.map(|(_slot, bank)| bank)
.collect();
frozen_banks.sort_by_key(|bank| bank.slot());
let root_bank = bank_forks.root_bank();
⋮----
Self::new_from_frozen_banks((root_bank.slot(), root_bank.hash()), &frozen_banks)
⋮----
pub fn new_from_tree<T: GetSlotHash>(forks: Tree<T>) -> Self {
let root = forks.root().data().slot_hash();
⋮----
while let Some(visit) = walk.get() {
let slot_hash = visit.node().data().slot_hash();
⋮----
.contains_key(&slot_hash)
⋮----
walk.forward();
⋮----
let parent_slot_hash = walk.get_parent().map(|n| n.data().slot_hash());
heaviest_subtree_fork_choice.add_new_leaf_slot(slot_hash, parent_slot_hash);
⋮----
pub fn contains_block(&self, key: &SlotHashKey) -> bool {
self.fork_infos.contains_key(key)
⋮----
pub fn best_slot(&self, key: &SlotHashKey) -> Option<SlotHashKey> {
⋮----
.get(key)
.map(|fork_info| fork_info.best_slot)
⋮----
pub fn deepest_slot(&self, key: &SlotHashKey) -> Option<SlotHashKey> {
⋮----
.map(|fork_info| fork_info.deepest_slot)
⋮----
pub fn best_overall_slot(&self) -> SlotHashKey {
self.best_slot(&self.tree_root).unwrap()
⋮----
pub fn deepest_overall_slot(&self) -> SlotHashKey {
self.deepest_slot(&self.tree_root).unwrap()
⋮----
pub fn stake_voted_subtree(&self, key: &SlotHashKey) -> Option<u64> {
⋮----
.map(|fork_info| fork_info.stake_voted_subtree)
⋮----
pub fn height(&self, key: &SlotHashKey) -> Option<usize> {
self.fork_infos.get(key).map(|fork_info| fork_info.height)
⋮----
pub fn tree_root(&self) -> SlotHashKey {
⋮----
pub fn max_by_weight(&self, slot1: SlotHashKey, slot2: SlotHashKey) -> std::cmp::Ordering {
let weight1 = self.stake_voted_subtree(&slot1).unwrap();
let weight2 = self.stake_voted_subtree(&slot2).unwrap();
⋮----
slot1.cmp(&slot2).reverse()
⋮----
weight1.cmp(&weight2)
⋮----
pub fn add_votes<'a, 'b>(
⋮----
// newly updated votes on a fork
⋮----
self.generate_update_operations(pubkey_votes, epoch_stakes, epoch_schedule);
self.process_update_operations(update_operations_batch);
self.best_overall_slot()
⋮----
pub fn is_empty(&self) -> bool {
self.fork_infos.is_empty()
⋮----
pub fn set_tree_root(&mut self, new_root: SlotHashKey) {
let remove_set = (&*self).subtree_diff(self.tree_root, new_root);
⋮----
.remove(&node_key)
.expect("Slots reachable from old root must exist in tree");
⋮----
let root_fork_info = self.fork_infos.get_mut(&new_root);
⋮----
.unwrap_or_else(|| panic!("New root: {new_root:?}, didn't exist in fork choice"))
⋮----
pub fn purge_prune(&mut self, new_root: SlotHashKey) -> (Vec<SlotHashKey>, Vec<Self>) {
let mut pruned_subtrees = vec![];
let mut purged_slots = vec![];
⋮----
let mut to_visit = vec![self.tree_root];
while let Some(cur_slot) = to_visit.pop() {
⋮----
tree_root = Some(new_root);
⋮----
.children(&cur_slot)
.expect("slot was discovered earlier, must exist")
⋮----
to_visit.push(*child);
⋮----
purged_slots.push(cur_slot);
⋮----
pruned_subtrees.push(self.split_off(&cur_slot));
⋮----
for slot in purged_slots.iter() {
⋮----
.remove(slot)
⋮----
.get_mut(&tree_root)
.expect("New tree_root must exist in fork_infos")
⋮----
pub fn add_root_parent(&mut self, root_parent: SlotHashKey) {
assert!(root_parent.0 < self.tree_root.0);
assert!(!self.fork_infos.contains_key(&root_parent));
⋮----
.get_mut(&self.tree_root)
.expect("entry for root must exist");
root_info.parent = Some(root_parent);
⋮----
self.fork_infos.insert(root_parent, root_parent_info);
⋮----
pub fn maybe_print_state(&mut self) {
if self.last_root_time.elapsed().as_secs() > MAX_ROOT_PRINT_SECONDS {
self.print_state();
⋮----
pub fn add_new_leaf_slot(&mut self, slot_hash_key: SlotHashKey, parent: Option<SlotHashKey>) {
if self.fork_infos.contains_key(&slot_hash_key) {
⋮----
parent.and_then(|parent| self.latest_invalid_ancestor(&parent));
⋮----
.entry(slot_hash_key)
.and_modify(|fork_info| fork_info.parent = parent)
.or_insert(ForkInfo {
⋮----
is_duplicate_confirmed: parent.is_none(),
⋮----
if parent.is_none() {
⋮----
let parent = parent.unwrap();
⋮----
.get_mut(&parent)
.unwrap()
⋮----
.insert(slot_hash_key);
self.propagate_new_leaf(&slot_hash_key, &parent);
⋮----
fn is_best_child(&self, maybe_best_child: &SlotHashKey) -> bool {
let maybe_best_child_weight = self.stake_voted_subtree(maybe_best_child).unwrap();
let parent = self.parent(maybe_best_child);
⋮----
for child in self.children(&parent.unwrap()).unwrap() {
⋮----
.stake_voted_subtree(child)
.expect("child must exist in `self.fork_infos`");
if !self.is_candidate(child).expect("child must exist in tree") {
⋮----
fn is_deepest_child(&self, maybe_deepest_child: &SlotHashKey) -> bool {
let maybe_deepest_child_weight = self.stake_voted_subtree(maybe_deepest_child).unwrap();
let maybe_deepest_child_height = self.height(maybe_deepest_child).unwrap();
let parent = self.parent(maybe_deepest_child);
⋮----
.height(child)
⋮----
child_height.cmp(&maybe_deepest_child_height),
child_weight.cmp(&maybe_deepest_child_weight),
child.cmp(maybe_deepest_child),
⋮----
pub fn all_slots_stake_voted_subtree(&self) -> impl Iterator<Item = (&SlotHashKey, u64)> {
⋮----
.iter()
.map(|(slot_hash, fork_info)| (slot_hash, fork_info.stake_voted_subtree))
⋮----
pub fn slots_iter(&self) -> impl Iterator<Item = Slot> + '_ {
self.fork_infos.iter().map(|((slot, _), _)| slot).copied()
⋮----
/// Split off the node at `slot_hash_key` and propagate the stake subtraction up to the root of the
    /// tree.
⋮----
/// tree.
    ///
⋮----
///
    /// Assumes that `slot_hash_key` is not the `tree_root`
⋮----
/// Assumes that `slot_hash_key` is not the `tree_root`
    /// Returns the subtree originating from `slot_hash_key`
⋮----
/// Returns the subtree originating from `slot_hash_key`
    pub fn split_off(&mut self, slot_hash_key: &SlotHashKey) -> Self {
⋮----
pub fn split_off(&mut self, slot_hash_key: &SlotHashKey) -> Self {
assert_ne!(self.tree_root, *slot_hash_key);
⋮----
.get_mut(slot_hash_key)
.expect("Slot hash key must exist in tree");
⋮----
node_to_split_at.clone(),
⋮----
.expect("Split node is not tree root"),
⋮----
// Insert aggregate operations up to the root
self.insert_aggregate_operations(&mut update_operations, *slot_hash_key);
// Remove child link so that this slot cannot be chosen as best or deepest
assert!(self
⋮----
// Aggregate
self.process_update_operations(update_operations);
// Remove node + all children and add to new tree
⋮----
let mut to_visit = vec![*slot_hash_key];
while let Some(current_node) = to_visit.pop() {
⋮----
.remove(&current_node)
.expect("Node must exist in tree");
to_visit.extend(current_fork_info.children.iter());
split_tree_fork_infos.insert(current_node, current_fork_info);
⋮----
// Remove link from parent
⋮----
.get_mut(&split_tree_root.parent.expect("Cannot split off from root"))
.expect("Parent must exist in fork infos");
parent_fork_info.children.remove(slot_hash_key);
// Update the root of the new tree with the proper info, now that we have finished
// aggregating
⋮----
split_tree_fork_infos.insert(*slot_hash_key, split_tree_root);
// Split off the relevant votes to the new tree
let mut split_tree_latest_votes = self.latest_votes.clone();
split_tree_latest_votes.retain(|_, node| split_tree_fork_infos.contains_key(node));
⋮----
.retain(|_, node| self.fork_infos.contains_key(node));
// Create a new tree from the split
⋮----
pub fn ancestors(&self, start_slot_hash_key: SlotHashKey) -> Vec<SlotHashKey> {
AncestorIterator::new(start_slot_hash_key, &self.fork_infos).collect()
⋮----
pub fn merge(
⋮----
assert!(self.fork_infos.contains_key(merge_leaf));
// Add all the nodes from `other` into our tree
⋮----
.map(|(slot_hash_key, fork_info)| {
(slot_hash_key, fork_info.parent.unwrap_or(*merge_leaf))
⋮----
other_slots_nodes.sort_by_key(|(slot_hash_key, _)| *slot_hash_key);
⋮----
self.add_new_leaf_slot(*slot_hash_key, Some(parent));
⋮----
// Add all votes, the outdated ones should be filtered out by
// self.add_votes()
self.add_votes(other.latest_votes.into_iter(), epoch_stakes, epoch_schedule);
⋮----
pub fn stake_voted_at(&self, slot: &SlotHashKey) -> Option<u64> {
⋮----
.get(slot)
.map(|fork_info| fork_info.stake_voted_at)
⋮----
pub fn latest_invalid_ancestor(&self, slot_hash_key: &SlotHashKey) -> Option<Slot> {
⋮----
.get(slot_hash_key)
.map(|fork_info| fork_info.latest_invalid_ancestor)
.unwrap_or(None)
⋮----
pub fn is_duplicate_confirmed(&self, slot_hash_key: &SlotHashKey) -> Option<bool> {
⋮----
.map(|fork_info| fork_info.is_duplicate_confirmed())
⋮----
/// Returns if the exact node with the specified key has been explicitly marked as a duplicate
    /// slot (doesn't count ancestors being marked as duplicate).
⋮----
/// slot (doesn't count ancestors being marked as duplicate).
    pub fn is_unconfirmed_duplicate(&self, slot_hash_key: &SlotHashKey) -> Option<bool> {
⋮----
pub fn is_unconfirmed_duplicate(&self, slot_hash_key: &SlotHashKey) -> Option<bool> {
⋮----
.map(|fork_info| fork_info.is_unconfirmed_duplicate(slot_hash_key.0))
⋮----
pub fn is_candidate(&self, slot_hash_key: &SlotHashKey) -> Option<bool> {
⋮----
.map(|fork_info| fork_info.is_candidate())
⋮----
pub fn is_strict_ancestor(
⋮----
let mut ancestor_iterator = self.ancestor_iterator(*node_key);
ancestor_iterator.any(|(ancestor_slot, ancestor_hash)| {
⋮----
fn propagate_new_leaf(
⋮----
.best_slot(parent_slot_hash_key)
.expect("parent must exist in self.fork_infos after its child leaf was created");
if self.is_best_child(slot_hash_key) {
let mut ancestor = Some(*parent_slot_hash_key);
⋮----
if ancestor.is_none() {
⋮----
let ancestor_fork_info = self.fork_infos.get_mut(&ancestor.unwrap()).unwrap();
⋮----
if !self.is_deepest_child(&current_child) {
⋮----
current_child = ancestor.unwrap();
⋮----
fn insert_aggregate_operations(
⋮----
self.do_insert_aggregate_operations_across_ancestors(
⋮----
fn do_insert_aggregate_operations_across_ancestors(
⋮----
for parent_slot_hash_key in self.ancestor_iterator(slot_hash_key) {
if !self.do_insert_aggregate_operation(
⋮----
fn do_insert_aggregate_operation(
⋮----
if update_operations.contains_key(&aggregate_label) {
⋮----
update_operations.insert(
⋮----
update_operations.insert(aggregate_label, UpdateOperation::Aggregate);
⋮----
fn ancestor_iterator(&self, start_slot_hash_key: SlotHashKey) -> AncestorIterator<'_> {
⋮----
fn aggregate_slot(&mut self, slot_hash_key: SlotHashKey) {
⋮----
if let Some(fork_info) = self.fork_infos.get(&slot_hash_key) {
⋮----
.get(child_key)
.expect("Child must exist in fork_info map");
⋮----
// Child forks that are not candidates still contribute to the weight
// of the subtree rooted at `slot_hash_key`. For instance:
/*
                    Build fork structure:
                          slot 0
                            |
                          slot 1
                          /    \
                    slot 2     |
                        |     slot 3 (34%)
                slot 4 (66%)
                    If slot 4 is a duplicate slot, so no longer qualifies as a candidate until
                    the slot is confirmed, the weight of votes on slot 4 should still count towards
                    slot 2, otherwise we might pick slot 3 as the heaviest fork to build blocks on
                    instead of slot 2.
                */
// See comment above for why this check is outside of the `is_candidate` check.
⋮----
// Note: If there's no valid children, then the best slot should default to the
if child_fork_info.is_candidate()
⋮----
child_height.cmp(&deepest_child_height),
child_stake_voted_subtree.cmp(&deepest_child_stake_voted_subtree),
child_key.cmp(&deepest_child_slot_key)
⋮----
let fork_info = self.fork_infos.get_mut(&slot_hash_key).unwrap();
⋮----
info!("Fork choice setting {slot_hash_key:?} to duplicate confirmed");
⋮----
fork_info.set_duplicate_confirmed();
⋮----
fn mark_fork_valid(&mut self, fork_to_modify_key: SlotHashKey, valid_slot: Slot) {
if let Some(fork_info_to_modify) = self.fork_infos.get_mut(&fork_to_modify_key) {
fork_info_to_modify.update_with_newly_valid_ancestor(&fork_to_modify_key, valid_slot);
⋮----
fn mark_fork_invalid(&mut self, fork_to_modify_key: SlotHashKey, invalid_slot: Slot) {
⋮----
.update_with_newly_invalid_ancestor(&fork_to_modify_key, invalid_slot);
⋮----
fn generate_update_operations<'a, 'b>(
⋮----
let (pubkey, new_vote_slot_hash) = pubkey_vote.borrow();
⋮----
match observed_pubkeys.entry(*pubkey) {
⋮----
panic!("Should not get multiple votes for same pubkey in the same batch");
⋮----
vacant_entry.insert(new_vote_slot);
⋮----
let mut pubkey_latest_vote = self.latest_votes.get_mut(pubkey);
match pubkey_latest_vote.as_mut() {
⋮----
self.latest_votes.insert(*pubkey, *new_vote_slot_hash)
⋮----
assert!(if new_vote_slot == old_latest_vote_slot {
⋮----
let epoch = epoch_schedule.get_epoch(old_latest_vote_slot);
⋮----
.get(&epoch)
.map(|epoch_stakes| epoch_stakes.vote_account_stake(pubkey))
.unwrap_or(0);
⋮----
.entry((
⋮----
.and_modify(|update| update.update_stake(stake_update))
.or_insert(UpdateOperation::Subtract(stake_update));
self.insert_aggregate_operations(
⋮----
let epoch = epoch_schedule.get_epoch(new_vote_slot_hash.0);
⋮----
.entry((*new_vote_slot_hash, UpdateLabel::Add))
⋮----
.or_insert(UpdateOperation::Add(stake_update));
self.insert_aggregate_operations(&mut update_operations, *new_vote_slot_hash);
⋮----
fn process_update_operations(&mut self, update_operations: UpdateOperations) {
for ((slot_hash_key, _), operation) in update_operations.into_iter().rev() {
⋮----
self.mark_fork_valid(slot_hash_key, valid_slot)
⋮----
self.mark_fork_invalid(slot_hash_key, invalid_slot)
⋮----
UpdateOperation::Aggregate => self.aggregate_slot(slot_hash_key),
UpdateOperation::Add(stake) => self.add_slot_stake(&slot_hash_key, stake),
UpdateOperation::Subtract(stake) => self.subtract_slot_stake(&slot_hash_key, stake),
⋮----
fn add_slot_stake(&mut self, slot_hash_key: &SlotHashKey, stake: u64) {
if let Some(fork_info) = self.fork_infos.get_mut(slot_hash_key) {
⋮----
fn subtract_slot_stake(&mut self, slot_hash_key: &SlotHashKey, stake: u64) {
⋮----
fn parent(&self, slot_hash_key: &SlotHashKey) -> Option<SlotHashKey> {
⋮----
.map(|fork_info| fork_info.parent)
⋮----
fn print_state(&self) {
let best_slot_hash_key = self.best_overall_slot();
let mut best_path: VecDeque<_> = self.ancestor_iterator(best_slot_hash_key).collect();
best_path.push_front(best_slot_hash_key);
⋮----
fn heaviest_slot_on_same_voted_fork(&self, tower: &Tower) -> Option<SlotHashKey> {
⋮----
.last_voted_slot_hash()
.and_then(|last_voted_slot_hash| {
match self.is_candidate(&last_voted_slot_hash) {
Some(true) => self.best_slot(&last_voted_slot_hash),
⋮----
self.deepest_slot(&last_voted_slot_hash)
⋮----
if !tower.is_stray_last_vote() {
panic!(
⋮----
fn set_stake_voted_at(&mut self, slot_hash_key: SlotHashKey, stake_voted_at: u64) {
⋮----
.get_mut(&slot_hash_key)
⋮----
fn is_leaf(&self, slot_hash_key: SlotHashKey) -> bool {
⋮----
.get(&slot_hash_key)
⋮----
.is_empty()
⋮----
type TreeKey = SlotHashKey;
type ChildIter = Iter<'a, SlotHashKey>;
fn contains_slot(&self, slot_hash_key: &SlotHashKey) -> bool {
self.fork_infos.contains_key(slot_hash_key)
⋮----
fn children(&self, slot_hash_key: &SlotHashKey) -> Option<Self::ChildIter> {
⋮----
.map(|fork_info| fork_info.children.iter())
⋮----
impl ForkChoice for HeaviestSubtreeForkChoice {
type ForkChoiceKey = SlotHashKey;
fn compute_bank_stats(
⋮----
let new_votes = latest_validator_votes_for_frozen_banks.take_votes_dirty_set(root);
let (best_overall_slot, best_overall_hash) = self.add_votes(
new_votes.into_iter(),
bank.epoch_stakes_map(),
bank.epoch_schedule(),
⋮----
start.stop();
datapoint_info!(
⋮----
fn select_forks(
⋮----
let r_bank_forks = bank_forks.read().unwrap();
⋮----
.get_with_checked_hash(self.best_overall_slot())
.unwrap(),
self.heaviest_slot_on_same_voted_fork(tower)
.and_then(|slot_hash| {
⋮----
if let Some(bank) = r_bank_forks.get(slot_hash.0) {
if bank.hash() != slot_hash.1 {
⋮----
Some(bank)
⋮----
fn mark_fork_invalid_candidate(&mut self, invalid_slot_hash_key: &SlotHashKey) {
info!("marking fork starting at: {invalid_slot_hash_key:?} invalid candidate");
let fork_info = self.fork_infos.get_mut(invalid_slot_hash_key);
⋮----
assert!(!fork_info.is_duplicate_confirmed);
⋮----
(&*self).subtree_diff(*invalid_slot_hash_key, SlotHashKey::default())
⋮----
self.do_insert_aggregate_operation(
⋮----
&Some(UpdateOperation::MarkInvalid(invalid_slot_hash_key.0)),
⋮----
self.insert_aggregate_operations(&mut update_operations, *invalid_slot_hash_key);
⋮----
fn mark_fork_valid_candidate(&mut self, valid_slot_hash_key: &SlotHashKey) -> Vec<SlotHashKey> {
info!("marking fork starting at: {valid_slot_hash_key:?} valid candidate");
let mut newly_duplicate_confirmed_ancestors = vec![];
⋮----
.chain(self.ancestor_iterator(*valid_slot_hash_key))
⋮----
if !self.is_duplicate_confirmed(&ancestor_key).unwrap() {
newly_duplicate_confirmed_ancestors.push(ancestor_key);
⋮----
for child_hash_key in (&*self).subtree_diff(*valid_slot_hash_key, SlotHashKey::default()) {
⋮----
&Some(UpdateOperation::MarkValid(valid_slot_hash_key.0)),
⋮----
self.insert_aggregate_operations(&mut update_operations, *valid_slot_hash_key);
⋮----
struct AncestorIterator<'a> {
⋮----
fn new(
⋮----
impl Iterator for AncestorIterator<'_> {
type Item = SlotHashKey;
fn next(&mut self) -> Option<Self::Item> {
⋮----
.get(&self.current_slot_hash_key)
⋮----
.unwrap_or(None);
⋮----
.map(|parent_slot_hash_key| {
⋮----
Some(self.current_slot_hash_key)
⋮----
mod test {
⋮----
fn test_max_by_weight() {
let forks = tr(0) / (tr(4) / (tr(5)));
⋮----
heaviest_subtree_fork_choice.add_votes(
[(vote_pubkeys[0], (4, Hash::default()))].iter(),
⋮----
assert_eq!(
⋮----
fn test_add_root_parent() {
let forks = tr(3) / (tr(4) / (tr(5)));
⋮----
[(vote_pubkeys[0], (5, Hash::default()))].iter(),
⋮----
heaviest_subtree_fork_choice.add_root_parent((2, Hash::default()));
⋮----
assert!(heaviest_subtree_fork_choice
⋮----
fn test_ancestor_iterator() {
let mut heaviest_subtree_fork_choice = setup_forks();
⋮----
.ancestor_iterator((6, Hash::default()))
⋮----
.ancestor_iterator((4, Hash::default()))
⋮----
.ancestor_iterator((1, Hash::default()))
⋮----
heaviest_subtree_fork_choice.set_tree_root((2, Hash::default()));
⋮----
fn test_new_from_frozen_banks() {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3)));
⋮----
vote_simulator.fill_bank_forks(forks, &HashMap::new(), true);
⋮----
.read()
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
let root = root_bank.slot();
let root_hash = root_bank.hash();
⋮----
let bank0_hash = bank_forks.read().unwrap().get(0).unwrap().hash();
⋮----
let bank1_hash = bank_forks.read().unwrap().get(1).unwrap().hash();
⋮----
let bank2_hash = bank_forks.read().unwrap().get(2).unwrap().hash();
let bank3_hash = bank_forks.read().unwrap().get(3).unwrap().hash();
⋮----
let bank4_hash = bank_forks.read().unwrap().get(4).unwrap().hash();
⋮----
assert!((&heaviest_subtree_fork_choice)
⋮----
fn test_set_root() {
⋮----
heaviest_subtree_fork_choice.set_tree_root((1, Hash::default()));
⋮----
heaviest_subtree_fork_choice.set_tree_root((5, Hash::default()));
⋮----
fn test_set_root_and_add_votes() {
⋮----
[(vote_pubkeys[0], (2, Hash::default()))].iter(),
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot().0, 4);
⋮----
[(vote_pubkeys[0], (3, Hash::default()))].iter(),
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot().0, 6);
⋮----
heaviest_subtree_fork_choice.set_tree_root((3, Hash::default()));
⋮----
.add_new_leaf_slot((7, Hash::default()), Some((6, Hash::default())));
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot().0, 7);
⋮----
fn test_set_root_and_add_outdated_votes() {
⋮----
[(vote_pubkeys[0], (0, Hash::default()))].iter(),
⋮----
fn test_best_overall_slot() {
let heaviest_subtree_fork_choice = setup_forks();
⋮----
fn test_add_new_leaf_duplicate() {
⋮----
) = setup_duplicate_forks();
⋮----
heaviest_subtree_fork_choice.add_new_leaf_slot(child, Some(duplicate_parent));
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot(), child);
⋮----
.chain(std::iter::once(&duplicate_leaves_descended_from_4[1]))
⋮----
.add_new_leaf_slot(duplicate_parent, Some((4, Hash::default())));
⋮----
fn test_propagate_new_leaf() {
⋮----
.add_new_leaf_slot((10, Hash::default()), Some((4, Hash::default())));
⋮----
.ancestor_iterator((10, Hash::default()))
.chain(std::iter::once((10, Hash::default())));
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_slot(&a).unwrap().0, 10);
assert_eq!(heaviest_subtree_fork_choice.deepest_slot(&a).unwrap().0, 10);
⋮----
.add_new_leaf_slot((9, Hash::default()), Some((4, Hash::default())));
⋮----
.ancestor_iterator((9, Hash::default()))
.chain(std::iter::once((9, Hash::default())));
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_slot(&a).unwrap().0, 9);
assert_eq!(heaviest_subtree_fork_choice.deepest_slot(&a).unwrap().0, 9);
⋮----
.add_new_leaf_slot((11, Hash::default()), Some((4, Hash::default())));
⋮----
.ancestor_iterator((11, Hash::default()))
⋮----
[(vote_pubkeys[0], (leaf6, Hash::default()))].iter(),
⋮----
.add_new_leaf_slot((8, Hash::default()), Some((4, Hash::default())));
⋮----
.ancestor_iterator((8, Hash::default()))
.chain(std::iter::once((8, Hash::default())));
⋮----
[(vote_pubkeys[1], (8, Hash::default()))].iter(),
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot().0, 8);
assert_eq!(heaviest_subtree_fork_choice.deepest_overall_slot().0, 8);
⋮----
.add_new_leaf_slot((7, Hash::default()), Some((4, Hash::default())));
⋮----
.ancestor_iterator((7, Hash::default()))
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_slot(&a).unwrap().0, 8);
assert_eq!(heaviest_subtree_fork_choice.deepest_slot(&a).unwrap().0, 8);
⋮----
for leaf in [8, 9, 10, 11].iter() {
⋮----
fn test_propagate_new_leaf_2() {
let forks = tr(0) / (tr(4) / (tr(6)));
⋮----
.add_new_leaf_slot((5, Hash::default()), Some((0, Hash::default())));
⋮----
.add_new_leaf_slot((2, Hash::default()), Some((0, Hash::default())));
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot().0, 2);
⋮----
.add_new_leaf_slot((1, Hash::default()), Some((0, Hash::default())));
⋮----
fn test_aggregate_slot() {
⋮----
heaviest_subtree_fork_choice.aggregate_slot((1, Hash::default()));
⋮----
let staked_voted_slots: HashSet<_> = vec![2, 4, 5, 6].into_iter().collect();
⋮----
heaviest_subtree_fork_choice.set_stake_voted_at((*slot, Hash::default()), *slot);
⋮----
.chain(heaviest_subtree_fork_choice.ancestor_iterator((6, Hash::default())))
.chain(std::iter::once((4, Hash::default())))
.chain(heaviest_subtree_fork_choice.ancestor_iterator((4, Hash::default())))
⋮----
heaviest_subtree_fork_choice.aggregate_slot(slot_hash);
⋮----
assert_eq!(heaviest_subtree_fork_choice.deepest_overall_slot().0, 6);
⋮----
let expected_stake = if staked_voted_slots.contains(&slot) {
⋮----
.stake_voted_at(&(*slot, Hash::default()))
.unwrap();
⋮----
fn test_process_update_operations() {
⋮----
let pubkey_votes: Vec<(Pubkey, SlotHashKey)> = vec![
⋮----
if !heaviest_subtree_fork_choice.is_leaf((slot, Hash::default())) {
⋮----
.contains(&(slot, Hash::default()))
⋮----
if [2, 4].contains(&slot) {
⋮----
check_process_update_correctness(
⋮----
fn test_generate_update_operations() {
⋮----
let expected_update_operations: UpdateOperations = vec![
⋮----
.into_iter()
⋮----
let generated_update_operations = heaviest_subtree_fork_choice.generate_update_operations(
pubkey_votes.iter(),
⋮----
assert_eq!(expected_update_operations, generated_update_operations);
⋮----
assert!(generated_update_operations.is_empty());
⋮----
vec![
⋮----
fn test_add_votes() {
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot().0, 4)
⋮----
fn test_add_votes_duplicate_tie() {
⋮----
vec![(vote_pubkeys[1], duplicate_leaves_descended_from_4[1])];
⋮----
heaviest_subtree_fork_choice.ancestor_iterator(duplicate_leaves_descended_from_4[1])
⋮----
fn test_add_votes_duplicate_greater_hash_ignored() {
⋮----
vec![(vote_pubkeys[0], duplicate_leaves_descended_from_4[1])];
⋮----
fn test_add_votes_duplicate_smaller_hash_prioritized() {
⋮----
vec![(vote_pubkeys[0], duplicate_leaves_descended_from_4[0])];
⋮----
heaviest_subtree_fork_choice.ancestor_iterator(duplicate_leaves_descended_from_4[0])
⋮----
fn test_add_votes_duplicate_then_outdated() {
⋮----
setup_duplicate_forks();
⋮----
.add_new_leaf_slot(higher_child_with_duplicate_parent, Some(duplicate_parent));
⋮----
Some(nonduplicate_parent),
⋮----
for (i, duplicate_leaf) in duplicate_leaves_descended_from_4.iter().enumerate() {
⋮----
for ancestor in heaviest_subtree_fork_choice.ancestor_iterator(node4) {
⋮----
fn test_add_votes_duplicate_zero_stake() {
⋮----
vec![(vote_pubkeys[0], higher_child_with_duplicate_parent)];
⋮----
fn test_is_best_child() {
let forks = tr(0) / (tr(4) / (tr(9)) / (tr(10)));
⋮----
assert!(heaviest_subtree_fork_choice.is_best_child(&(0, Hash::default())));
assert!(heaviest_subtree_fork_choice.is_best_child(&(4, Hash::default())));
assert!(heaviest_subtree_fork_choice.is_best_child(&(9, Hash::default())));
assert!(!heaviest_subtree_fork_choice.is_best_child(&(10, Hash::default())));
⋮----
assert!(heaviest_subtree_fork_choice.is_best_child(&(8, Hash::default())));
assert!(!heaviest_subtree_fork_choice.is_best_child(&(9, Hash::default())));
⋮----
[(vote_pubkeys[0], (9, Hash::default()))].iter(),
⋮----
assert!(!heaviest_subtree_fork_choice.is_best_child(&(8, Hash::default())));
⋮----
fn test_merge() {
⋮----
let forks = tr(0) / (tr(3) / (tr(5) / (tr(7))) / (tr(9) / (tr(11) / (tr(12)))));
⋮----
tree1.add_votes(
⋮----
let forks = tr(10) / (tr(15) / (tr(16) / (tr(17))) / (tr(18) / (tr(19) / (tr(20)))));
⋮----
tree2.add_votes(
⋮----
tree1.merge(
⋮----
let ancestors: Vec<_> = tree1.ancestor_iterator((20, Hash::default())).collect();
⋮----
let ancestors: Vec<_> = tree1.ancestor_iterator((17, Hash::default())).collect();
⋮----
assert_eq!(tree1.stake_voted_at(&(16, Hash::default())).unwrap(), stake);
assert_eq!(tree1.stake_voted_at(&(5, Hash::default())).unwrap(), 0);
assert_eq!(tree1.stake_voted_at(&(19, Hash::default())).unwrap(), stake);
assert_eq!(tree1.stake_voted_at(&(3, Hash::default())).unwrap(), 0);
assert_eq!(tree1.stake_voted_at(&(10, Hash::default())).unwrap(), 0);
assert_eq!(tree1.stake_voted_at(&(12, Hash::default())).unwrap(), stake);
assert_eq!(tree1.stake_voted_at(&(20, Hash::default())).unwrap(), stake);
⋮----
assert_eq!(tree1.best_overall_slot().0, 20);
⋮----
fn test_merge_duplicate() {
⋮----
.take(2)
⋮----
slot_5_duplicate_hashes.sort();
⋮----
tr((0, Hash::default())) / tr((2, Hash::default())) / tr(slot_5_duplicate_hashes[1]);
⋮----
let forks = tr((3, Hash::default())) / tr(slot_5_duplicate_hashes[0]);
⋮----
assert_eq!(tree1.best_overall_slot(), slot_5_duplicate_hashes[0]);
⋮----
.ancestor_iterator(slot_5_duplicate_hashes[1])
⋮----
assert_eq!(ancestors, vec![(0, Hash::default())]);
⋮----
.ancestor_iterator(slot_5_duplicate_hashes[0])
⋮----
fn test_subtree_diff() {
⋮----
fn test_stray_restored_slot() {
let forks = tr(0) / (tr(1) / tr(2));
⋮----
tower.record_vote(1, Hash::default());
assert!(!tower.is_stray_last_vote());
⋮----
slot_history.add(0);
slot_history.add(999);
⋮----
.adjust_lockouts_after_replay(0, &slot_history)
⋮----
assert!(tower.is_stray_last_vote());
⋮----
tower.record_vote(3, Hash::default());
⋮----
fn test_mark_valid_invalid_forks() {
⋮----
tower.record_vote(last_voted_slot_hash.0, last_voted_slot_hash.1);
⋮----
heaviest_subtree_fork_choice.mark_fork_invalid_candidate(&invalid_candidate);
assert!(!heaviest_subtree_fork_choice
⋮----
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot().0, 3);
⋮----
heaviest_subtree_fork_choice.add_new_leaf_slot(new_leaf7, Some((6, Hash::default())));
⋮----
let pubkey_votes: Vec<(Pubkey, SlotHashKey)> = vec![(vote_pubkeys[0], new_leaf7)];
⋮----
.add_new_leaf_slot(new_leaf8, Some((invalid_slot_ancestor, Hash::default())));
assert_eq!(heaviest_subtree_fork_choice.best_overall_slot(), new_leaf8,);
⋮----
heaviest_subtree_fork_choice.mark_fork_valid_candidate(&invalid_candidate);
⋮----
fn setup_mark_invalid_forks_duplicate_tests() -> (
⋮----
fn test_mark_invalid_then_valid_duplicate() {
⋮----
) = setup_mark_invalid_forks_duplicate_tests();
⋮----
Some(duplicate_leaves_descended_from_4[0]),
⋮----
fn test_mark_invalid_then_add_new_heavier_duplicate_slot() {
⋮----
assert!(new_duplicate_hash < duplicate_leaves_descended_from_4[0].1);
⋮----
heaviest_subtree_fork_choice.add_new_leaf_slot(new_duplicate, Some((3, Hash::default())));
⋮----
fn test_mark_valid_then_descendant_invalid() {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(3) / (tr(4) / (tr(5) / tr(6))))));
⋮----
let duplicate_confirmed_key = duplicate_confirmed_slot.slot_hash();
heaviest_subtree_fork_choice.mark_fork_valid_candidate(&duplicate_confirmed_key);
for slot_hash_key in heaviest_subtree_fork_choice.fork_infos.keys() {
⋮----
let invalid_descendant_key = invalid_descendant_slot.slot_hash();
heaviest_subtree_fork_choice.mark_fork_invalid_candidate(&invalid_descendant_key);
⋮----
assert!(
⋮----
let later_duplicate_confirmed_key = later_duplicate_confirmed_slot.slot_hash();
heaviest_subtree_fork_choice.mark_fork_valid_candidate(&later_duplicate_confirmed_key);
⋮----
let last_duplicate_confirmed_key = last_duplicate_confirmed_slot.slot_hash();
heaviest_subtree_fork_choice.mark_fork_valid_candidate(&last_duplicate_confirmed_key);
⋮----
fn test_mark_valid_then_ancestor_invalid() {
⋮----
heaviest_subtree_fork_choice.mark_fork_invalid_candidate(&3.slot_hash());
⋮----
fn setup_set_unconfirmed_and_confirmed_duplicate_slot_tests(
⋮----
let forks = tr(0) / (tr(1) / (tr(2) / (tr(3) / (tr(4) / tr(5)))));
⋮----
.mark_fork_invalid_candidate(&smaller_duplicate_slot.slot_hash());
⋮----
.mark_fork_invalid_candidate(&larger_duplicate_slot.slot_hash());
⋮----
fn test_set_unconfirmed_duplicate_confirm_smaller_slot_first() {
⋮----
setup_set_unconfirmed_and_confirmed_duplicate_slot_tests(
⋮----
heaviest_subtree_fork_choice.mark_fork_valid_candidate(&smaller_duplicate_slot.slot_hash());
⋮----
heaviest_subtree_fork_choice.mark_fork_valid_candidate(&larger_duplicate_slot.slot_hash());
⋮----
fn test_set_unconfirmed_duplicate_confirm_larger_slot_first() {
⋮----
fn test_split_off_simple() {
⋮----
let tree = heaviest_subtree_fork_choice.split_off(&(5, Hash::default()));
⋮----
assert!(tree
⋮----
fn test_split_off_unvoted() {
⋮----
let tree = heaviest_subtree_fork_choice.split_off(&(2, Hash::default()));
⋮----
assert_eq!(0, tree.stake_voted_subtree(&(2, Hash::default())).unwrap());
assert_eq!(0, tree.stake_voted_subtree(&(4, Hash::default())).unwrap());
⋮----
fn test_split_off_on_best_path() {
⋮----
assert_eq!(6, heaviest_subtree_fork_choice.best_overall_slot().0);
let tree = heaviest_subtree_fork_choice.split_off(&(6, Hash::default()));
assert_eq!(5, heaviest_subtree_fork_choice.best_overall_slot().0);
assert_eq!(6, tree.best_overall_slot().0);
let tree = heaviest_subtree_fork_choice.split_off(&(3, Hash::default()));
assert_eq!(4, heaviest_subtree_fork_choice.best_overall_slot().0);
assert_eq!(5, tree.best_overall_slot().0);
let tree = heaviest_subtree_fork_choice.split_off(&(1, Hash::default()));
assert_eq!(0, heaviest_subtree_fork_choice.best_overall_slot().0);
assert_eq!(4, tree.best_overall_slot().0);
⋮----
fn test_split_off_on_deepest_path() {
⋮----
assert_eq!(6, heaviest_subtree_fork_choice.deepest_overall_slot().0);
⋮----
assert_eq!(4, heaviest_subtree_fork_choice.deepest_overall_slot().0);
assert_eq!(6, tree.deepest_overall_slot().0);
⋮----
assert_eq!(5, tree.deepest_overall_slot().0);
⋮----
assert_eq!(0, heaviest_subtree_fork_choice.deepest_overall_slot().0);
assert_eq!(4, tree.deepest_overall_slot().0);
⋮----
fn test_split_off_on_deepest_path_complicated() {
let mut heaviest_subtree_fork_choice = setup_complicated_forks();
assert_eq!(23, heaviest_subtree_fork_choice.deepest_overall_slot().0);
⋮----
let tree = heaviest_subtree_fork_choice.split_off(&(13, Hash::default()));
assert_eq!(34, heaviest_subtree_fork_choice.deepest_overall_slot().0);
⋮----
assert_eq!(23, tree.deepest_overall_slot().0);
assert_eq!(6, tree.height(&(13, Hash::default())).unwrap());
assert_eq!(2, tree.height(&(18, Hash::default())).unwrap());
assert_eq!(1, tree.height(&(25, Hash::default())).unwrap());
⋮----
fn test_split_off_with_dups() {
⋮----
let tree = heaviest_subtree_fork_choice.split_off(&expected_best_slot_hash);
⋮----
assert_eq!(tree.best_overall_slot(), expected_best_slot_hash);
assert_eq!(tree.deepest_overall_slot(), expected_best_slot_hash);
⋮----
fn test_split_off_subtree_with_dups() {
⋮----
assert_eq!(tree.deepest_overall_slot(), expected_best_slot_hash,);
⋮----
fn test_split_off_complicated() {
⋮----
for &n in nodes_to_check.iter() {
assert!(tree.contains_block(&(n, Hash::default())));
⋮----
let split_tree = tree.split_off(&(node, Hash::default()));
⋮----
assert!(!tree.contains_block(&(n, Hash::default())));
assert!(split_tree.contains_block(&(n, Hash::default())));
⋮----
split_and_check(
⋮----
vec![14, 15, 16, 22, 23, 17, 21, 18, 19, 20, 24, 25],
⋮----
split_and_check(&mut heaviest_subtree_fork_choice, 12, vec![12, 13]);
⋮----
vec![2, 7, 8, 9, 33, 34, 10, 31, 32],
⋮----
split_and_check(&mut heaviest_subtree_fork_choice, 1, vec![1, 5, 6]);
⋮----
fn test_purge_prune() {
⋮----
assert_eq!(heaviest_subtree_fork_choice.tree_root().0, 0);
let (purged, pruned) = heaviest_subtree_fork_choice.purge_prune((0, Hash::default()));
assert!(purged.is_empty());
assert!(pruned.is_empty());
⋮----
let (mut purged, pruned) = heaviest_subtree_fork_choice.purge_prune((1, Hash::default()));
purged.sort();
assert_eq!(purged, vec![(0, Hash::default())]);
⋮----
assert_eq!(heaviest_subtree_fork_choice.tree_root().0, 1);
let (mut purged, pruned) = heaviest_subtree_fork_choice.purge_prune((3, Hash::default()));
⋮----
assert_eq!(purged, vec![(1, Hash::default()), (2, Hash::default())]);
⋮----
assert_eq!(heaviest_subtree_fork_choice.tree_root().0, 3);
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(5) / (tr(6))));
⋮----
heaviest_subtree_fork_choice.purge_prune((3, Hash::default()));
⋮----
pruned.sort();
⋮----
assert!(heaviest_subtree_fork_choice.fork_infos.is_empty());
⋮----
fn test_purge_prune_complicated() {
⋮----
tr(5),
tr(6),
tr(7),
tr(8),
tr(9) / (tr(33) / tr(34)),
tr(10),
tr(31) / tr(32),
⋮----
.clone()
.split_off(&(3, Hash::default()));
⋮----
assert_eq!(heaviest_subtree_fork_choice, expected_tree);
⋮----
tr(17) / tr(21),
tr(18) / tr(19) / tr(20),
tr(24),
tr(25),
tr(26),
⋮----
.split_off(&(16, Hash::default()));
⋮----
heaviest_subtree_fork_choice.purge_prune((16, Hash::default()));
⋮----
fn setup_forks() -> HeaviestSubtreeForkChoice {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3) / (tr(5) / (tr(6)))));
⋮----
fn setup_complicated_forks() -> HeaviestSubtreeForkChoice {
let tree_12 = tr(12)
/ (tr(13)
/ (tr(14)
/ (tr(15)
/ (tr(16) / (tr(22) / tr(23)))
/ (tr(17) / tr(21))
/ (tr(18) / tr(19) / tr(20))
/ tr(24))
/ tr(25)));
let forks = tr(0)
/ (tr(1) / tr(5) / tr(6))
/ (tr(2) / tr(7) / tr(8) / (tr(9) / (tr(33) / tr(34))) / tr(10) / (tr(31) / tr(32)))
/ (tr(3) / tr(11) / tree_12 / tr(26));
⋮----
fn setup_duplicate_forks() -> (
⋮----
duplicate_leaves_descended_from_4.sort();
duplicate_leaves_descended_from_5.sort();
duplicate_leaves_descended_from_6.sort();
⋮----
.add_new_leaf_slot(*duplicate_leaf, Some((4, Hash::default())));
⋮----
.add_new_leaf_slot(*duplicate_leaf, Some((5, Hash::default())));
⋮----
.add_new_leaf_slot(*duplicate_leaf, Some((6, Hash::default())));
⋮----
.children(&(4, Hash::default()))
⋮----
.copied()
.collect_vec();
dup_children.sort();
assert_eq!(dup_children, duplicate_leaves_descended_from_4);
⋮----
.children(&(5, Hash::default()))
⋮----
.filter(|(slot, _)| *slot == duplicate_slot)
⋮----
assert_eq!(dup_children, duplicate_leaves_descended_from_5);
⋮----
.children(&(6, Hash::default()))
⋮----
assert_eq!(dup_children, duplicate_leaves_descended_from_6);
⋮----
fn check_process_update_correctness<F, G>(
⋮----
let unique_votes: HashSet<Slot> = pubkey_votes.iter().map(|(_, (slot, _))| *slot).collect();
⋮----
.map(|v| {
⋮----
.ancestor_iterator((*v, Hash::default()))
.collect(),
⋮----
.entry(vote.0)
.and_modify(|c| *c += 1)
.or_insert(1);
⋮----
.map(|slot| {
⋮----
.map(|(vote_slot, ancestors)| {
(ancestors.contains(&(slot, Hash::default())) || *vote_slot == slot)
⋮----
* vote_count.get(vote_slot).unwrap()
⋮----
.sum();
⋮----
let update_operations_batch = heaviest_subtree_fork_choice.generate_update_operations(
⋮----
heaviest_subtree_fork_choice.process_update_operations(update_operations_batch);
⋮----
vote_count.get(&slot).cloned().unwrap_or(0) as u64 * stake;
⋮----
*num_voted_descendants.get(&slot).unwrap() as u64 * stake;

================
File: core/src/consensus/latest_validator_votes_for_frozen_banks.rs
================
pub struct LatestValidatorVotesForFrozenBanks {
⋮----
impl LatestValidatorVotesForFrozenBanks {
pub fn check_add_vote(
⋮----
let pubkey_max_frozen_votes = vote_map.entry(vote_pubkey);
⋮----
occupied_entry.get_mut();
⋮----
.insert(vote_pubkey, (vote_slot, vec![frozen_hash]));
⋮----
*latest_frozen_vote_hashes = vec![frozen_hash];
return (true, Some(vote_slot));
⋮----
&& !latest_frozen_vote_hashes.contains(&frozen_hash)
⋮----
self.fork_choice_dirty_set.entry(vote_pubkey).or_default();
assert!(!dirty_frozen_hashes.contains(&frozen_hash));
dirty_frozen_hashes.push(frozen_hash);
⋮----
latest_frozen_vote_hashes.push(frozen_hash);
⋮----
return (false, Some(*latest_frozen_vote_slot));
⋮----
vacant_entry.insert((vote_slot, vec![frozen_hash]));
⋮----
Entry::Occupied(occupied_entry) => Some(occupied_entry.get().0),
⋮----
pub fn take_votes_dirty_set(&mut self, root: Slot) -> Vec<(Pubkey, SlotHashKey)> {
⋮----
.into_iter()
.filter(|(_, (slot, _))| *slot >= root)
.flat_map(|(pk, (slot, hashes))| {
⋮----
.map(|hash| (pk, (slot, hash)))
⋮----
.collect()
⋮----
pub fn max_gossip_frozen_votes(&self) -> &HashMap<Pubkey, (Slot, Vec<Hash>)> {
⋮----
fn latest_vote(&self, pubkey: &Pubkey, is_replay_vote: bool) -> Option<&(Slot, Vec<Hash>)> {
⋮----
vote_map.get(pubkey)
⋮----
mod tests {
⋮----
fn run_test_latest_validator_votes_for_frozen_banks_check_add_vote(is_replay_vote: bool) {
⋮----
assert_eq!(
⋮----
assert!(latest_validator_votes_for_frozen_banks
⋮----
(true, Some(vote_slot))
⋮----
(false, Some(vote_slot))
⋮----
assert!(!latest_validator_votes_for_frozen_banks
⋮----
let all_frozen_hashes = vec![frozen_hash, duplicate_frozen_hash];
⋮----
fn test_latest_validator_votes_for_frozen_banks_check_add_vote_is_replay() {
run_test_latest_validator_votes_for_frozen_banks_check_add_vote(true)
⋮----
fn test_latest_validator_votes_for_frozen_banks_check_add_vote_is_not_replay() {
run_test_latest_validator_votes_for_frozen_banks_check_add_vote(false)
⋮----
fn run_test_latest_validator_votes_for_frozen_banks_take_votes_dirty_set(is_replay: bool) {
⋮----
.flat_map(|vote_slot| {
⋮----
vec![
⋮----
vec![]
⋮----
setup_dirty_set(&mut latest_validator_votes_for_frozen_banks);
⋮----
latest_validator_votes_for_frozen_banks.take_votes_dirty_set(root);
votes_dirty_set_output.sort();
expected_dirty_set.sort();
assert_eq!(votes_dirty_set_output, expected_dirty_set);
⋮----
let dirty_set = setup_dirty_set(&mut latest_validator_votes_for_frozen_banks);
⋮----
dirty_set[dirty_set.len().saturating_sub(2)..dirty_set.len()].to_vec();
⋮----
fn test_latest_validator_votes_for_frozen_banks_take_votes_dirty_set_is_replay() {
run_test_latest_validator_votes_for_frozen_banks_take_votes_dirty_set(true)
⋮----
fn test_latest_validator_votes_for_frozen_banks_take_votes_dirty_set_is_not_replay() {
run_test_latest_validator_votes_for_frozen_banks_take_votes_dirty_set(false)
⋮----
fn test_latest_validator_votes_for_frozen_banks_add_replay_and_gossip_vote() {

================
File: core/src/consensus/progress_map.rs
================
type VotedSlot = Slot;
type ExpirationSlot = Slot;
⋮----
pub struct LockoutInterval {
⋮----
pub type LockoutIntervals = Vec<LockoutInterval>;
⋮----
pub struct ValidatorStakeInfo {
⋮----
impl Default for ValidatorStakeInfo {
fn default() -> Self {
⋮----
impl ValidatorStakeInfo {
pub fn new(validator_vote_pubkey: Pubkey, stake: u64, total_epoch_stake: u64) -> Self {
⋮----
pub struct RetransmitInfo {
⋮----
impl RetransmitInfo {
pub fn reached_retransmit_threshold(&self) -> bool {
⋮----
self.retry_time.elapsed().as_millis() > u128::from(backoff_duration_ms)
⋮----
pub fn increment_retry_iteration(&mut self) {
self.retry_iteration = self.retry_iteration.saturating_add(1);
⋮----
pub struct ForkProgress {
⋮----
impl ForkProgress {
pub fn new(
⋮----
.map(|info| {
⋮----
vec![info.validator_vote_pubkey].into_iter().collect(),
⋮----
.unwrap_or((false, 0, HashSet::new(), false, 0));
⋮----
pub fn new_from_bank(
⋮----
if bank.collector_id() == validator_identity {
Some(ValidatorStakeInfo::new(
⋮----
bank.epoch_vote_account_stake(validator_vote_pubkey),
bank.total_epoch_stake(),
⋮----
bank.last_blockhash(),
⋮----
if bank.is_frozen() {
new_progress.fork_stats.bank_hash = Some(bank.hash());
⋮----
pub struct ForkStats {
⋮----
impl ForkStats {
pub fn fork_weight(&self) -> f64 {
⋮----
pub struct PropagatedStats {
⋮----
impl PropagatedStats {
pub fn add_vote_pubkey(&mut self, vote_pubkey: Pubkey, stake: u64) {
if self.propagated_validators.insert(vote_pubkey) {
⋮----
pub fn add_node_pubkey(&mut self, node_pubkey: &Pubkey, bank: &Bank) {
if !self.propagated_node_ids.contains(node_pubkey) {
⋮----
.epoch_vote_accounts_for_node_id(node_pubkey)
.map(|v| &v.vote_accounts);
⋮----
self.add_node_pubkey_internal(
⋮----
bank.epoch_vote_accounts(bank.epoch())
.expect("Epoch stakes for bank's own epoch must exist"),
⋮----
fn add_node_pubkey_internal(
⋮----
self.propagated_node_ids.insert(*node_pubkey);
for vote_account_pubkey in vote_account_pubkeys.iter() {
⋮----
.get(vote_account_pubkey)
.map(|(stake, _)| *stake)
.unwrap_or(0);
self.add_vote_pubkey(*vote_account_pubkey, stake);
⋮----
pub struct ProgressMap {
⋮----
type Target = HashMap<Slot, ForkProgress>;
fn deref(&self) -> &Self::Target {
⋮----
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
impl ProgressMap {
pub fn insert(&mut self, slot: Slot, fork_progress: ForkProgress) {
self.progress_map.insert(slot, fork_progress);
⋮----
pub fn get_propagated_stats(&self, slot: Slot) -> Option<&PropagatedStats> {
⋮----
.get(&slot)
.map(|fork_progress| &fork_progress.propagated_stats)
⋮----
pub fn get_propagated_stats_mut(&mut self, slot: Slot) -> Option<&mut PropagatedStats> {
⋮----
.get_mut(&slot)
.map(|fork_progress| &mut fork_progress.propagated_stats)
⋮----
pub fn get_propagated_stats_must_exist(&self, slot: Slot) -> &PropagatedStats {
self.get_propagated_stats(slot)
.unwrap_or_else(|| panic!("slot={slot} must exist in ProgressMap"))
⋮----
pub fn get_fork_stats(&self, slot: Slot) -> Option<&ForkStats> {
⋮----
.map(|fork_progress| &fork_progress.fork_stats)
⋮----
pub fn get_fork_stats_mut(&mut self, slot: Slot) -> Option<&mut ForkStats> {
⋮----
.map(|fork_progress| &mut fork_progress.fork_stats)
⋮----
pub fn get_retransmit_info(&self, slot: Slot) -> Option<&RetransmitInfo> {
⋮----
.map(|fork_progress| &fork_progress.retransmit_info)
⋮----
pub fn get_retransmit_info_mut(&mut self, slot: Slot) -> Option<&mut RetransmitInfo> {
⋮----
.map(|fork_progress| &mut fork_progress.retransmit_info)
⋮----
pub fn is_dead(&self, slot: Slot) -> Option<bool> {
⋮----
.map(|fork_progress| fork_progress.is_dead)
⋮----
pub fn get_hash(&self, slot: Slot) -> Option<Hash> {
⋮----
.and_then(|fork_progress| fork_progress.fork_stats.bank_hash)
⋮----
pub fn is_propagated(&self, slot: Slot) -> Option<bool> {
⋮----
.map(|stats| stats.is_propagated)
⋮----
pub fn get_latest_leader_slot_must_exist(&self, slot: Slot) -> Option<Slot> {
let propagated_stats = self.get_propagated_stats_must_exist(slot);
⋮----
Some(slot)
⋮----
pub fn get_leader_propagation_slot_must_exist(&self, slot: Slot) -> (bool, Option<Slot>) {
if let Some(leader_slot) = self.get_latest_leader_slot_must_exist(slot) {
⋮----
self.is_propagated(leader_slot).unwrap_or(true),
Some(leader_slot),
⋮----
pub fn my_latest_landed_vote(&self, slot: Slot) -> Option<Slot> {
⋮----
.and_then(|s| s.fork_stats.my_latest_landed_vote)
⋮----
pub fn set_duplicate_confirmed_hash(&mut self, slot: Slot, hash: Hash) {
let slot_progress = self.get_mut(&slot).unwrap();
slot_progress.fork_stats.duplicate_confirmed_hash = Some(hash);
⋮----
pub fn is_duplicate_confirmed(&self, slot: Slot) -> Option<bool> {
⋮----
.map(|s| s.fork_stats.duplicate_confirmed_hash.is_some())
⋮----
pub fn get_bank_prev_leader_slot(&self, bank: &Bank) -> Option<Slot> {
let parent_slot = bank.parent_slot();
self.get_propagated_stats(parent_slot)
.map(|stats| {
⋮----
Some(parent_slot)
⋮----
.unwrap_or(None)
⋮----
pub fn handle_new_root(&mut self, bank_forks: &BankForks) {
⋮----
.retain(|k, _| bank_forks.get(*k).is_some());
⋮----
pub fn log_propagated_stats(&self, slot: Slot, bank_forks: &RwLock<BankForks>) {
if let Some(stats) = self.get_propagated_stats(slot) {
info!(
⋮----
mod test {
⋮----
fn test_add_vote_pubkey() {
⋮----
stats.add_vote_pubkey(vote_pubkey, 1);
assert!(stats.propagated_validators.contains(&vote_pubkey));
assert_eq!(stats.propagated_validators_stake, 1);
⋮----
stats.add_vote_pubkey(vote_pubkey, 2);
⋮----
assert_eq!(stats.propagated_validators_stake, 3);
⋮----
fn test_add_node_pubkey_internal() {
⋮----
.take(num_vote_accounts)
.collect();
⋮----
.iter()
.skip(num_vote_accounts - staked_vote_accounts)
.map(|pubkey| (*pubkey, (1, VoteAccount::new_random())))
⋮----
stats.add_node_pubkey_internal(&node_pubkey, &vote_account_pubkeys, &epoch_vote_accounts);
assert!(stats.propagated_node_ids.contains(&node_pubkey));
assert_eq!(
⋮----
fn test_is_propagated_status_on_construction() {
let progress = ForkProgress::new(Hash::default(), Some(9), None, 0, 0);
assert!(!progress.propagated_stats.is_propagated);
⋮----
Some(9),
Some(ValidatorStakeInfo {
⋮----
assert!(progress.propagated_stats.is_propagated);
⋮----
Some(ValidatorStakeInfo::default()),
⋮----
fn test_is_propagated() {
⋮----
progress_map.insert(10, ForkProgress::new(Hash::default(), Some(9), None, 0, 0));
progress_map.insert(
⋮----
assert!(!progress_map.get_leader_propagation_slot_must_exist(9).0);
assert!(!progress_map.get_leader_propagation_slot_must_exist(10).0);
progress_map.insert(8, ForkProgress::new(Hash::default(), Some(7), None, 0, 0));
assert!(progress_map.get_leader_propagation_slot_must_exist(8).0);
⋮----
.get_propagated_stats_mut(9)
.unwrap()
⋮----
assert!(progress_map.get_leader_propagation_slot_must_exist(9).0);
assert!(progress_map.get(&9).unwrap().propagated_stats.is_propagated);
assert!(progress_map.get_leader_propagation_slot_must_exist(10).0);
⋮----
.get_propagated_stats_mut(10)

================
File: core/src/consensus/tower_storage.rs
================
pub enum SavedTowerVersions {
⋮----
impl SavedTowerVersions {
fn try_into_tower(&self, node_pubkey: &Pubkey) -> Result<Tower> {
assert_eq!(self.pubkey(), Pubkey::default());
⋮----
if !t.signature.verify(node_pubkey.as_ref(), &t.data) {
return Err(TowerError::InvalidSignature);
⋮----
bincode::deserialize(&t.data).map(TowerVersions::V1_7_14)
⋮----
bincode::deserialize(&t.data).map(TowerVersions::V1_14_11)
⋮----
tv.map_err(|e| e.into()).and_then(|tv: TowerVersions| {
let tower = tv.convert_to_current();
⋮----
return Err(TowerError::WrongTower(format!(
⋮----
Ok(tower)
⋮----
fn serialize_into(&self, file: &mut File) -> Result<()> {
bincode::serialize_into(file, self).map_err(|e| e.into())
⋮----
fn pubkey(&self) -> Pubkey {
⋮----
fn from(tower: SavedTower) -> SavedTowerVersions {
⋮----
fn from(tower: SavedTower1_7_14) -> SavedTowerVersions {
⋮----
pub struct SavedTower {
⋮----
impl SavedTower {
pub fn new<T: Signer>(tower: &Tower, keypair: &T) -> Result<Self> {
let node_pubkey = keypair.pubkey();
⋮----
let tower: Tower1_14_11 = tower.clone().into();
⋮----
let signature = keypair.sign_message(&data);
Ok(Self {
⋮----
pub trait TowerStorage: Sync + Send {
⋮----
pub struct NullTowerStorage {}
impl TowerStorage for NullTowerStorage {
fn load(&self, _node_pubkey: &Pubkey) -> Result<Tower> {
Err(TowerError::IoError(io::Error::other(
⋮----
fn store(&self, _saved_tower: &SavedTowerVersions) -> Result<()> {
Ok(())
⋮----
pub struct FileTowerStorage {
⋮----
impl FileTowerStorage {
pub fn new(tower_path: PathBuf) -> Self {
⋮----
pub fn old_filename(&self, node_pubkey: &Pubkey) -> PathBuf {
⋮----
.join(format!("tower-{node_pubkey}"))
.with_extension("bin")
⋮----
pub fn filename(&self, node_pubkey: &Pubkey) -> PathBuf {
⋮----
.join(format!("tower-1_9-{node_pubkey}"))
⋮----
fn store_old(&self, saved_tower: &SavedTower1_7_14) -> Result<()> {
⋮----
let filename = self.old_filename(&pubkey);
trace!("store: {}", filename.display());
let new_filename = filename.with_extension("bin.new");
⋮----
impl TowerStorage for FileTowerStorage {
fn load(&self, node_pubkey: &Pubkey) -> Result<Tower> {
let filename = self.filename(node_pubkey);
trace!("load {}", filename.display());
fs::create_dir_all(filename.parent().unwrap())?;
⋮----
.map_err(|e| e.into())
.and_then(|t: SavedTowerVersions| t.try_into_tower(node_pubkey))
⋮----
let file = File::open(self.old_filename(node_pubkey))?;
⋮----
.and_then(|t: SavedTower1_7_14| {
SavedTowerVersions::from(t).try_into_tower(node_pubkey)
⋮----
fn store(&self, saved_tower: &SavedTowerVersions) -> Result<()> {
let pubkey = saved_tower.pubkey();
let filename = self.filename(&pubkey);
⋮----
saved_tower.serialize_into(&mut file)?;
⋮----
pub mod test {
⋮----
fn test_tower_migration() {
let tower_path = TempDir::new().unwrap();
⋮----
let node_pubkey = identity_keypair.pubkey();
⋮----
.resize(MAX_LOCKOUT_HISTORY, Lockout::default());
vote_state.root_slot = Some(1);
let vote = Vote::new(vec![1, 2, 3, 4], Hash::default());
let tower_storage = FileTowerStorage::new(tower_path.path().to_path_buf());
⋮----
last_vote: vote.clone(),
⋮----
stray_restored_slot: Some(2),
⋮----
let saved_tower = SavedTower1_7_14::new(&old_tower, &identity_keypair).unwrap();
tower_storage.store_old(&saved_tower).unwrap();
⋮----
let loaded = Tower::restore(&tower_storage, &node_pubkey).unwrap();
assert_eq!(loaded.node_pubkey, old_tower.node_pubkey);
assert_eq!(loaded.last_vote(), VoteTransaction::from(vote));
assert_eq!(loaded.vote_state.root_slot, Some(1));
assert_eq!(loaded.stray_restored_slot(), None);

================
File: core/src/consensus/tower_vote_state.rs
================
pub struct TowerVoteState {
⋮----
impl TowerVoteState {
pub fn tower(&self) -> Vec<Slot> {
self.votes.iter().map(|v| v.slot()).collect()
⋮----
pub fn last_lockout(&self) -> Option<&Lockout> {
self.votes.back()
⋮----
pub fn last_voted_slot(&self) -> Option<Slot> {
self.last_lockout().map(|v| v.slot())
⋮----
pub fn nth_recent_lockout(&self, position: usize) -> Option<&Lockout> {
⋮----
.len()
.checked_sub(position.saturating_add(1))
.and_then(|pos| self.votes.get(pos))
⋮----
pub fn process_next_vote_slot(&mut self, next_vote_slot: Slot) {
⋮----
.last_voted_slot()
.is_some_and(|last_voted_slot| next_vote_slot <= last_voted_slot)
⋮----
self.pop_expired_votes(next_vote_slot);
if self.votes.len() == MAX_LOCKOUT_HISTORY {
let rooted_vote = self.votes.pop_front().unwrap();
self.root_slot = Some(rooted_vote.slot());
⋮----
self.votes.push_back(Lockout::new(next_vote_slot));
self.double_lockouts();
⋮----
fn pop_expired_votes(&mut self, next_vote_slot: Slot) {
while let Some(vote) = self.last_lockout() {
if !vote.is_locked_out_at_slot(next_vote_slot) {
self.votes.pop_back();
⋮----
fn double_lockouts(&mut self) {
let stack_depth = self.votes.len();
for (i, v) in self.votes.iter_mut().enumerate() {
⋮----
> i.checked_add(v.confirmation_count() as usize).expect(
⋮----
v.increase_confirmation_count(1);
⋮----
fn from(vote_state: VoteStateV4) -> Self {
⋮----
.into_iter()
.map(|landed_vote| landed_vote.into())
.collect(),
⋮----
fn from(vote_state: VoteStateV3) -> Self {
⋮----
fn from(vote_state: VoteState1_14_11) -> Self {
⋮----
fn from(vote_state: &VoteStateView) -> Self {
⋮----
votes: vote_state.votes_iter().collect(),
root_slot: vote_state.root_slot(),
⋮----
fn from(vote_state: TowerVoteState) -> Self {
⋮----
mod tests {
⋮----
fn check_lockouts(vote_state: &TowerVoteState) {
for (i, vote) in vote_state.votes.iter().enumerate() {
⋮----
.checked_sub(i)
.expect("`i` is less than `vote_state.votes.len()`");
assert_eq!(vote.lockout(), INITIAL_LOCKOUT.pow(num_votes as u32) as u64);
⋮----
fn test_basic_vote_state() {
⋮----
vote_state.process_next_vote_slot(1);
assert_eq!(vote_state.votes.len(), 1);
assert_eq!(vote_state.votes[0].slot(), 1);
assert_eq!(vote_state.votes[0].confirmation_count(), 1);
assert_eq!(vote_state.root_slot, None);
vote_state.process_next_vote_slot(2);
assert_eq!(vote_state.votes.len(), 2);
⋮----
assert_eq!(vote_state.votes[0].confirmation_count(), 2);
assert_eq!(vote_state.votes[1].slot(), 2);
assert_eq!(vote_state.votes[1].confirmation_count(), 1);
⋮----
fn test_vote_lockout() {
⋮----
vote_state.process_next_vote_slot(i as u64);
⋮----
assert_eq!(vote_state.votes.len(), MAX_LOCKOUT_HISTORY);
assert_eq!(vote_state.root_slot, Some(0));
check_lockouts(&vote_state);
⋮----
assert_eq!(vote.confirmation_count(), expected_count as u32);
⋮----
let top_vote = vote_state.votes.front().unwrap().slot();
let slot = vote_state.last_lockout().unwrap().last_locked_out_slot();
vote_state.process_next_vote_slot(slot);
assert_eq!(Some(top_vote), vote_state.root_slot);
let slot = vote_state.votes.front().unwrap().last_locked_out_slot();
⋮----
fn test_vote_double_lockout_after_expiration() {
⋮----
vote_state.process_next_vote_slot((2 + INITIAL_LOCKOUT + 1) as u64);
⋮----
vote_state.process_next_vote_slot((2 + INITIAL_LOCKOUT + 2) as u64);
⋮----
vote_state.process_next_vote_slot((2 + INITIAL_LOCKOUT + 3) as u64);
⋮----
fn test_expire_multiple_votes() {
⋮----
assert_eq!(vote_state.votes[0].confirmation_count(), 3);
let expire_slot = vote_state.votes[1].slot() + vote_state.votes[1].lockout() + 1;
vote_state.process_next_vote_slot(expire_slot);
⋮----
assert_eq!(vote_state.votes[0].slot(), 0);
assert_eq!(vote_state.votes[1].slot(), expire_slot);
vote_state.process_next_vote_slot(expire_slot + 1);
⋮----
assert_eq!(vote_state.votes[1].confirmation_count(), 2);
assert_eq!(vote_state.votes[2].confirmation_count(), 1);
⋮----
fn test_multiple_root_progress() {
⋮----
vote_state.process_next_vote_slot(MAX_LOCKOUT_HISTORY as u64 + 1);
assert_eq!(vote_state.root_slot, Some(1));
vote_state.process_next_vote_slot(MAX_LOCKOUT_HISTORY as u64 + 2);
assert_eq!(vote_state.root_slot, Some(2));
⋮----
fn test_duplicate_vote() {
⋮----
fn test_vote_state_roots() {
⋮----
root_slot: Some(5),
⋮----
vote_state.process_next_vote_slot(6);
vote_state.process_next_vote_slot(7);
⋮----
assert_eq!(vote_state.votes[0].slot(), 6);
assert_eq!(vote_state.votes[1].slot(), 7);
assert_eq!(vote_state.root_slot, Some(5));
⋮----
vote_state.process_next_vote_slot(i);
⋮----
assert!(vote_state.root_slot.unwrap() > 5);

================
File: core/src/consensus/tower1_14_11.rs
================
pub struct Tower1_14_11 {

================
File: core/src/consensus/tower1_7_14.rs
================
pub struct Tower1_7_14 {
⋮----
pub struct SavedTower1_7_14 {
⋮----
impl SavedTower1_7_14 {
pub fn new<T: Signer>(tower: &Tower1_7_14, keypair: &T) -> Result<Self> {
let node_pubkey = keypair.pubkey();
⋮----
return Err(TowerError::WrongTower(format!(
⋮----
let signature = keypair.sign_message(&data);
Ok(Self {

================
File: core/src/consensus/tree_diff.rs
================
pub trait TreeDiff<'a> {
⋮----
fn subtree_diff(&self, root1: Self::TreeKey, root2: Self::TreeKey) -> HashSet<Self::TreeKey> {
if !self.contains_slot(&root1) {
⋮----
let mut pending_keys = vec![root1];
⋮----
while let Some(current_key) = pending_keys.pop() {
⋮----
.children(&current_key)
.expect("slot was discovered earlier, must exist")
⋮----
pending_keys.push(*child);
⋮----
reachable_set.insert(current_key);

================
File: core/src/consensus/vote_stake_tracker.rs
================
pub struct VoteStakeTracker {
⋮----
impl VoteStakeTracker {
pub fn add_vote_pubkey(
⋮----
let is_new = !self.voted.contains(&vote_pubkey);
⋮----
self.voted.insert(vote_pubkey);
⋮----
.iter()
.map(|threshold| {
⋮----
.collect();
⋮----
(vec![false; thresholds_to_check.len()], is_new)
⋮----
pub fn voted(&self) -> &HashSet<Pubkey> {
⋮----
pub fn stake(&self) -> u64 {
⋮----
mod test {
⋮----
fn test_add_vote_pubkey() {
⋮----
let (is_confirmed_thresholds, is_new) = vote_stake_tracker.add_vote_pubkey(
⋮----
let stake = vote_stake_tracker.stake();
let (is_confirmed_thresholds2, is_new2) = vote_stake_tracker.add_vote_pubkey(
⋮----
let stake2 = vote_stake_tracker.stake();
assert_eq!(stake, stake2);
assert!(!is_confirmed_thresholds2[0]);
assert!(!is_confirmed_thresholds2[1]);
assert!(!is_new2);
assert_eq!(is_confirmed_thresholds.len(), 2);
assert_eq!(is_confirmed_thresholds2.len(), 2);
⋮----
assert!(is_confirmed_thresholds[0]);
⋮----
assert!(!is_confirmed_thresholds[0]);
⋮----
assert!(is_confirmed_thresholds[1]);
⋮----
assert!(!is_confirmed_thresholds[1]);
⋮----
assert!(is_new);

================
File: core/src/forwarding_stage/packet_container.rs
================
pub struct PacketContainer(MinMaxHeap<PacketContainerEntry>);
impl PacketContainer {
pub fn with_capacity(capacity: usize) -> Self {
Self(MinMaxHeap::with_capacity(capacity))
⋮----
pub fn is_empty(&self) -> bool {
self.0.is_empty()
⋮----
pub fn is_full(&self) -> bool {
self.0.len() == self.0.capacity()
⋮----
pub fn min_priority(&self) -> Option<u64> {
self.0.peek_min().map(|entry| entry.priority)
⋮----
pub fn pop_max(&mut self) -> Option<BytesPacket> {
self.0.pop_max().map(|entry| entry.packet)
⋮----
pub fn pop_min(&mut self) -> Option<BytesPacket> {
self.0.pop_min().map(|entry| entry.packet)
⋮----
pub fn insert(&mut self, packet: BytesPacket, priority: u64) {
self.0.push(PacketContainerEntry::new(packet, priority));
⋮----
struct PacketContainerEntry {
⋮----
impl Ord for PacketContainerEntry {
fn cmp(&self, other: &Self) -> std::cmp::Ordering {
self.priority.cmp(&other.priority)
⋮----
impl PartialOrd for PacketContainerEntry {
fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
Some(self.cmp(other))
⋮----
impl PacketContainerEntry {
fn new(packet: BytesPacket, priority: u64) -> Self {
⋮----
mod tests {
⋮----
fn simple_packet_with_flags(packet_flags: PacketFlags) -> BytesPacket {
⋮----
packet.meta_mut().flags = packet_flags;
⋮----
fn test_packet_container_status() {
⋮----
assert!(container.is_empty());
assert!(!container.is_full());
container.insert(simple_packet_with_flags(PacketFlags::empty()), 1);
assert!(!container.is_empty());
⋮----
container.insert(simple_packet_with_flags(PacketFlags::all()), 2);
⋮----
assert!(container.is_full());
⋮----
fn test_packet_container_pop_min() {
⋮----
assert!(container.pop_min().is_none());
⋮----
assert_eq!(
⋮----
fn test_packet_container_pop_max() {
⋮----
assert!(container.pop_max().is_none());

================
File: core/src/proxy/auth.rs
================
pub(crate) struct AuthInterceptor {
⋮----
impl AuthInterceptor {
pub(crate) fn new(access_token: Arc<Mutex<Token>>) -> Self {
⋮----
impl Interceptor for AuthInterceptor {
fn call(&mut self, mut request: Request<()>) -> Result<Request<()>, Status> {
request.metadata_mut().insert(
⋮----
format!("Bearer {}", self.access_token.lock().unwrap().value)
.parse()
.map_err(|_| Status::invalid_argument("Failed to parse authorization token"))?,
⋮----
Ok(request)
⋮----
pub async fn generate_auth_tokens(
⋮----
debug!("generate_auth_challenge");
⋮----
.generate_auth_challenge(GenerateAuthChallengeRequest {
⋮----
pubkey: keypair.pubkey().as_ref().to_vec(),
⋮----
.map_err(|e: Status| {
if e.code() == Code::PermissionDenied {
⋮----
ProxyError::AuthenticationError(e.to_string())
⋮----
let formatted_challenge = format!(
⋮----
.sign_message(formatted_challenge.as_bytes())
.as_ref()
.to_vec();
debug!("formatted_challenge: {formatted_challenge} signed_challenge: {signed_challenge:?}",);
⋮----
.generate_auth_tokens(GenerateAuthTokensRequest {
⋮----
client_pubkey: keypair.pubkey().as_ref().to_vec(),
⋮----
.map_err(|e| ProxyError::AuthenticationError(e.to_string()))?;
let inner = auth_tokens.into_inner();
let access_token = get_validated_token(inner.access_token)?;
let refresh_token = get_validated_token(inner.refresh_token)?;
Ok((access_token, refresh_token))
⋮----
pub async fn maybe_refresh_auth_tokens(
⋮----
.lock()
.unwrap()
⋮----
.map(|ts| ts.seconds as u64)
.unwrap_or_default();
⋮----
let now = Utc::now().timestamp() as u64;
⋮----
access_token_expiry.checked_sub(now).unwrap_or_default() <= refresh_within_s;
⋮----
refresh_token_expiry.checked_sub(now).unwrap_or_default() <= refresh_within_s;
⋮----
let kp = cluster_info.keypair().clone();
let (new_access_token, new_refresh_token) = timeout(
⋮----
generate_auth_tokens(auth_service_client, kp.as_ref()),
⋮----
.map_err(|_| ProxyError::MethodTimeout("generate_auth_tokens".to_string()))?
.map_err(|e| ProxyError::MethodError(e.to_string()))?;
return Ok((Some(new_access_token), Some(new_refresh_token)));
⋮----
let new_access_token = timeout(
⋮----
refresh_access_token(auth_service_client, refresh_token),
⋮----
.map_err(|_| ProxyError::MethodTimeout("refresh_access_token".to_string()))?
⋮----
return Ok((Some(new_access_token), None));
⋮----
Ok((None, None))
⋮----
pub async fn refresh_access_token(
⋮----
.refresh_access_token(RefreshAccessTokenRequest {
refresh_token: refresh_token.value.clone(),
⋮----
get_validated_token(response.into_inner().access_token)
⋮----
fn get_validated_token(maybe_token: Option<Token>) -> crate::proxy::Result<Token> {
⋮----
.ok_or_else(|| ProxyError::BadAuthenticationToken("received a null token".to_string()))?;
if token.expires_at_utc.is_none() {
Err(ProxyError::BadAuthenticationToken(
"expires_at_utc field is null".to_string(),
⋮----
Ok(token)

================
File: core/src/proxy/block_engine_stage.rs
================
struct BlockEngineStageStats {
⋮----
impl BlockEngineStageStats {
pub(crate) fn report(&self) {
datapoint_info!(
⋮----
pub struct BlockBuilderFeeInfo {
⋮----
pub struct BlockEngineConfig {
⋮----
pub struct BlockEngineStage {
⋮----
enum PingError<'a> {
⋮----
impl BlockEngineStage {
⋮----
pub fn new(
⋮----
let block_builder_fee_info = block_builder_fee_info.clone();
⋮----
.name("block-engine-stage".to_string())
.spawn(move || {
⋮----
.enable_all()
.build()
.unwrap();
rt.block_on(Self::start(
⋮----
t_hdls: vec![thread],
⋮----
pub fn join(self) -> thread::Result<()> {
⋮----
t.join()?;
⋮----
Ok(())
⋮----
async fn start(
⋮----
while !exit.load(Ordering::Relaxed) {
⋮----
task::block_in_place(|| block_engine_config.lock().unwrap().clone());
⋮----
shredstream_receiver_address.store(Arc::new(None));
sleep(Self::CONNECTION_BACKOFF).await;
⋮----
ProxyError::AuthenticationPermissionDenied => warn!(
⋮----
datapoint_warn!(
⋮----
async fn connect_auth_and_stream_maybe_autoconfig(
⋮----
if BamConnectionState::from_u8(bam_enabled.load(Ordering::Relaxed))
⋮----
return Ok(());
⋮----
.map_err(|err| Self::map_bam_enabled(bam_enabled, err));
⋮----
.map_err(|err| Self::map_bam_enabled(bam_enabled, err))?
.into_iter()
.min_by_key(|(_url, (_socket, latency_us))| *latency_us)
⋮----
if best_socket.is_some() {
shredstream_receiver_address.store(Arc::new(best_socket));
⋮----
.map_err(|err| Self::map_bam_enabled(bam_enabled, err))
.inspect(|_| {
⋮----
async fn connect_auth_and_stream_autoconfig(
⋮----
.map_err(|err| Self::map_bam_enabled(bam_enabled, err))?;
⋮----
let mut backend_endpoint = endpoint.clone();
let endpoint_count = candidates.len();
⋮----
.sorted_unstable_by_key(|(_endpoint, (_shredstream_socket, latency_us))| *latency_us)
⋮----
info!(
⋮----
backend_endpoint = Self::get_endpoint(block_engine_url.as_str())?;
⋮----
shredstream_receiver_address.store(Arc::new(Some(shredstream_socket)));
⋮----
ProxyError::BamEnabled => return Ok(()),
⋮----
if connect_start.elapsed() > Self::CONNECTION_TIMEOUT * 3 {
return Err(e);
⋮----
return Err(ProxyError::BlockEngineEndpointError(
"autoconfig failed: no endpoints available after ping ranking".to_string(),
⋮----
Err(ProxyError::BlockEngineEndpointError(format!(
⋮----
fn map_bam_enabled(bam_enabled: &Arc<AtomicU8>, err: ProxyError) -> ProxyError {
match BamConnectionState::from_u8(bam_enabled.load(Ordering::Relaxed)) {
⋮----
async fn get_ranked_endpoints(
⋮----
let mut endpoint_discovery = BlockEngineValidatorClient::connect(backend_endpoint.clone())
⋮----
.map_err(|e| ProxyError::BlockEngineConnectionError(Box::new(e)))?;
⋮----
.get_block_engine_endpoints(GetBlockEngineEndpointRequest {})
⋮----
.map_err(|e| ProxyError::BlockEngineRequestError(Box::new(e)))?
.into_inner();
⋮----
if endpoint_latencies.is_empty() {
⋮----
"Block engine configuration failed: no reachable endpoints found".to_owned(),
⋮----
.to_socket_addrs()
.inspect_err(|e| {
⋮----
.ok()
.and_then(|mut shredstream_sockets| shredstream_sockets.next());
return Ok(ahash::HashMap::from_iter([(
⋮----
Ok(endpoint_latencies)
⋮----
async fn connect_auth_and_stream(
⋮----
let keypair = cluster_info.keypair().clone();
debug!("connecting to auth: {}", backend_endpoint.uri());
let auth_channel = timeout(*connection_timeout, backend_endpoint.connect())
⋮----
.map_err(|_| ProxyError::AuthenticationConnectionTimeout)?
.map_err(|e| ProxyError::AuthenticationConnectionError(e.to_string()))
⋮----
debug!("generating authentication token");
let (access_token, refresh_token) = timeout(
⋮----
generate_auth_tokens(&mut auth_client, &keypair),
⋮----
.map_err(|_| ProxyError::AuthenticationTimeout)
.map_err(|err| Self::map_bam_enabled(bam_enabled, err))??;
let backend_url = backend_endpoint.uri().to_string();
⋮----
debug!("connecting to block engine: {}", backend_endpoint.uri());
let block_engine_channel = timeout(*connection_timeout, backend_endpoint.connect())
⋮----
.map_err(|_| ProxyError::BlockEngineConnectionTimeout)?
.map_err(|e| ProxyError::BlockEngineConnectionError(Box::new(e)))
⋮----
AuthInterceptor::new(access_token.clone()),
⋮----
fn get_endpoint(block_engine_url: &str) -> Result<Endpoint, ProxyError> {
let mut backend_endpoint = Endpoint::from_shared(block_engine_url.to_owned())
.map_err(|_| {
ProxyError::BlockEngineEndpointError(format!(
⋮----
.tcp_keepalive(Some(Duration::from_secs(60)));
if block_engine_url.starts_with("https") {
⋮----
.tls_config(tonic::transport::ClientTlsConfig::new())
⋮----
Ok(backend_endpoint)
⋮----
async fn ping<'a>(host: &'a str) -> Result<u64, PingError<'a>> {
⋮----
.arg("-c")
.arg("1") // ping once
.arg("-w")
.arg("2") // don't wait more than 2 secs for a response
.arg(host)
.output()
⋮----
if !output.status.success() {
warn!(
⋮----
return Err(PingError::NonZeroExit(host, output.status.code()));
⋮----
for line in stdout.lines() {
⋮----
.find("time=")
.map(|index| &line[index + "time=".len()..])
.and_then(|rtt_str| rtt_str.find(" ms").map(|index| &rtt_str[..index]))
⋮----
return Ok((rtt * 1000.0).round() as u64);
⋮----
Err(PingError::NoRttFound)
⋮----
async fn ping_and_rank_endpoints(
⋮----
.iter()
.flat_map(|endpoint| std::iter::repeat_n(endpoint, PING_COUNT))
.filter_map(|endpoint| {
⋮----
.ok()?;
let _ = uri.host()?;
Some((endpoint, uri))
⋮----
.collect_vec();
⋮----
.map(|(_endpoint, uri)| Self::ping(uri.host().unwrap())),
⋮----
> = ahash::HashMap::with_capacity(endpoints.len());
⋮----
ping_res.iter().zip(endpoints_to_ping.iter()).for_each(
⋮----
best_endpoint = (Some(endpoint), *latency_us);
⋮----
match agg_endpoints.entry(endpoint.block_engine_url.clone()) {
⋮----
let (_shredstream_socket, best_ping_us) = ent.get_mut();
⋮----
entry.insert((maybe_shredstream_socket, *latency_us));
⋮----
async fn start_consuming_block_engine_bundles_and_packets(
⋮----
let subscribe_packets_stream = timeout(
⋮----
client.subscribe_packets(block_engine::SubscribePacketsRequest {}),
⋮----
.map_err(|_| ProxyError::MethodTimeout("block_engine_subscribe_packets".to_string()))?
.map_err(|e| ProxyError::MethodError(e.to_string()))
⋮----
let subscribe_bundles_stream = timeout(
⋮----
client.subscribe_bundles(block_engine::SubscribeBundlesRequest {}),
⋮----
.map_err(|_| ProxyError::MethodTimeout("subscribe_bundles".to_string()))?
⋮----
let block_builder_info = timeout(
⋮----
client.get_block_builder_fee_info(BlockBuilderFeeInfoRequest {}),
⋮----
.map_err(|_| ProxyError::MethodTimeout("get_block_builder_fee_info".to_string()))?
⋮----
let mut bb_fee = block_builder_fee_info.lock().unwrap();
⋮----
async fn consume_bundle_and_packet_stream(
⋮----
let refresh_within_s: u64 = METRICS_TICK.as_secs().saturating_mul(3).saturating_div(2);
⋮----
let mut metrics_and_auth_tick = interval(METRICS_TICK);
let mut maintenance_tick = interval(MAINTENANCE_TICK);
info!("connected to packet and bundle stream");
⋮----
info!("bam enabled, exiting block engine stage");
⋮----
fn handle_block_engine_bundles(
⋮----
.filter_map(|bundle| {
Some(PacketBundle::new(
⋮----
.map(proto_packet_to_packet)
⋮----
.collect();
⋮----
.add_assign(bundles.len() as u64);
block_engine_stats.num_bundle_packets.add_assign(
⋮----
.map(|bundle| bundle.batch().len() as u64)
⋮----
.send(bundles)
.map_err(|_| ProxyError::PacketForwardError)
⋮----
fn handle_block_engine_packets(
⋮----
if batch.packets.is_empty() {
block_engine_stats.num_empty_packets.add_assign(1);
⋮----
.add_assign(packet_batch.len() as u64);
⋮----
.send(Arc::new(vec![packet_batch]))
.map_err(|_| ProxyError::PacketForwardError)?;
⋮----
.send(packet_batch)
⋮----
pub fn is_valid_block_engine_config(config: &BlockEngineConfig) -> bool {
if config.block_engine_url.is_empty() {
warn!("can't connect to block_engine. missing block_engine_url.");
⋮----
error!("can't connect to block engine. error creating block engine endpoint - {e}");

================
File: core/src/proxy/fetch_stage_manager.rs
================
struct FetchStageState {
⋮----
impl FetchStageState {
fn new() -> Self {
⋮----
fn reset_to_bam_state(&mut self) {
⋮----
fn switch_to_connected_mode(&mut self) {
⋮----
fn switch_to_disconnected_mode(&mut self) {
⋮----
fn set_to_pending_disconnect(&mut self) {
⋮----
fn needs_fallback_reconnect(&self) -> bool {
⋮----
fn should_start_pending_disconnect(&self) -> bool {
⋮----
fn should_disconnect_to_relayer(&self, pending_disconnect_ts: &std::time::Instant) -> bool {
⋮----
&& pending_disconnect_ts.elapsed() > DISCONNECT_DELAY
⋮----
pub struct FetchStageManager {
⋮----
impl FetchStageManager {
pub fn new(
⋮----
fn start(
⋮----
Builder::new().name("fetch-stage-manager".into()).spawn(move || {
⋮----
let heartbeat_tick = tick(HEARTBEAT_TIMEOUT);
let metrics_tick = tick(METRICS_CADENCE);
⋮----
while !exit.load(Ordering::Relaxed) {
if BamConnectionState::from_u8(bam_enabled.load(Ordering::Relaxed))
⋮----
state.reset_to_bam_state();
while packet_intercept_rx.try_recv().is_ok() {}
⋮----
select! {
⋮----
}).unwrap()
⋮----
fn set_tpu_addresses(
⋮----
cluster_info.set_tpu_quic(tpu_address)?;
cluster_info.set_tpu_forwards_quic(tpu_forward_address)?;
Ok(())
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_hdl.join()

================
File: core/src/proxy/mod.rs
================
mod auth;
pub mod block_engine_stage;
pub mod fetch_stage_manager;
pub mod relayer_stage;
⋮----
type Result<T> = result::Result<T, ProxyError>;
type HeartbeatEvent = (SocketAddr, SocketAddr);
⋮----
pub enum ProxyError {

================
File: core/src/proxy/relayer_stage.rs
================
struct RelayerStageStats {
⋮----
impl RelayerStageStats {
pub(crate) fn report(&self) {
datapoint_info!(
⋮----
pub struct RelayerConfig {
⋮----
pub struct RelayerStage {
⋮----
impl RelayerStage {
pub fn new(
⋮----
.name("relayer-stage".to_string())
.spawn(move || {
⋮----
.enable_all()
.build()
.unwrap();
rt.block_on(Self::start(
⋮----
t_hdls: vec![thread],
⋮----
pub fn join(self) -> thread::Result<()> {
⋮----
t.join()?;
⋮----
Ok(())
⋮----
async fn start(
⋮----
while !exit.load(Ordering::Relaxed) {
⋮----
let relayer_config = relayer_config.clone();
task::spawn_blocking(move || relayer_config.lock().unwrap().clone())
⋮----
.expect("Failed to get execute tokio task.")
⋮----
sleep(CONNECTION_BACKOFF).await;
⋮----
warn!(
⋮----
datapoint_warn!(
⋮----
async fn connect_auth_and_stream(
⋮----
let keypair = cluster_info.keypair().clone();
let mut backend_endpoint = Endpoint::from_shared(local_relayer_config.relayer_url.clone())
.map_err(|_| {
ProxyError::RelayerConnectionError(format!(
⋮----
.tcp_keepalive(Some(Duration::from_secs(60)));
if local_relayer_config.relayer_url.starts_with("https") {
⋮----
.tls_config(tonic::transport::ClientTlsConfig::new())
⋮----
"failed to set tls_config for relayer service".to_string(),
⋮----
debug!("connecting to auth: {}", local_relayer_config.relayer_url);
let auth_channel = timeout(*connection_timeout, backend_endpoint.connect())
⋮----
.map_err(|_| ProxyError::AuthenticationConnectionTimeout)?
.map_err(|e| ProxyError::AuthenticationConnectionError(e.to_string()))?;
⋮----
debug!("generating authentication token");
let (access_token, refresh_token) = timeout(
⋮----
generate_auth_tokens(&mut auth_client, &keypair),
⋮----
.map_err(|_| ProxyError::AuthenticationTimeout)??;
⋮----
debug!(
⋮----
let relayer_channel = timeout(*connection_timeout, backend_endpoint.connect())
⋮----
.map_err(|_| ProxyError::RelayerConnectionTimeout)?
.map_err(|e| ProxyError::RelayerConnectionError(e.to_string()))?;
⋮----
AuthInterceptor::new(access_token.clone()),
⋮----
async fn start_consuming_relayer_packets(
⋮----
let tpu_config = timeout(
⋮----
client.get_tpu_configs(relayer::GetTpuConfigsRequest {}),
⋮----
.map_err(|_| ProxyError::MethodTimeout("relayer_get_tpu_configs".to_string()))?
.map_err(|e| ProxyError::MethodError(e.to_string()))?
.into_inner();
⋮----
.ok_or_else(|| ProxyError::MissingTpuSocket("tpu".to_string()))?;
⋮----
.ok_or_else(|| ProxyError::MissingTpuSocket("tpu_fwd".to_string()))?;
⋮----
let packet_stream = timeout(
⋮----
client.subscribe_packets(relayer::SubscribePacketsRequest {}),
⋮----
.map_err(|_| ProxyError::MethodTimeout("relayer_subscribe_packets".to_string()))?
⋮----
async fn consume_packet_stream(
⋮----
let refresh_within_s: u64 = METRICS_TICK.as_secs().saturating_mul(3).saturating_div(2);
⋮----
let mut metrics_and_auth_tick = interval(METRICS_TICK);
⋮----
let mut heartbeat_check_interval = interval(local_config.expected_heartbeat_interval);
⋮----
info!("connected to packet stream");
⋮----
fn handle_relayer_packets(
⋮----
relayer_stats.num_empty_messages.add_assign(1);
⋮----
if proto_batch.packets.is_empty() {
⋮----
return Ok(());
⋮----
.into_iter()
.map(proto_packet_to_packet)
⋮----
.add_assign(packet_batch.len() as u64);
⋮----
.send(packet_batch)
.map_err(|_| ProxyError::PacketForwardError)?;
⋮----
relayer_stats.num_heartbeats.add_assign(1);
⋮----
.send(heartbeat_event)
.map_err(|_| ProxyError::HeartbeatChannelError)?;
⋮----
pub fn is_valid_relayer_config(config: &RelayerConfig) -> bool {
if config.relayer_url.is_empty() {
warn!("can't connect to relayer. missing relayer_url.");
⋮----
if config.oldest_allowed_heartbeat.is_zero() {
error!("can't connect to relayer. oldest allowed heartbeat must be greater than 0.");
⋮----
if config.expected_heartbeat_interval.is_zero() {
error!("can't connect to relayer. expected heartbeat interval must be greater than 0.");
⋮----
error!("can't connect to relayer. error creating relayer endpoint - {e}");

================
File: core/src/repair/ancestor_hashes_service.rs
================
pub enum AncestorHashesReplayUpdate {
⋮----
impl AncestorHashesReplayUpdate {
fn slot(&self) -> Slot {
⋮----
pub type AncestorHashesReplayUpdateSender = Sender<AncestorHashesReplayUpdate>;
pub type AncestorHashesReplayUpdateReceiver = Receiver<AncestorHashesReplayUpdate>;
type RetryableSlotsSender = Sender<(Slot, AncestorRequestType)>;
type RetryableSlotsReceiver = Receiver<(Slot, AncestorRequestType)>;
type OutstandingAncestorHashesRepairs = OutstandingRequests<AncestorHashesRepairType>;
⋮----
struct AncestorHashesResponsesStats {
⋮----
impl AncestorHashesResponsesStats {
fn report(&mut self) {
datapoint_info!(
⋮----
pub struct AncestorRepairRequestsStats {
⋮----
impl Default for AncestorRepairRequestsStats {
fn default() -> Self {
⋮----
impl AncestorRepairRequestsStats {
⋮----
.iter()
.map(|(slot, slot_repairs)| (slot, slot_repairs.pubkey_repairs().values().sum::<u64>()))
.collect();
⋮----
if self.last_report.elapsed().as_secs() > 2 && repair_total > 0 {
info!("ancestor_repair_requests_stats: {slot_to_count:?}");
⋮----
pub struct AncestorHashesChannels {
⋮----
pub struct AncestorHashesService {
⋮----
impl AncestorHashesService {
pub fn new(
⋮----
let (response_sender, response_receiver) = unbounded();
⋮----
"solRcvrAncHash".to_string(),
ancestor_hashes_request_socket.clone(),
exit.clone(),
response_sender.clone(),
⋮----
Some(Duration::from_millis(1)),
⋮----
let exit = exit.clone();
⋮----
.name(String::from("solAncHashQuic"))
.spawn(|| {
receive_quic_datagrams(
⋮----
.unwrap()
⋮----
let (retryable_slots_sender, retryable_slots_receiver) = unbounded();
⋮----
ancestor_hashes_request_statuses.clone(),
⋮----
blockstore.clone(),
outstanding_requests.clone(),
⋮----
repair_info.ancestor_duplicate_slots_sender.clone(),
⋮----
repair_info.cluster_info.clone(),
⋮----
thread_hdls: vec![
⋮----
pub(crate) fn join(self) -> thread::Result<()> {
self.thread_hdls.into_iter().try_for_each(JoinHandle::join)
⋮----
fn run_responses_listener(
⋮----
.name("solAncHashesSvc".to_string())
.spawn(move || {
⋮----
while !exit.load(Ordering::Relaxed) {
let keypair = cluster_info.keypair();
⋮----
info!("ancestors hashes responses listener disconnected");
⋮----
if last_stats_report.elapsed().as_secs() > 2 {
stats.report();
⋮----
fn process_new_packets_from_channel(
⋮----
let mut packet_batches = vec![response_receiver.recv_timeout(timeout)?];
let mut total_packets = packet_batches[0].len();
⋮----
while let Ok(batch) = response_receiver.try_recv() {
total_packets += batch.len();
if packet_threshold.should_drop(total_packets) {
dropped_packets += batch.len();
⋮----
packet_batches.push(batch);
⋮----
packet_threshold.update(total_packets, timer.elapsed());
Ok(())
⋮----
fn process_packet_batch(
⋮----
packet_batch.iter().for_each(|packet| {
⋮----
fn verify_and_process_ancestor_response<'a, P>(
⋮----
let packet = packet.into();
let from_addr = packet.meta().socket_addr();
let Some(packet_data) = packet.data(..) else {
⋮----
let Ok(response) = deserialize_from_with_limit(&mut cursor) else {
⋮----
let Ok(nonce) = deserialize_from_with_limit(&mut cursor) else {
⋮----
if cursor.bytes().next().is_some() {
⋮----
let request_slot = outstanding_requests.write().unwrap().register_response(
⋮----
timestamp(),
⋮----
if request_slot.is_none() {
⋮----
let request_slot = request_slot.unwrap();
⋮----
ancestor_hashes_request_statuses.entry(request_slot)
⋮----
let decision = ancestor_hashes_status_ref.get_mut().add_response(
⋮----
hashes.clone(),
⋮----
let request_type = ancestor_hashes_status_ref.get().request_type();
if decision.is_some() {
ancestor_hashes_status_ref.remove();
⋮----
decision.map(|decision| AncestorRequestDecision {
⋮----
if !ping.verify() {
⋮----
let _ = ancestor_socket.send_to(&pong, from_addr);
⋮----
fn handle_ancestor_request_decision(
⋮----
if ancestor_request_decision.is_retryable() {
let _ = retryable_slots_sender.send((
⋮----
let potential_slot_to_repair = ancestor_request_decision.slot_to_repair();
⋮----
let _ = ancestor_duplicate_slots_sender.send(slot_to_repair);
⋮----
fn process_replay_updates(
⋮----
for update in ancestor_hashes_replay_update_receiver.try_iter() {
let slot = update.slot();
if slot <= root_slot || ancestor_hashes_request_statuses.contains_key(&slot) {
⋮----
if repairable_dead_slot_pool.contains(&dead_slot) {
⋮----
} else if popular_pruned_slot_pool.contains(&dead_slot) {
popular_pruned_slot_pool.remove(&dead_slot);
repairable_dead_slot_pool.insert(dead_slot);
⋮----
dead_slot_pool.insert(dead_slot);
⋮----
dead_slot_pool.remove(&dead_slot);
⋮----
if dead_slot_pool.contains(&pruned_slot) {
info!(
⋮----
dead_slot_pool.remove(&pruned_slot);
repairable_dead_slot_pool.insert(pruned_slot);
} else if repairable_dead_slot_pool.contains(&pruned_slot) {
⋮----
popular_pruned_slot_pool.insert(pruned_slot);
⋮----
fn run_manage_ancestor_requests(
⋮----
repair_info.bank_forks.read().unwrap().sharable_banks(),
repair_info.repair_whitelist.clone(),
⋮----
let mut request_throttle = vec![];
⋮----
.name("solManAncReqs".to_string())
.spawn(move || loop {
if exit.load(Ordering::Relaxed) {
⋮----
sleep(Duration::from_millis(DEFAULT_MS_PER_SLOT));
⋮----
fn manage_ancestor_requests(
⋮----
let root_bank = repair_info.bank_forks.read().unwrap().root_bank();
let cluster_type = root_bank.cluster_type();
for (slot, request_type) in retryable_slots_receiver.try_iter() {
datapoint_info!("ancestor-repair-retry", ("slot", slot, i64));
if request_type.is_pruned() {
popular_pruned_slot_pool.insert(slot);
⋮----
repairable_dead_slot_pool.insert(slot);
⋮----
root_bank.slot(),
⋮----
dead_slot_pool.retain(|slot| *slot > root_bank.slot());
repairable_dead_slot_pool.retain(|slot| *slot > root_bank.slot());
popular_pruned_slot_pool.retain(|slot| *slot > root_bank.slot());
ancestor_hashes_request_statuses.retain(|slot, status| {
if *slot <= root_bank.slot() {
⋮----
} else if status.is_expired() {
if status.request_type().is_pruned() {
popular_pruned_slot_pool.insert(*slot);
⋮----
repairable_dead_slot_pool.insert(*slot);
⋮----
request_throttle.retain(|request_time| *request_time > (timestamp() - 1000));
let identity_keypair: &Keypair = &repair_info.cluster_info.keypair();
⋮----
MAX_ANCESTOR_HASHES_SLOT_REQUESTS_PER_SECOND.saturating_sub(request_throttle.len());
⋮----
.copied()
.zip(std::iter::repeat(
⋮----
.chain(
⋮----
.zip(std::iter::repeat(AncestorRequestType::PopularPruned)),
⋮----
.into_iter();
for (slot, request_type) in potential_slot_requests.take(number_of_allowed_requests) {
warn!(
⋮----
request_throttle.push(timestamp());
⋮----
popular_pruned_slot_pool.take(&slot).unwrap();
⋮----
repairable_dead_slot_pool.take(&slot).unwrap();
⋮----
repair_stats.report();
⋮----
fn find_epoch_slots_frozen_dead_slots(
⋮----
dead_slot_pool.retain(|dead_slot| {
let epoch = root_bank.get_epoch_and_slot_index(*dead_slot).0;
if let Some(_epoch_stakes) = root_bank.epoch_stakes(epoch) {
let status = cluster_slots.lookup(*dead_slot);
⋮----
let total_stake = completed_dead_slot_supporters.total_stake();
if completed_dead_slot_supporters.total_support() as f64 / total_stake as f64
⋮----
repairable_dead_slot_pool.insert(*dead_slot);
⋮----
fn initiate_ancestor_hashes_requests_for_duplicate_slot(
⋮----
let Ok(sampled_validators) = serve_repair.repair_request_ancestor_hashes_sample_peers(
⋮----
&identity_keypair.pubkey(),
⋮----
.update(pubkey, duplicate_slot, 0);
let ancestor_hashes_repair_type = AncestorHashesRepairType(duplicate_slot);
⋮----
.write()
⋮----
.add_request(ancestor_hashes_repair_type, timestamp());
let Ok(request_bytes) = serve_repair.ancestor_repair_request_bytes(
⋮----
let _ = ancestor_hashes_request_socket.send_to(&request_bytes, socket_addr);
⋮----
.blocking_send((*socket_addr, Bytes::from(request_bytes)))
.is_err()
⋮----
.into_iter()
.map(|(_pk, socket_addr)| socket_addr),
⋮----
assert!(ancestor_hashes_request_statuses
⋮----
mod test {
⋮----
pub fn test_ancestor_hashes_service_process_replay_updates() {
⋮----
unbounded();
⋮----
.send(AncestorHashesReplayUpdate::Dead(slot))
.unwrap();
⋮----
assert!(dead_slot_pool.contains(&slot));
assert!(repairable_dead_slot_pool.is_empty());
assert!(popular_pruned_slot_pool.is_empty());
⋮----
.send(AncestorHashesReplayUpdate::DeadDuplicateConfirmed(slot))
⋮----
assert!(dead_slot_pool.is_empty());
assert!(repairable_dead_slot_pool.contains(&slot));
⋮----
ancestor_hashes_request_statuses.insert(slot, AncestorRequestStatus::default());
dead_slot_pool.clear();
repairable_dead_slot_pool.clear();
popular_pruned_slot_pool.clear();
⋮----
.send(AncestorHashesReplayUpdate::PopularPrunedFork(slot))
⋮----
ancestor_hashes_request_statuses.insert(
⋮----
ancestor_hashes_request_statuses.clear();
⋮----
.send(AncestorHashesReplayUpdate::Dead(root_slot - 1))
⋮----
.send(AncestorHashesReplayUpdate::DeadDuplicateConfirmed(
⋮----
pub fn test_ancestor_hashes_service_process_pruned_replay_updates() {
⋮----
assert!(popular_pruned_slot_pool.contains(&slot));
⋮----
dead_slot_pool.insert(slot);
⋮----
fn test_ancestor_hashes_service_find_epoch_slots_frozen_dead_slots() {
⋮----
let root_bank = vote_simulator.bank_forks.read().unwrap().root_bank();
⋮----
assert_eq!(dead_slot_pool.len(), 1);
assert!(dead_slot_pool.contains(&dead_slot));
⋮----
let max_epoch = root_bank.epoch_stakes_map().keys().max().unwrap();
⋮----
.epoch_schedule()
.get_last_slot_in_epoch(*max_epoch)
⋮----
dead_slot_pool.insert(slot_outside_known_epochs);
⋮----
.zip(vote_simulator.node_pubkeys.iter())
.map(|(_i, pk)| (*pk, 42))
⋮----
cluster_slots.fake_epoch_info_for_tests(validator_stakes);
for (i, key) in (0..2).zip(vote_simulator.node_pubkeys.iter()) {
cluster_slots.insert_node_id(dead_slot, *key);
⋮----
assert_eq!(repairable_dead_slot_pool.len(), 1);
assert!(repairable_dead_slot_pool.contains(&dead_slot));
⋮----
struct ResponderThreads {
⋮----
impl ResponderThreads {
fn shutdown(self) {
self.exit.store(true, Ordering::Relaxed);
self.t_request_receiver.join().unwrap();
self.t_listen.join().unwrap();
self.t_packet_adapter.join().unwrap();
⋮----
fn new(slot_to_query: Slot) -> Self {
assert!(slot_to_query >= MAX_ANCESTOR_RESPONSES as Slot);
⋮----
let responder_node = Node::new_localhost_with_pubkey(&keypair.pubkey());
⋮----
responder_node.info.clone(),
⋮----
let ledger_path = get_tmp_ledger_path!();
let blockstore = Arc::new(Blockstore::open(&ledger_path).unwrap());
⋮----
vote_simulator.bank_forks.read().unwrap().sharable_banks(),
⋮----
Box::new(StandardRepairHandler::new(blockstore.clone())),
⋮----
let (requests_sender, requests_receiver) = unbounded();
⋮----
let (shreds, _) = make_many_slot_entries(
⋮----
.insert_shreds(shreds, None, false)
.expect("Expect successful ledger write");
⋮----
correct_bank_hashes.insert(duplicate_confirmed_slot, hash);
blockstore.insert_bank_hash(duplicate_confirmed_slot, hash, true);
⋮----
"solRcvrTest".to_string(),
⋮----
let (remote_request_sender, remote_request_receiver) = unbounded();
⋮----
.spawn(|| adapt_repair_requests_packets(requests_receiver, remote_request_sender))
⋮----
let t_listen = responder_serve_repair.listen(
⋮----
struct ManageAncestorHashesState {
⋮----
impl ManageAncestorHashesState {
fn new(bank_forks: Arc<RwLock<BankForks>>) -> Self {
⋮----
Arc::new(bind_to_localhost_unique().expect("should bind"));
⋮----
.read()
⋮----
.root_bank()
⋮----
.clone();
⋮----
Node::new_localhost_with_pubkey(&keypair.pubkey()).info,
⋮----
requester_cluster_info.clone(),
bank_forks.read().unwrap().sharable_banks(),
repair_whitelist.clone(),
⋮----
let (ancestor_duplicate_slots_sender, _ancestor_duplicate_slots_receiver) = unbounded();
⋮----
request_throttle: vec![],
⋮----
fn setup_dead_slot(
⋮----
assert!(dead_slot >= MAX_ANCESTOR_RESPONSES as Slot);
let mut forks = tr(0);
⋮----
forks.push_front(tr(slot));
⋮----
let mut replay_blockstore_components = replay_blockstore_components(Some(forks), 1, None);
⋮----
vote_simulator.fill_bank_forks(
tr(dead_slot - 1) / tr(dead_slot),
⋮----
let (shreds, _) = make_many_slot_entries(dead_slot, dead_slot, 5);
⋮----
.get(&duplicate_confirmed_slot)
.cloned()
.unwrap_or_else(Hash::new_unique);
blockstore.insert_bank_hash(duplicate_confirmed_slot, bank_hash, true);
⋮----
blockstore.insert_bank_hash(dead_slot - 1, Hash::new_unique(), false);
blockstore.set_dead_slot(dead_slot).unwrap();
⋮----
fn send_ancestor_repair_request(
⋮----
let request_bytes = requester_serve_repair.ancestor_repair_request_bytes(
&requester_cluster_info.keypair(),
responder_info.pubkey(),
⋮----
let socket = responder_info.serve_repair(Protocol::UDP).unwrap();
let _ = ancestor_hashes_request_socket.send_to(&request_bytes, socket);
⋮----
fn test_ancestor_hashes_service_initiate_ancestor_hashes_requests_for_duplicate_slot() {
⋮----
} = setup_dead_slot(dead_slot, correct_bank_hashes);
⋮----
assert!(ancestor_hashes_request_statuses.is_empty());
send_ancestor_repair_request(
⋮----
.recv_timeout(Duration::from_millis(1_000))
⋮----
let packet = &mut response_packet.first_mut().unwrap();
⋮----
.meta_mut()
.set_socket_addr(&responder_info.serve_repair(Protocol::UDP).unwrap());
⋮----
packet.as_ref(),
⋮----
assert_eq!(decision, None);
let responder_id = *responder_info.pubkey();
⋮----
cluster_slots.insert_node_id(dead_slot, responder_id);
requester_cluster_info.insert_info(responder_info.clone());
⋮----
assert_eq!(ancestor_hashes_request_statuses.len(), 1);
assert!(ancestor_hashes_request_statuses.contains_key(&dead_slot));
⋮----
.recv_timeout(Duration::from_millis(10_000))
⋮----
assert_eq!(slot, dead_slot);
assert_eq!(
⋮----
assert_matches!(
⋮----
responder_threads.shutdown();
⋮----
fn test_ancestor_hashes_service_manage_ancestor_requests() {
⋮----
cluster_info.insert_info(responder_node.info);
bank_forks.read().unwrap().root_bank().epoch_schedule();
⋮----
.send(AncestorHashesReplayUpdate::Dead(dead_slot))
⋮----
.send(AncestorHashesReplayUpdate::PopularPrunedFork(
⋮----
assert_eq!(ancestor_hashes_request_statuses.len(), 2);
assert!(ancestor_hashes_request_statuses.contains_key(&dead_duplicate_confirmed_slot));
assert!(ancestor_hashes_request_statuses.contains_key(&popular_pruned_slot));
⋮----
.get_mut(&dead_duplicate_confirmed_slot)
⋮----
.value_mut()
.make_expired();
⋮----
.get_mut(&popular_pruned_slot)
⋮----
request_throttle.resize(MAX_ANCESTOR_HASHES_SLOT_REQUESTS_PER_SECOND, u64::MAX);
⋮----
assert!(repairable_dead_slot_pool.contains(&dead_duplicate_confirmed_slot));
assert_eq!(popular_pruned_slot_pool.len(), 1);
assert!(popular_pruned_slot_pool.contains(&popular_pruned_slot));
⋮----
request_throttle.clear();
request_throttle.resize(
⋮----
timestamp() - 1001,
⋮----
assert!(repairable_dead_slot_pool.contains(&dead_duplicate_confirmed_slot_2));
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
⋮----
new_root_bank.freeze();
⋮----
let mut w_bank_forks = bank_forks.write().unwrap();
w_bank_forks.insert(new_root_bank);
w_bank_forks.set_root(new_root_slot, None, None);
⋮----
popular_pruned_slot_pool.insert(dead_duplicate_confirmed_slot);
assert!(!dead_slot_pool.is_empty());
assert!(!repairable_dead_slot_pool.is_empty());
assert!(!popular_pruned_slot_pool.is_empty());
assert!(!ancestor_hashes_request_statuses.is_empty());
⋮----
fn test_verify_and_process_ancestor_responses_invalid_packet() {
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
packet.meta_mut().size = 0;
assert!(AncestorHashesService::verify_and_process_ancestor_response(
⋮----
fn test_ancestor_hashes_service_manage_ancestor_hashes_after_replay_dump() {
⋮----
} = ManageAncestorHashesState::new(bank_forks.clone());
⋮----
let (dumped_slots_sender, _dumped_slots_receiver) = unbounded();
⋮----
duplicate_slots_to_repair.insert(dead_slot, Hash::new_unique());
⋮----
&mut bank_forks.read().unwrap().ancestors(),
&mut bank_forks.read().unwrap().descendants(),
⋮----
fn test_ancestor_hashes_service_retryable_duplicate_ancestor_decision() {
⋮----
assert!(decision.is_retryable());
⋮----
decision: decision.clone(),
⋮----
assert!(repairable_dead_slot_pool.contains(&request_slot));
⋮----
assert!(popular_pruned_slot_pool.contains(&request_slot));

================
File: core/src/repair/cluster_slot_state_verifier.rs
================
use qualifier_attr::qualifiers;
⋮----
pub(crate) type DuplicateSlotsTracker = BTreeSet<Slot>;
pub(crate) type DuplicateSlotsToRepair = HashMap<Slot, Hash>;
pub(crate) type PurgeRepairSlotCounter = BTreeMap<Slot, usize>;
⋮----
pub(crate) type EpochSlotsFrozenSlots = BTreeMap<Slot, Hash>;
⋮----
pub(crate) type DuplicateConfirmedSlots = BTreeMap<Slot, Hash>;
⋮----
pub enum ClusterConfirmedHash {
⋮----
pub enum BankStatus {
⋮----
impl BankStatus {
pub fn new(is_dead: impl Fn() -> bool, get_hash: impl Fn() -> Option<Hash>) -> Self {
if is_dead() {
⋮----
Self::new_from_hash(get_hash())
⋮----
fn new_dead() -> Self {
⋮----
fn new_from_hash(hash: Option<Hash>) -> Self {
⋮----
fn bank_hash(&self) -> Option<Hash> {
⋮----
BankStatus::Frozen(hash) => Some(*hash),
⋮----
fn is_dead(&self) -> bool {
⋮----
fn can_be_further_replayed(&self) -> bool {
⋮----
pub struct DeadState {
⋮----
impl DeadState {
pub fn new_from_state(
⋮----
let cluster_confirmed_hash = get_cluster_confirmed_hash_from_state(
⋮----
let is_slot_duplicate = duplicate_slots_tracker.contains(&slot);
⋮----
fn new(cluster_confirmed_hash: Option<ClusterConfirmedHash>, is_slot_duplicate: bool) -> Self {
⋮----
pub struct BankFrozenState {
⋮----
impl BankFrozenState {
⋮----
Some(frozen_hash),
⋮----
fn new(
⋮----
assert!(frozen_hash != Hash::default());
⋮----
pub struct DuplicateConfirmedState {
⋮----
impl DuplicateConfirmedState {
⋮----
fn new(duplicate_confirmed_hash: Hash, bank_status: BankStatus) -> Self {
⋮----
pub struct DuplicateState {
⋮----
impl DuplicateState {
⋮----
let duplicate_confirmed_hash = get_duplicate_confirmed_hash_from_state(
⋮----
bank_status.bank_hash(),
⋮----
fn new(duplicate_confirmed_hash: Option<Hash>, bank_status: BankStatus) -> Self {
⋮----
pub struct EpochSlotsFrozenState {
⋮----
impl EpochSlotsFrozenState {
⋮----
fn is_popular_pruned(&self) -> bool {
⋮----
pub enum SlotStateUpdate {
⋮----
impl SlotStateUpdate {
fn into_state_changes(self, slot: Slot) -> Vec<ResultingStateChange> {
if self.can_be_further_replayed() {
return vec![];
⋮----
SlotStateUpdate::Dead(dead_state) => on_dead_slot(slot, dead_state),
⋮----
on_frozen_slot(slot, bank_frozen_state)
⋮----
on_duplicate_confirmed(slot, duplicate_confirmed_state)
⋮----
SlotStateUpdate::Duplicate(duplicate_state) => on_duplicate(duplicate_state),
⋮----
on_epoch_slots_frozen(slot, epoch_slots_frozen_state)
⋮----
SlotStateUpdate::PopularPrunedFork => on_popular_pruned_fork(slot),
⋮----
.can_be_further_replayed()
⋮----
duplicate_state.bank_status.can_be_further_replayed()
⋮----
&& !epoch_slots_frozen_state.is_popular_pruned()
⋮----
pub enum ResultingStateChange {
⋮----
fn check_duplicate_confirmed_hash_against_bank_status(
⋮----
warn!(
⋮----
state_changes.push(ResultingStateChange::RepairDuplicateConfirmedVersion(
⋮----
state_changes.push(ResultingStateChange::DuplicateConfirmedSlotMatchesCluster(
⋮----
state_changes.push(ResultingStateChange::MarkSlotDuplicate(bank_frozen_hash));
⋮----
fn check_epoch_slots_hash_against_bank_status(
⋮----
assert!(is_popular_pruned);
⋮----
fn on_dead_slot(slot: Slot, dead_state: DeadState) -> Vec<ResultingStateChange> {
⋮----
let mut state_changes = vec![];
⋮----
state_changes.push(ResultingStateChange::SendAncestorHashesReplayUpdate(
⋮----
check_duplicate_confirmed_hash_against_bank_status(
⋮----
check_epoch_slots_hash_against_bank_status(
⋮----
fn on_frozen_slot(slot: Slot, bank_frozen_state: BankFrozenState) -> Vec<ResultingStateChange> {
⋮----
let mut state_changes = vec![ResultingStateChange::BankFrozen(frozen_hash)];
⋮----
state_changes.push(ResultingStateChange::MarkSlotDuplicate(frozen_hash));
⋮----
fn on_duplicate_confirmed(
⋮----
if bank_status.is_dead() {
⋮----
fn on_duplicate(duplicate_state: DuplicateState) -> Vec<ResultingStateChange> {
⋮----
if duplicate_confirmed_hash.is_none() {
if let Some(bank_hash) = bank_status.bank_hash() {
return vec![ResultingStateChange::MarkSlotDuplicate(bank_hash)];
⋮----
vec![]
⋮----
fn on_epoch_slots_frozen(
⋮----
fn on_popular_pruned_fork(slot: Slot) -> Vec<ResultingStateChange> {
⋮----
vec![ResultingStateChange::SendAncestorHashesReplayUpdate(
⋮----
fn get_cluster_confirmed_hash_from_state(
⋮----
let duplicate_confirmed_hash = duplicate_confirmed_slots.get(&slot).cloned();
⋮----
.is_duplicate_confirmed(&(slot, bank_frozen_hash))
.unwrap_or(false)
⋮----
get_duplicate_confirmed_hash(
⋮----
.map(ClusterConfirmedHash::DuplicateConfirmed)
.or_else(|| {
⋮----
.get(&slot)
.map(|hash| ClusterConfirmedHash::EpochSlotsFrozen(*hash))
⋮----
fn get_duplicate_confirmed_hash_from_state(
⋮----
fn get_duplicate_confirmed_hash(
⋮----
let bank_frozen_hash = bank_frozen_hash.unwrap();
Some(bank_frozen_hash)
⋮----
error!(
⋮----
Some(local_duplicate_confirmed_hash)
⋮----
(Some(bank_frozen_hash), None) => Some(bank_frozen_hash),
⋮----
fn apply_state_changes(
⋮----
.expect("frozen bank must exist in fork choice")
⋮----
not_duplicate_confirmed_frozen_hash = Some(bank_frozen_hash);
⋮----
fork_choice.mark_fork_invalid_candidate(&(slot, bank_frozen_hash));
⋮----
duplicate_slots_to_repair.insert(slot, duplicate_confirmed_hash);
⋮----
fork_choice.mark_fork_valid_candidate(&(slot, bank_frozen_hash));
⋮----
.set_duplicate_confirmed_slots_and_hashes(
new_duplicate_confirmed_slot_hashes.into_iter(),
⋮----
.unwrap();
duplicate_slots_to_repair.remove(&slot);
purge_repair_slot_counter.remove(&slot);
⋮----
let _ = ancestor_hashes_replay_update_sender.send(ancestor_hashes_replay_update);
⋮----
blockstore.insert_bank_hash(slot, frozen_hash, false);
⋮----
pub(crate) fn check_slot_agrees_with_cluster(
⋮----
info!(
⋮----
if !duplicate_slots_tracker.insert(slot) {
⋮----
datapoint_info!(
⋮----
if let Some(bank_hash) = state.bank_status.bank_hash() {
if let Some(true) = fork_choice.is_duplicate_confirmed(&(slot, bank_hash)) {
⋮----
epoch_slots_frozen_slots.insert(slot, epoch_slots_frozen_state.epoch_slots_frozen_hash)
⋮----
let state_changes = slot_state_update.into_state_changes(slot);
apply_state_changes(
⋮----
mod test {
⋮----
macro_rules! state_update_tests {
⋮----
state_update_tests! {
⋮----
struct InitialState {
⋮----
fn setup() -> InitialState {
let forks = tr(0) / (tr(1) / (tr(2) / tr(3)));
let (vote_simulator, blockstore) = setup_forks_from_tree(forks, 1, None);
let descendants = vote_simulator.bank_forks.read().unwrap().descendants();
⋮----
fn test_apply_state_changes() {
⋮----
} = setup();
⋮----
let duplicate_slot = bank_forks.read().unwrap().root() + 1;
⋮----
.read()
.unwrap()
.get(duplicate_slot)
⋮----
.hash();
⋮----
unbounded();
⋮----
vec![ResultingStateChange::MarkSlotDuplicate(duplicate_slot_hash)],
⋮----
assert!(!heaviest_subtree_fork_choice
⋮----
.get(&duplicate_slot)
⋮----
.iter()
.chain(std::iter::once(&duplicate_slot))
⋮----
assert_eq!(
⋮----
assert!(duplicate_slots_to_repair.is_empty());
assert!(purge_repair_slot_counter.is_empty());
⋮----
vec![ResultingStateChange::RepairDuplicateConfirmedVersion(
⋮----
assert_eq!(duplicate_slots_to_repair.len(), 1);
⋮----
fn test_apply_state_changes_bank_frozen() {
⋮----
assert!(blockstore.get_bank_hash(duplicate_slot).is_none());
⋮----
vec![ResultingStateChange::BankFrozen(duplicate_slot_hash)],
⋮----
assert!(!blockstore.is_duplicate_confirmed(duplicate_slot));
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
(root_bank.slot(), root_bank.hash())
⋮----
.add_new_leaf_slot((duplicate_slot, new_bank_hash), Some(root_slot_hash));
⋮----
vec![ResultingStateChange::BankFrozen(new_bank_hash)],
⋮----
fn run_test_apply_state_changes_duplicate_confirmed_matches_frozen(
⋮----
duplicate_slots_to_repair.insert(duplicate_slot, Hash::new_unique());
purge_repair_slot_counter.insert(duplicate_slot, 1);
⋮----
let mut state_changes = vec![ResultingStateChange::DuplicateConfirmedSlotMatchesCluster(
⋮----
modify_state_changes(our_duplicate_slot_hash, &mut state_changes);
⋮----
assert!(heaviest_subtree_fork_choice
⋮----
assert!(blockstore.is_duplicate_confirmed(duplicate_slot));
⋮----
fn test_apply_state_changes_duplicate_confirmed_matches_frozen() {
run_test_apply_state_changes_duplicate_confirmed_matches_frozen(
⋮----
fn test_apply_state_changes_bank_frozen_and_duplicate_confirmed_matches_frozen() {
⋮----
state_changes.push(ResultingStateChange::BankFrozen(our_duplicate_slot_hash));
⋮----
fn run_test_state_duplicate_then_bank_frozen(initial_bank_hash: Option<Hash>) {
⋮----
|| progress.is_dead(duplicate_slot).unwrap_or(false),
⋮----
check_slot_agrees_with_cluster(
⋮----
assert!(duplicate_slots_tracker.contains(&duplicate_slot));
⋮----
let slot_hash = bank_forks.read().unwrap().get(slot).unwrap().hash();
⋮----
let r_bank_forks = bank_forks.read().unwrap();
let parent_bank = r_bank_forks.get(duplicate_slot).unwrap().parent().unwrap();
(parent_bank.slot(), parent_bank.hash())
⋮----
fn test_state_unfrozen_bank_duplicate_then_bank_frozen() {
run_test_state_duplicate_then_bank_frozen(Some(Hash::default()));
⋮----
fn test_state_unreplayed_bank_duplicate_then_bank_frozen() {
run_test_state_duplicate_then_bank_frozen(None);
⋮----
fn test_state_ancestor_confirmed_descendant_duplicate() {
⋮----
let slot3_hash = bank_forks.read().unwrap().get(3).unwrap().hash();
⋮----
let slot2_hash = bank_forks.read().unwrap().get(2).unwrap().hash();
duplicate_confirmed_slots.insert(2, slot2_hash);
⋮----
|| progress.is_dead(2).unwrap_or(false),
|| Some(slot2_hash),
⋮----
|| progress.is_dead(3).unwrap_or(false),
|| Some(slot3_hash),
⋮----
assert!(duplicate_slots_tracker.contains(&3));
⋮----
fn test_state_ancestor_duplicate_descendant_confirmed() {
⋮----
assert!(duplicate_slots_tracker.contains(&2));
⋮----
let slot1_hash = bank_forks.read().unwrap().get(1).unwrap().hash();
⋮----
duplicate_confirmed_slots.insert(3, slot3_hash);
⋮----
fn verify_all_slots_duplicate_confirmed(
⋮----
fn test_state_descendant_confirmed_ancestor_duplicate() {
⋮----
verify_all_slots_duplicate_confirmed(&bank_forks, &heaviest_subtree_fork_choice, 3, true);
⋮----
|| progress.is_dead(1).unwrap_or(false),
|| Some(slot1_hash),
⋮----
assert!(duplicate_slots_tracker.contains(&1));
⋮----
fn test_duplicate_confirmed_and_epoch_slots_frozen() {
⋮----
verify_all_slots_duplicate_confirmed(
⋮----
assert_eq!(*epoch_slots_frozen_slots.get(&3).unwrap(), slot3_hash);
⋮----
fn test_duplicate_confirmed_and_epoch_slots_frozen_mismatched() {
⋮----
assert_eq!(*duplicate_slots_to_repair.get(&3).unwrap(), mismatched_hash);
⋮----
assert_eq!(*epoch_slots_frozen_slots.get(&3).unwrap(), mismatched_hash);

================
File: core/src/repair/duplicate_repair_status.rs
================
pub fn get_ancestor_hash_repair_sample_size() -> usize {
ANCESTOR_HASH_REPAIR_SAMPLE_SIZE.load(Ordering::Relaxed)
⋮----
pub fn set_ancestor_hash_repair_sample_size_for_tests_only(sample_size: usize) {
ANCESTOR_HASH_REPAIR_SAMPLE_SIZE.store(sample_size, Ordering::Relaxed);
⋮----
pub fn get_minimum_ancestor_agreement_size() -> usize {
get_ancestor_hash_repair_sample_size().div_ceil(2)
⋮----
pub enum DuplicateAncestorDecision {
⋮----
impl DuplicateAncestorDecision {
pub fn is_retryable(&self) -> bool {
⋮----
pub fn repair_status(&self) -> Option<&DuplicateSlotRepairStatus> {
⋮----
| DuplicateAncestorDecision::EarliestPrunedMismatchFound(status) => Some(status),
⋮----
pub fn repair_status_mut(&mut self) -> Option<&mut DuplicateSlotRepairStatus> {
⋮----
pub struct DuplicateSlotRepairStatus {
⋮----
impl DuplicateSlotRepairStatus {
fn new(correct_ancestor_to_repair: (Slot, Hash)) -> Self {
⋮----
start_ts: timestamp(),
⋮----
pub enum AncestorRequestType {
⋮----
impl AncestorRequestType {
pub fn is_pruned(&self) -> bool {
matches!(self, Self::PopularPruned)
⋮----
pub struct AncestorDuplicateSlotToRepair {
⋮----
pub struct AncestorRequestDecision {
⋮----
impl AncestorRequestDecision {
pub fn slot_to_repair(self) -> Option<AncestorDuplicateSlotToRepair> {
⋮----
.repair_status_mut()
.map(|status| AncestorDuplicateSlotToRepair {
⋮----
self.decision.is_retryable()
⋮----
pub struct AncestorRequestStatus {
⋮----
impl AncestorRequestStatus {
pub fn new(
⋮----
sampled_validators: sampled_validators.map(|p| (p, false)).collect(),
⋮----
pub fn add_response(
⋮----
if let Some(did_get_response) = self.sampled_validators.get_mut(from_addr) {
⋮----
.entry(response_slot_hashes.clone())
.or_default();
validators_with_same_response.push(*from_addr);
if validators_with_same_response.len()
== get_minimum_ancestor_agreement_size().min(self.sampled_validators.len())
⋮----
return Some(
self.handle_sampled_validators_reached_agreement(blockstore, response_slot_hashes),
⋮----
== get_ancestor_hash_repair_sample_size().min(self.sampled_validators.len())
⋮----
info!(
⋮----
return Some(DuplicateAncestorDecision::InvalidSample);
⋮----
pub fn request_type(&self) -> AncestorRequestType {
⋮----
fn handle_sampled_validators_reached_agreement(
⋮----
if agreed_response.is_empty() {
⋮----
if agreed_response.first().unwrap().0 != self.requested_mismatched_slot {
⋮----
for (i, (ancestor_slot, agreed_upon_hash)) in agreed_response.iter().rev().enumerate() {
⋮----
let our_frozen_hash = blockstore.get_bank_hash(*ancestor_slot);
⋮----
let mismatch_our_frozen_hash = blockstore.get_bank_hash(mismatch_slot);
⋮----
panic!("Programmer error, {decision:?} should not be set in decision loop")
⋮----
earliest_erroring_ancestor = Some((
agreed_response.len() - i - 1,
⋮----
} else if earliest_erroring_ancestor.is_none() && self.request_type.is_pruned() {
if let Ok(Some(meta)) = blockstore.meta(*ancestor_slot) {
if i != 0 && meta.parent_slot != Some(last_ancestor) {
⋮----
} else if earliest_erroring_ancestor.is_none() {
warn!(
⋮----
if earliest_erroring_ancestor_index == agreed_response.len() - 1 {
⋮----
DuplicateSlotRepairStatus::new(*agreed_response.last().unwrap());
⋮----
let repair_status = decision.repair_status_mut().unwrap();
⋮----
let repair_status = DuplicateSlotRepairStatus::new(*agreed_response.first().unwrap());
if self.request_type.is_pruned() {
⋮----
pub fn is_expired(&self) -> bool {
timestamp() - self.start_ts > RETRY_INTERVAL_SECONDS as u64 * 1000
⋮----
pub fn make_expired(&mut self) {
self.start_ts = timestamp() - RETRY_INTERVAL_SECONDS as u64 * 1000 - 1;
⋮----
pub mod tests {
⋮----
struct TestSetup {
⋮----
fn create_rand_socket_addr() -> SocketAddr {
⋮----
fn setup_add_response_test_with_type(
⋮----
assert!(request_slot >= num_ancestors_in_response as u64);
⋮----
.take(get_ancestor_hash_repair_sample_size())
.collect();
⋮----
sampled_addresses.iter().cloned(),
⋮----
let blockstore_temp_dir = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(blockstore_temp_dir.path()).unwrap();
⋮----
.map(|ancestor| (ancestor, Hash::new_unique()))
.rev()
⋮----
fn setup_add_response_test(request_slot: Slot, num_ancestors_in_response: usize) -> TestSetup {
setup_add_response_test_with_type(
⋮----
fn setup_add_response_test_pruned(
⋮----
fn test_add_response_invalid_peer() {
⋮----
} = setup_add_response_test(request_slot, 10);
let rand_addr = create_rand_socket_addr();
assert!(status
⋮----
assert_eq!(status.num_responses, 0);
assert!(status.ancestor_request_responses.is_empty());
⋮----
fn test_add_multiple_responses_same_peer() {
⋮----
let mut incorrect_ancestors_response = correct_ancestors_response.clone();
incorrect_ancestors_response.pop().unwrap();
let num_repeated_responses = get_ancestor_hash_repair_sample_size();
⋮----
correct_ancestors_response.clone()
⋮----
incorrect_ancestors_response.clone()
⋮----
assert_eq!(status.num_responses, 1);
assert_eq!(status.ancestor_request_responses.len(), 1);
⋮----
.get(&correct_ancestors_response)
.unwrap();
assert!(correct_responses.contains(responder_addr));
assert_eq!(correct_responses.len(), 1);
⋮----
fn run_add_multiple_correct_and_incorrect_responses(
⋮----
.into_iter()
.scan(
⋮----
assert!(num_responses > 0);
⋮----
Some((*total_count, incorrect_response))
⋮----
let total_incorrect_responses = events.iter().last().map(|(count, _)| *count).unwrap_or(0);
assert!(total_incorrect_responses <= get_ancestor_hash_repair_sample_size());
let mut event_order: Vec<usize> = (0..sampled_addresses.len()).collect();
event_order.shuffle(&mut thread_rng());
for (event, responder_addr) in event_order.iter().zip(sampled_addresses.iter()) {
⋮----
.range((event + 1)..)
.next()
.map(|(_count, response)| response)
.unwrap_or_else(|| correct_ancestors_response)
.clone();
if let Some(decision) = status.add_response(responder_addr, response, blockstore) {
⋮----
panic!("Decision must be made after hearing back from all the sampled validators");
⋮----
fn test_add_multiple_responses_invalid_sample_no_agreement() {
⋮----
let mut test_setup = setup_add_response_test(request_slot, 10);
let mut incorrect_ancestors_response_0 = test_setup.correct_ancestors_response.clone();
incorrect_ancestors_response_0.pop().unwrap();
let mut incorrect_ancestors_response_1 = incorrect_ancestors_response_0.clone();
incorrect_ancestors_response_1.pop().unwrap();
let desired_incorrect_responses = vec![
⋮----
.iter()
.map(|(_, count)| count)
.sum();
assert!(
⋮----
assert_eq!(
⋮----
fn test_add_multiple_responses_not_duplicate_confirmed() {
⋮----
let incorrect_ancestors_response = vec![];
let desired_incorrect_responses = vec![(
⋮----
fn test_add_multiple_responses_invalid_sample_missing_requested_slot() {
⋮----
let incorrect_ancestors_response = vec![(request_slot - 1, Hash::new_unique())];
⋮----
fn test_add_multiple_responses_invalid_sample_responses_not_ancestors() {
⋮----
let mut incorrect_ancestors_response = test_setup.correct_ancestors_response.clone();
incorrect_ancestors_response.push((request_slot + 1, Hash::new_unique()));
⋮----
fn test_add_multiple_responses_invalid_sample_responses_out_of_order() {
⋮----
incorrect_ancestors_response.swap_remove(0);
⋮----
fn test_add_multiple_responses_invalid_sample_mismatches_then_matches() {
⋮----
.insert_bank_hash(slot, correct_hash, false);
⋮----
fn test_add_multiple_responses_start_from_snapshot_missing_then_matches() {
⋮----
for &(slot, correct_hash) in test_setup.correct_ancestors_response.iter().take(5) {
⋮----
run_add_multiple_correct_and_incorrect_responses(vec![], &mut test_setup)
⋮----
panic!("Incorrect decision")
⋮----
fn test_add_multiple_responses_start_from_snapshot_missing_then_mismatch() {
⋮----
for &(slot, _) in test_setup.correct_ancestors_response.iter().take(5) {
⋮----
.insert_bank_hash(slot, Hash::new_unique(), false);
⋮----
match run_add_multiple_correct_and_incorrect_responses(vec![], &mut test_setup) {
⋮----
x => panic!("Incorrect decision {x:?}"),
⋮----
fn test_add_multiple_responses_ancestors_all_not_frozen() {
⋮----
incorrect_ancestors_response.push((request_slot, Hash::new_unique()));
⋮----
run_add_multiple_correct_and_incorrect_responses(
⋮----
fn test_add_multiple_responses_ancestors_some_not_frozen() {
⋮----
.find(|(slot, _)| *slot == if insert_even_or_odds == 0 { 94 } else { 93 })
⋮----
fn test_add_multiple_responses_ancestors_all_mismatched() {
⋮----
.insert_bank_hash(*slot, Hash::new_unique(), false);
⋮----
fn test_add_multiple_responses_ancestors_some_mismatched() {
⋮----
.find(|(slot, _)| *slot > 92)
⋮----
fn test_add_multiple_responses_ancestors_all_match() {
⋮----
fn test_add_multiple_responses_pruned_all_mismatch() {
⋮----
let mut test_setup = setup_add_response_test_pruned(request_slot, 10);
⋮----
fn test_add_multiple_responses_pruned_all_match() {
⋮----
.fold(tr(request_slot + 1), |tree, (slot, _)| tr(*slot) / tree);
⋮----
.add_tree(tree, true, true, 2, Hash::default());
⋮----
fn test_add_multiple_responses_pruned_some_ancestors_missing() {
⋮----
.filter(|(slot, _)| *slot <= 92 || *slot % 2 == 1)
.fold(tr(request_slot), |tree, (slot, _)| tr(*slot) / tree);
⋮----
.find(|(slot, _)| *slot > 93)
⋮----
fn test_add_multiple_responses_pruned_ancestor_is_bad() {
⋮----
let root_fork = tr(90) / (tr(91) / tr(92));
⋮----
.fold(tr(100), |tree, slot| tr(*slot) / tree);
⋮----
.add_tree(root_fork, true, true, 2, Hash::default());
⋮----
.add_tree(pruned_fork, true, true, 2, Hash::default());
⋮----
.find(|(slot, _)| *slot >= 93)

================
File: core/src/repair/malicious_repair_handler.rs
================
pub struct MaliciousRepairConfig {
⋮----
pub struct MaliciousRepairHandler {
⋮----
impl MaliciousRepairHandler {
⋮----
pub fn new(blockstore: Arc<Blockstore>, config: MaliciousRepairConfig) -> Self {
⋮----
impl RepairHandler for MaliciousRepairHandler {
fn blockstore(&self) -> &Blockstore {
⋮----
fn repair_response_packet(
⋮----
.get_data_shred(slot, shred_index)
.expect("Blockstore could not get data shred")?;
⋮----
.is_some_and(|freq| slot.is_multiple_of(freq))
⋮----
shred[Self::BAD_DATA_INDEX] = shred[Self::BAD_DATA_INDEX].wrapping_add(1);
⋮----
repair_response_packet_from_bytes(shred, dest, nonce)
⋮----
fn run_orphan(

================
File: core/src/repair/mod.rs
================
pub mod ancestor_hashes_service;
pub mod cluster_slot_state_verifier;
pub mod duplicate_repair_status;
pub(crate) mod malicious_repair_handler;
pub mod outstanding_requests;
pub mod packet_threshold;
pub(crate) mod quic_endpoint;
pub mod repair_generic_traversal;
pub mod repair_handler;
pub mod repair_response;
pub mod repair_service;
pub mod repair_weight;
pub mod repair_weighted_traversal;
pub mod request_response;
pub mod result;
pub mod serve_repair;
pub mod serve_repair_service;
pub(crate) mod standard_repair_handler;

================
File: core/src/repair/outstanding_requests.rs
================
pub struct OutstandingRequests<T> {
⋮----
pub fn add_request(&mut self, request: T, now: u64) -> Nonce {
let num_expected_responses = request.num_expected_responses();
let nonce = thread_rng().gen_range(0..Nonce::MAX);
self.requests.put(
⋮----
pub fn register_response<R>(
⋮----
.get_mut(&nonce)
.map(|status| {
⋮----
&& status.request.verify_response(response)
⋮----
Some(success_fn(&status.request)),
⋮----
.unwrap_or((None, false));
⋮----
.pop(&nonce)
.expect("Delete must delete existing object");
⋮----
impl<T> Default for OutstandingRequests<T> {
fn default() -> Self {
⋮----
pub struct RequestStatus<T> {
⋮----
pub(crate) mod tests {
⋮----
fn test_add_request() {
⋮----
let nonce = outstanding_requests.add_request(repair_type, timestamp());
let request_status = outstanding_requests.requests.get(&nonce).unwrap();
assert_eq!(request_status.request, repair_type);
assert_eq!(
⋮----
fn test_timeout_expired_remove() {
⋮----
.get(&nonce)
.unwrap()
⋮----
assert!(outstanding_requests
⋮----
assert!(outstanding_requests.requests.get(&nonce).is_none());
⋮----
fn test_register_response() {
⋮----
assert!(num_expected_responses > 1);
⋮----
assert!(outstanding_requests.requests.get(&nonce).is_some());

================
File: core/src/repair/packet_threshold.rs
================
use std::time::Duration;
enum PacketThresholdUpdate {
⋮----
impl PacketThresholdUpdate {
⋮----
fn calculate(&self, current: usize) -> usize {
⋮----
current.saturating_mul(100).saturating_div(Self::PERCENTAGE)
⋮----
current.saturating_mul(Self::PERCENTAGE).saturating_div(100)
⋮----
pub struct DynamicPacketToProcessThreshold {
⋮----
impl Default for DynamicPacketToProcessThreshold {
fn default() -> Self {
⋮----
impl DynamicPacketToProcessThreshold {
⋮----
pub fn update(&mut self, total_packets: usize, compute_time: Duration) {
⋮----
self.max_packets = threshold_update.calculate(self.max_packets);
⋮----
pub fn should_drop(&self, total: usize) -> bool {
⋮----
mod test {
⋮----
fn test_dynamic_packet_threshold() {
⋮----
assert_eq!(
⋮----
assert!(!threshold.should_drop(10));
assert!(threshold.should_drop(2000));
⋮----
threshold.update(total, compute_time);
assert!(threshold.max_packets > old);
⋮----
assert_eq!(threshold.max_packets, old - 1);

================
File: core/src/repair/quic_endpoint.rs
================
pub(crate) type AsyncTryJoinHandle = TryJoin3<
⋮----
pub(crate) struct RemoteRequest {
⋮----
pub(crate) struct RepairQuicAsyncSenders {
⋮----
pub(crate) struct RepairQuicSockets {
⋮----
pub(crate) struct RepairQuicSenders {
⋮----
pub(crate) enum Error {
⋮----
macro_rules! add_metric {
⋮----
pub(crate) fn new_quic_endpoints(
⋮----
new_quic_endpoint(
⋮----
bank_forks.clone(),
⋮----
) = new_quic_endpoint(
⋮----
Ok((
⋮----
fn new_quic_endpoint<T>(
⋮----
let (cert, key) = new_dummy_x509_certificate(keypair);
let server_config = new_server_config(cert.clone(), key.clone_key())?;
let client_config = new_client_config(cert, key)?;
⋮----
// Endpoint::new requires entering the runtime context,
// otherwise the code below will panic.
let _guard = runtime.enter();
⋮----
Some(server_config),
⋮----
endpoint.set_default_client_config(client_config);
⋮----
let server_task = runtime.spawn(run_server(
endpoint.clone(),
⋮----
sender.clone(),
⋮----
prune_cache_pending.clone(),
router.clone(),
cache.clone(),
⋮----
let client_task = runtime.spawn(run_client(
⋮----
Ok((endpoint, client_sender, task))
⋮----
pub(crate) fn close_quic_endpoint(endpoint: &Endpoint) {
endpoint.close(
⋮----
fn new_server_config(
⋮----
let mut config = tls_server_config_builder().with_single_cert(vec![cert], key)?;
config.alpn_protocols = vec![ALPN_REPAIR_PROTOCOL_ID.to_vec()];
⋮----
return Err(rustls::Error::InvalidCertificate(
⋮----
.transport_config(Arc::new(new_transport_config()))
.migration(false);
Ok(config)
⋮----
fn new_client_config(
⋮----
let mut config = tls_client_config_builder().with_client_auth_cert(vec![cert], key)?;
⋮----
let mut config = ClientConfig::new(Arc::new(QuicClientConfig::try_from(config).unwrap()));
config.transport_config(Arc::new(new_transport_config()));
⋮----
fn new_transport_config() -> TransportConfig {
let max_idle_timeout = IdleTimeout::try_from(MAX_IDLE_TIMEOUT).unwrap();
⋮----
.datagram_receive_buffer_size(Some(DATAGRAM_RECEIVE_BUFFER_SIZE))
.datagram_send_buffer_size(DATAGRAM_SEND_BUFFER_SIZE)
.initial_mtu(INITIAL_MAXIMUM_TRANSMISSION_UNIT)
.keep_alive_interval(Some(KEEP_ALIVE_INTERVAL))
.max_concurrent_bidi_streams(VarInt::from(0u8))
.max_concurrent_uni_streams(VarInt::from(0u8))
.max_idle_timeout(Some(max_idle_timeout))
.min_mtu(MINIMUM_MAXIMUM_TRANSMISSION_UNIT)
.mtu_discovery_config(None);
⋮----
async fn run_server<T>(
⋮----
let report_metrics_task = tokio::task::spawn(report_metrics_task(server_name, stats.clone()));
while let Some(incoming) = endpoint.accept().await {
let remote_addr: SocketAddr = incoming.remote_address();
match incoming.accept() {
⋮----
tokio::task::spawn(handle_connecting_task(
⋮----
stats.clone(),
⋮----
debug!("Error while accepting incoming connection: {err:?} from {remote_addr}");
record_error(&Error::from(err), &stats);
⋮----
report_metrics_task.abort();
⋮----
async fn run_client<T>(
⋮----
let report_metrics_task = tokio::task::spawn(report_metrics_task(client_name, stats.clone()));
while let Some((remote_address, bytes)) = receiver.recv().await {
let Some(bytes) = try_route_bytes(&remote_address, bytes, &*router.read().await, &stats)
⋮----
let mut router = router.write().await;
let Some(bytes) = try_route_bytes(&remote_address, bytes, &router, &stats) else {
⋮----
sender.try_send(bytes).unwrap();
router.insert(remote_address, sender);
⋮----
tokio::task::spawn(make_connection_task(
⋮----
close_quic_endpoint(&endpoint);
// Drop sender channels to unblock threads waiting on the receiving end.
router.write().await.clear();
⋮----
// Routes the payload to respective channel.
// Drops the payload if the channel is full.
// Bounces the payload back if the channel is closed or does not exist.
fn try_route_bytes(
⋮----
match router.get(remote_address) {
None => Some(bytes),
Some(sender) => match sender.try_send(bytes) {
⋮----
debug!("TrySendError::Full {remote_address}");
add_metric!(stats.router_try_send_error_full);
⋮----
Err(TrySendError::Closed(bytes)) => Some(bytes),
⋮----
async fn handle_connecting_task<T>(
⋮----
if let Err(err) = handle_connecting(
⋮----
debug!("handle_connecting: {err:?}");
record_error(&err, &stats);
⋮----
async fn handle_connecting<T>(
⋮----
let remote_address = connection.remote_address();
let remote_pubkey = get_remote_pubkey(&connection)?;
⋮----
router.write().await.insert(remote_address, sender);
⋮----
handle_connection(
⋮----
Ok(())
⋮----
async fn handle_connection<T>(
⋮----
cache_connection(
⋮----
connection.clone(),
⋮----
let send_datagram_task = tokio::task::spawn(send_datagram_task(connection.clone(), receiver));
let read_datagram_task = tokio::task::spawn(read_datagram_task(
⋮----
Err(err) => error!("handle_connection: {remote_pubkey}, {remote_address}, {err:?}"),
⋮----
debug!("send_datagram_task: {remote_pubkey}, {remote_address}, {err:?}");
record_error(err, &stats);
⋮----
debug!("read_datagram_task: {remote_pubkey}, {remote_address}, {err:?}");
⋮----
drop_connection(remote_pubkey, &connection, &cache).await;
if let Entry::Occupied(entry) = router.write().await.entry(remote_address) {
if entry.get().is_closed() {
entry.remove();
⋮----
async fn read_datagram_task<T>(
⋮----
debug_assert_eq!(sender.capacity(), None);
⋮----
match connection.read_datagram().await {
⋮----
if let Err(err) = sender.send(value) {
⋮----
return Err(Error::from(err));
⋮----
if let Some(err) = connection.close_reason() {
⋮----
debug!("connection.read_datagram: {remote_pubkey}, {remote_address}, {err:?}");
⋮----
async fn send_datagram_task(
⋮----
async fn make_connection_task<T>(
⋮----
if let Err(err) = make_connection(
⋮----
debug!("make_connection: {remote_address}, {err:?}");
⋮----
async fn make_connection<T>(
⋮----
let server_name = socket_addr_to_quic_server_name(remote_address);
let connection = endpoint.connect(remote_address, &server_name)?.await?;
⋮----
connection.remote_address(),
get_remote_pubkey(&connection)?,
⋮----
fn get_remote_pubkey(connection: &Connection) -> Result<Pubkey, Error> {
⋮----
Some(remote_pubkey) => Ok(remote_pubkey),
⋮----
connection.close(
⋮----
Err(Error::InvalidIdentity(connection.remote_address()))
⋮----
async fn cache_connection(
⋮----
let mut cache = cache.lock().await;
⋮----
cache.insert(remote_pubkey, connection),
cache.len() >= CONNECTION_CACHE_CAPACITY.saturating_mul(2),
⋮----
old.close(
⋮----
if should_prune_cache && !prune_cache_pending.swap(true, Ordering::Relaxed) {
tokio::task::spawn(prune_connection_cache(
⋮----
async fn drop_connection(
⋮----
if let Entry::Occupied(entry) = cache.lock().await.entry(remote_pubkey) {
if entry.get().stable_id() == connection.stable_id() {
⋮----
async fn prune_connection_cache(
⋮----
debug_assert!(prune_cache_pending.load(Ordering::Relaxed));
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
root_bank.current_epoch_staked_nodes()
⋮----
if cache.len() < CONNECTION_CACHE_CAPACITY.saturating_mul(2) {
prune_cache_pending.store(false, Ordering::Relaxed);
⋮----
.drain()
.filter(|(_, connection)| connection.close_reason().is_none())
.map(|entry @ (pubkey, _)| {
let stake = staked_nodes.get(&pubkey).copied().unwrap_or_default();
⋮----
.collect();
⋮----
.select_nth_unstable_by_key(CONNECTION_CACHE_CAPACITY, |&(stake, _)| Reverse(stake));
⋮----
cache.extend(
⋮----
.into_iter()
.take(CONNECTION_CACHE_CAPACITY)
.map(|(_, entry)| entry),
⋮----
router.write().await.retain(|_, sender| !sender.is_closed());
⋮----
impl RemoteRequest {
⋮----
pub(crate) fn protocol(&self) -> Protocol {
if self.remote_pubkey.is_some() {
⋮----
fn from((pubkey, remote_address, bytes): (Pubkey, SocketAddr, Bytes)) -> Self {
⋮----
remote_pubkey: Some(pubkey),
⋮----
impl RepairQuicAsyncSenders {
pub(crate) fn new_dummy() -> Self {
⋮----
fn from(_: crossbeam_channel::SendError<T>) -> Self {
⋮----
struct RepairQuicStats {
⋮----
async fn report_metrics_task(name: &'static str, stats: Arc<RepairQuicStats>) {
⋮----
report_metrics(name, &stats);
⋮----
fn record_error(err: &Error, stats: &RepairQuicStats) {
⋮----
add_metric!(stats.connect_error_cids_exhausted)
⋮----
add_metric!(stats.connect_error_other)
⋮----
add_metric!(stats.connect_error_invalid_remote_address)
⋮----
add_metric!(stats.connection_error_cids_exhausted)
⋮----
add_metric!(stats.connection_error_version_mismatch)
⋮----
add_metric!(stats.connection_error_transport_error)
⋮----
add_metric!(stats.connection_error_connection_closed)
⋮----
add_metric!(stats.connection_error_application_closed)
⋮----
Error::ConnectionError(ConnectionError::Reset) => add_metric!(stats.connection_error_reset),
⋮----
add_metric!(stats.connection_error_timed_out)
⋮----
add_metric!(stats.connection_error_locally_closed)
⋮----
Error::InvalidIdentity(_) => add_metric!(stats.invalid_identity),
⋮----
add_metric!(stats.send_datagram_error_unsupported_by_peer)
⋮----
add_metric!(stats.send_datagram_error_too_large)
⋮----
add_metric!(stats.send_datagram_error_connection_lost)
⋮----
fn report_metrics(name: &'static str, stats: &RepairQuicStats) {
macro_rules! reset_metric {
⋮----
datapoint_info!(
⋮----
mod tests {
⋮----
fn test_quic_endpoint() {
⋮----
.worker_threads(8)
.enable_all()
.build()
.unwrap();
let keypairs: Vec<Keypair> = repeat_with(Keypair::new).take(NUM_ENDPOINTS).collect();
let port_range = localhost_port_range_for_tests();
⋮----
.map(|port| bind_to(ip_addr, port).unwrap())
.take(NUM_ENDPOINTS)
⋮----
.iter()
.map(UdpSocket::local_addr)
⋮----
repeat_with(crossbeam_channel::unbounded::<(Pubkey, SocketAddr, Bytes)>)
⋮----
.unzip();
⋮----
create_genesis_config( 100_000);
⋮----
multiunzip(keypairs.iter().zip(sockets).zip(senders).map(
⋮----
runtime.handle(),
⋮----
.unwrap()
⋮----
for (i, (keypair, &address, sender)) in izip!(&keypairs, &addresses, &senders).enumerate() {
for (j, &address) in addresses.iter().enumerate() {
⋮----
let bytes = Bytes::from(format!("{i}=>{j}"));
sender.blocking_send((address, bytes)).unwrap();
⋮----
for (j, receiver) in receivers.iter().enumerate() {
⋮----
let entry = (keypair.pubkey(), address, bytes);
assert_eq!(receiver.recv_timeout(RECV_TIMEOUT).unwrap(), entry);
⋮----
drop(senders);
⋮----
runtime.block_on(task).unwrap();

================
File: core/src/repair/repair_generic_traversal.rs
================
struct GenericTraversal<'a> {
⋮----
pub fn new(tree: &'a HeaviestSubtreeForkChoice) -> Self {
⋮----
pending: vec![tree.tree_root().0],
⋮----
impl Iterator for GenericTraversal<'_> {
type Item = Slot;
fn next(&mut self) -> Option<Self::Item> {
let next = self.pending.pop();
⋮----
.children(&(slot, Hash::default()))
.unwrap()
.map(|(child_slot, _)| *child_slot)
.collect();
self.pending.extend(children);
⋮----
pub fn get_unknown_last_index(
⋮----
if processed_slots.contains(&slot) {
⋮----
.entry(slot)
.or_insert_with(|| blockstore.meta(slot).unwrap());
⋮----
if slot_meta.last_index.is_none() {
let shred_index = blockstore.get_index(slot).unwrap();
⋮----
shred_index.data().num_shreds() as u64
⋮----
unknown_last.push((slot, slot_meta.received, num_processed_shreds));
processed_slots.insert(slot);
⋮----
unknown_last.sort_by(|(_, _, count1), (_, _, count2)| count2.cmp(count1));
⋮----
.iter()
.filter_map(|(slot, received, _)| {
⋮----
.take(limit)
.collect()
⋮----
fn get_unrepaired_path(
⋮----
while visited.insert(slot) {
⋮----
if !slot_meta.is_full() {
path.push(slot);
⋮----
path.reverse();
⋮----
pub fn get_closest_completion(
⋮----
if slot_meta.is_full() {
⋮----
let shred_count = shred_index.data().num_shreds() as u64;
if last_index.saturating_add(1) < shred_count {
datapoint_error!(
⋮----
last_index.saturating_add(1).saturating_sub(shred_count)
⋮----
last_index.saturating_sub(slot_meta.consumed)
⋮----
slot_dists.push((slot, dist));
⋮----
slot_dists.sort_by(|(_, d1), (_, d2)| d1.cmp(d2));
⋮----
if repairs.len() >= limit {
⋮----
let path = get_unrepaired_path(slot, blockstore, slot_meta_cache, &mut visited);
⋮----
if !processed_slots.insert(path_slot) {
⋮----
let slot_meta = slot_meta_cache.get(&path_slot).unwrap().as_ref().unwrap();
⋮----
limit - repairs.len(),
⋮----
repairs.extend(new_repairs);
⋮----
pub mod test {
⋮----
fn test_get_unknown_last_index() {
let (blockstore, heaviest_subtree_fork_choice) = setup_forks();
let last_shred = blockstore.meta(0).unwrap().unwrap().received;
⋮----
let repairs = get_unknown_last_index(
⋮----
assert_eq!(
⋮----
assert_eq!(outstanding_requests.len(), repairs.len());
⋮----
assert_eq!(repairs, []);
⋮----
fn test_get_closest_completion() {
⋮----
let (repairs, _) = get_closest_completion(
⋮----
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3) / (tr(5))));
let ledger_path = get_tmp_ledger_path!();
let blockstore = Blockstore::open(&ledger_path).unwrap();
add_tree_with_missing_shreds(
⋮----
forks.clone(),
⋮----
sleep_shred_deferment_period();
⋮----
assert_eq!(repairs, [ShredRepairType::Shred(1, 30)]);
⋮----
assert_eq!(repairs.len(), 4);
assert_eq!(outstanding_requests.len(), 5);
⋮----
assert_eq!(repairs.len(), 0);
⋮----
fn add_tree_with_missing_shreds(
⋮----
while let Some(visit) = walk.get() {
let slot = *visit.node().data();
if blockstore.meta(slot).unwrap().is_some()
&& blockstore.orphan(slot).unwrap().is_none()
⋮----
walk.forward();
⋮----
let parent = walk.get_parent().map(|n| *n.data());
if parent.is_some() || !is_orphan {
⋮----
.and_then(|parent| blockhashes.get(&parent))
.unwrap_or(&starting_hash);
⋮----
num_ticks * (std::cmp::max(1, slot - parent.unwrap_or(slot))),
⋮----
blockhashes.insert(slot, entries.last().unwrap().hash);
⋮----
parent.unwrap_or(slot),
⋮----
let shred = shreds.pop().unwrap();
shreds.pop().unwrap();
shreds.push(shred);
blockstore.insert_shreds(shreds, None, false).unwrap();
⋮----
fn setup_forks() -> (Blockstore, HeaviestSubtreeForkChoice) {
⋮----
blockstore.add_tree(forks.clone(), false, false, 2, Hash::default());

================
File: core/src/repair/repair_handler.rs
================
pub trait RepairHandler {
⋮----
fn run_window_request(
⋮----
let packet = self.repair_response_packet(slot, shred_index, from_addr, nonce)?;
Some(
⋮----
vec![packet],
⋮----
.into(),
⋮----
fn run_highest_window_request(
⋮----
let meta = self.blockstore().meta(slot).ok()??;
⋮----
let packet = self.repair_response_packet(slot, meta.received - 1, from_addr, nonce)?;
return Some(
⋮----
fn run_ancestor_hashes(
⋮----
let ancestor_slot_hashes = if self.blockstore().is_duplicate_confirmed(slot) {
⋮----
AncestorIterator::new_inclusive(slot, self.blockstore()),
⋮----
ancestor_iterator.take(MAX_ANCESTOR_RESPONSES).collect()
⋮----
vec![]
⋮----
let serialized_response = serialize(&response).ok()?;
⋮----
pub enum RepairHandlerType {
⋮----
impl RepairHandlerType {
pub fn to_handler(&self, blockstore: Arc<Blockstore>) -> Box<dyn RepairHandler + Send + Sync> {
⋮----
pub fn create_serve_repair(
⋮----
self.to_handler(blockstore),

================
File: core/src/repair/repair_response.rs
================
pub fn repair_response_packet(
⋮----
.get_data_shred(slot, shred_index)
.expect("Blockstore could not get data shred");
⋮----
.map(|shred| repair_response_packet_from_bytes(shred, dest, nonce))
.unwrap_or(None)
⋮----
pub fn repair_response_packet_from_bytes(
⋮----
let bytes = bytes.as_ref();
⋮----
let size = bytes.len() + SIZE_OF_NONCE;
if size > packet.buffer_mut().len() {
⋮----
packet.meta_mut().size = size;
packet.meta_mut().set_socket_addr(dest);
packet.buffer_mut()[..bytes.len()].copy_from_slice(bytes);
let mut wr = io::Cursor::new(&mut packet.buffer_mut()[bytes.len()..]);
bincode::serialize_into(&mut wr, &nonce).expect("Buffer not large enough to fit nonce");
Some(packet)
⋮----
mod test {
⋮----
fn run_test_sigverify_shred_cpu_repair(slot: Slot) {
⋮----
trace!("signature {}", shred.signature());
⋮----
let mut packet = repair_response_packet_from_bytes(
shred.into_payload(),
⋮----
.unwrap();
packet.meta_mut().flags |= PacketFlags::REPAIR;
let leader_slots: SlotPubkeys = [(slot, keypair.pubkey())].into_iter().collect();
assert!(verify_shred_cpu((&packet).into(), &leader_slots, &cache));
⋮----
let leader_slots: SlotPubkeys = [(slot, wrong_keypair.pubkey())].into_iter().collect();
assert!(!verify_shred_cpu((&packet).into(), &leader_slots, &cache));
⋮----
fn test_sigverify_shred_cpu_repair() {
run_test_sigverify_shred_cpu_repair(0xdead_c0de);

================
File: core/src/repair/repair_service.rs
================
const DEFER_REPAIR_THRESHOLD_TICKS: u64 = DEFER_REPAIR_THRESHOLD.as_millis() as u64 / MS_PER_TICK;
⋮----
pub type AncestorDuplicateSlotsSender = CrossbeamSender<AncestorDuplicateSlotToRepair>;
pub type AncestorDuplicateSlotsReceiver = CrossbeamReceiver<AncestorDuplicateSlotToRepair>;
pub type ConfirmedSlotsSender = CrossbeamSender<Vec<Slot>>;
pub type ConfirmedSlotsReceiver = CrossbeamReceiver<Vec<Slot>>;
pub type DumpedSlotsSender = CrossbeamSender<Vec<(Slot, Hash)>>;
pub type DumpedSlotsReceiver = CrossbeamReceiver<Vec<(Slot, Hash)>>;
pub type OutstandingShredRepairs = OutstandingRequests<ShredRepairType>;
pub type PopularPrunedForksSender = CrossbeamSender<Vec<Slot>>;
pub type PopularPrunedForksReceiver = CrossbeamReceiver<Vec<Slot>>;
⋮----
pub struct SlotRepairs {
⋮----
impl SlotRepairs {
pub fn pubkey_repairs(&self) -> &HashMap<Pubkey, u64> {
⋮----
pub struct RepairStatsGroup {
⋮----
impl RepairStatsGroup {
pub fn update(&mut self, repair_peer_id: &Pubkey, slot: Slot, shred_index: u64) {
⋮----
let slot_repairs = self.slot_pubkeys.entry(slot).or_default();
⋮----
.entry(*repair_peer_id)
.or_default() += 1;
⋮----
pub struct RepairMetrics {
⋮----
impl Default for RepairMetrics {
fn default() -> Self {
⋮----
impl RepairMetrics {
pub fn maybe_report(&mut self) {
if self.last_report.elapsed().as_secs() > 2 {
self.stats.report();
self.timing.report();
self.best_repairs_stats.report();
⋮----
pub struct RepairStats {
⋮----
impl RepairStats {
fn report(&self) {
⋮----
.iter()
.chain(self.highest_shred.slot_pubkeys.iter())
.chain(self.orphan.slot_pubkeys.iter())
.map(|(slot, slot_repairs)| (slot, slot_repairs.pubkey_repairs.values().sum::<u64>()))
.collect();
info!("repair_stats: {slot_to_count:?}");
⋮----
let nonzero_num = |x| if x == 0 { None } else { Some(x) };
datapoint_info!(
⋮----
pub struct RepairTiming {
⋮----
impl RepairTiming {
⋮----
pub struct BestRepairsStats {
⋮----
impl BestRepairsStats {
⋮----
pub fn update(
⋮----
pub struct RepairInfo {
⋮----
pub struct RepairSlotRange {
⋮----
impl Default for RepairSlotRange {
⋮----
struct RepairChannels {
⋮----
pub struct RepairServiceChannels {
⋮----
impl RepairServiceChannels {
pub fn new(
⋮----
struct RepairTracker {
⋮----
pub struct RepairService {
⋮----
impl RepairService {
⋮----
let blockstore = blockstore.clone();
let exit = exit.clone();
let repair_info = repair_info.clone();
⋮----
.name("solRepairSvc".to_string())
.spawn(move || {
⋮----
.unwrap()
⋮----
fn update_weighting_heuristic(
⋮----
repair_weight.set_root(root_bank.slot());
set_root_elapsed.stop();
⋮----
.try_iter()
.for_each(|slot_hash_keys_to_dump| {
⋮----
if slot >= repair_weight.root() {
let dumped_slots = repair_weight.split_off(slot);
popular_pruned_forks_requests.retain(|slot| {
!dumped_slots.contains(slot) && repair_weight.is_pruned(*slot)
⋮----
dump_slots_elapsed.stop();
⋮----
.for_each(|(vote_pubkey, vote_slots)| {
⋮----
.entry(slot)
.or_default()
.push(vote_pubkey);
⋮----
get_votes_elapsed.stop();
⋮----
repair_weight.add_voters(
⋮----
slot_to_vote_pubkeys.into_iter(),
root_bank.epoch_stakes_map(),
root_bank.epoch_schedule(),
⋮----
add_voters_elapsed.stop();
repair_metrics.timing.set_root_elapsed += set_root_elapsed.as_us();
repair_metrics.timing.dump_slots_elapsed += dump_slots_elapsed.as_us();
repair_metrics.timing.get_votes_elapsed += get_votes_elapsed.as_us();
repair_metrics.timing.add_voters_elapsed += add_voters_elapsed.as_us();
⋮----
fn identify_repairs(
⋮----
outstanding_repairs.retain(|_repair_request, time| {
timestamp().saturating_sub(*time) < REPAIR_REQUEST_TIMEOUT_MS
⋮----
purge_outstanding_repairs.stop();
repair_metrics.timing.purge_outstanding_repairs = purge_outstanding_repairs.as_us();
match repair_info.wen_restart_repair_slots.clone() {
⋮----
&slots_to_repair.read().unwrap(),
⋮----
None => repair_weight.get_best_weighted_repairs(
⋮----
fn handle_popular_pruned_forks(
⋮----
.get_popular_pruned_forks(root_bank.epoch_stakes_map(), root_bank.epoch_schedule());
popular_pruned_forks.retain(|slot| {
⋮----
.any(|prev_req_slot| repair_weight.same_tree(*slot, *prev_req_slot))
⋮----
popular_pruned_forks_requests.insert(*slot);
⋮----
if !popular_pruned_forks.is_empty() {
warn!("Notifying repair of popular pruned forks {popular_pruned_forks:?}");
⋮----
.send(popular_pruned_forks)
.unwrap_or_else(|err| error!("failed to send popular pruned forks {err}"));
⋮----
handle_popular_pruned_forks.stop();
repair_metrics.timing.handle_popular_pruned_forks = handle_popular_pruned_forks.as_us();
⋮----
fn build_and_send_repair_batch(
⋮----
let identity_keypair = repair_info.cluster_info.keypair();
⋮----
let mut outstanding_requests = outstanding_requests.write().unwrap();
⋮----
.into_iter()
.filter_map(|repair_request| {
⋮----
.repair_request(
⋮----
.ok()??;
Some((req, to))
⋮----
.collect()
⋮----
build_repairs_batch_elapsed.stop();
⋮----
if !batch.is_empty() {
let num_pkts = batch.len();
let batch = batch.iter().map(|(bytes, addr)| (bytes, addr));
match batch_send(repair_socket, batch) {
⋮----
error!(
⋮----
batch_send_repairs_elapsed.stop();
repair_metrics.timing.build_repairs_batch_elapsed = build_repairs_batch_elapsed.as_us();
repair_metrics.timing.batch_send_repairs_elapsed = batch_send_repairs_elapsed.as_us();
⋮----
fn run_repair_iteration(
⋮----
let root_bank = sharable_banks.root();
⋮----
root_bank.clone(),
⋮----
serve_repair::get_repair_protocol(root_bank.cluster_type()),
⋮----
fn run(
⋮----
let sharable_banks = repair_info.bank_forks.read().unwrap().sharable_banks();
let root_bank_slot = sharable_banks.root().slot();
⋮----
repair_info.cluster_info.clone(),
repair_info.bank_forks.read().unwrap().sharable_banks(),
repair_info.repair_whitelist.clone(),
Box::new(StandardRepairHandler::new(blockstore.clone())),
⋮----
while !exit.load(Ordering::Relaxed) {
⋮----
blockstore.as_ref(),
⋮----
repair_tracker.repair_metrics.maybe_report();
sleep(Duration::from_millis(REPAIR_MS));
⋮----
pub fn generate_repairs_for_slot_throttled_by_tick(
⋮----
pub fn generate_repairs_for_slot_not_throttled_by_tick(
⋮----
fn generate_repairs_for_slot(
⋮----
if max_repairs == 0 || slot_meta.is_full() {
vec![]
⋮----
.checked_sub(1)
.and_then(|index| blockstore.get_data_shred(slot, index).ok()?)
.and_then(|shred| shred::layout::get_reference_tick(&shred).ok())
.map(u64::from)
⋮----
* timestamp().saturating_sub(slot_meta.first_shred_timestamp)
⋮----
< reference_tick.saturating_add(defer_repair_threshold_ticks)
⋮----
return vec![];
⋮----
Some(repair_request) => vec![repair_request],
None => vec![],
⋮----
.find_missing_data_indexes(
⋮----
.filter_map(|i| {
⋮----
pub fn generate_repairs_for_fork(
⋮----
let mut pending_slots = vec![slot];
while repairs.len() < max_repairs && !pending_slots.is_empty() {
let slot = pending_slots.pop().unwrap();
if let Some(slot_meta) = blockstore.meta(slot).unwrap() {
⋮----
max_repairs - repairs.len(),
⋮----
repairs.extend(new_repairs);
⋮----
pending_slots.extend(next_slots);
⋮----
pub(crate) fn generate_repairs_for_wen_restart(
⋮----
if let Some(slot_meta) = blockstore.meta(*slot).unwrap() {
⋮----
repairs.push(ShredRepairType::HighestShred(*slot, 0));
⋮----
if repairs.len() >= max_repairs {
⋮----
fn get_repair_peers(
⋮----
let Some(peers_with_slot) = cluster_slots.lookup(slot) else {
warn!("No repair peers have frozen slot: {slot}");
⋮----
.filter_map(|(pubkey, stake)| {
⋮----
.lookup_contact_info(pubkey, |node| node.serve_repair(Protocol::UDP));
⋮----
trace!("Repair peer {pubkey} has a valid repair socket: {peer_repair_addr:?}");
Some((
⋮----
let Ok(weighted_sample_repair_peers) = repair_peers.choose_multiple_weighted(
⋮----
.map(|(pubkey, addr, _)| (*pubkey, *addr))
⋮----
pub fn request_repair_for_shred_from_peer(
⋮----
let mut repair_peers = vec![];
⋮----
cluster_info.lookup_contact_info(&pubkey, |node| node.serve_repair(Protocol::UDP));
⋮----
trace!("Repair peer {pubkey} has valid repair socket: {peer_repair_addr:?}");
repair_peers.push((pubkey, peer_repair_addr));
⋮----
if repair_peers.is_empty() {
debug!(
⋮----
repair_peers = Self::get_repair_peers(cluster_info.clone(), cluster_slots, slot);
⋮----
cluster_info.clone(),
⋮----
outstanding_repair_requests.clone(),
⋮----
fn request_repair_for_shred_from_address(
⋮----
let identity_keypair = cluster_info.keypair();
⋮----
.write()
⋮----
.add_request(repair_request, timestamp());
⋮----
RepairRequestHeader::new(identity_keypair.pubkey(), pubkey, timestamp(), nonce);
⋮----
ServeRepair::repair_proto_to_bytes(&request_proto, &identity_keypair).unwrap();
⋮----
match batch_send(repair_socket, reqs) {
⋮----
debug!("successfully sent repair request to {pubkey} / {address}!");
⋮----
error!("batch_send failed to send packet - error = {err:?}");
⋮----
pub fn request_repair_if_needed(
⋮----
if let Entry::Vacant(entry) = outstanding_repairs.entry(repair_request) {
entry.insert(timestamp());
Some(repair_request)
⋮----
pub fn generate_repairs_in_range(
⋮----
let mut repairs: Vec<ShredRepairType> = vec![];
⋮----
.meta(slot)
.expect("Unable to lookup slot meta")
.unwrap_or(SlotMeta {
⋮----
fn generate_duplicate_repairs_for_slot(
⋮----
if slot_meta.is_full() {
⋮----
Some(Self::generate_repairs_for_slot_throttled_by_tick(
⋮----
error!("Slot meta for duplicate slot does not exist, cannot generate repairs");
⋮----
fn generate_and_send_duplicate_repairs(
⋮----
duplicate_slot_repair_statuses.retain(|slot, status| {
⋮----
&identity_keypair.pubkey(),
⋮----
let nonce = outstanding_requests.add_request(repair_type, timestamp());
match serve_repair.map_repair_request(
⋮----
if let Err(e) = repair_socket.send_to(&req, repair_addr) {
info!(
⋮----
Err(e) => info!("map_repair_request err={e}"),
⋮----
fn update_duplicate_slot_repair_addr(
⋮----
let now = timestamp();
if status.repair_pubkey_and_addr.is_none()
|| now.saturating_sub(status.start_ts) >= MAX_DUPLICATE_WAIT_MS as u64
⋮----
.repair_request_duplicate_compute_best_peer(
⋮----
status.start_ts = timestamp();
⋮----
fn initiate_repair_for_duplicate_slot(
⋮----
if duplicate_slot_repair_statuses.contains_key(&slot) {
⋮----
let repair_pubkey_and_addr = serve_repair.repair_request_duplicate_compute_best_peer(
⋮----
start_ts: timestamp(),
⋮----
duplicate_slot_repair_statuses.insert(slot, new_duplicate_slot_repair_status);
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_repair.join()?;
self.ancestor_hashes_service.join()
⋮----
pub(crate) fn sleep_shred_deferment_period() {
sleep(Duration::from_millis(
DEFAULT_MS_PER_SLOT + DEFER_REPAIR_THRESHOLD.as_millis() as u64,
⋮----
mod test {
⋮----
fn new_test_cluster_info() -> ClusterInfo {
⋮----
let contact_info = ContactInfo::new_localhost(&keypair.pubkey(), timestamp());
⋮----
pub fn test_request_repair_for_shred_from_address() {
let cluster_info = Arc::new(new_test_cluster_info());
let pubkey = cluster_info.id();
⋮----
let reader = bind_to_localhost_unique().expect("should bind");
let address = reader.local_addr().unwrap();
let sender = bind_to_localhost_unique().expect("should bind");
⋮----
let mut packets = vec![solana_packet::Packet::default(); 1];
let _recv_count = solana_streamer::recvmmsg::recv_mmsg(&reader, &mut packets[..]).unwrap();
⋮----
let Some(bytes) = packet.data(..).map(Vec::from) else {
panic!("packet data not found");
⋮----
remote_address: packet.meta().socket_addr(),
⋮----
serve_repair::deserialize_request::<RepairProtocol>(&remote_request).unwrap();
⋮----
assert_eq!(deserialized_slot, slot);
assert_eq!(deserialized_shred_index, shred_index);
⋮----
_ => panic!("unexpected repair protocol"),
⋮----
pub fn test_repair_orphan() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
let (mut shreds, _) = make_slot_entries(1, 0, 1);
let (shreds2, _) = make_slot_entries(5, 2, 1);
shreds.extend(shreds2);
blockstore.insert_shreds(shreds, None, false).unwrap();
⋮----
assert_eq!(
⋮----
pub fn test_repair_empty_slot() {
⋮----
let (shreds, _) = make_slot_entries(2, 0, 1);
⋮----
pub fn test_generate_repairs() {
⋮----
let (mut shreds, _) = make_many_slot_entries(0, num_slots, 150);
let num_shreds = shreds.len() as u64;
⋮----
let mut shreds_to_write = vec![];
let mut missing_indexes_per_slot = vec![];
for i in (0..num_shreds).rev() {
⋮----
if index.is_multiple_of(nth) || index + 1 == num_shreds_per_slot {
shreds_to_write.insert(0, shreds.remove(i as usize));
⋮----
missing_indexes_per_slot.insert(0, index);
⋮----
.insert_shreds(shreds_to_write, None, false)
.unwrap();
⋮----
.flat_map(|slot| {
⋮----
.map(move |shred_index| ShredRepairType::Shred(slot, *shred_index))
⋮----
sleep_shred_deferment_period();
⋮----
pub fn test_generate_highest_repair() {
⋮----
let (mut shreds, _) = make_slot_entries(
⋮----
let num_shreds_per_slot = shreds.len() as u64;
shreds.pop();
⋮----
vec![ShredRepairType::HighestShred(0, num_shreds_per_slot - 1)];
⋮----
pub fn test_repair_range() {
⋮----
let slots: Vec<u64> = vec![1, 3, 5, 7, 8];
let num_entries_per_slot = max_ticks_per_n_shreds(1, None) + 1;
let shreds = make_chaining_slot_entries(&slots, num_entries_per_slot, 0);
for (mut slot_shreds, _) in shreds.into_iter() {
slot_shreds.remove(0);
blockstore.insert_shreds(slot_shreds, None, false).unwrap();
⋮----
for start in 0..slots.len() {
for end in start..slots.len() {
⋮----
.map(|slot_index| {
if slots.contains(&slot_index) {
⋮----
pub fn test_repair_range_highest() {
⋮----
let (shreds, _) = make_slot_entries(
⋮----
let expected: Vec<ShredRepairType> = vec![
⋮----
pub fn test_generate_duplicate_repairs_for_slot() {
⋮----
assert!(
⋮----
.insert_shreds(shreds[..shreds.len() - 1].to_vec(), None, false)
⋮----
.insert_shreds(vec![shreds.pop().unwrap()], None, false)
⋮----
pub fn test_generate_and_send_duplicate_repairs() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
bank_forks.read().unwrap().sharable_banks(),
⋮----
let receive_socket = &bind_to_localhost_unique().expect("should bind - receive socket");
⋮----
let (mut shreds, _) = make_slot_entries(dead_slot, dead_slot - 1, num_entries_per_slot);
⋮----
duplicate_slot_repair_statuses.insert(dead_slot, duplicate_status);
⋮----
&bind_to_localhost_unique().expect("should bind - repair socket"),
⋮----
assert!(duplicate_slot_repair_statuses
⋮----
assert!(duplicate_slot_repair_statuses.contains_key(&dead_slot));
⋮----
.get_mut(&dead_slot)
⋮----
Some((Pubkey::default(), receive_socket.local_addr().unwrap()));
⋮----
assert_eq!(duplicate_slot_repair_statuses.len(), 1);
⋮----
assert!(duplicate_slot_repair_statuses.is_empty());
⋮----
pub fn test_update_duplicate_slot_repair_addr() {
⋮----
let dummy_addr = Some((
⋮----
bind_to_localhost_unique()
.expect("should bind - dummy socket")
.local_addr()
.unwrap(),
⋮----
let my_pubkey = cluster_info.id();
⋮----
cluster_slots.fake_epoch_info_for_tests(HashMap::from([(*valid_repair_peer.pubkey(), 42)]));
cluster_slots.insert_node_id(dead_slot, *valid_repair_peer.pubkey());
cluster_info.insert_info(valid_repair_peer);
⋮----
assert_eq!(duplicate_status.repair_pubkey_and_addr, dummy_addr);
⋮----
assert!(duplicate_status.repair_pubkey_and_addr.is_some());
⋮----
start_ts: timestamp() - MAX_DUPLICATE_WAIT_MS as u64,
⋮----
assert_ne!(duplicate_status.repair_pubkey_and_addr, dummy_addr);
⋮----
fn test_generate_repairs_for_wen_restart() {
⋮----
let slots: Vec<u64> = vec![2, 3, 5, 7];
let num_entries_per_slot = max_ticks_per_n_shreds(3, None) + 1;
⋮----
for (i, (mut slot_shreds, _)) in shreds.into_iter().enumerate() {
slot_shreds.remove(i);
⋮----
let mut slots_to_repair: Vec<Slot> = vec![];
⋮----
assert!(result.is_empty());
slots_to_repair = vec![3, 81];
⋮----
slots_to_repair = vec![2, 82, 7, 83, 84];
⋮----
assert_eq!(result.len(), max_repairs);

================
File: core/src/repair/repair_weight.rs
================
enum TreeRoot {
⋮----
impl TreeRoot {
pub fn is_pruned(&self) -> bool {
matches!(self, Self::PrunedRoot(_))
⋮----
pub fn slot(&self) -> Slot {
⋮----
fn from(val: TreeRoot) -> Self {
⋮----
pub struct RepairWeight {
⋮----
impl RepairWeight {
pub fn new(root: Slot) -> Self {
⋮----
pub fn add_voters(
⋮----
let mut tree_root = self.get_tree_root(slot);
⋮----
if tree_root.is_none() {
⋮----
self.find_ancestor_subtree_of_slot(blockstore, slot);
⋮----
match (tree_root, *new_ancestors.front().unwrap_or(&slot)) {
(Some(tree_root), _) if !tree_root.is_pruned() => (
⋮----
.get_mut(&tree_root.into())
.expect("If tree root was found, it must exist in `self.trees`"),
⋮----
self.pruned_trees.get_mut(&tree_root.into()).expect(
⋮----
let next_earliest_ancestor = *new_ancestors.get(1).unwrap_or(&slot);
assert!(next_earliest_ancestor > self.root);
self.insert_new_pruned_tree(next_earliest_ancestor);
assert_eq!(Some(earliest_ancestor), new_ancestors.pop_front());
⋮----
self.pruned_trees.get_mut(&next_earliest_ancestor).unwrap(),
⋮----
self.insert_new_tree(earliest_ancestor);
⋮----
self.trees.get_mut(&earliest_ancestor).unwrap(),
⋮----
new_ancestors.push_back(slot);
if new_ancestors.len() > 1 {
for i in 0..new_ancestors.len() - 1 {
tree.add_new_leaf_slot(
⋮----
Some((new_ancestors[i], Hash::default())),
⋮----
self.slot_to_tree.insert(new_ancestors[i + 1], tree_root);
⋮----
let subtree_updates = all_subtree_updates.entry(tree_root).or_default();
⋮----
let cur_max = subtree_updates.entry(pubkey).or_default();
⋮----
.get_tree_mut(tree_root)
.expect("Tree for `tree_root` must exist here");
let updates: Vec<_> = updates.into_iter().collect();
tree.add_votes(
⋮----
.iter()
.map(|(pubkey, slot)| (*pubkey, (*slot, Hash::default()))),
⋮----
pub fn get_best_weighted_repairs(
⋮----
let mut repairs = vec![];
⋮----
self.get_best_orphans(
⋮----
let num_orphan_slots = processed_slots.len() - 1;
let num_orphan_repairs = repairs.len();
get_best_orphans_elapsed.stop();
⋮----
self.get_best_shreds(
⋮----
let num_best_shreds_repairs = best_shreds_repairs.len();
⋮----
best_shreds_repairs.iter().map(|r| r.slot()).collect();
let num_best_shreds_slots = repair_slots_set.len();
processed_slots.extend(repair_slots_set);
repairs.extend(best_shreds_repairs);
get_best_shreds_elapsed.stop();
⋮----
let pre_num_slots = processed_slots.len();
let unknown_last_index_repairs = self.get_best_unknown_last_index(
⋮----
let num_unknown_last_index_repairs = unknown_last_index_repairs.len();
let num_unknown_last_index_slots = processed_slots.len() - pre_num_slots;
repairs.extend(unknown_last_index_repairs);
get_unknown_last_index_elapsed.stop();
⋮----
let (closest_completion_repairs, total_slots_processed) = self.get_best_closest_completion(
⋮----
let num_closest_completion_repairs = closest_completion_repairs.len();
let num_closest_completion_slots = processed_slots.len() - pre_num_slots;
⋮----
total_slots_processed.saturating_sub(num_closest_completion_slots);
repairs.extend(closest_completion_repairs);
get_closest_completion_elapsed.stop();
repair_metrics.best_repairs_stats.update(
⋮----
self.trees.len() as u64,
⋮----
repair_metrics.timing.get_best_orphans_elapsed += get_best_orphans_elapsed.as_us();
repair_metrics.timing.get_best_shreds_elapsed += get_best_shreds_elapsed.as_us();
⋮----
get_unknown_last_index_elapsed.as_us();
⋮----
get_closest_completion_elapsed.as_us();
⋮----
pub fn split_off(&mut self, slot: Slot) -> HashSet<Slot> {
assert!(slot >= self.root);
⋮----
error!("Trying to orphan root of repair tree {slot}");
⋮----
match self.slot_to_tree.get(&slot).copied() {
⋮----
info!("{slot} is already orphan, skipping");
⋮----
.get_mut(&subtree_root)
.expect("`self.slot_to_tree` and `self.trees` must be in sync");
let orphaned_tree = subtree.split_off(&(slot, Hash::default()));
self.rename_tree_root(&orphaned_tree, TreeRoot::Root(slot));
self.trees.insert(slot, orphaned_tree);
self.trees.get(&slot).unwrap().slots_iter().collect()
⋮----
info!("Dumping pruned slot {slot} of tree {subtree_root} in repair");
⋮----
.remove(&subtree_root)
.expect("`self.slot_to_tree` and `self.pruned_trees` must be in sync");
⋮----
self.rename_tree_root(&subtree, TreeRoot::Root(subtree_root));
self.trees.insert(subtree_root, subtree);
⋮----
.get(&subtree_root)
.unwrap()
.slots_iter()
.collect()
⋮----
self.pruned_trees.insert(subtree_root, subtree);
⋮----
warn!("Trying to split off slot {slot} which doesn't currently exist in repair");
⋮----
pub fn set_root(&mut self, new_root: Slot) {
assert!(self.root <= new_root);
⋮----
.get(&new_root)
.copied()
.map(|root| match root {
⋮----
panic!("New root {new_root} chains to a pruned tree with root {r}")
⋮----
.keys()
.filter(|subtree_root| {
⋮----
.map(|new_root_tree_root| **subtree_root != new_root_tree_root)
.unwrap_or(true)
⋮----
.collect();
⋮----
.expect("Must exist, was found in `self.trees` above");
self.rename_tree_root(&subtree, TreeRoot::PrunedRoot(subtree_root));
⋮----
.remove(&new_root_tree_root)
.expect("Found slot root earlier in self.slot_to_trees, tree must exist");
trace!("pruning tree {new_root_tree_root} with {new_root}");
let (removed, pruned) = new_root_tree.purge_prune((new_root, Hash::default()));
⋮----
let pruned_tree_root = pruned_tree.tree_root().0;
self.rename_tree_root(&pruned_tree, TreeRoot::PrunedRoot(pruned_tree_root));
self.pruned_trees.insert(pruned_tree_root, pruned_tree);
⋮----
self.slot_to_tree.remove(&slot);
⋮----
new_root_tree.set_tree_root((new_root, Hash::default()));
self.rename_tree_root(&new_root_tree, TreeRoot::Root(new_root));
self.trees.insert(new_root, new_root_tree);
⋮----
self.insert_new_tree(new_root);
⋮----
.drain()
.flat_map(|(tree_root, mut pruned_tree)| {
⋮----
trace!("pruning tree {tree_root} with {new_root}");
let (removed, pruned) = pruned_tree.purge_prune((new_root, Hash::default()));
⋮----
.into_iter()
.chain(iter::once(pruned_tree))
.filter(|pruned_tree| !pruned_tree.is_empty())
.map(|new_pruned_subtree| {
let new_pruned_tree_root = new_pruned_subtree.tree_root().0;
for ((slot, _), _) in new_pruned_subtree.all_slots_stake_voted_subtree()
⋮----
*self.slot_to_tree.get_mut(slot).unwrap() =
⋮----
vec![(tree_root, pruned_tree)]
⋮----
pub fn root(&self) -> Slot {
⋮----
fn get_best_shreds(
⋮----
let root_tree = self.trees.get(&self.root).expect("Root tree must exist");
⋮----
fn get_best_orphans(
⋮----
.map(|(slot, tree)| {
⋮----
tree.stake_voted_subtree(&(*slot, Hash::default()))
.expect("Tree must have weight at its own root"),
⋮----
if processed_slots.contains(&heaviest_tree_root) {
⋮----
if self.trees.contains_key(&heaviest_tree_root) {
let new_orphan_root = self.update_orphan_ancestors(
⋮----
repairs.push(repair_request);
processed_slots.insert(new_orphan_root);
⋮----
for new_orphan in blockstore.orphans_iterator(self.root + 1).unwrap() {
⋮----
processed_slots.insert(new_orphan);
⋮----
fn get_best_unknown_last_index(
⋮----
for (_slot, tree) in self.trees.iter() {
if repairs.len() >= max_new_repairs {
⋮----
let new_repairs = get_unknown_last_index(
⋮----
max_new_repairs - repairs.len(),
⋮----
repairs.extend(new_repairs);
⋮----
fn get_best_closest_completion(
⋮----
let (new_repairs, new_processed_slots) = get_closest_completion(
⋮----
fn update_orphan_ancestors(
⋮----
assert!(self.trees.contains_key(&orphan_tree_root));
⋮----
self.find_ancestor_subtree_of_slot(blockstore, orphan_tree_root);
⋮----
.get_mut(&orphan_tree_root)
.expect("Orphan must exist");
let num_skip = usize::from(parent_tree_root.is_some());
for ancestor in new_ancestors.iter().skip(num_skip).rev() {
⋮----
.insert(*ancestor, TreeRoot::Root(orphan_tree_root));
heaviest_tree.add_root_parent((*ancestor, Hash::default()));
⋮----
self.merge_trees(
⋮----
.front()
.expect("Must exist leaf to merge to if `tree_to_merge`.is_some()"),
⋮----
if let Some(earliest_ancestor) = new_ancestors.front() {
assert!(*earliest_ancestor != self.root);
⋮----
.remove(&orphan_tree_root)
.expect("orphan tree must exist");
⋮----
self.rename_tree_root(&orphan_tree, TreeRoot::Root(*earliest_ancestor));
assert!(self.trees.insert(*earliest_ancestor, orphan_tree).is_none());
⋮----
new_ancestors.get(1).unwrap_or(&orphan_tree_root);
self.rename_tree_root(
⋮----
assert!(self
⋮----
Some(orphan_tree_root)
⋮----
pub fn get_popular_pruned_forks(
⋮----
for (pruned_root, pruned_tree) in self.pruned_trees.iter() {
⋮----
.map(|slot| {
⋮----
.get(&epoch_schedule.get_epoch(slot))
.expect("Pruned tree cannot contain slots more than an epoch behind")
.total_stake()
⋮----
.min()
.expect("Pruned tree cannot be empty");
⋮----
.stake_voted_subtree(&slot_to_start_repair)
.expect("Root of tree must exist")
⋮----
.children(&slot_to_start_repair)
.expect("Found earlier, this slot should exist")
.find(|c| {
⋮----
.stake_voted_subtree(c)
.expect("Found in children must exist")
⋮----
repairs.push(slot_to_start_repair.0);
⋮----
fn get_tree(&self, tree_root: TreeRoot) -> Option<&HeaviestSubtreeForkChoice> {
⋮----
TreeRoot::Root(r) => self.trees.get(&r),
TreeRoot::PrunedRoot(r) => self.pruned_trees.get(&r),
⋮----
fn get_tree_mut(&mut self, tree_root: TreeRoot) -> Option<&mut HeaviestSubtreeForkChoice> {
⋮----
TreeRoot::Root(r) => self.trees.get_mut(&r),
TreeRoot::PrunedRoot(r) => self.pruned_trees.get_mut(&r),
⋮----
fn remove_tree(&mut self, tree_root: TreeRoot) -> Option<HeaviestSubtreeForkChoice> {
⋮----
TreeRoot::Root(r) => self.trees.remove(&r),
TreeRoot::PrunedRoot(r) => self.pruned_trees.remove(&r),
⋮----
fn get_tree_root(&self, slot: Slot) -> Option<TreeRoot> {
self.slot_to_tree.get(&slot).copied()
⋮----
pub fn is_pruned(&self, slot: Slot) -> bool {
self.get_tree_root(slot)
.as_ref()
.map(TreeRoot::is_pruned)
.unwrap_or(false)
⋮----
pub fn same_tree(&self, slot1: Slot, slot2: Slot) -> bool {
self.get_tree_root(slot1)
.and_then(|tree_root| self.get_tree(tree_root))
.map(|tree| tree.contains_block(&(slot2, Hash::default())))
⋮----
fn insert_new_tree(&mut self, new_tree_root: Slot) {
assert!(!self.trees.contains_key(&new_tree_root));
⋮----
.insert(new_tree_root, TreeRoot::Root(new_tree_root));
self.trees.insert(
⋮----
fn insert_new_pruned_tree(&mut self, new_pruned_tree_root: Slot) {
assert!(!self.pruned_trees.contains_key(&new_pruned_tree_root));
self.slot_to_tree.insert(
⋮----
self.pruned_trees.insert(
⋮----
fn find_ancestor_subtree_of_slot(
⋮----
ancestors_to_add.push_front(a);
tree = self.get_tree_root(a);
if tree.is_some() || a < self.root {
⋮----
fn merge_trees(
⋮----
let tree1 = self.remove_tree(root1).expect("tree to merge must exist");
self.rename_tree_root(&tree1, root2);
⋮----
.get_tree_mut(root2)
.expect("tree to be merged into must exist");
tree2.merge(
⋮----
fn rename_tree_root(&mut self, tree1: &HeaviestSubtreeForkChoice, root2: TreeRoot) {
let all_slots = tree1.all_slots_stake_voted_subtree();
⋮----
.get_mut(slot)
.expect("Nodes in tree must exist in `self.slot_to_tree`") = root2;
⋮----
fn sort_by_stake_weight_slot(slot_stake_voted: &mut [(Slot, u64)]) {
slot_stake_voted.sort_by(|(slot, stake_voted), (slot_, stake_voted_)| {
⋮----
slot.cmp(slot_)
⋮----
stake_voted.cmp(stake_voted_).reverse()
⋮----
mod test {
⋮----
fn test_sort_by_stake_weight_slot() {
let mut slots = vec![(3, 30), (2, 30), (5, 31)];
⋮----
assert_eq!(slots, vec![(5, 31), (2, 30), (3, 30)]);
⋮----
fn test_add_votes_invalid() {
let (blockstore, bank, mut repair_weight) = setup_orphan_repair_weight();
⋮----
repair_weight.set_root(root);
⋮----
assert!(repair_weight
⋮----
assert!(!repair_weight.slot_to_tree.contains(old_slot));
⋮----
let votes = vec![(*old_slot, vec![Pubkey::default()])];
repair_weight.add_voters(
⋮----
votes.into_iter(),
bank.epoch_stakes_map(),
bank.epoch_schedule(),
⋮----
assert!(repair_weight.pruned_trees.contains_key(old_slot));
⋮----
assert!(!repair_weight.trees.contains_key(old_slot));
assert!(!repair_weight.slot_to_tree.contains_key(old_slot));
⋮----
fn test_add_votes() {
let blockstore = setup_forks();
⋮----
let votes = vec![(1, vote_pubkeys.clone())];
⋮----
assert_eq!(repair_weight.trees.len(), 1);
assert_eq!(
⋮----
let votes = vec![(4, vote_pubkeys.clone()), (6, vote_pubkeys)];
⋮----
.get(&0)
⋮----
.stake_voted_at(&(slot, Hash::default()))
.unwrap();
⋮----
assert_eq!(stake_voted_at, 3 * stake);
⋮----
assert_eq!(stake_voted_at, 0);
⋮----
.stake_voted_subtree(&(*slot, Hash::default()))
⋮----
assert_eq!(stake_voted_subtree, 3 * stake);
⋮----
assert_eq!(stake_voted_subtree, 0);
⋮----
fn test_add_votes_orphans() {
let blockstore = setup_orphans();
⋮----
let votes = vec![(1, vote_pubkeys.clone()), (8, vote_pubkeys.clone())];
⋮----
assert_eq!(repair_weight.trees.len(), 2);
⋮----
let votes = vec![(1, vote_pubkeys.clone()), (10, vote_pubkeys.clone())];
⋮----
blockstore.add_tree(tr(6) / (tr(8)), true, true, 2, Hash::default());
⋮----
let votes = vec![(11, vote_pubkeys)];
⋮----
repair_weight.update_orphan_ancestors(
⋮----
assert!(repair_weight.trees.contains_key(&0));
⋮----
fn test_add_votes_pruned() {
⋮----
blockstore.add_tree(tr(4) / tr(8), true, true, 2, Hash::default());
⋮----
let votes = vec![(6, vote_pubkeys.clone()), (11, vote_pubkeys.clone())];
⋮----
assert_eq!(repair_weight.pruned_trees.len(), 0);
⋮----
repair_weight.set_root(2);
⋮----
assert_eq!(repair_weight.pruned_trees.len(), 1);
blockstore.add_tree(tr(6) / tr(20), true, true, 2, Hash::default());
let votes = vec![(23, vote_pubkeys.iter().take(1).copied().collect_vec())];
⋮----
let votes = vec![(23, vote_pubkeys.iter().skip(1).copied().collect_vec())];
⋮----
repair_weight.set_root(10);
blockstore.add_tree(
tr(7) / (tr(9) / (tr(12) / tr(13))),
⋮----
let votes = vec![(13, vote_pubkeys)];
⋮----
assert_eq!(repair_weight.pruned_trees.len(), 2);
⋮----
fn test_update_orphan_ancestors() {
⋮----
let votes = vec![
⋮----
assert_eq!(repair_weight.trees.len(), 3);
⋮----
assert!(repair_weight.trees.contains_key(&8));
assert!(repair_weight.trees.contains_key(&20));
⋮----
blockstore.add_tree(tr(11) / (tr(20)), true, true, 2, Hash::default());
⋮----
fn test_get_best_orphans() {
⋮----
let votes = vec![(8, vec![vote_pubkeys[0]]), (20, vec![vote_pubkeys[1]])];
⋮----
let mut processed_slots: HashSet<Slot> = vec![repair_weight.root].into_iter().collect();
repair_weight.get_best_orphans(
⋮----
assert_eq!(repairs.len(), 1);
assert_eq!(outstanding_repairs.len(), repairs.len());
assert_eq!(repairs[0].slot(), 8);
repairs = vec![];
⋮----
processed_slots = vec![repair_weight.root].into_iter().collect();
let votes = vec![(10, vec![vote_pubkeys[0]])];
⋮----
assert_eq!(repairs.len(), 2);
⋮----
assert_eq!(repairs[1].slot(), 20);
⋮----
let votes = vec![(20, vec![vote_pubkeys[0]])];
⋮----
assert_eq!(repairs[0].slot(), 20);
⋮----
assert!(repairs.is_empty());
assert!(outstanding_repairs.is_empty());
⋮----
fn test_get_extra_orphans() {
⋮----
let votes = vec![(8, vec![vote_pubkeys[0]])];
⋮----
blockstore.add_tree(tr(100) / (tr(101)), true, true, 2, Hash::default());
⋮----
assert_eq!(repairs.len(), 3);
⋮----
assert_eq!(repairs[2].slot(), 100);
⋮----
fn test_set_root() {
let (_, _, mut repair_weight) = setup_orphan_repair_weight();
repair_weight.set_root(1);
check_old_root_purged_verify_new_root(0, 1, &repair_weight);
assert!(repair_weight.pruned_trees.is_empty());
⋮----
assert_eq!(repair_weight.trees.get(&1).unwrap().tree_root().0, 1);
⋮----
fn test_set_missing_root() {
⋮----
assert!(!repair_weight.slot_to_tree.contains_key(&missing_root_slot));
repair_weight.set_root(missing_root_slot);
check_old_root_purged_verify_new_root(0, missing_root_slot, &repair_weight);
⋮----
assert!(!repair_weight.slot_to_tree.contains_key(&slot));
⋮----
fn test_set_root_existing_non_root_tree() {
⋮----
check_old_root_purged_verify_new_root(0, 10, &repair_weight);
⋮----
assert!(!repair_weight.slot_to_tree.contains_key(&8));
assert_eq!(repair_weight.trees.get(&20).unwrap().tree_root().0, 20);
⋮----
fn test_set_root_check_pruned_slots() {
⋮----
blockstore.add_tree(tr(4) / (tr(8)), true, true, 2, Hash::default());
⋮----
repair_weight.set_root(3);
check_old_root_purged_verify_new_root(0, 3, &repair_weight);
⋮----
assert!(!repair_weight.slot_to_tree.contains_key(&purged_slot));
assert!(!repair_weight.trees.contains_key(&purged_slot));
⋮----
assert!(!repair_weight.slot_to_tree.contains_key(&30));
repair_weight.set_root(30);
check_old_root_purged_verify_new_root(3, 30, &repair_weight);
⋮----
assert_eq!(repair_weight.slot_to_tree.len(), 1);
assert_eq!(repair_weight.root, 30);
assert_eq!(repair_weight.trees.get(&30).unwrap().tree_root().0, 30);
⋮----
fn test_set_root_pruned_tree_trim_and_cleanup() {
let blockstore = setup_big_forks();
⋮----
repair_weight.set_root(9);
⋮----
assert!(!repair_weight.slot_to_tree.contains(&6));
⋮----
assert!(!repair_weight.slot_to_tree.contains(&4));
⋮----
repair_weight.set_root(20);
⋮----
assert!(!repair_weight.slot_to_tree.contains(&11));
⋮----
fn test_set_root_pruned_tree_split() {
⋮----
repair_weight.set_root(4);
⋮----
assert_eq!(repair_weight.pruned_trees.len(), 3);
⋮----
fn test_add_votes_update_orphans_unrooted() {
⋮----
assert!(!repair_weight.slot_to_tree.contains_key(old_parent));
assert!(!repair_weight.trees.contains_key(old_parent));
assert!(!repair_weight.pruned_trees.contains_key(old_parent));
⋮----
blockstore.add_tree(tr(*old_parent) / (tr(8)), true, true, 2, Hash::default());
blockstore.add_tree(tr(8) / (tr(20)), true, true, 2, Hash::default());
⋮----
assert!(repair_weight.slot_to_tree.get(slot).unwrap().is_pruned());
⋮----
assert!(!repair_weight.slot_to_tree.contains_key(slot));
⋮----
assert_eq!(repair_weight.pruned_trees.get(&4).unwrap().tree_root().0, 4);
⋮----
assert_eq!(repair_weight.pruned_trees.get(&8).unwrap().tree_root().0, 8);
⋮----
tr(*old_parent) / tr(new_vote_slot),
⋮----
vec![(new_vote_slot, vec![Pubkey::default()])].into_iter(),
⋮----
fn test_find_ancestor_subtree_of_slot() {
let (blockstore, _, mut repair_weight) = setup_orphan_repair_weight();
⋮----
repair_weight.remove_tree(TreeRoot::Root(20)).unwrap();
repair_weight.insert_new_pruned_tree(20);
⋮----
blockstore.add_tree(tr(30) / (tr(31)), true, true, 2, Hash::default());
⋮----
repair_weight.set_root(5);
⋮----
blockstore.add_tree(tr(8) / (tr(40) / tr(41)), true, true, 2, Hash::default());
⋮----
fn test_split_off_copy_weight() {
⋮----
vec![(6, vote_pubkeys)].into_iter(),
⋮----
blockstore.clear_unconfirmed_slot(3);
repair_weight.split_off(3);
blockstore.clear_unconfirmed_slot(10);
repair_weight.split_off(10);
let mut orphans = repair_weight.trees.keys().copied().collect_vec();
orphans.sort();
assert_eq!(vec![0, 3, 8, 10, 20], orphans);
⋮----
let mut processed_slots = vec![repair_weight.root].into_iter().collect();
⋮----
assert_eq!(repairs.len(), 4);
assert_eq!(repairs[0].slot(), 10);
⋮----
assert_eq!(repairs[2].slot(), 3);
assert_eq!(repairs[3].slot(), 8);
⋮----
fn test_split_off_multi_dump_repair() {
⋮----
blockstore.clear_unconfirmed_slot(5);
repair_weight.split_off(5);
blockstore.clear_unconfirmed_slot(6);
repair_weight.split_off(6);
⋮----
assert_eq!(vec![0, 3, 5, 6], orphans);
⋮----
assert_eq!(repairs[0].slot(), 6);
assert_eq!(repairs[1].slot(), 3);
assert_eq!(repairs[2].slot(), 5);
⋮----
for (shreds, _) in make_chaining_slot_entries(&[5, 6], 100, 0) {
blockstore.insert_shreds(shreds, None, true).unwrap();
⋮----
assert_eq!(repairs[0].slot(), 3);
⋮----
assert_eq!(orphans, vec![0, 3]);
⋮----
fn test_get_popular_pruned_forks() {
⋮----
let epoch_stakes = bank.epoch_stakes_map();
let epoch_schedule = bank.epoch_schedule();
⋮----
let five_votes = vote_pubkeys.iter().copied().take(5).collect_vec();
let votes = vec![(11, five_votes.clone()), (6, five_votes)];
⋮----
let votes = vec![(11, vec![vote_pubkeys[5]]), (6, vec![vote_pubkeys[6]])];
⋮----
let six_votes = vote_pubkeys.iter().copied().take(6).collect_vec();
let votes = vec![(20, six_votes)];
⋮----
fn test_get_popular_pruned_forks_forks() {
⋮----
.iter_mut()
.for_each(|(_, s)| *s = TreeRoot::PrunedRoot(s.slot()));
let mut repair_weight_20 = repair_weight.clone();
repair_weight_20.add_voters(
⋮----
vec![(20, vote_pubkeys.clone())].into_iter(),
⋮----
let votes = vec![(10, vote_pubkeys.iter().copied().skip(6).collect_vec())];
⋮----
fn test_get_popular_pruned_forks_stake_change_across_epoch_boundary() {
⋮----
let mut epoch_stakes = bank.epoch_stakes_map().clone();
let mut epoch_schedule = bank.epoch_schedule().clone();
⋮----
.get(&epoch_schedule.get_epoch(0))
⋮----
.clone();
⋮----
dec_stakes.set_total_stake(dec_stakes.total_stake() - 5 * stake);
inc_stakes.set_total_stake(dec_stakes.total_stake() + 3 * stake);
epoch_stakes.insert(epoch_schedule.get_epoch(0), initial_stakes);
epoch_stakes.insert(epoch_schedule.get_epoch(10), dec_stakes);
epoch_stakes.insert(epoch_schedule.get_epoch(20), inc_stakes);
⋮----
let four_votes = vote_pubkeys.iter().copied().take(4).collect_vec();
⋮----
fn setup_orphan_repair_weight() -> (Blockstore, Bank, RepairWeight) {
⋮----
assert!(repair_weight.slot_to_tree.contains_key(&0));
⋮----
fn check_old_root_purged_verify_new_root(
⋮----
assert!(!repair_weight.trees.contains_key(&old_root));
assert!(!repair_weight.slot_to_tree.contains_key(&old_root));
⋮----
assert_eq!(repair_weight.root, new_root);
⋮----
fn setup_orphans() -> Blockstore {
⋮----
blockstore.add_tree(tr(8) / (tr(10) / (tr(11))), true, true, 2, Hash::default());
blockstore.add_tree(tr(20) / (tr(22) / (tr(23))), true, true, 2, Hash::default());
assert!(blockstore.orphan(8).unwrap().is_some());
⋮----
fn setup_big_forks() -> Blockstore {
⋮----
blockstore.add_tree(tr(2) / tr(8), true, true, 2, Hash::default());
blockstore.add_tree(tr(3) / (tr(9) / tr(20)), true, true, 2, Hash::default());
⋮----
fn setup_forks() -> Blockstore {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3) / (tr(5) / (tr(6)))));
let ledger_path = get_tmp_ledger_path!();
let blockstore = Blockstore::open(&ledger_path).unwrap();
blockstore.add_tree(forks, false, true, 2, Hash::default());

================
File: core/src/repair/repair_weighted_traversal.rs
================
enum Visit {
⋮----
impl Visit {
pub fn slot(&self) -> Slot {
⋮----
struct RepairWeightTraversal<'a> {
⋮----
fn new(tree: &'a HeaviestSubtreeForkChoice) -> Self {
⋮----
pending: vec![Visit::Unvisited(tree.tree_root().0)],
⋮----
impl Iterator for RepairWeightTraversal<'_> {
type Item = Visit;
fn next(&mut self) -> Option<Self::Item> {
let next = self.pending.pop();
next.map(|next| {
⋮----
self.pending.push(Visit::Visited(slot));
⋮----
.children(&(slot, Hash::default()))
.unwrap()
.map(|(child_slot, _)| Visit::Unvisited(*child_slot))
.collect();
children.sort_by(|slot1, slot2| {
self.tree.max_by_weight(
(slot1.slot(), Hash::default()),
(slot2.slot(), Hash::default()),
⋮----
self.pending.extend(children);
⋮----
pub fn get_best_repair_shreds(
⋮----
let initial_len = repairs.len();
⋮----
if repairs.len() > max_repairs {
⋮----
.entry(next.slot())
.or_insert_with(|| blockstore.meta(next.slot()).unwrap());
⋮----
max_repairs - repairs.len(),
⋮----
repairs.extend(new_repairs);
visited_set.insert(slot);
⋮----
if !visited_set.contains(new_child_slot) {
⋮----
visited_set.insert(*new_child_slot);
⋮----
pub mod test {
⋮----
fn test_weighted_repair_traversal_single() {
⋮----
let steps: Vec<_> = weighted_traversal.collect();
assert_eq!(steps, vec![Visit::Unvisited(42), Visit::Visited(42)]);
⋮----
fn test_weighted_repair_traversal() {
⋮----
let (_, mut heaviest_subtree_fork_choice) = setup_forks();
⋮----
assert_eq!(
⋮----
heaviest_subtree_fork_choice.add_votes(
[(vote_pubkeys[0], (5, Hash::default()))].iter(),
bank.epoch_stakes_map(),
bank.epoch_schedule(),
⋮----
fn test_get_best_repair_shreds() {
let (blockstore, heaviest_subtree_fork_choice) = setup_forks();
let mut repairs = vec![];
⋮----
let last_shred = blockstore.meta(0).unwrap().unwrap().received;
sleep_shred_deferment_period();
get_best_repair_shreds(
⋮----
assert_eq!(repairs.len(), outstanding_repairs.len());
repairs = vec![];
⋮----
let best_overall_slot = heaviest_subtree_fork_choice.best_overall_slot().0;
assert_eq!(best_overall_slot, 4);
blockstore.add_tree(
tr(best_overall_slot) / (tr(6) / tr(7)),
⋮----
.iter()
.flat_map(|slot| {
let shredder = Shredder::new(*slot, slot.saturating_sub(1), 0, 42).unwrap();
let (shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
.insert_shreds(completed_shreds, None, false)
.unwrap();
⋮----
blockstore.add_tree(tr(2) / (tr(8)), true, false, 2, Hash::default());
⋮----
.map(|slot| ShredRepairType::HighestShred(*slot, last_shred))
⋮----
assert_eq!(repairs, expected_repairs);
⋮----
fn test_get_best_repair_shreds_no_duplicates() {
⋮----
blockstore.add_tree(tr(2) / (tr(6) / tr(7)), true, false, 2, Hash::default());
⋮----
fn setup_forks() -> (Blockstore, HeaviestSubtreeForkChoice) {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3) / (tr(5))));
let ledger_path = get_tmp_ledger_path!();
let blockstore = Blockstore::open(&ledger_path).unwrap();
blockstore.add_tree(forks.clone(), false, false, 2, Hash::default());

================
File: core/src/repair/request_response.rs
================
pub trait RequestResponse {

================
File: core/src/repair/result.rs
================
pub enum RepairVerifyError {
⋮----
pub enum Error {
⋮----
pub type Result<T> = std::result::Result<T, Error>;

================
File: core/src/repair/serve_repair_service.rs
================
pub struct ServeRepairService {
⋮----
impl ServeRepairService {
pub(crate) fn new(
⋮----
let (request_sender, request_receiver) = unbounded();
⋮----
"solRcvrServeRep".to_string(),
serve_repair_socket.clone(),
exit.clone(),
⋮----
Some(Duration::from_millis(1)),
⋮----
.name(String::from("solServRAdapt"))
.spawn(|| adapt_repair_requests_packets(request_receiver, remote_request_sender))
.unwrap();
let (response_sender, response_receiver) = unbounded();
⋮----
Some(stats_reporter_sender),
⋮----
let t_listen = serve_repair.listen(
⋮----
let thread_hdls = vec![t_receiver, t_packet_adapter, t_responder, t_listen];
⋮----
pub(crate) fn join(self) -> thread::Result<()> {
self.thread_hdls.into_iter().try_for_each(JoinHandle::join)
⋮----
pub(crate) fn adapt_repair_requests_packets(
⋮----
let Some(bytes) = packet.data(..).map(Vec::from) else {
⋮----
remote_address: packet.meta().socket_addr(),
⋮----
if remote_request_sender.send(request).is_err() {

================
File: core/src/repair/serve_repair.rs
================
pub enum ShredRepairType {
⋮----
impl ShredRepairType {
pub fn slot(&self) -> Slot {
⋮----
impl RequestResponse for ShredRepairType {
type Response = [u8];
fn num_expected_responses(&self) -> u32 {
⋮----
fn verify_response(&self, shred: &Self::Response) -> bool {
⋮----
fn get_shred_index(shred: &[u8]) -> Option<u64> {
shred::layout::get_index(shred).map(u64::from)
⋮----
shred_slot == *slot && get_shred_index(shred) >= Some(*index)
⋮----
shred_slot == *slot && get_shred_index(shred) == Some(*index)
⋮----
pub struct AncestorHashesRepairType(pub Slot);
impl AncestorHashesRepairType {
⋮----
pub enum AncestorHashesResponse {
⋮----
impl RequestResponse for AncestorHashesRepairType {
type Response = AncestorHashesResponse;
⋮----
fn verify_response(&self, response: &AncestorHashesResponse) -> bool {
⋮----
AncestorHashesResponse::Hashes(hashes) => hashes.len() <= MAX_ANCESTOR_RESPONSES,
AncestorHashesResponse::Ping(ping) => ping.verify(),
⋮----
struct ServeRepairStats {
⋮----
pub struct RepairRequestHeader {
⋮----
impl RepairRequestHeader {
pub fn new(sender: Pubkey, recipient: Pubkey, timestamp: u64, nonce: Nonce) -> Self {
⋮----
type Ping = ping_pong::Ping<REPAIR_PING_TOKEN_SIZE>;
type PingCache = ping_pong::PingCache<REPAIR_PING_TOKEN_SIZE>;
⋮----
pub enum RepairProtocol {
⋮----
fn discard_malformed_repair_requests(
⋮----
let num_requests = requests.len();
requests.retain(|request| request.bytes.len() >= REPAIR_REQUEST_MIN_BYTES);
stats.err_malformed += num_requests - requests.len();
requests.len()
⋮----
pub(crate) enum RepairResponse {
⋮----
impl RepairProtocol {
fn sender(&self) -> Option<&Pubkey> {
⋮----
Self::Pong(pong) => Some(pong.from()),
Self::WindowIndex { header, .. } => Some(&header.sender),
Self::HighestWindowIndex { header, .. } => Some(&header.sender),
Self::Orphan { header, .. } => Some(&header.sender),
Self::AncestorHashes { header, .. } => Some(&header.sender),
⋮----
fn supports_signature(&self) -> bool {
⋮----
fn max_response_packets(&self) -> usize {
⋮----
fn max_response_bytes(&self) -> usize {
self.max_response_packets() * PACKET_DATA_SIZE
⋮----
pub struct ServeRepair {
⋮----
pub(crate) struct RepairPeers {
⋮----
struct Node {
⋮----
impl RepairPeers {
fn new(asof: Instant, peers: &[ContactInfo], weights: &[u64]) -> Result<Self> {
if peers.len() != weights.len() {
return Err(Error::from(WeightedError::InvalidWeight));
⋮----
.iter()
.zip(weights)
.filter_map(|(peer, &weight)| {
⋮----
pubkey: *peer.pubkey(),
serve_repair: peer.serve_repair(Protocol::UDP)?,
serve_repair_quic: peer.serve_repair(Protocol::QUIC)?,
⋮----
Some((node, weight))
⋮----
.unzip();
if peers.is_empty() {
return Err(Error::from(ClusterInfoError::NoPeers));
⋮----
Ok(Self {
⋮----
fn sample<R: Rng>(&self, rng: &mut R) -> &Node {
let index = self.weighted_index.sample(rng);
⋮----
struct RepairRequestWithMeta {
⋮----
impl ServeRepair {
pub fn new(
⋮----
pub fn new_for_test(
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
bank_forks.read().unwrap().sharable_banks(),
⋮----
pub(crate) fn my_id(&self) -> Pubkey {
self.cluster_info.id()
⋮----
fn handle_repair(
⋮----
let batch = self.repair_handler.run_window_request(
⋮----
if batch.is_none() {
⋮----
self.repair_handler.run_highest_window_request(
⋮----
self.repair_handler.run_orphan(
⋮----
.run_ancestor_hashes(recycler, from_addr, *slot, *nonce),
⋮----
ping_cache.add(pong, *from_addr, Instant::now());
⋮----
error!("Unexpected legacy request: {request:?}");
debug_assert!(
⋮----
Self::report_time_spent(label, &now.elapsed(), "");
⋮----
fn report_time_spent(label: &str, time: &Duration, extra: &str) {
let count = time.as_millis();
⋮----
info!("{label} took: {count} ms {extra}");
⋮----
fn decode_request(
⋮----
return Err(Error::from(RepairVerifyError::Malformed));
⋮----
if Some(&remote_pubkey) != request.sender() {
error!(
⋮----
if request.sender() == Some(my_id) {
error!("self repair: from_addr={from_addr} my_id={my_id} request={request:?}");
return Err(Error::from(RepairVerifyError::SelfRepair));
⋮----
.as_ref()
.and_then(|stakes| stakes.get(request.sender()?))
.unwrap_or(&0);
⋮----
.sender()
.map(|pubkey| whitelist.contains(pubkey))
.unwrap_or_default();
Ok(RepairRequestWithMeta {
⋮----
protocol: remote_request.protocol(),
⋮----
fn record_request_decode_error(error: &Error, stats: &mut ServeRepairStats) {
⋮----
debug_assert!(false, "unhandled error {error:?}");
⋮----
fn decode_requests(
⋮----
result.ok()
⋮----
requests.into_iter().filter_map(decode_request).collect()
⋮----
/// Process messages from the network
    fn run_listen(
⋮----
fn run_listen(
⋮----
let mut requests = vec![requests_receiver.recv_timeout(TIMEOUT)?];
⋮----
let mut total_requests = requests.len();
let socket_addr_space = *self.cluster_info.socket_addr_space();
let root_bank = self.sharable_banks.root();
let epoch_staked_nodes = root_bank.epoch_staked_nodes(root_bank.epoch());
let identity_keypair = self.cluster_info.keypair();
let my_id = identity_keypair.pubkey();
let max_buffered_packets = if !self.repair_whitelist.read().unwrap().is_empty() {
⋮----
let mut well_formed_requests = discard_malformed_repair_requests(&mut requests, stats);
⋮----
let mut more: Vec<_> = requests_receiver.try_iter().collect();
if more.is_empty() {
⋮----
total_requests += more.len();
⋮----
// Already exceeded max. Don't waste time discarding
dropped_requests += more.len();
⋮----
let retained = discard_malformed_repair_requests(&mut more, stats);
⋮----
requests.extend(more);
⋮----
let whitelist = self.repair_whitelist.read().unwrap();
⋮----
let whitelisted_request_count = decoded_requests.iter().filter(|r| r.whitelisted).count();
stats.decode_time_us += decode_start.elapsed().as_micros() as u64;
stats.whitelisted_requests += whitelisted_request_count.min(MAX_REQUESTS_PER_ITERATION);
if decoded_requests.len() > MAX_REQUESTS_PER_ITERATION {
stats.dropped_requests_low_stake += decoded_requests.len() - MAX_REQUESTS_PER_ITERATION;
decoded_requests.sort_unstable_by_key(|r| Reverse((r.whitelisted, r.stake)));
decoded_requests.truncate(MAX_REQUESTS_PER_ITERATION);
⋮----
self.handle_requests(
⋮----
stats.handle_requests_time_us += handle_requests_start.elapsed().as_micros() as u64;
Ok(())
⋮----
fn report_reset_stats(&self, stats: &mut ServeRepairStats) {
⋮----
let my_id = self.cluster_info.id();
warn!(
⋮----
datapoint_info!(
⋮----
pub(crate) fn listen(
⋮----
assert!(REPAIR_PING_CACHE_RATE_LIMIT_DELAY > Duration::from_millis(REPAIR_MS));
⋮----
.name("solRepairListen".to_string())
.spawn(move || {
⋮----
while !exit.load(Ordering::Relaxed) {
let result = self.run_listen(
⋮----
info!("repair listener disconnected");
⋮----
if last_print.elapsed().as_secs() > 2 {
self.report_reset_stats(&mut stats);
⋮----
data_budget.update(INTERVAL_MS, |_bytes| MAX_BYTES_PER_INTERVAL);
⋮----
.unwrap()
⋮----
fn verify_signed_packet(my_id: &Pubkey, bytes: &[u8], request: &RepairProtocol) -> Result<()> {
⋮----
return Err(Error::from(RepairVerifyError::Unsigned));
⋮----
if !pong.verify() {
return Err(Error::from(RepairVerifyError::SigVerify));
⋮----
return Err(Error::from(RepairVerifyError::IdMismatch));
⋮----
let time_diff_ms = timestamp().abs_diff(header.timestamp);
if u128::from(time_diff_ms) > SIGNED_REPAIR_TIME_WINDOW.as_millis() {
return Err(Error::from(RepairVerifyError::TimeSkew));
⋮----
let Some(leading_buf) = bytes.get(..4) else {
⋮----
let Some(trailing_buf) = bytes.get(4 + SIGNATURE_BYTES..) else {
⋮----
let Some(from_id) = request.sender() else {
⋮----
let signed_data = [leading_buf, trailing_buf].concat();
if !header.signature.verify(from_id.as_ref(), &signed_data) {
⋮----
fn check_ping_cache(
⋮----
.map(|&sender| {
ping_cache.check(
⋮----
Packet::from_data(Some(from_addr), ping).ok()
⋮----
fn handle_requests(
⋮----
} in requests.into_iter()
⋮----
if !data_budget.check(request.max_response_bytes()) {
⋮----
if !matches!(&request, RepairProtocol::Pong(_)) && protocol == Protocol::UDP {
⋮----
pending_pings.push(ping_pkt);
⋮----
let Some(rsp) = self.handle_repair(recycler, &from_addr, request, stats, ping_cache)
⋮----
let num_response_packets = rsp.len();
let num_response_bytes = rsp.iter().map(|p| p.meta().size).sum();
if data_budget.take(num_response_bytes)
&& send_response(
⋮----
if !pending_pings.is_empty() {
stats.pings_sent += pending_pings.len();
⋮----
let _ = packet_batch_sender.send(batch.into());
⋮----
pub fn ancestor_repair_request_bytes(
⋮----
sender: keypair.pubkey(),
⋮----
timestamp: timestamp(),
⋮----
pub(crate) fn repair_request(
⋮----
let slot = repair_request.slot();
let repair_peers = match peers_cache.get(&slot) {
Some(entry) if entry.asof.elapsed() < REPAIR_PEERS_CACHE_TTL => entry,
⋮----
peers_cache.pop(&slot);
⋮----
self.repair_peers(repair_validators, slot, &identity_keypair.pubkey());
let weights = cluster_slots.compute_weights(slot, &repair_peers);
⋮----
peers_cache.put(slot, repair_peers);
peers_cache.get(&slot).unwrap()
⋮----
let peer = repair_peers.sample(&mut rand::thread_rng());
let nonce = outstanding_requests.add_request(repair_request, timestamp());
let out = self.map_repair_request(
⋮----
debug!(
⋮----
Protocol::UDP => Ok(Some((peer.serve_repair, out))),
⋮----
.blocking_send((peer.serve_repair_quic, Bytes::from(out)))
.map_err(|_| Error::SendError)?;
Ok(None)
⋮----
pub(crate) fn repair_request_ancestor_hashes_sample_peers(
⋮----
let repair_peers: Vec<_> = self.repair_peers(repair_validators, slot, my_pubkey);
if repair_peers.is_empty() {
return Err(ClusterInfoError::NoPeers.into());
⋮----
let (weights, index) = cluster_slots.compute_weights_exclude_nonfrozen(slot, &repair_peers);
⋮----
.shuffle(&mut rand::thread_rng())
.map(|i| index[i])
.filter_map(|i| {
let addr = repair_peers[i].serve_repair(repair_protocol)?;
Some((*repair_peers[i].pubkey(), addr))
⋮----
.take(get_ancestor_hash_repair_sample_size())
.collect();
Ok(peers)
⋮----
pub(crate) fn repair_request_duplicate_compute_best_peer(
⋮----
.ok()?
.sample(&mut rand::thread_rng());
⋮----
Some((
*repair_peers[n].pubkey(),
repair_peers[n].serve_repair(Protocol::UDP)?,
⋮----
pub(crate) fn map_repair_request(
⋮----
sender: identity_keypair.pubkey(),
⋮----
.update(repair_peer_id, *slot, *shred_index);
⋮----
repair_stats.orphan.update(repair_peer_id, *slot, 0);
⋮----
pub(crate) fn handle_repair_response_pings(
⋮----
for mut packet in packet_batch.iter_mut() {
if packet.meta().size != REPAIR_RESPONSE_SERIALIZED_PING_BYTES {
⋮----
if let Ok(RepairResponse::Ping(ping)) = packet.deserialize_slice(..) {
if !ping.verify() {
⋮----
packet.meta_mut().set_discard(true);
⋮----
let from_addr = packet.meta().socket_addr();
pending_pongs.push((pong, from_addr));
⋮----
if !pending_pongs.is_empty() {
let num_pkts = pending_pongs.len();
let pending_pongs = pending_pongs.iter().map(|(bytes, addr)| (bytes, addr));
match batch_send(repair_socket, pending_pongs) {
⋮----
pub fn repair_proto_to_bytes(request: &RepairProtocol, keypair: &Keypair) -> Result<Vec<u8>> {
debug_assert!(request.supports_signature());
let mut payload = serialize(&request)?;
let signable_data = [&payload[..4], &payload[4 + SIGNATURE_BYTES..]].concat();
let signature = keypair.sign_message(&signable_data[..]);
payload[4..4 + SIGNATURE_BYTES].copy_from_slice(signature.as_ref());
Ok(payload)
⋮----
fn repair_peers(
⋮----
.filter_map(|key| {
⋮----
self.cluster_info.lookup_contact_info(key, |ci| ci.clone())
⋮----
.collect()
⋮----
self.cluster_info.repair_peers(slot)
⋮----
pub(crate) fn get_repair_protocol(_: ClusterType) -> Protocol {
⋮----
pub(crate) fn deserialize_request<T>(
⋮----
.with_limit(request.bytes.len() as u64)
.with_fixint_encoding()
.reject_trailing_bytes()
.deserialize(&request.bytes)
⋮----
fn send_response(
⋮----
Protocol::UDP => packet_batch_sender.send(packets).is_ok(),
⋮----
.filter_map(|packet| {
let bytes = Bytes::from(Vec::from(packet.data(..)?));
Some((packet.meta().socket_addr(), bytes))
⋮----
.all(|packet| repair_response_quic_sender.blocking_send(packet).is_ok()),
⋮----
mod tests {
⋮----
fn test_serialized_ping_size() {
⋮----
let ping = Ping::new(rng.gen(), &keypair);
⋮----
let pkt = Packet::from_data(None, ping).unwrap();
assert_eq!(pkt.meta().size, REPAIR_RESPONSE_SERIALIZED_PING_BYTES);
⋮----
fn test_deserialize_shred_as_ping() {
⋮----
shred.copy_to_packet(&mut pkt);
pkt.meta_mut().size = REPAIR_RESPONSE_SERIALIZED_PING_BYTES;
⋮----
assert!(!ping.verify());
⋮----
assert!(res.is_err());
⋮----
fn repair_request_header_for_tests() -> RepairRequestHeader {
⋮----
fn make_remote_request(packet: &Packet) -> RemoteRequest {
⋮----
remote_address: packet.meta().socket_addr(),
bytes: Bytes::from(Vec::from(packet.data(..).unwrap())),
⋮----
fn test_check_well_formed_repair_request() {
⋮----
let mut pkt = Packet::from_data(None, request).unwrap();
let mut batch = vec![make_remote_request(&pkt)];
⋮----
let num_well_formed = discard_malformed_repair_requests(&mut batch, &mut stats);
assert_eq!(num_well_formed, 1);
pkt.meta_mut().size = 5;
⋮----
assert_eq!(num_well_formed, 0);
assert_eq!(stats.err_malformed, 1);
⋮----
header: repair_request_header_for_tests(),
⋮----
pkt.meta_mut().size = 8;
⋮----
pkt.meta_mut().size = 1;
⋮----
pkt.meta_mut().size = 3;
⋮----
fn test_serialize_deserialize_signed_request() {
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let cluster_info = Arc::new(new_test_cluster_info());
⋮----
cluster_info.clone(),
⋮----
let keypair = cluster_info.keypair();
⋮----
.map_repair_request(
⋮----
.unwrap();
⋮----
deserialize_from_with_limit(&mut cursor).unwrap();
assert_eq!(cursor.position(), rsp.len() as u64);
⋮----
assert_eq!(slot, 123);
assert_eq!(header.nonce, 456);
assert_eq!(&header.sender, &serve_repair.my_id());
assert_eq!(&header.recipient, &repair_peer_id);
let signed_data = [&rsp[..4], &rsp[4 + SIGNATURE_BYTES..]].concat();
assert!(header
⋮----
panic!("unexpected request type {:?}", &deserialized_request);
⋮----
fn test_serialize_deserialize_ancestor_hashes_request() {
⋮----
.ancestor_repair_request_bytes(&keypair, &repair_peer_id, slot, nonce)
⋮----
assert_eq!(cursor.position(), request_bytes.len() as u64);
⋮----
assert_eq!(deserialized_slot, slot);
assert_eq!(header.nonce, nonce);
⋮----
let signed_data = [&request_bytes[..4], &request_bytes[4 + SIGNATURE_BYTES..]].concat();
⋮----
fn test_map_requests_signed() {
⋮----
assert_eq!(deserialized_shred_index, shred_index);
⋮----
fn test_verify_signed_packet() {
⋮----
fn sign_packet(packet: &mut Packet, keypair: &Keypair) {
⋮----
packet.data(..4).unwrap(),
packet.data(4 + SIGNATURE_BYTES..).unwrap(),
⋮----
.concat();
⋮----
packet.buffer_mut()[4..4 + SIGNATURE_BYTES].copy_from_slice(signature.as_ref());
⋮----
my_keypair.pubkey(),
other_keypair.pubkey(),
timestamp(),
⋮----
let mut packet = Packet::from_data(None, request).unwrap();
sign_packet(&mut packet, &my_keypair);
⋮----
let request: RepairProtocol = packet.deserialize_slice(..).unwrap();
assert_matches!(
⋮----
let time_diff_ms = u64::try_from(SIGNED_REPAIR_TIME_WINDOW.as_millis() * 2).unwrap();
let old_timestamp = timestamp().saturating_sub(time_diff_ms);
⋮----
sign_packet(&mut packet, &other_keypair);
⋮----
fn test_run_highest_window_request() {
run_highest_window_request(5, 3, 9);
⋮----
pub fn run_highest_window_request(slot: Slot, num_slots: u64, nonce: Nonce) {
⋮----
let handler = StandardRepairHandler::new(blockstore.clone());
let rv = handler.run_highest_window_request(&recycler, &socketaddr_any!(), 0, 0, nonce);
assert!(rv.is_none());
let _ = fill_blockstore_slot_with_ticks(
⋮----
max_ticks_per_n_shreds(1, None) + 1,
⋮----
.run_highest_window_request(&recycler, &socketaddr_any!(), slot, index, nonce)
.expect("packets");
⋮----
verify_responses(&request, rv.iter());
⋮----
.iter_mut()
.map(|mut packet| {
packet.meta_mut().flags |= PacketFlags::REPAIR;
⋮----
shred::layout::get_shred_and_repair_nonce(packet.as_ref()).unwrap();
assert_eq!(repair_nonce.unwrap(), nonce);
Shred::new_from_serialized_shred(shred.to_vec()).unwrap()
⋮----
assert!(!rv.is_empty());
let index = blockstore.meta(slot).unwrap().unwrap().received - 1;
assert_eq!(rv[0].index(), index as u32);
assert_eq!(rv[0].slot(), slot);
let rv = handler.run_highest_window_request(
⋮----
&socketaddr_any!(),
⋮----
fn test_run_window_request() {
⋮----
let rv = handler.run_window_request(&recycler, &socketaddr_any!(), slot, 0, nonce);
⋮----
let shredder = Shredder::new(slot, slot - 1, 0, 2).unwrap();
⋮----
let (mut shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
shreds.truncate(1);
⋮----
.insert_shreds(shreds, None, false)
.expect("Expect successful ledger write");
⋮----
.run_window_request(&recycler, &socketaddr_any!(), slot, index, nonce)
⋮----
assert_eq!(rv[0].index(), 1);
⋮----
fn new_test_cluster_info() -> ClusterInfo {
⋮----
let contact_info = ContactInfo::new_localhost(&keypair.pubkey(), timestamp());
⋮----
fn window_index_request() {
⋮----
let identity_keypair = cluster_info.keypair();
⋮----
let rv = serve_repair.repair_request(
⋮----
assert_matches!(rv, Err(Error::ClusterInfo(ClusterInfoError::NoPeers)));
let serve_repair_addr = socketaddr!(Ipv4Addr::LOCALHOST, 1243);
⋮----
nxt.set_gossip((Ipv4Addr::LOCALHOST, 1234)).unwrap();
nxt.set_tvu(UDP, (Ipv4Addr::LOCALHOST, 1235)).unwrap();
nxt.set_tvu(QUIC, (Ipv4Addr::LOCALHOST, 1236)).unwrap();
nxt.set_rpc((Ipv4Addr::LOCALHOST, 1241)).unwrap();
nxt.set_rpc_pubsub((Ipv4Addr::LOCALHOST, 1242)).unwrap();
nxt.set_serve_repair(UDP, serve_repair_addr).unwrap();
nxt.set_serve_repair(QUIC, (Ipv4Addr::LOCALHOST, 1237))
⋮----
cluster_info.insert_info(nxt.clone());
⋮----
.repair_request(
⋮----
assert_eq!(nxt.serve_repair(Protocol::UDP).unwrap(), serve_repair_addr);
assert_eq!(rv.0, nxt.serve_repair(Protocol::UDP).unwrap());
let serve_repair_addr2 = socketaddr!([127, 0, 0, 2], 1243);
⋮----
nxt.set_serve_repair(UDP, serve_repair_addr2).unwrap();
⋮----
cluster_info.insert_info(nxt);
⋮----
assert!(one && two);
⋮----
fn test_run_orphan() {
run_orphan(2, 3, 9);
⋮----
pub fn run_orphan(slot: Slot, num_slots: u64, nonce: Nonce) {
⋮----
let rv = handler.run_orphan(&recycler, &socketaddr_any!(), slot, 5, nonce);
⋮----
let (shreds, _) = make_many_slot_entries(slot, num_slots, 5);
⋮----
let rv = handler.run_orphan(&recycler, &socketaddr_any!(), slot + num_slots, 5, nonce);
⋮----
.run_orphan(
⋮----
.expect("run_orphan packets");
⋮----
.rev()
.filter_map(|slot| {
⋮----
assert_eq!(rv, expected);
⋮----
fn run_orphan_corrupted_shred_size() {
⋮----
let (mut shreds, _) = make_many_slot_entries(1, 2, 1);
assert_eq!(shreds[0].slot(), 1);
assert_eq!(shreds[0].index(), 0);
shreds.retain(|shred| shred.slot() != 1);
⋮----
assert!(repair_response::repair_response_packet(
⋮----
.run_orphan(&recycler, &socketaddr_any!(), 2, 5, nonce)
⋮----
let expected = RecycledPacketBatch::new(vec![repair_response::repair_response_packet(
⋮----
.into();
⋮----
fn test_run_ancestor_hashes() {
fn deserialize_ancestor_hashes_response(packet: PacketRef) -> AncestorHashesResponse {
⋮----
.deserialize_slice(..packet.meta().size - SIZE_OF_NONCE)
⋮----
.run_ancestor_hashes(&recycler, &socketaddr_any!(), slot + num_slots, nonce)
.expect("run_ancestor_hashes packets");
assert_eq!(rv.len(), 1);
let packet = rv.first().unwrap();
let ancestor_hashes_response = deserialize_ancestor_hashes_response(packet);
⋮----
assert!(hashes.is_empty());
⋮----
panic!("unexpected response: {:?}", &ancestor_hashes_response);
⋮----
.run_ancestor_hashes(&recycler, &socketaddr_any!(), slot + num_slots - 1, nonce)
⋮----
expected_ancestors.resize(num_slots as usize, (0, Hash::default()));
for (i, duplicate_confirmed_slot) in (slot..slot + num_slots).enumerate() {
⋮----
blockstore.insert_bank_hash(duplicate_confirmed_slot, frozen_hash, true);
⋮----
assert_eq!(hashes, expected_ancestors);
⋮----
fn test_repair_with_repair_validators() {
⋮----
let me = cluster_info.my_contact_info();
⋮----
let contact_info2 = ContactInfo::new_localhost(&solana_pubkey::new_rand(), timestamp());
let contact_info3 = ContactInfo::new_localhost(&solana_pubkey::new_rand(), timestamp());
cluster_info.insert_info(contact_info2.clone());
cluster_info.insert_info(contact_info3.clone());
⋮----
for pubkey in &[solana_pubkey::new_rand(), *me.pubkey()] {
let known_validators = Some(vec![*pubkey].into_iter().collect());
assert!(serve_repair
⋮----
let known_validators = Some(vec![*contact_info2.pubkey()].into_iter().collect());
⋮----
serve_repair.repair_peers(&known_validators, 1, &identity_keypair.pubkey());
assert_eq!(repair_peers.len(), 1);
assert_eq!(repair_peers[0].pubkey(), contact_info2.pubkey());
⋮----
.repair_peers(&None, 1, &identity_keypair.pubkey())
.into_iter()
.map(|node| *node.pubkey())
⋮----
assert_eq!(repair_peers.len(), 2);
assert!(repair_peers.contains(contact_info2.pubkey()));
assert!(repair_peers.contains(contact_info3.pubkey()));
⋮----
fn test_verify_shred_response() {
fn new_test_data_shred(slot: Slot, index: u32) -> Shred {
let shredder = Shredder::new(slot, slot.saturating_sub(1), 0, 0).unwrap();
⋮----
shreds.remove(index as usize)
⋮----
let shred = new_test_data_shred(slot, 0);
⋮----
assert!(request.verify_response(shred.payload()));
let shred = new_test_data_shred(slot - 1, 0);
⋮----
let shred = new_test_data_shred(slot + 1, 0);
assert!(!request.verify_response(shred.payload()));
let shred = new_test_data_shred(slot, index);
⋮----
let shred = new_test_data_shred(slot, index + 1);
⋮----
let shred = new_test_data_shred(slot, index - 1);
⋮----
let shred = new_test_data_shred(slot - 1, index);
⋮----
let shred = new_test_data_shred(slot + 1, index);
⋮----
fn verify_responses<'a>(
⋮----
let shred = shred::layout::get_shred(packet).unwrap();
assert!(request.verify_response(shred));
⋮----
fn test_verify_ancestor_response() {
⋮----
let repair = AncestorHashesRepairType(request_slot);
⋮----
.map(|slot| (slot, Hash::new_unique()))
⋮----
assert!(repair.verify_response(&AncestorHashesResponse::Hashes(response.clone())));
response.push((request_slot, Hash::new_unique()));
assert!(!repair.verify_response(&AncestorHashesResponse::Hashes(response)));

================
File: core/src/repair/standard_repair_handler.rs
================
pub(crate) struct StandardRepairHandler {
⋮----
impl StandardRepairHandler {
pub(crate) fn new(blockstore: Arc<Blockstore>) -> Self {
⋮----
impl RepairHandler for StandardRepairHandler {
fn blockstore(&self) -> &Blockstore {
⋮----
fn repair_response_packet(
⋮----
self.blockstore.as_ref(),
⋮----
fn run_orphan(
⋮----
let packets = std::iter::successors(self.blockstore.meta(slot).ok()?, |meta| {
self.blockstore.meta(meta.parent_slot?).ok()?
⋮----
.map_while(|meta| {
⋮----
meta.received.checked_sub(1u64)?,
⋮----
for packet in packets.take(max_responses) {
res.push(packet);
⋮----
(!res.is_empty()).then_some(res.into())

================
File: core/src/snapshot_packager_service/snapshot_gossip_manager.rs
================
pub struct SnapshotGossipManager {
⋮----
impl SnapshotGossipManager {
⋮----
pub fn new(
⋮----
this.push_starting_snapshot_hashes(starting_snapshot_hashes);
⋮----
fn push_starting_snapshot_hashes(&mut self, starting_snapshot_hashes: StartingSnapshotHashes) {
self.update_latest_full_snapshot_hash(starting_snapshot_hashes.full);
⋮----
self.update_latest_incremental_snapshot_hash(
⋮----
self.push_latest_snapshot_hashes_to_cluster();
⋮----
pub fn push_snapshot_hash(
⋮----
self.push_full_snapshot_hash(FullSnapshotHash(snapshot_hash));
⋮----
self.push_incremental_snapshot_hash(
IncrementalSnapshotHash(snapshot_hash),
⋮----
fn push_full_snapshot_hash(&mut self, full_snapshot_hash: FullSnapshotHash) {
self.update_latest_full_snapshot_hash(full_snapshot_hash);
⋮----
fn push_incremental_snapshot_hash(
⋮----
self.update_latest_incremental_snapshot_hash(incremental_snapshot_hash, base_slot);
⋮----
fn update_latest_full_snapshot_hash(&mut self, full_snapshot_hash: FullSnapshotHash) {
self.latest_snapshot_hashes = Some(LatestSnapshotHashes {
⋮----
fn update_latest_incremental_snapshot_hash(
⋮----
.as_mut()
.expect("there must already be a full snapshot hash");
assert_eq!(
⋮----
latest_snapshot_hashes.incremental = Some(incremental_snapshot_hash);
⋮----
fn push_latest_snapshot_hashes_to_cluster(&self) {
let Some(latest_snapshot_hashes) = self.latest_snapshot_hashes.as_ref() else {
⋮----
.push_snapshot_hashes(
latest_snapshot_hashes.full.clone_for_crds(),
⋮----
.iter()
.map(AsSnapshotHash::clone_for_crds)
.collect(),
⋮----
.expect(
⋮----
struct LatestSnapshotHashes {
⋮----
trait AsSnapshotHash {
⋮----
fn clone_for_crds(&self) -> (Slot, Hash) {
let (slot, snapshot_hash) = self.as_snapshot_hash();
⋮----
impl AsSnapshotHash for FullSnapshotHash {
fn as_snapshot_hash(&self) -> &(Slot, SnapshotHash) {
⋮----
impl AsSnapshotHash for IncrementalSnapshotHash {

================
File: core/src/tip_manager/tip_distribution.rs
================
pub enum TipDistributionError {
⋮----
pub type TipDistributionResult<T> = std::result::Result<T, TipDistributionError>;
pub struct TipDistributionAccount;
impl TipDistributionAccount {
pub(crate) fn find_program_address(
⋮----
vote_pubkey.to_bytes().as_ref(),
epoch.to_le_bytes().as_ref(),
⋮----
pub struct InitializeTipDistributionConfigInstruction;
impl InitializeTipDistributionConfigInstruction {
⋮----
pub(crate) fn to_instruction_data(
⋮----
let mut data = Vec::with_capacity(Self::DISCRIMINATOR.len() + 75);
data.extend_from_slice(Self::DISCRIMINATOR);
data.extend(borsh::to_vec(&authority).map_err(|e| {
error!("Error serializing authority: {e}");
⋮----
data.extend(borsh::to_vec(&expired_funds_account).map_err(|e| {
error!("Error serializing expired funds account: {e}");
⋮----
data.extend(borsh::to_vec(&num_epochs_valid).map_err(|e| {
error!("Error serializing num epochs valid: {e}");
⋮----
data.extend(borsh::to_vec(&max_validator_commission_bps).map_err(|e| {
error!("Error serializing max validator commission bps: {e}");
⋮----
data.extend(borsh::to_vec(&bump).map_err(|e| {
error!("Error serializing bump: {e}");
⋮----
Ok(data)
⋮----
pub struct InitializeTipDistributionAccountInstruction;
impl InitializeTipDistributionAccountInstruction {
⋮----
let mut data = Vec::with_capacity(Self::DISCRIMINATOR.len() + 35);
⋮----
data.extend(borsh::to_vec(&merkle_root_upload_authority).map_err(|e| {
error!("Error serializing merkle root upload authority: {e}");
⋮----
data.extend(borsh::to_vec(&validator_commission_bps).map_err(|e| {
error!("Error serializing validator commission bps: {e}");
⋮----
pub struct JitoTipDistributionConfig {
⋮----
impl JitoTipDistributionConfig {
⋮----
pub(crate) fn from_account_shared_data(
⋮----
if account_shared_data.owner() != program_id {
return Err(TipDistributionError::InvalidAccountOwner);
⋮----
if &account_shared_data.data()[0..8] != Self::DISCRIMINATOR {
return Err(TipDistributionError::InvalidDiscriminator);
⋮----
JitoTipDistributionConfig::try_from_slice(&account_shared_data.data()[8..83]).map_err(|e| {
error!("Error deserializing tip distribution config account: {e}");
⋮----
pub(crate) fn find_program_address(program_id: &Pubkey) -> (Pubkey, u8) {
⋮----
pub(crate) fn authority(&self) -> Pubkey {
⋮----
pub(crate) fn expired_funds_account(&self) -> Pubkey {
⋮----
pub(crate) fn num_epochs_valid(&self) -> u64 {
⋮----
pub(crate) fn max_validator_commission_bps(&self) -> u16 {
⋮----
pub(crate) fn bump(&self) -> u8 {

================
File: core/src/tip_manager/tip_payment.rs
================
pub enum TipPaymentError {
⋮----
pub type TipPaymentResult<T> = std::result::Result<T, TipPaymentError>;
⋮----
pub struct JitoTipPaymentConfig {
⋮----
impl JitoTipPaymentConfig {
⋮----
pub(crate) fn from_account_shared_data(
⋮----
if account_shared_data.owner() != program_id {
return Err(TipPaymentError::InvalidAccountOwner);
⋮----
// run cargo expand -p jito-tip-payment to find this discriminator
if &account_shared_data.data()[0..8] != Self::DISCRIMINATOR {
return Err(TipPaymentError::InvalidDiscriminator);
⋮----
JitoTipPaymentConfig::try_from_slice(&account_shared_data.data()[8..]).map_err(|e| {
error!("Error deserializing tip payment config account: {e}");
⋮----
pub fn find_program_address(program_id: &Pubkey) -> (Pubkey, u8) {
⋮----
pub fn find_tip_payment_account_pdas(program_id: &Pubkey) -> Vec<(Pubkey, u8)> {
vec![
⋮----
pub fn tip_receiver(&self) -> Pubkey {
⋮----
pub fn block_builder(&self) -> Pubkey {
⋮----
pub fn block_builder_commission_pct(&self) -> u64 {
⋮----
pub fn bumps(&self) -> &JitoTipPaymentInitBumps {
⋮----
/// https://github.com/jito-foundation/jito-programs/blob/8f55af0a9b31ac2192415b59ce2c47329ee255a2/mev-programs/programs/tip-payment/src/lib.rs#L362
#[derive(BorshDeserialize)]
pub struct JitoTipPaymentInitBumps {
⋮----
pub struct InitializeTipPaymentInstruction;
impl InitializeTipPaymentInstruction {
⋮----
pub fn to_instruction_data(
⋮----
struct InitBumps {
⋮----
let mut data = Vec::with_capacity(Self::DISCRIMINATOR.len() + 9);
data.extend_from_slice(Self::DISCRIMINATOR);
data.extend(
⋮----
.map_err(|e| {
error!("Error serializing init bumps: {e}");
⋮----
Ok(data)
⋮----
pub struct ChangeTipReceiverInstruction;
impl ChangeTipReceiverInstruction {
⋮----
pub fn to_instruction_data() -> Vec<u8> {
let mut data = Vec::with_capacity(Self::DISCRIMINATOR.len());
⋮----
pub struct ChangeBlockBuilderInstruction;
impl ChangeBlockBuilderInstruction {
⋮----
pub fn to_instruction_data(block_builder_commission: u64) -> TipPaymentResult<Vec<u8>> {
let mut data = Vec::with_capacity(Self::DISCRIMINATOR.len() + 8);
⋮----
data.extend(borsh::to_vec(&block_builder_commission).map_err(|e| {
error!("Error serializing block builder commission: {e}");

================
File: core/src/admin_rpc_post_init.rs
================
pub enum KeyUpdaterType {
⋮----
pub struct KeyUpdaters {
⋮----
impl KeyUpdaters {
pub fn add(
⋮----
self.updaters.insert(updater_type, updater);
⋮----
pub fn remove(&mut self, updater_type: &KeyUpdaterType) {
self.updaters.remove(updater_type);
⋮----
impl<'a> IntoIterator for &'a KeyUpdaters {
type Item = (
⋮----
type IntoIter = std::collections::hash_map::Iter<
⋮----
fn into_iter(self) -> Self::IntoIter {
self.updaters.iter()
⋮----
pub struct AdminRpcRequestMetadataPostInit {

================
File: core/src/bam_connection.rs
================
pub struct BamConnection {
⋮----
impl BamConnection {
pub async fn try_init(
⋮----
let backend_endpoint = tonic::transport::Endpoint::from_shared(url.clone())?;
let channel = timeout(CONNECTION_TIMEOUT, backend_endpoint.connect()).await??;
⋮----
.init_scheduler_stream(outbound_stream)
⋮----
.map_err(|e| {
error!("Failed to start scheduler stream: {e:?}");
⋮----
.into_inner();
⋮----
exit.clone(),
⋮----
config.clone(),
⋮----
metrics.clone(),
is_healthy.clone(),
⋮----
Ok(Self {
⋮----
async fn connection_task(
⋮----
let mut heartbeat_interval = interval(VALIDATOR_HEARTBEAT_INTERVAL);
heartbeat_interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);
let mut metrics_and_health_check_interval = interval(METRICS_AND_HEALTH_CHECK_INTERVAL);
⋮----
.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);
⋮----
error!("Failed to prepare auth response");
⋮----
msg: Some(Msg::AuthProof(auth_proof)),
⋮----
.send(v0_to_versioned_proto(start_message))
⋮----
.inspect_err(|_| {
error!("Failed to send initial auth proof message");
⋮----
.is_err()
⋮----
error!("Outbound sender channel closed before sending initial auth proof message");
⋮----
validator_client.clone(),
⋮----
outbound_sender.clone(),
⋮----
while !exit.load(Relaxed) {
⋮----
is_healthy.store(false, Relaxed);
let _ = builder_config_task.await.ok();
let _ = outbound_task.await.ok();
⋮----
fn send_batch_results(
⋮----
if !results.is_empty() {
⋮----
msg: Some(Msg::MultipleAtomicTxnBatchResult(
⋮----
.try_send(v0_to_versioned_proto(outbound))
⋮----
metrics.outbound_fail.fetch_add(1, Relaxed);
⋮----
metrics.outbound_sent.fetch_add(1, Relaxed);
⋮----
async fn refresh_config_task(
⋮----
let mut interval = interval(REFRESH_CONFIG_INTERVAL);
interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Skip);
⋮----
async fn outbound_task(
⋮----
let mut outbound_tick_interval = interval(OUTBOUND_TICK_INTERVAL);
outbound_tick_interval.set_missed_tick_behavior(tokio::time::MissedTickBehavior::Burst);
⋮----
fn sign_message(keypair: &Keypair, message: &[u8]) -> Option<String> {
let slot_signature = keypair.try_sign_message(message).ok()?;
let slot_signature = slot_signature.to_string();
Some(slot_signature)
⋮----
pub fn is_healthy(&self) -> bool {
self.is_healthy.load(Relaxed)
⋮----
pub fn wait_until_healthy_and_config_received(&self, duration: std::time::Duration) -> bool {
⋮----
while start.elapsed() < duration {
if self.is_healthy() && self.get_latest_config().is_some() {
⋮----
pub fn get_latest_config(&self) -> Option<ConfigResponse> {
if !self.is_healthy() {
⋮----
self.config.lock().unwrap().clone()
⋮----
pub fn url(&self) -> &str {
⋮----
pub fn labeled_bytes(challenge: &[u8]) -> Vec<u8> {
let mut v = Vec::with_capacity(AUTH_LABEL.len() + challenge.len());
v.extend_from_slice(AUTH_LABEL);
v.extend_from_slice(challenge);
⋮----
async fn prepare_auth_proof(
⋮----
let Ok(resp) = validator_client.get_auth_challenge(request).await else {
error!("Failed to get auth challenge");
⋮----
let resp = resp.into_inner();
⋮----
let challenge_bytes = challenge_to_sign.as_bytes();
⋮----
let signature = Self::sign_message(cluster_info.keypair().as_ref(), &to_sign)?;
Some(AuthProof {
⋮----
validator_pubkey: cluster_info.keypair().pubkey().to_string(),
⋮----
impl Drop for BamConnection {
fn drop(&mut self) {
self.is_healthy.store(false, Relaxed);
self.exit.store(true, Relaxed);
⋮----
self.connection_task.abort();
⋮----
struct BamConnectionMetrics {
⋮----
impl BamConnectionMetrics {
fn has_data(&self) -> bool {
self.bundle_received.load(Relaxed) > 0
|| self.bundle_forward_to_scheduler_fail.load(Relaxed) > 0
|| self.heartbeat_received.load(Relaxed) > 0
|| self.builder_config_received.load(Relaxed) > 0
|| self.unhealthy_connection_count.load(Relaxed) > 0
|| self.leaderstate_sent.load(Relaxed) > 0
|| self.bundleresult_sent.load(Relaxed) > 0
|| self.heartbeat_sent.load(Relaxed) > 0
|| self.outbound_sent.load(Relaxed) > 0
|| self.outbound_fail.load(Relaxed) > 0
|| self.ping_received.load(Relaxed) > 0
|| self.outbound_pong_send_fail.load(Relaxed) > 0
|| self.outbound_pong_sent.load(Relaxed) > 0
⋮----
pub fn report(&self) {
if !self.has_data() {
⋮----
datapoint_info!(
⋮----
pub enum TryInitError {

================
File: core/src/bam_dependencies.rs
================
pub enum BamOutboundMessage {
⋮----
pub enum BamConnectionState {
⋮----
impl BamConnectionState {
pub fn from_u8(state: u8) -> Self {
⋮----
pub struct BamDependencies {
⋮----
pub fn v0_to_versioned_proto(v0: SchedulerMessageV0) -> SchedulerMessage {
⋮----
versioned_msg: Some(VersionedMsg::V0(v0)),

================
File: core/src/bam_manager.rs
================
pub struct BamConnectionIdentityUpdater {
⋮----
impl NotifyKeyUpdate for BamConnectionIdentityUpdater {
fn update_key(&self, key: &solana_keypair::Keypair) -> Result<(), Box<dyn core::error::Error>> {
⋮----
.lock()
.unwrap()
.as_ref()
.map_or("None".to_string(), |u| u.clone());
datapoint_warn!(
⋮----
warn!(
⋮----
self.new_identity.store(Arc::new(Some(key.pubkey())));
⋮----
.store(true, Ordering::Relaxed);
Ok(())
⋮----
pub struct BamManager {
⋮----
impl BamManager {
pub fn new(
⋮----
fn run(
⋮----
.worker_threads(8)
.enable_all()
.build()
.unwrap();
⋮----
let shared_leader_state = poh_recorder.read().unwrap().shared_leader_state();
⋮----
bam_url: bam_url.clone(),
new_identity: new_identity.clone(),
identity_changed_force_reconnect: identity_changed.clone(),
⋮----
.write()
⋮----
.add(KeyUpdaterType::BamConnection, identity_updater);
info!("BAM Manager: Added BAM connection key updater");
⋮----
while !exit.load(Ordering::Relaxed) {
let current_url = bam_url.lock().unwrap().clone();
let mut connection = match current_connection.take() {
Some(connection) => Some(connection),
⋮----
if let Some(url) = current_url.as_ref() {
⋮----
.store(BamConnectionState::Connecting as u8, Ordering::Relaxed);
let result = runtime.block_on(BamConnection::try_init(
url.clone(),
dependencies.cluster_info.clone(),
dependencies.batch_sender.clone(),
dependencies.outbound_receiver.clone(),
⋮----
info!("BAM connection established");
if !connection.wait_until_healthy_and_config_received(
⋮----
dependencies.bam_enabled.store(
⋮----
if let Some(builder_config) = connection.get_latest_config() {
⋮----
Some(&builder_config),
⋮----
cached_builder_config = Some(builder_config);
⋮----
Some(connection)
⋮----
error!("Failed to connect to BAM with url: {url}: {e}");
⋮----
let Some(connection) = connection.as_mut() else {
⋮----
.store(BamConnectionState::Disconnected as u8, Ordering::Relaxed);
⋮----
if !connection.is_healthy() || identity_changed.load(Ordering::Relaxed) {
⋮----
if identity_changed.load(Ordering::Relaxed) {
⋮----
*new_identity.load().as_ref(),
⋮----
identity_changed.store(false, Ordering::Relaxed);
⋮----
warn!("BAM connection lost");
⋮----
.is_some_and(|url| url != connection.url())
⋮----
info!("BAM URL changed");
⋮----
if Some(&builder_config) != cached_builder_config.as_ref() {
Self::update_tpu_config(Some(&builder_config), &dependencies.cluster_info);
⋮----
let bam_state = if cached_builder_config.is_some() {
⋮----
.store(bam_state as u8, Ordering::Relaxed);
if let Some(bank) = shared_leader_state.load().working_bank() {
if !bank.is_frozen() {
⋮----
let _ = dependencies.outbound_sender.try_send(
⋮----
fn generate_leader_state(bank: &Bank) -> LeaderState {
let max_block_cu = bank.read_cost_tracker().unwrap().block_cost_limit();
let consumed_block_cu = bank.read_cost_tracker().unwrap().block_cost();
let slot_cu_budget_remaining = max_block_cu.saturating_sub(consumed_block_cu) as u32;
⋮----
slot: bank.slot(),
tick: (bank.tick_height() % bank.ticks_per_slot()) as u32,
⋮----
fn get_sockaddr(info: Option<&Socket>) -> Option<SocketAddr> {
⋮----
Some(SocketAddr::V4(SocketAddrV4::new(
Ipv4Addr::from_str(ip).ok()?,
⋮----
fn update_tpu_config(config: Option<&ConfigResponse>, cluster_info: &Arc<ClusterInfo>) {
let Some(tpu_info) = config.and_then(|c| c.bam_config.as_ref()) else {
⋮----
if let Some(tpu) = Self::get_sockaddr(tpu_info.tpu_sock.as_ref()) {
info!("Setting TPU: {tpu:?}");
let _ = cluster_info.set_tpu_quic(tpu);
⋮----
if let Some(tpu_fwd) = Self::get_sockaddr(tpu_info.tpu_fwd_sock.as_ref()) {
info!("Setting TPU forward: {tpu_fwd:?}");
let _ = cluster_info.set_tpu_forwards_quic(tpu_fwd);
⋮----
fn update_block_engine_key_and_commission(
⋮----
let Some(builder_info) = config.and_then(|c| c.block_engine_config.as_ref()) else {
⋮----
let Some(pubkey) = Pubkey::from_str(&builder_info.builder_pubkey).ok() else {
error!(
⋮----
block_builder_fee_info.lock().unwrap().block_builder = Pubkey::default();
⋮----
error!("Block builder commission must be <= 100");
⋮----
let mut block_builder_fee_info = block_builder_fee_info.lock().unwrap();
⋮----
fn update_bam_recipient_and_commission(
⋮----
let Some(bam_info) = config.bam_config.as_ref() else {
⋮----
let Some(pubkey) = Pubkey::from_str(&bam_info.prio_fee_recipient_pubkey).ok() else {
⋮----
.clone_from(&pubkey);
⋮----
fn wait_for_identity_in_cluster_info(
⋮----
while start.elapsed() < timeout {
if cluster_info.keypair().pubkey() == new_identity {
info!("BAM Manager: detected new identity {new_identity} in cluster info");
⋮----
fn set_client_id(cluster_info: &ClusterInfo, new_client_id: ClientId) {
let current_client_id = cluster_info.get_client_id();
⋮----
cluster_info.set_client_id(new_client_id);
⋮----
pub fn join(self) -> std::thread::Result<()> {
self.thread.join()

================
File: core/src/banking_simulation.rs
================
pub struct BankingSimulator {
⋮----
pub enum SimulateError {
⋮----
type PacketBatchesByTime = BTreeMap<SystemTime, (ChannelLabel, BankingPacketBatch)>;
type FreezeTimeBySlot = BTreeMap<Slot, SystemTime>;
type TimedBatchesToSend = Vec<(
⋮----
type EventSenderThread = JoinHandle<(TracedSender, TracedSender, TracedSender)>;
⋮----
pub struct BankingTraceEvents {
⋮----
impl BankingTraceEvents {
fn read_event_file(
⋮----
while !reader.fill_buf()?.is_empty() {
callback(deserialize_from(&mut reader)?);
⋮----
Ok(())
⋮----
pub fn load(event_file_paths: &[PathBuf]) -> Result<Self, SimulateError> {
⋮----
events.load_event(event);
⋮----
info!(
⋮----
if matches!(
⋮----
warn!(
⋮----
Ok(events)
⋮----
fn load_event(&mut self, TimedTracedEvent(event_time, event): TimedTracedEvent) {
⋮----
.insert(event_time, (label, batch))
.is_none();
assert!(is_new);
⋮----
let is_new = self.freeze_time_by_slot.insert(slot, event_time).is_none();
self.hash_overrides.add_override(slot, blockhash, bank_hash);
⋮----
pub fn hash_overrides(&self) -> &HashOverrides {
⋮----
struct DummyClusterInfo {
⋮----
impl LikeClusterInfo for Arc<DummyClusterInfo> {
fn id(&self) -> Pubkey {
*self.id.read().unwrap()
⋮----
fn lookup_contact_info<R>(&self, _: &Pubkey, _: impl ContactInfoQuery<R>) -> Option<R> {
⋮----
struct SimulatorLoopLogger {
⋮----
impl SimulatorLoopLogger {
fn bank_costs(bank: &Bank) -> (u64, u64) {
bank.read_cost_tracker()
.map(|t| (t.block_cost(), t.vote_cost()))
.unwrap()
⋮----
fn log_frozen_bank_cost(&self, bank: &Bank, bank_elapsed: Duration) {
⋮----
fn log_ongoing_bank_cost(&self, bank: &Bank, bank_elapsed: Duration) {
⋮----
fn log_jitter(&self, bank: &Bank) {
let old_slot = bank.slot();
if let Some(event_time) = self.freeze_time_by_slot.get(&old_slot) {
if log_enabled!(log::Level::Info) {
⋮----
.duration_since(self.base_simulation_time)
.unwrap();
let elapsed_event_time = event_time.duration_since(self.base_event_time).unwrap();
⋮----
fn on_new_leader(
⋮----
self.log_frozen_bank_cost(bank, bank_elapsed);
⋮----
struct SenderLoop {
⋮----
impl SenderLoop {
fn log_starting(&self) {
⋮----
fn spawn(self, base_simulation_time: SystemTime) -> Result<EventSenderThread, SimulateError> {
⋮----
.name("solSimSender".into())
.spawn(move || self.start(base_simulation_time))?;
Ok(handle)
⋮----
fn start(
⋮----
self.timed_batches_to_send.drain(..)
⋮----
.duration_since(base_simulation_time)
⋮----
ChannelLabel::Dummy => unreachable!(),
⋮----
sender.send(batches_with_stats).unwrap();
logger.on_sending_batches(&simulation_duration, label, batch_count, tx_count);
if self.exit.load(Ordering::Relaxed) {
⋮----
logger.on_terminating();
drop(self.timed_batches_to_send);
⋮----
struct SimulatorLoop {
⋮----
impl SimulatorLoop {
fn enter(
⋮----
sleep(WARMUP_DURATION);
info!("warmup done!");
self.start(base_simulation_time, sender_thread)
⋮----
if self.poh_recorder.read().unwrap().bank().is_none() {
let next_leader_slot = self.leader_schedule_cache.next_leader_slot(
⋮----
bank.slot(),
⋮----
Some(&self.blockstore),
⋮----
debug!("{next_leader_slot:?}");
⋮----
.reset_sync(bank.clone_without_scheduler(), next_leader_slot)
⋮----
info!("Bank::new_from_parent()!");
logger.log_jitter(&bank);
if let Some((result, _execute_timings)) = bank.wait_for_completed_scheduler() {
assert_matches!(result, Ok(()));
⋮----
bank.freeze();
let new_slot = if bank.slot() == self.parent_slot {
info!("initial leader block!");
⋮----
info!("next leader block!");
bank.slot() + 1
⋮----
.slot_leader_at(new_slot, None)
⋮----
logger.on_new_leader(&bank, bank_created.elapsed(), new_slot, new_leader);
⋮----
} else if sender_thread.is_finished() {
warn!("sender thread existed maybe due to completion of sending traced events");
⋮----
info!("new leader bank slot: {new_slot}");
⋮----
bank.clone_without_scheduler(),
⋮----
.hash_event(bank.slot(), &bank.last_blockhash(), &bank.hash());
if *bank.collector_id() == self.simulated_leader {
logger.log_frozen_bank_cost(&bank, bank_created.elapsed());
⋮----
self.retransmit_slots_sender.send(bank.slot()).unwrap();
update_bank_forks_and_poh_recorder_for_new_tpu_bank(
⋮----
while self.poh_controller.has_pending_message() {}
⋮----
.read()
⋮----
.working_bank_with_scheduler(),
⋮----
logger.log_ongoing_bank_cost(&bank, bank_created.elapsed());
⋮----
sleep(Duration::from_millis(10));
⋮----
struct SimulatorThreads {
⋮----
impl SimulatorThreads {
fn finish(self, sender_thread: EventSenderThread, retransmit_slots_sender: Sender<Slot>) {
info!("Sleeping a bit before signaling exit");
sleep(Duration::from_millis(100));
self.exit.store(true, Ordering::Relaxed);
sender_thread.join().unwrap();
self.banking_stage.join().unwrap();
self.poh_service.join().unwrap();
⋮----
retracer_thread.join().unwrap().unwrap();
⋮----
info!("Joining broadcast stage...");
drop(retransmit_slots_sender);
self.broadcast_stage.join().unwrap();
⋮----
struct SenderLoopLogger<'a> {
⋮----
fn new(
⋮----
fn on_sending_batches(
⋮----
debug!("sent {label:?} {batch_count} batches ({tx_count} txes)");
⋮----
Dummy => unreachable!(),
⋮----
let duration = log_interval.as_secs_f64();
⋮----
fn on_terminating(self) {
⋮----
fn format_as_timestamp(time: SystemTime) -> impl Display + use<> {
let time: chrono::DateTime<chrono::Utc> = time.into();
time.format("%Y-%m-%d %H:%M:%S.%f")
⋮----
impl BankingSimulator {
pub fn new(banking_trace_events: BankingTraceEvents, first_simulated_slot: Slot) -> Self {
⋮----
pub fn parent_slot(&self) -> Option<Slot> {
⋮----
.range(..self.first_simulated_slot)
.last()
.map(|(slot, _time)| slot)
.copied()
⋮----
fn prepare_simulation(
⋮----
let parent_slot = self.parent_slot().unwrap();
⋮----
let bank = bank_forks.read().unwrap().working_bank_with_scheduler();
⋮----
assert_eq!(parent_slot, bank.slot());
⋮----
.slot_leader_at(self.first_simulated_slot, None)
⋮----
.slot_meta_iterator(self.first_simulated_slot)
⋮----
.map(|(s, _)| s)
⋮----
info!("purging slots {}, {}", self.first_simulated_slot, end_slot);
blockstore.purge_from_next_slots(self.first_simulated_slot, end_slot);
blockstore.purge_slots(self.first_simulated_slot, end_slot, PurgeType::Exact);
info!("done: purging");
⋮----
info!("skipping purging...");
⋮----
info!("Poh is starting!");
⋮----
bank.tick_height(),
bank.last_blockhash(),
bank.clone(),
⋮----
bank.ticks_per_slot(),
⋮----
blockstore.clone(),
blockstore.get_new_shred_signal(0),
⋮----
exit.clone(),
⋮----
let (record_sender, record_receiver) = record_channels(false);
⋮----
poh_recorder.clone(),
⋮----
// Enable BankingTracer to approximate the real environment as close as possible because
// it's not expected to disable BankingTracer on production environments.
let (retracer, retracer_thread) = BankingTracer::new(Some((
&blockstore.banking_retracer_path(),
⋮----
assert!(retracer.is_enabled());
info!("Enabled banking retracer (dir_byte_limit: {BANKING_TRACE_DIR_DEFAULT_BYTE_LIMIT})",);
⋮----
} = retracer.create_channels(false);
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
let (retransmit_slots_sender, retransmit_slots_receiver) = unbounded();
let shred_version = compute_shred_version(
&genesis_config.hash(),
Some(&bank_forks.read().unwrap().root_bank().hard_forks()),
⋮----
Node::new_localhost_with_pubkey(&random_keypair.pubkey()).info,
⋮----
let (_, socket) = bind_in_range_with_config(
⋮----
.expect("should bind");
let broadcast_stage = BroadcastStageType::Standard.new_broadcast_stage(
vec![socket],
cluster_info_for_broadcast.clone(),
⋮----
bank_forks.clone(),
⋮----
info!("Start banking stage!...");
⋮----
block_production_method.clone(),
⋮----
prioritization_fee_cache.clone(),
⋮----
.range(parent_slot..)
.next()
.expect("timed hashes");
⋮----
let total_batch_count = packet_batches_by_time.len();
let timed_batches_to_send = packet_batches_by_time.split_off(&base_event_time);
⋮----
.values()
.map(|(_label, batches)| {
⋮----
batches.len(),
batches.iter().map(|batch| batch.len()).sum::<usize>(),
⋮----
.into_iter()
.map(|(event_time, batches)| {
(event_time.duration_since(base_event_time).unwrap(), batches)
⋮----
.zip_eq(batch_and_tx_counts)
⋮----
exit: exit.clone(),
⋮----
pub fn start(
⋮----
let (sender_loop, simulator_loop, simulator_threads) = self.prepare_simulation(
⋮----
sender_loop.log_starting();
⋮----
let sender_thread = sender_loop.spawn(base_simulation_time)?;
⋮----
simulator_loop.enter(base_simulation_time, sender_thread);
simulator_threads.finish(sender_thread, retransmit_slots_sender);
⋮----
pub fn event_file_name(index: usize) -> String {
⋮----
BASENAME.to_string()
⋮----
format!("{BASENAME}.{index}")

================
File: core/src/banking_stage.rs
================
use qualifier_attr::qualifiers;
⋮----
pub mod committer;
pub mod consumer;
pub mod leader_slot_metrics;
pub mod qos_service;
pub mod vote_storage;
pub(crate) mod consume_worker;
mod vote_worker;
pub mod decision_maker;
mod latest_validator_vote_packet;
pub(crate) mod leader_slot_timing_metrics;
mod read_write_account_set;
mod vote_packet_receiver;
pub mod scheduler_messages;
pub mod transaction_scheduler;
⋮----
pub mod unified_scheduler;
⋮----
pub(crate) mod unified_scheduler;
⋮----
mod progress_tracker;
⋮----
mod tpu_to_pack;
const MAX_NUM_WORKERS: NonZeroUsize = NonZeroUsize::new(64).unwrap();
const DEFAULT_NUM_WORKERS: NonZeroUsize = NonZeroUsize::new(4).unwrap();
⋮----
pub struct BankingStageStats {
⋮----
struct VoteSourceCounts {
⋮----
impl VoteSourceCounts {
fn is_empty(&self) -> bool {
⋮----
.load(Ordering::Relaxed)
+ self.dropped_packets_count.load(Ordering::Relaxed)
+ self.newly_buffered_packets_count.load(Ordering::Relaxed)
⋮----
impl BankingStageStats {
pub fn new() -> Self {
⋮----
.max_value(PACKETS_PER_BATCH as u64)
.build()
.unwrap(),
⋮----
self.gossip_counts.is_empty()
&& self.tpu_counts.is_empty()
⋮----
.load(Ordering::Relaxed) as u64
+ self.dropped_forward_packets_count.load(Ordering::Relaxed) as u64
+ self.current_buffered_packets_count.load(Ordering::Relaxed) as u64
+ self.rebuffered_packets_count.load(Ordering::Relaxed) as u64
+ self.consumed_buffered_packets_count.load(Ordering::Relaxed) as u64
⋮----
+ self.filter_pending_packets_elapsed.load(Ordering::Relaxed)
+ self.packet_conversion_elapsed.load(Ordering::Relaxed)
+ self.transaction_processing_elapsed.load(Ordering::Relaxed)
+ self.batch_packet_indexes_len.entries()
⋮----
fn report(&mut self, report_interval_ms: u64) {
if self.is_empty() {
⋮----
if self.last_report.should_update(report_interval_ms) {
datapoint_info!(
⋮----
self.batch_packet_indexes_len.clear();
⋮----
pub struct BatchedTransactionDetails {
⋮----
pub struct BatchedTransactionCostDetails {
⋮----
pub struct BatchedTransactionErrorDetails {
⋮----
pub trait LikeClusterInfo: Send + Sync + 'static + Clone {
⋮----
impl LikeClusterInfo for Arc<ClusterInfo> {
fn id(&self) -> Pubkey {
self.deref().id()
⋮----
fn lookup_contact_info<R>(&self, id: &Pubkey, query: impl ContactInfoQuery<R>) -> Option<R> {
self.deref().lookup_contact_info(id, query)
⋮----
pub struct BankingStage {
⋮----
impl BankingStage {
⋮----
pub fn new_num_threads(
⋮----
// Setup the manager thread state.
⋮----
banking_shutdown_signal: banking_shutdown_signal.clone(),
⋮----
// Spawn the manager thread.
⋮----
.name("BankingMgr".to_string())
.spawn(move || {
⋮----
.enable_all()
⋮----
.unwrap();
rt.block_on(manager.run(
⋮----
async fn run(
⋮----
self.spawn_scheduler(
⋮----
bundle_account_locker.clone(),
blacklisted_accounts.clone(),
tip_processing_dependencies.clone(),
bam_dependencies.clone(),
⋮----
// Signal shutdown & wait for all threads to exit.
self.worker_exit_signal.store(true, Ordering::Relaxed);
while let Some((_, res)) = self.threads.next().await {
res.unwrap()?;
⋮----
Ok(())
⋮----
async fn cycle_threads(
⋮----
// Shutdown all current threads.
⋮----
while let Some((name, res)) = self.threads.next().await {
match res.unwrap() {
Ok(()) => info!("Banking worker exited cleanly; name={name}"),
Err(err) => error!("Banking worker exited with error; name={name}; err={err:?}"),
⋮----
// Revert the exit signal.
self.worker_exit_signal.store(false, Ordering::Relaxed);
// Spawn the requested threads.
⋮----
fn spawn_scheduler(
⋮----
} => self.spawn_internal(
matches!(
⋮----
self.spawn_external(session, bundle_account_locker)
⋮----
self.threads.extend(threads.into_iter().map(|handle| {
let name = handle.thread().name().unwrap().to_string();
NamedTask::new(tokio::task::spawn_blocking(|| handle.join()), name)
⋮----
info!("Scheduler spawned");
⋮----
fn spawn_internal(
⋮----
info!("Spawning internal scheduler");
assert!(num_workers <= BankingStage::max_num_workers());
let num_workers = num_workers.get();
let exit = self.worker_exit_signal.clone();
// Setup receive & buffer.
⋮----
receiver: self.non_vote_receiver.clone(),
bank_forks: self.bank_forks.clone(),
blacklisted_accounts: blacklisted_accounts.clone(),
⋮----
// Spawn vote worker.
⋮----
threads.push(self.spawn_vote_worker(bundle_account_locker.clone()));
// Create channels for communication between scheduler and workers
⋮----
(0..num_workers).map(|_| unbounded()).unzip();
let (finished_work_sender, finished_work_receiver) = unbounded();
// Spawn the worker threads
let decision_maker = DecisionMaker::from(self.poh_recorder.read().unwrap().deref());
⋮----
for (index, work_receiver) in work_receivers.into_iter().enumerate() {
⋮----
exit.clone(),
⋮----
self.committer.clone(),
self.transaction_recorder.clone(),
⋮----
finished_work_sender.clone(),
self.poh_recorder.read().unwrap().shared_leader_state(),
⋮----
worker_metrics.push(consume_worker.metrics_handle());
threads.push(
⋮----
.name(format!("solCoWorker{id:02}"))
.spawn(|| {
if let Err(err) = consume_worker.run() {
error!("Internal consume worker error; err={err}");
⋮----
// Macro to spawn the scheduler. Different type on `scheduler` and thus
// scheduler_controller mean we cannot have an easy if for `scheduler`
// assignment without introducing `dyn`.
⋮----
.as_ref()
.map(|bam| bam.bam_enabled.clone())
.unwrap_or_default();
⋮----
macro_rules! spawn_scheduler {
⋮----
// Spawn the central scheduler thread
⋮----
spawn_scheduler!(scheduler);
⋮----
// Spawn BAM workers
⋮----
let (work_sender, work_receiver) = unbounded();
⋮----
work_receiver.clone(),
⋮----
let _ = consume_worker.run();
⋮----
// Spawn the BAM scheduler thread
let bam_scheduler_exit = exit.clone();
let bam_scheduler_bank_forks = self.bank_forks.clone();
let bam_shared_leader_state = self.poh_recorder.read().unwrap().shared_leader_state();
⋮----
.name("solBamSched".to_string())
⋮----
bam_dependencies.outbound_sender.clone(),
bam_scheduler_bank_forks.clone(),
⋮----
bam_scheduler_exit.clone(),
bam_dependencies.bam_enabled.clone(),
bam_dependencies.batch_receiver.clone(),
⋮----
Some(bam_shared_leader_state),
⋮----
decision_maker.clone(),
⋮----
match scheduler_controller.run() {
⋮----
warn!("Unexpected worker disconnect from scheduler")
⋮----
fn spawn_vote_worker(&self, bundle_account_locker: BundleAccountLocker) -> JoinHandle<()> {
let vote_storage = VoteStorage::new(&self.bank_forks.read().unwrap().working_bank());
let tpu_receiver = VotePacketReceiver::new(self.tpu_vote_receiver.clone());
let gossip_receiver = VotePacketReceiver::new(self.gossip_vote_receiver.clone());
⋮----
let worker_exit_signal = self.worker_exit_signal.clone();
let bank_forks = self.bank_forks.clone();
⋮----
.name("solBanknStgVote".to_string())
⋮----
.run()
⋮----
.unwrap()
⋮----
pub fn default_num_workers() -> NonZeroUsize {
⋮----
pub const fn max_num_workers() -> NonZeroUsize {
⋮----
pub const fn default_fill_time_millis() -> NonZeroU64 {
⋮----
mod external {
⋮----
pub(super) fn spawn_external(
⋮----
info!("Spawning external scheduler");
⋮----
assert!(workers.len() <= BankingStage::max_num_workers().get());
// Potentially spawn vote worker.
let mut threads = Vec::with_capacity(workers.len() + 3);
⋮----
non_vote_receiver: self.non_vote_receiver.clone(),
gossip_vote_receiver: Some(self.gossip_vote_receiver.clone()),
tpu_vote_receiver: Some(self.tpu_vote_receiver.clone()),
⋮----
// Spawn the external consumer workers.
let mut worker_metrics = Vec::with_capacity(workers.len());
⋮----
) in workers.into_iter().enumerate()
⋮----
self.worker_exit_signal.clone(),
⋮----
self.bank_forks.read().unwrap().sharable_banks(),
⋮----
.name(format!("solECoWorker{id:02}"))
⋮----
if let Err(err) = consume_worker.run(pack_to_worker) {
error!("External consume worker error; err={err}");
⋮----
// Spawn tpu to pack.
threads.push(tpu_to_pack::spawn(
⋮----
// Spawn progress tracker.
⋮----
let poh = self.poh_recorder.read().unwrap();
(poh.shared_leader_state(), poh.ticks_per_slot())
⋮----
threads.push(progress_tracker::spawn(
⋮----
pub struct BankingStageHandle {
⋮----
impl BankingStageHandle {
pub fn join(self) -> thread::Result<()> {
self.banking_shutdown_signal.cancel();
self.thread.join().unwrap()
⋮----
pub enum BankingControlMsg {
⋮----
pub fn update_bank_forks_and_poh_recorder_for_new_tpu_bank(
⋮----
let tpu_bank = bank_forks.write().unwrap().insert(tpu_bank);
if poh_controller.set_bank(tpu_bank).is_err() {
warn!("Failed to set poh bank, poh service is disconnected");
⋮----
struct NamedTask<Ret = (), Name = String>
⋮----
fn new(task: tokio::task::JoinHandle<Ret>, name: Name) -> Self {
⋮----
type Output = (I, Result<R, tokio::task::JoinError>);
fn poll(
⋮----
self.task.as_mut().poll(cx).map(|v| (self.name.clone(), v))
⋮----
mod tests {
⋮----
pub(crate) fn sanitize_transactions(
⋮----
txs.into_iter()
.map(RuntimeTransaction::from_transaction_for_tests)
.collect()
⋮----
fn test_banking_stage_shutdown1() {
let genesis_config = create_genesis_config(2).genesis_config;
⋮----
} = banking_tracer.create_channels(false);
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger"),
⋮----
) = create_test_recorder(bank, blockstore, None, None);
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
poh_recorder.clone(),
⋮----
drop(non_vote_sender);
drop(tpu_vote_sender);
drop(gossip_vote_sender);
exit.store(true, Ordering::Relaxed);
banking_stage.join().unwrap();
poh_service.join().unwrap();
⋮----
fn test_banking_stage_tick() {
⋮----
} = create_genesis_config(2);
⋮----
let start_hash = bank.last_blockhash();
⋮----
target_tick_count: Some(bank.max_tick_height() + num_extra_ticks),
⋮----
) = create_test_recorder(bank.clone(), blockstore, Some(poh_config), None);
⋮----
trace!("sending bank");
⋮----
drop(poh_recorder);
⋮----
trace!("getting entries");
⋮----
.iter()
.map(|(_bank, (entry, _tick_height))| entry)
.collect();
trace!("done");
assert_eq!(entries.len(), genesis_config.ticks_per_slot as usize);
assert!(entries
⋮----
assert_eq!(entries[entries.len() - 1].hash, bank.last_blockhash());
⋮----
fn test_banking_stage_entries_only_central_scheduler() {
⋮----
} = create_slow_genesis_config(10);
⋮----
) = create_test_recorder(bank.clone(), blockstore, None, None);
⋮----
bank_forks.clone(),
⋮----
let mut packet_batches = to_packet_batches(&[tx_no_ver, tx_anf, tx], 3);
⋮----
.first_mut()
⋮----
.meta_mut()
.set_discard(true);
assert_eq!(packet_batches.len(), 1);
⋮----
.send(BankingPacketBatch::new(packet_batches))
⋮----
if let Ok((_bank, (entry, _))) = entry_receiver.try_recv() {
let tx_entry = !entry.transactions.is_empty();
entries.push(entry);
⋮----
sleep(Duration::from_millis(10));
⋮----
entries.extend(
⋮----
.map(|(_bank, (entry, _tick_height))| entry),
⋮----
bank.process_entry_transactions(entry.transactions)
⋮----
.for_each(|x| assert_eq!(*x, Ok(())));
⋮----
assert_eq!(bank.get_balance(&to2), 1);
assert_eq!(bank.get_balance(&to), 0);
drop(entry_receiver);
⋮----
fn test_banking_stage_entryfication() {
⋮----
} = create_slow_genesis_config(2);
⋮----
system_transaction::transfer(&mint_keypair, &alice.pubkey(), 2, genesis_config.hash());
let packet_batches = to_packet_batches(&[tx], 1);
⋮----
system_transaction::transfer(&mint_keypair, &alice.pubkey(), 1, genesis_config.hash());
⋮----
while bank.get_balance(&alice.pubkey()) < 1 {
if start.elapsed() > TIMEOUT {
panic!("banking stage took too long to process transactions");
⋮----
.try_process_entry_transactions(entry.transactions)
.expect("All transactions should be processed");
⋮----
assert!(bank.get_balance(&alice.pubkey()) != 3);
⋮----
fn test_bank_record_transactions() {
⋮----
} = create_genesis_config(10_000);
⋮----
let (record_sender, mut record_receiver) = record_channels(false);
⋮----
record_receiver.restart(bank.bank_id());
⋮----
let txs = vec![
⋮----
let summary = recorder.record_transactions(bank.bank_id(), txs.clone());
assert!(summary.result.is_ok());
assert_eq!(
⋮----
assert!(record_receiver.try_recv().is_err());
let next_bank_id = bank.bank_id() + 1;
⋮----
recorder.record_transactions(next_bank_id, txs);
assert_matches!(result, Err(PohRecorderError::MaxHeightReached));
⋮----
pub(crate) fn create_slow_genesis_config(lamports: u64) -> GenesisConfigInfo {
create_slow_genesis_config_with_leader(lamports, &solana_pubkey::new_rand())
⋮----
pub(crate) fn create_slow_genesis_config_with_leader(
⋮----
let mut config_info = create_genesis_config_with_leader(
⋮----
bootstrap_validator_stake_lamports(),
⋮----
fn test_vote_storage_full_send() {
⋮----
} = create_slow_genesis_config(10000);
⋮----
let keypairs = (0..100).map(|_| Keypair::new()).collect_vec();
let vote_keypairs = (0..100).map(|_| Keypair::new()).collect_vec();
for keypair in keypairs.iter() {
bank.process_transaction(&system_transaction::transfer(
⋮----
&keypair.pubkey(),
⋮----
.map(|i| {
new_tower_sync_transaction(
TowerSync::from(vec![(0, 8), (1, 7), (i as u64 + 10, 6), (i as u64 + 11, 1)]),
⋮----
.collect_vec();
⋮----
TowerSync::from(vec![(0, 9), (1, 8), (i as u64 + 5, 6), (i as u64 + 63, 1)]),
⋮----
&keypairs[(i + 1) % 100].pubkey(),
⋮----
let non_vote_packet_batches = to_packet_batches(&txs, 10);
let tpu_packet_batches = to_packet_batches(&tpu_votes, 10);
let gossip_packet_batches = to_packet_batches(&gossip_votes, 10);
⋮----
.into_iter()
.map(|(packet_batches, sender)| {
⋮----
.for_each(|handle| handle.join().unwrap());
⋮----
fn test_blacklisted_accounts() {
⋮----
block_production_method.clone(),
⋮----
HashSet::from_iter([blacklisted_keypair.pubkey()]),
⋮----
&blacklisted_keypair.pubkey(),
⋮----
&good_keypair.pubkey(),
⋮----
let packet_batches = to_packet_batches(&[blacklisted_tx.clone(), ok_tx.clone()], 2);
⋮----
while start.elapsed() < Duration::from_secs(5) {
⋮----
entry_receiver.recv_timeout(Duration::from_millis(10))
⋮----
num_txs += entry.transactions.len();
⋮----
assert_eq!(bank.get_balance(&good_keypair.pubkey()), 2);
assert!(bank.has_signature(&ok_tx.signatures[0]));
⋮----
assert!(!bank.has_signature(&blacklisted_tx.signatures[0]));
⋮----
Blockstore::destroy(ledger_path.path()).unwrap();

================
File: core/src/banking_trace.rs
================
pub type BankingPacketSender = TracedSender;
pub type TracerThreadResult = Result<(), TraceError>;
pub type TracerThread = Option<JoinHandle<TracerThreadResult>>;
pub type DirByteLimit = u64;
⋮----
pub enum TraceError {
⋮----
struct ActiveTracer {
⋮----
pub struct BankingTracer {
⋮----
pub struct TimedTracedEvent(pub std::time::SystemTime, pub TracedEvent);
⋮----
pub enum TracedEvent {
⋮----
pub enum ChannelLabel {
⋮----
struct RollingConditionGrouped {
⋮----
impl RollingConditionGrouped {
fn new(basic: RollingConditionBasic) -> Self {
⋮----
fn reset(&mut self) {
⋮----
struct GroupedWriter<'a> {
⋮----
fn new(underlying: &'a mut RollingFileAppender<RollingConditionGrouped>) -> Self {
⋮----
impl RollingCondition for RollingConditionGrouped {
fn should_rollover(&mut self, now: &DateTime<Local>, current_filesize: u64) -> bool {
⋮----
// rollover normally if empty to reuse it if possible
⋮----
// forcibly rollover anew, so that we always avoid to append
// to a possibly-damaged tracing file even after unclean
// restarts
⋮----
self.basic.should_rollover(now, current_filesize)
⋮----
impl Write for GroupedWriter<'_> {
fn write(&mut self, buf: &[u8]) -> std::result::Result<usize, io::Error> {
self.underlying.write_with_datetime(buf, &self.now)
⋮----
fn flush(&mut self) -> std::result::Result<(), io::Error> {
self.underlying.flush()
⋮----
pub fn receiving_loop_with_minimized_sender_overhead<T, E, const SLEEP_MS: u64>(
⋮----
'outer: while !exit.load(Ordering::Relaxed) {
⋮----
match receiver.try_recv() {
Ok(message) => on_recv(message)?,
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
sleep(Duration::from_millis(SLEEP_MS));
⋮----
Ok(())
⋮----
pub struct Channels {
⋮----
impl Channels {
⋮----
pub fn unified_sender(&self) -> &BankingPacketSender {
⋮----
assert!(unified_sender
⋮----
pub(crate) fn unified_receiver(&self) -> &BankingPacketReceiver {
⋮----
assert!(unified_receiver.same_channel(&self.tpu_vote_receiver));
assert!(unified_receiver.same_channel(&self.gossip_vote_receiver));
⋮----
impl BankingTracer {
pub fn new(
⋮----
None => Ok((Self::new_disabled(), None)),
⋮----
return Err(TraceError::TooSmallDirByteLimit(
⋮----
let (trace_sender, trace_receiver) = unbounded();
⋮----
Self::spawn_background_thread(trace_receiver, file_appender, exit.clone())?;
Ok((
⋮----
active_tracer: Some(ActiveTracer { trace_sender, exit }),
⋮----
Some(tracer_thread),
⋮----
pub fn new_disabled() -> Arc<Self> {
⋮----
pub fn is_enabled(&self) -> bool {
self.active_tracer.is_some()
⋮----
pub fn create_channels(&self, unify_channels: bool) -> Channels {
⋮----
// Returning the same channel is needed when unified scheduler supports block
// production because unified scheduler doesn't distinguish them and treats them as
let (non_vote_sender, non_vote_receiver) = self.create_channel_non_vote();
⋮----
self.create_unified_channel_tpu_vote(&non_vote_sender, &non_vote_receiver);
⋮----
self.create_unified_channel_gossip_vote(&non_vote_sender, &non_vote_receiver);
⋮----
let (tpu_vote_sender, tpu_vote_receiver) = self.create_channel_tpu_vote();
let (gossip_vote_sender, gossip_vote_receiver) = self.create_channel_gossip_vote();
⋮----
fn create_channel(&self, label: ChannelLabel) -> (BankingPacketSender, BankingPacketReceiver) {
Self::channel(label, self.active_tracer.as_ref().cloned())
⋮----
pub fn create_channel_non_vote(&self) -> (BankingPacketSender, BankingPacketReceiver) {
self.create_channel(ChannelLabel::NonVote)
⋮----
fn create_channel_tpu_vote(&self) -> (BankingPacketSender, BankingPacketReceiver) {
self.create_channel(ChannelLabel::TpuVote)
⋮----
fn create_channel_gossip_vote(&self) -> (BankingPacketSender, BankingPacketReceiver) {
self.create_channel(ChannelLabel::GossipVote)
⋮----
fn create_unified_channel_tpu_vote(
⋮----
self.active_tracer.as_ref().cloned(),
sender.sender.clone(),
receiver.clone(),
⋮----
fn create_unified_channel_gossip_vote(
⋮----
pub fn hash_event(&self, slot: Slot, blockhash: &Hash, bank_hash: &Hash) {
self.trace_event(|| {
TimedTracedEvent(
⋮----
fn trace_event(&self, on_trace: impl Fn() -> TimedTracedEvent) {
⋮----
if !exit.load(Ordering::Relaxed) {
⋮----
.send(on_trace())
.expect("active tracer thread unless exited");
⋮----
pub fn channel_for_test() -> (TracedSender, Receiver<BankingPacketBatch>) {
⋮----
fn channel(
⋮----
let (sender, receiver) = unbounded();
⋮----
fn channel_inner(
⋮----
pub fn ensure_cleanup_path(path: &PathBuf) -> Result<(), io::Error> {
remove_dir_all(path).or_else(|err| {
if err.kind() == io::ErrorKind::NotFound {
⋮----
Err(err)
⋮----
fn create_file_appender(
⋮----
create_dir_all(path)?;
⋮----
.daily()
.max_size(rotate_threshold_size),
⋮----
path.join(BASENAME),
⋮----
(TRACE_FILE_ROTATE_COUNT - 1).try_into()?,
⋮----
Ok(appender)
⋮----
fn spawn_background_thread(
⋮----
let thread = thread::Builder::new().name("solBanknTracer".into()).spawn(
⋮----
file_appender.condition_mut().reset();
serialize_into(&mut GroupedWriter::new(&mut file_appender), &event)?;
⋮----
file_appender.flush()?;
⋮----
Ok(thread)
⋮----
pub struct TracedSender {
⋮----
impl TracedSender {
fn new(
⋮----
pub fn send(&self, batch: BankingPacketBatch) -> Result<(), SendError<BankingPacketBatch>> {
⋮----
.send(TimedTracedEvent(
⋮----
.map_err(|err| {
error!("unexpected error when tracing a banking event...: {err:?}");
SendError(BankingPacketBatch::clone(&batch))
⋮----
self.sender.send(batch)
⋮----
pub fn len(&self) -> usize {
self.sender.len()
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub mod for_test {
⋮----
pub fn sample_packet_batch() -> BankingPacketBatch {
BankingPacketBatch::new(to_packet_batches(&vec![test_tx(); 4], 10))
⋮----
pub fn drop_and_clean_temp_dir_unless_suppressed(temp_dir: TempDir) {
std::env::var("BANKING_TRACE_LEAVE_FILES").is_ok().then(|| {
warn!("prevented to remove {:?}", temp_dir.path());
drop(temp_dir.keep());
⋮----
pub fn terminate_tracer(
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
drop((sender, tracer));
main_thread.join().unwrap().unwrap();
⋮----
tracer_thread.join().unwrap().unwrap();
⋮----
mod tests {
⋮----
fn test_new_disabled() {
⋮----
let (non_vote_sender, non_vote_receiver) = tracer.create_channel_non_vote();
⋮----
|_packet_batch| Ok(()),
⋮----
.send(BankingPacketBatch::new(vec![]))
.unwrap();
⋮----
fn test_send_after_exited() {
let temp_dir = TempDir::new().unwrap();
let path = temp_dir.path().join("banking-trace");
⋮----
BankingTracer::new(Some((&path, exit.clone(), DirByteLimit::MAX))).unwrap();
⋮----
let exit_for_dummy_thread2 = exit_for_dummy_thread.clone();
⋮----
tracer_thread.unwrap().join().unwrap().unwrap();
let blockhash = Hash::from_str("B1ockhash1111111111111111111111111111111111").unwrap();
let bank_hash = Hash::from_str("BankHash11111111111111111111111111111111111").unwrap();
tracer.hash_event(4, &blockhash, &bank_hash);
drop(tracer);
⋮----
.send(for_test::sample_packet_batch())
⋮----
exit_for_dummy_thread2.store(true, Ordering::Relaxed);
dummy_main_thread.join().unwrap().unwrap();
⋮----
fn test_record_and_restore() {
⋮----
let mut stream = BufReader::new(File::open(path.join(BASENAME)).unwrap());
⋮----
.map(|_| bincode::deserialize_from::<_, TimedTracedEvent>(&mut stream))
⋮----
assert_matches!(
⋮----
fn test_spill_over_at_rotation() {
⋮----
BankingTracer::create_file_appender(&path, REALLY_SMALL_ROTATION_THRESHOLD).unwrap();
file_appender.write_all(b"foo").unwrap();
⋮----
file_appender.write_all(b"bar").unwrap();
⋮----
file_appender.flush().unwrap();
assert_eq!(
⋮----
fn test_reopen_with_blank_file() {
⋮----
file_appender.write_all(b"f").unwrap();

================
File: core/src/block_creation_loop.rs
================
mod stats;
pub struct BlockCreationLoop {
⋮----
impl BlockCreationLoop {
pub fn new(config: BlockCreationLoopConfig) -> Self {
⋮----
.name("solBlkCreatLoop".to_string())
.spawn(move || {
info!("BlockCreationLoop has started");
start_loop(config);
info!("BlockCreationLoop has stopped");
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread.join()
⋮----
pub struct BlockCreationLoopConfig {
⋮----
struct LeaderContext {
⋮----
pub struct ReplayHighestFrozen {
⋮----
enum StartLeaderError {
⋮----
fn start_loop(config: BlockCreationLoopConfig) {
⋮----
let _exit = Finalizer::new(exit.clone());
let mut my_pubkey = cluster_info.id();
let record_receiver = match record_receiver_receiver.recv() {
⋮----
error!("{my_pubkey}: Failed to receive RecordReceiver from PohService. Exiting: {e:?}",);
⋮----
info!("{my_pubkey}: PohService has shutdown, BlockCreationLoop is enabled");
⋮----
poh_recorder: poh_recorder.clone(),
⋮----
let mut w_poh_recorder = ctx.poh_recorder.write().unwrap();
w_poh_recorder.enable_alpenglow();
⋮----
reset_poh_recorder(&ctx.bank_forks.read().unwrap().working_bank(), &ctx);
while !ctx.exit.load(Ordering::Relaxed) {
if my_pubkey != cluster_info.id() {
⋮----
my_pubkey = cluster_info.id();
⋮----
warn!(
⋮----
.recv_timeout(Duration::from_secs(1))
.ok()
.and_then(|window| {
⋮----
.try_iter()
.last()
.or(Some(window))
⋮----
trace!("Received window notification for {start_slot} to {end_slot} parent: {parent_slot}");
if let Err(e) = produce_window(start_slot, end_slot, parent_slot, skip_timer, &mut ctx) {
error!(
⋮----
ctx.metrics.report(Duration::from_secs(1));
⋮----
fn reset_poh_recorder(bank: &Arc<Bank>, ctx: &LeaderContext) {
trace!("{}: resetting poh to {}", ctx.my_pubkey, bank.slot());
assert!(ctx.record_receiver.is_shutdown() && ctx.record_receiver.is_safe_to_restart());
let next_leader_slot = ctx.leader_schedule_cache.next_leader_slot(
⋮----
bank.slot(),
⋮----
Some(ctx.blockstore.as_ref()),
⋮----
.write()
.unwrap()
.reset(bank.clone(), next_leader_slot);
⋮----
fn produce_window(
⋮----
while !ctx.exit.load(Ordering::Relaxed) && slot <= end_slot {
start_leader_wait_for_parent_replay(slot, parent_slot, skip_timer, ctx)?;
let leader_index = leader_slot_index(slot);
let timeout = block_timeout(leader_index);
trace!(
⋮----
if let Err(e) = record_and_complete_block(
ctx.poh_recorder.as_ref(),
⋮----
panic!("PohRecorder record failed: {e:?}");
⋮----
assert!(!ctx.poh_recorder.read().unwrap().has_bank());
bank_completion_measure.stop();
ctx.slot_metrics.report();
⋮----
.increment(bank_completion_measure.as_us())
.inspect_err(|e| {
⋮----
trace!("{my_pubkey}: finished leader window {start_slot}-{end_slot}");
window_production_start.stop();
ctx.metrics.window_production_elapsed += window_production_start.as_us();
Ok(())
⋮----
fn record_and_complete_block(
⋮----
.saturating_sub(block_timer.elapsed())
.is_zero()
⋮----
let Ok(record) = record_receiver.try_recv() else {
⋮----
poh_recorder.write().unwrap().record(
⋮----
record_receiver.shutdown();
while !record_receiver.is_safe_to_restart() {
let Ok(record) = record_receiver.recv_timeout(Duration::ZERO) else {
⋮----
let mut w_poh_recorder = poh_recorder.write().unwrap();
⋮----
.bank()
.expect("Bank cannot have been cleared as BlockCreationLoop is the only modifier");
⋮----
let max_tick_height = bank.max_tick_height();
bank.set_tick_height(max_tick_height - 1);
drop(bank);
w_poh_recorder.tick_alpenglow(max_tick_height);
⋮----
fn start_leader_wait_for_parent_replay(
⋮----
let timeout = block_timeout(leader_slot_index(slot));
let end_slot = last_of_consecutive_leader_slots(slot);
⋮----
while !timeout.saturating_sub(skip_timer.elapsed()).is_zero() {
⋮----
let highest_parent_ready_slot = ctx.highest_parent_ready.read().unwrap().0;
⋮----
return Err(StartLeaderError::ClusterCertifiedBlocksAfterWindow(
⋮----
match maybe_start_leader(slot, parent_slot, ctx) {
⋮----
slot_delay_start.stop();
⋮----
.increment(slot_delay_start.as_us())
⋮----
return Ok(());
⋮----
.lock()
⋮----
.wait_timeout_while(
⋮----
timeout.saturating_sub(skip_timer.elapsed()),
⋮----
wait_start.stop();
ctx.slot_metrics.replay_is_behind_cumulative_wait_elapsed += wait_start.as_us();
⋮----
.increment(wait_start.as_us())
⋮----
Err(e) => return Err(e),
⋮----
Err(StartLeaderError::ReplayIsBehind(parent_slot, slot))
⋮----
fn maybe_start_leader(
⋮----
if ctx.bank_forks.read().unwrap().get(slot).is_some() {
⋮----
return Err(StartLeaderError::AlreadyHaveBank(slot));
⋮----
let Some(parent_bank) = ctx.bank_forks.read().unwrap().get(parent_slot) else {
⋮----
return Err(StartLeaderError::ReplayIsBehind(parent_slot, slot));
⋮----
if !parent_bank.is_frozen() {
⋮----
create_and_insert_leader_bank(slot, parent_bank, ctx);
⋮----
fn create_and_insert_leader_bank(slot: Slot, parent_bank: Arc<Bank>, ctx: &mut LeaderContext) {
let parent_slot = parent_bank.slot();
let root_slot = ctx.bank_forks.read().unwrap().root();
⋮----
if let Some(bank) = ctx.poh_recorder.read().unwrap().bank() {
panic!(
⋮----
if ctx.poh_recorder.read().unwrap().start_slot() != parent_slot {
reset_poh_recorder(&parent_bank, ctx);
⋮----
parent_bank.clone(),
⋮----
ctx.rpc_subscriptions.as_deref(),
⋮----
ctx.banking_tracer.hash_event(
⋮----
&parent_bank.last_blockhash(),
&parent_bank.hash(),
⋮----
let tpu_bank = ctx.bank_forks.write().unwrap().insert(tpu_bank);
ctx.poh_recorder.write().unwrap().set_bank(tpu_bank);
ctx.record_receiver.restart(slot);
ctx.slot_metrics.reset(slot);
info!(

================
File: core/src/bundle_sigverify_stage.rs
================
pub struct BundleSigverifyStage {
⋮----
impl BundleSigverifyStage {
pub fn new(
⋮----
let thread = spawn(move || Self::sigverify_service(receiver, sender, exit));
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread.join()
⋮----
fn sigverify_service(
⋮----
while !exit.load(Ordering::Relaxed) {
let bundles = match receiver.recv_timeout(Duration::from_millis(10)) {
⋮----
&& last_update.elapsed().as_millis() > 20
⋮----
datapoint_info!(
⋮----
workspace.extend(bundles.into_iter().map(|bundle| bundle.take()));
let packet_count = workspace.iter().map(|bundle| bundle.len()).sum();
num_bundles_received += workspace.len();
⋮----
ed25519_verify(&mut workspace, false, packet_count);
for bundle in workspace.drain(..) {
⋮----
.iter()
.filter(|packet| packet.meta().discard())
.count();
⋮----
&& sender.send(VerifiedPacketBundle::new(bundle)).is_err()
⋮----
warn!("failed to send verified packet bundle");
⋮----
mod tests {
⋮----
fn test_bundle_sigverify_stage_exit() {
let (_unverified_sender, unverified_receiver) = bounded(1024);
let (verified_sender, _verified_receiver) = bounded(1024);
⋮----
let stage = BundleSigverifyStage::new(unverified_receiver, verified_sender, exit.clone());
exit.store(true, Ordering::Relaxed);
stage.join().unwrap();
⋮----
fn test_bundle_sigverify_stage_many_packets_all_valid() {
let (unverified_sender, unverified_receiver) = bounded(1024);
let (verified_sender, verified_receiver) = bounded(1024);
⋮----
let txs_1 = (0..3).map(|_| test_tx()).collect::<Vec<_>>();
⋮----
.map(|tx| BytesPacket::from_data(None, tx).unwrap())
⋮----
"".to_string(),
⋮----
let txs_2 = (0..4).map(|_| test_tx()).collect::<Vec<_>>();
⋮----
.send(vec![packet_bundle_1, packet_bundle_2])
.unwrap();
⋮----
let verified_bundle_1 = verified_receiver.recv().unwrap();
assert_eq!(verified_bundle_1.batch().len(), 3);
assert!(verified_bundle_1
⋮----
.batch()
⋮----
.map(|packet| bincode::deserialize(packet.data(..).unwrap()).unwrap())
.collect();
assert_eq!(txs_1, txs_1_after);
let verified_bundle_2 = verified_receiver.recv().unwrap();
assert_eq!(verified_bundle_2.batch().len(), 4);
assert!(verified_bundle_2
⋮----
assert_eq!(txs_2, txs_2_after);
⋮----
fn test_bundle_sigverify_stage_many_packets_some_invalid() {
⋮----
let mut txs_1 = (0..3).map(|_| test_tx()).collect::<Vec<_>>();
⋮----
unverified_sender.send(vec![packet_bundle_1]).unwrap();
⋮----
assert_eq!(

================
File: core/src/bundle_stage.rs
================
pub mod bundle_account_locker;
mod bundle_consumer;
mod bundle_packet_deserializer;
mod bundle_storage;
⋮----
pub struct BundleStageLoopMetrics {
⋮----
impl Default for BundleStageLoopMetrics {
fn default() -> Self {
⋮----
num_bundles_received: Saturating(0),
num_packets_received: Saturating(0),
newly_buffered_bundles_count: Saturating(0),
current_buffered_bundles_count: Saturating(0),
current_buffered_packets_count: Saturating(0),
cost_model_buffered_bundles_count: Saturating(0),
num_bundles_dropped: Saturating(0),
receive_and_buffer_bundles_elapsed_us: Saturating(0),
process_buffered_bundles_elapsed_us: Saturating(0),
num_bundles_dropped_empty_batch: Saturating(0),
num_bundles_dropped_container_full: Saturating(0),
num_bundles_dropped_packet_marked_discard: Saturating(0),
num_bundles_dropped_packet_filter_error: Saturating(0),
num_bundles_dropped_bundle_too_large: Saturating(0),
num_bundles_dropped_duplicate_transaction: Saturating(0),
tip_programs_error: Saturating(0),
bundle_lock_errors: Saturating(0),
bundles_processed: Saturating(0),
⋮----
impl BundleStageLoopMetrics {
pub fn increment_num_bundles_received(&mut self, count: u64) {
⋮----
pub fn increment_num_packets_received(&mut self, count: u64) {
⋮----
pub fn increment_newly_buffered_bundles_count(&mut self, count: u64) {
⋮----
pub fn set_current_buffered_bundles_count(&mut self, count: u64) {
self.current_buffered_bundles_count = Saturating(count);
⋮----
pub fn set_current_buffered_packets_count(&mut self, count: u64) {
self.current_buffered_packets_count = Saturating(count);
⋮----
pub fn set_cost_model_buffered_bundles_count(&mut self, count: u64) {
self.cost_model_buffered_bundles_count = Saturating(count);
⋮----
pub fn increment_tip_programs_error(&mut self, count: u64) {
⋮----
pub fn increment_bundle_lock_errors(&mut self, count: u64) {
⋮----
pub fn increment_bundles_processed(&mut self, count: u64) {
⋮----
pub fn increment_bundle_dropped_error(&mut self, error: BundleStorageError) {
⋮----
pub fn increment_receive_and_buffer_bundles_elapsed_us(&mut self, count: u64) {
⋮----
pub fn increment_process_buffered_bundles_elapsed_us(&mut self, count: u64) {
⋮----
fn maybe_report(&mut self, report_interval_ms: u64) {
if self.last_report.elapsed().as_millis() >= report_interval_ms as u128 && self.has_data() {
datapoint_info!(
⋮----
self.clear();
⋮----
fn clear(&mut self) {
self.num_bundles_received = Saturating(0);
self.num_packets_received = Saturating(0);
self.newly_buffered_bundles_count = Saturating(0);
self.current_buffered_bundles_count = Saturating(0);
self.current_buffered_packets_count = Saturating(0);
self.cost_model_buffered_bundles_count = Saturating(0);
self.num_bundles_dropped = Saturating(0);
self.receive_and_buffer_bundles_elapsed_us = Saturating(0);
self.process_buffered_bundles_elapsed_us = Saturating(0);
self.num_bundles_dropped_empty_batch = Saturating(0);
self.num_bundles_dropped_container_full = Saturating(0);
self.num_bundles_dropped_packet_marked_discard = Saturating(0);
self.num_bundles_dropped_packet_filter_error = Saturating(0);
self.num_bundles_dropped_bundle_too_large = Saturating(0);
self.num_bundles_dropped_duplicate_transaction = Saturating(0);
self.tip_programs_error = Saturating(0);
self.bundle_lock_errors = Saturating(0);
self.bundles_processed = Saturating(0);
⋮----
pub fn has_data(&self) -> bool {
⋮----
type BundleExecutionResult<T> = Result<T, BundleExecutionError>;
⋮----
enum BundleExecutionError {
⋮----
pub struct BundleStage {
⋮----
impl BundleStage {
⋮----
pub fn new(
⋮----
pub fn join(self) -> thread::Result<()> {
self.bundle_thread.join()
⋮----
fn start_bundle_thread(
⋮----
prioritization_fee_cache.clone(),
⋮----
let decision_maker = DecisionMaker::from(poh_recorder.read().unwrap().deref());
⋮----
let block_builder_fee_info = block_builder_fee_info.clone();
let cluster_info = cluster_info.clone();
⋮----
.name("solBundleStgTx".to_string())
.spawn(move || {
⋮----
.unwrap();
⋮----
fn process_loop(
⋮----
while !exit.load(Ordering::Relaxed) {
if bundle_storage.unprocessed_bundles_len() > 0
|| last_metrics_update.elapsed() >= SLOT_BOUNDARY_CHECK_PERIOD
⋮----
measure_us!(Self::process_buffered_bundles(
⋮----
bundle_stage_metrics.increment_process_buffered_bundles_elapsed_us(
⋮----
let elapsed = start.elapsed();
⋮----
.increment_receive_and_buffer_bundles_elapsed_us(elapsed.as_micros() as u64);
bundle_stage_metrics.set_current_buffered_bundles_count(
bundle_storage.unprocessed_bundles_len() as u64,
⋮----
.set_current_buffered_packets_count(bundle_storage.num_packets_buffered() as u64);
bundle_stage_metrics.set_cost_model_buffered_bundles_count(
bundle_storage.cost_model_buffered_bundles_len() as u64,
⋮----
bundle_stage_metrics.maybe_report(20);
⋮----
fn receive_and_buffer_bundles(
⋮----
let bank_forks = bank_forks.read().unwrap();
let root_bank = bank_forks.root_bank();
let working_bank = bank_forks.working_bank();
⋮----
let recv_timeout = if bundle_storage.unprocessed_bundles_len() > 0 {
⋮----
let bundle = bundle_receiver.recv_timeout(recv_timeout)?;
⋮----
while let Ok(bundle) = bundle_receiver.try_recv() {
⋮----
Ok(())
⋮----
fn insert_bundle(
⋮----
let num_packets = bundle.batch().len();
bundle_stage_metrics.increment_num_bundles_received(1);
bundle_stage_metrics.increment_num_packets_received(num_packets as u64);
match bundle_storage.insert_bundle(bundle, root_bank, working_bank, blacklisted_accounts) {
⋮----
bundle_stage_metrics.increment_newly_buffered_bundles_count(1);
⋮----
bundle_stage_metrics.increment_bundle_dropped_error(e);
⋮----
fn process_buffered_bundles(
⋮----
match decision_maker.make_consume_or_forward_decision() {
⋮----
bundle_storage.clear();
⋮----
fn consume_bundles(
⋮----
const BUNDLE_WINDOW_SIZE: NonZeroUsize = NonZeroUsize::new(10).unwrap();
let mut bundles = VecDeque::with_capacity(BUNDLE_WINDOW_SIZE.get());
if bank.slot() != *last_tip_update_slot {
⋮----
.is_err()
⋮----
bundle_stage_metrics.increment_tip_programs_error(1);
error!("tip programs error, not processing bundles");
⋮----
*last_tip_update_slot = bank.slot();
⋮----
while let Some(bundle) = bundle_storage.pop_bundle(bank.slot()) {
⋮----
.lock_bundle(&bundle.transactions, bank)
⋮----
bundle_stage_metrics.increment_bundle_lock_errors(1);
bundle_storage.destroy_bundle(bundle);
⋮----
bundles.push_back(bundle);
if bundles.len() == BUNDLE_WINDOW_SIZE.get() {
let bundle = bundles.pop_front().unwrap();
⋮----
consume_worker_metrics.update_for_consume(&output);
consume_worker_metrics.set_has_data(true);
bundle_stage_metrics.increment_bundles_processed(1);
⋮----
consume_worker_metrics.maybe_report_and_reset();
⋮----
while let Some(bundle) = bundles.pop_front() {
⋮----
debug_assert!(
⋮----
fn handle_tip_programs(
⋮----
let keypair = cluster_info.keypair();
⋮----
fn handle_initialize_tip_programs(
⋮----
.get_initialize_tip_programs_bundle(bank, keypair)
.map_err(|e| {
warn!("tip programs initialize error: {e:?}");
⋮----
if initialize_tip_program_transactions.is_empty() {
return Ok(());
⋮----
let max_ages = vec![
⋮----
let _ = bundle_account_locker.lock_bundle(&initialize_tip_program_transactions, bank);
let output = consumer.process_and_record_aged_transactions(
⋮----
let _ = bundle_account_locker.unlock_bundle(&initialize_tip_program_transactions, bank);
⋮----
debug!(
⋮----
info!("initialize tip program output: {bundle_result:?}");
⋮----
fn handle_crank_tip_programs(
⋮----
.get_tip_programs_crank_bundle(bank, keypair, &block_builder_fee_info.lock().unwrap())
⋮----
warn!("tip programs crank error: {e:?}");
⋮----
if crank_tip_program_transactions.is_empty() {
⋮----
let _ = bundle_account_locker.lock_bundle(&crank_tip_program_transactions, bank);
⋮----
let _ = bundle_account_locker.unlock_bundle(&crank_tip_program_transactions, bank);
⋮----
info!("crank tip program output: {bundle_result:?}");
⋮----
fn to_bundle_result(output: &ProcessTransactionBatchOutput) -> BundleExecutionResult<()> {
⋮----
.is_ok()
⋮----
.as_ref()
.unwrap()
.iter()
.all(|r| matches!(r, CommitTransactionDetails::Committed { result: Ok(_), .. }))
⋮----
.any(|r| {
matches!(
⋮----
return Err(BundleExecutionError::ErrorRetryable);
⋮----
Err(BundleExecutionError::ErrorNonRetryable)
⋮----
fn process_bundle(
⋮----
if bank.is_complete() {
let _ = bundle_account_locker.unlock_bundle(&bundle.transactions, bank);
bundle_storage.retry_bundle(bundle);
⋮----
.map(|tx| tx.signatures())
.collect();
⋮----
Some(output)
⋮----
mod tests {
⋮----
struct TestFixture {
⋮----
fn create_genesis_config(mint_sol: u64) -> TestFixture {
⋮----
let mut genesis_config = create_genesis_config_with_leader_ex(
⋮----
&mint_keypair.pubkey(),
&leader_keypair.pubkey(),
&voting_keypair.pubkey(),
⋮----
rent.minimum_balance(VoteStateV4::size_of()) + (LAMPORTS_PER_SOL * 1_000_000),
⋮----
rent.clone(),
⋮----
spl_programs(&rent),
⋮----
validator_pubkey: leader_keypair.pubkey(),
⋮----
fn test_tip_programs_initialized_with_no_bundles() {
⋮----
} = create_genesis_config(2);
⋮----
bank_forks.write().unwrap().insert(bank);
let bank = bank_forks.read().unwrap().working_bank();
assert_eq!(bank.slot(), 1);
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger"),
⋮----
) = create_test_recorder(bank.clone(), blockstore, None, None);
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
ContactInfo::new_localhost(&leader_keypair.pubkey(), timestamp()),
Arc::new(leader_keypair.insecure_clone()),
⋮----
let (verified_bundle_sender, verified_bundle_receiver) = bounded(1024);
⋮----
exit.clone(),
⋮----
tip_payment_program_id: Pubkey::from(jito_tip_payment::id().to_bytes()),
tip_distribution_program_id: Pubkey::from(jito_tip_distribution::id().to_bytes()),
⋮----
vote_account: genesis_config_info.voting_keypair.pubkey(),
⋮----
while start.elapsed() < Duration::from_secs(2) {
⋮----
entry_receiever.recv_timeout(Duration::from_millis(1))
⋮----
tx_count += entry.transactions.len();
⋮----
assert!(tx_count <= NUM_TXS_EXPECTED, "tx_count: {tx_count}");
⋮----
.get_account(&JitoTipPaymentConfig::find_program_address(&jito_tip_payment::id()).0)
⋮----
.get_account(
⋮----
assert_eq!(
⋮----
assert_eq!(tip_payment_config.block_builder_commission_pct(), 10);
⋮----
assert_eq!(tip_distribution_config.num_epochs_valid(), 10);
⋮----
drop(verified_bundle_sender);
exit.store(true, Ordering::Relaxed);
poh_service.join().unwrap();
bundle_stage.join().unwrap();
⋮----
fn test_process_bad_bundle() {
⋮----
VerifiedPacketBundle::new(PacketBatch::from(vec![BytesPacket::from_data(
⋮----
verified_bundle_sender.send(verified_bundle).unwrap();
⋮----
assert!(tx_count <= MAX_EXPECTED_TXS, "tx_count: {tx_count}");
⋮----
fn test_process_sequential_bundles() {
⋮----
let verified_bundle = VerifiedPacketBundle::new(PacketBatch::from(vec![
⋮----
let balance = bank.get_balance(&kp1.pubkey());
⋮----
let balance = bank.get_balance(&kp2.pubkey());
⋮----
fn test_partial_revert_bundle() {
⋮----
fn test_already_processed_tx_drops_bundle() {
⋮----
let tx = transfer(
⋮----
&kp.pubkey(),
genesis_config_info.genesis_config.rent.minimum_balance(0) * 10,
bank.last_blockhash(),
⋮----
assert_eq!(tx_count, 0, "tx_count: {tx_count}");

================
File: core/src/bundle.rs
================
pub struct SanitizedBundle<Tx: TransactionWithMeta> {
⋮----
pub fn new(transactions: Vec<Tx>) -> Self {
let bundle_id = derive_bundle_id(&transactions);
⋮----
pub fn transactions(&self) -> &[Tx] {
⋮----
pub fn bundle_id(&self) -> &String {

================
File: core/src/cluster_info_vote_listener.rs
================
pub type ThresholdConfirmedSlots = Vec<(Slot, Hash)>;
pub type VerifiedVoteTransactionsSender = Sender<Vec<Transaction>>;
pub type VerifiedVoteTransactionsReceiver = Receiver<Vec<Transaction>>;
pub type VerifiedVoterSlotsSender = Sender<(Pubkey, Vec<Slot>)>;
pub type VerifiedVoterSlotsReceiver = Receiver<(Pubkey, Vec<Slot>)>;
pub type GossipVerifiedVoteHashSender = Sender<(Pubkey, Slot, Hash)>;
pub type GossipVerifiedVoteHashReceiver = Receiver<(Pubkey, Slot, Hash)>;
pub type DuplicateConfirmedSlotsSender = Sender<ThresholdConfirmedSlots>;
pub type DuplicateConfirmedSlotsReceiver = Receiver<ThresholdConfirmedSlots>;
⋮----
pub struct SlotVoteTracker {
⋮----
impl SlotVoteTracker {
pub(crate) fn get_voted_slot_updates(&mut self) -> Option<Vec<Pubkey>> {
self.voted_slot_updates.take()
⋮----
fn get_or_insert_optimistic_votes_tracker(&mut self, hash: Hash) -> &mut VoteStakeTracker {
self.optimistic_votes_tracker.entry(hash).or_default()
⋮----
pub(crate) fn optimistic_votes_tracker(&self, hash: &Hash) -> Option<&VoteStakeTracker> {
self.optimistic_votes_tracker.get(hash)
⋮----
pub struct VoteTracker {
⋮----
impl VoteTracker {
fn get_or_insert_slot_tracker(&self, slot: Slot) -> Arc<RwLock<SlotVoteTracker>> {
if let Some(slot_vote_tracker) = self.slot_vote_trackers.read().unwrap().get(&slot) {
return slot_vote_tracker.clone();
⋮----
let mut slot_vote_trackers = self.slot_vote_trackers.write().unwrap();
slot_vote_trackers.entry(slot).or_default().clone()
⋮----
pub(crate) fn get_slot_vote_tracker(&self, slot: Slot) -> Option<Arc<RwLock<SlotVoteTracker>>> {
self.slot_vote_trackers.read().unwrap().get(&slot).cloned()
⋮----
pub(crate) fn insert_vote(&self, slot: Slot, pubkey: Pubkey) {
let mut w_slot_vote_trackers = self.slot_vote_trackers.write().unwrap();
let slot_vote_tracker = w_slot_vote_trackers.entry(slot).or_default();
let mut w_slot_vote_tracker = slot_vote_tracker.write().unwrap();
w_slot_vote_tracker.voted.insert(pubkey, true);
⋮----
voted_slot_updates.push(pubkey)
⋮----
w_slot_vote_tracker.voted_slot_updates = Some(vec![pubkey]);
⋮----
fn purge_stale_state(&self, root_bank: &Bank) {
let new_root = root_bank.slot();
⋮----
.write()
.unwrap()
.retain(|slot, _| *slot >= new_root);
⋮----
fn progress_with_new_root_bank(&self, root_bank: &Bank) {
self.purge_stale_state(root_bank);
⋮----
struct VoteProcessingTiming {
⋮----
impl VoteProcessingTiming {
fn reset(&mut self) {
⋮----
fn update(&mut self, vote_txn_processing_time_us: u64, vote_slot_confirming_time_us: u64) {
⋮----
.should_update(VOTE_PROCESSING_REPORT_INTERVAL_MS)
⋮----
datapoint_info!(
⋮----
self.reset();
⋮----
pub struct ClusterInfoVoteListener {
⋮----
impl ClusterInfoVoteListener {
⋮----
pub fn new(
⋮----
let (verified_vote_transactions_sender, verified_vote_transactions_receiver) = unbounded();
⋮----
let exit = exit.clone();
let sharable_banks = bank_forks.read().unwrap().sharable_banks();
⋮----
.name("solCiVoteLstnr".to_string())
.spawn(move || {
⋮----
.name("solCiProcVotes".to_string())
⋮----
let dumped_slot_subscription = bank_hash_cache.dumped_slot_subscription();
⋮----
subscriptions.as_deref(),
⋮----
.unwrap();
⋮----
thread_hdls: vec![listen_thread, process_thread],
⋮----
pub(crate) fn join(self) -> thread::Result<()> {
self.thread_hdls.into_iter().try_for_each(JoinHandle::join)
⋮----
fn recv_loop(
⋮----
while !exit.load(Ordering::Relaxed) {
let votes = cluster_info.get_votes(&mut cursor);
inc_new_counter_debug!("cluster_info_vote_listener-recv_count", votes.len());
if !votes.is_empty() {
⋮----
verified_vote_transactions_sender.send(vote_txs)?;
verified_packets_sender.send(BankingPacketBatch::new(packets))?;
⋮----
sleep(Duration::from_millis(GOSSIP_SLEEP_MILLIS));
⋮----
Ok(())
⋮----
fn verify_votes(
⋮----
votes.len(),
⋮----
let root_bank = sharable_banks.root();
let epoch_schedule = root_bank.epoch_schedule();
⋮----
.into_iter()
.zip(packet_batches)
.filter(|(_, packet_batch)| {
assert_eq!(packet_batch.len(), 1);
!packet_batch.get(0).unwrap().meta().discard()
⋮----
.filter_map(|(tx, packet_batch)| {
⋮----
let slot = vote.last_voted_slot()?;
let epoch = epoch_schedule.get_epoch(slot);
⋮----
.epoch_stakes(epoch)?
.epoch_authorized_voters()
.get(&vote_account_key)?;
let mut keys = tx.message.account_keys.iter().enumerate();
if !keys.any(|(i, key)| tx.message.is_signer(i) && key == authorized_voter) {
⋮----
Some((tx, packet_batch))
⋮----
.unzip()
⋮----
fn process_votes_loop(
⋮----
let mut confirmation_verifier = OptimisticConfirmationVerifier::new(bank_hash_cache.root());
⋮----
let duplicate_confirmed_slot_sender = Some(duplicate_confirmed_slot_sender);
let mut vote_processing_time = Some(VoteProcessingTiming::default());
⋮----
if exit.load(Ordering::Relaxed) {
return Ok(());
⋮----
let root_bank = bank_hash_cache.get_root_bank_and_prune_cache();
if last_process_root.elapsed().as_millis() > DEFAULT_MS_PER_SLOT as u128 {
⋮----
.verify_for_unrooted_optimistic_slots(&root_bank, &blockstore);
⋮----
vote_tracker.progress_with_new_root_bank(&root_bank);
⋮----
.add_new_optimistic_confirmed_slots(confirmed_slots.clone(), &blockstore);
⋮----
error!("thread {:?} error {:?}", thread::current().name(), e);
⋮----
fn listen_and_confirm_votes(
⋮----
sel.recv(gossip_vote_txs_receiver);
sel.recv(replay_votes_receiver);
⋮----
let _ = sel.ready_timeout(remaining_wait_time)?;
let gossip_vote_txs: Vec<_> = gossip_vote_txs_receiver.try_iter().flatten().collect();
let replay_votes: Vec<_> = replay_votes_receiver.try_iter().collect();
if !gossip_vote_txs.is_empty() || !replay_votes.is_empty() {
return Ok(Self::filter_and_confirm_with_new_votes(
⋮----
remaining_wait_time = remaining_wait_time.saturating_sub(start.elapsed());
⋮----
Ok(vec![])
⋮----
fn track_new_votes_and_notify_confirmations(
⋮----
if vote.is_empty() {
⋮----
let mut slots_dumped = dumped_slot_subscription.lock().unwrap();
let (last_vote_slot, last_vote_hash) = vote.last_voted_slot_hash().unwrap();
⋮----
.entry(*vote_pubkey)
.or_insert(0);
let root = root_bank.slot();
⋮----
let vote_slots = vote.slots();
⋮----
if let Some(hash) = bank_hash_cache.hash(last_vote_slot, &mut slots_dumped) {
hash == last_vote_hash && vote.is_full_tower_vote()
⋮----
.then_some(last_vote_hash)
.or(bank_hash_cache.hash(slot, &mut slots_dumped))
⋮----
for slot in vote_slots.iter().filter(|slot| **slot > root).rev() {
⋮----
let epoch = root_bank.epoch_schedule().get_epoch(slot);
let epoch_stakes = root_bank.epoch_stakes(epoch);
if epoch_stakes.is_none() {
⋮----
let epoch_stakes = epoch_stakes.unwrap();
⋮----
let vote_accounts = epoch_stakes.stakes().vote_accounts();
let stake = vote_accounts.get_delegated_stake(vote_pubkey);
let total_stake = epoch_stakes.total_stake();
let Some(hash) = get_hash(slot) else {
⋮----
let _ = gossip_verified_vote_hash_sender.send((*vote_pubkey, slot, hash));
⋮----
let _ = sender.send(vec![(slot, hash)]);
⋮----
new_optimistic_confirmed_slots.push((slot, hash));
⋮----
.as_ref()
.map(|s| s.get_current_declared_work());
⋮----
.send((
⋮----
.unwrap_or_else(|err| {
warn!("bank_notification_sender failed: {err:?}")
⋮----
diff.entry(slot)
.or_default()
⋮----
.and_modify(|seen_in_gossip_previously| {
⋮----
.or_insert(is_gossip_vote);
⋮----
*latest_vote_slot = max(*latest_vote_slot, last_vote_slot);
⋮----
rpc_subscriptions.notify_vote(*vote_pubkey, vote, vote_transaction_signature);
⋮----
let _ = verified_voter_slots_sender.send((*vote_pubkey, vote_slots));
⋮----
fn filter_and_confirm_with_new_votes(
⋮----
let mut new_optimistic_confirmed_slots = vec![];
⋮----
.iter()
.filter_map(vote_parser::parse_vote_transaction)
.zip(repeat( true))
.chain(replayed_votes.into_iter().zip(repeat( false)));
⋮----
gossip_vote_txn_processing_time.stop();
let gossip_vote_txn_processing_time_us = gossip_vote_txn_processing_time.as_us();
⋮----
let slot_tracker = vote_tracker.get_or_insert_slot_tracker(slot);
⋮----
let r_slot_tracker = slot_tracker.read().unwrap();
slot_diff.retain(|pubkey, seen_in_gossip_above| {
let seen_in_gossip_previously = r_slot_tracker.voted.get(pubkey);
let is_new = seen_in_gossip_previously.is_none();
let is_new_from_gossip = !seen_in_gossip_previously.cloned().unwrap_or(false)
⋮----
let mut w_slot_tracker = slot_tracker.write().unwrap();
if w_slot_tracker.voted_slot_updates.is_none() {
w_slot_tracker.voted_slot_updates = Some(vec![]);
⋮----
w_slot_tracker.voted.insert(pubkey, seen_in_gossip_above);
⋮----
.as_mut()
⋮----
.push(pubkey);
⋮----
gossip_vote_slot_confirming_time.stop();
let gossip_vote_slot_confirming_time_us = gossip_vote_slot_confirming_time.as_us();
⋮----
vote_processing_time.update(
⋮----
fn track_optimistic_confirmation_vote(
⋮----
.get_or_insert_optimistic_votes_tracker(hash)
.add_vote_pubkey(pubkey, stake, total_epoch_stake, &THRESHOLDS_TO_CHECK)
⋮----
fn sum_stake(sum: &mut u64, epoch_stakes: Option<&VersionedEpochStakes>, pubkey: &Pubkey) {
⋮----
*sum += stakes.stakes().vote_accounts().get_delegated_stake(pubkey)
⋮----
mod tests {
⋮----
fn test_max_vote_tx_fits() {
⋮----
Some(Hash::default()),
⋮----
use bincode::serialized_size;
info!("max vote size {}", serialized_size(&vote_tx).unwrap());
⋮----
assert_eq!(packet_batches.len(), 1);
⋮----
fn test_update_new_root() {
⋮----
} = setup();
⋮----
vote_tracker.insert_vote(bank.slot(), new_voter_);
assert!(vote_tracker
⋮----
let bank1 = Bank::new_from_parent(bank.clone(), &Pubkey::default(), bank.slot() + 1);
vote_tracker.progress_with_new_root_bank(&bank1);
assert!(!vote_tracker
⋮----
let current_epoch = bank.epoch();
⋮----
.epoch_schedule()
.get_first_slot_in_epoch(current_epoch + 1);
⋮----
vote_tracker.progress_with_new_root_bank(&new_epoch_bank);
⋮----
fn test_update_new_leader_schedule_epoch() {
let SetupComponents { bank, .. } = setup();
let leader_schedule_epoch = bank.get_leader_schedule_epoch(bank.slot());
⋮----
let mut next_leader_schedule_computed = bank.slot();
⋮----
if bank.get_leader_schedule_epoch(next_leader_schedule_computed)
⋮----
assert_eq!(
⋮----
fn test_votes_in_range() {
⋮----
let (votes_sender, votes_receiver) = unbounded();
let (verified_voter_slots_sender, _verified_voter_slots_receiver) = unbounded();
let (gossip_verified_vote_hash_sender, _gossip_verified_vote_hash_receiver) = unbounded();
let (replay_votes_sender, replay_votes_receiver) = unbounded();
⋮----
vec![stake_per_validator; validator_voting_keypairs.len()],
⋮----
let vote_slots = vec![1, 2];
send_vote_txs(
⋮----
vec![],
⋮----
Some(&subscriptions),
⋮----
let max_epoch = bank3.get_leader_schedule_epoch(bank3.slot());
assert!(bank3.epoch_stakes(max_epoch).is_some());
⋮----
assert!(bank3.epoch_stakes(unknown_epoch).is_none());
⋮----
.get_first_slot_in_epoch(unknown_epoch);
let vote_slots = vec![first_slot_in_unknown_epoch, first_slot_in_unknown_epoch + 1];
⋮----
assert!(vote_tracker.slot_vote_trackers.read().unwrap().is_empty());
⋮----
fn send_vote_txs(
⋮----
validator_voting_keypairs.iter().for_each(|keypairs| {
⋮----
tower_sync.clone(),
⋮----
votes_sender.send(vec![vote_tx]).unwrap();
let replay_vote = Vote::new(replay_vote_slots.clone(), Hash::default());
⋮----
vote_keypair.pubkey(),
VoteTransaction::from(replay_vote.clone()),
⋮----
fn run_test_process_votes(hash: Option<Hash>) {
⋮----
let (votes_txs_sender, votes_txs_receiver) = unbounded();
⋮----
let (gossip_verified_vote_hash_sender, gossip_verified_vote_hash_receiver) = unbounded();
let (verified_voter_slots_sender, verified_voter_slots_receiver) = unbounded();
⋮----
let gossip_vote_slots = vec![1, 2];
let replay_vote_slots = vec![3, 4];
⋮----
gossip_vote_slots.clone(),
replay_vote_slots.clone(),
⋮----
for (pubkey, slot, hash) in gossip_verified_vote_hash_receiver.try_iter() {
⋮----
.get(&slot)
.and_then(|slot_hashes| slot_hashes.get(&hash))
.map(|slot_hash_voters| slot_hash_voters.contains(&pubkey))
.unwrap_or(false);
assert!(!exists);
⋮----
.entry(slot)
⋮----
.entry(hash)
⋮----
let last_gossip_vote_slot = *gossip_vote_slots.last().unwrap();
assert_eq!(gossip_verified_votes.len(), 1);
let slot_hashes = gossip_verified_votes.get(&last_gossip_vote_slot).unwrap();
assert_eq!(slot_hashes.len(), 1);
let slot_hash_votes = slot_hashes.get(&Hash::default()).unwrap();
assert_eq!(slot_hash_votes.len(), validator_voting_keypairs.len());
⋮----
let pubkey = voting_keypairs.vote_keypair.pubkey();
assert!(slot_hash_votes.contains(&pubkey));
⋮----
.clone()
⋮----
.chain(replay_vote_slots.clone())
.collect();
⋮----
for (received_pubkey, new_slots) in verified_voter_slots_receiver.try_iter() {
let already_received_slots = pubkey_to_slots.entry(received_pubkey).or_default();
⋮----
assert!(already_received_slots.insert(new_slot));
⋮----
assert_eq!(pubkey_to_slots.len(), validator_voting_keypairs.len());
⋮----
let slot_vote_tracker = vote_tracker.get_slot_vote_tracker(vote_slot).unwrap();
let r_slot_vote_tracker = slot_vote_tracker.read().unwrap();
⋮----
assert!(r_slot_vote_tracker.voted.contains_key(&pubkey));
assert!(r_slot_vote_tracker
⋮----
r_slot_vote_tracker.optimistic_votes_tracker(&Hash::default());
if vote_slot == *gossip_vote_slots.last().unwrap()
|| vote_slot == *replay_vote_slots.last().unwrap()
⋮----
let optimistic_votes_tracker = optimistic_votes_tracker.unwrap();
assert!(optimistic_votes_tracker.voted().contains(&pubkey));
⋮----
assert!(optimistic_votes_tracker.is_none())
⋮----
fn test_process_votes1() {
run_test_process_votes(None);
run_test_process_votes(Some(Hash::default()));
⋮----
fn test_process_votes2() {
⋮----
let (_replay_votes_sender, replay_votes_receiver) = unbounded();
⋮----
let mut expected_voter_slots = vec![];
⋮----
.chunks(num_voters_per_slot)
.enumerate()
⋮----
.map(|keypairs| {
⋮----
expected_voter_slots.push((vote_keypair.pubkey(), vec![i as Slot + 1]));
⋮----
TowerSync::new_from_slots(vec![(i as u64 + 1)], bank_hash, None);
⋮----
votes_txs_sender.send(validator_votes).unwrap();
⋮----
let received_voter_slots = verified_voter_slots_receiver.try_iter().collect::<Vec<_>>();
assert_eq!(received_voter_slots.len(), validator_voting_keypairs.len());
for (e, r) in expected_voter_slots.iter().zip(received_voter_slots.iter()) {
assert_eq!(e, r);
⋮----
for (i, keyset) in validator_voting_keypairs.chunks(2).enumerate() {
let slot_vote_tracker = vote_tracker.get_slot_vote_tracker(i as u64 + 1).unwrap();
let r_slot_vote_tracker = &slot_vote_tracker.read().unwrap();
⋮----
.optimistic_votes_tracker(&bank_hash)
⋮----
fn run_test_process_votes3(switch_proof_hash: Option<Hash>) {
⋮----
unbounded();
⋮----
let ordered_events = vec![
⋮----
TowerSync::new_from_slots(vec![(vote_slot)], vote_bank_hash, None);
⋮----
votes_sender.send(vec![vote_tx.clone()]).unwrap();
⋮----
VoteTransaction::from(Vote::new(vec![vote_slot], Hash::default())),
⋮----
if events == vec![1] {
assert_eq!(r_slot_vote_tracker.gossip_only_stake, 0);
⋮----
assert_eq!(r_slot_vote_tracker.gossip_only_stake, 100);
⋮----
fn test_run_test_process_votes3() {
run_test_process_votes3(None);
run_test_process_votes3(Some(Hash::default()));
⋮----
fn test_vote_tracker_references() {
⋮----
(0..2).map(|_| ValidatorVoteKeypairs::new_rand()).collect();
⋮----
vec![100; validator_keypairs.len()],
⋮----
let bank = bank_forks.read().unwrap().get(0).unwrap();
⋮----
bank_forks.clone(),
⋮----
let voted_slot = bank.slot() + 1;
let vote_tx = vec![vote_transaction::new_tower_sync_transaction(
⋮----
vec![(
⋮----
let old_epoch = bank.get_leader_schedule_epoch(bank.slot());
⋮----
let first_slot_in_new_epoch = bank.epoch_schedule().get_first_slot_in_epoch(new_epoch);
⋮----
.map(|slot| {
⋮----
TowerSync::from(vec![(*slot, 1)]),
⋮----
struct SetupComponents {
⋮----
fn setup() -> SetupComponents {
⋮----
(0..10).map(|_| ValidatorVoteKeypairs::new_rand()).collect();
⋮----
vec![100; validator_voting_keypairs.len()],
⋮----
fn test_verify_votes_empty() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
let votes = vec![];
⋮----
assert!(vote_txs.is_empty());
assert!(packets.is_empty());
⋮----
fn verify_packets_len(packets: &[PacketBatch], ref_value: usize) {
let num_packets: usize = packets.iter().map(|pb| pb.len()).sum();
assert_eq!(num_packets, ref_value);
⋮----
fn test_vote_tx(
⋮----
let validator_vote_keypair = validator_vote_keypairs.unwrap_or(&other);
⋮----
TowerSync::from(vec![(0, 1)]),
⋮----
fn run_test_verify_votes_1_pass(hash: Option<Hash>) {
let voting_keypairs: Vec<_> = repeat_with(ValidatorVoteKeypairs::new_rand)
.take(10)
⋮----
vec![100; voting_keypairs.len()],
⋮----
let vote_tx = test_vote_tx(voting_keypairs.first(), hash);
let votes = vec![vote_tx];
⋮----
assert_eq!(vote_txs.len(), 1);
verify_packets_len(&packets, 1);
⋮----
fn test_verify_votes_1_pass() {
run_test_verify_votes_1_pass(None);
run_test_verify_votes_1_pass(Some(Hash::default()));
⋮----
fn run_test_bad_vote(hash: Option<Hash>) {
⋮----
let mut bad_vote = vote_tx.clone();
⋮----
let votes = vec![vote_tx.clone(), bad_vote, vote_tx];
⋮----
assert_eq!(vote_txs.len(), 2);
verify_packets_len(&packets, 2);
⋮----
fn test_sum_stake() {
⋮----
let epoch_stakes = bank.epoch_stakes(bank.epoch()).unwrap();
⋮----
Some(epoch_stakes),
&vote_keypair.pubkey(),
⋮----
assert_eq!(gossip_only_stake, 100);
⋮----
fn test_bad_vote() {
run_test_bad_vote(None);
run_test_bad_vote(Some(Hash::default()));
⋮----
fn test_track_new_votes_filter() {
⋮----
TowerSync::from(vec![(1, 3), (2, 2), (6, 1)]),
⋮----
assert_eq!(diff.keys().copied().sorted().collect_vec(), vec![1, 2, 6]);
diff.clear();
⋮----
TowerSync::from(vec![(1, 6), (2, 5), (3, 4), (4, 3), (7, 2), (8, 1)]),
⋮----
assert_eq!(diff.keys().copied().sorted().collect_vec(), vec![7, 8]);

================
File: core/src/cluster_slots_service.rs
================
pub mod cluster_slots;
pub mod slot_supporters;
⋮----
pub type ClusterSlotsUpdateReceiver = Receiver<Vec<Slot>>;
pub type ClusterSlotsUpdateSender = Sender<Vec<Slot>>;
⋮----
struct ClusterSlotsServiceTiming {
⋮----
impl ClusterSlotsServiceTiming {
fn update(&mut self, lowest_slot_elapsed: u64, process_cluster_slots_updates_elapsed: u64) {
⋮----
pub struct ClusterSlotsService {
⋮----
impl ClusterSlotsService {
pub fn new(
⋮----
.name("solClusterSlots".to_string())
.spawn(move || {
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_cluster_slots_service.join()
⋮----
fn run(
⋮----
let mut epoch_specs = EpochSpecs::from(bank_forks.clone());
let root_bank = bank_forks.read().unwrap().root_bank();
cluster_slots.update(&root_bank, &cluster_info);
while !exit.load(Ordering::Relaxed) {
let slots = match cluster_slots_update_receiver.recv_timeout(Duration::from_millis(200))
⋮----
Ok(slots) => Some(slots),
⋮----
warn!("Cluster slots service - sender disconnected");
⋮----
let lowest_slot = blockstore.lowest_slot();
⋮----
lowest_slot_elapsed.stop();
⋮----
let node_id = cluster_info.id();
⋮----
.current_epoch_staked_nodes()
.get(&node_id)
.cloned()
.unwrap_or_default();
⋮----
while cluster_slots_update_receiver.try_recv().is_ok() {}
⋮----
process_cluster_slots_updates_elapsed.stop();
cluster_slots_service_timing.update(
lowest_slot_elapsed.as_us(),
process_cluster_slots_updates_elapsed.as_us(),
⋮----
if last_stats.elapsed().as_secs() > 2 {
datapoint_info!(
⋮----
fn process_cluster_slots_updates(
⋮----
while let Ok(mut more) = cluster_slots_update_receiver.try_recv() {
slots.append(&mut more);
⋮----
slots.sort();
if !slots.is_empty() {
cluster_info.push_epoch_slots(&slots);
⋮----
fn initialize_lowest_slot(blockstore: &Blockstore, cluster_info: &ClusterInfo) {
cluster_info.push_lowest_slot(blockstore.lowest_slot());
⋮----
fn update_lowest_slot(lowest_slot: Slot, cluster_info: &ClusterInfo) {
cluster_info.push_lowest_slot(lowest_slot);
⋮----
fn initialize_epoch_slots(bank_forks: &RwLock<BankForks>, cluster_info: &ClusterInfo) {
⋮----
.read()
.unwrap()
.frozen_banks()
.map(|(slot, _bank)| slot)
.collect();
frozen_bank_slots.sort_unstable();
if !frozen_bank_slots.is_empty() {
cluster_info.push_epoch_slots(&frozen_bank_slots);
⋮----
mod test {
⋮----
pub fn test_update_lowest_slot() {
⋮----
let pubkey = keypair.pubkey();
⋮----
cluster_info.flush_push_queue();
⋮----
let gossip_crds = cluster_info.gossip.crds.read().unwrap();
gossip_crds.get::<&LowestSlot>(pubkey).unwrap().clone()
⋮----
assert_eq!(lowest.lowest, 5);

================
File: core/src/commitment_service.rs
================
pub struct CommitmentAggregationData {
⋮----
impl CommitmentAggregationData {
pub fn new(
⋮----
fn get_highest_super_majority_root(mut rooted_stake: Vec<(Slot, u64)>, total_stake: u64) -> Slot {
rooted_stake.sort_by(|a, b| a.0.cmp(&b.0).reverse());
⋮----
pub struct AggregateCommitmentService {
⋮----
impl AggregateCommitmentService {
⋮----
) = unbounded();
⋮----
.name("solAggCommitSvc".to_string())
.spawn(move || loop {
if exit.load(Ordering::Relaxed) {
⋮----
subscriptions.as_deref(),
⋮----
.unwrap(),
⋮----
fn run(
⋮----
return Ok(());
⋮----
let aggregation_data = receiver.recv_timeout(Duration::from_secs(1))?;
let aggregation_data = receiver.try_iter().last().unwrap_or(aggregation_data);
let ancestors = aggregation_data.bank.status_cache_ancestors();
if ancestors.is_empty() {
⋮----
aggregate_commitment_time.stop();
datapoint_info!(
⋮----
rpc_subscriptions.notify_subscribers(update_commitment_slots);
⋮----
fn update_commitment_cache(
⋮----
get_highest_super_majority_root(rooted_stake, aggregation_data.total_stake);
⋮----
slot: aggregation_data.bank.slot(),
⋮----
let highest_confirmed_slot = new_block_commitment.calculate_highest_confirmed_slot();
new_block_commitment.set_highest_confirmed_slot(highest_confirmed_slot);
let mut w_block_commitment_cache = block_commitment_cache.write().unwrap();
let highest_super_majority_root = max(
new_block_commitment.highest_super_majority_root(),
w_block_commitment_cache.highest_super_majority_root(),
⋮----
new_block_commitment.set_highest_super_majority_root(highest_super_majority_root);
⋮----
w_block_commitment_cache.commitment_slots()
⋮----
pub fn aggregate_commitment(
⋮----
assert!(!ancestors.is_empty());
for a in ancestors.windows(2) {
assert!(a[0] < a[1]);
⋮----
for (pubkey, (lamports, account)) in bank.vote_accounts().iter() {
⋮----
node_vote_state.clone()
⋮----
TowerVoteState::from(account.vote_state_view())
⋮----
fn aggregate_commitment_for_vote_account(
⋮----
for (i, a) in ancestors.iter().enumerate() {
⋮----
.entry(*a)
.or_default()
.increase_rooted_stake(lamports);
⋮----
rooted_stake.push((root, lamports));
⋮----
while ancestors[ancestors_index] <= vote.slot() {
⋮----
.entry(ancestors[ancestors_index])
⋮----
.increase_confirmation_stake(vote.confirmation_count() as usize, lamports);
⋮----
if ancestors_index == ancestors.len() {
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_commitment.join()
⋮----
mod tests {
⋮----
fn test_get_highest_super_majority_root() {
assert_eq!(get_highest_super_majority_root(vec![], 10), 0);
let rooted_stake = vec![(0, 5), (1, 5)];
assert_eq!(get_highest_super_majority_root(rooted_stake, 10), 0);
let rooted_stake = vec![(1, 5), (0, 10), (2, 5), (1, 4)];
assert_eq!(get_highest_super_majority_root(rooted_stake, 10), 1);
⋮----
fn test_aggregate_commitment_for_vote_account_1() {
let ancestors = vec![3, 4, 5, 7, 9, 11];
⋮----
let mut rooted_stake = vec![];
⋮----
let root = *ancestors.last().unwrap();
vote_state.root_slot = Some(root);
⋮----
expected.increase_rooted_stake(lamports);
assert_eq!(*commitment.get(&a).unwrap(), expected);
⋮----
assert_eq!(rooted_stake[0], (root, lamports));
⋮----
fn test_aggregate_commitment_for_vote_account_2() {
⋮----
vote_state.process_next_vote_slot(*ancestors.last().unwrap());
⋮----
expected.increase_confirmation_stake(1, lamports);
⋮----
fn test_aggregate_commitment_for_vote_account_3() {
let ancestors = vec![3, 4, 5, 7, 9, 10, 11];
⋮----
assert!(ancestors[4] + 2 >= ancestors[6]);
vote_state.process_next_vote_slot(ancestors[4]);
vote_state.process_next_vote_slot(ancestors[6]);
⋮----
assert_eq!(*commitment.get(a).unwrap(), expected);
⋮----
expected.increase_confirmation_stake(2, lamports);
⋮----
fn do_test_aggregate_commitment_validity(with_node_vote_state: bool) {
⋮----
} = create_genesis_config(10_000);
⋮----
genesis_config.accounts.extend(
vec![
⋮----
.into_iter()
.map(|(key, account)| (key, Account::from(account))),
⋮----
let mut vote_state1 = VoteStateV4::deserialize(vote_account1.data(), &pk1).unwrap();
process_slot_vote_unchecked(&mut vote_state1, 3);
process_slot_vote_unchecked(&mut vote_state1, 5);
⋮----
let versioned = VoteStateVersions::new_v4(vote_state1.clone());
vote_account1.set_state(&versioned).unwrap();
bank.store_account(&pk1, &vote_account1);
⋮----
let mut vote_state2 = VoteStateV4::deserialize(vote_account2.data(), &pk2).unwrap();
process_slot_vote_unchecked(&mut vote_state2, 9);
process_slot_vote_unchecked(&mut vote_state2, 10);
⋮----
vote_account2.set_state(&versioned).unwrap();
bank.store_account(&pk2, &vote_account2);
let mut vote_state3 = VoteStateV4::deserialize(vote_account3.data(), &pk3).unwrap();
vote_state3.root_slot = Some(1);
⋮----
vote_account3.set_state(&versioned).unwrap();
bank.store_account(&pk3, &vote_account3);
let mut vote_state4 = VoteStateV4::deserialize(vote_account4.data(), &pk4).unwrap();
vote_state4.root_slot = Some(2);
⋮----
vote_account4.set_state(&versioned).unwrap();
bank.store_account(&pk4, &vote_account4);
⋮----
expected.increase_confirmation_stake(2, 150);
⋮----
expected.increase_confirmation_stake(1, 100);
expected.increase_confirmation_stake(2, 50);
⋮----
expected.increase_confirmation_stake(1, 50);
⋮----
assert!(!commitment.contains_key(&a));
⋮----
assert_eq!(rooted_stake.len(), 2);
assert_eq!(get_highest_super_majority_root(rooted_stake, 100), 1)
⋮----
fn test_aggregate_commitment_validity_with_node_vote_state() {
do_test_aggregate_commitment_validity(true)
⋮----
fn test_aggregate_commitment_validity_without_node_vote_state() {
do_test_aggregate_commitment_validity(false);
⋮----
fn test_highest_super_majority_root_advance() {
fn get_vote_state(vote_pubkey: Pubkey, bank: &Bank) -> TowerVoteState {
let vote_account = bank.get_vote_account(&vote_pubkey).unwrap();
TowerVoteState::from(vote_account.vote_state_view())
⋮----
let validator_keypairs = vec![&validator_vote_keypairs];
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config_with_vote_accounts(
⋮----
vec![100; 1],
⋮----
let previous_bank = bank_forks.read().unwrap().get(x).unwrap();
⋮----
bank_forks.as_ref(),
previous_bank.clone(),
⋮----
let tower_sync = TowerSync::new_from_slot(x, previous_bank.hash());
⋮----
previous_bank.last_blockhash(),
⋮----
bank.process_transaction(&vote).unwrap();
⋮----
let working_bank = bank_forks.read().unwrap().working_bank();
let vote_pubkey = validator_vote_keypairs.vote_keypair.pubkey();
let root = get_vote_state(vote_pubkey, &working_bank)
⋮----
.unwrap();
⋮----
bank_forks.write().unwrap().set_root(x, None, None);
⋮----
let bank33 = bank_forks.read().unwrap().get(33).unwrap();
⋮----
bank33.clone(),
⋮----
let tower_sync = TowerSync::new_from_slot(33, bank33.hash());
⋮----
bank33.last_blockhash(),
⋮----
bank34.process_transaction(&vote33).unwrap();
⋮----
let vote_state = get_vote_state(vote_pubkey, &working_bank);
let root = vote_state.root_slot.unwrap();
let ancestors = working_bank.status_cache_ancestors();
⋮----
node_vote_state: (vote_pubkey, vote_state.clone()),
⋮----
.read()
.unwrap()
.highest_super_majority_root();
⋮----
.write()
⋮----
.set_root(root, None, Some(highest_super_majority_root));
⋮----
bank_forks.read().unwrap().get(highest_super_majority_root);
assert!(highest_super_majority_root_bank.is_some());
⋮----
let slots: Vec<_> = (lowest_slot..(x + 1)).filter(|s| *s != 34).collect();
⋮----
TowerSync::new_from_slots(slots, previous_bank.hash(), Some(lowest_slot - 1));
⋮----
get_vote_state(validator_vote_keypairs.vote_keypair.pubkey(), &working_bank);

================
File: core/src/completed_data_sets_service.rs
================
pub type CompletedDataSetsReceiver = Receiver<Vec<CompletedDataSetInfo>>;
pub type CompletedDataSetsSender = Sender<Vec<CompletedDataSetInfo>>;
pub struct CompletedDataSetsService {
⋮----
impl CompletedDataSetsService {
pub fn new(
⋮----
.name("solComplDataSet".to_string())
.spawn(move || {
info!("CompletedDataSetsService has started");
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
info!("CompletedDataSetsService has stopped");
⋮----
.unwrap();
⋮----
fn recv_completed_data_sets(
⋮----
match blockstore.get_entries_in_data_block(slot, indices,  None) {
⋮----
if !transactions.is_empty() {
rpc_subscriptions.notify_signatures_received((slot, transactions));
⋮----
Err(e) => warn!("completed-data-set-service deserialize error: {e:?}"),
⋮----
.recv_timeout(RECV_TIMEOUT)
.map(std::iter::once)?
.chain(completed_sets_receiver.try_iter())
.flatten()
.map(handle_completed_data_set_info);
if let Some(slot) = slots.max() {
max_slots.shred_insert.fetch_max(slot, Ordering::Relaxed);
⋮----
Ok(())
⋮----
fn get_transaction_signatures(entries: Vec<Entry>) -> Vec<Signature> {
⋮----
.into_iter()
.flat_map(|e| {
⋮----
.filter_map(|mut t| t.signatures.drain(..).next())
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
pub mod test {
⋮----
fn test_zero_signatures() {
⋮----
let entries = vec![Entry::new(&Hash::default(), 1, vec![tx])];
⋮----
assert!(signatures.is_empty());
⋮----
fn test_multi_signatures() {
⋮----
Transaction::new_signed_with_payer(&[], Some(&kp.pubkey()), &[&kp], Hash::default());
let entries = vec![Entry::new(&Hash::default(), 1, vec![tx.clone()])];
⋮----
assert_eq!(signatures.len(), 1);
let entries = vec![
⋮----
assert_eq!(signatures.len(), 3);

================
File: core/src/consensus.rs
================
pub mod fork_choice;
pub mod heaviest_subtree_fork_choice;
pub(crate) mod latest_validator_votes_for_frozen_banks;
pub mod progress_map;
mod tower1_14_11;
mod tower1_7_14;
pub mod tower_storage;
pub(crate) mod tower_vote_state;
pub mod tree_diff;
pub mod vote_stake_tracker;
⋮----
pub enum ThresholdDecision {
⋮----
impl ThresholdDecision {
pub fn passed(&self) -> bool {
matches!(self, Self::PassedThreshold)
⋮----
pub enum SwitchForkDecision {
⋮----
impl SwitchForkDecision {
pub fn to_vote_instruction(
⋮----
assert_ne!(*total_stake, 0);
⋮----
(SwitchForkDecision::SameFork, VoteTransaction::Vote(v)) => Some(
⋮----
Some(vote_instruction::update_vote_state(
⋮----
(SwitchForkDecision::SameFork, VoteTransaction::TowerSync(t)) => Some(
⋮----
Some(vote_instruction::vote_switch(
⋮----
) => Some(vote_instruction::update_vote_state_switch(
⋮----
Some(vote_instruction::tower_sync_switch(
⋮----
Some(vote_instruction::compact_update_vote_state(
⋮----
) => Some(vote_instruction::compact_update_vote_state_switch(
⋮----
pub fn can_vote(&self) -> bool {
⋮----
pub type Result<T> = std::result::Result<T, TowerError>;
pub type Stake = u64;
pub type VotedStakes = HashMap<Slot, Stake, ahash::RandomState>;
pub type PubkeyVotes = Vec<(Pubkey, Slot)>;
pub(crate) struct ComputedBankState {
⋮----
pub enum TowerVersions {
⋮----
impl TowerVersions {
pub fn new_current(tower: Tower) -> Self {
⋮----
pub fn convert_to_current(self) -> Tower {
⋮----
TowerVersions::V1_7_14(tower) => tower.into(),
TowerVersions::V1_14_11(tower) => tower.into(),
⋮----
pub(crate) enum BlockhashStatus {
⋮----
pub struct Tower {
⋮----
impl Default for Tower {
fn default() -> Self {
⋮----
tower.vote_state.root_slot = Some(Slot::default());
⋮----
fn from(tower: Tower) -> Self {
⋮----
fn from(tower: Tower1_14_11) -> Self {
⋮----
fn from(tower: Tower1_7_14) -> Self {
let box_last_vote = VoteTransaction::from(tower.last_vote.clone());
⋮----
impl Tower {
pub fn new(
⋮----
tower.initialize_lockouts_from_bank(vote_account_pubkey, root, bank);
⋮----
pub fn new_for_tests(threshold_depth: usize, threshold_size: f64) -> Self {
⋮----
pub fn new_random(node_pubkey: Pubkey) -> Self {
⋮----
let root_slot = rng.gen();
⋮----
.map(|x| LandedVote {
⋮----
u64::from(x).saturating_add(root_slot),
32_u32.saturating_sub(x),
⋮----
.collect();
⋮----
root_slot: Some(root_slot),
⋮----
.iter()
.map(|lv| (lv.slot(), lv.confirmation_count()))
⋮----
pub fn new_from_bankforks(
⋮----
let root_bank = bank_forks.root_bank();
⋮----
.frozen_banks()
.map(|(_slot, bank)| bank)
⋮----
root_bank.deref(),
⋮----
vec![],
⋮----
let root = root_bank.slot();
let (best_slot, best_hash) = heaviest_subtree_fork_choice.best_overall_slot();
⋮----
.get_with_checked_hash((best_slot, best_hash))
.expect(
⋮----
pub(crate) fn collect_vote_lockouts(
⋮----
let total_slots = bank_slot.saturating_sub(root_slot) as usize;
vote_slots.reserve(total_slots);
⋮----
.values()
.filter(|(voted_stake, _)| *voted_stake != 0)
.map(|(_, account)| account.vote_state_view().votes_len())
.sum();
⋮----
for (&key, (voted_stake, account)) in vote_accounts.iter() {
⋮----
trace!("{vote_account_pubkey} {key} with stake {voted_stake}");
let mut vote_state = TowerVoteState::from(account.vote_state_view());
lockout_intervals.extend(vote_state.votes.iter().map(|v| LockoutInterval {
start: v.slot(),
end: v.last_locked_out_slot(),
⋮----
my_latest_landed_vote = vote_state.nth_recent_lockout(0).map(|l| l.slot());
debug!("vote state {vote_state:?}");
debug!(
⋮----
debug!("observed root {}", vote_state.root_slot.unwrap_or(0) as i64);
datapoint_info!(
⋮----
if let Some(last_landed_voted_slot) = vote_state.last_voted_slot() {
latest_validator_votes_for_frozen_banks.check_add_vote(
⋮----
get_frozen_hash(last_landed_voted_slot),
⋮----
vote_state.process_next_vote_slot(bank_slot);
for slot in vote_state.votes.iter().filter_map(|v| {
let slot = v.slot();
(slot > root_slot).then_some(slot)
⋮----
vote_slots.insert(slot);
⋮----
trace!("ROOT: {root}");
vote_slots.insert(root);
⋮----
assert_eq!(
⋮----
if let Some(vote) = vote_state.nth_recent_lockout(1) {
⋮----
vote.slot(),
⋮----
vote_slots.iter().copied(),
⋮----
.get(&bank_slot)
.and_then(|ancestors| {
⋮----
.max()
.and_then(|parent| voted_stakes.get(parent))
.copied()
⋮----
.unwrap_or(0);
vote_slots.clear();
⋮----
fn is_slot_confirmed(
⋮----
.get(&slot)
.map(|stake| (*stake as f64 / total_stake as f64) > self.threshold_size)
.unwrap_or(false)
⋮----
pub(crate) fn is_slot_duplicate_confirmed(
⋮----
.map(|stake| (*stake as f64 / total_stake as f64) > DUPLICATE_THRESHOLD)
⋮----
pub fn tower_slots(&self) -> Vec<Slot> {
self.vote_state.tower()
⋮----
pub(crate) fn last_vote_tx_blockhash(&self) -> BlockhashStatus {
⋮----
pub fn refresh_last_vote_timestamp(&mut self, heaviest_slot_on_same_fork: Slot) {
let timestamp = if let Some(last_vote_timestamp) = self.last_vote.timestamp() {
last_vote_timestamp.saturating_add(1)
⋮----
self.last_timestamp.timestamp.saturating_add(1)
⋮----
if let Some(last_voted_slot) = self.last_vote.last_voted_slot() {
⋮----
warn!(
⋮----
self.last_vote.set_timestamp(Some(timestamp));
⋮----
pub fn refresh_last_vote_tx_blockhash(&mut self, new_vote_tx_blockhash: Hash) {
⋮----
pub(crate) fn mark_last_vote_tx_blockhash_non_voting(&mut self) {
⋮----
pub(crate) fn mark_last_vote_tx_blockhash_hot_spare(&mut self) {
⋮----
pub fn last_voted_slot_in_bank(bank: &Bank, vote_account_pubkey: &Pubkey) -> Option<Slot> {
let vote_account = bank.get_vote_account(vote_account_pubkey)?;
vote_account.vote_state_view().last_voted_slot()
⋮----
pub fn record_bank_vote(&mut self, bank: &Bank) -> Option<Slot> {
let block_id = bank.block_id().unwrap_or_else(|| {
⋮----
self.record_bank_vote_and_update_lockouts(
bank.slot(),
bank.hash(),
⋮----
.is_active(&agave_feature_set::enable_tower_sync_ix::id()),
⋮----
pub(crate) fn update_last_vote_from_vote_state(
⋮----
self.vote_state.votes.clone(),
⋮----
new_vote.set_timestamp(self.maybe_timestamp(self.last_voted_slot().unwrap_or_default()));
⋮----
fn record_bank_vote_and_update_lockouts(
⋮----
if let Some(last_voted_slot) = self.vote_state.last_voted_slot() {
⋮----
panic!(
⋮----
trace!("{} record_vote for {}", self.node_pubkey, vote_slot);
let old_root = self.root();
self.vote_state.process_next_vote_slot(vote_slot);
self.update_last_vote_from_vote_state(vote_hash, enable_tower_sync_ix, block_id);
let new_root = self.root();
⋮----
Some(new_root)
⋮----
pub fn record_vote(&mut self, slot: Slot, hash: Hash) -> Option<Slot> {
self.record_bank_vote_and_update_lockouts(slot, hash, true, Hash::default())
⋮----
pub fn increase_lockout(&mut self, confirmation_count_increase: u32) {
for vote in self.vote_state.votes.iter_mut() {
vote.increase_confirmation_count(confirmation_count_increase);
⋮----
pub fn last_voted_slot(&self) -> Option<Slot> {
if self.last_vote.is_empty() {
⋮----
Some(self.last_vote.slot(self.last_vote.len() - 1))
⋮----
pub fn last_voted_slot_hash(&self) -> Option<(Slot, Hash)> {
Some((self.last_voted_slot()?, self.last_vote.hash()))
⋮----
pub fn stray_restored_slot(&self) -> Option<Slot> {
⋮----
pub fn last_vote(&self) -> VoteTransaction {
self.last_vote.clone()
⋮----
fn maybe_timestamp(&mut self, current_slot: Slot) -> Option<UnixTimestamp> {
⋮----
let timestamp = Utc::now().timestamp();
⋮----
return Some(timestamp);
⋮----
pub fn root(&self) -> Slot {
self.vote_state.root_slot.unwrap()
⋮----
pub fn is_recent(&self, slot: Slot) -> bool {
⋮----
pub fn has_voted(&self, slot: Slot) -> bool {
⋮----
if slot == vote.slot() {
⋮----
pub fn is_locked_out(&self, slot: Slot, ancestors: &HashSet<Slot>) -> bool {
if !self.is_recent(slot) {
⋮----
let mut vote_state = self.vote_state.clone();
vote_state.process_next_vote_slot(slot);
⋮----
if slot != vote.slot() && !ancestors.contains(&vote.slot()) {
⋮----
assert!(
⋮----
fn is_valid_switching_proof_vote(
⋮----
trace!(
⋮----
return Some(false);
⋮----
if last_vote_ancestors.is_empty() {
assert!(self.is_stray_last_vote());
return Some(true);
⋮----
.and_then(|ancestor| Self::is_descendant_slot(switch_slot, ancestor, ancestors))
⋮----
fn is_descendant_slot(
⋮----
.get(&maybe_descendant)
.map(|candidate_slot_ancestors| candidate_slot_ancestors.contains(&slot))
⋮----
fn greatest_common_ancestor(
⋮----
(ancestors.get(&slot_a)?)
.intersection(ancestors.get(&slot_b)?)
⋮----
fn make_check_switch_threshold_decision(
⋮----
let Some((last_voted_slot, last_voted_hash)) = self.last_voted_slot_hash() else {
⋮----
let root = self.root();
⋮----
if self.is_first_switch_check() && switch_slot < last_voted_slot {
let message = format!(
⋮----
warn!("{message}");
datapoint_warn!("tower_warn", ("warn", message, String));
⋮----
.get_hash(switch_slot)
.expect("Slot we're trying to switch to must exist AND be frozen in progress map");
⋮----
.latest_invalid_ancestor(&(last_voted_slot, last_voted_hash))
⋮----
if heaviest_subtree_fork_choice.is_strict_ancestor(
⋮----
return rollback_due_to_duplicate_ancestor(latest_duplicate_ancestor);
⋮----
.get_hash(last_voted_slot)
.map(|current_slot_hash| current_slot_hash != last_voted_hash)
.unwrap_or(true)
⋮----
info!(
⋮----
let last_vote_ancestors = ancestors.get(&last_voted_slot).unwrap_or_else(|| {
if self.is_stray_last_vote() {
empty_ancestors_due_to_minor_unsynced_ledger()
⋮----
panic!("no ancestors found with slot: {last_voted_slot}");
⋮----
let switch_slot_ancestors = ancestors.get(&switch_slot).unwrap();
if switch_slot == last_voted_slot || switch_slot_ancestors.contains(&last_voted_slot) {
⋮----
if last_vote_ancestors.contains(&switch_slot) {
⋮----
return suspended_decision_due_to_major_unsynced_ledger();
⋮----
for (candidate_slot, descendants) in descendants.iter() {
⋮----
.get_fork_stats(*candidate_slot)
.map(|stats| stats.computed)
⋮----
descendants.iter().any(|d| {
⋮----
.get_fork_stats(*d)
⋮----
.is_valid_switching_proof_vote(
⋮----
assert!(!last_vote_ancestors.contains(candidate_slot));
⋮----
.get(candidate_slot)
.unwrap()
⋮----
.filter(|interval| interval.end >= last_voted_slot)
⋮----
if locked_out_vote_accounts.contains(vote_account_pubkey) {
⋮----
if !last_vote_ancestors.contains(lockout_interval_start) && {
⋮----
.get(vote_account_pubkey)
.map(|(stake, _)| *stake)
⋮----
locked_out_vote_accounts.insert(vote_account_pubkey);
⋮----
) in latest_validator_votes_for_frozen_banks.max_gossip_frozen_votes()
⋮----
if locked_out_vote_accounts.contains(&vote_account_pubkey) {
⋮----
self.is_valid_switching_proof_vote(
⋮----
pub(crate) fn check_switch_threshold(
⋮----
let decision = self.make_check_switch_threshold_decision(
⋮----
let new_check = Some((switch_slot, decision.clone()));
⋮----
trace!("new switch threshold check: slot {switch_slot}: {decision:?}",);
⋮----
fn is_first_switch_check(&self) -> bool {
self.last_switch_threshold_check.is_none()
⋮----
fn optimistically_bypass_vote_stake_threshold_check<'a>(
⋮----
if old_vote.slot() == threshold_vote.slot()
&& old_vote.confirmation_count() == threshold_vote.confirmation_count()
⋮----
fn check_vote_stake_threshold<'a>(
⋮----
let Some(fork_stake) = voted_stakes.get(&threshold_vote.slot()) else {
⋮----
pub fn check_vote_stake_thresholds(
⋮----
let mut threshold_decisions = vec![];
⋮----
let vote_thresholds_and_depths = vec![
⋮----
vote_state.nth_recent_lockout(threshold_depth),
self.vote_state.votes.iter(),
⋮----
threshold_decisions.push(ThresholdDecision::FailedThreshold(vote_depth, stake));
⋮----
pub(crate) fn populate_ancestor_voted_stakes(
⋮----
if let Some(slot_ancestors) = ancestors.get(&vote_slot) {
voted_stakes.entry(vote_slot).or_default();
⋮----
voted_stakes.entry(*slot).or_default();
⋮----
fn update_ancestor_voted_stakes(
⋮----
if let Some(vote_slot_ancestors) = ancestors.get(&voted_slot) {
*voted_stakes.entry(voted_slot).or_default() += voted_stake;
⋮----
*voted_stakes.entry(*slot).or_default() += voted_stake;
⋮----
fn voted_slots(&self) -> Vec<Slot> {
⋮----
.map(|lockout| lockout.slot())
.collect()
⋮----
pub fn is_stray_last_vote(&self) -> bool {
self.stray_restored_slot.is_some() && self.stray_restored_slot == self.last_voted_slot()
⋮----
pub fn adjust_lockouts_after_replay(
⋮----
let tower_root = self.root();
⋮----
assert_eq!(slot_history.check(replayed_root), Check::Found);
⋮----
if let Some(last_voted_slot) = self.last_voted_slot() {
⋮----
if slot_history.check(last_voted_slot) == Check::TooOld {
return Err(TowerError::TooOldTower(
⋮----
slot_history.oldest(),
⋮----
self.adjust_lockouts_with_slot_history(slot_history)?;
self.initialize_root(replayed_root);
⋮----
error!("{message}");
datapoint_error!("tower_error", ("error", message, String));
let mut warped_slot_history = (*slot_history).clone();
warped_slot_history.add(tower_root);
self.adjust_lockouts_with_slot_history(&warped_slot_history)?;
⋮----
assert_eq!(tower_root, replayed_root);
⋮----
Ok(self)
⋮----
fn adjust_lockouts_with_slot_history(&mut self, slot_history: &SlotHistory) -> Result<()> {
⋮----
Vec::with_capacity(self.vote_state.votes.len());
⋮----
let mut slots_in_tower = vec![tower_root];
slots_in_tower.extend(self.voted_slots());
for slot_in_tower in slots_in_tower.iter().rev() {
let check = slot_history.check(*slot_in_tower);
if anchored_slot.is_none() && check == Check::Found {
anchored_slot = Some(*slot_in_tower);
} else if anchored_slot.is_some() && check == Check::NotFound {
return Err(TowerError::FatallyInconsistent("diverged ancestor?"));
⋮----
return Err(TowerError::FatallyInconsistent("time warped?"));
⋮----
return Err(TowerError::FatallyInconsistent(
⋮----
checked_slot = Some(*slot_in_tower);
retain_flags_for_each_vote_in_reverse.push(anchored_slot.is_none());
⋮----
info!("adjusted tower's anchored slot: {anchored_slot:?}");
if anchored_slot.is_none() {
⋮----
retain_flags_for_each_vote_in_reverse.pop();
⋮----
retain_flags_for_each_vote_in_reverse.into_iter().rev();
let original_votes_len = self.vote_state.votes.len();
self.initialize_lockouts(move |_| retain_flags_for_each_vote.next().unwrap());
if self.vote_state.votes.is_empty() {
info!("All restored votes were behind; resetting root_slot and last_vote in tower!");
⋮----
assert_eq!(self.last_voted_slot(), self.voted_slots().last().copied());
self.stray_restored_slot = self.last_vote.last_voted_slot()
⋮----
Ok(())
⋮----
fn initialize_lockouts_from_bank(
⋮----
if let Some(vote_account) = bank.get_vote_account(vote_account_pubkey) {
self.vote_state = TowerVoteState::from(vote_account.vote_state_view());
self.initialize_root(root);
self.initialize_lockouts(|v| v.slot() > root);
⋮----
fn initialize_lockouts<F: FnMut(&Lockout) -> bool>(&mut self, should_retain: F) {
self.vote_state.votes.retain(should_retain);
⋮----
fn initialize_root(&mut self, root: Slot) {
self.vote_state.root_slot = Some(root);
⋮----
pub fn save(&self, tower_storage: &dyn TowerStorage, node_keypair: &Keypair) -> Result<()> {
⋮----
tower_storage.store(&SavedTowerVersions::from(saved_tower))?;
⋮----
pub fn restore(tower_storage: &dyn TowerStorage, node_pubkey: &Pubkey) -> Result<Self> {
tower_storage.load(node_pubkey)
⋮----
pub enum TowerError {
⋮----
impl TowerError {
pub fn is_file_missing(&self) -> bool {
⋮----
io_err.kind() == std::io::ErrorKind::NotFound
⋮----
pub fn is_too_old(&self) -> bool {
matches!(self, TowerError::TooOldTower(_, _))
⋮----
pub enum ExternalRootSource {
⋮----
impl ExternalRootSource {
fn root(&self) -> Slot {
⋮----
// Given an untimely crash, tower may have roots that are not reflected in blockstore,
// or the reverse of this.
// That's because we don't impose any ordering guarantee or any kind of write barriers
pub fn reconcile_blockstore_roots_with_external_source(
⋮----
let external_root = external_source.root();
⋮----
.take_while(|current| match current.cmp(last_blockstore_root) {
⋮----
Ordering::Less => panic!(
⋮----
if !new_roots.is_empty() {
⋮----
blockstore.mark_slots_as_if_rooted_normally_at_startup(
new_roots.into_iter().map(|root| (root, None)).collect(),
⋮----
*last_blockstore_root = blockstore.max_root();
⋮----
pub mod test {
⋮----
fn gen_stakes(stake_votes: &[(u64, &[u64])]) -> VoteAccountsHashMap {
⋮----
.map(|(lamports, votes)| {
⋮----
data: vec![0; VoteStateV4::size_of()],
⋮----
process_slot_vote_unchecked(&mut vote_state, *slot);
⋮----
account.data_as_mut_slice(),
⋮----
.expect("serialize state");
⋮----
(*lamports, VoteAccount::try_from(account).unwrap()),
⋮----
fn test_to_vote_instruction() {
⋮----
assert!(decision
⋮----
fn test_simple_votes() {
⋮----
let forks = tr(0) / (tr(1) / (tr(2) / (tr(3) / (tr(4) / tr(5)))));
⋮----
let votes = vec![1, 2, 3, 4, 5];
cluster_votes.insert(node_pubkey, votes.clone());
vote_simulator.fill_bank_forks(forks, &cluster_votes, true);
⋮----
assert!(vote_simulator
⋮----
assert_eq!(tower.vote_state.votes[i - 1].slot() as usize, i);
⋮----
fn test_switch_threshold_duplicate_rollback() {
run_test_switch_threshold_duplicate_rollback(false);
⋮----
fn test_switch_threshold_duplicate_rollback_panic() {
run_test_switch_threshold_duplicate_rollback(true);
⋮----
fn setup_switch_test(num_accounts: usize) -> (Arc<Bank>, VoteSimulator, u64) {
assert!(num_accounts > 1);
⋮----
let bank0 = vote_simulator.bank_forks.read().unwrap().get(0).unwrap();
let total_stake = bank0.total_epoch_stake();
⋮----
let forks = tr(0)
/ (tr(1)
/ (tr(2)
/ (tr(10) / (tr(11) / (tr(12) / (tr(13) / (tr(14))))))
/ (tr(43)
/ (tr(44)
/ (tr(45) / (tr(46) / (tr(47) / (tr(48) / (tr(49) / (tr(50)))))))
/ (tr(110)))
/ tr(112))));
vote_simulator.fill_bank_forks(forks, &HashMap::new(), true);
for (_, fork_progress) in vote_simulator.progress.iter_mut() {
⋮----
fn run_test_switch_threshold_duplicate_rollback(should_panic: bool) {
let (bank0, mut vote_simulator, total_stake) = setup_switch_test(2);
let ancestors = vote_simulator.bank_forks.read().unwrap().ancestors();
let descendants = vote_simulator.bank_forks.read().unwrap().descendants();
⋮----
tower.record_vote(
⋮----
.read()
⋮----
.get(47)
⋮----
.hash(),
⋮----
.mark_fork_invalid_candidate(&(
⋮----
.get(duplicate_ancestor1)
⋮----
.get(duplicate_ancestor2)
⋮----
let mut confirm_ancestors = vec![duplicate_ancestor1];
⋮----
confirm_ancestors.push(duplicate_ancestor2);
⋮----
for (i, duplicate_ancestor) in confirm_ancestors.into_iter().enumerate() {
⋮----
.mark_fork_valid_candidate(&(
⋮----
.get(duplicate_ancestor)
⋮----
let res = tower.check_switch_threshold(
⋮----
bank0.epoch_vote_accounts(0).unwrap(),
⋮----
fn test_switch_threshold() {
⋮----
let mut descendants = vote_simulator.bank_forks.read().unwrap().descendants();
⋮----
tower.record_vote(47, Hash::default());
⋮----
vote_simulator.simulate_lockout_interval(50, (49, 100), &other_vote_account);
⋮----
vote_simulator.simulate_lockout_interval(50, (45, 100), &other_vote_account);
⋮----
vote_simulator.simulate_lockout_interval(14, (12, 46), &other_vote_account);
⋮----
vote_simulator.simulate_lockout_interval(13, (12, 47), &other_vote_account);
⋮----
vote_simulator.simulate_lockout_interval(14, (12, 47), &other_vote_account);
⋮----
descendants.get_mut(&14).unwrap().insert(10000);
⋮----
tower.vote_state.root_slot = Some(43);
⋮----
fn test_switch_threshold_use_gossip_votes() {
⋮----
.check_add_vote(
⋮----
Some(
⋮----
.get(112)
⋮----
vote_simulator.set_root(44);
⋮----
fn test_switch_threshold_votes() {
⋮----
/ (tr(45) / (tr(46))))
/ (tr(110)))));
⋮----
cluster_votes.insert(vote_simulator.node_pubkeys[1], vec![46]);
cluster_votes.insert(vote_simulator.node_pubkeys[2], vec![47]);
⋮----
let votes_to_simulate = (46..=48).collect();
let results = vote_simulator.create_and_vote_new_branch(
⋮----
assert!(results.get(&48).unwrap().is_empty());
⋮----
fn test_double_partition() {
⋮----
/ (tr(3)
/ (tr(4)
/ (tr(5)
/ (tr(6)
/ (tr(7)
/ (tr(8)
/ (tr(9)
⋮----
/ (tr(110) / (tr(110 + 2 * num_slots_to_try))))))))))))));
⋮----
let mut my_votes: Vec<Slot> = vec![];
⋮----
my_votes.extend(1..=14);
my_votes.extend(43..=44);
my_votes.extend(45..=50);
my_votes.push(next_unlocked_slot);
cluster_votes.insert(node_pubkey, my_votes.clone());
let other_votes = my_votes.clone();
cluster_votes.insert(vote_simulator.node_pubkeys[1], other_votes);
⋮----
info!("local tower: {:#?}", tower.vote_state.votes);
⋮----
.get(next_unlocked_slot)
⋮----
.get_vote_account(&vote_pubkey)
.unwrap();
let state = observed.vote_state_view();
info!("observed tower: {:#?}", state.votes_iter().collect_vec());
⋮----
.get_mut(&vote_simulator.node_pubkeys[1])
⋮----
.extend(next_unlocked_slot + 1..next_unlocked_slot + num_slots_to_try);
assert!(vote_simulator.can_progress_on_fork(
⋮----
fn test_collect_vote_lockouts_sums() {
let accounts = gen_stakes(&[(1, &[0]), (1, &[0])]);
⋮----
.sorted_by_key(|(pk, _)| *pk)
.map(|(pubkey, _)| (*pubkey, (0, Hash::default())))
⋮----
let ancestors = vec![(1, vec![0].into_iter().collect()), (0, HashSet::new())]
.into_iter()
⋮----
|_| Some(Hash::default()),
⋮----
assert_eq!(voted_stakes[&0], 2);
assert_eq!(total_stake, 2);
let mut new_votes = latest_validator_votes_for_frozen_banks.take_votes_dirty_set(0);
new_votes.sort();
assert_eq!(new_votes, account_latest_votes);
⋮----
fn test_collect_vote_lockouts_root() {
let votes: Vec<u64> = (0..MAX_LOCKOUT_HISTORY as u64).collect();
let accounts = gen_stakes(&[(1, &votes), (1, &votes)]);
⋮----
.map(|(pubkey, _)| {
⋮----
tower.record_vote(i as u64, Hash::default());
ancestors.insert(i as u64, (0..i as u64).collect());
⋮----
assert_eq!(tower.vote_state.root_slot, Some(0));
⋮----
assert_eq!(voted_stakes[&(i as u64)], 2);
⋮----
assert_eq!(fork_stake, expected_bank_stake);
assert_eq!(total_stake, expected_total_stake);
⋮----
latest_validator_votes_for_frozen_banks.take_votes_dirty_set(root.slot());
⋮----
fn test_check_vote_threshold_without_votes() {
⋮----
let stakes = vec![(0, 1)].into_iter().collect();
assert!(tower.check_vote_stake_thresholds(0, &stakes, 2).is_empty());
⋮----
fn test_check_vote_threshold_no_skip_lockout_with_new_root() {
⋮----
stakes.insert(i, 1);
tower.record_vote(i, Hash::default());
⋮----
assert!(!tower
⋮----
fn test_is_slot_confirmed_not_enough_stake_failure() {
⋮----
assert!(!tower.is_slot_confirmed(0, &stakes, 2));
⋮----
fn test_is_slot_confirmed_unknown_slot() {
⋮----
fn test_is_slot_confirmed_pass() {
⋮----
let stakes = vec![(0, 2)].into_iter().collect();
assert!(tower.is_slot_confirmed(0, &stakes, 2));
⋮----
fn test_is_slot_duplicate_confirmed_not_enough_stake_failure() {
⋮----
let stakes = vec![(0, 52)].into_iter().collect();
assert!(!tower.is_slot_duplicate_confirmed(0, &stakes, 100));
⋮----
fn test_is_slot_duplicate_confirmed_unknown_slot() {
⋮----
fn test_is_slot_duplicate_confirmed_pass() {
⋮----
let stakes = vec![(0, 53)].into_iter().collect();
assert!(tower.is_slot_duplicate_confirmed(0, &stakes, 100));
⋮----
fn test_is_locked_out_empty() {
⋮----
assert!(!tower.is_locked_out(1, &ancestors));
⋮----
fn test_is_locked_out_root_slot_child_pass() {
⋮----
let ancestors: HashSet<Slot> = vec![0].into_iter().collect();
tower.vote_state.root_slot = Some(0);
⋮----
fn test_is_locked_out_root_slot_sibling_fail() {
⋮----
tower.record_vote(1, Hash::default());
assert!(tower.is_locked_out(2, &ancestors));
⋮----
fn test_check_already_voted() {
⋮----
tower.record_vote(0, Hash::default());
assert!(tower.has_voted(0));
assert!(!tower.has_voted(1));
⋮----
fn test_check_recent_slot() {
⋮----
assert!(tower.is_recent(1));
assert!(tower.is_recent(32));
⋮----
assert!(!tower.is_recent(0));
assert!(!tower.is_recent(32));
assert!(!tower.is_recent(63));
assert!(tower.is_recent(65));
⋮----
fn test_is_locked_out_double_vote() {
⋮----
assert!(tower.is_locked_out(0, &ancestors));
⋮----
fn test_is_locked_out_child() {
⋮----
fn test_is_locked_out_sibling() {
⋮----
fn test_is_locked_out_last_vote_expired() {
⋮----
assert!(!tower.is_locked_out(4, &ancestors));
tower.record_vote(4, Hash::default());
assert_eq!(tower.vote_state.votes[0].slot(), 0);
assert_eq!(tower.vote_state.votes[0].confirmation_count(), 2);
assert_eq!(tower.vote_state.votes[1].slot(), 4);
assert_eq!(tower.vote_state.votes[1].confirmation_count(), 1);
⋮----
fn test_check_vote_threshold_below_threshold() {
⋮----
assert!(!tower.check_vote_stake_thresholds(1, &stakes, 2).is_empty());
⋮----
fn test_check_vote_threshold_above_threshold() {
⋮----
assert!(tower.check_vote_stake_thresholds(1, &stakes, 2).is_empty());
⋮----
fn test_check_vote_thresholds_above_thresholds() {
⋮----
let stakes = vec![
⋮----
tower.record_vote(slot as Slot, Hash::default());
⋮----
assert!(tower
⋮----
fn test_check_vote_threshold_deep_below_threshold() {
⋮----
let stakes = vec![(0, 6), (VOTE_THRESHOLD_DEPTH_SHALLOW as u64, 4)]
⋮----
fn test_check_vote_threshold_shallow_below_threshold() {
⋮----
let stakes = vec![(0, 7), (VOTE_THRESHOLD_DEPTH_SHALLOW as u64, 1)]
⋮----
fn test_check_vote_threshold_above_threshold_after_pop() {
⋮----
tower.record_vote(2, Hash::default());
assert!(tower.check_vote_stake_thresholds(6, &stakes, 2).is_empty());
⋮----
fn test_check_vote_threshold_above_threshold_no_stake() {
⋮----
fn test_check_vote_threshold_lockouts_not_updated() {
⋮----
let stakes = vec![(0, 1), (1, 2)].into_iter().collect();
⋮----
fn test_stake_is_updated_for_entire_branch() {
⋮----
let set: HashSet<u64> = vec![0u64, 1u64].into_iter().collect();
let ancestors: HashMap<u64, HashSet<u64>> = [(2u64, set)].iter().cloned().collect();
Tower::update_ancestor_voted_stakes(&mut voted_stakes, 2, account.lamports(), &ancestors);
assert_eq!(voted_stakes[&0], 1);
assert_eq!(voted_stakes[&1], 1);
assert_eq!(voted_stakes[&2], 1);
⋮----
fn test_check_vote_threshold_forks() {
⋮----
.map(|slot| {
let slot_parents: HashSet<_> = (0..slot).collect();
⋮----
let tower_votes: Vec<Slot> = (0..VOTE_THRESHOLD_DEPTH as u64).collect();
let accounts = gen_stakes(&[
⋮----
tower.record_vote(*vote, Hash::default());
⋮----
fn vote_and_check_recent(num_votes: usize) {
⋮----
.map(|i| {
⋮----
vec![]
⋮----
if num_votes > 0 { Some(0) } else { None },
⋮----
expected.timestamp = tower.last_vote.timestamp();
assert_eq!(VoteTransaction::from(expected), tower.last_vote)
⋮----
fn test_recent_votes_full() {
vote_and_check_recent(MAX_LOCKOUT_HISTORY)
⋮----
fn test_recent_votes_empty() {
vote_and_check_recent(0)
⋮----
fn test_recent_votes_exact() {
vote_and_check_recent(5)
⋮----
fn test_maybe_timestamp() {
⋮----
assert!(tower.maybe_timestamp(0).is_some());
assert!(tower.maybe_timestamp(1).is_some());
assert!(tower.maybe_timestamp(0).is_none());
assert!(tower.maybe_timestamp(1).is_none());
⋮----
assert!(tower.maybe_timestamp(2).is_some());
⋮----
assert!(tower.maybe_timestamp(3).is_none());
⋮----
fn test_refresh_last_vote_timestamp() {
⋮----
tower.last_vote.set_timestamp(None);
tower.refresh_last_vote_timestamp(5);
assert_eq!(tower.last_vote.timestamp(), None);
assert_eq!(tower.last_timestamp.slot, 0);
assert_eq!(tower.last_timestamp.timestamp, 0);
tower.last_vote = VoteTransaction::from(TowerSync::from(vec![(0, 3), (1, 2), (6, 1)]));
⋮----
tower.last_vote = VoteTransaction::from(TowerSync::from(vec![(0, 3), (1, 2), (2, 1)]));
⋮----
assert_eq!(tower.last_vote.timestamp(), Some(1));
assert_eq!(tower.last_timestamp.slot, 2);
assert_eq!(tower.last_timestamp.timestamp, 1);
⋮----
assert_eq!(tower.last_vote.timestamp(), Some(2));
⋮----
assert_eq!(tower.last_timestamp.timestamp, 2);
⋮----
fn run_test_load_tower_snapshot<F, G>(
⋮----
let tower_path = TempDir::new().unwrap();
⋮----
let node_pubkey = identity_keypair.pubkey();
⋮----
let tower_storage = FileTowerStorage::new(tower_path.path().to_path_buf());
modify_original(&mut tower, &node_pubkey);
tower.save(&tower_storage, &identity_keypair).unwrap();
modify_serialized(&tower_storage.filename(&node_pubkey));
⋮----
fn test_switch_threshold_across_tower_reload() {
⋮----
/ tr(10)
⋮----
/ (tr(110) / tr(111))))));
⋮----
tower.record_vote(43, Hash::default());
tower.record_vote(44, Hash::default());
tower.record_vote(45, Hash::default());
tower.record_vote(46, Hash::default());
⋮----
tower.record_vote(48, Hash::default());
tower.record_vote(49, Hash::default());
⋮----
vote_simulator.simulate_lockout_interval(111, (10, 49), &other_vote_account);
⋮----
assert_eq!(tower.voted_slots(), vec![43, 44, 45, 46, 47, 48, 49]);
⋮----
let mut tower = tower.clone();
tower.record_vote(110, Hash::default());
tower.record_vote(111, Hash::default());
assert_eq!(tower.voted_slots(), vec![43, 110, 111]);
⋮----
vote_simulator.set_root(replayed_root_slot);
⋮----
slot_history.add(*slot);
⋮----
.adjust_lockouts_after_replay(replayed_root_slot, &slot_history)
⋮----
assert_eq!(tower.voted_slots(), vec![45, 46, 47, 48, 49]);
⋮----
vote_simulator.simulate_lockout_interval(111, (45, 50), &other_vote_account);
⋮----
vote_simulator.simulate_lockout_interval(111, (110, 200), &other_vote_account);
⋮----
assert_eq!(tower.voted_slots(), vec![110, 111]);
assert_eq!(tower.vote_state.root_slot, Some(replayed_root_slot));
⋮----
fn test_load_tower_ok() {
⋮----
run_test_load_tower_snapshot(|tower, pubkey| tower.node_pubkey = *pubkey, |_| ());
let loaded = loaded.unwrap();
assert_eq!(loaded, tower);
assert_eq!(tower.threshold_depth, 10);
assert!((tower.threshold_size - 0.9_f64).abs() < f64::EPSILON);
assert_eq!(loaded.threshold_depth, 10);
assert!((loaded.threshold_size - 0.9_f64).abs() < f64::EPSILON);
⋮----
fn test_load_tower_wrong_identity() {
⋮----
assert_matches!(
⋮----
fn test_load_tower_invalid_signature() {
let (_, loaded) = run_test_load_tower_snapshot(
⋮----
.read(true)
.write(true)
.open(path)
⋮----
assert_eq!(file.seek(SeekFrom::Start(4)).unwrap(), 4);
⋮----
assert_eq!(file.read(&mut buf).unwrap(), 1);
⋮----
assert_eq!(file.write(&buf).unwrap(), 1);
⋮----
assert_matches!(loaded, Err(TowerError::InvalidSignature))
⋮----
fn test_load_tower_deser_failure() {
⋮----
.truncate(true)
⋮----
.unwrap_or_else(|_| panic!("Failed to truncate file: {path:?}"));
⋮----
assert_matches!(loaded, Err(TowerError::SerializeError(_)))
⋮----
fn test_load_tower_missing() {
⋮----
remove_file(path).unwrap();
⋮----
assert_matches!(loaded, Err(TowerError::IoError(_)))
⋮----
fn test_reconcile_blockstore_roots_with_tower_normal() {
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
let (shreds, _) = make_slot_entries(1, 0, 42);
blockstore.insert_shreds(shreds, None, false).unwrap();
let (shreds, _) = make_slot_entries(3, 1, 42);
⋮----
let (shreds, _) = make_slot_entries(4, 1, 42);
⋮----
assert!(!blockstore.is_root(0));
assert!(!blockstore.is_root(1));
assert!(!blockstore.is_root(3));
assert!(!blockstore.is_root(4));
⋮----
tower.vote_state.root_slot = Some(4);
reconcile_blockstore_roots_with_external_source(
ExternalRootSource::Tower(tower.root()),
⋮----
&mut blockstore.max_root(),
⋮----
assert!(blockstore.is_root(1));
⋮----
assert!(blockstore.is_root(4));
⋮----
fn test_reconcile_blockstore_roots_with_tower_panic_no_common_root() {
⋮----
blockstore.set_roots(std::iter::once(&3)).unwrap();
⋮----
assert!(blockstore.is_root(3));
⋮----
fn test_reconcile_blockstore_roots_with_tower_nop_no_parent() {
⋮----
assert_eq!(blockstore.max_root(), 0);
⋮----
fn test_adjust_lockouts_after_replay_future_slots() {
⋮----
tower.record_vote(3, Hash::default());
⋮----
slot_history.add(0);
slot_history.add(1);
⋮----
assert_eq!(tower.voted_slots(), vec![2, 3]);
assert_eq!(tower.root(), replayed_root_slot);
⋮----
fn test_adjust_lockouts_after_replay_not_found_slots() {
⋮----
slot_history.add(4);
⋮----
fn test_adjust_lockouts_after_replay_all_rooted_with_no_too_old() {
⋮----
slot_history.add(2);
slot_history.add(3);
⋮----
slot_history.add(5);
⋮----
assert_eq!(tower.voted_slots(), vec![] as Vec<Slot>);
⋮----
assert_eq!(tower.stray_restored_slot, None);
⋮----
fn test_adjust_lockouts_after_replay_all_rooted_with_too_old() {
use solana_slot_history::MAX_ENTRIES;
⋮----
slot_history.add(MAX_ENTRIES);
⋮----
.adjust_lockouts_after_replay(MAX_ENTRIES, &slot_history)
⋮----
assert_eq!(tower.root(), MAX_ENTRIES);
⋮----
fn test_adjust_lockouts_after_replay_anchored_future_slots() {
⋮----
assert_eq!(tower.voted_slots(), vec![3, 4]);
⋮----
fn test_adjust_lockouts_after_replay_all_not_found() {
⋮----
tower.record_vote(5, Hash::default());
tower.record_vote(6, Hash::default());
⋮----
slot_history.add(7);
⋮----
assert_eq!(tower.voted_slots(), vec![5, 6]);
⋮----
fn test_adjust_lockouts_after_replay_all_not_found_even_if_rooted() {
⋮----
let result = tower.adjust_lockouts_after_replay(replayed_root_slot, &slot_history);
⋮----
fn test_adjust_lockouts_after_replay_all_future_votes_only_root_found() {
⋮----
tower.vote_state.root_slot = Some(2);
⋮----
assert_eq!(tower.voted_slots(), vec![3, 4, 5]);
⋮----
fn test_adjust_lockouts_after_replay_empty() {
⋮----
fn test_adjust_lockouts_after_replay_too_old_tower() {
⋮----
let result = tower.adjust_lockouts_after_replay(MAX_ENTRIES, &slot_history);
⋮----
fn test_adjust_lockouts_after_replay_time_warped() {
⋮----
tower.vote_state.votes.push_back(Lockout::new(1));
tower.vote_state.votes.push_back(Lockout::new(0));
let vote = Vote::new(vec![0], Hash::default());
⋮----
let result = tower.adjust_lockouts_after_replay(0, &slot_history);
⋮----
fn test_adjust_lockouts_after_replay_diverged_ancestor() {
⋮----
tower.vote_state.votes.push_back(Lockout::new(2));
let vote = Vote::new(vec![2], Hash::default());
⋮----
let result = tower.adjust_lockouts_after_replay(2, &slot_history);
⋮----
fn test_adjust_lockouts_after_replay_out_of_order() {
⋮----
.push_back(Lockout::new(MAX_ENTRIES - 1));
⋮----
let vote = Vote::new(vec![1], Hash::default());
⋮----
fn test_adjust_lockouts_after_replay_reversed_votes() {
⋮----
.adjust_lockouts_after_replay(2, &slot_history)
⋮----
fn test_adjust_lockouts_after_replay_repeated_non_root_votes() {
⋮----
tower.vote_state.votes.push_back(Lockout::new(3));
⋮----
let vote = Vote::new(vec![3], Hash::default());
⋮----
fn test_adjust_lockouts_after_replay_vote_on_root() {
⋮----
tower.vote_state.root_slot = Some(42);
tower.vote_state.votes.push_back(Lockout::new(42));
tower.vote_state.votes.push_back(Lockout::new(43));
tower.vote_state.votes.push_back(Lockout::new(44));
let vote = Vote::new(vec![44], Hash::default());
⋮----
slot_history.add(42);
let tower = tower.adjust_lockouts_after_replay(42, &slot_history);
assert_eq!(tower.unwrap().voted_slots(), [43, 44]);
⋮----
fn test_adjust_lockouts_after_replay_vote_on_genesis() {
⋮----
assert!(tower.adjust_lockouts_after_replay(0, &slot_history).is_ok());
⋮----
fn test_adjust_lockouts_after_replay_future_tower() {
⋮----
tower.vote_state.votes.push_back(Lockout::new(13));
tower.vote_state.votes.push_back(Lockout::new(14));
let vote = Vote::new(vec![14], Hash::default());
⋮----
tower.initialize_root(12);
⋮----
assert_eq!(tower.root(), 12);
assert_eq!(tower.voted_slots(), vec![13, 14]);
assert_eq!(tower.stray_restored_slot, Some(14));
⋮----
fn test_default_tower_has_no_stray_last_vote() {
⋮----
assert!(!tower.is_stray_last_vote());
⋮----
fn test_switch_threshold_common_ancestor() {
⋮----
/ tr(51)
⋮----
/ (tr(45) / (tr(46) / (tr(47) / (tr(48) / tr(49) / tr(50)))))
/ tr(113)
/ (tr(110) / tr(111) / tr(112))))));
⋮----
vote_simulator.simulate_lockout_interval(50, (10, 49), &other_vote_account);
⋮----
vote_simulator.clear_lockout_intervals(50);
⋮----
vote_simulator.simulate_lockout_interval(candidate_slot, (10, 49), &other_vote_account);
⋮----
vote_simulator.clear_lockout_intervals(candidate_slot);
⋮----
.get(slot)
⋮----
insert_gossip_vote(&mut vote_simulator, 50);
⋮----
insert_gossip_vote(&mut vote_simulator, candidate_slot);

================
File: core/src/cost_update_service.rs
================
pub enum CostUpdate {
⋮----
pub type CostUpdateReceiver = Receiver<CostUpdate>;
pub struct CostUpdateService {
⋮----
impl CostUpdateService {
pub fn new(cost_update_receiver: CostUpdateReceiver) -> Self {
⋮----
.name("solCostUpdtSvc".to_string())
.spawn(move || {
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
fn service_loop(cost_update_receiver: CostUpdateReceiver) {
for cost_update in cost_update_receiver.iter() {
⋮----
let collector_fee_details = bank.get_collector_fee_details();
⋮----
collector_fee_details.total_transaction_fee(),
collector_fee_details.total_priority_fee(),
⋮----
let cost_tracker = bank.read_cost_tracker().unwrap();
⋮----
cost_tracker.in_flight_transaction_count();
⋮----
let slot = bank.slot();
trace!(
⋮----
cost_tracker.report_stats(

================
File: core/src/drop_bank_service.rs
================
pub struct DropBankService {
⋮----
impl DropBankService {
pub fn new(bank_receiver: Receiver<Vec<BankWithScheduler>>) -> Self {
⋮----
.name("solDropBankSrvc".to_string())
.spawn(move || {
for banks in bank_receiver.iter() {
let len = banks.len();
⋮----
drop(banks);
dropped_banks_time.stop();
if dropped_banks_time.as_ms() > 10 {
datapoint_info!(
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()

================
File: core/src/fetch_stage.rs
================
pub struct FetchStage {
⋮----
impl FetchStage {
pub fn new(
⋮----
let (sender, receiver) = unbounded();
let (vote_sender, vote_receiver) = unbounded();
let (forward_sender, forward_receiver) = unbounded();
⋮----
pub fn new_with_sender(
⋮----
let tx_sockets = sockets.into_iter().map(Arc::new).collect();
let tpu_forwards_sockets = tpu_forwards_sockets.into_iter().map(Arc::new).collect();
let tpu_vote_sockets = tpu_vote_sockets.into_iter().map(Arc::new).collect();
⋮----
fn handle_forwarded_packets(
⋮----
packet.meta_mut().flags |= PacketFlags::FORWARDED;
⋮----
let mut packet_batch = recvr.recv()?;
let mut num_packets = packet_batch.len();
packet_batch.iter_mut().for_each(mark_forwarded);
let mut packet_batches = vec![packet_batch];
while let Ok(mut packet_batch) = recvr.try_recv() {
⋮----
num_packets += packet_batch.len();
packet_batches.push(packet_batch);
⋮----
.read()
.unwrap()
.would_be_leader(HOLD_TRANSACTIONS_SLOT_OFFSET.saturating_mul(DEFAULT_TICKS_PER_SLOT))
⋮----
inc_new_counter_debug!("fetch_stage-honor_forwards", num_packets);
⋮----
if sendr.send(packet_batch).is_err() {
return Err(Error::Send);
⋮----
inc_new_counter_info!("fetch_stage-discard_forwards", num_packets);
⋮----
Ok(())
⋮----
fn new_multi_socket(
⋮----
.into_iter()
.enumerate()
.map(|(i, socket)| {
⋮----
format!("solRcvrTpu{i:02}"),
⋮----
exit.clone(),
sender.clone(),
recycler.clone(),
tpu_stats.clone(),
⋮----
in_vote_only_mode.clone(),
⋮----
.collect()
⋮----
format!("solRcvrTpuFwd{i:02}"),
⋮----
forward_sender.clone(),
⋮----
tpu_forward_stats.clone(),
⋮----
format!("solRcvrTpuVot{i:02}"),
⋮----
vote_sender.clone(),
⋮----
tpu_vote_stats.clone(),
⋮----
.collect();
let sender = sender.clone();
let poh_recorder = poh_recorder.clone();
⋮----
.name("solFetchStgFwRx".to_string())
.spawn(move || loop {
⋮----
_ => error!("{e:?}"),
⋮----
.unwrap();
⋮----
.name("solFetchStgMetr".to_string())
⋮----
sleep(Duration::from_secs(1));
tpu_stats.report();
tpu_vote_stats.report();
tpu_forward_stats.report();
if exit.load(Ordering::Relaxed) {
⋮----
vec![fwd_thread_hdl, metrics_thread_hdl],
⋮----
.flatten()
.collect(),
⋮----
pub fn join(self) -> thread::Result<()> {
⋮----
thread_hdl.join()?;

================
File: core/src/forwarding_stage.rs
================
mod packet_container;
pub enum ForwardingClientOption<'a> {
⋮----
pub(crate) struct ForwardAddressGetter {
⋮----
impl ForwardAddressGetter {
pub fn new(cluster_info: Arc<ClusterInfo>, poh_recorder: Arc<RwLock<PohRecorder>>) -> Self {
⋮----
fn get_non_vote_forwarding_addresses(
⋮----
next_leaders(&self.cluster_info, &self.poh_recorder, max_count, |node| {
node.tpu_forwards(protocol)
⋮----
fn get_vote_forwarding_addresses(&self, max_count: u64) -> Vec<SocketAddr> {
⋮----
node.tpu_vote(Protocol::UDP)
⋮----
pub(crate) struct SpawnForwardingStageResult {
⋮----
pub(crate) fn spawn_forwarding_stage(
⋮----
let vote_client = VoteClient::new(vote_client_udp_socket, forward_address_getter.clone());
⋮----
ConnectionCacheClient::new(connection_cache.clone(), forward_address_getter);
⋮----
.name("solFwdStage".to_string())
.spawn(move || forwarding_stage.run())
.unwrap(),
⋮----
// Create TPU clients for each socket provided.
// Number of clients is same as number of bind IP addresses.
⋮----
.into_vec()
.into_iter()
.map(|socket| {
⋮----
runtime_handle.clone(),
forward_address_getter.clone(),
Some(stake_identity),
⋮----
cancel.clone(),
⋮----
.collect();
⋮----
non_vote_clients.clone(),
⋮----
Some(node_multihoming.bind_ip_addrs.clone()),
⋮----
client_updater: Arc::new(UpdateHandles(non_vote_clients))
⋮----
/// Local struct to be able to update keys on all clients at once
struct UpdateHandles(Box<[TpuClientNextClient]>);
⋮----
struct UpdateHandles(Box<[TpuClientNextClient]>);
impl NotifyKeyUpdate for UpdateHandles {
fn update_key(&self, key: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
self.0.iter().try_for_each(|client| client.update_key(key))
⋮----
struct ForwardingStage<VoteClient: ForwardingClient, NonVoteClient: ForwardingClient> {
⋮----
fn new(
⋮----
/// Runs `ForwardingStage`'s main loop, to receive, order, and forward packets.
    fn run(mut self) {
⋮----
fn run(mut self) {
⋮----
let root_bank = self.sharable_banks.root();
if !self.receive_and_buffer(&root_bank) {
⋮----
self.forward_buffered_packets();
self.metrics.maybe_report();
⋮----
fn receive_and_buffer(&mut self, bank: &Bank) -> bool {
⋮----
match self.receiver.recv_timeout(TIMEOUT) {
⋮----
self.buffer_packet_batches(packet_batches, tpu_vote_batch, bank);
while now.elapsed() < TIMEOUT {
match self.receiver.try_recv() {
⋮----
self.buffer_packet_batches(packet_batches, tpu_vote_batch, bank)
⋮----
fn buffer_packet_batches(
⋮----
.is_active(&agave_feature_set::static_instruction_limit::id());
for batch in packet_batches.iter() {
⋮----
.iter()
.filter(|p| initial_packet_meta_filter(p.meta()))
⋮----
let Some(packet_data) = packet.data(..) else {
unreachable!(
⋮----
.map_err(|_| ())
.and_then(|transaction| {
⋮----
Some(packet.meta().is_simple_vote_tx()),
⋮----
.ok()
.and_then(|transaction| calculate_priority(&transaction, bank)) else {
⋮----
if self.packet_container.is_full() {
let min_priority = self.packet_container.min_priority().expect("not empty");
⋮----
let dropped_packet = self.packet_container.pop_min().expect("not empty");
⋮----
usize::from(dropped_packet.meta().is_simple_vote_tx());
⋮----
usize::from(!dropped_packet.meta().is_simple_vote_tx());
⋮----
.insert(packet.to_bytes_packet(), priority);
⋮----
fn forward_buffered_packets(&mut self) {
self.metrics.did_something |= !self.packet_container.is_empty();
self.refresh_data_budget();
⋮----
.as_ref()
.map(|binds| binds.active_index())
.unwrap_or(0);
⋮----
while let Some(packet) = self.packet_container.pop_max() {
if !self.data_budget.take(packet.meta().size) {
⋮----
usize::from(packet.meta().is_simple_vote_tx());
⋮----
usize::from(!packet.meta().is_simple_vote_tx());
⋮----
let packet_data_vec = packet.data(..).expect("packet has data").to_vec();
if packet.meta().is_simple_vote_tx() {
vote_batch.push(packet_data_vec);
send_batch_if_full(
⋮----
non_vote_batch.push(packet_data_vec);
⋮----
if !vote_batch.is_empty() {
let num_votes = vote_batch.len();
⋮----
.send_transactions_in_batch(vote_batch)
.is_err()
⋮----
if !non_vote_batch.is_empty() {
let num_non_votes = non_vote_batch.len();
⋮----
.send_transactions_in_batch(non_vote_batch)
⋮----
fn refresh_data_budget(&self) {
⋮----
self.data_budget.update(INTERVAL_MS, |bytes| {
⋮----
bytes.saturating_add(MAX_BYTES_PER_INTERVAL),
⋮----
enum ForwardingClientError {
⋮----
fn from(_err: SendPktsError) -> Self {
⋮----
fn from(_err: TransportError) -> Self {
⋮----
trait ForwardingClient: Send + Sync + 'static {
/// Sends a batch of serialized transactions to the currently configured
    /// address.
⋮----
/// address.
    fn send_transactions_in_batch(
⋮----
struct VoteClient {
⋮----
impl VoteClient {
fn new(bind_socket: UdpSocket, forward_address_getter: ForwardAddressGetter) -> Self {
⋮----
fn get_next_valid_leader(&self) -> Option<SocketAddr> {
⋮----
.get_vote_forwarding_addresses(NUM_LOOKAHEAD_LEADERS);
node_addresses.first().copied()
⋮----
impl ForwardingClient for VoteClient {
fn send_transactions_in_batch(
⋮----
let Some(current_address) = self.get_next_valid_leader() else {
return Err(ForwardingClientError::LeaderContactMissing);
⋮----
.map(|bytes| (bytes, current_address));
batch_send(&self.bind_socket, batch_with_addresses)?;
Ok(())
⋮----
struct ConnectionCacheClient {
⋮----
impl ConnectionCacheClient {
⋮----
.get_non_vote_forwarding_addresses(
⋮----
self.connection_cache.protocol(),
⋮----
impl ForwardingClient for ConnectionCacheClient {
⋮----
let conn = self.connection_cache.get_connection(&current_address);
conn.send_data_batch_async(wire_transactions)?;
⋮----
impl LeaderUpdater for ForwardAddressGetter {
fn next_leaders(&mut self, lookahead_slots: usize) -> Vec<SocketAddr> {
self.get_non_vote_forwarding_addresses(lookahead_slots as u64, Protocol::QUIC)
⋮----
async fn stop(&mut self) {}
⋮----
struct TpuClientNextClient {
⋮----
impl TpuClientNextClient {
⋮----
// For now use large channel, the more suitable size to be found later.
⋮----
let leader_updater = forward_address_getter.clone();
⋮----
// leaking handle to this task, as it will run until the cancel signal is received
runtime_handle.spawn(scheduler.get_stats().report_to_influxdb(
⋮----
let _handle = runtime_handle.spawn(scheduler.run(config));
⋮----
fn create_config(
⋮----
stake_identity: stake_identity.map(StakeIdentity::new),
// Cache size of 128 covers all nodes above the P90 slot count threshold,
// which together account for ~75% of total slots in the epoch.
⋮----
// Send to the next leader only, but verify that connections exist
// for the leaders of the next `4 * NUM_CONSECUTIVE_SLOTS`.
⋮----
impl ForwardingClient for TpuClientNextClient {
⋮----
.try_send(TransactionBatch::new(wire_transactions))
.map_err(|_e| ForwardingClientError::Failed)
⋮----
impl NotifyKeyUpdate for TpuClientNextClient {
fn update_key(&self, identity: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
⋮----
.send(Some(stake_identity))
.map_err(|e| Box::new(e) as Box<dyn std::error::Error>)
⋮----
/// Calculate priority for a transaction:
///
⋮----
///
/// The priority is calculated as:
⋮----
/// The priority is calculated as:
/// P = R / (1 + C)
⋮----
/// P = R / (1 + C)
/// where P is the priority, R is the reward,
⋮----
/// where P is the priority, R is the reward,
/// and C is the cost towards block-limits.
⋮----
/// and C is the cost towards block-limits.
///
⋮----
///
/// Current minimum costs are on the order of several hundred,
⋮----
/// Current minimum costs are on the order of several hundred,
/// so the denominator is effectively C, and the +1 is simply
⋮----
/// so the denominator is effectively C, and the +1 is simply
/// to avoid any division by zero due to a bug - these costs
⋮----
/// to avoid any division by zero due to a bug - these costs
/// are estimate by the cost-model and are not direct
⋮----
/// are estimate by the cost-model and are not direct
/// from user input. They should never be zero.
⋮----
/// from user input. They should never be zero.
/// Any difference in the prioritization is negligible for
⋮----
/// Any difference in the prioritization is negligible for
/// the current transaction costs.
⋮----
/// the current transaction costs.
fn calculate_priority(
⋮----
fn calculate_priority(
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(&bank.feature_set)
.ok()?;
⋮----
// Manually estimate fee here since currently interface doesn't allow a on SVM type.
⋮----
let signature_details = transaction.signature_details();
⋮----
.total_signatures()
.saturating_mul(bank.fee_structure().lamports_per_signature);
⋮----
.calculate_reward_and_burn_fee_details(&CollectorFeeDetails::from(fee_details))
.get_deposit();
⋮----
transaction.program_instructions_iter(),
transaction.num_requested_write_locks(),
⋮----
Some(
⋮----
.saturating_mul(reward)
.wrapping_div(cost.sum().saturating_add(1)),
⋮----
fn send_batch_if_full(
⋮----
if batch.len() == FORWARD_BATCH_SIZE {
*forwarded_counter += batch.len();
⋮----
if client.send_transactions_in_batch(swap_batch).is_err() {
⋮----
struct ForwardingStageMetrics {
⋮----
impl ForwardingStageMetrics {
fn maybe_report(&mut self) {
⋮----
if self.last_reported.elapsed() > REPORTING_INTERVAL {
⋮----
datapoint_info!(
⋮----
impl Default for ForwardingStageMetrics {
fn default() -> Self {
⋮----
fn initial_packet_meta_filter(meta: &packet::Meta) -> bool {
!meta.discard() && !meta.forwarded() && meta.is_from_staked_node()
⋮----
mod tests {
⋮----
pub struct MockClient {
⋮----
impl MockClient {
pub fn new() -> Self {
⋮----
pub fn get_packets(&self) -> Vec<Vec<u8>> {
self.packets.lock().unwrap().clone()
⋮----
impl ForwardingClient for MockClient {
⋮----
self.packets.lock().unwrap().extend(wire_transactions);
⋮----
fn meta_with_flags(packet_flags: PacketFlags) -> packet::Meta {
⋮----
fn simple_transfer_with_flags(packet_flags: PacketFlags) -> Packet {
⋮----
let mut packet = Packet::from_data(None, &transaction).unwrap();
packet.meta_mut().flags = packet_flags;
⋮----
fn test_initial_packet_meta_filter() {
assert!(!initial_packet_meta_filter(&meta_with_flags(
⋮----
assert!(initial_packet_meta_filter(&meta_with_flags(
⋮----
fn test_forwarding() {
let (packet_batch_sender, packet_batch_receiver) = unbounded();
⋮----
Bank::new_with_bank_forks_for_tests(&create_genesis_config(1).genesis_config);
let sharable_banks = bank_forks.read().unwrap().sharable_banks();
⋮----
vote_mock_client.clone(),
Box::new([non_vote_mock_client.clone()]),
⋮----
BankingPacketBatch::new(vec![PacketBatch::from(RecycledPacketBatch::new(vec![
⋮----
.send((non_vote_packets.clone(), false))
.unwrap();
⋮----
.send((vote_packets.clone(), true))
⋮----
let bank = forwarding_stage.sharable_banks.root();
forwarding_stage.receive_and_buffer(&bank);
if !packet_batch_sender.is_empty() {
⋮----
forwarding_stage.forward_buffered_packets();
assert_eq!(forwarding_stage.metrics.non_votes_forwarded, 1);
assert_eq!(forwarding_stage.metrics.votes_forwarded, 1);
let vote_wired_txs = vote_mock_client.get_packets();
assert_eq!(vote_wired_txs.len(), 1);
assert_eq!(
⋮----
let non_vote_wired_txs = non_vote_mock_client.get_packets();
assert_eq!(non_vote_wired_txs.len(), 1);

================
File: core/src/gen_keys.rs
================
pub struct GenKeys {
⋮----
impl GenKeys {
pub fn new(seed: [u8; 32]) -> GenKeys {
⋮----
fn gen_seed(&mut self) -> [u8; 32] {
⋮----
self.generator.fill(&mut seed);
⋮----
fn gen_n_seeds(&mut self, n: u64) -> Vec<[u8; 32]> {
(0..n).map(|_| self.gen_seed()).collect()
⋮----
pub fn gen_keypair(&mut self) -> Keypair {
⋮----
self.generator.fill(&mut seed[..]);
keypair_from_seed(&seed).unwrap()
⋮----
pub fn gen_n_keypairs(&mut self, n: u64) -> Vec<Keypair> {
self.gen_n_seeds(n)
.into_par_iter()
.map(|seed| {
⋮----
ChaChaRng::from_seed(seed).fill(&mut keypair_seed[..]);
keypair_from_seed(&keypair_seed).unwrap()
⋮----
.collect()
⋮----
mod tests {
pub use solana_pubkey::Pubkey;
⋮----
fn test_new_key_is_deterministic() {
⋮----
assert_eq!(gen0.gen_seed().to_vec(), gen1.gen_seed().to_vec());
⋮----
fn test_gen_keypair_is_deterministic() {
⋮----
assert_eq!(
⋮----
fn gen_n_pubkeys(seed: [u8; 32], n: u64) -> HashSet<Pubkey> {
⋮----
.gen_n_keypairs(n)
.into_iter()
.map(|x| x.pubkey())
⋮----
fn test_gen_n_pubkeys_deterministic() {
⋮----
assert_eq!(gen_n_pubkeys(seed, 50), gen_n_pubkeys(seed, 50));

================
File: core/src/lib.rs
================
pub mod admin_rpc_post_init;
pub mod bam_connection;
pub mod bam_dependencies;
pub mod bam_manager;
pub mod banking_simulation;
pub mod banking_stage;
pub mod banking_trace;
⋮----
mod block_creation_loop;
pub mod bundle;
mod bundle_sigverify_stage;
pub mod bundle_stage;
pub mod cluster_info_vote_listener;
pub mod cluster_slots_service;
pub mod commitment_service;
pub mod completed_data_sets_service;
pub mod consensus;
pub mod cost_update_service;
pub mod drop_bank_service;
pub mod fetch_stage;
pub mod forwarding_stage;
pub mod gen_keys;
mod mock_alpenglow_consensus;
pub mod next_leader;
pub mod optimistic_confirmation_verifier;
pub mod packet_bundle;
pub mod proxy;
pub mod repair;
pub mod replay_stage;
pub mod resource_limits;
mod result;
pub mod sample_performance_service;
⋮----
mod scheduler_bindings_server;
mod shred_fetch_stage;
pub mod sigverify;
pub mod sigverify_stage;
pub mod snapshot_packager_service;
pub mod staked_nodes_updater_service;
pub mod stats_reporter_service;
pub mod system_monitor_service;
pub mod tip_manager;
pub mod tpu;
mod tpu_entry_notifier;
pub mod tvu;
pub mod unfrozen_gossip_verified_vote_hashes;
pub mod validator;
mod vortexor_receiver_adapter;
pub mod vote_simulator;
pub mod voting_service;
pub mod warm_quic_cache_service;
pub mod window_service;
⋮----
extern crate log;
⋮----
extern crate solana_metrics;
⋮----
extern crate solana_frozen_abi_macro;
⋮----
extern crate assert_matches;
⋮----
pub fn proto_packet_to_packet(p: jito_protos::proto::packet::Packet) -> BytesPacket {
let copy_len = min(PACKET_DATA_SIZE, p.data.len());
⋮----
packet.meta_mut().size = meta.size as usize;
packet.meta_mut().addr = meta.addr.parse().unwrap_or(UNKNOWN_IP);
packet.meta_mut().port = meta.port as u16;
⋮----
packet.meta_mut().flags.insert(PacketFlags::SIMPLE_VOTE_TX);
⋮----
packet.meta_mut().flags.insert(PacketFlags::FORWARDED);
⋮----
packet.meta_mut().flags.insert(PacketFlags::REPAIR);
⋮----
.meta_mut()
⋮----
.insert(PacketFlags::FROM_STAKED_NODE);

================
File: core/src/mock_alpenglow_consensus.rs
================
pub(crate) struct MockAlpenglowConsensus {
⋮----
struct PeerData {
⋮----
struct AgStateMachine {
⋮----
struct SharedState {
⋮----
type StateArray = [Mutex<SharedState>; NUM_VOTE_ROUNDS as usize];
impl SharedState {
fn reset(&mut self) -> HashMap<Pubkey, PeerData> {
⋮----
fn new(current_slot: Slot) -> Self {
⋮----
fn available(&self) -> bool {
⋮----
fn is_ready_for_slot(&self, slot: Slot) -> bool {
⋮----
fn get_state_for_slot_index(states: &StateArray, slot: Slot) -> &Mutex<SharedState> {
⋮----
enum VotorMessageType {
⋮----
type Error = ();
fn try_from(value: u64) -> Result<Self, Self::Error> {
⋮----
0 => Ok(Self::Notarize),
1 => Ok(Self::NotarizeCertificateAndFinalize),
2 => Ok(Self::FinalizeCertificate),
_ => Err(()),
⋮----
struct MockVotePacketHeader {
⋮----
impl MockVotePacketHeader {
fn from_bytes_mut(buf: &mut [u8]) -> &mut Self {
⋮----
fn from_bytes(buf: &[u8]) -> &Self {
⋮----
pub(crate) enum SendCommand {
⋮----
impl MockAlpenglowConsensus {
pub(crate) fn new(
⋮----
info!("Mock Alpenglow consensus is enabled");
⋮----
let (command_sender, vote_command_receiver) = bounded(4);
⋮----
let (slot_sender, slot_receiver) = bounded(1);
⋮----
let slot_receiver = slot_receiver.clone();
let command_sender = command_sender.clone();
let state = shared_state.clone();
⋮----
state: shared_state.clone(),
⋮----
let shared_state = shared_state.clone();
let should_exit = should_exit.clone();
let socket = socket.clone();
let my_id = cluster_info.id();
⋮----
let cluster_info = cluster_info.clone();
⋮----
socket.clone(),
⋮----
slot_sender: Some(slot_sender),
⋮----
fn prepare_to_receive(&mut self, slot: Slot, slot_start: Instant) -> Result<(), Slot> {
trace!(
⋮----
let staked_nodes = self.epoch_specs.current_epoch_staked_nodes();
let mut state = get_state_for_slot_index(&self.state, slot).lock().unwrap();
if !state.available() {
return Err(state.current_slot);
⋮----
for (peer, &stake) in staked_nodes.iter() {
⋮----
.lookup_contact_info(peer, |ci| ci.alpenglow())
.flatten()
⋮----
state.peers.insert(
⋮----
Ok(())
⋮----
fn listener_thread(
⋮----
.set_read_timeout(Some(Duration::from_secs(1)))
.unwrap();
trace!("Listener thread started");
let mut packets: Vec<Packet> = vec![Packet::default(); 1024];
⋮----
for p in packets.iter_mut() {
*p.meta_mut() = Meta::default();
⋮----
if should_exit.load(Ordering::Relaxed) {
⋮----
let n = match recv_mmsg(&socket, &mut packets) {
⋮----
match e.kind() {
⋮----
error!(
⋮----
for pkt in packets.iter().take(n) {
if pkt.meta().size < MOCK_VOTE_HEADER_SIZE {
trace!("Packet too small {}", pkt.meta().size);
⋮----
let sender = SocketAddr::new(pkt.meta().addr, pkt.meta().port);
let Some(pkt_buf) = pkt.data(..) else {
⋮----
trace!("Sigverify failed");
⋮----
let mut state = get_state_for_slot_index(&self_state, vote_pkt.slot_number)
.lock()
⋮----
if !state.is_ready_for_slot(vote_pkt.slot_number) {
⋮----
let elapsed = state.current_slot_start.elapsed();
⋮----
let Some(peer_info) = state.peers.get_mut(&pk) else {
⋮----
if toa.is_none() {
*toa = Some(elapsed);
⋮----
trace!("Duplicate packet");
⋮----
let _ = command_sender.try_send(
⋮----
.try_send(SendCommand::FinalizeCertificate(state.current_slot));
⋮----
fn sender_thread(
⋮----
let mut packet_buf = vec![0u8; MOCK_VOTE_PACKET_SIZE];
let id = cluster_info.id();
for command in command.iter() {
⋮----
prep_and_sign_packet(
⋮----
cluster_info.keypair().as_ref(),
⋮----
let state = get_state_for_slot_index(&state, slot).lock().unwrap();
if !state.is_ready_for_slot(slot) {
⋮----
for (peer, info) in state.peers.iter() {
send_instructions.push((&packet_buf, info.address));
⋮----
for batch in send_instructions.as_slice().chunks(MAX_PACKETS_PER_BATCH) {
let _ = batch_send(&socket, batch.iter().copied());
⋮----
fn check_conditions_to_vote(&mut self, slot: Slot, root_bank: &Bank) -> bool {
⋮----
trace!("Alpenglow voting is disabled",);
⋮----
let root_slot = root_bank.slot();
⋮----
slot.is_multiple_of(interval)
⋮----
pub(crate) fn signal_new_slot(&mut self, slot: Slot, root_bank: &Bank) {
if !self.check_conditions_to_vote(slot, root_bank) {
⋮----
if self.prepare_to_receive(s, slot_start).is_err() {
error!("Can not initiate mock voting, slot {s} was not released");
datapoint_info!("mock_alpenglow", ("runner_stuck", 2, i64), ("slot", s, i64));
⋮----
if let Some(slot_sender) = self.slot_sender.as_ref() {
if slot_sender.try_send(slot).is_err() {
error!("Can not initiate mock voting, worker is busy");
datapoint_info!(
⋮----
fn runner(
⋮----
for start_slot in slot_receiver.iter() {
⋮----
let vote_slots = slot_range.clone().map(Some).chain(once(None));
let report_slots = once(None).chain(slot_range.map(Some));
for (vote_slot, report_slot) in vote_slots.zip(report_slots) {
⋮----
trace!("Starting voting in slot {slot}");
let _ = command_sender.send(SendCommand::Notarize(slot));
⋮----
get_state_for_slot_index(&state, slot).lock().unwrap();
⋮----
let peers = state_for_slot_index.reset();
⋮----
report_collected_votes(peers, total_staked, slot);
⋮----
pub(crate) fn join(mut self) -> thread::Result<()> {
self.should_exit.store(true, Ordering::Relaxed);
drop(self.slot_sender.take());
self.listener_thread.join()?;
self.runner_thread.join()?;
self.sender_thread.join()
⋮----
fn prep_and_sign_packet(
⋮----
pkt.sender = *keypair.pubkey().as_array();
⋮----
fn report_collected_votes(peers: HashMap<Pubkey, PeerData>, total_staked: Stake, slot: Slot) {
trace!("Reporting statistics for slot {slot}");
⋮----
compute_stake_weighted_means(&peers, total_staked);
⋮----
fn compute_stake_weighted_means(
⋮----
for (_pubkey, peer_data) in peers.iter() {
⋮----
total_delay_ms[i] += rel_toa.as_millis().clamp(0, 800) * peer_data.stake as u128;
⋮----
info!(
⋮----
mod control_pubkey {
⋮----
pub(crate) struct TestConfig {
⋮----
fn get_test_config_from_account<T: DeserializeOwned>(bank: &Bank) -> Option<T> {
⋮----
.accounts()
⋮----
.load_account_with(&bank.ancestors, &control_pubkey::ID, true)?
⋮----
data.deserialize_data().ok()
⋮----
mod tests {
⋮----
fn test_record_size() {
assert_eq!(
⋮----
fn test_mock_alpenglow_statemachine() {
⋮----
let keypairs: Vec<Keypair> = (0..num_nodes).map(|_| Keypair::new()).collect();
⋮----
.iter()
.map(|kp| (kp.pubkey(), bind_to_localhost_unique().unwrap()))
.collect();
let socket = Arc::new(peers[0].1.try_clone().unwrap());
let my_id = keypairs[0].pubkey();
⋮----
scope.spawn(|| {
⋮----
shared_state.clone(),
should_exit.clone(),
⋮----
if should_exit.load(std::sync::atomic::Ordering::Relaxed) {
⋮----
sleep(test_timeout);
⋮----
should_exit.store(true, std::sync::atomic::Ordering::Relaxed);
⋮----
debug!("Slot {slot} starting");
let peers_map = make_peer_map(peers.as_slice());
mock_prep_rx(&shared_state, slot, peers_map);
⋮----
let slot_state = get_state_for_slot_index(&shared_state, slot)
⋮----
assert_eq!(slot_state.alpenglow_state.notarize_stake_collected, 0);
assert!(!slot_state.alpenglow_state.block_notarized);
assert!(!slot_state.alpenglow_state.block_finalized);
⋮----
sleep(Duration::from_millis(1));
⋮----
send_packet(
⋮----
let cmd = vote_command_receiver.recv_timeout(test_timeout).unwrap();
assert_eq!(cmd, SendCommand::NotarizeCertificateAndFinalize(slot));
⋮----
let peerdata = slot_state.peers.get(&peers[1].0).unwrap();
assert!(peerdata.relative_time_of_arrival[0].unwrap().as_millis() > 0);
assert!(peerdata.relative_time_of_arrival[0].unwrap() < test_timeout);
assert!(peerdata.relative_time_of_arrival[1].is_none());
assert!(peerdata.relative_time_of_arrival[2].is_none());
assert_eq!(slot_state.alpenglow_state.notarize_stake_collected, 6);
assert!(slot_state.alpenglow_state.block_notarized);
⋮----
assert_eq!(cmd, SendCommand::FinalizeCertificate(slot));
⋮----
assert!(peerdata.relative_time_of_arrival[1].unwrap().as_millis() > 0);
assert!(peerdata.relative_time_of_arrival[1].unwrap() < test_timeout);
⋮----
assert_eq!(slot_state.alpenglow_state.finalize_stake_collected, 6);
assert!(slot_state.alpenglow_state.block_finalized);
⋮----
compute_stake_weighted_means(&slot_state.peers, peers.len() as u64);
assert_eq!(total_voted_nodes[0], 6);
assert_eq!(total_voted_nodes[1], 6);
assert!(stake_weighted_delay[0] < stake_weighted_delay[1]);
assert_eq!(stake_weighted_delay[2], 0.0);
⋮----
assert!(slot_state.alpenglow_state.notarize_stake_collected >= 8);
⋮----
assert_eq!(slot_state.alpenglow_state.finalize_stake_collected, 1);
⋮----
assert!(
⋮----
fn send_packet(
⋮----
prep_and_sign_packet(packet_buf, slot, votor_message, &keypairs[from_peer]);
⋮----
.send_to(
⋮----
peers[0].1.local_addr().unwrap(),
⋮----
fn mock_prep_rx(state: &StateArray, slot: Slot, peer_map: HashMap<Pubkey, PeerData>) {
let mut state = get_state_for_slot_index(state, slot).lock().unwrap();
state.reset();
⋮----
state.total_staked = peer_map.len() as u64;
⋮----
fn make_peer_map(sockets: &[(Pubkey, UdpSocket)]) -> HashMap<Pubkey, PeerData> {
⋮----
result.insert(
⋮----
address: socket.local_addr().unwrap(),

================
File: core/src/next_leader.rs
================
pub(crate) fn upcoming_leader_tpu_vote_sockets(
⋮----
let poh_recorder = poh_recorder.read().unwrap();
⋮----
.filter_map(|n_slots| poh_recorder.leader_after_n_slots(n_slots))
.collect_vec()
⋮----
.into_iter()
.dedup()
.filter_map(|leader_pubkey| {
cluster_info.lookup_contact_info(&leader_pubkey, |node| node.tpu_vote(protocol))?
⋮----
.collect()
⋮----
pub(crate) fn next_leaders(
⋮----
let recorder = poh_recorder.read().unwrap();
⋮----
.filter_map(|i| {
recorder.leader_after_n_slots(
⋮----
.collect();
drop(recorder);
⋮----
.iter()
⋮----
cluster_info.lookup_contact_info(leader_pubkey, &port_selector)?

================
File: core/src/optimistic_confirmation_verifier.rs
================
pub struct OptimisticConfirmationVerifier {
⋮----
impl OptimisticConfirmationVerifier {
pub fn new(snapshot_start_slot: Slot) -> Self {
⋮----
pub fn verify_for_unrooted_optimistic_slots(
⋮----
let root = root_bank.slot();
⋮----
.split_off(&((root + 1), Hash::default()));
⋮----
.into_iter()
.filter(|(optimistic_slot, optimistic_hash)| {
(*optimistic_slot == root && *optimistic_hash != root_bank.hash())
|| (!root_ancestors.contains_key(optimistic_slot) &&
!blockstore.is_root(*optimistic_slot))
⋮----
.collect()
⋮----
pub fn add_new_optimistic_confirmed_slots(
⋮----
if new_optimistic_slots.is_empty() {
⋮----
if let Err(e) = blockstore.insert_optimistic_slot(
⋮----
timestamp().try_into().unwrap(),
⋮----
error!(
⋮----
datapoint_info!("optimistic_slot", ("slot", new_optimistic_slot, i64),);
self.unchecked_slots.insert((new_optimistic_slot, hash));
⋮----
pub fn format_optimistic_confirmed_slot_violation_log(slot: Slot) -> String {
format!("Optimistically confirmed slot {slot} was not rooted")
⋮----
pub fn log_unrooted_optimistic_slots(
⋮----
for (optimistic_slot, hash) in unrooted_optimistic_slots.iter() {
let epoch = root_bank.epoch_schedule().get_epoch(*optimistic_slot);
let epoch_stakes = root_bank.epoch_stakes(epoch);
let total_epoch_stake = epoch_stakes.map(|e| e.total_stake()).unwrap_or(0);
⋮----
let slot_tracker = vote_tracker.get_slot_vote_tracker(*optimistic_slot);
let r_slot_tracker = slot_tracker.as_ref().map(|s| s.read().unwrap());
⋮----
.as_ref()
.and_then(|s| s.optimistic_votes_tracker(hash))
.map(|s| s.stake())
.unwrap_or(0);
⋮----
datapoint_warn!(
⋮----
mod test {
⋮----
fn test_add_new_optimistic_confirmed_slots() {
⋮----
let blockstore_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(blockstore_path.path()).unwrap();
optimistic_confirmation_verifier.add_new_optimistic_confirmed_slots(
vec![(snapshot_start_slot - 1, bank_hash)],
⋮----
assert_eq!(blockstore.get_latest_optimistic_slots(10).unwrap().len(), 0);
⋮----
vec![(snapshot_start_slot, bank_hash)],
⋮----
vec![(snapshot_start_slot + 1, bank_hash)],
⋮----
assert_eq!(blockstore.get_latest_optimistic_slots(10).unwrap().len(), 1);
assert_eq!(optimistic_confirmation_verifier.unchecked_slots.len(), 1);
assert!(optimistic_confirmation_verifier
⋮----
fn test_get_unrooted_optimistic_slots_same_slot_different_hash() {
⋮----
let optimistic_slots = vec![(1, bad_bank_hash), (3, Hash::default())];
⋮----
.add_new_optimistic_confirmed_slots(optimistic_slots, &blockstore);
assert_eq!(blockstore.get_latest_optimistic_slots(10).unwrap().len(), 2);
let vote_simulator = setup_forks();
let bank1 = vote_simulator.bank_forks.read().unwrap().get(1).unwrap();
assert_eq!(
⋮----
fn test_get_unrooted_optimistic_slots() {
⋮----
let mut vote_simulator = setup_forks();
let optimistic_slots: Vec<_> = vec![1, 3, 5]
⋮----
.map(|s| {
⋮----
.read()
.unwrap()
.get(s)
⋮----
.hash(),
⋮----
.collect();
⋮----
.add_new_optimistic_confirmed_slots(optimistic_slots.clone(), &blockstore);
assert_eq!(blockstore.get_latest_optimistic_slots(10).unwrap().len(), 3);
let bank5 = vote_simulator.bank_forks.read().unwrap().get(5).unwrap();
⋮----
assert!(optimistic_confirmation_verifier.unchecked_slots.is_empty());
⋮----
let bank3 = vote_simulator.bank_forks.read().unwrap().get(3).unwrap();
⋮----
let bank4 = vote_simulator.bank_forks.read().unwrap().get(4).unwrap();
⋮----
vote_simulator.set_root(5);
let bank6 = vote_simulator.bank_forks.read().unwrap().get(6).unwrap();
⋮----
.write()
⋮----
.insert(Bank::new_from_parent(bank6, &Pubkey::default(), 7));
let bank7 = vote_simulator.bank_forks.read().unwrap().get(7).unwrap();
assert!(!bank7.ancestors.contains_key(&3));
⋮----
blockstore.set_roots([1, 3].iter()).unwrap();
⋮----
fn setup_forks() -> VoteSimulator {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3) / (tr(5) / (tr(6)))));
⋮----
vote_simulator.fill_bank_forks(forks, &HashMap::new(), true);

================
File: core/src/packet_bundle.rs
================
use solana_perf::packet::PacketBatch;
⋮----
pub struct PacketBundle {
⋮----
impl PacketBundle {
pub fn new(batch: PacketBatch, bundle_id: String) -> Self {
⋮----
pub fn batch(&self) -> &PacketBatch {
⋮----
pub fn take(self) -> PacketBatch {
⋮----
pub struct VerifiedPacketBundle {
⋮----
impl VerifiedPacketBundle {
pub fn new(batch: PacketBatch) -> Self {

================
File: core/src/replay_stage.rs
================
pub enum HeaviestForkFailures {
⋮----
enum ForkReplayMode {
⋮----
enum GenerateVoteTxResult {
⋮----
impl GenerateVoteTxResult {
fn is_non_voting(&self) -> bool {
matches!(self, Self::NonVoting)
⋮----
fn is_hot_spare(&self) -> bool {
matches!(self, Self::HotSpare)
⋮----
pub(crate) struct Finalizer {
⋮----
impl Finalizer {
pub(crate) fn new(exit_sender: Arc<AtomicBool>) -> Self {
⋮----
impl Drop for Finalizer {
fn drop(&mut self) {
self.exit_sender.clone().store(true, Ordering::Relaxed);
⋮----
struct ReplaySlotFromBlockstore {
⋮----
struct LastVoteRefreshTime {
⋮----
pub struct TrackedVoteTransaction {
⋮----
struct SkippedSlotsInfo {
⋮----
pub struct TowerBFTStructures {
⋮----
struct PartitionInfo {
⋮----
impl PartitionInfo {
fn new() -> Self {
⋮----
fn update(
⋮----
if self.partition_start_time.is_none() && partition_detected {
warn!(
⋮----
datapoint_info!(
⋮----
self.partition_start_time = Some(Instant::now());
} else if self.partition_start_time.is_some() && !partition_detected {
⋮----
pub struct ReplayStageConfig {
⋮----
pub struct ReplaySenders {
⋮----
pub struct ReplayReceivers {
⋮----
struct ReplayLoopTiming {
⋮----
impl ReplayLoopTiming {
⋮----
fn update_non_alpenglow(
⋮----
fn update_common(
⋮----
self.maybe_submit();
⋮----
fn maybe_submit(&mut self) {
let now = timestamp();
⋮----
pub struct ReplayStage {
⋮----
impl ReplayStage {
pub fn new(
⋮----
.read()
.unwrap()
.root_bank()
⋮----
.activated_slot(&agave_feature_set::alpenglow::id());
⋮----
assert!(bank_forks.read().unwrap().highest_slot() >= first_alpenglow_slot);
info!("alpenglow active on startup");
⋮----
trace!("replay stage");
⋮----
exit.clone(),
block_commitment_cache.clone(),
rpc_subscriptions.clone(),
⋮----
let _exit = Finalizer::new(exit.clone());
let mut identity_keypair = cluster_info.keypair();
let mut my_pubkey = identity_keypair.pubkey();
⋮----
tower_storage.as_ref(),
⋮----
error!(
⋮----
warn!("Identity changed during startup from {my_old_pubkey} to {my_pubkey}");
⋮----
let r_bank_forks = bank_forks.read().unwrap();
⋮----
r_bank_forks.working_bank(),
r_bank_forks.get_vote_only_mode_signal(),
⋮----
let replay_mode = if replay_forks_threads.get() == 1 {
⋮----
.num_threads(replay_forks_threads.get())
.thread_name(|i| format!("solReplayFork{i:02}"))
.build()
.expect("new rayon threadpool");
⋮----
.num_threads(replay_transactions_threads.get())
.thread_name(|i| format!("solReplayTx{i:02}"))
⋮----
let poh_shared_leader_state = poh_recorder.read().unwrap().shared_leader_state();
⋮----
while poh_controller.has_pending_message() && !exit.load(Ordering::Relaxed) {}
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
rpc_subscriptions.as_deref(),
⋮----
generate_new_bank_forks_time.stop();
let tpu_has_bank = poh_shared_leader_state.load().working_bank().is_some()
|| poh_controller.has_pending_message();
⋮----
(r_bank_forks.ancestors(), r_bank_forks.descendants())
⋮----
transaction_status_sender.as_ref(),
entry_notification_sender.as_ref(),
⋮----
block_metadata_notifier.clone(),
⋮----
(!is_alpenglow_migration_complete).then_some(&mut tbft_structs),
⋮----
replay_active_banks_time.stop();
let forks_root = bank_forks.read().unwrap().root();
⋮----
process_ancestor_hashes_duplicate_slots_time.stop();
⋮----
process_duplicate_confirmed_slots_time.stop();
⋮----
for _ in gossip_verified_vote_hash_receiver.try_iter() {}
process_unfrozen_gossip_verified_vote_hashes_time.stop();
⋮----
process_popular_pruned_forks_time.stop();
⋮----
process_duplicate_slots_time.stop();
⋮----
.frozen_banks()
.filter(|(slot, _bank)| *slot >= forks_root)
.map(|(_slot, bank)| bank)
.collect();
collect_frozen_banks_time.stop();
⋮----
compute_bank_stats_time.stop();
⋮----
let fork_stats = progress.get_fork_stats(slot).unwrap();
⋮----
compute_slot_stats_time.stop();
⋮----
.select_forks(&frozen_banks, &tower, &progress, &ancestors, &bank_forks);
select_forks_time.stop();
⋮----
heaviest_bank.slot(),
⋮----
} = select_vote_and_reset_forks(
⋮----
heaviest_bank_on_same_voted_fork.as_ref(),
⋮----
select_vote_and_reset_forks_time.stop();
if vote_bank.is_none() {
⋮----
&authorized_voter_keypairs.read().unwrap(),
⋮----
if tower.is_recent(heaviest_bank.slot()) && !heaviest_fork_failures.is_empty() {
⋮----
heaviest_fork_failures_time.stop();
⋮----
leader_schedule_cache.slot_leader_at(vote_bank.slot(), Some(vote_bank))
⋮----
vote_bank.slot(),
⋮----
snapshot_controller.as_deref(),
⋮----
voting_time.stop();
⋮----
if last_reset == reset_bank.last_blockhash() {
⋮----
reset_bank.slot(),
⋮----
.write()
⋮----
.update_start_bank_active_descendants(
⋮----
info!(
⋮----
.get(&reset_bank.slot())
.expect("bank to reset to must exist in progress map");
⋮----
if my_pubkey != cluster_info.id() {
identity_keypair = cluster_info.keypair();
⋮----
my_pubkey = identity_keypair.pubkey();
⋮----
warn!("Identity changed from {my_old_pubkey} to {my_pubkey}");
⋮----
if !poh_controller.has_pending_message() {
⋮----
reset_bank.clone(),
⋮----
last_reset = reset_bank.last_blockhash();
last_reset_bank_descendants = vec![];
⋮----
if let Some(last_voted_slot) = tower.last_voted_slot() {
partition_info.update(
⋮----
reset_bank_time.stop();
⋮----
.load()
.working_bank()
.map(Arc::clone);
⋮----
poh_bank.map(|bank| bank.slot()),
⋮----
dump_then_repair_correct_slots_time.stop();
⋮----
retransmit_not_propagated_time.stop();
drop(ancestors);
drop(descendants);
if !tpu_has_bank && !poh_controller.has_pending_message() {
⋮----
start_leader_time.stop();
replay_timing.update_non_alpenglow(
collect_frozen_banks_time.as_us(),
compute_bank_stats_time.as_us(),
select_vote_and_reset_forks_time.as_us(),
reset_bank_time.as_us(),
voting_time.as_us(),
select_forks_time.as_us(),
compute_slot_stats_time.as_us(),
heaviest_fork_failures_time.as_us(),
⋮----
process_ancestor_hashes_duplicate_slots_time.as_us(),
process_duplicate_confirmed_slots_time.as_us(),
process_unfrozen_gossip_verified_vote_hashes_time.as_us(),
process_popular_pruned_forks_time.as_us(),
process_duplicate_slots_time.as_us(),
dump_then_repair_correct_slots_time.as_us(),
retransmit_not_propagated_time.as_us(),
start_leader_time.as_us(),
⋮----
let result = ledger_signal_receiver.recv_timeout(timer);
⋮----
Ok(_) => trace!("blockstore signal"),
⋮----
wait_receive_time.stop();
replay_timing.update_common(
generate_new_bank_forks_time.as_us(),
replay_active_banks_time.as_us(),
wait_receive_time.as_us(),
⋮----
.name("solReplayStage".to_string())
.spawn(run_replay)
.unwrap();
Ok(Self {
⋮----
fn load_tower(
⋮----
let tower = Tower::restore(tower_storage, node_pubkey).and_then(|restored_tower| {
let root_bank = bank_forks.read().unwrap().root_bank();
let slot_history = root_bank.get_slot_history();
restored_tower.adjust_lockouts_after_replay(root_bank.slot(), &slot_history)
⋮----
Ok(tower) => Ok(tower),
Err(err) if err.is_file_missing() => {
⋮----
Ok(Tower::new_from_bankforks(
&bank_forks.read().unwrap(),
⋮----
Err(err) if err.is_too_old() => {
⋮----
Err(err) => Err(err),
⋮----
fn check_for_vote_only_mode(
⋮----
if heaviest_bank_slot.saturating_sub(forks_root) > MAX_ROOT_DISTANCE_FOR_VOTE_ONLY {
if !in_vote_only_mode.load(Ordering::Relaxed)
⋮----
.compare_exchange(false, true, Ordering::Relaxed, Ordering::Relaxed)
.is_ok()
⋮----
let bank_forks = bank_forks.read().unwrap();
datapoint_warn!(
⋮----
} else if in_vote_only_mode.load(Ordering::Relaxed)
⋮----
.compare_exchange(true, false, Ordering::Relaxed, Ordering::Relaxed)
⋮----
fn maybe_retransmit_unpropagated_slots(
⋮----
let first_leader_group_slot = first_of_consecutive_leader_slots(latest_leader_slot);
⋮----
let is_propagated = progress.is_propagated(slot);
if let Some(retransmit_info) = progress.get_retransmit_info_mut(slot) {
if !is_propagated.expect(
⋮----
if retransmit_info.reached_retransmit_threshold() {
⋮----
let _ = retransmit_slots_sender.send(slot);
retransmit_info.increment_retry_iteration();
⋮----
debug!(
⋮----
fn retransmit_latest_unpropagated_leader_slot(
⋮----
let start_slot = poh_recorder.read().unwrap().start_slot();
// It is possible that bank corresponding to `start_slot` has been
// dumped, so we need to double check it exists before proceeding
if !progress.contains(&start_slot) {
⋮----
progress.get_leader_propagation_slot_must_exist(start_slot)
⋮----
fn is_partition_detected(
⋮----
.get(&heaviest_slot)
.map(|ancestors| ancestors.contains(&last_voted_slot))
.unwrap_or(true)
⋮----
fn get_active_descendants(
⋮----
let Some(slot_meta) = blockstore.meta(slot).ok().flatten() else {
return vec![];
⋮----
.iter()
.filter(|slot| !progress.is_dead(**slot).unwrap_or_default())
.copied()
.collect()
⋮----
fn initialize_progress_and_fork_choice_with_locked_bank_forks(
⋮----
let root_bank = bank_forks.root_bank();
⋮----
// It is important that the root bank is not marked as duplicate on initialization.
// Although this bank could contain a duplicate proof, the fact that it was rooted
// either during a previous run or artificially means that we should ignore any
// duplicate proofs for the root slot, thus we start consuming duplicate proofs
// from the root slot + 1
.duplicate_slots_iterator(root_bank.slot().saturating_add(1))
⋮----
let duplicate_slot_hashes = duplicate_slots.filter_map(|slot| {
let bank = bank_forks.get(slot)?;
Some((slot, bank.hash()))
⋮----
.collect(),
⋮----
pub fn initialize_progress_and_fork_choice(
⋮----
frozen_banks.sort_by_key(|bank| bank.slot());
// Initialize progress map with any root banks
⋮----
let prev_leader_slot = progress.get_bank_prev_leader_slot(bank);
progress.insert(
bank.slot(),
⋮----
let root = root_bank.slot();
⋮----
(root, root_bank.hash()),
⋮----
heaviest_subtree_fork_choice.mark_fork_invalid_candidate(&slot_hash);
⋮----
pub fn dump_then_repair_correct_slots(
⋮----
if duplicate_slots_to_repair.is_empty() {
⋮----
let mut dumped = vec![];
// TODO: handle if alternate version of descendant also got confirmed after ancestor was
// confirmed, what happens then? Should probably keep track of dumped list and skip things
// in `duplicate_slots_to_repair` that have already been dumped. Add test.
duplicate_slots_to_repair.retain(|duplicate_slot, correct_hash| {
// Should not dump duplicate slots if there is currently a poh bank building
// on top of that slot, as BankingStage might still be referencing/touching that state
// concurrently.
// Luckily for us, because the fork choice rule removes duplicate slots from fork
// choice, and this function is called after:
// 1) We have picked a bank to reset to in `select_vote_and_reset_forks()`
// 2) And also called `reset_poh_recorder()`
// Then we should have reset to a fork that doesn't include the duplicate block,
⋮----
.map(|poh_bank_slot| {
⋮----
.get(&poh_bank_slot)
.expect("Poh bank should exist in BankForks and thus in ancestors map")
.contains(duplicate_slot)
⋮----
.unwrap_or(false);
⋮----
let frozen_hash = bank_forks.read().unwrap().bank_hash(*duplicate_slot);
⋮----
&& !progress.is_dead(*duplicate_slot).expect(
⋮----
if Some(*my_pubkey)
== leader_schedule_cache.slot_leader_at(*duplicate_slot, None)
⋮----
if let Some(bank) = bank_forks.read().unwrap().get(*duplicate_slot) {
⋮----
.map_err(|err| {
warn!("Unable to write bank hash details file: {err}");
⋮----
.ok();
⋮----
panic!(
⋮----
.entry(*duplicate_slot)
.and_modify(|x| *x += 1)
.or_insert(1);
⋮----
dumped.push((*duplicate_slot, *correct_hash));
⋮----
trace!("Dumped {} slots", dumped.len());
dumped_slots_sender.send(dumped).unwrap();
⋮----
fn process_ancestor_hashes_duplicate_slots(
⋮----
let root = bank_forks.read().unwrap().root();
⋮----
} in ancestor_duplicate_slots_receiver.try_iter()
⋮----
|| progress.is_dead(epoch_slots_frozen_slot).unwrap_or(false),
⋮----
.get(epoch_slots_frozen_slot)
.map(|b| b.hash())
⋮----
request_type.is_pruned(),
⋮----
check_slot_agrees_with_cluster(
⋮----
fn purge_unconfirmed_duplicate_slot(
⋮----
warn!("purging slot {duplicate_slot}");
let slot_descendants = descendants.get(&duplicate_slot).cloned();
if slot_descendants.is_none() {
if root_bank.slot() <= duplicate_slot {
blockstore.clear_unconfirmed_slot(duplicate_slot);
⋮----
let slot_descendants = slot_descendants.unwrap();
⋮----
let mut w_bank_forks = bank_forks.write().unwrap();
w_bank_forks.dump_slots(
⋮----
.chain(std::iter::once(&duplicate_slot)),
⋮----
root_bank.remove_unrooted_slots(&slots_to_purge);
drop(removed_banks);
⋮----
root_bank.clear_slot_signatures(slot);
root_bank.prune_program_cache_by_deployment_slot(slot);
if let Some(bank_hash) = blockstore.get_bank_hash(slot) {
⋮----
blockstore.clear_unconfirmed_slot(slot);
⋮----
warn!("purging duplicate slot: {slot} with slot_id {slot_id}");
⋮----
blockstore.remove_dead_slot(slot).unwrap();
⋮----
let _ = progress.remove(&slot);
⋮----
fn purge_ancestors_descendants(
⋮----
if !ancestors.contains_key(&slot) {
⋮----
.get(&slot)
.expect("must exist based on earlier check")
⋮----
.get_mut(a)
.expect("If exists in ancestor map must exist in descendants map")
.retain(|d| *d != slot && !slot_descendants.contains(d));
⋮----
.remove(&slot)
.expect("must exist based on earlier check");
⋮----
ancestors.remove(descendant).expect("must exist");
⋮----
.remove(descendant)
⋮----
fn process_popular_pruned_forks(
⋮----
for new_popular_pruned_slots in popular_pruned_forks_receiver.try_iter() {
⋮----
fn process_duplicate_confirmed_slots(
⋮----
for new_duplicate_confirmed_slots in duplicate_confirmed_slots_receiver.try_iter() {
⋮----
duplicate_confirmed_slots.insert(confirmed_slot, duplicate_confirmed_hash)
⋮----
assert_eq!(
⋮----
|| progress.is_dead(confirmed_slot).unwrap_or(false),
|| bank_forks.read().unwrap().bank_hash(confirmed_slot),
⋮----
fn process_gossip_verified_vote_hashes(
⋮----
for (pubkey, slot, hash) in gossip_verified_vote_hash_receiver.try_iter() {
let is_frozen = heaviest_subtree_fork_choice.contains_block(&(slot, hash));
unfrozen_gossip_verified_vote_hashes.add_vote(
⋮----
fn process_duplicate_slots(
⋮----
let new_duplicate_slots: Vec<Slot> = duplicate_slots_receiver.try_iter().collect();
⋮----
.map(|duplicate_slot| r_bank_forks.bank_hash(*duplicate_slot))
⋮----
(r_bank_forks.root(), bank_hashes)
⋮----
new_duplicate_slots.into_iter().zip(bank_hashes.into_iter())
⋮----
|| progress.is_dead(duplicate_slot).unwrap_or(false),
⋮----
fn log_leader_change(
⋮----
if let Some(current_leader) = current_leader.as_ref() {
⋮----
info!("LEADER CHANGE at slot: {bank_slot} leader: {new_leader}{msg}");
⋮----
current_leader.replace(new_leader.to_owned());
⋮----
fn check_propagation_for_start_leader(
⋮----
// Assume `NUM_CONSECUTIVE_LEADER_SLOTS` = 4. Then `skip_propagated_check`
// below is true if `poh_slot` is within the same `NUM_CONSECUTIVE_LEADER_SLOTS`
// set of blocks as `latest_leader_slot`.
//
// Example 1 (`poh_slot` directly descended from `latest_leader_slot`):
⋮----
// [B B B B] [B B B latest_leader_slot] poh_slot
⋮----
// Example 2:
⋮----
// [B latest_leader_slot B poh_slot]
⋮----
// In this example, even if there's a block `B` on another fork between
// `poh_slot` and `parent_slot`, because they're in the same
// `NUM_CONSECUTIVE_LEADER_SLOTS` block, we still skip the propagated
// check because it's still within the propagation grace period.
⋮----
// We've already checked in start_leader() that parent_slot hasn't been
// dumped, so we should get it in the progress map.
⋮----
progress_map.get_latest_leader_slot_must_exist(parent_slot)
⋮----
// Note that `is_propagated(parent_slot)` doesn't necessarily check
// propagation of `parent_slot`, it checks propagation of the latest ancestor
// of `parent_slot` (hence the call to `get_latest_leader_slot()` in the
// check above)
⋮----
.get_leader_propagation_slot_must_exist(parent_slot)
⋮----
fn should_retransmit(poh_slot: Slot, last_retransmit_slot: &mut Slot) -> bool {
⋮----
/// Checks if it is time for us to start producing a leader block.
    /// Fails if:
⋮----
/// Fails if:
    /// - Current PoH has not satisfied criteria to start my leader block
⋮----
/// - Current PoH has not satisfied criteria to start my leader block
    /// - Startup verification is not complete,
⋮----
/// - Startup verification is not complete,
    /// - Bank forks already contains a bank for this leader slot
⋮----
/// - Bank forks already contains a bank for this leader slot
    /// - We have not landed a vote yet and the `wait_for_vote_to_start_leader` flag is set
⋮----
/// - We have not landed a vote yet and the `wait_for_vote_to_start_leader` flag is set
    /// - We have failed the propagated check
⋮----
/// - We have failed the propagated check
    ///
⋮----
///
    /// Returns Some new working bank slot if created and inserted into bank forks.
⋮----
/// Returns Some new working bank slot if created and inserted into bank forks.
    #[allow(clippy::too_many_arguments)]
fn maybe_start_leader(
⋮----
// all the individual calls to poh_recorder.read() are designed to
// increase granularity, decrease contention
⋮----
match poh_recorder.read().unwrap().reached_leader_slot(my_pubkey) {
⋮----
trace!("{my_pubkey} poh_recorder hasn't reached_leader_slot");
⋮----
trace!("{my_pubkey} reached_leader_slot");
let Some(parent) = bank_forks.read().unwrap().get(parent_slot) else {
⋮----
assert!(parent.is_frozen());
if bank_forks.read().unwrap().get(poh_slot).is_some() {
warn!("{my_pubkey} already have bank in forks at {poh_slot}?");
⋮----
trace!("{my_pubkey} poh_slot {poh_slot} parent_slot {parent_slot}");
if let Some(next_leader) = leader_schedule_cache.slot_leader_at(poh_slot, Some(&parent)) {
⋮----
info!("Haven't landed a vote, so skipping my leader slot");
⋮----
trace!("{my_pubkey} leader {next_leader} at poh slot: {poh_slot}");
⋮----
.get_latest_leader_slot_must_exist(parent_slot)
.expect(
⋮----
progress_map.log_propagated_stats(latest_unconfirmed_leader_slot, bank_forks);
⋮----
let root_slot = bank_forks.read().unwrap().root();
datapoint_info!("replay_stage-my_leader_slot", ("slot", poh_slot, i64),);
info!("new fork:{poh_slot} parent:{parent_slot} (leader) root:{root_slot}");
⋮----
datapoint_info!("vote-only-bank", ("slot", poh_slot, i64));
⋮----
parent.clone(),
⋮----
banking_tracer.hash_event(parent.slot(), &parent.last_blockhash(), &parent.hash());
update_bank_forks_and_poh_recorder_for_new_tpu_bank(
⋮----
Some(poh_slot)
⋮----
error!("{my_pubkey} No next leader found");
⋮----
fn replay_blockstore_into_bank(
⋮----
let mut w_replay_stats = replay_stats.write().unwrap();
let mut w_replay_progress = replay_progress.write().unwrap();
⋮----
Some(replay_vote_sender),
⋮----
Ok(tx_count)
⋮----
fn mark_dead_slot(
⋮----
let is_serious = !matches!(
⋮----
let slot = bank.slot();
let parent_slot = bank.parent_slot();
⋮----
datapoint_error!(
⋮----
progress.get_mut(&slot).unwrap().is_dead = true;
⋮----
.set_dead_slot(slot)
.expect("Failed to mark slot as dead in blockstore");
blockstore.slots_stats.mark_dead(slot);
let err = format!("error: {err:?}");
⋮----
.notify_slot_dead(slot, parent_slot, err.clone());
⋮----
rpc_subscriptions.notify_slot_update(SlotUpdate::Dead {
⋮----
timestamp: timestamp(),
⋮----
if !duplicate_slots_tracker.contains(&slot)
&& blockstore.get_duplicate_slot(slot).is_some()
⋮----
fn handle_votable_bank(
⋮----
if bank.is_empty() {
datapoint_info!("replay_stage-voted_empty_bank", ("slot", bank.slot(), i64));
⋮----
trace!("handle votable bank {}", bank.slot());
let new_root = tower.record_bank_vote(bank);
⋮----
if first_alpenglow_slot.is_none() {
⋮----
.compute_pending_activation_slot(&agave_feature_set::alpenglow::id())
⋮----
*first_alpenglow_slot = Some(activation_slot);
⋮----
let highest_super_majority_root = Some(
⋮----
.highest_super_majority_root(),
⋮----
&identity_keypair.pubkey(),
bank.parent_slot(),
⋮----
let node_vote_state = (*vote_account_pubkey, tower.vote_state.clone());
⋮----
bank.clone(),
bank_forks.read().unwrap().root(),
progress.get_fork_stats(bank.slot()).unwrap().total_stake,
⋮----
update_commitment_cache_time.stop();
replay_timing.update_commitment_cache_us += update_commitment_cache_time.as_us();
⋮----
fn generate_vote_tx(
⋮----
if authorized_voter_keypairs.is_empty() {
⋮----
if bank.slot() < slot {
⋮----
let Some(vote_account) = bank.get_vote_account(vote_account_pubkey) else {
warn!("Vote account {vote_account_pubkey} does not exist.  Unable to vote",);
⋮----
let vote_state_view = vote_account.vote_state_view();
if vote_state_view.node_pubkey() != &node_keypair.pubkey() {
⋮----
let Some(authorized_voter_pubkey) = vote_state_view.get_authorized_voter(bank.epoch())
⋮----
.find(|keypair| &keypair.pubkey() == authorized_voter_pubkey)
⋮----
.to_vote_instruction(
⋮----
&authorized_voter_keypair.pubkey(),
⋮----
.expect("Switch failure should not lead to voting");
let mut vote_tx = Transaction::new_with_payer(&[vote_ix], Some(&node_keypair.pubkey()));
let blockhash = bank.last_blockhash();
vote_tx.partial_sign(&[node_keypair], blockhash);
vote_tx.partial_sign(&[authorized_voter_keypair.as_ref()], blockhash);
⋮----
let message_hash = vote_tx.message.hash();
⋮----
tracked_vote_transactions.push(TrackedVoteTransaction {
⋮----
if tracked_vote_transactions.len() > MAX_VOTE_SIGNATURES {
tracked_vote_transactions.remove(0);
⋮----
tracked_vote_transactions.clear();
⋮----
fn maybe_refresh_last_vote(
⋮----
let Some(heaviest_bank_on_same_fork) = heaviest_bank_on_same_fork.as_ref() else {
⋮----
progress.my_latest_landed_vote(heaviest_bank_on_same_fork.slot())
⋮----
let Some(last_voted_slot) = tower.last_voted_slot() else {
⋮----
&& last_vote_refresh_time.last_print_time.elapsed().as_secs() >= 1
⋮----
let last_vote_tx_blockhash = match tower.last_vote_tx_blockhash() {
⋮----
BlockhashStatus::Blockhash(blockhash) => Some(blockhash),
⋮----
if last_vote_tx_blockhash.is_some()
⋮----
.is_hash_valid_for_age(&last_vote_tx_blockhash.unwrap(), REFRESH_VOTE_BLOCKHEIGHT)
⋮----
.elapsed()
.as_millis()
⋮----
fn refresh_last_vote(
⋮----
tower.refresh_last_vote_timestamp(heaviest_bank_on_same_fork.slot());
⋮----
tower.last_vote(),
⋮----
tower.refresh_last_vote_tx_blockhash(recent_blockhash);
let hash_string = format!("{recent_blockhash}");
⋮----
.send(VoteOp::RefreshVote {
⋮----
.unwrap_or_else(|err| warn!("Error: {err:?}"));
⋮----
} else if vote_tx_result.is_non_voting() {
tower.mark_last_vote_tx_blockhash_non_voting();
⋮----
} else if vote_tx_result.is_hot_spare() {
tower.mark_last_vote_tx_blockhash_hot_spare();
⋮----
fn push_vote(
⋮----
generate_time.stop();
replay_timing.generate_vote_us += generate_time.as_us();
⋮----
tower.refresh_last_vote_tx_blockhash(vote_tx.message.recent_blockhash);
let saved_tower = SavedTower::new(tower, identity_keypair).unwrap_or_else(|err| {
error!("Unable to create saved tower: {err:?}");
⋮----
let tower_slots = tower.tower_slots();
⋮----
.send(VoteOp::PushVote {
⋮----
fn update_commitment_cache(
⋮----
if let Err(e) = lockouts_sender.send(CommitmentAggregationData::new(
⋮----
trace!("lockouts_sender failed: {e:?}");
⋮----
fn reset_poh_recorder(
⋮----
let tick_height = bank.tick_height();
let next_leader_slot = leader_schedule_cache.next_leader_slot(
⋮----
Some(blockstore),
⋮----
if poh_controller.reset(bank, next_leader_slot).is_err() {
warn!("Failed to reset poh, poh service is disconnected");
⋮----
format!("My next leader slot is {}", next_leader_slot.0)
⋮----
"I am not in the leader schedule yet".to_owned()
⋮----
fn replay_active_banks_concurrently(
⋮----
let replay_result_vec: Vec<ReplaySlotFromBlockstore> = fork_thread_pool.install(|| {
⋮----
.into_par_iter()
.map(|bank_slot| {
⋮----
let my_pubkey = &my_pubkey.clone();
trace!(
⋮----
let mut progress_lock = progress.write().unwrap();
⋮----
.get(&bank_slot)
.map(|p| p.is_dead)
.unwrap_or(false)
⋮----
debug!("bank_slot {bank_slot:?} is marked dead");
⋮----
.get_with_scheduler(bank_slot)
⋮----
.get(&parent_slot)
.expect("parent of active bank must exist in progress map");
⋮----
let new_dropped_blocks = bank.slot() - parent_slot - 1;
⋮----
let prev_leader_slot = progress_lock.get_bank_prev_leader_slot(&bank);
let bank_progress = progress_lock.entry(bank.slot()).or_insert_with(|| {
⋮----
&vote_account.clone(),
⋮----
let replay_stats = bank_progress.replay_stats.clone();
let replay_progress = bank_progress.replay_progress.clone();
drop(progress_lock);
if bank.collector_id() != my_pubkey {
⋮----
&replay_vote_sender.clone(),
⋮----
replay_blockstore_time.stop();
replay_result.replay_result = Some(blockstore_result);
⋮----
.fetch_max(replay_blockstore_time.as_us(), Ordering::Relaxed);
⋮----
replay_timing.replay_blockstore_us += longest_replay_time_us.load(Ordering::Relaxed);
⋮----
fn replay_active_bank(
⋮----
trace!("Replay active bank: slot {bank_slot}");
if progress.get(&bank_slot).map(|p| p.is_dead).unwrap_or(false) {
⋮----
let Some(bank) = bank_forks.read().unwrap().get_with_scheduler(bank_slot) else {
info!("Abandoning replay of unrooted slot {bank_slot}");
⋮----
let prev_leader_slot = progress.get_bank_prev_leader_slot(&bank);
⋮----
let Some(stats) = progress.get(&parent_slot) else {
⋮----
let bank_progress = progress.entry(bank.slot()).or_insert_with(|| {
⋮----
replay_timing.replay_blockstore_us += replay_blockstore_time.as_us();
⋮----
fn maybe_initiate_alpenglow_migration(
⋮----
error!("Attempting to start alpenglow migration but it is already complete");
⋮----
info!("initiating alpenglow migration");
poh_recorder.write().unwrap().is_alpenglow_enabled = true;
⋮----
info!("alpenglow migration complete!");
⋮----
fn process_replay_results(
⋮----
let Some(bank) = &bank_forks.read().unwrap().get_with_scheduler(bank_slot) else {
⋮----
assert_eq!(bank_slot, bank.slot());
if bank.is_complete() {
⋮----
if !*is_alpenglow_migration_complete && bank.slot() >= first_alpenglow_slot {
⋮----
.get_mut(&bank.slot())
.expect("Bank fork progress entry missing for completed bank");
⋮----
bank.wait_for_completed_scheduler()
⋮----
.accumulate(metrics, is_unified_scheduler_enabled);
⋮----
let is_leader_block = bank.collector_id() == my_pubkey;
⋮----
match blockstore.check_last_fec_set_and_get_block_id(
⋮----
bank.hash(),
⋮----
bank.set_block_id(block_id);
bank.freeze();
⋮----
let r_replay_stats = replay_stats.read().unwrap();
⋮----
let r_replay_progress = replay_progress.read().unwrap();
⋮----
let _ = cluster_slots_update_sender.send(vec![bank_slot]);
⋮----
transaction_status_sender.send_transaction_status_freeze_message(bank);
⋮----
.send(CostUpdate::FrozenBank {
bank: bank.clone_without_scheduler(),
⋮----
.unwrap_or_else(|err| {
warn!("cost_update_sender failed sending bank stats: {err:?}")
⋮----
assert_ne!(bank.hash(), Hash::default());
bank_progress.fork_stats.bank_hash = Some(bank.hash());
⋮----
heaviest_subtree_fork_choice.add_new_leaf_slot(
(bank.slot(), bank.hash()),
Some((bank.parent_slot(), bank.parent_hash())),
⋮----
heaviest_subtree_fork_choice.maybe_print_state();
⋮----
if !duplicate_slots_tracker.contains(&bank.slot())
&& blockstore.get_duplicate_slot(bank.slot()).is_some()
⋮----
|| Some(bank.hash()),
⋮----
.as_ref()
.map(|s| s.get_current_declared_work());
⋮----
.send((
BankNotification::Frozen(bank.clone_without_scheduler()),
⋮----
.unwrap_or_else(|err| warn!("bank_notification_sender failed: {err:?}"));
⋮----
let bank_hash = bank.hash();
if let Some(new_frozen_voters) = tbft_structs.as_mut().and_then(|tbft| {
⋮----
.remove_slot_hash(bank.slot(), &bank_hash)
⋮----
latest_validator_votes_for_frozen_banks.check_add_vote(
⋮----
Some(bank_hash),
⋮----
.parent()
.map(|bank| bank.last_blockhash())
.unwrap_or_default();
block_metadata_notifier.notify_block_metadata(
⋮----
&parent_blockhash.to_string(),
⋮----
&bank.last_blockhash().to_string(),
&bank.get_rewards_and_num_partitions(),
Some(bank.clock().unix_timestamp),
Some(bank.block_height()),
bank.executed_transaction_count(),
⋮----
bank_complete_time.stop();
r_replay_stats.report_stats(
⋮----
bank_complete_time.as_us(),
⋮----
execute_timings.accumulate(&r_replay_stats.batch_execute.totals);
⋮----
fn replay_active_banks(
⋮----
let active_bank_slots = bank_forks.read().unwrap().active_bank_slots();
let num_active_banks = active_bank_slots.len();
trace!("{num_active_banks} active bank(s) to replay: {active_bank_slots:?}");
if active_bank_slots.is_empty() {
⋮----
pub fn compute_bank_stats(
⋮----
let mut new_stats = vec![];
⋮----
for bank in frozen_banks.iter() {
let bank_slot = bank.slot();
⋮----
.get_fork_stats_mut(bank_slot)
.expect("All frozen banks must exist in the Progress map")
⋮----
&bank.vote_accounts(),
⋮----
|slot| progress.get_hash(slot),
⋮----
heaviest_subtree_fork_choice.compute_bank_stats(
⋮----
.expect("All frozen banks must exist in the Progress map");
⋮----
stats.block_height = bank.block_height();
⋮----
new_stats.push(bank_slot);
⋮----
vote_slots.shrink_to(MAX_LOCKOUT_HISTORY + 1);
⋮----
fn adopt_on_chain_tower_if_behind(
⋮----
let Some(vote_account) = bank.get_vote_account(my_vote_pubkey) else {
⋮----
let mut bank_vote_state = TowerVoteState::from(vote_account.vote_state_view());
if bank_vote_state.last_voted_slot() <= tower.vote_state.last_voted_slot() {
⋮----
.map(|bank_root| local_root > bank_root)
⋮----
bank_vote_state.root_slot = Some(local_root);
⋮----
.retain(|lockout| lockout.slot() > local_root);
⋮----
if let Some(first_vote) = bank_vote_state.votes.front() {
assert!(ancestors
⋮----
let last_voted_slot = tower.vote_state.last_voted_slot().unwrap_or(
⋮----
.expect("root_slot cannot be None here"),
⋮----
.get(last_voted_slot)
.expect("Last voted slot that we are adopting must exist in bank forks");
bank.block_id().unwrap_or_default()
⋮----
tower.update_last_vote_from_vote_state(
⋮----
.get_hash(last_voted_slot)
.expect("Must exist for us to have frozen descendant"),
⋮----
.is_active(&agave_feature_set::enable_tower_sync_ix::id()),
⋮----
for slot in frozen_banks.iter().map(|b| b.slot()) {
⋮----
.get_fork_stats(slot)
.expect("All frozen banks must exist in fork stats")
⋮----
fn cache_tower_stats(
⋮----
.get_fork_stats_mut(slot)
⋮----
tower.check_vote_stake_thresholds(slot, &stats.voted_stakes, stats.total_stake);
stats.is_locked_out = tower.is_locked_out(
⋮----
.expect("Ancestors map should contain slot for is_locked_out() check"),
⋮----
stats.has_voted = tower.has_voted(slot);
stats.is_recent = tower.is_recent(slot);
⋮----
fn update_propagation_status(
⋮----
if progress.get_leader_propagation_slot_must_exist(slot).0 {
⋮----
.get_propagated_stats_mut(slot)
.unwrap_or_else(|| panic!("slot={slot} must exist in ProgressMap"));
if propagated_stats.slot_vote_tracker.is_none() {
propagated_stats.slot_vote_tracker = vote_tracker.get_slot_vote_tracker(slot);
⋮----
let slot_vote_tracker = propagated_stats.slot_vote_tracker.clone();
if propagated_stats.cluster_slot_pubkeys.is_none() {
propagated_stats.cluster_slot_pubkeys = cluster_slots.lookup(slot);
⋮----
let cluster_slot_pubkeys = propagated_stats.cluster_slot_pubkeys.clone();
⋮----
.and_then(|slot_vote_tracker| {
slot_vote_tracker.write().unwrap().get_voted_slot_updates()
⋮----
.map(|v| v.keys().cloned().collect())
⋮----
fn update_fork_propagated_threshold_from_votes(
⋮----
let mut current_leader_slot = progress.get_latest_leader_slot_must_exist(fork_tip);
⋮----
let root = bank_forks.root();
⋮----
if current_leader_slot.is_none() || current_leader_slot.unwrap() < root {
⋮----
.get_propagated_stats_mut(current_leader_slot.unwrap())
.expect("current_leader_slot >= root, so must exist in the progress map");
⋮----
newly_voted_pubkeys.is_empty()
&& cluster_slot_pubkeys.is_empty()
⋮----
assert!(leader_propagated_stats.is_leader_slot);
⋮----
.get(current_leader_slot.unwrap())
.expect("Entry in progress map must exist in BankForks")
.clone();
⋮----
fn update_slot_propagated_threshold_from_votes(
⋮----
newly_voted_pubkeys.retain(|vote_pubkey| {
⋮----
.contains(vote_pubkey);
leader_propagated_stats.add_vote_pubkey(
⋮----
leader_bank.epoch_vote_account_stake(vote_pubkey),
⋮----
cluster_slot_pubkeys.retain(|node_pubkey| {
⋮----
.contains(node_pubkey);
leader_propagated_stats.add_node_pubkey(node_pubkey, leader_bank);
⋮----
fn mark_slots_duplicate_confirmed(
⋮----
for (slot, frozen_hash) in confirmed_slots.iter() {
assert!(*frozen_hash != Hash::default());
⋮----
progress.set_duplicate_confirmed_hash(*slot, *frozen_hash);
if let Some(prev_hash) = duplicate_confirmed_slots.insert(*slot, *frozen_hash) {
⋮----
|| Some(*frozen_hash),
⋮----
fn tower_duplicate_confirmed_forks(
⋮----
let mut duplicate_confirmed_forks = vec![];
for (slot, prog) in progress.iter() {
if prog.fork_stats.duplicate_confirmed_hash.is_some() {
⋮----
.get(*slot)
.expect("bank in progress must exist in BankForks");
⋮----
.as_millis();
if !bank.is_frozen() {
⋮----
if tower.is_slot_duplicate_confirmed(*slot, voted_stakes, total_stake) {
⋮----
duplicate_confirmed_forks.push((*slot, bank.hash()));
⋮----
fn check_and_handle_new_root(
⋮----
.expect("rooting must succeed")
⋮----
fn set_progress_and_tower_bft_root(
⋮----
} in tracked_vote_transactions.iter()
⋮----
.get_committed_transaction_status_and_slot(message_hash, transaction_blockhash)
.is_some()
⋮----
progress.handle_new_root(bank_forks);
⋮----
heaviest_subtree_fork_choice.set_tree_root((new_root, bank_forks.root_bank().hash()));
*duplicate_slots_tracker = duplicate_slots_tracker.split_off(&new_root);
*duplicate_confirmed_slots = duplicate_confirmed_slots.split_off(&new_root);
unfrozen_gossip_verified_vote_hashes.set_root(new_root);
*epoch_slots_frozen_slots = epoch_slots_frozen_slots.split_off(&new_root);
⋮----
pub fn handle_new_root(
⋮----
fn generate_new_bank_forks(
⋮----
let forks = bank_forks.read().unwrap();
generate_new_bank_forks_read_lock.stop();
let frozen_banks: HashMap<_, _> = forks.frozen_banks().collect();
⋮----
.keys()
.cloned()
.filter(|slot| *slot >= forks.root())
⋮----
.get_slots_since(&frozen_bank_slots)
.expect("Db error");
generate_new_bank_forks_get_slots_since.stop();
trace!("generate new forks {:?}", {
⋮----
.expect("missing parent in bank forks");
⋮----
if forks.get(child_slot).is_some() || new_banks.contains_key(&child_slot) {
trace!("child already active or frozen {child_slot}");
⋮----
.slot_leader_at(child_slot, Some(parent_bank))
⋮----
parent_bank.clone(),
⋮----
forks.root(),
⋮----
let empty: Vec<Pubkey> = vec![];
⋮----
vec![leader],
parent_bank.slot(),
⋮----
new_banks.insert(child_slot, child_bank);
⋮----
drop(forks);
generate_new_bank_forks_loop.stop();
⋮----
if !new_banks.is_empty() {
let mut forks = bank_forks.write().unwrap();
let root = forks.root();
⋮----
if forks.get(bank.parent_slot()).is_none() {
⋮----
forks.insert(bank);
⋮----
generate_new_bank_forks_write_lock.stop();
⋮----
generate_new_bank_forks_read_lock.as_us();
⋮----
generate_new_bank_forks_get_slots_since.as_us();
replay_timing.generate_new_bank_forks_loop_us += generate_new_bank_forks_loop.as_us();
⋮----
generate_new_bank_forks_write_lock.as_us();
⋮----
pub(crate) fn new_bank_from_parent_with_notify(
⋮----
rpc_subscriptions.notify_slot(slot, parent.slot(), root_slot);
⋮----
.notify_created_bank(slot, parent.slot());
⋮----
fn log_heaviest_fork_failures(
⋮----
progress.get_latest_leader_slot_must_exist(*slot)
⋮----
progress.log_propagated_stats(latest_leader_slot, bank_forks);
⋮----
let in_partition = if let Some(last_voted_slot) = tower.last_voted_slot() {
⋮----
pub fn join(self) -> thread::Result<()> {
self.commitment_service.join()?;
self.t_replay.join().map(|_| ())
⋮----
pub(crate) mod tests {
⋮----
fn test_is_partition_detected() {
let (VoteSimulator { bank_forks, .. }, _) = setup_default_forks(1, None::<GenerateVotes>);
let ancestors = bank_forks.read().unwrap().ancestors();
assert!(!ReplayStage::is_partition_detected(&ancestors, 1, 3));
assert!(!ReplayStage::is_partition_detected(&ancestors, 3, 3));
assert!(ReplayStage::is_partition_detected(&ancestors, 2, 3));
assert!(ReplayStage::is_partition_detected(&ancestors, 4, 3));
⋮----
pub struct ReplayBlockstoreComponents {
⋮----
pub fn replay_blockstore_components(
⋮----
let (vote_simulator, blockstore) = setup_forks_from_tree(
forks.unwrap_or_else(|| tr(0)),
⋮----
.values()
.map(|keypairs| {
⋮----
keypairs.node_keypair.pubkey(),
keypairs.vote_keypair.pubkey(),
⋮----
let my_keypairs = validator_keypairs.values().next().unwrap();
let my_pubkey = my_keypairs.node_keypair.pubkey();
⋮----
Arc::new(my_keypairs.node_keypair.insecure_clone()),
⋮----
assert_eq!(my_pubkey, cluster_info.id());
⋮----
let working_bank = bank_forks.read().unwrap().working_bank();
⋮----
create_test_recorder(
⋮----
blockstore.clone(),
Some(PohConfig::default()),
Some(leader_schedule_cache.clone()),
⋮----
let my_vote_pubkey = my_keypairs.vote_keypair.pubkey();
⋮----
&cluster_info.id(),
⋮----
bank_forks.clone(),
⋮----
fn test_child_slots_of_same_parent() {
⋮----
} = replay_blockstore_components(None, 1, None::<GenerateVotes>);
⋮----
bank_forks.read().unwrap().get(0).unwrap(),
&leader_schedule_cache.slot_leader_at(1, None).unwrap(),
⋮----
bank1.collector_id(),
⋮----
.get(bank1.collector_id())
.unwrap(),
Some(0),
⋮----
assert!(progress.get_propagated_stats(1).unwrap().is_leader_slot);
bank1.freeze();
bank_forks.write().unwrap().insert(bank1);
let rpc_subscriptions = Some(rpc_subscriptions);
let (shreds, _) = make_slot_entries(
⋮----
blockstore.insert_shreds(shreds, None, false).unwrap();
assert!(bank_forks
⋮----
let (shreds, _) = make_slot_entries(2 * NUM_CONSECUTIVE_LEADER_SLOTS, 1, 8);
⋮----
let expected_leader_slots = vec![
⋮----
let leader = leader_schedule_cache.slot_leader_at(slot, None).unwrap();
let vote_key = validator_node_to_vote_keys.get(&leader).unwrap();
assert!(progress
⋮----
fn test_handle_new_root() {
let genesis_config = create_genesis_config(10_000).genesis_config;
⋮----
root_bank.freeze();
let root_hash = root_bank.hash();
bank_forks.write().unwrap().insert(root_bank);
⋮----
progress.insert(i, ForkProgress::new(Hash::default(), None, None, 0, 0));
⋮----
vec![root - 1, root, root + 1].into_iter().collect();
let duplicate_confirmed_slots: DuplicateConfirmedSlots = vec![root - 1, root, root + 1]
.into_iter()
.map(|s| (s, Hash::default()))
⋮----
votes_per_slot: vec![root - 1, root, root + 1]
⋮----
.map(|s| (s, HashMap::new()))
⋮----
let epoch_slots_frozen_slots: EpochSlotsFrozenSlots = vec![root - 1, root, root + 1]
⋮----
.map(|slot| (slot, Hash::default()))
⋮----
let (drop_bank_sender, _drop_bank_receiver) = unbounded();
⋮----
assert_eq!(bank_forks.read().unwrap().root(), root);
assert_eq!(progress.len(), 1);
assert!(progress.get(&root).is_some());
⋮----
fn test_handle_new_root_ahead_of_highest_super_majority_root() {
⋮----
bank_forks.read().unwrap().get(confirmed_root).unwrap(),
⋮----
bank_forks.write().unwrap().insert(bank2);
⋮----
Some(confirmed_root),
⋮----
assert!(bank_forks.read().unwrap().get(confirmed_root).is_some());
assert!(bank_forks.read().unwrap().get(fork).is_none());
assert_eq!(progress.len(), 2);
⋮----
assert!(progress.get(&confirmed_root).is_some());
assert!(progress.get(&fork).is_none());
⋮----
fn test_dead_fork_transaction_error() {
⋮----
let res = check_dead_fork(|_keypair, bank| {
⋮----
let hashes_per_tick = bank.hashes_per_tick().unwrap_or(0);
⋮----
hashes_per_tick.saturating_sub(1),
vec![
⋮----
entries_to_test_shreds(
⋮----
slot.saturating_sub(1),
⋮----
assert_matches!(
⋮----
fn test_dead_fork_entry_verification_failure() {
⋮----
let res = check_dead_fork(|genesis_keypair, bank| {
⋮----
let bad_hash = hash(&[2; 30]);
⋮----
vec![system_transaction::transfer(
⋮----
assert_eq!(block_error, BlockError::InvalidEntryHash);
⋮----
panic!();
⋮----
fn test_dead_fork_invalid_tick_hash_count() {
⋮----
assert!(hashes_per_tick > 0);
let too_few_hashes_tick = Entry::new(&blockhash, hashes_per_tick - 1, vec![]);
⋮----
assert_eq!(block_error, BlockError::InvalidTickHashCount);
⋮----
fn test_dead_fork_invalid_slot_tick_count() {
⋮----
&entry::create_ticks(bank.ticks_per_slot() + 1, hashes_per_tick, blockhash),
⋮----
assert_eq!(block_error, BlockError::TooManyTicks);
⋮----
&entry::create_ticks(bank.ticks_per_slot() - 1, hashes_per_tick, blockhash),
⋮----
assert_eq!(block_error, BlockError::TooFewTicks);
⋮----
fn test_dead_fork_invalid_last_tick() {
⋮----
&entry::create_ticks(bank.ticks_per_slot(), hashes_per_tick, blockhash),
⋮----
assert_eq!(block_error, BlockError::InvalidLastTick);
⋮----
fn test_dead_fork_trailing_entry() {
⋮----
let res = check_dead_fork(|funded_keypair, bank| {
⋮----
entry::create_ticks(bank.ticks_per_slot(), hashes_per_tick, blockhash);
let last_entry_hash = entries.last().unwrap().hash;
let tx = system_transaction::transfer(funded_keypair, &keypair.pubkey(), 2, blockhash);
let trailing_entry = entry::next_entry(&last_entry_hash, 1, vec![tx]);
entries.push(trailing_entry);
⋮----
assert_eq!(block_error, BlockError::TrailingEntry);
⋮----
fn test_dead_fork_entry_deserialize_failure() {
let res = check_dead_fork(|_, bank| {
⋮----
let shredder = Shredder::new(bank.slot(), bank.parent_slot(), 0, 0).unwrap();
⋮----
.make_shreds_from_data_slice(
⋮----
struct SlotStatusNotifierForTest {
⋮----
impl SlotStatusNotifierForTest {
pub fn new(dead_slots: Arc<Mutex<HashSet<Slot>>>) -> Self {
⋮----
impl SlotStatusNotifierInterface for SlotStatusNotifierForTest {
fn notify_slot_confirmed(&self, _slot: Slot, _parent: Option<Slot>) {}
fn notify_slot_processed(&self, _slot: Slot, _parent: Option<Slot>) {}
fn notify_slot_rooted(&self, _slot: Slot, _parent: Option<Slot>) {}
fn notify_first_shred_received(&self, _slot: Slot) {}
fn notify_completed(&self, _slot: Slot) {}
fn notify_created_bank(&self, _slot: Slot, _parent: Slot) {}
fn notify_slot_dead(&self, slot: Slot, _parent: Slot, _error: String) {
self.dead_slots.lock().unwrap().insert(slot);
⋮----
fn check_dead_fork<F>(shred_to_insert: F) -> result::Result<(), BlockstoreProcessorError>
⋮----
let ledger_path = get_tmp_ledger_path!();
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
} = replay_blockstore_components(Some(tr(0)), 1, None);
⋮----
let bank0 = bank_forks.read().unwrap().get(0).unwrap();
assert!(bank0.is_frozen());
assert_eq!(bank0.tick_height(), bank0.max_tick_height());
⋮----
let bank1 = bank_forks.read().unwrap().get_with_scheduler(1).unwrap();
⋮----
.entry(bank1.slot())
.or_insert_with(|| ForkProgress::new(bank1.last_blockhash(), None, None, 0, 0));
let shreds = shred_to_insert(
&validator_keypairs.values().next().unwrap().node_keypair,
bank1.clone(),
⋮----
.num_threads(1)
.thread_name(|i| format!("solReplayTest{i:02}"))
⋮----
unbounded();
⋮----
let slot_status_notifier: Option<SlotStatusNotifier> = Some(Arc::new(RwLock::new(
SlotStatusNotifierForTest::new(dead_slots.clone()),
⋮----
&mut Some(&mut tbft_structs),
⋮----
assert!(dead_slots.lock().unwrap().contains(&bank1.slot()));
⋮----
assert!(blockstore.is_dead(bank1.slot()));
res.map(|_| ())
⋮----
let _ignored = remove_dir_all(ledger_path);
⋮----
fn test_replay_commitment_cache() {
fn leader_vote(vote_slot: Slot, bank: &Bank, pubkey: &Pubkey) -> (Pubkey, TowerVoteState) {
let mut leader_vote_account = bank.get_account(pubkey).unwrap();
⋮----
VoteStateV4::deserialize(leader_vote_account.data(), pubkey).unwrap();
⋮----
let versioned = VoteStateVersions::new_v4(vote_state.clone());
leader_vote_account.set_state(&versioned).unwrap();
bank.store_account(pubkey, &leader_vote_account);
⋮----
create_genesis_config_with_leader(50, &leader_pubkey, leader_lamports);
⋮----
let leader_voting_pubkey = genesis_config_info.voting_keypair.pubkey();
⋮----
bank0.register_default_tick_for_test();
⋮----
bank0.freeze();
⋮----
let rpc_subscriptions = Some(Arc::new(RpcSubscriptions::new_for_tests(
⋮----
assert!(block_commitment_cache
⋮----
let prev_bank = bank_forks.read().unwrap().get(i - 1).unwrap();
let slot = prev_bank.slot() + 1;
⋮----
bank_forks.as_ref(),
⋮----
let _res = bank.transfer(
⋮----
bank.register_default_tick_for_test();
⋮----
let arc_bank = bank_forks.read().unwrap().get(i).unwrap();
let node_vote_state = leader_vote(i - 1, &arc_bank, &leader_voting_pubkey);
⋮----
arc_bank.clone(),
⋮----
arc_bank.freeze();
⋮----
let bcc = block_commitment_cache.read().unwrap();
bcc.get_block_commitment(0).is_some()
&& bcc.get_block_commitment(1).is_some()
&& bcc.get_block_commitment(2).is_some()
⋮----
expected0.increase_confirmation_stake(3, leader_lamports);
⋮----
expected1.increase_confirmation_stake(2, leader_lamports);
⋮----
expected2.increase_confirmation_stake(1, leader_lamports);
⋮----
fn test_write_persist_transaction_status() {
⋮----
} = create_genesis_config(solana_native_token::LAMPORTS_PER_SOL * 1000);
⋮----
let (ledger_path, _) = create_new_tmp_ledger!(&genesis_config);
⋮----
.expect("Expected to successfully open database ledger");
⋮----
.transfer(
bank0.get_minimum_balance_for_rent_exemption(0),
⋮----
&keypair2.pubkey(),
⋮----
.insert(Bank::new_from_parent(bank0, &Pubkey::default(), 1))
.clone_without_scheduler();
let slot = bank1.slot();
let (entries, test_signatures) = create_test_transaction_entries(
vec![&mint_keypair, &keypair1, &keypair2, &keypair3],
⋮----
populate_blockstore_for_tests(
⋮----
let mut test_signatures_iter = test_signatures.into_iter();
let confirmed_block = blockstore.get_rooted_block(slot, false).unwrap();
⋮----
.map(|VersionedTransactionWithStatusMeta { transaction, meta }| {
⋮----
let expected_tx_results = vec![
⋮----
assert_eq!(actual_tx_results, expected_tx_results);
assert!(test_signatures_iter.next().is_none());
⋮----
Blockstore::destroy(&ledger_path).unwrap();
⋮----
fn test_compute_bank_stats_confirmed() {
⋮----
let my_node_pubkey = vote_keypairs.node_keypair.pubkey();
let my_vote_pubkey = vote_keypairs.vote_keypair.pubkey();
let keypairs: HashMap<_, _> = vec![(my_node_pubkey, vote_keypairs)].into_iter().collect();
⋮----
let my_keypairs = keypairs.get(&my_node_pubkey).unwrap();
let tower_sync = TowerSync::new_from_slots(vec![0], bank0.hash(), None);
⋮----
bank0.last_blockhash(),
⋮----
assert_eq!(newly_computed, vec![0]);
⋮----
let fork_progress = progress.get(&0).unwrap();
⋮----
assert!(confirmed_forks.is_empty());
⋮----
bank0.clone(),
⋮----
bank1.process_transaction(&vote_tx).unwrap();
⋮----
ForkProgress::new(bank0.last_blockhash(), None, None, 0, 0),
⋮----
assert_eq!(newly_computed, vec![1]);
⋮----
let fork_progress = progress.get(&1).unwrap();
⋮----
assert_eq!(confirmed_forks, vec![(0, bank0.hash())]);
⋮----
assert!(newly_computed.is_empty());
⋮----
fn test_same_weight_select_lower_slot() {
⋮----
let forks = tr(0) / (tr(1)) / (tr(2));
vote_simulator.fill_bank_forks(forks, &HashMap::new(), true);
⋮----
let ancestors = vote_simulator.bank_forks.read().unwrap().ancestors();
⋮----
let bank1 = vote_simulator.bank_forks.read().unwrap().get(1).unwrap();
let bank2 = vote_simulator.bank_forks.read().unwrap().get(2).unwrap();
⋮----
let (heaviest_bank, _) = heaviest_subtree_fork_choice.select_forks(
⋮----
assert_eq!(heaviest_bank.slot(), 1);
⋮----
fn test_child_bank_heavier() {
⋮----
let forks = tr(0) / (tr(1) / (tr(2) / (tr(3))));
⋮----
let votes = vec![2];
cluster_votes.insert(my_node_pubkey, votes.clone());
vote_simulator.fill_bank_forks(forks, &cluster_votes, true);
⋮----
assert!(vote_simulator
⋮----
&vote_simulator.bank_forks.read().unwrap().ancestors(),
⋮----
for pair in frozen_banks.windows(2) {
⋮----
.get_fork_stats(pair[0].slot())
⋮----
.fork_weight();
⋮----
.get_fork_stats(pair[1].slot())
⋮----
assert!(second >= first);
⋮----
fn test_should_retransmit() {
⋮----
assert!(!ReplayStage::should_retransmit(
⋮----
assert_eq!(last_retransmit_slot, 4);
⋮----
assert!(ReplayStage::should_retransmit(
⋮----
assert_eq!(last_retransmit_slot, poh_slot);
⋮----
fn test_update_slot_propagated_threshold_from_votes() {
⋮----
(vote_keypairs.node_keypair.pubkey(), vote_keypairs)
⋮----
.take(10)
⋮----
.map(|keys| keys.vote_keypair.pubkey())
⋮----
.map(|keys| keys.node_keypair.pubkey())
⋮----
run_test_update_slot_propagated_threshold_from_votes(&keypairs, &new_vote_pubkeys, &[], 4);
run_test_update_slot_propagated_threshold_from_votes(&keypairs, &[], &new_node_pubkeys, 4);
run_test_update_slot_propagated_threshold_from_votes(
⋮----
fn run_test_update_slot_propagated_threshold_from_votes(
⋮----
total_epoch_stake: stake * all_keypairs.len() as u64,
⋮----
for i in 0..std::cmp::max(new_vote_pubkeys.len(), new_node_pubkeys.len()) {
⋮----
let len = std::cmp::min(i, new_vote_pubkeys.len());
let mut voted_pubkeys = new_vote_pubkeys[..len].to_vec();
let len = std::cmp::min(i, new_node_pubkeys.len());
let mut node_pubkeys = new_node_pubkeys[..len].to_vec();
⋮----
if i == 0 || i >= new_vote_pubkeys.len() {
vec![]
⋮----
vec![new_vote_pubkeys[i - 1]]
⋮----
if i == 0 || i >= new_node_pubkeys.len() {
⋮----
vec![new_node_pubkeys[i - 1]]
⋮----
assert_eq!(voted_pubkeys, remaining_vote_pubkeys);
assert_eq!(node_pubkeys, remaining_node_pubkeys);
⋮----
assert!(propagated_stats.is_propagated);
assert!(did_newly_reach_threshold);
⋮----
assert!(!propagated_stats.is_propagated);
assert!(!did_newly_reach_threshold);
⋮----
fn test_update_slot_propagated_threshold_from_votes2() {
let mut empty: Vec<Pubkey> = vec![];
let genesis_config = create_genesis_config(100_000_000).genesis_config;
⋮----
let mut newly_voted_pubkeys: Vec<Pubkey> = vec![];
assert!(ReplayStage::update_slot_propagated_threshold_from_votes(
⋮----
newly_voted_pubkeys = vec![];
assert!(!ReplayStage::update_slot_propagated_threshold_from_votes(
⋮----
fn test_update_propagation_status() {
⋮----
let node_pubkey = vote_keypairs.node_keypair.pubkey();
let vote_pubkey = vote_keypairs.vote_keypair.pubkey();
let keypairs: HashMap<_, _> = vec![(node_pubkey, vote_keypairs)].into_iter().collect();
⋮----
let mut bank_forks = bank_forks_arc.write().unwrap();
let bank0 = bank_forks.get(0).unwrap();
bank_forks.insert(Bank::new_from_parent(bank0.clone(), &Pubkey::default(), 9));
let bank9 = bank_forks.get(9).unwrap();
bank_forks.insert(Bank::new_from_parent(bank9, &Pubkey::default(), 10));
bank_forks.set_root(9, None, None);
let total_epoch_stake = bank0.total_epoch_stake();
progress_map.insert(
⋮----
Some(9),
Some(ValidatorStakeInfo {
⋮----
Some(8),
⋮----
assert!(!progress_map.get_leader_propagation_slot_must_exist(10).0);
drop(bank_forks);
⋮----
vote_tracker.insert_vote(10, vote_pubkey);
⋮----
let propagated_stats = &progress_map.get(&10).unwrap().propagated_stats;
assert!(propagated_stats.slot_vote_tracker.is_some());
assert!(propagated_stats
⋮----
assert_eq!(propagated_stats.propagated_validators_stake, stake);
⋮----
fn test_chain_update_propagation_status() {
⋮----
.get_propagated_stats_mut(0)
⋮----
bank_forks.set_root(0, None, None);
let total_epoch_stake = bank_forks.root_bank().total_epoch_stake();
⋮----
let parent_bank = bank_forks.get(i - 1).unwrap().clone();
⋮----
bank_forks.insert(Bank::new_from_parent(parent_bank, &Pubkey::default(), i));
⋮----
Some(prev_leader_slot),
⋮----
vote_tracker.insert_vote(10, *vote_pubkey);
⋮----
let propagated_stats = &progress_map.get(&i).unwrap().propagated_stats;
⋮----
fn test_chain_update_propagation_status2() {
⋮----
.take(num_validators)
⋮----
vote_pubkeys[0..end_range].iter().copied().collect();
⋮----
progress_map.insert(i, fork_progress);
⋮----
vote_tracker.insert_vote(10, vote_pubkeys[2]);
⋮----
fn test_check_propagation_for_start_leader() {
⋮----
assert!(ReplayStage::check_propagation_for_start_leader(
⋮----
Some(ValidatorStakeInfo::default()),
⋮----
assert!(!ReplayStage::check_propagation_for_start_leader(
⋮----
.get_mut(&parent_slot)
⋮----
ForkProgress::new(Hash::default(), Some(previous_leader_slot), None, 0, 0),
⋮----
.get_mut(&previous_leader_slot)
⋮----
let mut bank_forks = bank_forks.write().unwrap();
⋮----
Bank::new_from_parent(bank_forks.get(parent_slot).unwrap(), &Pubkey::default(), 5);
bank_forks.insert(bank5);
progress_map.handle_new_root(&bank_forks);
⋮----
fn test_check_propagation_skip_propagation_check() {
⋮----
.get_mut(&3)
⋮----
fn test_purge_unconfirmed_duplicate_slot() {
let (vote_simulator, blockstore) = setup_default_forks(2, None::<GenerateVotes>);
⋮----
bank_forks.read().unwrap().get(6).unwrap(),
⋮----
bank_forks.write().unwrap().insert(bank7);
blockstore.add_tree(tr(6) / tr(7), false, false, 3, Hash::default());
let bank7 = bank_forks.read().unwrap().get(7).unwrap();
let mut descendants = bank_forks.read().unwrap().descendants();
let mut ancestors = bank_forks.read().unwrap().ancestors();
⋮----
let old_balance = bank7.get_balance(&sender);
⋮----
&validator_keypairs.get(&sender).unwrap().node_keypair,
⋮----
let validator0_keypairs = &validator_keypairs.get(&sender).unwrap();
⋮----
bank7.process_transaction(&vote_tx).unwrap();
assert!(bank7.get_signature_status(&vote_tx.signatures[0]).is_some());
⋮----
assert!(bank7.get_signature_status(&transfer_sig).is_some());
⋮----
blockstore.insert_bank_hash(i, Hash::new_unique(), false);
⋮----
.set_dead_slot(7)
⋮----
assert!(bank_forks.read().unwrap().get(i).is_none());
assert!(progress.get(&i).is_none());
⋮----
assert!(bank_forks.read().unwrap().get(i).is_some());
assert!(progress.get(&i).is_some());
⋮----
assert!(!blockstore.is_full(*slot));
assert!(!blockstore.is_dead(*slot));
assert!(blockstore.get_slot_entries(*slot, 0).unwrap().is_empty());
⋮----
assert!(!blockstore.is_dead(7));
assert!(!blockstore.get_slot_entries(7, 0).unwrap().is_empty());
assert!(bank7.get_signature_status(&vote_tx.signatures[0]).is_none());
assert!(bank7.get_signature_status(&transfer_sig).is_none());
assert_eq!(bank7.get_balance(&sender), old_balance);
⋮----
assert!(blockstore.get_slot_entries(i, 0).unwrap().is_empty());
⋮----
assert!(!blockstore.get_slot_entries(i, 0).unwrap().is_empty());
⋮----
assert!(bank_forks.read().unwrap().get(0).is_some());
assert!(progress.get(&0).is_some());
⋮----
fn test_purge_unconfirmed_duplicate_slots_and_reattach() {
⋮----
} = replay_blockstore_components(
Some(tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3) / (tr(5) / (tr(6)))))),
⋮----
.set_dead_slot(6)
.expect("Failed to mark slot 6 as dead in blockstore");
⋮----
assert!(bank_forks.read().unwrap().get(*slot).is_none());
assert!(progress.get(slot).is_none());
⋮----
assert!(!blockstore.is_dead(slot));
assert!(!blockstore.get_slot_entries(slot, 0).unwrap().is_empty())
⋮----
assert_eq!(bank_forks.read().unwrap().active_bank_slots(), vec![3]);
⋮----
let bank3 = bank_forks.read().unwrap().get(3).unwrap();
⋮----
bank3.collector_id(),
⋮----
.get(bank3.collector_id())
⋮----
Some(1),
⋮----
bank3.freeze();
⋮----
assert_eq!(bank_forks.read().unwrap().active_bank_slots(), vec![5]);
⋮----
let bank5 = bank_forks.read().unwrap().get(5).unwrap();
⋮----
bank5.collector_id(),
⋮----
.get(bank5.collector_id())
⋮----
Some(3),
⋮----
bank5.freeze();
⋮----
assert_eq!(bank_forks.read().unwrap().active_bank_slots(), vec![6]);
⋮----
let bank6 = bank_forks.read().unwrap().get(6).unwrap();
⋮----
bank6.collector_id(),
⋮----
.get(bank6.collector_id())
⋮----
Some(5),
⋮----
bank6.freeze();
⋮----
assert_eq!(bank_forks.read().unwrap().active_bank_slots(), vec![7]);
⋮----
fn test_purge_ancestors_descendants() {
⋮----
let slot_2_descendants = descendants.get(&2).unwrap().clone();
⋮----
bank_forks.write().unwrap().remove(d);
⋮----
bank_forks.write().unwrap().remove(2);
assert!(check_map_eq(
⋮----
bank_forks.write().unwrap().set_root(3, None, None);
⋮----
let slot_3_descendants = descendants.get(&3).unwrap().clone();
⋮----
assert!(ancestors.is_empty());
for k in descendants.keys() {
assert!(*k < 3);
⋮----
fn test_leader_snapshot_restart_propagation() {
⋮----
.slot_leader_at(root_bank.slot(), Some(&root_bank))
⋮----
assert!(
⋮----
for vote_key in validator_node_to_vote_keys.values() {
vote_tracker.insert_vote(root_bank.slot(), *vote_key);
⋮----
&mut HeaviestSubtreeForkChoice::new_from_bank_forks(bank_forks.clone()),
⋮----
fn test_unconfirmed_duplicate_slots_and_lockouts_for_non_heaviest_fork() {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(3) / (tr(4)))) / tr(5));
⋮----
vote_simulator.fill_bank_forks(forks, &HashMap::<Pubkey, Vec<u64>>::new(), true);
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path())
.expect("Expected to be able to open database ledger"),
⋮----
let (vote_fork, reset_fork, _) = run_compute_and_select_forks(
⋮----
assert_eq!(vote_fork.unwrap(), 4);
assert_eq!(reset_fork.unwrap(), 4);
tower.record_bank_vote(&bank_forks.read().unwrap().get(5).unwrap());
⋮----
assert!(vote_fork.is_none());
assert_eq!(reset_fork, Some(5));
blockstore.store_duplicate_slot(5, vec![], vec![]).unwrap();
⋮----
let bank5_hash = bank_forks.read().unwrap().bank_hash(5).unwrap();
assert_ne!(bank5_hash, Hash::default());
⋮----
|| progress.is_dead(5).unwrap_or(false),
|| Some(bank5_hash),
⋮----
let (vote_fork, reset_fork, heaviest_fork_failures) = run_compute_and_select_forks(
⋮----
let forks = tr(5) / (tr(6) / (tr(7) / (tr(8) / (tr(9)))) / tr(10));
⋮----
assert_eq!(reset_fork, Some(9));
⋮----
duplicate_confirmed_slots.insert(5, bank5_hash);
⋮----
assert!(duplicate_slots_to_repair.is_empty());
⋮----
assert_eq!(reset_fork.unwrap(), 9);
⋮----
let bank6_hash = bank_forks.read().unwrap().bank_hash(6).unwrap();
⋮----
.split_off(&(6, bank6_hash));
⋮----
assert_eq!(reset_fork.unwrap(), 5);
⋮----
fn test_unconfirmed_duplicate_slots_and_lockouts() {
let forks = tr(0) / (tr(1) / (tr(2) / (tr(3) / (tr(4)))) / (tr(5) / (tr(6))));
⋮----
let validator_votes: HashMap<Pubkey, Vec<u64>> = vec![
⋮----
vote_simulator.fill_bank_forks(forks, &validator_votes, true);
⋮----
tower.record_bank_vote(&bank_forks.read().unwrap().get(4).unwrap());
blockstore.store_duplicate_slot(4, vec![], vec![]).unwrap();
⋮----
let bank4_hash = bank_forks.read().unwrap().bank_hash(4).unwrap();
assert_ne!(bank4_hash, Hash::default());
⋮----
|| progress.is_dead(4).unwrap_or(false),
|| Some(bank4_hash),
⋮----
assert_eq!(reset_fork, Some(3));
blockstore.store_duplicate_slot(2, vec![], vec![]).unwrap();
let bank2_hash = bank_forks.read().unwrap().bank_hash(2).unwrap();
assert_ne!(bank2_hash, Hash::default());
⋮----
|| progress.is_dead(2).unwrap_or(false),
|| Some(bank2_hash),
⋮----
assert_eq!(reset_fork.unwrap(), 6);
⋮----
duplicate_confirmed_slots.insert(4, bank4_hash);
⋮----
fn test_dump_then_repair_correct_slots() {
⋮----
} = replay_blockstore_components(Some(forks), 1, None);
⋮----
duplicate_slots_to_repair.insert(1, Hash::new_unique());
duplicate_slots_to_repair.insert(2, Hash::new_unique());
⋮----
let (dumped_slots_sender, dumped_slots_receiver) = unbounded();
⋮----
.map(|(&s, &h)| (s, h))
.collect_vec();
⋮----
assert_eq!(should_be_dumped, dumped_slots_receiver.recv().ok().unwrap());
⋮----
let bank = r_bank_forks.get(slot);
let ancestor_result = ancestors.get(&slot);
let descendants_result = descendants.get(&slot);
⋮----
assert!(bank.is_some());
assert!(ancestor_result.is_some());
assert!(descendants_result.is_some());
⋮----
assert!(bank.is_none());
assert!(ancestor_result.is_none());
assert!(descendants_result.is_none());
⋮----
assert_eq!(2, purge_repair_slot_counter.len());
assert_eq!(1, *purge_repair_slot_counter.get(&1).unwrap());
assert_eq!(1, *purge_repair_slot_counter.get(&2).unwrap());
⋮----
fn setup_vote_then_rollback(
⋮----
let forks = tr(0) / (tr(1) / (tr(2) / (tr(3) / (tr(4) / (tr(5))))) / (tr(6) / (tr(7))));
⋮----
replay_blockstore_components(Some(forks), num_validators, generate_votes);
⋮----
tower.record_bank_vote(&bank_forks.read().unwrap().get(first_vote).unwrap());
let our_bank2_hash = bank_forks.read().unwrap().bank_hash(2).unwrap();
⋮----
duplicate_confirmed_slots.insert(2, duplicate_confirmed_bank2_hash);
⋮----
|| Some(our_bank2_hash),
⋮----
let old_descendants_of_2 = descendants.get(&2).unwrap().clone();
let (dumped_slots_sender, _dumped_slots_receiver) = unbounded();
⋮----
for purged_slot in std::iter::once(&2).chain(old_descendants_of_2.iter()) {
assert!(!ancestors.contains_key(purged_slot));
assert!(!descendants.contains_key(purged_slot));
⋮----
fn run_test_duplicate_rollback_then_vote(first_vote: Slot) -> SelectVoteAndResetForkResult {
let replay_components = setup_vote_then_rollback(
⋮----
Some(Box::new(|node_keys| {
node_keys.into_iter().map(|k| (k, vec![6])).collect()
⋮----
let descendants = bank_forks.read().unwrap().descendants();
⋮----
assert_eq!(heaviest_bank.slot(), 7);
assert!(heaviest_bank_on_same_fork.is_none());
select_vote_and_reset_forks(
⋮----
heaviest_bank_on_same_fork.as_ref(),
⋮----
fn test_duplicate_rollback_then_vote_locked_out() {
⋮----
} = run_test_duplicate_rollback_then_vote(5);
assert!(vote_bank.is_none());
assert_eq!(reset_bank.unwrap().slot(), 7);
⋮----
fn test_duplicate_rollback_then_vote_success() {
⋮----
} = run_test_duplicate_rollback_then_vote(4);
⋮----
assert!(heaviest_fork_failures.is_empty());
⋮----
fn run_test_duplicate_rollback_then_vote_on_other_duplicate(
⋮----
let replay_components = setup_vote_then_rollback(first_vote, 10, None::<GenerateVotes>);
⋮----
.map(|k| (*k, vec![1, 2]))
⋮----
vote_simulator.create_and_vote_new_branch(
⋮----
let bank_1_hash = bank_forks.read().unwrap().bank_hash(1).unwrap();
⋮----
.children(&(1, bank_1_hash))
⋮----
let duplicate_versions_of_2 = children_of_1.filter(|(slot, _hash)| *slot == 2).count();
assert_eq!(duplicate_versions_of_2, 2);
⋮----
assert_eq!(heaviest_bank.slot(), 5);
⋮----
fn test_duplicate_rollback_then_vote_on_other_duplicate_success() {
⋮----
} = run_test_duplicate_rollback_then_vote_on_other_duplicate(3);
⋮----
assert_eq!(reset_bank.unwrap().slot(), 5);
⋮----
fn test_duplicate_rollback_then_vote_on_other_duplicate_same_slot_locked_out() {
⋮----
} = run_test_duplicate_rollback_then_vote_on_other_duplicate(5);
⋮----
fn test_duplicate_rollback_then_vote_on_other_duplicate_different_slot_locked_out() {
⋮----
} = run_test_duplicate_rollback_then_vote_on_other_duplicate(4);
⋮----
fn test_gossip_vote_doesnt_affect_fork_choice() {
⋮----
) = setup_default_forks(1, None::<GenerateVotes>);
⋮----
let (gossip_verified_vote_hash_sender, gossip_verified_vote_hash_receiver) = unbounded();
⋮----
let vote_bank = bank_forks.read().unwrap().get(vote_slot).unwrap();
⋮----
.send((vote_pubkey, vote_slot, vote_bank.hash()))
.expect("Send should succeed");
⋮----
.compute_bank_stats(
⋮----
fn test_replay_stage_refresh_last_vote() {
⋮----
} = replay_blockstore_components(None, 10, None::<GenerateVotes>);
⋮----
let mut tracked_vote_transactions = vec![];
let identity_keypair = cluster_info.keypair();
let my_vote_keypair = vec![Arc::new(
⋮----
let my_vote_pubkey = my_vote_keypair[0].pubkey();
⋮----
bank0.slot(),
⋮----
bank0.collector_id(),
⋮----
let (voting_sender, voting_receiver) = unbounded();
⋮----
bank1.fill_bank_with_ticks_for_tests();
⋮----
bank1.slot(),
⋮----
tower.record_bank_vote(&bank0);
⋮----
.recv_timeout(Duration::from_secs(1))
⋮----
let votes = cluster_info.get_votes(&mut cursor);
assert_eq!(votes.len(), 1);
⋮----
assert_eq!(vote_tx.message.recent_blockhash, bank0.last_blockhash());
⋮----
assert_eq!(tower.last_voted_slot().unwrap(), 0);
bank1.process_transaction(vote_tx).unwrap();
⋮----
bank2.fill_bank_with_ticks_for_tests();
bank2.freeze();
⋮----
bank2.slot(),
⋮----
bank2.collector_id(),
⋮----
for refresh_bank in &[bank1.clone(), bank2.clone()] {
⋮----
.get_fork_stats_mut(refresh_bank.slot())
⋮----
assert!(!ReplayStage::maybe_refresh_last_vote(
⋮----
assert!(votes.is_empty());
⋮----
tower.record_bank_vote(&bank1);
⋮----
assert_eq!(vote_tx.message.recent_blockhash, bank1.last_blockhash());
⋮----
assert_eq!(tower.last_voted_slot().unwrap(), 1);
⋮----
.get_fork_stats_mut(bank1.slot())
⋮----
.block_height = bank1.block_height();
⋮----
.get_fork_stats_mut(bank2.slot())
⋮----
.block_height = bank2.block_height();
⋮----
let mut parent_bank = bank2.clone();
⋮----
let slot = parent_bank.slot() + 1;
⋮----
parent_bank.fill_bank_with_ticks_for_tests();
parent_bank.freeze();
⋮----
expired_bank.slot(),
⋮----
expired_bank.collector_id(),
⋮----
.get_fork_stats_mut(expired_bank.slot())
⋮----
.block_height = expired_bank.block_height();
⋮----
.checked_sub(Duration::from_millis(
⋮----
assert!(ReplayStage::maybe_refresh_last_vote(
⋮----
assert!(last_vote_refresh_time.last_refresh_time > clone_refresh_time);
⋮----
let expired_bank_child_slot = expired_bank.slot() + 1;
⋮----
expired_bank.clone(),
⋮----
expired_bank_child.process_transaction(vote_tx).unwrap();
⋮----
.get_vote_account(&my_vote_pubkey)
⋮----
expired_bank_child.fill_bank_with_ticks_for_tests();
expired_bank_child.freeze();
⋮----
let slot = expired_bank_child.slot() + i + 1;
⋮----
Some(expired_bank_sibling),
⋮----
fn send_vote_in_new_bank(
⋮----
let my_vote_pubkey = &my_vote_keypair[0].pubkey();
tower.record_bank_vote(&parent_bank);
⋮----
let votes = cluster_info.get_votes(cursor);
⋮----
assert_eq!(tower.last_voted_slot().unwrap(), parent_bank.slot());
⋮----
bank.fill_bank_with_ticks_for_tests();
⋮----
bank.process_transaction(vote_tx).unwrap();
⋮----
progress.entry(my_slot).or_insert_with(|| {
⋮----
bank_forks.read().unwrap().get(my_slot).unwrap()
⋮----
fn test_replay_stage_last_vote_outside_slot_hashes() {
⋮----
other_fork_bank.fill_bank_with_ticks_for_tests();
other_fork_bank.freeze();
progress.entry(other_fork_slot).or_insert_with(|| {
⋮----
&my_vote_keypair[0].pubkey(),
⋮----
let mut new_bank = send_vote_in_new_bank(
⋮----
new_bank = send_vote_in_new_bank(
new_bank.clone(),
new_bank.slot() + 1,
⋮----
let last_voted_slot = tower.last_voted_slot().unwrap();
while new_bank.is_in_slot_hashes_history(&last_voted_slot) {
let new_slot = new_bank.slot() + 1;
⋮----
progress.entry(new_slot).or_insert_with(|| {
⋮----
new_bank = bank_forks.read().unwrap().get(new_slot).unwrap();
⋮----
let tip_of_voted_fork = new_bank.slot();
⋮----
&bank_forks.read().unwrap().ancestors(),
⋮----
assert_eq!(tower.last_voted_slot(), Some(last_voted_slot));
assert_eq!(progress.my_latest_landed_vote(tip_of_voted_fork), Some(0));
let other_fork_bank = &bank_forks.read().unwrap().get(other_fork_slot).unwrap();
let SelectVoteAndResetForkResult { vote_bank, .. } = select_vote_and_reset_forks(
⋮----
Some(&new_bank),
⋮----
&bank_forks.read().unwrap().descendants(),
⋮----
assert!(vote_bank.is_some());
assert_eq!(vote_bank.unwrap().0.slot(), tip_of_voted_fork);
let last_voted_bank = &bank_forks.read().unwrap().get(last_voted_slot).unwrap();
⋮----
Some(last_voted_bank),
⋮----
let last_voted_bank_plus_1 = &bank_forks.read().unwrap().get(last_voted_slot + 1).unwrap();
⋮----
Some(last_voted_bank_plus_1),
⋮----
.entry(new_bank.slot())
.and_modify(|s| s.fork_stats.my_latest_landed_vote = Some(last_voted_slot));
assert!(!new_bank.is_in_slot_hashes_history(&last_voted_slot));
⋮----
fn test_retransmit_latest_unpropagated_leader_slot() {
⋮----
let (retransmit_slots_sender, retransmit_slots_receiver) = unbounded();
⋮----
progress.get_retransmit_info_mut(0).unwrap().retry_time = Instant::now();
⋮----
let res = retransmit_slots_receiver.recv_timeout(Duration::from_millis(10));
assert_matches!(res, Err(_));
⋮----
progress.get_retransmit_info_mut(0).unwrap().retry_time = Instant::now()
.checked_sub(Duration::from_millis(RETRANSMIT_BASE_DELAY_MS + 1))
⋮----
.checked_sub(Duration::from_millis(2 * RETRANSMIT_BASE_DELAY_MS + 1))
⋮----
.get_retransmit_info_mut(0)
⋮----
.increment_retry_iteration();
⋮----
.checked_sub(Duration::from_millis(8 * RETRANSMIT_BASE_DELAY_MS + 1))
⋮----
fn receive_slots(retransmit_slots_receiver: &Receiver<Slot>) -> Vec<Slot> {
⋮----
while let Ok(slot) = retransmit_slots_receiver.recv_timeout(Duration::from_millis(10)) {
slots.push(slot);
⋮----
fn test_maybe_retransmit_unpropagated_slots() {
⋮----
progress.get_retransmit_info_mut(0).unwrap().retry_time = retry_time;
⋮----
for i in (1..10).chain(11..15) {
⋮----
bank_forks.read().unwrap().get(prev_index).unwrap(),
&leader_schedule_cache.slot_leader_at(i, None).unwrap(),
⋮----
bank.collector_id(),
⋮----
.get(bank.collector_id())
⋮----
assert!(progress.get_propagated_stats(i).unwrap().is_leader_slot);
⋮----
bank_forks.write().unwrap().insert(bank);
⋮----
progress.get_retransmit_info_mut(i).unwrap().retry_time = retry_time;
⋮----
let received_slots = receive_slots(&retransmit_slots_receiver);
assert_eq!(received_slots, vec![0]);
⋮----
assert_eq!(received_slots, vec![4, 5, 6]);
⋮----
assert_eq!(received_slots, vec![8, 9, 11]);
⋮----
fn test_dumped_slot_not_causing_panic() {
⋮----
let (retransmit_slots_sender, _) = unbounded();
⋮----
.find(|i| leader_schedule_cache.slot_leader_at(*i, None) != Some(*my_pubkey))
⋮----
.slot_leader_at(slot_to_dump, None)
⋮----
bank_to_dump.collector_id(),
⋮----
.get(bank_to_dump.collector_id())
⋮----
assert!(progress.get_propagated_stats(slot_to_dump).is_some());
bank_to_dump.freeze();
bank_forks.write().unwrap().insert(bank_to_dump);
⋮----
.get(slot_to_dump)
.expect("Just inserted");
⋮----
.reset_sync(bank_to_dump, Some((slot_to_dump + 1, slot_to_dump + 1)))
⋮----
assert_eq!(poh_recorder.read().unwrap().start_slot(), slot_to_dump);
⋮----
duplicate_slots_to_repair.insert(slot_to_dump, bank_to_dump_bad_hash);
⋮----
let (banking_tracer, _) = BankingTracer::new(None).unwrap();
⋮----
assert!(ReplayStage::maybe_start_leader(
⋮----
fn test_dump_own_slots_fails() {
⋮----
let (dumped_slots_sender, _) = unbounded();
⋮----
fn run_compute_and_select_forks(
⋮----
let ancestors = &bank_forks.read().unwrap().ancestors();
let descendants = &bank_forks.read().unwrap().descendants();
⋮----
&my_vote_pubkey.unwrap_or_default(),
⋮----
.select_forks(&frozen_banks, tower, progress, ancestors, bank_forks);
⋮----
vote_bank.map(|(b, _)| b.slot()),
reset_bank.map(|b| b.slot()),
⋮----
type GenerateVotes = Box<dyn Fn(Vec<Pubkey>) -> HashMap<Pubkey, Vec<Slot>>>;
pub fn setup_forks_from_tree(
⋮----
.map(|k| k.node_keypair.pubkey())
⋮----
.map(|generate_votes| generate_votes(pubkeys))
⋮----
vote_simulator.fill_bank_forks(tree.clone(), &cluster_votes, true);
⋮----
let blockstore = Blockstore::open(&ledger_path).unwrap();
blockstore.add_tree(tree, false, true, 2, Hash::default());
⋮----
fn setup_default_forks(
⋮----
let tree = tr(0) / (tr(1) / (tr(2) / (tr(4))) / (tr(3) / (tr(5) / (tr(6)))));
setup_forks_from_tree(tree, num_keys, generate_votes)
⋮----
fn check_map_eq<K: Eq + std::hash::Hash + std::fmt::Debug, T: PartialEq + std::fmt::Debug>(
⋮----
map1.len() == map2.len() && map1.iter().all(|(k, v)| map2.get(k).unwrap() == v)
⋮----
fn wait_for_poh_service(poh_controller: &PohController) {
while poh_controller.has_pending_message() {
⋮----
fn test_check_for_vote_only_mode() {
⋮----
assert!(in_vote_only_mode.load(Ordering::Relaxed));
⋮----
assert!(!in_vote_only_mode.load(Ordering::Relaxed));
⋮----
fn test_tower_sync_from_bank_failed_switch() {
⋮----
.zip(iter::once(vec![0, 1, 3, 5, 6]).chain(iter::repeat_n(vec![0, 1, 2, 4], 2)))
⋮----
setup_default_forks(3, Some(Box::new(generate_votes)));
⋮----
let bank_hash = |slot| bank_forks.read().unwrap().bank_hash(slot).unwrap();
⋮----
tower.record_vote(0, bank_hash(0));
tower.record_vote(1, bank_hash(1));
let (vote_fork, reset_fork, failures) = run_compute_and_select_forks(
⋮----
Some(my_vote_pubkey),
⋮----
assert_eq!(vote_fork, None);
assert_eq!(reset_fork, Some(6));
⋮----
fn test_tower_sync_from_bank_failed_lockout() {
⋮----
.zip(iter::once(vec![0, 1, 2, 5, 6]).chain(iter::repeat_n(vec![0, 1, 3, 4], 2)))
⋮----
let tree = tr(0) / (tr(1) / (tr(3) / (tr(4))) / (tr(2) / (tr(5) / (tr(6)))));
⋮----
setup_forks_from_tree(tree, 3, Some(Box::new(generate_votes)));
⋮----
assert_eq!(reset_fork, Some(4));
assert_eq!(failures, vec![HeaviestForkFailures::LockedOut(4),]);
⋮----
fn test_tower_adopt_from_bank_cache_only_computed() {
⋮----
let fork_stats_3 = progress.get_fork_stats_mut(3).unwrap();
fork_stats_3.vote_threshold = vec![ThresholdDecision::FailedThreshold(4, 4000)];
⋮----
let fork_stats_4 = progress.get_fork_stats_mut(4).unwrap();
⋮----
let bank_6 = frozen_banks.get(6).unwrap();
assert_eq!(bank_6.slot(), 6);
⋮----
let fork_stats_3 = progress.get_fork_stats(3).unwrap();
assert!(fork_stats_3.vote_threshold.is_empty());
assert!(fork_stats_3.is_locked_out);
assert!(fork_stats_3.computed);
let fork_stats_4 = progress.get_fork_stats(4).unwrap();
assert!(!fork_stats_4.is_locked_out);
assert!(!fork_stats_4.computed);
⋮----
fn test_tower_load_missing() {
let tower_file = tempdir().unwrap().keep();
⋮----
assert_eq!(tower.vote_state, expected_tower.vote_state);
assert_eq!(tower.node_pubkey, node_pubkey);
⋮----
fn test_tower_load() {
⋮----
let node_pubkey = node_keypair.pubkey();
⋮----
expected_tower.save(&tower_storage, &node_keypair).unwrap();
⋮----
assert_eq!(tower.node_pubkey, expected_tower.node_pubkey);
⋮----
fn test_initialize_progress_and_fork_choice_with_duplicates() {
⋮----
} = create_genesis_config(123);
⋮----
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
fill_blockstore_slot_with_ticks(&blockstore, ticks_per_slot, i + 1, i, last_hash);
⋮----
blockstore.set_roots([3, 4].iter()).unwrap();
⋮----
let bank0 = bank_forks.read().unwrap().get_with_scheduler(0).unwrap();
⋮----
process_bank_0(
⋮----
blockstore.store_duplicate_slot(1, vec![], vec![]).unwrap();
blockstore.store_duplicate_slot(3, vec![], vec![]).unwrap();
⋮----
let bank1 = bank_forks.write().unwrap().insert(Bank::new_from_parent(
bank0.clone_without_scheduler(),
⋮----
confirm_full_slot(
⋮----
&mut ConfirmationProgress::new(bank0.last_blockhash()),
⋮----
bank_forks.write().unwrap().set_root(1, None, None);
⋮----
assert_eq!(bank_forks.read().unwrap().root(), 4);
⋮----
assert_eq!(fork_choice.tree_root().0, 4);
assert!(fork_choice
⋮----
assert!(!fork_choice
⋮----
fn test_skip_leader_slot_for_existing_slot() {
⋮----
} = replay_blockstore_components(None, 1, None);
⋮----
assert!(working_bank.is_complete());
assert!(working_bank.is_frozen());
let dummy_slot = working_bank.slot() + 2;
let initial_slot = working_bank.slot();
⋮----
let (shreds, _) = make_slot_entries(dummy_slot, initial_slot, num_entries);
⋮----
working_bank.clone(),
⋮----
wait_for_poh_service(&poh_controller);
⋮----
(working_bank.ticks_per_slot() * working_bank.hashes_per_tick().unwrap()) + 1;
⋮----
.map(|mut poh_recorder| {
⋮----
poh_recorder.tick();
⋮----
assert_eq!(working_bank.slot(), good_slot);
assert_eq!(working_bank.parent_slot(), initial_slot);
⋮----
fn test_mark_slots_duplicate_confirmed() {
⋮----
let (ancestor_hashes_replay_update_sender, _) = unbounded();
⋮----
let bank_hash_0 = bank_forks.read().unwrap().bank_hash(0).unwrap();
⋮----
assert!(!duplicate_confirmed_slots.contains_key(&0));
let bank_hash_5 = bank_forks.read().unwrap().bank_hash(5).unwrap();
⋮----
assert_eq!(*duplicate_confirmed_slots.get(&5).unwrap(), bank_hash_5);
assert!(tbft_structs
⋮----
let bank_hash_6 = bank_forks.read().unwrap().bank_hash(6).unwrap();
⋮----
assert_eq!(*duplicate_confirmed_slots.get(&6).unwrap(), bank_hash_6);
⋮----
fn test_process_duplicate_confirmed_slots(same_batch: bool) {
⋮----
let (sender, receiver) = unbounded();
⋮----
sender.send(vec![(0, bank_hash_0)]).unwrap();
⋮----
sender.send(vec![(5, bank_hash_5)]).unwrap();
⋮----
.send(vec![(5, bank_hash_5), (6, bank_hash_6)])
⋮----
sender.send(vec![(6, bank_hash_6)]).unwrap();
⋮----
sender.send(vec![(6, Hash::new_unique())]).unwrap();
⋮----
fn test_alpenglow_poh_migration_from_leader() {
⋮----
let poh_slot = working_bank.slot() + 2;
let alpenglow_slot = working_bank.slot() + 3;
⋮----
assert!(!is_alpenglow_migration_complete);
⋮----
assert_eq!(working_bank.slot(), poh_slot);
⋮----
assert!(is_alpenglow_migration_complete);
⋮----
fn test_alpenglow_poh_migration_from_replay() {
⋮----
} = replay_blockstore_components(Some(tr(0) / tr(1) / tr(2) / tr(3) / tr(4)), 1, None);
⋮----
let (cluster_slots_update_sender, _cluster_slots_update_receiver) = unbounded();
let (cost_update_sender, _cost_update_receiver) = unbounded();
⋮----
let poh_slot = bank_forks.read().unwrap().highest_slot() + 1;
let first_alpenglow_slot = bank_forks.read().unwrap().highest_slot() + 2;
⋮----
let parent_bank = bank_forks.read().unwrap().working_bank();
⋮----
poh_bank.set_tick_height(poh_bank.max_tick_height());
⋮----
poh_bank.collector_id(),
⋮----
bank_forks.write().unwrap().insert(poh_bank);
let replay_result_vec = vec![ReplaySlotFromBlockstore {
⋮----
Some(first_alpenglow_slot),
⋮----
Some(&mut tbft_structs),
⋮----
ag_bank.set_tick_height(ag_bank.max_tick_height());
⋮----
ag_bank.collector_id(),
⋮----
bank_forks.write().unwrap().insert(ag_bank);

================
File: core/src/resource_limits.rs
================
pub enum ResourceLimitError {
⋮----
pub fn adjust_nofile_limit(_enforce_nofile_limit: bool) -> Result<(), ResourceLimitError> {
Ok(())
⋮----
pub fn adjust_nofile_limit(enforce_nofile_limit: bool) -> Result<(), ResourceLimitError> {
⋮----
fn get_nofile() -> libc::rlimit {
⋮----
warn!("getrlimit(RLIMIT_NOFILE) failed");
⋮----
let mut nofile = get_nofile();
⋮----
if cfg!(target_os = "macos") {
error!(
⋮----
error!("{error}");
⋮----
return Err(error);
⋮----
nofile = get_nofile();
⋮----
info!("Maximum open file descriptors: {}", nofile.rlim_cur);
⋮----
fn adjust_ulimit_memlock(min_required: usize) -> io::Result<()> {
fn get_memlock() -> libc::rlimit {
⋮----
let mut memlock = get_memlock();
⋮----
return Err(io::Error::new(
⋮----
memlock = get_memlock();
⋮----
pub fn validate_memlock_limit_for_disk_io(required_size: usize) -> io::Result<()> {
⋮----
adjust_ulimit_memlock(required_size)

================
File: core/src/result.rs
================
pub enum Error {
⋮----
pub type Result<T> = std::result::Result<T, Error>;
⋮----
fn from(_e: crossbeam_channel::ReadyTimeoutError) -> Error {
⋮----
fn from(_e: crossbeam_channel::TrySendError<T>) -> Error {
⋮----
fn from(_e: crossbeam_channel::SendError<T>) -> Error {
⋮----
mod tests {
⋮----
fn send_error() -> Result<()> {
let (s, r) = unbounded();
drop(r);
s.send(())?;
Ok(())
⋮----
fn from_test() {
assert_matches!(Error::from(RecvError {}), Error::Recv(_));
assert_matches!(
⋮----
assert_matches!(send_error(), Err(Error::Send));
⋮----
assert_matches!(Error::from(ioe), Error::Io(_));
⋮----
fn fmt_test() {
write!(io::sink(), "{:?}", Error::from(RecvError {})).unwrap();
write!(io::sink(), "{:?}", Error::from(RecvTimeoutError::Timeout)).unwrap();
write!(io::sink(), "{:?}", send_error()).unwrap();
write!(
⋮----
.unwrap();

================
File: core/src/sample_performance_service.rs
================
pub struct SamplePerformanceService {
⋮----
impl SamplePerformanceService {
pub fn new(
⋮----
let bank_forks = bank_forks.clone();
⋮----
.name("solSamplePerf".to_string())
.spawn(move || {
info!("SamplePerformanceService has started");
⋮----
info!("SamplePerformanceService has stopped");
⋮----
.unwrap();
⋮----
fn run(bank_forks: Arc<RwLock<BankForks>>, blockstore: Arc<Blockstore>, exit: Arc<AtomicBool>) {
⋮----
while !exit.load(Ordering::Relaxed) {
let elapsed = last_sample_time.elapsed();
⋮----
new_snapshot.diff_since(&snapshot);
⋮----
sample_period_secs: elapsed.as_secs() as u16,
⋮----
if let Err(e) = blockstore.write_perf_sample(highest_slot, &perf_sample) {
error!("write_perf_sample failed: slot {highest_slot:?} {e:?}");
⋮----
sleep(SLEEP_INTERVAL);
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
struct StatsSnapshot {
⋮----
impl StatsSnapshot {
fn from_forks(forks: &RwLock<BankForks>) -> Self {
let forks = forks.read().unwrap();
let bank = forks.root_bank();
⋮----
num_transactions: bank.transaction_count(),
num_non_vote_transactions: bank.non_vote_transaction_count_since_restart(),
highest_slot: forks.highest_slot(),
⋮----
fn diff_since(&self, predecessor: &Self) -> (u64, u64, u64) {
⋮----
.saturating_sub(predecessor.num_transactions),
⋮----
.saturating_sub(predecessor.num_non_vote_transactions),
self.highest_slot.saturating_sub(predecessor.highest_slot),

================
File: core/src/scheduler_bindings_server.rs
================
pub(crate) fn spawn(path: &Path, session_sender: mpsc::Sender<BankingControlMsg>) {
⋮----
let mut listener = handshake::server::Server::new(path).unwrap();
⋮----
.name("solBindingSrv".to_string())
.spawn(move || loop {
match listener.accept() {
⋮----
.blocking_send(BankingControlMsg::External { session })
.is_err()
⋮----
error!("External scheduler handshake failed; err={err}")
⋮----
.unwrap();

================
File: core/src/shred_fetch_stage.rs
================
pub(crate) struct ShredFetchStage {
⋮----
struct RepairContext {
⋮----
impl ShredFetchStage {
fn modify_packets(
⋮----
// Only repair shreds need repair context.
debug_assert_eq!(
⋮----
let mut keypair = repair_context.as_ref().copied().map(RepairContext::keypair);
⋮----
let root_bank = sharable_banks.root();
⋮----
root_bank.slot(),
root_bank.get_slots_in_epoch(root_bank.epoch()),
root_bank.feature_set.clone(),
root_bank.epoch_schedule().clone(),
sharable_banks.working().slot(),
⋮----
if last_updated.elapsed().as_millis() as u64 > DEFAULT_MS_PER_SLOT {
⋮----
last_slot = sharable_banks.working().slot();
⋮----
feature_set = root_bank.feature_set.clone();
epoch_schedule = root_bank.epoch_schedule().clone();
last_root = root_bank.slot();
slots_per_epoch = root_bank.get_slots_in_epoch(root_bank.epoch());
keypair = repair_context.as_ref().copied().map(RepairContext::keypair);
⋮----
stats.shred_count += packet_batch.len();
⋮----
debug_assert_eq!(flags, PacketFlags::REPAIR);
debug_assert!(keypair.is_some());
⋮----
// Discard packets if repair nonce does not verify.
⋮----
repair_context.outstanding_repair_requests.write().unwrap();
⋮----
.iter_mut()
.filter(|packet| !packet.meta().discard())
.for_each(|mut packet| {
// Have to set repair flag here so that the nonce is
// taken off the shred's payload.
packet.meta_mut().flags |= PacketFlags::REPAIR;
if !verify_repair_nonce(
packet.as_ref(),
⋮----
packet.meta_mut().set_discard(true);
⋮----
let max_slot = last_slot + MAX_SHRED_DISTANCE_MINIMUM.max(2 * slots_per_epoch);
⋮----
check_feature_activation(
⋮----
let turbine_disabled = turbine_disabled.load(Ordering::Relaxed);
for mut packet in packet_batch.iter_mut().filter(|p| !p.meta().discard()) {
⋮----
|| should_discard_shred(
⋮----
packet.meta_mut().flags.insert(flags);
⋮----
if stats.maybe_submit(name, STATS_SUBMIT_CADENCE) {
if let Some(stats) = recvr_stats.as_ref() {
stats.report();
⋮----
if let Err(send_err) = sendr.try_send(packet_batch) {
⋮----
stats.overflow_shreds += v.len();
⋮----
_ => unreachable!("EvictingSender holds on to both ends of the channel"),
⋮----
fn packet_modifier(
⋮----
let sharable_banks = bank_forks.read().unwrap().sharable_banks();
⋮----
.into_iter()
.enumerate()
.map(|(i, socket)| {
⋮----
format!("{receiver_thread_name}{i:02}"),
⋮----
exit.clone(),
packet_sender.clone(),
recycler.clone(),
receiver_stats.clone(),
Some(Duration::from_millis(5)),
⋮----
.collect();
⋮----
.name(modifier_thread_name.to_string())
.spawn(move || {
⋮----
Some(receiver_stats),
⋮----
repair_context.as_ref(),
⋮----
.unwrap();
⋮----
pub(crate) fn new(
⋮----
repair_socket: repair_socket.clone(),
⋮----
sender.clone(),
⋮----
bank_forks.clone(),
⋮----
turbine_disabled.clone(),
⋮----
vec![repair_socket],
⋮----
Some(repair_context.clone()),
⋮----
tvu_threads.extend(repair_receiver);
tvu_threads.push(tvu_filter);
tvu_threads.push(repair_handler);
⋮----
let (packet_sender, packet_receiver) = unbounded();
let bank_forks = bank_forks.clone();
let exit = exit.clone();
let sender = sender.clone();
let turbine_disabled = turbine_disabled.clone();
tvu_threads.extend([
⋮----
.name("solTvuRecvRpr".to_string())
.spawn(|| {
receive_quic_datagrams(
⋮----
.unwrap(),
⋮----
.name("solTvuFetchRpr".to_string())
⋮----
Some(&repair_context),
⋮----
.name("solTvuRecvQuic".to_string())
⋮----
.name("solTvuFetchQuic".to_string())
⋮----
pub(crate) fn join(self) -> thread::Result<()> {
⋮----
thread_hdl.join()?;
⋮----
Ok(())
⋮----
impl RepairContext {
fn keypair(&self) -> Arc<Keypair> {
self.cluster_info.keypair()
⋮----
fn verify_repair_nonce(
⋮----
debug_assert!(packet.meta().flags.contains(PacketFlags::REPAIR));
⋮----
.register_response(nonce, shred, now, |_| ())
.is_some()
⋮----
pub(crate) fn receive_quic_datagrams(
⋮----
while !exit.load(Ordering::Relaxed) {
let entry = match quic_datagrams_receiver.recv_timeout(RECV_TIMEOUT) {
⋮----
let entries = std::iter::once(entry).chain(
std::iter::repeat_with(|| quic_datagrams_receiver.recv_deadline(deadline).ok())
.while_some(),
⋮----
.filter(|(_, _, bytes)| bytes.len() <= PACKET_DATA_SIZE)
.map(|(_pubkey, addr, bytes)| {
⋮----
meta.size = bytes.len();
meta.addr = addr.ip();
meta.port = addr.port();
⋮----
if !packet_batch.is_empty() && sender.send(packet_batch.into()).is_err() {
⋮----
fn check_feature_activation(
⋮----
match feature_set.activated_slot(feature) {
⋮----
let feature_epoch = epoch_schedule.get_epoch(feature_slot);
let shred_epoch = epoch_schedule.get_epoch(shred_slot);

================
File: core/src/sigverify_stage.rs
================
pub enum SigVerifyServiceError<SendType> {
⋮----
type Result<T, SendType> = std::result::Result<T, SigVerifyServiceError<SendType>>;
pub struct SigVerifyStage {
⋮----
pub trait SigVerifier {
⋮----
pub struct DisabledSigVerifier {}
⋮----
struct SigVerifierStats {
⋮----
impl SigVerifierStats {
fn maybe_report(&self, name: &'static str) {
// No need to report a datapoint if no batches/packets received
⋮----
datapoint_info!(
⋮----
impl SigVerifier for DisabledSigVerifier {
type SendType = ();
fn verify_batches(
⋮----
fn send_packets(&mut self, _packet_batches: Vec<PacketBatch>) -> Result<(), Self::SendType> {
Ok(())
⋮----
impl SigVerifyStage {
pub fn new<T: SigVerifier + 'static + Send>(
⋮----
pub fn discard_excess_packets(batches: &mut [PacketBatch], mut max_packets: usize) {
⋮----
.iter_mut()
.rev()
.flat_map(|batch| batch.iter_mut().rev())
.filter(|packet| !packet.meta().discard())
.map(|packet| (packet.meta().addr, packet))
.into_group_map();
while max_packets > 0 && !addrs.is_empty() {
let num_addrs = addrs.len();
addrs.retain(|_, packets| {
let cap = max_packets.div_ceil(num_addrs);
max_packets -= packets.len().min(cap);
packets.truncate(packets.len().saturating_sub(cap));
!packets.is_empty()
⋮----
for mut packet in addrs.into_values().flatten() {
packet.meta_mut().set_discard(true);
⋮----
pub fn maybe_shrink_batches(
⋮----
let num_packets = count_packets_in_batches(&packet_batches);
let num_discarded_packets = count_discarded_packets(&packet_batches);
let pre_packet_batches_len = packet_batches.len();
⋮----
shrink_batches(packet_batches)
⋮----
let post_packet_batches_len = packet_batches.len();
let shrink_total = pre_packet_batches_len.saturating_sub(post_packet_batches_len);
shrink_time.stop();
(shrink_time.as_us(), shrink_total, packet_batches)
⋮----
fn verifier<const K: usize, T: SigVerifier>(
⋮----
let batches_len = batches.len();
debug!(
⋮----
let num_discarded_randomly = num_packets.saturating_sub(non_discarded_packets);
discard_random_time.stop();
⋮----
dedup_time.stop();
let num_unique = non_discarded_packets.saturating_sub(discard_or_dedup_fail);
⋮----
let excess_fail = num_unique.saturating_sub(MAX_SIGVERIFY_BATCH);
discard_time.stop();
⋮----
let batches = verifier.verify_batches(batches, num_packets_to_verify);
let num_valid_packets = count_valid_packets(&batches);
verify_time.stop();
⋮----
verifier.send_packets(batches)?;
⋮----
.increment(recv_duration.as_micros() as u64)
.unwrap();
⋮----
.increment(verify_time.as_us() / (num_packets as u64))
⋮----
.increment(discard_time.as_us() / (num_packets as u64))
⋮----
.increment(dedup_time.as_us() / (num_packets as u64))
⋮----
stats.batches_hist.increment(batches_len as u64).unwrap();
stats.packets_hist.increment(num_packets as u64).unwrap();
⋮----
stats.total_discard_random_time_us += discard_random_time.as_us() as usize;
⋮----
stats.total_dedup_time_us += dedup_time.as_us() as usize;
stats.total_discard_time_us += discard_time.as_us() as usize;
stats.total_verify_time_us += verify_time.as_us() as usize;
⋮----
fn verifier_service<T: SigVerifier + 'static + Send>(
⋮----
.name(thread_name.to_string())
.spawn(move || {
⋮----
if deduper.maybe_reset(&mut rng, DEDUPER_FALSE_POSITIVE_RATE, MAX_DEDUPER_AGE) {
⋮----
_ => error!("{e:?}"),
⋮----
if last_print.elapsed().as_secs() > 2 {
stats.maybe_report(metrics_name);
⋮----
.unwrap()
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
mod tests {
⋮----
fn count_non_discard(packet_batches: &[PacketBatch]) -> usize {
⋮----
.iter()
.flatten()
.filter(|p| !p.meta().discard())
.count()
⋮----
fn test_packet_discard() {
⋮----
batch.resize(batch_size, packet);
batch[3].meta_mut().addr = std::net::IpAddr::from([1u16; 8]);
batch[3].meta_mut().set_discard(true);
batch[4].meta_mut().addr = std::net::IpAddr::from([2u16; 8]);
let mut batches = vec![PacketBatch::from(batch)];
⋮----
let total_non_discard = count_non_discard(&batches);
assert_eq!(total_non_discard, max);
assert!(!batches[0].get(0).unwrap().meta().discard());
assert!(batches[0].get(3).unwrap().meta().discard());
assert!(!batches[0].get(4).unwrap().meta().discard());
⋮----
fn gen_batches(
⋮----
let tx = test_tx();
to_packet_batches(&vec![tx; total_packets], packets_per_batch)
⋮----
let txs: Vec<_> = (0..total_packets).map(|_| test_tx()).collect();
to_packet_batches(&txs, packets_per_batch)
⋮----
fn test_sigverify_stage_with_same_tx() {
test_sigverify_stage(true)
⋮----
fn test_sigverify_stage_without_same_tx() {
test_sigverify_stage(false)
⋮----
fn test_sigverify_stage(use_same_tx: bool) {
⋮----
trace!("start");
let (packet_s, packet_r) = unbounded();
⋮----
// This is important so that we don't discard any packets and fail asserts below about
assert!(total_packets < MAX_SIGVERIFY_BATCH);
let batches = gen_batches(use_same_tx, packets_per_batch, total_packets);
trace!(
⋮----
for batch in batches.into_iter() {
sent_len += batch.len();
assert_eq!(batch.len(), packets_per_batch);
packet_s.send(batch).unwrap();
⋮----
let mut packet_s = Some(packet_s);
⋮----
trace!("sent: {sent_len}");
⋮----
if let Ok(verifieds) = verified_r.recv() {
⋮----
.map(|batch| batch.iter().filter(|p| !p.meta().discard()).count())
⋮----
if packet_s.as_ref().map(|s| s.is_empty()).unwrap_or(true) {
packet_s.take();
⋮----
trace!("received: {valid_received}");
⋮----
assert_eq!(valid_received, 1);
⋮----
assert_eq!(valid_received, total_packets);
⋮----
stage.join().unwrap();
⋮----
fn test_maybe_shrink_batches() {
⋮----
let batches = gen_batches(true, packets_per_batch, total_packets);
let num_generated_batches = batches.len();
let num_packets = count_packets_in_batches(&batches);
⋮----
assert_eq!(num_shrunk_batches, 0);
⋮----
batches.iter_mut().for_each(|batch| {
batch.iter_mut().for_each(|mut p| {
⋮----
p.meta_mut().set_discard(true);
⋮----
.last_mut()
⋮----
.first_mut()
⋮----
.meta_mut()
.set_discard(true);
⋮----
1.max((num_generated_batches as f64 * MAX_DISCARDED_PACKET_RATE) as usize);
⋮----
assert_eq!(num_shrunk_batches, expected_num_shrunk_batches);
⋮----
assert_eq!(batches.len(), expected_remaining_batches);

================
File: core/src/sigverify.rs
================
pub struct TransactionSigVerifier {
⋮----
impl TransactionSigVerifier {
pub fn new_reject_non_vote(
⋮----
pub fn new(
⋮----
impl SigVerifier for TransactionSigVerifier {
type SendType = BankingPacketBatch;
fn send_packets(
⋮----
.send(banking_packet_batch.clone())?;
⋮----
forward_stage_sender.try_send((banking_packet_batch, self.reject_non_vote))
⋮----
warn!("forwarding stage channel is full, dropping packets.");
⋮----
self.banking_stage_sender.send(banking_packet_batch)?;
⋮----
Ok(())
⋮----
fn verify_batches(

================
File: core/src/snapshot_packager_service.rs
================
mod snapshot_gossip_manager;
⋮----
pub struct SnapshotPackagerService {
⋮----
impl SnapshotPackagerService {
⋮----
pub fn new(
⋮----
.name("solSnapshotPkgr".to_string())
.spawn(move || {
⋮----
exit_backpressure.store(true, Ordering::Relaxed);
⋮----
info!("{} has started", Self::NAME);
let snapshot_config = snapshot_controller.snapshot_config();
renice_this_thread(snapshot_config.packager_thread_niceness_adj).unwrap();
⋮----
.then(|| SnapshotGossipManager::new(cluster_info, starting_snapshot_hashes));
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
info!("Received exit request, tearing down...");
let (_, dur) = meas_dur!(Self::teardown(
⋮----
info!("Teardown completed in {dur:?}.");
⋮----
info!("handling snapshot package: {snapshot_package:?}");
let enqueued_time = snapshot_package.enqueued.elapsed();
⋮----
if exit_backpressure.is_some() {
// With exit backpressure, we will delay flushing snapshot storages
// until we receive a graceful exit request.
// Save the snapshot storages here, so we can flush later (as needed).
teardown_state = Some(TeardownState {
⋮----
snapshot_storages: snapshot_package.snapshot_storages.clone(),
⋮----
// Archiving the snapshot package is not allowed to fail.
// AccountsBackgroundService calls `clean_accounts()` with a value for
// latest_full_snapshot_slot that requires this archive call to succeed.
⋮----
measure_us!(snapshot_utils::serialize_and_archive_snapshot_package(
⋮----
// Without exit backpressure, always flush the snapshot storages,
// which is required for fastboot.
⋮----
error!(
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
if let Some(snapshot_gossip_manager) = snapshot_gossip_manager.as_mut() {
⋮----
.push_snapshot_hash(snapshot_kind, (snapshot_slot, snapshot_hash));
⋮----
measure_us!(snapshot_utils::purge_old_snapshot_archives(
⋮----
// Now that this snapshot package has been archived, it is safe to remove
// all bank snapshots older than this slot.  We want to keep the bank
// snapshot *at this slot* so that it can be used during restarts, when
// booting from local state.
⋮----
measure_us!(snapshot_utils::purge_bank_snapshots_older_than_slot(
⋮----
let handling_time_us = measure_handling.end_as_us();
datapoint_info!(
⋮----
info!("{} has stopped", Self::NAME);
⋮----
exit_backpressure.store(false, Ordering::Relaxed);
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.t_snapshot_packager.join()
⋮----
fn get_next_snapshot_package(
⋮----
pending_snapshot_packages.lock().unwrap().pop()
⋮----
fn teardown(state: &TeardownState, snapshot_config: &SnapshotConfig) {
info!("Flushing account storages...");
⋮----
let result = storage.flush();
⋮----
warn!(
⋮----
info!("Flushing account storages... Done in {:?}", start.elapsed());
⋮----
info!("Hard linking account storages...");
⋮----
warn!("Failed to hard link account storages: {err}");
⋮----
info!(
⋮----
info!("Saving obsolete accounts...");
⋮----
warn!("Failed to serialize obsolete accounts: {err}");
⋮----
info!("Saving obsolete accounts... Done in {:?}", start.elapsed());
⋮----
warn!("Failed to mark bank snapshot as loadable: {err}");
⋮----
struct TeardownState {

================
File: core/src/staked_nodes_updater_service.rs
================
pub struct StakedNodesUpdaterService {
⋮----
impl StakedNodesUpdaterService {
pub fn new(
⋮----
.name("solStakedNodeUd".to_string())
.spawn(move || {
while !exit.load(Ordering::Relaxed) {
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
root_bank.current_epoch_staked_nodes()
⋮----
let overrides = staked_nodes_overrides.read().unwrap().clone();
*staked_nodes.write().unwrap() = StakedNodes::new(stakes, overrides);
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()

================
File: core/src/stats_reporter_service.rs
================
pub struct StatsReporterService {
⋮----
impl StatsReporterService {
pub fn new(
⋮----
.name("solStatsReport".to_owned())
.spawn(move || loop {
if exit.load(Ordering::Relaxed) {
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()?;
Ok(())
⋮----
fn receive_reporting_func(
⋮----
let func = r.recv_timeout(timer)?;
func();

================
File: core/src/system_monitor_service.rs
================
pub struct SystemMonitorService {
⋮----
struct UdpStats {
⋮----
struct NetDevStats {
⋮----
struct NetStats {
⋮----
struct CpuInfo {
⋮----
enum CpuManufacturer {
⋮----
enum CpuidParamValue {
⋮----
struct DiskStats {
⋮----
impl UdpStats {
fn from_map(udp_stats: &HashMap<String, u64>) -> Self {
⋮----
in_datagrams: *udp_stats.get("InDatagrams").unwrap_or(&0),
no_ports: *udp_stats.get("NoPorts").unwrap_or(&0),
in_errors: *udp_stats.get("InErrors").unwrap_or(&0),
out_datagrams: *udp_stats.get("OutDatagrams").unwrap_or(&0),
rcvbuf_errors: *udp_stats.get("RcvbufErrors").unwrap_or(&0),
sndbuf_errors: *udp_stats.get("SndbufErrors").unwrap_or(&0),
in_csum_errors: *udp_stats.get("InCsumErrors").unwrap_or(&0),
ignored_multi: *udp_stats.get("IgnoredMulti").unwrap_or(&0),
⋮----
impl DiskStats {
⋮----
fn accumulate(&mut self, other: &DiskStats) {
⋮----
fn platform_id() -> String {
format!(
⋮----
fn read_net_stats() -> Result<NetStats, String> {
⋮----
let file_snmp = File::open(file_path_snmp).map_err(|e| e.to_string())?;
⋮----
let file_dev = File::open(file_path_dev).map_err(|e| e.to_string())?;
⋮----
let udp_stats = parse_udp_stats(&mut reader_snmp)?;
let net_dev_stats = parse_net_dev_stats(&mut reader_dev)?;
Ok(NetStats {
⋮----
fn parse_udp_stats(reader_snmp: &mut impl BufRead) -> Result<UdpStats, String> {
⋮----
for line in reader_snmp.lines() {
let line = line.map_err(|e| e.to_string())?;
if line.starts_with("Udp:") {
udp_lines.push(line);
if udp_lines.len() == 2 {
⋮----
if udp_lines.len() != 2 {
return Err(format!(
⋮----
.split_ascii_whitespace()
.zip(udp_lines[1].split_ascii_whitespace())
.collect();
⋮----
.iter()
.map(|(label, val)| (label.to_string(), val.parse::<u64>().unwrap()))
⋮----
Ok(stats)
⋮----
fn parse_net_dev_stats(reader_dev: &mut impl BufRead) -> Result<NetDevStats, String> {
⋮----
for (line_number, line) in reader_dev.lines().enumerate() {
⋮----
let values: Vec<_> = line.split_ascii_whitespace().collect();
if values.len() != 17 {
return Err("parse error, expected exactly 17 stat elements".to_string());
⋮----
stats.rx_bytes += values[1].parse::<u64>().map_err(|e| e.to_string())?;
stats.rx_packets += values[2].parse::<u64>().map_err(|e| e.to_string())?;
stats.rx_errs += values[3].parse::<u64>().map_err(|e| e.to_string())?;
stats.rx_drops += values[4].parse::<u64>().map_err(|e| e.to_string())?;
stats.rx_fifo += values[5].parse::<u64>().map_err(|e| e.to_string())?;
stats.rx_frame += values[6].parse::<u64>().map_err(|e| e.to_string())?;
stats.rx_compressed += values[7].parse::<u64>().map_err(|e| e.to_string())?;
stats.rx_multicast += values[8].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_bytes += values[9].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_packets += values[10].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_errs += values[11].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_drops += values[12].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_fifo += values[13].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_colls += values[14].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_carrier += values[15].parse::<u64>().map_err(|e| e.to_string())?;
stats.tx_compressed += values[16].parse::<u64>().map_err(|e| e.to_string())?;
⋮----
pub fn verify_net_stats_access() -> Result<(), String> {
read_net_stats()?;
Ok(())
⋮----
fn read_disk_stats() -> Result<DiskStats, String> {
⋮----
let blk_device_dir_iter = std::fs::read_dir(SYS_BLOCK_PATH).map_err(|e| e.to_string())?;
⋮----
.filter_map(|blk_device_dir| {
⋮----
let blk_device_dir_name = &blk_device_dir.file_name();
let blk_device_dir_name = blk_device_dir_name.to_string_lossy();
if blk_device_dir_name.starts_with("loop")
|| blk_device_dir_name.starts_with("dm")
|| blk_device_dir_name.starts_with("md")
⋮----
let mut path = blk_device_dir.path();
path.push("stat");
File::open(path).ok()
⋮----
.for_each(|file_diskstats| {
⋮----
stats.accumulate(&parse_disk_stats(&mut reader_diskstats).unwrap_or_default());
⋮----
fn parse_disk_stats(reader_diskstats: &mut impl BufRead) -> Result<DiskStats, String> {
⋮----
.read_line(&mut line)
.map_err(|e| e.to_string())?;
⋮----
let num_elements = values.len();
⋮----
return Err("parse error, unknown number of disk stat elements".to_string());
⋮----
stats.reads_completed = values[0].parse::<u64>().map_err(|e| e.to_string())?;
stats.reads_merged = values[1].parse::<u64>().map_err(|e| e.to_string())?;
stats.sectors_read = values[2].parse::<u64>().map_err(|e| e.to_string())?;
stats.time_reading_ms = values[3].parse::<u64>().map_err(|e| e.to_string())?;
stats.writes_completed = values[4].parse::<u64>().map_err(|e| e.to_string())?;
stats.writes_merged = values[5].parse::<u64>().map_err(|e| e.to_string())?;
stats.sectors_written = values[6].parse::<u64>().map_err(|e| e.to_string())?;
stats.time_writing_ms = values[7].parse::<u64>().map_err(|e| e.to_string())?;
stats.io_in_progress = values[8].parse::<u64>().map_err(|e| e.to_string())?;
stats.time_io_ms = values[9].parse::<u64>().map_err(|e| e.to_string())?;
stats.time_io_weighted_ms = values[10].parse::<u64>().map_err(|e| e.to_string())?;
⋮----
stats.discards_completed = values[11].parse::<u64>().map_err(|e| e.to_string())?;
stats.discards_merged = values[12].parse::<u64>().map_err(|e| e.to_string())?;
stats.sectors_discarded = values[13].parse::<u64>().map_err(|e| e.to_string())?;
stats.time_discarding = values[14].parse::<u64>().map_err(|e| e.to_string())?;
⋮----
stats.flushes_completed = values[15].parse::<u64>().map_err(|e| e.to_string())?;
stats.time_flushing = values[16].parse::<u64>().map_err(|e| e.to_string())?;
⋮----
pub struct SystemMonitorStatsReportConfig {
⋮----
enum InterestingLimit {
⋮----
impl SystemMonitorService {
pub fn new(exit: Arc<AtomicBool>, config: SystemMonitorStatsReportConfig) -> Self {
info!("Starting SystemMonitorService");
⋮----
.name("solSystemMonitr".to_string())
.spawn(move || {
⋮----
.unwrap();
⋮----
fn linux_get_current_network_limits() -> Vec<(&'static str, &'static InterestingLimit, i64)> {
use sysctl::Sysctl;
fn sysctl_read(name: &str) -> Result<String, sysctl::SysctlError> {
⋮----
let val = ctl.value_string()?;
Ok(val)
⋮----
fn normalize_err<E: std::fmt::Display>(key: &str, error: E) -> String {
format!("Failed to query value for {key}: {error}")
⋮----
.map(|(key, interesting_limit)| {
let current_value = sysctl_read(key)
.map_err(|e| normalize_err(key, e))
.and_then(|val| val.parse::<i64>().map_err(|e| normalize_err(key, e)))
.unwrap_or_else(|e| {
error!("{e}");
⋮----
fn linux_report_network_limits(
⋮----
.all(|(key, interesting_limit, current_value)| {
datapoint_warn!("os-config", (key, *current_value, i64));
⋮----
warn!(
⋮----
info!("  {key}: recommended={recommended_value} current={current_value}");
⋮----
info!("  {key}: report-only --  current={current_value}");
⋮----
pub fn check_os_network_limits() -> bool {
datapoint_info!("os-config", ("platform", platform_id(), String));
⋮----
fn process_net_stats(net_stats: &mut Option<NetStats>) {
match read_net_stats() {
⋮----
*net_stats = Some(new_stats);
⋮----
Err(e) => warn!("read_net_stats: {e}"),
⋮----
fn process_net_stats(_net_stats: &mut Option<NetStats>) {}
⋮----
fn report_net_stats(old_stats: &NetStats, new_stats: &NetStats) {
datapoint_info!(
⋮----
fn calc_percent(numerator: u64, denom: u64) -> f64 {
⋮----
fn report_mem_stats() {
⋮----
fn cpu_info() -> Result<CpuInfo, Error> {
⋮----
Ok(CpuInfo {
⋮----
fn report_cpuid_values() {
⋮----
let cpuid_mfr = __cpuid(0);
⋮----
__cpuid(CpuidParamValue::Processor.into())
⋮----
__cpuid(CpuidParamValue::Cache.into())
⋮----
__cpuid(CpuidParamValue::Topology.into())
⋮----
__cpuid_count(CpuidParamValue::Extended.into(), 0)
⋮----
if 1 <= __get_cpuid_max(CpuidParamValue::Extended.into()).1 {
__cpuid_count(CpuidParamValue::Extended.into(), 1)
⋮----
fn report_cpu_stats() {
⋮----
fn process_disk_stats(disk_stats: &mut Option<DiskStats>) {
match read_disk_stats() {
⋮----
*disk_stats = Some(new_stats);
⋮----
Err(e) => warn!("read_disk_stats: {e}"),
⋮----
fn process_disk_stats(_disk_stats: &mut Option<DiskStats>) {}
⋮----
fn report_disk_stats(old_stats: &DiskStats, new_stats: &DiskStats) {
⋮----
pub fn run(exit: Arc<AtomicBool>, config: SystemMonitorStatsReportConfig) {
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
if network_limits_timer.should_update(SAMPLE_INTERVAL_OS_NETWORK_LIMITS_MS) {
⋮----
if udp_timer.should_update(SAMPLE_INTERVAL_UDP_MS) {
⋮----
if config.report_os_memory_stats && mem_timer.should_update(SAMPLE_INTERVAL_MEM_MS) {
⋮----
if cpu_timer.should_update(SAMPLE_INTERVAL_CPU_MS) {
⋮----
if cpuid_timer.should_update(SAMPLE_INTERVAL_CPU_ID_MS) {
⋮----
if config.report_os_disk_stats && disk_timer.should_update(SAMPLE_INTERVAL_DISK_MS) {
⋮----
sleep(SLEEP_INTERVAL);
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()
⋮----
mod tests {
⋮----
fn test_parse_udp_stats() {
⋮----
let stats = parse_udp_stats(&mut mock_snmp).unwrap();
assert_eq!(stats.out_datagrams, 30);
assert_eq!(stats.no_ports, 7);
⋮----
let stats = parse_udp_stats(&mut mock_snmp);
assert!(stats.is_err());
⋮----
fn test_parse_net_dev_stats() {
⋮----
let stats = parse_net_dev_stats(&mut mock_dev).unwrap();
assert_eq!(stats.rx_bytes, 500);
assert_eq!(stats.rx_packets, 5);
assert_eq!(stats.rx_errs, 0);
assert_eq!(stats.rx_drops, 1);
assert_eq!(stats.tx_bytes, 450);
assert_eq!(stats.tx_packets, 8);
assert_eq!(stats.tx_errs, 2);
assert_eq!(stats.tx_drops, 0);
⋮----
let stats = parse_net_dev_stats(&mut mock_dev);
⋮----
fn test_parse_disk_stats() {
⋮----
let stats = parse_disk_stats(&mut mock_disk).unwrap();
assert_eq!(stats.reads_completed, 2095701);
assert_eq!(stats.time_io_weighted_ms, 285220738);
⋮----
assert_eq!(stats.time_discarding, 0);
⋮----
assert_eq!(stats.time_flushing, 2922);
⋮----
let stats = parse_disk_stats(&mut mock_disk);
⋮----
fn test_calc_percent() {
assert!(SystemMonitorService::calc_percent(99, 100) < 100.0);
⋮----
assert!(SystemMonitorService::calc_percent(one_tb_as_kb - 1, one_tb_as_kb) < 100.0);

================
File: core/src/tip_manager.rs
================
pub(crate) mod tip_distribution;
pub(crate) mod tip_payment;
⋮----
pub enum TipManagerError {
⋮----
pub type Result<T> = std::result::Result<T, TipManagerError>;
⋮----
struct TipPaymentProgramInfo {
⋮----
struct TipDistributionProgramInfo {
⋮----
pub struct TipDistributionAccountConfig {
⋮----
impl Default for TipDistributionAccountConfig {
fn default() -> Self {
⋮----
pub struct TipManager {
⋮----
pub struct TipManagerConfig {
⋮----
impl Default for TipManagerConfig {
⋮----
impl TipManager {
pub fn new(config: TipManagerConfig) -> TipManager {
⋮----
let tip_accounts = HashSet::from_iter(tip_payment_account_pdas.iter().map(|pda| pda.0));
⋮----
pub fn tip_payment_program_id(&self) -> Pubkey {
⋮----
pub fn tip_distribution_program_id(&self) -> Pubkey {
⋮----
pub fn tip_payment_config_pubkey(&self) -> Pubkey {
⋮----
pub fn tip_distribution_config_pubkey(&self) -> Pubkey {
⋮----
pub fn get_tip_accounts(&self) -> &HashSet<Pubkey> {
⋮----
fn get_tip_payment_config_account(&self, bank: &Bank) -> Result<JitoTipPaymentConfig> {
⋮----
.get_account(&self.tip_payment_program_info.config_pda_bump.0)
.ok_or(TipManagerError::AccountMissing)?;
⋮----
.map_err(TipManagerError::TipPaymentError)
⋮----
pub fn initialize_tip_payment_program_tx(
⋮----
accounts: vec![
⋮----
Some(&keypair.pubkey()),
⋮----
bank.last_blockhash(),
⋮----
Ok(RuntimeTransaction::try_create(
⋮----
bank.get_reserved_account_keys(),
⋮----
.unwrap())
⋮----
pub fn get_my_tip_distribution_pda(&self, epoch: Epoch) -> Pubkey {
⋮----
pub fn should_initialize_tip_payment_program(&self, bank: &Bank) -> bool {
match bank.get_account(&self.tip_payment_config_pubkey()) {
⋮----
Some(account) => account.owner() != &self.tip_payment_program_info.program_id,
⋮----
pub fn should_initialize_tip_distribution_config(&self, bank: &Bank) -> bool {
match bank.get_account(&self.tip_distribution_config_pubkey()) {
⋮----
Some(account) => account.owner() != &self.tip_distribution_program_info.program_id,
⋮----
pub fn should_init_tip_distribution_account(&self, bank: &Bank) -> bool {
let pda = self.get_my_tip_distribution_pda(bank.epoch());
match bank.get_account(&pda) {
⋮----
pub fn initialize_tip_distribution_config_tx(
⋮----
kp.pubkey(),
⋮----
Some(&kp.pubkey()),
⋮----
pub fn initialize_tip_distribution_account_tx(
⋮----
bank.epoch(),
⋮----
pub fn change_tip_receiver_and_block_builder_tx(
⋮----
self.build_change_tip_receiver_and_block_builder_tx(
&tip_payment_config.tip_receiver(),
⋮----
&tip_payment_config.block_builder(),
⋮----
pub fn build_change_tip_receiver_and_block_builder_tx(
⋮----
pub fn get_initialize_tip_programs_bundle(
⋮----
if self.should_initialize_tip_payment_program(bank) {
info!("should_initialize_tip_payment_program=true");
transactions.push(self.initialize_tip_payment_program_tx(bank, keypair)?);
⋮----
if self.should_initialize_tip_distribution_config(bank) {
info!("should_initialize_tip_distribution_config=true");
transactions.push(self.initialize_tip_distribution_config_tx(bank, keypair)?);
⋮----
Ok(transactions)
⋮----
pub fn get_tip_programs_crank_bundle(
⋮----
if self.should_init_tip_distribution_account(bank) {
info!("should_init_tip_distribution_account=true");
transactions.push(self.initialize_tip_distribution_account_tx(bank, keypair)?);
⋮----
let tip_payment_config = self.get_tip_payment_config_account(bank)?;
let my_tip_receiver = self.get_my_tip_distribution_pda(bank.epoch());
if tip_payment_config.tip_receiver() != my_tip_receiver
|| tip_payment_config.block_builder() != block_builder_fee_info.block_builder
|| tip_payment_config.block_builder_commission_pct()
⋮----
debug!("change_tip_receiver=true");
transactions.push(self.change_tip_receiver_and_block_builder_tx(

================
File: core/src/tpu_entry_notifier.rs
================
pub(crate) struct TpuEntryNotifier {
⋮----
impl TpuEntryNotifier {
pub(crate) fn new(
⋮----
.name("solTpuEntry".to_string())
.spawn(move || {
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
exit.clone(),
⋮----
.unwrap();
⋮----
pub(crate) fn send_entry_notification(
⋮----
let (bank, (entry, tick_height)) = entry_receiver.recv_timeout(Duration::from_secs(1))?;
let slot = bank.slot();
⋮----
num_transactions: entry.transactions.len() as u64,
⋮----
if let Err(err) = entry_notification_sender.send(EntryNotification {
⋮----
warn!(
⋮----
*current_transaction_index += entry.transactions.len();
if let Err(err) = broadcast_entry_sender.send((bank, (entry, tick_height))) {
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
Ok(())
⋮----
pub(crate) fn join(self) -> thread::Result<()> {
self.thread_hdl.join()

================
File: core/src/tpu.rs
================
pub use crate::forwarding_stage::ForwardingClientOption;
⋮----
pub struct TpuSockets {
⋮----
enum SigVerifier {
⋮----
impl SigVerifier {
fn join(self) -> thread::Result<()> {
⋮----
SigVerifier::Local(sig_verify_stage) => sig_verify_stage.join(),
SigVerifier::Remote(vortexor_receiver_adapter) => vortexor_receiver_adapter.join(),
⋮----
pub struct Tpu {
⋮----
impl Tpu {
⋮----
pub fn new_with_client(
⋮----
let (fetch_stage_manager_sender, fetch_stage_manager_receiver) = unbounded();
let (sigverify_stage_sender, sigverify_stage_receiver) = unbounded();
let (vote_packet_sender, vote_packet_receiver) = unbounded();
let (forwarded_packet_sender, forwarded_packet_receiver) = unbounded();
⋮----
exit.clone(),
⋮----
Some(bank_forks.read().unwrap().get_vote_only_mode_signal()),
⋮----
bank_forks.clone(),
staked_nodes.clone(),
⋮----
} = spawn_simple_qos_server(
⋮----
vote_packet_sender.clone(),
⋮----
cancel.clone(),
⋮----
.unwrap();
let (tpu_quic_t, key_updater) = if vortexor_receivers.is_none() {
⋮----
} = spawn_stake_wighted_qos_server(
⋮----
fetch_stage_manager_sender.clone(),
⋮----
(Some(tpu_quic_t), Some(key_updater))
⋮----
let (tpu_forwards_quic_t, forwards_key_updater) = if vortexor_receivers.is_none() {
⋮----
(Some(tpu_forwards_quic_t), Some(forwards_key_updater))
⋮----
let (forward_stage_sender, forward_stage_receiver) = bounded(1024);
⋮----
info!("starting vortexor adapter");
let sockets = vortexor_receivers.into_iter().map(Arc::new).collect();
⋮----
banking_stage_sender.clone(),
enable_block_production_forwarding.then(|| forward_stage_sender.clone()),
⋮----
info!("starting regular sigverify stage");
⋮----
Some(forward_stage_sender),
⋮----
block_builder: cluster_info.keypair().pubkey(),
⋮----
let (unverified_bundle_sender, unverified_bundle_receiver) = bounded(1024);
⋮----
cluster_info.clone(),
sigverify_stage_sender.clone(),
⋮----
shredstream_receiver_address.clone(),
bam_enabled.clone(),
⋮----
let (verified_bundle_sender, verified_bundle_receiver) = bounded(1024);
⋮----
let (heartbeat_tx, heartbeat_rx) = unbounded();
⋮----
cluster_info.my_contact_info().clone(),
⋮----
blockstore.clone(),
⋮----
let (bam_batch_sender, bam_batch_receiver) = bounded(100_000);
let (bam_outbound_sender, bam_outbound_receiver) = bounded(100_000);
⋮----
bam_enabled: bam_enabled.clone(),
⋮----
cluster_info: cluster_info.clone(),
⋮----
bank_forks: bank_forks.clone(),
⋮----
blacklisted_accounts.insert(tip_manager.tip_payment_program_id());
⋮----
poh_recorder.clone(),
transaction_recorder.clone(),
⋮----
transaction_status_sender.clone(),
replay_vote_sender.clone(),
⋮----
prioritization_fee_cache.clone(),
blacklisted_accounts.clone(),
bundle_account_locker.clone(),
Some(TipProcessingDependencies {
tip_manager: tip_manager.clone(),
⋮----
block_builder_fee_info: bam_dependencies.block_builder_fee_info.clone(),
⋮----
bundle_account_locker: bundle_account_locker.clone(),
⋮----
Some(bam_dependencies.clone()),
⋮----
assert!(scheduler_bindings.is_none());
⋮----
} = spawn_forwarding_stage(
⋮----
bank_forks.read().unwrap().sharable_banks(),
ForwardAddressGetter::new(cluster_info.clone(), poh_recorder.clone()),
⋮----
key_notifiers.clone(),
⋮----
let (broadcast_entry_sender, broadcast_entry_receiver) = unbounded();
⋮----
(broadcast_entry_receiver, Some(tpu_entry_notifier))
⋮----
let broadcast_stage = broadcast_type.new_broadcast_stage(
⋮----
let mut key_notifiers = key_notifiers.write().unwrap();
⋮----
key_notifiers.add(KeyUpdaterType::Tpu, key_updater);
⋮----
key_notifiers.add(KeyUpdaterType::TpuForwards, forwards_key_updater);
⋮----
key_notifiers.add(KeyUpdaterType::TpuVote, vote_streamer_key_updater);
key_notifiers.add(KeyUpdaterType::Forward, client_updater);
⋮----
pub fn join(self) -> thread::Result<()> {
let results = vec![
⋮----
let broadcast_result = self.broadcast_stage.join();
⋮----
tpu_entry_notifier.join()?;
⋮----
if let Err(tracer_result) = tracer_thread_hdl.join()? {
error!(
⋮----
Ok(())

================
File: core/src/tvu.rs
================
pub struct Tvu {
⋮----
pub struct TvuSockets {
⋮----
pub struct TvuConfig {
⋮----
impl Default for TvuConfig {
fn default() -> Self {
⋮----
replay_forks_threads: NonZeroUsize::new(1).expect("1 is non-zero"),
replay_transactions_threads: NonZeroUsize::new(1).expect("1 is non-zero"),
shred_sigverify_threads: NonZeroUsize::new(1).expect("1 is non-zero"),
⋮----
impl Tvu {
⋮----
pub fn new(
⋮----
let in_wen_restart = wen_restart_repair_slots.is_some();
⋮----
let fetch_sockets: Vec<Arc<UdpSocket>> = fetch_sockets.into_iter().map(Arc::new).collect();
⋮----
repair_socket.clone(),
⋮----
bank_forks.clone(),
cluster_info.clone(),
outstanding_repair_requests.clone(),
⋮----
exit.clone(),
⋮----
let (verified_sender, verified_receiver) = unbounded();
⋮----
leader_schedule_cache.clone(),
⋮----
retransmit_sender.clone(),
⋮----
max_slots.clone(),
rpc_subscriptions.clone(),
slot_status_notifier.clone(),
⋮----
let (ancestor_duplicate_slots_sender, ancestor_duplicate_slots_receiver) = unbounded();
let (duplicate_slots_sender, duplicate_slots_receiver) = unbounded();
⋮----
unbounded();
let (dumped_slots_sender, dumped_slots_receiver) = unbounded();
let (popular_pruned_forks_sender, popular_pruned_forks_receiver) = unbounded();
⋮----
.read()
.unwrap()
.working_bank()
.epoch_schedule()
.clone();
⋮----
bank_forks: bank_forks.clone(),
⋮----
cluster_info: cluster_info.clone(),
cluster_slots: cluster_slots.clone(),
⋮----
duplicate_slots_sender.clone(),
⋮----
blockstore.clone(),
⋮----
let (cluster_slots_update_sender, cluster_slots_update_receiver) = unbounded();
⋮----
cluster_slots.clone(),
⋮----
let (cost_update_sender, cost_update_receiver) = unbounded();
let (drop_bank_sender, drop_bank_receiver) = unbounded();
let (voting_sender, voting_receiver) = unbounded();
⋮----
exit: exit.clone(),
leader_schedule_cache: leader_schedule_cache.clone(),
⋮----
tower_storage: tower_storage.clone(),
⋮----
blockstore: blockstore.clone(),
⋮----
poh_recorder: poh_recorder.clone(),
⋮----
prioritization_fee_cache: prioritization_fee_cache.clone(),
⋮----
poh_recorder.clone(),
⋮----
vote_connection_cache.clone(),
⋮----
let warm_quic_cache_service = create_cache_warmer_if_needed(
⋮----
Some(ReplayStage::new(
⋮----
let blockstore_cleanup_service = tvu_config.max_ledger_shreds.map(|max_ledger_shreds| {
BlockstoreCleanupService::new(blockstore.clone(), max_ledger_shreds, exit.clone())
⋮----
Ok(Tvu {
⋮----
pub fn join(self) -> thread::Result<()> {
self.retransmit_stage.join()?;
self.window_service.join()?;
self.cluster_slots_service.join()?;
self.fetch_stage.join()?;
self.shred_sigverify.join()?;
if self.blockstore_cleanup_service.is_some() {
self.blockstore_cleanup_service.unwrap().join()?;
⋮----
if self.replay_stage.is_some() {
self.replay_stage.unwrap().join()?;
⋮----
self.cost_update_service.join()?;
self.voting_service.join()?;
⋮----
warmup_service.join()?;
⋮----
self.drop_bank_service.join()?;
self.duplicate_shred_listener.join()?;
Ok(())
⋮----
fn create_cache_warmer_if_needed(
⋮----
let tpu_connection_cache = connection_cache.filter(|cache| cache.use_quic()).cloned();
let vote_connection_cache = Some(vote_connection_cache).filter(|cache| cache.use_quic());
(tpu_connection_cache.is_some() || vote_connection_cache.is_some()).then(|| {
⋮----
pub mod tests {
⋮----
fn test_tvu_exit(enable_wen_restart: bool) {
⋮----
let target1 = Node::new_localhost_with_pubkey(&target1_keypair.pubkey());
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(starting_balance);
⋮----
let (_turbine_quic_endpoint_sender, turbine_quic_endpoint_receiver) = unbounded();
let (_, repair_response_quic_receiver) = unbounded();
⋮----
let (_, ancestor_hashes_response_quic_receiver) = unbounded();
⋮----
target1.info.clone(),
target1_keypair.into(),
⋮----
cluster_info1.insert_info(leader.info);
⋮----
let (blockstore_path, _) = create_new_tmp_ledger!(&genesis_config);
⋮----
.expect("Expected to successfully open ledger");
⋮----
let bank = bank_forks.read().unwrap().working_bank();
⋮----
) = create_test_recorder(bank.clone(), blockstore.clone(), None, None);
⋮----
let (retransmit_slots_sender, _retransmit_slots_receiver) = unbounded();
let (_gossip_verified_vote_hash_sender, gossip_verified_vote_hash_receiver) = unbounded();
let (_verified_vote_sender, verified_vote_receiver) = unbounded();
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
let (_, gossip_confirmed_slots_receiver) = unbounded();
⋮----
Some(Arc::new(RwLock::new(vec![])))
⋮----
&vote_keypair.pubkey(),
Arc::new(RwLock::new(vec![Arc::new(vote_keypair)])),
⋮----
Some(Arc::new(RpcSubscriptions::new_for_tests(
⋮----
block_commitment_cache.clone(),
⋮----
.expect("assume success");
⋮----
assert!(tvu.replay_stage.is_none())
⋮----
assert!(tvu.replay_stage.is_some())
⋮----
exit.store(true, Ordering::Relaxed);
tvu.join().unwrap();
poh_service.join().unwrap();
⋮----
fn test_tvu_exit_no_wen_restart() {
test_tvu_exit(false);
⋮----
fn test_tvu_exit_with_wen_restart() {
test_tvu_exit(true);

================
File: core/src/unfrozen_gossip_verified_vote_hashes.rs
================
pub struct UnfrozenGossipVerifiedVoteHashes {
⋮----
impl UnfrozenGossipVerifiedVoteHashes {
pub(crate) fn add_vote(
⋮----
let frozen_hash = if is_frozen { Some(hash) } else { None };
⋮----
.check_add_vote(pubkey, vote_slot, frozen_hash, false);
⋮----
.map(|latest_frozen_vote_slot| vote_slot >= latest_frozen_vote_slot)
.unwrap_or(true)
⋮----
.entry(vote_slot)
.or_default()
.entry(hash)
⋮----
.push(pubkey);
⋮----
pub fn set_root(&mut self, new_root: Slot) {
self.votes_per_slot = self.votes_per_slot.split_off(&new_root);
⋮----
pub fn remove_slot_hash(&mut self, slot: Slot, hash: &Hash) -> Option<Vec<Pubkey>> {
self.votes_per_slot.get_mut(&slot).and_then(|slot_hashes| {
slot_hashes.remove(hash)
⋮----
mod tests {
⋮----
fn test_unfrozen_gossip_verified_vote_hashes_add_vote() {
⋮----
.take(num_validators)
.collect();
⋮----
for vote_pubkey in validator_keys.iter() {
unfrozen_gossip_verified_vote_hashes.add_vote(
⋮----
assert!(unfrozen_gossip_verified_vote_hashes
⋮----
.get(unfrozen_vote_slot)
.unwrap();
assert_eq!(vote_hashes_map.len(), num_duplicate_hashes);
for pubkey_votes in vote_hashes_map.values() {
assert_eq!(*pubkey_votes, validator_keys);

================
File: core/src/validator.rs
================
use crate::tip_manager::TipManagerConfig;
pub use solana_perf::report_target_features;
⋮----
pub enum BlockVerificationMethod {
⋮----
impl BlockVerificationMethod {
pub const fn cli_names() -> &'static [&'static str] {
⋮----
pub fn cli_message() -> &'static str {
⋮----
pub enum BlockProductionMethod {
⋮----
impl BlockProductionMethod {
⋮----
pub enum TransactionStructure {
⋮----
impl TransactionStructure {
⋮----
pub enum SchedulerPacing {
⋮----
impl FromStr for SchedulerPacing {
type Err = String;
fn from_str(s: &str) -> std::result::Result<Self, Self::Err> {
if s.eq_ignore_ascii_case("disabled") {
Ok(SchedulerPacing::Disabled)
⋮----
Ok(v) if v > 0 => Ok(SchedulerPacing::FillTimeMillis(
NonZeroU64::new(v).ok_or_else(|| "value must be non-zero".to_string())?,
⋮----
_ => Err("value must be a positive integer or 'disabled'".to_string()),
⋮----
impl SchedulerPacing {
pub fn fill_time(&self) -> Option<Duration> {
⋮----
SchedulerPacing::FillTimeMillis(millis) => Some(Duration::from_millis(millis.get())),
⋮----
/// Configuration for the block generator invalidator for replay.
#[derive(Clone, Debug)]
pub struct GeneratorConfig {
⋮----
pub struct ValidatorConfig {
/// The destination file for validator logs; `stderr` is used if `None`
    pub logfile: Option<PathBuf>,
⋮----
/// Specifies which plugins to start up with
    pub on_start_geyser_plugin_config_files: Option<Vec<PathBuf>>,
⋮----
pub rpc_addrs: Option<(SocketAddr, SocketAddr)>, // (JsonRpc, JsonRpcPubSub)
⋮----
pub known_validators: Option<HashSet<Pubkey>>, // None = trust all
pub repair_validators: Option<HashSet<Pubkey>>, // None = repair from all
pub repair_whitelist: Arc<RwLock<HashSet<Pubkey>>>, // Empty = repair with all
pub gossip_validators: Option<HashSet<Pubkey>>, // None = gossip with all
⋮----
/// Run PoH, transaction signature and other transaction verification during blockstore
    /// processing.
⋮----
/// processing.
    pub run_verification: bool,
⋮----
// jito configuration
⋮----
impl ValidatorConfig {
pub fn default_for_test() -> Self {
⋮----
NonZeroUsize::new(num_cpus::get()).expect("thread count is non-zero");
⋮----
ip_echo_server_threads: NonZeroUsize::new(1).expect("1 is non-zero"),
⋮----
replay_forks_threads: NonZeroUsize::new(1).expect("1 is non-zero"),
⋮----
tvu_shred_sigverify_threads: NonZeroUsize::new(get_thread_count())
.expect("thread count is non-zero"),
⋮----
pub fn enable_default_rpc_block_subscribe(&mut self) {
⋮----
pub enum ValidatorStartProgress {
⋮----
struct BlockstoreRootScan {
⋮----
impl BlockstoreRootScan {
fn new(config: &ValidatorConfig, blockstore: Arc<Blockstore>, exit: Arc<AtomicBool>) -> Self {
let thread = if config.rpc_addrs.is_some()
⋮----
Some(
⋮----
.name("solBStoreRtScan".to_string())
.spawn(move || blockstore.scan_and_fix_roots(None, None, &exit))
.unwrap(),
⋮----
fn join(self) {
⋮----
if let Err(err) = blockstore_root_scan.join() {
warn!("blockstore_root_scan failed to join {err:?}");
⋮----
struct TransactionHistoryServices {
⋮----
pub struct ValidatorTpuConfig {
⋮----
impl ValidatorTpuConfig {
pub fn new_for_tests(tpu_enable_udp: bool) -> Self {
⋮----
pub struct Validator {
⋮----
impl Validator {
⋮----
pub fn new(
⋮----
info!("debug-assertion status: {DEBUG_ASSERTION_STATUS}");
⋮----
adjust_nofile_limit(config.enforce_ulimit_nofile)?;
⋮----
.thread_name(|i| format!("solRayonGlob{i:02}"))
.num_threads(config.rayon_global_threads.get())
.build_global()
.is_err()
⋮----
warn!("Rayon global thread pool already initialized");
⋮----
let id = identity_keypair.pubkey();
assert_eq!(&id, node.info.pubkey());
info!("identity pubkey: {id}");
info!("vote account pubkey: {vote_account}");
⋮----
verify_net_stats_access().map_err(|e| {
ValidatorError::Other(format!("Failed to access network stats: {e:?}"))
⋮----
.as_ref()
.map(Cow::Borrowed)
.or_else(|| {
⋮----
.then_some(Cow::Owned(vec![]))
⋮----
let (confirmed_bank_sender, confirmed_bank_receiver) = unbounded();
bank_notification_senders.push(confirmed_bank_sender);
⋮----
rpc_to_plugin_manager_receiver.map(|receiver| (receiver, exit.clone()));
⋮----
geyser_plugin_config_files.as_ref(),
⋮----
.map_err(|err| {
ValidatorError::Other(format!("Failed to load the Geyser plugin: {err:?}"))
⋮----
warn!("voting disabled");
authorized_voter_keypairs.write().unwrap().clear();
⋮----
for authorized_voter_keypair in authorized_voter_keypairs.read().unwrap().iter() {
warn!("authorized voter: {}", authorized_voter_keypair.pubkey());
⋮----
info!("entrypoint: {cluster_entrypoint:?}");
⋮----
validate_memlock_limit_for_disk_io(config.accounts_db_config.memlock_budget_size)?;
if !ledger_path.is_dir() {
return Err(anyhow!(
⋮----
let genesis_config = load_genesis(config, ledger_path)?;
metrics_config_sanity_check(genesis_config.cluster_type)?;
info!("Cleaning accounts paths..");
*start_progress.write().unwrap() = ValidatorStartProgress::CleaningAccounts;
⋮----
cleanup_accounts_paths(config);
timer.stop();
info!("Cleaning accounts paths done. {timer}");
⋮----
info!("Cleaning orphaned account snapshot directories..");
⋮----
clean_orphaned_account_snapshot_dirs(
⋮----
.context("failed to clean orphaned account snapshot directories")?;
⋮----
info!("Cleaning orphaned account snapshot directories done. {timer}");
⋮----
let exit = exit.clone();
⋮----
.write()
.unwrap()
.register_exit(Box::new(move || exit.store(true, Ordering::Relaxed)));
let cancel = cancel.clone();
⋮----
.register_exit(Box::new(move || cancel.cancel()));
⋮----
service.get_accounts_update_notifier(),
service.get_transaction_notifier(),
service.get_entry_notifier(),
service.get_block_metadata_notifier(),
service.get_slot_status_notifier(),
⋮----
info!(
⋮----
let system_monitor_service = Some(SystemMonitorService::new(
exit.clone(),
⋮----
) = load_blockstore(
⋮----
.is_some()
.then(|| dependency_tracker.clone()),
⋮----
.map_err(ValidatorError::Other)?;
⋮----
check_poh_speed(&bank_forks.read().unwrap().root_bank(), None)?;
⋮----
let root_bank = bank_forks.read().unwrap().root_bank();
(root_bank.slot(), root_bank.hard_forks())
⋮----
let shred_version = compute_shred_version(&genesis_config.hash(), Some(&hard_forks));
info!("shred version: {shred_version}, hard forks: {hard_forks:?}");
⋮----
return Err(ValidatorError::ShredVersionMismatch {
⋮----
.into());
⋮----
if let Some(start_slot) = should_cleanup_blockstore_incorrect_shred_versions(
⋮----
*start_progress.write().unwrap() = ValidatorStartProgress::CleaningBlockStore;
cleanup_blockstore_incorrect_shred_versions(
⋮----
info!("Skipping the blockstore check for shreds with incorrect version");
⋮----
node.info.set_shred_version(shred_version);
node.info.set_wallclock(timestamp());
⋮----
node.info.clone(),
identity_keypair.clone(),
⋮----
cluster_info.set_contact_debug_interval(config.contact_debug_interval);
cluster_info.set_entrypoints(cluster_entrypoints);
cluster_info.restore_contact_info(ledger_path, config.contact_save_interval);
cluster_info.set_bind_ip_addrs(node.bind_ip_addrs.clone());
⋮----
assert!(is_snapshot_config_valid(&config.snapshot_config));
let (snapshot_request_sender, snapshot_request_receiver) = unbounded();
⋮----
snapshot_request_sender.clone(),
config.snapshot_config.clone(),
bank_forks.read().unwrap().root(),
⋮----
.snapshot_config()
.should_generate_snapshots()
⋮----
.get(SnapshotPackagerService::NAME)
.cloned();
⋮----
pending_snapshot_packages.clone(),
⋮----
cluster_info.clone(),
snapshot_controller.clone(),
⋮----
Some(snapshot_packager_service)
⋮----
snapshot_controller: snapshot_controller.clone(),
⋮----
bank_forks.clone(),
⋮----
let (replay_vote_sender, replay_vote_receiver) = unbounded();
⋮----
let bank = &bank_forks.read().unwrap().working_bank();
⋮----
bank.tick_height(),
bank.last_blockhash(),
bank.clone(),
⋮----
bank.ticks_per_slot(),
⋮----
blockstore.clone(),
blockstore.get_new_shred_signal(0),
⋮----
let (record_sender, record_receiver) = record_channels(transaction_status_sender.is_some());
⋮----
BankingTracer::new((config.banking_trace_dir_byte_limit > 0).then_some((
&blockstore.banking_trace_path(),
⋮----
if banking_tracer.is_enabled() {
⋮----
info!("Disabled banking trace");
⋮----
let banking_tracer_channels = banking_tracer.create_channels(false);
⋮----
info!("no scheduler pool is installed for block verification...");
⋮----
warn!(
⋮----
transaction_status_sender.clone(),
Some(replay_vote_sender.clone()),
prioritization_fee_cache.clone(),
⋮----
.install_scheduler_pool(scheduler_pool);
⋮----
.map(|service| service.sender());
⋮----
transaction_status_sender.as_ref(),
⋮----
maybe_warp_slot(
⋮----
.process()
⋮----
*start_progress.write().unwrap() = ValidatorStartProgress::StartingServices;
⋮----
if config.rpc_addrs.is_some() && config.rpc_config.enable_rpc_transaction_history {
Some(SamplePerformanceService::new(
⋮----
let bank_forks_guard = bank_forks.read().unwrap();
block_commitment_cache.initialize_slots(
bank_forks_guard.working_bank().slot(),
bank_forks_guard.root(),
⋮----
drop(bank_forks_guard);
⋮----
Some(node.sockets.tpu_transaction_forwarding_clients);
⋮----
Some(node.sockets.quic_vote_client),
Some((
⋮----
.tpu_vote(Protocol::QUIC)
.ok_or_else(|| {
⋮----
.ip(),
⋮----
Some((&staked_nodes, &identity_keypair.pubkey())),
⋮----
let tpu_client_next_runtime = current_runtime_handle.is_err().then(|| {
⋮----
.enable_all()
.worker_threads(2)
.thread_name("solTpuClientRt")
.build()
⋮----
assert_eq!(
⋮----
let (bank_notification_sender, bank_notification_receiver) = unbounded();
let confirmed_bank_subscribers = if !bank_notification_senders.is_empty() {
Some(Arc::new(RwLock::new(bank_notification_senders)))
⋮----
.map(TokioRuntime::handle)
.unwrap_or_else(|| current_runtime_handle.as_ref().unwrap());
RpcTpuClientArgs(
⋮----
runtime_handle.clone(),
cancel.clone(),
⋮----
rpc_config: config.rpc_config.clone(),
snapshot_config: Some(snapshot_controller.snapshot_config().clone()),
bank_forks: bank_forks.clone(),
block_commitment_cache: block_commitment_cache.clone(),
blockstore: blockstore.clone(),
cluster_info: cluster_info.clone(),
poh_recorder: Some(poh_recorder.clone()),
genesis_hash: genesis_config.hash(),
ledger_path: ledger_path.to_path_buf(),
validator_exit: config.validator_exit.clone(),
exit: exit.clone(),
override_health_check: rpc_override_health_check.clone(),
optimistically_confirmed_bank: optimistically_confirmed_bank.clone(),
send_transaction_service_config: config.send_transaction_service_config.clone(),
max_slots: max_slots.clone(),
leader_schedule_cache: leader_schedule_cache.clone(),
max_complete_transaction_status_slot: max_complete_transaction_status_slot.clone(),
prioritization_fee_cache: prioritization_fee_cache.clone(),
⋮----
JsonRpcService::new_with_config(rpc_svc_config).map_err(ValidatorError::Other)?;
⋮----
block_commitment_cache.clone(),
optimistically_confirmed_bank.clone(),
⋮----
config.pubsub_config.clone(),
⋮----
.register_exit(Box::new(move || trigger.cancel()));
Some(pubsub_service)
⋮----
bounded(MAX_COMPLETED_DATA_SETS_IN_CHANNEL);
⋮----
rpc_subscriptions.clone(),
⋮----
max_slots.clone(),
⋮----
Some(completed_data_sets_sender),
Some(completed_data_sets_service),
⋮----
if config.rpc_config.full_api || geyser_plugin_service.is_some() {
⋮----
bounded(MAX_COMPLETED_SLOTS_IN_CHANNEL);
blockstore.add_completed_slots_signal(completed_slots_sender);
Some(RpcCompletedSlotsService::spawn(
⋮----
slot_status_notifier.clone(),
⋮----
.then_some(dependency_tracker);
⋮----
Some(OptimisticallyConfirmedBankTracker::new(
⋮----
dependency_tracker.clone(),
⋮----
let bank_notification_sender_config = Some(BankNotificationSenderConfig {
⋮----
should_send_parents: geyser_plugin_service.is_some(),
⋮----
Some(json_rpc_service),
Some(rpc_subscriptions),
⋮----
if config.halt_at_slot.is_some() {
⋮----
.set_highest_super_majority_root(bank_forks.read().unwrap().root());
warn!("Validator halted");
*start_progress.write().unwrap() = ValidatorStartProgress::Halted;
⋮----
Some(tcp_listener) => Some(solana_net_utils::ip_echo_server(
⋮----
Some(node.info.shred_version()),
⋮----
let (stats_reporter_sender, stats_reporter_receiver) = unbounded();
⋮----
StatsReporterService::new(stats_reporter_receiver, exit.clone());
⋮----
Some(bank_forks.clone()),
node.sockets.gossip.clone(),
config.gossip_validators.clone(),
⋮----
Some(stats_reporter_sender.clone()),
⋮----
let serve_repair = config.repair_handler_type.create_serve_repair(
⋮----
bank_forks.read().unwrap().sharable_banks(),
config.repair_whitelist.clone(),
⋮----
let (repair_request_quic_sender, repair_request_quic_receiver) = unbounded();
let (repair_response_quic_sender, repair_response_quic_receiver) = unbounded();
⋮----
unbounded();
let waited_for_supermajority = wait_for_supermajority(
⋮----
Some(&mut process_blockstore),
⋮----
BlockstoreMetricReportService::new(blockstore.clone(), exit.clone());
⋮----
poh_recorder.clone(),
⋮----
bank_forks.read().unwrap().root_bank().ticks_per_slot(),
⋮----
let (retransmit_slots_sender, retransmit_slots_receiver) = unbounded();
let (verified_vote_sender, verified_vote_receiver) = unbounded();
let (gossip_verified_vote_hash_sender, gossip_verified_vote_hash_receiver) = unbounded();
let (duplicate_confirmed_slot_sender, duplicate_confirmed_slots_receiver) = unbounded();
⋮----
.map(|service| service.sender_cloned());
let turbine_quic_endpoint_runtime = (current_runtime_handle.is_err()
⋮----
.then(|| {
⋮----
.thread_name("solTurbineQuic")
⋮----
let (turbine_quic_endpoint_sender, turbine_quic_endpoint_receiver) = unbounded();
⋮----
.unwrap_or_else(|| current_runtime_handle.as_ref().unwrap()),
⋮----
.map(|(endpoint, sender, join_handle)| (Some(endpoint), sender, Some(join_handle)))
⋮----
let repair_quic_endpoints_runtime = (current_runtime_handle.is_err()
⋮----
.thread_name("solRepairQuic")
⋮----
repair_request_quic_sender: repair_request_quic_sender.clone(),
⋮----
.map(|(endpoints, senders, join_handle)| {
(Some(endpoints), senders, Some(join_handle))
⋮----
let in_wen_restart = config.wen_restart_proto_path.is_some() && !waited_for_supermajority;
⋮----
Some(Arc::new(RwLock::new(Vec::new())))
⋮----
let tower = match process_blockstore.process_to_create_tower() {
⋮----
info!("Tower state: {tower:?}");
⋮----
warn!("Unable to retrieve tower: {e:?} creating default tower....");
⋮----
let last_vote = tower.last_vote();
⋮----
if let Some(xdp_config) = config.retransmit_xdp.clone() {
⋮----
.local_addr()
.expect("failed to get local address")
.port();
let src_ip = match node.bind_ip_addrs.active() {
IpAddr::V4(ip) if !ip.is_unspecified() => Some(ip),
⋮----
.and_then(|iface| master_ip_if_bonded(iface)),
_ => panic!("IPv6 not supported"),
⋮----
.expect("failed to create xdp retransmitter");
(Some(rtx), Some(sender))
⋮----
repair: node.sockets.repair.try_clone().unwrap(),
⋮----
config.tower_storage.clone(),
⋮----
config.turbine_disabled.clone(),
⋮----
entry_notification_sender.clone(),
vote_tracker.clone(),
⋮----
replay_vote_sender.clone(),
⋮----
bank_notification_sender.clone(),
⋮----
shred_version: node.info.shred_version(),
repair_validators: config.repair_validators.clone(),
repair_whitelist: config.repair_whitelist.clone(),
⋮----
xdp_sender: xdp_sender.clone(),
⋮----
Some(snapshot_controller.clone()),
⋮----
banking_tracer.clone(),
turbine_quic_endpoint_sender.clone(),
⋮----
outstanding_repair_requests.clone(),
cluster_slots.clone(),
wen_restart_repair_slots.clone(),
⋮----
config.shred_retransmit_receiver_address.clone(),
⋮----
info!("Waiting for wen_restart to finish");
wait_for_wen_restart(WenRestartConfig {
wen_restart_path: config.wen_restart_proto_path.clone().unwrap(),
wen_restart_coordinator: config.wen_restart_coordinator.unwrap(),
⋮----
wen_restart_repair_slots: wen_restart_repair_slots.clone(),
⋮----
snapshot_controller: Some(snapshot_controller.clone()),
abs_status: accounts_background_service.status().clone(),
genesis_config_hash: genesis_config.hash(),
⋮----
return Err(ValidatorError::WenRestartFinished.into());
⋮----
tpu_transactions_forwards_client_sockets.take().unwrap(),
⋮----
node_multihoming.clone(),
⋮----
node.info.shred_version(),
⋮----
config.staked_nodes_overrides.clone(),
⋮----
config.block_production_method.clone(),
⋮----
config.block_production_scheduler_config.clone(),
⋮----
config.generator_config.clone(),
key_notifiers.clone(),
⋮----
config.enable_scheduler_bindings.then(|| {
⋮----
ledger_path.join("scheduler_bindings.ipc"),
banking_control_sender.clone(),
⋮----
config.block_engine_config.clone(),
config.relayer_config.clone(),
config.tip_manager_config.clone(),
config.shred_receiver_address.clone(),
config.bam_url.clone(),
⋮----
datapoint_info!(
⋮----
*start_progress.write().unwrap() = ValidatorStartProgress::Running;
⋮----
key_notifiers.write().unwrap().add(
⋮----
json_rpc_service.get_client_key_updater(),
⋮----
*admin_rpc_service_post_init.write().unwrap() = Some(AdminRpcRequestMetadataPostInit {
⋮----
node: Some(node_multihoming),
⋮----
block_engine_config: config.block_engine_config.clone(),
relayer_config: config.relayer_config.clone(),
shred_receiver_address: config.shred_receiver_address.clone(),
shred_retransmit_receiver_address: config.shred_retransmit_receiver_address.clone(),
⋮----
Ok(Self {
logfile: config.logfile.clone(),
⋮----
pub fn listen_for_signals(&self) -> Result<()> {
⋮----
if self.logfile.is_some() {
signal_hook::flag::register(libc::SIGUSR1, sigusr1_flag.clone())?;
⋮----
info!("Validator::listen_for_signals() has started");
⋮----
if self.exit.load(Ordering::Relaxed) {
⋮----
if sigusr1_flag.load(Ordering::Relaxed) {
⋮----
if let Some(logfile) = self.logfile.as_ref() {
info!("Received SIGUSR1, reopening {}", logfile.display());
⋮----
sigusr1_flag.store(false, Ordering::Relaxed);
⋮----
unreachable!("The SIGUSR1 signal is only handled on unix systems");
⋮----
info!("Validator::listen_for_signals() has stopped");
Ok(())
⋮----
pub fn exit(&mut self) {
self.validator_exit.write().unwrap().exit();
self.blockstore.drop_signal();
⋮----
pub fn close(mut self) {
self.exit();
self.join();
⋮----
fn print_node_info(node: &Node) {
info!("{:?}", node.info);
⋮----
pub fn join(self) {
drop(self.bank_forks);
drop(self.cluster_info);
self.poh_service.join().expect("poh_service");
drop(self.poh_recorder);
⋮----
json_rpc_service.join().expect("rpc_service");
⋮----
pubsub_service.join().expect("pubsub_service");
⋮----
.join()
.expect("rpc_completed_slots_service");
⋮----
.expect("optimistically_confirmed_bank_tracker");
⋮----
.expect("transaction_status_service");
⋮----
.expect("system_monitor_service");
⋮----
.expect("sample_performance_service");
⋮----
.expect("entry_notifier_service");
⋮----
s.join().expect("snapshot_packager_service");
⋮----
self.gossip_service.join().expect("gossip_service");
⋮----
.iter()
.flatten()
.for_each(repair::quic_endpoint::close_quic_endpoint);
⋮----
.expect("serve_repair_service");
⋮----
.map(|runtime| runtime.block_on(repair_quic_endpoints_join_handle))
.transpose()
.unwrap();
⋮----
.expect("stats_reporter_service");
⋮----
.expect("ledger_metric_report_service");
⋮----
.expect("accounts_background_service");
⋮----
xdp_retransmitter.join().expect("xdp_retransmitter");
⋮----
self.tpu.join().expect("tpu");
self.tvu.join().expect("tvu");
⋮----
.map(|runtime| runtime.block_on(turbine_quic_endpoint_join_handle))
⋮----
.expect("completed_data_sets_service");
⋮----
ip_echo_server.shutdown_background();
⋮----
geyser_plugin_service.join().expect("geyser_plugin_service");
⋮----
fn active_vote_account_exists_in_bank(bank: &Bank, vote_account: &Pubkey) -> bool {
if let Some(account) = &bank.get_account(vote_account) {
if let Ok(vote_state) = VoteStateV4::deserialize(account.data(), vote_account) {
return !vote_state.votes.is_empty();
⋮----
fn check_poh_speed(bank: &Bank, maybe_hash_samples: Option<u64>) -> Result<(), ValidatorError> {
let Some(hashes_per_tick) = bank.hashes_per_tick() else {
warn!("Unable to read hashes per tick from Bank, skipping PoH speed check");
return Ok(());
⋮----
let ticks_per_slot = bank.ticks_per_slot();
⋮----
let hash_samples = maybe_hash_samples.unwrap_or(hashes_per_slot);
let hash_time = compute_hash_time(hash_samples);
let my_hashes_per_second = (hash_samples as f64 / hash_time.as_secs_f64()) as u64;
⋮----
(hashes_per_slot as f64 / target_slot_duration.as_secs_f64()) as u64;
⋮----
return Err(ValidatorError::PohTooSlow {
⋮----
fn maybe_cluster_restart_with_hard_fork(config: &ValidatorConfig, root_slot: Slot) -> Option<Slot> {
⋮----
return Some(wait_slot_for_supermajority);
⋮----
fn post_process_restored_tower(
⋮----
let restored_tower = restored_tower.and_then(|tower| {
let root_bank = bank_forks.root_bank();
let slot_history = root_bank.get_slot_history();
let tower = tower.adjust_lockouts_after_replay(root_bank.slot(), &slot_history);
⋮----
maybe_cluster_restart_with_hard_fork(config, root_bank.slot())
⋮----
format!("Hard fork is detected; discarding tower restoration result: {tower:?}");
datapoint_error!("tower_error", ("error", message, String),);
error!("{message}");
⋮----
return Err(crate::consensus::TowerError::HardFork(
⋮----
return Err(crate::consensus::TowerError::HardFork(warp_slot));
⋮----
active_vote_account_exists_in_bank(&bank_forks.working_bank(), vote_account);
if !err.is_file_missing() {
datapoint_error!(
⋮----
return Err(format!(
⋮----
if err.is_file_missing() && !voting_has_been_active {
⋮----
error!(
⋮----
Ok(restored_tower)
⋮----
fn load_genesis(
⋮----
let genesis_config = open_genesis_config(ledger_path, config.max_genesis_archive_unpacked_size)
.map_err(ValidatorError::OpenGenesisConfig)?;
⋮----
let leader_epoch_offset = leader_schedule_slot_offset.div_ceil(slots_per_epoch);
assert!(leader_epoch_offset <= MAX_LEADER_SCHEDULE_EPOCH_OFFSET);
let genesis_hash = genesis_config.hash();
info!("genesis hash: {genesis_hash}");
⋮----
return Err(ValidatorError::GenesisHashMismatch(
⋮----
Ok(genesis_config)
⋮----
fn load_blockstore(
⋮----
info!("loading ledger from {ledger_path:?}...");
*start_progress.write().unwrap() = ValidatorStartProgress::LoadingLedger;
let blockstore = Blockstore::open_with_options(ledger_path, config.blockstore_options.clone())
.map_err(|err| format!("Failed to open Blockstore: {err:?}"))?;
let (ledger_signal_sender, ledger_signal_receiver) = bounded(MAX_REPLAY_WAKE_UP_SIGNALS);
blockstore.add_new_shred_signal(ledger_signal_sender);
let original_blockstore_root = blockstore.max_root();
⋮----
let blockstore_root_scan = BlockstoreRootScan::new(config, blockstore.clone(), exit.clone());
⋮----
.or_else(|| blockstore.highest_slot().unwrap_or(None));
⋮----
new_hard_forks: config.new_hard_forks.clone(),
debug_keys: config.debug_keys.clone(),
accounts_db_config: config.accounts_db_config.clone(),
⋮----
runtime_config: config.runtime_config.clone(),
⋮----
config.rpc_addrs.is_some() && config.rpc_config.enable_rpc_transaction_history;
let is_plugin_transaction_history_required = transaction_notifier.as_ref().is_some();
⋮----
initialize_rpc_transaction_history_services(
⋮----
.map(|entry_notifier| EntryNotifierService::new(entry_notifier, exit.clone()));
⋮----
config.account_paths.clone(),
⋮----
.as_ref(),
⋮----
.map(|service| service.sender()),
⋮----
.map_err(|err| err.to_string())?;
⋮----
AccountsBackgroundService::setup_bank_drop_callback(bank_forks.clone());
leader_schedule_cache.set_fixed_leader_schedule(config.fixed_leader_schedule.clone());
Ok((
⋮----
pub struct ProcessBlockStore<'a> {
⋮----
fn new(
⋮----
blockstore_root_scan: Some(blockstore_root_scan),
⋮----
pub(crate) fn process(&mut self) -> Result<(), String> {
if self.tower.is_none() {
let previous_start_process = *self.start_progress.read().unwrap();
*self.start_progress.write().unwrap() = ValidatorStartProgress::LoadingLedger;
⋮----
if let Ok(Some(max_slot)) = self.blockstore.highest_slot() {
let bank_forks = self.bank_forks.clone();
⋮----
let start_progress = self.start_progress.clone();
⋮----
.name("solRptLdgrStat".to_string())
.spawn(move || {
while !exit.load(Ordering::Relaxed) {
let slot = bank_forks.read().unwrap().working_bank().slot();
*start_progress.write().unwrap() =
⋮----
Some(self.snapshot_controller),
⋮----
exit.store(true, Ordering::Relaxed);
format!("Failed to load ledger: {err:?}")
⋮----
if let Some(blockstore_root_scan) = self.blockstore_root_scan.take() {
blockstore_root_scan.join();
⋮----
self.tower = Some({
let restored_tower = Tower::restore(self.config.tower_storage.as_ref(), self.id);
⋮----
// reconciliation attempt 1 of 2 with tower
reconcile_blockstore_roots_with_external_source(
ExternalRootSource::Tower(tower.root()),
⋮----
.map_err(|err| format!("Failed to reconcile blockstore with tower: {err:?}"))?;
⋮----
post_process_restored_tower(
⋮----
&self.bank_forks.read().unwrap(),
⋮----
if let Some(hard_fork_restart_slot) = maybe_cluster_restart_with_hard_fork(
⋮----
self.bank_forks.read().unwrap().root(),
⋮----
// reconciliation attempt 2 of 2 with hard fork
// this should be #2 because hard fork root > tower root in almost all cases
⋮----
.map_err(|err| format!("Failed to reconcile blockstore with hard fork: {err:?}"))?;
⋮----
*self.start_progress.write().unwrap() = previous_start_process;
⋮----
pub(crate) fn process_to_create_tower(mut self) -> Result<Tower, String> {
self.process()?;
Ok(self.tower.unwrap())
⋮----
fn maybe_warp_slot(
⋮----
let mut bank_forks = bank_forks.write().unwrap();
let working_bank = bank_forks.working_bank();
if warp_slot <= working_bank.slot() {
⋮----
info!("warping to slot {warp_slot}");
⋮----
// An accounts hash calculation from storages will occur in warp_from_parent() below.  This
// requires that the accounts cache has been flushed, which requires the parent slot to be
// rooted.
root_bank.squash();
root_bank.force_flush_accounts_cache();
bank_forks.insert(Bank::warp_from_parent(
⋮----
bank_forks.set_root(warp_slot, Some(snapshot_controller), Some(warp_slot));
leader_schedule_cache.set_root(&bank_forks.root_bank());
⋮----
&bank_forks.root_bank(),
⋮----
Err(e) => return Err(format!("Unable to create snapshot: {e}")),
⋮----
drop(bank_forks);
// Process blockstore after warping bank forks to make sure tower and
// bank forks are in sync.
process_blockstore.process()?;
⋮----
/// Returns the starting slot at which the blockstore should be scanned for
/// shreds with an incorrect shred version, or None if the check is unnecessary
⋮----
/// shreds with an incorrect shred version, or None if the check is unnecessary
fn should_cleanup_blockstore_incorrect_shred_versions(
⋮----
fn should_cleanup_blockstore_incorrect_shred_versions(
⋮----
// Perform the check if we are booting as part of a cluster restart at slot root_slot
let maybe_cluster_restart_slot = maybe_cluster_restart_with_hard_fork(config, root_slot);
if maybe_cluster_restart_slot.is_some() {
return Ok(Some(root_slot + 1));
⋮----
// If there are no hard forks, the shred version cannot have changed
let Some(latest_hard_fork) = hard_forks.iter().last().map(|(slot, _)| *slot) else {
return Ok(None);
⋮----
// If the blockstore is empty, there are certainly no shreds with an incorrect version
let Some(blockstore_max_slot) = blockstore.highest_slot()? else {
⋮----
let blockstore_min_slot = blockstore.lowest_slot();
⋮----
// latest_hard_fork < blockstore_min_slot <= blockstore_max_slot
//
// All slots in the blockstore are newer than the latest hard fork, and only shreds with
// the correct shred version should have been inserted since the latest hard fork
⋮----
// This is the normal case where the last cluster restart & hard fork was a while ago; we
// can skip the check for this case
Ok(None)
⋮----
// blockstore_min_slot < latest_hard_fork < blockstore_max_slot
⋮----
// This could be a case where there was a cluster restart, but this node was not part of
// the supermajority that actually restarted the cluster. Rather, this node likely
// downloaded a new snapshot while retaining the blockstore, including slots beyond the
// chosen restart slot. We need to perform the blockstore check for this case
⋮----
// Note that the downloaded snapshot slot (root_slot) could be greater than the latest hard
// fork slot. Even though this node will only replay slots after root_slot, start the check
// at latest_hard_fork + 1 to check (and possibly purge) any invalid state.
Ok(Some(latest_hard_fork + 1))
⋮----
// blockstore_min_slot <= blockstore_max_slot <= latest_hard_fork
⋮----
// All slots in the blockstore are older than the latest hard fork. The blockstore check
// would start from latest_hard_fork + 1; skip the check as there are no slots to check
⋮----
// This is kind of an unusual case to hit, maybe a node has been offline for a long time
// and just restarted with a new downloaded snapshot but the old blockstore
⋮----
/// Searches the blockstore for data shreds with a shred version that differs
/// from the passed `expected_shred_version`
⋮----
/// from the passed `expected_shred_version`
fn scan_blockstore_for_incorrect_shred_version(
⋮----
fn scan_blockstore_for_incorrect_shred_version(
⋮----
// Search for shreds with incompatible version in blockstore
let slot_meta_iterator = blockstore.slot_meta_iterator(start_slot)?;
info!("Searching blockstore for shred with incorrect version from slot {start_slot}");
⋮----
let shreds = blockstore.get_data_shreds_for_slot(slot, 0)?;
⋮----
if shred.version() != expected_shred_version {
return Ok(Some(shred.version()));
⋮----
if timer.elapsed() > TIMEOUT {
info!("Didn't find incorrect shreds after 60 seconds, aborting");
⋮----
fn cleanup_blockstore_incorrect_shred_versions(
⋮----
let incorrect_shred_version = scan_blockstore_for_incorrect_shred_version(
⋮----
info!("Only shreds with the correct version were found in the blockstore");
⋮----
let end_slot = blockstore.highest_slot()?.unwrap();
let backup_folder = format!(
⋮----
&blockstore.ledger_path().join(backup_folder),
config.blockstore_options.clone(),
⋮----
info!("Backing up slots from {start_slot} to {end_slot}");
⋮----
let shreds = shreds.into_iter().map(Cow::Owned);
let _ = backup_blockstore.insert_cow_shreds(shreds, None, true);
⋮----
if print_timer.elapsed() > PRINT_INTERVAL {
info!("Backed up {num_slots_copied} slots thus far");
⋮----
info!("Backing up slots done. {timer}");
⋮----
warn!("Unable to backup shreds with incorrect shred version: {err}");
⋮----
info!("Purging slots {start_slot} to {end_slot} from blockstore");
⋮----
blockstore.purge_from_next_slots(start_slot, end_slot);
blockstore.purge_slots(start_slot, end_slot, PurgeType::Exact);
⋮----
info!("Purging slots done. {timer}");
⋮----
fn initialize_rpc_transaction_history_services(
⋮----
let max_complete_transaction_status_slot = Arc::new(AtomicU64::new(blockstore.max_root()));
let (transaction_status_sender, transaction_status_receiver) = unbounded();
let transaction_status_sender = Some(TransactionStatusSender {
⋮----
dependency_tracker: dependency_tracker.clone(),
⋮----
let transaction_status_service = Some(TransactionStatusService::new(
⋮----
max_complete_transaction_status_slot.clone(),
⋮----
pub enum ValidatorError {
⋮----
fn wait_for_supermajority(
⋮----
None => Ok(false),
⋮----
let bank = bank_forks.read().unwrap().working_bank();
match wait_for_supermajority_slot.cmp(&bank.slot()) {
std::cmp::Ordering::Less => return Ok(false),
⋮----
return Err(ValidatorError::NotEnoughLedgerData(
bank.slot(),
⋮----
if bank.hash() != expected_bank_hash {
return Err(ValidatorError::BankHashMismatch(
bank.hash(),
⋮----
get_stake_percent_in_gossip(&bank, cluster_info, logging);
⋮----
rpc_override_health_check.store(true, Ordering::Relaxed);
⋮----
rpc_override_health_check.store(false, Ordering::Relaxed);
Ok(true)
⋮----
fn get_stake_percent_in_gossip(bank: &Bank, cluster_info: &ClusterInfo, log: bool) -> u64 {
⋮----
let mut wrong_shred_nodes = vec![];
⋮----
let mut offline_nodes = vec![];
⋮----
let now = timestamp();
⋮----
.tvu_peers(ContactInfo::clone)
.into_iter()
.filter(|node| {
let age = now.saturating_sub(node.wallclock());
⋮----
.map(|node| (*node.pubkey(), node))
.collect();
let my_shred_version = cluster_info.my_shred_version();
let my_id = cluster_info.id();
for (activated_stake, vote_account) in bank.vote_accounts().values() {
⋮----
let vote_state_node_pubkey = *vote_account.node_pubkey();
if let Some(peer) = peers.get(&vote_state_node_pubkey) {
if peer.shred_version() == my_shred_version {
trace!(
⋮----
wrong_shred_nodes.push((activated_stake, vote_state_node_pubkey));
⋮----
offline_nodes.push((activated_stake, vote_state_node_pubkey));
⋮----
info!("{online_stake_percentage:.3}% of active stake visible in gossip");
if !wrong_shred_nodes.is_empty() {
⋮----
wrong_shred_nodes.sort_by(|b, a| a.0.cmp(&b.0));
⋮----
if !offline_nodes.is_empty() {
⋮----
offline_nodes.sort_by(|b, a| a.0.cmp(&b.0));
⋮----
fn cleanup_accounts_paths(config: &ValidatorConfig) {
⋮----
move_and_async_delete_path_contents(account_path);
⋮----
move_and_async_delete_path_contents(shrink_path);
⋮----
pub fn is_snapshot_config_valid(snapshot_config: &SnapshotConfig) -> bool {
if !snapshot_config.should_generate_snapshots() {
⋮----
mod tests {
⋮----
fn validator_exit() {
⋮----
let leader_node = Node::new_localhost_with_pubkey(&leader_keypair.pubkey());
⋮----
let validator_node = Node::new_localhost_with_pubkey(&validator_keypair.pubkey());
⋮----
create_genesis_config_with_leader(10_000, &leader_keypair.pubkey(), 1000)
⋮----
let (validator_ledger_path, _blockhash) = create_new_tmp_ledger!(&genesis_config);
⋮----
rpc_addrs: Some((
validator_node.info.rpc().unwrap(),
validator_node.info.rpc_pubsub().unwrap(),
⋮----
&voting_keypair.pubkey(),
Arc::new(RwLock::new(vec![voting_keypair])),
vec![leader_node.info],
⋮----
start_progress.clone(),
⋮----
.expect("assume successful validator start");
⋮----
validator.close();
remove_dir_all(validator_ledger_path).unwrap();
⋮----
fn test_should_cleanup_blockstore_incorrect_shred_versions() {
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
validator_config.wait_for_supermajority = Some(root_slot);
⋮----
hard_forks.register(10);
⋮----
blockstore.insert_shreds(shreds, None, true).unwrap();
⋮----
hard_forks.register(root_slot);
⋮----
let latest_hard_fork = hard_forks.iter().last().unwrap().0;
⋮----
blockstore.purge_slots(0, latest_hard_fork, PurgeType::Exact);
⋮----
fn test_cleanup_blockstore_incorrect_shred_versions() {
⋮----
cleanup_blockstore_incorrect_shred_versions(&blockstore, &validator_config, 5, 2).unwrap();
assert!(blockstore.meta(4).unwrap().unwrap().next_slots.is_empty());
⋮----
assert!(blockstore
⋮----
fn validator_parallel_exit() {
⋮----
let mut ledger_paths = vec![];
⋮----
.map(|_| {
⋮----
ledger_paths.push(validator_ledger_path.clone());
⋮----
&vote_account_keypair.pubkey(),
Arc::new(RwLock::new(vec![Arc::new(vote_account_keypair)])),
vec![leader_node.info.clone()],
⋮----
.expect("assume successful validator start")
⋮----
validators.iter_mut().for_each(|v| v.exit());
let (sender, receiver) = bounded(0);
⋮----
validators.into_iter().for_each(|validator| {
validator.join();
⋮----
sender.send(()).unwrap();
⋮----
if let Err(RecvTimeoutError::Timeout) = receiver.recv_timeout(timeout) {
panic!("timeout for shutting down validators",);
⋮----
remove_dir_all(path).unwrap();
⋮----
fn test_wait_for_supermajority() {
⋮----
ContactInfo::new_localhost(&node_keypair.pubkey(), timestamp()),
⋮----
let (genesis_config, _mint_keypair) = create_genesis_config(1);
⋮----
assert!(!wait_for_supermajority(
⋮----
config.wait_for_supermajority = Some(1);
assert!(matches!(
⋮----
bank_forks.read().unwrap().root_bank(),
⋮----
config.wait_for_supermajority = Some(0);
⋮----
config.expected_bank_hash = Some(hash(&[1]));
⋮----
fn test_is_snapshot_config_valid() {
fn new_snapshot_config(
⋮----
NonZeroU64::new(full_snapshot_archive_interval_slots).unwrap(),
⋮----
NonZeroU64::new(incremental_snapshot_archive_interval_slots).unwrap(),
⋮----
assert!(is_snapshot_config_valid(&SnapshotConfig::default()));
assert!(is_snapshot_config_valid(&SnapshotConfig {
⋮----
assert!(!is_snapshot_config_valid(&SnapshotConfig {
⋮----
assert!(is_snapshot_config_valid(&new_snapshot_config(400, 200)));
assert!(is_snapshot_config_valid(&new_snapshot_config(100, 42)));
assert!(is_snapshot_config_valid(&new_snapshot_config(444, 200)));
assert!(is_snapshot_config_valid(&new_snapshot_config(400, 222)));
assert!(!is_snapshot_config_valid(&new_snapshot_config(42, 100)));
assert!(!is_snapshot_config_valid(&new_snapshot_config(100, 100)));
assert!(!is_snapshot_config_valid(&new_snapshot_config(100, 200)));
assert!(is_snapshot_config_valid(&SnapshotConfig::new_disabled()));
assert!(is_snapshot_config_valid(&SnapshotConfig::new_load_only()));
⋮----
fn target_tick_duration() -> Duration {
⋮----
assert_eq!(target_tick_duration_us, 6250);
⋮----
fn test_poh_speed() {
⋮----
target_tick_duration: target_tick_duration(),
hashes_per_tick: Some(100 * solana_clock::DEFAULT_HASHES_PER_TICK),
⋮----
assert!(check_poh_speed(&bank, Some(10_000)).is_err());
⋮----
fn test_poh_speed_no_hashes_per_tick() {
⋮----
check_poh_speed(&bank, Some(10_000)).unwrap();

================
File: core/src/vortexor_receiver_adapter.rs
================
fn send(sender: &TracedSender, batch: Arc<Vec<PacketBatch>>, count: usize) -> Result<(), String> {
match sender.send(batch) {
⋮----
trace!("Sent batch: {count} received from vortexor successfully");
Ok(())
⋮----
Err(err) => Err(format!("Failed to send batch {count} down {err:?}")),
⋮----
pub struct VortexorReceiverAdapter {
⋮----
impl VortexorReceiverAdapter {
pub fn new(
⋮----
let (batch_sender, batch_receiver) = unbounded();
let receiver = VerifiedPacketReceiver::new(sockets, &batch_sender, None, exit.clone());
⋮----
.name("vtxRcvAdptr".to_string())
.spawn(move || {
⋮----
info!("Quitting VortexorReceiverAdapter: {msg}");
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()?;
self.receiver.join()
⋮----
fn recv_send(
⋮----
match Self::receive_until(packet_batch_receiver.clone(), recv_timeout, batch_size) {
⋮----
let count = packet_batch.len();
⋮----
send(&traced_sender, packet_batch.clone(), count)?;
⋮----
.try_send((packet_batch, false ));
⋮----
send(&traced_sender, packet_batch, count)?;
⋮----
return Err("Disconnected from the input channel".to_string());
⋮----
fn receive_until(
⋮----
let message = packet_batch_receiver.recv_timeout(recv_timeout)?;
⋮----
packet_batches.push(message);
while let Ok(message) = packet_batch_receiver.try_recv() {
⋮----
if start.elapsed() >= recv_timeout || packet_batches.len() >= batch_size {
⋮----
Ok(Arc::new(packet_batches))

================
File: core/src/vote_simulator.rs
================
pub struct VoteSimulator {
⋮----
impl VoteSimulator {
pub fn new(num_keypairs: usize) -> Self {
⋮----
pub fn fill_bank_forks(
⋮----
let root = *forks.root().data();
assert!(self.bank_forks.read().unwrap().get(root).is_some());
⋮----
while let Some(visit) = walk.get() {
let slot = *visit.node().data();
if self.bank_forks.read().unwrap().get(slot).is_some() {
walk.forward();
⋮----
let parent = *walk.get_parent().unwrap().data();
let parent_bank = self.bank_forks.read().unwrap().get(parent).unwrap();
let new_bank = Bank::new_from_parent(parent_bank.clone(), &Pubkey::default(), slot);
⋮----
.write()
.unwrap()
.insert(new_bank)
.clone_without_scheduler();
⋮----
.entry(slot)
.or_insert_with(|| ForkProgress::new(Hash::default(), None, None, 0, 0));
for (pubkey, vote) in cluster_votes.iter() {
if vote.contains(&parent) {
let keypairs = self.validator_keypairs.get(pubkey).unwrap();
let latest_blockhash = parent_bank.last_blockhash();
⋮----
parent_bank.get_vote_account(&keypairs.vote_keypair.pubkey())
⋮----
let mut vote_state = TowerVoteState::from(vote_account.vote_state_view());
vote_state.process_next_vote_slot(parent);
⋮----
parent_bank.hash(),
⋮----
Some(root),
⋮----
info!("voting {} {}", parent_bank.slot(), parent_bank.hash());
new_bank.process_transaction(&vote_tx).unwrap();
⋮----
.get_vote_account(&keypairs.vote_keypair.pubkey())
.unwrap();
let vote_state_view = vote_account.vote_state_view();
assert!(vote_state_view
⋮----
new_bank.fill_bank_with_ticks_for_tests();
if !visit.node().has_no_child() || is_frozen {
new_bank.set_block_id(Some(Hash::new_unique()));
new_bank.freeze();
⋮----
.get_fork_stats_mut(new_bank.slot())
.expect("All frozen banks must exist in the Progress map")
.bank_hash = Some(new_bank.hash());
⋮----
.add_new_leaf_slot(
(new_bank.slot(), new_bank.hash()),
Some((new_bank.parent_slot(), new_bank.parent_hash())),
⋮----
pub fn simulate_vote(
⋮----
let ancestors = self.bank_forks.read().unwrap().ancestors();
⋮----
.read()
⋮----
.frozen_banks()
.map(|(_slot, bank)| bank)
.collect();
⋮----
.get(vote_slot)
.expect("Bank must have been created before vote simulation");
let descendants = self.bank_forks.read().unwrap().descendants();
⋮----
} = select_vote_and_reset_forks(
⋮----
info!("Checking vote: {}", vote_bank.slot());
if !heaviest_fork_failures.is_empty() {
⋮----
let new_root = tower.record_bank_vote(&vote_bank);
⋮----
self.set_root(new_root);
⋮----
vec![]
⋮----
pub fn set_root(&mut self, new_root: Slot) {
let (drop_bank_sender, _drop_bank_receiver) = unbounded();
⋮----
pub fn create_and_vote_new_branch(
⋮----
.filter_map(|slot| {
let mut fork_tip_parent = tr(slot - 1);
fork_tip_parent.push_front(tr(slot));
self.fill_bank_forks(fork_tip_parent, cluster_votes, true);
if votes_to_simulate.contains(&slot) {
Some((slot, self.simulate_vote(slot, my_pubkey, tower)))
⋮----
.collect()
⋮----
pub fn simulate_lockout_interval(
⋮----
.or_insert_with(|| ForkProgress::new(Hash::default(), None, None, 0, 0))
⋮----
.push(LockoutInterval {
⋮----
pub fn clear_lockout_intervals(&mut self, slot: Slot) {
⋮----
.clear()
⋮----
pub fn can_progress_on_fork(
⋮----
let old_root = tower.root();
⋮----
let mut fork_tip_parent = tr(start_slot + i - 1);
fork_tip_parent.push_front(tr(start_slot + i));
⋮----
.simulate_vote(i + start_slot, my_pubkey, tower)
.is_empty()
⋮----
.entry(*my_pubkey)
.or_default()
.push(start_slot + i);
⋮----
if old_root != tower.root() {
⋮----
fn init_state(
⋮----
(vote_keypairs.node_keypair.pubkey(), vote_keypairs)
⋮----
.take(num_keypairs)
⋮----
.values()
.map(|keys| keys.node_keypair.pubkey())
⋮----
.map(|keys| keys.vote_keypair.pubkey())
⋮----
initialize_state(&keypairs, 10_000);
⋮----
pub fn initialize_state(
⋮----
let validator_keypairs: Vec<_> = validator_keypairs_map.values().collect();
⋮----
} = create_genesis_config_with_vote_accounts(
⋮----
vec![stake; validator_keypairs.len()],
⋮----
genesis_config.poh_config.hashes_per_tick = Some(2);
⋮----
bank0.set_block_id(Some(Hash::new_unique()));
for pubkey in validator_keypairs_map.keys() {
bank0.transfer(10_000, &mint_keypair, pubkey).unwrap();
⋮----
bank0.fill_bank_with_ticks_for_tests();
bank0.freeze();
⋮----
progress.insert(
⋮----
ForkProgress::new_from_bank(&bank0, bank0.collector_id(), &Pubkey::default(), None, 0, 0),
⋮----
HeaviestSubtreeForkChoice::new_from_bank_forks(bank_forks.clone());

================
File: core/src/voting_service.rs
================
pub enum VoteOp {
⋮----
impl VoteOp {
fn tx(&self) -> &Transaction {
⋮----
enum SendVoteError {
⋮----
fn send_vote_transaction(
⋮----
.or_else(|| {
⋮----
.my_contact_info()
.tpu(connection_cache.protocol())
⋮----
.ok_or(SendVoteError::InvalidTpuAddress)?;
let buf = Arc::new(serialize(transaction)?);
let client = connection_cache.get_connection(&tpu);
client.send_data_async(buf).map_err(|err| {
error!("Ran into an error when sending vote: {err:?} to {tpu:?}");
⋮----
pub struct VotingService {
⋮----
impl VotingService {
pub fn new(
⋮----
.name("solVoteService".to_string())
.spawn({
let mut mock_alpenglow = alpenglow_socket.map(|s| {
⋮----
cluster_info.clone(),
EpochSpecs::from(bank_forks.clone()),
⋮----
for vote_op in vote_receiver.iter() {
⋮----
} => tower_slots.iter().copied().last(),
⋮----
tower_storage.as_ref(),
⋮----
connection_cache.clone(),
⋮----
if let Some(ag) = mock_alpenglow.as_mut() {
let root_bank = { bank_forks.read().unwrap().root_bank() };
ag.signal_new_slot(slot, &root_bank);
⋮----
let _ = ag.join();
⋮----
.unwrap();
⋮----
pub fn handle_vote(
⋮----
if let Err(err) = tower_storage.store(saved_tower) {
error!("Unable to save tower to storage: {err:?}");
⋮----
measure.stop();
trace!("{measure}");
⋮----
FORWARD_TRANSACTIONS_TO_LEADER_AT_SLOT_OFFSET.saturating_add(1);
⋮----
let upcoming_leader_sockets = upcoming_leader_tpu_vote_sockets(
⋮----
connection_cache.protocol(),
⋮----
if !upcoming_leader_sockets.is_empty() {
⋮----
let _ = send_vote_transaction(
⋮----
vote_op.tx(),
Some(tpu_vote_socket),
⋮----
let _ = send_vote_transaction(cluster_info, vote_op.tx(), None, &connection_cache);
⋮----
cluster_info.push_vote(&tower_slots, tx);
⋮----
cluster_info.refresh_vote(tx, last_voted_slot);
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()

================
File: core/src/warm_quic_cache_service.rs
================
pub struct WarmQuicCacheService {
⋮----
impl WarmQuicCacheService {
fn warmup_connection(
⋮----
cluster_info.lookup_contact_info(leader_pubkey, contact_info_selector)
⋮----
let conn = connection_cache.get_connection(&addr);
if let Err(err) = conn.send_data(&[]) {
warn!(
⋮----
pub fn new(
⋮----
assert!(matches!(
⋮----
.name("solWarmQuicSvc".to_string())
.spawn(move || {
let slot_jitter = thread_rng().gen_range(-CACHE_JITTER_SLOT..CACHE_JITTER_SLOT);
⋮----
while !exit.load(Ordering::Relaxed) {
⋮----
.read()
.unwrap()
.leader_after_n_slots((CACHE_OFFSET_SLOT + slot_jitter) as u64);
⋮----
if maybe_last_leader != Some(leader_pubkey) {
maybe_last_leader = Some(leader_pubkey);
⋮----
tpu_connection_cache.as_deref(),
⋮----
|node| node.tpu(Protocol::QUIC),
⋮----
vote_connection_cache.as_deref(),
⋮----
|node| node.tpu_vote(Protocol::QUIC),
⋮----
sleep(Duration::from_millis(200));
⋮----
.unwrap();
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_hdl.join()

================
File: core/src/window_service.rs
================
type DuplicateSlotSender = Sender<Slot>;
pub(crate) type DuplicateSlotReceiver = Receiver<Slot>;
⋮----
struct WindowServiceMetrics {
⋮----
impl WindowServiceMetrics {
fn report_metrics(&self, metric_name: &'static str) {
datapoint_info!(
⋮----
fn record_error(&mut self, err: &Error) {
⋮----
error!("blockstore error: {err}");
⋮----
fn run_check_duplicate(
⋮----
let mut root_bank = bank_forks.read().unwrap().root_bank();
⋮----
if last_updated.elapsed().as_millis() as u64 > DEFAULT_MS_PER_SLOT {
// Grabs bank forks lock once a slot
⋮----
root_bank = bank_forks.read().unwrap().root_bank();
⋮----
let shred_slot = shred.slot();
⋮----
// Although this proof can be immediately stored on detection, we wait until
// here in order to check the feature flag, as storage in blockstore can
// preclude the detection of other duplicate proofs in this slot
if blockstore.has_duplicate_shreds_in_slot(shred_slot) {
return Ok(());
⋮----
blockstore.store_duplicate_slot(
⋮----
conflict.clone(),
shred.clone().into_payload(),
⋮----
// Unlike the other cases we have to wait until here to decide to handle the duplicate and store
// in blockstore. This is because the duplicate could have been part of the same insert batch,
// so we wait until the batch has been written.
⋮----
return Ok(()); // A duplicate is already recorded
⋮----
let Some(existing_shred_payload) = blockstore.is_shred_duplicate(&shred) else {
return Ok(()); // Not a duplicate
⋮----
existing_shred_payload.clone(),
⋮----
// Propagate duplicate proof through gossip
cluster_info.push_duplicate_shred(&shred1, &shred2)?;
// Notify duplicate consensus state machine
duplicate_slots_sender.send(shred_slot)?;
Ok(())
⋮----
std::iter::once(shred_receiver.recv_timeout(RECV_TIMEOUT)?)
.chain(shred_receiver.try_iter())
.try_for_each(check_duplicate)
⋮----
fn run_insert<F>(
⋮----
verified_receiver: &Receiver<Vec<(shred::Payload, /*is_repaired:*/ bool)>>,
⋮----
let mut shreds = verified_receiver.recv_timeout(RECV_TIMEOUT)?;
shreds.extend(verified_receiver.try_iter().flatten());
shred_receiver_elapsed.stop();
ws_metrics.shred_receiver_elapsed_us += shred_receiver_elapsed.as_us();
⋮----
ws_metrics.num_repairs.fetch_add(1, Ordering::Relaxed);
⋮----
let shred = Shred::new_from_serialized_shred(shred).ok()?;
Some((Cow::Owned(shred), repair))
⋮----
let shreds: Vec<_> = thread_pool.install(|| {
⋮----
.into_par_iter()
.with_min_len(32)
.filter_map(handle_shred)
.collect()
⋮----
ws_metrics.handle_packets_elapsed_us += now.elapsed().as_micros() as u64;
ws_metrics.num_shreds_received += shreds.len();
let completed_data_sets = blockstore.insert_shreds_handle_duplicate(
⋮----
Some(leader_schedule_cache),
false, // is_trusted
⋮----
sender.try_send(completed_data_sets)?;
⋮----
pub struct WindowServiceChannels {
pub verified_receiver: Receiver<Vec<(shred::Payload, /*is_repaired:*/ bool)>>,
⋮----
impl WindowServiceChannels {
pub fn new(
verified_receiver: Receiver<Vec<(shred::Payload, /*is_repaired:*/ bool)>>,
⋮----
pub(crate) struct WindowService {
⋮----
impl WindowService {
pub(crate) fn new(
⋮----
let cluster_info = repair_info.cluster_info.clone();
let bank_forks = repair_info.bank_forks.clone();
// In wen_restart, we discard all shreds from Turbine and keep only those from repair to
// avoid new shreds make validator OOM before wen_restart is over.
let accept_repairs_only = repair_info.wen_restart_repair_slots.is_some();
⋮----
blockstore.clone(),
exit.clone(),
⋮----
outstanding_repair_requests.clone(),
⋮----
let (duplicate_sender, duplicate_receiver) = unbounded();
⋮----
fn start_check_duplicate_thread(
⋮----
inc_new_counter_error!("solana-check-duplicate-error", 1, 1);
⋮----
.name("solWinCheckDup".to_string())
.spawn(move || {
while !exit.load(Ordering::Relaxed) {
if let Err(e) = run_check_duplicate(
⋮----
.unwrap()
⋮----
fn start_window_insert_thread(
⋮----
inc_new_counter_error!("solana-window-insert-error", 1, 1);
⋮----
.name("solWinInsert".to_string())
⋮----
.num_threads(get_thread_count().min(8))
// Use the current thread as one of the workers. This reduces overhead when the
// pool is used to process a small number of shreds, since they'll be processed
.use_current_thread()
.thread_name(|i| format!("solWinInsert{i:02}"))
.build()
.unwrap();
⋮----
let _ = check_duplicate_sender.send(possible_duplicate_shred);
⋮----
if let Err(e) = run_insert(
⋮----
completed_data_sets_sender.as_ref(),
⋮----
ws_metrics.record_error(&e);
⋮----
if last_print.elapsed().as_secs() > 2 {
metrics.report_metrics("blockstore-insert-shreds");
⋮----
ws_metrics.report_metrics("recv-window-insert-shreds");
⋮----
fn should_exit_on_error<H>(e: Error, handle_error: &H) -> bool
⋮----
handle_error();
error!("thread {:?} error {:?}", thread::current().name(), e);
⋮----
pub(crate) fn join(self) -> thread::Result<()> {
self.t_insert.join()?;
self.t_check_duplicate.join()?;
self.repair_service.join()
⋮----
mod test {
⋮----
fn local_entries_to_shred(
⋮----
let shredder = Shredder::new(slot, parent, 0, 0).unwrap();
let (data_shreds, _) = shredder.entries_to_merkle_shreds_for_tests(
⋮----
Hash::new_from_array(rand::thread_rng().gen()),
⋮----
fn test_process_shred() {
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
let original_entries = create_ticks(num_entries, 0, Hash::default());
let mut shreds = local_entries_to_shred(&original_entries, 0, 0, &Keypair::new());
shreds.reverse();
⋮----
.insert_shreds(shreds, None, false)
.expect("Expect successful processing of shred");
assert_eq!(blockstore.get_slot_entries(0, 0).unwrap(), original_entries);
⋮----
fn test_run_check_duplicate() {
⋮----
let genesis_config = create_genesis_config(10_000).genesis_config;
⋮----
let (sender, receiver) = unbounded();
let (duplicate_slot_sender, duplicate_slot_receiver) = unbounded();
let (shreds, _) = make_many_slot_entries(5, 5, 10);
⋮----
.insert_shreds(shreds.clone(), None, false)
⋮----
let original_shred = shreds[duplicate_index].clone();
⋮----
let (mut shreds, _) = make_many_slot_entries(5, 1, 10);
shreds.swap_remove(duplicate_index)
⋮----
assert_eq!(duplicate_shred.slot(), shreds[0].slot());
let duplicate_shred_slot = duplicate_shred.slot();
⋮----
.send(PossibleDuplicateShred::Exists(duplicate_shred.clone()))
⋮----
assert!(!blockstore.has_duplicate_shreds_in_slot(duplicate_shred_slot));
⋮----
let contact_info = ContactInfo::new_localhost(&keypair.pubkey(), timestamp());
⋮----
run_check_duplicate(
⋮----
let duplicate_proof = blockstore.get_duplicate_slot(duplicate_shred_slot).unwrap();
assert_eq!(duplicate_proof.shred1, *original_shred.payload());
assert_eq!(duplicate_proof.shred2, *duplicate_shred.payload());
assert_eq!(
⋮----
fn test_store_duplicate_shreds_same_batch() {
⋮----
let (duplicate_shred_sender, duplicate_shred_receiver) = unbounded();
⋮----
let _ = duplicate_shred_sender.send(shred);
⋮----
let (shreds, _) = make_many_slot_entries(slot, 1, 10);
⋮----
let (mut shreds, _) = make_many_slot_entries(slot, 1, 10);
⋮----
assert_eq!(duplicate_shred.slot(), slot);
⋮----
.into_iter()
.map(|shred| (Cow::Borrowed(shred),  false));
⋮----
.insert_shreds_handle_duplicate(
⋮----
let duplicate_proof = blockstore.get_duplicate_slot(slot).unwrap();
⋮----
exit.store(true, Ordering::Relaxed);
t_check_duplicate.join().unwrap();

================
File: core/tests/bam_connection.rs
================
fn v0_response(resp: Resp) -> SchedulerResponse {
⋮----
versioned_msg: Some(
⋮----
SchedulerResponseV0 { resp: Some(resp) },
⋮----
fn heartbeat_response() -> SchedulerResponse {
v0_response(Resp::HeartBeat(BuilderHeartBeat {
⋮----
.duration_since(SystemTime::UNIX_EPOCH)
.unwrap()
.as_micros() as u64,
⋮----
struct MockConfig {
⋮----
impl Default for MockConfig {
fn default() -> Self {
⋮----
builder_pubkey: "11111111111111111111111111111111".to_string(),
⋮----
prio_fee_recipient: "22222222222222222222222222222222".to_string(),
⋮----
struct MockBamNode {
⋮----
impl MockBamNode {
fn new(send_heartbeats: Arc<AtomicBool>, heartbeat_interval: Duration) -> Self {
⋮----
fn with_config(self, config: MockConfig) -> Self {
*self.config.lock().unwrap() = config;
⋮----
impl BamNodeApi for MockBamNode {
async fn get_auth_challenge(
⋮----
Ok(Response::new(AuthChallengeResponse {
challenge_to_sign: "test-challenge-12345".to_string(),
⋮----
async fn get_builder_config(
⋮----
let config = self.config.lock().unwrap().clone();
Ok(Response::new(ConfigResponse {
block_engine_config: Some(BlockEngineBuilderConfig {
⋮----
bam_config: Some(BamConfig {
⋮----
tpu_sock: Some(Socket {
ip: "127.0.0.1".to_string(),
⋮----
tpu_fwd_sock: Some(Socket {
⋮----
type InitSchedulerStreamStream = ReceiverStream<Result<SchedulerResponse, Status>>;
async fn init_scheduler_stream(
⋮----
let mut inbound = request.into_inner();
⋮----
let send_heartbeats = self.send_heartbeats.clone();
⋮----
let auth_proofs_received = self.auth_proofs_received.clone();
let batch_to_send = self.batch_to_send.clone();
⋮----
while let Ok(Some(msg)) = inbound.message().await {
⋮----
auth_proofs_received.fetch_add(1, Ordering::Relaxed);
⋮----
if send_heartbeats.load(Ordering::Relaxed)
&& tx.send(Ok(heartbeat_response())).await.is_err()
⋮----
let batch = batch_to_send.lock().unwrap().take();
⋮----
let resp = v0_response(Resp::MultipleAtomicTxnBatch(MultipleAtomicTxnBatch {
batches: vec![batch],
⋮----
if tx.send(Ok(resp)).await.is_err() {
⋮----
Ok(Response::new(ReceiverStream::new(rx)))
⋮----
struct MockServerHandle {
⋮----
async fn start_mock_server(
⋮----
let mock = MockBamNode::new(send_heartbeats.clone(), heartbeat_interval);
let config = mock.config.clone();
let batch_to_send = mock.batch_to_send.clone();
let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
let addr = listener.local_addr().unwrap();
⋮----
let server_started_clone = server_started.clone();
⋮----
server_started_clone.store(true, Ordering::Relaxed);
⋮----
.add_service(BamNodeApiServer::new(mock))
.serve_with_incoming(tokio_stream::wrappers::TcpListenerStream::new(listener))
⋮----
while !server_started.load(Ordering::Relaxed) {
⋮----
fn create_test_cluster_info() -> Arc<ClusterInfo> {
⋮----
let node = Node::new_localhost_with_pubkey(&keypair.pubkey());
⋮----
fn create_channels() -> (
⋮----
async fn wait_until_healthy(
⋮----
while start.elapsed() < timeout {
if connection.is_healthy() && connection.get_latest_config().is_some() {
⋮----
mod bam_connection_tests {
⋮----
async fn test_connection_healthy_with_heartbeats() {
⋮----
let server = start_mock_server(send_heartbeats, Duration::from_secs(2)).await;
let cluster_info = create_test_cluster_info();
let (batch_tx, _batch_rx, _outbound_tx, outbound_rx) = create_channels();
⋮----
format!("http://{}", server.addr),
⋮----
.expect("should connect");
assert!(
⋮----
assert!(connection.is_healthy());
assert!(connection.get_latest_config().is_some());
⋮----
async fn test_connection_unhealthy_when_heartbeats_stop() {
⋮----
let server = start_mock_server(send_heartbeats.clone(), Duration::from_millis(500)).await;
⋮----
server.send_heartbeats.store(false, Ordering::Relaxed);
⋮----
async fn test_connection_stays_unhealthy_without_initial_heartbeats() {
⋮----
let became_healthy = wait_until_healthy(&connection, Duration::from_secs(3)).await;
assert!(!became_healthy);
assert!(!connection.is_healthy());
⋮----
async fn test_connection_fails_when_server_unreachable() {
⋮----
"http://127.0.0.1:1".to_string(),
⋮----
assert!(result.is_err());
⋮----
async fn test_config_available_when_healthy() {
⋮----
let server = start_mock_server(send_heartbeats, Duration::from_secs(1)).await;
⋮----
assert!(wait_until_healthy(&connection, Duration::from_secs(10)).await);
let config = connection.get_latest_config().expect("config should exist");
let bam_config = config.bam_config.expect("bam_config should exist");
assert_eq!(bam_config.commission_bps, 100);
⋮----
async fn test_batches_forwarded_to_receiver() {
⋮----
let server = start_mock_server(send_heartbeats, Duration::from_millis(100)).await;
⋮----
let (batch_tx, batch_rx, _outbound_tx, outbound_rx) = create_channels();
⋮----
packets: vec![Packet {
⋮----
*server.batch_to_send.lock().unwrap() = Some(test_batch);
⋮----
let received = batch_rx.try_recv().expect("should receive batch");
assert_eq!(received.seq_id, 42);
assert_eq!(received.max_schedule_slot, 100);
⋮----
async fn test_drop_sets_unhealthy() {
⋮----
drop(connection);
⋮----
mod bam_manager_tests {
⋮----
fn create_test_bam_dependencies(
⋮----
fn bam_is_connected(bam_enabled: &Arc<AtomicU8>) -> bool {
BamConnectionState::from_u8(bam_enabled.load(Ordering::Relaxed))
⋮----
fn create_test_poh_recorder() -> (
⋮----
let genesis_config = create_genesis_config(1000).genesis_config;
⋮----
let ledger_path = tempfile::tempdir().unwrap();
⋮----
Arc::new(Blockstore::open(ledger_path.path()).expect("Failed to open blockstore"));
⋮----
create_test_recorder(bank, blockstore, None, None);
⋮----
async fn test_bam_enabled_when_connection_healthy() {
⋮----
let server = start_mock_server(send_heartbeats.clone(), Duration::from_secs(1)).await;
⋮----
create_test_poh_recorder();
let dependencies = create_test_bam_dependencies(cluster_info, bank_forks);
let bam_enabled = dependencies.bam_enabled.clone();
let bam_url = Arc::new(Mutex::new(Some(format!("http://{}", server.addr))));
⋮----
exit.clone(),
⋮----
while start.elapsed() < Duration::from_secs(15) {
if bam_is_connected(&bam_enabled) {
⋮----
assert!(bam_is_connected(&bam_enabled), "bam_enabled should be true");
exit.store(true, Ordering::Relaxed);
poh_service.join().unwrap();
⋮----
async fn test_bam_disabled_when_connection_unhealthy() {
⋮----
assert!(bam_is_connected(&bam_enabled));
⋮----
if !bam_is_connected(&bam_enabled) {
⋮----
async fn test_bam_disabled_when_no_url() {
⋮----
async fn test_reconnects_when_url_changes() {
⋮----
let server1 = start_mock_server(send_heartbeats.clone(), Duration::from_secs(1)).await;
let server2 = start_mock_server(send_heartbeats.clone(), Duration::from_secs(1)).await;
⋮----
let bam_url = Arc::new(Mutex::new(Some(format!("http://{}", server1.addr))));
⋮----
bam_url.clone(),
⋮----
*bam_url.lock().unwrap() = Some(format!("http://{}", server2.addr));
⋮----
async fn test_block_builder_fee_info_updated() {
⋮----
let block_builder_fee_info = dependencies.block_builder_fee_info.clone();
⋮----
let fee_info = block_builder_fee_info.lock().unwrap();
assert_eq!(fee_info.block_builder_commission, 10);
⋮----
drop(fee_info);

================
File: core/tests/fork-selection.rs
================
extern crate rand;
⋮----
pub struct Fork {
⋮----
impl Fork {
fn is_trunk_of(&self, other: &Fork, fork_tree: &HashMap<usize, Fork>) -> bool {
⋮----
assert!(fork_tree.get(&0).is_none());
⋮----
if fork_tree.get(&current.base).is_none() {
⋮----
current = fork_tree.get(&current.base).unwrap();
⋮----
pub struct Vote {
⋮----
impl Vote {
pub fn new(fork: Fork, time: usize) -> Vote {
⋮----
pub fn lock_height(&self) -> usize {
⋮----
pub fn is_trunk_of(&self, other: &Vote, fork_tree: &HashMap<usize, Fork>) -> bool {
self.fork.is_trunk_of(&other.fork, fork_tree)
⋮----
pub struct Tower {
⋮----
impl Tower {
pub fn new(max_size: usize, converge_depth: usize, delay_count: usize) -> Self {
⋮----
pub fn submit_vote(
⋮----
.get_vote(self.converge_depth)
.map(|v| v.is_trunk_of(&vote, fork_tree))
.unwrap_or(true);
⋮----
self.delayed_votes.push_front(vote);
⋮----
if self.delayed_votes.len() <= self.delay_count {
⋮----
let votes = self.pop_best_votes(fork_tree, scores);
⋮----
self.push_vote(vote, fork_tree, converge_map);
⋮----
let trunk = self.votes.get(self.converge_depth).cloned();
⋮----
self.delayed_votes.retain(|v| v.fork.id > t.fork.id);
⋮----
pub fn pop_best_votes(
⋮----
.iter()
.enumerate()
.map(|(i, v)| (*scores.get(v).unwrap_or(&0), v.time, i))
.collect();
best.sort_unstable();
⋮----
best.reverse();
⋮----
.last()
.and_then(|v| self.delayed_votes.remove(v.2))
.into_iter()
⋮----
if votes.is_empty() {
⋮----
for i in 0..self.delayed_votes.len() {
⋮----
v.is_trunk_of(votes.front().unwrap(), fork_tree)
⋮----
votes.push_front(self.delayed_votes.remove(i).unwrap());
⋮----
pub fn push_vote(
⋮----
self.rollback(vote.time);
if !self.is_valid(&vote, fork_tree) {
⋮----
if !self.is_converged(converge_map) {
⋮----
self.process_vote(vote);
if self.is_full() {
self.pop_full();
⋮----
fn is_converged(&self, converge_map: &HashMap<usize, usize>) -> bool {
self.get_vote(self.converge_depth)
.map(|v| {
let v = *converge_map.get(&v.fork.id).unwrap_or(&0);
assert!(v <= 100);
⋮----
.unwrap_or(true)
⋮----
pub fn score(&self, vote: &Vote, fork_tree: &HashMap<usize, Fork>) -> usize {
let st = self.rollback_count(vote.time);
if st < self.votes.len() && !self.votes[st].is_trunk_of(vote, fork_tree) {
⋮----
for i in st..self.votes.len() {
⋮----
fn rollback_count(&self, time: usize) -> usize {
⋮----
for (i, v) in self.votes.iter().enumerate() {
if v.lock_height() < time {
⋮----
fn rollback(&mut self, time: usize) {
let last = self.rollback_count(time);
⋮----
self.votes.pop_front();
⋮----
fn is_valid(&self, vote: &Vote, fork_tree: &HashMap<usize, Fork>) -> bool {
self.last_fork().is_trunk_of(&vote.fork, fork_tree)
⋮----
fn process_vote(&mut self, vote: Vote) {
⋮----
assert!(!self.is_full());
assert_eq!(vote.lockout, 2);
self.votes.push_front(vote);
for i in 1..self.votes.len() {
assert!(self.votes[i].time <= vote_time);
⋮----
fn pop_full(&mut self) {
assert!(self.is_full());
self.fork_trunk = self.votes.pop_back().unwrap().fork;
⋮----
fn is_full(&self) -> bool {
assert!(self.votes.len() <= self.max_size);
self.votes.len() == self.max_size
⋮----
fn last_vote(&self) -> Option<&Vote> {
self.votes.front()
⋮----
fn get_vote(&self, ix: usize) -> Option<&Vote> {
self.votes.get(ix)
⋮----
pub fn first_vote(&self) -> Option<&Vote> {
self.votes.back()
⋮----
pub fn last_fork(&self) -> Fork {
self.last_vote()
.map(|v| v.fork.clone())
.unwrap_or_else(|| self.fork_trunk.clone())
⋮----
fn test_is_trunk_of_1() {
⋮----
assert!(!b1.is_trunk_of(&b2, &tree));
⋮----
fn test_is_trunk_of_2() {
⋮----
fn test_is_trunk_of_3() {
⋮----
assert!(b1.is_trunk_of(&b2, &tree));
⋮----
fn test_is_trunk_of_4() {
⋮----
tree.insert(b1.id, b1.clone());
⋮----
assert!(!b2.is_trunk_of(&b1, &tree));
⋮----
fn test_push_vote() {
⋮----
let vote = Vote::new(b0.clone(), 0);
assert!(tower.push_vote(vote, &tree, &bmap));
assert_eq!(tower.votes.len(), 1);
let vote = Vote::new(b0.clone(), 1);
⋮----
assert_eq!(tower.votes.len(), 2);
let vote = Vote::new(b0.clone(), 2);
⋮----
assert_eq!(tower.votes.len(), 3);
let vote = Vote::new(b0.clone(), 3);
⋮----
assert_eq!(tower.votes.len(), 4);
assert_eq!(tower.votes[0].lockout, 2);
assert_eq!(tower.votes[1].lockout, 4);
assert_eq!(tower.votes[2].lockout, 8);
assert_eq!(tower.votes[3].lockout, 16);
assert_eq!(tower.votes[1].lock_height(), 6);
assert_eq!(tower.votes[2].lock_height(), 9);
let vote = Vote::new(b0.clone(), 7);
⋮----
assert!(!tower.push_vote(vote, &tree, &bmap));
let vote = Vote::new(b0.clone(), 8);
⋮----
assert_eq!(tower.votes[1].lockout, 16);
⋮----
fn create_towers(sz: usize, height: usize, delay_count: usize) -> Vec<Tower> {
⋮----
.map(|_| Tower::new(32, height, delay_count))
.collect()
⋮----
fn calc_fork_depth(fork_tree: &HashMap<usize, Fork>, id: usize) -> usize {
⋮----
let mut start = fork_tree.get(&id);
⋮----
if start.is_none() {
⋮----
start = fork_tree.get(&start.unwrap().base);
⋮----
fn calc_fork_map(towers: &[Tower], fork_tree: &HashMap<usize, Fork>) -> HashMap<usize, usize> {
⋮----
let mut start = tower.last_fork();
⋮----
*lca_map.entry(start.id).or_insert(0) += 1;
if !fork_tree.contains_key(&start.base) {
⋮----
start = fork_tree.get(&start.base).unwrap().clone();
⋮----
fn calc_newest_trunk(bmap: &HashMap<usize, usize>) -> (usize, usize) {
let mut data: Vec<_> = bmap.iter().collect();
data.sort_by_key(|x| (x.1, x.0));
data.last().map(|v| (*v.0, *v.1)).unwrap()
⋮----
fn calc_tip_converged(towers: &[Tower], bmap: &HashMap<usize, usize>) -> usize {
⋮----
.map(|n| *bmap.get(&n.last_fork().id).unwrap_or(&0))
.sum();
sum / towers.len()
⋮----
fn test_no_partitions() {
⋮----
let mut towers = create_towers(len, 32, 0);
⋮----
for i in 0..towers.len() {
⋮----
let base = towers[i].last_fork().clone();
⋮----
tree.insert(fork.id, fork.clone());
⋮----
let bmap = calc_fork_map(&towers, &tree);
for tower in towers.iter_mut() {
assert!(tower.push_vote(vote.clone(), &tree, &bmap));
⋮----
assert_eq!(calc_tip_converged(&towers, &bmap), len);
⋮----
fn test_with_partitions(
⋮----
let mut towers = create_towers(len, warmup, delay_count);
⋮----
let bmap = calc_fork_map(&towers, &fork_tree);
⋮----
let mut fork = tower.last_fork().clone();
⋮----
fork.id = thread_rng().gen_range(1..1 + num_partitions);
fork_tree.insert(fork.id, fork.clone());
⋮----
assert!(tower.is_valid(&vote, &fork_tree));
assert!(tower.push_vote(vote, &fork_tree, &bmap));
⋮----
assert_eq!(tower.votes.len(), warmup);
assert_eq!(tower.first_vote().unwrap().lockout, 1 << warmup);
assert!(tower.first_vote().unwrap().lock_height() >= 1 << warmup);
tower.parasite = parasite_rate > thread_rng().gen_range(0.0..1.0);
⋮----
let converge_map = calc_fork_map(&towers, &fork_tree);
assert_ne!(calc_tip_converged(&towers, &converge_map), len);
⋮----
let base = towers[i].last_fork();
⋮----
towers.iter().for_each(|n| {
n.delayed_votes.iter().for_each(|v| {
*scores.entry(v.clone()).or_insert(0) += n.score(v, &fork_tree);
⋮----
if thread_rng().gen_range(0f64..1.0f64) < fail_rate {
⋮----
tower.submit_vote(vote.clone(), &fork_tree, &converge_map, &scores);
⋮----
let trunk = calc_newest_trunk(&converge_map);
⋮----
println!(
⋮----
if break_early && calc_tip_converged(&towers, &converge_map) == len {
⋮----
if calc_tip_converged(&towers, &converge_map) == len {
⋮----
assert_eq!(trunk.1, len);
⋮----
fn test_3_partitions() {
test_with_partitions(3, 0.0, 0, 0.0, true)
⋮----
fn test_3_partitions_large_packet_drop() {
test_with_partitions(3, 0.9, 0, 0.0, false)
⋮----
fn test_all_partitions() {
test_with_partitions(100, 0.0, 5, 0.25, false)

================
File: core/tests/scheduler_cost_adjustment.rs
================
struct TestResult {
⋮----
struct TestSetup {
⋮----
impl TestSetup {
fn new() -> Self {
let (mut genesis_config, mint_keypair) = create_genesis_config(LAMPORTS_PER_SOL);
⋮----
fn install_memo_program_account(&mut self) {
⋮----
.unwrap()
.swap_remove(0);
self.genesis_config.add_account(pubkey, account);
⋮----
fn execute_test_transaction(&mut self, ixs: &[Instruction]) -> TestResult {
⋮----
let (bank, bank_forks) = root_bank.wrap_with_bank_forks_for_tests();
⋮----
.get_first_slot_in_epoch(1),
⋮----
Message::new(ixs, Some(&self.mint_keypair.pubkey())),
self.genesis_config.hash(),
⋮----
&RuntimeTransaction::from_transaction_for_tests(tx.clone()),
⋮----
.programs_execution_cost();
let batch = bank.prepare_batch_for_tests(vec![tx]);
⋮----
.load_execute_and_commit_transactions(
⋮----
.remove(0);
⋮----
.saturating_sub(committed_tx.executed_units as i64),
⋮----
unreachable!(
⋮----
fn transfer_ix(&self) -> Instruction {
⋮----
&self.mint_keypair.pubkey(),
⋮----
self.genesis_config.rent.minimum_balance(0),
⋮----
fn set_cu_limit_ix(&self, cu_limit: u32) -> Instruction {
⋮----
fn set_cu_price_ix(&self, cu_price: u64) -> Instruction {
⋮----
fn memo_ix(&self) -> (Instruction, u32) {
let memo = "The quick brown fox jumped over the lazy dog. ".repeat(22) + "!";
⋮----
memo.as_bytes(),
⋮----
fn deploy_with_max_data_len_ix(&mut self) -> Instruction {
⋮----
let payer_address = self.mint_keypair.pubkey();
⋮----
let space = UpgradeableLoaderState::size_of_buffer(memo.data().len());
let lamports = self.genesis_config.rent.minimum_balance(space);
let mut data = vec![0; space];
⋮----
authority_address: Some(upgrade_authority_address),
⋮----
.unwrap();
data[metadata_offset..].copy_from_slice(memo.data());
self.genesis_config.accounts.insert(
⋮----
data: vec![0; space],
⋮----
memo.data().len().saturating_mul(2),
⋮----
.pop()
⋮----
fn test_builtin_ix_cost_adjustment_with_cu_limit_too_low() {
⋮----
execution_status: Err(TransactionError::InstructionError(
⋮----
assert_eq!(
⋮----
fn test_builtin_ix_cost_adjustment_with_cu_limit_high() {
⋮----
execution_status: Ok(()),
⋮----
fn test_builtin_ix_cost_adjustment_with_memo_no_cu_limit() {
⋮----
test_setup.install_memo_program_account();
let (memo_ix, _memo_ix_cost) = test_setup.memo_ix();
⋮----
fn test_builtin_ix_cost_adjustment_with_memo_and_cu_limit() {
⋮----
let (memo_ix, memo_ix_cost) = test_setup.memo_ix();
⋮----
fn test_builtin_ix_cost_adjustment_with_bpf_v3_no_cu_limit() {
⋮----
let ix = test_setup.deploy_with_max_data_len_ix();
assert_eq!(expected, test_setup.execute_test_transaction(&[ix]));
⋮----
fn test_builtin_ix_cost_adjustment_with_bpf_v3_and_cu_limit_high() {
⋮----
test_setup.deploy_with_max_data_len_ix(),
test_setup.set_cu_limit_ix(cu_limit),
⋮----
assert_eq!(expected, test_setup.execute_test_transaction(&ixs));
⋮----
fn test_builtin_ix_set_cu_price_only() {
⋮----
fn test_builtin_ix_precompiled() {

================
File: core/tests/snapshots.rs
================
struct SnapshotTestConfig {
⋮----
impl SnapshotTestConfig {
fn new(
⋮----
let (accounts_tmp_dir, accounts_dir) = create_tmp_accounts_dir_for_tests();
let bank_snapshots_dir = TempDir::new().unwrap();
let full_snapshot_archives_dir = TempDir::new().unwrap();
let incremental_snapshot_archives_dir = TempDir::new().unwrap();
let genesis_config_info = create_genesis_config_with_leader(
⋮----
vec![accounts_dir.clone()],
⋮----
bank0.freeze();
⋮----
full_snapshot_archives_dir: full_snapshot_archives_dir.path().to_path_buf(),
⋮----
.path()
.to_path_buf(),
bank_snapshots_dir: bank_snapshots_dir.path().to_path_buf(),
⋮----
bank_forks: bank_forks_arc.clone(),
⋮----
fn restore_from_snapshot(
⋮----
let old_bank_forks = old_bank_forks.read().unwrap();
let old_last_bank = old_bank_forks.get(old_last_slot).unwrap();
⋮----
old_last_bank.slot(),
&old_last_bank.get_snapshot_hash(),
⋮----
FullSnapshotArchiveInfo::new_from_path(full_snapshot_archive_path).unwrap();
⋮----
.unwrap();
let bank = old_bank_forks.get(deserialized_bank.slot()).unwrap();
assert_eq!(bank.as_ref(), &deserialized_bank);
⋮----
fn run_bank_forks_snapshot_n<F>(last_slot: Slot, f: F, set_root_interval: u64)
⋮----
SnapshotInterval::Slots(NonZeroU64::new(set_root_interval).unwrap()),
⋮----
let bank_forks = snapshot_test_config.bank_forks.clone();
⋮----
let (snapshot_request_sender, snapshot_request_receiver) = unbounded();
⋮----
snapshot_request_sender.clone(),
snapshot_test_config.snapshot_config.clone(),
bank_forks.read().unwrap().root(),
⋮----
snapshot_controller: snapshot_controller.clone(),
⋮----
bank_forks.read().unwrap().get(slot - 1).unwrap().clone(),
⋮----
let bank = bank_forks.write().unwrap().insert(bank);
f(bank.clone_without_scheduler().as_ref(), mint_keypair);
⋮----
if !bank.is_complete() {
bank.fill_bank_with_ticks_for_tests();
⋮----
bank_forks.read().unwrap().prune_program_cache(bank.slot());
⋮----
.write()
.unwrap()
.set_root(bank.slot(), Some(&snapshot_controller), None);
snapshot_request_handler.handle_snapshot_requests(0);
⋮----
let snapshot_config = snapshot_controller.snapshot_config();
let last_bank = bank_forks.read().unwrap().get(last_slot).unwrap();
⋮----
Some(snapshot_config.snapshot_version),
⋮----
let (_tmp_dir, temporary_accounts_dir) = create_tmp_accounts_dir_for_tests();
⋮----
restore_from_snapshot(
snapshot_test_config.bank_forks.clone(),
⋮----
fn test_bank_forks_snapshot() {
run_bank_forks_snapshot_n(
⋮----
let key1 = Keypair::new().pubkey();
let tx = system_transaction::transfer(mint_keypair, &key1, 1, bank.last_blockhash());
assert_eq!(bank.process_transaction(&tx), Ok(()));
let key2 = Keypair::new().pubkey();
let tx = system_transaction::transfer(mint_keypair, &key2, 0, bank.last_blockhash());
⋮----
fn goto_end_of_slot(bank: &Bank) {
let mut tick_hash = bank.last_blockhash();
⋮----
tick_hash = hashv(&[tick_hash.as_ref(), &[42]]);
bank.register_tick_for_test(&tick_hash);
if tick_hash == bank.last_blockhash() {
bank.freeze();
⋮----
fn test_slots_to_snapshot() {
⋮----
let (snapshot_sender, _snapshot_receiver) = unbounded();
⋮----
NonZeroU64::new((*add_root_interval * num_set_roots * 2) as Slot).unwrap(),
⋮----
let bank_forks_r = bank_forks.read().unwrap();
let mut current_bank = bank_forks_r[0].clone();
drop(bank_forks_r);
⋮----
let new_slot = current_bank.slot() + 1;
⋮----
current_bank = bank_forks.write().unwrap().insert(new_bank).clone();
⋮----
.read()
⋮----
.prune_program_cache(current_bank.slot());
bank_forks.write().unwrap().set_root(
current_bank.slot(),
Some(&snapshot_controller),
⋮----
.root_bank()
⋮----
.roots()
.iter()
.cloned()
.sorted();
assert!(slots_to_snapshot.into_iter().eq(expected_slots_to_snapshot));
⋮----
fn test_bank_forks_status_cache_snapshot() {
⋮----
bank.parent().unwrap().last_blockhash(),
⋮----
goto_end_of_slot(bank);
⋮----
fn test_bank_forks_incremental_snapshot() {
⋮----
info!(
⋮----
SnapshotInterval::Slots(NonZeroU64::new(FULL_SNAPSHOT_ARCHIVE_INTERVAL_SLOTS).unwrap()),
⋮----
NonZeroU64::new(INCREMENTAL_SNAPSHOT_ARCHIVE_INTERVAL_SLOTS).unwrap(),
⋮----
trace!(
⋮----
let parent = bank_forks.read().unwrap().get(slot - 1).unwrap();
⋮----
let bank_scheduler = bank_forks.write().unwrap().insert(bank);
let bank = bank_scheduler.clone_without_scheduler();
⋮----
let tx = system_transaction::transfer(mint_keypair, &key, 1, bank.last_blockhash());
⋮----
let tx = system_transaction::transfer(mint_keypair, &key, 0, bank.last_blockhash());
⋮----
make_full_snapshot_archive(&bank, snapshot_controller.snapshot_config()).unwrap();
latest_full_snapshot_slot = Some(slot);
⋮----
) && slot != latest_full_snapshot_slot.unwrap()
⋮----
make_incremental_snapshot_archive(
⋮----
latest_full_snapshot_slot.unwrap(),
snapshot_controller.snapshot_config(),
⋮----
restore_from_snapshots_and_check_banks_are_equal(
⋮----
fn make_full_snapshot_archive(
⋮----
Ok(())
⋮----
fn make_incremental_snapshot_archive(
⋮----
fn restore_from_snapshots_and_check_banks_are_equal(
⋮----
assert_eq!(bank, &deserialized_bank);
⋮----
fn test_snapshots_with_background_services() {
⋮----
info!("Running snapshots with background services test...");
⋮----
ContactInfo::new_localhost(&node_keypair.pubkey(), timestamp()),
⋮----
let (pruned_banks_sender, pruned_banks_receiver) = unbounded();
⋮----
.enable_bank_drop_callback();
⋮----
for bank in bank_forks.read().unwrap().banks().values() {
bank.set_callback(Some(Box::new(callback.clone())));
⋮----
pending_snapshot_packages: pending_snapshot_packages.clone(),
⋮----
pending_snapshot_packages.clone(),
⋮----
exit.clone(),
⋮----
cluster_info.clone(),
snapshot_controller.clone(),
⋮----
AccountsBackgroundService::new(bank_forks.clone(), exit.clone(), abs_request_handler);
⋮----
bank_forks.read().unwrap().get(slot - 1).unwrap(),
⋮----
.insert(bank)
.clone_without_scheduler();
⋮----
.set_root(slot, Some(&snapshot_controller), None);
⋮----
) != Some(slot)
⋮----
assert!(
⋮----
&& latest_full_snapshot_slot.is_some()
⋮----
latest_incremental_snapshot_slot = Some(slot);
⋮----
assert_eq!(
⋮----
info!("Shutting down background services...");
exit.store(true, Ordering::Relaxed);
_ = accounts_background_service.join();
_ = snapshot_packager_service.join();

================
File: core/tests/unified_scheduler.rs
================
fn test_scheduler_waited_by_drop_bank_service() {
⋮----
struct StallingHandler;
impl TaskHandler for StallingHandler {
fn handle(
⋮----
info!("Stalling at StallingHandler::handle()...");
*LOCK_TO_STALL.lock().unwrap();
⋮----
info!("Now entering into DefaultTaskHandler::handle()...");
⋮----
} = create_genesis_config(10_000);
⋮----
let pool = pool_raw.clone();
bank_forks.write().unwrap().install_scheduler_pool(pool);
⋮----
let genesis_bank = &bank_forks.read().unwrap().get(genesis).unwrap();
genesis_bank.set_fork_graph_in_program_cache(Arc::downgrade(&bank_forks));
⋮----
let pruned_bank = Bank::new_from_parent(genesis_bank.clone(), &Pubkey::default(), pruned);
let pruned_bank = bank_forks.write().unwrap().insert(pruned_bank);
⋮----
let root_bank = Bank::new_from_parent(genesis_bank.clone(), &Pubkey::default(), root);
root_bank.freeze();
let root_hash = root_bank.hash();
bank_forks.write().unwrap().insert(root_bank);
⋮----
genesis_config.hash(),
⋮----
let lock_to_stall = LOCK_TO_STALL.lock().unwrap();
⋮----
.schedule_transaction_executions([(tx, 0)].into_iter())
.unwrap();
drop(pruned_bank);
assert_eq!(pool_raw.pooled_scheduler_count(), 0);
drop(lock_to_stall);
let (drop_bank_sender1, drop_bank_receiver1) = unbounded();
let (drop_bank_sender2, drop_bank_receiver2) = unbounded();
⋮----
info!("calling handle_new_root()...");
⋮----
progress.insert(i, ForkProgress::new(Hash::default(), None, None, 0, 0));
⋮----
vec![root - 1, root, root + 1].into_iter().collect();
let duplicate_confirmed_slots: DuplicateConfirmedSlots = vec![root - 1, root, root + 1]
.into_iter()
.map(|s| (s, Hash::default()))
.collect();
⋮----
votes_per_slot: vec![root - 1, root, root + 1]
⋮----
.map(|s| (s, HashMap::new()))
.collect(),
⋮----
let epoch_slots_frozen_slots: EpochSlotsFrozenSlots = vec![root - 1, root, root + 1]
⋮----
.map(|slot| (slot, Hash::default()))
⋮----
let pruned_banks = drop_bank_receiver1.recv().unwrap();
assert_eq!(
⋮----
info!("sending pruned banks to DropBankService...");
drop_bank_sender2.send(pruned_banks).unwrap();
info!("joining the drop bank service...");
drop((
⋮----
drop_bank_service.join().unwrap();
info!("finally joined the drop bank service!");
assert_eq!(pool_raw.pooled_scheduler_count(), 1);
⋮----
fn test_scheduler_producing_blocks() {
⋮----
let (ledger_path, _blockhash) = create_new_tmp_ledger_auto_delete!(&genesis_config);
let blockstore = Arc::new(Blockstore::open(ledger_path.path()).unwrap());
⋮----
let genesis_bank = bank_forks.read().unwrap().working_bank_with_scheduler();
⋮----
) = create_test_recorder(
genesis_bank.clone(),
blockstore.clone(),
⋮----
Some(leader_schedule_cache),
⋮----
banking_tracer.create_channels(true)
⋮----
ensure_banking_stage_setup(
⋮----
while poh_recorder.read().unwrap().bank().is_some() {
sleep(Duration::from_millis(100));
⋮----
let banking_packet_batch = BankingPacketBatch::new(to_packet_batches(&vec![tx.clone(); 1], 1));
⋮----
let tpu_bank = Bank::new_from_parent(genesis_bank.clone(), &Pubkey::default(), 2);
⋮----
.write()
.unwrap()
.insert_with_scheduling_mode(SchedulingMode::BlockProduction, tpu_bank);
⋮----
.set_bank_sync(tpu_bank.clone_with_scheduler())
⋮----
tpu_bank.unpause_new_block_production_scheduler();
let tpu_bank = bank_forks.read().unwrap().working_bank_with_scheduler();
assert_eq!(tpu_bank.transaction_count(), 0);
⋮----
.unified_sender()
.send(banking_packet_batch)
⋮----
assert_matches!(tpu_bank.wait_for_completed_scheduler(), Some((Ok(()), _)));
assert_eq!(tpu_bank.transaction_count(), 1);
assert_matches!(
⋮----
exit.store(true, Ordering::Relaxed);
poh_service.join().unwrap();

================
File: core/.gitignore
================
/target/
/farf/

================
File: core/Cargo.toml
================
[package]
name = "solana-core"
documentation = "https://docs.rs/solana-core"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "solana-perf/dev-context-only-utils",
    "solana-runtime/dev-context-only-utils",
    "solana-streamer/dev-context-only-utils",
]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-accounts-db/frozen-abi",
    "solana-bloom/frozen-abi",
    "solana-compute-budget/frozen-abi",
    "solana-cost-model/frozen-abi",
    "solana-frozen-abi/frozen-abi",
    "solana-gossip/frozen-abi",
    "solana-ledger/frozen-abi",
    "solana-packet/frozen-abi",
    "solana-perf/frozen-abi",
    "solana-program-runtime/frozen-abi",
    "solana-runtime/frozen-abi",
    "solana-short-vec/frozen-abi",
    "solana-signature/frozen-abi",
    "solana-svm/frozen-abi",
    "solana-vote/frozen-abi",
    "solana-vote-program/frozen-abi",
]

[dependencies]
agave-banking-stage-ingress-types = { workspace = true }
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
agave-scheduler-bindings = { workspace = true }
agave-scheduling-utils = { workspace = true }
agave-snapshots = { workspace = true }
agave-transaction-view = { workspace = true }
agave-verified-packet-receiver = { workspace = true }
agave-votor = { workspace = true, features = ["agave-unstable-api"] }
ahash = { workspace = true }
anyhow = { workspace = true }
arc-swap = { workspace = true }
arrayvec = { workspace = true }
assert_matches = { workspace = true }
async-trait = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
borsh = { workspace = true }
bs58 = { workspace = true }
bytemuck = { workspace = true }
bytes = { workspace = true }
chrono = { workspace = true, features = ["default", "serde"] }
crossbeam-channel = { workspace = true }
dashmap = { workspace = true, features = ["rayon", "raw-api"] }
derive_more = { workspace = true }
futures = { workspace = true }
histogram = { workspace = true }
itertools = { workspace = true }
jito-protos = { workspace = true }
lazy_static = { workspace = true }
libc = { workspace = true }
log = { workspace = true }
lru = { workspace = true }
min-max-heap = { workspace = true }
num_cpus = { workspace = true }
num_enum = { workspace = true }
prio-graph = { workspace = true }
prost = { workspace = true }
prost-types = { workspace = true }
qualifier_attr = { workspace = true }
quinn = { workspace = true }
rand = "0.8.5"
rand_chacha = "0.3.1"
rayon = { workspace = true }
rolling-file = { workspace = true }
rustls = { workspace = true }
serde = { workspace = true }
serde_bytes = { workspace = true }
signal-hook = { workspace = true }
slab = { workspace = true }
smallvec = { workspace = true }
solana-account = { workspace = true }
solana-accounts-db = { workspace = true }
solana-address-lookup-table-interface = { workspace = true }
solana-bincode = { workspace = true }
solana-bloom = { workspace = true }
solana-builtins-default-costs = { workspace = true }
solana-bundle = { workspace = true }
# solana-bundle-sdk = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-instruction = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-connection-cache = { workspace = true }
solana-cost-model = { workspace = true }
solana-entry = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-fee = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-fee-structure = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-genesis-config = { workspace = true }
solana-genesis-utils = { workspace = true }
solana-geyser-plugin-manager = { workspace = true }
solana-gossip = { workspace = true, features = ["agave-unstable-api"] }
solana-hard-forks = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true, features = ["agave-unstable-api"] }
solana-loader-v3-interface = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-native-token = { workspace = true }
solana-net-utils = { workspace = true, features = ["agave-unstable-api"] }
solana-nonce = { workspace = true }
solana-nonce-account = { workspace = true }
solana-packet = { workspace = true }
solana-perf = { workspace = true }
solana-poh = { workspace = true }
solana-poh-config = { workspace = true }
solana-pubkey = { workspace = true }
solana-quic-client = { workspace = true }
solana-quic-definitions = { workspace = true }
solana-rayon-threadlimit = { workspace = true }
solana-rent = { workspace = true }
solana-rpc = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-runtime = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-sanitize = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-send-transaction-service = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-short-vec = { workspace = true }
solana-shred-version = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-slot-history = { workspace = true }
solana-streamer = { workspace = true }
solana-svm = { workspace = true }
solana-svm-timings = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }
solana-sysvar = { workspace = true }
solana-time-utils = { workspace = true }
solana-tls-utils = { workspace = true }
solana-tpu-client = { workspace = true }
solana-tpu-client-next = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status = { workspace = true }
solana-turbine = { workspace = true, features = ["agave-unstable-api"] }
solana-unified-scheduler-logic = { workspace = true }
solana-unified-scheduler-pool = { workspace = true }
solana-validator-exit = { workspace = true }
solana-version = { workspace = true }
solana-vote = { workspace = true }
solana-vote-program = { workspace = true }
solana-wen-restart = { workspace = true }
spl-memo-interface = { workspace = true }
static_assertions = { workspace = true }
strum = { workspace = true, features = ["derive"] }
strum_macros = { workspace = true }
sys-info = { workspace = true }
tempfile = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tokio-stream = { workspace = true }
tokio-util = { workspace = true }
tonic = { workspace = true }
trees = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dependencies]
jemallocator = { workspace = true }

[target."cfg(unix)".dependencies]
rts-alloc = { workspace = true }
shaq = { workspace = true }
sysctl = { workspace = true }

[build-dependencies]
tonic-build = { workspace = true }

[dev-dependencies]
agave-reserved-account-keys = { workspace = true }
agave-scheduler-bindings = { workspace = true, features = ["dev-context-only-utils"] }
bencher = { workspace = true }
criterion = { workspace = true }
fs_extra = { workspace = true }
serde_json = { workspace = true }
serial_test = { workspace = true }
solana-account = { workspace = true, features = ["dev-context-only-utils"] }
solana-bpf-loader-program = { workspace = true }
solana-client = { workspace = true, features = ["dev-context-only-utils"] }
# solana-bundle = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-compute-budget-program = { workspace = true }
# See order-crates-for-publishing.py for using this unusual `path = "."`
solana-core = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-cost-model = { workspace = true, features = ["dev-context-only-utils"] }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true, features = ["dev-context-only-utils"] }
solana-net-utils = { workspace = true, features = ["dev-context-only-utils"] }
solana-poh = { workspace = true, features = ["dev-context-only-utils"] }
solana-program-binaries = { workspace = true }
solana-program-runtime = { workspace = true, features = ["metrics"] }
solana-program-test = { workspace = true }
solana-rpc = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime-transaction = { workspace = true }
solana-system-program = { workspace = true }
solana-unified-scheduler-pool = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-vote = { workspace = true, features = ["dev-context-only-utils"] }
solana-vote-interface = { workspace = true }
spl-memo-interface = { workspace = true }
static_assertions = { workspace = true }
test-case = { workspace = true }

[[bench]]
name = "banking_stage"

[[bench]]
name = "gen_keys"

[[bench]]
name = "sigverify_stage"
harness = false

[[bench]]
name = "shredder"
harness = false

[[bench]]
name = "receive_and_buffer"
harness = false

[[bench]]
name = "scheduler"
harness = false

[lints]
workspace = true

================
File: cost-model/benches/cost_model.rs
================
extern crate test;
⋮----
struct BenchSetup {
⋮----
fn setup(num_transactions: usize) -> BenchSetup {
⋮----
.map(|_| {
⋮----
Vec::from_iter(std::iter::repeat_with(|| (Pubkey::new_unique(), 1)).take(24));
let ixs = system_instruction::transfer_many(&from_keypair.pubkey(), &to_lamports);
let message = Message::new(&ixs, Some(&from_keypair.pubkey()));
⋮----
.collect();
⋮----
fn bench_cost_model(bencher: &mut Bencher) {
⋮----
} = setup(NUM_TRANSACTIONS_PER_ITER);
bencher.iter(|| {

================
File: cost-model/benches/cost_tracker.rs
================
extern crate test;
⋮----
struct BenchSetup {
⋮----
fn setup(num_transactions: usize, contentious_transactions: bool) -> BenchSetup {
⋮----
cost_tracker.set_limits(u64::MAX, u64::MAX, u64::MAX);
⋮----
.map(|_| {
⋮----
(0..max_accounts_per_tx).for_each(|_| {
⋮----
writable_accounts.push(writable_account_key)
⋮----
WritableKeysTransaction(writable_accounts)
⋮----
.collect_vec();
⋮----
fn get_costs(
⋮----
.iter()
.map(|transaction| {
⋮----
.collect_vec()
⋮----
fn bench_cost_tracker_non_contentious_transaction(bencher: &mut Bencher) {
⋮----
} = setup(1024, false);
let tx_costs = get_costs(&transactions);
bencher.iter(|| {
for tx_cost in tx_costs.iter() {
if cost_tracker.try_add(tx_cost).is_err() {
⋮----
cost_tracker.update_execution_cost(tx_cost, 0, 0);
⋮----
fn bench_cost_tracker_contentious_transaction(bencher: &mut Bencher) {
⋮----
} = setup(1024, true);

================
File: cost-model/src/block_cost_limits.rs
================
pub const fn simd_0286_block_limits() -> u64 {

================
File: cost-model/src/cost_model.rs
================
pub struct CostModel;
⋮----
enum SystemProgramAccountAllocation {
⋮----
impl CostModel {
pub fn calculate_cost<'a, Tx: StaticMeta + SVMStaticMessage>(
⋮----
if transaction.is_simple_vote_transaction() {
⋮----
transaction.program_instructions_iter(),
transaction.num_write_locks(),
⋮----
// Calculate executed transaction CU cost, with actual execution and loaded accounts size
// costs.
pub fn calculate_cost_for_executed_transaction<'a, Tx: StaticMeta + SVMStaticMessage>(
⋮----
pub fn estimate_cost<'a, Tx: StaticMeta>(
⋮----
fn calculate_non_vote_transaction_cost<'a, Tx: StaticMeta>(
⋮----
fn get_signature_cost(transaction: &impl StaticMeta, feature_set: &FeatureSet) -> u64 {
let signatures_count_detail = transaction.signature_details();
⋮----
if feature_set.is_active(&feature_set::ed25519_precompile_verify_strict::id()) {
⋮----
if feature_set.is_active(&feature_set::enable_secp256r1_precompile::id()) {
⋮----
.num_transaction_signatures()
.saturating_mul(SIGNATURE_COST)
.saturating_add(
⋮----
.num_secp256k1_instruction_signatures()
.saturating_mul(SECP256K1_VERIFY_COST),
⋮----
.num_ed25519_instruction_signatures()
.saturating_mul(ed25519_verify_cost),
⋮----
.num_secp256r1_instruction_signatures()
.saturating_mul(secp256r1_verify_cost),
⋮----
fn get_write_lock_cost(num_write_locks: u64) -> u64 {
WRITE_LOCK_UNITS.saturating_mul(num_write_locks)
⋮----
fn get_estimated_execution_cost(
⋮----
.compute_budget_instruction_details()
.sanitize_and_convert_to_compute_budget_limits(feature_set)
⋮----
compute_budget_limits.loaded_accounts_bytes.get(),
⋮----
fn get_instructions_data_cost(transaction: &impl StaticMeta) -> u16 {
transaction.instruction_data_len() / (INSTRUCTION_DATA_BYTES_COST as u16)
⋮----
pub fn calculate_loaded_accounts_data_size_cost(
⋮----
fn calculate_account_data_size_on_deserialized_system_instruction(
⋮----
fn calculate_account_data_size_on_instruction(
⋮----
limited_deserialize(instruction.data, solana_packet::PACKET_DATA_SIZE as u64)
⋮----
fn calculate_allocated_accounts_data_size<'a>(
⋮----
let mut tx_attempted_allocation_size = Saturating(0u64);
⋮----
// If any system program instructions can be statically
// determined to fail, no allocations will actually be
// persisted by the transaction. So return 0 here so that no
// account allocation budget is used for this failed
// transaction.
⋮----
// The runtime prevents transactions from allocating too much account
// data so clamp the attempted allocation size to the max amount.
//
// Note that if there are any custom bpf instructions in the transaction
// it's tricky to know whether a newly allocated account will be freed
⋮----
.min(tx_attempted_allocation_size.0)
⋮----
mod tests {
⋮----
fn test_setup() -> (Keypair, Hash) {
⋮----
fn test_calculate_allocated_accounts_data_size_no_allocation() {
⋮----
Some(&Pubkey::new_unique()),
⋮----
assert_eq!(
⋮----
fn test_calculate_allocated_accounts_data_size_multiple_allocations() {
⋮----
fn test_calculate_allocated_accounts_data_size_max_limit() {
⋮----
assert!(
⋮----
fn test_calculate_allocated_accounts_data_size_overflow() {
⋮----
fn test_calculate_allocated_accounts_data_size_invalid_ix() {
⋮----
Instruction::new_with_bincode(system_program::id(), &(), vec![]),
⋮----
fn test_cost_model_data_len_cost() {
⋮----
seed: seed.clone(),
⋮----
fn test_cost_model_simple_transaction() {
let (mint_keypair, start_hash) = test_setup();
⋮----
system_transaction::transfer(&mint_keypair, &keypair.pubkey(), 2, start_hash),
⋮----
assert_eq!(expected_execution_cost, programs_execution_cost);
⋮----
fn test_cost_model_token_transaction() {
⋮----
let instructions = vec![CompiledInstruction::new(3, &(), vec![1, 2, 0])];
⋮----
vec![Pubkey::new_unique()],
⋮----
assert_eq!(0, data_bytes_cost);
⋮----
fn test_cost_model_demoted_write_lock() {
⋮----
assert_eq!(2 * WRITE_LOCK_UNITS, tx_cost.write_lock_cost());
assert_eq!(1, tx_cost.writable_accounts().count());
⋮----
fn test_cost_model_compute_budget_transaction() {
⋮----
let instructions = vec![
⋮----
vec![Pubkey::new_unique(), compute_budget::id()],
⋮----
assert_eq!(1, data_bytes_cost);
⋮----
fn test_cost_model_with_failed_compute_budget_transaction() {
⋮----
assert_eq!(0, programs_execution_cost);
⋮----
fn test_cost_model_transaction_many_transfer_instructions() {
⋮----
system_instruction::transfer_many(&mint_keypair.pubkey(), &[(key1, 1), (key2, 1)]);
let message = Message::new(&instructions, Some(&mint_keypair.pubkey()));
⋮----
assert_eq!(6, data_bytes_cost);
⋮----
fn test_cost_model_message_many_different_instructions() {
⋮----
vec![prog1, prog2],
⋮----
assert_eq!(expected_cost, programs_execution_cost);
⋮----
fn test_cost_model_sort_message_accounts_by_type() {
⋮----
let writable_accounts = tx_cost.writable_accounts().collect_vec();
assert_eq!(2 + 2, writable_accounts.len());
assert_eq!(signer1.pubkey(), *writable_accounts[0]);
assert_eq!(signer2.pubkey(), *writable_accounts[1]);
assert_eq!(key1, *writable_accounts[2]);
assert_eq!(key2, *writable_accounts[3]);
⋮----
fn test_cost_model_calculate_cost_all_default() {
⋮----
&Keypair::new().pubkey(),
⋮----
solana_compute_budget::compute_budget_limits::MAX_LOADED_ACCOUNTS_DATA_SIZE_BYTES.get()
⋮----
assert_eq!(expected_account_cost, tx_cost.write_lock_cost());
assert_eq!(expected_execution_cost, tx_cost.programs_execution_cost());
assert_eq!(2, tx_cost.writable_accounts().count());
⋮----
fn test_cost_model_calculate_cost_with_limit() {
⋮----
system_instruction::transfer(&mint_keypair.pubkey(), &to_keypair.pubkey(), 2),
⋮----
Some(&mint_keypair.pubkey()),
⋮----
fn test_transaction_cost_with_mix_instruction_without_compute_budget() {
⋮----
Instruction::new_with_bincode(Pubkey::new_unique(), &0_u8, vec![]),
system_instruction::transfer(&mint_keypair.pubkey(), &Pubkey::new_unique(), 2),
⋮----
fn test_transaction_cost_with_mix_instruction_with_cu_limit() {

================
File: cost-model/src/cost_tracker_post_analysis.rs
================
pub trait CostTrackerPostAnalysis {

================
File: cost-model/src/cost_tracker.rs
================
pub enum CostTrackerError {
⋮----
fn from(err: CostTrackerError) -> Self {
⋮----
pub struct UpdatedCosts {
⋮----
pub struct CostTracker {
⋮----
impl Default for CostTracker {
fn default() -> Self {
⋮----
const _: () = assert!(MAX_WRITABLE_ACCOUNT_UNITS <= MAX_BLOCK_UNITS);
const _: () = assert!(MAX_VOTE_UNITS <= MAX_BLOCK_UNITS);
⋮----
transaction_count: Saturating(0),
allocated_accounts_data_size: Saturating(0),
transaction_signature_count: Saturating(0),
secp256k1_instruction_signature_count: Saturating(0),
ed25519_instruction_signature_count: Saturating(0),
in_flight_transaction_count: Saturating(0),
secp256r1_instruction_signature_count: Saturating(0),
⋮----
impl CostTracker {
pub fn new_from_parent_limits(&self) -> Self {
⋮----
new.set_limits(
⋮----
pub fn get_account_limit(&self) -> u64 {
⋮----
pub fn get_block_limit(&self) -> u64 {
⋮----
pub fn get_vote_limit(&self) -> u64 {
⋮----
pub fn set_limits(
⋮----
pub fn in_flight_transaction_count(&self) -> usize {
⋮----
pub fn add_transactions_in_flight(&mut self, in_flight_transaction_count: usize) {
⋮----
pub fn sub_transactions_in_flight(&mut self, in_flight_transaction_count: usize) {
⋮----
pub fn try_add(
⋮----
self.would_fit(tx_cost)?;
let updated_costliest_account_cost = self.add_transaction_cost(tx_cost);
Ok(UpdatedCosts {
updated_block_cost: self.block_cost(),
⋮----
pub fn update_execution_cost(
⋮----
actual_execution_units.saturating_add(actual_loaded_accounts_data_size_cost);
⋮----
.programs_execution_cost()
.saturating_add(estimated_tx_cost.loaded_accounts_data_size_cost());
match actual_load_and_execution_units.cmp(&estimated_load_and_execution_units) {
⋮----
self.add_transaction_execution_cost(
⋮----
self.sub_transaction_execution_cost(
⋮----
pub fn remove(&mut self, tx_cost: &TransactionCost<impl TransactionWithMeta>) {
self.remove_transaction_cost(tx_cost);
⋮----
pub fn block_cost(&self) -> u64 {
self.block_cost.load()
⋮----
pub fn shared_block_cost(&self) -> SharedBlockCost {
self.block_cost.clone()
⋮----
pub fn vote_cost(&self) -> u64 {
⋮----
pub fn block_cost_limit(&self) -> u64 {
⋮----
pub fn transaction_count(&self) -> u64 {
⋮----
pub fn report_stats(
⋮----
let (costliest_account, costliest_account_cost) = self.find_costliest_account();
let number_of_contended_accounts = self.find_number_of_contended_accounts();
datapoint_info!(
⋮----
fn find_costliest_account(&self) -> (Pubkey, u64) {
⋮----
.iter()
.max_by_key(|(_, cost)| **cost)
.map(|(&pubkey, &cost)| (pubkey, cost))
.unwrap_or_default()
⋮----
fn find_number_of_contended_accounts(&self) -> usize {
⋮----
.saturating_mul(95)
.saturating_div(100);
⋮----
.values()
.filter(|&&cost| cost >= contended_cost_mark)
.count()
⋮----
fn would_fit(
⋮----
let cost: u64 = tx_cost.sum();
if tx_cost.is_simple_vote() {
if self.vote_cost.saturating_add(cost) > self.vote_cost_limit {
return Err(CostTrackerError::WouldExceedVoteMaxLimit);
⋮----
if self.block_cost().saturating_add(cost) > self.block_cost_limit {
return Err(CostTrackerError::WouldExceedBlockMaxLimit);
⋮----
return Err(CostTrackerError::WouldExceedAccountMaxLimit);
⋮----
self.allocated_accounts_data_size + Saturating(tx_cost.allocated_accounts_data_size());
⋮----
return Err(CostTrackerError::WouldExceedAccountDataBlockLimit);
⋮----
for account_key in tx_cost.writable_accounts() {
match self.cost_by_writable_accounts.get(account_key) {
⋮----
if chained_cost.saturating_add(cost) > self.account_cost_limit {
⋮----
Ok(())
⋮----
fn add_transaction_cost(&mut self, tx_cost: &TransactionCost<impl TransactionWithMeta>) -> u64 {
self.allocated_accounts_data_size += tx_cost.allocated_accounts_data_size();
⋮----
self.transaction_signature_count += tx_cost.num_transaction_signatures();
⋮----
tx_cost.num_secp256k1_instruction_signatures();
self.ed25519_instruction_signature_count += tx_cost.num_ed25519_instruction_signatures();
⋮----
tx_cost.num_secp256r1_instruction_signatures();
self.add_transaction_execution_cost(tx_cost, tx_cost.sum())
⋮----
fn remove_transaction_cost(&mut self, tx_cost: &TransactionCost<impl TransactionWithMeta>) {
let cost = tx_cost.sum();
self.sub_transaction_execution_cost(tx_cost, cost);
self.allocated_accounts_data_size -= tx_cost.allocated_accounts_data_size();
⋮----
self.transaction_signature_count -= tx_cost.num_transaction_signatures();
⋮----
self.ed25519_instruction_signature_count -= tx_cost.num_ed25519_instruction_signatures();
⋮----
fn add_transaction_execution_cost(
⋮----
.entry(*account_key)
.or_insert(0);
*account_cost = account_cost.saturating_add(adjustment);
costliest_account_cost = costliest_account_cost.max(*account_cost);
⋮----
self.block_cost.fetch_add(adjustment);
⋮----
self.vote_cost = self.vote_cost.saturating_add(adjustment);
⋮----
fn sub_transaction_execution_cost(
⋮----
*account_cost = account_cost.saturating_sub(adjustment);
⋮----
self.block_cost.fetch_sub(adjustment);
⋮----
self.vote_cost = self.vote_cost.saturating_sub(adjustment);
⋮----
fn number_of_accounts(&self) -> usize {
⋮----
.filter(|units| **units > 0)
⋮----
impl CostTrackerPostAnalysis for CostTracker {
fn get_cost_by_writable_accounts(&self) -> &HashMap<Pubkey, u64, ahash::RandomState> {
⋮----
pub struct SharedBlockCost(Arc<AtomicU64>);
impl SharedBlockCost {
pub fn new(value: u64) -> Self {
Self(Arc::new(AtomicU64::new(value)))
⋮----
fn fetch_add(&self, value: u64) -> u64 {
self.0.fetch_add(value, Ordering::Release)
⋮----
fn fetch_sub(&self, value: u64) -> u64 {
self.0.fetch_sub(value, Ordering::Release)
⋮----
pub fn load(&self) -> u64 {
self.0.load(Ordering::Acquire)
⋮----
mod tests {
⋮----
fn new(account_cost_limit: u64, block_cost_limit: u64, vote_cost_limit: u64) -> Self {
assert!(account_cost_limit <= block_cost_limit);
assert!(vote_cost_limit <= block_cost_limit);
⋮----
fn test_setup() -> Keypair {
⋮----
fn build_simple_transaction(mint_keypair: &Keypair) -> WritableKeysTransaction {
WritableKeysTransaction(vec![mint_keypair.pubkey()])
⋮----
fn simple_usage_cost_details(
⋮----
fn simple_transaction_cost(
⋮----
TransactionCost::Transaction(simple_usage_cost_details(
⋮----
fn simple_vote_transaction_cost(
⋮----
fn test_cost_tracker_initialization() {
⋮----
assert_eq!(10, testee.account_cost_limit);
assert_eq!(11, testee.block_cost_limit);
assert_eq!(8, testee.vote_cost_limit);
assert_eq!(0, testee.cost_by_writable_accounts.len());
assert_eq!(0, testee.block_cost());
⋮----
fn test_cost_tracker_ok_add_one() {
let mint_keypair = test_setup();
let tx = build_simple_transaction(&mint_keypair);
let tx_cost = simple_transaction_cost(&tx, 5);
⋮----
assert!(testee.would_fit(&tx_cost).is_ok());
testee.add_transaction_cost(&tx_cost);
assert_eq!(cost, testee.block_cost());
assert_eq!(0, testee.vote_cost);
let (_costliest_account, costliest_account_cost) = testee.find_costliest_account();
assert_eq!(cost, costliest_account_cost);
⋮----
fn test_cost_tracker_ok_add_one_vote() {
⋮----
let tx_cost = simple_vote_transaction_cost(&tx);
⋮----
assert_eq!(cost, testee.vote_cost);
⋮----
fn test_cost_tracker_add_data() {
⋮----
let mut tx_cost = simple_transaction_cost(&tx, 5);
⋮----
unreachable!();
⋮----
assert_eq!(old.0 + 1, testee.allocated_accounts_data_size.0);
⋮----
fn test_cost_tracker_ok_add_two_same_accounts() {
⋮----
let tx1 = build_simple_transaction(&mint_keypair);
let tx_cost1 = simple_transaction_cost(&tx1, 5);
let cost1 = tx_cost1.sum();
let tx2 = build_simple_transaction(&mint_keypair);
let tx_cost2 = simple_transaction_cost(&tx2, 5);
let cost2 = tx_cost2.sum();
⋮----
assert!(testee.would_fit(&tx_cost1).is_ok());
testee.add_transaction_cost(&tx_cost1);
⋮----
assert!(testee.would_fit(&tx_cost2).is_ok());
testee.add_transaction_cost(&tx_cost2);
⋮----
assert_eq!(cost1 + cost2, testee.block_cost());
assert_eq!(1, testee.cost_by_writable_accounts.len());
let (_ccostliest_account, costliest_account_cost) = testee.find_costliest_account();
assert_eq!(cost1 + cost2, costliest_account_cost);
⋮----
fn test_cost_tracker_ok_add_two_diff_accounts() {
⋮----
let tx2 = build_simple_transaction(&second_account);
⋮----
assert_eq!(2, testee.cost_by_writable_accounts.len());
⋮----
assert_eq!(std::cmp::max(cost1, cost2), costliest_account_cost);
⋮----
fn test_cost_tracker_chain_reach_limit() {
⋮----
assert!(testee.would_fit(&tx_cost2).is_err());
⋮----
fn test_cost_tracker_reach_limit() {
⋮----
fn test_cost_tracker_reach_vote_limit() {
⋮----
let tx_cost1 = simple_vote_transaction_cost(&tx1);
⋮----
let tx_cost2 = simple_vote_transaction_cost(&tx2);
⋮----
let tx3 = build_simple_transaction(&third_account);
let tx_cost3 = simple_transaction_cost(&tx3, 5);
assert!(testee.would_fit(&tx_cost3).is_ok());
⋮----
fn test_cost_tracker_reach_data_block_limit() {
⋮----
let mut tx_cost1 = simple_transaction_cost(&tx1, 5);
⋮----
let mut tx_cost2 = simple_transaction_cost(&tx2, 5);
⋮----
assert_eq!(
⋮----
fn test_cost_tracker_remove() {
⋮----
assert!(testee.try_add(&tx_cost1).is_ok());
assert!(testee.try_add(&tx_cost2).is_ok());
⋮----
testee.remove(&tx_cost1);
assert_eq!(cost2, testee.block_cost());
⋮----
assert!(testee.try_add(&tx_cost1).is_err());
⋮----
fn test_cost_tracker_try_add_is_atomic() {
⋮----
let transaction = WritableKeysTransaction(vec![acct1, acct2, acct3]);
let tx_cost = simple_transaction_cost(&transaction, cost);
assert!(testee.try_add(&tx_cost).is_ok());
⋮----
assert_eq!(3, testee.cost_by_writable_accounts.len());
⋮----
let transaction = WritableKeysTransaction(vec![acct2]);
⋮----
let (costliest_account, costliest_account_cost) = testee.find_costliest_account();
assert_eq!(cost * 2, testee.block_cost());
⋮----
assert_eq!(cost * 2, costliest_account_cost);
assert_eq!(acct2, costliest_account);
⋮----
let transaction = WritableKeysTransaction(vec![acct1, acct2]);
⋮----
assert!(testee.try_add(&tx_cost).is_err());
⋮----
fn test_adjust_transaction_execution_cost() {
⋮----
let mut expected_block_cost = tx_cost.sum();
⋮----
assert_eq!(expected_block_cost, testee.block_cost());
assert_eq!(expected_tx_count, testee.transaction_count());
⋮----
.for_each(|(_key, units)| {
assert_eq!(expected_block_cost, *units);
⋮----
testee.add_transaction_execution_cost(&tx_cost, adjustment);
⋮----
testee.sub_transaction_execution_cost(&tx_cost, adjustment);
⋮----
fn test_update_execution_cost() {
⋮----
let transaction = WritableKeysTransaction(
⋮----
.take(number_writeble_accounts)
.collect(),
⋮----
simple_usage_cost_details(&transaction, estimated_programs_execution_cost);
⋮----
let estimated_tx_cost = tx_cost.sum();
⋮----
assert!(cost_tracker.try_add(&tx_cost).is_ok());
⋮----
cost_tracker.update_execution_cost(
⋮----
assert_eq!(expected_cost, cost_tracker.block_cost());
assert_eq!(0, cost_tracker.vote_cost);
⋮----
for writable_account_cost in cost_tracker.cost_by_writable_accounts.values() {
assert_eq!(expected_cost, *writable_account_cost);
⋮----
assert_eq!(1, cost_tracker.transaction_count.0);
⋮----
test_update_cost_tracker(0, 0);
test_update_cost_tracker(0, 9);
test_update_cost_tracker(0, -9);
test_update_cost_tracker(9, 0);
test_update_cost_tracker(9, 9);
test_update_cost_tracker(9, -9);
test_update_cost_tracker(-9, 0);
test_update_cost_tracker(-9, 9);
test_update_cost_tracker(-9, -9);
⋮----
fn test_remove_transaction_cost() {
⋮----
let transaction = WritableKeysTransaction(vec![Pubkey::new_unique()]);
⋮----
cost_tracker.add_transaction_cost(&tx_cost);
⋮----
assert_eq!(1, cost_tracker.number_of_accounts());
assert_eq!(cost, cost_tracker.block_cost());
⋮----
assert_eq!(0, cost_tracker.allocated_accounts_data_size.0);
cost_tracker.remove_transaction_cost(&tx_cost);
assert_eq!(0, cost_tracker.transaction_count.0);
assert_eq!(0, cost_tracker.number_of_accounts());
assert_eq!(0, cost_tracker.block_cost());
⋮----
fn test_get_cost_by_writable_accounts_post_analysis() {
⋮----
let cost_by_writable_accounts = cost_tracker.get_cost_by_writable_accounts();
assert_eq!(1, cost_by_writable_accounts.len());
assert_eq!(cost, *cost_by_writable_accounts.values().next().unwrap());

================
File: cost-model/src/lib.rs
================
pub mod block_cost_limits;
pub mod cost_model;
pub mod cost_tracker;
pub mod cost_tracker_post_analysis;
pub mod transaction_cost;
⋮----
extern crate solana_frozen_abi_macro;

================
File: cost-model/src/transaction_cost.rs
================
use solana_compute_budget_instruction::compute_budget_instruction_details::ComputeBudgetInstructionDetails;
⋮----
pub enum TransactionCost<'a, Tx> {
⋮----
pub fn sum(&self) -> u64 {
⋮----
const _: () = assert!(
⋮----
Self::Transaction(usage_cost) => usage_cost.sum(),
⋮----
pub fn programs_execution_cost(&self) -> u64 {
⋮----
pub fn is_simple_vote(&self) -> bool {
⋮----
pub fn data_bytes_cost(&self) -> u16 {
⋮----
pub fn allocated_accounts_data_size(&self) -> u64 {
⋮----
pub fn loaded_accounts_data_size_cost(&self) -> u64 {
⋮----
pub fn signature_cost(&self) -> u64 {
⋮----
pub fn write_lock_cost(&self) -> u64 {
⋮----
Self::SimpleVote { .. } => block_cost_limits::WRITE_LOCK_UNITS.saturating_mul(2),
⋮----
pub fn writable_accounts(&self) -> impl Iterator<Item = &Pubkey> {
⋮----
.account_keys()
.iter()
.enumerate()
.filter_map(|(index, key)| transaction.is_writable(index).then_some(key))
⋮----
pub fn num_transaction_signatures(&self) -> u64 {
⋮----
.signature_details()
.num_transaction_signatures(),
⋮----
pub fn num_secp256k1_instruction_signatures(&self) -> u64 {
⋮----
.num_secp256k1_instruction_signatures(),
⋮----
pub fn num_ed25519_instruction_signatures(&self) -> u64 {
⋮----
.num_ed25519_instruction_signatures(),
⋮----
pub fn num_secp256r1_instruction_signatures(&self) -> u64 {
⋮----
.num_secp256r1_instruction_signatures(),
⋮----
pub struct UsageCostDetails<'a, Tx> {
⋮----
.saturating_add(self.write_lock_cost)
.saturating_add(u64::from(self.data_bytes_cost))
.saturating_add(self.programs_execution_cost)
.saturating_add(self.loaded_accounts_data_size_cost)
⋮----
pub struct WritableKeysTransaction(pub Vec<Pubkey>);
⋮----
fn num_transaction_signatures(&self) -> u64 {
unimplemented!("WritableKeysTransaction::num_transaction_signatures")
⋮----
fn num_write_locks(&self) -> u64 {
unimplemented!("WritableKeysTransaction::num_write_locks")
⋮----
fn recent_blockhash(&self) -> &solana_hash::Hash {
unimplemented!("WritableKeysTransaction::recent_blockhash")
⋮----
fn num_instructions(&self) -> usize {
unimplemented!("WritableKeysTransaction::num_instructions")
⋮----
fn instructions_iter(
⋮----
fn program_instructions_iter(
⋮----
fn static_account_keys(&self) -> &[Pubkey] {
⋮----
fn fee_payer(&self) -> &Pubkey {
unimplemented!("WritableKeysTransaction::fee_payer")
⋮----
fn num_lookup_tables(&self) -> usize {
unimplemented!("WritableKeysTransaction::num_lookup_tables")
⋮----
fn message_address_table_lookups(
⋮----
fn account_keys(&self) -> solana_message::AccountKeys<'_> {
⋮----
fn is_writable(&self, _index: usize) -> bool {
⋮----
fn is_signer(&self, _index: usize) -> bool {
unimplemented!("WritableKeysTransaction::is_signer")
⋮----
fn is_invoked(&self, _key_index: usize) -> bool {
unimplemented!("WritableKeysTransaction::is_invoked")
⋮----
fn signature(&self) -> &solana_signature::Signature {
unimplemented!("WritableKeysTransaction::signature")
⋮----
fn signatures(&self) -> &[solana_signature::Signature] {
unimplemented!("WritableKeysTransaction::signatures")
⋮----
fn message_hash(&self) -> &solana_hash::Hash {
unimplemented!("WritableKeysTransaction::message_hash")
⋮----
fn is_simple_vote_transaction(&self) -> bool {
unimplemented!("WritableKeysTransaction::is_simple_vote_transaction")
⋮----
fn signature_details(&self) -> &solana_message::TransactionSignatureDetails {
⋮----
fn compute_budget_instruction_details(&self) -> &ComputeBudgetInstructionDetails {
unimplemented!("WritableKeysTransaction::compute_budget_instruction_details")
⋮----
fn instruction_data_len(&self) -> u16 {
unimplemented!("WritableKeysTransaction::instruction_data_len")
⋮----
fn as_sanitized_transaction(
⋮----
unimplemented!("WritableKeysTransaction::as_sanitized_transaction");
⋮----
fn to_versioned_transaction(&self) -> solana_transaction::versioned::VersionedTransaction {
unimplemented!("WritableKeysTransaction::to_versioned_transaction")
⋮----
mod tests {
⋮----
fn get_example_transaction() -> VersionedTransaction {
⋮----
fn test_vote_transaction_cost() {
⋮----
get_example_transaction(),
⋮----
Some(true),
⋮----
.unwrap();
⋮----
assert_eq!(SIMPLE_VOTE_USAGE_COST, vote_cost.sum());
⋮----
fn test_non_vote_transaction_cost() {
⋮----
Some(false),
⋮----
assert_eq!(expected_non_vote_cost, non_vote_cost.sum());

================
File: cost-model/Cargo.toml
================
[package]
name = "solana-cost-model"
description = "Solana cost model"
documentation = "https://docs.rs/solana-cost-model"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_cost_model"

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "dep:solana-hash",
    "dep:solana-message",
    "dep:solana-signature",
    "dep:solana-transaction",
    "solana-compute-budget-interface/dev-context-only-utils",
]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-compute-budget/frozen-abi",
    "solana-pubkey/frozen-abi",
    "solana-vote-program/frozen-abi",
]

[dependencies]
agave-feature-set = { workspace = true }
ahash = { workspace = true }
log = { workspace = true }
solana-bincode = { workspace = true }
solana-borsh = { workspace = true }
solana-builtins-default-costs = { workspace = true }
solana-clock = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-instruction = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-fee-structure = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-hash = { workspace = true, optional = true }
solana-message = { workspace = true, optional = true }
solana-metrics = { workspace = true }
solana-packet = { workspace = true }
solana-pubkey = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signature = { workspace = true, optional = true }
solana-svm-transaction = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true, optional = true }
solana-transaction-error = { workspace = true }
solana-vote-program = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
agave-reserved-account-keys = { workspace = true }
itertools = { workspace = true }
rand = "0.8.5"
# See order-crates-for-publishing.py for using this unusual `path = "."`
solana-compute-budget-instruction = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-compute-budget-interface = { workspace = true }
solana-compute-budget-program = { workspace = true }
solana-cost-model = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-runtime-transaction = { workspace = true, features = [
    "dev-context-only-utils",
] }
solana-signer = { workspace = true }
solana-system-program = { workspace = true }
solana-system-transaction = { workspace = true }
solana-vote = { workspace = true }
static_assertions = { workspace = true }
test-case = { workspace = true }

[[bench]]
name = "cost_tracker"

[lints]
workspace = true

================
File: curves/curve25519/src/curve_syscall_traits.rs
================
pub trait PointValidation {
⋮----
pub trait GroupOperations {
⋮----
pub trait MultiScalarMultiplication {
⋮----
pub trait Pairing {

================
File: curves/curve25519/src/edwards.rs
================
pub struct PodEdwardsPoint(pub [u8; 32]);
⋮----
mod target_arch {
⋮----
pub fn validate_edwards(point: &PodEdwardsPoint) -> bool {
point.validate_point()
⋮----
pub fn add_edwards(
⋮----
pub fn subtract_edwards(
⋮----
pub fn multiply_edwards(
⋮----
pub fn multiscalar_multiply_edwards(
⋮----
fn from(point: &EdwardsPoint) -> Self {
Self(point.compress().to_bytes())
⋮----
type Error = Curve25519Error;
fn try_from(pod: &PodEdwardsPoint) -> Result<Self, Self::Error> {
⋮----
return Err(Curve25519Error::PodConversion);
⋮----
.decompress()
.ok_or(Curve25519Error::PodConversion)
⋮----
impl PointValidation for PodEdwardsPoint {
type Point = Self;
fn validate_point(&self) -> bool {
⋮----
compressed_edwards_y.decompress().is_some()
⋮----
impl GroupOperations for PodEdwardsPoint {
type Scalar = PodScalar;
⋮----
fn add(left_point: &Self, right_point: &Self) -> Option<Self> {
let left_point: EdwardsPoint = left_point.try_into().ok()?;
let right_point: EdwardsPoint = right_point.try_into().ok()?;
⋮----
Some((&result).into())
⋮----
fn subtract(left_point: &Self, right_point: &Self) -> Option<Self> {
⋮----
fn multiply(scalar: &PodScalar, point: &Self) -> Option<Self> {
let scalar: Scalar = scalar.try_into().ok()?;
let point: EdwardsPoint = point.try_into().ok()?;
⋮----
impl MultiScalarMultiplication for PodEdwardsPoint {
⋮----
fn multiscalar_multiply(scalars: &[PodScalar], points: &[Self]) -> Option<Self> {
⋮----
.iter()
.map(|scalar| Scalar::try_from(scalar).ok())
⋮----
.map(|point| EdwardsPoint::try_from(point).ok()),
⋮----
.map(|result| PodEdwardsPoint::from(&result))
⋮----
Some(result_point)
⋮----
scalars.as_ptr() as *const u8,
points.as_ptr() as *const u8,
points.len() as u64,
⋮----
mod tests {
⋮----
fn test_validate_edwards() {
let pod = PodEdwardsPoint(G.compress().to_bytes());
assert!(validate_edwards(&pod));
⋮----
assert!(!validate_edwards(&PodEdwardsPoint(invalid_bytes)));
⋮----
fn test_edwards_add_subtract() {
let identity = PodEdwardsPoint(EdwardsPoint::identity().compress().to_bytes());
let point = PodEdwardsPoint([
⋮----
assert_eq!(add_edwards(&point, &identity).unwrap(), point);
assert_eq!(subtract_edwards(&point, &identity).unwrap(), point);
let point_a = PodEdwardsPoint([
⋮----
let point_b = PodEdwardsPoint([
⋮----
let point_c = PodEdwardsPoint([
⋮----
assert_eq!(
⋮----
let point = PodEdwardsPoint(G.compress().to_bytes());
let point_negated = PodEdwardsPoint((-G).compress().to_bytes());
assert_eq!(point_negated, subtract_edwards(&identity, &point).unwrap(),)
⋮----
fn test_edwards_mul() {
let scalar_a = PodScalar([
⋮----
let point_x = PodEdwardsPoint([
⋮----
let point_y = PodEdwardsPoint([
⋮----
let ax = multiply_edwards(&scalar_a, &point_x).unwrap();
let bx = multiply_edwards(&scalar_a, &point_y).unwrap();
⋮----
fn test_multiscalar_multiplication_edwards() {
let scalar = PodScalar([
⋮----
let basic_product = multiply_edwards(&scalar, &point).unwrap();
let msm_product = multiscalar_multiply_edwards(&[scalar], &[point]).unwrap();
assert_eq!(basic_product, msm_product);
⋮----
let scalar_b = PodScalar([
⋮----
let by = multiply_edwards(&scalar_b, &point_y).unwrap();
let basic_product = add_edwards(&ax, &by).unwrap();
⋮----
multiscalar_multiply_edwards(&[scalar_a, scalar_b], &[point_x, point_y]).unwrap();

================
File: curves/curve25519/src/errors.rs
================
use thiserror::Error;
⋮----
pub enum Curve25519Error {

================
File: curves/curve25519/src/lib.rs
================
pub mod curve_syscall_traits;
pub mod edwards;
pub mod errors;
pub mod ristretto;
pub mod scalar;

================
File: curves/curve25519/src/ristretto.rs
================
pub struct PodRistrettoPoint(pub [u8; 32]);
⋮----
mod target_arch {
⋮----
pub fn validate_ristretto(point: &PodRistrettoPoint) -> bool {
point.validate_point()
⋮----
pub fn add_ristretto(
⋮----
pub fn subtract_ristretto(
⋮----
pub fn multiply_ristretto(
⋮----
pub fn multiscalar_multiply_ristretto(
⋮----
fn from(point: &RistrettoPoint) -> Self {
Self(point.compress().to_bytes())
⋮----
type Error = Curve25519Error;
fn try_from(pod: &PodRistrettoPoint) -> Result<Self, Self::Error> {
⋮----
return Err(Curve25519Error::PodConversion);
⋮----
.decompress()
.ok_or(Curve25519Error::PodConversion)
⋮----
impl PointValidation for PodRistrettoPoint {
type Point = Self;
fn validate_point(&self) -> bool {
⋮----
compressed_ristretto.decompress().is_some()
⋮----
impl GroupOperations for PodRistrettoPoint {
type Scalar = PodScalar;
⋮----
fn add(left_point: &Self, right_point: &Self) -> Option<Self> {
let left_point: RistrettoPoint = left_point.try_into().ok()?;
let right_point: RistrettoPoint = right_point.try_into().ok()?;
⋮----
Some((&result).into())
⋮----
fn subtract(left_point: &Self, right_point: &Self) -> Option<Self> {
⋮----
fn multiply(scalar: &PodScalar, point: &Self) -> Option<Self> {
let scalar: Scalar = scalar.try_into().ok()?;
let point: RistrettoPoint = point.try_into().ok()?;
⋮----
impl MultiScalarMultiplication for PodRistrettoPoint {
⋮----
fn multiscalar_multiply(scalars: &[PodScalar], points: &[Self]) -> Option<Self> {
⋮----
.iter()
.map(|scalar| Scalar::try_from(scalar).ok())
⋮----
.map(|point| RistrettoPoint::try_from(point).ok()),
⋮----
.map(|result| PodRistrettoPoint::from(&result))
⋮----
Some(result_point)
⋮----
scalars.as_ptr() as *const u8,
points.as_ptr() as *const u8,
points.len() as u64,
⋮----
mod tests {
⋮----
fn test_validate_ristretto() {
let pod = PodRistrettoPoint(G.compress().to_bytes());
assert!(validate_ristretto(&pod));
⋮----
assert!(!validate_ristretto(&PodRistrettoPoint(invalid_bytes)));
⋮----
fn test_add_subtract_ristretto() {
let identity = PodRistrettoPoint(RistrettoPoint::identity().compress().to_bytes());
let point = PodRistrettoPoint([
⋮----
assert_eq!(add_ristretto(&point, &identity).unwrap(), point);
assert_eq!(subtract_ristretto(&point, &identity).unwrap(), point);
let point_a = PodRistrettoPoint([
⋮----
let point_b = PodRistrettoPoint([
⋮----
let point_c = PodRistrettoPoint([
⋮----
assert_eq!(
⋮----
let point = PodRistrettoPoint(G.compress().to_bytes());
let point_negated = PodRistrettoPoint((-G).compress().to_bytes());
⋮----
fn test_multiply_ristretto() {
let scalar_x = PodScalar([
⋮----
let ax = multiply_ristretto(&scalar_x, &point_a).unwrap();
let bx = multiply_ristretto(&scalar_x, &point_b).unwrap();
⋮----
fn test_multiscalar_multiplication_ristretto() {
let scalar = PodScalar([
⋮----
let basic_product = multiply_ristretto(&scalar, &point).unwrap();
let msm_product = multiscalar_multiply_ristretto(&[scalar], &[point]).unwrap();
assert_eq!(basic_product, msm_product);
let scalar_a = PodScalar([
⋮----
let scalar_b = PodScalar([
⋮----
let point_x = PodRistrettoPoint([
⋮----
let point_y = PodRistrettoPoint([
⋮----
let ax = multiply_ristretto(&scalar_a, &point_x).unwrap();
let by = multiply_ristretto(&scalar_b, &point_y).unwrap();
let basic_product = add_ristretto(&ax, &by).unwrap();
⋮----
multiscalar_multiply_ristretto(&[scalar_a, scalar_b], &[point_x, point_y]).unwrap();

================
File: curves/curve25519/src/scalar.rs
================
pub struct PodScalar(pub [u8; 32]);
⋮----
mod target_arch {
⋮----
fn from(scalar: &Scalar) -> Self {
Self(scalar.to_bytes())
⋮----
type Error = Curve25519Error;
fn try_from(pod: &PodScalar) -> Result<Self, Self::Error> {
⋮----
.into_option()
.ok_or(Curve25519Error::PodConversion)
⋮----
fn from(scalar: Scalar) -> Self {
⋮----
fn try_from(pod: PodScalar) -> Result<Self, Self::Error> {

================
File: curves/curve25519/.gitignore
================
/farf/

================
File: curves/curve25519/Cargo.toml
================
[package]
name = "solana-curve25519"
description = "Solana Curve25519 Syscalls"
documentation = "https://docs.rs/solana-curve25519"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
bytemuck = { workspace = true }
bytemuck_derive = { workspace = true }
# this crate uses `subtle::CtOption<Scalar>::into_option` via curve25519-dalek,
# which requires subtle v2.6.1, but curve25519-dalek only requires v2.3.0
# The line below help users of this crate obtain correct subtle version.
# (that is, the version specified by our workspace or greater minor version, not the version specified by curve25519-dalek)
subtle = { workspace = true }
thiserror = { workspace = true }

[target.'cfg(not(target_os = "solana"))'.dependencies]
curve25519-dalek = { workspace = true, features = ["serde"] }

[target.'cfg(target_os = "solana")'.dependencies]
solana-define-syscall = { workspace = true }

[lints]
workspace = true

================
File: deploy_programs
================
#!/usr/bin/env sh
# Deploys the tip payment and tip distribution programs on local validator at predetermined address
set -eux

WALLET_LOCATION=~/.config/solana/id.json

# build this solana binary to ensure we're using a version compatible with the validator
cargo b --release --bin solana

./target/release/solana airdrop -ul 1000 $WALLET_LOCATION

(cd jito-programs/tip-payment && anchor build)

# NOTE: make sure the declare_id! is set correctly in the programs
# Also, || true to make sure if fails the first time around, tip_payment can still be deployed
RUST_INFO=trace ./target/release/solana deploy --keypair $WALLET_LOCATION -ul ./jito-programs/tip-payment/target/deploy/tip_distribution.so ./jito-programs/tip-payment/dev/dev_tip_distribution.json || true
RUST_INFO=trace ./target/release/solana deploy --keypair $WALLET_LOCATION -ul ./jito-programs/tip-payment/target/deploy/tip_payment.so  ./jito-programs/tip-payment/dev/dev_tip_payment.json

================
File: dev/Dockerfile
================
FROM rust:1.64-slim-bullseye as builder

# Add Google Protocol Buffers for Libra's metrics library.
ENV PROTOC_VERSION 3.8.0
ENV PROTOC_ZIP protoc-$PROTOC_VERSION-linux-x86_64.zip

RUN set -x \
 && apt update \
 && apt install -y \
      clang \
      libclang-dev \
      cmake \
      libudev-dev \
      make \
      unzip \
      libssl-dev \
      pkg-config \
      zlib1g-dev \
      curl \
 && rustup component add rustfmt \
 && rustup component add clippy \
 && rustc --version \
 && cargo --version \
 && curl -OL https://github.com/google/protobuf/releases/download/v$PROTOC_VERSION/$PROTOC_ZIP \
 && unzip -o $PROTOC_ZIP -d /usr/local bin/protoc \
 && unzip -o $PROTOC_ZIP -d /usr/local include/* \
 && rm -f $PROTOC_ZIP


WORKDIR /solana
COPY . .
RUN mkdir -p docker-output

ARG ci_commit
# NOTE: Keep this here before build since variable is referenced during CI build step.
ENV CI_COMMIT=$ci_commit

ARG debug

# Uses docker buildkit to cache the image.
# /usr/local/cargo/git needed for crossbeam patch
RUN --mount=type=cache,mode=0777,target=/solana/target \
    --mount=type=cache,mode=0777,target=/usr/local/cargo/registry \
    --mount=type=cache,mode=0777,target=/usr/local/cargo/git \
    if [ "$debug" = "false" ] ; then \
      ./cargo stable build --release && cp target/release/solana* ./docker-output && cp target/release/agave* ./docker-output; \
    else \
      RUSTFLAGS='-g -C force-frame-pointers=yes' ./cargo stable build --release && cp target/release/solana* ./docker-output && cp target/release/agave* ./docker-output; \
    fi

================
File: dev-bins/.config/nextest.toml
================
[store]
dir = "target/nextest"

[test-groups]
build-sbf = { max-threads = 1 }

[profile.ci]
failure-output = "immediate-final"
slow-timeout = { period = "60s", terminate-after = 1 }
retries = { backoff = "fixed", count = 3, delay = "1s" }

[profile.ci.junit]
path = "../junit.xml"

[[profile.ci.overrides]]
filter = "package(solana-bench-tps)"
threads-required = "num-cpus"

================
File: dev-bins/Cargo.toml
================
[workspace]
members = [
    "../accounts-db/store-tool",
    "../banking-bench",
    "../bench-tps",
    "../dos",
    "../ledger-tool",
]

resolver = "2"

[workspace.package]
version = "4.0.0-alpha.0"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
description = "Blockchain, Rebuilt for Scale"
repository = "https://github.com/anza-xyz/agave"
homepage = "https://anza.xyz/"
license = "Apache-2.0"
edition = "2021"

[workspace.lints.rust]
warnings = "deny"

# List of rust-2024-compatibility lints that are already satisfied
# See https://doc.rust-lang.org/rustc/lints/groups.html
boxed_slice_into_iter = "deny"
dependency_on_unit_never_type_fallback = "deny"
deprecated_safe_2024 = "deny"
impl_trait_overcaptures = "deny"
missing_unsafe_on_extern = "deny"
never_type_fallback_flowing_into_unsafe = "deny"
rust_2024_guarded_string_incompatible_syntax = "deny"
rust_2024_incompatible_pat = "deny"
rust_2024_prelude_collisions = "deny"
static_mut_refs = "deny"
unsafe_attr_outside_unsafe = "deny"
unsafe_op_in_unsafe_fn = "deny"

[workspace.lints.rust.unexpected_cfgs]
level = "warn"
check-cfg = [
    'cfg(target_os, values("solana"))',
    'cfg(feature, values("frozen-abi", "no-entrypoint"))',
]

# Clippy lint configuration that can not be applied in clippy.toml
[workspace.lints.clippy]
arithmetic_side_effects = "deny"
default_trait_access = "deny"
manual_let_else = "deny"
used_underscore_binding = "deny"

[workspace.dependencies]
agave-banking-stage-ingress-types = { path = "../banking-stage-ingress-types", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-feature-set = { path = "../feature-set", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-logger = { path = "../logger", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-reserved-account-keys = { path = "../reserved-account-keys", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-snapshots = { path = "../snapshots", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-syscalls = { path = "../syscalls", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
ahash = "0.8.11"
assert_cmd = "2.0"
assert_matches = "1.5.0"
bincode = "1.3.3"
bs58 = { version = "0.5.1", default-features = false }
chrono = { version = "0.4.42", default-features = false }
clap = "2.33.1"
crossbeam-channel = "0.5.15"
csv = "1.4.0"
dashmap = "5.5.3"
futures = "0.3.31"
histogram = "0.6.9"
itertools = "0.13.0"
jemallocator = { package = "tikv-jemallocator", version = "0.6.0", features = [
    "unprefixed_malloc_on_supported_platforms",
] }
log = "0.4.28"
num_cpus = "1.17.0"
pretty-hex = "0.3.0"
rand = "0.9.2"
rayon = "1.11.0"
regex = "1.12.2"
serde = { version = "1.0.228", features = ["derive"] }
serde_bytes = "0.11.19"
serde_json = "1.0.145"
serde_yaml = "0.9.34"
serial_test = "2.0.0"
signal-hook = "0.3.18"
solana-account = "3.2.0"
solana-account-decoder = { path = "../account-decoder", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-accounts-db = { path = "../accounts-db", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-bench-tps = { path = "../bench-tps", version = "=4.0.0-alpha.0" }
solana-bpf-loader-program = { path = "../programs/bpf_loader", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-clap-utils = { path = "../clap-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-cli-config = { path = "../cli-config", version = "=4.0.0-alpha.0" }
solana-cli-output = { path = "../cli-output", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-client = { path = "../client", version = "=4.0.0-alpha.0" }
solana-clock = "3.0.0"
solana-cluster-type = "3.0.0"
solana-commitment-config = "3.0.0"
solana-compute-budget = { path = "../compute-budget", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-compute-budget-interface = "3.0.0"
solana-connection-cache = { path = "../connection-cache", version = "=4.0.0-alpha.0", default-features = false, features = ["agave-unstable-api"] }
solana-core = { path = "../core", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-cost-model = { path = "../cost-model", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-entry = { path = "../entry", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-faucet = { path = "../faucet", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-feature-gate-interface = "3.0.0"
solana-fee-calculator = "3.0.0"
solana-genesis = { path = "../genesis", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-genesis-config = "3.0.0"
solana-genesis-utils = { path = "../genesis-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-geyser-plugin-manager = { path = "../geyser-plugin-manager", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-gossip = { path = "../gossip", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-hash = "3.1.0"
solana-inflation = "3.0.0"
solana-instruction = "3.0.0"
solana-keypair = "3.0.1"
solana-ledger = { path = "../ledger", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-loader-v3-interface = "6.1.0"
solana-local-cluster = { path = "../local-cluster", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-measure = { path = "../measure", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-message = "3.0.1"
solana-metrics = { path = "../metrics", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-native-token = "3.0.0"
solana-net-utils = { path = "../net-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-nonce = "3.0.0"
solana-perf = { path = "../perf", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-poh = { path = "../poh", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-program-runtime = { path = "../program-runtime", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-pubkey = { version = "3.0.0", default-features = false }
solana-quic-client = { path = "../quic-client", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-rent = "3.0.0"
solana-rpc = { path = "../rpc", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-rpc-client = { path = "../rpc-client", version = "=4.0.0-alpha.0", default-features = false }
solana-rpc-client-api = { path = "../rpc-client-api", version = "=4.0.0-alpha.0" }
solana-rpc-client-nonce-utils = { path = "../rpc-client-nonce-utils", version = "=4.0.0-alpha.0" }
solana-runtime = { path = "../runtime", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-runtime-transaction = { path = "../runtime-transaction", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-sbpf = { version = "=0.13.1", default-features = false }
solana-sdk-ids = "3.0.0"
solana-shred-version = "3.0.0"
solana-signature = { version = "3.1.0", default-features = false }
solana-signer = "3.0.0"
solana-stake-interface = { version = "2.0.1" }
solana-storage-bigtable = { path = "../storage-bigtable", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-streamer = { path = "../streamer", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-callback = { path = "../svm-callback", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-feature-set = { path = "../svm-feature-set", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-log-collector = { path = "../svm-log-collector", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-type-overrides = { path = "../svm-type-overrides", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-system-interface = "2.0"
solana-system-transaction = "3.0.0"
solana-test-validator = { path = "../test-validator", version = "=4.0.0-alpha.0" }
solana-time-utils = "3.0.0"
solana-tps-client = { path = "../tps-client", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-tpu-client = { path = "../tpu-client", version = "=4.0.0-alpha.0", default-features = false, features = ["agave-unstable-api"] }
solana-transaction = "3.0.2"
solana-transaction-context = { path = "../transaction-context", version = "=4.0.0-alpha.0", features = ["agave-unstable-api", "bincode"] }
solana-transaction-status = { path = "../transaction-status", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-unified-scheduler-pool = { path = "../unified-scheduler-pool", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-version = { path = "../version", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-vote = { path = "../vote", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-vote-program = { path = "../programs/vote", version = "=4.0.0-alpha.0", default-features = false, features = ["agave-unstable-api"] }
tempfile = "3.23.0"
thiserror = "2.0.17"
tokio = "1.48.0"

[profile.release-with-debug]
inherits = "release"
debug = true
strip = false
split-debuginfo = "off"

[profile.release]
split-debuginfo = "unpacked"
lto = "thin"

[profile.release-with-lto]
inherits = "release"
lto = "fat"
codegen-units = 1

# curve25519-dalek uses the simd backend by default in v4 if possible,
# which has very slow performance on some platforms with opt-level 0,
# which is the default for dev and test builds.
# This slowdown causes certain interactions in the solana-test-validator,
# such as verifying ZK proofs in transactions, to take much more than 400ms,
# creating problems in the testing environment.
# To enable better performance in solana-test-validator during tests and dev builds,
# we override the opt-level to 3 for the crate.
[profile.dev.package.curve25519-dalek]
opt-level = 3

[patch.crates-io]
# We include the following crates as our dependencies above from crates.io:
#
#  * spl-associated-token-account-interface
#  * spl-instruction-padding
#  * spl-memo-interface
#  * spl-pod
#  * spl-token
#  * spl-token-2022-interface
#  * spl-token-metadata-interface
#
# They, in turn, depend on a number of crates that we also include directly
# using `path` specifications.  For example, `spl-token` depends on
# `solana-program`.  And we explicitly specify `solana-program` above as a local
# path dependency:
#
#     solana-program = { path = "../../../sdk/program", version = "=1.16.0" }
#
# Unfortunately, Cargo will try to resolve the `spl-token` `solana-program`
# dependency only using what is available on crates.io.  Crates.io normally
# contains a previous version of these crates, and we end up with two versions
# of `solana-program` and `solana-zk-token-sdk` and all of their dependencies in
# our build tree.
#
# If you are developing downstream using non-crates-io solana-program (local or
# forked repo, or from github rev, eg), duplicate the following patch statements
# in your Cargo.toml. If you still hit duplicate-type errors with the patch
# statements in place, run `cargo update -p solana-program` and/or `cargo update
# -p solana-zk-token-sdk` to remove extraneous versions from your Cargo.lock
# file.
#
# There is a similar override in `programs/sbf/Cargo.toml`.  Please keep both
# comments and the overrides in sync.
solana-curve25519 = { path = "../curves/curve25519" }

================
File: docker-solana/.gitignore
================
cargo-install/
usr/

================
File: docker-solana/build.sh
================
set -ex
cd "$(dirname "$0")"/..
eval "$(ci/channel-info.sh)"
source ci/rust-version.sh
source ci/docker/env.sh
CHANNEL_OR_TAG=
if [[ -n "$CI_TAG" ]]; then
  CHANNEL_OR_TAG=$CI_TAG
else
  CHANNEL_OR_TAG=$CHANNEL
fi
if [[ -z $CHANNEL_OR_TAG ]]; then
  echo Unable to determine channel or tag to publish into, exiting.
  echo "^^^ +++"
  exit 0
fi
cd "$(dirname "$0")"
rm -rf usr/
../ci/docker-run-default-image.sh scripts/cargo-install-all.sh docker-solana/usr
cp -f ../scripts/run.sh usr/bin/solana-run.sh
cp -f ../fetch-core-bpf.sh usr/bin/
cp -f ../fetch-spl.sh usr/bin/
cp -f ../fetch-programs.sh usr/bin/
(
  cd usr/bin
  ./fetch-core-bpf.sh
  ./fetch-spl.sh
)
docker build \
  --build-arg "BASE_IMAGE=${CI_DOCKER_ARG_BASE_IMAGE}" \
  -t anzaxyz/agave:"$CHANNEL_OR_TAG" .
maybeEcho=
if [[ -z $CI ]]; then
  echo "Not CI, skipping |docker push|"
  maybeEcho="echo"
else
  (
    set +x
    if [[ -n $DOCKER_PASSWORD && -n $DOCKER_USERNAME ]]; then
      echo "$DOCKER_PASSWORD" | docker login --username "$DOCKER_USERNAME" --password-stdin
    fi
  )
fi
$maybeEcho docker push anzaxyz/agave:"$CHANNEL_OR_TAG"

================
File: docker-solana/Dockerfile
================
ARG BASE_IMAGE=
FROM ${BASE_IMAGE}

# RPC JSON
EXPOSE 8899/tcp
# RPC pubsub
EXPOSE 8900/tcp
# entrypoint
EXPOSE 8001/tcp
# (future) bank service
EXPOSE 8901/tcp
# bank service
EXPOSE 8902/tcp
# faucet
EXPOSE 9900/tcp
# tvu
EXPOSE 8000/udp
# gossip
EXPOSE 8001/udp
# tvu_quic
EXPOSE 8002/udp
# tpu
EXPOSE 8003/udp
# tpu_forwards
EXPOSE 8004/udp
# retransmit
EXPOSE 8005/udp
# repair
EXPOSE 8006/udp
# serve_repair
EXPOSE 8007/udp
# broadcast
EXPOSE 8008/udp
# tpu_vote
EXPOSE 8009/udp

RUN apt-get update && \
    apt-get install -y bzip2 libssl-dev ca-certificates && \
    rm -rf /var/lib/apt/lists/*

COPY usr/bin /usr/bin/
ENTRYPOINT [ "/usr/bin/solana-run.sh" ]
CMD [""]

================
File: docker-solana/README.md
================
## Minimal Solana Docker image
This image is automatically updated by CI

https://hub.docker.com/r/anzaxyz/agave/

### Usage:
Run the latest beta image:
```bash
$ docker run --rm -p 8899:8899 --ulimit nofile=1000000 anzaxyz/agave:beta
```

Run the latest edge image:
```bash
$ docker run --rm -p 8899:8899 --ulimit nofile=1000000 anzaxyz/agave:edge
```

Port *8899* is the JSON RPC port, which is used by clients to communicate with the network.

================
File: docs/art/fork-generation.bob
================
validator action
               +----+                                           ----------------
         |     | L1 |                  E1
         |     +----+            /             \                vote(E1)
         |     | L2 |          E2                x
         |     +----+        /    \           /     \           vote(E2)
  time   |     | L3 |     E3        x        E3'      x
         |     +----+    /  \     /   \     /  \     /  \       slash(E3)
         |     | L4 |  x     x  E4     x   x    x   x    x
         |     +----+  |     |  |      |   |    |   |    |      vote(E4)
         v     | L5 |  xx   xx  xx    E5  xx   xx  xx   xx
               +----+                                           hang on to E4 and E5 for more...

================
File: docs/art/forks-pruned.bob
================
1
     |
     2
    /|
   / |
  |  |
  |  4
  |
  5

================
File: docs/art/forks-pruned2.bob
================
1
 |
 3
 |\
 | \
 |  |
 |  |
 |  |
 6  |
    |
    7

================
File: docs/art/forks.bob
================
1
     |\
     2 \
    /|  |
   / |  3
  |  |  |\
  |  4  | \
  |     |  |
  5     |  |
        |  |
        6  |
           |
           7

================
File: docs/art/passive-staking-callflow.msc
================
msc {
  hscale="2.2";
   VoteSigner,
   Validator,
   Cluster,
   StakerX,
   StakerY;

   |||;
  Validator box Validator [label="boot.."];

  VoteSigner <:> Validator [label="register\n\n(optional)"];
  Validator => Cluster [label="VoteState::Initialize(VoteSigner)"];
  StakerX => Cluster [label="StakeStateV2::Delegate(Validator)"];
  StakerY => Cluster [label="StakeStateV2::Delegate(Validator)"];

     |||;
  Validator box Cluster [label="\nvalidate\n"];
  Validator => VoteSigner [label="sign(vote)"];
  VoteSigner >> Validator [label="signed vote"];

  Validator => Cluster [label="gossip(vote)"];
  ...;
  ... ;
  Validator abox Validator [label="\nmax\nlockout\n"];
       |||;
  Cluster box Cluster [label="credits redeemed (at epoch)"];


}

================
File: docs/art/retransmit_stage.bob
================
+------------+
                                                   |  Gossip    |
                                                   |  Service   |
                                                   |            |
                                                   +------------+
                                                          |ContactInfo
                                                          |
                          +------------------------------------------------------------------------+
                          |                               |                                        |
                          |  Retransmit Stage             |                                        |
                          |                               |                                        |
                          |  +--------------------------------------+                              |
                          |  |Window Service              |         |                              |                      +---------+
                          |  |                            |         |          +---------------+   |  +-----------+       |         |
           Packets        |  | +----------------+         |         |  Shreds  |               |   |  | Deshredder|       | Replay  |
          +------------------->+ ShredFilter    +----------------------------->+ Blockstore    +------| (entries) |------>+ Stage   |
                          |  | +-----------+-+--+         |         |          |               |   |  +-----------+       |         |
+--------+                |  |        ^      |            |         |          |               |   |                      +---------+
|        | Leader Schedule|  |        |      |            |         |          |               |   |
| Bank   +----------------------------+      |     +------+----+    |          +-------+-------+   |
|        |                |  |               |     | Repair    |    |                  |           |
+--------+    +----------------------------------->+ Service   |    | Incomplete       |           |
              |           |  |               |     |           +<----------------------+           |
              |           |  |               |     +-----------+    | Slots                        |
              |           |  |               |                      |                              |
              |           |  +--------------------------------------+                              |
              |           |                  |Shreds                                               |
              |           |                  v                                                     |
              v           |          +-------+---------+                                           |
     +--------+-----+     |          |                 |                                           |
     |              |     |          | Retransmitter   |                                           |
     | Peer         |     |          |                 |                                           |
     | Validators   +<---------------+                 |                                           |
     |              |     |          +--------+--------+                                           |
     |              |     |                   ^                                                    |
     +--------------+     |                   |                                                    |
                          +------------------------------------------------------------------------+
                                              |
                                   ContactInfo|
                                              |
                                      +-------+-+
                                      | Gossip  |
                                      | Service |
                                      |         |
                                      +---------+

================
File: docs/art/runtime.bob
================
.------------.     .-----------.    .---------------.    .--------------.    .-----------------------.
| PoH verify +---> | sigverify +--->| lock accounts +--->| validate fee +--->| allocate new accounts +--->
|     TVU    |     `-----------`    `---------------`    `--------------`    `-----------------------`
`------------`

    .---------------.    .---------.    .------------.    .-----------------.   .-----------------.
--->| load accounts +--->| execute +--->| PoH record +--->| commit accounts +-->| unlock accounts |
    `---------------`    `---------`    |    TPU     |    `-----------------`   `-----------------`
                                        `------------`

================
File: docs/art/sdk-tools.bob
================
.-------------------------------------.
                 | Solana Runtime                      |
                 |                                     |
  .----------.   |   .------------.   .------------.   |
  | Program  |   |   | SBF        |   | Executable |   |
  | Author   +------>| Bytecode   +-->| Account    |   |
  |          |   |   | Verifier   |   |            |   |
  `----------`   |   `------------`   `------------`   |
                 |                           |         |
                 |          .----------------`         |
                 |          |  LoadAccounts            |
                 |          V                          |
  .----------.   |   .------------.   .-------------.  |
  |          |   |   |  SBF       |   | SBF         |  |
  |  Client  +------>|  Loader    +-->| Interpreter |  |
  |          |   |   |            |   |             |  |
  `----------`   |   `------------`   `-------------`  |
                 |                                     |
                 `-------------------------------------`

================
File: docs/art/spv-bank-hash.bob
================
+----------+
                                   | Bank-Hash|
                                   +----------+
                                        ^
                                        |
                  +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+
                  :                                              :
                  :       +--------------+     +-------------+   :
                  : Hash( | Accounts-Hash|  +  | Block-Merkle| ) :
                  :       +--------------+     +-------------+   :
                  :              ^                               :
                  +~~~~~~~~~~~~~ | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+
                                 |
 +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+ 
 :       +---------------+   +---------------+          +---------------+   : 
 : Hash( | Hash(Account1)| + | Hash(Account2)| +  ... + | Hash(AccountN)| ) : 
 :       +---------------+   +---------------+          +---------------+   : 
 +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+

================
File: docs/art/spv-block-merkle.bob
================
+---------------+
                                   |  Block-Merkle |
                                   +---------------+
                                  ^                ^
                                 /                  \
                      +-------------+            +-------------+
                      | Entry-Merkle|            | Entry-Merkle|
                      +-------------+            +-------------+
                         ^                                 ^
                        /                                   \
                  +-------+                               +-------+
                  |  Hash |                               |  Hash |
                  +-------+                               +-------+
                 ^        ^                                ^      ^
                /         |                                |       \
 +-----------------+   +-----------------+   +-----------------+   +---+
 | Hash(T1, status)|   | Hash(T2, status)|   | Hash(T3, status)|   | 0 | 
 +-----------------+   +-----------------+   +-----------------+   +---+

================
File: docs/art/tpu.bob
================
.-------------.
                                          | PoH Service |
                                          `--------+----`
                                              ^    |
             .--------------------------------|----|--------------------.
             | TPU                            |    v                    |
             |  .-------.  .-----------.    .-+-------.  .-----------.  |  .------------.
 .---------. |  | Fetch |  | SigVerify |    | Banking |  | Broadcast |  |  | Downstream |
 | Clients |--->| Stage |->| Stage     |-+->| Stage   |->| Stage     |---->| Validators |
 `---------` |  |       |  |           | |  |         |  |           |  |  |            |
             |  `-------`  `-----------` |  `----+----`  `-----------`  |  `------------`
             |                           v       |                      |
             |                  .--------+---.   |                      |
             |                  | Forwarding |   |                      |
             |                  | Stage      |   |                      |
             |                  |            |   |                      |
             |                  `------------`   |                      |
             |                                   |                      |
             `-----------------------------------|----------------------`
                                                 |
                                                 v
                                              .------.
                                              | Bank |
                                              `------`

================
File: docs/art/transaction.bob
================
+------------+-----------------------------------------------------+
  |            |                        Message                      |
  |            |----------+-------------+-------------+--------------|
  |            |          |             |             |              |
  | Signatures |          |  Account    |  Recent     |              |
  |            |  Header  |  addresses  |  blockhash  | Instructions |
  |            |          |             |             |              |
  +------------+----------+-------------+-------------+--------------+

================
File: docs/art/tvu.bob
================
+------------+
                                                                         |   Gossip   |
                                                                         |   Service  |
                          +-------------+                                |            |
                          |Child        |           +------------+       +-+-----+----+
                          |Validators   |           |Neighborhood|         |     ^
                          |             |           |Validators  |         |     |
                          +-------+-----+           |            |         |     |
                                  ^                 +---------+--+     Peer|     |Votes
                                  |                           ^        List|     |
                                  +------------------------+  |            |     |
                                   shreds(forward = true)  |  |            |     |
                         +---------------------------------------------------------------------------------------------+
                         |                                 |  |            |     |                                     |
                         |  TVU                            |  |   +--------+     |                                     |
                         |                                 |  |   v              |                                     |
+------------+           |  +-------+  +------------+    +-+--+---+---+      +---+--------------+      +-------------+ |
|            |   Repair  |  |       |  |            |    | Retransmit |      | Replay           |      | Transaction | |
| Upstream   +------------->+       +->+ Shred      +--->+ Stage      +----->+ Stage            +----->+ Status      | |
| Validators |  Turbine  |  | Shred |  | Verify     |    |            |      | +--------------+ |      | Service     | |
|            +------------->+ Fetch |  | Leader Sig |    +------+-----+      | | PoH Verify   | |      | (optional)  | |
|            |           |  | Stage |  | Stage      |           ^            | | TX Sig Verify| |      +-------------+ |
|            +------------->+       |  |            |           |            | |              | |                      |
|            |  Turbine  |  |       |  +--+---------+           |            | +-+------------+ |                      |
+------------+  Forwards |  +-------+     ^                     |            |   |              |                      |
                         |                |                     |            +------------------+                      |
                         |                |                     |                |                                     |
                         |                |                     |                |                                     |
                         |                |                     |                |                                     |
                         +---------------------------------------------------------------------------------------------+
                                          |                     |                |
                                          |                     |Validator       v
                                          |                     |Stakes      +---+-----+
                                          |                     +------------+         |
                                          +----------------------------------+ Bank    |
                                                Leader                       |         |
                                                Schedule                     +---------+

================
File: docs/art/validator-proposal.bob
================
.------------.
                                 | Upstream   |
                                 | Validators |
                                 `----+-------`
                                      |
                                      |
             .-----------------------------------.
             | Validator              |          |
             |                        v          |
             |  .-----------.    .------------.  |
 .--------.  |  | Fetch     |    | Repair     |  |
 | Client +---->| Stage     |    | Stage      |  |
 `--------`  |  `---+-------`    `----+-------`  |
             |      |                 |          |
             |      v                 v          |
             |  .-----------.    .------------.  |
             |  | TPU       |<-->| Blockstore |  |
             |  |           |    |            |  |
             |  `-----------`    `----+-------`  |
             |                        |          |
             |                        v          |
             |                   .------------.  |
             |                   | Multicast  |  |
             |                   | Stage      |  |
             |                   `----+-------`  |
             |                        |          |
             `-----------------------------------`
                                      |
                                      v
                                 .------------.
                                 | Downstream |
                                 | Validators |
                                 `------------`



                                 .------------.
                                 | PoH        |
                                 | Service    |
                                 `-------+----`
                                     ^   |
                                     |   |
             .-----------------------------------.
             | TPU                   |   |       |
             |                       |   v       |
  .-------.  |  .-----------.    .---+--------.  |  .------------.
  | Fetch +---->| SigVerify +--->| Banking    |<--->| Blockstore |
  | Stage |  |  | Stage     |    | Stage      |  |  |            |
  `-------`  |  `-----------`    `-----+------`  |  `------------`
             |                         |         |
             |                         |         |
             `-----------------------------------`
                                       |
                                       v
                                 .------------.
                                 | Banktree   |
                                 |            |
                                 `------------`

================
File: docs/art/validator.bob
================
.---------------------------------------.
             |  Validator                            |
             |                                       |
 .--------.  |  .-------------------.                |
 |        |---->|                   |                |
 | Client |  |  | JSON RPC Service  |                |
 |        |<----|                   |                |
 `----+---`  |  `-------------------`                |
      |      |     ^                                 |
      |      |     |     .----------------.          | .------------------.
      |      |     |     | Gossip Service |<-----------| Validators       |
      |      |     |     `----------------`          | |                  |
      |      |     |           ^                     | |                  |
      |      |     |           |                     | |  .------------.  |
      |      | .---+---.  .----+---. .------------.  | |  |            |  |
      |      | | Bank  |<-+ Replay | | ShredFetch |<------+ Upstream   |  |
      |      | | Forks |  | Stage  | |  Stage     |  | |  | Validators |  |
      |      | `-------`  `--------` `--+---------`  | |  |            |  |
      |      |     ^              ^     |            | |  `------------`  |
      |      |     |              |     v            | |                  |
      |      |     |           .--+---------.        | |                  |
      |      |     |           | Blockstore |        | |                  |
      |      |     |           `------------`        | |  .------------.  |
      |      |     |                ^                | |  |            |  |
      |      |     |                |                | |  | Downstream |  |
      |      |  .--+--.     .-------+---.            | |  | Validators |  |
      `-------->| TPU +---->| Broadcast +---------------->|            |  |
             |  `-----`     | Stage     |            | |  `------------`  |
             |              `-----------`            | `------------------`
             `---------------------------------------`

================
File: docs/components/Card.jsx
================
function Card(

================
File: docs/components/HomeCtaLinks.jsx
================
export default function HomeCtaLinks()

================
File: docs/src/cli/examples/_category_.json
================
{
  "position": 4.5,
  "label": "Command Examples",
  "collapsible": true,
  "collapsed": false,
  "link": null
}

================
File: docs/src/cli/examples/choose-a-cluster.md
================
---
title: Connecting to a Cluster with the Solana CLI
pagination_label: "Solana CLI: Connecting to a Cluster"
sidebar_label: Connecting to a Cluster
---

See [Solana Clusters](../../clusters/available.md) for general information about the
available clusters.

## Configure the command-line tool

You can check what cluster the Solana command-line tool (CLI) is currently targeting by
running the following command:

```bash
solana config get
```

Use `solana config set` command to target a particular cluster. After setting
a cluster target, any future subcommands will send/receive information from that
cluster.

For example to target the Devnet cluster, run:

```bash
solana config set --url https://api.devnet.solana.com
```

## Ensure Versions Match

Though not strictly necessary, the CLI will generally work best when its version
matches the software version running on the cluster. To get the locally-installed
CLI version, run:

```bash
solana --version
```

To get the cluster version, run:

```bash
solana cluster-version
```

Ensure the local CLI version is greater than or equal to the cluster version.

================
File: docs/src/cli/examples/delegate-stake.md
================
---
title: Staking SOL with the Solana CLI
pagination_label: "Solana CLI: Staking"
sidebar_label: Staking
---

After you have [received SOL](./transfer-tokens.md), you might consider putting it
to use by delegating _stake_ to a validator. Stake is what we call tokens in a
_stake account_. Solana weights validator votes by the amount of stake delegated
to them, which gives those validators more influence in determining the next
valid block of transactions in the blockchain. Solana then generates new SOL
periodically to reward stakers and validators. You earn more rewards the more
stake you delegate.

:::info
For an overview of staking, read first the
[Staking and Inflation FAQ](https://solana.com/staking).
:::

## Create a Stake Account

To delegate stake, you will need to transfer some tokens into a stake account.
To create an account, you will need a keypair. Its public key will be used as
the
[stake account address](https://solana.com/docs/references/staking/stake-accounts#account-address).
No need for a password or encryption here; this keypair will be discarded right
after creating the stake account.

```bash
solana-keygen new --no-passphrase -o stake-account.json
```

The output will contain the public key after the text `pubkey:`.

```text
pubkey: GKvqsuNcnwWqPzzuhLmGi4rzzh55FhJtGizkhHaEJqiV
```

Copy the public key and store it for safekeeping. You will need it any time you
want to perform an action on the stake account you create next.

Now, create a stake account:

```bash
solana create-stake-account --from <KEYPAIR> stake-account.json <AMOUNT> \
    --stake-authority <KEYPAIR> --withdraw-authority <KEYPAIR> \
    --fee-payer <KEYPAIR>
```

`<AMOUNT>` tokens are transferred from the account at the "from" `<KEYPAIR>` to
a new stake account at the public key of stake-account.json.

The stake-account.json file can now be discarded. To authorize additional
actions, you will use the `--stake-authority` or `--withdraw-authority` keypair,
not stake-account.json.

View the new stake account with the `solana stake-account` command:

```bash
solana stake-account <STAKE_ACCOUNT_ADDRESS>
```

The output will look similar to this:

```text
Total Stake: 5000 SOL
Stake account is undelegated
Stake Authority: EXU95vqs93yPeCeAU7mPPu6HbRUmTFPEiGug9oCdvQ5F
Withdraw Authority: EXU95vqs93yPeCeAU7mPPu6HbRUmTFPEiGug9oCdvQ5F
```

### Set Stake and Withdraw Authorities

[Stake and withdraw authorities](https://solana.com/docs/references/staking/stake-accounts#understanding-account-authorities)
can be set when creating an account via the `--stake-authority` and
`--withdraw-authority` options, or afterward with the `solana stake-authorize`
command. For example, to set a new stake authority, run:

```bash
solana stake-authorize <STAKE_ACCOUNT_ADDRESS> \
    --stake-authority <KEYPAIR> --new-stake-authority <PUBKEY> \
    --fee-payer <KEYPAIR>
```

This will use the existing stake authority `<KEYPAIR>` to authorize a new stake
authority `<PUBKEY>` on the stake account `<STAKE_ACCOUNT_ADDRESS>`.

### Advanced: Derive Stake Account Addresses

When you delegate stake, you delegate all tokens in the stake account to a
single validator. To delegate to multiple validators, you will need multiple
stake accounts. Creating a new keypair for each account and managing those
addresses can be cumbersome. Fortunately, you can derive stake addresses using
the `--seed` option:

```bash
solana create-stake-account --from <KEYPAIR> <STAKE_ACCOUNT_KEYPAIR> --seed <STRING> <AMOUNT> \
    --stake-authority <PUBKEY> --withdraw-authority <PUBKEY> --fee-payer <KEYPAIR>
```

`<STRING>` is an arbitrary string up to 32 bytes, but will typically be a number
corresponding to which derived account this is. The first account might be "0",
then "1", and so on. The public key of `<STAKE_ACCOUNT_KEYPAIR>` acts as the
base address. The command derives a new address from the base address and seed
string. To see what stake address the command will derive, use
`solana create-address-with-seed`:

```bash
solana create-address-with-seed --from <PUBKEY> <SEED_STRING> STAKE
```

`<PUBKEY>` is the public key of the `<STAKE_ACCOUNT_KEYPAIR>` passed to
`solana create-stake-account`.

The command will output a derived address, which can be used for the
`<STAKE_ACCOUNT_ADDRESS>` argument in staking operations.

## Delegate Stake

To delegate your stake to a validator, you will need its vote account address.
Find it by querying the cluster for the list of all validators and their vote
accounts with the `solana validators` command:

```bash
solana validators
```

The first column of each row contains the validator's identity and the second is
the vote account address. Choose a validator and use its vote account address in
`solana delegate-stake`:

```bash
solana delegate-stake --stake-authority <KEYPAIR> <STAKE_ACCOUNT_ADDRESS> <VOTE_ACCOUNT_ADDRESS> \
    --fee-payer <KEYPAIR>
```

The stake authority `<KEYPAIR>` authorizes the operation on the account with
address `<STAKE_ACCOUNT_ADDRESS>`. The stake is delegated to the vote account
with address `<VOTE_ACCOUNT_ADDRESS>`.

After delegating stake, use `solana stake-account` to observe the changes to the
stake account:

```bash
solana stake-account <STAKE_ACCOUNT_ADDRESS>
```

You will see new fields "Delegated Stake" and "Delegated Vote Account Address"
in the output. The output will look similar to this:

```text
Total Stake: 5000 SOL
Credits Observed: 147462
Delegated Stake: 4999.99771712 SOL
Delegated Vote Account Address: CcaHc2L43ZWjwCHART3oZoJvHLAe9hzT2DJNUpBzoTN1
Stake activates starting from epoch: 42
Stake Authority: EXU95vqs93yPeCeAU7mPPu6HbRUmTFPEiGug9oCdvQ5F
Withdraw Authority: EXU95vqs93yPeCeAU7mPPu6HbRUmTFPEiGug9oCdvQ5F
```

## Deactivate Stake

Once delegated, you can undelegate stake with the `solana deactivate-stake`
command:

```bash
solana deactivate-stake --stake-authority <KEYPAIR> <STAKE_ACCOUNT_ADDRESS> \
    --fee-payer <KEYPAIR>
```

The stake authority `<KEYPAIR>` authorizes the operation on the account with
address `<STAKE_ACCOUNT_ADDRESS>`.

Note that stake takes several epochs to "cool down". Attempts to delegate stake
in the cool down period will fail.

## Withdraw Stake

Transfer tokens out of a stake account with the `solana withdraw-stake` command:

```bash
solana withdraw-stake --withdraw-authority <KEYPAIR> <STAKE_ACCOUNT_ADDRESS> <RECIPIENT_ADDRESS> <AMOUNT> \
    --fee-payer <KEYPAIR>
```

`<STAKE_ACCOUNT_ADDRESS>` is the existing stake account, the stake authority
`<KEYPAIR>` is the withdraw authority, and `<AMOUNT>` is the number of tokens to
transfer to `<RECIPIENT_ADDRESS>`.

## Split Stake

You may want to delegate stake to additional validators while your existing
stake is not eligible for withdrawal. It might not be eligible because it is
currently staked, cooling down, or locked up. To transfer tokens from an
existing stake account to a new one, use the `solana split-stake` command:

```bash
solana split-stake --stake-authority <KEYPAIR> <STAKE_ACCOUNT_ADDRESS> <NEW_STAKE_ACCOUNT_KEYPAIR> <AMOUNT> \
    --fee-payer <KEYPAIR>
```

`<STAKE_ACCOUNT_ADDRESS>` is the existing stake account, the stake authority
`<KEYPAIR>` is the stake authority, `<NEW_STAKE_ACCOUNT_KEYPAIR>` is the keypair
for the new account, and `<AMOUNT>` is the number of tokens to transfer to the
new account.

To split a stake account into a derived account address, use the `--seed`
option. See
[Derive Stake Account Addresses](#advanced-derive-stake-account-addresses) for
details.

================
File: docs/src/cli/examples/deploy-a-program.md
================
---
title: Deploy a Solana Program with the CLI
pagination_label: "Solana CLI: Deploy a Program"
sidebar_label: Deploy a Program
---

[Redirect](https://solana.com/docs/programs/deploying)

================
File: docs/src/cli/examples/durable-nonce.md
================
---
title: Durable Transaction Nonces in the Solana CLI
pagination_label: "Solana CLI: Durable Transaction Nonces"
sidebar_label: Durable Transaction Nonces
---

Durable transaction nonces are a mechanism for getting around the typical short
lifetime of a transaction's
[`recent_blockhash`](https://solana.com/docs/core/transactions#recent-blockhash).
They are implemented as a Solana Program, the mechanics of which can be read
about in the [proposal](../../implemented-proposals/durable-tx-nonces.md).

## Usage Examples

Full usage details for durable nonce CLI commands can be found in the
[CLI reference](../usage.md).

### Nonce Authority

Authority over a nonce account can optionally be assigned to another account. In
doing so the new authority inherits full control over the nonce account from the
previous authority, including the account creator. This feature enables the
creation of more complex account ownership arrangements and derived account
addresses not associated with a keypair. The
`--nonce-authority <AUTHORITY_KEYPAIR>` argument is used to specify this account
and is supported by the following commands

- `create-nonce-account`
- `new-nonce`
- `withdraw-from-nonce-account`
- `authorize-nonce-account`

### Nonce Account Creation

The durable transaction nonce feature uses an account to store the next nonce
value. Durable nonce accounts must be
[rent-exempt](../../implemented-proposals/rent.md#two-tiered-rent-regime), so need
to carry the minimum balance to achieve this.

A nonce account is created by first generating a new keypair, then create the
account on chain

- Command

```bash
solana-keygen new -o nonce-keypair.json
solana create-nonce-account nonce-keypair.json 1
```

- Output

```text
2SymGjGV4ksPdpbaqWFiDoBz8okvtiik4KE9cnMQgRHrRLySSdZ6jrEcpPifW4xUpp4z66XM9d9wM48sA7peG2XL
```

> To keep the keypair entirely offline, use the
> [Paper Wallet](../wallets/paper.md) keypair generation
> [instructions](../wallets/paper.md#seed-phrase-generation) instead

> [Full usage documentation](../usage.md#solana-create-nonce-account)

### Querying the Stored Nonce Value

Creating a durable nonce transaction requires passing the stored nonce value as
the value to the `--blockhash` argument upon signing and submission. Obtain the
presently stored nonce value with

- Command

```bash
solana nonce nonce-keypair.json
```

- Output

```text
8GRipryfxcsxN8mAGjy8zbFo9ezaUsh47TsPzmZbuytU
```

> [Full usage documentation](../usage.md#solana-get-nonce)

### Advancing the Stored Nonce Value

While not typically needed outside a more useful transaction, the stored nonce
value can be advanced by

- Command

```bash
solana new-nonce nonce-keypair.json
```

- Output

```text
44jYe1yPKrjuYDmoFTdgPjg8LFpYyh1PFKJqm5SC1PiSyAL8iw1bhadcAX1SL7KDmREEkmHpYvreKoNv6fZgfvUK
```

> [Full usage documentation](../usage.md#solana-new-nonce)

### Display Nonce Account

Inspect a nonce account in a more human friendly format with

- Command

```bash
solana nonce-account nonce-keypair.json
```

- Output

```text
balance: 0.5 SOL
minimum balance required: 0.00136416 SOL
nonce: DZar6t2EaCFQTbUP4DHKwZ1wT8gCPW2aRfkVWhydkBvS
```

> [Full usage documentation](../usage.md#solana-nonce-account)

### Withdraw Funds from a Nonce Account

Withdraw funds from a nonce account with

- Command

```bash
solana withdraw-from-nonce-account nonce-keypair.json ~/.config/solana/id.json 0.5
```

- Output

```text
3foNy1SBqwXSsfSfTdmYKDuhnVheRnKXpoPySiUDBVeDEs6iMVokgqm7AqfTjbk7QBE8mqomvMUMNQhtdMvFLide
```

> Close a nonce account by withdrawing the full balance

> [Full usage documentation](../usage.md#solana-withdraw-from-nonce-account)

### Assign a New Authority to a Nonce Account

Reassign the authority of a nonce account after creation with

- Command

```bash
solana authorize-nonce-account nonce-keypair.json nonce-authority.json
```

- Output

```text
3F9cg4zN9wHxLGx4c3cUKmqpej4oa67QbALmChsJbfxTgTffRiL3iUehVhR9wQmWgPua66jPuAYeL1K2pYYjbNoT
```

> [Full usage documentation](../usage.md#solana-authorize-nonce-account)

## Other Commands Supporting Durable Nonces

To make use of durable nonces with other CLI subcommands, two arguments must be
supported.

- `--nonce`, specifies the account storing the nonce value
- `--nonce-authority`, specifies an optional [nonce authority](#nonce-authority)

The following subcommands have received this treatment so far

- [`pay`](../usage.md#solana-pay)
- [`delegate-stake`](../usage.md#solana-delegate-stake)
- [`deactivate-stake`](../usage.md#solana-deactivate-stake)

### Example Pay Using Durable Nonce

Here we demonstrate Alice paying Bob 1 SOL using a durable nonce. The procedure
is the same for all subcommands supporting durable nonces

#### - Create accounts

First we need some accounts for Alice, Alice's nonce and Bob

```bash
$ solana-keygen new -o alice.json
$ solana-keygen new -o nonce.json
$ solana-keygen new -o bob.json
```

#### - Fund Alice's account

Alice will need some funds to create a nonce account and send to Bob. Airdrop
her some SOL

```bash
$ solana airdrop -k alice.json 1
1 SOL
```

#### - Create Alice's nonce account

Now Alice needs a nonce account. Create one

> Here, no separate [nonce authority](#nonce-authority) is employed, so
> `alice.json` has full authority over the nonce account

```bash
$ solana create-nonce-account -k alice.json nonce.json 0.1
3KPZr96BTsL3hqera9up82KAU462Gz31xjqJ6eHUAjF935Yf8i1kmfEbo6SVbNaACKE5z6gySrNjVRvmS8DcPuwV
```

#### - A failed first attempt to pay Bob

Alice attempts to pay Bob, but takes too long to sign. The specified blockhash
expires and the transaction fails

```bash
$ solana transfer -k alice.json --blockhash expiredDTaxfagttWjQweib42b6ZHADSx94Tw8gHx11 bob.json 0.01
[2020-01-02T18:48:28.462911000Z ERROR solana_cli::cli] Io(Custom { kind: Other, error: "Transaction \"33gQQaoPc9jWePMvDAeyJpcnSPiGUAdtVg8zREWv4GiKjkcGNufgpcbFyRKRrA25NkgjZySEeKue5rawyeH5TzsV\" failed: None" })
Error: Io(Custom { kind: Other, error: "Transaction \"33gQQaoPc9jWePMvDAeyJpcnSPiGUAdtVg8zREWv4GiKjkcGNufgpcbFyRKRrA25NkgjZySEeKue5rawyeH5TzsV\" failed: None" })
```

#### - Nonce to the rescue!

Alice retries the transaction, this time specifying her nonce account and the
blockhash stored there

> Remember, `alice.json` is the [nonce authority](#nonce-authority) in this
> example

```bash
$ solana nonce-account nonce.json
balance: 0.1 SOL
minimum balance required: 0.00136416 SOL
nonce: F7vmkY3DTaxfagttWjQweib42b6ZHADSx94Tw8gHx3W7
```

```bash
$ solana transfer -k alice.json --blockhash F7vmkY3DTaxfagttWjQweib42b6ZHADSx94Tw8gHx3W7 --nonce nonce.json bob.json 0.01
HR1368UKHVZyenmH7yVz5sBAijV6XAPeWbEiXEGVYQorRMcoijeNAbzZqEZiH8cDB8tk65ckqeegFjK8dHwNFgQ
```

#### - Success!

The transaction succeeds! Bob receives 0.01 SOL from Alice and Alice's stored
nonce advances to a new value

```bash
$ solana balance -k bob.json
0.01 SOL
```

```bash
$ solana nonce-account nonce.json
balance: 0.1 SOL
minimum balance required: 0.00136416 SOL
nonce: 6bjroqDcZgTv6Vavhqf81oBHTv3aMnX19UTB51YhAZnN
```

================
File: docs/src/cli/examples/offline-signing.md
================
---
title: Offline Transaction Signing with the Solana CLI
pagination_label: "Solana CLI: Offline Transaction Signing"
sidebar_label: Offline Transaction Signing
---

Some security models require keeping signing keys, and thus the signing
process, separated from transaction creation and network broadcast. Examples
include:

- Collecting signatures from geographically disparate signers in a
  [multi-signature scheme](https://spl.solana.com/token#multisig-usage)
- Signing transactions using an [air-gapped](<https://en.wikipedia.org/wiki/Air_gap_(networking)>)
  signing device

This document describes using Solana's CLI to separately sign and submit a
transaction.

## Commands Supporting Offline Signing

At present, the following commands support offline signing:

- [`create-stake-account`](../usage.md#solana-create-stake-account)
- [`create-stake-account-checked`](../usage.md#solana-create-stake-account-checked)
- [`deactivate-stake`](../usage.md#solana-deactivate-stake)
- [`delegate-stake`](../usage.md#solana-delegate-stake)
- [`split-stake`](../usage.md#solana-split-stake)
- [`stake-authorize`](../usage.md#solana-stake-authorize)
- [`stake-authorize-checked`](../usage.md#solana-stake-authorize-checked)
- [`stake-set-lockup`](../usage.md#solana-stake-set-lockup)
- [`stake-set-lockup-checked`](../usage.md#solana-stake-set-lockup-checked)
- [`transfer`](../usage.md#solana-transfer)
- [`withdraw-stake`](../usage.md#solana-withdraw-stake)

- [`create-vote-account`](../usage.md#solana-create-vote-account)
- [`vote-authorize-voter`](../usage.md#solana-vote-authorize-voter)
- [`vote-authorize-voter-checked`](../usage.md#solana-vote-authorize-voter-checked)
- [`vote-authorize-withdrawer`](../usage.md#solana-vote-authorize-withdrawer)
- [`vote-authorize-withdrawer-checked`](../usage.md#solana-vote-authorize-withdrawer-checked)
- [`vote-update-commission`](../usage.md#solana-vote-update-commission)
- [`vote-update-validator`](../usage.md#solana-vote-update-validator)
- [`withdraw-from-vote-account`](../usage.md#solana-withdraw-from-vote-account)

## Signing Transactions Offline

To sign a transaction offline, pass the following arguments on the command line

1. `--sign-only`, prevents the client from submitting the signed transaction
   to the network. Instead, the pubkey/signature pairs are printed to stdout.
2. `--blockhash BASE58_HASH`, allows the caller to specify the value used to
   fill the transaction's `recent_blockhash` field. This serves a number of
   purposes, namely:
   _ Eliminates the need to connect to the network and query a recent blockhash
   via RPC
   _ Enables the signers to coordinate the blockhash in a multiple-signature
   scheme

### Example: Offline Signing a Payment

Command

```bash
solana@offline$ solana transfer --sign-only --blockhash 5Tx8F3jgSHx21CbtjwmdaKPLM5tWmreWAnPrbqHomSJF \
    recipient-keypair.json 1
```

Output

```text

Blockhash: 5Tx8F3jgSHx21CbtjwmdaKPLM5tWmreWAnPrbqHomSJF
Signers (Pubkey=Signature):
  FhtzLVsmcV7S5XqGD79ErgoseCLhZYmEZnz9kQg1Rp7j=4vC38p4bz7XyiXrk6HtaooUqwxTWKocf45cstASGtmrD398biNJnmTcUCVEojE7wVQvgdYbjHJqRFZPpzfCQpmUN

{"blockhash":"5Tx8F3jgSHx21CbtjwmdaKPLM5tWmreWAnPrbqHomSJF","signers":["FhtzLVsmcV7S5XqGD79ErgoseCLhZYmEZnz9kQg1Rp7j=4vC38p4bz7XyiXrk6HtaooUqwxTWKocf45cstASGtmrD398biNJnmTcUCVEojE7wVQvgdYbjHJqRFZPpzfCQpmUN"]}'
```

## Submitting Offline Signed Transactions to the Network

To submit a transaction that has been signed offline to the network, pass the
following arguments on the command line

1. `--blockhash BASE58_HASH`, must be the same blockhash as was used to sign
2. `--signer BASE58_PUBKEY=BASE58_SIGNATURE`, one for each offline signer. This
   includes the pubkey/signature pairs directly in the transaction rather than
   signing it with any local keypair(s)

### Example: Submitting an Offline Signed Payment

Command

```bash
solana@online$ solana transfer --blockhash 5Tx8F3jgSHx21CbtjwmdaKPLM5tWmreWAnPrbqHomSJF \
    --signer FhtzLVsmcV7S5XqGD79ErgoseCLhZYmEZnz9kQg1Rp7j=4vC38p4bz7XyiXrk6HtaooUqwxTWKocf45cstASGtmrD398biNJnmTcUCVEojE7wVQvgdYbjHJqRFZPpzfCQpmUN
    recipient-keypair.json 1
```

Output

```text
4vC38p4bz7XyiXrk6HtaooUqwxTWKocf45cstASGtmrD398biNJnmTcUCVEojE7wVQvgdYbjHJqRFZPpzfCQpmUN
```

## Offline Signing Over Multiple Sessions

Offline signing can also take place over multiple sessions. In this scenario,
pass the absent signer's public key for each role. All pubkeys that were specified,
but no signature was generated for will be listed as absent in the offline signing
output

### Example: Transfer with Two Offline Signing Sessions

Command (Offline Session #1)

```text
solana@offline1$ solana transfer Fdri24WUGtrCXZ55nXiewAj6RM18hRHPGAjZk3o6vBut 10 \
    --blockhash 7ALDjLv56a8f6sH6upAZALQKkXyjAwwENH9GomyM8Dbc \
    --sign-only \
    --keypair fee_payer.json \
    --from 674RgFMgdqdRoVtMqSBg7mHFbrrNm1h1r721H1ZMquHL
```

Output (Offline Session #1)

```text
Blockhash: 7ALDjLv56a8f6sH6upAZALQKkXyjAwwENH9GomyM8Dbc
Signers (Pubkey=Signature):
  3bo5YiRagwmRikuH6H1d2gkKef5nFZXE3gJeoHxJbPjy=ohGKvpRC46jAduwU9NW8tP91JkCT5r8Mo67Ysnid4zc76tiiV1Ho6jv3BKFSbBcr2NcPPCarmfTLSkTHsJCtdYi
Absent Signers (Pubkey):
  674RgFMgdqdRoVtMqSBg7mHFbrrNm1h1r721H1ZMquHL
```

Command (Offline Session #2)

```text
solana@offline2$ solana transfer Fdri24WUGtrCXZ55nXiewAj6RM18hRHPGAjZk3o6vBut 10 \
    --blockhash 7ALDjLv56a8f6sH6upAZALQKkXyjAwwENH9GomyM8Dbc \
    --sign-only \
    --keypair from.json \
    --fee-payer 3bo5YiRagwmRikuH6H1d2gkKef5nFZXE3gJeoHxJbPjy
```

Output (Offline Session #2)

```text
Blockhash: 7ALDjLv56a8f6sH6upAZALQKkXyjAwwENH9GomyM8Dbc
Signers (Pubkey=Signature):
  674RgFMgdqdRoVtMqSBg7mHFbrrNm1h1r721H1ZMquHL=3vJtnba4dKQmEAieAekC1rJnPUndBcpvqRPRMoPWqhLEMCty2SdUxt2yvC1wQW6wVUa5putZMt6kdwCaTv8gk7sQ
Absent Signers (Pubkey):
  3bo5YiRagwmRikuH6H1d2gkKef5nFZXE3gJeoHxJbPjy
```

Command (Online Submission)

```text
solana@online$ solana transfer Fdri24WUGtrCXZ55nXiewAj6RM18hRHPGAjZk3o6vBut 10 \
    --blockhash 7ALDjLv56a8f6sH6upAZALQKkXyjAwwENH9GomyM8Dbc \
    --from 674RgFMgdqdRoVtMqSBg7mHFbrrNm1h1r721H1ZMquHL \
    --signer 674RgFMgdqdRoVtMqSBg7mHFbrrNm1h1r721H1ZMquHL=3vJtnba4dKQmEAieAekC1rJnPUndBcpvqRPRMoPWqhLEMCty2SdUxt2yvC1wQW6wVUa5putZMt6kdwCaTv8gk7sQ \
    --fee-payer 3bo5YiRagwmRikuH6H1d2gkKef5nFZXE3gJeoHxJbPjy \
    --signer 3bo5YiRagwmRikuH6H1d2gkKef5nFZXE3gJeoHxJbPjy=ohGKvpRC46jAduwU9NW8tP91JkCT5r8Mo67Ysnid4zc76tiiV1Ho6jv3BKFSbBcr2NcPPCarmfTLSkTHsJCtdYi
```

Output (Online Submission)

```text
ohGKvpRC46jAduwU9NW8tP91JkCT5r8Mo67Ysnid4zc76tiiV1Ho6jv3BKFSbBcr2NcPPCarmfTLSkTHsJCtdYi
```

## Buying More Time to Sign

Typically a Solana transaction must be signed and accepted by the network within
a number of slots from the blockhash in its `recent_blockhash` field (~1min at
the time of this writing). If your signing procedure takes longer than this, a
[Durable Transaction Nonce](./durable-nonce.md) can give you the extra time you
need.

================
File: docs/src/cli/examples/sign-offchain-message.md
================
---
title: Off-Chain Message Signing with the Solana CLI
pagination_label: "Solana CLI: Off-Chain Message Signing"
sidebar_label: Off-Chain Message Signing
---

Off-chain message signing is a method of signing non-transaction messages with
a Solana wallet. This feature can be used to authenticate users or provide
proof of wallet ownership.

## Sign Off-Chain Message

To sign an arbitrary off-chain message, run the following command:

```bash
solana sign-offchain-message <MESSAGE>
```

The message will be encoded and signed with CLI's default private key and
signature printed to the output. If you want to sign it with another key, just
use the `-k/--keypair` option:

```bash
solana sign-offchain-message -k <KEYPAIR> <MESSAGE>
```

By default, the messages constructed are version 0, the only version currently
supported. When other versions become available, you can override the default
value with the `--version` option:

```bash
solana sign-offchain-message -k <KEYPAIR> --version <VERSION> <MESSAGE>
```

The message format is determined automatically based on the version and text
of the message.

Version `0` headers specify three message formats allowing for trade-offs
between compatibility and composition of messages:

| ID  |      Encoding       | Maximum Length | Hardware Wallet Support |
| :-: | :-----------------: | :------------: | :---------------------: |
|  0  | Restricted ASCII \* |      1212      |           Yes           |
|  1  |        UTF-8        |      1212      |     Blind sign only     |
|  2  |        UTF-8        |     65515      |           No            |

\* Those characters for which [`isprint(3)`](https://linux.die.net/man/3/isprint)
returns true. That is, `0x20..=0x7e`.

Formats `0` and `1` are motivated by hardware wallet support where both RAM to
store the payload and font character support are limited.

To sign an off-chain message with Ledger, ensure your Ledger is running latest
firmware and Solana Ledger App version 1.3.0 or later. After Ledger is
unlocked and Solana Ledger App is open, run:

```bash
solana sign-offchain-message -k usb://ledger <MESSAGE>
```

For more information on how to setup and work with the ledger device see this
[link](../wallets/hardware/ledger.md).

Please note that UTF-8 encoded messages require `Allow blind sign` option
enabled in Solana Ledger App. Also, due to the lack of UTF-8 support in Ledger
devices, only the hash of the message will be displayed in such cases.

If `Display mode` is set to `Expert`, Ledger will display technical
information about the message to be signed.

## Verify Off-Chain Message Signature

To verify the off-chain message signature, run the following command:

```bash
solana verify-offchain-signature <MESSAGE> <SIGNATURE>
```

The public key of the default CLI signer will be used. You can specify another
key with the `--signer` option:

```bash
solana verify-offchain-signature --signer <PUBKEY> <MESSAGE> <SIGNATURE>
```

If the signed message has a version different from the default, you need to
specify the matching version explicitly:

```bash
solana verify-offchain-signature --version <VERSION> <MESSAGE> <SIGNATURE>
```

## Protocol Specification

To ensure that off-chain messages are not valid transactions, they are encoded
with a fixed prefix: `\xffsolana offchain`, where first byte is chosen such
that it is implicitly illegal as the first byte in a transaction
`MessageHeader` today. More details about the payload format and other
considerations are available in the
[proposal](https://github.com/anza-xyz/agave/blob/master/docs/src/proposals/off-chain-message-signing.md).

================
File: docs/src/cli/examples/test-validator.md
================
---
title: Solana Test Validator
pagination_label: "Solana CLI: Test Validator"
sidebar_label: Test Validator
---

During early stage development, it is often convenient to target a cluster with
fewer restrictions and more configuration options than the public offerings
provide. This is easily achieved with the `solana-test-validator` binary, which
starts a full-featured, single-node cluster on the developer's workstation.

## Advantages

- No RPC rate-limits
- No airdrop limits
- Direct [on-chain program](https://solana.com/docs/programs/rust) deployment
  (`--bpf-program ...`)
- Clone accounts from a public cluster, including programs (`--clone ...`)
- Load accounts from files
- Configurable transaction history retention (`--limit-ledger-size ...`)
- Configurable epoch length (`--slots-per-epoch ...`)
- Jump to an arbitrary slot (`--warp-slot ...`)

## Installation

The `solana-test-validator` binary ships with the Solana CLI Tool Suite.
[Install](../install.md) before continuing.

## Running

First take a look at the configuration options

```
solana-test-validator --help
```

Next start the test validator

```
solana-test-validator
```

By default, basic status information is printed while the process is running.
See [Appendix I](#appendix-i-status-output) for details

```
Ledger location: test-ledger
Log: test-ledger/validator.log
Identity: EPhgPANa5Rh2wa4V2jxt7YbtWa3Uyw4sTeZ13cQjDDB8
Genesis Hash: 4754oPEMhAKy14CZc8GzQUP93CB4ouELyaTs4P8ittYn
Version: 1.6.7
Shred Version: 13286
Gossip Address: 127.0.0.1:1024
TPU Address: 127.0.0.1:1027
JSON RPC URL: http://127.0.0.1:8899
⠈ 00:36:02 | Processed Slot: 5142 | Confirmed Slot: 5142 | Finalized Slot: 5110 | Snapshot Slot: 5100 | Transactions: 5142 | ◎499.974295000
```

Leave `solana-test-validator` running in its own terminal. When it is no longer
needed, it can be stopped with ctrl-c.

## Interacting

Open a new terminal to interact with a [running](#running) `solana-test-validator`
instance using other binaries from the Solana CLI Tool Suite or your own client
software.

#### Configure the CLI Tool Suite to target a local cluster by default

```
solana config set --url http://127.0.0.1:8899
```

#### Verify the CLI Tool Suite configuration

```
solana genesis-hash
```

- **NOTE:** The result should match the `Genesis Hash:` field in the
  `solana-test-validator` status output

#### Check the wallet balance

```
solana balance
```

- **NOTE:** `Error: No such file or directory (os error 2)` means that the default
  wallet does not yet exist. Create it with `solana-keygen new`.
- **NOTE:** If the wallet has a zero SOL balance, airdrop some localnet SOL with
  `solana airdrop 10`

#### Perform a basic transfer transaction

```
solana transfer EPhgPANa5Rh2wa4V2jxt7YbtWa3Uyw4sTeZ13cQjDDB8 1
```

#### Monitor `msg!()` output from on-chain programs

```
solana logs
```

- **NOTE:** This command needs to be running when the target transaction is
  executed. Run it in its own terminal

## Appendix I: Status Output

```
Ledger location: test-ledger
```

- File path of the ledger storage directory. This directory can get large. Store
  less transaction history with `--limit-ledger-size ...` or relocate it with
  `--ledger ...`

```
Log: test-ledger/validator.log
```

- File path of the validator text log file. The log can also be streamed by
  passing `--log`. Status output is suppressed in this case.

```
Identity: EPhgPANa5Rh2wa4V2jxt7YbtWa3Uyw4sTeZ13cQjDDB8
```

- The validator's identity in the [gossip network](../../validator/gossip.md#gossip-overview)

```
Version: 1.6.7
```

- The software version

```
Gossip Address: 127.0.0.1:1024
TPU Address: 127.0.0.1:1027
JSON RPC URL: http://127.0.0.1:8899
```

- The network address of the [Gossip](../../validator/gossip.md#gossip-overview),
  [Transaction Processing Unit](../../validator/tpu.md) and [JSON RPC](https://solana.com/docs/rpc)
  service, respectively

```
⠈ 00:36:02 | Processed Slot: 5142 | Confirmed Slot: 5142 | Finalized Slot: 5110 | Snapshot Slot: 5100 | Transactions: 5142 | ◎499.974295000
```

- Session running time, current slot of the three block
  [commitment levels](https://solana.com/docs/rpc#configuring-state-commitment),
  slot height of the last snapshot, transaction count,
  [voting authority](../../operations/guides/vote-accounts.md#vote-authority) balance

## Appendix II: Runtime Features

By default, the test validator runs with all [runtime features](https://solana.com/docs/core/runtime#features) activated.

You can verify this using the [Solana command-line tools](../install.md):

```bash
solana feature status -ul
```

Since this may not always be desired, especially when testing programs meant for deployment to mainnet, the CLI provides an option to deactivate specific features:

```bash
solana-test-validator --deactivate-feature <FEATURE_PUBKEY_1> --deactivate-feature <FEATURE_PUBKEY_2>
```

================
File: docs/src/cli/examples/transfer-tokens.md
================
---
title: Send and Receive Tokens with the Solana CLI
pagination_label: "Solana CLI: Send and Receive Tokens"
sidebar_label: Send and Receive Tokens
---

This page describes how to receive and send SOL tokens using the command line
tools with a command line wallet such as a [paper wallet](../wallets/paper.md),
a [file system wallet](../wallets/file-system.md), or a
[hardware wallet](../wallets/hardware/index.md). Before you begin, make sure
you have created a wallet and have access to its address (pubkey) and the
signing keypair. Check out our
[conventions for entering keypairs for different wallet types](../intro.md#keypair-conventions).

## Testing your Wallet

Before sharing your public key with others, you may want to first ensure the
key is valid and that you indeed hold the corresponding private key.

In this example, we will create a second wallet in addition to your first wallet,
and then transfer some tokens to it. This will confirm that you can send and
receive tokens on your wallet type of choice.

This test example uses our Developer Testnet, called devnet. Tokens issued
on devnet have **no** value, so don't worry if you lose them.

#### Airdrop some tokens to get started

First, _airdrop_ yourself some play tokens on the devnet.

```bash
solana airdrop 1 <RECIPIENT_ACCOUNT_ADDRESS> --url https://api.devnet.solana.com
```

where you replace the text `<RECIPIENT_ACCOUNT_ADDRESS>` with your base58-encoded
public key/wallet address.

A response with the signature of the transaction will be returned. If the balance
of the address does not change by the expected amount, run the following command
for more information on what potentially went wrong:

```bash
solana confirm -v <TRANSACTION_SIGNATURE>
```

#### Check your balance

Confirm the airdrop was successful by checking the account's balance.
It should output `1 SOL`:

```bash
solana balance <ACCOUNT_ADDRESS> --url https://api.devnet.solana.com
```

#### Create a second wallet address

We will need a new address to receive our tokens. Create a second
keypair and record its pubkey:

```bash
solana-keygen new --no-passphrase --no-outfile
```

The output will contain the address after the text `pubkey:`. Copy the
address. We will use it in the next step.

```text
pubkey: GKvqsuNcnwWqPzzuhLmGi4rzzh55FhJtGizkhHaEJqiV
```

You can also create a second (or more) wallet of any type:
[paper](../wallets/paper.md#creating-multiple-paper-wallet-addresses),
[file system](../wallets/file-system.md#creating-multiple-file-system-wallet-addresses),
or [hardware](../wallets/hardware/index.md#multiple-addresses-on-a-single-hardware-wallet).

#### Transfer tokens from your first wallet to the second address

Next, prove that you own the airdropped tokens by transferring them.
The Solana cluster will only accept the transfer if you sign the transaction
with the private keypair corresponding to the sender's public key in the
transaction.

```bash
solana transfer --from <KEYPAIR> <RECIPIENT_ACCOUNT_ADDRESS> 0.5 --allow-unfunded-recipient --url https://api.devnet.solana.com --fee-payer <KEYPAIR>
```

where you replace `<KEYPAIR>` with the path to a keypair in your first wallet,
and replace `<RECIPIENT_ACCOUNT_ADDRESS>` with the address of your second
wallet.

Confirm the updated balances with `solana balance`:

```bash
solana balance <ACCOUNT_ADDRESS> --url http://api.devnet.solana.com
```

where `<ACCOUNT_ADDRESS>` is either the public key from your keypair or the
recipient's public key.

#### Full example of test transfer

```bash
$ solana-keygen new --outfile my_solana_wallet.json   # Creating my first wallet, a file system wallet
Generating a new keypair
For added security, enter a passphrase (empty for no passphrase):
Wrote new keypair to my_solana_wallet.json
==========================================================================
pubkey: DYw8jCTfwHNRJhhmFcbXvVDTqWMEVFBX6ZKUmG5CNSKK                          # Here is the address of the first wallet
==========================================================================
Save this seed phrase to recover your new keypair:
width enhance concert vacant ketchup eternal spy craft spy guard tag punch    # If this was a real wallet, never share these words on the internet like this!
==========================================================================

$ solana airdrop 1 DYw8jCTfwHNRJhhmFcbXvVDTqWMEVFBX6ZKUmG5CNSKK --url https://api.devnet.solana.com  # Airdropping 1 SOL to my wallet's address/pubkey
Requesting airdrop of 1 SOL from 35.233.193.70:9900
1 SOL

$ solana balance DYw8jCTfwHNRJhhmFcbXvVDTqWMEVFBX6ZKUmG5CNSKK --url https://api.devnet.solana.com # Check the address's balance
1 SOL

$ solana-keygen new --no-outfile  # Creating a second wallet, a paper wallet
Generating a new keypair
For added security, enter a passphrase (empty for no passphrase):
====================================================================
pubkey: 7S3P4HxJpyyigGzodYwHtCxZyUQe9JiBMHyRWXArAaKv                   # Here is the address of the second, paper, wallet.
====================================================================
Save this seed phrase to recover your new keypair:
clump panic cousin hurt coast charge engage fall eager urge win love   # If this was a real wallet, never share these words on the internet like this!
====================================================================

$ solana transfer --from my_solana_wallet.json 7S3P4HxJpyyigGzodYwHtCxZyUQe9JiBMHyRWXArAaKv 0.5 --allow-unfunded-recipient --url https://api.devnet.solana.com --fee-payer my_solana_wallet.json  # Transferring tokens to the public address of the paper wallet
3gmXvykAd1nCQQ7MjosaHLf69Xyaqyq1qw2eu1mgPyYXd5G4v1rihhg1CiRw35b9fHzcftGKKEu4mbUeXY2pEX2z  # This is the transaction signature

$ solana balance DYw8jCTfwHNRJhhmFcbXvVDTqWMEVFBX6ZKUmG5CNSKK --url https://api.devnet.solana.com
0.499995 SOL  # The sending account has slightly less than 0.5 SOL remaining due to the 0.000005 SOL transaction fee payment

$ solana balance 7S3P4HxJpyyigGzodYwHtCxZyUQe9JiBMHyRWXArAaKv --url https://api.devnet.solana.com
0.5 SOL  # The second wallet has now received the 0.5 SOL transfer from the first wallet

```

## Receive Tokens

To receive tokens, you will need an address for others to send tokens to. In
Solana, the wallet address is the public key of a keypair. There are a variety
of techniques for generating keypairs. The method you choose will depend on how
you choose to store keypairs. Keypairs are stored in wallets. Before receiving
tokens, you will need to [create a wallet](../wallets/index.md).
Once completed, you should have a public key
for each keypair you generated. The public key is a long string of base58
characters. Its length varies from 32 to 44 characters.

## Send Tokens

If you already hold SOL and want to send tokens to someone, you will need
a path to your keypair, their base58-encoded public key, and a number of
tokens to transfer. Once you have that collected, you can transfer tokens
with the `solana transfer` command:

```bash
solana transfer --from <KEYPAIR> <RECIPIENT_ACCOUNT_ADDRESS> <AMOUNT> --fee-payer <KEYPAIR>
```

Confirm the updated balances with `solana balance`:

```bash
solana balance <ACCOUNT_ADDRESS>
```

================
File: docs/src/cli/wallets/hardware/_category_.json
================
{
  "position": 5,
  "label": "Hardware Wallets",
  "collapsible": false,
  "collapsed": false,
  "link": null
}

================
File: docs/src/cli/wallets/hardware/index.md
================
---
title: Using Hardware Wallets in the Solana CLI
pagination_label: "Using Hardware Wallets in the Solana CLI"
sidebar_label: Using in the Solana CLI
sidebar_position: 0
---

Signing a transaction requires a private key, but storing a private
key on your personal computer or phone leaves it subject to theft.
Adding a password to your key adds security, but many people prefer
to take it a step further and move their private keys to a separate
physical device called a _hardware wallet_. A hardware wallet is a
small handheld device that stores private keys and provides some
interface for signing transactions.

The Solana CLI has first class support for hardware wallets. Anywhere
you use a keypair filepath (denoted as `<KEYPAIR>` in usage docs), you
can pass a _keypair URL_ that uniquely identifies a keypair in a
hardware wallet.

## Supported Hardware Wallets

The Solana CLI supports the following hardware wallets:

- [Ledger Nano S and Ledger Nano X](./ledger.md)

## Specify a Keypair URL

Solana defines a keypair URL format to uniquely locate any Solana keypair on a
hardware wallet connected to your computer.

The keypair URL has the following form, where square brackets denote optional
fields:

```text
usb://<MANUFACTURER>[/<WALLET_ID>][?key=<DERIVATION_PATH>]
```

`WALLET_ID` is a globally unique key used to disambiguate multiple devices.

`DERVIATION_PATH` is used to navigate to Solana keys within your hardware wallet.
The path has the form `<ACCOUNT>[/<CHANGE>]`, where each `ACCOUNT` and `CHANGE`
are nonnegative integers.

For example, a fully qualified URL for a Ledger device might be:

```text
usb://ledger/BsNsvfXqQTtJnagwFWdBS7FBXgnsK8VZ5CmuznN85swK?key=0/0
```

All derivation paths implicitly include the prefix `44'/501'`, which indicates
the path follows the [BIP44 specifications](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki)
and that any derived keys are Solana keys (Coin type 501). The single quote
indicates a "hardened" derivation. Because Solana uses Ed25519 keypairs, all
derivations are hardened and therefore adding the quote is optional and
unnecessary.

================
File: docs/src/cli/wallets/hardware/ledger.md
================
---
title: Using Ledger Nano Hardware Wallets in the Solana CLI
pagination_label: "Hardware Wallets in the Solana CLI: Ledger Nano"
sidebar_label: Ledger Nano
---

This page describes how to use a Ledger Nano S, Nano S Plus, or Nano X to
interact with Solana using the command line tools.

## Before You Begin

- [Set up a Nano with the Solana App](https://support.ledger.com/hc/en-us/articles/360016265659-Solana-SOL-?docs=true)
- [Install the Solana command-line tools](../../install.md)

## Use Ledger Nano with Solana CLI

1. Ensure the Ledger Live application is closed
2. Plug your Nano into your computer's USB port
3. Enter your pin and start the Solana app on the Nano
4. Ensure the screen reads "Application is ready"

### View your Wallet ID

On your computer, run:

```bash
solana-keygen pubkey usb://ledger
```

This confirms your Ledger device is connected properly and in the correct state
to interact with the Solana CLI. The command returns your Ledger's unique
_wallet ID_. When you have multiple Nano devices connected to the same computer,
you can use your wallet ID to specify which Ledger hardware wallet you want to
use. If you only plan to use a single Nano on your computer at a time, you don't
need to include the wallet ID. For information on using the wallet ID to use a
specific Ledger, see
[Manage Multiple Hardware Wallets](#manage-multiple-hardware-wallets).

### View your Wallet Addresses

Your Nano supports an arbitrary number of valid wallet addresses and signers. To
view any address, use the `solana-keygen pubkey` command, as shown below,
followed by a valid [keypair URL](./index.md#specify-a-keypair-url).

Multiple wallet addresses can be useful if you want to transfer tokens between
your own accounts for different purposes, or use different keypairs on the
device as signing authorities for a stake account, for example.

All of the following commands will display different addresses, associated with
the keypair path given. Try them out!

```bash
solana-keygen pubkey usb://ledger
solana-keygen pubkey usb://ledger?key=0
solana-keygen pubkey usb://ledger?key=1
solana-keygen pubkey usb://ledger?key=2
```

- NOTE: keypair url parameters are ignored in **zsh**
  &nbsp;[see troubleshooting for more info](#troubleshooting)

You can use other values for the number after `key=` as well. Any of the
addresses displayed by these commands are valid Solana wallet addresses. The
private portion associated with each address is stored securely on the Nano, and
is used to sign transactions from this address. Just make a note of which
keypair URL you used to derive any address you will be using to receive tokens.

If you are only planning to use a single address/keypair on your device, a good
easy-to-remember path might be to use the address at `key=0`. View this address
with:

```bash
solana-keygen pubkey usb://ledger?key=0
```

Now you have a wallet address (or multiple addresses), you can share any of
these addresses publicly to act as a receiving address, and you can use the
associated keypair URL as the signer for transactions from that address.

### View your Balance

To view the balance of any account, regardless of which wallet it uses, use the
`solana balance` command:

```bash
solana balance SOME_WALLET_ADDRESS
```

For example, if your address is `7cvkjYAkUYs4W8XcXsca7cBrEGFeSUjeZmKoNBvEwyri`,
then enter the following command to view the balance:

```bash
solana balance 7cvkjYAkUYs4W8XcXsca7cBrEGFeSUjeZmKoNBvEwyri
```

You can also view the balance of any account address on the Accounts tab in the
[Explorer](https://explorer.solana.com/accounts) and paste the address in the
box to view the balance in your web browser.

Note: Any address with a balance of 0 SOL, such as a newly created one on your
Ledger, will show as "Not Found" in the explorer. Empty accounts and
non-existent accounts are treated the same in Solana. This will change when your
account address has some SOL in it.

### Send SOL from a Nano

To send some tokens from an address controlled by your Nano, you will need to
use the device to sign a transaction, using the same keypair URL you used to
derive the address. To do this, make sure your Nano is plugged in, unlocked with
the PIN, Ledger Live is not running, and the Solana App is open on the device,
showing "Application is Ready".

The `solana transfer` command is used to specify to which address to send
tokens, how many tokens to send, and uses the `--keypair` argument to specify
which keypair is sending the tokens, which will sign the transaction, and the
balance from the associated address will decrease.

```bash
solana transfer RECIPIENT_ADDRESS AMOUNT --keypair KEYPAIR_URL_OF_SENDER
```

Below is a full example. First, an address is viewed at a certain keypair URL.
Second, the balance of that address is checked. Lastly, a transfer transaction
is entered to send `1` SOL to the recipient address
`7cvkjYAkUYs4W8XcXsca7cBrEGFeSUjeZmKoNBvEwyri`. When you hit Enter for a
transfer command, you will be prompted to approve the transaction details on
your Ledger device. On the device, use the right and left buttons to review the
transaction details. If they look correct, click both buttons on the "Approve"
screen, otherwise push both buttons on the "Reject" screen.

```bash
~$ solana-keygen pubkey usb://ledger?key=42
CjeqzArkZt6xwdnZ9NZSf8D1CNJN1rjeFiyd8q7iLWAV

~$ solana balance CjeqzArkZt6xwdnZ9NZSf8D1CNJN1rjeFiyd8q7iLWAV
1.000005 SOL

~$ solana transfer 7cvkjYAkUYs4W8XcXsca7cBrEGFeSUjeZmKoNBvEwyri 1 --keypair usb://ledger?key=42
Waiting for your approval on Ledger hardware wallet usb://ledger/2JT2Xvy6T8hSmT8g6WdeDbHUgoeGdj6bE2VueCZUJmyN
✅ Approved

Signature: kemu9jDEuPirKNRKiHan7ycybYsZp7pFefAdvWZRq5VRHCLgXTXaFVw3pfh87MQcWX4kQY4TjSBmESrwMApom1V
```

After approving the transaction on your device, the program will display the
transaction signature, and wait for the maximum number of confirmations (32)
before returning. This only takes a few seconds, and then the transaction is
finalized on the Solana network. You can view details of this or any other
transaction by going to the Transaction tab in the
[Explorer](https://explorer.solana.com/transactions) and paste in the
transaction signature.

## Advanced Operations

### Manage Multiple Hardware Wallets

It is sometimes useful to sign a transaction with keys from multiple hardware
wallets. Signing with multiple wallets requires _fully qualified keypair URLs_.
When the URL is not fully qualified, the Solana CLI will prompt you with the
fully qualified URLs of all connected hardware wallets, and ask you to choose
which wallet to use for each signature.

Instead of using the interactive prompts, you can generate fully qualified URLs
using the Solana CLI `resolve-signer` command. For example, try connecting a
Nano to USB, unlock it with your pin, and running the following command:

```text
solana resolve-signer usb://ledger?key=0/0
```

You will see output similar to:

```text
usb://ledger/BsNsvfXqQTtJnagwFWdBS7FBXgnsK8VZ5CmuznN85swK?key=0/0
```

but where `BsNsvfXqQTtJnagwFWdBS7FBXgnsK8VZ5CmuznN85swK` is your `WALLET_ID`.

With your fully qualified URL, you can connect multiple hardware wallets to the
same computer and uniquely identify a keypair from any of them. Use the output
from the `resolve-signer` command anywhere a `solana` command expects a
`<KEYPAIR>` entry to use that resolved path as the signer for that part of the
given transaction.

## Troubleshooting

### Keypair URL parameters are ignored in zsh

The question mark character is a special character in zsh. If that's not a
feature you use, add the following line to your `~/.zshrc` to treat it as a
normal character:

```bash
unsetopt nomatch
```

Then either restart your shell window or run `~/.zshrc`:

```bash
source ~/.zshrc
```

If you would prefer not to disable zsh's special handling of the question mark
character, you can disable it explicitly with a backslash in your keypair URLs.
For example:

```bash
solana-keygen pubkey usb://ledger\?key=0
```

## Support

You can find additional support and get help on the
[Solana StackExchange](https://solana.stackexchange.com).

Read more about [sending and receiving tokens](../../examples/transfer-tokens.md) and
[delegating stake](../../examples/delegate-stake.md). You can use your Ledger keypair
URL anywhere you see an option or argument that accepts a `<KEYPAIR>`.

================
File: docs/src/cli/wallets/_category_.json
================
{
  "position": 3.5,
  "label": "Command-line Wallets",
  "collapsible": true,
  "collapsed": false,
  "link": null
}

================
File: docs/src/cli/wallets/file-system.md
================
---
title: File System Wallets using the CLI
pagination_label: File System Wallets using the CLI
sidebar_label: File System Wallets
sidebar_position: 2
---

This document describes how to create and use a file system wallet with the
Solana CLI tools. A file system wallet exists as an unencrypted keypair file
on your computer system's filesystem.

> File system wallets are the **least secure** method of storing SOL tokens. Storing large amounts of tokens in a file system wallet is **not recommended**.

## Before you Begin

Make sure you have
[installed the Solana Command Line Tools](../install.md)

## Generate a File System Wallet Keypair

Use Solana's command-line tool `solana-keygen` to generate keypair files. For
example, run the following from a command-line shell:

```bash
mkdir ~/my-solana-wallet
solana-keygen new --outfile ~/my-solana-wallet/my-keypair.json
```

This file contains your **unencrypted** keypair. In fact, even if you specify
a password, that password applies to the recovery seed phrase, not the file. Do
not share this file with others. Anyone with access to this file will have access
to all tokens sent to its public key. Instead, you should share only its public
key. To display its public key, run:

```bash
solana-keygen pubkey ~/my-solana-wallet/my-keypair.json
```

It will output a string of characters, such as:

```text
ErRr1caKzK8L8nn4xmEWtimYRiTCAZXjBtVphuZ5vMKy
```

This is the public key corresponding to the keypair in
`~/my-solana-wallet/my-keypair.json`. The public key of the keypair file is
your _wallet address_.

## Verify your Address against your Keypair file

To verify you hold the private key for a given address, use
`solana-keygen verify`:

```bash
solana-keygen verify <PUBKEY> ~/my-solana-wallet/my-keypair.json
```

where `<PUBKEY>` is replaced with your wallet address.
The command will output "Success" if the given address matches the
one in your keypair file, and "Failed" otherwise.

## Creating Multiple File System Wallet Addresses

You can create as many wallet addresses as you like. Simply re-run the
steps in [Generate a File System Wallet](#generate-a-file-system-wallet-keypair)
and make sure to use a new filename or path with the `--outfile` argument.
Multiple wallet addresses can be useful if you want to transfer tokens between
your own accounts for different purposes.

================
File: docs/src/cli/wallets/index.md
================
---
title: Solana Wallets with the CLI
pagination_label: Command Line Wallets
sidebar_label: Overview
sidebar_position: 0
---

Solana supports several different types of wallets that can be used to interface
directly with the Solana command-line tools.

To use a Command Line Wallet, you must first [install the Solana CLI tools](../install.md)

## File System Wallet

A _file system wallet_, aka an FS wallet, is a directory in your computer's
file system. Each file in the directory holds a keypair.

### File System Wallet Security

A file system wallet is the most convenient and least secure form of wallet. It
is convenient because the keypair is stored in a simple file. You can generate as
many keys as you would like and trivially back them up by copying the files. It
is insecure because the keypair files are **unencrypted**. If you are the only
user of your computer and you are confident it is free of malware, an FS wallet
is a fine solution for small amounts of cryptocurrency. If, however, your
computer contains malware and is connected to the Internet, that malware may
upload your keys and use them to take your tokens. Likewise, because the
keypairs are stored on your computer as files, a skilled hacker with physical
access to your computer may be able to access it. Using an encrypted hard
drive, such as FileVault on MacOS, minimizes that risk.

See [File System Wallets](./file-system.md) for more details.

## Paper Wallet

A _paper wallet_ is a collection of _seed phrases_ written on paper. A seed
phrase is some number of words (typically 12 or 24) that can be used to
regenerate a keypair on demand.

### Paper Wallet Security

In terms of convenience versus security, a paper wallet sits at the opposite
side of the spectrum from an FS wallet. It is terribly inconvenient to use, but
offers excellent security. That high security is further amplified when paper
wallets are used in conjunction with [offline signing](../examples/offline-signing.md).

See [Paper Wallets](./paper.md) for more details

## Hardware Wallet

A hardware wallet is a small handheld device that stores keypairs and provides
some interface for signing transactions.

### Hardware Wallet Security

A hardware wallet, such as the
[Ledger hardware wallet](https://www.ledger.com/), offers a great blend of
security and convenience for cryptocurrencies. It effectively automates the
process of offline signing while retaining nearly all the convenience of a file
system wallet.

See [Hardware Wallets](./hardware/index.md) for more details

================
File: docs/src/cli/wallets/paper.md
================
---
title: Paper Wallets using the Solana CLI
pagination_label: Paper Wallets using the CLI
sidebar_label: Paper Wallets
sidebar_position: 1
---

This document describes how to create and use a paper wallet with the Solana CLI
tools.

> We do not intend to advise on how to _securely_ create or manage paper
> wallets. Please research the security concerns carefully.

## Overview

Solana provides a key generation tool to derive keys from
[BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki)-compliant
seed phrases. Solana CLI commands for running a validator and staking tokens all
support keypair input via seed phrases.

## Paper Wallet Usage

Solana commands can be run without ever saving a keypair to disk on a machine.
If avoiding writing a private key to disk is a security concern of yours, you've
come to the right place.

> Even using this secure input method, it's still possible that a private key
> gets written to disk by unencrypted memory swaps. It is the user's
> responsibility to protect against this scenario.

## Before You Begin

- [Install the Solana command-line tools](../install.md)

### Check your installation

Check that `solana-keygen` is installed correctly by running:

```bash
solana-keygen --version
```

## Creating a Paper Wallet

Using the `solana-keygen` tool, it is possible to generate new seed phrases as
well as derive a keypair from an existing seed phrase and (optional) passphrase.
The seed phrase and passphrase can be used together as a paper wallet. As long
as you keep your seed phrase and passphrase stored safely, you can use them to
access your account.

> For more information about how seed phrases work, review this
> [Bitcoin Wiki page](https://en.bitcoin.it/wiki/Seed_phrase).

### Seed Phrase Generation

Generating a new keypair can be done using the `solana-keygen new` command. The
command will generate a random seed phrase, ask you to enter an optional
passphrase, and then will display the derived public key and the generated seed
phrase for your paper wallet.

After copying down your seed phrase, you can use the
[public key derivation](#public-key-derivation) instructions to verify that you
have not made any errors.

```bash
solana-keygen new --no-outfile
```

> If the `--no-outfile` flag is **omitted**, the default behavior is to write
> the keypair to `~/.config/solana/id.json`, resulting in a
> [file system wallet](./file-system.md).

The output of this command will display a line like this:

```bash
pubkey: 9ZNTfG4NyQgxy2SWjSiQoUyBPEvXT2xo7fKc5hPYYJ7b
```

The value shown after `pubkey:` is your _wallet address_.

**Note:** In working with paper wallets and file system wallets, the terms
"pubkey" and "wallet address" are sometimes used interchangeably.

> For added security, increase the seed phrase word count using the
> `--word-count` argument

For full usage details, run:

```bash
solana-keygen new --help
```

### Public Key Derivation

Public keys can be derived from a seed phrase and a passphrase if you choose to
use one. This is useful for using an offline-generated seed phrase to derive a
valid public key. The `solana-keygen pubkey` command will walk you through how
to use your seed phrase (and a passphrase if you chose to use one) as a signer
with the solana command-line tools using the `prompt` URI scheme.

```bash
solana-keygen pubkey prompt://
```

> Note that you could potentially use different passphrases for the same seed
> phrase. Each unique passphrase will yield a different keypair.

The `solana-keygen` tool uses the same BIP39 standard English word list as it
does to generate seed phrases. If your seed phrase was generated with another
tool that uses a different word list, you can still use `solana-keygen`, but
will need to pass the `--skip-seed-phrase-validation` argument and forego this
validation.

```bash
solana-keygen pubkey prompt:// --skip-seed-phrase-validation
```

After entering your seed phrase with `solana-keygen pubkey prompt://` the
console will display a string of base-58 characters. This is the
[derived](#hierarchical-derivation) solana BIP44 _wallet address_ associated
with your seed phrase.

> Copy the derived address to a USB stick for easy usage on networked computers

If needed, you can access the legacy, raw keypair's pubkey by instead passing
the `ASK` keyword:

```bash
solana-keygen pubkey ASK
```

> A common next step is to [check the balance](#checking-account-balance) of the
> account associated with a public key

For full usage details, run:

```bash
solana-keygen pubkey --help
```

### Hierarchical Derivation

The solana-cli supports
[BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki) and
[BIP44](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki)
hierarchical derivation of private keys from your seed phrase and passphrase by
adding either the `?key=` query string or the `?full-path=` query string.

By default, `prompt:` will derive solana's base derivation path `m/44'/501'`. To
derive a child key, supply the `?key=<ACCOUNT>/<CHANGE>` query string.

```bash
solana-keygen pubkey 'prompt://?key=0/1'
```

To use a derivation path other than solana's standard BIP44, you can supply
`?full-path=m/<PURPOSE>/<COIN_TYPE>/<ACCOUNT>/<CHANGE>`.

```bash
solana-keygen pubkey 'prompt://?full-path=m/44/2017/0/1'
```

Because Solana uses Ed25519 keypairs, as per
[SLIP-0010](https://github.com/satoshilabs/slips/blob/master/slip-0010.md) all
derivation-path indexes will be promoted to hardened indexes -- eg.
`?key=0'/0'`, `?full-path=m/44'/2017'/0'/1'` -- regardless of whether ticks are
included in the query-string input.

## Verifying the Keypair

To verify you control the private key of a paper wallet address, use
`solana-keygen verify`:

```bash
solana-keygen verify <PUBKEY> prompt://
```

where `<PUBKEY>` is replaced with the wallet address and the keyword `prompt://`
tells the command to prompt you for the keypair's seed phrase; `key` and
`full-path` query-strings accepted. Note that for security reasons, your seed
phrase will not be displayed as you type. After entering your seed phrase, the
command will output "Success" if the given public key matches the keypair
generated from your seed phrase, and "Failed" otherwise.

## Checking Account Balance

All that is needed to check an account balance is the public key of an account.
To retrieve public keys securely from a paper wallet, follow the
[Public Key Derivation](#public-key-derivation) instructions on an
[air gapped computer](<https://en.wikipedia.org/wiki/Air_gap_(networking)>).
Public keys can then be typed manually or transferred via a USB stick to a
networked machine.

Next, configure the `solana` CLI tool to
[connect to a particular cluster](../examples/choose-a-cluster.md):

```bash
solana config set --url <CLUSTER URL> # (i.e. https://api.mainnet-beta.solana.com)
```

Finally, to check the balance, run the following command:

```bash
solana balance <PUBKEY>
```

## Creating Multiple Paper Wallet Addresses

You can create as many wallet addresses as you like. Simply re-run the steps in
[Seed Phrase Generation](#seed-phrase-generation) or
[Public Key Derivation](#public-key-derivation) to create a new address.
Multiple wallet addresses can be useful if you want to transfer tokens between
your own accounts for different purposes.

## Support

You can find additional support and get help on the
[Solana StackExchange](https://solana.stackexchange.com).

================
File: docs/src/cli/.usage.md.header
================
---
title: Solana CLI Reference and Usage
pagination_label: Solana CLI Reference and Usage
sidebar_label: Reference & Usage
sidebar_position: 3
---

The [solana-cli crate](https://crates.io/crates/solana-cli) provides a command-line interface tool for Solana

## Examples

### Get Pubkey

```bash
// Command
$ solana-keygen pubkey

// Return
<PUBKEY>
```

### Airdrop SOL/Lamports

```bash
// Command
$ solana airdrop 1

// Return
"1 SOL"
```

### Get Balance

```bash
// Command
$ solana balance

// Return
"3.00050001 SOL"
```

### Confirm Transaction

```bash
// Command
$ solana confirm <TX_SIGNATURE>

// Return
"Confirmed" / "Not found" / "Transaction failed with error <ERR>"
```

### Deploy program

```bash
// Command
$ solana program deploy <PATH>

// Return
<PROGRAM_ID>
```

## Usage

================
File: docs/src/cli/index.md
================
---
title: Solana CLI Tool Suite
sidebar_position: 0
sidebar_label: Overview
pagination_label: Solana CLI Tool Suite
---

In this section, we will describe how to use the Solana command-line tools to
create a _wallet_, to send and receive SOL tokens, and to participate in the
cluster by delegating stake.

To interact with a Solana cluster, we will use its command-line interface, also
known as the CLI. We use the command-line because it is the first place the
Anza core team deploys new functionality. The command-line interface is not
necessarily the easiest to use, but it provides the most direct, flexible, and
secure access to your Solana accounts.

## Getting Started

To get started using the Solana Command Line (CLI) tools:

- [Install the Solana CLI Tool Suite](./install.md)
- [Introduction to our CLI conventions](./intro.md)
- [Create a Wallet using the CLI](./wallets/index.md)
- [Choose a Cluster to connect to using the CLI](./examples/choose-a-cluster.md)

================
File: docs/src/cli/install.md
================
---
title: Install the Solana CLI
pagination_label: Install the Solana CLI
sidebar_label: Installation
sidebar_position: 1
---

There are multiple ways to install the Solana tools on your computer depending
on your preferred workflow:

- [Use the Solana Install Tool (Simplest option)](#use-solanas-install-tool)
- [Download Prebuilt Binaries](#download-prebuilt-binaries)
- [Build from Source](#build-from-source)
- [Use Homebrew](#use-homebrew)

## Use The Solana Install Tool

### MacOS & Linux

- Open your favorite Terminal application

- Install the Agave release
  [LATEST_AGAVE_RELEASE_VERSION](https://github.com/jito-foundation/jito-solana/releases/tag/LATEST_AGAVE_RELEASE_VERSION)
  on your machine by running:

```bash
sh -c "$(curl -sSfL https://release.jito.wtf/LATEST_AGAVE_RELEASE_VERSION/install)"
```

- You can replace `LATEST_AGAVE_RELEASE_VERSION` with the release tag matching
  the software version of your desired release, or use one of the three symbolic
  channel names: `stable`, `beta`, or `edge`.

- The following output indicates a successful update:

```text
downloading LATEST_AGAVE_RELEASE_VERSION installer
Configuration: /home/solana/.config/solana/install/config.yml
Active release directory: /home/solana/.local/share/solana/install/active_release
* Release version: LATEST_AGAVE_RELEASE_VERSION
* Release URL: https://github.com/jito-foundation/jito-solana/releases/download/LATEST_AGAVE_RELEASE_VERSION/solana-release-x86_64-unknown-linux-gnu.tar.bz2
Update successful
```

- Depending on your system, the end of the installer messaging may prompt you to

```bash
Please update your PATH environment variable to include the solana programs:
```

- If you get the above message, copy and paste the recommended command below it
  to update `PATH`
- Confirm you have the desired version of `solana` installed by running:

```bash
solana --version
```

- After a successful install, `agave-install update` may be used to easily
  update the Solana software to a newer version at any time.

---

### Windows

- Open a Command Prompt (`cmd.exe`) as an Administrator

- Search for Command Prompt in the Windows search bar. When the Command Prompt
  app appears, right-click and select “Open as Administrator”. If you are
  prompted by a pop-up window asking “Do you want to allow this app to make
  changes to your device?”, click Yes.

- Copy and paste the following command, then press Enter to download the Solana
  installer into a temporary directory:

```bash
cmd /c "curl https://release.jito.wtf/LATEST_AGAVE_RELEASE_VERSION/agave-install-init-x86_64-pc-windows-msvc.exe --output C:\agave-install-tmp\agave-install-init.exe --create-dirs"
```

- Copy and paste the following command, then press Enter to install the latest
  version of Solana. If you see a security pop-up by your system, please select
  to allow the program to run.

```bash
C:\agave-install-tmp\agave-install-init.exe LATEST_AGAVE_RELEASE_VERSION
```

- When the installer is finished, press Enter.

- Close the command prompt window and re-open a new command prompt window as a
  normal user
- Search for "Command Prompt" in the search bar, then left click on the
  Command Prompt app icon, no need to run as Administrator)
- Confirm you have the desired version of `solana` installed by entering:

```bash
solana --version
```

- After a successful install, `agave-install update` may be used to easily
  update the Solana software to a newer version at any time.

## Download Prebuilt Binaries

If you would rather not use `agave-install` to manage the install, you can
manually download and install the binaries.

### Linux

Download the binaries by navigating to
[https://github.com/jito-foundation/jito-solana/releases/latest](https://github.com/jito-foundation/jito-solana/releases/latest),

```bash
tar jxf solana-release-x86_64-unknown-linux-gnu.tar.bz2
cd solana-release/
export PATH=$PWD/bin:$PATH
```

### MacOS

Download the binaries by navigating to
[https://github.com/jito-foundation/jito-solana/releases/latest](https://github.com/jito-foundation/jito-solana/releases/latest),

```bash
tar jxf solana-release-x86_64-apple-darwin.tar.bz2
cd solana-release/
export PATH=$PWD/bin:$PATH
```

### Windows

- Download the binaries by navigating to
  [https://github.com/jito-foundation/jito-solana/releases/latest](https://github.com/jito-foundation/jito-solana/releases/latest),
- Open a Command Prompt and navigate to the directory into which you extracted
  the binaries and run:

```bash
cd solana-release/
set PATH=%cd%/bin;%PATH%
```

## Build From Source

If you are unable to use the prebuilt binaries or prefer to build it yourself
from source, follow these steps, ensuring you have the necessary prerequisites
installed on your system.

### Prerequisites

Before building from source, make sure to install the following prerequisites:

#### Rust

For all platforms, check "Install Rust" at
[https://www.rust-lang.org/tools/install](https://www.rust-lang.org/tools/install)
for the latest installation instructions.

#### For Debian and Other Linux Distributions:

Install build dependencies:

- Build essential
- Package config
- Udev & LLM & libclang
- Protocol buffers

```bash
apt-get install \
    build-essential \
    pkg-config \
    libudev-dev llvm libclang-dev \
    protobuf-compiler
```

#### For Other Linux Distributions:

Replace `apt` with your distribution's package manager (e.g., `yum`, `dnf`,
`pacman`) and adjust package names as needed.

#### For macOS:

Check "Install Homebrew" at [https://brew.sh/](https://brew.sh/) for the latest
installation instruction for Homebrew if not already installed.

Then, install build dependencies with `brew`:

```bash
brew install pkg-config libudev protobuf llvm coreutils
```

Follow the instructions given at the end of the brew install command about
`PATH` configurations.

#### For Windows:

- Download and install the Build Tools for Visual Studio (2019 or later) from
  the
  [Visual Studio downloads page](https://visualstudio.microsoft.com/downloads/).
  Make sure to include the C++ build tools in the installation.
- Install LLVM: Download and install LLVM from the
  [official LLVM download page](https://releases.llvm.org/download.html).
- Install Protocol Buffers Compiler (protoc): Download `protoc` from the
  [GitHub releases page of Protocol Buffers](https://github.com/protocolbuffers/protobuf/releases),
  and add it to your `PATH`.

:::info

Users on Windows 10 or 11 may need to install
[Windows Subsystem for Linux](https://learn.microsoft.com/en-us/windows/wsl/install)
(WSL) in order to be able to build from source. WSL provides a Linux environment
that runs inside your existing Windows installation. You can then run regular
Linux software, including the Linux versions of Solana CLI.

After installed, run `wsl` from your Windows terminal, then continue through the
[Debian and Other Linux Distributions](#for-debian-and-other-linux-distributions)
above.

:::

### Building from Source

After installing the prerequisites, proceed with building Solana from source,
navigate to the
[Solana's GitHub releases page](https://github.com/jito-foundation/jito-solana/releases/latest),
and download the **Source Code** archive. Extract the code and build the
binaries with:

```bash
./scripts/cargo-install-all.sh .
export PATH=$PWD/bin:$PATH
```

## Use Homebrew

This option requires you to have [Homebrew](https://brew.sh/) package manager on
your MacOS or Linux machine.

### MacOS & Linux

- Follow instructions at: https://formulae.brew.sh/formula/solana

[Homebrew formulae](https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/s/solana.rb)
is updated after each `solana` release, however it is possible that the Homebrew
version is outdated.

- Confirm you have the desired version of `solana` installed by entering:

```bash
solana --version
```

================
File: docs/src/cli/intro.md
================
---
title: Introduction to the Solana CLI
pagination_label: Introduction to the Solana CLI
sidebar_label: Introduction
sidebar_position: 2
---

Before running any Solana CLI commands, let's go over some conventions that
you will see across all commands. First, the Solana CLI is actually a collection
of different commands for each action you might want to take. You can view the list
of all possible commands by running:

```bash
solana --help
```

To zoom in on how to use a particular command, run:

```bash
solana <COMMAND> --help
```

where you replace the text `<COMMAND>` with the name of the command you want
to learn more about.

The command's usage message will typically contain words such as `<AMOUNT>`,
`<ACCOUNT_ADDRESS>` or `<KEYPAIR>`. Each word is a placeholder for the _type_ of
text you can execute the command with. For example, you can replace `<AMOUNT>`
with a number such as `42` or `100.42`. You can replace `<ACCOUNT_ADDRESS>` with
the base58 encoding of your public key, such as
`9grmKMwTiZwUHSExjtbFzHLPTdWoXgcg1bZkhvwTrTww`.

## Keypair conventions

Many commands using the CLI tools require a value for a `<KEYPAIR>`. The value
you should use for the keypair depends on what type of
[command line wallet you created](./wallets/index.md).

For example, the CLI help shows that the way to display any wallet's address
(also known as the keypair's pubkey), is:

```bash
solana-keygen pubkey <KEYPAIR>
```

Below, we show how to resolve what you should put in `<KEYPAIR>` depending
on your wallet type.

## Paper Wallet

In a paper wallet, the keypair is securely derived from the seed words and
optional passphrase you entered when the wallet was created. To use a paper
wallet keypair anywhere the `<KEYPAIR>` text is shown in examples or help
documents, enter the uri scheme `prompt://` and the program will prompt you to
enter your seed words when you run the command.

To display the wallet address of a Paper Wallet:

```bash
solana-keygen pubkey prompt://
```

## File System Wallet

With a file system wallet, the keypair is stored in a file on your computer.
Replace `<KEYPAIR>` with the complete file path to the keypair file.

For example, if the file system keypair file location is
`/home/solana/my_wallet.json`, to display the address, do:

```bash
solana-keygen pubkey /home/solana/my_wallet.json
```

## Hardware Wallet

If you chose a hardware wallet, use your
[keypair URL](./wallets/hardware/index.md#specify-a-hardware-wallet-key),
such as `usb://ledger?key=0`.

```bash
solana-keygen pubkey usb://ledger?key=0
```

================
File: docs/src/clusters/available.md
================
---
title: Available Solana Clusters
sidebar_label: Solana Clusters
pagination_label: Available Solana Clusters
---

Solana maintains several different clusters with different purposes.

Before you begin, make sure you have first built the Solana command line tools.
See [Installing the Solana Command Line Tools](../cli/install.md#build-from-source).

Explorers:

- [http://explorer.solana.com/](https://explorer.solana.com/).
- [http://solanabeach.io/](http://solanabeach.io/).

## Devnet

- Devnet serves as a playground for anyone who wants to take Solana for a
  test drive, as a user, token holder, app developer or validator.
- Application developers should target Devnet.
- Potential validators should first target Devnet.
- Key differences between Devnet and Mainnet Beta:
  - Devnet tokens are **not real**
  - Devnet includes a token faucet for airdrops for application testing
  - Devnet may be subject to ledger resets
  - Devnet typically runs the same software release branch version as Mainnet Beta,
    but may run a newer minor release version than Mainnet Beta.
- Gossip entrypoint for Devnet: `entrypoint.devnet.solana.com:8001`
- Metrics environment variable for Devnet:

```bash
export SOLANA_METRICS_CONFIG="host=https://metrics.solana.com:8086,db=devnet,u=scratch_writer,p=topsecret"
```

- RPC URL for Devnet: `https://api.devnet.solana.com`

##### Example `solana` command-line configuration

```bash
solana config set --url https://api.devnet.solana.com
```

##### Example `agave-validator` command-line

```bash
$ agave-validator \
    --identity validator-keypair.json \
    --vote-account vote-account-keypair.json \
    --known-validator dv1ZAGvdsz5hHLwWXsVnM94hWf1pjbKVau1QVkaMJ92 \
    --known-validator dv2eQHeP4RFrJZ6UeiZWoc3XTtmtZCUKxxCApCDcRNV \
    --known-validator dv4ACNkpYPcE3aKmYDqZm9G5EB3J4MRoeE7WNDRBVJB \
    --known-validator dv3qDFk1DTF36Z62bNvrCXe9sKATA6xvVy6A798xxAS \
    --only-known-rpc \
    --ledger ledger \
    --rpc-port 8899 \
    --dynamic-port-range 8000-8020 \
    --entrypoint entrypoint.devnet.solana.com:8001 \
    --entrypoint entrypoint2.devnet.solana.com:8001 \
    --entrypoint entrypoint3.devnet.solana.com:8001 \
    --entrypoint entrypoint4.devnet.solana.com:8001 \
    --entrypoint entrypoint5.devnet.solana.com:8001 \
    --expected-genesis-hash EtWTRABZaYq6iMfeYKouRu166VU2xqa1wcaWoxPkrZBG \
    --wal-recovery-mode skip_any_corrupted_record \
    --limit-ledger-size
```

The [`--known-validator`s](../operations/guides/validator-start.md#known-validators)
are operated by Anza

## Testnet

- Testnet is where the Solana core contributors stress test recent release features on a live
  cluster, particularly focused on network performance, stability and validator
  behavior.
- Testnet tokens are **not real**
- Testnet may be subject to ledger resets.
- Testnet includes a token faucet for airdrops for application testing
- Testnet typically runs a newer software release branch than both
  Devnet and Mainnet Beta
- Gossip entrypoint for Testnet: `entrypoint.testnet.solana.com:8001`
- Metrics environment variable for Testnet:

```bash
export SOLANA_METRICS_CONFIG="host=https://metrics.solana.com:8086,db=tds,u=testnet_write,p=c4fa841aa918bf8274e3e2a44d77568d9861b3ea"
```

- RPC URL for Testnet: `https://api.testnet.solana.com`

##### Example `solana` command-line configuration

```bash
solana config set --url https://api.testnet.solana.com
```

##### Example `agave-validator` command-line

```bash
$ agave-validator \
    --identity validator-keypair.json \
    --vote-account vote-account-keypair.json \
    --known-validator 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on \
    --known-validator dDzy5SR3AXdYWVqbDEkVFdvSPCtS9ihF5kJkHCtXoFs \
    --known-validator Ft5fbkqNa76vnsjYNwjDZUXoTWpP7VYm3mtsaQckQADN \
    --known-validator eoKpUABi59aT4rR9HGS3LcMecfut9x7zJyodWWP43YQ \
    --known-validator 9QxCLckBiJc783jnMvXZubK4wH86Eqqvashtrwvcsgkv \
    --only-known-rpc \
    --ledger ledger \
    --rpc-port 8899 \
    --dynamic-port-range 8000-8020 \
    --entrypoint entrypoint.testnet.solana.com:8001 \
    --entrypoint entrypoint2.testnet.solana.com:8001 \
    --entrypoint entrypoint3.testnet.solana.com:8001 \
    --expected-genesis-hash 4uhcVJyU9pJkvQyS88uRDiswHXSCkY3zQawwpjk2NsNY \
    --wal-recovery-mode skip_any_corrupted_record \
    --limit-ledger-size
```

The identities of the
[`--known-validator`s](../operations/guides/validator-start.md#known-validators) are:

- `5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on` - Anza
- `dDzy5SR3AXdYWVqbDEkVFdvSPCtS9ihF5kJkHCtXoFs` - MonkeDAO
- `Ft5fbkqNa76vnsjYNwjDZUXoTWpP7VYm3mtsaQckQADN` - Certus One
- `eoKpUABi59aT4rR9HGS3LcMecfut9x7zJyodWWP43YQ` - SerGo
- `9QxCLckBiJc783jnMvXZubK4wH86Eqqvashtrwvcsgkv` - Algo|Stake

## Mainnet Beta

A permissionless, persistent cluster for Solana users, builders, validators and token holders.

- Tokens that are issued on Mainnet Beta are **real** SOL
- Gossip entrypoint for Mainnet Beta: `entrypoint.mainnet-beta.solana.com:8001`
- Metrics environment variable for Mainnet Beta:

```bash
export SOLANA_METRICS_CONFIG="host=https://metrics.solana.com:8086,db=mainnet-beta,u=mainnet-beta_write,p=password"
```

- RPC URL for Mainnet Beta: `https://api.mainnet-beta.solana.com`

##### Example `solana` command-line configuration

```bash
solana config set --url https://api.mainnet-beta.solana.com
```

##### Example `agave-validator` command-line

```bash
$ agave-validator \
    --identity ~/validator-keypair.json \
    --vote-account ~/vote-account-keypair.json \
    --known-validator 7Np41oeYqPefeNQEHSv1UDhYrehxin3NStELsSKCT4K2 \
    --known-validator GdnSyH3YtwcxFvQrVVJMm1JhTS4QVX7MFsX56uJLUfiZ \
    --known-validator DE1bawNcRJB9rVm3buyMVfr8mBEoyyu73NBovf2oXJsJ \
    --known-validator CakcnaRDHka2gXyfbEd2d3xsvkJkqsLw2akB3zsN1D2S \
    --only-known-rpc \
    --ledger ledger \
    --rpc-port 8899 \
    --private-rpc \
    --dynamic-port-range 8000-8020 \
    --entrypoint entrypoint.mainnet-beta.solana.com:8001 \
    --entrypoint entrypoint2.mainnet-beta.solana.com:8001 \
    --entrypoint entrypoint3.mainnet-beta.solana.com:8001 \
    --entrypoint entrypoint4.mainnet-beta.solana.com:8001 \
    --entrypoint entrypoint5.mainnet-beta.solana.com:8001 \
    --expected-genesis-hash 5eykt4UsFv8P8NJdTREpY1vzqKqZKvdpKuc147dw2N9d \
    --wal-recovery-mode skip_any_corrupted_record \
    --limit-ledger-size
```

:::info
The above four [`--known-validator`s](../operations/guides/validator-start.md#known-validators)
are operated by Anza.
:::

================
File: docs/src/clusters/benchmark.md
================
---
title: Benchmark a Cluster
---

The Solana git repository contains all the scripts you might need to spin up your own local testnet. Depending on what
you're looking to achieve, you may want to run a different variation, as the full-fledged, performance-enhanced
multinode testnet is considerably more complex to set up than a Rust-only, singlenode testnode. If you are looking to
develop high-level features, such as experimenting with smart contracts, save yourself some setup headaches and stick to
the Rust-only singlenode demo. If you're doing performance optimization of the transaction pipeline, consider the
enhanced singlenode demo. If you're doing consensus work, you'll need at least a Rust-only multinode demo. If you want
to reproduce our TPS metrics, run the enhanced multinode demo.

For all four variations, you'd need the latest Rust toolchain and the Solana source code:

First, setup Rust, Cargo and system packages as described in the
Solana [README](https://github.com/jito-foundation/jito-solana#1-install-rustc-cargo-and-rustfmt)

Now checkout the code from github:

```bash
git clone https://github.com/jito-foundation/jito-solana.git
cd jito-solana
```

The demo code is sometimes broken between releases as we add new low-level features, so if this is your first time
running the demo, you'll improve your odds of success if you check out
the [latest release](https://github.com/solana-labs/solana/releases) before proceeding:

```bash
TAG=$(git describe --tags $(git rev-list --tags --max-count=1))
git checkout $TAG
```

### Configuration Setup

Ensure important programs such as the vote program are built before any nodes are started. Note that we are using the release build here for good performance.

```bash
cargo build --release
```

Then set the following environment variable to enforce release builds across all commands we are going to use later:

```bash
export CARGO_BUILD_PROFILE=release
```

If you want to profile the validator(s), you can use the `release-with-debug` build profile, which leverages the most of optimizations used in the `release` profile, while keeping the debug symbols.

```bash
cargo build --profile release-with-debug
export CARGO_BUILD_PROFILE=release-with-debug
```

If you want the debug build without optimizations, use just `cargo build` and do not set any environment variables mentioned above.

The network is initialized with a genesis ledger generated by running the following script.

```bash
./multinode-demo/setup.sh
```

### Faucet

In order for the validators and clients to work, we'll need to spin up a faucet to give out some test tokens. The faucet
delivers Milton Friedman-style "air drops" \(free tokens to requesting clients\) to be used in test transactions.

Start the faucet with:

```bash
./multinode-demo/faucet.sh
```

### Singlenode Testnet

Before you start a validator, make sure you know the IP address of the machine you want to be the bootstrap validator
for the demo, and make sure that udp ports 8000-10000 are open on all the machines you want to test with.

Now start the bootstrap validator in a separate shell:

```bash
./multinode-demo/bootstrap-validator.sh
```

Wait a few seconds for the server to initialize. It will print "leader ready..." when it's ready to receive
transactions. The leader will request some tokens from the faucet if it doesn't have any. The faucet does not need to be
running for subsequent leader starts.

### Multinode Testnet

To run a multinode testnet, after starting a leader node, spin up some additional validators in separate shells:

```bash
./multinode-demo/validator-x.sh
```

### Testnet Client Demo

Now that your singlenode or multinode testnet is up and running let's send it some transactions!

In a separate shell start the client:

```bash
./multinode-demo/bench-tps.sh # runs against localhost by default
```

What just happened? The client demo spins up several threads to send 500,000 transactions to the testnet as quickly as
it can. The client then pings the testnet periodically to see how many transactions it processed in that time. Take note
that the demo intentionally floods the network with UDP packets, such that the network will almost certainly drop a
bunch of them. This ensures the testnet has an opportunity to reach 710k TPS. The client demo completes after it has
convinced itself the testnet won't process any additional transactions. You should see several TPS measurements printed
to the screen. In the multinode variation, you'll see TPS measurements for each validator node as well.

### Testnet Debugging

There are some useful debug messages in the code, you can enable them on a per-module and per-level basis. Before
running a leader or validator set the normal RUST_LOG environment variable.

For example

- To enable `info` everywhere and `debug` only in the solana::banking_stage module:

  ```bash
  export RUST_LOG=solana=info,solana::banking_stage=debug
  ```

- To enable SBF program logging:

  ```bash
  export RUST_LOG=solana_bpf_loader=trace
  ```

Generally we are using `debug` for infrequent debug messages, `trace` for potentially frequent messages and `info` for
performance-related logging.

You can also attach to a running process with GDB. The leader's process is named _agave-validator_:

```bash
sudo gdb
attach <PID>
set logging on
thread apply all bt
```

This will dump all the threads stack traces into gdb.txt

## Developer Testnet

In this example the client connects to our public testnet. To run validators on the testnet you would need to open udp
ports `8000-10000`.

```bash
./multinode-demo/bench-tps.sh --entrypoint entrypoint.devnet.solana.com:8001 --faucet api.devnet.solana.com:9900 --duration 60 --tx_count 50
```

You can observe the effects of your client's transactions on
our [metrics dashboard](https://metrics.solana.com:3000/d/monitor/cluster-telemetry?var-testnet=devnet)

================
File: docs/src/clusters/index.md
================
---
title: Overview of a Solana Cluster
sidebar_position: 0
sidebar_label: Overview
pagination_label: Overview of a Solana Cluster
---

A Solana cluster is a set of validators working together to serve client transactions and maintain the integrity of the ledger. Many clusters may coexist. When two clusters share a common genesis block, they attempt to converge. Otherwise, they simply ignore the existence of the other. Transactions sent to the wrong one are quietly rejected. In this section, we'll discuss how a cluster is created, how nodes join the cluster, how they share the ledger, how they ensure the ledger is replicated, and how they cope with buggy and malicious nodes.

## Creating a Cluster

Before starting any validators, one first needs to create a _genesis config_. The config references two public keys, a _mint_ and a _bootstrap validator_. The validator holding the bootstrap validator's private key is responsible for appending the first entries to the ledger. It initializes its internal state with the mint's account. That account will hold the number of native tokens defined by the genesis config. The second validator then contacts the bootstrap validator to register as a _validator_. Additional validators then register with any registered member of the cluster.

A validator receives all entries from the leader and submits votes confirming those entries are valid. After voting, the validator is expected to store those entries. Once the validator observes a sufficient number of copies exist, it deletes its copy.

## Joining a Cluster

Validators enter the cluster via registration messages sent to its _control plane_. The control plane is implemented using a _gossip_ protocol, meaning that a node may register with any existing node, and expect its registration to propagate to all nodes in the cluster. The time it takes for all nodes to synchronize is proportional to the square of the number of nodes participating in the cluster. Algorithmically, that's considered very slow, but in exchange for that time, a node is assured that it eventually has all the same information as every other node, and that information cannot be censored by any one node.

## Sending Transactions to a Cluster

Clients send transactions to any validator's Transaction Processing Unit \(TPU\) port. If the node is in the validator role, it forwards the transaction to the designated leader. If in the leader role, the node bundles incoming transactions, timestamps them creating an _entry_, and pushes them onto the cluster's _data plane_. Once on the data plane, the transactions are validated by validator nodes, effectively appending them to the ledger.

## Confirming Transactions

A Solana cluster is capable of subsecond _confirmation_ for thousands of nodes with plans to scale up to hundreds of thousands of nodes.  Confirmation times are expected to increase only with the logarithm of the number of validators, where the logarithm's base is very high. If the base is one thousand, for example, it means that for the first thousand nodes, confirmation will be the duration of three network hops plus the time it takes the slowest validator of a supermajority to vote. For the next million nodes, confirmation increases by only one network hop.

Solana defines confirmation as the duration of time from when the leader timestamps a new entry to the moment when it recognizes a supermajority of ledger votes.

Scalable confirmation can be achieved using the following combination of techniques:

1. Timestamp transactions with a VDF sample and sign the timestamp.

2. Split the transactions into batches, send each to separate nodes and have each node share its batch with its peers.

3. Repeat the previous step recursively until all nodes have all batches.

Solana rotates leaders at fixed intervals, called _slots_. Each leader may only produce entries during its allotted slot. The leader therefore timestamps transactions so that validators may lookup the public key of the designated leader. The leader then signs the timestamp so that a validator may verify the signature, proving the signer is owner of the designated leader's public key.

Next, transactions are broken into batches so that a node can send transactions to multiple parties without making multiple copies. If, for example, the leader needed to send 60 transactions to 6 nodes, it would break that collection of 60 into batches of 10 transactions and send one to each node. This allows the leader to put 60 transactions on the wire, not 60 transactions for each node. Each node then shares its batch with its peers. Once the node has collected all 6 batches, it reconstructs the original set of 60 transactions.

A batch of transactions can only be split so many times before it is so small that header information becomes the primary consumer of network bandwidth. At the time of this writing (December, 2021), the approach is scaling well up to about 1,250 validators. To scale up to hundreds of thousands of validators, each node can apply the same technique as the leader node to another set of nodes of equal size. We call the technique [_Turbine Block Propagation_](../consensus/turbine-block-propagation.md).

================
File: docs/src/clusters/metrics.md
================
---
title: Solana Cluster Performance Metrics
sidebar_label: Performance Metrics
pagination_label: Cluster Performance Metrics
---

Solana cluster performance is measured as average number of transactions per second that the network can sustain \(TPS\). And, how long it takes for a transaction to be confirmed by super majority of the cluster \(Confirmation Time\).

Each cluster node maintains various counters that are incremented on certain events. These counters are periodically uploaded to a cloud based database. Solana's metrics dashboard fetches these counters, and computes the performance metrics and displays it on the dashboard.

## TPS

Each node's bank runtime maintains a count of transactions that it has processed. The dashboard first calculates the median count of transactions across all metrics enabled nodes in the cluster. The median cluster transaction count is then averaged over a 2-second period and displayed in the TPS time series graph. The dashboard also shows the Mean TPS, Max TPS and Total Transaction Count stats which are all calculated from the median transaction count.

## Confirmation Time

Each validator node maintains a list of active ledger forks that are visible to the node. A fork is considered to be frozen when the node has received and processed all entries corresponding to the fork. A fork is considered to be confirmed when it receives cumulative super majority vote, and when one of its children forks is frozen.

The node assigns a timestamp to every new fork, and computes the time it took to confirm the fork. This time is reflected as validator confirmation time in performance metrics. The performance dashboard displays the average of each validator node's confirmation time as a time series graph.

## Hardware setup

The validator software is deployed to GCP n1-standard-16 instances with 1TB pd-ssd disk, and 2x Nvidia V100 GPUs. These are deployed in the us-west-1 region.

solana-bench-tps is started after the network converges from a client machine with n1-standard-16 CPU-only instance with the following arguments: `--tx\_count=50000 --thread-batch-sleep 1000`

TPS and confirmation metrics are captured from the dashboard numbers over a 5-minute average of when the bench-tps transfer stage begins.

================
File: docs/src/clusters/testnet.md
================
# Test Network Management
The `./net/` directory in the monorepo contains scripts useful for creation and manipulation of a test network.
The test network allows you to run a fully isolated set of validators and clients on a configurable hardware setup.
It's intended to be both dev and CD friendly.


### Cloud account prerequisites

The test networks to be created can run in GCP, AWS or colo. Whichever cloud provider you choose, you will need the credentials set up on your machine.

#### GCP
You will need a working `gcloud` command from google SDK,
if you do not have it follow the guide in [https://cloud.google.com/sdk?hl=en](https://cloud.google.com/sdk?hl=en)

Before running any scripts, authenticate with
```bash
$ gcloud auth login
```
If you are running the scripts on a headless machine, you can use curl to issue requests to confirm your auth.

If you are doing it the first time, you might need to set up project
```bash
gcloud config set project principal-lane-200702
```

#### AWS
Obtain your credentials from the AWS IAM Console and configure the AWS CLI with
```bash
$ aws configure
```
More information on AWS CLI configuration can be found [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration)

## Metrics configuration (Optional)
Metrics collection relies on 2 environment variables that are patched to the remote nodes by net.sh:
 * `RUST_LOG` to enable metrics reporting in principle
 * `SOLANA_METRICS_CONFIG` to tell agave where to log the metrics

### Preparation
> [!NOTE]
> Anza employees should follow the guide in notion to set up the influxDB account.

 * Ensure that `${host}` is the host name of the InfluxDB you can access, for example `https://internal-metrics.solana.com:8086`
 * Ensure that `${user}` is the name of an InfluxDB user account with enough
rights to create a new InfluxDB database, for example `solana`.

### To set up the metrics
You will normally only need to do this once. Once this is done, you will be able to save the metrics configuration and load it later from the environment.

* Go to ./net/ in agave repo
* Run `./init-metrics.sh -c testnet-dev-${user} ${user} `
  * Script will ask for a password, it is the same one you’ve created when making a user in the InfluxDB UI
  * Put the username you have used in preparation, not your login user name
  * If you need to set influxDb host, edit the script
* The script will configure the database (recreating one if necessary) and append a config line in the very end of `net/config/config` file like the following:
  * `export SOLANA_METRICS_CONFIG="host=${host},db=testnet-dev-${user},u=${user},p=some_secret"`
  * You can store that line somewhere and append it to the config file when you need to reuse the database.
  * You can also store it into your shell’s environment so you can run `./init-metrics.sh -e` to quickly load it
  * Alternatively, you'll need to run `./init-metrics.sh` with appropriate arguments every time you set up a new cluster
* Assuming no errors, your influxDB setup is now done.
* For simple cases, storing `SOLANA_METRICS_CONFIG` in your env is appropriate, but you may want to use different databases for different runs of net.sh
  * You can call ./init-metrics.sh before you call net.sh start, this will change the metrics config for a particular run.
  * You can manually write `SOLANA_METRICS_CONFIG` in the `./net/config/config` file
* By default, metrics are only logged by agave if `RUST_LOG` is set to `info` or higher. You can provide it as environment for `./net.sh start` command, or set this in your shell environment.
  ```bash
  RUST_LOG="solana_metrics=info"
  ```

### To validate that your database and metrics environment variables are set up 100% correctly

Note: this only works if you store `SOLANA_METRICS_CONFIG` in your shell environment

```bash
  cd ./scripts/
  source  ./configure-metrics.sh
    INFLUX_HOST=https://internal-metrics.solana.com:8086
    INFLUX_DATABASE=testnet-dev-solana
    INFLUX_USERNAME=solana
    INFLUX_PASSWORD=********
  ./metrics-write-datapoint.sh "testnet-deploy net-create-begin=1"

  ```
  * All commands should complete with no errors, this indicates your influxDB config is usable
  * Ensure that `RUST_LOG` is set to `info` or `debug`

## Quick Start

NOTE: This example uses GCE.  If you are using AWS EC2, replace `./gce.sh` with
`./ec2.sh` in the commands.

```bash
# In Agave repo
cd net/

# Create a GCE testnet with 4 additional validator nodes (beyond the bootstrap node) and 1 client (billing starts here)
./gce.sh create -n 4 -c 1

# Configure the metrics database and validate credentials using environment variable `SOLANA_METRICS_CONFIG` (skip this if you are not using metrics)
./init-metrics.sh -c testnet-dev-${USER} ${USER}

# Deploy the network from the local workspace and start processes on all nodes including bench-tps on the client node
RUST_LOG=info ./net.sh start

# Show a help to ssh into any testnet node to access logs/etc
./ssh.sh

# Stop running processes on all nodes
./net.sh stop

# Dispose of the network (billing stops here)
./gce.sh delete
```

## Full guide
* If you expect metrics to work, make sure you have configured them before proceeding
* Go to `./net/` directory in agave repo
* `./gce.sh` command controls creation and destruction of the nodes in the test net. It does not actually run any software.
  * `./gce.sh create \-n 4 \-c 2` creates cluster with 4 validators and 1 node for load generation, this is minimal viable setup for all solana features to work
    * If the creation succeeds, `net/config/config` will contain the config file of the testnet just created
    * If you do not have `SOLANA_METRICS_CONFIG` set in your shell env, `gce.sh` may complain about metrics not being configured, this is perfectly fine
  * `./gce.sh info`  lists active test cluster nodes, this allows you to get their IP addresses for SSH access and/or debugging
  * `./gce.sh delete`  destroys the nodes (save the electricity and $$$ - destroy your test nets the moment you no longer need them).
  * On GCE, if you do not delete nodes, they will self-destruct in 8 hours anyway, you can configure self-destruct timer by supplying `--self-destruct-hours=N` argument to `gce.sh`
  * On other cloud platforms the testnet will not self-destruct!
* To enable metrics in the testnet, at this point you need to either:
  * `./init-metrics.sh -c testnet-dev-${user} ${user}` to create a new metrics database from scratch
  * Manually set `SOLANA_METRICS_CONFIG` in `./net/config/config` (which is exactly what `init-metrics.sh` does for you)
  * `./init-metrics.sh -e` to load metrics config from `SOLANA_METRICS_CONFIG` into the testnet config file or
* `./net.sh` controls the payload on the testnet nodes, i.e. bootstrapping, the validators and bench-tps. In principle, you can run everything by hand, but `./net.sh` makes it easier.
  * `./net.sh start` to actually run the test network.
    * This will actually upload your current sources to the bootstrap host, build them there and upload the result to all the nodes
    * The script will take 5-10 of minutes to run, in the end it should print something like
     ```
     --- Deployment Successful
     Bootstrap validator deployment took 164 seconds
     ```
    * You can also make sure it logs successful test transfers:
    ```✅ 1 lamport(s) transferred: seq=0   time= 402ms signature=33uJtPJM6ekBGrWCgWHKw1TTQJVrLxYMe3sp2PUmSRVb21LyXn3nDbQmzsgQyihE7VP2zD2iR66Du8aDUnSSd6pb```
  * `./net.sh start  bench-tps=2="--tx_count 2500"` will start 2 clients with bench-tps workload sending 2500 transactions per batch.
    * --tx_count argument is passed to the bench-tps program, see its manual for more options
  * `./net.sh sanity`  to test the deployment, it is also run by start command
  * `./net.sh stop`  to stop the validators and client. This does not kill the machines, so you can study the logs etc.
  * `./net.sh start --nobuild` will skip the source compilation, you will generally want that if you are only changing configuration files rather than code, or just want to re-run the last test.
* To connect to the nodes:
  * `./gce.sh info ` to get the public IPs
  * `./ssh.sh <IP> ` to get a shell on the node
  * `sudo su` will give you root access on the nodes
  * Nodes run latest ubuntu LTS image
* You can also interact with the nodes using solana cli:
```bash
# source ip list  use as ${validatorIpList[4]}
source net/config/config

# airdrop
../target/release/solana -u http://${validatorIpList[1]}:8899 airdrop 1

# check feature
../target/release/solana -u http://${validatorIpList[1]}:8899 feature status

# activate a feature
../target/release/solana -u http://${validatorIpList[1]}:8899 feature activate <path to .json>

# check the stakes on current validators
../target/release/solana --url http://${validatorIpList[0]}:8899 validators
```

## Tips

### Automation
You will want to have a script like this pretty much immediately to avoid making mistakes in the init process:
```bash
# Create the testnet with reasonable node sizes for a small test
# This particular one will have 7 nodes: 1 bootstrap validator, 4 regular validators, and 2 clients
./gce.sh create -n4 -c2 --custom-machine-type "--machine-type n1-standard-16" --client-machine-type "--machine-type n1-standard-4"
# Patch metrics config from env into config file
./init-metrics.sh -e
# Enable metrics and start the network (this will also build software)
RUST_LOG=info ./net.sh start  -c bench-tps=2="--tx_count 25000"
```

### Inscrutable "nothing works everything times out state"
 Note that net.sh and `gce.sh info` commands do not actually check if all the nodes are still alive in gcloud,
 they just assume the config file information is correct. So if your nodes got killed/timed out they will lie to you. In such case, just use `gce.sh delete` to reset.

### Running the network over public IP addresses
By default private IP addresses are used with all instances in the same
availability zone to avoid GCE network egress charges. However to run the
network over public IP addresses:
```bash
$ ./gce.sh create -P ...
```
or
```bash
$ ./ec2.sh create -P ...
```

### Deploying a tarball-based network
To deploy the latest pre-built `edge` channel tarball (ie, latest from the `master`
branch), once the testnet has been created run:

```bash
$ ./net.sh start -t edge
```

================
File: docs/src/consensus/commitments.md
================
---
title: Solana Commitment Status
sidebar_label: Commitment Status
pagination_label: Consensus Commitment Status
description:
  "Processed, confirmed, and finalized. Learn the differences between the
  different commitment statuses on the Solana blockchain."
keywords:
  - processed
  - confirmed
  - finalized
  - stake level
  - block
  - blockhash
---

The [commitment](https://solana.com/docs/terminology#commitment) metric gives
clients a standard measure of the network confirmation for the block. Clients
can then use this information to derive their own measures of commitment.

There are three specific commitment statuses:

- Processed
- Confirmed
- Finalized

| Property                              | Processed | Confirmed | Finalized |
| ------------------------------------- | --------- | --------- | --------- |
| Received block                        | X         | X         | X         |
| Block on majority fork                | X         | X         | X         |
| Block contains target tx              | X         | X         | X         |
| 66%+ stake voted on block             | -         | X         | X         |
| 31+ confirmed blocks built atop block | -         | -         | X         |

================
File: docs/src/consensus/fork-generation.md
================
---
title: Fork Generation
description:
  "A fork is created when validators do not agree on a newly produced block.
  Using a consensus algorithm validators vote on which will be finalized."
---

The Solana protocol doesn’t wait for all validators to agree on a newly produced
block before the next block is produced. Because of that, it’s quite common for
two different blocks to be chained to the same parent block. In those
situations, we call each conflicting chain a [“fork.”](./fork-generation.md)

Solana validators need to vote on one of these forks and reach agreement on
which one to use through a consensus algorithm (that is beyond the scope of this
article). The main point you need to remember is that when there are competing
forks, only one fork will be finalized by the cluster and the abandoned blocks
in competing forks are all discarded.

This section describes how forks naturally occur as a consequence of
[leader rotation](./leader-rotation.md).

## Overview

Nodes take turns being [leader](https://solana.com/docs/terminology#leader) and
generating the PoH that encodes state changes. The cluster can tolerate loss of
connection to any leader by synthesizing what the leader _**would**_ have
generated had it been connected but not ingesting any state changes.

The possible number of forks is thereby limited to a "there/not-there" skip list
of forks that may arise on leader rotation slot boundaries. At any given slot,
only a single leader's transactions will be accepted.

### Forking example

The table below illustrates what competing forks could look like. Time
progresses from left to right and each slot is assigned to a validator that
temporarily becomes the cluster “leader” and may produce a block for that slot.

In this example, the leader for slot 3 chose to chain its “Block 3” directly to
“Block 1” and in doing so skipped “Block 2”. Similarly, the leader for slot 5
chose to chain “Block 5” directly to “Block 3” and skipped “Block 4”.

> Note that across different forks, the block produced for a given slot is
> _always_ the same because producing two different blocks for the same slot is
> a slashable offense. So the conflicting forks above can be distinguished from
> each other by which slots they have _skipped_.

|        | Slot 1  | Slot 2  | Slot 3  | Slot 4  | Slot 5  |
| ------ | ------- | ------- | ------- | ------- | ------- |
| Fork 1 | Block 1 |         | Block 3 |         | Block 5 |
| Fork 2 | Block 1 |         | Block 3 | Block 4 |         |
| Fork 3 | Block 1 | Block 2 |         |         |         |

## Message Flow

1. Transactions are ingested by the current leader.
2. Leader filters valid transactions.
3. Leader executes valid transactions updating its state.
4. Leader packages transactions into entries based off its current PoH slot.
5. Leader transmits the entries to validator nodes \(in signed shreds\)
   1. The PoH stream includes ticks; empty entries that indicate liveness of the
      leader and the passage of time on the cluster.
   2. A leader's stream begins with the tick entries necessary to complete PoH
      back to the leader's most recently observed prior leader slot.
6. Validators retransmit entries to peers in their set and to further downstream
   nodes.
7. Validators validate the transactions and execute them on their state.
8. Validators compute the hash of the state.
9. At specific times, i.e. specific PoH tick counts, validators transmit votes
   to the leader.
   1. Votes are signatures of the hash of the computed state at that PoH tick
      count.
   2. Votes are also propagated via gossip.
10. Leader executes the votes, the same as any other transaction, and broadcasts
    them to the cluster.
11. Validators observe their votes and all the votes from the cluster.

## Partitions, Forks

Forks can arise at PoH tick counts that correspond to a vote. The next leader
may not have observed the last vote slot and may start their slot with generated
virtual PoH entries. These empty ticks are generated by all nodes in the cluster
at a cluster-configured rate for hashes/per/tick `Z`.

There are only two possible versions of the PoH during a voting slot: PoH with
`T` ticks and entries generated by the current leader, or PoH with just ticks.
The "just ticks" version of the PoH can be thought of as a virtual ledger, one
that all nodes in the cluster can derive from the last tick in the previous
slot.

Validators can ignore forks at other points \(e.g. from the wrong leader\), or
slash the leader responsible for the fork.

Validators vote based on a greedy choice to maximize their reward described in
[Tower BFT](../implemented-proposals/tower-bft.md).

### Validator's View

#### Time Progression

The diagram below represents a validator's view of the PoH stream with possible
forks over time. L1, L2, etc. are leader slots, and `E`s represent entries from
that leader during that leader's slot. The `x`s represent ticks only, and time
flows downwards in the diagram.

![Fork generation](/img/fork-generation.svg)

Note that an `E` appearing on 2 forks at the same slot is a slashable condition,
so a validator observing `E3` and `E3'` can slash L3 and safely choose `x` for
that slot. Once a validator commits to a fork, other forks can be discarded
below that tick count. For any slot, validators need only consider a single "has
entries" chain or a "ticks only" chain to be proposed by a leader. But multiple
virtual entries may overlap as they link back to the previous slot.

#### Time Division

It's useful to consider leader rotation over PoH tick count as time division of
the job of encoding state for the cluster. The following table presents the
above tree of forks as a time-divided ledger.

| leader slot      | L1  | L2  | L3  | L4  | L5  |
| :--------------- | :-- | :-- | :-- | :-- | :-- |
| data             | E1  | E2  | E3  | E4  | E5  |
| ticks since prev |     |     |     | x   | xx  |

Note that only data from leader L3 will be accepted during leader slot L3. Data
from L3 may include "catchup" ticks back to a slot other than L2 if L3 did not
observe L2's data. L4 and L5's transmissions include the "ticks to prev" PoH
entries.

This arrangement of the network data streams permits nodes to save exactly this
to the ledger for replay, restart, and checkpoints.

### Leader's View

When a new leader begins a slot, it must first transmit any PoH \(ticks\)
required to link the new slot with the most recently observed and voted slot.
The fork the leader proposes would link the current slot to a previous fork that
the leader has voted on with virtual ticks.

================
File: docs/src/consensus/leader-rotation.md
================
---
title: Solana Leader Rotation
sidebar_label: Leader Rotation
pagination_label: Leader Rotation
---

At any given moment, a cluster expects only one validator to produce ledger entries. By having only one leader at a time, all validators are able to replay identical copies of the ledger. The drawback of only one leader at a time, however, is that a malicious leader is capable of censoring votes and transactions. Since censoring cannot be distinguished from the network dropping packets, the cluster cannot simply elect a single node to hold the leader role indefinitely. Instead, the cluster minimizes the influence of a malicious leader by rotating which node takes the lead.

Each validator selects the expected leader using the same algorithm, described below. When the validator receives a new signed ledger entry, it can be certain that an entry was produced by the expected leader. The order of slots which each leader is assigned a slot is called a _leader schedule_.

## Leader Schedule Rotation

A validator rejects blocks that are not signed by the _slot leader_. The list of identities of all slot leaders is called a _leader schedule_. The leader schedule is recomputed locally and periodically. It assigns slot leaders for a duration of time called an _epoch_. The schedule must be computed far in advance of the slots it assigns, such that the ledger state it uses to compute the schedule is finalized. That duration is called the _leader schedule offset_. Solana sets the offset to the duration of slots until the next epoch. That is, the leader schedule for an epoch is calculated from the ledger state at the start of the previous epoch. The offset of one epoch is fairly arbitrary and assumed to be sufficiently long such that all validators will have finalized their ledger state before the next schedule is generated. A cluster may choose to shorten the offset to reduce the time between stake changes and leader schedule updates.

While operating without partitions lasting longer than an epoch, the schedule only needs to be generated when the root fork crosses the epoch boundary. Since the schedule is for the next epoch, any new stakes committed to the root fork will not be active until the next epoch. The block used for generating the leader schedule is the first block to cross the epoch boundary.

Without a partition lasting longer than an epoch, the cluster will work as follows:

1. A validator continuously updates its own root fork as it votes.
2. The validator updates its leader schedule each time the slot height crosses an epoch boundary.

For example:

Let's assume an epoch duration of 100 slots, which in reality is magnitudes higher. The root fork is updated from fork computed at slot height 99 to a fork computed at slot height 102. Forks with slots at height 100, 101 were skipped because of failures. The new leader schedule is computed using fork at slot height 102. It is active from slot 200 until it is updated again.

No inconsistency can exist because every validator that is voting with the cluster has skipped 100 and 101 when its root passes 102. All validators, regardless of voting pattern, would be committing to a root that is either 102, or a descendant of 102.

### Leader Schedule Rotation with Epoch Sized Partitions.

The duration of the leader schedule offset has a direct relationship to the likelihood of a cluster having an inconsistent view of the correct leader schedule.

Consider the following scenario:

Two partitions that are generating half of the blocks each. Neither is coming to a definitive supermajority fork. Both will cross epoch 100 and 200 without actually committing to a root and therefore a cluster-wide commitment to a new leader schedule.

In this unstable scenario, multiple valid leader schedules exist.

- A leader schedule is generated for every fork whose direct parent is in the previous epoch.
- The leader schedule is valid after the start of the next epoch for descendant forks until it is updated.

Each partition's schedule will diverge after the partition lasts more than an epoch. For this reason, the epoch duration should be selected to be much larger then slot time and the expected length for a fork to be committed to root.

After observing the cluster for a sufficient amount of time, the leader schedule offset can be selected based on the median partition duration and its standard deviation. For example, an offset longer then the median partition duration plus six standard deviations would reduce the likelihood of an inconsistent ledger schedule in the cluster to 1 in 1 million.

## Leader Schedule Generation at Genesis

The genesis config declares the first leader for the first epoch. This leader ends up scheduled for the first two epochs because the leader schedule is also generated at slot 0 for the next epoch. The length of the first two epochs can be specified in the genesis config as well. The minimum length of the first epochs must be greater than or equal to the maximum rollback depth as defined in [Tower BFT](../implemented-proposals/tower-bft.md).

## Leader Schedule Generation Algorithm

Leader schedule is generated using a predefined seed. The process is as follows:

1. Periodically use the PoH tick height \(a monotonically increasing counter\) to seed a stable pseudo-random algorithm.
2. At that height, sample the bank for all the staked accounts with leader identities that have voted within a cluster-configured number of ticks. The sample is called the _active set_.
3. Sort the active set by stake weight.
4. Use the random seed to select nodes weighted by stake to create a stake-weighted ordering.
5. This ordering becomes valid after a cluster-configured number of ticks.

## Schedule Attack Vectors

### Seed

The seed that is selected is predictable but unbiasable. There is no grinding attack to influence its outcome.

### Active Set

A leader can bias the active set by censoring validator votes. Two possible ways exist for leaders to censor the active set:

- Ignore votes from validators
- Refuse to vote for blocks with votes from validators

To reduce the likelihood of censorship, the active set is calculated at the leader schedule offset boundary over an _active set sampling duration_. The active set sampling duration is long enough such that votes will have been collected by multiple leaders.

### Staking

Leaders can censor new staking transactions or refuse to validate blocks with new stakes. This attack is similar to censorship of validator votes.

### Validator operational key loss

Leaders and validators are expected to use ephemeral keys for operation, and stake owners authorize the validators to do work with their stake via delegation.

The cluster should be able to recover from the loss of all the ephemeral keys used by leaders and validators, which could occur through a common software vulnerability shared by all the nodes. Stake owners should be able to vote directly by co-signing a validator vote even though the stake is currently delegated to a validator.

## Appending Entries

The lifetime of a leader schedule is called an _epoch_. The epoch is split into _slots_, where each slot has a duration of `T` PoH ticks.

A leader transmits entries during its slot. After `T` ticks, all the validators switch to the next scheduled leader. Validators must ignore entries sent outside a leader's assigned slot.

All `T` ticks must be observed by the next leader for it to build its own entries on. If entries are not observed \(leader is down\) or entries are invalid \(leader is buggy or malicious\), the next leader must produce ticks to fill the previous leader's slot. Note that the next leader should do repair requests in parallel, and postpone sending ticks until it is confident other validators also failed to observe the previous leader's entries. If a leader incorrectly builds on its own ticks, the leader following it must replace all its ticks.

================
File: docs/src/consensus/managing-forks.md
================
---
title: Managing Forks
---

The ledger is permitted to fork at slot boundaries. The resulting data structure forms a tree called a _blockstore_. When the validator interprets the blockstore, it must maintain state for each fork in the chain. It is the responsibility of a validator to weigh those forks, such that it may eventually select a fork. Details for selection and voting on these forks can be found in [Tower Bft](../implemented-proposals/tower-bft.md)

## Forks

A fork is as a sequence of slots originating from some root. For example:

```
      2 - 4 - 6 - 8
     /
0 - 1       12 - 13
     \     /
      3 - 5
           \
            7 - 9 - 10 - 11
```

The following sequences are forks:

```
- {0, 1, 2, 4, 6, 8}
- {0, 1, 3, 5, 12, 13}
- {0, 1, 3, 5, 7, 9, 10, 11}
```

## Pruning and Squashing

As the chain grows, storing the local forks view becomes detrimental to performance. Fortunately we can take advantage of the properties of tower bft roots to prune this data structure. Recall a root is a slot that has reached the max lockout depth. The assumption is that this slot has accrued enough lockout that it would be impossible to roll this slot back.

Thus, the validator prunes forks that do not originate from its local root, and then takes the opportunity to minimize its memory usage by squashing any nodes it can into the root. Although not necessary for consensus, to enable some RPC use cases the validator chooses to keep ancestors of its local root up until the last slot rooted by the super majority of the cluster. We call this the super majority root (SMR).

Starting from the above example imagine a max lockout depth of 3. Our validator votes on slots `0, 1, 3, 5, 7, 9`. Upon the final vote at `9`, our local root is `3`. Assume the latest super majority root is `0`. After pruning this is our local fork view.

```
SMR
 0 - 1       12 - 13
      \     /
       3 - 5
     ROOT   \
             7 - 9 - 10 - 11
```

Now imagine we vote on `10`, which roots `5`. At the same time the cluster catches up and the latest super majority root is now `3`. After pruning this is our local fork view.

```
             12 - 13
            /
       3 - 5 ROOT
      SMR   \
             7 - 9 - 10 - 11
```

Finally a vote on `11` will root `7`, pruning the final fork
```
       3 - 5 - 7 - 9 - 10 - 11
      SMR     ROOT
```

================
File: docs/src/consensus/stake-delegation-and-rewards.md
================
---
title: Stake Delegation and Rewards
---

Stakers are rewarded for helping to validate the ledger. They do this by
delegating their stake to validator nodes. Those validators do the legwork of
replaying the ledger and sending votes to a per-node vote account to which
stakers can delegate their stakes. The rest of the cluster uses those
stake-weighted votes to select a block when forks arise. Both the validator and
staker need some economic incentive to play their part. The validator needs to
be compensated for its hardware and the staker needs to be compensated for the
risk of getting its stake slashed. The economics are covered in
[staking rewards](../implemented-proposals/staking-rewards.md). This section, on
the other hand, describes the underlying mechanics of its implementation.

## Basic Design

The general idea is that the validator owns a Vote account. The Vote account
tracks validator votes, counts validator generated credits, and provides any
additional validator specific state. The Vote account is not aware of any stakes
delegated to it and has no staking weight.

A separate Stake account \(created by a staker\) names a Vote account to which
the stake is delegated. Rewards generated are proportional to the amount of
lamports staked. The Stake account is owned by the staker only. Some portion of
the lamports stored in this account are the stake.

## Passive Delegation

Any number of Stake accounts can delegate to a single Vote account without an
interactive action from the identity controlling the Vote account or submitting
votes to the account.

The total stake allocated to a Vote account can be calculated by the sum of all
the Stake accounts that have the Vote account pubkey as the
`StakeStateV2::Stake::voter_pubkey`.

## Vote and Stake accounts

The rewards process is split into two on-chain programs. The Vote program solves
the problem of making stakes slashable. The Stake program acts as custodian of
the rewards pool and provides for passive delegation. The Stake program is
responsible for paying rewards to staker and voter when shown that a staker's
delegate has participated in validating the ledger.

### VoteState

VoteState is the current state of all the votes the validator has submitted to
the network. VoteState contains the following state information:

- `votes` - The submitted votes data structure.
- `credits` - The total number of rewards this Vote program has generated over
  its lifetime.
- `root_slot` - The last slot to reach the full lockout commitment necessary for
  rewards.
- `commission` - The commission taken by this VoteState for any rewards claimed
  by staker's Stake accounts. This is the percentage ceiling of the reward.
- Account::lamports - The accumulated lamports from the commission. These do not
  count as stakes.
- `authorized_voter` - Only this identity is authorized to submit votes. This
  field can only modified by this identity.
- `node_pubkey` - The Solana node that votes in this account.
- `authorized_withdrawer` - the identity of the entity in charge of the lamports
  of this account, separate from the account's address and the authorized vote
  signer.

### VoteInstruction::Initialize\(VoteInit\)

- `account[0]` - RW - The VoteState.

  `VoteInit` carries the new vote account's `node_pubkey`, `authorized_voter`,
  `authorized_withdrawer`, and `commission`.

  other VoteState members defaulted.

### VoteInstruction::Authorize\(Pubkey, VoteAuthorize\)

Updates the account with a new authorized voter or withdrawer, according to the
VoteAuthorize parameter \(`Voter` or `Withdrawer`\). The transaction must be
signed by the Vote account's current `authorized_voter` or
`authorized_withdrawer`.

- `account[0]` - RW - The VoteState. `VoteState::authorized_voter` or
  `authorized_withdrawer` is set to `Pubkey`.

### VoteInstruction::AuthorizeWithSeed\(VoteAuthorizeWithSeedArgs\)

Updates the account with a new authorized voter or withdrawer, according to the
VoteAuthorize parameter \(`Voter` or `Withdrawer`\). Unlike
`VoteInstruction::Authorize` this instruction is for use when the Vote account's
current `authorized_voter` or `authorized_withdrawer` is a derived key. The
transaction must be signed by someone who can sign for the base key of that
derived key.

- `account[0]` - RW - The VoteState. `VoteState::authorized_voter` or
  `authorized_withdrawer` is set to `Pubkey`.

### VoteInstruction::Vote\(Vote\)

- `account[0]` - RW - The VoteState. `VoteState::lockouts` and
  `VoteState::credits` are updated according to voting lockout rules see
  [Tower BFT](../implemented-proposals/tower-bft.md).
- `account[1]` - RO - `sysvar::slot_hashes` A list of some N most recent slots
  and their hashes for the vote to be verified against.
- `account[2]` - RO - `sysvar::clock` The current network time, expressed in
  slots, epochs.

### StakeStateV2

A StakeStateV2 takes one of four forms, StakeStateV2::Uninitialized,
StakeStateV2::Initialized, StakeStateV2::Stake, and StakeStateV2::RewardsPool.
Only the first three forms are used in staking, but only StakeStateV2::Stake is
interesting. All RewardsPools are created at genesis.

### StakeStateV2::Stake

StakeStateV2::Stake is the current delegation preference of the **staker** and
contains the following state information:

- Account::lamports - The lamports available for staking.
- `stake` - the staked amount \(subject to warmup and cooldown\) for generating
  rewards, always less than or equal to Account::lamports.
- `voter_pubkey` - The pubkey of the VoteState instance the lamports are
  delegated to.
- `credits_observed` - The total credits claimed over the lifetime of the
  program.
- `activated` - the epoch at which this stake was activated/delegated. The full
  stake will be counted after warmup.
- `deactivated` - the epoch at which this stake was de-activated, some cooldown
  epochs are required before the account is fully deactivated, and the stake
  available for withdrawal.
- `authorized_staker` - the pubkey of the entity that must sign delegation,
  activation, and deactivation transactions.
- `authorized_withdrawer` - the identity of the entity in charge of the lamports
  of this account, separate from the account's address, and the authorized
  staker.

### StakeStateV2::RewardsPool

To avoid a single network-wide lock or contention in redemption, 256
RewardsPools are part of genesis under pre-determined keys, each with
std::u64::MAX credits to be able to satisfy redemptions according to point
value.

The Stakes and the RewardsPool are accounts that are owned by the same `Stake`
program.

### StakeInstruction::DelegateStake

The Stake account is moved from Initialized to StakeStateV2::Stake form, or from
a deactivated (i.e. fully cooled-down) StakeStateV2::Stake to activated
StakeStateV2::Stake. This is how stakers choose the vote account and validator
node to which their stake account lamports are delegated. The transaction must
be signed by the stake's `authorized_staker`.

- `account[0]` - RW - The StakeStateV2::Stake instance.
  `StakeStateV2::Stake::credits_observed` is initialized to
  `VoteState::credits`, `StakeStateV2::Stake::voter_pubkey` is initialized to
  `account[1]`. If this is the initial delegation of stake,
  `StakeStateV2::Stake::stake` is initialized to the account's balance in
  lamports, `StakeStateV2::Stake::activated` is initialized to the current Bank
  epoch, and `StakeStateV2::Stake::deactivated` is initialized to std::u64::MAX
- `account[1]` - R - The VoteState instance.
- `account[2]` - R - sysvar::clock account, carries information about current
  Bank epoch.
- `account[3]` - R - sysvar::stakehistory account, carries information about
  stake history.
- `account[4]` - R - stake::Config account, carries warmup, cooldown, and
  slashing configuration.

### StakeInstruction::Authorize\(Pubkey, StakeAuthorize\)

Updates the account with a new authorized staker or withdrawer, according to the
StakeAuthorize parameter \(`Staker` or `Withdrawer`\). The transaction must be
by signed by the Stakee account's current `authorized_staker` or
`authorized_withdrawer`. Any stake lock-up must have expired, or the lock-up
custodian must also sign the transaction.

- `account[0]` - RW - The StakeStateV2.

  `StakeStateV2::authorized_staker` or `authorized_withdrawer` is set to
  `Pubkey`.

### StakeInstruction::Deactivate

A staker may wish to withdraw from the network. To do so he must first
deactivate his stake, and wait for cooldown. The transaction must be signed by
the stake's `authorized_staker`.

- `account[0]` - RW - The StakeStateV2::Stake instance that is deactivating.
- `account[1]` - R - sysvar::clock account from the Bank that carries current
  epoch.

StakeStateV2::Stake::deactivated is set to the current epoch + cooldown. The
account's stake will ramp down to zero by that epoch, and Account::lamports will
be available for withdrawal.

### StakeInstruction::Withdraw\(u64\)

Lamports build up over time in a Stake account and any excess over activated
stake can be withdrawn. The transaction must be signed by the stake's
`authorized_withdrawer`.

- `account[0]` - RW - The StakeStateV2::Stake from which to withdraw.
- `account[1]` - RW - Account that should be credited with the withdrawn
  lamports.
- `account[2]` - R - sysvar::clock account from the Bank that carries current
  epoch, to calculate stake.
- `account[3]` - R - sysvar::stake_history account from the Bank that carries
  stake warmup/cooldown history.

## Benefits of the design

- Single vote for all the stakers.
- Clearing of the credit variable is not necessary for claiming rewards.
- Each delegated stake can claim its rewards independently.
- Commission for the work is deposited when a reward is claimed by the delegated
  stake.

## Example Callflow

![Passive Staking Callflow](/img/passive-staking-callflow.png)

## Staking Rewards

The specific mechanics and rules of the validator rewards regime is outlined
here. Rewards are earned by delegating stake to a validator that is voting
correctly. Voting incorrectly exposes that validator's stakes to
[slashing](../proposals/slashing.md).

### Basics

The network pays rewards from a portion of network
[inflation](https://solana.com/docs/terminology#inflation). The number of
lamports available to pay rewards for an epoch is fixed and must be evenly
divided among all staked nodes according to their relative stake weight and
participation. The weighting unit is called a
[point](https://solana.com/docs/terminology#point).

Rewards for an epoch are not available until the end of that epoch.

At the end of each epoch, the total number of points earned during the epoch is
summed and used to divide the rewards portion of epoch inflation to arrive at a
point value. This value is recorded in the bank in a
[sysvar](https://solana.com/docs/terminology#sysvar) that maps epochs to point
values.

During redemption, the stake program counts the points earned by the stake for
each epoch, multiplies that by the epoch's point value, and transfers lamports
in that amount from a rewards account into the stake and vote accounts according
to the vote account's commission setting.

### Economics

Point value for an epoch depends on aggregate network participation. If
participation in an epoch drops off, point values are higher for those that do
participate.

### Earning credits

Validators earn one vote credit for every correct vote that exceeds maximum
lockout, i.e. every time the validator's vote account retires a slot from its
lockout list, making that vote a root for the node.

Stakers who have delegated to that validator earn points in proportion to their
stake. Points earned is the product of vote credits and stake.

### Stake warmup, cooldown, withdrawal

Stakes, once delegated, do not become effective immediately. They must first
pass through a warmup period. During this period some portion of the stake is
considered "effective", the rest is considered "activating". Changes occur on
epoch boundaries.

The stake program limits the rate of change to total network stake, reflected in
the stake program's `config::warmup_rate` \(set to 25% per epoch in the current
implementation\).

The amount of stake that can be warmed up each epoch is a function of the
previous epoch's total effective stake, total activating stake, and the stake
program's configured warmup rate.

Cooldown works the same way. Once a stake is deactivated, some part of it is
considered "effective", and also "deactivating". As the stake cools down, it
continues to earn rewards and be exposed to slashing, but it also becomes
available for withdrawal.

Bootstrap stakes are not subject to warmup.

Rewards are paid against the "effective" portion of the stake for that epoch.

#### Warmup example

Consider the situation of a single stake of 1,000 activated at epoch N, with
network warmup rate of 20%, and a quiescent total network stake at epoch N of
2,000.

At epoch N+1, the amount available to be activated for the network is 400 \(20%
of 2000\), and at epoch N, this example stake is the only stake activating, and
so is entitled to all of the warmup room available.

| epoch | effective | activating | total effective | total activating |
| :---- | --------: | ---------: | --------------: | ---------------: |
| N-1   |           |            |           2,000 |                0 |
| N     |         0 |      1,000 |           2,000 |            1,000 |
| N+1   |       400 |        600 |           2,400 |              600 |
| N+2   |       880 |        120 |           2,880 |              120 |
| N+3   |      1000 |          0 |           3,000 |                0 |

Were 2 stakes \(X and Y\) to activate at epoch N, they would be awarded a
portion of the 20% in proportion to their stakes. At each epoch effective and
activating for each stake is a function of the previous epoch's state.

| epoch | X eff | X act | Y eff | Y act | total effective | total activating |
| :---- | ----: | ----: | ----: | ----: | --------------: | ---------------: |
| N-1   |       |       |       |       |           2,000 |                0 |
| N     |     0 | 1,000 |     0 |   200 |           2,000 |            1,200 |
| N+1   |   333 |   667 |    67 |   133 |           2,400 |              800 |
| N+2   |   733 |   267 |   146 |    54 |           2,880 |              321 |
| N+3   |  1000 |     0 |   200 |     0 |           3,200 |                0 |

### Withdrawal

Only lamports in excess of effective+activating stake may be withdrawn at any
time. This means that during warmup, effectively no stake can be withdrawn.
During cooldown, any tokens in excess of effective stake may be withdrawn
\(activating == 0\). Because earned rewards are automatically added to stake,
withdrawal is generally only possible after deactivation.

### Lock-up

Stake accounts support the notion of lock-up, wherein the stake account balance
is unavailable for withdrawal until a specified time. Lock-up is specified as an
epoch height, i.e. the minimum epoch height that must be reached by the network
before the stake account balance is available for withdrawal, unless the
transaction is also signed by a specified custodian. This information is
gathered when the stake account is created, and stored in the Lockup field of
the stake account's state. Changing the authorized staker or withdrawer is also
subject to lock-up, as such an operation is effectively a transfer.

================
File: docs/src/consensus/synchronization.md
================
---
title: Synchronization
---

Fast, reliable synchronization is the biggest reason Solana is able to achieve such high throughput. Traditional blockchains synchronize on large chunks of transactions called blocks. By synchronizing on blocks, a transaction cannot be processed until a duration, called "block time", has passed. In Proof of Work consensus, these block times need to be very large \(~10 minutes\) to minimize the odds of multiple validators producing a new valid block at the same time. There's no such constraint in Proof of Stake consensus, but without reliable timestamps, a validator cannot determine the order of incoming blocks. The popular workaround is to tag each block with a [wallclock timestamp](https://en.bitcoin.it/wiki/Block_timestamp). Because of clock drift and variance in network latencies, the timestamp is only accurate within an hour or two. To workaround the workaround, these systems lengthen block times to provide reasonable certainty that the median timestamp on each block is always increasing.

Solana takes a very different approach, which it calls _Proof of History_ or _PoH_. Leader nodes "timestamp" blocks with cryptographic proofs that some duration of time has passed since the last proof. All data hashed into the proof most certainly have occurred before the proof was generated. The node then shares the new block with validator nodes, which are able to verify those proofs. The blocks can arrive at validators in any order or even could be replayed years later. With such reliable synchronization guarantees, Solana is able to break blocks into smaller batches of transactions called _entries_. Entries are streamed to validators in realtime, before any notion of block consensus.

Solana technically never sends a _block_, but uses the term to describe the sequence of entries that validators vote on to achieve _confirmation_. In that way, Solana's confirmation times can be compared apples to apples to block-based systems. The current implementation sets block time to 800ms.

What's happening under the hood is that entries are streamed to validators as quickly as a leader node can batch a set of valid transactions into an entry. Validators process those entries long before it is time to vote on their validity. By processing the transactions optimistically, there is effectively no delay between the time the last entry is received and the time when the node can vote. In the event consensus is **not** achieved, a node simply rolls back its state. This optimistic processing technique was introduced in 1981 and called [Optimistic Concurrency Control](https://en.wikipedia.org/wiki/Optimistic_concurrency_control). It can be applied to blockchain architecture where a cluster votes on a hash that represents the full ledger up to some _block height_. In Solana, it is implemented trivially using the last entry's PoH hash.

## Relationship to VDFs

The Proof of History technique was first described for use in blockchain by Solana in November of 2017. In June of the following year, a similar technique was described at Stanford and called a [verifiable delay function](https://eprint.iacr.org/2018/601.pdf) or _VDF_.

A desirable property of a VDF is that verification time is very fast. Solana's approach to verifying its delay function is proportional to the time it took to create it. Split over a 4000 core GPU, it is sufficiently fast for Solana's needs, but if you asked the authors of the paper cited above, they might tell you \([and have](https://github.com/solana-labs/solana/issues/388)\) that Solana's approach is algorithmically slow and it shouldn't be called a VDF. We argue the term VDF should represent the category of verifiable delay functions and not just the subset with certain performance characteristics. Until that's resolved, Solana will likely continue using the term PoH for its application-specific VDF.

Another difference between PoH and VDFs is that a VDF is used only for tracking duration. PoH's hash chain, on the other hand, includes hashes of any data the application observed. That data is a double-edged sword. On one side, the data "proves history" - that the data most certainly existed before hashes after it. On the other side, it means the application can manipulate the hash chain by changing _when_ the data is hashed. The PoH chain therefore does not serve as a good source of randomness whereas a VDF without that data could. Solana's [leader rotation algorithm](./leader-rotation.md), for example, is derived only from the VDF _height_ and not its hash at that height.

## Relationship to Consensus Mechanisms

Proof of History is not a consensus mechanism, but it is used to improve the performance of Solana's Proof of Stake consensus. It is also used to improve the performance of the data plane protocols.

## More on Proof of History

- [water clock analogy](https://medium.com/solana-labs/proof-of-history-explained-by-a-water-clock-e682183417b8)
- [Proof of History overview](https://medium.com/solana-labs/proof-of-history-a-clock-for-blockchain-cf47a61a9274)

================
File: docs/src/consensus/turbine-block-propagation.md
================
---
title: Turbine Block Propagation
---

A Solana cluster uses a multi-layer block propagation mechanism called _Turbine_
to broadcast ledger entries to all nodes. The cluster divides itself into layers
of nodes, and each node in a given layer is responsible for propagating any data
it receives on to a small set of nodes in the next downstream layer. This way
each node only has to communicate with a small number of nodes.

## Layer Structure

The leader communicates with a special root node. The root can be thought of as
layer 0 and communicates with layer 1, which is made up of at most
`DATA_PLANE_FANOUT` nodes. If the number of nodes in the cluster is greater than
layer 1, then the data plane fanout mechanism adds layers below. The number of
nodes in each additional layer grows by a factor of `DATA_PLANE_FANOUT`.

A good way to think about this is, layer 0 starts with a single node, layer 1
starts with fanout nodes, and layer 2 will have `fanout * number of nodes in
layer 1` and so on.

### Layer Assignment  - Weighted Selection

In order for data plane fanout to work, the entire cluster must agree on how the
cluster is divided into layers. To achieve this, all the recognized validator
nodes \(the TVU peers\) are shuffled with a stake weighting and stored in a
list. This list is then indexed in different ways to figure out layer boundaries
and retransmit peers - referred to as the \(turbine tree\). For example, the
list is shuffled and leader selects the first node to be the root node, and the
root node selects the next `DATA_PLANE_FANOUT` nodes to make up layer 1. The
shuffle is biased towards higher staked nodes, allowing heavier votes to come
back to the leader first. Layer 2 and lower-layer nodes use the same logic to
find their next layer peers.

To reduce the possibility of attack vectors, the list is shuffled and indexed on
every shred. The turbine tree is generated from the set of validator nodes for
each shred using a seed derived from the slot leader id, slot, shred index, and
shred type.

### Configuration Values

`DATA_PLANE_FANOUT` - Determines the size of layer 1. Subsequent layers grow by
a factor of `DATA_PLANE_FANOUT`. Layers will fill to capacity before new ones are
added, i.e if a layer isn't full, it _must_ be the last one.

Currently, configuration is set when the cluster is launched. In the future,
these parameters may be hosted on-chain, allowing modification on the fly as the
cluster sizes change.

## Shred Propagation Flow

During its slot, the leader node makes its initial broadcasts to a special root
node \(layer 0\) sitting atop the turbine tree. This root node is rotated every
shred based on the weighted shuffle previously mentioned. The root shares data
with layer 1. Nodes in this layer then retransmit shreds to a subset of nodes in
the next layer \(layer 2\). In general, every node in layer-1 retransmits to a
unique subset of nodes in the next layer, etc, until all nodes in the cluster
have received all the shreds.

To prevent redundant transmission, each node uses the deterministically
generated turbine tree, its own index in the tree, and `DATA_PLANE_FANOUT` to
iterate through the tree and identify downstream nodes. Each node in a layer
only has to broadcast its shreds to a maximum of `DATA_PLANE_FANOUT` nodes in
the next layer instead of to every TVU peer in the cluster.

The following diagram shows how shreds propagate through a cluster with 15 nodes
and a fanout of 3.

![Shred propagation through 15 node cluster with fanout of 3](/img/data-plane-propagation.png)

## Calculating the required FEC rate

Turbine relies on retransmission of packets between validators. Due to
retransmission, any network wide packet loss is compounded, and the probability
of the packet failing to reach its destination increases on each hop. The FEC
rate needs to take into account the network wide packet loss, and the
propagation depth.

A shred group is the set of data and coding packets that can be used to
reconstruct each other. Each shred group has a chance of failure, based on the
likelihood of the number of packets failing that exceeds the FEC rate. If a
validator fails to reconstruct the shred group, then the block cannot be
reconstructed, and the validator has to rely on repair to fixup the blocks.

The probability of the shred group failing can be computed using the binomial
distribution. If the FEC rate is `16:4`, then the group size is 20, and at least
5 of the shreds must fail for the group to fail. Which is equal to the sum of
the probability of 5 or more trials failing out of 20.

Probability of a block succeeding in turbine:

- Probability of packet failure: `P = 1 - (1 - network_packet_loss_rate)^2`
- FEC rate: `K:M`
- Number of trials: `N = K + M`
- Shred group failure rate: `S = 1 - (SUM of i=0 -> M for binomial(prob_failure = P, trials = N, failures = i))`
- Shreds per block: `G`
- Block success rate: `B = (1 - S) ^ (G / N)`
- Binomial distribution for exactly `i` results with probability of P in N trials is defined as `(N choose i) * P^i * (1 - P)^(N-i)`

For example:

- Network packet loss rate is 15%.
- 50k tps network generates 6400 shreds per second.
- FEC rate increases the total shreds per block by the FEC ratio.

With a FEC rate: `16:4`

- `G = 8000`
- `P = 1 - 0.85 * 0.85 = 1 - 0.7225 = 0.2775`
- `S = 1 - (SUM of i=0 -> 4 for binomial(prob_failure = 0.2775, trials = 20, failures = i)) = 0.689414`
- `B = (1 - 0.689) ^ (8000 / 20) = 10^-203`

With FEC rate of `16:16`

- `G = 12800`
- `S = 1 - (SUM of i=0 -> 16 for binomial(prob_failure = 0.2775, trials = 32, failures = i)) = 0.002132`
- `B = (1 - 0.002132) ^ (12800 / 32) = 0.42583`

With FEC rate of `32:32`

- `G = 12800`
- `S = 1 - (SUM of i=0 -> 32 for binomial(prob_failure = 0.2775, trials = 64, failures = i)) = 0.000048`
- `B = (1 - 0.000048) ^ (12800 / 64) = 0.99045`

================
File: docs/src/consensus/vote-signing.md
================
---
title: Secure Vote Signing
---

A validator receives entries from the current leader and submits votes confirming those entries are valid. This vote submission presents a security challenge, because forged votes that violate consensus rules could be used to slash the validator's stake.

The validator votes on its chosen fork by submitting a transaction that uses an asymmetric key to sign the result of its validation work. Other entities can verify this signature using the validator's public key. If the validator's key is used to sign incorrect data \(e.g. votes on multiple forks of the ledger\), the node's stake or its resources could be compromised.

## Validators, Vote Signers, and Stakeholders

When a validator receives multiple blocks for the same slot, it tracks all possible forks until it can determine a "best" one. A validator selects the best fork by submitting a vote to it.

A stakeholder is an identity that has control of the staked capital. The stakeholder can delegate its stake to the vote signer. Once a stake is delegated, the vote signer's votes represent the voting weight of all the delegated stakes, and produce rewards for all the delegated stakes.

## Validator voting

A validator node, at startup, creates a new vote account and registers it with the cluster via gossip. The other nodes on the cluster include the new validator in the active set. Subsequently, the validator submits a "new vote" transaction signed with the validator's voting private key on each voting event.

================
File: docs/src/css/custom.css
================
:root {
⋮----
main {
.docusaurus-highlight-code-line {
.button {
.container__spacer {
.cards__container {
.cards__container .col {
.card {
[data-theme="dark"] .card {
.card a {
.card:hover {
.footer--dark {
footer .text--center {
.card__header h3 {
.header-link-icon:before {
.header-link-icon {
[data-theme="dark"] .header-github-link:before {
.header-github-link:before {
[data-theme="dark"] .header-discord-link:before {
.header-discord-link:before {

================
File: docs/src/implemented-proposals/ed_overview/ed_validation_client_economics/ed_vce_overview.md
================
---
title: Validation-client Economics
---

**Subject to change.**

Validator-clients are eligible to charge commission on inflationary rewards distributed to staked tokens. This compensation is for providing compute resources to validate and vote on a given PoH state. These protocol-based rewards are determined through an algorithmic disinflationary schedule as a function of total token supply. The network is expected to launch with an annual inflation rate around 8%, set to decrease by 15% per year until a long-term stable rate of 1.5% is reached, however these parameters are yet to be finalized by the community. These issuances are to be split and distributed to participating validators, with around 95% of the issued tokens allocated for validator rewards initial (the remaining 5% reserved for Foundation operating expenses). Because the network will be distributing a fixed amount of inflation rewards across the stake-weighted validator set, the yield observed for staked tokens will be primarily a function of the amount of staked tokens in relation to the total token supply.

Additionally, validator clients may earn revenue through fees via state-validation transactions. For clarity, we separately describe the design and motivation of these revenue distributions for validation-clients below: state-validation protocol-based rewards and state-validation transaction fees and rent.

================
File: docs/src/implemented-proposals/ed_overview/ed_validation_client_economics/ed_vce_state_validation_protocol_based_rewards.md
================
---
title: Inflation Schedule
---

**Subject to change.**

Validator-clients have two functional roles in the Solana network:

- Validate \(vote\) the current global state of their observed PoH.
- Be elected as ‘leader’ on a stake-weighted round-robin schedule during which time they are responsible for collecting outstanding transactions and incorporating them into their observed PoH, thus updating the global state of the network and providing chain continuity.

Validator-client rewards for these services are to be distributed at the end of each Solana epoch. As previously discussed, compensation for validator-clients is provided via a commission charged on the protocol-based annual inflation rate dispersed in proportion to the stake-weight of each validator-node \(see below\) along with leader-claimed transaction fees available during each leader rotation. I.e. during the time a given validator-client is elected as leader, it has the opportunity to keep a portion of each transaction fee, less a protocol-specified amount that is destroyed \(see [Validation-client State Transaction Fees](ed_vce_state_validation_transaction_fees.md)\).

The effective protocol-based annual staking yield \(%\) per epoch received by validation-clients is to be a function of:

- the current global inflation rate, derived from the pre-determined disinflationary issuance schedule \(see [Validation-client Economics](ed_vce_overview.md)\)
- the fraction of staked SOLs out of the current total circulating supply,
- the commission charged by the validation service,
- the up-time/participation \[% of available slots that validator had opportunity to vote on\] of a given validator over the previous epoch.

The first factor is a function of protocol parameters only \(i.e. independent of validator behavior in a given epoch\) and results in an inflation schedule designed to incentivize early participation, provide clear monetary stability and provide optimal security in the network.

As a first step to understanding the impact of the _Inflation Schedule_ on the Solana economy, we’ve simulated the upper and lower ranges of what token issuance over time might look like given the current ranges of Inflation Schedule parameters under study.

Specifically:

- _Initial Inflation Rate_: 7-9%
- _Disinflation Rate_: -14-16%
- _Long-term Inflation Rate_: 1-2%

Using these ranges to simulate a number of possible Inflation Schedules, we can explore inflation over time:

![](/img/p_inflation_schedule_ranges_w_comments.png)

In the above graph, the average values of the range are identified to illustrate the contribution of each parameter.
From these simulated _Inflation Schedules_, we can also project ranges for token issuance over time.

![](/img/p_total_supply_ranges.png)

Finally we can estimate the _Staked Yield_ on staked SOL, if we introduce an additional parameter, previously discussed, _% of Staked SOL_:

$$
\%~\text{SOL Staked} = \frac{\text{Total SOL Staked}}{\text{Total Current Supply}}
$$

In this case, because _% of Staked SOL_ is a parameter that must be estimated (unlike the _Inflation Schedule_ parameters), it is easier to use specific _Inflation Schedule_ parameters and explore a range of _% of Staked SOL_. For the below example, we’ve chosen the middle of the parameter ranges explored above:

- _Initial Inflation Rate_: 8%
- _Disinflation Rate_: -15%
- _Long-term Inflation Rate_: 1.5%

The values of _% of Staked SOL_ range from 60% - 90%, which we feel covers the likely range we expect to observe, based on feedback from the investor and validator communities as well as what is observed on comparable Proof-of-Stake protocols.

![](/img/p_ex_staked_yields.png)

Again, the above shows an example _Staked Yield_ that a staker might expect over time on the Solana network with the _Inflation Schedule_ as specified. This is an idealized _Staked Yield_ as it neglects validator uptime impact on rewards, validator commissions, potential yield throttling and potential slashing incidents. It additionally ignores that _% of Staked SOL_ is dynamic by design - the economic incentives set up by this _Inflation Schedule_.

### Adjusted Staking Yield

A complete appraisal of earning potential from staking tokens should take into account staked _Token Dilution_ and its impact on staking yield. For this, we define _adjusted staking yield_ as the change in fractional token supply ownership of staked tokens due to the distribution of inflation issuance. I.e. the positive dilutive effects of inflation.

We can examine the _adjusted staking yield_ as a function of the inflation rate and the percent of staked tokens on the network. We can see this plotted for various staking fractions here:

![](/img/p_ex_staked_dilution.png)

================
File: docs/src/implemented-proposals/ed_overview/ed_validation_client_economics/ed_vce_state_validation_transaction_fees.md
================
---
title: State-validation Transaction Fees
---

**Subject to change.**

Each transaction sent through the network, to be processed by the current leader validation-client and confirmed as a global state transaction, must contain a transaction fee. Transaction fees offer many benefits in the Solana economic design, for example they:

- provide unit compensation to the validator network for the compute resources necessary to process the state transaction,
- reduce network spam by introducing real cost to transactions,
- open avenues for a transaction market to incentivize validation-client to collect and process submitted transactions in their function as leader,
- and provide potential long-term economic stability of the network through a protocol-captured minimum fee amount per transaction, as described below.

Many current blockchain economies \(e.g. Bitcoin, Ethereum\), rely on protocol-based rewards to support the economy in the short term, with the assumption that the revenue generated through transaction fees will support the economy in the long term, when the protocol derived rewards expire. In an attempt to create a sustainable economy through protocol-based rewards and transaction fees, a fixed portion of each transaction fee is destroyed, with the remaining fee going to the current leader processing the transaction. A scheduled global inflation rate provides a source for rewards distributed to validation-clients, through the process described above.

Transaction fees are set by the network cluster based on recent historical throughput, see [Congestion Driven Fees](../../transaction-fees.md#congestion-driven-fees). This minimum portion of each transaction fee can be dynamically adjusted depending on historical gas usage. In this way, the protocol can use the minimum fee to target a desired hardware utilization. By monitoring a protocol specified gas usage with respect to a desired, target usage amount, the minimum fee can be raised/lowered which should, in turn, lower/raise the actual gas usage per block until it reaches the target amount. This adjustment process can be thought of as similar to the difficulty adjustment algorithm in the Bitcoin protocol, however in this case it is adjusting the minimum transaction fee to guide the transaction processing hardware usage to a desired level.

As mentioned, a fixed-proportion of each transaction fee is to be destroyed. The intent of this design is to retain leader incentive to include as many transactions as possible within the leader-slot time, while providing an inflation limiting mechanism that protects against "tax evasion" attacks \(i.e. side-channel fee payments\)[1](../ed_references.md).

Additionally, the burnt fees can be a consideration in fork selection. In the case of a PoH fork with a malicious, censoring leader, we would expect the total fees destroyed to be less than a comparable honest fork, due to the fees lost from censoring. If the censoring leader is to compensate for these lost protocol fees, they would have to replace the burnt fees on their fork themselves, thus potentially reducing the incentive to censor in the first place.

================
File: docs/src/implemented-proposals/ed_overview/ed_validation_client_economics/ed_vce_validation_stake_delegation.md
================
---
title: Validation Stake Delegation
---

**Subject to change.**

Running a Solana validation-client required relatively modest upfront hardware capital investment. **Table 2** provides an example hardware configuration to support ~1M tx/s with estimated ‘off-the-shelf’ costs:

| Component         | Example                                          | Estimated Cost |
| :---------------- | :----------------------------------------------- | :------------- |
| GPU               | 2x 2080 Ti                                       | \$2500         |
| or                | 4x 1080 Ti                                       | \$2800         |
| OS/Ledger Storage | Samsung 860 Evo 2TB                              | \$370          |
| Accounts storage  | 2x Samsung 970 Pro M.2 512GB                     | \$340          |
| RAM               | 32 Gb                                            | \$300          |
| Motherboard       | AMD x399                                         | \$400          |
| CPU               | AMD Threadripper 2920x                           | \$650          |
| Case              |                                                  | \$100          |
| Power supply      | EVGA 1600W                                       | \$300          |
| Network           | &gt; 500 mbps                                    |                |
| Network \(1\)     | Google webpass business bay area 1gbps unlimited | \$5500/mo      |
| Network \(2\)     | Hurricane Electric bay area colo 1gbps           | \$500/mo       |

**Table 2** example high-end hardware setup for running a Solana client.

Despite the low-barrier to entry as a validation-client, from a capital investment perspective, as in any developing economy, there will be much opportunity and need for competent validation services as evidenced by node reliability, UX/UI, APIs and other software accessibility tools. Additionally, although Solana’s validator node startup costs are nominal when compared to similar networks, they may still be somewhat restrictive for some potential participants. In the spirit of developing a true decentralized, permissionless network, these interested parties can become involved in the Solana network/economy via delegation of previously acquired tokens with a reliable validation node to earn a portion of the interest generated.

Delegation of tokens to validation-clients provides a way for passive Solana token holders to become part of the active Solana economy and earn interest rates proportional to the interest rate generated by the delegated validation-client. Additionally, this feature intends to create a healthy validation-client market, with potential validation-client nodes competing to build reliable, transparent and profitable delegation services.

================
File: docs/src/implemented-proposals/ed_overview/ed_economic_sustainability.md
================
---
title: Economic Sustainability
---

**Subject to change.**

Long term economic sustainability is one of the guiding principles of Solana’s economic design. While it is impossible to predict how decentralized economies will develop over time, especially economies with flexible decentralized governances, we can arrange economic components such that, under certain conditions, a sustainable economy may take shape in the long term. In the case of Solana’s network, these components take the form of token issuance \(via inflation\) and token burning.

The dominant remittances from the Solana mining pool are validator rewards. The disinflationary mechanism is a flat, protocol-specified and adjusted, % of each transaction fee.

================
File: docs/src/implemented-proposals/ed_overview/ed_mvp.md
================
---
title: Economic Design MVP
---

**Subject to change.**

The preceding sections, outlined in the
[Economic Design Overview](ed_overview.md),
describe a long-term vision of a sustainable Solana economy.
Of course, we don't expect the final implementation to perfectly match what has
been described above. We intend to fully engage with network stakeholders
throughout the implementation phases \(i.e. pre-testnet, testnet, mainnet\)
to ensure the system supports, and is representative of, the various network
participants' interests. The first step toward this goal, however, is outlining
some desired MVP economic features to be available for early pre-testnet and
testnet participants. Below is a rough sketch outlining basic economic
functionality from which a more complete and functional system can be developed.

## MVP Economic Features

- Faucet to deliver testnet SOLs to validators for staking and application development.
- Mechanism by which validators are rewarded via network inflation.
- Ability to delegate tokens to validator nodes
- Validator set commission fees on interest from delegated tokens.

================
File: docs/src/implemented-proposals/ed_overview/ed_overview.md
================
---
title: Cluster Economics
---

**Subject to change.**

Solana’s crypto-economic system is designed to promote a healthy, long term self-sustaining economy with participant incentives aligned to the security and decentralization of the network. The main participants in this economy are validation-clients. Their contributions to the network, state validation, and their requisite incentive mechanisms are discussed below.

The main channels of participant remittances are referred to as protocol-based (inflationary) rewards and transaction fees. Protocol-based rewards are issuances from a global, protocol-defined, inflation rate. These rewards will constitute the total reward delivered to validation clients, the remaining sourced from transaction fees. In the early days of the network, it is likely that protocol-based rewards, deployed based on predefined issuance schedule, will drive the majority of participant incentives to participate in the network.

These protocol-based rewards, to be distributed across the actively staked tokens on the network, are to be a result of a global supply inflation rate, calculated per Solana epoch and distributed amongst the active validator set. As discussed further below, the per annum inflation rate is based on a pre-determined disinflationary schedule. This provides the network with monetary supply predictability which supports long term economic stability and security.

Transaction fees are market-based participant-to-participant transfers, attached to network interactions as a necessary motivation and compensation for the inclusion and execution of a proposed transaction. A mechanism for long-term economic stability and forking protection through partial burning of each transaction fee is also discussed below.

A high-level schematic of Solana’s crypto-economic design is shown below in **Figure 1**. The specifics of validation-client economics are described in sections: [Validation-client Economics](ed_validation_client_economics/ed_vce_overview.md), [Inflation Schedule](ed_validation_client_economics/ed_vce_state_validation_protocol_based_rewards.md), and [Transaction Fees](ed_validation_client_economics/ed_vce_state_validation_transaction_fees.md). Also, the section titled [Validation Stake Delegation](ed_validation_client_economics/ed_vce_validation_stake_delegation.md) closes with a discussion of validator delegation opportunities and marketplace. Additionally, in [Storage Rent Economics](ed_storage_rent_economics.md), we describe an implementation of storage rent to account for the externality costs of maintaining the active state of the ledger. An outline of features for an MVP economic design is discussed in the [Economic Design MVP](ed_mvp.md) section.

![](/img/economic_design_infl_230719.png)

**Figure 1**: Schematic overview of Solana economic incentive design.

================
File: docs/src/implemented-proposals/ed_overview/ed_references.md
================
---
title: References
---

1. [https://blog.ethereum.org/2016/07/27/inflation-transaction-fees-cryptocurrency-monetary-policy/](https://blog.ethereum.org/2016/07/27/inflation-transaction-fees-cryptocurrency-monetary-policy/)
2. [https://medium.com/solana-labs/how-to-create-decentralized-storage-for-a-multi-petabyte-digital-ledger-2499a3a8c281](https://medium.com/solana-labs/how-to-create-decentralized-storage-for-a-multi-petabyte-digital-ledger-2499a3a8c281)
3. [https://medium.com/solana-labs/how-to-create-decentralized-storage-for-a-multi-petabyte-digital-ledger-2499a3a8c281](https://medium.com/solana-labs/how-to-create-decentralized-storage-for-a-multi-petabyte-digital-ledger-2499a3a8c281)

================
File: docs/src/implemented-proposals/ed_overview/ed_storage_rent_economics.md
================
---
title: Storage Rent Economics
---

Each transaction that is submitted to the Solana ledger imposes costs. Transaction fees paid by the submitter, and collected by a validator, in theory, account for the acute, transactional, costs of validating and adding that data to the ledger. Unaccounted in this process is the mid-term storage of active ledger state, necessarily maintained by the rotating validator set. This type of storage imposes costs not only to validators but also to the broader network as active state grows so does data transmission and validation overhead. To account for these costs, we describe here our preliminary design and implementation of storage rent.

Storage rent can be paid via one of two methods:

Method 1: Set it and forget it

With this approach, accounts with two-years worth of rent deposits secured are exempt from network rent charges. By maintaining this minimum-balance, the broader network benefits from reduced liquidity and the account holder can rest assured that their `Account::data` will be retained for continual access/usage.

Method 2: Pay per byte

If an account has less than two-years worth of deposited rent the network charges rent on a per-epoch basis, in credit for the next epoch. This rent is deducted at a rate specified in genesis, in lamports per kilobyte-year.

For information on the technical implementation details of this design, see the [Rent](../rent.md) section.

================
File: docs/src/implemented-proposals/abi-management.md
================
---
title: Solana ABI management process
---

This document proposes the Solana ABI management process. The ABI management
process is an engineering practice and a supporting technical framework to avoid
introducing unintended incompatible ABI changes.

# Problem

The Solana ABI (binary interface to the cluster) is currently only defined
implicitly by the implementation and requires a very careful eye to notice
breaking changes. This makes it extremely difficult to upgrade the software
on an existing cluster without rebooting the ledger.

# Requirements and objectives

- Unintended ABI changes can be detected as CI failures mechanically.
- Newer implementation must be able to process the oldest data (since genesis)
  once we go mainnet.
- The objective of this proposal is to protect the ABI while sustaining rather
  rapid development by opting for a mechanical process rather than a very long
  human-driven auditing process.
- Once signed cryptographically, data blob must be identical, so no
  in-place data format update is possible regardless of inbound and outbound of
  the online system. Also, considering the sheer volume of transactions we're
  aiming to handle, retrospective in-place update is undesirable at best.

# Solution

Instead of natural human's eye due-diligence, which should be assumed to fail
regularly, we need a systematic assurance of not breaking the cluster when
changing the source code.

For that purpose, we introduce a mechanism of marking every ABI-related things
in source code (`struct`s, `enum`s) with the new `#[frozen_abi]` attribute. This
takes hard-coded digest value derived from types of its fields via
`ser::Serialize`. And the attribute automatically generates a unit test to try
to detect any unsanctioned changes to the marked ABI-related things.

However, the detection cannot be complete; no matter how hard we statically
analyze the source code, it's still possible to break ABI. For example, this
includes not-`derive`d hand-written `ser::Serialize`, underlying library's
implementation changes (for example `bincode`), CPU architecture differences.
The detection of these possible ABI incompatibilities is out-of-scope for this
ABI management.

# Definitions

ABI item/type: various types to be used for serialization, which collectively
comprises the whole ABI for any system components. For example, those types
include `struct`s and `enum`s.

ABI item digest: Some fixed hash derived from type information of ABI item's
fields.

# Example

```patch
+#[frozen_abi(digest="eXSMM7b89VY72V...")]
 #[derive(Serialize, Default, Deserialize, Debug, PartialEq, Eq, Clone)]
 pub struct Vote {
     /// A stack of votes starting with the oldest vote
     pub slots: Vec<Slot>,
     /// signature of the bank's state at the last slot
     pub hash: Hash,
 }
```

# Developer's workflow

To know the digest for new ABI items, developers can add `frozen_abi` with a
random digest value and run the unit tests and replace it with the correct
digest from the assertion test error message.

Run unit tests using the following command to generate digest values:
```
SOLANA_ABI_DUMP_DIR=. cargo +nightly test abi
```

In general, once we add `frozen_abi` and its change is published in the stable
release channel, its digest should never change. If such a change is needed, we
should opt for defining a new `struct` like `FooV1`. And special release flow
like hard forks should be approached.

# Implementation remarks

We use some degree of macro machinery to automatically generate unit tests
and calculate a digest from ABI items. This is doable by clever use of
`serde::Serialize` (`[1]`) and `any::type_name` (`[2]`). For a precedent for similar
implementation, `ink` from the Parity Technologies `[3]` could be informational.

# Implementation details

The implementation's goal is to detect unintended ABI changes automatically as
much as possible. To that end, the digest of structural ABI information is
calculated with best-effort accuracy and stability.

When the ABI digest check is run, it dynamically computes an ABI digest by
recursively digesting the ABI of fields of the ABI item, by re-using the
`serde`'s serialization functionality, proc macro and generic specialization.
And then, the check `assert!`s that its finalized digest value is identical as
what is specified in the `frozen_abi` attribute.

To realize that, it creates an example instance of the type and a custom
`Serializer` instance for `serde` to recursively traverse its fields as if
serializing the example for real. This traversing must be done via `serde` to
really capture what kinds of data actually would be serialized by `serde`, even
considering custom non-`derive`d `Serialize` trait implementations.

# The ABI digesting process

This part is a bit complex. There is three inter-depending parts: `AbiExample`,
`AbiDigester` and `AbiEnumVisitor`.

First, the generated test creates an example instance of the digested type with
a trait called `AbiExample`, which should be implemented for all of digested
types like the `Serialize` and return `Self` like the `Default` trait. Usually,
it's provided via generic trait specialization for most of common types. Also
it is possible to `derive` for `struct` and `enum` and can be hand-written if
needed.

The custom `Serializer` is called `AbiDigester`. And when it's called by `serde`
to serialize some data, it recursively collects ABI information as much as
possible. `AbiDigester`'s internal state for the ABI digest is updated
differentially depending on the type of data. This logic is specifically
redirected via with a trait called `AbiEnumVisitor` for each `enum` type. As the
name suggests, there is no need to implement `AbiEnumVisitor` for other types.

To summarize this interplay, `serde` handles the recursive serialization control
flow in tandem with `AbiDigester`. The initial entry point in tests and child
`AbiDigester`s use `AbiExample` recursively to create an example object
hierarchical graph. And `AbiDigester` uses `AbiEnumVisitor` to inquiry the actual
ABI information using the constructed sample.

`Default` isn't enough for `AbiExample`. Various collection's `::default()` is
empty, yet, we want to digest them with actual items. And, ABI digesting can't
be realized only with `AbiEnumVisitor`. `AbiExample` is required because an
actual instance of type is needed to actually traverse the data via `serde`.

On the other hand, ABI digesting can't be done only with `AbiExample`, either.
`AbiEnumVisitor` is required because all variants of an `enum` cannot be
traversed just with a single variant of it as a ABI example.

Digestible information:

- rust's type name
- `serde`'s data type name
- all fields in `struct`
- all variants in `enum`
- `struct`: normal(`struct {...}`) and tuple-style (`struct(...)`)
- `enum`: normal variants and `struct`- and `tuple`- styles.
- attributes: `serde(serialize_with=...)` and `serde(skip)`

Not digestible information:

- Any custom serialize code path not touched by the sample provided by
  `AbiExample`. (technically not possible)
- generics (must be a concrete type; use `frozen_abi` on concrete type
  aliases)

# References

1. [(De)Serialization with type info · Issue #1095 · serde-rs/serde](https://github.com/serde-rs/serde/issues/1095#issuecomment-345483479)
2. [`std::any::type_name` - Rust](https://doc.rust-lang.org/std/any/fn.type_name.html)
3. [Parity's ink to write smart contracts](https://github.com/paritytech/ink)

================
File: docs/src/implemented-proposals/bank-timestamp-correction.md
================
---
title: Bank Timestamp Correction
---

Each Bank has a timestamp that is stashed in the Clock sysvar and used to assess
time-based stake account lockups. However, since genesis, this value has been
based on a theoretical slots-per-second instead of reality, so it's quite
inaccurate. This poses a problem for lockups, since the accounts will not
register as lockup-free on (or anytime near) the date the lockup is set to
expire.

Block times are already being estimated to cache in Blockstore and long-term
storage using a [validator timestamp oracle](validator-timestamp-oracle.md);
this data provides an opportunity to align the bank timestamp more closely with
real-world time.

The general outline of the proposed implementation is as follows:

- Correct each Bank timestamp using the validator-provided timestamp.
- Update the validator-provided timestamp calculation to use a stake-weighted
  median, rather than a stake-weighted mean.
- Bound the timestamp correction so that it cannot deviate too far from the
  expected theoretical estimate

## Timestamp Correction

On every new Bank, the runtime calculates a realistic timestamp estimate using
validator timestamp-oracle data. The Bank timestamp is corrected to this value
if it is greater than or equal to the previous Bank's timestamp. That is, time
should not ever go backward, so that locked up accounts may be released by the
correction, but once released, accounts can never be relocked by a time
correction.

### Calculating Stake-Weighted Median Timestamp

In order to calculate the estimated timestamp for a particular Bank, the runtime
first needs to get the most recent vote timestamps from the active validator
set. The `Bank::vote_accounts()` method provides the vote accounts state, and
these can be filtered to all accounts whose most recent timestamp was provided
within the last epoch.

From each vote timestamp, an estimate for the current Bank is calculated using
the epoch's target ns_per_slot for any delta between the Bank slot and the
timestamp slot. Each timestamp estimate is associated with the stake delegated
to that vote account, and all the timestamps are collected to create a
stake-weighted timestamp distribution.

From this set, the stake-weighted median timestamp -- that is, the timestamp at
which 50% of the stake estimates a greater-or-equal timestamp and 50% of the
stake estimates a lesser-or-equal timestamp -- is selected as the potential
corrected timestamp.

This stake-weighted median timestamp is preferred over the stake-weighted mean
because the multiplication of stake by proposed timestamp in the mean
calculation allows a node with very small stake to still have a large effect on
the resulting timestamp by proposing a timestamp that is very large or very
small. For example, using the previous `calculate_stake_weighted_timestamp()`
method, a node with 0.00003% of the stake proposing a timestamp of `i64::MAX`
can shift the timestamp forward 97k years!

### Bounding Timestamps

In addition to preventing time moving backward, we can prevent malicious
activity by bounding the corrected timestamp to an acceptable level of deviation
from the theoretical expected time.

This proposal suggests that each timestamp be allowed to deviate up to 25% from
the expected time since the start of the epoch.

In order to calculate the timestamp deviation, each Bank needs to log the
`epoch_start_timestamp` in the Clock sysvar. This value is set to the
`Clock::unix_timestamp` on the first slot of each epoch.

Then, the runtime compares the expected elapsed time since the start of the
epoch with the proposed elapsed time based on the corrected timestamp. If the
corrected elapsed time is within +/- 25% of expected, the corrected timestamp is
accepted. Otherwise, it is bounded to the acceptable deviation.

================
File: docs/src/implemented-proposals/commitment.md
================
---
title: Commitment
---

The commitment metric aims to give clients a measure of the network confirmation
and stake levels on a particular block. Clients can then use this information to
derive their own [measures of commitment](../consensus/commitments.md).

# Calculation RPC

Clients can request commitment metrics from a validator for a signature `s`
through `get_block_commitment(s: Signature) -> BlockCommitment` over RPC. The
`BlockCommitment` struct contains an array of u64 `[u64, MAX_CONFIRMATIONS]`. This
array represents the commitment metric for the particular block `N` that
contains the signature `s` as of the last block `M` that the validator voted on.

An entry `s` at index `i` in the `BlockCommitment` array implies that the
validator observed `s` total stake in the cluster reaching `i` confirmations on
block `N` as observed in some block `M`. There will be `MAX_CONFIRMATIONS` elements in
this array, representing all the possible number of confirmations from 1 to
`MAX_CONFIRMATIONS`.

# Computation of commitment metric

Building this `BlockCommitment` struct leverages the computations already being
performed for building consensus. The `collect_vote_lockouts` function in
`consensus.rs` builds a HashMap, where each entry is of the form `(b, s)`
where `s` is the amount of stake on a bank `b`.

This computation is performed on a votable candidate bank `b` as follows.

```text
   let output: HashMap<b, Stake> = HashMap::new();
   for vote_account in b.vote_accounts {
       for v in vote_account.vote_stack {
           for a in ancestors(v) {
               f(*output.get_mut(a), vote_account, v);
           }
       }
   }
```

Where `f` is some accumulation function that modifies the `Stake` entry
for slot `a` with some data derivable from vote `v` and `vote_account`
(stake, lockout, etc.). Note here that the `ancestors` here only includes
slots that are present in the current status cache. Signatures for banks earlier
than those present in the status cache would not be queryable anyway, so those
banks are not included in the commitment calculations here.

Now we can naturally augment the above computation to also build a
`BlockCommitment` array for every bank `b` by:

1. Adding a `ForkCommitmentCache` to collect the `BlockCommitment` structs
2. Replacing `f` with `f'` such that the above computation also builds this
   `BlockCommitment` for every bank `b`.

We will proceed with the details of 2) as 1) is trivial.

Before continuing, it is noteworthy that for some validator's vote account `a`,
the number of local confirmations for that validator on slot `s` is
`v.num_confirmations`, where `v` is the smallest vote in the stack of votes
`a.votes` such that `v.slot >= s` (i.e. there is no need to look at any
votes > v as the number of confirmations will be lower).

Now more specifically, we augment the above computation to:

```text
   let output: HashMap<b, Stake> = HashMap::new();
   let fork_commitment_cache = ForkCommitmentCache::default();
   for vote_account in b.vote_accounts {
       // vote stack is sorted from oldest vote to newest vote
       for (v1, v2) in vote_account.vote_stack.windows(2) {
           for a in ancestors(v1).difference(ancestors(v2)) {
               f'(*output.get_mut(a), *fork_commitment_cache.get_mut(a), vote_account, v);
           }
       }
   }
```

where `f'` is defined as:

```text
    fn f`(
        stake: &mut Stake,
        some_ancestor: &mut BlockCommitment,
        vote_account: VoteAccount,
        v: Vote, total_stake: u64
    ){
        f(stake, vote_account, v);
        *some_ancestor.commitment[v.num_confirmations] += vote_account.stake;
    }
```

================
File: docs/src/implemented-proposals/durable-tx-nonces.md
================
---
title: Durable Transaction Nonces
---

## Problem

To prevent replay, Solana transactions contain a nonce field populated with a
"recent" blockhash value. A transaction containing a blockhash that is too old
(~2min as of this writing) is rejected by the network as invalid. Unfortunately
certain use cases, such as custodial services, require more time to produce a
signature for the transaction. A mechanism is needed to enable these potentially
offline network participants.

## Requirements

1. The transaction's signature needs to cover the nonce value
2. The nonce must not be reusable, even in the case of signing key disclosure

## A Contract-based Solution

Here we describe a contract-based solution to the problem, whereby a client can
"stash" a nonce value for future use in a transaction's `recent_blockhash`
field. This approach is akin to the Compare and Swap atomic instruction,
implemented by some CPU ISAs.

When making use of a durable nonce, the client must first query its value from
account data. A transaction is now constructed in the normal way, but with the
following additional requirements:

1. The durable nonce value is used in the `recent_blockhash` field
2. An `AdvanceNonceAccount` instruction is the first issued in the transaction

### Contract Mechanics

TODO: svgbob this into a flowchart

```text
Start
Create Account
  state = Uninitialized
NonceInstruction
  if state == Uninitialized
    if account.balance < rent_exempt
      error InsufficientFunds
    state = Initialized
  elif state != Initialized
    error BadState
  if sysvar.recent_blockhashes.is_empty()
    error EmptyRecentBlockhashes
  if !sysvar.recent_blockhashes.contains(stored_nonce)
    error NotReady
  stored_hash = sysvar.recent_blockhashes[0]
  success
WithdrawInstruction(to, lamports)
  if state == Uninitialized
    if !signers.contains(owner)
      error MissingRequiredSignatures
  elif state == Initialized
    if !sysvar.recent_blockhashes.contains(stored_nonce)
      error NotReady
    if lamports != account.balance && lamports + rent_exempt > account.balance
      error InsufficientFunds
  account.balance -= lamports
  to.balance += lamports
  success
```

A client wishing to use this feature starts by creating a nonce account under
the system program. This account will be in the `Uninitialized` state with no
stored hash, and thus unusable.

To initialize a newly created account, an `InitializeNonceAccount` instruction must be
issued. This instruction takes one parameter, the `Pubkey` of the account's
[authority](../cli/examples/durable-nonce.md#nonce-authority). Nonce accounts
must be [rent-exempt](./rent.md#two-tiered-rent-regime) to meet the data-persistence
requirements of the feature, and as such, require that sufficient lamports be
deposited before they can be initialized. Upon successful initialization, the
cluster's most recent blockhash is stored along with specified nonce authority
`Pubkey`.

The `AdvanceNonceAccount` instruction is used to manage the account's stored nonce
value. It stores the cluster's most recent blockhash in the account's state data,
failing if that matches the value already stored there. This check prevents
replaying transactions within the same block.

Due to nonce accounts' [rent-exempt](./rent.md#two-tiered-rent-regime) requirement,
a custom withdraw instruction is used to move funds out of the account.
The `WithdrawNonceAccount` instruction takes a single argument, lamports to withdraw,
and enforces rent-exemption by preventing the account's balance from falling
below the rent-exempt minimum. An exception to this check is if the final balance
would be zero lamports, which makes the account eligible for deletion. This
account closure detail has an additional requirement that the stored nonce value
must not match the cluster's most recent blockhash, as per `AdvanceNonceAccount`.

The account's [nonce authority](../cli/examples/durable-nonce.md#nonce-authority)
can be changed using the `AuthorizeNonceAccount` instruction. It takes one parameter,
the `Pubkey` of the new authority. Executing this instruction grants full
control over the account and its balance to the new authority.

> `AdvanceNonceAccount`, `WithdrawNonceAccount` and `AuthorizeNonceAccount` all require the current [nonce authority](../cli/examples/durable-nonce.md#nonce-authority) for the account to sign the transaction.

### Runtime Support

The contract alone is not sufficient for implementing this feature. To enforce
an extant `recent_blockhash` on the transaction and prevent fee theft via
failed transaction replay, runtime modifications are necessary.

Any transaction failing the usual `check_hash_age` validation will be tested
for a Durable Transaction Nonce. This is signaled by including a `AdvanceNonceAccount`
instruction as the first instruction in the transaction.

If the runtime determines that a Durable Transaction Nonce is in use, it will
take the following additional actions to validate the transaction:

1. The `NonceAccount` specified in the `Nonce` instruction is loaded.
2. The `NonceState` is deserialized from the `NonceAccount`'s data field and
   confirmed to be in the `Initialized` state.
3. The nonce value stored in the `NonceAccount` is tested to match against the
   one specified in the transaction's `recent_blockhash` field.

If all three of the above checks succeed, the transaction is allowed to continue
validation.

Since transactions that fail with an `InstructionError` are charged a fee and
changes to their state rolled back, there is an opportunity for fee theft if an
`AdvanceNonceAccount` instruction is reverted. A malicious validator could replay the
failed transaction until the stored nonce is successfully advanced. Runtime
changes prevent this behavior. When a durable nonce transaction fails with an
`InstructionError` aside from the `AdvanceNonceAccount` instruction, the nonce account
is rolled back to its pre-execution state as usual. Then the runtime advances
its nonce value and the advanced nonce account stored as if it succeeded.

================
File: docs/src/implemented-proposals/epoch_accounts_hash.md
================
---
title: Epoch Accounts Hash
---

*Paraphrasing from https://github.com/solana-labs/solana/issues/26847*

## Background

Rent collection checks every account at least once per epoch.  At each slot, a
deterministic set of accounts (based on the pubkey range) is loaded, checked
for rent collection, and stored back to the Accounts DB at this new slot.

Accounts are stored (rewritten) _even if_ they are unchanged.  This has a few
positive effects.
  1. Once an account is rewritten, the previous version at an older slot is now
     dead. As entire slots and AppendVecs become full of only dead accounts,
     they can then be dropped/recycled.
  2. Each account rewritten due to rent collection is included in that slot's
     bank hash.  Since the bank hash is part of what is voted on for consensus,
     this means every account is verified by the network at least once per
     epoch.

However, there is a big downside to rewriting unchanged accounts: performance.
Storing accounts can be very expensive.  And since accounts now are required to
be rent-exempt, the majority of accounts rewritten due to rent collection are
unchanged.  What if unchanged accounts were no longer rewritten? This would
minimally be a big performance win.


## Problem

If rent collection no longer is rewriting unchanged accounts, we lose the two
positive effects.  Dealing with Positive Effect 1 (from above) will be handled
by _Ancient AppendVecs_, and will not be discussed here.  So how do we still
get the security from Positive Effect 2?  How can we still verify every account
at least once per epoch, *as part of consensus*, but without rewriting
accounts?


## Proposed Solution

Perform a full accounts hash calculation once per epoch, and hash the result
into a bank's `hash`.  This will be known as the _Epoch Accounts Hash_, or
_EAH_.

This retains the Positive Effect 2 from rent collection by checking every
account at least once per epoch.  Thus, any validators with missing, corrupt,
or extra accounts will identify those issues within 1-2 epochs.


### Implementation

Performing a full accounts hash takes a relatively long time to complete.  For
this reason, the EAH calculation must take place in the background.

In order for all the validators to calculate the same accounts hash, the
calculation must be based on the same view of all accounts.  This means the EAH
must be based on a predetermined slot.  This will be known as the `start slot`.
The `start slot` is calculated as an offset into the epoch.  This offset will
be known as the `start offset`.  Formally, the `start slot` is the first root
*greater-than-or-equal-to* `first slot in epoch + start offset`.

Similarly, all the validators must save the EAH into a bank at a predetermined
slot, and offset from the first slot of an epoch.  This will be known as the
`stop slot` and `stop offset`, respectively.

* The `start offset` will be set at one-quarter into the epoch.
* The `stop offset` will be set at three-quarters into the epoch.
* For epochs with 432,000 slots, the `start offset` will be 108,000 and the
  `stop offset` will be 324,000.

These constants may be changed in the future, or may be determined at runtime.
The main justifications for these values are:
1. Do not start the EAH calculation at the beginning of an epoch, as the
   beginning of an epoch is already a time of contention and stress.  There is
   no reason to make this worse.
2. The bank to save the EAH into—the `stop offset`—should be sufficiently far
   in the future to guarantee all validators are able to complete the accounts
   hash calculation in time.
3. The `start offset` should be *after* the `rewarding interval`
   (from [Partitioned Inflationary Rewards Distribution](https://github.com/solana-labs/solana/pull/27455)).
   This ensures stake rewards have been distributed and stored into the
   accounts for this epoch.

Once the EAH calculation is complete, it must be saved somewhere.  Since this
occurs in the background, there is not an associated `Bank` that would make
sense to save into.  Instead, a new field will be added to `AccountsDb` that
will store the EAH.  Later, the bank at slot `stop slot`†¹ will read the EAH from
`AccountsDb` and hash it into its own hash (aka _bank hash_).

EAH calculation will use the existing _accounts background services_ (_ABS_) to
perform the actual calculation.  Requests for EAH calculation will be sent from
`bank_forks::set_root()`†², with a new request type to distinguish an EAH request
from a Snapshot request.  Since the EAH will be part of consensus, it is not
optional; EAH requests will have the highest priority in ABS, and will be
processed first/instead of other requests.

†¹: More precisely, all banks where `bank slot >= stop slot` and `parent slot <
    stop slot` will include the EAH in their _bank hash_.  This ensures EAH
    handles forking around `stop slot`, since only one of these banks will end
    up rooted.

†²: An EAH calculation will be requested when `root bank slot >= start slot`
    and `root parent slot < start slot`.  This handles the scenario where
    validators call `bank_forks::set_root()` at different intervals.


#### Details

#### Snapshots

A snapshot contains all the state necessary to reconstruct the cluster as of a
certain slot.  A snapshot may then need to contain the EAH so that the `stop
slot` can include it.  Consider the following scenarios within an epoch where a
snapshot is requested for slot `X`:


##### 1. `X >= first slot in epoch` and `X < start slot`

Since the `start slot` has not been reached yet, there is nothing special to do
in order to take a snapshot in this scenario.


##### 2. `X == start slot`

The EAH *must* be included in the snapshot.  Since the snapshot process always
calculates the accounts hash, no additional calculations are required.  The
accounts hash calculation result will be used both to store in the snapshot as
the EAH, and for the snapshot hash (which is used at load-time for verification).


##### 3. `X > start slot` and `X < stop slot`

If a snapshot is requested to be created *after* the `start slot` but *before*
the EAH calculation has completed, then it will be impossible to create a
snapshot with the correct EAH.  The snapshot process will wait until the EAH
calculation has completed before proceeding.


##### 4. `X == stop slot`

The EAH has been calculated for this epoch, and has been included in the `stop
slot` bank.  No further handling is required; the snapshot does not need to
contain the EAH.


##### 5. `X > stop slot` and `X <= last slot in epoch`

Same as (4).


#### Snapshot Verification

If a snapshot archive includes an EAH, we want to verify the EAH is correct at
load time (instead of waiting until `stop slot`, which could be far in the
future).

If the snapshot archive is for a slot within the `calculation window`†¹, then it
*must* include an EAH.  The snapshot hash itself will now also incorporate the
EAH.  In pseudo code:
```pseudo
if slot is in calculation window
    let snapshot hash = hash(accounts hash, epoch accounts hash)
else
    let snapshot hash = accounts hash
endif
```
Since loading from a snapshot archive already verifies the snapshot archive's
hash against the deserialized bank, the EAH will be implicitly verified as
well.

†¹: The `calculation window` is `[start slot, stop slot)`, based on the epoch
    of the referenced `Bank`.


#### Corner Cases

#### Minimum Slots per Epoch

An EAH is requested by `BankForks::set_root()`, which happens while setting
*roots*.  The EAH is stored into `Bank`s when they are *frozen*.  Banks are
frozen *at least* 32 slots before they are rooted.  For the expected behavior,
the EAH start slot really should be 32 slots before the stop slot. If the
number of slots per epoch is small, this can result in surprising behavior.

Example: Assume there are 40 slots per epoch.  The EAH start offset is 10, and
the EAH stop offset is 30.  When Bank 30 is frozen it will include the EAH in
its hash.  However, Bank 10 has not yet been rooted, so a new EAH has not been
calculated for this epoch.  This means Bank 30 will have included the EAH *from
the previous epoch* in its hash.

Later, when Bank 10 is rooted, it will request a new EAH be calculated.  If a
snapshot is taken for Bank 12 (or any bank between 10 and 30), it will include
the EAH *from this epoch*.  If a node loads the snapshot from Bank 12, once it
gets to freezing Bank 30, it will end up with a different bank hash since it
included the EAH from this epoch (versus the other node's Bank 30 included the
EAH from the previous epoch).  Different bank hashes will result in consensus
failures.

The above example is clearly bad.  It can be observed that short epochs only
occur (1) during warmup, or (2) in tests.  Real clusters have much longer
epochs (432,000 slots by default).

Tests can be fixed as needed; that leaves fixing warmup.  Since warmup is
transient, we disable EAH until slots-per-epoch is large enough.  More
precisely, we disable EAH until the `calculation window` is big enough.  During
warmup, slots-per-epoch doubles each epoch until reaching the desired number,
so only a few epochs will skip EAH (which also is a small total number of
slots).


#### Warping

Warping introduces corner cases into EAH because many slots may be skipped,
including the entire range of `start slot` to `stop slot`.

Now, a new EAH will always be calculated based on the parent.  Thus, if the
warp slot is `stop slot` or greater (and the parent slot is less than `stop
slot`), the warp bank will include this newly-calculated EAH.  This is safe
because warping cannot be used on a live cluster; only for a new cluster or
tests/debugging.  Therefore _when_ the EAH was calculated is not germane.

For specific examples, refer to Appendix A.


#### Implementation Alternatives

##### Perform the EAH calculation in the foreground

The accounts hash calculation takes around 15 seconds (median, on Mainnet-Beta
today).  This is far beyond the slot time; this would be bad UX, and also
decrease network stability.


##### Remove `stop offset`

Instead of having two offsets—one for `start` and one for `stop`—use a single
offset for both.  This delays when the EAH is saved into a Bank and voted on.
The saved EAH is now the EAH from the previous epoch.  This could work; would
reduce the number of "special" slots from two to one.  No significant
advantages observed.


##### Send EAH requests when making a new bank, instead of a new root

When a bank is created, we don't yet know if it will be finalized until it is
rooted, which could result in multiple EAH requests due to forking.  This would
be bad for performance.


### Appendix A: All Warping Scenarios

To enumerate how EAH interacts with warping, refer to the following diagram for
the scenarios below:

```text
  +---------+-----------------+-----------+---------+-----------------+-----------+
  |         >                 <           |         >                 <           |
  |    A    >     B           <     C     |    D    >      E          <     F     |
  |         >                 <           |         >                 <           |
  +---------+-----------------+-----------+---------+-----------------+-----------+
  |         |                 |           |         |                 |           |
  v         v                 v           v         v                 v           v
  epoch 1   start slot 1      stop slot 1 epoch 2   start slot 2      stop slot 2 epoch 3
```


#### parent slot: `A`, warp slot: `A`

No slots important to the EAH have been skipped, so no change in behavior.


#### parent slot: `A`, warp slot: `B`

An EAH calculation will be requested when the warp slot is rooted, and then
will be included in the Bank at `slot slot 1`; behavior is unchanged.


#### parent slot: `A`, warp slot: `C` or `D`

The entire EAH range has been skipped; no new EAH calculation will have been
requested for `start slot 1`.  The warp slot will include the EAH calculated at
the parent slot.  This is different from the normal behavior.


#### parent slot: `A`, warp slot: `E`

Similar to `A -> C`, the warp slot will include the EAH calculated at the
parent slot.  This behavior appears different.

Then when the warp slot is rooted, a new EAH calculation will be requested,
which will be included in `stop slot 2`.  This behavior is expected.


#### parent slot: `A`, warp slot: `F`

Similar to `A -> C`, no new EAH calculation will be requested.  The warp slot
will include the EAH calculated at the parent slot.  This is different from the
normal behavior.


#### parent slot: `B`, warp slot: `B`

A new EAH will be calculated at parent slot, which will be included in `stop
slot 1`, _not_ the previously-calculated EAH from `start slot 1`.  This behavior
is different.


#### parent slot: `B`, warp slot: `C` or `D`

The warp slot will include the EAH calculated at the parent slot, _not_ the
previously-calculated EAH from `start slot 1`.  This is different from normal
behavior.


#### parent slot: `B`, warp slot: `E`

Similar to `B -> C`, the warp slot will include the EAH calculated at the parent
slot, _not_ the previously-calculated EAH from `start slot 1`.  This is
different from normal behavior.

And similar to `A -> E`, when the warp slot is rooted, a new EAH calculation
will be requested, which will be included in `stop slot 2`.  This behavior is
expected.


#### parent slot: `B`, warp slot: `F`

Similar to `B -> C`, the warp slot will include the EAH calculated at the parent
slot, _not_ the previously-calculated EAH from `start slot 1`.  This is
different from normal behavior.

================
File: docs/src/implemented-proposals/index.md
================
---
title: Implemented Design Proposals
sidebar_position: 1
sidebar_label: Overview
---

These architectural proposals have been accepted and implemented by the Solana maintainers. Any designs that may be subject to future change are noted in the specific proposal page.

## Not Yet Implemented

Design proposals that have been accepted but not yet implemented are found in [Accepted Proposals](../proposals/accepted-design-proposals.md).

## Submit a New Proposal

To submit a new design proposal, consult this guide on [how to submit a design proposal](../proposals.md#submit-a-design-proposal).

================
File: docs/src/implemented-proposals/installer.md
================
---
title: Cluster Software Installation and Updates
---

Currently users are required to build the solana cluster software themselves from the git repository and manually update
it, which is error prone and inconvenient.

This document proposes an easy to use software install and updater that can be used to deploy pre-built binaries for
supported platforms. Users may elect to use binaries supplied by Solana or any other party provider. Deployment of
updates is managed using an on-chain update manifest program.

## Motivating Examples

### Fetch and run a pre-built installer using a bootstrap curl/shell script

The easiest install method for supported platforms:

```bash
$ curl -sSf https://raw.githubusercontent.com/jito-foundation/jito-solana/v1.0.0/install/agave-install-init.sh | sh
```

This script will check github for the latest tagged release and download and run the `agave-install-init` binary from
there.

If additional arguments need to be specified during the installation, the following shell syntax is used:

```bash
$ init_args=.... # arguments for `agave-install-init ...`
$ curl -sSf https://raw.githubusercontent.com/jito-foundation/jito-solana/v1.0.0/install/agave-install-init.sh | sh -s - ${init_args}
```

### Fetch and run a pre-built installer from a Github release

With a well-known release URL, a pre-built binary can be obtained for supported platforms:

```bash
$ curl -o agave-install-init https://github.com/jito-foundation/jito-solana/releases/download/v1.0.0/agave-install-init-x86_64-apple-darwin
$ chmod +x ./agave-install-init
$ ./agave-install-init --help
```

### Build and run the installer from source

If a pre-built binary is not available for a given platform, building the installer from source is always an option:

```bash
$ git clone https://github.com/jito-foundation/jito-solana.git
$ cd jito-solana/install
$ cargo run -- --help
```

### Deploy a new update to a cluster

Given a solana release tarball \(as created by `ci/publish-tarball.sh`\) that has already been uploaded to a publicly
accessible URL, the following commands will deploy the update:

```bash
$ solana-keygen new -o update-manifest.json  # <-- only generated once, the public key is shared with users
$ agave-install deploy http://example.com/path/to/solana-release.tar.bz2 update-manifest.json
```

### Run a validator node that auto updates itself

```bash
$ agave-install init --pubkey 92DMonmBYXwEMHJ99c9ceRSpAmk9v6i3RdvDdXaVcrfj  # <-- pubkey is obtained from whoever is deploying the updates
$ export PATH=~/.local/share/agave-install/bin:$PATH
$ solana-keygen ...  # <-- runs the latest solana-keygen
$ agave-install run agave-validator ...  # <-- runs a validator, restarting it as necessary when an update is applied
```

## On-chain Update Manifest

An update manifest is used to advertise the deployment of new release tarballs on a solana cluster. The update manifest
is stored using the `config` program, and each update manifest account describes a logical update channel for a given
target triple \(eg, `x86_64-apple-darwin`\). The account public key is well-known between the entity deploying new
updates and users consuming those updates.

The update tarball itself is hosted elsewhere, off-chain and can be fetched from the specified `download_url`.

```text
use solana_signature::Signature;

/// Information required to download and apply a given update
pub struct UpdateManifest {
    pub timestamp_secs: u64, // When the release was deployed in seconds since UNIX EPOCH
    pub download_url: String, // Download URL to the release tar.bz2
    pub download_sha256: String, // SHA256 digest of the release tar.bz2 file
}

/// Data of an Update Manifest program Account.
#[derive(Serialize, Deserialize, Default, Debug, PartialEq)]
pub struct SignedUpdateManifest {
    pub manifest: UpdateManifest,
    pub manifest_signature: Signature,
}
```

Note that the `manifest` field itself contains a corresponding signature \(`manifest_signature`\) to guard against
man-in-the-middle attacks between the `agave-install` tool and the solana cluster RPC API.

To guard against rollback attacks, `agave-install` will refuse to install an update with an older `timestamp_secs` than
what is currently installed.

## Release Archive Contents

A release archive is expected to be a tar file compressed with bzip2 with the following internal structure:

- `/version.yml` - a simple YAML file containing the field `"target"` - the

  target tuple. Any additional fields are ignored.

- `/bin/` -- directory containing available programs in the release.

  `agave-install` will symlink this directory to

  `~/.local/share/agave-install/bin` for use by the `PATH` environment

  variable.

- `...` -- any additional files and directories are permitted

## agave-install Tool

The `agave-install` tool is used by the user to install and update their cluster software.

:::info
As of v3.0 `agave-install` does not install the `agave-validator` binary, which is required to run a validator node.
Validator operators are required to [build from source](../cli/install.md#build-from-source).

:::

It manages the following files and directories in the user's home directory:

<<<<<<< HEAD
- `~/.config/solana/install/config.yml` - user configuration and information about the currently installed software version
- `~/.local/share/solana/install/bin` - a symlink to the current release. eg, `~/.local/share/solana-update/<update-pubkey>-<manifest_signature>/bin`
=======
- `~/.config/solana/install/config.yml` - user configuration and information about currently installed software version
- `~/.local/share/solana/install/bin` - a symlink to the current release.
  eg, `~/.local/share/solana-update/<update-pubkey>-<manifest_signature>/bin`
>>>>>>> 042f9c17ab (Jito Patch)
- `~/.local/share/solana/install/releases/<download_sha256>/` - contents of a release

### Command-line Interface

```text
agave-install 0.16.0
The solana cluster software installer

USAGE:
    agave-install [OPTIONS] <SUBCOMMAND>

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
    -c, --config <PATH>    Configuration file to use [default: .../Library/Preferences/solana/install.yml]

SUBCOMMANDS:
    deploy    deploys a new update
    help      Prints this message or the help of the given subcommand(s)
    info      displays information about the current installation
    init      initializes a new installation
    run       Runs a program while periodically checking and applying software updates
    update    checks for an update, and if available downloads and applies it
```

```text
agave-install-init
initializes a new installation

USAGE:
    agave-install init [OPTIONS]

FLAGS:
    -h, --help    Prints help information

OPTIONS:
    -d, --data_dir <PATH>    Directory to store install data [default: .../Library/Application Support/solana]
    -u, --url <URL>          JSON RPC URL for the solana cluster [default: http://api.devnet.solana.com]
    -p, --pubkey <PUBKEY>    Public key of the update manifest [default: 9XX329sPuskWhH4DQh6k16c87dHKhXLBZTL3Gxmve8Gp]
```

```text
agave-install info
displays information about the current installation

USAGE:
    agave-install info [FLAGS]

FLAGS:
    -h, --help     Prints help information
    -l, --local    only display local information, don't check the cluster for new updates
```

```text
agave-install deploy
deploys a new update

USAGE:
    agave-install deploy <download_url> <update_manifest_keypair>

FLAGS:
    -h, --help    Prints help information

ARGS:
    <download_url>               URL to the solana release archive
    <update_manifest_keypair>    Keypair file for the update manifest (/path/to/keypair.json)
```

```text
agave-install update
checks for an update, and if available downloads and applies it

USAGE:
    agave-install update

FLAGS:
    -h, --help    Prints help information
```

```text
agave-install run
Runs a program while periodically checking and applying software updates

USAGE:
    agave-install run <program_name> [program_arguments]...

FLAGS:
    -h, --help    Prints help information

ARGS:
    <program_name>            program to run
    <program_arguments>...    arguments to supply to the program

The program will be restarted upon a successful software update
```

================
File: docs/src/implemented-proposals/instruction_introspection.md
================
---
title: instruction introspection
---

## Problem

Some smart contract programs may want to verify that another Instruction is present in a
given Message since that Instruction could be performing a verification of certain data,
in a precompiled function. (See secp256k1_instruction for an example).

## Solution

Add a new sysvar Sysvar1nstructions1111111111111111111111111 that a program can reference
and received the Message's instruction data inside, and also the index of the current instruction.

Two helper functions to extract this data can be used:

```
fn load_current_index_checked(instruction_data: &[u8]) -> u16;
fn load_instruction_at_checked(instruction_index: usize, instruction_sysvar_account_info: &AccountInfo) -> Result<Instruction>;
```

The runtime will recognize this special instruction, serialize the Message instruction data
for it and also write the current instruction index and then the bpf program can extract the
necessary information from there.

Note: custom serialization of instructions is used because bincode is about 10x slower
in native code and exceeds current SBF instruction limits.

================
File: docs/src/implemented-proposals/leader-leader-transition.md
================
---
title: Leader-to-Leader Transition
---

This design describes how leaders transition production of the PoH ledger between each other as each leader generates its own slot.

## Challenges

Current leader and the next leader are both racing to generate the final tick for the current slot. The next leader may arrive at that slot while still processing the current leader's entries.

The ideal scenario would be that the next leader generated its own slot right after it was able to vote for the current leader. It is very likely that the next leader will arrive at their PoH slot height before the current leader finishes broadcasting the entire block.

The next leader has to make the decision of attaching its own block to the last completed block, or wait to finalize the pending block. It is possible that the next leader will produce a block that proposes that the current leader failed, even though the rest of the network observes that block succeeding.

The current leader has incentives to start its slot as early as possible to capture economic rewards. Those incentives need to be balanced by the leader's need to attach its block to a block that has the most commitment from the rest of the network.

## Leader timeout

While a leader is actively receiving entries for the previous slot, the leader can delay broadcasting the start of its block in real time. The delay is locally configurable by each leader, and can be dynamically based on the previous leader's behavior. If the previous leader's block is confirmed by the leader's TVU before the timeout, the PoH is reset to the start of the slot and this leader produces its block immediately.

The downsides:

- Leader delays its own slot, potentially allowing the next leader more time to

  catch up.

The upsides compared to guards:

- All the space in a block is used for entries.
- The timeout is not fixed.
- The timeout is local to the leader, and therefore can be clever. The leader's heuristic can take into account turbine performance.
- This design doesn't require a ledger hard fork to update.
- The previous leader can redundantly transmit the last entry in the block to the next leader, and the next leader can speculatively decide to trust it to generate its block without verification of the previous block.
- The leader can speculatively generate the last tick from the last received entry.
- The leader can speculatively process transactions and guess which ones are not going to be encoded by the previous leader. This is also a censorship attack vector. The current leader may withhold transactions that it receives from the clients so it can encode them into its own slot. Once processed, entries can be replayed into PoH quickly.

## Alternative design options

### Guard tick at the end of the slot

A leader does not produce entries in its block after the _penultimate tick_, which is the last tick before the first tick of the next slot. The network votes on the _last tick_, so the time difference between the _penultimate tick_ and the _last tick_ is the forced delay for the entire network, as well as the next leader before a new slot can be generated. The network can produce the _last tick_ from the _penultimate tick_.

If the next leader receives the _penultimate tick_ before it produces its own _first tick_, it will reset its PoH and produce the _first tick_ from the previous leader's _penultimate tick_. The rest of the network will also reset its PoH to produce the _last tick_ as the id to vote on.

The downsides:

- Every vote, and therefore confirmation, is delayed by a fixed timeout. 1 tick, or around 100ms.
- Average case confirmation time for a transaction would be at least 50ms worse.
- It is part of the ledger definition, so to change this behavior would require a hard fork.
- Not all the available space is used for entries.

The upsides compared to leader timeout:

- The next leader has received all the previous entries, so it can start processing transactions without recording them into PoH.
- The previous leader can redundantly transmit the last entry containing the _penultimate tick_ to the next leader. The next leader can speculatively generate the _last tick_ as soon as it receives the _penultimate tick_, even before verifying it.

================
File: docs/src/implemented-proposals/leader-validator-transition.md
================
---
title: Leader-to-Validator Transition
---

A validator typically spends its time validating blocks. If, however, a staker
delegates its stake to a validator, it will occasionally be selected as a _slot
leader_. As a slot leader, the validator is responsible for producing blocks
during an assigned _slot_. A slot has a duration of some number of preconfigured
_ticks_. The duration of those ticks are estimated with a _PoH Recorder_
described later in this document.

## BankFork

BankFork tracks changes to the bank state over a specific slot. Once the final
tick has been registered the state is frozen. Any attempts to write to are
rejected.

## Validator

A validator operates on many different concurrent forks of the bank state until
it generates a PoH hash with a height within its leader slot.

## Slot Leader

A slot leader builds blocks on top of only one fork, the one it last voted on.

## PoH Recorder

Slot leaders and validators use a PoH Recorder for both estimating slot height
and for recording transactions.

### PoH Recorder when Validating

The PoH Recorder acts as a simple VDF when validating. It tells the validator
when it needs to switch to the slot leader role. Every time the validator votes
on a fork, it should use the fork's latest
[blockhash](https://solana.com/docs/terminology#blockhash) to re-seed the VDF.
Re-seeding solves two problems. First, it synchronizes its VDF to the leader's,
allowing it to more accurately determine when its leader slot begins. Second, if
the previous leader goes down, all wallclock time is accounted for in the next
leader's PoH stream. For example, if one block is missing when the leader
starts, the block it produces should have a PoH duration of two blocks. The
longer duration ensures the following leader isn't attempting to snip all the
transactions from the previous leader's slot.

### PoH Recorder when Leading

A slot leader use the PoH Recorder to record transactions, locking their
positions in time. The PoH hash must be derived from a previous leader's last
block. If it isn't, its block will fail PoH verification and be rejected by the
cluster.

The PoH Recorder also serves to inform the slot leader when its slot is over.
The leader needs to take care not to modify its bank if recording the
transaction would generate a PoH height outside its designated slot. The leader,
therefore, should not commit account changes until after it generates the
entry's PoH hash. When the PoH height falls outside its slot any transactions in
its pipeline may be dropped or forwarded to the next leader. Forwarding is
preferred, as it would minimize network congestion, allowing the cluster to
advertise higher TPS capacity.

## Validator Loop

The PoH Recorder manages the transition between modes. Once a ledger is
replayed, the validator can run until the recorder indicates it should be the
slot leader. As a slot leader, the node can then execute and record
transactions.

The loop is synchronized to PoH and does a synchronous start and stop of the
slot leader functionality. After stopping, the validator's TVU should find
itself in the same state as if a different leader had sent it the same block.
The following is pseudocode for the loop:

1. Query the LeaderScheduler for the next assigned slot.
2. Run the TVU over all the forks. 1. TVU will send votes to what it believes is
   the "best" fork. 2. After each vote, restart the PoH Recorder to run until
   the next assigned slot.
3. When time to be a slot leader, start the TPU. Point it to the last fork the
   TVU voted on.
4. Produce entries until the end of the slot. 1. For the duration of the slot,
   the TVU must not vote on other forks. 2. After the slot ends, the TPU freezes
   its BankFork. After freezing, the TVU may resume voting.
5. Goto 1.

================
File: docs/src/implemented-proposals/persistent-account-storage.md
================
---
title: Persistent Account Storage
---

## Persistent Account Storage

The set of accounts represent the current computed state of all the transactions that have been processed by a validator. Each validator needs to maintain this entire set. Each block that is proposed by the network represents a change to this set, and since each block is a potential rollback point, the changes need to be reversible.

Persistent storage like NVMEs are 20 to 40 times cheaper than DDR. The problem with persistent storage is that write and read performance is much slower than DDR. Care must be taken in how data is read or written to. Both reads and writes can be split between multiple storage drives and accessed in parallel. This design proposes a data structure that allows for concurrent reads and concurrent writes of storage. Writes are optimized by using an AppendVec data structure, which allows a single writer to append while allowing access to many concurrent readers. The accounts index maintains a pointer to a spot where the account was appended to every fork, thus removing the need for explicit checkpointing of state.

## AppendVec

AppendVec is a data structure that allows for random reads concurrent with a single append-only writer. Growing or resizing the capacity of the AppendVec requires exclusive access. This is implemented with an atomic `offset`, which is updated at the end of a completed append.

The underlying memory for an AppendVec is a memory-mapped file. Memory-mapped files allow for fast random access and paging is handled by the OS.

## Account Index

The account index is designed to support a single index for all the currently forked Accounts.

```text
type AccountsFileId = usize;

type Fork = u64;

struct AccountMap(Hashmap<Fork, (AccountsFileId, u64)>);

type AccountIndex = HashMap<Pubkey, AccountMap>;
```

The index is a map of account Pubkeys to a map of Forks and the location of the Account data in an AppendVec. To get the version of an account for a specific Fork:

```text
/// Load the account for the pubkey.
/// This function will load the account from the specified fork, falling back to the fork's parents
/// * fork - a virtual Accounts instance, keyed by Fork.  Accounts keep track of their parents with Forks,
///       the persistent store
/// * pubkey - The Account's public key.
pub fn load_slow(&self, id: Fork, pubkey: &Pubkey) -> Option<&Account>
```

The read is satisfied by pointing to a memory-mapped location in the `AccountsFileId` at the stored offset. A reference can be returned without a copy.

### Root Forks

[Tower BFT](tower-bft.md) eventually selects a fork as a root fork and the fork is squashed. A squashed/root fork cannot be rolled back.

When a fork is squashed, all accounts in its parents not already present in the fork are pulled up into the fork by updating the indexes. Accounts with zero balance in the squashed fork are removed from fork by updating the indexes.

An account can be _garbage-collected_ when squashing makes it unreachable.

Three possible options exist:

- Maintain a HashSet of root forks. One is expected to be created every second. The entire tree can be garbage-collected later. Alternatively, if every fork keeps a reference count of accounts, garbage collection could occur any time an index location is updated.
- Remove any pruned forks from the index. Any remaining forks lower in number than the root are can be considered root.
- Scan the index, migrate any old roots into the new one. Any remaining forks lower than the new root can be deleted later.

## Garbage collection

As accounts get updated, they move to the end of the AppendVec. Once capacity has run out, a new AppendVec can be created and updates can be stored there. Eventually references to an older AppendVec will disappear because all the accounts have been updated, and the old AppendVec can be deleted.

To speed up this process, it's possible to move Accounts that have not been recently updated to the front of a new AppendVec. This form of garbage collection can be done without requiring exclusive locks to any of the data structures except for the index update.

The initial implementation for garbage collection is that once all the accounts in an AppendVec become stale versions, it gets reused. The accounts are not updated or moved around once appended.

## Index Recovery

Each bank thread has exclusive access to the accounts during append, since the accounts locks cannot be released until the data is committed. But there is no explicit order of writes between the separate AppendVec files. To create an ordering, the index maintains an atomic write version counter. Each append to the AppendVec records the index write version number for that append in the entry for the Account in the AppendVec.

To recover the index, all the AppendVec files can be read in any order, and the latest write version for every fork should be stored in the index.

## Snapshots

To snapshot, the underlying memory-mapped files in the AppendVec need to be flushed to disk. The index can be written out to disk as well.

## Performance

- Append-only writes are fast. SSDs and NVMEs, as well as all the OS level kernel data structures, allow for appends to run as fast as PCI or NVMe bandwidth will allow \(2,700 MB/s\).
- Each replay and banking thread writes concurrently to its own AppendVec.
- Each AppendVec could potentially be hosted on a separate NVMe.
- Each replay and banking thread has concurrent read access to all the AppendVecs without blocking writes.
- Index requires an exclusive write lock for writes. Single-thread performance for HashMap updates is on the order of 10m per second.
- Banking and Replay stages should use 32 threads per NVMe. NVMes have optimal performance with 32 concurrent readers or writers.

================
File: docs/src/implemented-proposals/readonly-accounts.md
================
---
title: Read-Only Accounts
---

This design covers the handling of readonly and writable accounts in the [runtime](../validator/runtime.md). Multiple transactions that modify the same account must be processed serially so that they are always replayed in the same order. Otherwise, this could introduce non-determinism to the ledger. Some transactions, however, only need to read, and not modify, the data in particular accounts. Multiple transactions that only read the same account can be processed in parallel, since replay order does not matter, providing a performance benefit.

In order to identify readonly accounts, the transaction MessageHeader structure contains `num_readonly_signed_accounts` and `num_readonly_unsigned_accounts`. Instruction `program_ids` are included in the account vector as readonly, unsigned accounts, since executable accounts likewise cannot be modified during instruction processing.

## Runtime handling

Runtime transaction processing rules need to be updated slightly. Programs still can't write or spend accounts that they do not own. But new runtime rules ensure that readonly accounts cannot be modified, even by the programs that own them.

Readonly accounts have the following property:

- Read-only access to all account fields, including lamports (cannot be credited or debited), and account data

Instructions that credit, debit, or modify the readonly account will fail.

## Account Lock Optimizations

The Accounts module keeps track of current locked accounts in the runtime, which separates readonly accounts from the writable accounts. The default account lock gives an account the "writable" designation, and can only be accessed by one processing thread at one time. Readonly accounts are locked by a separate mechanism, allowing for parallel reads.

Although not yet implemented, readonly accounts could be cached in memory and shared between all the threads executing transactions. An ideal design would hold this cache while a readonly account is referenced by any transaction moving through the runtime, and release the cache when the last transaction exits the runtime.

Readonly accounts could also be passed into the processor as references, saving an extra copy.

================
File: docs/src/implemented-proposals/reliable-vote-transmission.md
================
---
title: Reliable Vote Transmission
---

Validator votes are messages that have a critical function for consensus and continuous operation of the network. Therefore it is critical that they are reliably delivered and encoded into the ledger.

## Challenges

1. Leader rotation is triggered by PoH, which is clock with high drift. So many nodes are likely to have an incorrect view if the next leader is active in realtime or not.
2. The next leader may be easily be flooded. Thus a DDOS would not only prevent delivery of regular transactions, but also consensus messages.
3. UDP is unreliable, and our asynchronous protocol requires any message that is transmitted to be retransmitted until it is observed in the ledger. Retransmission could potentially cause an unintentional _thundering herd_ against the leader with a large number of validators. Worst case flood would be `(num_nodes * num_retransmits)`.
4. Tracking if the vote has been transmitted or not via the ledger does not guarantee it will appear in a confirmed block. The current observed block may be unrolled. Validators would need to maintain state for each vote and fork.

## Design

1. Send votes as a push message through gossip. This ensures delivery of the vote to all the next leaders, not just the next future one.
2. Leaders will read the Crds table for new votes and encode any new received votes into the blocks they propose. This allows for validator votes to be included in rollback forks by all the future leaders.
3. Validators that receive votes in the ledger will add them to their local crds table, not as a push request, but simply add them to the table. This shortcuts the push message protocol, so the validation messages do not need to be retransmitted twice around the network.
4. CrdsValue for vote should look like this `Votes(Vec<Transaction>)`

Each vote transaction should maintain a `wallclock` in its data. The merge strategy for Votes will keep the last N set of votes as configured by the local client. For push/pull the vector is traversed recursively and each Transaction is treated as an individual CrdsValue with its own local wallclock and signature.

Gossip is designed for efficient propagation of state. Messages that are sent through gossip-push are batched and propagated with a minimum spanning tree to the rest of the network. Any partial failures in the tree are actively repaired with the gossip-pull protocol while minimizing the amount of data transferred between any nodes.

## How this design solves the Challenges

1. Because there is no easy way for validators to be in sync with leaders on the leader's "active" state, gossip allows for eventual delivery regardless of that state.
2. Gossip will deliver the messages to all the subsequent leaders, so if the current leader is flooded the next leader would have already received these votes and is able to encode them.
3. Gossip minimizes the number of requests through the network by maintaining an efficient spanning tree, and using bloom filters to repair state. So retransmit back-off is not necessary and messages are batched.
4. Leaders that read the crds table for votes will encode all the new valid votes that appear in the table. Even if this leader's block is unrolled, the next leader will try to add the same votes without any additional work done by the validator. Thus ensuring not only eventual delivery, but eventual encoding into the ledger.

## Performance

1. Worst case propagation time to the next leader is Log\(N\) hops with a base depending on the fanout. With our current default fanout of 6, it is about 6 hops to 20k nodes.
2. The leader should receive 20k validation votes aggregated by gossip-push into MTU-sized shreds. Which would reduce the number of packets for 20k network to 80 shreds.
3. Each validators votes is replicated across the entire network. To maintain a queue of 5 previous votes the Crds table would grow by 25 megabytes. `(20,000 nodes * 256 bytes * 5)`.

## Two step implementation rollout

Initially the network can perform reliably with just 1 vote transmitted and maintained through the network with the current Vote implementation. For small networks a fanout of 6 is sufficient. With small network the memory and push overhead is minor.

### Sub 1k validator network

1. Crds just maintains the validators latest vote.
2. Votes are pushed and retransmitted regardless if they are appearing in the ledger.
3. Fanout of 6.
4. Worst case 256kb memory overhead per node.
5. Worst case 4 hops to propagate to every node.
6. Leader should receive the entire validator vote set in 4 push message shreds.

### Sub 20k network

Everything above plus the following:

1. CRDS table maintains a vector of 5 latest validator votes.
2. Votes encode a wallclock. CrdsValue::Votes is a type that recurses into the transaction vector for all the gossip protocols.
3. Increase fanout to 20.
4. Worst case 25mb memory overhead per node.
5. Sub 4 hops worst case to deliver to the entire network.
6. 80 shreds received by the leader for all the validator messages.

================
File: docs/src/implemented-proposals/rent.md
================
---
title: Rent
---

Accounts on Solana may have owner-controlled state \(`Account::data`\) that's separate from the account's balance \(`Account::lamports`\). Since validators on the network need to maintain a working copy of this state in memory, the network charges a time-and-space based fee for this resource consumption, also known as Rent.

## Two-tiered rent regime

Accounts which maintain a minimum balance equivalent to 2 years of rent payments are exempt. The _2 years_ is drawn from the fact hardware cost drops by 50% in price every 2 years and the resulting convergence due to being a geometric series. Accounts whose balance falls below this threshold are charged rent at a rate specified in genesis, in lamports per byte-year. The network charges rent on a per-epoch basis, in credit for the next epoch, and `Account::rent_epoch` keeps track of the next time rent should be collected from the account.

Currently, the rent cost is fixed at the genesis. However, it's anticipated to be dynamic, reflecting the underlying hardware storage cost at the time. So the price is generally expected to decrease as the hardware cost declines as the technology advances.

## Timings of collecting rent

There are two timings of collecting rent from accounts: \(1\) when referenced by a transaction, \(2\) periodically once an epoch. \(1\) includes the transaction to create the new account itself, and it happens during the normal transaction processing by the bank as part of the load phase. \(2\) exists to ensure to collect rents from stale accounts, which aren't referenced in recent epochs at all. \(2\) requires the whole scan of accounts and is spread over an epoch based on account address prefix to avoid load spikes due to this rent collection.

On the contrary, rent collection isn't applied to accounts that are directly manipulated by any of protocol-level bookkeeping processes including:

- The distribution of rent collection itself (Otherwise, it may cause recursive rent collection handling)
- The distribution of staking rewards at the start of every epoch (To reduce as much as processing spike at the start of new epoch)
- The distribution of transaction fee at the end of every slot

Even if those processes are out of scope of rent collection, all of manipulated accounts will eventually be handled by the \(2\) mechanism.

## Actual processing of collecting rent

Rent is due for one epoch's worth of time, and accounts have `Account::rent_epoch` of `current_epoch` or `current_epoch + 1` depending on the rent regime.

If the account is in the exempt regime, `Account::rent_epoch` is simply updated to `current_epoch`.

If the account is non-exempt, the difference between the next epoch and `Account::rent_epoch` is used to calculate the amount of rent owed by this account \(via `Rent::due()`\). Any fractional lamports of the calculation are truncated. Rent due is deducted from `Account::lamports` and `Account::rent_epoch` is updated to `current_epoch + 1` (= next epoch). If the amount of rent due is less than one lamport, no changes are made to the account.

Accounts whose balance is insufficient to satisfy the rent that would be due simply fail to load.

A percentage of the rent collected is destroyed. The rest is distributed to validator accounts by stake weight, a la transaction fees, at the end of every slot.

Finally, rent collection happens according to the protocol-level account updates like the rent distribution to validators, meaning there is no corresponding transaction for rent deductions. So, rent collection is rather invisible, only implicitly observable by a recent transaction or predetermined timing given its account address prefix.

## Design considerations

### Current design rationale

Under the preceding design, it is NOT possible to have accounts that linger, never get touched, and never have to pay rent. Accounts always pay rent exactly once for each epoch, except rent-exempt, sysvar and executable accounts.

This is an intended design choice. Otherwise, it would be possible to trigger unauthorized rent collection with `Noop` instruction by anyone who may unfairly profit from the rent (a leader at the moment) or save the rent given anticipated fluctuating rent cost.

As another side-effect of this choice, also note that this periodic rent collection effectively forces validators not to store stale accounts into a cold storage optimistically and save the storage cost, which is unfavorable for account owners and may cause transactions on them to stall longer than others. On the flip side, this prevents malicious users from creating significant numbers of garbage accounts, burdening validators.

As the overall consequence of this design, all accounts are stored equally as a validator's working set with the same performance characteristics, reflecting the uniform rent pricing structure.

### Ad-hoc collection

Collecting rent on an as-needed basis \(i.e. whenever accounts were loaded/accessed\) was considered. The issues with such an approach are:

- accounts loaded as "credit only" for a transaction could very reasonably be expected to have rent due,

  but would not be writable during any such transaction

- a mechanism to "beat the bushes" \(i.e. go find accounts that need to pay rent\) is desirable,

  lest accounts that are loaded infrequently get a free ride

### System instruction for collecting rent

Collecting rent via a system instruction was considered, as it would naturally have distributed rent to active and stake-weighted nodes and could have been done incrementally. However:

- it would have adversely affected network throughput
- it would require special-casing by the runtime, as accounts with non-SystemProgram owners may be debited by this instruction
- someone would have to issue the transactions

================
File: docs/src/implemented-proposals/repair-service.md
================
---
title: Repair Service
---

## Repair Service

The RepairService is in charge of retrieving missing shreds that failed to be
delivered by primary communication protocols like Turbine. It is in charge of
managing the protocols described below in the `Repair Protocols` section below.

## Challenges:

1\) Validators can fail to receive particular shreds due to network failures

2\) Consider a scenario where blockstore contains the set of slots {1, 3, 5}.
Then Blockstore receives shreds for some slot 7, where for each of the shreds
b, b.parent == 6, so then the parent-child relation 6 -&gt; 7 is stored in
blockstore. However, there is no way to chain these slots to any of the
existing banks in Blockstore, and thus the `Shred Repair` protocol will not
repair these slots. If these slots happen to be part of the main chain, this
will halt replay progress on this node.

## Repair-related primitives

Epoch Slots:
Each validator advertises separately on gossip the various parts of an
`Epoch Slots`:

- The `stash`: An epoch-long compressed set of all completed slots.
- The `cache`: The Run-length Encoding (RLE) of the latest `N` completed
  slots starting from some slot `M`, where `N` is the number of slots
  that will fit in an MTU-sized packet.

`Epoch Slots` in gossip are updated every time a validator receives a
complete slot within the epoch. Completed slots are detected by blockstore
and sent over a channel to RepairService. It is important to note that we
know that by the time a slot `X` is complete, the epoch schedule must exist
for the epoch that contains slot `X` because WindowService will reject
shreds for unconfirmed epochs.

Every `N/2` completed slots, the oldest `N/2` slots are moved from the
`cache` into the `stash`. The base value `M` for the RLE should also
be updated.

## Repair Request Protocols

The repair protocol makes best attempts to progress the forking structure of
Blockstore.

The different protocol strategies to address the above challenges:

1. Shred Repair \(Addresses Challenge \#1\): This is the most basic repair
   protocol, with the purpose of detecting and filling "holes" in the ledger.
   Blockstore tracks the latest root slot. RepairService will then periodically
   iterate every fork in blockstore starting from the root slot, sending repair
   requests to validators for any missing shreds. It will send at most some `N`
   repair requests per iteration. Shred repair should prioritize repairing
   forks based on the leader's fork weight. Validators should only send repair
   requests to validators who have marked that slot as completed in their
   EpochSlots. Validators should prioritize repairing shreds in each slot
   that they are responsible for retransmitting through turbine. Validators can
   compute which shreds they are responsible for retransmitting because the
   seed for turbine is based on leader id, slot, and shred index.

   Note: Validators will only accept shreds within the current verifiable
   epoch \(epoch the validator has a leader schedule for\).

2. Preemptive Slot Repair \(Addresses Challenge \#2\): The goal of this
   protocol is to discover the chaining relationship of "orphan" slots that do not
   currently chain to any known fork. Shred repair should prioritize repairing
   orphan slots based on the leader's fork weight.

   - Blockstore will track the set of "orphan" slots in a separate column family.
   - RepairService will periodically make `Orphan` requests for each of
     the orphans in blockstore.

     `Orphan(orphan)` request - `orphan` is the orphan slot that the
     requestor wants to know the parents of `Orphan(orphan)` response -
     The highest shreds for each of the first `N` parents of the requested
     `orphan`

     On receiving the responses `p`, where `p` is some shred in a parent slot,
     validators will:

     - Insert an empty `SlotMeta` in blockstore for `p.slot` if it doesn't
       already exist.
     - If `p.slot` does exist, update the parent of `p` based on `parents`

     Note: that once these empty slots are added to blockstore, the
     `Shred Repair` protocol should attempt to fill those slots.

     Note: Validators will only accept responses containing shreds within the
     current verifiable epoch \(epoch the validator has a leader schedule
     for\).

Validators should try to send orphan requests to validators who have marked that
orphan as completed in their EpochSlots. If no such validators exist, then
randomly select a validator in a stake-weighted fashion.

## Repair Response Protocol

When a validator receives a request for a shred `S`, they respond with the
shred if they have it.

When a validator receives a shred through a repair response, they check
`EpochSlots` to see if <= `1/3` of the network has marked this slot as
completed. If so, they resubmit this shred through its associated turbine
path, but only if this validator has not retransmitted this shred before.

================
File: docs/src/implemented-proposals/rpc-transaction-history.md
================
# Long term RPC Transaction History

There's a need for RPC to serve at least 6 months of transaction history. The
current history, on the order of days, is insufficient for downstream users.

6 months of transaction data cannot be stored practically in a validator's
rocksdb ledger so an external data store is necessary. The validator's rocksdb
ledger will continue to serve as the primary data source, and then will fall
back to the external data store.

The affected RPC endpoints are:

- [getFirstAvailableBlock](https://solana.com/docs/rpc/http/getfirstavailableblock)
- [getConfirmedBlock](https://solana.com/docs/rpc/deprecated/getconfirmedblock)
- [getConfirmedBlocks](https://solana.com/docs/rpc/deprecated/getconfirmedblocks)
- [getConfirmedSignaturesForAddress](https://solana.com/docs/rpc/http/getconfirmedsignaturesforaddress)
- [getConfirmedTransaction](https://solana.com/docs/rpc/deprecated/getConfirmedTransaction)
- [getSignatureStatuses](https://solana.com/docs/rpc/http/getsignaturestatuses)
- [getBlockTime](https://solana.com/docs/rpc/http/getblocktime)

Some system design constraints:

- The volume of data to store and search can quickly jump into the terabytes,
  and is immutable.
- The system should be as light as possible for SREs. For example an SQL
  database cluster that requires an SRE to continually monitor and rebalance
  nodes is undesirable.
- Data must be searchable in real time - batched queries that take minutes or
  hours to run are unacceptable.
- Easy to replicate the data worldwide to co-locate it with the RPC endpoints
  that will utilize it.
- Interfacing with the external data store should be easy and not require
  depending on risky lightly-used community-supported code libraries

Based on these constraints, Google's BigTable product is selected as the data
store.

## Table Schema

A BigTable instance is used to hold all transaction data, broken up into
different tables for quick searching.

New data may be copied into the instance at anytime without affecting the
existing data, and all data is immutable. Generally the expectation is that new
data will be uploaded once a current epoch completes but there is no limitation
on the frequency of data dumps.

Cleanup of old data is automatic by configuring the data retention policy of the
instance tables appropriately, it just disappears. Therefore the order of when
data is added becomes important. For example if data from epoch N-1 is added
after data from epoch N, the older epoch data will outlive the newer data.
However beyond producing _holes_ in query results, this kind of unordered
deletion will have no ill effect. Note that this method of cleanup effectively
allows for an unlimited amount of transaction data to be stored, restricted only
by the monetary costs of doing so.

The table layout s supports the existing RPC endpoints only. New RPC endpoints
in the future may require additions to the schema and potentially iterating over
all transactions to build up the necessary metadata.

## Accessing BigTable

BigTable has a gRPC endpoint that can be accessed using the
[tonic](https://crates.io/crates/tonic) and the raw protobuf API, as currently
no higher-level Rust crate for BigTable exists. Practically this makes parsing
the results of BigTable queries more complicated but is not a significant issue.

## Data Population

The ongoing population of instance data will occur on an epoch cadence through
the use of a new `agave-ledger-tool` command that will convert rocksdb data for
a given slot range into the instance schema.

The same process will be run once, manually, to backfill the existing ledger
data.

### Block Table: `block`

This table contains the compressed block data for a given slot.

The row key is generated by taking the 16 digit lower case hexadecimal
representation of the slot, to ensure that the oldest slot with a confirmed
block will always be first when the rows are listed. eg, The row key for slot 42
would be 000000000000002a.

The row data is a compressed `StoredConfirmedBlock` struct.

### Account Address Transaction Signature Lookup Table: `tx-by-addr`

This table contains the transactions that affect a given address.

The row key is
`<base58 address>/<slot-id-one's-compliment-hex-slot-0-prefixed-to-16-digits>`.
The row data is a compressed `TransactionByAddrInfo` struct.

Taking the one's compliment of the slot allows for listing of slots ensures that
the newest slot with transactions that affect an address will always be listed
first.

Sysvar addresses are not indexed. However frequently used programs such as Vote
or System are, and will likely have a row for every confirmed slot.

### Transaction Signature Lookup Table: `tx`

This table maps a transaction signature to its confirmed block, and index within
that block.

The row key is the base58-encoded transaction signature.
The row data is a compressed `TransactionInfo` struct.

### Entries Table: `entries`

> Support for the `entries` table was added in v1.18.0.

This table contains data about the entries in a slot.

The row key is the same as a `block` row key.

The row data is a compressed `Entries` struct, which is a list of entry-summary
data, including hash, number of hashes since previous entry, number of
transactions, and starting transaction index.

================
File: docs/src/implemented-proposals/snapshot-verification.md
================
---
title: Snapshot Verification
---

## Problem

When a validator boots up from a snapshot, it needs a way to verify the account set matches what the rest of the network sees quickly. A potential
attacker could give the validator an incorrect state, and then try to convince it to accept a transaction that would otherwise be rejected.

## Solution

Currently the bank hash is derived from hashing the delta state of the accounts in a slot which is then combined with the previous bank hash value.
The problem with this is that the list of hashes will grow on the order of the number of slots processed by the chain and become a burden to both
transmit and verify successfully.

Another naive method could be to create a merkle tree of the account state. This has the downside that with each account update, the merkle tree
would have to be recomputed from the entire account state of all live accounts in the system.

To verify the snapshot, we do the following:

On account store of non-zero lamport accounts, we hash the following data:

- Account owner
- Account data
- Account pubkey
- Account lamports balance
- Fork the account is stored on

Use this resulting hash value as input to an expansion function which expands the hash value into an image value.
The function will create a 440 byte block of data where the first 32 bytes are the hash value, and the next 440 - 32 bytes are
generated from a Chacha RNG with the hash as the seed.

The account images are then combined with xor. The previous account value will be xored into the state and the new account value also xored into the state.

Voting and sysvar hash values occur with the hash of the resulting full image value.

On validator boot, when it loads from a snapshot, it would verify the hash value with the accounts set. It would then
use SPV to display the percentage of the network that voted for the hash value given.

The resulting value can be verified by a validator to be the result of xoring all current account states together.

A snapshot must be purged of zero lamport accounts before creation and during verify since the zero lamport accounts do not affect the hash value but may cause
a validator bank to read that an account is not present when it really should be.

An attack on the xor state could be made to influence its value:

Thus the 440 byte image size comes from this paper, avoiding xor collision with 0 \(or thus any other given bit pattern\): \[[https://link.springer.com/content/pdf/10.1007%2F3-540-45708-9_19.pdf](https://link.springer.com/content/pdf/10.1007%2F3-540-45708-9_19.pdf)\]

The math provides 128 bit security in this case:

```text
O(k * 2^(n/(1+lg(k)))
k=2^40 accounts
n=440
2^(40) * 2^(448 * 8 / 41) ~= O(2^(128))
```

================
File: docs/src/implemented-proposals/staking-rewards.md
================
---
title: Staking Rewards
---

A Proof of Stake \(PoS\), \(i.e. using in-protocol asset, SOL, to provide secure consensus\) design is outlined here. Solana implements a proof of stake reward/security scheme for validator nodes in the cluster. The purpose is threefold:

- Align validator incentives with that of the greater cluster through skin-in-the-game deposits at risk

- Avoid 'nothing at stake' fork voting issues by implementing slashing rules aimed at promoting fork convergence

- Provide an avenue for validator rewards provided as a function of validator participation in the cluster.

While many of the details of the specific implementation are currently under consideration and are expected to come into focus through specific modeling studies and parameter exploration on the Solana testnet, we outline here our current thinking on the main components of the PoS system. Much of this thinking is based on the current status of Casper FFG, with optimizations and specific attributes to be modified as is allowed by Solana's Proof of History \(PoH\) blockchain data structure.

## General Overview

Solana's ledger validation design is based on a rotating, stake-weighted selected leader broadcasting transactions in a PoH data structure to validating nodes. These nodes, upon receiving the leader's broadcast, have the opportunity to vote on the current state and PoH height by signing a transaction into the PoH stream.

To become a Solana validator, one must deposit/lock-up some amount of SOL in a contract. This SOL will not be accessible for a specific time period. The precise duration of the staking lockup period has not been determined. However we can consider three phases of this time for which specific parameters will be necessary:

- _Warm-up period_: which SOL is deposited and inaccessible to the node, however PoH transaction validation has not begun. Most likely on the order of days to weeks

- _Validation period_: a minimum duration for which the deposited SOL will be inaccessible, at risk of slashing \(see slashing rules below\) and earning rewards for the validator participation. Likely duration of months to a year.

- _Cool-down period_: a duration of time following the submission of a 'withdrawal' transaction. During this period validation responsibilities have been removed and the funds continue to be inaccessible. Accumulated rewards should be delivered at the end of this period, along with the return of the initial deposit.

Solana's trustless sense of time and ordering provided by its PoH data structure, along with its [turbine](https://docs.anza.xyz/consensus/turbine-block-propagation) data broadcast and transmission design, should provide sub-second transaction confirmation times that scale with the log of the number of nodes in the cluster. This means we shouldn't have to restrict the number of validating nodes with a prohibitive 'minimum deposits' and expect nodes to be able to become validators with nominal amounts of SOL staked. At the same time, Solana's focus on high-throughput should create incentive for validation clients to provide high-performant and reliable hardware. Combined with potentially a minimum network speed threshold to join as a validation-client, we expect a healthy validation delegation market to emerge.

## Penalties

As discussed in the [Economic Design](ed_overview/ed_overview.md) section, annual validator interest rates are to be specified as a function of total percentage of circulating supply that has been staked. The cluster rewards validators who are online and actively participating in the validation process throughout the entirety of their _validation period_. For validators that go offline/fail to validate transactions during this period, their annual reward is effectively reduced.

Similarly, we may consider an algorithmic reduction in a validator's active staked amount in the case that they are offline. I.e. if a validator is inactive for some amount of time, either due to a partition or otherwise, the amount of their stake that is considered ‘active’ \(eligible to earn rewards\) may be reduced. This design would be structured to help long-lived partitions to eventually reach finality on their respective chains as the % of non-voting total stake is reduced over time until a supermajority can be achieved by the active validators in each partition. Similarly, upon re-engaging, the ‘active’ amount staked will come back online at some defined rate. Different rates of stake reduction may be considered depending on the size of the partition/active set.

================
File: docs/src/implemented-proposals/testing-programs.md
================
---
title: Testing Programs
---

Applications send transactions to a Solana cluster and query validators to confirm the transactions were processed and to check each transaction's result. When the cluster doesn't behave as anticipated, it could be for a number of reasons:

- The program is buggy
- The BPF loader rejected an unsafe program instruction
- The transaction was too big
- The transaction was invalid
- The Runtime tried to execute the transaction when another one was accessing

  the same account

- The network dropped the transaction
- The cluster rolled back the ledger
- A validator responded to queries maliciously

## The AsyncClient and SyncClient Traits

To troubleshoot, the application should retarget a lower-level component, where fewer errors are possible. Retargeting can be done with different implementations of the AsyncClient and SyncClient traits.

Components implement the following primary methods:

```text
trait AsyncClient {
    fn async_send_transaction(&self, transaction: Transaction) -> io::Result<Signature>;
}

trait SyncClient {
    fn get_signature_status(&self, signature: &Signature) -> Result<Option<transaction::Result<()>>>;
}
```

Users send transactions and asynchronously and synchronously await results.

### TpuClient for the TPU

The next level is the TPU implementation, which is not yet implemented. At the TPU level, the application sends transactions over Rust channels, where there can be no surprises from network queues or dropped packets. The TPU implements all "normal" transaction errors. It does signature verification, may report account-in-use errors, and otherwise results in the ledger, complete with proof of history hashes.

## Low-level testing

### BankClient for the Bank

Below the TPU level is the Bank. The Bank doesn't do signature verification or generate a ledger. The Bank is a convenient layer at which to test new on-chain programs. It allows developers to toggle between native program implementations and BPF-compiled variants. No need for the Transact trait here. The Bank's API is synchronous.

## Unit-testing with the Runtime

Below the Bank is the Runtime. The Runtime is the ideal test environment for unit-testing. By statically linking the Runtime into a native program implementation, the developer gains the shortest possible edit-compile-run loop. Without any dynamic linking, stack traces include debug symbols and program errors are straightforward to troubleshoot.

================
File: docs/src/implemented-proposals/tower-bft.md
================
---
title: Tower BFT
---

This design describes Solana's _Tower BFT_ algorithm. It addresses the following problems:

- Some forks may not end up accepted by the supermajority of the cluster, and voters need to recover from voting on such forks.
- Many forks may be votable by different voters, and each voter may see a different set of votable forks. The selected forks should eventually converge for the cluster.
- Reward based votes have an associated risk. Voters should have the ability to configure how much risk they take on.
- The [cost of rollback](tower-bft.md#cost-of-rollback) needs to be computable. It is important to clients that rely on some measurable form of Consistency. The costs to break consistency need to be computable, and increase super-linearly for older votes.
- ASIC speeds are different between nodes, and attackers could employ Proof of History ASICS that are much faster than the rest of the cluster. Consensus needs to be resistant to attacks that exploit the variability in Proof of History ASIC speed.

For brevity this design assumes that a single voter with a stake is deployed as an individual validator in the cluster.

## Time

The Solana cluster generates a source of time via a Verifiable Delay Function we are calling [Proof of History](../consensus/synchronization.md).

The unit of time is called a "slot". Each slot has a designated leader that can
produce a block `B`. The `slot` of block `B` is designated `slot(B)`. A leader
does not necessarily need to generate a block for its slot, in which case there
may not be blocks for some slots.

For more details, see [fork generation](../consensus/fork-generation.md) and [leader rotation](../consensus/leader-rotation.md).

## Votes

Validators communicate which fork they think is the heaviest through votes.
Each vote `v` is signed by the validator that produces it, and is of the form `(i, B)`, where `i` is the public key of the validator producing the vote and `B` is a hash identifying the block being voted for.

## Lockouts

Making votes on a particular fork incurs a lockout on that particular fork. A lockout is a designated period of time, measured in slots, within which a validator cannot vote on another fork. The purpose of the lockout is to force a
validator to commit opportunity cost to a specific fork. Lockouts are measured
in slots, and therefore represent a real-time forced delay that a validator
needs to wait before breaking the commitment to a fork.

Validators that violate the lockouts and vote for a diverging fork within the lockout should be punished. The proposed punishment is to slash the validator stake if a concurrent vote within a lockout for a non-descendant fork can be proven to the cluster.

## Algorithm

The basic idea to this approach is to stack consensus votes and double lockouts. Each vote in the stack is a confirmation of a fork. Each confirmed fork is an ancestor of the fork above it. Each vote has a `lockout` in units of slots before the validator can submit a vote that does not contain the confirmed fork as an ancestor.

We call this stack the Vote Tower.

When a vote is added to the tower, the lockouts of all the previous votes in the tower are doubled (more on this in [Vote Tower](#vote-tower)). With each new vote, a validator commits the previous votes to an ever-increasing lockout. At 32 votes we can consider the vote to be at `max lockout` any votes with a lockout equal to or above `1<<32` are dequeued \(FIFO\). Dequeuing a vote is the trigger for a reward. If the vote on the top of the tower expires before it is dequeued, it and subsequent expired votes are popped in a LIFO fashion from the vote tower. The validator needs to start rebuilding the tower from that point.

### Vote Tower

Before a vote is pushed to the tower, all the votes leading up to vote with a lower lock expiration slot than the new vote are popped. After rollback
lockouts are not doubled until the validator catches up to the rollback height of votes.

For example, a vote tower with the following state:

| vote | vote slot | lockout | lock expiration slot |
| ---: | --------: | ------: | -------------------: |
|    4 |         4 |       2 |                    6 |
|    3 |         3 |       4 |                    7 |
|    2 |         2 |       8 |                   10 |
|    1 |         1 |      16 |                   17 |

_Vote 5_ is at slot 9, and the resulting state is

| vote | vote slot | lockout | lock expiration slot |
| ---: | --------: | ------: | -------------------: |
|    5 |         9 |       2 |                   11 |
|    2 |         2 |       8 |                   10 |
|    1 |         1 |      16 |                   17 |

_Vote 6_ is at slot 10

| vote | vote slot | lockout | lock expiration slot |
| ---: | --------: | ------: | -------------------: |
|    6 |        10 |       2 |                   12 |
|    5 |         9 |       4 |                   13 |
|    2 |         2 |       8 |                   10 |
|    1 |         1 |      16 |                   17 |

At slot 10 the new votes caught up to the previous votes. When _vote 7_ at slot 11 is applied we scan top down to pop expired votes. Although _vote 2_ has expired, since _vote 6_ has not expired, we do not continue scanning. Finally we have reached a new stack depth, lockouts are doubled

| vote | vote slot | lockout | lock expiration slot |
| ---: | --------: | ------: | -------------------: |
|    7 |        11 |       2 |                   13 |
|    6 |        10 |       4 |                   14 |
|    5 |         9 |       8 |                   17 |
|    2 |         2 |      16 |                   18 |
|    1 |         1 |      32 |                   33 |

Finally we have _vote 8_ at slot 18, this leads to the expiry of _vote 7_, _vote 6_, and _vote 5_.

| vote | vote slot | lockout | lock expiration slot |
| ---: | --------: | ------: | -------------------: |
|    8 |        18 |       2 |                   20 |
|    2 |         2 |      16 |                   18 |
|    1 |         1 |      32 |                   33 |

### Cost of Rollback

Cost of rollback of _fork A_ is defined as the cost in terms of lockout time to the validator to confirm any other fork that does not include _fork A_ as an ancestor.

The **Economic Finality** of _fork A_ can be calculated as the loss of all the rewards from rollback of _fork A_ and its descendants, plus the opportunity cost of reward due to the exponentially growing lockout of the votes that have confirmed _fork A_.

### Threshold Check
In order to prevent a validator from locking itself out on the wrong fork
in the case of a partition, there also needs to be a check to ensure that the rest of the cluster is committing to the same fork. This check is called the
"threshold check", and is outlined as follows.

In deciding whether to vote for a block `B`:

1. Simulate a vote for `B` on your current tower
2. Simulate popping off all the votes that would be expired by `B`
3. Now index every vote in the tower from `[0, tower.length()]`, assuming that the most recent simulated vote `B` is index 0, the second most recent vote is index 1, etc.
4. Let `T` be the vote in the tower with index equal to `threshold_check_depth`, currently hardcoded to `8`.
5. Check all the blocks descended from `T`. Let `Votes` be the set of all votes in these blocks for `T` or any descendants `D_n` of `T`. Let `V` be the set of all validators that have made a vote in `V`. If the sum of the validators' stakes in `V` totals `>= 2/3` of the stake of the network, then we commit a vote to `T`.

### Algorithm parameters

The following parameters need to be tuned:

- Number of votes in the stack before dequeue occurs \(32\).
- Rate of growth for lockouts in the stack \(2x\).
- Starting default lockout \(2\).
- Threshold check depth for minimum cluster commitment before committing to the fork \(8\).
- Minimum cluster commitment size at threshold depth \(50%+\).

### Fork Choice

Fork choice is how each validator determines which fork to vote on when multiple
concurrent forks exist. Forks are weighted based on the latest votes made by the validator set, and individual validators then vote on the "heaviest"
such fork.

Given the view of a single validator `i`:

Let `V` be the set of "most recent" valid votes received by `i`, i.e., `v = (j, B)` is in `V` and `i` has not also received a vote of the form `(j, B′) `such that `slot(B′) > slot(B)`.

Now the algorithm proceeds as follows:

1. For each vote `(j, B)` in `V`, add the stake of `j` to `B` and all of its
ancestors.
2. Now Set `B` to be the rooted block. Set `finish := 0`.
3. Perform the following loop:

```
*While* `finish == 0`
*Do*:
    *If*: `i` has received no children of `B` then set `finish := 1` and return
    `B`.
    *Else*: Let `B′` be the child of `B` (amongst those received by `i`) with
    most the most stake-weighted votes in `V`, breaking ties by the smallest
    slot. Set `B` equal to `B'`.
```

### Voting Algorithm

Each validator maintains a vote tower `T` which follows the rules described above in [Vote Tower](#vote-tower), which is a sequence of blocks it has voted for (initially empty). The variable `l` records the length of the stack. For each entry in the tower, denoted by `B = T(x)` for `x < l` where `B` is the `xth` entry in the tower, we record also a value `confcount(B)`. Define the lock expiration slot `lockexp(B) := slot(B) + 2 ^ confcount(B)`.

The validator `i` runs a voting loop as follows. Let `B` be the heaviest
block returned by the fork choice rule above [Fork Choice](#fork-choice). If `i` has not voted for `B` before, then `i` votes for `B` so long as the following conditions are satisfied:

1. Respecting lockouts: For any block `B′` in the tower that is not an ancestor of `B`, `lockexp(B′) ≤ slot(B)`.
2. Threshold check: Described above in [Threshold Check](#threshold-check)
3. Switching threshold: Have sufficiently many votes on other forks if switching forks. Let `Btop` denote the block at the top of the stack. If `Btop` is not an ancestor of `B`, then:
    - Let `VBtop ⊆ V` be the set of votes on `Btop` or ancestors or descendents of `Btop`.
    - We need `|V \ VBtop | > 38%`. More details on this can be found in [Optimistic Confirmation](../proposals/optimistic_confirmation.md)

If all the conditions are satisfied and validator `i` votes for block `B` then it adjusts its tower as follows (same rules described above in [Vote Tower](#vote-tower)).
1. Remove expired blocks top down. Let `x := l - 1`. While `x >= 0 && lockexp(T(x)) < slot(B)`, remove `T(x)` from the tower, and set `l := l - 1` and `x := x - 1`.
2. Add block to tower. `T(l) := B`, `confcount(B) := 1`, and set `l := l + 1`.
3. Double lockouts. For each element `B = T(x)` if `l > x + confcount(B)`, then `confcount(B) := confcount(B) + 1`.

## PoH ASIC Resistance

Votes and lockouts grow exponentially while ASIC speed up is linear. There are possible attack vectors involving a faster ASIC outlined below.

### ASIC Rollback

An attacker generates a concurrent fork from an older block to try to rollback the cluster. In this attack the concurrent fork is competing with forks that have already been voted on. This attack is limited by the exponential growth of the lockouts.

- 1 vote has a lockout of 2 slots. Concurrent fork must be at least 2 slots ahead, and be produced in 1 slot. Therefore requires an ASIC 2x faster.
- 2 votes have a lockout of 4 slots. Concurrent fork must be at least 4 slots ahead and produced in 2 slots. Therefore requires an ASIC 2x faster.
- 3 votes have a lockout of 8 slots. Concurrent fork must be at least 8 slots ahead and produced in 3 slots. Therefore requires an ASIC 2.6x faster.
- 10 votes have a lockout of 1024 slots. 1024/10, or 102.4x faster ASIC.
- 20 votes have a lockout of 2^20 slots. 2^20/20, or 52,428.8x faster ASIC.

================
File: docs/src/implemented-proposals/transaction-fees.md
================
---
title: Deterministic Transaction Fees
---

## Calculating fees

Before sending a transaction to the cluster, a client may query the network to
determine what the transaction's fee will be via the rpc request
[getFeeForMessage](https://solana.com/docs/rpc/http/getfeeformessage).

## Fee Parameters

The fee is based on the number of signatures in the transaction, the more
signatures a transaction contains, the higher the fee. In addition, a
transaction can specify an additional fee that determines how the transaction is
relatively prioritized against others. A transaction's prioritization fee is
calculated by multiplying the number of compute units by the compute unit price
(measured in micro-lamports) set by the transaction via compute budget
instructions.

================
File: docs/src/implemented-proposals/validator-timestamp-oracle.md
================
---
title: Validator Timestamp Oracle
---

Third-party users of Solana sometimes need to know the real-world time a block
was produced, generally to meet compliance requirements for external auditors or
law enforcement. This proposal describes a validator timestamp oracle that
would allow a Solana cluster to satisfy this need.

The general outline of the proposed implementation is as follows:

- At regular intervals, each validator records its observed time for a known slot
  on-chain (via a Timestamp added to a slot Vote)
- A client can request a block time for a rooted block using the `getBlockTime`
  RPC method. When a client requests a timestamp for block N:

  1. A validator determines a "cluster" timestamp for a recent timestamped slot
     before block N by observing all the timestamped Vote instructions recorded on
     the ledger that reference that slot, and determining the stake-weighted mean
     timestamp.

  2. This recent mean timestamp is then used to calculate the timestamp of
     block N using the cluster's established slot duration

Requirements:

- Any validator replaying the ledger in the future must come up with the same
  time for every block since genesis
- Estimated block times should not drift more than an hour or so before resolving
  to real-world (oracle) data
- The block times are not controlled by a single centralized oracle, but
  ideally based on a function that uses inputs from all validators
- Each validator must maintain a timestamp oracle

The same implementation can provide a timestamp estimate for a not-yet-rooted
block. However, because the most recent timestamped slot may or may not be
rooted yet, this timestamp would be unstable (potentially failing requirement
1). Initial implementation will target rooted blocks, but if there is a use case
for recent-block timestamping, it will be trivial to add the RPC apis in the
future.

## Recording Time

At regular intervals as it is voting on a particular slot, each validator
records its observed time by including a timestamp in its Vote instruction
submission. The corresponding slot for the timestamp is the newest Slot in the
Vote vector (`Vote::slots.iter().max()`). It is signed by the validator's
identity keypair as a usual Vote. In order to enable this reporting, the Vote
struct needs to be extended to include a timestamp field, `timestamp: Option<UnixTimestamp>`, which will be set to `None` in most Votes.

As of https://github.com/solana-labs/solana/pull/10630, validators submit a
timestamp every vote. This enables implementation of a block time caching
service that allows nodes to calculate the estimated timestamp immediately after
the block is rooted, and cache that value in Blockstore. This provides
persistent data and quick queries, while still meeting requirement 1) above.

### Vote Accounts

A validator's vote account will hold its most recent slot-timestamp in VoteState.

### Vote Program

The on-chain Vote program needs to be extended to process a timestamp sent with
a Vote instruction from validators. In addition to its current process_vote
functionality (including loading the correct Vote account and verifying that the
transaction signer is the expected validator), this process needs to compare the
timestamp and corresponding slot to the currently stored values to verify that
they are both monotonically increasing, and store the new slot and timestamp in
the account.

## Calculating Stake-Weighted Mean Timestamp

In order to calculate the estimated timestamp for a particular block, a
validator first needs to identify the most recently timestamped slot:

```text
let timestamp_slot = floor(current_slot / timestamp_interval);
```

Then the validator needs to gather all Vote WithTimestamp transactions from the
ledger that reference that slot, using `Blockstore::get_slot_entries()`. As these
transactions could have taken some time to reach and be processed by the leader,
the validator needs to scan several completed blocks after the timestamp_slot to
get a reasonable set of Timestamps. The exact number of slots will need to be
tuned: More slots will enable greater cluster participation and more timestamp
datapoints; fewer slots will speed how long timestamp filtering takes.

From this collection of transactions, the validator calculates the
stake-weighted mean timestamp, cross-referencing the epoch stakes from
`staking_utils::staked_nodes_at_epoch()`.

Any validator replaying the ledger should derive the same stake-weighted mean
timestamp by processing the Timestamp transactions from the same number of
slots.

## Calculating Estimated Time for a Particular Block

Once the mean timestamp for a known slot is calculated, it is trivial to
calculate the estimated timestamp for subsequent block N:

```text
let block_n_timestamp = mean_timestamp + (block_n_slot_offset * slot_duration);
```

where `block_n_slot_offset` is the difference between the slot of block N and
the timestamp_slot, and `slot_duration` is derived from the cluster's
`slots_per_year` stored in each Bank

================
File: docs/src/operations/best-practices/_category_.json
================
{
  "position": 7,
  "label": "Best Practices",
  "collapsible": true,
  "collapsed": true,
  "link": null
}

================
File: docs/src/operations/best-practices/general.md
================
---
title: Agave Validator Operations Best Practices
sidebar_label: General Operations
pagination_label: "Best Practices: Validator Operations"
---

After you have successfully setup and started a
[validator on testnet](../setup-a-validator.md) (or another cluster
of your choice), you will want to become familiar with how to operate your
validator on a day-to-day basis. During daily operations, you will be
[monitoring your server](./monitoring.md), updating software regularly (both the
Solana validator software and operating system packages), and managing your vote
account and identity account.

All of these skills are critical to practice. Maximizing your validator uptime
is an important part of being a good operator.

## Educational Workshops

The Solana validator community holds regular educational workshops. You can
watch past workshops through the
[Solana validator educational workshops playlist](https://www.youtube.com/watch?v=86zySQ5vGW8&list=PLilwLeBwGuK6jKrmn7KOkxRxS9tvbRa5p).

## Community Validator calls

The Solana validator community holds regular calls. 
There is the 'Solana Foundation Validator Discussion' which is hosted by the Solana Foundation and the 'Community Led Validator Call'
which is hosted by the community itself. 

### Solana Foundation Validator Discussion

This is a monthly call that is hosted by the Solana Foundation. 
- Schedule: every second Thursday of the month 18:00 CET
- Agenda: See [validator-announcements channel in Discord](https://discord.com/channels/428295358100013066/586252910506016798). 
- This call **is recorded** and past calls can be watched back on the [Community Validator Discussions playlist](https://www.youtube.com/playlist?list=PLilwLeBwGuK78yjGBZwYhTf7rao0t13Zw)

### Community Led Validator Call

This is also a monthly call hosted by the Solana validator community itself.
- Schedule: every fourth Thursday of the month 18:00 CET
- Agenda: See [HackMD site](https://hackmd.io/1DFauFMWTZG37-U7CXhxMg?view#Solana-Community-Validator-Call-Agendas). 
- This call is **not recorded**

***Please note that the scheduling of these calls can be changed last minute due to any circumstances. For the most up-to-date information go to the [validator-announcements channel in Discord](https://discord.com/channels/428295358100013066/586252910506016798).***

## Help with the validator command line

From within the Solana CLI, you can execute the `agave-validator` command with
the `--help` flag to get a better understanding of the flags and sub commands
available.

```
agave-validator --help
```

## Restarting your validator

There are many operational reasons you may want to restart your validator. As a
best practice, you should avoid a restart during a leader slot. A
[leader slot](https://solana.com/docs/terminology#leader-schedule) is the time
when your validator is expected to produce blocks. For the health of the cluster
and also for your validator's ability to earn transaction fee rewards, you do
not want your validator to be offline during an opportunity to produce blocks.

To see the full leader schedule for an epoch, use the following command:

```
solana leader-schedule
```

Based on the current slot and the leader schedule, you can calculate open time
windows where your validator is not expected to produce blocks.

Assuming you are ready to restart, you may use the `agave-validator exit`
command. The command exits your validator process when an appropriate idle time
window is reached. Assuming that you have systemd implemented for your validator
process, the validator should restart automatically after the exit. See the
below help command for details:

```
agave-validator exit --help
```

## Upgrading

There are many ways to upgrade the
[Solana CLI software](../../cli/install.md). As an operator, you
will need to upgrade often, so it is important to get comfortable with this
process.

> **Note** validator nodes do not need to be offline while the newest version is
> being built from source. All methods below can be done before
> the validator process is restarted.

### Building the newest version from source

The easiest way to upgrade the Solana CLI software is to build the newest
version from source. See the
[build from source](../../cli/install.md#build-from-source) instructions for details.

### Restart

The validator process will need to be restarted before
the newly installed version is in use. Use `agave-validator exit` to restart
your validator process.

### Verifying version

The best way to verify that your validator process has changed to the desired
version is to grep the logs after a restart. The following grep command should
show you the version that your validator restarted with:

```
grep -B1 'Starting validator with' <path/to/logfile>
```

## Snapshots

Validators operators who have not experienced significant downtime (multiple
hours of downtime), should avoid downloading snapshots. It is important for the
health of the cluster as well as your validator history to maintain the local
ledger. Therefore, you should not download a new snapshot any time your
validator is offline or experiences an issue. Downloading a snapshot should only
be reserved for occasions when you do not have local state. Prolonged downtime
or the first install of a new validator are examples of times when you may not
have state locally. In other cases, such as restarts for upgrades, a snapshot
download should be avoided.

To avoid downloading a snapshot on restart, add the following flag to the
`agave-validator` command:

```
--no-snapshot-fetch
```

If you use this flag with the `agave-validator` command, make sure that you run
`solana catchup <pubkey>` after your validator starts to make sure that the
validator is catching up in a reasonable time. After some time (potentially a
few hours), if it appears that your validator continues to fall behind, then you
may have to download a new snapshot.

### Downloading Snapshots

If you are starting a validator for the first time, or your validator has fallen
too far behind after a restart, then you may have to download a snapshot.

To download a snapshot, you must **_NOT_** use the `--no-snapshot-fetch` flag.
Without the flag, your validator will automatically download a snapshot from
your known validators that you specified with the `--known-validator` flag.

If one of the known validators is downloading slowly, you can try adding the
`--minimal-snapshot-download-speed` flag to your validator. This flag will
switch to another known validator if the initial download speed is below the
threshold that you set.

### Manually Downloading Snapshots

In the case that there are network troubles with one or more of your known
validators, then you may have to manually download the snapshot. To manually
download a snapshot from one of your known validators, first, find the IP
address of the validator in using the `solana gossip` command. In the example
below, `5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on` is the pubkey of one of my
known validators:

```
solana gossip | grep 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on
```

The IP address of the validators is `139.178.68.207` and the open port on this
validator is `80`. You can see the IP address and port in the fifth column in
the gossip output:

```
139.178.68.207  | 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on | 8001   | 8004  | 139.178.68.207:80     | 1.10.27 | 1425680972
```

Now that the IP and port are known, you can download a full snapshot or an
incremental snapshot:

```
wget --trust-server-names http://139.178.68.207:80/snapshot.tar.bz2
wget --trust-server-names http://139.178.68.207:80/incremental-snapshot.tar.bz2
```

Now move those files into your snapshot directory. If you have not specified a
snapshot directory, then you should put the files in your ledger directory.

Once you have a local snapshot, you can restart your validator with the
`--no-snapshot-fetch` flag.

## Regularly Check Account Balances

It is important that you do not accidentally run out of funds in your identity
account, as your node will stop voting. It is also important to note that this
account keypair is the most vulnerable of the three keypairs in a vote account
because the keypair for the identity account is stored on your validator when
running the `agave-validator` software. How much SOL you should store there is
up to you. As a best practice, make sure to check the account regularly and
refill or deduct from it as needed. To check the account balance do:

```
solana balance validator-keypair.json
```

> **Note** `agave-watchtower` can monitor for a minimum validator identity
> balance. See [monitoring best practices](./monitoring.md) for details.

## Withdrawing From The Vote Account

As a reminder, your withdrawer's keypair should **_NEVER_** be stored on your
server. It should be stored on a hardware wallet, paper wallet, or multisig
mitigates the risk of hacking and theft of funds.

To withdraw your funds from your vote account, you will need to run
`solana withdraw-from-vote-account` on a trusted computer. For example, on a
trusted computer, you could withdraw all of the funds from your vote account
(excluding the rent exempt minimum). The below example assumes you have a
separate keypair to store your funds called `person-keypair.json`

```
solana withdraw-from-vote-account \
   vote-account-keypair.json \
   person-keypair.json ALL \
   --authorized-withdrawer authorized-withdrawer-keypair.json
```

To get more information on the command, use
`solana withdraw-from-vote-account --help`.

For a more detailed explanation of the different keypairs and other related
operations refer to
[vote account management](../guides/vote-accounts.md).

================
File: docs/src/operations/best-practices/monitoring.md
================
---
title: Agave Validator Monitoring Best Practices
sidebar_label: Monitoring
pagination_label: "Best Practices: Validator Monitoring"
---

It is essential that you have monitoring in place on your validator. In the event that your validator is delinquent (behind the rest of the network) you want to respond immediately to fix the issue. One very useful tool to monitor your validator is [`agave-watchtower`](#agave-watchtower).

## Solana Watchtower

Solana Watchtower is an extremely useful monitoring tool that will regularly monitor the health of your validator. It can monitor your validator for delinquency then notify you on your application of choice: Slack, Discord, Telegram or Twilio. Additionally, `agave-watchtower` has the ability to monitor the health of the entire cluster so that you can be aware of any cluster wide problems.

### Getting Started

To get started with Solana Watchtower, run `agave-watchtower --help`. From the help menu, you can see the optional flags and an explanation of the command.

Here is a sample command that will monitor a validator node with an identity public key of `2uTk98rqqwENevkPH2AHHzGHXgeGc1h6ku8hQUqWeXZp`:

```
agave-watchtower --monitor-active-stake --validator-identity \
  2uTk98rqqwENevkPH2AHHzGHXgeGc1h6ku8hQUqWeXZp
```

The command will monitor your validator, but you will not get notifications unless you added the environment variables mentioned in `agave-watchtower --help`. Since getting each of these services setup for notifications is not straight forward, the next section will walk through [setting up watchtower notifications on Telegram](#setup-telegram-notifications).

### Best Practices

It is a best practice to run the `agave-watchtower` command on a separate server from your validator.

In the case that you run `agave-watchtower` on the same computer as your `agave-validator` process, then during catastrophic events like a power outage, you will not be aware of the issue, because your `agave-watchtower` process will stop at the same time as your `agave-validator` process.

Additionally, while running the `agave-watchtower` process manually with environment variables set in the terminal is a good way to test out the command, it is not operationally sound because the process will not be restarted when the terminal closes or during a system restart.

Instead, you could run your `agave-watchtower` command as a system process similar to `agave-validator`. In the system process file, you can specify the environment variables for your bot.

### Setup Telegram Notifications

To send validator health notifications to your Telegram account, we are going to do a few things:

1. Create a bot to send the messages. The bot will be created using BotFather on Telegram
2. Send a message to the bot
3. Create a Telegram group that will get the watchtower notifications
4. Add the environment variables to your command line environment
5. Restart the `agave-watchtower` command

#### Create a Bot Using BotFather

In Telegram, search for `@BotFather`. Send the following message to _@BotFather_: `/newbot`.

Now you will have to come up with a name for the bot. The only requirement is that it cannot have dashes or spaces, and it **must** end in the word `bot`. Many names have already been taken, so you may have to try a few. Once you find an available name, you will get a response from _@BotFather_ that includes a link to chat with the bot as well as a token for the bot. Take note of the token. You will need it when you setup your environment variables.

#### Send a Message to The Bot

Find the bot in Telegram and send it the following message: `/start`. Messaging the bot will help you later when looking for the bot chatroom id.

#### Create Telegram Group

In Telegram, click on the new message icon and then select new group. Find your newly created bot and add the bot to the group. Next, name the group whatever you'd like.

#### Set Environment Variables For Watchtower

Now that you have a bot setup, you will need to set the environment variables for the bot so that watchtower can send notifications.

First, recall the chat message that you got from _@BotFather_. In the message, there was an HTTP API token for your bot. The token will have this format: `389178471:MMTKMrnZB4ErUzJmuFIXTKE6DupLSgoa7h4o`. You will use that token to set the `TELEGRAM_BOT_TOKEN` environment variable. In the terminal where you plan to run `agave-watchtower`, run the following:

```
export TELEGRAM_BOT_TOKEN=<HTTP API Token>
```

Next, you need the chat id for your group so that `solana-watcher` knows where to send the message. First, send a message to your bot in the chat group that you created. Something like `@newvalidatorbot hello`.

Next, in your browser, go to `https://api.telegram.org/bot<HTTP API Token>/getUpdates`. Make sure to replace `<HTTP API TOKEN>` with your API token that you got in the _@BotFather_ message. Also make sure that you include the word `bot` in the URL before the API token. Make the request in the browser.

The response should be in JSON. Search for the string `"chat":` in the JSON. The `id` value of that chat is your `TELEGRAM_CHAT_ID`. It will be a negative number like: `-781559558`. Remember to include the negative sign! If you cannot find `"chat":` in the JSON, then you may have to remove the bot from your chat group and add it again.

With your Telegram chat id in hand, export the environment variable where you plan to run `agave-watchtower`:

```
export TELEGRAM_CHAT_ID=<negative chat id number>
```

#### Restart agave-watchtower

Once your environment variables are set, restart `agave-watchtower`. You should see output about your validator.

To test that your Telegram configuration is working properly, you could stop your validator briefly until it is labeled as delinquent. Up to a minute after the validator is delinquent, you should receive a message in the Telegram group from your bot. Start the validator again and verify that you get another message in your Telegram group from the bot. The message should say `all clear`.

## Collecting metrics

It is important to collect metrics: it helps diagnose existing problems and allows to anticipate future ones.

### metrics.solana.com

There are several public dashboards available, one of them is hosted at [metrics.solana.com](https://metrics.solana.com). Reporting to the solana.com public dashboard is even required if you participate in the [Solana Foundation Delegation Program](https://solana.org/delegation-program). Using it is done by simply setting the `$SOLANA_METRICS_CONFIG` variable in your validator's environment (e.g. at the beginning of your `validator.sh` script).

Refer to the [available Solana clusters documentation](../../clusters/available.md) to get the appropriate value of `$SOLANA_METRICS_CONFIG` for your validator.

================
File: docs/src/operations/best-practices/security.md
================
---
title: Agave Validator Security Best Practices
sidebar_label: Security
pagination_label: "Best Practices: Validator Security"
---

Being a system administrator for an Ubuntu computer requires technical knowledge of the system and best security practices. The following list should help you get started and is considered the bare minimum for keeping your system safe.

## Keep Your System Up To Date

Make sure to regularly update packages in your Ubuntu system. Out of date packages may contain known security vulnerabilities that a hacker could exploit. A good practice would be to update weekly at least. To update your system, do the following:

```
sudo apt update
sudo apt upgrade
```

## DO NOT Store Your Withdrawer Key On Your Validator Machine

Your withdrawer key gives the operator full control of the vote account. It is highly sensitive information and should not be stored on the validator itself.

There are a number of options for withdrawer key management.  Some operators choose to use hardware wallets or paper wallets for the withdrawer keypair.  Another option is a multisig where each key of the multisig is a hardware wallet or paper wallet. Whichever option you choose, make sure the authorized withdrawer key is stored securely and that it has been generated on a trusted computer (other than your validator computer).

To reiterate, the withdrawer keypair should never be stored on your validator at any time.

## DO NOT Run The Solana Validator as a Root User

It may be easier to get started by running your application as root, but it is a bad practice.

If there is an exploit in your system, a hacker could have full access if your Solana application is running as the `root` user. Instead, see the [setup instructions](../setup-a-validator.md#sol-user) for creating a user called `sol` and running the application as the `sol` user.

## Close Ports That Are Not In Use

Your system should close all ports that do not need to be open to the outside world. A common firewall for closing ports is `ufw` (uncomplicated firewall). You can find a guide to using `ufw` from [Digital Ocean](https://www.digitalocean.com/community/tutorials/ufw-essentials-common-firewall-rules-and-commands).

## Eliminate Brute Force Attacks With fail2ban

[fail2ban](https://github.com/fail2ban/fail2ban) is a network security tool that checks your logs for suspicious login attempts and bans those IP addresses after repeated attempts. This will help mitigate brute force attacks on your server.

The default setup should work out-of-the-box by simply installing `fail2ban`:

```
sudo apt install fail2ban
```

## DO NOT Use Password Authentication for SSH

In addition to installing `fail2ban`, it is recommended to disable password based authentication for SSH access.  SSH key based authentication is preferred.

================
File: docs/src/operations/guides/_category_.json
================
{
  "position": 8,
  "label": "Validator Guides",
  "collapsible": true,
  "collapsed": true,
  "link": null
}

================
File: docs/src/operations/guides/restart-cluster.md
================
---
title: "Restarting a Solana Cluster"
# really high number to ensure it is listed last in the sidebar
sidebar_position: 999
sidebar_label: Restart a Cluster
pagination_label: "Validator Guides: Restart a Cluster"
---

### Step 1. Identify the latest optimistically confirmed slot for the cluster

In Agave 1.14 or greater, run the following command to output the latest
optimistically confirmed slot your validator observed:
```bash
agave-ledger-tool -l ledger latest-optimistic-slots
```

In Agave 1.13 or less, the latest optimistically confirmed can be found by looking for the more recent occurrence of
[this](https://github.com/solana-labs/solana/blob/0264147d42d506fb888f5c4c021a998e231a3e74/core/src/optimistic_confirmation_verifier.rs#L71)
metrics datapoint.

Call this slot `SLOT_X`

Note that it's possible that some validators observed an optimistically
confirmed slot that's greater than others before the outage.  Survey the other
validators on the cluster to ensure that a greater optimistically confirmed slot
does not exist before proceeding. If a greater slot value is found use it
instead.


### Step 2. Stop the validator(s)

### Step 3. Optionally install the new solana version

### Step 4. Create a new snapshot for slot `SLOT_X` with a hard fork at slot `SLOT_X`

```bash
$ agave-ledger-tool -l <LEDGER_PATH> --snapshot-archive-path <SNAPSHOTS_PATH> --incremental-snapshot-archive-path <INCREMENTAL_SNAPSHOTS_PATH> create-snapshot SLOT_X <SNAPSHOTS_PATH> --hard-fork SLOT_X
```

The snapshots directory should now contain the new snapshot.
`agave-ledger-tool create-snapshot` will also output the new shred version, and bank hash value,
call this NEW_SHRED_VERSION and NEW_BANK_HASH respectively.

Adjust your validator's arguments:

```bash
 --wait-for-supermajority SLOT_X
 --expected-bank-hash NEW_BANK_HASH
```

Then restart the validator.

Confirm with the log that the validator booted and is now in a holding pattern at `SLOT_X`, waiting for a super majority.

Once NEW_SHRED_VERSION is determined, nudge foundation entrypoint operators to update entrypoints.

### Step 5. Announce the restart on Discord:

Post something like the following to #announcements (adjusting the text as appropriate):

> Hi @Validators,
>
> We've released v1.1.12 and are ready to get testnet back up again.
>
> Steps:
>
> 1. Install the v1.1.12 release: https://github.com/solana-labs/solana/releases/tag/v1.1.12
> 2. a. Preferred method, start from your local ledger with:
>
> ```bash
> agave-validator
>   --wait-for-supermajority SLOT_X     # <-- NEW! IMPORTANT! REMOVE AFTER THIS RESTART
>   --expected-bank-hash NEW_BANK_HASH  # <-- NEW! IMPORTANT! REMOVE AFTER THIS RESTART
>   --hard-fork SLOT_X                  # <-- NEW! IMPORTANT! REMOVE AFTER THIS RESTART
>   --no-snapshot-fetch                 # <-- NEW! IMPORTANT! REMOVE AFTER THIS RESTART
>   --entrypoint entrypoint.testnet.solana.com:8001
>   --known-validator 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on
>   --expected-genesis-hash 4uhcVJyU9pJkvQyS88uRDiswHXSCkY3zQawwpjk2NsNY
>   --only-known-rpc
>   --limit-ledger-size
>   ...                                # <-- your other --identity/--vote-account/etc arguments
> ```
>
> b. If your validator doesn't have ledger up to slot SLOT_X or if you have deleted your ledger, have it instead download a snapshot with:
>
> ```bash
> agave-validator
>   --wait-for-supermajority SLOT_X     # <-- NEW! IMPORTANT! REMOVE AFTER THIS RESTART
>   --expected-bank-hash NEW_BANK_HASH  # <-- NEW! IMPORTANT! REMOVE AFTER THIS RESTART
>   --entrypoint entrypoint.testnet.solana.com:8001
>   --known-validator 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on
>   --expected-genesis-hash 4uhcVJyU9pJkvQyS88uRDiswHXSCkY3zQawwpjk2NsNY
>   --only-known-rpc
>   --limit-ledger-size
>   ...                                # <-- your other --identity/--vote-account/etc arguments
> ```
>
>      You can check for which slots your ledger has with: `agave-ledger-tool -l path/to/ledger bounds`
>
> 3. Wait until 80% of the stake comes online
>
> To confirm your restarted validator is correctly waiting for the 80%:
> a. Look for `N% of active stake visible in gossip` log messages
> b. Ask it over RPC what slot it's on: `solana --url http://127.0.0.1:8899 slot`. It should return `SLOT_X` until we get to 80% stake
>
> Thanks!

### Step 7. Wait and listen

Monitor the validators as they restart. Answer questions, help folks,

## Troubleshooting

### 80% of the stake didn't participate in the restart, now what?
If less than 80% of the stake join the restart after a reasonable amount of
time, it will be necessary to retry the restart attempt with the stake from the
non-responsive validators removed.

The community should identify and come to social consensus on the set of
non-responsive validators. Then all participating validators return to Step 4
and create a new snapshot with additional `--destake-vote-account <PUBKEY>`
arguments for each of the non-responsive validator's vote account address

```bash
$ agave-ledger-tool -l ledger create-snapshot SLOT_X ledger --hard-fork SLOT_X \
    --destake-vote-account <VOTE_ACCOUNT_1> \
    --destake-vote-account <VOTE_ACCOUNT_2> \
    .
    .
    --destake-vote-account <VOTE_ACCOUNT_N> \
```

This will cause all stake associated with the non-responsive validators to be
immediately deactivated. All their stakers will need to re-delegate their stake
once the cluster restart is successful.

================
File: docs/src/operations/guides/validator-failover.md
================
---
title: "Validator Guide: Setup Node Failover"
sidebar_position: 9
sidebar_label: Node Failover
pagination_label: "Validator Guides: Node Failover"
---

A simple two machine instance failover method is described here, which allows you to:
* Upgrade your validator software with virtually no downtime, and
* Failover to the secondary instance when your monitoring detects a problem with the primary instance
without any safety issues that would otherwise be associated with running two instances of your validator.

You will need:
* Two non-delinquent validator nodes
* Identities that are not associated with a staked vote account on both validators to use when not actively voting
* Validator startup scripts both modified to use a symbolic link as the identity
* Validator startup scripts both modified to include staked identity as authorized voter

## Setup

### Generating an Unstaked Secondary Identity

Both validators need to have secondary (unstaked) identities to assume when not actively voting.
You can generate these secondary identities on each of your validators like so:
```
solana-keygen new -s --no-bip39-passphrase -o unstaked-identity.json
```
### Validator Startup Script Modifications

The identity flag and authorized voter flags should be modified on both validators.

Note that `identity.json` is not a real file but a symbolic link we will create shortly.
However, the authorized voter flag does need to point to the staked identity file (your main identity).
In this guide, the main identity is renamed to `staked-identity.json` for clarity and simplicity.
You can certainly name your main identity file however you'd like; make sure it is specified as an authorized voter as shown below:

```
exec /home/sol/bin/agave-validator \
    --identity /home/sol/identity.json \
    --vote-account /home/sol/vote.json \
    --authorized-voter /home/sol/staked-identity.json \
```

Summary:

* Identity is a symbolic link we will create in the next section. It may point to your staked identity or your inactive unstaked identity.
* No changed to the vote account, just shown for context.
* Authorized voter points to your main, staked identity.

### Creating Identity Symlinks
An important part of how this system functions is the identity.json symbolic link.
This link allows us to soft link the desired identity so that the validator can restart or stop/start with the same identity we last intended it to have.

On your actively voting validator, link this to your staked identity
```
ln -sf /home/sol/staked-identity.json /home/sol/identity.json
```

On your inactive, non-voting validator, link this to your unstaked identity
```
ln -sf /home/sol/unstaked-identity.json /home/sol/identity.json
```

### Transition Preparation Checklist
At this point, you should have completed the following on each validator:
* Generated an unstaked identity
* Updated your validator startup script
* Created a symbolic link to point to the respective identity file

If you have done this - great! You're ready to transition!

###  Transition Process
#### Active Validator
* Wait for a restart window
* Set identity to unstaked identity
* Correct symbolic link to reflect this change
* Copy the tower file to the inactive validator

```
#!/bin/bash

# example script of the above steps - change specifics such as user / IP / ledger path
agave-validator -l /mnt/ledger wait-for-restart-window --min-idle-time 2 --skip-new-snapshot-check
agave-validator -l /mnt/ledger set-identity /home/sol/unstaked-identity.json
ln -sf /home/sol/unstaked-identity.json /home/sol/identity.json
scp /mnt/ledger/tower-1_9-$(solana-keygen pubkey /home/sol/staked-identity.json).bin <user>@<IP>/mnt/ledger
```

(At this point your primary identity is no longer voting)

#### Inactive Validator
* Set identity to your staked identity (requiring the tower)
* Rewrite the symbolic link to reflect this

```
#!/bin/bash

# example script of the above steps
agave-validator -l /mnt/ledger set-identity --require-tower /home/sol/staked-identity.json
ln -sf /home/sol/staked-identity.json /home/sol/identity.json
```

### Verification
Verify identities transitioned successfully using either `agave-validator monitor` or `solana catchup --our-localhost 8899`

================
File: docs/src/operations/guides/validator-info.md
================
---
title: "Validator Guide: Publishing Validator Info"
sidebar_position: 1
sidebar_label: Publishing Validator Info
pagination_label: "Validator Guides: Publishing Validator Info"
---

You can publish your validator information to the chain to be publicly visible to other users.

## Run solana validator-info

Run the solana CLI to populate a validator info account:

```bash
solana validator-info publish --keypair ~/validator-keypair.json <VALIDATOR_INFO_ARGS> <VALIDATOR_NAME>
```

For details about optional fields for VALIDATOR_INFO_ARGS:

```bash
solana validator-info publish --help
```

The recommended dimensions for the validator icon are 360x360px and PNG format.

## Example Commands

Example publish command:

```bash
solana validator-info publish "Elvis Validator" -w "https://elvis-validates.com" -i "https://elvis-validates.com/my-icon.png"
```

Example query command:

```bash
solana validator-info get
```

which outputs

```text
Validator info from 8WdJvDz6obhADdxpGCiJKZsDYwTLNEDFizayqziDc9ah
  Validator pubkey: 6dMH3u76qZ7XG4bVboVRnBHR2FfrxEqTTTyj4xmyDMWo
  Info: {"iconUrl":"elvis","name":"Elvis Validator","website":"https://elvis-validates.com"}
```

For older accounts instead of `iconUrl` you might see `keybaseUsername` as those accounts used Keybase for their validator icon (see next section for further information).

## Keybase

Previously Keybase was used by validators to provide their validator icon, however Keybase has sunset its service and thus is no longer supported. Some old validator info accounts will still contain keybase usernames this is reflected in the serialized data returned when querying these validator info accounts.

================
File: docs/src/operations/guides/validator-monitor.md
================
---
title: "Validator Guide: Monitoring a Validator"
sidebar_position: 2
sidebar_label: Monitoring a Validator
pagination_label: "Validator Guides: Monitoring a Validator"
---

## Check Gossip

Confirm the IP address and **identity pubkey** of your validator is visible in
the gossip network by running:

```bash
solana gossip
```

## Check Your Balance

Your account balance should decrease by the transaction fee amount as your
validator submits votes, and increase after serving as the leader. Pass the
`--lamports` are to observe in finer detail:

```bash
solana balance --lamports
```

## Check Vote Activity

The `solana vote-account` command displays the recent voting activity from
your validator:

```bash
solana vote-account ~/vote-account-keypair.json
```

## Get Cluster Info

There are several useful JSON-RPC endpoints for monitoring your validator on the
cluster, as well as the health of the cluster:

```bash
# Similar to solana-gossip, you should see your validator in the list of cluster nodes
curl -X POST -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","id":1, "method":"getClusterNodes"}' http://api.devnet.solana.com
# If your validator is properly voting, it should appear in the list of `current` vote accounts. If staked, `stake` should be > 0
curl -X POST -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","id":1, "method":"getVoteAccounts"}' http://api.devnet.solana.com
# Returns the current leader schedule
curl -X POST -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","id":1, "method":"getLeaderSchedule"}' http://api.devnet.solana.com
# Returns info about the current epoch. slotIndex should progress on subsequent calls.
curl -X POST -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","id":1, "method":"getEpochInfo"}' http://api.devnet.solana.com
```

================
File: docs/src/operations/guides/validator-stake.md
================
---
title: "Validator Guide: Staking"
sidebar_position: 3
sidebar_label: Staking
pagination_label: "Validator Guides: Staking"
---

**By default your validator will have no stake.** This means it will be
ineligible to become leader, and unable to land votes.

## Monitoring Catch Up

To delegate stake, first make sure your validator is running and has caught up
to the cluster. It may take some time to catch up after your validator boots.
Use the `catchup` command to monitor your validator through this process:

```bash
solana catchup ~/validator-keypair.json
```

Until your validator has caught up, it will not be able to vote successfully and
stake cannot be delegated to it.

Also if you find the cluster's slot advancing faster than yours, you will likely
never catch up. This typically implies some kind of networking issue between
your validator and the rest of the cluster.

## Create Stake Keypair

If you haven’t already done so, create a staking keypair. If you have completed
this step, you should see the “validator-stake-keypair.json” in your Solana
runtime directory.

```bash
solana-keygen new -o ~/validator-stake-keypair.json
```

## Delegate Stake

Now delegate 1 SOL to your validator by first creating your stake account:

```bash
solana create-stake-account ~/validator-stake-keypair.json 1
```

and then delegating that stake to your validator:

```bash
solana delegate-stake ~/validator-stake-keypair.json ~/vote-account-keypair.json
```

> Don’t delegate your remaining SOL, as your validator will use those tokens to
> vote.

Stakes can be re-delegated to another node at any time with the same command,
but only one re-delegation is permitted per epoch:

```bash
solana delegate-stake ~/validator-stake-keypair.json ~/some-other-vote-account-keypair.json
```

## Validator Stake Warm-up

To combat various attacks on consensus, new stake delegations are subject to a
[warm-up](https://solana.com/docs/references/staking/stake-accounts#delegation-warmup-and-cooldown) period.

Monitor a validator's stake during warmup by:

- View your vote account:`solana vote-account ~/vote-account-keypair.json` This
  displays the current state of all the votes the validator has submitted to the
  network.
- View your stake account, the delegation preference and details of your
  stake:`solana stake-account ~/validator-stake-keypair.json`
- `solana validators` displays the current active stake of all validators,
  including yours
- `solana stake-history` shows the history of stake warming up and cooling down
  over recent epochs
- Look for log messages on your validator indicating your next leader slot:
  `[2019-09-27T20:16:00.319721164Z INFO solana_core::replay_stage] <VALIDATOR_IDENTITY_PUBKEY> voted and reset PoH at tick height ####. My next leader slot is ####`
- Once your stake is warmed up, you will see a stake balance listed for your
  validator by running `solana validators`

## Validator Rewards

Once your stake is warmed up, and assuming the node is voting, you will now be
generating validator rewards. Rewards are paid automatically on epoch
boundaries.

The rewards lamports earned are split between your stake account and the vote
account according to the commission rate set in the vote account. Rewards can
only be earned while the validator is up and running. Further, once staked, the
validator becomes an important part of the network. In order to safely remove a
validator from the network, first deactivate its stake.

At the end of each slot, a validator is expected to send a vote transaction.
These vote transactions are paid for by lamports from a validator's identity
account.

This is a normal transaction so the standard transaction fee will apply. The
transaction fee range is defined by the genesis block. The actual fee will
fluctuate based on transaction load. You can determine the current fee via the
[RPC API “getRecentBlockhash”](https://solana.com/docs/rpc/deprecated/getrecentblockhash) before submitting
a transaction.

Learn more about
[transaction fees here](../../implemented-proposals/transaction-fees.md).

## Monitor Your Staked Validator

Confirm your validator becomes a
[leader](https://solana.com/docs/terminology#leader)

- After your validator is caught up, use the `solana balance` command to monitor
  the earnings as your validator is selected as leader and collects transaction
  fees
- Solana nodes offer a number of useful JSON-RPC methods to return information
  about the network and your validator's participation. Make a request by using
  curl \(or another http client of your choosing\), specifying the desired
  method in JSON-RPC-formatted data. For example:

```bash
  // Request
  curl -X POST -H "Content-Type: application/json" -d '{"jsonrpc":"2.0","id":1, "method":"getEpochInfo"}' http://localhost:8899

  // Result
  {"jsonrpc":"2.0","result":{"epoch":3,"slotIndex":126,"slotsInEpoch":256},"id":1}
```

Helpful JSON-RPC methods:

- `getEpochInfo`[An epoch](https://solana.com/docs/terminology#epoch) is the
  time, i.e. number of [slots](https://solana.com/docs/terminology#slot), for
  which a [leader schedule](https://solana.com/docs/terminology#leader-schedule)
  is valid. This will tell you what the current epoch is and how far into it the
  cluster is.
- `getVoteAccounts` This will tell you how much active stake your validator
  currently has. A % of the validator's stake is activated on an epoch boundary.
  You can learn more about staking on Solana
  [here](../../consensus/stake-delegation-and-rewards.md).
- `getLeaderSchedule` At any given moment, the network expects only one
  validator to produce ledger entries. The
  [validator currently selected to produce ledger entries](../../consensus/leader-rotation.md#leader-rotation)
  is called the “leader”. This will return the complete leader schedule \(on a
  slot-by-slot basis\) for currently activated stake, the identity pubkey will
  show up 1 or more times here.

## Deactivating Stake

Before detaching your validator from the cluster, you should deactivate the
stake that was previously delegated by running:

```bash
solana deactivate-stake ~/validator-stake-keypair.json
```

Stake is not deactivated immediately and instead cools down in a similar fashion
as stake warm up. Your validator should remain attached to the cluster while the
stake is cooling down. While cooling down, your stake will continue to earn
rewards. Only after stake cooldown is it safe to turn off your validator or
withdraw it from the network. Cooldown may take several epochs to complete,
depending on active stake and the size of your stake.

Note that a stake account may only be used once, so after deactivation, use the
cli's `withdraw-stake` command to recover the previously staked lamports.

================
File: docs/src/operations/guides/validator-start.md
================
---
title: "Validator Guide: Starting a Validator"
sidebar_position: 0
sidebar_label: Starting a Validator
pagination_label: "Validator Guides: Starting a Validator"
---

## Configure Solana CLI

The solana cli includes `get` and `set` configuration commands to automatically
set the `--url` argument for cli commands. For example:

```bash
solana config set --url http://api.devnet.solana.com
```

While this section demonstrates how to connect to the Devnet cluster, the steps
are similar for the other [Solana Clusters](../../clusters/available.md).

## Confirm The Cluster Is Reachable

Before attaching a validator node, sanity check that the cluster is accessible
to your machine by fetching the transaction count:

```bash
solana transaction-count
```

View the [metrics dashboard](https://metrics.solana.com:3000/d/monitor-edge/cluster-telemetry) for more
detail on cluster activity.

## System Tuning

### Linux

If you would prefer to manage system settings on your own, you may do so with
the following commands.

#### **Optimize sysctl knobs**

```bash
sudo bash -c "cat >/etc/sysctl.d/21-agave-validator.conf <<EOF
# Increase max UDP buffer sizes
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728

# Increase memory mapped files limit
vm.max_map_count = 1000000

# Increase number of allowed open file descriptors
fs.nr_open = 1000000
EOF"
```

```bash
sudo sysctl -p /etc/sysctl.d/21-agave-validator.conf
```

#### **Increase systemd and session file limits**

Add

```
LimitNOFILE=1000000
LimitMEMLOCK=2000000000
```

to the `[Service]` section of your systemd service file, if you use one,
otherwise add

```
DefaultLimitNOFILE=1000000
DefaultLimitMEMLOCK=2000000000
```

to the `[Manager]` section of `/etc/systemd/system.conf`.

```bash
sudo systemctl daemon-reload
```

```bash
sudo bash -c "cat >/etc/security/limits.d/90-solana-nofiles.conf <<EOF
# Increase process file descriptor count limit
* - nofile 1000000
# Increase memory locked limit (kB)
* - memlock 2000000
EOF"
```

```bash
### Close all open sessions (log out then, in again) ###
```

#### System Clock

Large system clock drift can prevent a node from properly participating in Solana's [gossip protocol](../../validator/gossip.md).  Ensure that your system clock is accurate.  To check the current system clock, use:

```bash
timedatectl
```

Operators commonly use an ntp server to maintain an accurate system clock.

## Generate identity

Create an identity keypair for your validator by running:

```bash
solana-keygen new -o ~/validator-keypair.json
```

The identity public key can now be viewed by running:

```bash
solana-keygen pubkey ~/validator-keypair.json
```

> Note: The "validator-keypair.json” file is also your \(ed25519\) private key.

### Paper Wallet identity

You can create a paper wallet for your identity file instead of writing the
keypair file to disk with:

```bash
solana-keygen new --no-outfile
```

The corresponding identity public key can now be viewed by running:

```bash
solana-keygen pubkey ASK
```

and then entering your seed phrase.

See [Paper Wallet Usage](../../cli/wallets/paper.md) for more info.

---

### Vanity Keypair

You can generate a custom vanity keypair using solana-keygen. For instance:

```bash
solana-keygen grind --starts-with e1v1s:1
```

You may request that the generated vanity keypair be expressed as a seed phrase
which allows recovery of the keypair from the seed phrase and an optionally
supplied passphrase (note that this is significantly slower than grinding without
a mnemonic):

```bash
solana-keygen grind --use-mnemonic --starts-with e1v1s:1
```

Depending on the string requested, it may take days to find a match...

---

Your validator identity keypair uniquely identifies your validator within the
network. **It is crucial to back-up this information.**

If you don’t back up this information, you WILL NOT BE ABLE TO RECOVER YOUR
VALIDATOR if you lose access to it. If this happens, YOU WILL LOSE YOUR
ALLOCATION OF SOL TOO.

To back-up your validator identify keypair, **back-up your
"validator-keypair.json” file or your seed phrase to a secure location.**

## More Solana CLI Configuration

Now that you have a keypair, set the solana configuration to use your validator
keypair for all following commands:

```bash
solana config set --keypair ~/validator-keypair.json
```

You should see the following output:

```text
Config File: /home/solana/.config/solana/cli/config.yml
RPC URL: http://api.devnet.solana.com
WebSocket URL: ws://api.devnet.solana.com/ (computed)
Keypair Path: /home/solana/validator-keypair.json
Commitment: confirmed
```

## Airdrop & Check Validator Balance

Airdrop yourself some SOL to get started:

```bash
solana airdrop 1
```

Note that airdrops are only available on Devnet and Testnet. Both are limited
to 1 SOL per request.

To view your current balance:

```text
solana balance
```

Or to see in finer detail:

```text
solana balance --lamports
```

Read more about the difference between SOL and lamports here: [What is SOL?](https://solana.com/docs/references/terminology#sol), [What is a lamport?](https://solana.com/docs/references/terminology#lamport).

## Create Authorized Withdrawer Account

If you haven't already done so, create an authorized-withdrawer keypair to be used
as the ultimate authority over your validator. This keypair will have the
authority to withdraw from your vote account, and will have the additional
authority to change all other aspects of your vote account. Needless to say,
this is a very important keypair as anyone who possesses it can make any
changes to your vote account, including taking ownership of it permanently.
So it is very important to keep your authorized-withdrawer keypair in a safe
location. It does not need to be stored on your validator, and should not be
stored anywhere from where it could be accessed by unauthorized parties. To
create your authorized-withdrawer keypair:

```bash
solana-keygen new -o ~/authorized-withdrawer-keypair.json
```

## Create Vote Account

If you haven’t already done so, create a vote-account keypair and create the
vote account on the network. If you have completed this step, you should see the
“vote-account-keypair.json” in your Solana runtime directory:

```bash
solana-keygen new -o ~/vote-account-keypair.json
```

The following command can be used to create your vote account on the blockchain
with all the default options:

```bash
solana create-vote-account ~/vote-account-keypair.json ~/validator-keypair.json ~/authorized-withdrawer-keypair.json
```

Remember to move your authorized withdrawer keypair into a very secure location after running the above command.

Read more about [creating and managing a vote account](./vote-accounts.md).

## Known validators

If you know and respect other validator operators, you can specify this on the
command line with the `--known-validator <PUBKEY>` argument to
`agave-validator`. You can specify multiple ones by repeating the argument
`--known-validator <PUBKEY1> --known-validator <PUBKEY2>`. This has the effect
that when the validator is booting with `--only-known-rpc`, it will only ask
that set of known nodes for downloading genesis and snapshot data.

It is highly recommended you use this option to prevent malicious snapshot
state download.

## Connect Your Validator

Connect to the cluster by running:

```bash
agave-validator \
  --identity ~/validator-keypair.json \
  --vote-account ~/vote-account-keypair.json \
  --rpc-port 8899 \
  --entrypoint entrypoint.devnet.solana.com:8001 \
  --limit-ledger-size \
  --log ~/agave-validator.log
```

To force validator logging to the console add a `--log -` argument, otherwise
the validator will automatically log to a file.

The ledger will be placed in the `ledger/` directory by default, use the
`--ledger` argument to specify a different location.

> Note: You can use a
> [paper wallet seed phrase](../../cli/wallets/paper.md)
> for your `--identity` and/or
> `--authorized-voter` keypairs. To use these, pass the respective argument as
> `agave-validator --identity ASK ... --authorized-voter ASK ...`
> and you will be prompted to enter your seed phrases and optional passphrase.

Confirm your validator is connected to the network by opening a new terminal and
running:

```bash
solana gossip
```

If your validator is connected, its public key and IP address will appear in the list.

### Controlling local network port allocation

By default the validator will dynamically select available network ports in the
8000-10000 range, and may be overridden with `--dynamic-port-range`. For
example, `agave-validator --dynamic-port-range 11000-11020 ...` will restrict
the validator to ports 11000-11020.

### Limiting ledger size to conserve disk space

The `--limit-ledger-size` parameter allows you to specify how many ledger
[shreds](https://solana.com/docs/terminology#shred) your node retains on disk.
If you do not include this parameter, the validator will keep all received
ledger data until it runs out of disk space. Otherwise, the validator will
periodically purge the oldest data (FIFO) to remain under the specified
`--limit-ledger-size` value.

The default value attempts to keep the blockstore (data within the rocksdb
directory) disk usage under 500 GB. More or less disk usage may be requested
by adding an argument to `--limit-ledger-size` if desired. More information
about selecting a custom limit value is [available
here](https://github.com/solana-labs/solana/blob/aa72aa87790277619d12c27f1ebc864d23739557/core/src/ledger_cleanup_service.rs#L26-L37).

Note that the above target of 500 GB does not account for other items that
may reside in the `ledger` directory, depending on validator configuration.
These items may include (but are not limited to):
- Persistent accounts data
- Persistent accounts index
- Snapshots

Additionally, specifying `--enable-rpc-transaction-history` will store extra
block and transaction metadata. The space required to store this data varies
with cluster activity, and is hard to account for. Thus, using this flag will
likely cause the 500 GB target to be exceeded.

### Systemd Unit

Running the validator as a systemd unit is one easy way to manage running in the
background.

Assuming you have a user called `sol` on your machine, create the file `/etc/systemd/system/sol.service` with
the following:

```
[Unit]
Description=Solana Validator
After=network.target
StartLimitIntervalSec=0

[Service]
Type=simple
Restart=always
RestartSec=1
User=sol
LimitNOFILE=1000000
LimitMEMLOCK=2000000000
LogRateLimitIntervalSec=0
Environment="PATH=/bin:/usr/bin:/home/sol/.local/share/solana/install/active_release/bin"
ExecStart=/home/sol/bin/validator.sh

[Install]
WantedBy=multi-user.target
```

Now create `/home/sol/bin/validator.sh` to include the desired
`agave-validator` command-line. Ensure that the 'exec' command is used to
start the validator process (i.e. "exec agave-validator ..."). This is
important because without it, logrotate will end up killing the validator
every time the logs are rotated.

Ensure that running `/home/sol/bin/validator.sh` manually starts
the validator as expected. Don't forget to mark it executable with `chmod +x /home/sol/bin/validator.sh`

Start the service with:

```bash
sudo systemctl enable --now sol
```

### Logging

#### Log output tuning

The messages that a validator emits to the log can be controlled by the `RUST_LOG`
environment variable. Details can by found in the [documentation](https://docs.rs/env_logger/latest/env_logger/#enabling-logging)
for the `env_logger` Rust crate.

Note that if logging output is reduced, this may make it difficult to debug issues
encountered later. Should support be sought from the team, any changes will need
to be reverted and the issue reproduced before help can be provided.

#### Log rotation

The validator log file, as specified by `--log ~/agave-validator.log`, can get
very large over time and it's recommended that log rotation be configured.

The validator will re-open its log file when it receives the `USR1` signal, which is the
basic primitive that enables log rotation.

If the validator is being started by a wrapper shell script, it is important to
launch the process with `exec` (`exec agave-validator ...`) when using logrotate.
This will prevent the `USR1` signal from being sent to the script's process
instead of the validator's, which will kill them both.

#### Using logrotate

An example setup for the `logrotate`, which assumes that the validator is
running as a systemd service called `sol.service` and writes a log file at
/home/sol/agave-validator.log:

```bash
# Setup log rotation

cat > logrotate.sol <<EOF
/home/sol/agave-validator.log {
  rotate 7
  daily
  missingok
  postrotate
    systemctl kill -s USR1 sol.service
  endscript
}
EOF
sudo cp logrotate.sol /etc/logrotate.d/sol
systemctl restart logrotate.service
```

As mentioned earlier, be sure that if you use logrotate, any script you create
which starts the solana validator process uses "exec" to do so (example: "exec
agave-validator ..."); otherwise, when logrotate sends its signal to the
validator, the enclosing script will die and take the validator process with
it.

### Account indexing

As the number of populated accounts on the cluster grows, account-data RPC
requests that scan the entire account set -- like
[`getProgramAccounts`](https://solana.com/docs/rpc/http/getprogramaccounts) and
[SPL-token-specific requests](https://solana.com/docs/rpc/http/gettokenaccountsbydelegate) --
may perform poorly. If your validator needs to support any of these requests,
you can use the `--account-index` parameter to activate one or more in-memory
account indexes that significantly improve RPC performance by indexing accounts
by the key field. Currently supports the following parameter values:

- `program-id`: each account indexed by its owning program; used by [getProgramAccounts](https://solana.com/docs/rpc/http/getprogramaccounts)
- `spl-token-mint`: each SPL token account indexed by its token Mint; used by [getTokenAccountsByDelegate](https://solana.com/docs/rpc/http/gettokenaccountsbydelegate), and [getTokenLargestAccounts](https://solana.com/docs/rpc/http/gettokenlargestaccounts)
- `spl-token-owner`: each SPL token account indexed by the token-owner address; used by [getTokenAccountsByOwner](https://solana.com/docs/rpc/http/gettokenaccountsbyowner), and [getProgramAccounts](https://solana.com/docs/rpc/http/getprogramaccounts) requests that include an spl-token-owner filter.

================
File: docs/src/operations/guides/validator-troubleshoot.md
================
---
title: "Validator Guide: Troubleshooting"
sidebar_position: 4
sidebar_label: Troubleshooting
pagination_label: "Validator Guides: Troubleshooting"
---

There is a `#validator-support` Discord channel available to reach other
testnet participants, [https://solana.com/discord](https://solana.com/discord)

## Useful Links & Discussion

- [Network Explorer](http://explorer.solana.com/)
- [Testnet Metrics Dashboard](https://metrics.solana.com:3000/d/monitor-edge/cluster-telemetry-edge?refresh=60s&orgId=2)
- Validator [Discord](https://solana.com/discord) channels
  - `#validator-support` --  General support channel for any Validator related queries.
  - `#testnet-announcements` -- The single source of truth for critical information relating Testnet
- [Core software repo](https://github.com/solana-labs/solana)

## Blockstore

The validator blockstore rocksdb database can be inspected using the `ldb` tool.
`ldb` is part of the `rocksdb` code base and is also available in the `rocksdb-tools`
package.

[RocksDB Administration and Data Access Tool](https://github.com/facebook/rocksdb/wiki/Administration-and-Data-Access-Tool)

## Upgrade

If a new software version introduces a new column family to the blockstore,
that new (empty) column will be automatically created. This is the same logic
that allows a validator to start fresh without the blockstore directory.

## Downgrade

If a new column family has been introduced to the validator blockstore, a
subsequent downgrade of the validator to a version that predates the new column
family will cause the validator to fail while opening the blockstore during
startup.

List column families:
```
ldb --db=<validator ledger path>/rocksdb/ list_column_families
```

**Warning**: Please seek guidance on discord before modifying the validator
blockstore.

Drop a column family:
```
ldb --db=<validator ledger path>/rocksdb drop_column_family <column family name>
```

================
File: docs/src/operations/guides/vote-accounts.md
================
---
title: "Validator Guide: Vote Account Management"
sidebar_position: 5
sidebar_label: Vote Account Management
pagination_label: "Validator Guides: Vote Account Management"
---

This page describes how to set up an on-chain _vote account_. Creating a vote
account is needed if you plan to run a validator node on Solana.

## Create a Vote Account

A vote account can be created with the
[create-vote-account](../../cli/usage.md#solana-create-vote-account) command. The
vote account can be configured when first created or after the validator is
running. All aspects of the vote account can be changed except for the
[vote account address](#vote-account-address), which is fixed for the lifetime
of the account.

### Configure an Existing Vote Account

- To change the [validator identity](#validator-identity), use
  [vote-update-validator](../../cli/usage.md#solana-vote-update-validator).
- To change the [vote authority](#vote-authority), use
  [vote-authorize-voter-checked](../../cli/usage.md#solana-vote-authorize-voter-checked).
- To change the [authorized withdrawer](#authorized-withdrawer), use
  [vote-authorize-withdrawer-checked](../../cli/usage.md#solana-vote-authorize-withdrawer-checked).
- To change the [commission](#commission), use
  [vote-update-commission](../../cli/usage.md#solana-vote-update-commission).

## Vote Account Structure

### Vote Account Address

A vote account is created at an address that is either the public key of a
keypair file, or at a derived address based on a keypair file's public key and a
seed string.

The address of a vote account is never needed to sign any transactions, but is
just used to look up the account information.

When someone wants to
[delegate tokens in a stake account](https://solana.com/staking),
the delegation command is pointed at the vote account address of the validator
to whom the token-holder wants to delegate.

### Validator Identity

The _validator identity_ is a system account that is used to pay for all the
vote transaction fees submitted to the vote account. Because the validator is
expected to vote on most valid blocks it receives, the validator identity
account is frequently (potentially multiple times per second) signing
transactions and paying fees. For this reason the validator identity keypair
must be stored as a "hot wallet" in a keypair file on the same system the
validator process is running.

Because a hot wallet is generally less secure than an offline or "cold" wallet,
the validator operator may choose to store only enough SOL on the identity
account to cover voting fees for a limited amount of time, such as a few weeks
or months. The validator identity account could be periodically topped off from
a more secure wallet.

This practice can reduce the risk of loss of funds if the validator node's disk
or file system becomes compromised or corrupted.

The validator identity is required to be provided when a vote account is
created. The validator identity can also be changed after an account is created
by using the
[vote-update-validator](../../cli/usage.md#solana-vote-update-validator) command.

### Vote Authority

The _vote authority_ keypair is used to sign each vote transaction the validator
node wants to submit to the cluster. This doesn't necessarily have to be unique
from the validator identity, as you will see later in this document. Because the
vote authority, like the validator identity, is signing transactions frequently,
this also must be a hot keypair on the same file system as the validator
process.

The vote authority can be set to the same address as the validator identity. If
the validator identity is also the vote authority, only one signature per vote
transaction is needed in order to both sign the vote and pay the transaction
fee. Because transaction fees on Solana are assessed per-signature, having one
signer instead of two will result in half the transaction fee paid compared to
setting the vote authority and validator identity to two different accounts.

The vote authority can be set when the vote account is created. If it is not
provided, the default behavior is to assign it the same as the validator
identity. The vote authority can be changed later with the
[vote-authorize-voter-checked](../../cli/usage.md#solana-vote-authorize-voter-checked)
command.

The vote authority can be changed at most once per epoch. If the authority is
changed with
[vote-authorize-voter-checked](../../cli/usage.md#solana-vote-authorize-voter-checked),
this will not take effect until the beginning of the next epoch. To support a
smooth transition of the vote signing, `agave-validator` allows the
`--authorized-voter` argument to be specified multiple times. This allows the
validator process to keep voting successfully when the network reaches an epoch
boundary at which the validator's vote authority account changes.

### Authorized Withdrawer

The _authorized withdrawer_ keypair is used to withdraw funds from a vote
account using the
[withdraw-from-vote-account](../../cli/usage.md#solana-withdraw-from-vote-account)
command. Any network rewards a validator earns are deposited into the vote
account and are only retrievable by signing with the authorized withdrawer
keypair.

The authorized withdrawer is also required to sign any transaction to change a
vote account's [commission](#commission), and to change the validator identity
on a vote account.

Because theft of an authorized withdrawer keypair can give complete control over
the operation of a validator to an attacker, it is advised to keep the withdraw
authority keypair in an offline/cold wallet in a secure location. The withdraw
authority keypair is not needed during operation of a validator and should not
stored on the validator itself.

The authorized withdrawer must be set when the vote account is created. It must
not be set to a keypair that is the same as either the validator identity
keypair or the vote authority keypair.

The authorized withdrawer can be changed later with the
[vote-authorize-withdrawer-checked](../../cli/usage.md#solana-vote-authorize-withdrawer-checked)
command.

### Commission

_Commission_ is the percent of network rewards earned by a validator that are
deposited into the validator's vote account. The remainder of the rewards are
distributed to all of the stake accounts delegated to that vote account,
proportional to the active stake weight of each stake account.

For example, if a vote account has a commission of 10%, for all rewards earned
by that validator in a given epoch, 10% of these rewards will be deposited into
the vote account in the first block of the following epoch. The remaining 90%
will be deposited into delegated stake accounts as immediately active stake.

A validator may choose to set a low commission to try to attract more stake
delegations as a lower commission results in a larger percentage of rewards
passed along to the delegator. As there are costs associated with setting up and
operating a validator node, a validator would ideally set a high enough
commission to at least cover their expenses.

Commission can be set upon vote account creation with the `--commission` option.
If it is not provided, it will default to 100%, which will result in all rewards
deposited in the vote account, and none passed on to any delegated stake
accounts.

Commission can also be changed later with the
[vote-update-commission](../../cli/usage.md#solana-vote-update-commission) command.

When setting the commission, only integer values in the set [0-100] are
accepted. The integer represents the number of percentage points for the
commission, so creating an account with `--commission 10` will set a 10%
commission.

Note that validators can only update their commission during the first half of
any epoch. This prevents validators from stealing delegator rewards by setting a
low commission, increasing it right before the end of the epoch, and then
changing it back after reward distribution.

## Key Rotation

Rotating the vote account authority keys requires special handling when dealing
with a live validator.

Note that vote account key rotation has no effect on the stake accounts that
have been delegated to the vote account. For example it is possible to use key
rotation to transfer all authority of a vote account from one entity to another
without any impact to staking rewards.

### Vote Account Validator Identity

You will need access to the _authorized withdrawer_ keypair for the vote account
to change the validator identity. The following steps assume that
`~/authorized_withdrawer.json` is that keypair.

1. Create the new validator identity keypair,
   `solana-keygen new -o ~/new-validator-keypair.json`.
2. Ensure that the new identity account has been funded,
   `solana transfer ~/new-validator-keypair.json 500`.
3. Run
   `solana vote-update-validator ~/vote-account-keypair.json ~/new-validator-keypair.json ~/authorized_withdrawer.json`
   to modify the validator identity in your vote account
4. Restart your validator with the new identity keypair for the `--identity`
   argument

**Additional steps are required if your validator has stake.** The leader
schedule is computed two epochs in advance. Therefore if your old validator
identity was in the leader schedule, it will remain in the leader schedule for
up to two epochs after the validator identity change. If extra steps are not
taken your validator will produce no blocks until your new validator identity is
added to the leader schedule.

After your validator is restarted with the new identity keypair, per step 4,
start a second non-voting validator on a different machine with the old identity
keypair without providing the `--vote-account` argument, as well as with the
`--no-wait-for-vote-to-start-leader` argument.

This temporary validator should be run for two full epochs. During this time it
will:

- Produce blocks for the remaining slots that are assigned to your old validator
  identity
- Receive the transaction fees and rent rewards for your old validator identity

It is safe to stop this temporary validator when your old validator identity is
no longer listed in the `solana leader-schedule` output.

### Vote Account Authorized Voter

The _vote authority_ keypair may only be changed at epoch boundaries and
requires some additional arguments to `agave-validator` for a seamless
migration.

1. Run `solana epoch-info`. If there is not much time remaining time in the
   current epoch, consider waiting for the next epoch to allow your validator
   plenty of time to restart and catch up.
2. Create the new vote authority keypair,
   `solana-keygen new -o ~/new-vote-authority.json`.
3. Determine the current _vote authority_ keypair by running
   `solana vote-account ~/vote-account-keypair.json`. It may be validator's
   identity account (the default) or some other keypair. The following steps
   assume that `~/validator-keypair.json` is that keypair.
4. Run
   `solana vote-authorize-voter-checked ~/vote-account-keypair.json ~/validator-keypair.json ~/new-vote-authority.json`.
   The new vote authority is scheduled to become active starting at the next
   epoch.
5. `agave-validator` now needs to be restarted with the old and new vote
   authority keypairs, so that it can smoothly transition at the next epoch. Add
   the two arguments on restart:
   `--authorized-voter ~/validator-keypair.json --authorized-voter ~/new-vote-authority.json`
6. After the cluster reaches the next epoch, remove the
   `--authorized-voter ~/validator-keypair.json` argument and restart
   `agave-validator`, as the old vote authority keypair is no longer required.

### Vote Account Authorized Withdrawer

No special handling or timing considerations are required. Use the
`solana vote-authorize-withdrawer-checked` command as needed.

### Consider Durable Nonces for a Trustless Transfer of the Authorized Voter or Withdrawer

If the Authorized Voter or Withdrawer is to be transferred to another entity
then a two-stage signing process using a
[Durable Nonce](../../cli/examples/durable-nonce.md) is recommended.

1. Entity B creates a durable nonce using `solana create-nonce-account`
2. Entity B then runs a `solana vote-authorize-voter-checked` or
   `solana vote-authorize-withdrawer-checked` command, including:

- the `--sign-only` argument
- the `--nonce`, `--nonce-authority`, and `--blockhash` arguments to specify the
  nonce particulars
- the address of the Entity A's existing authority, and the keypair for Entity
  B's new authority

3. When the `solana vote-authorize-...-checked` command successfully executes,
   it will output transaction signatures that Entity B must share with Entity A
4. Entity A then runs a similar `solana vote-authorize-voter-checked` or
   `solana vote-authorize-withdrawer-checked` command with the following
   changes:

- the `--sign-only` argument is removed, and replaced with a `--signer` argument
  for each of the signatures provided by Entity B
- the address of Entity A's existing authority is replaced with the
  corresponding keypair, and the keypair for Entity B's new authority is
  replaced with the corresponding address

On success the authority is now changed without Entity A or B having to reveal
keypairs to the other even though both entities signed the transaction.

## Close a Vote Account

A vote account can be closed with the
[close-vote-account](../../cli/usage.md#solana-close-vote-account) command. Closing
a vote account withdraws all remaining SOL funds to a supplied recipient address
and renders it invalid as a vote account. It is not possible to close a vote
account with active stake.

================
File: docs/src/operations/_category_.json
================
{
  "position": 4,
  "label": "Operating a Validator",
  "collapsible": true,
  "collapsed": true
}

================
File: docs/src/operations/index.md
================
---
title: Operating a Validator
sidebar_position: 0
---

This section describes how to run an Agave validator node.

There are several clusters available to connect to; see [Choosing a Cluster](../cli/examples/choose-a-cluster.md) for an overview of each.

================
File: docs/src/operations/prerequisites.md
================
---
title: Agave Validator Prerequisites
sidebar_position: 2
sidebar_label: Prerequisites
pagination_label: Prerequisites to run a Validator
---

Operating an Agave validator is an interesting and rewarding task. Generally speaking, it requires someone with a technical background but also involves community engagement and marketing.

## How to be a good Validator Operator

Here is a list of some of the requirements for being a good operator:

- Performant computer hardware and a fast internet connection
  - You can find a list of [hardware requirements here](./requirements.md)
- Knowledge of the Linux terminal
- Linux system administration
  - Accessing your machine via ssh and scp
  - Installing software (installing from source is encouraged)
  - Keeping your Linux distribution up to date
  - Managing users and system access
  - Understanding computer processes
  - Understanding networking basics
  - Formatting and mounting drives
  - Managing firewall rules (UFW/iptables)
- Hardware performance monitoring
- Cluster and node monitoring
- Quick response times in case of a validator issue
- Marketing and communications to attract delegators
- Customer support

Whether you decide to run a [validator](../what-is-a-validator.md) or an [RPC node](../what-is-an-rpc-node.md), you should consider all of these areas of expertise. A team of people is likely necessary for you to achieve your goals.

## Can I use my computer at home?

While anyone can join the network, you should make sure that your home computer and network meets the specifications in the [hardware requirements](./requirements.md) doc. Most home internet service providers do not provide consistent service that would allow your validator to perform well. If your home network or personal hardware is not performant enough to keep up with the Solana cluster, your validator will not be able to participate in consensus.

In addition to performance considerations, you will want to make sure that your home computer is resistant to outages caused by loss of power, flooding, fire, theft, etc. If you are just getting started on the testnet cluster and learning about being an operator, a home setup may be sufficient, but you will want to consider all of these factors when you start operating your validator on the mainnet-beta cluster.

================
File: docs/src/operations/requirements.md
================
---
title: Agave Validator Requirements
sidebar_position: 3
sidebar_label: Requirements
pagination_label: Requirements to Operate a Validator
---

## Minimum SOL requirements

There is no strict minimum amount of SOL required to run an Agave validator on Solana.

However in order to participate in consensus, a vote account is required which
has a rent-exempt reserve of 0.02685864 SOL. Voting also requires sending a vote
transaction for each block the validator agrees with, which can cost up to
1.1 SOL per day.

## Hardware Recommendations

The hardware recommendations below are provided as a guide.  Operators are encouraged to do their own performance testing.

| Component | Validator Requirements | Additional RPC Node Requirements |
|-----------|------------------------|----------------------------------|
| **CPU**   | - 2.8GHz base clock speed, or faster<br />- SHA extensions instruction support<br />- AMD Gen 3 or newer<br />- Intel Ice Lake or newer<br />- Higher clock speed is preferable over more cores<br />- AVX2 instruction support (to use official release binaries, self-compile otherwise)<br />- Support for AVX512f is helpful<br />||
| | 12 cores / 24 threads, or more  | 16 cores / 32 threads, or more |
| **RAM**   | Error Correction Code (ECC) memory is suggested<br />Motherboard with 512GB capacity suggested ||
| | 256GB or more| 512 GB or more for **all [account indexes](https://docs.anza.xyz/operations/setup-an-rpc-node#account-indexing)** |
| **Disk**  | PCIe Gen3 x4 NVME SSD, or better, on each of: <br />- **Accounts**: 1TB, or larger. High TBW (Total Bytes Written)<br />- **Ledger**: 1TB or larger. High TBW suggested<br />- **Snapshots**: 500GB or larger. High TBW suggested<br />- **OS**: (Optional) 500GB, or larger. SATA OK<br /><br />The OS may be installed on the ledger disk, though testing has shown better performance with the ledger on its own disk<br /><br />Accounts and ledger *can* be stored on the same disk, however due to high IOPS, this is not recommended<br /><br />The Samsung 970 and 980 Pro series SSDs are popular with the validator community | Consider a larger ledger disk if longer transaction history is required<br /><br />Accounts and ledger **should not** be stored on the same disk |

A community maintained list of currently optimal hardware can be found here: [solanahcl.org](https://solanahcl.org/). It is updated automatically from the [solanahcl/solanahcl Github repo](https://github.com/solanahcl/solanahcl).

## Virtual machines on Cloud Platforms

Running an Agave node in the cloud requires significantly greater
operational expertise to achieve stability and performance. Do not
expect to find sympathetic voices should you chose this route and
find yourself in need of support.

## Docker

Running an Agave validator for live clusters (including mainnet-beta) inside Docker is
not recommended and generally not supported. This is due to general concerns of
Docker's containerization overhead and resultant performance degradation unless
specially configured. We use Docker only for development purposes.

## Software

- We build and run on Ubuntu 24.04.
- See [Installing Solana CLI](../cli/install.md) for the current Solana CLI software release.

Prebuilt binaries are currently available targeting x86_64 with AVX2 support on Ubuntu 20.04. We
will stop publishing these in the near future (as of May 2025).
[Building from source](https://docs.anza.xyz/cli/install#build-from-source) is required for all
other supported target platforms (MacOS and Windows) today and suggested for linux as it will soon
be required there as well

## Networking
A stable connection with a public IPv4 address is required.

- For an unstaked node: 1 GBit/s symmetric connection is sufficient.
- For a staked node: at least 2 GBit/s symmetric connection is required, 10 GBit/s of available bandwidth is recommended for stable operation.

### Firewall
It is not recommended to run a validator behind a NAT. Operators who choose to
do so should be comfortable configuring their networking equipment and debugging
any traversal issues on their own. Upstream agave will generally not accept
patches to support operation behind NAT.

The following traffic needs to be allowed. Furthermore, there should not be any traffic filtering from your validator to internet.

#### Required

| Source | Destination         | Protocol    | Port(s)    | Comment                                                                                                                  |
|--------|---------------------|-------------|------------|--------------------------------------------------------------------------------------------------------------------------|
| any    | your validator's IP | TCP and UDP | 8000-8030 | P2P protocols (gossip, turbine, repair, etc). This can be limited to any free port range with  `--dynamic-port-range` |

#### Recommended
When you manage your validator via SSH it is recommended to limit the allowed SSH traffic to your validator management IP.

| Source          | Destination                    | Protocol | Port(s) | Comment                                                                     |
|-----------------|--------------------------------|----------|---------|-----------------------------------------------------------------------------|
| your IP address | your validator's management IP | TCP      | 22      | SSH management traffic. Port can be different depending on your SSH config. |

***Please note that your source IP address should be a static public IP address. Please check with your service provider if you're not sure if your public IP address is static.***

#### Optional
For security purposes, it is not suggested that the following ports be open to
the internet on staked, mainnet-beta validators.

| Source   | Destination         | Protocol | Port(s) | Comment                                                |
|----------|---------------------|----------|---------|--------------------------------------------------------|
| RPC user | your validator's IP | TCP      | 8899    | JSONRPC over HTTP. Change with `--rpc-port RPC_PORT`   |
| RPC user | your validator's IP | TCP      | 8900    | JSONRPC over Websockets. Derived. Uses  `RPC_PORT + 1` |

================
File: docs/src/operations/setup-a-validator.md
================
---
title: Setup an Agave Validator
sidebar_label: Setup an Agave Validator
sidebar_position: 5
---

This is a guide for getting your validator setup on the Solana testnet cluster
for the first time. Testnet is a Solana cluster used for performance
testing of the software before the software is used on mainnet. Since testnet is
stress tested daily, it is a good cluster to practice validator operations.

Once you have a working validator on testnet, you will want to learn about
[operational best practices](./best-practices/general.md) in the next section.
Although the guide is specific to testnet, it can be adapted to mainnet or
devnet as well.

> Refer to the [Available Clusters](../clusters/available.md) section of the
> documentation to see example commands for each cluster.

Now let's get started.

## Open The Terminal Program

To start this guide, you will be running commands on your trusted computer, not
on the remote machine that you plan to use for validator operations. First,
locate the terminal program on your _trusted computer_.

- on Mac, you can search for the word _terminal_ in spotlight.
- on Ubuntu, you can type `CTRL + Alt + T`.
- on Windows, you will have to open the command prompt as an Administrator.

## Install The Solana CLI Locally

Validator operators are required to install the tools included in the Solana CLI using the [installation instructions](../cli/install.md).

Once the Solana CLI is installed, you can return to this document once you are
able to run two commands and get an answer on your terminal.

First, run the following command to verify that the Solana CLI is installed:

```
solana --version
```

You should see an output that looks similar to this (note your version number
may be higher):

```
solana-cli 1.14.17 (src:b29a37cf; feat:3488713414)
```

Now, run the following command to verify that the agave-validator binary is
installed:

```
agave-validator --version
```

You should see an output that looks similar to this (note your version number
may be higher):

```
agave-validator 2.3.1 (src:e3eca4c1; feat:3640012085, client:Agave)
```

Once you have successfully installed the cli and validator binary, the next step is to change your
config so that it is making requests to the `testnet` cluster:

```
solana config set --url https://api.testnet.solana.com
```

To verify that your config has changed, run:

```
solana config get
```

You should see a line that says: `RPC URL: https://api.testnet.solana.com`

## Create Keys

On your local computer, create the 3 keypairs that you will need to run your
validator ([docs for reference](./guides/validator-start.md#generate-identity)):

> **NOTE** Some operators choose to make vanity keypairs for their identity and
> vote account using the `grind` sub command
> ([docs for reference](./guides/validator-start.md#vanity-keypair)).

```
solana-keygen new -o validator-keypair.json
```

```
solana-keygen new -o vote-account-keypair.json
```

```
solana-keygen new -o authorized-withdrawer-keypair.json
```

> **IMPORTANT** the `authorized-withdrawer-keypair.json` should be considered
> very sensitive information. Many operators choose to use a multisig, hardware
> wallet, or paper wallet for the authorized withdrawer keypair. A keypair is
> created on disk in this example for simplicity. Additionally, the withdrawer
> keypair should always be stored safely. The authorized withdrawer keypair
> should **never** be stored on the remote machine that the validator software
> runs on. For more information, see
> [validator security best practices](./best-practices/security.md#do-not-store-your-withdrawer-key-on-your-validator)

## Create a Vote Account

Before you can create your vote account, you need to configure the Solana
command line tool a bit more.

The below command sets the default keypair that the Solana CLI uses to the
`validator-keypair.json` file that you just created in the terminal:

```
solana config set --keypair ./validator-keypair.json
```

Now verify your account balance of `0`:

```
solana balance
```

Next, you need to deposit some SOL into that keypair account in order create a
transaction (in this case, making your vote account):

```
solana airdrop 1
```

> **NOTE** The `airdrop` sub command does not work on mainnet, so you will have
> to acquire SOL and transfer it into this keypair's account if you are setting
> up a mainnet validator.

Now, use the Solana cluster to create a vote account.

As a reminder, all commands mentioned so far **should be done on your trusted
computer** and **NOT** on a server where you intend to run your validator. It is
especially important that the following command is done on a **trusted
computer**:

```
solana create-vote-account -ut \
    --fee-payer ./validator-keypair.json \
    ./vote-account-keypair.json \
    ./validator-keypair.json \
    ./authorized-withdrawer-keypair.json
```

> Note `-ut` tells the cli command that we would like to use the testnet
> cluster. `--fee-payer` specifies the keypair that will be used to pay the
> transaction fees. Both flags are not necessary if you configured the solana
> cli properly above but they are useful to ensure you're using the intended
> cluster and keypair.

## Save the Withdrawer Keypair Securely

Make sure your `authorized-withdrawer-keypair.json` is stored in a safe place.
If you have chosen to create a keypair on disk, you should first backup the
keypair and then delete it from your local machine.

**IMPORTANT**: If you lose your withdrawer key pair, you will lose control of
your vote account. You will not be able to withdraw tokens from the vote account
or update the withdrawer. Make sure to store the
`authorized-withdrawer-keypair.json` securely before you move on.

## SSH To Your Validator

Connect to your remote server. This is specific to your server but will look
something like this:

```
ssh user@<server.hostname>
```

You will have to check with your server provider to get the correct user account
and hostname that you will ssh into.

## Update Your Ubuntu Packages

Make sure you have the latest and greatest package versions on your server

```
sudo apt update
sudo apt upgrade
```

## Sol User

Create a new Ubuntu user, named `sol`, for running the validator:

```
sudo adduser sol
```

It is a best practice to always run your validator as a non-root user, like the
`sol` user we just created.

## Hard Drive Setup

On your Ubuntu computer make sure that you have at least `2TB` of disk space
mounted. You can check disk space using the `df` command:

```
df -h
```

> If you have a drive that is not mounted/formatted, you will have to set up the
> partition and mount the drive.

To see the hard disk devices that you have available, use the list block devices
command:

```
lsblk -f
```

You may see some devices in the list that have a name but do not have a UUID.
Any device without a UUID is unformatted.

### Drive Formatting: Ledger

Assuming you have an nvme drive that is not formatted, you will have to format
the drive and then mount it.

For example, if your computer has a device located at `/dev/nvme0n1`, then you
can format the drive with the command:

```
sudo mkfs -t ext4 /dev/nvme0n1
```

For your computer, the device name and location may be different.

Next, check that you now have a UUID for that device:

```
lsblk -f
```

In the fourth column, next to your device name, you should see a string of
letters and numbers that look like this: `6abd1aa5-8422-4b18-8058-11f821fd3967`.
That is the UUID for the device.

### Mounting Your Drive: Ledger

So far we have created a formatted drive, but you do not have access to it until
you mount it. Make a directory for mounting your drive:

```
sudo mkdir -p /mnt/ledger
```

Next, change the ownership of the directory to your `sol` user:

```
sudo chown -R sol:sol /mnt/ledger
```

Now you can mount the drive:

```
sudo mount /dev/nvme0n1 /mnt/ledger
```

### Formatting And Mounting Drive: AccountsDB

You will also want to mount the accounts db on a separate hard drive. The
process will be similar to the ledger example above.

Assuming you have a device at `/dev/nvme1n1`, format the device and verify it
exists:

```
sudo mkfs -t ext4 /dev/nvme1n1
```

Then verify the UUID for the device exists:

```
lsblk -f
```

Create a directory for mounting:

```
sudo mkdir -p /mnt/accounts
```

Change the ownership of that directory:

```
sudo chown -R sol:sol /mnt/accounts
```

And lastly, mount the drive:

```
sudo mount /dev/nvme1n1 /mnt/accounts
```

## System Tuning

### Linux

Your system will need to be tuned to run properly. Your validator may
not start without the settings below.

#### **Optimize sysctl knobs**

```bash
sudo bash -c "cat >/etc/sysctl.d/21-agave-validator.conf <<EOF
# Increase max UDP buffer sizes
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728

# Increase memory mapped files limit
vm.max_map_count = 1000000

# Increase number of allowed open file descriptors
fs.nr_open = 1000000
EOF"
```

```bash
sudo sysctl -p /etc/sysctl.d/21-agave-validator.conf
```

#### **Increase systemd and session file limits**

Add

```
LimitNOFILE=1000000
LimitMEMLOCK=2000000000
```

to the `[Service]` section of your systemd service file, if you use one,
otherwise add

```
DefaultLimitNOFILE=1000000
DefaultLimitMEMLOCK=2000000000
```

to the `[Manager]` section of `/etc/systemd/system.conf`.

```bash
sudo systemctl daemon-reload
```

```bash
sudo bash -c "cat >/etc/security/limits.d/90-solana-nofiles.conf <<EOF
# Increase process file descriptor count limit
* - nofile 1000000
# Increase memory locked limit (kB)
* - memlock 2000000
EOF"
```

```bash
### Close all open sessions (log out then, in again) ###
```

## Copy Key Pairs

On your personal computer, not on the validator, securely copy your
`validator-keypair.json` file and your `vote-account-keypair.json` file to the
validator server:

```
scp validator-keypair.json sol@<server.hostname>:
scp vote-account-keypair.json sol@<server.hostname>:
```

> **Note**: The `vote-account-keypair.json` does not have any function other
> than identifying the vote account to potential delegators. Only the public key
> of the vote account is important once the account is created.

## Switch to the sol User

On the validator server, switch to the `sol` user:

```
su - sol
```

## Install agave-validator on Remote Machine

Your remote machine will need `agave-validator` installed to run the Agave validator
software. For simplicity, install the application with user `sol`. Refer again to
[build from source](../cli/install.md#build-from-source).

## Create A Validator Startup Script

In your sol home directory (e.g. `/home/sol/`), create a folder called `bin`.
Inside that folder create a file called `validator.sh` and make it executable:

```
mkdir -p /home/sol/bin
touch /home/sol/bin/validator.sh
chmod +x /home/sol/bin/validator.sh
```

Next, open the `validator.sh` file for editing:

```
nano /home/sol/bin/validator.sh
```

Copy and paste the following contents into `validator.sh` then save the file:

```
#!/bin/bash
exec agave-validator \
    --identity /home/sol/validator-keypair.json \
    --vote-account /home/sol/vote-account-keypair.json \
    --known-validator 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on \
    --known-validator 7XSY3MrYnK8vq693Rju17bbPkCN3Z7KvvfvJx4kdrsSY \
    --known-validator Ft5fbkqNa76vnsjYNwjDZUXoTWpP7VYm3mtsaQckQADN \
    --known-validator 9QxCLckBiJc783jnMvXZubK4wH86Eqqvashtrwvcsgkv \
    --only-known-rpc \
    --log /home/sol/agave-validator.log \
    --ledger /mnt/ledger \
    --accounts /mnt/accounts \
    --rpc-port 8899 \
    --dynamic-port-range 8000-8020 \
    --entrypoint entrypoint.testnet.solana.com:8001 \
    --entrypoint entrypoint2.testnet.solana.com:8001 \
    --entrypoint entrypoint3.testnet.solana.com:8001 \
    --expected-genesis-hash 4uhcVJyU9pJkvQyS88uRDiswHXSCkY3zQawwpjk2NsNY \
    --wal-recovery-mode skip_any_corrupted_record \
    --limit-ledger-size
```

Refer to `agave-validator --help` for more information on what each flag is
doing in this script. Also refer to the section on
[best practices for operating a validator](./best-practices/general.md).

This startup script is specifically intended for testnet. For more startup script examples intended for other clusters, refer to the
[clusters section.](./../clusters/available.md).
## Verifying Your Validator Is Working

Test that your `validator.sh` file is running properly by executing the
`validator.sh` script:

```
/home/sol/bin/validator.sh
```

The script should execute the `agave-validator` process. In a new terminal
window, ssh into your server, then verify that the process is running:

```
ps aux | grep agave-validator
```

You should see a line in the output that includes `agave-validator` with all
the flags that were added to your `validator.sh` script.

Next, we need to look at the logs to make sure everything is operating properly.

### Tailing The Logs

As a spot check, you will want to make sure your validator is producing
reasonable log output (**warning**, there will be a lot of log output).

In a new terminal window, ssh into your validator machine, switch users to the
`sol` user and `tail` the logs:

```
su - sol
tail -f agave-validator.log
```

The `tail` command will continue to display the output of a file as the file
changes. You should see a continuous stream of log output as your validator
runs. Keep an eye out for any lines that say `_ERROR_`.

Assuming you do not see any error messages, exit out of the command.

### Gossip Protocol

Gossip is a protocol used in the Solana clusters to communicate between
validator nodes. For more information on gossip, see
[Gossip Service](../validator/gossip.md). To verify that your validator is
running properly, make sure that the validator has registered itself with the
gossip network.

In a new terminal window, connect to your server via ssh. Identify your
validator's pubkey:

```
solana-keygen pubkey ~/validator-keypair.json
```

The command `solana gossip` lists all validators that have registered with the
protocol. To check that the newly setup validator is in gossip, we will `grep`
for our pubkey in the output:

```
solana gossip | grep <pubkey>
```

After running the command, you should see a single line that looks like this:

```
139.178.68.207  | 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on | 8001   | 8004  | 139.178.68.207:80     | 1.14.17 | 3488713414
```

If you do not see any output after grep-ing the output of gossip, your validator
may be having startup problems. If that is the case, start debugging by looking
through the validator log output.

### Solana Validators

After you have verified that your validator is in gossip, you should stake some
SOL to your validator. Once the stake has activated (which happens at the start
of the next epoch), you can verify that your validator is ready to be a voting
participant of the network with the `solana validators` command. The command
lists all validators in the network, but like before, we can `grep` the output
for the validator we care about:

```
solana validators | grep <pubkey>
```

You should see a line of output that looks like this:

```
5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on  FX6NNbS5GHc2kuzgTZetup6GZX6ReaWyki8Z8jC7rbNG  100%  197434166 (  0)  197434133 (  0)   2.11%   323614  1.14.17   2450110.588302720 SOL (1.74%)
```

### Solana Catchup

The `solana catchup` command is a useful tool for seeing how quickly your
validator is processing blocks. The Solana network has the capability to produce
many transactions per second. Since your validator is new to the network, it has
to ask another validator (listed as a `--known-validator` in your startup
script) for a recent snapshot of the ledger. By the time you receive the
snapshot, you may already be behind the network. Many transactions may have been
processed and finalized in that time. In order for your validator to participate
in consensus, it must _catchup_ to the rest of the network by asking for the
more recent transactions that it does not have.

The `solana catchup` command is a tool that tells you how far behind the network
your validator is and how quickly you are catching up:

```
solana catchup <pubkey>
```

If you see a message about trying to connect, your validator may not be part of
the network yet. Make sure to check the logs and double check `solana gossip`
and `solana validators` to make sure your validator is running properly.

Once you are happy that the validator can start up without errors, the next step
is to create a system service to run the `validator.sh` file automatically. Stop
the currently running validator by pressing `CTRL+C` in the window where
`validator.sh` is running.

## Create a System Service

Follow these instructions for
[running the validator as a system service](./guides/validator-start.md#systemd-unit)

Make sure to implement log rotate as well. Once you have the system service
configured, start your validator using the newly configured service:

```
sudo systemctl enable --now sol
```

Now verify that the validator is running properly by tailing the logs and using
the commands mentioned earlier to check gossip and Solana validators:

```
tail -f /home/sol/agave-validator*.log
```

## Monitoring

`agave-watchtower` is a command you can run on a separate machine to monitor
your server. You can read more about handling
[automatic restarts and monitoring](./best-practices/monitoring.md#agave-watchtower)
using Solana Watchtower here in the docs.

## Common issues

### Out of disk space

Make sure your ledger is on drive with at least `2TB` of space.

### Validator not catching up

This could be a networking/hardware issue, or you may need to get the latest
snapshot from another validator node.

### PoH hashes/second rate is slower than the cluster target

If you are using `agave-validator` built from source, ensure that you are using a `release` build and not a `debug` build

Ensure that your machine's CPU base clock speed is 2.8GHz or faster. Use `lscpu` to check your clock speed. `CPU(s) scaling MHz` can cause your clock speed to be underclocked. Some additional tuning:

Set performance governor
```bash
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
```

Force minimum frequency to maximum
```bash
# Example if your maximum GHz is 2.8
echo 2850000 | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_min_freq

================
File: docs/src/operations/setup-an-rpc-node.md
================
---
title: Setup an Agave RPC Node
sidebar_label: Setup an Agave RPC Node
sidebar_position: 6
---

Since a Solana RPC server runs the same process as a consensus validator, first follow the instructions on
[how to setup a Solana validator](./setup-a-validator.md) to get started.
Note that you do not need to create a vote account if you are operating an RPC node.
An RPC node typically does not vote.

After your validator is running, you can refer to this section for the RPC node specific setup instructions.

## Sample RPC Node

Below is an example `validator.sh` file for a `testnet` RPC server.

You will want to be aware of the following flags:

- `--full-rpc-api`: enables all RPC operations on this validator.
- `--no-voting`: runs the validator without participating in consensus. Typically, you do not want to run a validator as _both_ a consensus node and a full RPC node due to resource constraints.
- `--private-rpc`: does not publish the validator's open RPC port in the `solana gossip` command

> For more explanation on the flags used in the command, refer to the `agave-validator --help` command

```
#!/bin/bash
exec agave-validator \
    --identity /home/sol/validator-keypair.json \
    --known-validator 5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on \
    --known-validator dDzy5SR3AXdYWVqbDEkVFdvSPCtS9ihF5kJkHCtXoFs \
    --known-validator eoKpUABi59aT4rR9HGS3LcMecfut9x7zJyodWWP43YQ \
    --known-validator 7XSY3MrYnK8vq693Rju17bbPkCN3Z7KvvfvJx4kdrsSY \
    --known-validator Ft5fbkqNa76vnsjYNwjDZUXoTWpP7VYm3mtsaQckQADN \
    --known-validator 9QxCLckBiJc783jnMvXZubK4wH86Eqqvashtrwvcsgkv \
    --only-known-rpc \
    --full-rpc-api \
    --no-voting \
    --ledger /mnt/ledger \
    --accounts /mnt/accounts \
    --log /home/sol/solana-rpc.log \
    --rpc-port 8899 \
    --rpc-bind-address 0.0.0.0 \
    --private-rpc \
    --dynamic-port-range 8000-8020 \
    --entrypoint entrypoint.testnet.solana.com:8001 \
    --entrypoint entrypoint2.testnet.solana.com:8001 \
    --entrypoint entrypoint3.testnet.solana.com:8001 \
    --expected-genesis-hash 4uhcVJyU9pJkvQyS88uRDiswHXSCkY3zQawwpjk2NsNY \
    --wal-recovery-mode skip_any_corrupted_record \
    --limit-ledger-size
```

### Solana Bigtable

The Solana blockchain is able to create many transactions per second. Because of the volume of transactions on the chain, it is not practical for an RPC node to store the entire blockchain on the machine. Instead, RPC operators use the `--limit-ledger-size` flag to specify how many blocks to store on the RPC node. If the user of the RPC node needs historical blockchain data, then the RPC server will have to access older blocks through a Solana bigtable instance.

If you are interested in setting up your own bigtable instance, see these docs in the Solana GitHub repository: [solana-labs/solana-bigtable](https://github.com/solana-labs/solana-bigtable)

### Example Known Validators

The identities of the [known validators](./guides/validator-start.md#known-validators) supplied in these example snippets (via the `--known-validator` flag) are:

- `5D1fNXzvv5NjV1ysLjirC4WY92RNsVH18vjmcszZd8on` - Anza
- `dDzy5SR3AXdYWVqbDEkVFdvSPCtS9ihF5kJkHCtXoFs` - MonkeDAO
- `Ft5fbkqNa76vnsjYNwjDZUXoTWpP7VYm3mtsaQckQADN` - Certus One
- `eoKpUABi59aT4rR9HGS3LcMecfut9x7zJyodWWP43YQ` - SerGo
- `9QxCLckBiJc783jnMvXZubK4wH86Eqqvashtrwvcsgkv` - Algo|Stake

## Examples for other clusters

Additional examples of other Solana cluster-specific validator commands can be found on the [Clusters](../clusters/available.md) page.

Keep in mind, you will still need to customize these commands to operate as an RPC node, as well as other
operator-specific configuration settings.

## Account indexing

As the number of populated accounts on the cluster grows, account-data RPC
requests that scan the entire account set -- like
[`getProgramAccounts`](https://solana.com/docs/rpc/http/getprogramaccounts) and
[SPL-token-specific requests](https://solana.com/docs/rpc/http/gettokenaccountsbydelegate) --
may perform poorly. If your validator needs to support any of these requests,
you can use the `--account-index` parameter to activate one or more in-memory
account indexes that significantly improve RPC performance by indexing accounts
by the key field. Currently, it supports the following parameter values:

- `program-id`: each account indexed by its owning program; used by [getProgramAccounts](https://solana.com/docs/rpc/http/getprogramaccounts)
- `spl-token-mint`: each SPL token account indexed by its token Mint; used by [getTokenAccountsByDelegate](https://solana.com/docs/rpc/http/gettokenaccountsbydelegate), and [getTokenLargestAccounts](https://solana.com/docs/rpc/http/gettokenlargestaccounts)
- `spl-token-owner`: each SPL token account indexed by the token-owner address; used by [getTokenAccountsByOwner](https://solana.com/docs/rpc/http/gettokenaccountsbyowner), and [getProgramAccounts](https://solana.com/docs/rpc/http/getprogramaccounts) requests that include an spl-token-owner filter.

================
File: docs/src/operations/validator-or-rpc-node.md
================
---
title: Consensus Validator or RPC Node?
sidebar_position: 1
sidebar_label: Validator vs RPC Node
pagination_label: Consensus Validator vs RPC Node
---

Operators who run a [consensus validator](../what-is-a-validator.md) have much
different incentives than operators who run an
[RPC node](../what-is-an-rpc-node.md). You will have to decide which choice is
best for you based on your interests, technical background, and goals.

## Consensus Validators

As a validator your primary focus is maintaining the network and making sure
that your node is performing optimally so that you can fully participate in the
cluster consensus. You will want to attract a delegation of SOL to your
validator which will allow your validator the opportunity to produce more blocks
and earn rewards.

Each staked validator earns inflation rewards from
[vote credits](https://solana.com/docs/terminology#vote-credit). Vote credits
are assigned to validators that vote on
[blocks](https://solana.com/docs/terminology#block) produced by the
[leader](https://solana.com/docs/terminology#leader). The vote credits are given
to all validators that successfully vote on blocks that are added to the
blockchain. Additionally, when the validator is the leader, it can earn
transaction fees and storage
[rent fees](https://solana.com/docs/core/accounts#rent) for each block that it
produces that is added to the blockchain.

Since all votes in Solana happen on the blockchain, a validator incurs a
transaction cost for each vote that it makes. These transaction fees amount to
approximately 1.0 SOL per day.

> It is important to make sure your validator always has enough SOL in its
> identity account to pay for these transactions!

### Economics of running a consensus validator

As an operator, it is important to understand how a consensus validator spends
and earns sol through the protocol.

All validators who vote (consensus validators) must pay vote transaction fees
for blocks that they agree with. The cost of voting can be up to 1.1 SOL per
day.

A voting validator can earn SOL through 2 methods:

1. Inflationary rewards paid at the end of an epoch. See
   [staking rewards](../implemented-proposals/staking-rewards.md)
2. Earning 50% of transaction fees for the blocks produced by the validator. See
   [transaction fee basic economic design](https://solana.com/docs/intro/transaction_fees#basic-economic-design)

The following links are community provided resources that discuss the economics
of running a validator:

- Michael Hubbard wrote an
  [article](https://laine-sa.medium.com/solana-staking-rewards-validator-economics-how-does-it-work-6718e4cccc4e)
  that explains the economics of Solana in more depth for stakers and for
  validators.
- Congent Crypto has written a
  [blog post](https://medium.com/@Cogent_Crypto/how-to-become-a-validator-on-solana-9dc4288107b7)
  that discusses economics and getting started.
- Cogent Crypto also provides a
  [validator profit calculator](https://cogentcrypto.io/ValidatorProfitCalculator)

## RPC Nodes

While RPC operators **do NOT** receive rewards (because the node is not
participating in voting), there are different motivations for running an RPC
node.

An RPC operator is providing a service to users who want to interact with the
Solana blockchain. Because your primary user is often technical, you will have
to be able to answer technical questions about performance of RPC calls. This
option may require more understanding of the
[core Solana architecture](../clusters/index.md).

If you are operating an RPC node as a business, your job will also involve
scaling your system to meet the demands of the users. For example, some RPC
providers create dedicated servers for projects that require a high volume of
requests to the node. Someone with a background in development operations or
software engineering will be a very important part of your team. You will need a
strong understanding of the Solana architecture and the
[JSON RPC API](https://solana.com/docs/rpc/http).

Alternatively, you may be a development team that would like to run their own
infrastructure. In this case, the RPC infrastructure could be a part of your
production stack. A development team could use the
[Geyser plugin](../validator/geyser.md), for example, to get
real time access to information about accounts or blocks in the cluster.

================
File: docs/src/pages/styles.module.css
================
.heroBanner {
⋮----
.cardTitle {
.buttons {
.features {
.featureImage {
.iconExternalIcon {

================
File: docs/src/proposals/accepted-design-proposals.md
================
---
title: Accepted Design Proposals
sidebar_position: 1
sidebar_label: Overview
---

These architectural proposals have been accepted by the Solana maintainers, but are not yet fully implemented. These proposals may be implemented as described, implemented differently as issues in the designs become evident, or not implemented at all.

## After Implemented

Once a proposal has been implemented, it will be moved to [Implemented Proposals](../implemented-proposals/index.md) and the details will be added to relevant sections of the docs.

## Submit a New Proposal

To submit a new design proposal, consult this guide on [how to submit a design proposal](../proposals.md#submit-a-design-proposal).

================
File: docs/src/proposals/accounts-db-replication.md
================
---
title: AccountsDB Replication for RPC Services
---

## Problem

Validators fall behind the network when bogged down by heavy RPC load. This
seems to be due to a combination of CPU load and lock contention caused by
serving RPC requests. The most expensive RPC requests involve account scans.

## Solution Overview

AccountsDB `replicas` that run separately from the main validator can be used to
offload account-scan requests. Replicas would only: request and pull account
updates from the validator, serve client account-state RPC requests, and manage
AccountsDb and AccountsBackgroundService clean + shrink.

The replica communicates to the main validator via a new RPC mechanism to fetch
metadata information about the replication and the accounts update from the validator.
The main validator supports only one replica node. A replica node can relay the
information to 1 or more other replicas forming a replication tree.

At the initial start of the replica node, it downloads the latest snapshot
from a validator and constructs the bank and AccountsDb from it. After that, it queries
its main validator for new slots and requests the validator to send the updated
accounts for that slot and update to its own AccountsDb.

The same RPC replication mechanism can be used between a replica to another replica.
This requires the replica to serve both the client and server in the replication model.

On the client RPC serving side, `JsonRpcAccountsService` is responsible for serving
the client RPC calls for accounts related information similar to the existing
`JsonRpcService`.

The replica will also take snapshots periodically so that it can start quickly after
a restart if the snapshot is not too old.

## Detailed Solution
The following sections provide more details of the design.

### Consistency Model
The AccountsDb information is replicated asynchronously from the main validator to the replica.
When a query against the replica's AccountsDb is made, the replica may not have the latest
information of the latest slot. In this regard, it will eventually be consistent. However, for
a particular slot, the information provided is consistent with that of its peer validator
for commitment levels confirmed and finalized. For V1, we only support queries at these two
levels.

### Solana Replica Node
A new node named solana-replica-node will be introduced whose main responsibility is to maintain
the AccountsDb replica. The RPC node or replica node is used interchangeably in this document.
It will be a separate executable from the validator.

The replica consists of the following major components:

The `ReplicaSlotConfirmationRequestor`: this service is responsible for periodically sending the
request `ReplicaSlotConfirmationRequest` to its peer validator or replica for the latest slots.
It specifies the latest slot (last_replicated_slot) for which the replica has already
fetched the accounts information for. This maintains the ReplWorkingSlotSet and manages
the lifecycle of BankForks, BlockCommitmentCache (for the highest confirmed slot) and
the optimistically confirmed bank.

The `ReplicaSlotConfirmationServer`: this service is responsible for serving the
`ReplicaSlotConfirmationRequest` and sends the `ReplicaSlotConfirmationResponse` back to the requestor.
The response consists of a vector of new slots the validator knows of which is later than the
specified last_replicated_slot. This service also runs in the main validator. This service
gets the slots for replication from the BankForks, BlockCommitmentCache and OptimisticallyConfirmedBank.

The `ReplicaAccountsRequestor`: this service is responsible for sending the request
`ReplicaAccountsRequest` to its peer validator or replica for the `ReplicaAccountInfo` for a
slot for which it has not completed accounts db replication. The `ReplicaAccountInfo` contains
the `ReplicaAccountMeta`, Hash and the AccountData. The `ReplicaAccountMeta` contains info about
the existing `AccountMeta` in addition to the account data length in bytes.

The `ReplicaAccountsServer`: this service is responsible for serving the `ReplicaAccountsRequest`
and sends `ReplicaAccountsResponse` to the requestor. The response contains the count of the
ReplAccountInfo and the vector of ReplAccountInfo. This service runs both in the validator
and the replica relaying replication information. The server can stream the account information
from its AccountCache or from the storage if already flushed. This is similar to how a snapshot
package is created from the AccountsDb with the difference that the storage does not need to be
flushed to the disk before streaming to the client. If the account data is in the cache, it can
be directly streamed. Care must be taken to avoid account data for a slot being cleaned while
serving the streaming. When attempting a replication of a slot, if the slot is already cleaned
up with accounts data cleaned as result of update in later rooted slots, the replica should
forsake this slot and try the later uncleaned root slot.

During replication we also need to replicate the information of accounts that have been cleaned
up due to zero lamports, i.e. we need to be able to tell the difference between an account in a
given slot which was not updated and hence has no storage entry in that slot, and one that
holds 0 lamports and has been cleaned up through the history. We may record this via some
"Tombstone" mechanism -- recording the dead accounts cleaned up for a slot. The tombstones
themselves can be removed after exceeding the retention period expressed as epochs. Any
attempt to replicate slots with tombstones removed will fail and the replica should skip
this slot and try later ones.

The `JsonRpcAccountsService`: this is the RPC service serving client requests for account
information. The existing JsonRpcService serves other client calls than AccountsDb ones.
The replica node only serves the AccountsDb calls.

The existing JsonRpcService requires `BankForks`, `OptimisticallyConfirmedBank` and
`BlockCommitmentCache` to load the Bank. The JsonRpcAccountsService will need to use
information obtained from ReplicaSlotConfirmationResponse to construct the AccountsDb.

The `AccountsBackgroundService`: this service also runs in the replica which is responsible
for taking snapshots periodically and shrinking the AccountsDb and doing accounts cleaning.
The existing code also uses BankForks which we need to keep in the replica.

### Compatibility Consideration

For protocol compatibility considerations, all the requests have the replication version which
is initially set to 1. Alternatively, we can use the validator's version. The RPC server side
shall check the request version and fail if it is not supported.

### Replication Setup
To limit adverse effects on the validator and the replica due to replication, they can be
configured with a list of replica nodes which can form a replication pair with it. And the
replica node is configured with the validator which can serve its requests.


### Fault Tolerance
The main responsibility of making sure the replication is tolerant of faults lies with the
replica. In case of request failures, the replica shall retry the requests.


### Interface

Following are the client RPC APIs supported by the replica node in JsonRpcAccountsService.

- getAccountInfo
- getBlockCommitment
- getMultipleAccounts
- getProgramAccounts
- getMinimumBalanceForRentExemption
- getInflationGovernor
- getInflationRate
- getEpochSchedule
- getRecentBlockhash
- getFees
- getFeeCalculatorForBlockhash
- getFeeRateGovernor
- getLargestAccounts
- getSupply
- getStakeActivation
- getTokenAccountBalance
- getTokenSupply
- getTokenLargestAccounts
- getTokenAccountsByOwner
- getTokenAccountsByDelegate

Following APIs are not included:

- getInflationReward
- getClusterNodes
- getRecentPerformanceSamples
- getGenesisHash
- getSignatureStatuses
- getMaxRetransmitSlot
- getMaxShredInsertSlot
- sendTransaction
- simulateTransaction
- getSlotLeader
- getSlotLeaders
- minimumLedgerSlot
- getBlock
- getBlockTime
- getBlocks
- getBlocksWithLimit
- getTransaction
- getSignaturesForAddress
- getFirstAvailableBlock
- getBlockProduction


Action Items

1. Build the replica framework and executable
2. Integrate snapshot restore code for bootstrap the AccountsDb.
3. Develop the ReplicaSlotConfirmationRequestor and ReplicaSlotConfirmationServer interface code
4. Develop the ReplicaSlotConfirmationRequestor and ReplicaSlotConfirmationServer detailed implementations: managing the ReplEligibleSlotSet lifecycle: adding new roots and deleting root to it. And interfaces managing ReplWorkingSlotSet interface: adding and removing. Develop component synthesising information from BankForks, BlockCommitmentCache and OptimistcallyConfirmedBank on the server side and maintaining information on the client side.
5. Develop the interface code for ReplicaAccountsRequestor and ReplicaAccountsServer
6. Develop detailed implementation for ReplicaAccountsRequestor and ReplicaAccountsServer and develop the replication account storage serializer and deserializer.
7. Develop the interface code JsonRpcAccountsService
8. Detailed Implementation of JsonRpcAccountsService, refactor code to share with part of JsonRpcService.
9. Integrate with the AccountsBackgroundService in the replica for shrinking, cleaning, snapshotting.
10. Metrics and performance testing

================
File: docs/src/proposals/bankless-leader.md
================
---
title: Bankless Leader
---

A bankless leader does the minimum amount of work to produce a valid block. The
leader is tasked with ingress transactions, sorting and filtering valid
transactions, arranging them into entries, shredding the entries and
broadcasting the shreds. While a validator only needs to reassemble the block
and replay execution of well formed entries. The leader does 3x more memory
operations before any bank execution than the validator per processed
transaction.

## Rationale

Normal bank operation for a spend needs to do 2 loads and 2 stores. With this
design leader just does 1 load. so 4x less account_db work before generating the
block. The store operations are likely to be more expensive than reads.

When replay stage starts processing the same transactions, it can assume that
PoH is valid, and that all the entries are safe for parallel execution. The fee
accounts that have been loaded to produce the block are likely to still be in
memory, so the additional load should be warm and the cost is likely to be
amortized.

## Fee Account

The [fee account](https://solana.com/docs/terminology#fee_account) pays for the
transaction to be included in the block. The leader only needs to validate that
the fee account has the balance to pay for the fee.

## Balance Cache

For the duration of the leaders consecutive blocks, the leader maintains a
temporary balance cache for all the processed fee accounts. The cache is a map
of pubkeys to lamports.

At the start of the first block the balance cache is empty. At the end of the
last block the cache is destroyed.

The balance cache lookups must reference the same base fork for the entire
duration of the cache. At the block boundary, the cache can be reset along with
the base fork after replay stage finishes verifying the previous block.

## Balance Check

Prior to the balance check, the leader validates all the signatures in the
transaction.

1. Verify the accounts are not in use and BlockHash is valid.
2. Check if the fee account is present in the cache, or load the account from
   accounts_db and store the lamport balance in the cache.
3. If the balance is less than the fee, drop the transaction.
4. Subtract the fee from the balance.
5. For all the keys in the transaction that are Credit-Debit and are referenced
   by an instruction, reduce their balance to 0 in the cache. The account fee is
   declared as Credit-Debit, but as long as it is not used in any instruction
   its balance will not be reduced to 0.

## Leader Replay

Leaders will need to replay their blocks as part of the standard replay stage
operation.

## Leader Replay With Consecutive Blocks

A leader can be scheduled to produce multiple blocks in a row. In that scenario
the leader is likely to be producing the next block while the replay stage for
the first block is playing.

When the leader finishes the replay stage it can reset the balance cache by
clearing it, and set a new fork as the base for the cache which can become
active on the next block.

## Resetting the Balance Cache

1. At the start of the block, if the balance cache is uninitialized, set the
   base fork for the balance cache to be the parent of the block and create an
   empty cache.
2. if the cache is initialized, check if block's parents has a new frozen bank
   that is newer than the current base fork for the balance cache.
3. if a parent newer than the cache's base fork exist, reset the cache to the
   parent.

## Impact on Clients

The same fee account can be reused many times in the same block until it is used
once as Credit-Debit by an instruction.

Clients that transmit a large number of transactions per second should use a
dedicated fee account that is not used as Credit-Debit in any instruction.

Once an account fee is used as Credit-Debit, it will fail the balance check
until the balance cache is reset.

### Check out the [SIMD here to contribute](https://github.com/solana-foundation/solana-improvement-documents/pull/5)

================
File: docs/src/proposals/block-confirmation.md
================
---
title: Block Confirmation
---

A validator votes on a PoH hash for two purposes. First, the vote indicates it
believes the ledger is valid up until that point in time. Second, since many
valid forks may exist at a given height, the vote also indicates exclusive
support for the fork. This document describes only the former. The latter is
described in [Tower BFT](../implemented-proposals/tower-bft.md).

## Current Design

To start voting, a validator first registers an account to which it will send
its votes. It then sends votes to that account. The vote contains the tick
height of the block it is voting on. The account stores the 32 highest heights.

### Problems

- Only the validator knows how to find its own votes directly.

  Other components, such as the one that calculates confirmation time, needs to
  be baked into the validator code. The validator code queries the bank for all
  accounts owned by the vote program.

- Voting ballots do not contain a PoH hash. The validator is only voting that
  it has observed an arbitrary block at some height.

- Voting ballots do not contain a hash of the bank state. Without that hash,
  there is no evidence that the validator executed the transactions and
  verified there were no double spends.

## Proposed Design

### No Cross-block State Initially

At the moment a block is produced, the leader shall add a NewBlock transaction
to the ledger with a number of tokens that represents the validation reward.
It is effectively an incremental multisig transaction that sends tokens from
the mining pool to the validators. The account should allocate just enough
space to collect the votes required to achieve a supermajority. When a
validator observes the NewBlock transaction, it has the option to submit a vote
that includes a hash of its ledger state (the bank state). Once the account has
sufficient votes, the vote program should disperse the tokens to the
validators, which causes the account to be deleted.

#### Logging Confirmation Time

The bank will need to be aware of the vote program. After each transaction, it
should check if it is a vote transaction and if so, check the state of that
account. If the transaction caused the supermajority to be achieved, it should
log the time since the NewBlock transaction was submitted.

### Finality and Payouts

[Tower BFT](../implemented-proposals/tower-bft.md) is the proposed fork selection algorithm. It proposes
that payment to miners be postponed until the _stack_ of validator votes reaches
a certain depth, at which point rollback is not economically feasible. The vote
program may therefore implement Tower BFT. Vote instructions would need to
reference a global Tower account so that it can track cross-block state.

## Challenges

### On-chain voting

Using programs and accounts to implement this is a bit tedious. The hardest
part is figuring out how much space to allocate in NewBlock. The two variables
are the _active set_ and the stakes of those validators. If we calculate the
active set at the time NewBlock is submitted, the number of validators to
allocate space for is known upfront. If, however, we allow new validators to
vote on old blocks, then we'd need a way to allocate space dynamically.

Similar in spirit, if the leader caches stakes at the time of NewBlock, the
vote program doesn't need to interact with the bank when it processes votes. If
we don't, then we have the option to allow stakes to float until a vote is
submitted. A validator could conceivably reference its own staking account, but
that'd be the current account value instead of the account value of the most
recently finalized bank state. The bank currently doesn't offer a means to
reference accounts from particular points in time.

### Voting Implications on Previous Blocks

Does a vote on one height imply a vote on all blocks of lower heights of
that fork? If it does, we'll need a way to lookup the accounts of all
blocks that haven't yet reached supermajority. If not, the validator could
send votes to all blocks explicitly to get the block rewards.

================
File: docs/src/proposals/cluster-test-framework.md
================
---
title: Cluster Test Framework
---

This document proposes the Cluster Test Framework \(CTF\). CTF is a test harness that allows tests to execute against a local, in-process cluster or a deployed cluster.

## Motivation

The goal of CTF is to provide a framework for writing tests independent of where and how the cluster is deployed. Regressions can be captured in these tests and the tests can be run against deployed clusters to verify the deployment. The focus of these tests should be on cluster stability, consensus, fault tolerance, API stability.

Tests should verify a single bug or scenario, and should be written with the least amount of internal plumbing exposed to the test.

## Design Overview

Tests are provided an entry point, which is a `contact_info::ContactInfo` structure, and a keypair that has already been funded.

Each node in the cluster is configured with a `validator::ValidatorConfig` at boot time. At boot time this configuration specifies any extra cluster configuration required for the test. The cluster should boot with the configuration when it is run in-process or in a data center.

Once booted, the test will discover the cluster through a gossip entry point and configure any runtime behaviors via validator RPC.

## Test Interface

Each CTF test starts with an opaque entry point and a funded keypair. The test should not depend on how the cluster is deployed, and should be able to exercise all the cluster functionality through the publicly available interfaces.

```text
use crate::contact_info::ContactInfo;
use solana_keypair::Keypair;
use solana_signer::Signer;
pub fn test_this_behavior(
    entry_point_info: &ContactInfo,
    funding_keypair: &Keypair,
    num_nodes: usize,
)
```

## Cluster Discovery

At test start, the cluster has already been established and is fully connected. The test can discover most of the available nodes over a few second.

```text
use crate::gossip_service::discover_nodes;

// Discover the cluster over a few seconds.
let cluster_nodes = discover_nodes(&entry_point_info, num_nodes);
```

## Cluster Configuration

To enable specific scenarios, the cluster needs to be booted with special configurations. These configurations can be captured in `validator::ValidatorConfig`.

For example:

```text
let mut validator_config = ValidatorConfig::default_for_test();
let local = LocalCluster::new_with_config(
                num_nodes,
                10_000,
                100,
                &validator_config
                );
```

## How to design a new test

For example, there is a bug that shows that the cluster fails when it is flooded with invalid advertised gossip nodes. Our gossip library and protocol may change, but the cluster still needs to stay resilient to floods of invalid advertised gossip nodes.

Configure the RPC service:

```text
let mut validator_config = ValidatorConfig::default_for_test();
validator_config.rpc_config.enable_rpc_gossip_push = true;
validator_config.rpc_config.enable_rpc_gossip_refresh_active_set = true;
```

Wire the RPCs and write a new test:

```text
pub fn test_large_invalid_gossip_nodes(
    entry_point_info: &ContactInfo,
    funding_keypair: &Keypair,
    num_nodes: usize,
) {
    let cluster = discover_nodes(&entry_point_info, num_nodes);

    // Poison the cluster.
    let client = create_client(entry_point_info.client_facing_addr(), VALIDATOR_PORT_RANGE);
    for _ in 0..(num_nodes * 100) {
        client.gossip_push(
            cluster_info::invalid_contact_info()
        );
    }
    sleep(Durration::from_millis(1000));

    // Force refresh of the active set.
    for node in &cluster {
        let client = create_client(node.client_facing_addr(), VALIDATOR_PORT_RANGE);
        client.gossip_refresh_active_set();
    }

    // Verify that spends still work.
    verify_spends(&cluster);
}
```

================
File: docs/src/proposals/comprehensive-compute-fees.md
================
---
title: Comprehensive Compute Fees
---

## Motivation

The current fee structure lacks a comprehensive account of the work required by
a validator to process a transaction.  The fee structure is only based on the
number of signatures in a transaction but is meant to account for the work that
the validator must perform to validate each transaction.  The validator performs
a lot more user-defined work than just signature verification.  Processing a
transaction typically includes signature verification, account locking, account
loading, and instruction processing.

## Proposed Solution

The following solution does not specify what native token costs are to be
associated with the new fee structure.  Instead, it sets the criteria and
provides the knobs that a cost model can use to determine those costs.

### Fee

The goal of the fees is to cover the computation cost of processing a
transaction.  Each of the fee categories below will be represented as a compute
unit cost that, when added together, encompasses the entire cost of processing
the transaction.  By calculating the total cost of the transaction, the runtime
can charge a more representative fee and make better transaction scheduling
decisions.

A fee could be calculated based on:

1. Number of signatures
   - Fixed rate per signature
2. Number of write locks
   - Fixed rate per writable account
3. Data byte cost
   - Fixed rate per byte of the sum of the length all a transactions instruction
     data
4. Account sizes
   - Account sizes can't be known up-front but can account for a considerable
     amount of the load the transaction incurs on the network.  The payer will
     be charged for a maximum account size (10m) upfront and refunded the
     difference after the actual account sizes are known.
5. Compute budget
   - Each transaction will be given a default transaction-wide compute budget of
     200k units with the option of requesting a larger budget via a compute
     budget instruction up to a maximum of 1m units.  This budget is used to
     limit the time it takes to process a transaction.  The compute budget
     portion of the fee will be charged up-front based on the default or
     requested amount.  After processing, the actual number of units consumed
     will be known, and the payer will be refunded the difference, so the payer
     only pays for what they used.  Builtin programs will have a fixed cost
     while SBF program's cost will be measured at runtime.
6. Precompiled programs
   - Precompiled programs are performing compute-intensive operations.  The work
     incurred by a precompiled program is predictable based on the instruction's
     data array.  Therefore a cost will be assigned per precompiled program
     based on the parsing of instruction data.  Because precompiled programs are
     processed outside of the bank, their compute cost will not be reflected in
     the compute budget and will not be used in transaction scheduling
     decisions. The methods used to determine the fixed cost of the components
     above are described in
     [#19627](https://github.com/solana-labs/solana/issues/19627)

### Cost model

The cost model is used to assess what load a transaction will incur during
in-slot processing and then make decisions on how to best schedule transaction
into batches.

The cost model's criteria are identical to the fee's criteria except for
signatures and precompiled programs.  These two costs are incurred before a
transaction is scheduled and therefore do not affect how long a transaction
takes within a slot to process.

### Cache account sizes and use them instead of the max

https://github.com/solana-labs/solana/issues/20511

### Requestable compute budget caps and heap sizes

The precompiled
[ComputeBudget](https://github.com/solana-labs/solana/blob/00929f836348d76cb3503d0ba5f76f0d275bcc66/sdk/src/compute_budget.rs#L34)
program can be used to request higher transaction-wide compute budget caps and
program heap sizes.  The requested increases will be reflected in the
transaction's fee.

### Fees for precompiled program failures

https://github.com/solana-labs/solana/issues/20481

### Rate governing

Current rate governing needs to be re-assessed.  Fees are being rate
governed down to their minimums because the number of signatures in each slot is
far lower than the "target" signatures per slot.

Instead of using the number of signatures to rate govern, the cost model will
feed back information based on the batch/queue load it is seeing.  The fees will
sit at a target rate and only increase if the load goes above a specified but to
be determined threshold.  The governing will be applied across all the fee
criteria.

### Deterministic fees

Solana's fees are currently deterministic based on a given blockhash.  This
determinism is a nice feature that simplifies client interactions.  An example
is when draining an account that is also the payer, the transaction issuer can
pre-compute the fee and then set the entire remaining balance to be transferred
out without worrying that the fee will change leaving a very small amount
remaining in the account.  Another example is for offline signing, the payer
signer can guarantee what fee that will be charged for the transaction based on
the nonce's blockhash.

Determinism is achieved in two ways:
- blockhash queue contains a list of recent (<=~2min) blockhashes and a
  `lamports_per_signature` value.  The blockhash queue is one of the snapshot's
  serialized members and thus bank hash depends on it.
- Nonce accounts used for offline signing contain a `lamports_per_signature`
  value in its account data

In both cases, when a transaction is assessed a fee, the
`lamports_per_signature` to use is looked up (either in the queue or in the
nonce account's data) using the transaction's blockhash.

This currently comes with the following challenges:
- Exposing the `FeeCalculator` object to the clients (holds the
  `lamports_per_signature`) makes it hard to evolve the fee criteria due to
  backward-compatibility.  This issue is being solved by deprecating the
  `FeeCalculator` object and instead the new apis take a message and return a
  fee.
- Blockhash queue entries contain the fee criteria specifics and are part of the
  bankhash so evolving the fees over time involves more work/risk
- Nonce accounts store the fee criteria directly in their account data so
  evolving the fees over time requires changes to nonce account data and data
  size.

Two solutions to the latter two challenges
- Get rid of the concept of deterministic fees.  Clients ask via RPC to
  calculate the current fee estimate and the actual fee is assessed when the
  transaction is processed.  Fee changes will be governed and change slowly
  based on network load so the fee differences will be small within the 2min
  window.  Nonce accounts no longer store the fee criteria but instead a fee
  cap.  If the assessed fee at the time of processing exceeds the cap then the
  transaction fails.  This solution removes fee criteria entirely from the
  blockhash queue and nonce accounts and removes the need for either of those to
  evolve if there is a need for fee criteria to evolve.
- Retain the concept of deterministic fees.  Clients ask via RPC to calculate
  the current fee and pass in a blockhash that fee will be associated with.
  Blockhash queue and nonce accounts switch to a versioned but internal "Fee"
  object (similar to "FeeCalculator").  Each time there is a need for fees to
  evolve the fee object will add a new version and new blockhash queue entries
  and new nonce accounts will use the new version.

================
File: docs/src/proposals/embedding-move.md
================
---
title: Embedding the Move Language
---

### This document is outdated and a new approach to support 'move' is in the works.

## Problem

Solana enables developers to write on-chain programs in general purpose programming languages such as C or Rust, but those programs contain Solana-specific mechanisms. For example, there isn't another chain that asks developers to create a Rust module with a `process_instruction(KeyedAccounts)` function. Whenever practical, Solana should offer application developers more portable options.

Until just recently, no popular blockchain offered a language that could expose the value of Solana's massively parallel [runtime](../validator/runtime.md). Solidity contracts, for example, do not separate references to shared data from contract code, and therefore need to be executed serially to ensure deterministic behavior. In practice we see that the most aggressively optimized EVM-based blockchains all seem to peak out around 1,200 TPS - a small fraction of what Solana can do. The Libra project, on the other hand, designed an on-chain programming language called Move that is more suitable for parallel execution. Like Solana's runtime, Move programs depend on accounts for all shared state.

The biggest design difference between Solana's runtime and Libra's Move VM is how they manage safe invocations between modules. Solana took an operating systems approach and Libra took the domain-specific language approach. In the runtime, a module must trap back into the runtime to ensure the caller's module did not write to data owned by the callee. Likewise, when the callee completes, it must again trap back to the runtime to ensure the callee did not write to data owned by the caller. Move, on the other hand, includes an advanced type system that allows these checks to be run by its bytecode verifier. Because Move bytecode can be verified, the cost of verification is paid just once, at the time the module is loaded on-chain. In the runtime, the cost is paid each time a transaction crosses between modules. The difference is similar in spirit to the difference between a dynamically-typed language like Python versus a statically-typed language like Java. Solana's runtime allows applications to be written in general purpose programming languages, but that comes with the cost of runtime checks when jumping between programs.

This proposal attempts to define a way to embed the Move VM such that:

- cross-module invocations within Move do not require the runtime's

  cross-program runtime checks

- Move programs can leverage functionality in other Solana programs and vice

  versa

- Solana's runtime parallelism is exposed to batches of Move and non-Move

  transactions

## Proposed Solution

### Move VM as a Solana loader

The Move VM shall be embedded as a Solana loader under the identifier `MOVE_PROGRAM_ID`, so that Move modules can be marked as `executable` with the VM as its `owner`. This will allow modules to load module dependencies, as well as allow for parallel execution of Move scripts.

All data accounts owned by Move modules must set their owners to the loader, `MOVE_PROGRAM_ID`. Since Move modules encapsulate their account data in the same way Solana programs encapsulate theirs, the Move module owner should be embedded in the account data. The runtime will grant write access to the Move VM, and Move grants access to the module accounts.

### Interacting with Solana programs

To invoke instructions in non-Move programs, Solana would need to extend the Move VM with a `process_instruction()` system call. It would work the same as `process_instruction()` Rust SBF programs.

================
File: docs/src/proposals/fee_transaction_priority.md
================
---
title: Fee Transaction Priority
---

Additional fees were introduced to transactions as a method to allow users to bid for priority for
their transactions in the leader's queue.

The fee priority of a transaction `T` can then be defined as `F(T)`, where `F(T)` is the "fee-per
compute-unit", calculated by:

`(additional_fee + base_fee) / requested_compute_units`

To ensure users get fair priority based on their fee, the proposed scheduler for the leader must
guarantee that given `T1` and `T2` in the pending queue, and `F(T1) > F(T2)`:

1. `T1` should be considered for processing before `T2`
2. If `T1` cannot be processed before `T2` because there's already a transaction currently being
processed that contends on an account `A`, then `T2` should not be scheduled if it would grab
any account locks needed by `T1`. This prevents lower fee transactions like `T2` from starving
higher paying transactions like `T1`.


### Transaction Pipeline

Pipeline:
1. Sigverify
2. Scheduler
3. BankingStage threads

Transactions from sigverify are connected via a channel to the scheduler. The scheduler maintains
`N` bi-directional channels with the `N` BankingStage threads, implemented as a pair of two
unidirectional channels.

The scheduler's job is to run an algorithm in which it determines how to schedule transactions
received from sigverify to the `N` BankingStage threads. A transaction is scheduled to be executed
on a particular BankingStage thread by sending that transaction to the thread via its associated
channel.

Once a BankingStage thread finishes processing a transaction `T` , it sends the `T` back
to the scheduler via the same channel to signal of completion.

### Scheduler Implementation

The scheduler is the most complex piece of the above pipeline, its implementation is made up of a
few pieces. Note for now, all these pieces are maintained by the single scheduler thread to avoid
locking complexity.

#### Components of the `Scheduler`:

1. `default_transaction_queue` - A max-heap `BinaryHeap<Transaction>` that tracks all pending transactions.
The priority in the heap is the additional fee of the transaction. Transactions are added to this queue
from sigverify before the leader slot begins.
2. `all_transaction_queues` - A `VecDeque<BinaryHeap<Transaction>>` that tracks all pending queues of work.
Some pending queues have higher priority than others (as will be explained later in the `Handling Completion Signals from BankingStage Threads` section below). The list is ordered in priority from highest to lowest priority. On
initialization, `all_transaction_queues[0] = default_transaction_queue`.
3. `locked_accounts` - A `HashMap<LockedPubkey, usize>` that tracks the set of accounts needed to execute the
current set of transactions scheduled/sent to banking threads. Accounts are added to this set
*before* being sent to BankingStage threads. The `usize` represents a refcount, because multiple read
accounts could be grabbed.`LockedPubkey` is defined as:
```
enum LockedPubkey {
    Read(Pubkey),
    Write(Pubkey),
}
```
4. `blocked_transactions` -  A `HashMap<Signature, Rc<BlockedTransactionsQueue>>` keyed by
transaction signature, and maps to a `BlockedTransactionsQueue` defined as:
```
/// Represents a heap of transactions that cannot be scheduled because they
/// would take locks on accounts needed by a higher paying transaction
struct BlockedTransactionsQueue {
    // The higher priority transaction blocking all the other transactions in
    // `blocked_transactions` below
    highest_priority_blocked_transaction: Transaction,
    other_blocked_transactions: BinaryHeap<Transaction>
}
```
5. `blocked_transaction_queues_by_accounts` - A `HashMap<Pubkey, Rc<BlockedTransactionsQueue>>` keyed by
account keys.

#### Algorithm (Main Loop):

Assume `N` BankingStage threads:

The scheduler will run for each banking thread a function `find_next_highest_transaction()`:

1. Pop off the highest priority transaction `next_highest_transaction` from `self.all_transaction_queues[0]`.
If ``self.all_transaction_queues[0]` is empty, pop off the first deque item and continue.

2. Let `transaction_accounts` be the set of `LockedPubkey` keys needed by
`next_highest_transaction`. We run the following:

```
    for account_key in transaction_accounts {
        // Check if the `LockedPubkey` conflicts with any key in the `locked_accounts` set, which
        // would indicate a transaction using this account with a conflicting lock is already
        // running
        if self.locked_accounts.is_conflicting(account_key) {
            return Conflict;
        }

        // Check if any higher fee transaction has already reserved this account. This prevents
        // lower fee transactions from starving higher fee transactions.
        if self.blocked_transaction_queues_by_accounts.contains_key(account_key) {
            return Conflict;
        }
        return NoConflict;
    }
```

3. In the case of a `NoConflict` we run:

```
    for account_key in transaction_accounts {
        self.locked_accounts.insert_reference(account_key.key());
    }

    banking_thread_channel.send(next_highest_transaction);
```

4. In the case of a `Conflict` we run:

```
for locked_account_key in transaction_accounts {
    let account_key = locked_account_key.key()
    let blocked_transaction_entry = self.blocked_transaction_queues_by_accounts.entry(account_key);
    match blocked_transaction_entry {
        Occupied(existing_blocked_transaction) => {
            // If there is already a set of transactions blocked on this account, add
            // this transaction to the priority queue.
            existing_blocked_transaction.insert_transaction(next_highest_transaction);
        }

        Vacant(vacant_entry) => {
            // Create a new queue blocked on this transaction
            let new_blocked_transaction_queue =
                Rc::new(BlockedTransactionsQueue {
                    highest_priority_blocked_transaction: next_highest_transaction,
                    other_blocked_transactions: BinaryHeap::new(),
                });
            // Insert into the hashmap for this `account_key`
            vacant_entry.insert(new_blocked_transaction_queue.clone());
            // Insert into the `blocked_transactions` hashmap to indicate this set of transactions
            // is blocked by `next_highest_transaction`
            self.blocked_transactions.insert(
                next_highest_transaction.signature(),
                new_blocked_transaction_queue
            );
        }
    }
}
```

5. Run until all `N` BankingStage threads have been sent `processing_batch` transactions (i.e. hit step 3 above).

#### Banking Threads
1. Banking threads maintain a queue of transactions sent to them by the scheduler, sorted by priority.
2. Because the scheduler has guaranteed that there are no locking conflicts, the banking thread can process
some `M` of these transactions at a time and pack them into entries

#### Handling Completion Signals from BankingStage Threads

Outside of the main loop above, we rely on BankingThreads threads signaling us they've finished their
task to schedule the next transactions.

1. Once a BankingStage thread finishes processing a batch of transactions `completed_transactions_batch` ,
it sends the `completed_transactions_batch` back to the scheduler via the same channel to signal of completion.

2. Upon receiving this signal, the BankingStage thread processes the locked accounts
`transaction_accounts` for each `completed_transaction` in `completed_transactions_batch`:
```
let mut unlocked_accounts = vec![];
// First remove all the locks from the tracking list
for locked_account in transaction_accounts {
    if self.locked_accounts.remove_reference(locked_account) {
        unlocked_accounts.push(locked_account.key());
    }
}

// Check if freeing up these accounts has now allowed any new
// blocked transactions to run
for account_key in unlocked_accounts {
    if let Some(blocked_transaction_queue) = self.blocked_transaction_queues_by_accounts.get(account_key) {
        // Check if the transaction blocking this queue can be run now, thereby unblocking this queue
        if blocked_transaction_queue.highest_priority_blocked_transaction.can_get_locks() {
            // Schedule the transaction to the banking thread
            banking_thread_channel.send(blocked_transaction_queue.highest_priority_blocked_transaction);

            return;
        }
    }

    // If no higher priority transactions were unblocked, continue scheduling from the main queue,
    // described in the main loop section above
    find_next_highest_transaction();
}
```

3. Check if the finished transaction was the blocking transaction for any queue:

```
if let Some(blocked_transaction_queue) = self.blocked_transactions.get(completed_transaction.signature) {
    // Now push the rest of the queue to the head of `all_transaction_queues`, since we know
    // everything in this blocked queue must be of higher priority, (since they were
    // added to this queue earlier, this means they must have been popped off the main
    // `transaction_accounts` queue earlier, hence higher priority)
    self.all_transaction_queues.push_front(blocked_transaction_queue.other_blocked_transactions);
    self.blocked_transactions.remove(completed_transaction.signature);
}
```

================
File: docs/src/proposals/handle-duplicate-block.md
================
---
title: Handle Duplicate Block
---

# Leader Duplicate Block Slashing

This design describes how the cluster slashes leaders that produce duplicate
blocks.

Leaders that produce multiple blocks for the same slot increase the number of
potential forks that the cluster has to resolve.

## Primitives
1. gossip_root: Nodes now gossip their current root
2. gossip_duplicate_slots: Nodes can gossip up to `N` duplicate slot proofs.
3. `DUPLICATE_THRESHOLD`: The minimum percentage of stake that needs to vote on a fork with version `X` of a duplicate slot, in order for that fork to become votable.

## Protocol
1. When WindowStage detects a duplicate slot proof `P`, it checks the new `gossip_root` to see if `<= 1/3` of the nodes have rooted a slot `S >= P`. If so, it pushes a proof to `gossip_duplicate_slots` to gossip. WindowStage then signals ReplayStage about this duplicate slot `S`. These proofs can be purged from gossip once the validator sees > 2/3 of people gossiping roots `R > S`.

2. When ReplayStage receives the signal for a duplicate slot `S` from `1)` above, the validator monitors gossip and replay waiting for`>= DUPLICATE_THRESHOLD` votes for the same hash which implies the same version of the slot. If this condition is met for some version with hash `H` of slot `S`, this is then known as the `duplicate_confirmed` version of the slot.

Before a duplicate slot `S` is `duplicate_confirmed`, it's first excluded from the vote candidate set in the fork choice rules. In addition, ReplayStage also resets PoH to the *latest* ancestor of the *earliest* `non-duplicate/confirmed_duplicate_slot`, so that block generation can start happening on the earliest known *safe* block.

Some notes about the `DUPLICATE_THRESHOLD`. In the cases below, assume `DUPLICATE_THRESHOLD = 52`:

a) If less than `2 * DUPLICATE_THRESHOLD - 1` percentage of the network is malicious, then there can only be one such `duplicate_confirmed` version of the slot. With `DUPLICATE_THRESHOLD = 52`, this is
a malicious tolerance of `4%`

b) The liveness of the network is at most `1 - DUPLICATE_THRESHOLD - SWITCH_THRESHOLD`. This is because if you need at least `SWITCH_THRESHOLD` percentage of the stake voting on a different fork in order to switch off of a duplicate fork that has `< DUPLICATE_THRESHOLD` stake voting on it, and is *not* `duplicate_confirmed`. For `DUPLICATE_THRESHOLD = 52` and `DUPLICATE_THRESHOLD = 38`, this implies a liveness tolerance of `10%`.

For example in the situation below, validators that voted on `2` can't vote any further on fork `2` because it's been removed from fork choice. Now slot 6 better have enough stake for a switching proof, or the network halts.

```text
    |-------- 2 (51% voted, then detected this slot was a duplicate and removed this slot from fork choice)
0---|
    |---------- 6 (39%)

```

3. Switching proofs need to be extended to allow including vote hashes from different versions of the same slot (detected through 1). Right now this is not supported since switching proofs can
only be built using votes from banks in BankForks, and two different versions of the same slot cannot
simultaneously exist in BankForks. For instance:

```text
    |-------- 2
    |
0------------- 1 ------ 2'
    |
    |---------- 6

```

Imagine each version of slot 2 and 2' have `DUPLICATE_THRESHOLD / 2` of the votes on them, so neither duplicate can be confirmed. At most slot 6 has `1 - DUPLICATE_THRESHOLD / 2` of the votes
on it, which is less than the switching threshold. Thus, in order for validators voting on `2` or `2'` to switch to slot 6, and make progress, they need to incorporate votes from the other version of the slot into their switching proofs.


### The repair problem.
Now what happens if one of the following occurs:

1) Due to network blips/latencies, some validators fail to observe the gossip votes before they are overwritten by newer votes? Then some validators may conclude a slot `S` is `duplicate_confirmed` while others don't.

2) Due to lockouts, no version of duplicate slot `S` reaches `duplicate_confirmed` status, but one of its descendants may reach `duplicate_confirmed` after those lockouts expire, which by definition, means `S` is also `duplicate_confirmed`.

3) People who are catching up and don't see the votes in gossip encounter a dup block and can't make progress.

We assume that given a network is eventually stable, if at least one correct validator observed `S` is `duplicate_confirmed`, then if `S` is part of the heaviest fork, then eventually all validators will observe some descendant of `S` is duplicate confirmed.

This problem we need to solve is modeled simply by the below scenario:

```text
1 -> 2 (duplicate) -> 3 -> 4 (duplicate)
```
Assume the following:

1. Due to gossiping duplicate proofs, we assume everyone will eventually see duplicate proofs for 2 and 4, so everyone agrees to remove them from fork choice until they are `duplicate_confirmed`.

2. Due to lockouts, `> DUPLICATE_THRESHOLD` of the stake votes on 4, but not 2. This means at least `DUPLICATE_THRESHOLD` of people have the "correct" version of both slots 2 and 4.

3. However, the remaining `1-DUPLICATE_THRESHOLD` of people have wrong version of 2. This means in replay, their slot 3 will be marked dead, *even though the faulty slot is 2*. The goal is to get these people on the right fork again.

Possible solution:

1. Change `EpochSlots` to signal when a bank is frozen, not when a slot is complete. If we see > `DUPLICATE_THRESHOLD` have frozen the dead slot 3, then we attempt recovery. Note this does not mean that all `DUPLICATE_THRESHOLD` have frozen the same version of the bank, it's just a signal to us that something may be wrong with our version of the bank.

2. Recovery takes the form of a special repair request, `RepairDuplicateConfirmed(dead_slot, Vec<(Slot, Hash)>)`, which specifies a  dead slot, and then a vector of `(slot, hash)` of `N` of its latest parents.

3. The repairer sees this request and responds with the correct hash only if any element of the `(slot, hash)` vector is both `duplicate_confirmed` and the hash doesn't match the requester's hash in the vector.

4. Once the requester sees the "correct" hash is different than their frozen hash, they dump the block so that they can accept a new block, and ask the network for the block with the correct hash.

Of course the repairer might lie to you, and you'll get the wrong version of the block, in which case you'll end up with another dead block and repeat the procedure.

================
File: docs/src/proposals/interchain-transaction-verification.md
================
---
title: Inter-chain Transaction Verification
---

## Problem

Inter-chain applications are not new to the digital asset ecosystem; in fact, even the smaller centralized exchanges still categorically dwarf all single chain applications put together in terms of users and volume. They command massive valuations and have spent years effectively optimizing their core products for a broad range of end users. However, their basic operations center around mechanisms that require their users to unilaterally trust them, typically with little to no recourse or protection from accidental loss. This has led to the broader digital asset ecosystem being fractured along network lines because interoperability solutions typically:

- Are technically complex to fully implement
- Create unstable network scale incentive structures
- Require consistent and high level cooperation between stakeholders

## Proposed Solution

Simple Payment Verification \(SPV\) is a generic term for a range of different methodologies used by light clients on most major blockchain networks to verify aspects of the network state without the burden of fully storing and maintaining the chain itself. In most cases, this means relying on a form of hash tree to supply a proof of the presence of a given transaction in a certain block by comparing against a root hash in that block’s header or equivalent. This allows a light client or wallet to reach a probabilistic level of certainty about on-chain events by itself with a minimum of trust required with regard to network nodes.

Traditionally the process of assembling and validating these proofs is carried out off chain by nodes, wallets, or other clients, but it also offers a potential mechanism for inter-chain state verification. However, by moving the capability to validate SPV proofs on-chain as a smart contract while leveraging the archival properties inherent to the blockchain, it is possible to construct a system for programmatically detecting and verifying transactions on other networks without the involvement of any type of trusted oracle or complex multi-stage consensus mechanism. This concept is broadly generalisable to any network with an SPV mechanism and can even be operated bilaterally on other smart contract platforms, opening up the possibility of cheap, fast, inter-chain transfer of value without relying on collateral, hashlocks, or trusted intermediaries.

Opting to take advantage of well established and developmentally stable mechanisms already common to all major blockchains allows SPV based interoperability solutions to be dramatically simpler than orchestrated multi-stage approaches. As part of this, they dispense with the need for widely agreed upon cross chain communication standards and the large multi-party organizations that write them in favor of a set of discrete contract-based services that can be easily utilized by caller contracts through a common abstraction format. This will set the groundwork for a broad range of applications and contracts able to interoperate across the variegated and every growing platform ecosystem.

## Terminology

SPV Program - Client-facing interface for the inter-chain SPV system, manages participant roles. SPV Engine - Validates transaction proofs, subset of the SPV Program. Client - The caller to the SPV Program, typically another solana contract. Prover - Party who generates proofs for transactions and submits them to the SPV Program. Transaction Proof - Created by Provers, contains a merkle proof, transaction, and blockheader reference. Merkle Proof - Basic SPV proof that validates the presence of a transaction in a certain block. Block Header - Represents the basic parameters and relative position of a given block. Proof Request - An order placed by a client for verification of transaction\(s\) by provers. Header Store - A data structure for storing and referencing ranges of block headers in proofs. Client Request - Transaction from the client to the SPV Program to trigger creation of a Proof Request. Sub-account - A Solana account owned by another contract account, without its own private key.

## Service

SPV Programs run as contracts deployed on the Solana network and maintain a type of public marketplace for SPV proofs that allows any party to submit both requests for proofs as well as proofs themselves for verification in response to requests. There will be multiple SPV Program instances active at any given time, at least one for each connected external network and potentially multiple instances per network. SPV program instances will be relatively consistent in their high level API and feature sets with some variation between currency platforms \(Bitcoin, Litecoin\) and smart contract platforms owing to the potential for verification of network state changes beyond simply transactions. In every case regardless of network, the SPV Program relies on an internal component called an SPV engine to provide stateless verification of the actual SPV proofs upon which the higher level client facing features and api are built. The SPV engine requires a network specific implementation, but allows easy extension of the larger inter-chain ecosystem by any team who chooses to carry out that implementation and drop it into the standard SPV program for deployment.

For purposes of Proof Requests, the requester is referred to as the program client, which in most if not all cases will be another Solana Contract. The client can choose to submit a request pertaining to a specific transaction or to include a broader filter that can apply to any of a range of parameters of a transaction including its inputs, outputs, and amount. For example, A client could submit a request for any transaction sent from a given address A to address B with the amount X after a certain time. This structure can be used in a range of applications, such as verifying a specific intended payment in the case of an atomic swap or detecting the movement of collateral assets for a loan.

Following submission of a Client Request, assuming that it is successfully validated, a proof request account is created by the SPV program to track the progress of the request. Provers use the account to specify the request they intend to fill in the proofs they submit for validation, at which point the SPV program validates those proofs and if successful, saves them to the account data of the request account. Clients can monitor the status of their requests and see any applicable transactions alongside their proofs by querying the account data of the request account. In future iterations when supported by Solana, this process will be simplified by contracts publishing events rather than requiring a polling style process as described.

## Implementation

The Solana Inter-chain SPV mechanism consists of the following components and participants:

### SPV engine

A contract deployed on Solana which statelessly verifies SPV proofs for the caller. It takes as arguments for validation:

- An SPV proof in the correct format of the blockchain associated with the program
- Reference\(s\) to the relevant block headers to compare that proof against
- The necessary parameters of the transaction to verify

  If the proof in question is successfully validated, the SPV program saves proof

  of that verification to the request account, which can be saved by the caller to

  its account data or otherwise handled as necessary. SPV programs also expose

  utilities and structs used for representation and validation of headers,

  transactions, hashes, etc. on a chain by chain basis.

### SPV program

A contract deployed on Solana which coordinates and intermediates the interaction between Clients and Provers and manages the validation of requests, headers, proofs, etc. It is the primary point of access for Client contracts to access the inter-chain. SPV mechanism. It offers the following core features:

- Submit Proof Request - allows client to place a request for a specific proof or set of proofs
- Cancel Proof Request - allows client to invalidate a pending request
- Fill Proof Request - used by Provers to submit for validation a proof corresponding to a given Proof Request

  The SPV program maintains a publicly available listing of valid pending Proof

  Requests in its account data for the benefit of the Provers, who monitor it and

  enclose references to target requests with their submitted proofs.

### Proof Request

A message sent by the Client to the SPV engine denoting a request for a proof of a specific transaction or set of transactions. Proof Requests can either manually specify a certain transaction by its hash or can elect to submit a filter that matches multiple transactions or classes of transactions. For example, a filter matching “any transaction from address xxx to address yyy” could be used to detect payment of a debt or settlement of an inter-chain swap. Likewise, a filter matching “any transaction from address xxx” could be used by a lending or synthetic token minting contract to monitor and react to changes in collateralization. Proof Requests are sent with a fee, which is disbursed by the SPV engine contract to the appropriate Prover once a proof matching that request is validated.

### Request Book

The public listing of valid, open Proof Requests available to provers to fill or for clients to cancel. Roughly analogous to an orderbook in an exchange, but with a single type of listing rather than two separate sides. It is stored in the account data of the SPV program.

### Proof

A proof of the presence of a given transaction in the blockchain in question. Proofs encompass both the actual merkle proof and reference\(s\) to a chain of valid sequential block headers. They are constructed and submitted by Provers in accordance with the specifications of the publicly available Proof Requests hosted on the request book by the SPV program. Upon Validation, they are saved to the account data of the relevant Proof Request, which can be used by the Client to monitor the state of the request.

### Client

The originator of a request for a transaction proof. Clients will most often be other contracts as parts of applications or specific financial products like loans, swaps, escrow, etc. The client in any given verification process cycle initially submits a ClientRequest which communicates the parameters and fee and if successfully validated, results in the creation of a Proof Request account by the SPV program. The Client may also submit a CancelRequest referencing an active Proof Request in order to denote it as invalid for purposes of proof submission.

### Prover

The submitter of a proof that fills a Proof Request. Provers monitor the request book of the SPV program for outstanding Proof Requests and generate matching proofs, which they submit to the SPV program for validation. If the proof is accepted, the fee associated with the Proof Request in question is disbursed to the Prover. Provers typically operate as Solana Blockstreamer nodes that also have access to a Bitcoin node, which they use for purposes of constructing proofs and accessing block headers.

### Header Store

An account-based data structure used to maintain block headers for the purpose of inclusion in submitted proofs by reference to the header store account. header stores can be maintained by independent entities, since header chain validation is a component of the SPV program proof validation mechanism. Fees that are paid out by Proof Requests to Provers are split between the submitter of the merkle proof itself and the header store that is referenced in the submitted proof. Due to the current inability to grow already allocated account data capacity, the use case necessitates a data structure that can grow indefinitely without rebalancing. Sub-accounts are accounts owned by the SPV program without their own private keys that are used for storage by allocating blockheaders to their account data. Multiple potential approaches to the implementation of the header store system are feasible:

Store Headers in program sub-accounts indexed by Public address:

- Each sub-account holds one header and has a public key matching the blockhash
- Requires same number of account data lookups as confirmations per verification
- Limit on number of confirmations \(15-20\) via max transaction data ceiling
- No network-wide duplication of individual headers

Linked List of multiple sub-accounts storing headers:

- Maintain sequential index of storage accounts, many headers per storage account
- Max 2 account data lookups for &gt;99.9% of verification \(1 for most\)
- Compact sequential data address format allows any number of confirmations and fast lookups
- Facilitates network-wide header duplication inefficiencies

================
File: docs/src/proposals/ledger-replication-to-implement.md
================
---
title: Ledger Replication
---

Note: this ledger replication solution was partially implemented, but not
completed. The partial implementation was removed by
https://github.com/solana-labs/solana/pull/9992 in order to prevent the security
risk of unused code. The first part of this design document reflects the
once-implemented parts of ledger replication. The
[second part of this document](#ledger-replication-not-implemented) describes the
parts of the solution never implemented.

## Proof of Replication

At full capacity on a 1gbps network solana will generate 4 petabytes of data per year. To prevent the network from centralizing around validators that have to store the full data set this protocol proposes a way for mining nodes to provide storage capacity for pieces of the data.

The basic idea to Proof of Replication is encrypting a dataset with a public symmetric key using CBC encryption, then hash the encrypted dataset. The main problem with the naive approach is that a dishonest storage node can stream the encryption and delete the data as it's hashed. The simple solution is to periodically regenerate the hash based on a signed PoH value. This ensures that all the data is present during the generation of the proof and it also requires validators to have the entirety of the encrypted data present for verification of every proof of every identity. So the space required to validate is `number_of_proofs * data_size`

## Optimization with PoH

Our improvement on this approach is to randomly sample the encrypted segments faster than it takes to encrypt, and record the hash of those samples into the PoH ledger. Thus the segments stay in the exact same order for every PoRep and verification can stream the data and verify all the proofs in a single batch. This way we can verify multiple proofs concurrently. The total space required for verification is `1_ledger_segment + 2_cbc_blocks * number_of_identities` with core count equal to `number_of_identities`. We use a 64-byte chacha CBC block size.

## Network

Validators for PoRep are the same validators that are verifying transactions. If an archiver can prove that a validator verified a fake PoRep, then the validator will not receive a reward for that storage epoch.

Archivers are specialized _light clients_. They download a part of the ledger \(a.k.a Segment\) and store it, and provide PoReps of storing the ledger. For each verified PoRep archivers earn a reward of sol from the mining pool.

## Constraints

We have the following constraints:

- Verification requires generating the CBC blocks. That requires space of 2

  blocks per identity. So as many identities at once should be batched with as

  many proofs for those identities verified concurrently for the same dataset.

- Validators will randomly sample the set of storage proofs to the set that

  they can handle, and only the creators of those chosen proofs will be

  rewarded. The validator can run a benchmark whenever its hardware configuration

  changes to determine what rate it can validate storage proofs.

## Validation and Replication Protocol

### Constants

1. SLOTS_PER_SEGMENT: Number of slots in a segment of ledger data. The

   unit of storage for an archiver.

2. NUM_KEY_ROTATION_SEGMENTS: Number of segments after which archivers

   regenerate their encryption keys and select a new dataset to store.

3. NUM_STORAGE_PROOFS: Number of storage proofs required for a storage proof

   claim to be successfully rewarded.

4. RATIO_OF_FAKE_PROOFS: Ratio of fake proofs to real proofs that a storage

   mining proof claim has to contain to be valid for a reward.

5. NUM_STORAGE_SAMPLES: Number of samples required for a storage mining

   proof.

6. NUM_CHACHA_ROUNDS: Number of encryption rounds performed to generate

   encrypted state.

7. NUM_SLOTS_PER_TURN: Number of slots that define a single storage epoch or

   a "turn" of the PoRep game.

### Validator behavior

1. Validators join the network and begin looking for archiver accounts at each

   storage epoch/turn boundary.

2. Every turn, Validators sign the PoH value at the boundary and use that signature

   to randomly pick proofs to verify from each storage account found in the turn boundary.

   This signed value is also submitted to the validator's storage account and will be used by

   archivers at a later stage to cross-verify.

3. Every `NUM_SLOTS_PER_TURN` slots the validator advertises the PoH value. This is value

   is also served to Archivers via RPC interfaces.

4. For a given turn N, all validations get locked out until turn N+3 \(a gap of 2 turn/epoch\).

   At which point all validations during that turn are available for reward collection.

5. Any incorrect validations will be marked during the turn in between.

### Archiver behavior

1. Since an archiver is somewhat of a light client and not downloading all the

   ledger data, they have to rely on other validators and archivers for information.

   Any given validator may or may not be malicious and give incorrect information, although

   there are not any obvious attack vectors that this could accomplish besides having the

   archiver do extra wasted work. For many of the operations there are a number of options

   depending on how paranoid an archiver is:

   - \(a\) archiver can ask a validator
   - \(b\) archiver can ask multiple validators
   - \(c\) archiver can ask other archivers
   - \(d\) archiver can subscribe to the full transaction stream and generate

     the information itself \(assuming the slot is recent enough\)

   - \(e\) archiver can subscribe to an abbreviated transaction stream to

     generate the information itself \(assuming the slot is recent enough\)

2. An archiver obtains the PoH hash corresponding to the last turn with its slot.
3. The archiver signs the PoH hash with its keypair. That signature is the

   seed used to pick the segment to replicate and also the encryption key. The

   archiver mods the signature with the slot to get which segment to

   replicate.

4. The archiver retrieves the ledger by asking peer validators and

   archivers. See 6.5.

5. The archiver then encrypts that segment with the key with chacha algorithm

   in CBC mode with `NUM_CHACHA_ROUNDS` of encryption.

6. The archiver initializes a chacha rng with the signed recent PoH value as

   the seed.

7. The archiver generates `NUM_STORAGE_SAMPLES` samples in the range of the

   entry size and samples the encrypted segment with sha256 for 32-bytes at each

   offset value. Sampling the state should be faster than generating the encrypted

   segment.

8. The archiver sends a PoRep proof transaction which contains its sha state

   at the end of the sampling operation, its seed and the samples it used to the

   current leader and it is put onto the ledger.

9. During a given turn the archiver should submit many proofs for the same segment

   and based on the `RATIO_OF_FAKE_PROOFS` some of those proofs must be fake.

10. As the PoRep game enters the next turn, the archiver must submit a

    transaction with the mask of which proofs were fake during the last turn. This

    transaction will define the rewards for both archivers and validators.

11. Finally for a turn N, as the PoRep game enters turn N + 3, archiver's proofs for

    turn N will be counted towards their rewards.

### The PoRep Game

The Proof of Replication game has 4 primary stages. For each "turn" multiple PoRep games can be in progress but each in a different stage.

The 4 stages of the PoRep Game are as follows:

1. Proof submission stage
   - Archivers: submit as many proofs as possible during this stage
   - Validators: No-op
2. Proof verification stage
   - Archivers: No-op
   - Validators: Select archivers and verify their proofs from the previous turn
3. Proof challenge stage
   - Archivers: Submit the proof mask with justifications \(for fake proofs submitted 2 turns ago\)
   - Validators: No-op
4. Reward collection stage
   - Archivers: Collect rewards for 3 turns ago
   - Validators: Collect rewards for 3 turns ago

For each turn of the PoRep game, both Validators and Archivers evaluate each stage. The stages are run as separate transactions on the storage program.

### Finding who has a given block of ledger

1. Validators monitor the turns in the PoRep game and look at the rooted bank

   at turn boundaries for any proofs.

2. Validators maintain a map of ledger segments and corresponding archiver public keys.

   The map is updated when a Validator processes an archiver's proofs for a segment.

   The validator provides an RPC interface to access this map. Using this API, clients

   can map a segment to an archiver's network address \(correlating it via cluster_info table\).

   The clients can then send repair requests to the archiver to retrieve segments.

3. Validators would need to invalidate this list every N turns.

## Sybil attacks

For any random seed, we force everyone to use a signature that is derived from a PoH hash at the turn boundary. Everyone uses the same count, so the same PoH hash is signed by every participant. The signatures are then each cryptographically tied to the keypair, which prevents a leader from grinding on the resulting value for more than 1 identity.

Since there are many more client identities than encryption identities, we need to split the reward for multiple clients, and prevent Sybil attacks from generating many clients to acquire the same block of data. To remain BFT we want to avoid a single human entity from storing all the replications of a single chunk of the ledger.

Our solution to this is to force the clients to continue using the same identity. If the first round is used to acquire the same block for many client identities, the second round for the same client identities will force a redistribution of the signatures, and therefore PoRep identities and blocks. Thus to get a reward for archivers need to store the first block for free and the network can reward long lived client identities more than new ones.

## Validator attacks

- If a validator approves fake proofs, archiver can easily out them by

  showing the initial state for the hash.

- If a validator marks real proofs as fake, no on-chain computation can be done

  to distinguish who is correct. Rewards would have to rely on the results from

  multiple validators to catch bad actors and archivers from being denied rewards.

- Validator stealing mining proof results for itself. The proofs are derived

  from a signature from an archiver, since the validator does not know the

  private key used to generate the encryption key, it cannot be the generator of

  the proof.

## Reward incentives

Fake proofs are easy to generate but difficult to verify. For this reason, PoRep proof transactions generated by archivers may require a higher fee than a normal transaction to represent the computational cost required by validators.

Some percentage of fake proofs are also necessary to receive a reward from storage mining.

## Notes

- We can reduce the costs of verification of PoRep by using PoH, and actually

  make it feasible to verify a large number of proofs for a global dataset.

- We can eliminate grinding by forcing everyone to sign the same PoH hash and

  use the signatures as the seed

- The game between validators and archivers is over random blocks and random

  encryption identities and random data samples. The goal of randomization is

  to prevent colluding groups from having overlap on data or validation.

- Archiver clients fish for lazy validators by submitting fake proofs that

  they can prove are fake.

- To defend against Sybil client identities that try to store the same block we

  force the clients to store for multiple rounds before receiving a reward.

- Validators should also get rewarded for validating submitted storage proofs

  as incentive for storing the ledger. They can only validate proofs if they

  are storing that slice of the ledger.

# Ledger Replication Not Implemented

Replication behavior yet to be implemented.

## Storage epoch

The storage epoch should be the number of slots which results in around 100GB-1TB of ledger to be generated for archivers to store. Archivers will start storing ledger when a given fork has a high probability of not being rolled back.

## Validator behavior

1. Every NUM_KEY_ROTATION_TICKS it also validates samples received from

   archivers. It signs the PoH hash at that point and uses the following

   algorithm with the signature as the input:

   - The low 5 bits of the first byte of the signature creates an index into

     another starting byte of the signature.

   - The validator then looks at the set of storage proofs where the byte of

     the proof's sha state vector starting from the low byte matches exactly

     with the chosen byte\(s\) of the signature.

   - If the set of proofs is larger than the validator can handle, then it

     increases to matching 2 bytes in the signature.

   - Validator continues to increase the number of matching bytes until a

     workable set is found.

   - It then creates a mask of valid proofs and fake proofs and sends it to

     the leader. This is a storage proof confirmation transaction.

2. After a lockout period of NUM_SECONDS_STORAGE_LOCKOUT seconds, the

   validator then submits a storage proof claim transaction which then causes the

   distribution of the storage reward if no challenges were seen for the proof to

   the validators and archivers party to the proofs.

## Archiver behavior

1. The archiver then generates another set of offsets which it submits a fake

   proof with an incorrect sha state. It can be proven to be fake by providing the

   seed for the hash result.

   - A fake proof should consist of an archiver hash of a signature of a PoH

     value. That way when the archiver reveals the fake proof, it can be

     verified on chain.

2. The archiver monitors the ledger, if it sees a fake proof integrated, it

   creates a challenge transaction and submits it to the current leader. The

   transaction proves the validator incorrectly validated a fake storage proof.

   The archiver is rewarded and the validator's staking balance is slashed or

   frozen.

## Storage proof contract logic

Each archiver and validator will have their own storage account. The validator's account would be separate from their gossip id similar to their vote account. These should be implemented as two programs one which handles the validator as the keysigner and one for the archiver. In that way when the programs reference other accounts, they can check the program id to ensure it is a validator or archiver account they are referencing.

### SubmitMiningProof

```text
SubmitMiningProof {
    slot: u64,
    sha_state: Hash,
    signature: Signature,
};
keys = [archiver_keypair]
```

Archivers create these after mining their stored ledger data for a certain hash value. The slot is the end slot of the segment of ledger they are storing, the sha_state the result of the archiver using the hash function to sample their encrypted ledger segment. The signature is the signature that was created when they signed a PoH value for the current storage epoch. The list of proofs from the current storage epoch should be saved in the account state, and then transferred to a list of proofs for the previous epoch when the epoch passes. In a given storage epoch a given archiver should only submit proofs for one segment.

The program should have a list of slots which are valid storage mining slots. This list should be maintained by keeping track of slots which are rooted slots in which a significant portion of the network has voted on with a high lockout value, maybe 32-votes old. Every SLOTS_PER_SEGMENT number of slots would be added to this set. The program should check that the slot is in this set. The set can be maintained by receiving a AdvertiseStorageRecentBlockHash and checking with its bank/Tower BFT state.

The program should do a signature verify check on the signature, public key from the transaction submitter and the message of the previous storage epoch PoH value.

### ProofValidation

```text
ProofValidation {
   proof_mask: Vec<ProofStatus>,
}
keys = [validator_keypair, archiver_keypair(s) (unsigned)]
```

A validator will submit this transaction to indicate that a set of proofs for a given segment are valid/not-valid or skipped where the validator did not look at it. The keypairs for the archivers that it looked at should be referenced in the keys so the program logic can go to those accounts and see that the proofs are generated in the previous epoch. The sampling of the storage proofs should be verified ensuring that the correct proofs are skipped by the validator according to the logic outlined in the validator behavior of sampling.

The included archiver keys will indicate the storage samples which are being referenced; the length of the proof_mask should be verified against the set of storage proofs in the referenced archiver account\(s\), and should match with the number of proofs submitted in the previous storage epoch in the state of said archiver account.

### ClaimStorageReward

```text
ClaimStorageReward {
}
keys = [validator_keypair or archiver_keypair, validator/archiver_keypairs (unsigned)]
```

Archivers and validators will use this transaction to get paid tokens from a program state where SubmitStorageProof, ProofValidation and ChallengeProofValidations are in a state where proofs have been submitted and validated and there are no ChallengeProofValidations referencing those proofs. For a validator, it should reference the archiver keypairs to which it has validated proofs in the relevant epoch. And for an archiver it should reference validator keypairs for which it has validated and wants to be rewarded.

### ChallengeProofValidation

```text
ChallengeProofValidation {
    proof_index: u64,
    hash_seed_value: Vec<u8>,
}
keys = [archiver_keypair, validator_keypair]
```

This transaction is for catching lazy validators who are not doing the work to validate proofs. An archiver will submit this transaction when it sees a validator has approved a fake SubmitMiningProof transaction. Since the archiver is a light client not looking at the full chain, it will have to ask a validator or some set of validators for this information maybe via RPC call to obtain all ProofValidations for a certain segment in the previous storage epoch. The program will look in the validator account state see that a ProofValidation is submitted in the previous storage epoch and hash the hash_seed_value and see that the hash matches the SubmitMiningProof transaction and that the validator marked it as valid. If so, then it will save the challenge to the list of challenges that it has in its state.

### AdvertiseStorageRecentBlockhash

```text
AdvertiseStorageRecentBlockhash {
    hash: Hash,
    slot: u64,
}
```

Validators and archivers will submit this to indicate that a new storage epoch has passed and that the storage proofs which are current proofs should now be for the previous epoch. Other transactions should check to see that the epoch that they are referencing is accurate according to current chain state.

================
File: docs/src/proposals/log_data.md
================
# Program log binary data

## Problem

There is no support for logging binary data in Solidity.

### Events in Solidity

In Solidity, events can be reported. These look like structures with zero or
more fields, and can be emitted with specific values. For example:

```
event PaymentReceived {
    address sender;
    uint amount;
}

contract c {
    function pay() public payable {
        emit PaymentReceived(msg.sender, msg.value);
    }
}
```

Events are write-only from a Solidity/VM perspective and are written to
the blocks in the tx records.

Some of these fields can be marked `indexed`, which affects how the data is
encoded. All non-indexed fields are eth abi encoded into a variable length
byte array. All indexed fields go into so-called topics.

Topics are fixed length fields of 32 bytes. There are a maximum of 4 topics;
if a type does not always fit into 32 bytes (e.g. string types), then the topic
is keccak256 hashed.

The first topic is a keccak256 hash of the event signature, in this case
`keccak256('PaymentReceived(address,uint)')`. The four remaining are available
for `indexed` fields. The event may be declared `anonymous`, in which case
the first field is not a hash of the signature, and it is permitted to have
4 indexed fields.

### Listening to events in a client

The reason for the distinction between topics/indexed and regular fields is
that it easier to filter on topics.

```
const Web3 = require('web3');
const url = 'ws://127.0.0.1:8546';
const web3 = new Web3(url);

var options = {
    address: '0xfbBE8f06FAda977Ea1E177da391C370EFbEE3D25',
    topics: [
        '0xdf50c7bb3b25f812aedef81bc334454040e7b27e27de95a79451d663013b7e17',
        //'0x0000000000000000000000000d8a3f5e71560982fb0eb5959ecf84412be6ae3e'
      ]
};

var subscription = web3.eth.subscribe('logs', options, function(error, result){
    if (!error) console.log('got result');
    else console.log(error);
}).on("data", function(log){
    console.log('got data', log);
}).on("changed", function(log){
    console.log('changed');
});
```

In order to decode the non-indexed fields (the data), the abi of the contract
is needed. So, the topic is first used to discover what event was used, and
then the data can be decoded.

### Ethereum Tx in block

The transaction calls event logs. Here is a tx with a single event, with 3
topics and some data.

```
{
  "tx": {
    "nonce": "0x2",
    "gasPrice": "0xf224d4a00",
    "gas": "0xc350",
    "to": "0x6B175474E89094C44Da98b954EedeAC495271d0F",
    "value": "0x0",
    "input": "0xa9059cbb000000000000000000000000a12431d0b9db640034b0cdfceef9cce161e62be40000000000000000000000000000000000000000000000a030dcebbd2f4c0000",
    "hash": "0x98a67f0a35ebc0ac068acf0885d38419c632ffa4354e96641d6d5103a7681910",
    "blockNumber": "0xc96431",
    "from": "0x82f890D638478d211eF2208f3c1466B5Abf83551",
    "transactionIndex": "0xe1"
  },
  "receipt": {
    "gasUsed": "0x74d2",
    "status": "0x1",
    "logs": [
      {
        "address": "0x6B175474E89094C44Da98b954EedeAC495271d0F",
        "topics": [
          "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
          "0x00000000000000000000000082f890d638478d211ef2208f3c1466b5abf83551",
          "0x000000000000000000000000a12431d0b9db640034b0cdfceef9cce161e62be4"
        ],
        "data": "0x0000000000000000000000000000000000000000000000a030dcebbd2f4c0000"
      }
    ]
  }
}
```

### Further considerations

In Ethereum, events are stored in blocks. Events mark certain state changes in
smart contracts. This serves two purposes:

 - Listen to events (i.e. state changes) as they happen by reading new blocks
   as they are published
 - Re-read historical events by reading old blocks

So for example, smart contracts may emit changes as they happen but never the
complete state, so the only way to recover the entire state of the contract
is by re-reading all events from the chain. So an application will read events
from block 1 or whatever block the application was deployed at and then use
that state for local processing. This is a local cache and may re-populated
from the chain at any point.

## Proposed Solution

Binary logging should be added to the program log. The program log should include the base64 encoded data (zero or more one permitted).

So if we encoding the topics first, followed by the data then the event in the
tx above would look like:
```
program data: 3fJSrRviyJtpwrBo/DeNqpUrpFjxKEWKPVaTfUjs8AAAAAAAAAAAAAAACC+JDWOEeNIR7yII88FGa1q/g1UQAAAAAAAAAAAAAAAKEkMdC522QANLDN/O75zOFh5ivk AAAAAAAAAAAAAAAAAAAAAAAAAAAAAACgMNzrvS9MAAA=
```

This requires a new system call:

```
void sol_log_data(SolBytes *fields, uint64_t length);
```

### Considerations

- Should there be text field in the program log so we can have a little bit of
  metadata on the binary data, to make it more human readable

================
File: docs/src/proposals/off-chain-message-signing.md
================
# Off-chain message signing
## Motivation

There is ecosystem demand for a method of signing non-transaction messages with
a Solana wallet. Typically this is some kind of "proof of wallet ownership" for
entry into a whitelisted system.

Some inspiration can be gleaned from relevant portions of Ethereum's
[EIP-712](https://eips.ethereum.org/EIPS/eip-712)

## Considerations

* Security
  * Off-chain message signatures should not be valid transaction message signatures
* Future-proofing
  * Versioning
  * Co-exist with versioned transaction messages and extended length transactions
* Compatibility
  * Hardware wallet signing
  * Localization

## Message Preamble

| Field | Start offset | Length (bytes) | Description
| :---- | :----------: | :------------: | :----------
| Signing Domain | 0x00 | 16 | \[[1](#signing-domain-specifier)\]
| Header version | 0x10 | 1 | \[[2](#header-version)\]
| Application domain | 0x11 | 32 | \[[3](#application-domain)\]
| Message format | 0x31 | 1 | \[[4](#message-format)\]
| Signer count | 0x32 | 1 | Number of signers committing to the message. An 8-bit, unsigned integer. **MUST NOT** be zero.
| Signers | 0x33 | `SIGNER_COUNT` * 32 | `SIGNER_COUNT` ed25519 public keys of signers
| Message length | 0x33 + `SIGNER_CNT` * 32 | 2 | Length of message in bytes. A 16-bit, unsigned, little-endian integer. **MUST NOT** be zero.

### Signing Domain Specifier

The signing domain specifier is a prefix byte string used to give similar structure
to all off-chain message signatures. We assign its value to be:
```
b"\xffsolana offchain"
```
The first byte, `\xff`, was chosen for the following reasons
1. It corresponds to a value that is illegal as the first byte in a transaction
`MessageHeader`.
1. It avoids unintentional misuse in languages with C-like, null-terminated strings.

The remaining bytes, `b"solana offchain"`, were chosen to be descriptive and
reasonably long, but are otherwise arbitrary.

This field **SHOULD NOT** be displayed to users

### Header

#### Header version

The header version is represented as an 8-bit unsigned integer. Only the version
0 header format is specified in this document

This field **SHOULD NOT** be displayed to users

#### Application domain

A 32-byte array identifying the application requesting off-chain message signing.
This may be any arbitrary bytes. For instance the on-chain address of a program,
DAO instance, Candy Machine, etc.

This field **SHOULD** be displayed to users as a base58-encoded ASCII string rather
than interpreted otherwise.

#### Message Format

Version `0` headers specify three message formats allowing for trade-offs between
compatibility and composition of messages.

| ID  | Encoding              | Maximum Length \* | Hardware Wallet Support |
| :-: | :-------------------: | :---------------: | :---------------------: |
|  0  | Restricted ASCII \*\* | 1232              | Yes                     |
|  1  | UTF-8                 | 1232              | Blind sign only         |
|  2  | UTF-8                 | 65535             | No                      |

\* Combined length of the [message preamble](#message-preamble) and message body<br/>
\*\* Those characters for which [`isprint(3)`](https://linux.die.net/man/3/isprint)
returns true.  That is, `0x20..=0x7e`.

Both the message encoding and maximum message length **MUST** be enforced by
signer and verifier.

Formats `0` and `1` are motivated by hardware wallet support where both RAM
to store the payload and font character support are limited.

This field **SHOULD NOT** be displayed to users

## Signing

Solana off-chain messages **MUST** only be signed using the ed25519 digital
signature scheme. Before signing, the message **MUST** be strictly checked to
conform to the associated preamble. The message body is then appended to the
[message preamble](#message-preamble). Finally the result is ed25519 signed.

## Verification
Upon successful ed25519 verification of _all_ attached signatures, the message
**MUST** be strictly checked to conform to the [message preamble](#message-preamble).
A message that does not conform to its preamble is invalid, regardless of the
validity of any signatures.

## Envelope

When passing around signed off-chain messages a common format is helpful. The
recommended binary representation is as follows:

| Field | Start offset | Length (bytes) | Description
| :---- | :----------: | :------------: | :----------
| Signature Count \* | 0x00 | 1 | Number of signatures. An 8-bit, unsigned integer
| Signatures | 0x01 | `SIG_COUNT` * 64 | `SIG_COUNT` ed25519 signatures
| Message Preamble | 0x01 + `SIG_COUNT` * 64 | `PREAMBLE_LEN` | The [message preamble](#message-preamble)
| Message Body | 0x01 + `SIG_COUNT * 64 + `PREAMBLE_LEN` | `MESSAGE_LEN` | The message content

The signature count **MUST** match the value of signers count from the message
preamble.

Signatures **MUST** be ordered to match their corresponding public keys as
specified in the message preamble.

## Runtime Considerations

To prevent social attacks by which the signer is tricked into signing a transaction,
the runtime **MUST NOT** accept signed off-chain messages as transactions under any
circumstances. The first byte of the [signing domain specifier](#signing-domain-specifier)
is chosen such that it corresponds to a value (`0xff`) which is implicitly illegal
as the first byte in a transaction `MessageHeader` today. The property is implicit
because the top bit in the first byte of a `MessageHeader` being set signals a
versioned transaction, but only a value of
[zero is supported](https://github.com/solana-labs/solana/blob/b6ae6c1fe17e4b64c5051c651ca2585e4f55468c/sdk/program/src/message/versions/mod.rs#L269-L281)
at this time. The runtime will need to be modified to reserve 127 as an illegal
version number, making this property explicit.

### Implementation

The runtime changes described above have been implemented in PR [#29807](https://github.com/solana-labs/solana/pull/29807)

================
File: docs/src/proposals/optimistic_confirmation.md
================
---
title: Optimistic Confirmation
---

## Primitives

`vote(X, S)` - Votes will be augmented with a "reference" slot, `X`
which is the **latest** ancestor of this fork that this validator voted on
with a proof of switching. As long as the validator makes consecutive votes
that are all descended from each other, the same `X` should be used for all
those votes. When the validator makes a vote for a slot `s` that is not
descended from the previous, `X` will be set to the new slot `s`. All votes
will then be of the form `vote(X, S)`, where `S` is the sorted list of slots
`(s, s.lockout)` being voted for.

Given a vote `vote(X, S)`, let `S.last == vote.last` be the last slot in `S`.

Now we define some "Optimistic Slashing" slashing conditions. The intuition
for these is described below:

- `Intuition`: If a validator submits `vote(X, S)`, the same validator
  should not have voted on a different fork that "overlaps" this fork.
  More concretely, this validator should not have cast another vote
  `vote(X', S')` where the range `[X, S.last]` overlaps the range
  `[X', S'.last]`, `X != X'`, as shown below:

```text
                                  +-------+
                                  |       |
                        +---------+       +--------+
                        |         |       |        |
                        |         +-------+        |
                        |                          |
                        |                          |
                        |                          |
                    +---+---+                      |
                    |       |                      |
                X   |       |                      |
                    |       |                      |
                    +---+---+                      |
                        |                          |
                        |                      +---+---+
                        |                      |       |
                        |                      |       |  X'
                        |                      |       |
                        |                      +---+---+
                        |                          |
                        |                          |
                        |                          |
                        |                          |
                        |                      +---+---+
                        |                      |       |
                        |                      |       |  S'.last
                        |                      |       |
                        |                      +-------+
                        |
                    +---+---+
                    |       |
                 s  |       |
                    |       |
                    +---+---+
                        |
                        |
                        |
                        |
                    +---+---+
                    |       |
             S.last |       |
                    |       |
                    +-------+
```

(Example of slashable votes vote(X', S') and vote(X, S))

In the diagram above, note that the vote for `S.last` must have been sent after
the vote for `S'.last` (due to lockouts, the higher vote must have been sent
later). Thus, the sequence of votes must have been: `X ... S'.last ... S.last`.
This means after the vote on `S'.last`, the validator must have switched back
to the other fork at some slot `s > S'.last > X`. Thus, the vote for `S.last`
should have used `s` as the "reference" point, not `X`, because that was the
last "switch" on the fork.

To enforce this, we define the "Optimistic Slashing" slashing conditions. Given
any two distinct votes `vote(X, S)`and `vote(X', S')` by the same validator,
the votes must satisfy:

- `X <= S.last`, `X' <= S'.last`
- All `s` in `S` are ancestors/descendants of one another,
  all `s'` in `S'` are ancestors/descendants of one another,
-
- `X == X'` implies `S` is parent of `S'` or `S'` is a parent of `S`
- `X' > X` implies `X' > S.last` and `S'.last > S.last`
  and for all `s` in `S`, `s + lockout(s) < X'`
- `X > X'` implies `X > S'.last` and `S.last > S'.last`
  and for all `s` in `S'`, `s + lockout(s) < X`

(The last two rules imply the ranges cannot overlap):
Otherwise the validator is slashed.

`Range(vote)` - Given a vote `v = vote(X, S)`, define `Range(v)` to be the range
of slots `[X, S.last]`.

`SP(old_vote, new_vote)` - This is the "Switching Proof" for `old_vote`, the
validator's latest vote. Such a proof is necessary anytime a validator switches
their "reference" slot (see vote section above). The switching proof includes
a reference to `old_vote`, so that there's a record of what the "range" of that
`old_vote` was (to make other conflicting switches in this range slashable).
Such a switch must still respect lockouts.

A switching proof shows that `> 1/3` of the network is locked out at slot
`old_vote.last`.

The proof is a list of elements `(validator_id, validator_vote(X, S))`, where:

1. The sum of the stakes of all the validator id's `> 1/3`

2. For each `(validator_id, validator_vote(X, S))`, there exists some slot `s`
   in `S` where:
   _ a.`s` is not a common ancestor of both `validator_vote.last` and
   `old_vote.last` and `new_vote.last`.
   _ b. `s` is not a descendant of `validator_vote.last`. \* c. `s + s.lockout() >= old_vote.last` (implies validator is still locked
   out on slot `s` at slot `old_vote.last`).

Switching forks without a valid switching proof is slashable.

## Definitions:

Optimistic Confirmation - A block `B` is then said to have achieved
"optimistic confirmation" if `>2/3` of stake have voted with votes `v`
where `Range(v)` for each such `v` includes `B.slot`.

Finalized - A block `B` is said to be finalized if at least one
correct validator has rooted `B` or a descendant of `B`.

Reverted - A block `B` is said to be reverted if another block `B'` that
is not a parent or descendant of `B` was finalized.

## Guarantees:

A block `B` that has reached optimistic confirmation will not be reverted
unless at least one validator is slashed.

## Proof:

Assume for the sake of contradiction, a block `B` has achieved
`optimistic confirmation` at some slot `B + n` for some `n`, and:

- Another block `B'` that is not a parent or descendant of `B`
  was finalized.
- No validators violated any slashing conditions.

By the definition of `optimistic confirmation`, this means `> 2/3` of validators
have each shown some vote `v` of the form `Vote(X, S)` where `X <= B <= v.last`.
Call this set of validators the `Optimistic Validators`.

Now given a validator `v` in `Optimistic Validators`, given two votes made by
`v`, `Vote(X, S)` and `Vote(X', S')` where `X <= B <= S.last`, and
`X' <= B <= S'.last`, then `X == X'` otherwise an "Optimistic Slashing" condition
is violated (the "ranges" of each vote would overlap at `B`).

Thus define the `Optimistic Votes` to be the set of votes made by
`Optimistic Validators`, where for each optimistic validator `v`, the vote made
by `v` included in the set is the `maximal` vote `Vote(X, S)` with the
greatest `S.last` out of any votes made by `v` that satisfy `X <= B <= S.last`.
Because we know from above `X` for all such votes made by `v` is unique, we know
there is such a unique `maximal` vote.

### Lemma 1:

`Claim:` Given a vote `Vote(X, S)` made by a validator `V` in the
`Optimistic Validators` set, and `S` contains a vote for a slot `s`
for which:

- `s + s.lockout > B`,
- `s` is not an ancestor or descendant of `B`,

then `X > B`.

```text
                                  +-------+
                                  |       |
                        +---------+       +--------+
                        |         |       |        |
                        |         +-------+        |
                        |                          |
                        |                          |
                        |                          |
                        |                      +---+---+
                        |                      |       |
                        |                      |       |  X'
                        |                      |       |
                        |                      +---+---+
                        |                          |
                        |                          |
                        |                      +---+---+
                        |                      |       |
                        |                      |       |  B (Optimistically Confirmed)
                        |                      |       |
                        |                      +---+---+
                        |                          |
                        |                          |
                        |                          |
                        |                      +---+---+
                        |                      |       |
                        |                      |       |  S'.last
                        |                      |       |
                        |                      +-------+
                        |
                    +---+---+
                    |       |
                 X  |       |
                    |       |
                    +---+---+
                        |
                        |
                        |
                        |
                        |
                        |
                    +---+---+
                    |       |
            S.last  |       |
                    |       |
                    +---+---+
                        |
                        |
                        |
                        |
                    +---+---+
                    |       |
      s + s.lockout |       |
                    +-------+
```

`Proof`: Assume for the sake of contradiction a validator `V` from the
"Optimistic Validators" set made such a vote `Vote(X, S)` where `S` contains
a vote for a slot `s` not an ancestor or descendant of `B`, where
`s + s.lockout > B`, but `X <= B`.

Let `Vote(X', S')` be the vote in `Optimistic Votes` set made by validator `V`.
By definition of that set (all votes optimistically confirmed `B`),
`X' <= B <= S'.last` (see diagram above).

This implies that because it's assumed above `X <= B`, then `X <= S'.last`,
so by the slashing rules, either `X == X'` or `X < X'` (otherwise would
overlap the range `(X', S'.last)`).

`Case X == X'`:

Consider `s`. We know `s != X` because it is assumed `s` is not an ancestor
or descendant of `B`, and `X` is an ancestor of `B`. Because `S'.last` is a
descendant of `B`, this means `s` is also not an ancestor or descendant of
`S'.last`. Then because `S.last` is descended from `s`, then `S'.last` cannot
be an ancestor or descendant of `S.last` either. This implies `X != X'` by the
"Optimistic Slashing" rules.

`Case X < X'`:

Intuitively, this implies that `Vote(X, S)` was made "before" `Vote(X', S')`.

From the assumption above, `s + s.lockout > B > X'`. Because `s` is not an
ancestor of `X'`, lockouts would have been violated when this validator
first attempted to submit a switching vote to `X'` with some vote of the
form `Vote(X', S'')`.

Since none of these cases are valid, the assumption must have been invalid,
and the claim is proven.

### Lemma 2:

Recall `B'` was the block finalized on a different fork than
"optimistically" confirmed" block `B`.

`Claim`: For any vote `Vote(X, S)` in the `Optimistic Votes` set, it must be
true that `B' > X`

```text
                                +-------+
                                |       |
                       +--------+       +---------+
                       |        |       |         |
                       |        +-------+         |
                       |                          |
                       |                          |
                       |                          |
                       |                      +---+---+
                       |                      |       |
                       |                      |       |  X
                       |                      |       |
                       |                      +---+---+
                       |                          |
                       |                          |
                       |                      +---+---+
                       |                      |       |
                       |                      |       |  B (Optimistically Confirmed)
                       |                      |       |
                       |                      +---+---+
                       |                          |
                       |                          |
                       |                          |
                       |                      +---+---+
                       |                      |       |
                       |                      |       |  S.last
                       |                      |       |
                       |                      +-------+
                       |
                   +---+---+
                   |       |
    B'(Finalized)  |       |
                   |       |
                   +-------+
```

`Proof`: Let `Vote(X, S)` be a vote in the `Optimistic Votes` set. Then by
definition, given the "optimistically confirmed" block `B`, `X <= B <= S.last`.

Because `X` is a parent of `B`, and `B'` is not a parent or ancestor of `B`,
then:

- `B' != X`
- `B'` is not a parent of `X`

Now consider if `B'` < `X`:

`Case B' < X`: We will show this is a violation of lockouts.
From above, we know `B'` is not a parent of `X`. Then because `B'` was rooted,
and `B'` is not a parent of `X`, then the validator should not have been able
to vote on the higher slot `X` that does not descend from `B'`.

### Proof of Safety:

We now aim to show at least one of the validators in the
`Optimistic Validators` set violated a slashing rule.

First note that in order for `B'` to have been rooted, there must have been
`> 2/3` stake that voted on `B'` or a descendant of `B'`. Given that the
`Optimistic Validator` set also contains `> 2/3` of the staked validators,
it follows that `> 1/3` of the staked validators:

- Rooted `B'` or a descendant of `B'`
- Also submitted a vote `v` of the form `Vote(X, S)` where `X <= B <= v.last`.

Let the `Delinquent` set be the set of validators that meet the above
criteria.

By definition, in order to root `B'`, each validator `V` in `Delinquent`
must have each made some "switching vote" of the form `Vote(X_v, S_v)` where:

- `S_v.last > B'`
- `S_v.last` is a descendant of `B'`, so it can't be a descendant of `B`
- Because `S_v.last` is not a descendant of `B`, then `X_v` cannot be a
  descendant or ancestor of `B`.

By definition, this delinquent validator `V` also made some vote `Vote(X, S)`
in the `Optimistic Votes` where by definition of that set (optimistically
confirmed `B`), we know `S.last >= B >= X`.

By `Lemma 2` we know `B' > X`, and from above `S_v.last > B'`, so then
`S_v.last > X`. Because `X_v != X` (cannot be a descendant or ancestor of
`B` from above), then by the slashing rules then, we know `X_v > S.last`.
From above, `S.last >= B >= X` so for all such "switching votes", `X_v > B`.

Now ordering all these "switching votes" in time, let `V` to be the validator
in `Optimistic Validators` that first submitted such a "switching vote"
`Vote(X', S')`, where `X' > B`. We know that such a validator exists because
we know from above that all delinquent validators must have submitted such
a vote, and the delinquent validators are a subset of the
`Optimistic Validators`.

Let `Vote(X, S)` be the unique vote in `Optimistic Votes` made by
validator `V` (maximizing `S.last`).

Given `Vote(X, S)` because `X' > B >= X`, then `X' > X`, so
by the "Optimistic Slashing" rules, `X' > S.last`.

In order to perform such a "switching vote" to `X'`, a switching proof
`SP(Vote(X, S), Vote(X', S'))` must show `> 1/3` of stake being locked
out at this validator's latest vote, `S.last`. Combine this `>1/3` with the
fact that the set of validators in the `Optimistic Voters` set consists of
`> 2/3` of the stake, implies at least one optimistic validator `W` from the
`Optimistic Voters` set must have submitted a vote (recall the definition of
a switching proof),`Vote(X_w, S_w)` that was included in validator `V`'s
switching proof for slot `X'`, where `S_w` contains a slot `s` such that:

- `s` is not a common ancestor of `S.last` and `X'`
- `s` is not a descendant of `S.last`.
- `s' + s'.lockout > S.last`

Because `B` is an ancestor of `S.last`, it is also true then:

- `s` is not a common ancestor of `B` and `X'`
- `s' + s'.lockout > B`

which was included in `V`'s switching proof.

Now because `W` is also a member of `Optimistic Voters`, then by the `Lemma 1`
above, given a vote by `W`, `Vote(X_w, S_w)`, where `S_w` contains a vote for
a slot `s` where `s + s.lockout > B`, and `s` is not an ancestor of `B`, then
`X_w > B`.

Because validator `V` included vote `Vote(X_w, S_w)` in its proof of switching
for slot `X'`, then his implies validator `V'` submitted vote `Vote(X_w, S_w)`
**before** validator `V` submitted its switching vote for slot `X'`,
`Vote(X', S')`.

But this is a contradiction because we chose `Vote(X', S')` to be the first vote
made by any validator in the `Optimistic Voters` set where `X' > B` and `X'` is
not a descendant of `B`.

================
File: docs/src/proposals/optimistic-confirmation-and-slashing.md
================
---
title: Optimistic Confirmation and Slashing
---

Progress on optimistic confirmation can be tracked here

https://github.com/solana-labs/solana/projects/52

At the end of May, the mainnet-beta is moving to 1.1, and testnet is
moving to 1.2. With 1.2, testnet will behave as if it has optimistic
finality as long as at least no more than 4.66% of the validators are
acting maliciously. Applications can assume that 2/3+ votes observed in
gossip confirm a block or that at least 4.66% of the network is violating
the protocol.

## How does it work?

The general idea is that validators must continue voting following their
last fork, unless the validator can construct a proof that their current
fork may not reach finality. The way validators construct this proof is
by collecting votes for all the forks excluding their own. If the set
of valid votes represents over 1/3+X of the epoch stake weight, there
may not be a way for the validators current fork to reach 2/3+ finality.
The validator hashes the proof (creates a witness) and submits it with
their vote for the alternative fork. But if 2/3+ votes for the same
block, it is impossible for any of the validators to construct this proof,
and therefore no validator is able to switch forks and this block will
be eventually finalized.

## Tradeoffs

The safety margin is 1/3+X, where X represents the minimum amount of stake
that will be slashed in case the protocol is violated. The tradeoff is
that liveness is now reduced by 2X in the worst case. If more than 1/3 -
2X of the network is unavailable, the network may stall and will only
resume finalizing blocks after the network recovers below 1/3 - 2X of
failing nodes. So far, we haven’t observed a large unavailability hit
on our mainnet, cosmos, or tezos. For our network, which is primarily
composed of high availability systems, this seems unlikely. Currently,
we have set the threshold percentage to 4.66%, which means that if 23.68%
have failed the network may stop finalizing blocks. For our network,
which is primarily composed of high availability systems, a 23.68% drop
in availability seems unlikely. 1:10^12 odds, assuming five 4.7% staked
nodes with 0.995 uptime.

## Security

Long term average votes per slot has been 670,000,000 votes / 12,000,000
slots, or 55 out of 64 voting validators. This includes missed blocks due
to block producer failures. When a client sees 55/64, or ~86% confirming
a block, it can expect that ~24% or `(86 - 66.666.. + 4.666..)%` of
the network must be slashed for this block to fail full finalization.

## Why Solana?

This approach can be built on other networks, but the implementation
complexity is significantly reduced on Solana because our votes
have provable VDF-based timeouts. It’s not clear if switching proofs
can be easily constructed in networks with weak assumptions about
time.

## Slashing roadmap

Slashing is a hard problem, and it becomes harder when the goal of
the network is to have the lowest possible latency. The tradeoffs are
especially apparent when optimizing for latency. For example, ideally
validators should cast and propagate their votes before the
memory has been synced to disk, which means that the risk of local state
corruption is much higher.

Fundamentally, our goal for slashing is to slash 100% in cases where
the node is maliciously trying to violate safety rules and 0% during
routine operation. How we aim to achieve that is to first implement
slashing proofs without any automatic slashing whatsoever.

Right now, for regular consensus, after a safety violation, the
network will halt. We can analyze the data and figure out who was
responsible and propose that the stake should be slashed after
restart. A similar approach will be used with a optimistic conf.
An optimistic conf safety violation is easily observable, but under
normal circumstances, an optimistic confirmation safety violation
may not halt the network. Once the violation has been observed, the
validators will freeze the affected stake in the next epoch and
will decide on the next upgrade if the violation requires slashing.

In the long term, transactions should be able to recover a portion
of the slashing collateral if the optimistic safety violation is
proven. In that scenario, each block is effectively insured by the
network.

================
File: docs/src/proposals/optimistic-transaction-propagation-signal.md
================
---
title: Optimistic Transaction Propagation Signal
---

## Current Retransmit behavior

The retransmission tree currently considers:
1. epoch staked nodes
2. tvu peers (filtered by contact info and shred version)
3. current validator

concatenating (1), (2), and (3)
deduplicating this list of entries by pubkey favoring entries with contact info
filtering this list by entries with contact info

This list is then randomly shuffled by stake weight.

Shreds are then retransmitted to up to FANOUT neighbors and up to FANOUT
children.

## Deterministic retransmission tree

`weighted_shuffle` will use a deterministic seed when
`enable_deterministic_seed` has been enabled based on the triple (shred slot,
shred index, leader pubkey):

```
if enable_deterministic_seed(self.slot(), root_bank) {
    hashv(&[
        &self.slot().to_le_bytes(),
        &self.index().to_le_bytes(),
        &leader_pubkey.to_bytes(),
    ])
```

First, only epoch staked nodes will be considered regardless of presence of
contact info (and possibly including the validator node itself).

A deterministic ordering of the epoch staked nodes will be created based on the
deterministic shred seed using weighted_shuffle.

Let `neighbor_set` be selected from up to FANOUT neighbors of the current node.
Let `child_set` be selected from up to FANOUT children of the current node.

Filter `neighbor_set` by contact info.
Filter `child_set` by contact info.

Let `epoch_set` be the union of `neighbor_set` and `child_set`.

Let `remaining_set` be all other nodes with contact info not contained in
`epoch_set`.

If `epoch_set.len < 2*FANOUT` then we may randomly select up to
`2*FANOUT - epoch_set.len` nodes to retransmit to from `remaining_set`.

## Receiving retransmitted shred

If the current validator node is not in the set of epoch staked nodes for the
shred epoch then no early retransmission information can be obtained.

Compute the deterministic shred seed.

Run the deterministic epoch_stakes shuffle.

Find position of self in the neighbor or child sets.

Calculate the sum of the stakes of all nodes in the current and prior
distribution levels.

### Stake summation considerations:

- Stake sum could include stakes of nodes which had been skipped in prior
distribution levels because of lack of contact info.
- Current node was part of original epoch staked shuffle from retransmitter
but was filtered out because of missing contact info. Current node subsequently
receives retransmission of shred and assumes that the retransmit was a result
of the deterministic tree calculation and not from subsequent random selection.
This should be benign because the current node will underestimate prior stake
weight in the retransmission tree.

### General considerations:

attack by leader (level 0):
- transmits shred for distribution through the tree as normal
- additionally transmits shred (or fake shred) directly to node(s) at level >=2
leading the node(s) to believe a greater percentage of the tree retransmission
tree had been processed

attack by node at level n:
- retransmits shred to node(s) at level >=n+2 leading the node(s) to believe a
greater percentage of the tree retransmission tree had been processed

### Questions

- Should receiving nodes attempt to verify that the origin of the shred was
retransmitted from the expected node? If so, consideration of spoofing?
- How is this information consumed?

## Notes

Practically, signals should fall into the following buckets:
1. current leader (can signal layer 1 when broadcast is sent)
2. layer 1
1.1. can signal layer 1 when shred is received
1.2. can signal layer 1 + subset of layer 2 when retransmit is sent
3. layer 2
3.1. can signal layer 2 when shred is received
3.2. can signal layer 2 + subset of layer 3 when retransmit is sent
4. current node not a member of epoch staked nodes, no signal can be sent

================
File: docs/src/proposals/partitioned-inflationary-rewards-distribution.md
================
---
title: Partitioned Inflationary Rewards Distribution
---

## Problem

With the increase of number of stake accounts, computing and redeeming the stake
rewards at the start block of the epoch boundary becomes very expensive.
Currently, with 550K stake accounts, the stake reward time has already taken
more than 10 seconds. This prolonged computation slows down the network, and can
cause large number of forks at the epoch boundary, which makes the matter even
worse.

## Proposed Solutions

Instead of computing and reward stake accounts at epoch boundary, we will
decouple reward computation and reward credit into two phases.

A separate service, "EpochRewardCalculationService" will be created. The service
will listen to a channel for any incoming rewards calculation requests, and
perform the calculation for the rewards. For each block that cross the epoch
boundary, the bank will send a request to the `EpochRewardCalculationService`.
This marks the start of the reward computation phase.

```
N-1 -- N -- N+1
     \
      \
        N+2
```

In the above example, N is the start of the new epoch. Two rewards calculation
requests will be sent out at slot N and slot N+2 because they both cross the
epoch boundary and are on different forks. To avoid repeated computation with
the same input, the signature of the computation requests, `hash(epoch_number,
hash(stake_accounts_data), hash(vote_accounts), hash(delegation_map))`, are
calculated. Duplicated computation requests will be discard. For the above
example, if there are no stake/vote accounts changes between slot N and slot
N+2, the 2nd computation request will be discarded.

When reaching block height `N` after the start of the `reward computation
phase`, the bank starts the second phase - reward credit, in which, the bank
first query the `epoch calc service` with the request signature to get the
rewards result, which will be resented as a map from accounts_pubkey->rewards,
then credit the rewards to the stake accounts for the next `M` blocks. If the
rewards result is not available, the bank will wait until the results are
available.

We call them: <br/>
(a) calculating interval: `[epoch_start, epoch_start+N]` <br/>
(b) credit interval: `[epoch_start+N+1, epoch_start+N+M]`, respectively. <br/>
And the combined interval `[epoch_start, epoch_start+N+M]` is called
`rewarding interval`.

For `calculating interval`, `N` is chosen to be sufficiently large so that the
background computation should have completed and the result of the reward
computation is available at the end of `calculating interval`. `N` can be fixed
such as 100 (roughly equivalent to 50 seconds), or chosen as a function of the
number of stake accounts, `f(num_stake_accounts)`.

In `credit interval`,  the bank will fetch the reward computation results from
the background thread and start credit the rewards during the next `M` blocks.
The idea is partition the accounts into `M` partitions. And each block, the bank
credit `1/M` accounts. The partition is required to be deterministic for the
current epoch, but must also be random across different epochs. One way to
achieve these properties is to hash the account's pubkey with some epoch
dependent values, sort the results, and divide them into `M` bins. The epoch
dependent value can be the epoch number, total rewards for the epoch, the leader
pubkey for the epoch block, etc. `M` can be choses based on 50K account per
block, which equal to `ceil(num_stake_accounts/50,000)`.

`num_stake_account` is extracted from `leader_schedule_epoch` block, so we don't
run into discrepancy where new transactions right before an epoch boundary
creates one fork with `X` stake accounts and another fork with `Y` stake accounts.

In order to avoid putting extra burden of computing and credit the stake reward
for blocks produced during the `rewarding interval`, we can reduce the compute
budget limits on those blocks in `rewarding interval`, and reserve some computing
and read/write capacity to perform stake rewarding.

### Challenges

1. stake accounts reads/writes during the `rewarding interval`

`epoch_start..epoch_start+N+M` Because of the delayed credit of the rewards,
Reads to those stake accounts will not return the value that the user are
expecting (viz. not include the recent epoch stake rewards). Writes to those
stake accounts will be lost once the reward are credited on block
`epoch_start+N+M`. We will need to modify the runtime to restrict read/writes to
stake accounts during the `rewarding interval`. Any transactions, which involves
stake accounts, will result in a new execution error, i.e. "stake rewards
pending, account access is restricted". However, normal rpc queries, such as
'getBalance', will return the current lamport of the account. The user can
expect the rewards to be credit as some time point during the 'rewarding
interval'.

2. voting during `reward interval`

During reward interval, vote transactions must be processed normally for
achieving consensus and making progress for rooted blocks. However, those vote
transactions may potentially change the vote accounts balance (i.e. pay for the
voting transaction fee if vote_account and block reward recipient accounts
are the same), before the epoch rewards are paid. When the epoch rewards are
paid, those block rewards will be wiped out by the stale cached value. To
prevent this, we will enforce that the vote_account and authorized_voter
authority must be different.

3. snapshot taken during the `rewarding interval`

If a snapshot is taken during the `rewarding interval`, it would miss the
rewards for the stake accounts. Any plain restart from those snapshots will be
wrong, unless we reconstruct the rewards from the recent epoch boundary. This
will add some complexity to validator restart. In the first implementation, we
will force *not* taking any snapshot and *not* performing accounts hash
calculation during the `rewarding interval`. Incremental snapshot request will
be skipped. Full snapshot request will be re-queued be picked up later at the
end of the `reward interval`.

In future, if needed, we can
revisit to enable taking snapshots and perform hash calculation during reward
interval.

4. account-db related action during the `rewarding interval`

Account-db related action such as flush, clean, squash, shrink etc. may touch
and evict the stake accounts from account db's cache during the `rewarding
interval`. This will slow down the credit in the future at bank `epoch_start+N`.
We may need to exclude such accounts_db actions for stake_accounts during
`rewarding interval`. This is going to be a performance tuning problem. In the
first implementation, for simplicity, we will keep the account-db action as it
is, and make the `credit interval` larger to accommodate the performance hit
when writing back those accounts. In future, we can continue tuning account db
actions during 'rewarding interval'.

5. view of total epoch capitalization change

The view of total epoch capitalization, instead of being available at every
epoch boundary, is only available after the `rewarding interval`. Any third
party application logic, which depends on total epoch capitalization, need to
wait after `rewarding interval`.

6. `getInflationReward` JSONRPC API method call

Today, the `getInflationReward` JSONRPC API method call can simply grab the
first block in the target epoch and lookup the target stake account's rewards
entry.  With these changes, the call will need updated to derive the target
stake account's credit block, grab _that_ block, then lookup rewards.
Additionally we'll need to return more informative errors for queries made
during the lockout period, so users can know that their rewards are pending for
the target epoch. A new rpc API, i.e. `getRewardInterval`, will be added for
querying the `rewarding interval` for the current epoch.

================
File: docs/src/proposals/program-instruction-macro.md
================
# Program Instruction Macro

## Problem

Currently, inspecting an on-chain transaction requires depending on a
client-side, language-specific decoding library to parse the instruction. If
rpc methods could return decoded instruction details, these custom solutions
would be unnecessary.

We can deserialize instruction data using a program's Instruction enum, but
decoding the account-key list into human-readable identifiers requires manual
parsing. Our current Instruction enums have that account information, but only
in variant docs.

Similarly, we have instruction constructor functions that duplicate nearly all
the information in the enum, but we can't generate that constructor from the
enum definition because the list of account references is in code comments.

Also, Instruction docs can vary between implementations, as there is no
mechanism to ensure consistency.

## Proposed Solution

Move the data from code comments to attributes, such that the constructors
can be generated, and include all the documentation from the enum definition.

Here is an example of an Instruction enum using the new accounts format:

```rust,ignore
#[instructions(test_program::id())]
pub enum TestInstruction {
    /// Transfer lamports
    #[accounts(
        from_account(SIGNER, WRITABLE, desc = "Funding account"),
        to_account(WRITABLE, desc = "Recipient account"),
    )]
    Transfer {
        lamports: u64,
    },

    /// Provide M of N required signatures
    #[accounts(
        data_account(WRITABLE, desc = "Data account"),
        signers(SIGNER, multiple, desc = "Signer"),
    )]
    Multisig,

    /// Consumes a stored nonce, replacing it with a successor
    #[accounts(
        nonce_account(SIGNER, WRITABLE, desc = "Nonce account"),
        recent_blockhashes_sysvar(desc = "RecentBlockhashes sysvar"),
        nonce_authority(SIGNER, optional, desc = "Nonce authority"),
    )]
    AdvanceNonceAccount,
}
```

An example of the generated TestInstruction with docs:

```rust,ignore
pub enum TestInstruction {
    /// Transfer lamports
    ///
    /// * Accounts expected by this instruction:
    ///   0. `[WRITABLE, SIGNER]` Funding account
    ///   1. `[WRITABLE]` Recipient account
    Transfer {
        lamports: u64,
    },

    /// Provide M of N required signatures
    ///
    /// * Accounts expected by this instruction:
    ///   0. `[WRITABLE]` Data account
    ///   * (Multiple) `[SIGNER]` Signers
    Multisig,

    /// Consumes a stored nonce, replacing it with a successor
    ///
    /// * Accounts expected by this instruction:
    ///   0. `[WRITABLE, SIGNER]` Nonce account
    ///   1. `[]` RecentBlockhashes sysvar
    ///   2. (Optional) `[SIGNER]` Nonce authority
    AdvanceNonceAccount,
}
```

Generated constructors:

```rust,ignore
/// Transfer lamports
///
/// * `from_account` - `[WRITABLE, SIGNER]` Funding account
/// * `to_account` - `[WRITABLE]` Recipient account
pub fn transfer(from_account: Pubkey, to_account: Pubkey, lamports: u64) -> Instruction {
    let account_metas = vec![
        AccountMeta::new(from_pubkey, true),
        AccountMeta::new(to_pubkey, false),
    ];
    Instruction::new_with_bincode(
        test_program::id(),
        &SystemInstruction::Transfer { lamports },
        account_metas,
    )
}

/// Provide M of N required signatures
///
/// * `data_account` - `[WRITABLE]` Data account
/// * `signers` - (Multiple) `[SIGNER]` Signers
pub fn multisig(data_account: Pubkey, signers: &[Pubkey]) -> Instruction {
    let mut account_metas = vec![
        AccountMeta::new(nonce_pubkey, false),
    ];
    for pubkey in signers.iter() {
        account_metas.push(AccountMeta::new_readonly(pubkey, true));
    }

    Instruction::new_with_bincode(
        test_program::id(),
        &TestInstruction::Multisig,
        account_metas,
    )
}

/// Consumes a stored nonce, replacing it with a successor
///
/// * nonce_account - `[WRITABLE, SIGNER]` Nonce account
/// * recent_blockhashes_sysvar - `[]` RecentBlockhashes sysvar
/// * nonce_authority - (Optional) `[SIGNER]` Nonce authority
pub fn advance_nonce_account(
    nonce_account: Pubkey,
    recent_blockhashes_sysvar: Pubkey,
    nonce_authority: Option<Pubkey>,
) -> Instruction {
    let mut account_metas = vec![
        AccountMeta::new(nonce_account, false),
        AccountMeta::new_readonly(recent_blockhashes_sysvar, false),
    ];
    if let Some(pubkey) = authorized_pubkey {
        account_metas.push(AccountMeta::new_readonly*nonce_authority, true));
    }
    Instruction::new_with_bincode(
        test_program::id(),
        &TestInstruction::AdvanceNonceAccount,
        account_metas,
    )
}

```

Generated TestInstructionVerbose enum:

```rust,ignore
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub enum TestInstruction {
    /// Transfer lamports
    Transfer {
        /// Funding account
        funding_account: u8

        /// Recipient account
        recipient_account: u8

        lamports: u64,
    },

    /// Provide M of N required signatures
    Multisig {
        data_account: u8,
        signers: Vec<u8>,
    },

    /// Consumes a stored nonce, replacing it with a successor
    AdvanceNonceAccount {
        nonce_account: u8,
        recent_blockhashes_sysvar: u8,
        nonce_authority: Option<u8>,
    }
}

impl TestInstructionVerbose {
    pub fn from_instruction(instruction: TestInstruction, account_keys: Vec<u8>) -> Self {
        match instruction {
            TestInstruction::Transfer { lamports } => TestInstructionVerbose::Transfer {
                funding_account: account_keys[0],
                recipient_account: account_keys[1],
                lamports,
            }
            TestInstruction::Multisig => TestInstructionVerbose::Multisig {
                data_account: account_keys[0],
                signers: account_keys[1..],
            }
            TestInstruction::AdvanceNonceAccount => TestInstructionVerbose::AdvanceNonceAccount {
                nonce_account: account_keys[0],
                recent_blockhashes_sysvar: account_keys[1],
                nonce_authority: &account_keys.get(2),
            }
        }
    }
}

```

## Considerations

1. **Named fields** - Since the resulting Verbose enum constructs variants with
   named fields, any unnamed fields in the original Instruction variant will need
   to have names generated. As such, it would be considerably more straightforward
   if all Instruction enum fields are converted to named types, instead of unnamed
   tuples. This seems worth doing anyway, adding more precision to the variants and
   enabling real documentation (so developers don't have to do
   [this](https://github.com/solana-labs/solana/blob/3aab13a1679ba2b7846d9ba39b04a52f2017d3e0/sdk/src/system_instruction.rs#L140)
   This will cause a little churn in our current code base, but not a lot.
2. **Variable account lists** - This approach offers a couple options for
   variable account lists. First, optional accounts may be added and tagged with
   the `optional` keyword. However, currently only one optional account is
   supported per instruction. Additional data will need to be added to the
   instruction to support multiples, enabling identification of which accounts are
   present when some but not all are included. Second, accounts that share the same
   features may be added as a set, tagged with the `multiple` keyword. Like
   optional accounts, only one multiple account set is supported per instruction
   (and optional and multiple may not coexist). More complex instructions that
   cannot be accommodated by `optional` or `multiple`, requiring logic to figure
   out account order/representation, should probably be made into separate
   instructions.

================
File: docs/src/proposals/return-data.md
================
# Return data from SBF programs

## Problem

In the Solidity language it is permitted to return any number of values from a function,
for example a variable length string can be returned:

```
function foo1() public returns (string) {
    return "Hello, world!\n";
}
```

Multiple values, arrays and structs are permitted too.

```
struct S {
    int f1;
    bool f2
};

function foo2() public returns (string, int[], S) {
    return (a, b, c);
}
```

All the return values are eth abi encoded to a variable-length byte array.

On ethereum errors can be returned too:

```
function withdraw() public {
    require(msg.sender == owner, "Permission denied");
}

function failure() public {
    revert("I afraid I can't do that dave");
}
```
These errors help the developer debug any issue they are having, and can
also be caught in a Solidity `try` .. `catch` block. Outside of a `try` .. `catch`
block, any of these would cause the transaction or rpc to fail.

## Existing solution

The existing solution that Solang uses, writes the return data to the callee account data.
The caller's account cannot be used, since the callee may not be the same SBF program, so
it will not have permission to write to the callee's account data.

Another solution would be to have a single return data account which is passed
around through CPI. Again this does not work for CPI as the callee may not have
permission to write to it.

The problem with this solution is:

- It does not work for RPC calls
- It is very racey; a client has to submit the Tx and then retrieve the account
  data. This is not atomic so the return data can be overwritten by another transaction.

## Requirements for Solution

It must work for:

- RPC: An RPC should be able to return any number of values without writing to account data
- Transaction: An transaction should be able to return any number of values without needing to write them account data
- CPI: The callee must "set" return value, and the caller must be able to retrieve it.

## Review of other chains

### Ethereum (EVM)

The `RETURN` opcode allows a contract to set a buffer as a returndata. This opcode takes a pointer to memory and a size. The `REVERT` opcode works similarly but signals that the call failed, and all account data changes must be reverted.

For CPI, the caller can retrieve the returned data of the callee using the `RETURNDATASIZE` opcode which returns the length, and the `RETURNDATACOPY` opcode, which takes a memory destination pointer, offset into the returndata, and a length argument.

Ethereum stores the returndata in blocks.

### Parity Substrate

The return data can be set using the `seal_return(u32 flags, u32 pointer, u32 size)` syscall.
- Flags can be 1 for revert, 0 for success (nothing else defined)
- Function does not return

CPI: The `seal_call()` syscall takes pointer to buffer and pointer to buffer size where return data goes
 - There is a 32KB limit for return data.

Parity Substrate does not write the return data to blocks.

## Rejected Solution

The concept of ephemeral accounts has been proposed a solution for this. This would
certainly work for the CPI case, but this would not work RPC or Transaction case.

## Proposed Solution

The callee can set the return data using a new system call `sol_set_return_data(buf: *const u8, length: u64)`.
There is a limit of 1024 bytes for the returndata. This function can be called multiple times, and
will simply overwrite what was written in the last call.

The return data can be retrieved with `sol_get_return_data(buf: *mut u8, length: u64, program_id: *mut Pubkey) -> u64`.
This function copies the return buffer, and the program_id that set the return data, and
returns the length of the return data, or `0` if no return data is set. In this case, program_id is not set.

When an instruction calls `sol_invoke()`, the return data of the callee is copied into the return data
of the current instruction. This means that any return data is automatically passed up the call stack,
to the callee of the current instruction (or the RPC call).

Note that `sol_invoke()` clears the returns data before invoking the callee, so that any return data from
a previous invoke is not reused if the invoked fails to set a return data. For example:

 - A invokes B
 - Before entry to B, return data is cleared.0
 - B sets some return data and returns
 - A invokes C
 - Before entry to C, return data is cleared.
 - C does not set return data and returns
 - A checks return data and finds it empty

Another scenario to consider:

 - A invokes B
 - B invokes C
 - C sets return data and returns
 - B does not touch return data and returns
 - A gets return data from C
 - A does not touch return data
 - Return data from transaction is what C set.

The compute costs are calculated for getting and setting the return data using
the syscalls.

For a normal RPC or Transaction, the returndata is base64-encoded and stored along side the sol_log
strings in the [stable log](https://github.com/solana-labs/solana/blob/95292841947763bdd47ef116b40fc34d0585bca8/sdk/src/process_instruction.rs#L275-L281).

## Note on returning errors

Solidity on Ethereum allows the contract to return an error in the return data. In this case, all
the account data changes for the account should be reverted. On Solana, any non-zero exit code
for a SBF program means the entire transaction fails. We do not wish to support an error return
by returning success and then returning an error in the return data. This would mean we would have
to support reverting the account data changes; this too expensive both on the VM side and the SBF
contract side.

Errors will be reported via sol_log.

================
File: docs/src/proposals/rip-curl.md
================
# RiP Curl: low-latency, transaction-oriented RPC

## Problem

Solana's initial RPC implementation was created for the purpose of allowing
users to confirm transactions that had just recently been sent to the cluster.
It was designed with memory usage in mind such that any validator should be
able to support the API without concern of DoS attacks.

Later down the line, it became desirable to use that same API to support the
Solana explorer. The original design only supported minutes of history, so we
changed it to instead store transaction statuses in a local RocksDB instance
and offer days of history. We then extended that to 6 months via BigTable.

With each modification, the API became more suitable for applications serving
static content and less appealing for transaction processing. The clients poll
for transaction status instead of being notified, giving the false impression
of higher confirmation times. Furthermore, what clients can poll for is
limited, preventing them from making reasonable real-time decisions, such as
recognizing a transaction is confirmed as soon as particular, known
validators vote on it.

## Proposed Solution

A web-friendly, transaction-oriented, streaming API built around the
validator's ReplayStage.

Improved client experience:

- Support connections directly from WebAssembly apps.
- Clients can be notified of confirmation progress in real-time, including votes
  and voter stake weight.
- Clients can be notified when the heaviest fork changes, if it affects the
  transactions confirmation count.

Easier for validators to support:

- Each validator supports some number of concurrent connections and otherwise
  has no significant resource constraints.
- Transaction status is never stored in memory and cannot be polled for.
- Signatures are only stored in memory until the desired commitment level or
  until the blockhash expires, whichever is later.

How it works:

1. The client connects to a validator using a reliable communication channel,
   such as a web socket.
2. The validator registers the signature with ReplayStage.
3. The validator sends the transaction into the Gulf Stream and retries all
   known forks until the blockhash expires (not until the transaction is
   accepted on only the heaviest fork). If the blockhash expires, the
   signature is unregistered, the client is notified, and connection is closed.
4. As ReplayStage detects events affecting the transaction's status, it
   notifies the client in real-time.
5. After confirmation that the transaction is rooted (`CommitmentLevel::Max`),
   the signature is unregistered and the server closes the upstream channel.

================
File: docs/src/proposals/simple-payment-and-state-verification.md
================
---
title: Simple Payment and State Verification
---

It is often useful to allow low resourced clients to participate in a Solana
cluster. Be this participation economic or contract execution, verification
that a client's activity has been accepted by the network is typically
expensive. This proposal lays out a mechanism for such clients to confirm that
their actions have been committed to the ledger state with minimal resource
expenditure and third-party trust.

## A Naive Approach

Validators store the signatures of recently confirmed transactions for a short
period of time to ensure that they are not processed more than once. Validators
provide a JSON RPC endpoint, which clients can use to query the cluster if a
transaction has been recently processed. Validators also provide a PubSub
notification, whereby a client registers to be notified when a given signature
is observed by the validator. While these two mechanisms allow a client to
verify a payment, they are not a proof and rely on completely trusting a
validator.

We will describe a way to minimize this trust using Merkle Proofs to anchor the
validator's response in the ledger, allowing the client to confirm on their own
that a sufficient number of their preferred validators have confirmed a
transaction. Requiring multiple validator attestations further reduces trust in
the validator, as it increases both the technical and economic difficulty of
compromising several other network participants.

## Light Clients

A 'light client' is a cluster participant that does not itself run a validator.
This light client would provide a level of security greater than trusting a
remote validator, without requiring the light client to spend a lot of resources
verifying the ledger.

Rather than providing transaction signatures directly to a light client, the
validator instead generates a Merkle Proof from the transaction of interest to
the root of a Merkle Tree of all transactions in the including block. This
Merkle Root is stored in a ledger entry which is voted on by validators,
providing it consensus legitimacy. The additional level of security for a light
client depends on an initial canonical set of validators the light client
considers to be the stakeholders of the cluster. As that set is changed, the
client can update its internal set of known validators with
[receipts](simple-payment-and-state-verification.md#receipts). This may become
challenging with a large number of delegated stakes.

Validators themselves may want to use light client APIs for performance reasons.
For example, during the initial launch of a validator, the validator may use a
cluster provided checkpoint of the state and verify it with a receipt.

## Receipts

A receipt is a minimal proof that; a transaction has been included in a block,
that the block has been voted on by the client's preferred set of validators
and that the votes have reached the desired confirmation depth.

### Transaction Inclusion Proof

A transaction inclusion proof is a data structure that contains a Merkle Path
from a transaction, through an Entry-Merkle to a Block-Merkle, which is included
in a Bank-Hash with the required set of validator votes. A chain of PoH Entries
containing subsequent validator votes, deriving from the Bank-Hash, is the proof
of confirmation.

#### Transaction Merkle

An Entry-Merkle is a Merkle Root including all transactions in a given entry,
sorted by signature. Each transaction in an entry is already merkled here:
https://github.com/solana-labs/solana/blob/b6bfed64cb159ee67bb6bdbaefc7f833bbed3563/ledger/src/entry.rs#L205.
This means we can show a transaction `T` was included in an entry `E`.

A Block-Merkle is the Merkle Root of all the Entry-Merkles sequenced in the
block.

![Block Merkle Diagram](/img/spv-block-merkle.svg)

Together the two merkle proofs show a transaction `T` was included in a block
with bank hash `B`.

An Accounts-Hash is the hash of the concatenation of the state hashes of
each account modified during the current slot.

Transaction status is necessary for the receipt because the state receipt is
constructed for the block. Two transactions over the same state can appear in
the block, and therefore, there is no way to infer from just the state whether
a transaction that is committed to the ledger has succeeded or failed in
modifying the intended state. It may not be necessary to encode the full status
code, but a single status bit to indicate the transaction's success.

Currently, the Block-Merkle is not implemented, so to verify `E` was an entry
in the block with bank hash `B`, we would need to provide all the entry hashes
in the block. Ideally this Block-Merkle would be implemented, as the alternative
is very inefficient.

#### Block Headers

In order to verify transaction inclusion proofs, light clients need to be able
to infer the topology of the forks in the network

More specifically, the light client will need to track incoming block headers
such that given two bank hashes for blocks `A` and `B`, they can determine
whether `A` is an ancestor of `B` (Below section on
`Optimistic Confirmation Proof` explains why!). Contents of header are the
fields necessary to compute the bank hash.

A Bank-Hash is the hash of the concatenation of the Block-Merkle and
Accounts-Hash described in the `Transaction Merkle` section above.

![Bank Hash Diagram](/img/spv-bank-hash.svg)

In the code:

https://github.com/solana-labs/solana/blob/b6bfed64cb159ee67bb6bdbaefc7f833bbed3563/runtime/src/bank.rs#L3468-L3473

```
        let mut hash = hashv(&[
            // bank hash of the parent block
            self.parent_hash.as_ref(),
            // hash of all the modified accounts
            accounts_delta_hash.hash.as_ref(),
            // Number of signatures processed in this block
            &signature_count_buf,
            // Last PoH hash in this block
            self.latest_blockhash().as_ref(),
        ]);
```

A good place to implement this logic along existing streaming logic in the
validator's replay logic: https://github.com/solana-labs/solana/blob/b6bfed64cb159ee67bb6bdbaefc7f833bbed3563/core/src/replay_stage.rs#L1092-L1096

#### Optimistic Confirmation Proof

Currently optimistic confirmation is detected via a listener that monitors
gossip and the replay pipeline for votes:
https://github.com/solana-labs/solana/blob/b6bfed64cb159ee67bb6bdbaefc7f833bbed3563/core/src/cluster_info_vote_listener.rs#L604-L614.

Each vote is a signed transaction that includes the bank hash of the block the
validator voted for, i.e. the `B` from the `Transaction Merkle` section above.
Once a certain threshold `T` of the network has voted on a block, the block is
considered optimistically confirmed. The votes made by this group of `T`
validators is needed to show the block with bank hash `B` was optimistically
confirmed.

However other than some metadata, the signed votes themselves are not
currently stored anywhere, so they can't be retrieved on demand. These votes
probably need to be persisted in Rocksdb database, indexed by a key
`(Slot, Hash, Pubkey)` which represents the slot of the vote, bank hash of the
vote, and vote account pubkey responsible for the vote.

Together, the transaction merkle and optimistic confirmation proofs can be
provided over RPC to subscribers by extending the existing signature
subscription logic. Clients who subscribe to the "Confirmed" confirmation
level are already notified when optimistic confirmation is detected, a flag
can be provided to signal the two proofs above should also be returned.

It is important to note that optimistically confirming `B` also implies that all
ancestor blocks of `B` are also optimistically confirmed, and also that not
all blocks will be optimistically confirmed.

```

B -> B'

```

So in the example above if a block `B'` is optimistically confirmed, then so is
`B`. Thus if a transaction was in block `B`, the transaction merkle in the
proof will be for block `B`, but the votes presented in the proof will be for
block `B'`. This is why the headers in the `Block headers` section above are
important, the client will need to verify that `B` is indeed an ancestor of
`B'`.

#### Proof of Stake Distribution

Once presented with the transaction merkle and optimistic confirmation proofs
above, a client can verify a transaction `T` was optimistically confirmed in a
block with bank hash `B`. The last missing piece is how to verify that the
votes in the optimistic proofs above actually constitute the valid `T`
percentage of the stake necessary to uphold the safety guarantees of
"optimistic confirmation".

One way to approach this might be for every epoch, when the stake set changes,
to write all the stakes to a system account, and then have validators subscribe
to that system account. Full nodes can then provide a merkle proving that the
system account state was updated in some block `B`, and then show that the
block `B` was optimistically confirmed/rooted.

### Account State Verification

An account's state (balance or other data) can be verified by submitting a
transaction with a **_TBD_** Instruction to the cluster. The client can then
use a [Transaction Inclusion Proof](#transaction-inclusion-proof) to verify
whether the cluster agrees that the account has reached the expected state.

### Validator Votes

Leaders should coalesce the validator votes by stake weight into a single entry.
This will reduce the number of entries necessary to create a receipt.

### Chain of Entries

A receipt has a PoH link from the payment or state Merkle Path root to a list
of consecutive validation votes.

It contains the following:

- Transaction -&gt; Entry-Merkle -&gt; Block-Merkle -&gt; Bank-Hash

And a vector of PoH entries:

- Validator vote entries
- Ticks
- Light entries

```text
/// This Entry definition skips over the transactions and only contains the
/// hash of the transactions used to modify PoH.
LightEntry {
    /// The number of hashes since the previous Entry ID.
    pub num_hashes: u64,
    /// The SHA-256 hash `num_hashes` after the previous Entry ID.
    hash: Hash,
    /// The Merkle Root of the transactions encoded into the Entry.
    entry_hash: Hash,
}
```

The light entries are reconstructed from Entries and simply show the entry
Merkle Root that was mixed in to the PoH hash, instead of the full transaction
set.

Clients do not need the starting vote state. The
[fork selection](../implemented-proposals/tower-bft.md) algorithm is defined
such that only votes that appear after the transaction provide finality for the
transaction, and finality is independent of the starting state.

### Verification

A light client that is aware of the supermajority set validators can verify a
receipt by following the Merkle Path to the PoH chain. The Block-Merkle is the
Merkle Root and will appear in votes included in an Entry. The light client can
simulate [fork selection](../implemented-proposals/tower-bft.md) for the
consecutive votes and verify that the receipt is confirmed at the desired
lockout threshold.

### Synthetic State

Synthetic state should be computed into the Bank-Hash along with the bank
generated state.

For example:

- Epoch validator accounts and their stakes and weights.
- Computed fee rates

These values should have an entry in the Bank-Hash. They should live under known
accounts, and therefore have an index into the hash concatenation.

================
File: docs/src/proposals/slashing.md
================
---
title: Slashing rules
---

Unlike Proof of Work \(PoW\) where off-chain capital expenses are already
deployed at the time of block construction/voting, PoS systems require
capital-at-risk to prevent a logical/optimal strategy of multiple chain voting.
We intend to implement slashing rules which, if broken, result some amount of
the offending validator's deposited stake to be removed from circulation. Given
the ordering properties of the PoH data structure, we believe we can simplify
our slashing rules to the level of a voting lockout time assigned per vote.

I.e. Each vote has an associated lockout time \(PoH duration\) that represents
a duration by any additional vote from that validator must be in a PoH that
contains the original vote, or a portion of that validator's stake is
slashable. This duration time is a function of the initial vote PoH count and
all additional vote PoH counts. It will likely take the form:

```text
Lockouti\(PoHi, PoHj\) = PoHj + K \* exp\(\(PoHj - PoHi\) / K\)
```

Where PoHi is the height of the vote that the lockout is to be applied to and
PoHj is the height of the current vote on the same fork. If the validator
submits a vote on a different PoH fork on any PoHk where k &gt; j &gt; i and
PoHk &lt; Lockout\(PoHi, PoHj\), then a portion of that validator's stake is at
risk of being slashed.

In addition to the functional form lockout described above, early
implementation may be a numerical approximation based on a First In, First Out
\(FIFO\) data structure and the following logic:

- FIFO queue holding 32 votes per active validator
- new votes are pushed on top of queue \(`push_front`\)
- expired votes are popped off top \(`pop_front`\)
- as votes are pushed into the queue, the lockout of each queued vote doubles
- votes are removed from back of queue if `queue.len() > 32`
- the earliest and latest height that has been removed from the back of the
  queue should be stored

It is likely that a reward will be offered as a % of the slashed amount to any
node that submits proof of this slashing condition being violated to the PoH.

### Partial Slashing

In the schema described so far, when a validator votes on a given PoH stream,
they are committing themselves to that fork for a time determined by the vote
lockout. An open question is whether validators will be hesitant to begin
voting on an available fork if the penalties are perceived too harsh for an
honest mistake or flipped bit.

One way to address this concern would be a partial slashing design that results
in a slashable amount as a function of either:

1. the fraction of validators, out of the total validator pool, that were also
   slashed during the same time period \(ala Casper\)
2. the amount of time since the vote was cast \(e.g. a linearly increasing % of
   total deposited as slashable amount over time\), or both.

This is an area currently under exploration.

================
File: docs/src/proposals/snapshot-verification.md
================
---
title: Snapshot Verification
---

## Problem

Snapshot verification of the account states is implemented, but the bank hash of the snapshot which is used to verify is falsifiable.

## Solution

While a validator is processing transactions to catch up to the cluster from the snapshot, use incoming vote transactions and the commitment calculator to confirm that the cluster is indeed building on the snapshotted bank hash. Once a threshold commitment level is reached, accept the snapshot as valid and start voting.

================
File: docs/src/proposals/tick-verification.md
================
---
title: Tick Verification
---

This design the criteria and validation of ticks in a slot. It also describes
error handling and slashing conditions encompassing how the system handles
transmissions that do not meet these requirements.

# Slot structure

Each slot must contain an expected `ticks_per_slot` number of ticks. The last
shred in a slot must contain only the entirety of the last tick, and nothing
else. The leader must also mark this shred containing the last tick with the
`LAST_SHRED_IN_SLOT` flag. Between ticks, there must be `hashes_per_tick`
number of hashes.

# Handling bad transmissions

Malicious transmissions `T` are handled in two ways:

1. If a leader can generate some erroneous transmission `T` and also some
   alternate transmission `T'` for the same slot without violating any slashing
   rules for duplicate transmissions (for instance if `T'` is a subset of `T`),
   then the cluster must handle the possibility of both transmissions being live.

Thus this means we cannot mark the erroneous transmission `T` as dead because
the cluster may have reached consensus on `T'`. These cases necessitate a
slashing proof to punish this bad behavior.

2. Otherwise, we can simply mark the slot as dead and not playable. A slashing
   proof may or may not be necessary depending on feasibility.

# Blockstore receiving shreds

When blockstore receives a new shred `s`, there are two cases:

1. `s` is marked as `LAST_SHRED_IN_SLOT`, then check if there exists a shred
   `s'` in blockstore for that slot where `s'.index > s.index` If so, together `s`
   and `s'` constitute a slashing proof.

2. Blockstore has already received a shred `s'` marked as `LAST_SHRED_IN_SLOT`
   with index `i`. If `s.index > i`, then together `s` and `s'`constitute a
   slashing proof. In this case, blockstore will also not insert `s`.

3. Duplicate shreds for the same index are ignored. Non-duplicate shreds for
   the same index are a slashable condition. Details for this case are covered
   in the `Leader Duplicate Block Slashing` section.

# Replaying and validating ticks

1. Replay stage replays entries from blockstore, keeping track of the number of
   ticks it has seen per slot, and verifying there are `hashes_per_tick` number of
   hashes between ticks. After the tick from this last shred has been played,
   replay stage then checks the total number of ticks.

Failure scenario 1: If ever there are two consecutive ticks between which the
number of hashes is `!= hashes_per_tick`, mark this slot as dead.

Failure scenario 2: If the number of ticks != `ticks_per_slot`, mark slot as
dead.

Failure scenario 3: If the number of ticks reaches `ticks_per_slot`, but we still
haven't seen the `LAST_SHRED_IN_SLOT`, mark this slot as dead.

2. When ReplayStage reaches a shred marked as the last shred, it checks if this
   last shred is a tick.

Failure scenario: If the signed shred with the `LAST_SHRED_IN_SLOT` flag cannot
be deserialized into a tick (either fails to deserialize or deserializes into
an entry), mark this slot as dead.

================
File: docs/src/proposals/timely-vote-credits.md
================
---
title: Timely Vote Credits
---

## Timely Vote Credits

This design describes a modification to the method that is used to calculate
vote credits earned by validator votes.

Vote credits are the accounting method used to determine what percentage of
inflation rewards a validator earns on behalf of its stakers.  Currently, when
a slot that a validator has previously voted on is "rooted", it earns 1 vote
credit.  A "rooted" slot is one which has received full commitment by the
validator (i.e. has been finalized).

One problem with this simple accounting method is that it awards one credit
regardless of how "old" the slot that was voted on at the time that it was
voted on.  This means that a validator can delay its voting for many slots in
order to survey forks and wait to make votes that are less likely to be
expired, and without incurring any penalty for doing so.  This is not just a
theoretical concern: there are known and documented instances of validators
using this technique to significantly delay their voting while earning more
credits as a result.


### Proposed Change

The proposal is to award a variable number of vote credits per voted on slot,
with more credits being given for votes that have "less latency" than votes
that have "more latency".

In this context, "latency" is the number of slots in between the slot that is
being voted on and the slot in which the vote has landed.  Because a slot
cannot be voted on until after it has been completed, the minimum possible
latency is 1, which would occur when a validator voted as quickly as possible,
transmitting its vote on that slot in time for it to be included in the very
next slot.

Credits awarded would become a function of this latency, with lower latencies
awarding more credits.  This will discourage intentional "lagging", because
delaying a vote for any slots decreases the number of credits that vote will
earn, because it will necessarily land in a later slot if it is delayed, and
then earn a lower number of credits than it would have earned had it been
transmitted immediately and landed in an earlier slot.

### Grace Period

If landing a vote with 1 slot latency awarded more credit than landing that
same vote in 2 slots latency, then validators who could land votes
consistently within 1 slot would have a credits earning advantage over those
who could not.  Part of the latency when transmitting votes is unavoidable as
it's a function of geographical distance between the sender and receiver of
the vote.  The Solana network is spread around the world but it is not evenly
distributed over the whole planet; there are some locations which are, on
average, more distant from the network than others are.

It would likely be harmful to the network to encourage tight geographical
concentration - if, for example, the only way to achieve 1 slot latency was to
be within a specific country - then a very strict credit rewards schedule
would encourage all validators to move to the same country in order to
maximize their credit earnings.

For this reason, the credits reward schedule should have a built-in "grace
period" that gives all validators a "reasonable" amount of time to land their
votes.  This will reduce the credits earning disadvantage that comes from
being more distant from the network.  A balance needs to be struck between the
strictest rewards schedule, which most strongly discourages intentional
lagging, and more lenient rewards schedules, which improves credit earnings
for distant validators who are not artificially lagging.

Historical voting data has been analyzed over many epochs and the data
suggests that the smallest grace period that allows for very minimal impact on
well behaved distant validators is 3 slots, which means that all slots voted
on within 3 slots will award maximum vote credits to the voting validator.
This gives validators nearly 2 seconds to land their votes without penalty.
The maximum latency between two points on Earth is about 100 ms, so allowing a
full 1,500 ms to 2,000 ms latency without penalty should not have adverse
impact on distant validators.

### Maximum Vote Credits

Another factor to consider is what the maximum vote credits to award for a
vote should be.  Assuming linear reduction in vote credits awarded (where 1
slot of additional lag reduces earned vote credits by 1), the maximum vote
credits value determines how much "penalty" there is for each additional slot
of latency.  For example, a value of 10 would mean that after the grace period
slots, every additional slot of latency would result in a 10% reduction in
vote credits earned as each subsequent slot earns 1 credit less out of a
maximum possible 10 credits.

Again, historical voting data was analyzed over many epochs and the conclusion
drawn was that a maximum credits of 10 is the largest value that can be used
and still have a noticeable effect on known laggers.  Values higher than that
result in such a small penalty for each slot of lagging that intentional
lagging is still too profitable.  Lower values are even more punishing to
intentional lagging; but an attempt has been made to conservatively choose the
highest value that produces noticeable results.

The selection of these values is partially documented here:

https://www.shinobi-systems.com/timely_voting_proposal

The above document is somewhat out of date with more recent analysis, which
occurred in this github issue:

https://github.com/solana-labs/solana/issues/19002

To summarize the findings of these documents: analysis over many epochs showed
that almost all validators from all regions have an average vote latency of 1
slot or less.  The validators with higher average latency are either known
laggers, or are not representative of their region since many other validators
in the same region achieve low latency voting.  With a maximum vote credit of
10, there is almost no change in vote credits earned relative to the highest vote
earner by the majority of validators, aside from a general uplift of about 0.4%.
Additionally, data centers were analyzed to ensure that there aren't regions of
the world that would be adversely affected, and none were found.


### Method of Implementation

When a Vote or VoteStateUpdate instruction is received by a validator, it will
use the Clock sysvar to identify the slot in which that instruction has
landed.  For any newly voted on slot within that Vote or VoteStateUpdate
transaction, the validator will record the vote latency of that slot as
(voted_in_slot - voted_on_slot).

These vote latencies will be stored a new vector of u8 latency values appended
to the end of the VoteState.  VoteState currently has ~200 bytes of free space
at the end that is unused, so this new vector of u8 values should easily fit
within this available space.  Because VoteState is an ABI frozen structure,
utilizing the mechanisms for updating frozen ABI will be required, which will
complicate the change.  Furthermore, because VoteState is embedded in the
Tower data structure and it is frozen ABI as well, updates to the frozen ABI
mechanisms for Tower will be needed also.  These are almost entirely
mechanical changes though, that involve ensuring that older versions of these
data structures can be updated to the new version as they are read in, and the
new version written out when the data structure is next persisted.

The credits to award for a rooted slot will be calculated using the latency
value stored in latency vector for the slot, and a formula that awards
latencies of 1 - 3 slots ten credits, with a 1 credit reduction for each vote
latency after 3.  Rooted slots will always be awarded a minimum credit of 1
(never 0) so that very old votes, possibly necessary in times of network
stress, are not discouraged.

To summarize the above: latency is recorded in a new Vector at the end of
VoteState when a vote first lands, but the credits for that slot are not
awarded until the slot becomes rooted, at which point the latency that was
recorded is used to compute the credits to award for that newly rooted slot.

When a Vote instruction is processed, the changes are fairly easy to implement
as Vote can only add new slots to the end of Lockouts and pop existing slots
off of the back (which become rooted), so the logic merely has to compute
rewards for the new roots, and new latencies for the newly added slots, both
of which can be processed in the fairly simple existing logic for Vote
processing.

When a VoteStateUpdate instruction is processed:

1. For each slot that was in the previous VoteState but are not in the new
VoteState because they have been rooted in the transition from the old
VoteState to the new VoteState, credits to award are calculated based on the
latency that was recorded for them and still available in the old VoteState.

2. For each slot that was in both the previous VoteState and the new
VoteState, the latency that was previously recorded for that slot is copied
from the old VoteState to the new VoteState.

3. For each slot that is in the new VoteState but wasn't in the old VoteState,
the latency value is calculated for this new slot according to what slot the
vote is for and what slot is in the Clock (i.e. the slot this VoteStateUpdate
tx landed in) and this latency is stored in VoteState for that slot.

The code to handle this is more complex, because VoteStateUpdate may include
removal of slots that expired as performed by the voting validator, in
addition to slots that have been rooted and new slots added.  However, the
assumptions that are needed to handle VoteStateUpdate with timely vote credits
are already guaranteed by existing VoteStateUpdate correctness checking code:

The existing VoteStateUpdate processing code already ensures that (1) only
roots slots that could actually have been rooted in the transition from the
old VoteState to the new VoteState, so there is no danger of over-counting
credits (i.e. imagine that a 'cheating' validator "pretended" that slots were
rooted by dropping them off of the back of the new VoteState before they have
actually achieved 32 confirmations; the existing logic prevents this).

The existing VoteStateUpdate processing code already ensures that (2) new
slots included in the new VoteState are only slots after slots that have
already been voted on in the old VoteState (i.e. can't inject new slots in the
middle of slots already voted on).

================
File: docs/src/proposals/validator-proposal.md
================
---
title: Validator
---

## History

When we first started Solana, the goal was to de-risk our TPS claims. We knew
that between optimistic concurrency control and sufficiently long leader slots,
that PoS consensus was not the biggest risk to TPS. It was GPU-based signature
verification, software pipelining and concurrent banking. Thus, the TPU was
born. After topping 100k TPS, we split the team into one group working toward
710k TPS and another to flesh out the validator pipeline. Hence, the TVU was
born. The current architecture is a consequence of incremental development with
that ordering and project priorities. It is not a reflection of what we ever
believed was the most technically elegant cross-section of those technologies.
In the context of leader rotation, the strong distinction between leading and
validating is blurred.

## Difference between validating and leading

The fundamental difference between the pipelines is when the PoH is present. In
a leader, we process transactions, removing bad ones, and then tag the result
with a PoH hash. In the validator, we verify that hash, peel it off, and
process the transactions in exactly the same way. The only difference is that
if a validator sees a bad transaction, it can't simply remove it like the
leader does, because that would cause the PoH hash to change. Instead, it
rejects the whole block. The other difference between the pipelines is what
happens _after_ banking. The leader broadcasts entries to downstream validators
whereas the validator will have already done that in RetransmitStage, which is
a confirmation time optimization. The validation pipeline, on the other hand,
has one last step. Any time it finishes processing a block, it needs to weigh
any forks it's observing, possibly cast a vote, and if so, reset its PoH hash
to the block hash it just voted on.

## Proposed Design

We unwrap the many abstraction layers and build a single pipeline that can
toggle leader mode on whenever the validator's ID shows up in the leader
schedule.

![Validator block diagram](/img/validator-proposal.svg)

## Notable changes

- Hoist FetchStage and BroadcastStage out of TPU
- BankForks renamed to Banktree
- TPU moves to new socket-free crate called solana-tpu.
- TPU's BankingStage absorbs ReplayStage
- TVU goes away
- New RepairStage absorbs Shred Fetch Stage and repair requests
- JSON RPC Service is optional - used for debugging. It should instead be part
  of a separate `solana-blockstreamer` executable.
- New MulticastStage absorbs retransmit part of RetransmitStage
- MulticastStage downstream of Blockstore

================
File: docs/src/proposals/versioned-transactions.md
================
# Versioned Transactions - v0: Address Lookup Tables

## Problem

Messages transmitted to Solana validators must not exceed the IPv6 MTU size to
ensure fast and reliable network transmission of cluster info over UDP.
Solana's networking stack uses a conservative MTU size of 1280 bytes which,
after accounting for headers, leaves 1232 bytes for packet data like serialized
transactions.

Developers building applications on Solana must design their on-chain program
interfaces within the above transaction size limit constraint. One common
work-around is to store state temporarily on-chain and consume that state in
later transactions. This is the approach used by the BPF loader program for
deploying Solana programs.

However, this workaround doesn't work well when developers compose many on-chain
programs in a single atomic transaction. With more composition comes more
account inputs, each of which takes up 32 bytes. There is currently no available
workaround for increasing the number of accounts used in a single transaction
since each transaction must list all accounts that it needs to properly lock
accounts for parallel execution. Therefore the current cap is about 35 accounts
after accounting for signatures and other transaction metadata.

## Proposed Solution

1. Introduce a new program which manages on-chain address lookup tables
2. Add a new transaction format which can make use of on-chain
   address lookup tables to efficiently load more accounts in a single transaction.

### Address Lookup Table Program

Here we describe a program-based solution to the problem, whereby a protocol
developer or end-user can create collections of related addresses on-chain for
concise use in a transaction's account inputs.

After addresses are stored on-chain in an address lookup table account, they may be
succinctly referenced in a transaction using a 1-byte u8 index rather than a
full 32-byte address. This will require a new transaction format to make use of
these succinct references as well as runtime handling for looking up and loading
addresses from the on-chain lookup tables.

#### State

Address lookup tables must be rent-exempt when initialized and after
each time new addresses are appended. Lookup tables can either be extended
from an on-chain buffered list of addresses or directly by appending
addresses through instruction data. Newly appended addresses require
one slot to warmup before being available to transactions for lookups.

Since transactions use a `u8` index to look up addresses, address tables can
store up to 256 addresses each. In addition to stored addresses, address table
accounts also tracks various metadata explained below.

```rust
/// The maximum number of addresses that a lookup table can hold
pub const LOOKUP_TABLE_MAX_ADDRESSES: usize = 256;

/// The serialized size of lookup table metadata
pub const LOOKUP_TABLE_META_SIZE: usize = 56;

pub struct LookupTableMeta {
    /// Lookup tables cannot be closed until the deactivation slot is
    /// no longer "recent" (not accessible in the `SlotHashes` sysvar).
    pub deactivation_slot: Slot,
    /// The slot that the table was last extended. Address tables may
    /// only be used to lookup addresses that were extended before
    /// the current bank's slot.
    pub last_extended_slot: Slot,
    /// The start index where the table was last extended from during
    /// the `last_extended_slot`.
    pub last_extended_slot_start_index: u8,
    /// Authority address which must sign for each modification.
    pub authority: Option<Pubkey>,
    // Raw list of addresses follows this serialized structure in
    // the account's data, starting from `LOOKUP_TABLE_META_SIZE`.
}
```

#### Cleanup

Once an address lookup table is no longer needed, it can be deactivated and closed
to have its rent balance reclaimed. Address lookup tables may not be recreated
at the same address because each new lookup table must be initialized at an address
derived from a recent slot.

Address lookup tables can be deactivated at any time but can continue to be used
by transactions until the deactivation slot is no longer present in the slot hashes
sysvar. This cool-down period ensures that in-flight transactions cannot be
censored and that address lookup tables cannot be closed and recreated for the same
slot.

### Versioned Transactions

In order to support address table lookups, the structure of serialized
transactions must be modified. The new transaction format should not
affect transaction processing in the Solana program runtime beyond the
increased capacity for accounts and program invocations. Invoked
programs will be unaware of which transaction format was used.

The new transaction format must be distinguished from the current transaction
format. Current transactions can fit at most 19 signatures (64-bytes each) but
the message header encodes `num_required_signatures` as a `u8`. Since the upper
bit of the `u8` will never be set for a valid transaction, we can enable it to
denote whether a transaction should be decoded with the versioned format or not.

The original transaction format will be referred to as the legacy transaction
version and the first versioned transaction format will start at version 0.

#### New Transaction Format

```rust
#[derive(Serialize, Deserialize)]
pub struct VersionedTransaction {
    /// List of signatures
    #[serde(with = "short_vec")]
    pub signatures: Vec<Signature>,
    /// Message to sign.
    pub message: VersionedMessage,
}

// Uses custom serialization. If the first bit is set, the remaining bits
// in the first byte will encode a version number. If the first bit is not
// set, the first byte will be treated as the first byte of an encoded
// legacy message.
pub enum VersionedMessage {
    Legacy(LegacyMessage),
    V0(v0::Message),
}

// The structure of the new v0 Message
#[derive(Serialize, Deserialize)]
pub struct Message {
  // unchanged
  pub header: MessageHeader,

  // unchanged
  #[serde(with = "short_vec")]
  pub account_keys: Vec<Pubkey>,

  // unchanged
  pub recent_blockhash: Hash,

  // unchanged
  //
  // # Notes
  //
  // Account and program indexes will index into the list of addresses
  // constructed from the concatenation of three key lists:
  //   1) message `account_keys`
  //   2) ordered list of keys loaded from address table `writable_indexes`
  //   3) ordered list of keys loaded from address table `readonly_indexes`
  #[serde(with = "short_vec")]
  pub instructions: Vec<CompiledInstruction>,

  /// List of address table lookups used to load additional accounts
  /// for this transaction.
  #[serde(with = "short_vec")]
  pub address_table_lookups: Vec<MessageAddressTableLookup>,
}

/// Address table lookups describe an on-chain address lookup table to use
/// for loading more readonly and writable accounts in a single tx.
#[derive(Serialize, Deserialize)]
pub struct MessageAddressTableLookup {
  /// Address lookup table account key
  pub account_key: Pubkey,
  /// List of indexes used to load writable account addresses
  #[serde(with = "short_vec")]
  pub writable_indexes: Vec<u8>,
  /// List of indexes used to load readonly account addresses
  #[serde(with = "short_vec")]
  pub readonly_indexes: Vec<u8>,
}
```

#### Size changes

- 1 extra byte for the `version` field
- 1 extra byte for the `address_table_lookups` length
- 34 extra bytes for each address table lookup which includes 32 bytes for the
  address of the table and a byte each for the lengths of the `writable_indexes`
  and `readonly_indexes` fields
- 1 extra byte for each lookup table index

#### Cost

Address lookups require extra computational overhead during transaction
processing, but also reduce network bandwidth due to smaller transactions and
therefore smaller blocks.

#### Metadata changes

Each resolved address from an address lookup table should be stored in
the transaction metadata for quick reference. This will avoid the need for
clients to make multiple RPC round trips to fetch all accounts loaded by a
v0 transaction. It will also make it easier to use the ledger tool to
analyze account access patterns.

#### RPC changes

Fetched transaction responses will likely require a new version field to
indicate to clients which transaction structure to use for deserialization.
Clients using pre-existing RPC methods will receive error responses when
attempting to fetch a versioned transaction which will indicate that they
must upgrade.

The RPC API should also support an option for returning fully expanded
transactions to abstract away the address lookup table details from
downstream clients.

### Limitations

- Max of 256 unique accounts may be loaded by a transaction because `u8`
  is used by compiled instructions to index into transaction message `account_keys`.
- Address lookup tables can hold up to 256 entries because lookup table indexes are also `u8`.
- Transaction signers may not be loaded through an address lookup table, the full
  address of each signer must be serialized in the transaction. This ensures that
  the performance of transaction signature checks is not affected.
- Hardware wallets will not be able to display details about accounts referenced
  through address lookup tables due to inability to verify on-chain data.
- Only single level address lookup tables can be used. Recursive lookups will not be supported.

## Security Concerns

### Lookup table re-initialization

If an address lookup table can be closed and re-initialized with new addresses,
any client which is unaware of the change could inadvertently lookup unexpected
addresses. To avoid this, all address lookup tables must be initialized at an
address derived from a recent slot and they cannot be closed until the slot
used for deactivation is no longer in the slot hashes sysvar.

### Resource consumption

Enabling more account inputs in a transaction allows for more program
invocations, write-locks, and data reads / writes. Before address tables are
enabled, transaction-wide compute limits and increased costs for write locks and
data reads are required.

### Front running

If the addresses listed within an address lookup table are mutable, front
running attacks could modify which addresses are resolved for a later
transaction. For this reason, address lookup tables are append-only and may
only be closed if it's no longer possible to create a new lookup table at the
same derived address.

Additionally, a malicious actor could try to fork the chain immediately after a
new address lookup table account is added to a block. If successful, they could
add a different unexpected table entry in the fork. In order to deter this attack,
clients should wait for address lookup tables to be finalized before using them in a
transaction. Clients may also append integrity check instructions to the
transaction which verify that the correct accounts are looked up.

### Denial of service

Address lookup table accounts may be read very frequently and will therefore
be a more high profile target for denial of service attacks through write locks
similar to sysvar accounts.

For this reason, special handling should be given to address lookup tables.
When an address lookup table is used to lookup addresses for a transaction,
it can be loaded without waiting for a read lock. To avoid race conditions,
only the addresses appended in previous blocks can be used for lookups and
deactivation requires a cool-down period.

### Duplicate accounts

Transactions may not load an account more than once whether directly through
`account_keys` or indirectly through `address_table_lookups`.

## Other Proposals

1. Account prefixes

Needing to pre-register accounts in an on-chain address lookup table is cumbersome
because it adds an extra step for transaction processing. Instead, Solana
transactions could use variable length address prefixes to specify accounts.
These prefix shortcuts can save on data usage without needing to setup on-chain
state.

However, this model requires nodes to keep a mapping of prefixes to active account
addresses. Attackers can create accounts with the same prefix as a popular account
to disrupt transactions.

2. Transaction builder program

Solana can provide a new on-chain program which allows "Big" transactions to be
constructed on-chain by normal transactions. Once the transaction is
constructed, a final "Execute" transaction can trigger a node to process the big
transaction as a normal transaction without needing to fit it into an MTU sized
packet.

The UX of this approach is tricky. A user could in theory sign a big transaction
but it wouldn't be great if they had to use their wallet to sign multiple
transactions to build that transaction that they already signed and approved. This
could be a use-case for transaction relay services, though. A user could pay a
relayer to construct the large pre-signed transaction on-chain for them.

In order to prevent the large transaction from being reconstructed and replayed,
its message hash will need to be added to the status cache when executed.

3. Epoch account indexes

Similarly to leader schedule calculation, validators could create a global index
of the most accessed accounts in the previous epoch and make that index
available to transactions in the following epoch.

This approach has a downside of only updating the index at epoch boundaries
which means there would be a few day delay before popular new accounts could be
referenced. It also needs to be consistently generated by all validators by
using some criteria like adding accounts in order by access count.

4. Address lists

Extend the transaction structure to support addresses that, when loaded, expand
to a list of addresses. After expansion, all account inputs are concatenated to
form a single list of account keys which can be indexed into by instructions.
Address lists would likely need to be immutable to prevent attacks. They would
also need to be limited in length to limit resource consumption.

This proposal can be thought of a special case of the proposed index account
approach. Since the full account list would be expanded, there's no need to add
additional offsets that use up the limited space in a serialized transaction.
However, the expected size of an address list may need to be encoded into the
transaction to aid the sanitization of account indexes. We would also need to
encode how many addresses in the list should be loaded as readonly vs
read-write. Lastly, special attention must be given to watch out for addresses
that exist in multiple account lists.

5. Increase transaction size

Significantly larger serialized transactions have an increased likelihood of being
dropped over the wire but this might not be a big issue since clients can retry
transactions anyways. The only time validators need to send individual transactions
over the network is when a leader forwards unprocessed transactions to the next
leader.

================
File: docs/src/proposals/vote-signing-to-implement.md
================
---
title: Secure Vote Signing
---

## Secure Vote Signing

This design describes additional vote signing behavior that will make the process more secure.

Currently, Solana implements a vote-signing service that evaluates each vote to ensure it does not violate a slashing condition. The service could potentially have different variations, depending on the hardware platform capabilities. In particular, it could be used in conjunction with a secure enclave \(such as SGX\). The enclave could generate an asymmetric key, exposing an API for user \(untrusted\) code to sign the vote transactions, while keeping the vote-signing private key in its protected memory.

The following sections outline how this architecture would work:

### Message Flow

1. The node initializes the enclave at startup

   - The enclave generates an asymmetric key and returns the public key to the

     node

   - The keypair is ephemeral. A new keypair is generated on node bootup. A

     new keypair might also be generated at runtime based on some to be determined

     criteria.

   - The enclave returns its attestation report to the node

2. The node performs attestation of the enclave \(e.g using Intel's IAS APIs\)

   - The node ensures that the Secure Enclave is running on a TPM and is

     signed by a trusted party

3. The stakeholder of the node grants ephemeral key permission to use its stake.

   This process is to be determined.

4. The node's untrusted, non-enclave software calls trusted enclave software

   using its interface to sign transactions and other data.

   - In case of vote signing, the node needs to verify the PoH. The PoH

     verification is an integral part of signing. The enclave would be

     presented with some verifiable data to check before signing the vote.

   - The process of generating the verifiable data in untrusted space is to be determined

### PoH Verification

1. When the node votes on an en entry `X`, there's a lockout period `N`, for

   which it cannot vote on a fork that does not contain `X` in its history.

2. Every time the node votes on the derivative of `X`, say `X+y`, the lockout

   period for `X` increases by a factor `F` \(i.e. the duration node cannot vote on

   a fork that does not contain `X` increases\).

   - The lockout period for `X+y` is still `N` until the node votes again.

3. The lockout period increment is capped \(e.g. factor `F` applies maximum 32

   times\).

4. The signing enclave must not sign a vote that violates this policy. This

   means

   - Enclave is initialized with `N`, `F` and `Factor cap`
   - Enclave stores `Factor cap` number of entry IDs on which the node had

     previously voted

   - The sign request contains the entry ID for the new vote
   - Enclave verifies that new vote's entry ID is on the correct fork

     \(following the rules \#1 and \#2 above\)

### Ancestor Verification

This is alternate, albeit, less certain approach to verifying voting fork. 1. The validator maintains an active set of nodes in the cluster 2. It observes the votes from the active set in the last voting period 3. It stores the ancestor/last_tick at which each node voted 4. It sends new vote request to vote-signing service

- It includes previous votes from nodes in the active set, and their

  corresponding ancestors

  1. The signer checks if the previous votes contains a vote from the validator,

     and the vote ancestor matches with majority of the nodes

- It signs the new vote if the check is successful
- It asserts \(raises an alarm of some sort\) if the check is unsuccessful

The premise is that the validator can be spoofed at most once to vote on incorrect data. If someone hijacks the validator and submits a vote request for bogus data, that vote will not be included in the PoH \(as it'll be rejected by the cluster\). The next time the validator sends a request to sign the vote, the signing service will detect that validator's last vote is missing \(as part of

## 5 above\).

### Fork determination

Due to the fact that the enclave cannot process PoH, it has no direct knowledge of fork history of a submitted validator vote. Each enclave should be initiated with the current _active set_ of public keys. A validator should submit its current vote along with the votes of the active set \(including itself\) that it observed in the slot of its previous vote. In this way, the enclave can surmise the votes accompanying the validator's previous vote and thus the fork being voted on. This is not possible for the validator's initial submitted vote, as it will not have a 'previous' slot to reference. To account for this, a short voting freeze should apply until the second vote is submitted containing the votes within the active set, along with it's own vote, at the height of the initial vote.

### Enclave configuration

A staking client should be configurable to prevent voting on inactive forks. This mechanism should use the client's known active set `N_active` along with a threshold vote `N_vote` and a threshold depth `N_depth` to determine whether or not to continue voting on a submitted fork. This configuration should take the form of a rule such that the client will only vote on a fork if it observes more than `N_vote` at `N_depth`. Practically, this represents the client from confirming that it has observed some probability of economic finality of the submitted fork at a depth where an additional vote would create a lockout for an undesirable amount of time if that fork turns out not to be live.

### Challenges

1. Generation of verifiable data in untrusted space for PoH verification in the

   enclave.

2. Need infrastructure for granting stake to an ephemeral key.

================
File: docs/src/runtime/programs.md
================
---
title: "Native Programs in the Solana Runtime"
pagination_label: Runtime Native Programs
sidebar_label: Native Programs
---

[Redirect](https://solana.com/docs/core/accounts)

================
File: docs/src/runtime/sysvars.md
================
---
title: Solana Sysvar Cluster Data
pagination_label: Runtime Sysvar Cluster Data
sidebar_label: Sysvar Cluster Data
---

Solana exposes a variety of cluster state data to programs via
[`sysvar`](https://solana.com/docs/terminology#sysvar) accounts. These accounts
are populated at known addresses published along with the account layouts in the
[`solana-program` crate](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/sysvar/index.html),
and outlined below.

There are two ways for a program to access a sysvar.

The first is to query the sysvar at runtime via the sysvar's `get()` function:

```
let clock = Clock::get()
```

The following sysvars support `get`:

- Clock
- EpochSchedule
- Fees
- Rent
- EpochRewards

The second is to pass the sysvar to the program as an account by including its
address as one of the accounts in the `Instruction` and then deserializing the
data during execution. Access to sysvars accounts is always _readonly_.

```
let clock_sysvar_info = next_account_info(account_info_iter)?;
let clock = Clock::from_account_info(&clock_sysvar_info)?;
```

The first method is more efficient and does not require that the sysvar account
be passed to the program, or specified in the `Instruction` the program is
processing.

## Clock

The Clock sysvar contains data on cluster time, including the current slot,
epoch, and estimated wall-clock Unix timestamp. It is updated every slot.

- Address: `SysvarC1ock11111111111111111111111111111111`
- Layout:
  [Clock](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/clock/struct.Clock.html)
- Fields:

  - `slot`: the current slot
  - `epoch_start_timestamp`: the Unix timestamp of the first slot in this epoch.
    In the first slot of an epoch, this timestamp is identical to the
    `unix_timestamp` (below).
  - `epoch`: the current epoch
  - `leader_schedule_epoch`: the most recent epoch for which the leader schedule
    has already been generated
  - `unix_timestamp`: the Unix timestamp of this slot.

  Each slot has an estimated duration based on Proof of History. But in reality,
  slots may elapse faster and slower than this estimate. As a result, the Unix
  timestamp of a slot is generated based on oracle input from voting validators.
  This timestamp is calculated as the stake-weighted median of timestamp
  estimates provided by votes, bounded by the expected time elapsed since the
  start of the epoch.

  More explicitly: for each slot, the most recent vote timestamp provided by
  each validator is used to generate a timestamp estimate for the current slot
  (the elapsed slots since the vote timestamp are assumed to be
  Bank::ns_per_slot). Each timestamp estimate is associated with the stake
  delegated to that vote account to create a distribution of timestamps by
  stake. The median timestamp is used as the `unix_timestamp`, unless the
  elapsed time since the `epoch_start_timestamp` has deviated from the expected
  elapsed time by more than 25%.

## EpochSchedule

The EpochSchedule sysvar contains epoch scheduling constants that are set in
genesis, and enables calculating the number of slots in a given epoch, the epoch
for a given slot, etc. (Note: the epoch schedule is distinct from the
[`leader schedule`](https://solana.com/docs/terminology#leader-schedule))

- Address: `SysvarEpochSchedu1e111111111111111111111111`
- Layout:
  [EpochSchedule](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/epoch_schedule/struct.EpochSchedule.html)

## Fees

The Fees sysvar contains the fee calculator for the current slot. It is updated
every slot, based on the fee-rate governor.

- Address: `SysvarFees111111111111111111111111111111111`
- Layout:
  [Fees](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/sysvar/fees/struct.Fees.html)

## Instructions

The Instructions sysvar contains the serialized instructions in a Message while
that Message is being processed. This allows program instructions to reference
other instructions in the same transaction. Read more information on
[instruction introspection](implemented-proposals/instruction_introspection.md).

- Address: `Sysvar1nstructions1111111111111111111111111`
- Layout:
  [Instructions](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/sysvar/instructions/struct.Instructions.html)

## RecentBlockhashes

The RecentBlockhashes sysvar contains the active recent blockhashes as well as
their associated fee calculators. It is updated every slot. Entries are ordered
by descending block height, so the first entry holds the most recent block hash,
and the last entry holds an old block hash.

- Address: `SysvarRecentB1ockHashes11111111111111111111`
- Layout:
  [RecentBlockhashes](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/sysvar/recent_blockhashes/struct.RecentBlockhashes.html)

## Rent

The Rent sysvar contains the rental rate. Currently, the rate is static and set
in genesis. The Rent burn percentage is modified by manual feature activation.

- Address: `SysvarRent111111111111111111111111111111111`
- Layout:
  [Rent](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/rent/struct.Rent.html)

## SlotHashes

The SlotHashes sysvar contains the most recent hashes of the slot's parent
banks. It is updated every slot.

- Address: `SysvarS1otHashes111111111111111111111111111`
- Layout:
  [SlotHashes](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/slot_hashes/struct.SlotHashes.html)

## SlotHistory

The SlotHistory sysvar contains a bitvector of slots present over the last
epoch. It is updated every slot.

- Address: `SysvarS1otHistory11111111111111111111111111`
- Layout:
  [SlotHistory](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/slot_history/struct.SlotHistory.html)

## StakeHistory

The StakeHistory sysvar contains the history of cluster-wide stake activations
and de-activations per epoch. It is updated at the start of every epoch.

- Address: `SysvarStakeHistory1111111111111111111111111`
- Layout:
  [StakeHistory](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/stake_history/struct.StakeHistory.html)

## EpochRewards

The EpochRewards sysvar tracks whether the rewards period (including calculation
and distribution) is in progress, as well as the details needed to resume
distribution when starting from a snapshot during the rewards period. The sysvar
is repopulated at the start of the first block of each epoch.


- Address: `SysvarEpochRewards1111111111111111111111111`
- Layout:
  [EpochRewards](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/epoch_rewards/struct.EpochRewards.html)
- Fields:

  - `distribution_starting_block_height` - starting block height for distribution for the current epoch
  - `num_partitions` - the number of partitions in the distribution
  - `parent_blockhash` - the blockhash seed used to generate the partition hasher, ie. the blockhash of the parent of the first block in the epoch
  - `total_points` - the total rewards points calculated for the epoch
  - `total_rewards` - total rewards for epoch, in lamports
  - `distributed_rewards` - rewards for the epoch distributed so far, in lamports
  - `active` - whether the rewards period is currently active

## LastRestartSlot

The LastRestartSlot sysvar contains the slot number of the last restart or _0_
(zero) if none ever happened.

- Address: `SysvarLastRestartS1ot1111111111111111111111`
- Layout:
  [LastRestartSlot](https://docs.rs/solana-program/VERSION_FOR_DOCS_RS/solana_program/last_restart_slot/struct.LastRestartSlot.html)

================
File: docs/src/runtime/zk-elgamal-proof.md
================
---
title: Solana ZK ElGamal Proof Program
pagination_label: Native ZK ElGamal Proof Program
sidebar_label: ZK ElGamal Proof Program
---
The native Solana ZK ElGamal Proof program verifies a number of zero-knowledge
proofs that are tailored to work with Pedersen commitments and ElGamal
encryption over the elliptic curve
[curve25519](https://www.rfc-editor.org/rfc/rfc7748#section-4.1). The proof
verification instructions in the ZK ElGamal Proof program are flexibly designed
so that they can be combined to enable a number different applications.

- Program id: `ZkE1Gama1Proof11111111111111111111111111111`
- Instructions:
  [ProofInstruction](https://github.com/anza-xyz/agave/blob/master/zk-sdk/src/zk_elgamal_proof_program/instruction.rs)

### Pedersen commitments and ElGamal encryption

The ZK ElGamal Proof program verifies zero-knowledge proofs for Pedersen
commitments and ElGamal encryption, which are common cryptographic primitives
that are incorporated in many existing cryptographic protocols.

ElGamal encryption is a popular instantiation of a public-key encryption scheme.
An ElGamal keypair consists of an ElGamal public key and an ElGamal secret key.
Messages can be encrypted under a public key to produce a ciphertext. A
ciphertext can then be decrypted using a corresponding ElGamal secret key. The
variant that is used in the proof program is the
[twisted ElGamal encryption](https://eprint.iacr.org/2019/319) over the elliptic
curve [curve25519](https://www.rfc-editor.org/rfc/rfc7748#section-4.1).

The Pedersen commitment scheme is a popular instantiation of a cryptographic
commitment scheme. A commitment scheme allows a user to wrap a message into a
commitment with a purpose of revealing the committed message later on. Like a
ciphertext, the resulting commitment does not reveal any information about the
containing message. At the same time, the commitment is binding in that the user
cannot change the original value that is contained in a commitment.

Interested readers can refer to the following resources for a more in-depth
treatment of Pedersen commitment and the (twisted) ElGamal encryption schemes.

- [Notes](https://github.com/solana-labs/solana/blob/master/docs/src/runtime/zk-docs/twisted_elgamal.pdf)
  on the twisted ElGamal encryption
- A technical
  [overview](https://github.com/solana-program/token-2022/blob/main/zk-token-protocol-paper/part1.pdf)
  of the SPL Token 2022 confidential extension
- Pretty Good Confidentiality [research paper](https://eprint.iacr.org/2019/319)

The ZK ElGamal Proof program contains proof verification instructions on various
zero-knowledge proofs for working with the Pedersen commitment and ElGamal
encryption schemes. For example, the `VerifyBatchedRangeProofU64` instruction
verifies a zero-knowledge proof certifying that a Pedersen commitment contains
an unsigned 64-bit number as the message. The `VerifyPubkeyValidity` instruction
verifies a zero-knowledge proof certifying that an ElGamal public key is a
properly formed public key.

### Context Data

The proof data associated with each of the ZK ElGamal Proof instructions are
logically divided into two parts:

- The <em>context</em> component contains the data that a zero-knowledge proof
  is certifying. For example, context component for a
  `VerifyBatchedRangeProofU64` instruction data is the Pedersen commitment that
  holds an unsigned 64-bit number. The context component for a
  `VerifyPubkeyValidity` instruction data is the ElGamal public key that is
  properly formed.
- The <em>proof</em> component contains the actual mathematical pieces that
  certify different properties of the context data.

The ZK Token proof program processes a proof instruction in two steps:

1. Verify the zero-knowledge proof data associated with the proof instruction.
2. If specified in the instruction, the program stores the context data in a
   dedicated context state account.

The simplest way to use a proof instruction is to execute it without producing a
context state account. In this case, the proof instruction can be included as
part of a larger Solana transaction that contains instructions of other Solana
programs. Programs should directly access the context data from the proof
instruction data and use it in its program logic.

Alternatively, a proof instruction can be executed to produce a context state
account. In this case, the context data associated with a proof instruction
persists even after the transaction containing the proof instruction is finished
with its execution. The creation of context state accounts can be useful in
settings where ZK proofs are required from PDAs or when proof data is too large
to fit inside a single transaction.

## Proof Instructions

The ZK ElGamal Proof program supports the following list of zero-knowledge
proofs.

#### Proofs on ElGamal encryption

- `VerifyPubkeyValidity`:

  - The ElGamal public-key validity proof instruction certifies that an ElGamal
    public-key is a properly formed public key.
  - Mathematical description and proof of security:
    [[Notes]](https://github.com/anza-xyz/agave/blob/master/docs/src/runtime/zk-docs/pubkey_proof.pdf)

- `VerifyZeroCiphertext`:

  - The zero-ciphertext proof certifies that an ElGamal ciphertext encrypts the
    number zero.
  - Mathematical description and proof of security:
    [[Notes]](https://github.com/anza-xyz/agave/blob/master/docs/src/runtime/zk-docs/zero_proof.pdf)

#### Equality proofs

- `VerifyCiphertextCommitmentEquality`:

  - The ciphertext-commitment equality proof certifies that an ElGamal
    ciphertext and a Pedersen commitment encode the same message.
  - Mathematical description and proof of security:
    [[Notes]](https://github.com/anza-xyz/agave/blob/master/docs/src/runtime/zk-docs/ciphertext_commitment_equality.pdf)

- `VerifyCiphertextCiphertextEquality`:

  - The ciphertext-ciphertext equality proof certifies that two ElGamal
    ciphertexts encrypt the same message.
  - Mathematical description and proof of security:
    [[Notes]](https://github.com/anza-xyz/agave/blob/master/docs/src/runtime/zk-docs/ciphertext_ciphertext_equality.pdf)

#### Ciphertext Validity proofs

- `VerifyGroupedCiphertextValidity`:

  - The grouped ciphertext validity proof certifies that a grouped ElGamal
    cipehrtext is well-formed
    - Mathematical description and proof of security:
      [[Notes]](https://github.com/anza-xyz/agave/blob/master/docs/src/runtime/zk-docs/ciphertext_validity.pdf)

#### Percentage with Cap proof

- `PercentageWithCap`:

  - The percentage with cap proof certifies that percentage relation useful
    for fee calcluations
    - Mathematical description and proof of security:
      [[Notes]](https://github.com/anza-xyz/agave/blob/master/docs/src/runtime/zk-docs/percentage_with_cap.pdf)

================
File: docs/src/theme/Footer/index.js
================
function FooterLink(
const FooterLogo = ({ url, alt }) => (
  <img className="footer__logo" alt={alt} src={url} />
);
function Footer()

================
File: docs/src/theme/Footer/styles.module.css
================
.footerLogoLink {
.footerLogoLink:hover {

================
File: docs/src/validator/anatomy.md
================
---
title: Anatomy of a Validator
sidebar_position: 1
sidebar_label: Anatomy
pagination_label: Anatomy of a Validator
---

![Validator block diagrams](/img/validator.svg)

## Pipelining

The validators make extensive use of an optimization common in CPU design, called _pipelining_. Pipelining is the right tool for the job when there's a stream of input data that needs to be processed by a sequence of steps, and there's different hardware responsible for each. The quintessential example is using a washer and dryer to wash/dry/fold several loads of laundry. Washing must occur before drying and drying before folding, but each of the three operations is performed by a separate unit. To maximize efficiency, one creates a pipeline of _stages_. We'll call the washer one stage, the dryer another, and the folding process a third. To run the pipeline, one adds a second load of laundry to the washer just after the first load is added to the dryer. Likewise, the third load is added to the washer after the second is in the dryer and the first is being folded. In this way, one can make progress on three loads of laundry simultaneously. Given infinite loads, the pipeline will consistently complete a load at the rate of the slowest stage in the pipeline.

## Pipelining in the Validator

The validator contains two pipelined processes, one used in leader mode called the TPU and one used in validator mode called the TVU. In both cases, the hardware being pipelined is the same, the network input, the GPU cards, the CPU cores, writes to disk, and the network output. What it does with that hardware is different. The TPU exists to create ledger entries whereas the TVU exists to validate them.

================
File: docs/src/validator/blockstore.md
================
---
title: Blockstore in a Solana Validator
sidebar_position: 3
sidebar_label: Blockstore
pagination_label: Validator Blockstore
---

After a block reaches finality, all blocks from that one on down to the genesis block form a linear chain with the familiar name blockchain. Until that point, however, the validator must maintain all potentially valid chains, called _forks_. The process by which forks naturally form as a result of leader rotation is described in [fork generation](../consensus/fork-generation.md). The _blockstore_ data structure described here is how a validator copes with those forks until blocks are finalized.

The blockstore allows a validator to record every shred it observes on the network, in any order, as long as the shred is signed by the expected leader for a given slot.

Shreds are moved to a fork-able key space the tuple of `leader slot` + `shred index` \(within the slot\). This permits the skip-list structure of the Solana protocol to be stored in its entirety, without a-priori choosing which fork to follow, which Entries to persist or when to persist them.

Repair requests for recent shreds are served out of RAM or recent files and out of deeper storage for less recent shreds, as implemented by the store backing Blockstore.

## Functionalities of Blockstore

1. Persistence: the Blockstore lives in the front of the nodes verification

   pipeline, right behind network receive and signature verification. If the

   shred received is consistent with the leader schedule \(i.e. was signed by the

   leader for the indicated slot\), it is immediately stored.

2. Repair: repair is the same as window repair above, but able to serve any

   shred that's been received. Blockstore stores shreds with signatures,

   preserving the chain of origination.

3. Forks: Blockstore supports random access of shreds, so can support a

   validator's need to rollback and replay from a Bank checkpoint.

4. Restart: with proper pruning/culling, the Blockstore can be replayed by

   ordered enumeration of entries from slot 0. The logic of the replay stage

   \(i.e. dealing with forks\) will have to be used for the most recent entries in

   the Blockstore.

## Blockstore Design

1. Entries in the Blockstore are stored as key-value pairs, where the key is the concatenated slot index and shred index for an entry, and the value is the entry data. Note shred indexes are zero-based for each slot \(i.e. they're slot-relative\).
2. The Blockstore maintains metadata for each slot, in the `SlotMeta` struct containing:

   - `slot_index` - The index of this slot
   - `num_blocks` - The number of blocks in the slot \(used for chaining to a previous slot\)
   - `consumed` - The highest shred index `n`, such that for all `m < n`, there exists a shred in this slot with shred index equal to `n` \(i.e. the highest consecutive shred index\).
   - `received` - The highest received shred index for the slot
   - `next_slots` - A list of future slots this slot could chain to. Used when rebuilding

     the ledger to find possible fork points.

   - `last_index` - The index of the shred that is flagged as the last shred for this slot. This flag on a shred will be set by the leader for a slot when they are transmitting the last shred for a slot.
   - `is_connected` - True iff every block from 0...slot forms a full sequence without any holes. We can derive is_connected for each slot with the following rules. Let slot\(n\) be the slot with index `n`, and slot\(n\).is_full\(\) is true if the slot with index `n` has all the ticks expected for that slot. Let is_connected\(n\) be the statement that "the slot\(n\).is_connected is true". Then:

     is_connected\(0\) is_connected\(n+1\) iff \(is_connected\(n\) and slot\(n\).is_full\(\)

3. Chaining - When a shred for a new slot `x` arrives, we check the number of blocks \(`num_blocks`\) for that new slot \(this information is encoded in the shred\). We then know that this new slot chains to slot `x - num_blocks`.
4. Subscriptions - The Blockstore records a set of slots that have been "subscribed" to. This means entries that chain to these slots will be sent on the Blockstore channel for consumption by the ReplayStage. See the `Blockstore APIs` for details.
5. Update notifications - The Blockstore notifies listeners when slot\(n\).is_connected is flipped from false to true for any `n`.

## Blockstore APIs

The Blockstore offers a subscription based API that ReplayStage uses to ask for entries it's interested in. These subscription API's are as follows:

1. `fn get_slots_since(slots: &[u64]) -> Result<HashMap<u64, Vec<u64>>>`: Returns slots that are connected to any of the elements of `slots`. This method enables the discovery of new children slots.

2. `fn get_slot_entries(slot: Slot, shred_start_index: u64) -> Result<Vec<Entry>>`: For the specified `slot`, return a vector of the available, contiguous entries starting from `shred_start_index`. Shreds are fragments of serialized entries so the conversion from entry index to shred index is not one-to-one. However, there is a similar function `get_slot_entries_with_shred_info()` that returns the number of shreds that comprise the returned entry vector. This allows a caller to track progress through the slot.

Note: Cumulatively, this means that the replay stage will now have to know when a slot is finished, and subscribe to the next slot it's interested in to get the next set of entries. Previously, the burden of chaining slots fell on the Blockstore.

## Interfacing with Bank

The bank exposes to replay stage:

1. `prev_hash`: which PoH chain it's working on as indicated by the hash of the last entry it processed

2. `tick_height`: the ticks in the PoH chain currently being verified by this bank

3. `votes`: a stack of records that contains:
    * `prev_hashes`: what anything after this vote must chain to in PoH
    * `tick_height`: the tick height at which this vote was cast
    * `lockout period`: how long a chain must be observed to be in the ledger to be able to be chained below this vote

Replay stage uses Blockstore APIs to find the longest chain of entries it can hang off a previous vote. If that chain of entries does not hang off the latest vote, the replay stage rolls back the bank to that vote and replays the chain from there.

## Pruning Blockstore

Once Blockstore entries are old enough, representing all the possible forks becomes less useful, perhaps even problematic for replay upon restart. Once a validator's votes have reached max lockout, however, any Blockstore contents that are not on the PoH chain for that vote for can be pruned, expunged.

================
File: docs/src/validator/geyser.md
================
---
title: Solana Validator Geyser Plugins
sidebar_label: Geyser Plugins
pagination_label: Validator Geyser Plugins
---

## Overview

Validators under heavy RPC loads, such as when serving getProgramAccounts calls,
can fall behind the network. To solve this problem, the validator has been
enhanced to support a plugin mechanism, called a "Geyser" plugin, through which
the information about accounts, slots, blocks, and transactions can be
transmitted to external data stores such as relational databases, NoSQL
databases or Kafka. RPC services then can be developed to consume data from
these external data stores with the possibility of more flexible and targeted
optimizations such as caching and indexing. This allows the validator to focus
on processing transactions without being slowed down by busy RPC requests.

This document describes the interfaces of the plugin and the referential plugin
implementation for the PostgreSQL database.

[crates.io]: https://crates.io/search?q=solana-
[docs.rs]: https://docs.rs/releases/search?query=solana-

### Important Crates:

- [`agave-geyser-plugin-interface`] &mdash; This crate defines the plugin
interfaces.

- [`solana-accountsdb-plugin-postgres`] &mdash; The crate for the referential
plugin implementation for the PostgreSQL database.

[`agave-geyser-plugin-interface`]: https://docs.rs/agave-geyser-plugin-interface
[`solana-accountsdb-plugin-postgres`]: https://docs.rs/solana-accountsdb-plugin-postgres
[`solana-sdk`]: https://docs.rs/solana-sdk
[`solana-transaction-status`]: https://docs.rs/solana-transaction-status

## The Plugin Interface

The Plugin interface is declared in [`agave-geyser-plugin-interface`]. It
is defined by the trait `GeyserPlugin`. The plugin should implement the
trait and expose a "C" function `_create_plugin` to return the pointer to this
trait. For example, in the referential implementation, the following code
instantiates the PostgreSQL plugin `GeyserPluginPostgres ` and returns its
pointer.

```
#[no_mangle]
#[allow(improper_ctypes_definitions)]
/// # Safety
///
/// This function returns the GeyserPluginPostgres pointer as trait GeyserPlugin.
pub unsafe extern "C" fn _create_plugin() -> *mut dyn GeyserPlugin {
    let plugin = GeyserPluginPostgres::new();
    let plugin: Box<dyn GeyserPlugin> = Box::new(plugin);
    Box::into_raw(plugin)
}
```

A plugin implementation can implement the `on_load` method to initialize itself.
This function is invoked after a plugin is dynamically loaded into the validator
when it starts. The configuration of the plugin is controlled by a configuration
file in JSON5 format. The JSON5 file must have a field `libpath` that points
to the full path name of the shared library implementing the plugin, and may
have other configuration information, like connection parameters for the external
database. The plugin configuration file is specified by the validator's CLI
parameter `--geyser-plugin-config` and the file must be readable to the
validator process.

Please see the [config file](#config) for the referential
PostgreSQL plugin below for an example.

The plugin can implement the `on_unload` method to do any cleanup before the
plugin is unloaded when the validator is gracefully shutdown.

The plugin framework supports streaming either accounts, transactions or both.
A plugin uses the following function to indicate if it is interested in receiving
account data:

```
fn account_data_notifications_enabled(&self) -> bool
```

And it uses the following function to indicate if it is interested in receiving
transaction data:

```
    fn transaction_notifications_enabled(&self) -> bool
```

The following method is used for notifying on an account update:

```
    fn update_account(
        &mut self,
        account: ReplicaAccountInfoVersions,
        slot: u64,
        is_startup: bool,
    ) -> Result<()>
```

The `ReplicaAccountInfoVersions` struct contains the metadata and data of the account
streamed. The `slot` points to the slot the account is being updated at. When
`is_startup` is true, it indicates the account is loaded from snapshots when
the validator starts up. When `is_startup` is false, the account is updated
when processing a transaction.


The following method is called when all accounts have been notified when the
validator restores the AccountsDb from snapshots at startup.

```
fn notify_end_of_startup(&mut self) -> Result<()>
```

When `update_account` is called during processing transactions, the plugin
should process the notification as fast as possible because any delay may
cause the validator to fall behind the network. Persistence to external data
store is best to be done asynchronously.

The following method is used for notifying slot status changes:

```
    fn update_slot_status(
        &mut self,
        slot: u64,
        parent: Option<u64>,
        status: SlotStatus,
    ) -> Result<()>
```

To ensure data consistency, the plugin implementation can choose to abort
the validator in case of error persisting to external stores. When the
validator restarts the account data will be re-transmitted.

The following method is used for notifying transactions:

```
    fn notify_transaction(
        &mut self,
        transaction: ReplicaTransactionInfoVersions,
        slot: u64,
    ) -> Result<()>
```

The `ReplicaTransactionInfoVersions` struct
contains the information about a streamed transaction. It wraps `ReplicaTransactionInfo`

```
pub struct ReplicaTransactionInfo<'a> {
    /// The first signature of the transaction, used for identifying the transaction.
    pub signature: &'a Signature,

    /// Indicates if the transaction is a simple vote transaction.
    pub is_vote: bool,

    /// The sanitized transaction.
    pub transaction: &'a SanitizedTransaction,

    /// Metadata of the transaction status.
    pub transaction_status_meta: &'a TransactionStatusMeta,
}
```
For details of `SanitizedTransaction` and `TransactionStatusMeta `,
please refer to [`solana-sdk`] and [`solana-transaction-status`]

The `slot` points to the slot the transaction is executed at.
For more details, please refer to the Rust documentation in
[`agave-geyser-plugin-interface`].

# Timing Relationships of Various Plugin Callbacks.

Account update via update_account: As mentioned previously when is_startup is
false, the account is updated during transaction processing. The account update
has information about the transaction causing the update in the `txn` field.
Note, when account update is sent during start up, the txn field is None as
there is no transaction.

```
pub struct ReplicaAccountInfoV3<'a> {
    /// The Pubkey for the account
    pub pubkey: &'a [u8],

    /// The lamports for the account
    pub lamports: u64,

    /// The Pubkey of the owner program account
    pub owner: &'a [u8],

    /// This account's data contains a loaded program (and is now read-only)
    pub executable: bool,

    /// The epoch at which this account will next owe rent
    pub rent_epoch: u64,

    /// The data held in this account.
    pub data: &'a [u8],

    /// A global monotonically increasing atomic number, which can be used
    /// to tell the order of the account update. For example, when an
    /// account is updated in the same slot multiple times, the update
    /// with higher write_version should supersede the one with lower
    /// write_version.
    pub write_version: u64,

    /// Reference to transaction causing this account modification
    pub txn: Option<&'a SanitizedTransaction>,
}
```

The updates are sent serially for different accounts via update_slot_status
in the transaction for a slot. After the accounts notifications are sent, the
SlotStatus::Processed event is sent.

Starting with Agave 3.0, transaction notifications are sent before
SlotStatus::Processed. In prior Agave version, even though SlotStatus::Processed
is sent logically after the transaction events, because there are intermediate
threads emitting the notitications to the plugin, the plugin can see the
transaction notifications and the SlotStatus::Processed for a slot in either
order.

Within a block, transactions are ordered with transaction index. Transactions
within a block are processed and notified in parallel. A plugin should use the
transaction index to determine their relative order.

A plugin can use the notify_block_metadata to know the
executed_transaction_count for a given slot in the following structure:

```
/// Extending ReplicaBlockInfo by sending RewardsAndNumPartitions.
#[derive(Clone, Debug)]
#[repr(C)]
pub struct ReplicaBlockInfoV4<'a> {
    pub parent_slot: Slot,
    pub parent_blockhash: &'a str,
    pub slot: Slot,
    pub blockhash: &'a str,
    pub rewards: &'a RewardsAndNumPartitions,
    pub block_time: Option<UnixTimestamp>,
    pub block_height: Option<u64>,
    pub executed_transaction_count: u64,
    pub entry_count: u64,
}
```

The plugin can associate accounts with transactions via the txn field in the
ReplicaAccountInfoV3 structure. It can also use ReplicaTransactionInfoV2 in
the notify_transaction callback to get the account addresses.

The SlotStatus::Confirmed and SlotStatus::Processed events can reach the plugin
in any order as they are sent asynchronous to each other. A plugin should wait
for both events to confirm they are processed and confirmed.

The SlotStatus::Rooted is sent after SlotStatus::Processed.

## Example PostgreSQL Plugin

The [`solana-accountsdb-plugin-postgres`] repository implements a plugin storing
account data to a PostgreSQL database to illustrate how a plugin can be
developed.

<a name="config">
### Configuration File Format
</a>

The plugin is configured using the input configuration file. An example
configuration file looks like the following:


```
{
	"libpath": "/solana/target/release/libsolana_geyser_plugin_postgres.so",
	"host": "postgres-server",
	"user": "solana",
	"port": 5433,
	"threads": 20,
	"batch_size": 20,
	"panic_on_db_errors": true,
	"accounts_selector" : {
		"accounts" : ["*"]
	}
}
```

The `host`, `user`, and `port` control the PostgreSQL configuration
information. For more advanced connection options, please use the
`connection_str` field. Please see [Rust postgres configuration]
(https://docs.rs/postgres/0.19.2/postgres/config/struct.Config.html).

To improve the throughput to the database, the plugin supports connection pooling
using multiple threads, each maintaining a connection to the PostgreSQL database.
The count of the threads is controlled by the `threads` field. A higher thread
count usually offers better performance.

To further improve performance when saving large numbers of accounts at
startup, the plugin uses bulk inserts. The batch size is controlled by the
`batch_size` parameter. This can help reduce the round trips to the database.

The `panic_on_db_errors` can be used to panic the validator in case of database
errors to ensure data consistency.

### Account Selection

The `accounts_selector` can be used to filter the accounts that should be persisted.

For example, one can use the following to persist only the accounts with particular
Base58-encoded Pubkeys,

```
    "accounts_selector" : {
         "accounts" : ["pubkey-1", "pubkey-2", ..., "pubkey-n"],
    }
```

Or use the following to select accounts with certain program owners:

```
    "accounts_selector" : {
         "owners" : ["pubkey-owner-1", "pubkey-owner-2", ..., "pubkey-owner-m"],
    }
```

To select all accounts, use the wildcard character (*):

```
    "accounts_selector" : {
         "accounts" : ["*"],
    }
```

### Transaction Selection

`transaction_selector`, controls if and what transactions to store.
If this field is missing, none of the transactions are stored.

For example, one can use the following to select only the transactions
referencing accounts with particular Base58-encoded Pubkeys,

```
"transaction_selector" : {
    "mentions" : \["pubkey-1", "pubkey-2", ..., "pubkey-n"\],
}
```

The `mentions` field supports wildcards to select all transaction or
all 'vote' transactions. For example, to select all transactions:

```
"transaction_selector" : {
    "mentions" : \["*"\],
}
```

To select all vote transactions:

```
"transaction_selector" : {
    "mentions" : \["all_votes"\],
}
```

### Database Setup

#### Install PostgreSQL Server

Please follow [PostgreSQL Ubuntu Installation](https://www.postgresql.org/download/linux/ubuntu/)
on instructions to install the PostgreSQL database server. For example, to
install postgresql-14,

```
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo apt-get update
sudo apt-get -y install postgresql-14
```
#### Control the Database Access

Modify the pg_hba.conf as necessary to grant the plugin to access the database.
For example, in /etc/postgresql/14/main/pg_hba.conf, the following entry allows
nodes with IPs in the CIDR 10.138.0.0/24 to access all databases. The validator
runs in a node with an ip in the specified range.

```
host    all             all             10.138.0.0/24           trust
```

It is recommended to run the database server on a separate node from the validator for
better performance.

#### Configure the Database Performance Parameters

Please refer to the [PostgreSQL Server Configuration](https://www.postgresql.org/docs/14/runtime-config.html)
for configuration details. The referential implementation uses the following
configurations for better database performance in the /etc/postgresql/14/main/postgresql.conf
which are different from the default postgresql-14 installation.

```
max_connections = 200                  # (change requires restart)
shared_buffers = 1GB                   # min 128kB
effective_io_concurrency = 1000        # 1-1000; 0 disables prefetching
wal_level = minimal                    # minimal, replica, or logical
fsync = off                            # flush data to disk for crash safety
synchronous_commit = off               # synchronization level;
full_page_writes = off                 # recover from partial page writes
max_wal_senders = 0                    # max number of walsender processes
```

The sample [postgresql.conf](https://github.com/solana-labs/solana/blob/7ac43b16d2c766df61ae0a06d7aaf14ba61996ac/accountsdb-plugin-postgres/scripts/postgresql.conf)
can be used for reference.

#### Create the Database Instance and the Role

Start the server:

```
sudo systemctl start postgresql@14-main
```

Create the database. For example, the following creates a database named 'solana':

```
sudo -u postgres createdb solana -p 5433
```

Create the database user. For example, the following creates a regular user named 'solana':

```
sudo -u postgres createuser -p 5433 solana
```

Verify the database is working using psql. For example, assuming the node running
PostgreSQL has the ip 10.138.0.9, the following command will land in a shell where
SQL commands can be entered:

```
psql -U solana -p 5433 -h 10.138.0.9 -w -d solana
```

#### Create the Schema Objects

Use the [create_schema.sql](https://github.com/solana-labs/solana/blob/a70eb098f4ae9cd359c1e40bbb7752b3dd61de8d/accountsdb-plugin-postgres/scripts/create_schema.sql)
to create the objects for storing accounts and slots.

Download the script from github:

```
wget https://raw.githubusercontent.com/solana-labs/solana/a70eb098f4ae9cd359c1e40bbb7752b3dd61de8d/accountsdb-plugin-postgres/scripts/create_schema.sql
```

Then run the script:

```
psql -U solana -p 5433 -h 10.138.0.9 -w -d solana -f create_schema.sql
```

After this, start the validator with the plugin by using the `--geyser-plugin-config`
argument mentioned above.

#### Destroy the Schema Objects

To destroy the database objects, created by `create_schema.sql`, use
[drop_schema.sql](https://github.com/solana-labs/solana/blob/a70eb098f4ae9cd359c1e40bbb7752b3dd61de8d/accountsdb-plugin-postgres/scripts/drop_schema.sql).
For example,

```
psql -U solana -p 5433 -h 10.138.0.9 -w -d solana -f drop_schema.sql
```

### Capture Historical Account Data

To capture account historical data, in the configuration file, turn
`store_account_historical_data` to true.

And ensure the database trigger is created to save data in the `audit_table` when
records in `account` are updated, as shown in `create_schema.sql`,

```
CREATE FUNCTION audit_account_update() RETURNS trigger AS $audit_account_update$
    BEGIN
		INSERT INTO account_audit (pubkey, owner, lamports, slot, executable, rent_epoch, data, write_version, updated_on)
            VALUES (OLD.pubkey, OLD.owner, OLD.lamports, OLD.slot,
                    OLD.executable, OLD.rent_epoch, OLD.data, OLD.write_version, OLD.updated_on);
        RETURN NEW;
    END;

$audit_account_update$ LANGUAGE plpgsql;

CREATE TRIGGER account_update_trigger AFTER UPDATE OR DELETE ON account
    FOR EACH ROW EXECUTE PROCEDURE audit_account_update();
```

The trigger can be dropped to disable this feature, for example,

```
DROP TRIGGER account_update_trigger ON account;
```

Over time, the account_audit can accumulate large amount of data. You may choose to
limit that by deleting older historical data.

For example, the following SQL statement can be used to keep up to 1000 of the most
recent records for an account:

```
delete from account_audit a2 where (pubkey, write_version) in
    (select pubkey, write_version from
        (select a.pubkey, a.updated_on, a.slot, a.write_version, a.lamports,
            rank() OVER ( partition by pubkey order by write_version desc) as rnk
            from account_audit a) ranked
            where ranked.rnk > 1000)
```

### Main Tables

The following are the tables in the Postgres database

| Table         | Description             |
|:--------------|:------------------------|
| account       | Account data            |
| slot          | Slot metadata           |
| transaction   | Transaction data        |
| account_audit | Account historical data |


### Performance Considerations

When a validator lacks sufficient compute power, the overhead of saving the
account data can cause it to fall behind the network especially when all
accounts or a large number of accounts are selected. The node hosting the
PostgreSQL database needs to be powerful enough to handle the database loads
as well. It has been found using GCP n2-standard-64 machine type for the
validator and n2-highmem-32 for the PostgreSQL node is adequate for handling
transmitting all accounts while keeping up with the network. In addition, it is
best to keep the validator and the PostgreSQL in the same local network to
reduce latency. You may need to size the validator and database nodes
differently if serving other loads.

================
File: docs/src/validator/gossip.md
================
---
title: Gossip Service in a Solana Validator
sidebar_position: 5
sidebar_label: Gossip Service
pagination_label: Validator Gossip Service
---

The Gossip Service acts as a gateway to nodes in the
[control plane](https://solana.com/docs/terminology#control-plane). Validators
use the service to ensure information is available to all other nodes in a
cluster. The service broadcasts information using a
[gossip protocol](https://en.wikipedia.org/wiki/Gossip_protocol).

## Gossip Overview

Nodes continuously share signed data objects among themselves in order to manage
a cluster. For example, they share their contact information, ledger height, and
votes.

Every tenth of a second, each node sends a "push" message and/or a "pull"
message. Push and pull messages may elicit responses, and push messages may be
forwarded on to others in the cluster.

Gossip runs on a well-known UDP/IP port or a port in a well-known range. Once a
cluster is bootstrapped, nodes advertise to each other where to find their
gossip endpoint (a socket address).

## Gossip Records

Records shared over gossip are arbitrary, but signed and versioned (with a
timestamp) as needed to make sense to the node receiving them. If a node
receives two records from the same source, it updates its own copy with the
record with the most recent timestamp.

## Gossip Service Interface

### Push Message

A node sends a push message to tell the cluster it has information to share.
Nodes send push messages to `PUSH_FANOUT` push peers.

Upon receiving a push message, a node examines the message for:

1. Duplication: if the message has been seen before, the node drops the message
   and may respond with `PushMessagePrune` if forwarded from a low staked node

2. New data: if the message is new to the node

   - Stores the new information with an updated version in its cluster info and
     purges any previous older value

   - Stores the message in `pushed_once` (used for detecting duplicates, purged
     after `PUSH_MSG_TIMEOUT * 5` ms)

   - Retransmits the messages to its own push peers

3. Expiration: nodes drop push messages that are older than `PUSH_MSG_TIMEOUT`

### Push Peers, Prune Message

A node selects its push peers at random from the active set of known peers. The
node keeps this selection for a relatively long time. When a prune message is
received, the node drops the push peer that sent the prune. Prune is an
indication that there is another, higher stake weighted path to that node than
direct push.

The set of push peers is kept fresh by rotating a new node into the set every
`PUSH_MSG_TIMEOUT/2` milliseconds.

### Pull Message

A node sends a pull message to ask the cluster if there is any new information.
A pull message is sent to a single peer at random and comprises a Bloom filter
that represents things it already has. A node receiving a pull message iterates
over its values and constructs a pull response of things that miss the filter
and would fit in a message.

A node constructs the pull Bloom filter by iterating over current values and
recently purged values.

A node handles items in a pull response the same way it handles new data in a
push message.

## Purging

Nodes retain prior versions of values (those updated by a pull or push) and
expired values (those older than `GOSSIP_PULL_CRDS_TIMEOUT_MS`) in
`purged_values` (things I recently had). Nodes purge `purged_values` that are
older than `5 * GOSSIP_PULL_CRDS_TIMEOUT_MS`.

## Eclipse Attacks

An eclipse attack is an attempt to take over the set of node connections with
adversarial endpoints.

This is relevant to our implementation in the following ways.

- Pull messages select a random node from the network. An eclipse attack on
  _pull_ would require an attacker to influence the random selection in such a
  way that only adversarial nodes are selected for pull.
- Push messages maintain an active set of nodes and select a random fanout for
  every push message. An eclipse attack on _push_ would influence the active set
  selection, or the random fanout selection.

### Time and Stake based weights

Weights are calculated based on `time since last picked` and the `natural log`
of the `stake weight`.

Taking the `ln` of the stake weight allows giving all nodes a fairer chance of
network coverage in a reasonable amount of time. It helps normalize the large
possible `stake weight` differences between nodes. This way a node with low
`stake weight`, compared to a node with large `stake weight` will only have to
wait a few multiples of ln(`stake`) seconds before it gets picked.

There is no way for an adversary to influence these parameters.

### Pull Message

A node is selected as a pull target based on the weights described above.

### Push Message

A prune message can only remove an adversary from a potential connection.

Just like _pull message_, nodes are selected into the active set based on
weights.

## Notable differences from PlumTree

The active push protocol described here is based on
[Plum Tree](https://haslab.uminho.pt/sites/default/files/jop/files/lpr07a.pdf).
The main differences are:

- Push messages have a wallclock that is signed by the originator. Once the
  wallclock expires the message is dropped. A hop limit is difficult to
  implement in an adversarial setting.
- Lazy Push is not implemented because it's not obvious how to prevent an
  adversary from forging the message fingerprint. A naive approach would allow
  an adversary to be prioritized for pull based on their input.

================
File: docs/src/validator/runtime.md
================
---
title: Solana Runtime on a Solana Validator
sidebar_position: 6
sidebar_label: Runtime
pagination_label: Validator Runtime
---

The runtime is a concurrent transaction processor. Transactions specify their data dependencies upfront and dynamic memory allocation is explicit. By separating program code from the state it operates on, the runtime is able to choreograph concurrent access. Transactions accessing only read-only accounts are executed in parallel whereas transactions accessing writable accounts are serialized. The runtime interacts with the program through an entrypoint with a well-defined interface. The data stored in an account is an opaque type, an array of bytes. The program has full control over its contents.

The transaction structure specifies a list of public keys and signatures for those keys and a sequential list of instructions that will operate over the states associated with the account keys. For the transaction to be committed all the instructions must execute successfully; if any abort the whole transaction fails to commit.

#### Account Structure

Accounts maintain a lamport balance and program-specific memory.

## Transaction Engine

The engine maps public keys to accounts and routes them to the program's entrypoint.

### Execution

Transactions are batched and processed in a pipeline. The TPU and TVU follow a slightly different path. The TPU runtime ensures that PoH record occurs before memory is committed.

The TVU runtime ensures that PoH verification occurs before the runtime processes any transactions.

![Runtime pipeline](/img/runtime.svg)

At the _execute_ stage, the loaded accounts have no data dependencies, so all the programs can be executed in parallel.

The runtime enforces the following rules:

1. Only the _owner_ program may modify the contents of an account. This means that upon assignment data vector is guaranteed to be zero.
2. Total balances on all the accounts are equal before and after execution of a transaction.
3. After the transaction is executed, balances of read-only accounts must be equal to the balances before the transaction.
4. All instructions in the transaction are executed atomically. If one fails, all account modifications are discarded.

Execution of the program involves mapping the program's public key to an entrypoint which takes a pointer to the transaction, and an array of loaded accounts.

### SystemProgram Interface

The interface is best described by the `Instruction::data` that the user encodes.

- `CreateAccount` - This allows the user to create an account with an allocated data array and assign it to a Program.
- `CreateAccountWithSeed` - Same as `CreateAccount`, but the new account's address is derived from
  - the funding account's pubkey,
  - a mnemonic string (seed), and
  - the pubkey of the Program
- `Assign` - Allows the user to assign an existing account to a program.
- `Transfer` - Transfers lamports between accounts.

### Program State Security

For blockchain to function correctly, the program code must be resilient to user inputs. That is why in this design the program specific code is the only code that can change the state of the data byte array in the Accounts that are assigned to it. It is also the reason why `Assign` or `CreateAccount` must zero out the data. Otherwise there would be no possible way for the program to distinguish the recently assigned account data from a natively generated state transition without some additional metadata from the runtime to indicate that this memory is assigned instead of natively generated.

To pass messages between programs, the receiving program must accept the message and copy the state over. But in practice a copy isn't needed and is undesirable. The receiving program can read the state belonging to other Accounts without copying it, and during the read it has a guarantee of the sender program's state.

### Notes

- There is no dynamic memory allocation. Client's need to use `CreateAccount` instructions to create memory before passing it to another program. This instruction can be composed into a single transaction with the call to the program itself.
- `CreateAccount` and `Assign` guarantee that when account is assigned to the program, the Account's data is zero initialized.
- Transactions that assign an account to a program or allocate space must be signed by the Account address' private key unless the Account is being created by `CreateAccountWithSeed`, in which case there is no corresponding private key for the account's address/pubkey.
- Once assigned to program an Account cannot be reassigned.
- Runtime guarantees that a program's code is the only code that can modify Account data that the Account is assigned to.
- Runtime guarantees that the program can only spend lamports that are in accounts that are assigned to it.
- Runtime guarantees the balances belonging to accounts are balanced before and after the transaction.
- Runtime guarantees that instructions all executed successfully when a transaction is committed.

## Future Work

- [Continuations and Signals for long running Transactions](https://github.com/solana-labs/solana/issues/1485)

================
File: docs/src/validator/tpu.md
================
---
title: Transaction Processing Unit in a Solana Validator
sidebar_position: 2
sidebar_label: TPU
pagination_label: Validator's Transaction Processing Unit (TPU)
---

TPU (Transaction Processing Unit) is the logic of the validator
responsible for block production.

![TPU Block Diagram](/img/tpu.svg)

Transactions are encoded and sent in QUIC streams into the validator
from clients (other validators/users of the network) as follows:

* The quic streamer: allocates packet memory and reads the packet data from
the QUIC endpoint and applies some coalescing of packets received at
the same time. Each stream is used to transmit a packet. And there is limit on the
maximum of QUIC connections can be concurrently established between a client
identified by (IP Address, Node Pubkey) and the server. And there is a limit on the
maximum streams can be concurrently opened per connection based on the sender's
stake. Clients with higher stakes will be allowed to open more streams within
a maximum limit. The system also does rate limiting on the packets per
second(PPS) and applied the limit to the connection based on the stake.
Higher stakes offers better bandwidth. If the transfer rate is exceeded,
the server can drop the stream with the error code (15 -- STREAM_STOP_CODE_THROTTLING).
The client is expected to do some sort of exponential back off in retrying the
transactions when running into this situation.

* sigverify stage: deduplicates packets and applies some load-shedding
to remove excessive packets before then filtering packets with invalid
signatures by setting the packet's discard flag.

* banking stage: receives and buffers packet when the node is close to
becoming the leader. Once it detects the node is the block producer it
processes held packets and newly received packets with a Bank at the tip slot.

* forwarding stage: forwards received packets to a node that is or will soon
be leader. Sorts packets by priority and forwards them. Non-vote transactions
are only forwarded if the node has the option enabled (stake overrides) but
will always forward tpu votes.

* broadcast stage: receives the valid transactions formed into Entry's from
banking stage and packages them into shreds to send to network peers through
the turbine tree structure. Serializes, signs, and generates erasure codes
before sending the packets to the appropriate network peer.

================
File: docs/src/validator/tvu.md
================
---
title: Transaction Validation Unit in a Solana Validator
sidebar_position: 3
sidebar_label: TVU
pagination_label: Validator's Transaction Validation Unit (TVU)
---

TVU (Transaction Validation Unit) is the logic of the validator
responsible for propagating blocks between validators and ensuring that
those blocks' transactions reach the replay stage. Its principal external
interface is the turbine protocol.

![TVU Block Diagram](/img/tvu.svg)

## Retransmit Stage

![Retransmit Block Diagram](/img/retransmit_stage.svg)

## TVU sockets

Externally, TVU UDP receiver appears to bind to one port, typically 8002 UDP.
Internally, TVU is actually bound with multiple sockets to improve kernel's handling of the packet queues.

> **NOTE:** TPU sockets use similar logic

A node advertises one external ip/port for TVU while binding multiple sockets to that same port:

```rust
let (tvu_port, tvu_sockets) = multi_bind_in_range_with_config(
    bind_ip_addr,
    port_range,
    socket_config,
    num_tvu_sockets.get(),
)
.expect("tvu multi_bind");
```

 `multi_bind_in_range_with_config` sets `SO_REUSEPORT`. This means that other nodes only need to know about the one ip/port pair for TVU (similar principle applies in the case of TPU UDP sockets). The kernel distributes the incoming packets to all sockets bound to that port, and each socket can be serviced by a different thread.

> **NOTE:** TVU QUIC socket does not use `SO_REUSEPORT`, but otherwise works similarly to UDP

The TVU socket information is published via Gossip and is available in the `ContactInfo` struct.
To set a TVU socket, the node calls `set_tvu(...)`. The `set_tvu()` method is created by the macro `set_socket!`. For example:

```rust
info.set_tvu(QUIC, (addr, tvu_quic_port)).unwrap();
info.set_tvu(UDP, (addr, tvu_udp_port)).unwrap();
```

Under the hood, `set_tvu()` calls `set_socket()`.

In the `ContactInfo` struct, all sockets are identified by a tag/key, e.g.:

```rust
const SOCKET_TAG_TVU: u8 = 10;       // For UDP
const SOCKET_TAG_TVU_QUIC: u8 = 11;  // For QUIC
```

 * `set_socket()` creates a `SocketEntry` and stores that into `ContactInfo::sockets`
 * `set_socket()` updates `ContactInfo::cache`

```rust
cache: [SocketAddr; SOCKET_CACHE_SIZE]
```

`cache` is purely for quick lookups and optimization; it is not serialized and sent to peer nodes.
`SocketEntry` is serialized and sent to peer nodes within the gossip message type `CrdsData::ContactInfo`. Upon receiving the `ContactInfo`, the peer node calls the `get_socket!` macro to retrieve the TVU port associated with the node.
For example, to retrieve the TVU ports of the remote node, the peer node calls:
```rust
get_socket!(tvu, SOCKET_TAG_TVU, SOCKET_TAG_TVU_QUIC);
```

================
File: docs/src/architecture.md
================
---
title: Agave Validator Architecture
sidebar_position: 0
sidebar_label: Overview
pagination_label: Agave Validator Architecture
---

In this section, we will describe the architecture of the Agave Validator.

================
File: docs/src/backwards-compatibility.md
================
---
title: Backward Compatibility Policy
---

As the Solana developer ecosystem grows, so does the need for clear expectations around
breaking API and behavior changes affecting applications and tooling built for Solana by Anza.
In a perfect world, Solana development could continue at a very fast pace without ever
causing issues for existing developers. However, some compromises will need to be made
and so this document attempts to clarify and codify the process for new releases. Furthermore,
there will be a growing number of validator clients maintained separately by distinct teams.
Coordinating across these teams to ensure the reliability of the network will require ongoing
communication.

### Expectations

- Agave software releases include APIs, SDKs, and CLI tooling (with a few [exceptions](#exceptions)).
- Agave software releases follow semantic versioning, more details below.
- Software for a `MINOR` version release will be compatible across all software on the
  same `MAJOR` version.

### Deprecation Process

1. In any `PATCH` or `MINOR` release, a feature, API, endpoint, etc. could be marked as deprecated.
2. According to code upgrade difficulty, some features will remain deprecated for a few release
   cycles.
3. In a future `MAJOR` release, deprecated features will be removed in an incompatible way.

### Release Cadence

The Solana RPC API, Rust SDK, CLI tooling, and SBF Program SDK are all updated and shipped
along with each Solana software release and should always be compatible between `PATCH`
updates of a particular `MINOR` version release.

#### Release Channels

- `edge` software that contains cutting-edge features with no backward compatibility policy
- `beta` software that runs on the Solana Testnet cluster
- `stable` software that run on the Solana Mainnet Beta and Devnet clusters

#### Major Releases (x.0.0)

`MAJOR` version releases (e.g. 2.0.0) may contain breaking changes and removal of previously
deprecated features. Client SDKs and tooling will begin using new features and endpoints
that were enabled in the previous `MAJOR` version.

#### Minor Releases (1.x.0)

New features and proposal implementations are added to _new_ `MINOR` version
releases (e.g. 1.4.0) and are first run on Solana's Testnet cluster. While running
on the testnet, `MINOR` versions are considered to be in the `beta` release channel. After
those changes have been patched as needed and proven to be reliable, the `MINOR` version will
be upgraded to the `stable` release channel and deployed to the Mainnet Beta cluster.

#### Patch Releases (1.0.x)

Low risk features, non-breaking changes, and security and bug fixes are shipped as part
of `PATCH` version releases (e.g. 1.0.11). Patches may be applied to both `beta` and `stable`
release channels.

### RPC API

Patch releases:

- Bug fixes
- Security fixes
- Endpoint / feature deprecation

Minor releases:

- New RPC endpoints and features

Major releases:

- Removal of deprecated features

### Rust Crates

- [`solana-sdk`](https://docs.rs/solana-sdk/) - Rust SDK for creating transactions and parsing account state
- [`solana-program`](https://docs.rs/solana-program/) - Rust SDK for writing programs
- [`solana-client`](https://docs.rs/solana-client/) - Rust client for connecting to RPC API
- [`solana-cli-config`](https://docs.rs/solana-cli-config/) - Rust client for managing Solana CLI config files
- [`agave-geyser-plugin-interface`](https://docs.rs/agave-geyser-plugin-interface/) - Rust interface for developing Solana Geyser plugins.

Patch releases:

- Bug fixes
- Security fixes
- Performance improvements

Minor releases:

- New APIs

Major releases

- Removal of deprecated APIs
- Backwards incompatible behavior changes

### CLI Tools

Patch releases:

- Bug and security fixes
- Performance improvements
- Subcommand / argument deprecation

Minor releases:

- New subcommands

Major releases:

- Switch to new RPC API endpoints / configuration introduced in the previous major version.
- Removal of deprecated features

### Runtime Features

New Agave runtime features are feature-switched and manually activated. Runtime features
include: the introduction of new native programs, sysvars, and syscalls; and changes to
their behavior. Feature activation is cluster agnostic, allowing confidence to be built on
Testnet before activation on Mainnet-beta.

The release process is as follows:

1. New runtime feature is included in a new release, deactivated by default
2. Once sufficient staked validators upgrade to the new release, the runtime feature switch
   is activated manually with an instruction
3. The feature takes effect at the beginning of the next epoch

### Infrastructure Changes

#### Local cluster scripts and Docker images

Breaking changes will be limited to `MAJOR` version updates. `MINOR` and `PATCH` updates should always
be backwards compatible.

### Exceptions

#### Web3 JavaScript SDK

The Web3.JS SDK also follows semantic versioning specifications but is shipped separately from Solana
software releases.

#### Attack Vectors

If a new attack vector is discovered in existing code, the above processes may be
circumvented in order to rapidly deploy a fix, depending on the severity of the issue.

#### CLI Tooling Output

CLI tooling json output (`output --json`) compatibility will be preserved; however, output directed
for a human reader is subject to change. This includes output as well as potential help, warning, or
error messages.

================
File: docs/src/faq.md
================
---
title: Validator Frequently Asked Questions
sidebar_label: Frequently Asked Questions
---

### What is Agave? How is it different from other possible Solana validators?

Solana is an open source, decentralized, proof-of-stake blockchain. It is therefore possible for multiple distinct teams to fork and maintain their own validator software. The original Solana validator was maintained by Solana Labs. A new organization, Anza, was formed in 2024 consisting of former Solana Labs core engineering members. Anza forked the Solana validator and renamed it to Agave (this project). Agave is the version of the original Solana validator maintained by the team at Anza. As of the writing of this FAQ, Agave is the most popular validator client for the Solana network, but it is likely in the future there may be several validators running in parallel to help support the network. We recommend checking the community and doing research before making a selection.

### What is a validator?

A validator is a computer that runs a software program to verify transactions that are added to the Solana blockchain.  A validator can be a voting validator or a non voting validator. To learn more, see [what is a validator](./what-is-a-validator.md).

### What is an RPC node?

An RPC node is also a computer that runs the validator software.  Typically, an RPC node does not vote on the network.  Instead the RPC node's job is to respond to API requests.  See [what is an rpc node](./what-is-an-rpc-node.md) for more information.

### What is a cluster?

For a definition and an overview of the topic, see [what is a cluster?](./clusters/index.md). Solana maintains several clusters. For details on each, see [Solana clusters](./clusters/available.md).

### What is Proof of Stake?

Proof of Stake (PoS) is a blockchain architecture. Solana is a Proof of Stake blockchain. To read more, see [Proof of Stake](./what-is-a-validator.md#proof-of-stake).

### What is Proof of Work? Is running a Solana validator the same as mining?

No, a Solana validator uses Proof of Stake. It does not use Proof of Work (often called mining). See [Proof of Work: For Contrast](./what-is-a-validator.md#proof-of-stake).

### Who can operate a validator?

Anyone can operate a validator.  All Solana clusters are permissionless. A new operator can choose to join at any time.

### Is there a validator set or limited number of validators that can operate?

No, all Solana clusters are permissionless.  There is no limit to the number of active validators that can participate in consensus.  Validators participating in consensus (voting validators) incur transaction fees for each vote.  A voting validator can expect to incur up to 1.1 SOL per day in vote transaction fees.

### What are the hardware requirements for running a validator?

See [validator requirements](./operations/requirements.md).

### Can I run my validator at home?

Anyone can join the cluster including home users. You must make sure that your system can perform well and keep up with the cluster. Many home internet connections are not suitable to run a Solana validator.  Most operators choose to operate their validator in a data center either by using a server provider or by supplying your own hardware at a colocation data center.

See the [validator requirements](./operations/requirements.md) for more information.

### What skills does a Solana validator operator need?

See [Solana validator prerequisites](./operations/prerequisites.md).

### What are the economics of running a validator?

See [economics of running a validator](./operations/validator-or-rpc-node.md#economics-of-running-a-consensus-validator).

================
File: docs/src/index.mdx
================
---
slug: /
id: home
title: Home
sidebar_label: Home
pagination_label: Agave Validator Documentation Home
description: "Agave is a validator for the Solana blockchain maintained by the Anza core engineering team \
  it is a fork of the original Solana validator software."
---

# Agave Validator Documentation

Solana is a blockchain built for mass adoption. It's a high performance network
that is utilized for a range of use cases, including finance, NFTs, payments,
and gaming. Solana operates as a single global state machine, and is open,
interoperable and decentralized. Agave is a fork of the original Solana validator
previously maintained by the Solana Labs team. Agave is now under active development by the
core engineering team at Anza, one of several Solana validator clients.

## Command Line Interface and Tool Suite

To get started using the Solana Command Line (CLI) tools:

- [Install the Solana CLI Tool Suite](./cli/install.md) - Quickly get setup
  locally with the CLI, optionally build from source
- [Introduction to the CLI conventions](./cli/intro.md) - Understand the common
  conventions used within the CLI tool suite
- [Choose a cluster](./cli/examples/choose-a-cluster.md) - Select a Solana
  network cluster to connect (e.g. `devnet`, `testnet`, `mainnet-beta`)
- [Create a wallet](./cli/wallets/index.md) - Create a command line wallet for
  use within the CLI and beyond

## Understanding the Architecture

Get to know the underlying architecture of how the proof-of-stake blockchain
works:

- [Clusters](./clusters/index.md) - a collection of validators that work
  together for consensus
- [Validators](./validator/anatomy.md) - the individual nodes that are the
  backbone of the network
- [Runtime](./runtime/programs.md) - the native programs that are core to the
  validator and the blockchain

## Running a Validator

Explore what it takes to operate an Agave validator and help secure the network.

- [Validator vs RPC node](./operations/validator-or-rpc-node.md) - Understand
  the important differences between voting and non-voting validators on the
  network
- [System requirements](./operations/requirements.md) - Recommended hardware
  requirements and expected SOL needed to operate a validator
- [Quick start guide](./operations/setup-a-validator.md) - Setup a validator and
  get connected to a cluster for the first time

## Learn more

import HomeCtaLinks from "@site/components/HomeCtaLinks";

<HomeCtaLinks />

================
File: docs/src/proposals.md
================
---
title: System Design Proposals
sidebar_label: Overview
---

Changes to the Solana architecture are performed through a public proposal process (via pull requests) on the [Solana GitHub repository](https://github.com/solana-labs/solana). New proposals should be submitted with the "[Submit a Design Proposal](#submit-a-design-proposal)" guide below.

There are currently two different states of these design proposals:

1. [Accepted Proposals](./proposals/accepted-design-proposals.md)
2. [Implemented Proposals](./implemented-proposals/index.md)

## Accepted Proposals

These architectural proposals have been accepted by the Solana team, but are not yet fully implemented.

Each proposal may be implemented as described, implemented differently as issues in the designs become evident, or not implemented at all. If implemented, the proposal will be moved to [Implemented Proposals](./implemented-proposals/index.md) and the details will be added to relevant sections of the docs.

## Implemented Proposals

These architectural proposals have been accepted and implemented by the Solana team.

Any designs that may be subject to future change are noted in their specific proposal page.

## Submit a Design Proposal

To submit a new design proposal for Solana:

1. Propose a design by creating a PR that adds a markdown document to the `docs/src/proposals` directory.
2. Add any relevant Solana maintainers to the PR review.
3. Publish the PR for community review and feedback.

> **NOTE:** All people submitting PRs to the Solana repo should consult the [CONTRIBUTING](https://github.com/solana-labs/solana/blob/master/CONTRIBUTING.md) doc in the repo.

### After Accepted

Once a design proposal has been accepted, the PR will be merged into the `master` branch of the Solana repo. This also signifies the maintainers support your plan of attack.

> **NOTE:** The merging of the PR will **automatically** create a link in the "Accepted Proposals" table of contents sidebar.
> Once approved, continue to submit PRs that implement the proposal. When the implementation reveals the need for tweaks to the proposal, be sure to update the "accepted proposal" document and have these changes reviewed by the same approving maintainers.

### After Implemented

After a proposal has been fully implemented into the Solana architecture, a PR should be created to perform the following:

1. Move the newly implemented proposal file from `docs/src/proposals` to `docs/src/implemented-proposals`
2. Create a new redirect in the `publish-docs.sh` to redirect the old `accepted` proposal page to the new `implemented-proposal` page
3. Publish the PR

> **NOTE:** Moving the proposal document into the `implemented-proposals` directory will **automatically** move the link in the "Accepted Proposals" table of contents sidebar to the "Implemented Proposals" sidebar.

================
File: docs/src/what-is-a-validator.md
================
---
title: What is a Validator?
---

A validator is a computer that helps to run the Solana network. Each validator executes a program that keeps track of all accounts on the Solana cluster and validates transactions being added to the network. Without validators, Solana would not be able to function. Agave is one of several possible validator clients operators can use to help run the Solana network.

The more independent entities that run validators, the less vulnerable the cluster is to an attack or catastrophe that affects the cluster.

> For a more in depth look at the health of the Solana network, see the [Solana Foundation Validator Health Report](https://solana.com/news/validator-health-report-march-2023).

By becoming a validator, you are helping to grow the network. You are also learning first hand how the Solana cluster functions at the lowest level. You will become part of an active community of operators that are passionate about the Solana ecosystem.

## Consensus vs RPC

Before we discuss validators in more detail, it's useful to make some distinctions. Using the same validator software, you have the option of running a voting/consensus node or choosing to instead run an RPC node. An RPC node helps Solana devs and others interact with the blockchain but for performance reasons should not vote. We go into more detail on RPC nodes in the next section, [what is an rpc node](./what-is-an-rpc-node.md).

For this document, when a validator is mentioned, we are talking about a voting/consensus node. Now, to better understand what your validator is doing, it would help to understand how the Solana network functions in more depth. This documentation specifically focuses on the Agave client.

## Proof Of Stake

Proof of stake is the blockchain architecture that is used in Solana. It is called proof of stake because token holders can stake their tokens to a validator of their choice. When a person stakes their tokens, that person still owns the tokens and can remove the stake at any time. The staked tokens represent their trust in that validator. When a person stakes their tokens to a validator, they are given a return of some amount of tokens as a reward for helping to run and secure the network. The more tokens you have staked to a validator the more rewards you receive. A validator that has a large amount of tokens staked to it has a larger vote share in consensus. A validator, therefore, is given more opportunities to produce blocks in the network proportional to the size of the stake in the validator. The validator that is currently producing blocks in the network is known as the leader.

## Proof Of Work: For Contrast

Solana is not a proof of work system. Proof of work is a different blockchain architecture in which a computer (often called a miner), works to solve a cryptographic problem before anyone else on the network is able to solve it. The more often the computer solves these problems, the more rewards the miner receives. Because of the incentive to solve a hard computational problem first, miners often use many computers at the same time. The number of computers used to solve these problems leads to large energy consumption and resulting environmental challenges.

Solana, in contrast, does not incentivize validators to use many computers to solve a computational problem. Because a validator would like to have a larger amount staked to it, there is no real advantage for an independent validator to using many different computers. Here, you can see a comparison of [Solana's environmental impact](https://solana.com/news/solana-energy-usage-report-november-2021).

## Proof Of History

Proof of history, PoH, is one of the key innovations in Solana that allows transactions to be finalized very quickly. At a high level, PoH allows validators in the cluster to agree on a cryptographically repeatable clock. Both proof of stake and proof of work architectures mentioned above are architectures that bring the cluster to consensus. In other words, these algorithms decide which blocks should be added to the blockchain. Proof of history is not a consensus architecture, but rather a feature in Solana that makes block finalization faster in the proof of stake system.

Understanding how PoH works is not necessary to run a good validator, but a very approachable discussion can be found [in this Medium article](https://medium.com/solana-labs/proof-of-history-explained-by-a-water-clock-e682183417b8). Also, the [Solana whitepaper](https://solana.com/solana-whitepaper.pdf) does a good job of explaining the algorithm in an approachable way for the more technically minded.

## Your Role As A Validator

As a validator, you are helping to secure the network by producing and voting on blocks and to improve decentralization by running an independent node. You have the right to participate in discussions of changes on the network. You are also assuming a responsibility to keep your system running properly, to make sure your system is secure, and to keep it up to date with the latest software. As more individuals stake their tokens to your validator, you can reward their trust by running a high performing and reliable validator. Hopefully, your validator is performing well a majority of the time, but you should also have systems in place to respond to an outage at any time of the day. If your validator is not responding late at night, someone (either you or other team members) need to be available to investigate and fix the issues.

Running a validator is a [technical and important task](./operations/prerequisites.md), but it can also be very rewarding. Good luck and welcome to the community.

================
File: docs/src/what-is-an-rpc-node.md
================
---
title: What is an RPC Node?
---

An RPC (Remote Procedure Call) node runs the same software as a [validator](./what-is-a-validator.md), but it does not participate in the consensus process. Technically you could run the RPC software and also allow your node to vote as a consensus node, but it is strongly discouraged because your node will not be performant enough to do either task well.

A node that runs RPC has a much different purpose in the cluster. An RPC node responds to requests about the blockchain and also allows users of the RPC node to submit new transactions to be included in blocks.

For example, a website might request to transfer tokens from wallet A to wallet B (given wallet A's permission). That website would have to use wallet A to sign a transaction and then send it to an RPC node to be submitted to the leader. So you could think of running an RPC node as a similar engineering task to providing an api for others to use.

The users of the RPC node are often developers, so this option may require a more technical understanding of Solana. To better understand RPC node operations, you'll want to become familiar with the different RPC calls.
You can find the [RPC API here](https://solana.com/docs/rpc/http).

================
File: docs/static/katex/contrib/auto-render.js
================
function __webpack_require__(moduleId)
⋮----
/******/
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = 1);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports) {
⋮----
/***/ }),
/* 1 */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

================
File: docs/static/katex/contrib/auto-render.min.js
================
!function(e,t)

================
File: docs/static/katex/contrib/auto-render.mjs
================
/* eslint no-constant-condition:0 */
⋮----
// Adapted from
// https://github.com/Khan/perseus/blob/master/src/perseus-markdown.jsx
⋮----
/* eslint no-console:0 */
⋮----
/* Note: optionsCopy is mutated by this method. If it is ever exposed in the
 * API, we should copy it before mutating.
 */
⋮----
// There is no formula in the text.
// Let's return null which means there is no need to replace
// the current text node with a new one.
⋮----
let math = data[i].data; // Override any display mode defined in the settings with that
// defined by the text itself
⋮----
// Text node
⋮----
// Element node
⋮----
} // Otherwise, it's something else, and ignore it.
⋮----
const optionsCopy = {}; // Object.assign(optionsCopy, option)
⋮----
} // default options
⋮----
}, // LaTeX uses $…$, but it ruins the display of normal `$` in text:
// {left: "$", right: "$", display: false},
//  \[…\] must come last in this array. Otherwise, renderMathInElement
//  will search for \[ before it searches for $$ or  \(
// That makes it susceptible to finding a \\[0.3em] row delimiter and
// treating it as if it were the start of a KaTeX math zone.
⋮----
optionsCopy.errorCallback = optionsCopy.errorCallback || console.error; // Enable sharing of global macros defined via `\gdef` between different
// math elements within a single call to `renderMathInElement`.

================
File: docs/static/katex/contrib/copy-tex.css
================
.katex,

================
File: docs/static/katex/contrib/copy-tex.js
================
function __webpack_require__(moduleId)
⋮----
/******/
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = 1);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports, __webpack_require__) {
// extracted by mini-css-extract-plugin
/***/ }),
/* 1 */
/***/ (function(module, __webpack_exports__, __webpack_require__) {
⋮----
event.clipboardData.setData('text/html', html.join('')); // Rewrite plain-text version.

================
File: docs/static/katex/contrib/copy-tex.min.css
================
.katex,.katex-display{user-select:all;-moz-user-select:all;-webkit-user-select:all;-ms-user-select:all}

================
File: docs/static/katex/contrib/copy-tex.min.js
================
!function(e,t)

================
File: docs/static/katex/contrib/copy-tex.mjs
================
// Set these to how you want inline and display math to be delimited.
⋮----
// alternative: ['\(', '\)']
display: ['$$', '$$'] // alternative: ['\[', '\]']
⋮----
}; // Replace .katex elements with their TeX source (<annotation> element).
// Modifies fragment in-place.  Useful for writing your own 'copy' handler,
// as in copy-tex.js.
⋮----
// Remove .katex-html blocks that are preceded by .katex-mathml blocks
// (which will get replaced below).
⋮----
} // Replace .katex-mathml elements with their annotation (TeX source)
// descendant, with inline delimiters.
⋮----
} // Switch display math to display delimiters.
⋮----
return; // default action OK if selection is empty
⋮----
return; // default action OK if no .katex-mathml elements
} // Preserve usual HTML copy/paste behavior.
⋮----
event.clipboardData.setData('text/html', html.join('')); // Rewrite plain-text version.
⋮----
event.clipboardData.setData('text/plain', katexReplaceWithTex(fragment).textContent); // Prevent normal copy handling.

================
File: docs/static/katex/contrib/mathtex-script-type.js
================
function __webpack_require__(moduleId)
⋮----
/******/
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = 1);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports) {
⋮----
/***/ }),
/* 1 */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

================
File: docs/static/katex/contrib/mathtex-script-type.min.js
================
!function(e,t)

================
File: docs/static/katex/contrib/mathtex-script-type.mjs
================
//console.error(err); linter doesn't like this

================
File: docs/static/katex/contrib/mhchem.js
================
function __webpack_require__(moduleId)
⋮----
/******/
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = 1);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports) {
⋮----
/***/ }),
/* 1 */
/***/ (function(module, __webpack_exports__, __webpack_require__) {
⋮----
// context.consumeArgs has eaten a space.
⋮----
}; //
// Core parser for mhchem syntax  (recursive)
//
/** @type {MhchemParser} */
⋮----
//
// Parses mchem \ce syntax
//
// Call like
//   go("H2O");
⋮----
var a = mhchemParser.patterns.findObserveGroups(input, "", /^\([a-z]{1,3}(?=[\),])/, ")", ""); // (aq), (aq,$\infty$), (aq, sat)
⋮----
} //  AND end of 'phrase'
var m = input.match(/^(?:\((?:\\ca\s?)?\$[amothc]\$\))/); // OR crystal system ($o$) (\ca$c$)
⋮----
// -space -, -; -] -/ -$ -state-of-aggregation
⋮----
// \\x - but output no space before
⋮----
// only those with numbers in front, because the others will be formatted correctly anyway
⋮----
// 0 could be oxidation or charge
⋮----
var match; // e.g. 2, 0.5, 1/2, -2, n/2, +;  $a$ could be added later in parsing
⋮----
// e.g. $2n-1$, $-$
⋮----
} // state of aggregation = no formula
⋮----
/** @type {{(input: string, pattern: string | RegExp): string | string[] | null;}} */
⋮----
//
// Matching function
// e.g. match("a", input) will look for the regexp called "a" and see if it matches
⋮----
/** @type {ParserOutput[]} */
⋮----
//
// createTransitions
// convert  { 'letter': { 'state': { action_: 'output' } } }  to  { 'state' => [ { pattern: 'letter', task: { action_: [{type_: 'output'}] } } ] }
// with expansion of 'a|b' to 'a' and 'b' (at 2 places)
//
⋮----
/** @type {string[]} */
⋮----
var i; //
// 1. Collect all states
//
/** @type {Transitions} */
⋮----
} //
// 2. Fill states
//
⋮----
//
// 2a. Normalize actions into array:  'text=' ==> [{type_:'text='}]
// (Note to myself: Resolving the function here would be problematic. It would need .bind (for *this*) and currying (for *option*).)
//
/** @type {any} */
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
// entityFollows:
//   undefined = if we have nothing else to output, also ignore the just read space (buffer.sb)
//   1 = an entity follows, never omit the space if there was one just read before (can only apply to state 1)
//   2 = 1 + the entity can have an amount, so output a\, instead of converting it to o (can only apply to states a|as)
/** @type {ParserOutput | ParserOutput[]} */
⋮----
if (!buffer.a && !buffer.b && !buffer.p && !buffer.o && !buffer.q && !buffer.d && !entityFollows) {//ret = [];
⋮----
// r
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput} */
⋮----
/** @type {ParserOutput} */
⋮----
/** @type {ParserOutput} */
⋮----
//#endregion
//
// \pu state machines
//
//#region pu
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
} //#endregion
⋮----
}; //
// texify: Take MhchemParser output and convert it to TeX
//
/** @type {Texify} */
⋮----
// (recursive, max 4 levels)
⋮----
}; //
// a
//
⋮----
} //
// b and p
//
⋮----
} //
// o
//
⋮----
} //
// q and d
//
⋮----
function assertNever(a)
function assertString(a)

================
File: docs/static/katex/contrib/mhchem.min.js
================
!function(t,e)

================
File: docs/static/katex/contrib/mhchem.mjs
================
/* eslint-disable */
⋮----
/* -*- Mode: Javascript; indent-tabs-mode:nil; js-indent-level: 2 -*- */
⋮----
/* vim: set ts=2 et sw=2 tw=80: */
⋮----
/*************************************************************
 *
 *  KaTeX mhchem.js
 *
 *  This file implements a KaTeX version of mhchem version 3.3.0.
 *  It is adapted from MathJax/extensions/TeX/mhchem.js
 *  It differs from the MathJax version as follows:
 *    1. The interface is changed so that it can be called from KaTeX, not MathJax.
 *    2. \rlap and \llap are replaced with \mathrlap and \mathllap.
 *    3. Four lines of code are edited in order to use \raisebox instead of \raise.
 *    4. The reaction arrow code is simplified. All reaction arrows are rendered
 *       using KaTeX extensible arrows instead of building non-extensible arrows.
 *    5. \tripledash vertical alignment is slightly adjusted.
 *
 *    This code, as other KaTeX code, is released under the MIT license.
 *
 * /*************************************************************
 *
 *  MathJax/extensions/TeX/mhchem.js
 *
 *  Implements the \ce command for handling chemical formulas
 *  from the mhchem LaTeX package.
 *
 *  ---------------------------------------------------------------------
 *
 *  Copyright (c) 2011-2015 The MathJax Consortium
 *  Copyright (c) 2015-2018 Martin Hensel
 *
 *  Licensed under the Apache License, Version 2.0 (the "License");
 *  you may not use this file except in compliance with the License.
 *  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  Unless required by applicable law or agreed to in writing, software
 *  distributed under the License is distributed on an "AS IS" BASIS,
 *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  See the License for the specific language governing permissions and
 *  limitations under the License.
 */
//
// Coding Style
//   - use '' for identifiers that can by minified/uglified
//   - use "" for strings that need to stay untouched
// version: "3.3.0" for MathJax and KaTeX
// Add \ce, \pu, and \tripledash to the KaTeX macros.
⋮----
}); //  Needed for \bond for the ~ forms
//  Raise by 2.56mu, not 2mu. We're raising a hyphen-minus, U+002D, not
//  a mathematical minus, U+2212. So we need that extra 0.56.
⋮----
//  This is the main function for handing the \ce and \pu commands.
//  It takes the argument to \ce or \pu and returns the corresponding TeX string.
//
⋮----
// Recreate the argument string from KaTeX's array of tokens.
⋮----
// context.consumeArgs has eaten a space.
⋮----
}; //
// Core parser for mhchem syntax  (recursive)
//
⋮----
/** @type {MhchemParser} */
⋮----
//
// Parses mchem \ce syntax
//
// Call like
//   go("H2O");
//
⋮----
var state = '0'; //
// String buffers for parsing:
//
// buffer.a == amount
// buffer.o == element
// buffer.b == left-side superscript
// buffer.p == left-side subscript
// buffer.q == right-side subscript
// buffer.d == right-side superscript
//
// buffer.r == arrow
// buffer.rdt == arrow, script above, type
// buffer.rd == arrow, script above, content
// buffer.rqt == arrow, script below, type
// buffer.rq == arrow, script below, content
//
// buffer.text_
// buffer.rm
// etc.
//
// buffer.parenthesisLevel == int, starting at 0
// buffer.sb == bool, space before
// buffer.beginsWithBond == bool
//
// These letters are also used as state names.
//
// Other states:
// 0 == begin of main part (arrow/operator unlikely)
// 1 == next entity
// 2 == next entity (arrow/operator unlikely)
// 3 == next atom
// c == macro
//
⋮----
/** @type {Buffer} */
⋮----
input = input.replace(/[\u2026]/g, "..."); //
// Looks through mhchemParser.transitions, to execute a matching action
// (recursive)
//
⋮----
/** @type {ParserOutput[]} */
⋮----
} //
// Find actions in transition table
//
⋮----
//
// Execute actions
//
⋮----
var o; //
// Find and execute action
//
⋮----
throw ["MhchemBugA", "mhchem bug A. Please report. (" + task.action_[iA].type_ + ")"]; // Trying to use non-existing action
} //
// Add output
//
⋮----
} //
// Set next state,
// Shorten input,
// Continue with next character
//   (= apply only one transition per position)
//
⋮----
} //
// Prevent infinite loop
//
⋮----
throw ["MhchemBugU", "mhchem bug U. Please report."]; // Unexpected character
⋮----
//
// Matching patterns
// either regexps or function that return null or {match_:"a", remainder:"bc"}
//
⋮----
// property names must not look like integers ("2") for correct property traversal order, later on
⋮----
// ... or crystal system
var a = mhchemParser.patterns.findObserveGroups(input, "", /^\([a-z]{1,3}(?=[\),])/, ")", ""); // (aq), (aq,$\infty$), (aq, sat)
⋮----
} //  AND end of 'phrase'
⋮----
var m = input.match(/^(?:\((?:\\ca\s?)?\$[amothc]\$\))/); // OR crystal system ($o$) (\ca$c$)
⋮----
// -space -, -; -] -/ -$ -state-of-aggregation
⋮----
// \\x - but output no space before
⋮----
// only those with numbers in front, because the others will be formatted correctly anyway
⋮----
// 0 could be oxidation or charge
⋮----
var match; // e.g. 2, 0.5, 1/2, -2, n/2, +;  $a$ could be added later in parsing
⋮----
// e.g. $2n-1$, $-$
⋮----
} // state of aggregation = no formula
⋮----
/** @type {{(input: string, pattern: string | RegExp): string | string[] | null;}} */
⋮----
/** @type {{(input: string, i: number, endChars: string | RegExp): {endMatchBegin: number, endMatchEnd: number} | null;}} */
⋮----
/** @type {string[]} */
⋮----
//
// Matching function
// e.g. match("a", input) will look for the regexp called "a" and see if it matches
// returns null or {match_:"a", remainder:"bc"}
//
⋮----
throw ["MhchemBugP", "mhchem bug P. Please report. (" + m + ")"]; // Trying to use non-existing pattern
⋮----
return mhchemParser.patterns.patterns[m](input); // cannot use cached var pattern here, because some pattern functions need this===mhchemParser
⋮----
// RegExp
⋮----
//
// Generic state machine actions
//
⋮----
/** @type {ParserOutput[]} */
⋮----
//
// createTransitions
// convert  { 'letter': { 'state': { action_: 'output' } } }  to  { 'state' => [ { pattern: 'letter', task: { action_: [{type_: 'output'}] } } ] }
// with expansion of 'a|b' to 'a' and 'b' (at 2 places)
//
⋮----
/** @type {string[]} */
⋮----
var i; //
// 1. Collect all states
//
⋮----
/** @type {Transitions} */
⋮----
} //
// 2. Fill states
//
⋮----
//
// 2a. Normalize actions into array:  'text=' ==> [{type_:'text='}]
// (Note to myself: Resolving the function here would be problematic. It would need .bind (for *this*) and currying (for *option*).)
//
⋮----
/** @type {any} */
⋮----
} //
// 2.b Multi-insert
//
⋮----
// insert into all
⋮----
}; //
// Definition of state machines
//
⋮----
//
// \ce state machines
//
//#region ce
⋮----
// main parser
⋮----
// ^ and _ without a sensible argument
⋮----
// 2$n$
⋮----
// not 'amount'
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
// entityFollows:
//   undefined = if we have nothing else to output, also ignore the just read space (buffer.sb)
//   1 = an entity follows, never omit the space if there was one just read before (can only apply to state 1)
//   2 = 1 + the entity can have an amount, so output a\, instead of converting it to o (can only apply to states a|as)
⋮----
/** @type {ParserOutput | ParserOutput[]} */
⋮----
// r
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput} */
⋮----
/** @type {ParserOutput} */
⋮----
/** @type {ParserOutput} */
⋮----
//#endregion
//
// \pu state machines
//
//#region pu
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput | ParserOutput[]} */
⋮----
// fraction
⋮----
// no fraction
⋮----
/** @type {ParserOutput | ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
/** @type {ParserOutput[]} */
⋮----
} //#endregion
⋮----
}; //
// texify: Take MhchemParser output and convert it to TeX
//
⋮----
/** @type {Texify} */
⋮----
// (recursive, max 4 levels)
⋮----
/** @type {undefined | string} */
⋮----
}; //
// a
//
⋮----
} //
// b and p
//
⋮----
} //
// o
//
⋮----
} //
// q and d
//
⋮----
res = buf.p1 + " "; // &, \\\\, \\hlin
⋮----
// Missing texify rule or unknown MhchemParser output
⋮----
}; //

================
File: docs/static/katex/contrib/render-a11y-string.js
================
function __webpack_require__(moduleId)
⋮----
/******/
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = 1);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports) {
⋮----
/***/ }),
/* 1 */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

================
File: docs/static/katex/contrib/render-a11y-string.min.js
================
!function(e,r)

================
File: docs/static/katex/contrib/render-a11y-string.mjs
================
/**
 * renderA11yString returns a readable string.
 *
 * In some cases the string will have the proper semantic math
 * meaning,:
 *   renderA11yString("\\frac{1}{2}"")
 *   -> "start fraction, 1, divided by, 2, end fraction"
 *
 * However, other cases do not:
 *   renderA11yString("f(x) = x^2")
 *   -> "f, left parenthesis, x, right parenthesis, equals, x, squared"
 *
 * The commas in the string aim to increase ease of understanding
 * when read by a screenreader.
 */
⋮----
// TODO: add entries for all accents
⋮----
const buildString = (str, type, a11yStrings) =>
⋮----
} // If the text to add is a number and there is already a string
// in the list and the last string is a number then we should
// combine them into a single number
⋮----
if (/^\d+$/.test(ret) && a11yStrings.length > 0 && // TODO(kevinb): check that the last item in a11yStrings is a string
// I think we might be able to drop the nested arrays, which would make
// this easier to type - $FlowFixMe
⋮----
const buildRegion = (a11yStrings, callback) =>
⋮----
const handleObject = (tree, a11yStrings, atomType) =>
⋮----
// Everything else is assumed to be an object...
⋮----
// Used internally by accent symbols.
⋮----
// TODO(kevinb): figure out what should be done for inner
⋮----
// Used by \color, \colorbox, and \fcolorbox but not directly rendered.
// It's a leaf node and has no children so just break.
⋮----
// genfrac can have unbalanced delimiters
⋮----
rightDelim = tree.rightDelim; // NOTE: Not sure if this is a safe assumption
// hasBarLine true -> fraction, false -> binomial
⋮----
// No op: we don't attempt to present kerning information
// to the screen reader.
⋮----
// TODO: double check that this is a no-op
⋮----
// Used internally by operator symbols.
⋮----
// We ignore the styling and just pass through the contents
⋮----
// TODO: handle other fonts
⋮----
// TODO: create a map for these.
// TODO: differentiate between a body with a single atom, e.g.
// "cancel a" instead of "start cancel, a, end cancel"
⋮----
// Although there are nodes of type "size" in the parse tree, they have
// no semantic meaning and should be ignored.
⋮----
// All infix nodes are replace with other nodes.
⋮----
// TODO: callout the start/end of specific fonts
// TODO: map \BBb{N} to "the naturals" or something like that
⋮----
// This is used by environments.
⋮----
// \neq and \ne are macros so we let "htmlmathml" render the mathmal
// side of things and extract the text from that.
const atomType = tree.mclass.slice(1); // $FlowFixMe: drop the leading "m" from the values in mclass
⋮----
// TODO: track which which style we're using, e.g. dispaly, text, etc.
// default to text style if even that may not be the correct style
⋮----
// internal nodes are never included in the parse tree

================
File: docs/static/katex/katex.css
================
@font-face {
⋮----
.katex {
.katex * {
.katex .katex-version::after {
.katex .katex-mathml {
.katex .katex-html {
.katex .katex-html > .newline {
.katex .base {
.katex .strut {
.katex .textbf {
.katex .textit {
.katex .textrm {
.katex .textsf {
.katex .texttt {
.katex .mathnormal {
.katex .mathit {
.katex .mathrm {
.katex .mathbf {
.katex .boldsymbol {
.katex .amsrm {
.katex .mathbb,
.katex .mathcal {
.katex .mathfrak,
.katex .mathtt {
.katex .mathscr,
.katex .mathsf,
.katex .mathboldsf,
.katex .mathitsf,
.katex .mainrm {
.katex .vlist-t {
.katex .vlist-r {
.katex .vlist {
.katex .vlist > span {
.katex .vlist > span > span {
.katex .vlist > span > .pstrut {
.katex .vlist-t2 {
.katex .vlist-s {
.katex .vbox {
.katex .hbox {
.katex .thinbox {
.katex .msupsub {
.katex .mfrac > span > span {
.katex .mfrac .frac-line {
.katex .mfrac .frac-line,
.katex .mspace {
.katex .llap,
.katex .llap > .inner,
.katex .llap > .fix,
.katex .llap > .inner {
.katex .rlap > .inner,
.katex .clap > .inner > span {
.katex .rule {
.katex .overline .overline-line,
.katex .hdashline {
.katex .sqrt > .root {
.katex .sizing.reset-size1.size1,
.katex .sizing.reset-size1.size2,
.katex .sizing.reset-size1.size3,
.katex .sizing.reset-size1.size4,
.katex .sizing.reset-size1.size5,
.katex .sizing.reset-size1.size6,
.katex .sizing.reset-size1.size7,
.katex .sizing.reset-size1.size8,
.katex .sizing.reset-size1.size9,
.katex .sizing.reset-size1.size10,
.katex .sizing.reset-size1.size11,
.katex .sizing.reset-size2.size1,
.katex .sizing.reset-size2.size2,
.katex .sizing.reset-size2.size3,
.katex .sizing.reset-size2.size4,
.katex .sizing.reset-size2.size5,
.katex .sizing.reset-size2.size6,
.katex .sizing.reset-size2.size7,
.katex .sizing.reset-size2.size8,
.katex .sizing.reset-size2.size9,
.katex .sizing.reset-size2.size10,
.katex .sizing.reset-size2.size11,
.katex .sizing.reset-size3.size1,
.katex .sizing.reset-size3.size2,
.katex .sizing.reset-size3.size3,
.katex .sizing.reset-size3.size4,
.katex .sizing.reset-size3.size5,
.katex .sizing.reset-size3.size6,
.katex .sizing.reset-size3.size7,
.katex .sizing.reset-size3.size8,
.katex .sizing.reset-size3.size9,
.katex .sizing.reset-size3.size10,
.katex .sizing.reset-size3.size11,
.katex .sizing.reset-size4.size1,
.katex .sizing.reset-size4.size2,
.katex .sizing.reset-size4.size3,
.katex .sizing.reset-size4.size4,
.katex .sizing.reset-size4.size5,
.katex .sizing.reset-size4.size6,
.katex .sizing.reset-size4.size7,
.katex .sizing.reset-size4.size8,
.katex .sizing.reset-size4.size9,
.katex .sizing.reset-size4.size10,
.katex .sizing.reset-size4.size11,
.katex .sizing.reset-size5.size1,
.katex .sizing.reset-size5.size2,
.katex .sizing.reset-size5.size3,
.katex .sizing.reset-size5.size4,
.katex .sizing.reset-size5.size5,
.katex .sizing.reset-size5.size6,
.katex .sizing.reset-size5.size7,
.katex .sizing.reset-size5.size8,
.katex .sizing.reset-size5.size9,
.katex .sizing.reset-size5.size10,
.katex .sizing.reset-size5.size11,
.katex .sizing.reset-size6.size1,
.katex .sizing.reset-size6.size2,
.katex .sizing.reset-size6.size3,
.katex .sizing.reset-size6.size4,
.katex .sizing.reset-size6.size5,
.katex .sizing.reset-size6.size6,
.katex .sizing.reset-size6.size7,
.katex .sizing.reset-size6.size8,
.katex .sizing.reset-size6.size9,
.katex .sizing.reset-size6.size10,
.katex .sizing.reset-size6.size11,
.katex .sizing.reset-size7.size1,
.katex .sizing.reset-size7.size2,
.katex .sizing.reset-size7.size3,
.katex .sizing.reset-size7.size4,
.katex .sizing.reset-size7.size5,
.katex .sizing.reset-size7.size6,
.katex .sizing.reset-size7.size7,
.katex .sizing.reset-size7.size8,
.katex .sizing.reset-size7.size9,
.katex .sizing.reset-size7.size10,
.katex .sizing.reset-size7.size11,
.katex .sizing.reset-size8.size1,
.katex .sizing.reset-size8.size2,
.katex .sizing.reset-size8.size3,
.katex .sizing.reset-size8.size4,
.katex .sizing.reset-size8.size5,
.katex .sizing.reset-size8.size6,
.katex .sizing.reset-size8.size7,
.katex .sizing.reset-size8.size8,
.katex .sizing.reset-size8.size9,
.katex .sizing.reset-size8.size10,
.katex .sizing.reset-size8.size11,
.katex .sizing.reset-size9.size1,
.katex .sizing.reset-size9.size2,
.katex .sizing.reset-size9.size3,
.katex .sizing.reset-size9.size4,
.katex .sizing.reset-size9.size5,
.katex .sizing.reset-size9.size6,
.katex .sizing.reset-size9.size7,
.katex .sizing.reset-size9.size8,
.katex .sizing.reset-size9.size9,
.katex .sizing.reset-size9.size10,
.katex .sizing.reset-size9.size11,
.katex .sizing.reset-size10.size1,
.katex .sizing.reset-size10.size2,
.katex .sizing.reset-size10.size3,
.katex .sizing.reset-size10.size4,
.katex .sizing.reset-size10.size5,
.katex .sizing.reset-size10.size6,
.katex .sizing.reset-size10.size7,
.katex .sizing.reset-size10.size8,
.katex .sizing.reset-size10.size9,
.katex .sizing.reset-size10.size10,
.katex .sizing.reset-size10.size11,
.katex .sizing.reset-size11.size1,
.katex .sizing.reset-size11.size2,
.katex .sizing.reset-size11.size3,
.katex .sizing.reset-size11.size4,
.katex .sizing.reset-size11.size5,
.katex .sizing.reset-size11.size6,
.katex .sizing.reset-size11.size7,
.katex .sizing.reset-size11.size8,
.katex .sizing.reset-size11.size9,
.katex .sizing.reset-size11.size10,
.katex .sizing.reset-size11.size11,
.katex .delimsizing.size1 {
.katex .delimsizing.size2 {
.katex .delimsizing.size3 {
.katex .delimsizing.size4 {
.katex .delimsizing.mult .delim-size1 > span {
.katex .delimsizing.mult .delim-size4 > span {
.katex .nulldelimiter {
.katex .delimcenter {
.katex .op-symbol {
.katex .op-symbol.small-op {
.katex .op-symbol.large-op {
.katex .op-limits > .vlist-t {
.katex .accent > .vlist-t {
.katex .accent .accent-body {
.katex .accent .accent-body:not(.accent-full) {
.katex .overlay {
.katex .mtable .vertical-separator {
.katex .mtable .arraycolsep {
.katex .mtable .col-align-c > .vlist-t {
.katex .mtable .col-align-l > .vlist-t {
.katex .mtable .col-align-r > .vlist-t {
.katex .svg-align {
.katex svg {
.katex svg path {
.katex img {
.katex .stretchy {
.katex .stretchy::before,
.katex .hide-tail {
.katex .halfarrow-left {
.katex .halfarrow-right {
.katex .brace-left {
.katex .brace-center {
.katex .brace-right {
.katex .x-arrow-pad {
.katex .x-arrow,
.katex .boxpad {
.katex .fbox,
.katex .cancel-pad {
.katex .cancel-lap {
.katex .sout {
.katex-display {
.katex-display > .katex {
.katex-display > .katex > .katex-html {
.katex-display > .katex > .katex-html > .tag {
.katex-display.leqno > .katex > .katex-html > .tag {
.katex-display.fleqn > .katex {

================
File: docs/static/katex/katex.js
================
function __webpack_require__(moduleId)
⋮----
/******/
/******/
/******/ 	// Load entry module and return exports
/******/ 	return __webpack_require__(__webpack_require__.s = 1);
/******/ })
/************************************************************************/
/******/ ([
/* 0 */
/***/ (function(module, exports, __webpack_require__) {
// extracted by mini-css-extract-plugin
/***/ }),
/* 1 */
/***/ (function(module, __webpack_exports__, __webpack_require__) {
⋮----
function SourceLocation(lexer, start, end)
⋮----
function Token(text,
loc)
⋮----
/**
 * Escapes text to prevent scripting attacks.
 */
function utils_escape(text)
/**
 * Sometimes we want to pull out the innermost element of a group. In most
 * cases, this will just be the group itself, but when ordgroups and colors have
 * a single element, we want to pull that out.
 */
⋮----
function Settings(options)
⋮----
function Style(id, size, cramped)
⋮----
function scriptFromCodepoint(codepoint)
⋮----
function supportedCodepoint(codepoint)
⋮----
function DocumentFragment(children)
⋮----
var markup = ""; // Simply concatenate the markup for the children together.
⋮----
/**
   * Converts the math node into a string, similar to innerText. Applies to
   * MathDomNode's only.
   */
⋮----
// To avoid this, we would subclass documentFragment separately for
// MathML, but polyfills for subclassing is expensive per PR 1469.
// $FlowFixMe: Only works for ChildType = MathDomNode.
⋮----
// CONCATENATED MODULE: ./src/domTree.js
/**
 * These objects store the data about the DOM nodes we create, as well as some
 * extra data. They can then be transformed into real DOM nodes with the
 * `toNode` function or HTML markup using `toMarkup`. They are useful for both
 * storing extra properties on the nodes, as well as providing a way to easily
 * work with the DOM.
 *
 * Similar functions for working with MathML nodes exist in mathMLTree.js.
 *
 * TODO: refactor `span` and `anchor` into common superclass when
 * target environments support class inheritance
 */
/**
 * Create an HTML className based on a list of classes. In addition to joining
 * with spaces, we also remove empty classes.
 */
⋮----
var styles = ""; // Add the styles, after hyphenation
⋮----
} // Add the attributes
⋮----
markup += ">"; // Add the markup of the children, also as markup
⋮----
}; // Making the type below exact with all optional fields doesn't work due to
// - https://github.com/facebook/flow/issues/4582
// - https://github.com/facebook/flow/issues/5688
// However, since *all* fields are optional, $Shape<> works as suggested in 5688
// above.
// This type does not include all CSS properties. Additional properties should
// be added as needed.
/**
 * This node represents a span node, with a className, a list of children, and
 * an inline style. It also contains information about its height, depth, and
 * maxFontSize.
 *
 * Represents two types with different uses: SvgSpan to wrap an SVG and DomSpan
 * otherwise. This typesafety is important when HTML builders access a span's
 * children.
 */
⋮----
/*#__PURE__*/
⋮----
function Span(classes, children, options, style)
/**
   * Sets an arbitrary attribute on the span. Warning: use this wisely. Not
   * all browsers support attributes the same, and having too many custom
   * attributes is probably bad.
   */
⋮----
function Anchor(href, classes, children, options)
⋮----
function Img(src, alt, style)
⋮----
// 'ī': '\u0131\u0304',
⋮----
function SymbolNode(text, height, depth, italic, skew, width, classes, style)
⋮----
/**
 * SVG nodes are used to render stretchy wide elements.
 */
⋮----
/*#__PURE__*/
⋮----
function SvgNode(children, attributes)
⋮----
function PathNode(pathName, alternate)
⋮----
function LineNode(attributes)
⋮----
function assertSymbolDomNode(group)
function assertSpan(group)
⋮----
function setFontMetrics(fontName, metrics)
function getCharacterMetrics(character, font, mode)
⋮----
function getGlobalMetrics(size)
⋮----
function defineSymbol(mode, font, group, replace, name, acceptUnicodeChar)
⋮----
var _ch3 = letters.charAt(symbols_i3); // The hex numbers in the next line are a surrogate pair.
// 0xD835 is the high surrogate for all letters in the range we support.
// 0xDC00 is the low surrogate for bold A.
symbols_wideChar = String.fromCharCode(0xD835, 0xDC00 + symbols_i3); // A-Z a-z bold
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDC34 + symbols_i3); // A-Z a-z italic
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDC68 + symbols_i3); // A-Z a-z bold italic
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDD04 + symbols_i3); // A-Z a-z Fractur
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDDA0 + symbols_i3); // A-Z a-z sans-serif
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDDD4 + symbols_i3); // A-Z a-z sans bold
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDE08 + symbols_i3); // A-Z a-z sans italic
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDE70 + symbols_i3); // A-Z a-z monospace
⋮----
// KaTeX fonts have only capital letters for blackboard bold and script.
// See exception for k below.
symbols_wideChar = String.fromCharCode(0xD835, 0xDD38 + symbols_i3); // A-Z double struck
⋮----
symbols_wideChar = String.fromCharCode(0xD835, 0xDC9C + symbols_i3); // A-Z script
⋮----
} // TODO: Add bold script when it is supported by a KaTeX font.
} // "k" is the only double struck lower case letter in the KaTeX fonts.
⋮----
["", "", ""], // a-z script.  No font
["", "", ""], // A-Z bold script. No font
["", "", ""], // a-z bold script. No font
⋮----
["", "", ""], // A-Z bold Fraktur No font metrics
["", "", ""], // a-z bold Fraktur.   No font.
⋮----
["", "", ""], // A-Z bold italic sans. No font
["", "", ""], // a-z bold italic sans. No font
⋮----
["", "", ""], // 0-9 double-struck. No KaTeX font.
⋮----
// We don't support any wide characters outside 1D400–1D7FF.
⋮----
// CONCATENATED MODULE: ./src/Options.js
/**
 * This file contains information about the options that the Parser carries
 * around with it while parsing. Data is held in an `Options` object, and when
 * recursing, a new `Options` object can be created with the `.with*` and
 * `.reset` functions.
 */
var sizeStyleMap = [// Each element contains [textsize, scriptsize, scriptscriptsize].
// The size mappings are taken from TeX with \normalsize=10pt.
[1, 1, 1], // size1: [5, 5, 5]              \tiny
[2, 1, 1], // size2: [6, 5, 5]
[3, 1, 1], // size3: [7, 5, 5]              \scriptsize
[4, 2, 1], // size4: [8, 6, 5]              \footnotesize
[5, 2, 1], // size5: [9, 6, 5]              \small
[6, 3, 1], // size6: [10, 7, 5]             \normalsize
[7, 4, 2], // size7: [12, 8, 6]             \large
[8, 6, 3], // size8: [14.4, 10, 7]          \Large
[9, 7, 6], // size9: [17.28, 12, 10]        \LARGE
[10, 8, 7], // size10: [20.74, 14.4, 12]     \huge
⋮----
var sizeMultipliers = [// fontMetrics.js:getGlobalMetrics also uses size indexes, so if
// you change size indexes, change that function.
⋮----
}; // In these types, "" (empty string) means "no change".
⋮----
function Options(data)
/**
   * Returns a new options object with the same properties as "this".  Properties
   * from "extension" will be copied to the new options object.
   */
⋮----
/**
   * Creates a new options object with the given font weight
   */
⋮----
/**
   * Creates a new options object with the given font weight
   */
⋮----
/**
   * Return the CSS sizing classes required to switch from enclosing options
   * `oldOptions` to `this`. Returns an array of classes.
   */
⋮----
function defineFunction(_ref)
function defineFunctionBuilders(_ref2)
⋮----
function buildHTMLUnbreakable(children, options)
function buildHTML(tree, options)
function newDocumentFragment(children)
⋮----
function MathNode(type, children)
⋮----
/**
 * This node represents a piece of text.
 */
⋮----
/*#__PURE__*/
⋮----
function TextNode(text)
/**
   * Converts the text node into a DOM text node.
   */
⋮----
/**
   * Converts the text node into escaped HTML markup
   * (representing the text itself).
   */
⋮----
/**
   * Converts the text node into a string
   * (representing the text iteself).
   */
⋮----
/**
 * This node represents a space, but may render as <mspace.../> or as text,
 * depending on the width.
 */
⋮----
/*#__PURE__*/
⋮----
/**
   * Create a Space node with width given in CSS ems.
   */
function SpaceNode(width)
⋮----
this.width = width; // See https://www.w3.org/TR/2000/WD-MathML2-20000328/chapter6.html
// for a table of space-like characters.  We use Unicode
// representations instead of &LongNames; as it's not clear how to
// make the latter via document.createTextNode.
⋮----
function buildMathML(tree, texExpression, options, isDisplayMode, forMathmlOnly)
⋮----
function buildSvgSpan_()
⋮----
function assertNodeType(node, type)
function assertSymbolNodeType(node)
function checkSymbolNodeType(node)
⋮----
}).join("|")); // Accents
⋮----
function checkDelimiter(delim, context)
⋮----
function assertParsed(group)
⋮----
function defineEnvironment(_ref)
function getHLines(parser)
function parseArray(parser, _ref, style)
function dCellStyle(envName)
⋮----
function setHLinePos(hlinesInGap)
⋮----
// Find column alignment, column spacing, and  vertical lines.
⋮----
rowLines += hlines[_i2].length === 0 ? "none " // MathML accepts only a single line between rows. Read one element.
⋮----
function mclass_htmlBuilder(group, options)
function mclass_mathmlBuilder(group, options)
⋮----
// No alt given. Use the file name. Strip away the path.
⋮----
function sizingGroup(value, options, baseOptions)
⋮----
var node = tbArg.body[i]; // $FlowFixMe: Not every node type has a `text` property.
⋮----
// Figure out what style we're changing to.
⋮----
var className = regularSpace[group.text].className || ""; // Spaces are generated by adding an actual space. Each of these
// things has an entry in the symbols table, so these will be turned
// into appropriate outputs.
⋮----
var tokenRegexString = "(" + spaceRegexString + "+)|" + // whitespace
"([!-\\[\\]-\u2027\u202A-\uD7FF\uF900-\uFFFF]" + ( // single codepoint
combiningDiacriticalMarkString + "*") + // ...plus accents
"|[\uD800-\uDBFF][\uDC00-\uDFFF]" + ( // surrogate pair
combiningDiacriticalMarkString + "*") + // ...plus accents
⋮----
function Lexer(input, settings)
⋮----
function Namespace(builtins, globalMacros)
⋮----
function defineMacro(name, body)
⋮----
// Parse a number in the given base, starting with first `token`.
⋮----
}); // \newcommand{\macro}[args]{definition}
// \renewcommand{\macro}[args]{definition}
// TODO: Optional arguments: \newcommand{\macro}[args][default]{definition}
⋮----
// TODO: Should properly expand arg, e.g., ignore {}s
⋮----
} // Final arg is the expansion of the macro
⋮----
}); // terminal (console) tools
⋮----
var arg = context.consumeArgs(1)[0]; // eslint-disable-next-line no-console
⋮----
var arg = context.consumeArgs(1)[0]; // eslint-disable-next-line no-console
⋮----
var name = tok.text; // eslint-disable-next-line no-console
⋮----
}); //////////////////////////////////////////////////////////////////////
// Grouping
// \let\bgroup={ \let\egroup=}
⋮----
defineMacro("\\egroup", "}"); // Symbols from latex.ltx:
// \def\lq{`}
// \def\rq{'}
⋮----
function MacroExpander(input, settings, mode)
⋮----
/**
   * Returns the expanded macro as a reversed array of tokens and a macro
   * argument count.  Or returns `null` if no such macro.
   */
⋮----
// mainly checking for undefined here
⋮----
function Parser(input, settings)
⋮----
var nested = 0; // allow nested braces in raw string group
⋮----
/**
   * Parses a color description.
   */
⋮----
} // Switch mode back
⋮----
/**
   * Form ligature-like combinations of characters for text mode.
   * This includes inputs like "--", "---", "``" and "''".
   * The result will simply replace multiple textord nodes with a single
   * character in each value by a single textord node having multiple
   * characters in its value.  The representation is still ASCII source.
   * The group will be modified in place.
   */
⋮----
var a = group[i]; // $FlowFixMe: Not every node type has a `text` property.
⋮----
}; // KaTeX's styles don't work properly in quirks mode. Print out an error, and
// disable rendering.

================
File: docs/static/katex/katex.min.css
================
@font-face{font-family:KaTeX_AMS;src:url(fonts/KaTeX_AMS-Regular.woff2) format("woff2"),url(fonts/KaTeX_AMS-Regular.woff) format("woff"),url(fonts/KaTeX_AMS-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Caligraphic;src:url(fonts/KaTeX_Caligraphic-Bold.woff2) format("woff2"),url(fonts/KaTeX_Caligraphic-Bold.woff) format("woff"),url(fonts/KaTeX_Caligraphic-Bold.ttf) format("truetype");font-weight:700;font-style:normal}@font-face{font-family:KaTeX_Caligraphic;src:url(fonts/KaTeX_Caligraphic-Regular.woff2) format("woff2"),url(fonts/KaTeX_Caligraphic-Regular.woff) format("woff"),url(fonts/KaTeX_Caligraphic-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Fraktur;src:url(fonts/KaTeX_Fraktur-Bold.woff2) format("woff2"),url(fonts/KaTeX_Fraktur-Bold.woff) format("woff"),url(fonts/KaTeX_Fraktur-Bold.ttf) format("truetype");font-weight:700;font-style:normal}@font-face{font-family:KaTeX_Fraktur;src:url(fonts/KaTeX_Fraktur-Regular.woff2) format("woff2"),url(fonts/KaTeX_Fraktur-Regular.woff) format("woff"),url(fonts/KaTeX_Fraktur-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Main;src:url(fonts/KaTeX_Main-Bold.woff2) format("woff2"),url(fonts/KaTeX_Main-Bold.woff) format("woff"),url(fonts/KaTeX_Main-Bold.ttf) format("truetype");font-weight:700;font-style:normal}@font-face{font-family:KaTeX_Main;src:url(fonts/KaTeX_Main-BoldItalic.woff2) format("woff2"),url(fonts/KaTeX_Main-BoldItalic.woff) format("woff"),url(fonts/KaTeX_Main-BoldItalic.ttf) format("truetype");font-weight:700;font-style:italic}@font-face{font-family:KaTeX_Main;src:url(fonts/KaTeX_Main-Italic.woff2) format("woff2"),url(fonts/KaTeX_Main-Italic.woff) format("woff"),url(fonts/KaTeX_Main-Italic.ttf) format("truetype");font-weight:400;font-style:italic}@font-face{font-family:KaTeX_Main;src:url(fonts/KaTeX_Main-Regular.woff2) format("woff2"),url(fonts/KaTeX_Main-Regular.woff) format("woff"),url(fonts/KaTeX_Main-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Math;src:url(fonts/KaTeX_Math-BoldItalic.woff2) format("woff2"),url(fonts/KaTeX_Math-BoldItalic.woff) format("woff"),url(fonts/KaTeX_Math-BoldItalic.ttf) format("truetype");font-weight:700;font-style:italic}@font-face{font-family:KaTeX_Math;src:url(fonts/KaTeX_Math-Italic.woff2) format("woff2"),url(fonts/KaTeX_Math-Italic.woff) format("woff"),url(fonts/KaTeX_Math-Italic.ttf) format("truetype");font-weight:400;font-style:italic}@font-face{font-family:"KaTeX_SansSerif";src:url(fonts/KaTeX_SansSerif-Bold.woff2) format("woff2"),url(fonts/KaTeX_SansSerif-Bold.woff) format("woff"),url(fonts/KaTeX_SansSerif-Bold.ttf) format("truetype");font-weight:700;font-style:normal}@font-face{font-family:"KaTeX_SansSerif";src:url(fonts/KaTeX_SansSerif-Italic.woff2) format("woff2"),url(fonts/KaTeX_SansSerif-Italic.woff) format("woff"),url(fonts/KaTeX_SansSerif-Italic.ttf) format("truetype");font-weight:400;font-style:italic}@font-face{font-family:"KaTeX_SansSerif";src:url(fonts/KaTeX_SansSerif-Regular.woff2) format("woff2"),url(fonts/KaTeX_SansSerif-Regular.woff) format("woff"),url(fonts/KaTeX_SansSerif-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Script;src:url(fonts/KaTeX_Script-Regular.woff2) format("woff2"),url(fonts/KaTeX_Script-Regular.woff) format("woff"),url(fonts/KaTeX_Script-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Size1;src:url(fonts/KaTeX_Size1-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size1-Regular.woff) format("woff"),url(fonts/KaTeX_Size1-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Size2;src:url(fonts/KaTeX_Size2-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size2-Regular.woff) format("woff"),url(fonts/KaTeX_Size2-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Size3;src:url(fonts/KaTeX_Size3-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size3-Regular.woff) format("woff"),url(fonts/KaTeX_Size3-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Size4;src:url(fonts/KaTeX_Size4-Regular.woff2) format("woff2"),url(fonts/KaTeX_Size4-Regular.woff) format("woff"),url(fonts/KaTeX_Size4-Regular.ttf) format("truetype");font-weight:400;font-style:normal}@font-face{font-family:KaTeX_Typewriter;src:url(fonts/KaTeX_Typewriter-Regular.woff2) format("woff2"),url(fonts/KaTeX_Typewriter-Regular.woff) format("woff"),url(fonts/KaTeX_Typewriter-Regular.ttf) format("truetype");font-weight:400;font-style:normal}.katex{font:normal 1.21em KaTeX_Main,Times New Roman,serif;line-height:1.2;text-indent:0;text-rendering:auto;border-color:currentColor}.katex *{-ms-high-contrast-adjust:none!important}.katex .katex-version:after{content:"0.12.0"}.katex .katex-mathml{position:absolute;clip:rect(1px,1px,1px,1px);padding:0;border:0;height:1px;width:1px;overflow:hidden}.katex .katex-html>.newline{display:block}.katex .base{position:relative;white-space:nowrap;width:min-content}.katex .base,.katex .strut{display:inline-block}.katex .textbf{font-weight:700}.katex .textit{font-style:italic}.katex .textrm{font-family:KaTeX_Main}.katex .textsf{font-family:KaTeX_SansSerif}.katex .texttt{font-family:KaTeX_Typewriter}.katex .mathnormal{font-family:KaTeX_Math;font-style:italic}.katex .mathit{font-family:KaTeX_Main;font-style:italic}.katex .mathrm{font-style:normal}.katex .mathbf{font-family:KaTeX_Main;font-weight:700}.katex .boldsymbol{font-family:KaTeX_Math;font-weight:700;font-style:italic}.katex .amsrm,.katex .mathbb,.katex .textbb{font-family:KaTeX_AMS}.katex .mathcal{font-family:KaTeX_Caligraphic}.katex .mathfrak,.katex .textfrak{font-family:KaTeX_Fraktur}.katex .mathtt{font-family:KaTeX_Typewriter}.katex .mathscr,.katex .textscr{font-family:KaTeX_Script}.katex .mathsf,.katex .textsf{font-family:KaTeX_SansSerif}.katex .mathboldsf,.katex .textboldsf{font-family:KaTeX_SansSerif;font-weight:700}.katex .mathitsf,.katex .textitsf{font-family:KaTeX_SansSerif;font-style:italic}.katex .mainrm{font-family:KaTeX_Main;font-style:normal}.katex .vlist-t{display:inline-table;table-layout:fixed;border-collapse:collapse}.katex .vlist-r{display:table-row}.katex .vlist{display:table-cell;vertical-align:bottom;position:relative}.katex .vlist>span{display:block;height:0;position:relative}.katex .vlist>span>span{display:inline-block}.katex .vlist>span>.pstrut{overflow:hidden;width:0}.katex .vlist-t2{margin-right:-2px}.katex .vlist-s{display:table-cell;vertical-align:bottom;font-size:1px;width:2px;min-width:2px}.katex .vbox{-ms-flex-direction:column;flex-direction:column;align-items:baseline}.katex .hbox,.katex .vbox{display:-ms-inline-flexbox;display:inline-flex}.katex .hbox{-ms-flex-direction:row;flex-direction:row;width:100%}.katex .thinbox{display:inline-flex;flex-direction:row;width:0;max-width:0}.katex .msupsub{text-align:left}.katex .mfrac>span>span{text-align:center}.katex .mfrac .frac-line{display:inline-block;width:100%;border-bottom-style:solid}.katex .hdashline,.katex .hline,.katex .mfrac .frac-line,.katex .overline .overline-line,.katex .rule,.katex .underline .underline-line{min-height:1px}.katex .mspace{display:inline-block}.katex .clap,.katex .llap,.katex .rlap{width:0;position:relative}.katex .clap>.inner,.katex .llap>.inner,.katex .rlap>.inner{position:absolute}.katex .clap>.fix,.katex .llap>.fix,.katex .rlap>.fix{display:inline-block}.katex .llap>.inner{right:0}.katex .clap>.inner,.katex .rlap>.inner{left:0}.katex .clap>.inner>span{margin-left:-50%;margin-right:50%}.katex .rule{display:inline-block;border:0 solid;position:relative}.katex .hline,.katex .overline .overline-line,.katex .underline .underline-line{display:inline-block;width:100%;border-bottom-style:solid}.katex .hdashline{display:inline-block;width:100%;border-bottom-style:dashed}.katex .sqrt>.root{margin-left:.27777778em;margin-right:-.55555556em}.katex .fontsize-ensurer.reset-size1.size1,.katex .sizing.reset-size1.size1{font-size:1em}.katex .fontsize-ensurer.reset-size1.size2,.katex .sizing.reset-size1.size2{font-size:1.2em}.katex .fontsize-ensurer.reset-size1.size3,.katex .sizing.reset-size1.size3{font-size:1.4em}.katex .fontsize-ensurer.reset-size1.size4,.katex .sizing.reset-size1.size4{font-size:1.6em}.katex .fontsize-ensurer.reset-size1.size5,.katex .sizing.reset-size1.size5{font-size:1.8em}.katex .fontsize-ensurer.reset-size1.size6,.katex .sizing.reset-size1.size6{font-size:2em}.katex .fontsize-ensurer.reset-size1.size7,.katex .sizing.reset-size1.size7{font-size:2.4em}.katex .fontsize-ensurer.reset-size1.size8,.katex .sizing.reset-size1.size8{font-size:2.88em}.katex .fontsize-ensurer.reset-size1.size9,.katex .sizing.reset-size1.size9{font-size:3.456em}.katex .fontsize-ensurer.reset-size1.size10,.katex .sizing.reset-size1.size10{font-size:4.148em}.katex .fontsize-ensurer.reset-size1.size11,.katex .sizing.reset-size1.size11{font-size:4.976em}.katex .fontsize-ensurer.reset-size2.size1,.katex .sizing.reset-size2.size1{font-size:.83333333em}.katex .fontsize-ensurer.reset-size2.size2,.katex .sizing.reset-size2.size2{font-size:1em}.katex .fontsize-ensurer.reset-size2.size3,.katex .sizing.reset-size2.size3{font-size:1.16666667em}.katex .fontsize-ensurer.reset-size2.size4,.katex .sizing.reset-size2.size4{font-size:1.33333333em}.katex .fontsize-ensurer.reset-size2.size5,.katex .sizing.reset-size2.size5{font-size:1.5em}.katex .fontsize-ensurer.reset-size2.size6,.katex .sizing.reset-size2.size6{font-size:1.66666667em}.katex .fontsize-ensurer.reset-size2.size7,.katex .sizing.reset-size2.size7{font-size:2em}.katex .fontsize-ensurer.reset-size2.size8,.katex .sizing.reset-size2.size8{font-size:2.4em}.katex .fontsize-ensurer.reset-size2.size9,.katex .sizing.reset-size2.size9{font-size:2.88em}.katex .fontsize-ensurer.reset-size2.size10,.katex .sizing.reset-size2.size10{font-size:3.45666667em}.katex .fontsize-ensurer.reset-size2.size11,.katex .sizing.reset-size2.size11{font-size:4.14666667em}.katex .fontsize-ensurer.reset-size3.size1,.katex .sizing.reset-size3.size1{font-size:.71428571em}.katex .fontsize-ensurer.reset-size3.size2,.katex .sizing.reset-size3.size2{font-size:.85714286em}.katex .fontsize-ensurer.reset-size3.size3,.katex .sizing.reset-size3.size3{font-size:1em}.katex .fontsize-ensurer.reset-size3.size4,.katex .sizing.reset-size3.size4{font-size:1.14285714em}.katex .fontsize-ensurer.reset-size3.size5,.katex .sizing.reset-size3.size5{font-size:1.28571429em}.katex .fontsize-ensurer.reset-size3.size6,.katex .sizing.reset-size3.size6{font-size:1.42857143em}.katex .fontsize-ensurer.reset-size3.size7,.katex .sizing.reset-size3.size7{font-size:1.71428571em}.katex .fontsize-ensurer.reset-size3.size8,.katex .sizing.reset-size3.size8{font-size:2.05714286em}.katex .fontsize-ensurer.reset-size3.size9,.katex .sizing.reset-size3.size9{font-size:2.46857143em}.katex .fontsize-ensurer.reset-size3.size10,.katex .sizing.reset-size3.size10{font-size:2.96285714em}.katex .fontsize-ensurer.reset-size3.size11,.katex .sizing.reset-size3.size11{font-size:3.55428571em}.katex .fontsize-ensurer.reset-size4.size1,.katex .sizing.reset-size4.size1{font-size:.625em}.katex .fontsize-ensurer.reset-size4.size2,.katex .sizing.reset-size4.size2{font-size:.75em}.katex .fontsize-ensurer.reset-size4.size3,.katex .sizing.reset-size4.size3{font-size:.875em}.katex .fontsize-ensurer.reset-size4.size4,.katex .sizing.reset-size4.size4{font-size:1em}.katex .fontsize-ensurer.reset-size4.size5,.katex .sizing.reset-size4.size5{font-size:1.125em}.katex .fontsize-ensurer.reset-size4.size6,.katex .sizing.reset-size4.size6{font-size:1.25em}.katex .fontsize-ensurer.reset-size4.size7,.katex .sizing.reset-size4.size7{font-size:1.5em}.katex .fontsize-ensurer.reset-size4.size8,.katex .sizing.reset-size4.size8{font-size:1.8em}.katex .fontsize-ensurer.reset-size4.size9,.katex .sizing.reset-size4.size9{font-size:2.16em}.katex .fontsize-ensurer.reset-size4.size10,.katex .sizing.reset-size4.size10{font-size:2.5925em}.katex .fontsize-ensurer.reset-size4.size11,.katex .sizing.reset-size4.size11{font-size:3.11em}.katex .fontsize-ensurer.reset-size5.size1,.katex .sizing.reset-size5.size1{font-size:.55555556em}.katex .fontsize-ensurer.reset-size5.size2,.katex .sizing.reset-size5.size2{font-size:.66666667em}.katex .fontsize-ensurer.reset-size5.size3,.katex .sizing.reset-size5.size3{font-size:.77777778em}.katex .fontsize-ensurer.reset-size5.size4,.katex .sizing.reset-size5.size4{font-size:.88888889em}.katex .fontsize-ensurer.reset-size5.size5,.katex .sizing.reset-size5.size5{font-size:1em}.katex .fontsize-ensurer.reset-size5.size6,.katex .sizing.reset-size5.size6{font-size:1.11111111em}.katex .fontsize-ensurer.reset-size5.size7,.katex .sizing.reset-size5.size7{font-size:1.33333333em}.katex .fontsize-ensurer.reset-size5.size8,.katex .sizing.reset-size5.size8{font-size:1.6em}.katex .fontsize-ensurer.reset-size5.size9,.katex .sizing.reset-size5.size9{font-size:1.92em}.katex .fontsize-ensurer.reset-size5.size10,.katex .sizing.reset-size5.size10{font-size:2.30444444em}.katex .fontsize-ensurer.reset-size5.size11,.katex .sizing.reset-size5.size11{font-size:2.76444444em}.katex .fontsize-ensurer.reset-size6.size1,.katex .sizing.reset-size6.size1{font-size:.5em}.katex .fontsize-ensurer.reset-size6.size2,.katex .sizing.reset-size6.size2{font-size:.6em}.katex .fontsize-ensurer.reset-size6.size3,.katex .sizing.reset-size6.size3{font-size:.7em}.katex .fontsize-ensurer.reset-size6.size4,.katex .sizing.reset-size6.size4{font-size:.8em}.katex .fontsize-ensurer.reset-size6.size5,.katex .sizing.reset-size6.size5{font-size:.9em}.katex .fontsize-ensurer.reset-size6.size6,.katex .sizing.reset-size6.size6{font-size:1em}.katex .fontsize-ensurer.reset-size6.size7,.katex .sizing.reset-size6.size7{font-size:1.2em}.katex .fontsize-ensurer.reset-size6.size8,.katex .sizing.reset-size6.size8{font-size:1.44em}.katex .fontsize-ensurer.reset-size6.size9,.katex .sizing.reset-size6.size9{font-size:1.728em}.katex .fontsize-ensurer.reset-size6.size10,.katex .sizing.reset-size6.size10{font-size:2.074em}.katex .fontsize-ensurer.reset-size6.size11,.katex .sizing.reset-size6.size11{font-size:2.488em}.katex .fontsize-ensurer.reset-size7.size1,.katex .sizing.reset-size7.size1{font-size:.41666667em}.katex .fontsize-ensurer.reset-size7.size2,.katex .sizing.reset-size7.size2{font-size:.5em}.katex .fontsize-ensurer.reset-size7.size3,.katex .sizing.reset-size7.size3{font-size:.58333333em}.katex .fontsize-ensurer.reset-size7.size4,.katex .sizing.reset-size7.size4{font-size:.66666667em}.katex .fontsize-ensurer.reset-size7.size5,.katex .sizing.reset-size7.size5{font-size:.75em}.katex .fontsize-ensurer.reset-size7.size6,.katex .sizing.reset-size7.size6{font-size:.83333333em}.katex .fontsize-ensurer.reset-size7.size7,.katex .sizing.reset-size7.size7{font-size:1em}.katex .fontsize-ensurer.reset-size7.size8,.katex .sizing.reset-size7.size8{font-size:1.2em}.katex .fontsize-ensurer.reset-size7.size9,.katex .sizing.reset-size7.size9{font-size:1.44em}.katex .fontsize-ensurer.reset-size7.size10,.katex .sizing.reset-size7.size10{font-size:1.72833333em}.katex .fontsize-ensurer.reset-size7.size11,.katex .sizing.reset-size7.size11{font-size:2.07333333em}.katex .fontsize-ensurer.reset-size8.size1,.katex .sizing.reset-size8.size1{font-size:.34722222em}.katex .fontsize-ensurer.reset-size8.size2,.katex .sizing.reset-size8.size2{font-size:.41666667em}.katex .fontsize-ensurer.reset-size8.size3,.katex .sizing.reset-size8.size3{font-size:.48611111em}.katex .fontsize-ensurer.reset-size8.size4,.katex .sizing.reset-size8.size4{font-size:.55555556em}.katex .fontsize-ensurer.reset-size8.size5,.katex .sizing.reset-size8.size5{font-size:.625em}.katex .fontsize-ensurer.reset-size8.size6,.katex .sizing.reset-size8.size6{font-size:.69444444em}.katex .fontsize-ensurer.reset-size8.size7,.katex .sizing.reset-size8.size7{font-size:.83333333em}.katex .fontsize-ensurer.reset-size8.size8,.katex .sizing.reset-size8.size8{font-size:1em}.katex .fontsize-ensurer.reset-size8.size9,.katex .sizing.reset-size8.size9{font-size:1.2em}.katex .fontsize-ensurer.reset-size8.size10,.katex .sizing.reset-size8.size10{font-size:1.44027778em}.katex .fontsize-ensurer.reset-size8.size11,.katex .sizing.reset-size8.size11{font-size:1.72777778em}.katex .fontsize-ensurer.reset-size9.size1,.katex .sizing.reset-size9.size1{font-size:.28935185em}.katex .fontsize-ensurer.reset-size9.size2,.katex .sizing.reset-size9.size2{font-size:.34722222em}.katex .fontsize-ensurer.reset-size9.size3,.katex .sizing.reset-size9.size3{font-size:.40509259em}.katex .fontsize-ensurer.reset-size9.size4,.katex .sizing.reset-size9.size4{font-size:.46296296em}.katex .fontsize-ensurer.reset-size9.size5,.katex .sizing.reset-size9.size5{font-size:.52083333em}.katex .fontsize-ensurer.reset-size9.size6,.katex .sizing.reset-size9.size6{font-size:.5787037em}.katex .fontsize-ensurer.reset-size9.size7,.katex .sizing.reset-size9.size7{font-size:.69444444em}.katex .fontsize-ensurer.reset-size9.size8,.katex .sizing.reset-size9.size8{font-size:.83333333em}.katex .fontsize-ensurer.reset-size9.size9,.katex .sizing.reset-size9.size9{font-size:1em}.katex .fontsize-ensurer.reset-size9.size10,.katex .sizing.reset-size9.size10{font-size:1.20023148em}.katex .fontsize-ensurer.reset-size9.size11,.katex .sizing.reset-size9.size11{font-size:1.43981481em}.katex .fontsize-ensurer.reset-size10.size1,.katex .sizing.reset-size10.size1{font-size:.24108004em}.katex .fontsize-ensurer.reset-size10.size2,.katex .sizing.reset-size10.size2{font-size:.28929605em}.katex .fontsize-ensurer.reset-size10.size3,.katex .sizing.reset-size10.size3{font-size:.33751205em}.katex .fontsize-ensurer.reset-size10.size4,.katex .sizing.reset-size10.size4{font-size:.38572806em}.katex .fontsize-ensurer.reset-size10.size5,.katex .sizing.reset-size10.size5{font-size:.43394407em}.katex .fontsize-ensurer.reset-size10.size6,.katex .sizing.reset-size10.size6{font-size:.48216008em}.katex .fontsize-ensurer.reset-size10.size7,.katex .sizing.reset-size10.size7{font-size:.57859209em}.katex .fontsize-ensurer.reset-size10.size8,.katex .sizing.reset-size10.size8{font-size:.69431051em}.katex .fontsize-ensurer.reset-size10.size9,.katex .sizing.reset-size10.size9{font-size:.83317261em}.katex .fontsize-ensurer.reset-size10.size10,.katex .sizing.reset-size10.size10{font-size:1em}.katex .fontsize-ensurer.reset-size10.size11,.katex .sizing.reset-size10.size11{font-size:1.19961427em}.katex .fontsize-ensurer.reset-size11.size1,.katex .sizing.reset-size11.size1{font-size:.20096463em}.katex .fontsize-ensurer.reset-size11.size2,.katex .sizing.reset-size11.size2{font-size:.24115756em}.katex .fontsize-ensurer.reset-size11.size3,.katex .sizing.reset-size11.size3{font-size:.28135048em}.katex .fontsize-ensurer.reset-size11.size4,.katex .sizing.reset-size11.size4{font-size:.32154341em}.katex .fontsize-ensurer.reset-size11.size5,.katex .sizing.reset-size11.size5{font-size:.36173633em}.katex .fontsize-ensurer.reset-size11.size6,.katex .sizing.reset-size11.size6{font-size:.40192926em}.katex .fontsize-ensurer.reset-size11.size7,.katex .sizing.reset-size11.size7{font-size:.48231511em}.katex .fontsize-ensurer.reset-size11.size8,.katex .sizing.reset-size11.size8{font-size:.57877814em}.katex .fontsize-ensurer.reset-size11.size9,.katex .sizing.reset-size11.size9{font-size:.69453376em}.katex .fontsize-ensurer.reset-size11.size10,.katex .sizing.reset-size11.size10{font-size:.83360129em}.katex .fontsize-ensurer.reset-size11.size11,.katex .sizing.reset-size11.size11{font-size:1em}.katex .delimsizing.size1{font-family:KaTeX_Size1}.katex .delimsizing.size2{font-family:KaTeX_Size2}.katex .delimsizing.size3{font-family:KaTeX_Size3}.katex .delimsizing.size4{font-family:KaTeX_Size4}.katex .delimsizing.mult .delim-size1>span{font-family:KaTeX_Size1}.katex .delimsizing.mult .delim-size4>span{font-family:KaTeX_Size4}.katex .nulldelimiter{display:inline-block;width:.12em}.katex .delimcenter,.katex .op-symbol{position:relative}.katex .op-symbol.small-op{font-family:KaTeX_Size1}.katex .op-symbol.large-op{font-family:KaTeX_Size2}.katex .op-limits>.vlist-t{text-align:center}.katex .accent>.vlist-t{text-align:center}.katex .accent .accent-body{position:relative}.katex .accent .accent-body:not(.accent-full){width:0}.katex .overlay{display:block}.katex .mtable .vertical-separator{display:inline-block;min-width:1px}.katex .mtable .arraycolsep{display:inline-block}.katex .mtable .col-align-c>.vlist-t{text-align:center}.katex .mtable .col-align-l>.vlist-t{text-align:left}.katex .mtable .col-align-r>.vlist-t{text-align:right}.katex .svg-align{text-align:left}.katex svg{display:block;position:absolute;width:100%;height:inherit;fill:currentColor;stroke:currentColor;fill-rule:nonzero;fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1}.katex svg path{stroke:none}.katex img{border-style:none;min-width:0;min-height:0;max-width:none;max-height:none}.katex .stretchy{width:100%;display:block;position:relative;overflow:hidden}.katex .stretchy:after,.katex .stretchy:before{content:""}.katex .hide-tail{width:100%;position:relative;overflow:hidden}.katex .halfarrow-left{position:absolute;left:0;width:50.2%;overflow:hidden}.katex .halfarrow-right{position:absolute;right:0;width:50.2%;overflow:hidden}.katex .brace-left{position:absolute;left:0;width:25.1%;overflow:hidden}.katex .brace-center{position:absolute;left:25%;width:50%;overflow:hidden}.katex .brace-right{position:absolute;right:0;width:25.1%;overflow:hidden}.katex .x-arrow-pad{padding:0 .5em}.katex .mover,.katex .munder,.katex .x-arrow{text-align:center}.katex .boxpad{padding:0 .3em}.katex .fbox,.katex .fcolorbox{box-sizing:border-box;border:.04em solid}.katex .cancel-pad{padding:0 .2em}.katex .cancel-lap{margin-left:-.2em;margin-right:-.2em}.katex .sout{border-bottom-style:solid;border-bottom-width:.08em}.katex-display{display:block;margin:1em 0;text-align:center}.katex-display>.katex{display:block;text-align:center;white-space:nowrap}.katex-display>.katex>.katex-html{display:block;position:relative}.katex-display>.katex>.katex-html>.tag{position:absolute;right:0}.katex-display.leqno>.katex>.katex-html>.tag{left:0;right:auto}.katex-display.fleqn>.katex{text-align:left;padding-left:2em}

================
File: docs/static/katex/katex.min.js
================
!function(t,e)

================
File: docs/static/katex/katex.mjs
================
/**
 * Lexing or parsing positional information for error reporting.
 * This object is immutable.
 */
class SourceLocation
⋮----
// The + prefix indicates that these fields aren't writeable
// Lexer holding the input string.
// Start offset, zero-based inclusive.
// End offset, zero-based exclusive.
⋮----
/**
   * Merges two `SourceLocation`s from location providers, given they are
   * provided in order of appearance.
   * - Returns the first one's location if only the first is provided.
   * - Returns a merged range of the first and the last if both are provided
   *   and their lexers match.
   * - Otherwise, returns null.
   */
⋮----
static range(first, second)
⋮----
/**
 * Interface required to break circular dependency between Token, Lexer, and
 * ParseError.
 */
⋮----
/**
 * The resulting token returned from `lex`.
 *
 * It consists of the token text plus some position information.
 * The position information is essentially a range in an input string,
 * but instead of referencing the bare input string, we refer to the lexer.
 * That way it is possible to attach extra metadata to the input string,
 * like for example a file name or similar.
 *
 * The position information is optional, so it is OK to construct synthetic
 * tokens if appropriate. Not providing available position information may
 * lead to degraded error reporting, though.
 */
class Token
⋮----
// don't expand the token
// used in \noexpand
constructor(text, // the text of this token
⋮----
/**
   * Given a pair of tokens (this and endToken), compute a `Token` encompassing
   * the whole input range enclosed by these two.
   */
⋮----
range(endToken, // last token of the range, inclusive
  text) // the text of the newly constructed token
  {
    return new Token(text, SourceLocation.range(this, endToken));
⋮----
text) // the text of the newly constructed token
⋮----
/**
 * This is the ParseError class, which is the main error thrown by KaTeX
 * functions when something has gone wrong. This is used to distinguish internal
 * errors from errors in the expression that the user provided.
 *
 * If possible, a caller should provide a Token or ParseNode with information
 * about where in the source string the problem occurred.
 */
class ParseError
⋮----
// Error position based on passed-in Token or ParseNode.
constructor(message, // The error message
token) // An object providing position information
⋮----
// If we have the input and a position, make the error a bit fancier
// Get the input
const input = loc.lexer.input; // Prepend some information
⋮----
} // Underline token in question using combining underscores
⋮----
const underlined = input.slice(start, end).replace(/[^]/g, "$&\u0332"); // Extract some context from the input and add it to the error
⋮----
} // Some hackery to make ParseError a prototype of Error
// See http://stackoverflow.com/a/8460753
⋮----
self.name = "ParseError"; // $FlowFixMe
⋮----
self.__proto__ = ParseError.prototype; // $FlowFixMe
⋮----
} // $FlowFixMe More hackery
⋮----
/**
 * This file contains a list of utility functions which are useful in other
 * files.
 */
⋮----
/**
 * Return whether an element is contained in a list
 */
⋮----
/**
 * Provide a default value if a setting is undefined
 * NOTE: Couldn't use `T` as the output type due to facebook/flow#5022.
 */
⋮----
}; // hyphenate and escape adapted from Facebook's React under Apache 2 license
⋮----
/**
 * Escapes text to prevent scripting attacks.
 */
⋮----
function escape(text)
/**
 * Sometimes we want to pull out the innermost element of a group. In most
 * cases, this will just be the group itself, but when ordgroups and colors have
 * a single element, we want to pull that out.
 */
⋮----
/**
 * TeXbook algorithms often reference "character boxes", which are simply groups
 * with a single character in them. To decide if something is a character box,
 * we find its innermost group, and see if it is a single character.
 */
⋮----
const baseElem = getBaseElem(group); // These are all they types of groups which hold single characters
⋮----
/**
 * Return the protocol of a URL, or "_relative" if the URL does not specify a
 * protocol (and thus is relative).
 */
⋮----
/* eslint no-console:0 */
⋮----
/**
 * The main Settings object
 *
 * The current options stored are:
 *  - displayMode: Whether the expression should be typeset as inline math
 *                 (false, the default), meaning that the math starts in
 *                 \textstyle and is placed in an inline-block); or as display
 *                 math (true), meaning that the math starts in \displaystyle
 *                 and is placed in a block with vertical margin.
 */
class Settings
⋮----
// allow null options
⋮----
/**
   * Report nonstrict (non-LaTeX-compatible) input.
   * Can safely not be called if `this.strict` is false in JavaScript.
   */
⋮----
reportNonstrict(errorCode, errorMsg, token)
⋮----
// Allow return value of strict function to be boolean or string
// (or null/undefined, meaning no further processing).
⋮----
// won't happen in type-safe code
⋮----
/**
   * Check whether to apply strict (LaTeX-adhering) behavior for unusual
   * input (like `\\`).  Unlike `nonstrict`, will not throw an error;
   * instead, "error" translates to a return value of `true`, while "ignore"
   * translates to a return value of `false`.  May still print a warning:
   * "warn" prints a warning and returns `false`.
   * This is for the second category of `errorCode`s listed in the README.
   */
⋮----
useStrictBehavior(errorCode, errorMsg, token)
⋮----
// Allow return value of strict function to be boolean or string
// (or null/undefined, meaning no further processing).
// But catch any exceptions thrown by function, treating them
// like "error".
⋮----
// won't happen in type-safe code
⋮----
/**
   * Check whether to test potentially dangerous input, and return
   * `true` (trusted) or `false` (untrusted).  The sole argument `context`
   * should be an object with `command` field specifying the relevant LaTeX
   * command (as a string starting with `\`), and any other arguments, etc.
   * If `context` has a `url` field, a `protocol` field will automatically
   * get added by this function (changing the specified object).
   */
⋮----
isTrusted(context)
⋮----
/**
 * This file contains information and classes for the various kinds of styles
 * used in TeX. It provides a generic `Style` class, which holds information
 * about a specific style. It then provides instances of all the different kinds
 * of styles possible, and provides functions to move between them and get
 * information about them.
 */
⋮----
/**
 * The main style class. Contains a unique id for the style, a size (which is
 * the same for cramped and uncramped version of a style), and a cramped flag.
 */
class Style
⋮----
/**
   * Get the style of a superscript given a base in the current style.
   */
⋮----
sup()
/**
   * Get the style of a subscript given a base in the current style.
   */
⋮----
sub()
/**
   * Get the style of a fraction numerator given the fraction in the current
   * style.
   */
⋮----
fracNum()
/**
   * Get the style of a fraction denominator given the fraction in the current
   * style.
   */
⋮----
fracDen()
/**
   * Get the cramped version of a style (in particular, cramping a cramped style
   * doesn't change the style).
   */
⋮----
cramp()
/**
   * Get a text or display version of this style.
   */
⋮----
text()
/**
   * Return true if this style is tightly spaced (scriptstyle/scriptscriptstyle)
   */
⋮----
isTight()
⋮----
} // Export an interface for type checking, but don't expose the implementation.
// This way, no more styles can be generated.
⋮----
// IDs of the different styles
⋮----
const SSc = 7; // Instances of the different styles
⋮----
const styles = [new Style(D, 0, false), new Style(Dc, 0, true), new Style(T, 1, false), new Style(Tc, 1, true), new Style(S, 2, false), new Style(Sc, 2, true), new Style(SS, 3, false), new Style(SSc, 3, true)]; // Lookup tables for switching from one style to another
⋮----
const text = [D, Dc, T, Tc, T, Tc, T, Tc]; // We only export some of the styles.
⋮----
/*
 * This file defines the Unicode scripts and script families that we
 * support. To add new scripts or families, just add a new entry to the
 * scriptData array below. Adding scripts to the scriptData array allows
 * characters from that script to appear in \text{} environments.
 */
⋮----
/**
 * Each script or script family has a name and an array of blocks.
 * Each block is an array of two numbers which specify the start and
 * end points (inclusive) of a block of Unicode codepoints.
 */
⋮----
/**
 * Unicode block data for the families of scripts we support in \text{}.
 * Scripts only need to appear here if they do not have font metrics.
 */
⋮----
// Latin characters beyond the Latin-1 characters we have metrics for.
// Needed for Czech, Hungarian and Turkish text, for example.
⋮----
blocks: [[0x0100, 0x024f], // Latin Extended-A and Latin Extended-B
⋮----
// The Cyrillic script used by Russian and related languages.
// A Cyrillic subset used to be supported as explicitly defined
// symbols in symbols.js
⋮----
// The Brahmic scripts of South and Southeast Asia
// Devanagari (0900–097F)
// Bengali (0980–09FF)
// Gurmukhi (0A00–0A7F)
// Gujarati (0A80–0AFF)
// Oriya (0B00–0B7F)
// Tamil (0B80–0BFF)
// Telugu (0C00–0C7F)
// Kannada (0C80–0CFF)
// Malayalam (0D00–0D7F)
// Sinhala (0D80–0DFF)
// Thai (0E00–0E7F)
// Lao (0E80–0EFF)
// Tibetan (0F00–0FFF)
// Myanmar (1000–109F)
⋮----
// Chinese and Japanese.
// The "k" in cjk is for Korean, but we've separated Korean out
⋮----
blocks: [[0x3000, 0x30FF], // CJK symbols and punctuation, Hiragana, Katakana
[0x4E00, 0x9FAF], // CJK ideograms
⋮----
// Korean
⋮----
/**
 * Given a codepoint, return the name of the script or script family
 * it is from, or null if it is not part of a known block
 */
⋮----
function scriptFromCodepoint(codepoint)
/**
 * A flattened version of all the supported blocks in a single array.
 * This is an optimization to make supportedCodepoint() fast.
 */
⋮----
/**
 * Given a codepoint, return true if it falls within one of the
 * scripts or script families defined above and false otherwise.
 *
 * Micro benchmarks shows that this is faster than
 * /[\u3000-\u30FF\u4E00-\u9FAF\uFF00-\uFF60\uAC00-\uD7AF\u0900-\u109F]/.test()
 * in Firefox, Chrome and Node.
 */
⋮----
function supportedCodepoint(codepoint)
⋮----
/**
 * This file provides support to domTree.js and delimiter.js.
 * It's a storehouse of path geometry for SVG images.
 */
// In all paths below, the viewBox-to-em scale is 1000:1.
const hLinePad = 80; // padding above a sqrt viniculum. Prevents image cropping.
// The viniculum of a \sqrt can be made thicker by a KaTeX rendering option.
// Think of variable extraViniculum as two detours in the SVG path.
// The detour begins at the lower left of the area labeled extraViniculum below.
// The detour proceeds one extraViniculum distance up and slightly to the right,
// displacing the radiused corner between surd and viniculum. The radius is
// traversed as usual, then the detour resumes. It goes right, to the end of
// the very long viniculumn, then down one extraViniculum distance,
// after which it resumes regular path geometry for the radical.
⋮----
/*                                                  viniculum
                                                   /
         /▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒←extraViniculum
        / █████████████████████←0.04em (40 unit) std viniculum thickness
       / /
      / /
     / /\
    / / surd
*/
⋮----
// sqrtMain path geometry is from glyph U221A in the font KaTeX Main
⋮----
// size1 is from glyph U221A in the font KaTeX_Size1-Regular
⋮----
// size2 is from glyph U221A in the font KaTeX_Size2-Regular
⋮----
// size3 is from glyph U221A in the font KaTeX_Size3-Regular
⋮----
// size4 is from glyph U221A in the font KaTeX_Size4-Regular
⋮----
// sqrtTall is from glyph U23B7 in the font KaTeX_Size4-Regular
// One path edge has a variable length. It runs vertically from the viniculumn
// to a point near (14 units) the bottom of the surd. The viniculum
// is normally 40 units thick. So the length of the line in question is:
⋮----
extraViniculum = 1000 * extraViniculum; // Convert from document ems to viewBox.
⋮----
// Two paths that cover gaps in built-up parentheses.
⋮----
// The doubleleftarrow geometry is from glyph U+21D0 in the font KaTeX Main
⋮----
// doublerightarrow is from glyph U+21D2 in font KaTeX Main
⋮----
// leftarrow is from glyph U+2190 in font KaTeX Main
⋮----
// overbrace is from glyphs U+23A9/23A8/23A7 in font KaTeX_Size4-Regular
⋮----
// overgroup is from the MnSymbol package (public domain)
⋮----
// Harpoons are from glyph U+21BD in font KaTeX Main
⋮----
// hook is from glyph U+21A9 in font KaTeX Main
⋮----
// tofrom is from glyph U+21C4 in font KaTeX AMS Regular
⋮----
// twoheadleftarrow is from glyph U+219E in font KaTeX AMS Regular
⋮----
// tilde1 is a modified version of a glyph from the MnSymbol package
⋮----
// ditto tilde2, tilde3, & tilde4
⋮----
// vec is from glyph U+20D7 in font KaTeX Main
⋮----
// widehat1 is a modified version of a glyph from the MnSymbol package
⋮----
// ditto widehat2, widehat3, & widehat4
⋮----
// widecheck paths are all inverted versions of widehat
⋮----
// The next ten paths support reaction arrows from the mhchem package.
// Arrows for \ce{<-->} are offset from xAxis by 0.22ex, per mhchem in LaTeX
// baraboveleftarrow is mostly from from glyph U+2190 in font KaTeX Main
⋮----
// rightarrowabovebar is mostly from glyph U+2192, KaTeX Main
⋮----
// The short left harpoon has 0.5em (i.e. 500 units) kern on the left end.
// Ref from mhchem.sty: \rlap{\raisebox{-.22ex}{$\kern0.5em
⋮----
/**
 * This node represents a document fragment, which contains elements, but when
 * placed into the DOM doesn't have any representation itself. It only contains
 * children and doesn't have any DOM node properties.
 */
class DocumentFragment
⋮----
// HtmlDomNode
// Never used; needed for satisfying interface.
⋮----
hasClass(className)
/** Convert the fragment into a node. */
⋮----
toNode()
/** Convert the fragment into HTML markup. */
⋮----
toMarkup()
⋮----
let markup = ""; // Simply concatenate the markup for the children together.
⋮----
/**
   * Converts the math node into a string, similar to innerText. Applies to
   * MathDomNode's only.
   */
⋮----
toText()
⋮----
// To avoid this, we would subclass documentFragment separately for
// MathML, but polyfills for subclassing is expensive per PR 1469.
// $FlowFixMe: Only works for ChildType = MathDomNode.
const toText = child
⋮----
/**
 * These objects store the data about the DOM nodes we create, as well as some
 * extra data. They can then be transformed into real DOM nodes with the
 * `toNode` function or HTML markup using `toMarkup`. They are useful for both
 * storing extra properties on the nodes, as well as providing a way to easily
 * work with the DOM.
 *
 * Similar functions for working with MathML nodes exist in mathMLTree.js.
 *
 * TODO: refactor `span` and `anchor` into common superclass when
 * target environments support class inheritance
 */
⋮----
/**
 * Create an HTML className based on a list of classes. In addition to joining
 * with spaces, we also remove empty classes.
 */
⋮----
/**
 * Convert into an HTML node
 */
⋮----
const node = document.createElement(tagName); // Apply the class
⋮----
node.className = createClass(this.classes); // Apply inline styles
⋮----
// $FlowFixMe Flow doesn't seem to understand span.style's type.
⋮----
} // Apply attributes
⋮----
} // Append the children, also as HTML nodes
⋮----
/**
 * Convert into an HTML markup string
 */
⋮----
let markup = `<${tagName}`; // Add the class
⋮----
let styles = ""; // Add the styles, after hyphenation
⋮----
} // Add the attributes
⋮----
markup += ">"; // Add the markup of the children, also as markup
⋮----
}; // Making the type below exact with all optional fields doesn't work due to
// - https://github.com/facebook/flow/issues/4582
// - https://github.com/facebook/flow/issues/5688
// However, since *all* fields are optional, $Shape<> works as suggested in 5688
// above.
// This type does not include all CSS properties. Additional properties should
// be added as needed.
⋮----
/**
 * This node represents a span node, with a className, a list of children, and
 * an inline style. It also contains information about its height, depth, and
 * maxFontSize.
 *
 * Represents two types with different uses: SvgSpan to wrap an SVG and DomSpan
 * otherwise. This typesafety is important when HTML builders access a span's
 * children.
 */
class Span
⋮----
/**
   * Sets an arbitrary attribute on the span. Warning: use this wisely. Not
   * all browsers support attributes the same, and having too many custom
   * attributes is probably bad.
   */
⋮----
setAttribute(attribute, value)
⋮----
/**
 * This node represents an anchor (<a>) element with a hyperlink.  See `span`
 * for further details.
 */
⋮----
class Anchor
/**
 * This node represents an image embed (<img>) element.
 */
⋮----
class Img
⋮----
node.className = "mord"; // Apply inline styles
⋮----
// $FlowFixMe
⋮----
let markup = `<img  src='${this.src} 'alt='${this.alt}' `; // Add the styles, after hyphenation
⋮----
// 'ī': '\u0131\u0304', // enable when we add Extended Latin
⋮----
/**
 * A symbol node contains information about a single symbol. It either renders
 * to a single text node, or a span with a single text node in it, depending on
 * whether it has CSS classes, styles, or needs italic correction.
 */
⋮----
class SymbolNode
⋮----
this.maxFontSize = 0; // Mark text from non-Latin scripts with specific classes so that we
// can specify which fonts to use.  This allows us to render these
// characters with a serif font in situations where the browser would
// either default to a sans serif or render a placeholder character.
// We use CSS class names like cjk_fallback, hangul_fallback and
// brahmic_fallback. See ./unicodeScripts.js for the set of possible
// script names
⋮----
// add ī when we add Extended Latin
⋮----
/**
   * Creates a text node or span from a symbol node. Note that a span is only
   * created if it is needed.
   */
⋮----
span = span || document.createElement("span"); // $FlowFixMe Flow doesn't seem to understand span.style's type.
⋮----
/**
   * Creates markup for a symbol node.
   */
⋮----
// TODO(alpert): More duplication than I'd like from
// span.prototype.toMarkup and symbolNode.prototype.toNode...
⋮----
/**
 * SVG nodes are used to render stretchy wide elements.
 */
⋮----
class SvgNode
⋮----
const node = document.createElementNS(svgNS, "svg"); // Apply attributes
⋮----
let markup = "<svg"; // Apply attributes
⋮----
class PathNode
⋮----
this.alternate = alternate; // Used only for \sqrt
⋮----
class LineNode
⋮----
const node = document.createElementNS(svgNS, "line"); // Apply attributes
⋮----
function assertSymbolDomNode(group)
function assertSpan(group)
⋮----
// This file is GENERATED by buildMetrics.sh. DO NOT MODIFY.
⋮----
/**
 * This file contains metrics regarding fonts and individual symbols. The sigma
 * and xi variables, as well as the metricMap map contain data extracted from
 * TeX, TeX font metrics, and the TTF files. These data are then exposed via the
 * `metrics` variable and the getCharacterMetrics function.
 */
// In TeX, there are actually three sets of dimensions, one for each of
// textstyle (size index 5 and higher: >=9pt), scriptstyle (size index 3 and 4:
// 7-8pt), and scriptscriptstyle (size index 1 and 2: 5-6pt).  These are
// provided in the arrays below, in that order.
//
// The font metrics are stored in fonts cmsy10, cmsy7, and cmsy5 respectively.
// This was determined by running the following script:
//
//     latex -interaction=nonstopmode \
//     '\documentclass{article}\usepackage{amsmath}\begin{document}' \
//     '$a$ \expandafter\show\the\textfont2' \
//     '\expandafter\show\the\scriptfont2' \
//     '\expandafter\show\the\scriptscriptfont2' \
//     '\stop'
//
// The metrics themselves were retrieved using the following commands:
//
//     tftopl cmsy10
//     tftopl cmsy7
//     tftopl cmsy5
//
// The output of each of these commands is quite lengthy.  The only part we
// care about is the FONTDIMEN section. Each value is measured in EMs.
⋮----
// sigma1
⋮----
// sigma2
⋮----
// sigma3
⋮----
// sigma4
⋮----
// sigma5
⋮----
// sigma6
⋮----
// sigma7
⋮----
// sigma8
⋮----
// sigma9
⋮----
// sigma10
⋮----
// sigma11
⋮----
// sigma12
⋮----
// sigma13
⋮----
// sigma14
⋮----
// sigma15
⋮----
// sigma16
⋮----
// sigma17
⋮----
// sigma18
⋮----
// sigma19
⋮----
// sigma20
⋮----
// sigma21
⋮----
// sigma22
// These font metrics are extracted from TeX by using tftopl on cmex10.tfm;
// they correspond to the font parameters of the extension fonts (family 3).
// See the TeXbook, page 441. In AMSTeX, the extension fonts scale; to
// match cmex7, we'd use cmex7.tfm values for script and scriptscript
// values.
⋮----
// xi8; cmex7: 0.049
⋮----
// xi9
⋮----
// xi10
⋮----
// xi11
⋮----
// xi12; cmex7: 0.611
⋮----
// xi13; cmex7: 0.143
// The \sqrt rule width is taken from the height of the surd character.
// Since we use the same font at all sizes, this thickness doesn't scale.
⋮----
// This value determines how large a pt is, for metrics which are defined
// in terms of pts.
// This value is also used in katex.less; if you change it make sure the
// values match.
⋮----
// The space between adjacent `|` columns in an array definition. From
// `\showthe\doublerulesep` in LaTeX. Equals 2.0 / ptPerEm.
⋮----
// The width of separator lines in {array} environments. From
// `\showthe\arrayrulewidth` in LaTeX. Equals 0.4 / ptPerEm.
⋮----
// Two values from LaTeX source2e:
⋮----
//        3 pt / ptPerEm
fboxrule: [0.04, 0.04, 0.04] // 0.4 pt / ptPerEm
⋮----
}; // This map contains a mapping from font name and character code to character
// should have Latin-1 and Cyrillic characters, but may not depending on the
// operating system.  The metrics do not account for extra height from the
// accents.  In the case of Cyrillic characters which have both ascenders and
// descenders we prefer approximations with ascenders, primarily to prevent
// the fraction bar or root line from intersecting the glyph.
// TODO(kevinb) allow union of multiple glyph metrics for better accuracy.
⋮----
// Latin-1
⋮----
// Cyrillic
⋮----
/**
 * This function adds new font metrics to default metricMap
 * It can also override existing metrics
 */
function setFontMetrics(fontName, metrics)
/**
 * This function is a convenience function for looking up information in the
 * metricMap table. It takes a character as a string, and a font.
 *
 * Note: the `width` property may be undefined if fontMetricsData.js wasn't
 * built using `Make extended_metrics`.
 */
⋮----
function getCharacterMetrics(character, font, mode)
⋮----
// We don't typically have font metrics for Asian scripts.
// But since we support them in text mode, we need to return
// some sort of metrics.
// So if the character is in a script we support but we
// don't have metrics for it, just use the metrics for
// the Latin capital letter M. This is close enough because
// we (currently) only care about the height of the glyph
// not its width.
⋮----
metrics = metricMap[font][77]; // 77 is the charcode for 'M'
⋮----
/**
 * Get the font metrics for a given size.
 */
⋮----
function getGlobalMetrics(size)
⋮----
/**
 * This file holds a list of all no-argument functions and single-character
 * symbols (like 'a' or ';').
 *
 * For each of the symbols, there are three properties they can have:
 * - font (required): the font to be used for this symbol. Either "main" (the
     normal font), or "ams" (the ams fonts).
 * - group (required): the ParseNode group type the symbol should have (i.e.
     "textord", "mathord", etc).
     See https://github.com/KaTeX/KaTeX/wiki/Examining-TeX#group-types
 * - replace: the character that this symbol or function should be
 *   replaced with (i.e. "\phi" has a replace value of "\u03d5", the phi
 *   character in the main font).
 *
 * The outermost map in the table indicates what mode the symbols should be
 * accepted in (e.g. "math" or "text").
 */
// Some of these have a "-token" suffix since these are also used as `ParseNode`
// types for raw text tokens, and we want to avoid conflicts with higher-level
// `ParseNode` types. These `ParseNode`s are constructed within `Parser` by
// looking up the `symbols` map.
⋮----
/** `acceptUnicodeChar = true` is only applicable if `replace` is set. */
⋮----
function defineSymbol(mode, font, group, replace, name, acceptUnicodeChar)
⋮----
} // Some abbreviations for commonly used strings.
// This helps minify the code, and also spotting typos using jshint.
// modes:
⋮----
const text$1 = "text"; // fonts:
⋮----
const ams = "ams"; // groups:
⋮----
const textord = "textord"; // Now comes the symbol table
// Relation Symbols
⋮----
defineSymbol(math, main, rel, "\u220b", "\\owns"); // Punctuation
⋮----
defineSymbol(math, main, punct, "\u22c5", "\\cdotp"); // Misc Symbols
⋮----
defineSymbol(text$1, main, textord, "\u00b6", "\\P", true); // Math and Text
⋮----
defineSymbol(text$1, main, textord, "\u2021", "\\textdaggerdbl"); // Large Delimiters
⋮----
defineSymbol(math, main, open, "\u27ee", "\\lgroup", true); // Binary Operators
⋮----
defineSymbol(math, main, bin, "\u0026", "\\And"); // from amsmath
// Arrow Symbols
⋮----
defineSymbol(math, main, rel, "\u21cc", "\\rightleftharpoons", true); // AMS Negated Binary Relations
⋮----
defineSymbol(math, ams, rel, "\u226e", "\\nless", true); // Symbol names proceeded by "@" each have a corresponding macro.
⋮----
defineSymbol(math, ams, rel, "\u2280", "\\nprec", true); // unicode-math maps \u22e0 to \npreccurlyeq. We'll use the AMS synonym.
⋮----
defineSymbol(math, ams, rel, "\u2281", "\\nsucc", true); // unicode-math maps \u22e1 to \nsucccurlyeq. We'll use the AMS synonym.
⋮----
defineSymbol(math, ams, rel, "\u2aba", "\\succnapprox", true); // unicode-math maps \u2246 to \simneqq. We'll use the AMS synonym.
⋮----
defineSymbol(math, ams, bin, "\u22b5", "\\unrhd"); // AMS Negated Arrows
⋮----
defineSymbol(math, ams, rel, "\u21ce", "\\nLeftrightarrow", true); // AMS Misc
⋮----
defineSymbol(math, ams, textord, "\u2201", "\\complement", true); // unicode-math maps U+F0 to \matheth. We map to AMS function \eth
⋮----
defineSymbol(math, ams, textord, "\u25ca", "\\Diamond"); // unicode-math maps U+A5 to \mathyen. We map to AMS function \yen
⋮----
defineSymbol(text$1, ams, textord, "\u2713", "\\checkmark"); // AMS Hebrew
⋮----
defineSymbol(math, ams, textord, "\u2137", "\\gimel", true); // AMS Greek
⋮----
defineSymbol(math, ams, textord, "\u03f0", "\\varkappa"); // AMS Delimiters
⋮----
defineSymbol(math, ams, close, "\u2518", "\\@lrcorner", true); // AMS Binary Relations
⋮----
defineSymbol(math, ams, rel, "\u25c0", "\\blacktriangleleft"); // unicode-math says that \therefore is a mathord atom.
// We kept the amssymb atom type, which is rel.
⋮----
defineSymbol(math, ams, rel, "\u25b6", "\\blacktriangleright"); // unicode-math says that \because is a mathord atom.
// We kept the amssymb atom type, which is rel.
⋮----
defineSymbol(math, ams, rel, "\u2251", "\\Doteq", true); // AMS Binary Operators
⋮----
defineSymbol(math, ams, bin, "\u22a0", "\\boxtimes", true); // AMS Arrows
// Note: unicode-math maps \u21e2 to their own function \rightdasharrow.
// We'll map it to AMS function \dashrightarrow. It produces the same atom.
⋮----
defineSymbol(math, ams, rel, "\u21e2", "\\dashrightarrow", true); // unicode-math maps \u21e0 to \leftdasharrow. We'll use the AMS synonym.
⋮----
defineSymbol(math, ams, rel, "\u21b6", "\\curvearrowleft", true); // unicode-math maps \u21ba to \acwopencirclearrow. We'll use the AMS synonym.
⋮----
defineSymbol(math, ams, rel, "\u21b7", "\\curvearrowright", true); // unicode-math maps \u21bb to \cwopencirclearrow. We'll use the AMS synonym.
⋮----
defineSymbol(math, main, spacing, "\u00a0", "\\space"); // Ref: LaTeX Source 2e: \DeclareRobustCommand{\nobreakspace}{%
⋮----
defineSymbol(text$1, main, textord, "<", "\\textless", true); // in T1 fontenc
⋮----
defineSymbol(text$1, main, textord, ">", "\\textgreater", true); // in T1 fontenc
⋮----
defineSymbol(text$1, main, textord, "|", "\\textbar", true); // in T1 fontenc
⋮----
defineSymbol(math, main, textord, "\u22ee", "\\varvdots"); // \vdots is a macro
⋮----
defineSymbol(math, main, accent, "\u02da", "\\mathring"); // \imath and \jmath should be invariant to \mathrm, \mathbf, etc., so use PUA
⋮----
defineSymbol(text$1, main, accent, "\u02ca", "\\'"); // acute
⋮----
defineSymbol(text$1, main, accent, "\u02cb", "\\`"); // grave
⋮----
defineSymbol(text$1, main, accent, "\u02c6", "\\^"); // circumflex
⋮----
defineSymbol(text$1, main, accent, "\u02dc", "\\~"); // tilde
⋮----
defineSymbol(text$1, main, accent, "\u02c9", "\\="); // macron
⋮----
defineSymbol(text$1, main, accent, "\u02d8", "\\u"); // breve
⋮----
defineSymbol(text$1, main, accent, "\u02d9", "\\."); // dot above
⋮----
defineSymbol(text$1, main, accent, "\u02da", "\\r"); // ring above
⋮----
defineSymbol(text$1, main, accent, "\u02c7", "\\v"); // caron
⋮----
defineSymbol(text$1, main, accent, "\u00a8", '\\"'); // diaeresis
⋮----
defineSymbol(text$1, main, accent, "\u02dd", "\\H"); // double acute
⋮----
defineSymbol(text$1, main, accent, "\u25ef", "\\textcircled"); // \bigcirc glyph
// These ligatures are detected and created in Parser.js's `formLigatures`.
⋮----
defineSymbol(text$1, main, textord, "\u201d", "\\textquotedblright"); //  \degree from gensymb package
⋮----
defineSymbol(text$1, main, textord, "\u00b0", "\\degree"); // \textdegree from inputenc package
⋮----
defineSymbol(text$1, main, textord, "\u00b0", "\\textdegree", true); // TODO: In LaTeX, \pounds can generate a different character in text and math
// mode, but among our fonts, only Main-Regular defines this character "163".
⋮----
defineSymbol(text$1, ams, textord, "\u2720", "\\maltese"); // There are lots of symbols which are the same, so we add them in afterwards.
// All of these are textords in math mode
⋮----
} // All of these are textords in text mode
⋮----
} // All of these are textords in text mode, and mathords in math mode
⋮----
} // Blackboard bold and script letters in Unicode range
⋮----
defineSymbol(math, ams, textord, "C", "\u2102"); // blackboard bold
⋮----
defineSymbol(math, main, mathord, "h", "\u210E"); // italic h, Planck constant
⋮----
defineSymbol(text$1, main, mathord, "h", "\u210E"); // The next loop loads wide (surrogate pair) characters.
// We support some letters in the Unicode range U+1D400 to U+1D7FF,
// Mathematical Alphanumeric Symbols.
// Some editors do not deal well with wide characters. So don't write the
// string into this file. Instead, create the string from the surrogate pair.
⋮----
const ch = letters.charAt(i); // The hex numbers in the next line are a surrogate pair.
// 0xD835 is the high surrogate for all letters in the range we support.
// 0xDC00 is the low surrogate for bold A.
⋮----
wideChar = String.fromCharCode(0xD835, 0xDC00 + i); // A-Z a-z bold
⋮----
wideChar = String.fromCharCode(0xD835, 0xDC34 + i); // A-Z a-z italic
⋮----
wideChar = String.fromCharCode(0xD835, 0xDC68 + i); // A-Z a-z bold italic
⋮----
wideChar = String.fromCharCode(0xD835, 0xDD04 + i); // A-Z a-z Fractur
⋮----
wideChar = String.fromCharCode(0xD835, 0xDDA0 + i); // A-Z a-z sans-serif
⋮----
wideChar = String.fromCharCode(0xD835, 0xDDD4 + i); // A-Z a-z sans bold
⋮----
wideChar = String.fromCharCode(0xD835, 0xDE08 + i); // A-Z a-z sans italic
⋮----
wideChar = String.fromCharCode(0xD835, 0xDE70 + i); // A-Z a-z monospace
⋮----
// KaTeX fonts have only capital letters for blackboard bold and script.
// See exception for k below.
wideChar = String.fromCharCode(0xD835, 0xDD38 + i); // A-Z double struck
⋮----
wideChar = String.fromCharCode(0xD835, 0xDC9C + i); // A-Z script
⋮----
} // TODO: Add bold script when it is supported by a KaTeX font.
⋮----
} // "k" is the only double struck lower case letter in the KaTeX fonts.
⋮----
wideChar = String.fromCharCode(0xD835, 0xDD5C); // k double struck
⋮----
defineSymbol(text$1, main, textord, "k", wideChar); // Next, some wide character numerals
⋮----
wideChar = String.fromCharCode(0xD835, 0xDFCE + i); // 0-9 bold
⋮----
wideChar = String.fromCharCode(0xD835, 0xDFE2 + i); // 0-9 sans serif
⋮----
wideChar = String.fromCharCode(0xD835, 0xDFEC + i); // 0-9 bold sans
⋮----
wideChar = String.fromCharCode(0xD835, 0xDFF6 + i); // 0-9 monospace
⋮----
} // We add these Latin-1 letters as symbols for backwards-compatibility,
// but they are not actually in the font, nor are they supported by the
// Unicode accent mechanism, so they fall back to Times font and look ugly.
// TODO(edemaine): Fix this.
⋮----
/**
 * This file provides support for Unicode range U+1D400 to U+1D7FF,
 * Mathematical Alphanumeric Symbols.
 *
 * Function wideCharacterFont takes a wide character as input and returns
 * the font information necessary to render it properly.
 */
/**
 * Data below is from https://www.unicode.org/charts/PDF/U1D400.pdf
 * That document sorts characters into groups by font type, say bold or italic.
 *
 * In the arrays below, each subarray consists three elements:
 *      * The CSS class of that group when in math mode.
 *      * The CSS class of that group when in text mode.
 *      * The font name, so that KaTeX can get font metrics.
 */
⋮----
const wideLatinLetterData = [["mathbf", "textbf", "Main-Bold"], // A-Z bold upright
["mathbf", "textbf", "Main-Bold"], // a-z bold upright
["mathnormal", "textit", "Math-Italic"], // A-Z italic
["mathnormal", "textit", "Math-Italic"], // a-z italic
["boldsymbol", "boldsymbol", "Main-BoldItalic"], // A-Z bold italic
["boldsymbol", "boldsymbol", "Main-BoldItalic"], // a-z bold italic
// Map fancy A-Z letters to script, not calligraphic.
// This aligns with unicode-math and math fonts (except Cambria Math).
["mathscr", "textscr", "Script-Regular"], // A-Z script
["", "", ""], // a-z script.  No font
["", "", ""], // A-Z bold script. No font
["", "", ""], // a-z bold script. No font
["mathfrak", "textfrak", "Fraktur-Regular"], // A-Z Fraktur
["mathfrak", "textfrak", "Fraktur-Regular"], // a-z Fraktur
["mathbb", "textbb", "AMS-Regular"], // A-Z double-struck
["mathbb", "textbb", "AMS-Regular"], // k double-struck
["", "", ""], // A-Z bold Fraktur No font metrics
["", "", ""], // a-z bold Fraktur.   No font.
["mathsf", "textsf", "SansSerif-Regular"], // A-Z sans-serif
["mathsf", "textsf", "SansSerif-Regular"], // a-z sans-serif
["mathboldsf", "textboldsf", "SansSerif-Bold"], // A-Z bold sans-serif
["mathboldsf", "textboldsf", "SansSerif-Bold"], // a-z bold sans-serif
["mathitsf", "textitsf", "SansSerif-Italic"], // A-Z italic sans-serif
["mathitsf", "textitsf", "SansSerif-Italic"], // a-z italic sans-serif
["", "", ""], // A-Z bold italic sans. No font
["", "", ""], // a-z bold italic sans. No font
["mathtt", "texttt", "Typewriter-Regular"], // A-Z monospace
⋮----
const wideNumeralData = [["mathbf", "textbf", "Main-Bold"], // 0-9 bold
["", "", ""], // 0-9 double-struck. No KaTeX font.
["mathsf", "textsf", "SansSerif-Regular"], // 0-9 sans-serif
["mathboldsf", "textboldsf", "SansSerif-Bold"], // 0-9 bold sans-serif
⋮----
// IE doesn't support codePointAt(). So work with the surrogate pair.
const H = wideChar.charCodeAt(0); // high surrogate
⋮----
const L = wideChar.charCodeAt(1); // low surrogate
⋮----
const j = mode === "math" ? 0 : 1; // column index for CSS class.
⋮----
// wideLatinLetterData contains exactly 26 chars on each row.
// So we can calculate the relevant row. No traverse necessary.
⋮----
// Numerals, ten per row.
⋮----
// dotless i or j
⋮----
// Greek letters. Not supported, yet.
⋮----
// We don't support any wide characters outside 1D400–1D7FF.
⋮----
/**
 * This file contains information about the options that the Parser carries
 * around with it while parsing. Data is held in an `Options` object, and when
 * recursing, a new `Options` object can be created with the `.with*` and
 * `.reset` functions.
 */
const sizeStyleMap = [// Each element contains [textsize, scriptsize, scriptscriptsize].
// The size mappings are taken from TeX with \normalsize=10pt.
[1, 1, 1], // size1: [5, 5, 5]              \tiny
[2, 1, 1], // size2: [6, 5, 5]
[3, 1, 1], // size3: [7, 5, 5]              \scriptsize
[4, 2, 1], // size4: [8, 6, 5]              \footnotesize
[5, 2, 1], // size5: [9, 6, 5]              \small
[6, 3, 1], // size6: [10, 7, 5]             \normalsize
[7, 4, 2], // size7: [12, 8, 6]             \large
[8, 6, 3], // size8: [14.4, 10, 7]          \Large
[9, 7, 6], // size9: [17.28, 12, 10]        \LARGE
[10, 8, 7], // size10: [20.74, 14.4, 12]     \huge
⋮----
const sizeMultipliers = [// fontMetrics.js:getGlobalMetrics also uses size indexes, so if
// you change size indexes, change that function.
⋮----
}; // In these types, "" (empty string) means "no change".
⋮----
/**
 * This is the main options class. It contains the current style, size, color,
 * and font.
 *
 * Options objects should not be modified. To create a new Options with
 * different properties, call a `.having*` method.
 */
class Options
⋮----
// A font family applies to a group of fonts (i.e. SansSerif), while a font
// represents a specific font (i.e. SansSerif Bold).
// See: https://tex.stackexchange.com/questions/22350/difference-between-textrm-and-mathrm
⋮----
/**
   * The base size index.
   */
⋮----
/**
   * Returns a new options object with the same properties as "this".  Properties
   * from "extension" will be copied to the new options object.
   */
⋮----
extend(extension)
/**
   * Return an options object with the given style. If `this.style === style`,
   * returns `this`.
   */
⋮----
havingStyle(style)
/**
   * Return an options object with a cramped version of the current style. If
   * the current style is cramped, returns `this`.
   */
⋮----
havingCrampedStyle()
/**
   * Return an options object with the given size and in at least `\textstyle`.
   * Returns `this` if appropriate.
   */
⋮----
havingSize(size)
/**
   * Like `this.havingSize(BASESIZE).havingStyle(style)`. If `style` is omitted,
   * changes to at least `\textstyle`.
   */
⋮----
havingBaseStyle(style)
/**
   * Remove the effect of sizing changes such as \Huge.
   * Keep the effect of the current style, such as \scriptstyle.
   */
⋮----
havingBaseSizing()
⋮----
size = 3; // normalsize in scriptstyle
⋮----
size = 1; // normalsize in scriptscriptstyle
⋮----
// normalsize in textstyle or displaystyle
⋮----
/**
   * Create a new options object with the given color.
   */
⋮----
withColor(color)
/**
   * Create a new options object with "phantom" set to true.
   */
⋮----
withPhantom()
/**
   * Creates a new options object with the given math font or old text font.
   * @type {[type]}
   */
⋮----
withFont(font)
/**
   * Create a new options objects with the given fontFamily.
   */
⋮----
withTextFontFamily(fontFamily)
/**
   * Creates a new options object with the given font weight
   */
⋮----
withTextFontWeight(fontWeight)
/**
   * Creates a new options object with the given font weight
   */
⋮----
withTextFontShape(fontShape)
/**
   * Return the CSS sizing classes required to switch from enclosing options
   * `oldOptions` to `this`. Returns an array of classes.
   */
⋮----
sizingClasses(oldOptions)
/**
   * Return the CSS sizing classes required to switch to the base size. Like
   * `this.havingSize(BASESIZE).sizingClasses(this)`.
   */
⋮----
baseSizingClasses()
/**
   * Return the font metrics for this size.
   */
⋮----
fontMetrics()
/**
   * Gets the CSS color of the current options object
   */
⋮----
getColor()
⋮----
/**
 * This file does conversion between units.  In particular, it provides
 * calculateSize to convert other units into ems.
 */
// Thus, multiplying a length by this number converts the length from units
// into pts.  Dividing the result by ptPerEm gives the number of ems
// *assuming* a font size of ptPerEm (normal size, normal style).
⋮----
// https://en.wikibooks.org/wiki/LaTeX/Lengths and
// https://tex.stackexchange.com/a/8263
⋮----
// TeX point
⋮----
// millimeter
⋮----
// centimeter
⋮----
// inch
⋮----
// big (PostScript) points
⋮----
// pica
⋮----
// didot
⋮----
// cicero (12 didot)
⋮----
// new didot
⋮----
// new cicero (12 new didot)
⋮----
// scaled point (TeX's internal smallest unit)
// https://tex.stackexchange.com/a/41371
"px": 803 / 800 // \pdfpxdimen defaults to 1 bp in pdfTeX and LuaTeX
⋮----
}; // Dictionary of relative units, for fast validity testing.
⋮----
/**
 * Determine whether the specified unit (either a string defining the unit
 * or a "size" parse node containing a unit field) is valid.
 */
⋮----
/*
 * Convert a "size" parse node (with numeric "number" and string "unit" fields,
 * as parsed by functions.js argType "size") into a CSS em value for the
 * current style/scale.  `options` gives the current options.
 */
⋮----
// Absolute units
scale = ptPerUnit[sizeValue.unit] // Convert unit to pt
/ options.fontMetrics().ptPerEm // Convert pt to CSS em
/ options.sizeMultiplier; // Unscale to make absolute units
⋮----
// `mu` units scale with scriptstyle/scriptscriptstyle.
⋮----
// Other relative units always refer to the *textstyle* font
// in the current size.
⋮----
// isTight() means current style is script/scriptscript.
⋮----
} // TODO: In TeX these units are relative to the quad of the current
// *text* font, e.g. cmr10. KaTeX instead uses values from the
// comparably-sized *Computer Modern symbol* font. At 10pt, these
// match. At 7pt and 5pt, they differ: cmr7=1.138894, cmsy7=1.170641;
// cmr5=1.361133, cmsy5=1.472241. Consider $\scriptsize a\kern1emb$.
// TeX \showlists shows a kern of 1.13889 * fontsize;
// KaTeX shows a kern of 1.171 * fontsize.
⋮----
/* eslint no-console:0 */
⋮----
/**
 * Looks up the given symbol in fontMetrics, after applying any symbol
 * replacements defined in symbol.js
 */
const lookupSymbol = function lookupSymbol(value, // TODO(#963): Use a union type for this.
⋮----
// Replace the value with its replaced value from symbol.js
⋮----
/**
 * Makes a symbolNode after translation via the list of symbols in symbols.js.
 * Correctly pulls out metrics for the character, and optionally takes a list of
 * classes to be attached to the node.
 *
 * TODO: make argument order closer to makeSpan
 * TODO: add a separate argument for math class (e.g. `mop`, `mbin`), which
 * should if present come first in `classes`.
 * TODO(#953): Make `options` mandatory and always pass it in.
 */
⋮----
// TODO(emily): Figure out a good way to only print this in development
⋮----
/**
 * Makes a symbol in Main-Regular or AMS-Regular.
 * Used for rel, bin, open, close, inner, and punct.
 */
⋮----
// Decide what font to render the symbol in by its entry in the symbols
// table.
// Have a special case for when the value = \ because the \ is used as a
// textord in unsupported command errors but cannot be parsed as a regular
// text ordinal and is therefore not present as a symbol in the symbols
// table for text, as well as a special case for boldsymbol because it
// can be used for bold + and -
⋮----
/**
 * Determines which of the two font names (Main-Bold and Math-BoldItalic) and
 * corresponding style tags (mathbf or boldsymbol) to use for font "boldsymbol",
 * depending on the symbol.  Use this function instead of fontMap for font
 * "boldsymbol".
 */
⋮----
// Some glyphs do not exist in Math-BoldItalic so we need to use
// Main-Bold instead.
⋮----
/**
 * Makes either a mathord or textord in the correct font and color.
 */
⋮----
const classes = ["mord"]; // Math mode or Old font (i.e. \rm)
⋮----
// surrogate pairs get special treatment
⋮----
// Deconstruct ligatures in monospace fonts (\texttt, \tt).
⋮----
} // Makes a symbol in the default font for mathords and textords.
⋮----
// fonts added by plugins
const fontName = retrieveTextFontName(font, options.fontWeight, options.fontShape); // We add font name as a css class
⋮----
/**
 * Returns true if subsequent symbolNodes have the same classes, skew, maxFont,
 * and styles.
 */
⋮----
const canCombine = (prev, next) =>
/**
 * Combine consecutive domTree.symbolNodes into a single symbolNode.
 * Note: this function mutates the argument.
 */
⋮----
const tryCombineChars = chars => {
for (let i = 0; i < chars.length - 1; i++)
⋮----
prev.depth = Math.max(prev.depth, next.depth); // Use the last character's italic correction since we use
// it to add padding to the right of the span created from
// the combined characters.
⋮----
/**
 * Calculate the height, depth, and maxFontSize of an element based on its
 * children.
 */
⋮----
/**
 * Makes a span with the given list of classes, list of children, and options.
 *
 * TODO(#953): Ensure that `options` is always provided (currently some call
 * sites don't pass it) and make the type below mandatory.
 * TODO: add a separate argument for math class (e.g. `mop`, `mbin`), which
 * should if present come first in `classes`.
 */
⋮----
}; // SVG one is simpler -- doesn't require height, depth, max-font setting.
// This is also a separate method for typesafety.
⋮----
const makeSvgSpan = (classes, children, options, style)
⋮----
/**
 * Makes an anchor with the given href, list of classes, list of children,
 * and options.
 */
⋮----
/**
 * Makes a document fragment with the given list of children.
 */
⋮----
/**
 * Wraps group in a span if it's a document fragment, allowing to apply classes
 * and styles
 */
⋮----
}; // These are exact object types to catch typos in the names of the optional fields.
⋮----
// Computes the updated `children` list and the overall depth.
//
// This helper function for makeVList makes it easier to enforce type safety by
// allowing early exits (returns) in the logic.
⋮----
const children = [oldChildren[0]]; // Add in kerns to the list of params.children to get each element to be
// shifted to the correct specified shift
⋮----
// We always start at the bottom, so calculate the bottom by adding up
// all the sizes
⋮----
/**
 * Makes a vertical list by stacking elements and kerns on top of each other.
 * Allows for many different ways of specifying the positioning method.
 *
 * See VListParam documentation above.
 */
⋮----
depth = _getVListChildrenAndD.depth; // Create a strut that is taller than any list item. The strut is added to
// each item, where it will determine the item's baseline. Since it has
// `overflow:hidden`, the strut's top edge will sit on the item's line box's
// top edge and the strut's bottom edge will sit on the item's baseline,
// with no additional line-height spacing. This allows the item baseline to
// be positioned precisely without worrying about font ascent and
// line-height.
⋮----
pstrut.style.height = pstrutSize + "em"; // Create a new list of actual children at the correct offsets
⋮----
} // The vlist contents go in a table-cell with `vertical-align:bottom`.
// This cell's bottom edge will determine the containing table's baseline
// without overly expanding the containing line-box.
⋮----
vlist.style.height = maxPos + "em"; // A second row is used if necessary to represent the vlist's depth.
⋮----
// We will define depth in an empty span with display: table-cell.
// It should render with the height that we define. But Chrome, in
// contenteditable mode only, treats that span as if it contains some
// text content. And that min-height over-rides our desired height.
// So we put another empty span inside the depth strut span.
⋮----
depthStrut.style.height = -minPos + "em"; // Safari wants the first row to have inline content; otherwise it
// puts the bottom of the *second* row on the baseline.
⋮----
}; // Glue is a concept from TeX which is a flexible space between elements in
// either a vertical or horizontal list. In KaTeX, at least for now, it's
// static space between elements in a horizontal layout.
⋮----
const makeGlue = (measurement, options) =>
⋮----
// Make an empty span for the space
⋮----
}; // Takes font options, and returns the appropriate fontLookup name
⋮----
// use fonts added by a plugin
⋮----
/**
 * Maps TeX font commands to objects containing:
 * - variant: string used for "mathvariant" attribute in buildMathML.js
 * - fontName: the "style" parameter to fontMetrics.getCharacterMetrics
 */
// A map between tex font commands an MathML mathvariant attribute values
⋮----
// styles
⋮----
// "boldsymbol" is missing because they require the use of multiple fonts:
// Math-BoldItalic and Main-Bold.  This is handled by a special case in
// makeOrd which ends up calling boldsymbol.
// families
⋮----
//   path, width, height
⋮----
// values from the font glyph
⋮----
// oval to overlay the integrand
⋮----
// Create a span with inline SVG for the element.
⋮----
// Override CSS rule `.katex svg { width: 100% }`
⋮----
/**
 * Describes spaces between different classes of atoms.
 */
⋮----
}; // Making the type below exact with all optional fields doesn't work due to
// - https://github.com/facebook/flow/issues/4582
// - https://github.com/facebook/flow/issues/5688
// However, since *all* fields are optional, $Shape<> works as suggested in 5688
// above.
⋮----
// Spacing relationships for display and text styles
⋮----
}; // Spacing relationships for script and scriptscript styles
⋮----
/** Context provided to function handlers for error messages. */
// Note: reverse the order of the return type union will cause a flow error.
// See https://github.com/facebook/flow/issues/3663.
// More general version of `HtmlBuilder` for nodes (e.g. \sum, accent types)
// whose presence impacts super/subscripting. In this case, ParseNode<"supsub">
// delegates its HTML building to the HtmlBuilder corresponding to these nodes.
⋮----
/**
 * Final function spec for use at parse time.
 * This is almost identical to `FunctionPropSpec`, except it
 * 1. includes the function handler, and
 * 2. requires all arguments except argTypes.
 * It is generated by `defineFunction()` below.
 */
⋮----
/**
 * All registered functions.
 * `functions.js` just exports this same dictionary again and makes it public.
 * `Parser.js` requires this dictionary.
 */
⋮----
/**
 * All HTML builders. Should be only used in the `define*` and the `build*ML`
 * functions.
 */
⋮----
/**
 * All MathML builders. Should be only used in the `define*` and the `build*ML`
 * functions.
 */
⋮----
function defineFunction(_ref)
⋮----
// Set default values of functions
⋮----
/**
 * Use this to register only the HTML and MathML builders for a function (e.g.
 * if the function's ParseNode is generated in Parser.js rather than via a
 * stand-alone handler provided to `defineFunction`).
 */
⋮----
function defineFunctionBuilders(_ref2)
⋮----
handler()
⋮----
} // Since the corresponding buildHTML/buildMathML function expects a
// list of elements, we normalize for different kinds of arguments
⋮----
/**
 * This file does the main work of building a domTree structure from a parse
 * tree. The entry point is the `buildHTML` function, which takes a parse tree.
 * Then, the buildExpression, buildGroup, and various groupBuilders functions
 * are called, to produce a final HTML tree.
 */
const makeSpan$1 = buildCommon.makeSpan; // Binary atoms (first class `mbin`) change into ordinary atoms (`mord`)
// depending on their surroundings. See TeXbook pg. 442-446, Rules 5 and 6,
// and the text before Rule 19.
⋮----
/**
 * Take a list of nodes, build them in order, and return a list of the built
 * nodes. documentFragments are flattened into their contents, so the
 * returned list contains no fragments. `isRealGroup` is true if `expression`
 * is a real group (no atoms will be added on either side), as opposed to
 * a partial group (e.g. one created by \color). `surrounding` is an array
 * consisting type of nodes that will be added to the left and right.
 */
⋮----
// Parse expressions into `groups`.
⋮----
} // If `expression` is a partial group, let the parent handle spacings
// to avoid processing groups multiple times.
⋮----
} // Dummy spans for determining spacings between surrounding atoms.
// If `expression` has no atoms on the left or right, class "leftmost"
// or "rightmost", respectively, is used to indicate it.
⋮----
const dummyNext = makeSpan$1([surrounding[1] || "rightmost"], [], options); // TODO: These code assumes that a node's math class is the first element
// of its `classes` array. A later cleanup should ensure this, for
// instance by changing the signature of `makeSpan`.
// Before determining what spaces to insert, perform bin cancellation.
// Binary operators change to ordinary symbols in some contexts.
⋮----
const type = getTypeOfDomTree(node); // 'mtight' indicates that the node is script or scriptscript style.
⋮----
// Insert glue (spacing) after the `prev`.
⋮----
}; // Depth-first traverse non-space `nodes`, calling `callback` with the current and
// previous node as arguments, optionally returning a node to insert after the
// previous node. `prev` is an object with the previous node and `insertAfter`
// function to insert after it. `next` is a node that will be added to the right.
// Used for bin cancellation and inserting spacings.
⋮----
// temporarily append the right node, if exists
⋮----
// Recursive DFS
// $FlowFixMe: make nodes a $ReadOnlyArray by returning a new array
⋮----
} // Ignore explicit spaces (e.g., \;, \,) when determining what implicit
// spacing should go between atoms of different classes
⋮----
// insert at front
⋮----
prev.node = makeSpan$1(["leftmost"]); // treat like beginning of line
⋮----
}; // Check if given node is a partial group, i.e., does not affect spacing around.
⋮----
}; // Return the outermost node of a domTree.
⋮----
}; // Return math atom class (mclass) of a domTree.
// If `side` is given, it will get the type of the outermost node at given side.
⋮----
} // This makes a lot of assumptions as to where the type of atom
// appears.  We should do a better job of enforcing this.
⋮----
/**
 * buildGroup is the function that takes a group and calls the correct groupType
 * function for it. It also handles the interaction of size and style changes
 * between parents and children.
 */
⋮----
// Call the groupBuilders function
let groupNode = _htmlGroupBuilders[group.type](group, options); // If the size changed between the parent and the current group, account
// for that size difference.
⋮----
/**
 * Combine an array of HTML DOM nodes (e.g., the output of `buildExpression`)
 * into an unbreakable HTML node of class .base, with proper struts to
 * guarantee correct vertical extent.  `buildHTML` calls this repeatedly to
 * make up the entire expression as a sequence of unbreakable units.
 */
⋮----
function buildHTMLUnbreakable(children, options)
⋮----
// Compute height and depth of this chunk.
const body = makeSpan$1(["base"], children, options); // Add strut, which ensures that the top of the HTML element falls at
// the height of the expression, and the bottom of the HTML element
// falls at the depth of the expression.
⋮----
/**
 * Take an entire parse tree, and build it into an appropriate set of HTML
 * nodes.
 */
⋮----
function buildHTML(tree, options)
⋮----
// Strip off outer tag wrapper for processing below.
⋮----
} // Build the expression contained in the tree
⋮----
const children = []; // Create one base node for each chunk between potential line breaks.
// The TeXBook [p.173] says "A formula will be broken only after a
// relation symbol like $=$ or $<$ or $\rightarrow$, or after a binary
// operation symbol like $+$ or $-$ or $\times$, where the relation or
// binary operation is on the ``outer level'' of the formula (i.e., not
// enclosed in {...} and not part of an \over construction)."
⋮----
// Put any post-operator glue on same line as operator.
// Watch for \nobreak along the way, and stop at \newline.
⋮----
} // Don't allow break if \nobreak among the post-operator glue.
⋮----
// Write the line except the newline
⋮----
} // Put the newline at the top level
⋮----
} // Now, if there was a tag, build it too and append it as a final child.
⋮----
htmlNode.setAttribute("aria-hidden", "true"); // Adjust the strut of the tag to be the maximum height of all children
// (the height of the enclosing htmlNode) for proper vertical alignment.
⋮----
/**
 * These objects store data about MathML nodes. This is the MathML equivalent
 * of the types in domTree.js. Since MathML handles its own rendering, and
 * since we're mainly using MathML to improve accessibility, we don't manage
 * any of the styling state that the plain DOM nodes do.
 *
 * The `toNode` and `toMarkup` functions work similarly to how they do in
 * domTree.js, creating namespaced DOM nodes and HTML text markup respectively.
 */
function newDocumentFragment(children)
/**
 * This node represents a general purpose MathML node of any type. The
 * constructor requires the type of node to create (for example, `"mo"` or
 * `"mspace"`, corresponding to `<mo>` and `<mspace>` tags).
 */
⋮----
class MathNode
⋮----
/**
   * Sets an attribute on a MathML node. MathML depends on attributes to convey a
   * semantic content, so this is used heavily.
   */
⋮----
setAttribute(name, value)
/**
   * Gets an attribute on a MathML node.
   */
⋮----
getAttribute(name)
/**
   * Converts the math node into a MathML-namespaced DOM element.
   */
⋮----
/**
   * Converts the math node into an HTML markup string.
   */
⋮----
let markup = "<" + this.type; // Add the attributes
⋮----
/**
   * Converts the math node into a string, similar to innerText, but escaped.
   */
⋮----
/**
 * This node represents a piece of text.
 */
⋮----
class TextNode
⋮----
/**
   * Converts the text node into a DOM text node.
   */
⋮----
/**
   * Converts the text node into escaped HTML markup
   * (representing the text itself).
   */
⋮----
/**
   * Converts the text node into a string
   * (representing the text itself).
   */
⋮----
/**
 * This node represents a space, but may render as <mspace.../> or as text,
 * depending on the width.
 */
⋮----
class SpaceNode
⋮----
/**
   * Create a Space node with width given in CSS ems.
   */
⋮----
this.width = width; // See https://www.w3.org/TR/2000/WD-MathML2-20000328/chapter6.html
// for a table of space-like characters.  We use Unicode
// representations instead of &LongNames; as it's not clear how to
// make the latter via document.createTextNode.
⋮----
this.character = "\u200a"; // &VeryThinSpace;
⋮----
this.character = "\u2009"; // &ThinSpace;
⋮----
this.character = "\u2005"; // &MediumSpace;
⋮----
this.character = "\u2005\u200a"; // &ThickSpace;
⋮----
this.character = "\u200a\u2063"; // &NegativeVeryThinSpace;
⋮----
this.character = "\u2009\u2063"; // &NegativeThinSpace;
⋮----
this.character = "\u205f\u2063"; // &NegativeMediumSpace;
⋮----
this.character = "\u2005\u2063"; // &NegativeThickSpace;
⋮----
/**
   * Converts the math node into a MathML-namespaced DOM element.
   */
⋮----
/**
   * Converts the math node into an HTML markup string.
   */
⋮----
/**
   * Converts the math node into a string, similar to innerText.
   */
⋮----
/**
 * This file converts a parse tree into a corresponding MathML tree. The main
 * entry point is the `buildMathML` function, which takes a parse tree from the
 * parser.
 */
⋮----
/**
 * Takes a symbol and converts it into a MathML text node after performing
 * optional replacement from symbols.js.
 */
⋮----
/**
 * Wrap the given array of nodes in an <mrow> node if needed, i.e.,
 * unless the array has length 1.  Always returns a single node.
 */
⋮----
/**
 * Returns the math variant as a string or null if none is required.
 */
⋮----
// Handle \text... font specifiers as best we can.
// MathML has a limited list of allowable mathvariant specifiers; see
// https://www.w3.org/TR/MathML3/chapter3.html#presm.commatt
⋮----
// MathML makes no distinction between script and caligrahpic
⋮----
/**
 * Takes a list of nodes, builds them, and returns a list of the generated
 * MathML nodes.  Also combine consecutive <mtext> outputs into a single
 * <mtext> tag.
 */
⋮----
// When TeX writers want to suppress spacing on an operator,
// they often put the operator by itself inside braces.
⋮----
// Concatenate adjacent <mtext>s
⋮----
continue; // Concatenate adjacent <mn>s
⋮----
continue; // Concatenate <mn>...</mn> followed by <mi>.</mi>
⋮----
// Overlay with combining character long solidus
⋮----
/**
 * Equivalent to buildExpression, but wraps the elements in an <mrow>
 * if there's more than one.  Returns a single node instead of an array.
 */
⋮----
/**
 * Takes a group from the parser and calls the appropriate groupBuilders function
 * on it to produce a MathML node.
 */
⋮----
// Call the groupBuilders function
⋮----
/**
 * Takes a full parse tree and settings and builds a MathML representation of
 * it. In particular, we put the elements from building the parse tree into a
 * <semantics> tag so we can also include that TeX source as an annotation.
 *
 * Note that we actually return a domTree element with a `<math>` inside it so
 * we can do appropriate styling.
 */
⋮----
function buildMathML(tree, texExpression, options, isDisplayMode, forMathmlOnly)
⋮----
const expression = buildExpression$1(tree, options); // Wrap up the expression in an mrow so it is presented in the semantics
// tag correctly, unless it's a single <mrow> or <mtable>.
⋮----
} // Build a TeX annotation of the source
⋮----
} // You can't style <math> nodes, so we wrap the node in a span.
// NOTE: The span class is not typed to have <math> nodes as children, and
// we don't want to make the children type more generic since the children
// of span are expected to have more fields in `buildHtml` contexts.
⋮----
const wrapperClass = forMathmlOnly ? "katex" : "katex-mathml"; // $FlowFixMe
⋮----
/**
 * This file provides support to buildMathML.js and buildHTML.js
 * for stretchy wide elements rendered from SVG files
 * and other CSS trickery.
 */
⋮----
// Not a perfect match.
xleftequilibrium: "\u21cb" // None better available.
⋮----
}; // Many of the KaTeX SVG images have been adapted from glyphs in KaTeX fonts.
// Copyright (c) 2009-2010, Design Science, Inc. (<www.mathjax.org>)
// Copyright (c) 2014-2017 Khan Academy (<www.khanacademy.org>)
// Licensed under the SIL Open Font License, Version 1.1.
// See \nhttp://scripts.sil.org/OFL
// Very Long SVGs
//    Many of the KaTeX stretchy wide elements use a long SVG image and an
//    overflow: hidden tactic to achieve a stretchy image while avoiding
//    distortion of arrowheads or brace corners.
//    The SVG typically contains a very long (400 em) arrow.
//    The SVG is in a container span that has overflow: hidden, so the span
//    acts like a window that exposes only part of the  SVG.
//    The SVG always has a longer, thinner aspect ratio than the container span.
//    After the SVG fills 100% of the height of the container span,
//    there is a long arrow shaft left over. That left-over shaft is not shown.
//    Instead, it is sliced off because the span's CSS has overflow: hidden.
//    Thus, the reader sees an arrow that matches the subject matter width
//    without distortion.
//    Some functions, such as \cancel, need to vary their aspect ratio. These
//    functions do not get the overflow SVG treatment.
// Second Brush Stroke
//    Low resolution monitors struggle to display images in fine detail.
//    So browsers apply anti-aliasing. A long straight arrow shaft therefore
//    will sometimes appear as if it has a blurred edge.
//    To mitigate this, these SVG files contain a second "brush-stroke" on the
//    arrow shafts. That is, a second long thin rectangular SVG path has been
//    written directly on top of each arrow shaft. This reinforcement causes
//    some of the screen pixels to display as black instead of the anti-aliased
//    gray pixel that a  single path would generate. So we get arrow shafts
//    whose edges appear to be sharper.
// In the katexImagesData object just below, the dimensions all
// correspond to path geometry inside the relevant SVG.
// For example, \overrightarrow uses the same arrowhead as glyph U+2192
// from the KaTeX Main font. The scaling factor is 1000.
// That is, inside the font, that arrowhead is 522 units tall, which
// corresponds to 0.522 em inside the document.
⋮----
//   path(s), minWidth, height, align
⋮----
// The next three arrows are from the mhchem package.
// In mhchem.sty, min-length is 2.0em. But these arrows might appear in the
// document as \xrightarrow or \xrightleftharpoons. Those have
// min-length = 1.75em, so we set min-length on these next three to match.
⋮----
// Create a span with inline SVG for the element.
function buildSvgSpan_()
⋮----
let viewBoxWidth = 400000; // default
⋮----
// Each type in the `if` statement corresponds to one of the ParseNode
// types below. This narrowing is required to access `grp.base`.
const grp = group; // There are four SVG images available for each function.
// Choose a taller image when there are more characters.
⋮----
// $FlowFixMe: All these cases must be of the 4-tuple type.
⋮----
} // buildSvgSpan_()
⋮----
height = _buildSvgSpan_.height; // Note that we are returning span.depth = 0.
// Any adjustments relative to the baseline must be done in buildHTML.
⋮----
// Return an image span for \cancel, \bcancel, \xcancel, or \fbox
⋮----
// \cancel, \bcancel, or \xcancel
// Since \cancel's SVG is inline and it omits the viewBox attribute,
// its stroke-width will not vary with span area.
⋮----
/**
 * Asserts that the node is of the given type and returns it with stricter
 * typing. Throws if the node's type does not match.
 */
function assertNodeType(node, type)
/**
 * Returns the node more strictly typed iff it is of the given type. Otherwise,
 * returns null.
 */
⋮----
function assertSymbolNodeType(node)
/**
 * Returns the node more strictly typed iff it is of the given type. Otherwise,
 * returns null.
 */
⋮----
function checkSymbolNodeType(node)
⋮----
// $FlowFixMe
⋮----
// NOTE: Unlike most `htmlBuilder`s, this one handles not only "accent", but
const htmlBuilder = (grp, options) =>
⋮----
// Accents are handled in the TeXbook pg. 443, rule 12.
⋮----
// If our base is a character box, and we have superscripts and
// subscripts, the supsub will defer to us. In particular, we want
// to attach the superscripts and subscripts to the inner body (so
// that the position of the superscripts and subscripts won't be
// affected by the height of the accent). We accomplish this by
// sticking the base of the accent into the base of the supsub, and
// rendering that, while keeping track of where the accent is.
// The real accent group is the base of the supsub group
group = assertNodeType(grp.base, "accent"); // The character box is the base of the accent group
⋮----
base = group.base; // Stick the character box into the base of the supsub group
⋮----
grp.base = base; // Rerender the supsub group with its new base, and store that
// result.
⋮----
supSubGroup = assertSpan(buildGroup(grp, options)); // reset original base
⋮----
} // Build the base group
⋮----
const body = buildGroup(base, options.havingCrampedStyle()); // Does the accent need to shift for the skew of a character?
⋮----
const mustShift = group.isShifty && utils.isCharacterBox(base); // Calculate the skew of the accent. This is based on the line "If the
// nucleus is not a single character, let s = 0; otherwise set s to the
// kern amount for the nucleus followed by the \skewchar of its font."
// Note that our skew metrics are just the kern between each character
// and the skewchar.
⋮----
// If the base is a character box, then we want the skew of the
// innermost character. To do that, we find the innermost character:
const baseChar = utils.getBaseElem(base); // Then, we render its group to get the symbol inside it
⋮----
const baseGroup = buildGroup(baseChar, options.havingCrampedStyle()); // Finally, we pull the skew off of the symbol.
⋮----
skew = assertSymbolDomNode(baseGroup).skew; // Note that we now throw away baseGroup, because the layers we
// removed with getBaseElem might contain things like \color which
// we can't get rid of.
// TODO(emily): Find a better way to get the skew
} // calculate the amount of space between the body and the accent
⋮----
let clearance = Math.min(body.height, options.fontMetrics().xHeight); // Build the accent
⋮----
// Before version 0.9, \vec used the combining font glyph U+20D7.
// But browsers, especially Safari, are not consistent in how they
// render combining characters when not preceded by a character.
// So now we use an SVG.
// If Safari reforms, we should consider reverting to the glyph.
⋮----
accent = assertSymbolDomNode(accent); // Remove the italic correction of the accent, because it only serves to
// shift the accent over to a place we don't want.
⋮----
accentBody = buildCommon.makeSpan(["accent-body"], [accent]); // "Full" accents expand the width of the resulting symbol to be
// at least the width of the accent, and overlap directly onto the
// character without any vertical offset.
⋮----
} // Shift the accent over by the skew.
⋮----
let left = skew; // CSS defines `.katex .accent .accent-body:not(.accent-full) { width: 0 }`
// so that the accent doesn't contribute to the bounding box.
// We need to shift the character by its width (effectively half
// its width) to compensate.
⋮----
accentBody.style.left = left + "em"; // \textcircled uses the \bigcirc glyph, so it needs some
// vertical adjustment to match LaTeX.
⋮----
// Here, we replace the "base" child of the supsub with our newly
// generated accent.
supSubGroup.children[0] = accentWrap; // Since we don't rerun the height calculation after replacing the
// accent, we manually recalculate height.
⋮----
supSubGroup.height = Math.max(accentWrap.height, supSubGroup.height); // Accents should always be ords, even when their innards are not.
⋮----
const mathmlBuilder = (group, options) =>
⋮----
const NON_STRETCHY_ACCENT_REGEX = new RegExp(["\\acute", "\\grave", "\\ddot", "\\tilde", "\\bar", "\\breve", "\\check", "\\hat", "\\vec", "\\dot", "\\mathring"].map(accent => `\\${accent}`).join("|")); // Accents
⋮----
handler: (context, args) =>
⋮----
}); // Text-mode accents
⋮----
// Horizontal overlap functions
⋮----
handler: (_ref, args) =>
htmlBuilder: (group, options) =>
⋮----
// Treat under accents much like underlines.
⋮----
const kern = group.label === "\\utilde" ? 0.12 : 0; // Generate the vlist, with the appropriate kerns
⋮----
mathmlBuilder: (group, options) =>
⋮----
// Helper function
const paddedNode = group => {
  const node = new mathMLTree.MathNode("mpadded", group ? [group] : []);
⋮----
}; // Stretchy arrows with an optional argument
⋮----
names: ["\\xleftarrow", "\\xrightarrow", "\\xLeftarrow", "\\xRightarrow", "\\xleftrightarrow", "\\xLeftrightarrow", "\\xhookleftarrow", "\\xhookrightarrow", "\\xmapsto", "\\xrightharpoondown", "\\xrightharpoonup", "\\xleftharpoondown", "\\xleftharpoonup", "\\xrightleftharpoons", "\\xleftrightharpoons", "\\xlongequal", "\\xtwoheadrightarrow", "\\xtwoheadleftarrow", "\\xtofrom", // The next 3 functions are here to support the mhchem extension.
// Direct use of these functions is discouraged and may break someday.
⋮----
handler(_ref, args, optArgs)
⋮----
// Flow is unable to correctly infer the type of `group`, even though it's
// unamibiguously determined from the passed-in `type` above.
htmlBuilder(group, options)
⋮----
const style = options.style; // Build the argument groups in the appropriate style.
// Ref: amsmath.dtx:   \hbox{$\scriptstyle\mkern#3mu{#6}\mkern#4mu$}%
// Some groups can return document fragments.  Handle those by wrapping
// them in a span.
⋮----
// Build the lower group
⋮----
const arrowBody = stretchy.svgSpan(group, options); // Re shift: Note that stretchy.svgSpan returned arrowBody.depth = 0.
// The point we want on the math axis is at 0.5 * arrowBody.height.
⋮----
const arrowShift = -options.fontMetrics().axisHeight + 0.5 * arrowBody.height; // 2 mu kern. Ref: amsmath.dtx: #7\if0#2\else\mkern#2mu\fi
⋮----
let upperShift = -options.fontMetrics().axisHeight - 0.5 * arrowBody.height - 0.111; // 0.111 em = 2 mu
⋮----
upperShift -= upperGroup.depth; // shift up if depth encroaches
} // Generate the vlist
⋮----
} // $FlowFixMe: Replace this with passing "svg-align" into makeVList.
⋮----
mathmlBuilder(group, options)
⋮----
// This should never happen.
// Parser.js throws an error if there is no argument.
⋮----
// {123} and converts into symbol with code 123.  It is used by the *macro*
// \char defined in macros.js.
⋮----
handler(_ref, args)
⋮----
const htmlBuilder$1 = (group, options) =>
⋮----
const elements = buildExpression(group.body, options.withColor(group.color), false); // \color isn't supposed to affect the type of the elements it contains.
// To accomplish this, we wrap the results in a fragment, so the inner
// elements will be able to directly interact with their neighbors. For
// example, `\color{red}{2 +} 3` has the same spacing as `2 + 3`
⋮----
const mathmlBuilder$1 = (group, options) =>
⋮----
handler(_ref2, args)
⋮----
const color = assertNodeType(args[0], "color-token").color; // Set macro \current@color in current namespace to store the current
// color, mimicking the behavior of color.sty.
// This is currently used just to correctly color a \right
// that follows a \color command.
⋮----
parser.gullet.macros.set("\\current@color", color); // Parse out the implicit body that should be colored.
⋮----
// Row breaks within tabular environments, and line breaks at top level
// same signature, we implement them as one megafunction, with newRow
// indicating whether we're in the \cr case, and newLine indicating whether
// to break the line in the \newline case.
⋮----
// The following builders are called only at the top level,
// not within tabular/array environments.
⋮----
const checkControlSequence = tok => {
  const name = tok.text;

if (/^(?:[\\
⋮----
const getRHS = parser => {
  let tok = parser.gullet.popToken();
⋮----
// consume optional equals
⋮----
// consume one optional space
⋮----
const letCommand = (parser, name, tok, global) =>
⋮----
// don't expand it later even if a macro with the same name is defined
// e.g., \let\foo=\frac \def\frac{\relax} \frac12
⋮----
// reproduce the same behavior in expansion
⋮----
}; // <assignment> -> <non-macro assignment>|<macro assignment>
// <non-macro assignment> -> <simple assignment>|\global<non-macro assignment>
// <macro assignment> -> <definition>|<prefix><macro assignment>
// <prefix> -> \global|\long|\outer
⋮----
handler(_ref)
⋮----
// KaTeX doesn't have \par, so ignore \long
⋮----
}); // Basic support for macro definitions: \def, \gdef, \edef, \xdef
// <definition> -> <def><control sequence><definition text>
// <def> -> \def|\gdef|\edef|\xdef
// <definition text> -> <parameter text><left brace><balanced text><right brace>
⋮----
handler(_ref2)
⋮----
const name = arg[0].text; // Count argument specifiers, and check they are in the order #1 #2 ...
⋮----
arg.reverse(); // to fit in with stack order
} // Final arg is the expansion of the macro
⋮----
}); // <simple assignment> -> <let assignment>
// <let assignment> -> \futurelet<control sequence><token><token>
//     | \let<control sequence><equals><one optional space><token>
// <equals> -> <optional spaces>|<optional spaces>=
⋮----
handler(_ref3)
⋮----
}); // ref: https://www.tug.org/TUGboat/tb09-3/tb22bechtolsheim.pdf
⋮----
handler(_ref4)
⋮----
/**
 * This file deals with creating delimiters of various sizes. The TeXbook
 * discusses these routines on page 441-442, in the "Another subroutine sets box
 * x to a specified variable delimiter" paragraph.
 *
 * There are three main routines here. `makeSmallDelim` makes a delimiter in the
 * normal font, but in either text, script, or scriptscript style.
 * `makeLargeDelim` makes a delimiter in textstyle, but in one of the Size1,
 * Size2, Size3, or Size4 fonts. `makeStackedDelim` makes a delimiter out of
 * smaller pieces that are stacked on top of one another.
 *
 * The functions take a parameter `center`, which determines if the delimiter
 * should be centered around the axis.
 *
 * Then, there are three exposed functions. `sizedDelim` makes a delimiter in
 * one of the given sizes. This is used for things like `\bigl`.
 * `customSizedDelim` makes a delimiter with a given total height+depth. It is
 * called in places like `\sqrt`. `leftRightDelim` makes an appropriate
 * delimiter which surrounds an expression of a given height an depth. It is
 * used in `\left` and `\right`.
 */
⋮----
/**
 * Get the metrics for a given symbol and font, after transformation (i.e.
 * after following replacement from symbols.js)
 */
⋮----
/**
 * Puts a delimiter span in a given style, and adds appropriate height, depth,
 * and maxFontSizes.
 */
⋮----
/**
 * Makes a small delimiter. This is a delimiter that comes in the Main-Regular
 * font, but is restyled to either be in textstyle, scriptstyle, or
 * scriptscriptstyle.
 */
⋮----
/**
 * Builds a symbol in the given font size (note size is an integer)
 */
⋮----
/**
 * Makes a large delimiter. This is a delimiter that comes in the Size1, Size2,
 * Size3, or Size4 fonts. It is always rendered in textstyle.
 */
⋮----
/**
 * Make an inner span with the given offset and in the given font. This is used
 * in `makeStackedDelim` to make the stacking pieces for the delimiter.
 */
⋮----
let sizeClass; // Apply the correct CSS class to choose the right font.
⋮----
/* if (font === "Size4-Regular") */
⋮----
const inner = buildCommon.makeSpan(["delimsizinginner", sizeClass], [buildCommon.makeSpan([], [buildCommon.makeSymbol(symbol, font, mode)])]); // Since this will be passed into `makeVList` in the end, wrap the element
// in the appropriate tag that VList uses.
⋮----
}; // Helper for makeStackedDelim
⋮----
/**
 * Make a stacked delimiter out of a given delimiter, with the total height at
 * least `heightTotal`. This routine is mentioned on page 442 of the TeXbook.
 */
⋮----
// There are four parts, the top, an optional middle, a repeated part, and a
// bottom.
⋮----
middle = null; // Also keep track of what font the delimiters are in
⋮----
let font = "Size1-Regular"; // We set the parts and font based on the symbol. Note that we use
// '\u23d0' instead of '|' and '\u2016' instead of '\\|' for the
// repeats of the arrows
⋮----
} // Get the metrics of the four sections
⋮----
middleFactor = 2; // repeat symmetrically above and below middle
} // Calculate the minimal height that the delimiter can have.
// It is at least the size of the top, bottom, and optional middle combined.
⋮----
const minHeight = topHeightTotal + bottomHeightTotal + middleHeightTotal; // Compute the number of copies of the repeat symbol we will need
⋮----
const repeatCount = Math.max(0, Math.ceil((heightTotal - minHeight) / (middleFactor * repeatHeightTotal))); // Compute the total height of the delimiter including all the symbols
⋮----
const realHeightTotal = minHeight + repeatCount * middleFactor * repeatHeightTotal; // The center of the delimiter is placed at the center of the axis. Note
// that in this context, "center" means that the delimiter should be
// centered around the axis in the current style, while normally it is
// centered around the axis in textstyle.
⋮----
} // Calculate the depth
⋮----
const depth = realHeightTotal / 2 - axisHeight; // This function differs from the TeX procedure in one way.
// We shift each repeat element downwards by 0.005em, to prevent a gap
// due to browser floating point rounding error.
// Then, at the last element-to element joint, we add one extra repeat
// element to cover the gap created by the shifts.
// Find the shift needed to align the upper end of the extra element at a point
// 0.005em above the lower end of the top element.
⋮----
const shiftOfExtraElement = (repeatCount + 1) * 0.005 - repeatHeightTotal; // Now, we start building the pieces that will go into the vlist
// Keep a list of the inner pieces
⋮----
const inners = []; // Add the bottom symbol
⋮----
// Add that many symbols
⋮----
inners.push(lap); // overlap
⋮----
// When there is a middle bit, we need the middle part and two repeated
// sections
⋮----
} // Insert one extra repeat element.
⋮----
inners.push(lap); // Now insert the middle of the brace.
⋮----
} // To cover the gap create by the overlaps, insert one more repeat element,
// at a position that juts 0.005 above the bottom of the top element.
⋮----
// Parentheses need a short repeat element in order to avoid an overrun.
// We'll make a 0.3em tall element from a SVG.
⋮----
} // Add the top symbol
⋮----
inners.push(makeInner(top, font, mode)); // Finally, build the vlist
⋮----
}; // All surds have 0.08em padding above the viniculum inside the SVG.
// That keeps browser span height rounding error from pinching the line.
⋮----
const vbPad = 80; // padding above the surd, measured inside the viewBox.
⋮----
const emPad = 0.08; // padding, in ems, measured in the document.
⋮----
// Note: 1000:1 ratio of viewBox to document em width.
⋮----
/**
 * Make a sqrt image of the given height,
 */
⋮----
// Define a newOptions that removes the effect of size changes such as \Huge.
// We don't pick different a height surd for \Huge. For it, we scale up.
const newOptions = options.havingBaseSizing(); // Pick the desired surd glyph from a sequence of surds.
⋮----
let sizeMultiplier = newOptions.sizeMultiplier; // default
// The standard sqrt SVGs each have a 0.04em thick viniculum.
// If Settings.minRuleThickness is larger than that, we add extraViniculum.
⋮----
const extraViniculum = Math.max(0, options.minRuleThickness - options.fontMetrics().sqrtRuleThickness); // Create a span containing an SVG image of a sqrt symbol.
⋮----
let advanceWidth; // We create viewBoxes with 80 units of "padding" above each surd.
// Then browser rounding error on the parent span height will not
// encroach on the ink of the viniculum. But that padding is not
// included in the TeX-like `height` used for calculation of
// vertical alignment. So texHeight = span.height < span.style.height.
⋮----
// Get an SVG that is derived from glyph U+221A in font KaTeX-Main.
// 1000 unit normal glyph height.
⋮----
sizeMultiplier = 1.0; // mimic a \textfont radical
⋮----
sizeMultiplier = 0.7; // mimic a \scriptfont radical
⋮----
advanceWidth = 0.833 / sizeMultiplier; // from the font.
⋮----
// These SVGs come from fonts: KaTeX_Size1, _Size2, etc.
⋮----
advanceWidth = 1.0 / sizeMultiplier; // 1.0 from the font.
⋮----
// Tall sqrt. In TeX, this would be stacked using multiple glyphs.
// We'll use a single SVG to accomplish the same thing.
⋮----
// Calculate the actual line width.
// This actually should depend on the chosen font -- e.g. \boldmath
// should use the thicker surd symbols from e.g. KaTeX_Main-Bold, and
// have thicker rules.
⋮----
}; // There are three kinds of delimiters, delimiters that stack when they become
// too large
⋮----
const stackLargeDelimiters = ["(", "\\lparen", ")", "\\rparen", "[", "\\lbrack", "]", "\\rbrack", "\\{", "\\lbrace", "\\}", "\\rbrace", "\\lfloor", "\\rfloor", "\u230a", "\u230b", "\\lceil", "\\rceil", "\u2308", "\u2309", "\\surd"]; // delimiters that always stack
⋮----
const stackAlwaysDelimiters = ["\\uparrow", "\\downarrow", "\\updownarrow", "\\Uparrow", "\\Downarrow", "\\Updownarrow", "|", "\\|", "\\vert", "\\Vert", "\\lvert", "\\rvert", "\\lVert", "\\rVert", "\\lgroup", "\\rgroup", "\u27ee", "\u27ef", "\\lmoustache", "\\rmoustache", "\u23b0", "\u23b1"]; // and delimiters that never stack
⋮----
const stackNeverDelimiters = ["<", ">", "\\langle", "\\rangle", "/", "\\backslash", "\\lt", "\\gt"]; // Metrics of the different sizes. Found by looking at TeX's output of
// $\bigl| // \Bigl| \biggl| \Biggl| \showlists$
// Used to create stacked delimiters of appropriate sizes in makeSizedDelim.
⋮----
/**
 * Used to create a delimiter of a specific size, where `size` is 1, 2, 3, or 4.
 */
⋮----
// < and > turn into \langle and \rangle in delimiters
⋮----
} // Sized delimiters are never centered.
⋮----
/**
 * There are three different sequences of delimiter sizes that the delimiters
 * follow depending on the kind of delimiter. This is used when creating custom
 * sized delimiters to decide whether to create a small, large, or stacked
 * delimiter.
 *
 * In real TeX, these sequences aren't explicitly defined, but are instead
 * defined inside the font metrics. Since there are only three sequences that
 * are possible for the delimiters that TeX defines, it is easier to just encode
 * them explicitly here.
 */
⋮----
// Delimiters that never stack try small delimiters and large delimiters only
⋮----
}]; // Delimiters that always stack try the small delimiters first, then stack
⋮----
}]; // Delimiters that stack when large try the small and then large delimiters, and
// stack afterwards
⋮----
/**
 * Get the font used in a delimiter based on what kind of delimiter it is.
 * TODO(#963) Use more specific font family return type once that is introduced.
 */
⋮----
/**
 * Traverse a sequence of types of delimiters to decide what kind of delimiter
 * should be used to create a delimiter of the given height+depth.
 */
⋮----
// Here, we choose the index we should start at in the sequences. In smaller
// sizes (which correspond to larger numbers in style.size) we start earlier
// in the sequence. Thus, scriptscript starts at index 3-3=0, script starts
// at index 3-2=1, text starts at 3-1=2, and display starts at min(2,3-0)=2
⋮----
// This is always the last delimiter, so we just break the loop now.
⋮----
let heightDepth = metrics.height + metrics.depth; // Small delimiters are scaled down versions of the same font, so we
// account for the style change size.
⋮----
} // Check if the delimiter at this size works for the given height.
⋮----
} // If we reached the end of the sequence, return the last sequence element.
⋮----
/**
 * Make a delimiter of a given height+depth, with optional centering. Here, we
 * traverse the sequences, and create a delimiter that the sequence tells us to.
 */
⋮----
} // Decide what sequence to use
⋮----
} // Look through the sequence
⋮----
const delimType = traverseSequence(delim, height, sequence, options); // Get the delimiter from font glyphs.
// Depending on the sequence element we decided on, call the
// appropriate function.
⋮----
/* if (delimType.type === "stack") */
⋮----
/**
 * Make a delimiter for use with `\left` and `\right`, given a height and depth
 * of an expression that the delimiters surround.
 */
⋮----
// We always center \left/\right delimiters, so the axis is always shifted
const axisHeight = options.fontMetrics().axisHeight * options.sizeMultiplier; // Taken from TeX source, tex.web, function make_left_right
⋮----
const totalHeight = Math.max( // In real TeX, calculations are done using integral values which are
// 65536 per pt, or 655360 per em. So, the division here truncates in
// TeX but doesn't here, producing different results. If we wanted to
// exactly match TeX's calculation, we could do
//   Math.floor(655360 * maxDistFromAxis / 500) *
//    delimiterFactor / 655360
// (To see the difference, compare
//    x^{x^{\left(\rule{0.1em}{0.68em}\right)}}
// in TeX and KaTeX)
maxDistFromAxis / 500 * delimiterFactor, 2 * maxDistFromAxis - delimiterExtend); // Finally, we defer to `makeCustomSizedDelim` with our calculated total
// height
⋮----
// Extra data needed for the delimiter handler down below
⋮----
// Delimiter functions
function checkDelimiter(delim, context)
⋮----
// Empty delimiters still count as elements, even though they don't
// show anything.
⋮----
} // Use delimiter.sizedDelim to generate the delimiter.
⋮----
mathmlBuilder: group => {
    const children = [];

if (group.delim !== ".")
⋮----
// Only some of the delimsizing functions act as fences, and they
// return "mopen" or "mclose" mclass.
⋮----
// Explicitly disable fencing if it's not a fence, to override the
// defaults.
⋮----
function assertParsed(group)
⋮----
// \left case below triggers parsing of \right in
//   `const right = parser.parseFunction();`
// uses this return value.
⋮----
color // undefined if not set via \color
⋮----
const parser = context.parser; // Parse out the implicit body
⋮----
++parser.leftrightDepth; // parseExpression stops before '\\right'
⋮----
--parser.leftrightDepth; // Check the next token
⋮----
assertParsed(group); // Build the inner expression
⋮----
let hadMiddle = false; // Calculate its height and depth
⋮----
// Property `isMiddle` not defined on `span`. See comment in
// "middle"'s htmlBuilder.
// $FlowFixMe
⋮----
} // The size of delimiters is the same, regardless of what style we are
// in. Thus, to correctly calculate the size of delimiter we need around
// a group, we scale down the inner size based on the size.
⋮----
// Empty delimiters in \left and \right make null delimiter spaces.
⋮----
// Otherwise, use leftRightDelim to generate the correct sized
// delimiter.
⋮----
} // Add it to the beginning of the expression
⋮----
inner.unshift(leftDelim); // Handle middle delimiters
⋮----
const middleDelim = inner[i]; // Property `isMiddle` not defined on `span`. See comment in
// "middle"'s htmlBuilder.
// $FlowFixMe
⋮----
// Apply the options that were active when \middle was called
⋮----
let rightDelim; // Same for the right delimiter, but using color specified by \color
⋮----
} // Add it to the end of the expression.
⋮----
}; // Property `isMiddle` not defined on `span`. It is only used in
// this file above.
// TODO: Fix this violation of the `span` type and possibly rename
// things since `isMiddle` sounds like a boolean, but is a struct.
// $FlowFixMe
⋮----
// A Firefox \middle will stretch a character vertically only if it
// is in the fence part of the operator dictionary at:
// https://www.w3.org/TR/MathML3/appendixc.html.
// So we need to avoid U+2223 and use plain "|" instead.
⋮----
middleNode.setAttribute("fence", "true"); // MathML gives 5/18em spacing to each <mo> element.
// \middle should get delimiter spacing instead.
⋮----
const htmlBuilder$2 = (group, options) =>
⋮----
// \cancel, \bcancel, \xcancel, \sout, \fbox, \colorbox, \fcolorbox
// Some groups can return document fragments.  Handle those by wrapping
// them in a span.
⋮----
let imgShift = 0; // In the LaTeX cancel package, line geometry is slightly different
// depending on whether the subject is wider than it is tall, or vice versa.
// We don't know the width of a group, so as a proxy, we test if
// the subject is a single character. This captures most of the
// subjects that should get the "tall" treatment.
⋮----
// Add horizontal padding
⋮----
} // Add vertical padding
⋮----
let ruleThickness = 0; // ref: cancel package: \advance\totalheight2\p@ % "+2"
⋮----
ruleThickness = Math.max(options.fontMetrics().fboxrule, // default
options.minRuleThickness // User override.
⋮----
children: [// Put the color background behind inner;
⋮----
children: [// Write the \cancel stroke on top of inner.
⋮----
// The cancel package documentation says that cancel lines add their height
// to the expression, but tests show that isn't how it actually works.
⋮----
// cancel does not create horiz space for its line extension.
⋮----
const mathmlBuilder$2 = (group, options) =>
⋮----
// <menclose> doesn't have a good notation option. So use <mpadded>
// instead. Set some attributes that come included with <menclose>.
⋮----
node.setAttribute("lspace", `${fboxsep}pt`); //
⋮----
const thk = Math.max(options.fontMetrics().fboxrule, // default
options.minRuleThickness // user override
⋮----
handler(_ref2, args, optArgs)
⋮----
handler(_ref3, args)
⋮----
handler(_ref4, args, optArgs)
⋮----
/**
 * All registered environments.
 * `environments.js` exports this same dictionary again and makes it public.
 * `Parser.js` requires this dictionary via `environments.js`.
 */
⋮----
function defineEnvironment(_ref)
⋮----
// Set default values of environments.
⋮----
// TODO: The value type of _environments should be a type union of all
// possible `EnvSpec<>` possibilities instead of `EnvSpec<*>`, which is
// an existential type.
// $FlowFixMe
⋮----
function getHLines(parser)
⋮----
// Return an array. The array length = number of hlines.
// Each element in the array tells if the line is dashed.
⋮----
/**
 * Parse the body of the environment, with rows delimited by \\ and
 * columns delimited by &, and create a nested list in row-major order
 * with one group per cell.  If given an optional argument style
 * ("text", "display", etc.), then each cell is cast into that style.
 */
⋮----
function parseArray(parser, _ref, style)
⋮----
// Parse body of array with \\ temporarily mapped to \cr
⋮----
parser.gullet.macros.set("\\\\", "\\cr"); // Get current arraystretch if it's not set by the environment
⋮----
// Default \arraystretch from lttab.dtx
⋮----
} // Start group for first cell
⋮----
const hLinesBeforeRow = []; // Test for \hline at the top of the array.
⋮----
// eslint-disable-line no-constant-condition
// Parse each cell in its own group (namespace)
⋮----
// Arrays terminate newlines with `\crcr` which consumes a `\cr` if
// the last line is empty.
// NOTE: Currently, `cell` is the last item added into `row`.
⋮----
rowGaps.push(cr.size); // check for \hline(s) following the row separator
⋮----
} // End cell group
⋮----
parser.gullet.endGroup(); // End array group defining \\
⋮----
} // Decides on a style for cells in an array according to whether the given
// environment name starts with the letter 'd'.
⋮----
function dCellStyle(envName)
⋮----
const ruleThickness = Math.max( // From LaTeX \showthe\arrayrulewidth. Equals 0.04 em.
options.fontMetrics().arrayRuleWidth, options.minRuleThickness // User override.
); // Horizontal spacing
⋮----
let arraycolsep = 5 * pt; // default value, i.e. \arraycolsep in article.cls
⋮----
// We're in a {smallmatrix}. Default column space is \thickspace,
// i.e. 5/18em = 0.2778em, per amsmath.dtx for {smallmatrix}.
// But that needs adjustment because LaTeX applies \scriptstyle to the
// entire array, including the colspace, but this function applies
// \scriptstyle only inside each element.
⋮----
} // Vertical spacing
⋮----
const baselineskip = 12 * pt; // see size10.clo
// Default \jot from ltmath.dtx
// TODO(edemaine): allow overriding \jot via \setlength (#687)
⋮----
const arstrutHeight = 0.7 * arrayskip; // \strutbox in ltfsstrc.dtx and
⋮----
const arstrutDepth = 0.3 * arrayskip; // \@arstrutbox in lttab.dtx
⋮----
let totalHeight = 0; // Set a position for \hline(s) at the top of the array, if any.
⋮----
function setHLinePos(hlinesInGap)
⋮----
let height = arstrutHeight; // \@array adds an \@arstrut
⋮----
let depth = arstrutDepth; // to each tow (via the template)
⋮----
// \@argarraycr
⋮----
depth = gap; // \@xargarraycr
⋮----
} // In AMS multiline environments such as aligned and gathered, rows
// correspond to lines that have additional \jot added to the
// \baselineskip via \openup.
⋮----
totalHeight += depth + gap; // \@yargarraycr
⋮----
body[r] = outrow; // Set a position for \hline(s), if any.
⋮----
for (c = 0, colDescrNum = 0; // Continue while either there are more columns or more column
// descriptions, so trailing separators don't get lost.
⋮----
// If there is more than one separator in a row, add a space
// between them.
⋮----
body = buildCommon.makeSpan(["mtable"], cols); // Add \hline(s), if any.
⋮----
})); // Set column alignment, row spacing, column spacing, and
// array lines by setting attributes on the table element.
// Set the row spacing. In MathML, we specify a gap distance.
// We do not use rowGap[] because MathML automatically increases
// cell height with the height/depth of the element content.
// LaTeX \arraystretch multiplies the row baseline-to-baseline distance.
// We simulate this by adding (arraystretch - 1)em to the gap. This
// does a reasonable job of adjusting arrays containing 1 em tall content.
// The 0.16 and 0.09 values are found empirically. They produce an array
// similar to LaTeX and in which content does not interfere with \hines.
⋮----
const gap = group.arraystretch === 0.5 ? 0.1 // {smallmatrix}, {subarray}
⋮----
table.setAttribute("rowspacing", gap + "em"); // MathML table lines go only between cells.
// To place a line on an edge we'll use <menclose>, if necessary.
⋮----
// Find column alignment, column spacing, and  vertical lines.
⋮----
// MathML accepts only single lines between cells.
// So we read only the first of consecutive separators.
⋮----
} // Set column spacing.
⋮----
} // Address \hline and \hdashline
⋮----
rowLines += hlines[i].length === 0 ? "none " // MathML accepts only a single line between rows. Read one element.
⋮----
// A small array. Wrap in scriptstyle so row gap is not too large.
⋮----
}; // Convenience function for aligned and alignedat environments.
⋮----
}, "display"); // Determining number of columns.
// 1. If the first argument is given, we use it as a number of columns,
//    and makes sure that each row doesn't exceed that number.
// 2. Otherwise, just count number of columns = maximum number
//    of cells in each row ("aligned" mode -- isAligned will be true).
//
// At the same time, prepend empty group {} at beginning of every second
// cell in each row (starting with second cell) so that operators become
// binary.  This behavior is implemented in amsmath's \start@aligned.
⋮----
// Modify ordgroup node within styling node
⋮----
// Case 1
⋮----
// Case 2
⋮----
}); // Adjusting alignment.
// In aligned mode, we add one \qquad between columns;
// otherwise we add nothing.
⋮----
// "aligned" mode.
pregap = 1; // add one \quad
⋮----
}; // Arrays are part of LaTeX, defined in lttab.dtx so its documentation
// is part of the source2e.pdf file of LaTeX2e source documentation.
// {darray} is an {array} environment where cells are set in \displaystyle,
// as defined in nccmath.sty.
⋮----
handler(context, args)
⋮----
// Since no types are specified above, the two possibilities are
// - The argument is wrapped in {} or [], in which case Parser's
//   parseGroup() returns an "ordgroup" wrapping some symbol node.
// - The argument is a bare symbol node.
⋮----
hskipBeforeAndAfter: true // \@preamble in lttab.dtx
⋮----
}); // The matrix environments of amsmath builds on the array environment
// of LaTeX, which is discussed above.
⋮----
handler(context)
⋮----
}[context.envName]; // \hskip -\arraycolsep in amsmath
⋮----
rightColor: undefined // \right uninfluenced by \color in array
⋮----
// Parsing of {subarray} is similar to {array}
⋮----
const ca = node.text; // {subarray} only recognizes "l" & "c"
⋮----
}); // A cases environment (in amsmath.sty) is almost equivalent to
// \def\arraystretch{1.2}%
// \left\{\begin{array}{@{}l@{\quad}l@{}} … \end{array}\right.
// {dcases} is a {cases} environment where cells are set in \displaystyle,
// as defined in mathtools.sty.
// {rcases} is another mathtools environment. It's brace is on the right side.
⋮----
// TODO(kevinb) get the current style.
// For now we use the metrics for TEXT style which is what we were
// doing before.  Before attempting to get the current style we
// should look at TeX's behavior especially for \over and matrices.
⋮----
/* 1em quad */
⋮----
}); // An aligned environment is like the align* environment
// except it operates within math mode.
// Note that we assume \nomallineskiplimit to be zero,
// so that \strut@ is the same as \strut.
⋮----
}); // A gathered environment is like an array environment with one centered
// column, but where rows are considered lines so get \jot line spacing
// and contents are set in \displaystyle.
⋮----
}); // alignat environment is like an align environment, but one must explicitly
// specify maximum number of columns in each row, and can adjust spacing between
// each columns.
⋮----
// One for numbered and for unnumbered;
// but, KaTeX doesn't supports math numbering yet,
// they make no difference for now.
⋮----
}); // Catch \hline outside array environment
⋮----
// Doesn't matter what this is.
⋮----
// defineEnvironment definitions.
// $FlowFixMe, "environment" handler returns an environment ParseNode
⋮----
// begin...end is similar to left...right
⋮----
} // Build the environment object. Arguments and other information will
// be made available to the begin and end methods using properties.
⋮----
function htmlBuilder$4(group, options)
⋮----
function mathmlBuilder$4(group, options)
⋮----
} // Set spacing based on what is the most likely adjacent atom type.
// See TeXbook p170.
⋮----
node.attributes.lspace = "0.22em"; // medium space
⋮----
node.attributes.rspace = "0.17em"; // thinspace
⋮----
} // MathML <mo> default space is 5/18 em, so <mrel> needs no action.
// Ref: https://developer.mozilla.org/en-US/docs/Web/MathML/Element/mo
⋮----
} // Math class commands except \mathop
⋮----
// TODO(kevinb): don't prefix with 'm'
⋮----
const binrelClass = arg => {
  // \binrel@ spacing varies with (bin|rel|ord) of the atom in the argument.
  // (by rendering separately and with {}s before and after, and measuring
  // the change in spacing).  We'll do roughly the same by detecting the
  // atom type directly.
  const atom = arg.type === "ordgroup" && arg.body.length ? arg.body[0] : arg;

if (atom.type === "atom" && (atom.family === "bin" || atom.family === "rel"))
⋮----
// \binrel@ spacing varies with (bin|rel|ord) of the atom in the argument.
// (by rendering separately and with {}s before and after, and measuring
// the change in spacing).  We'll do roughly the same by detecting the
// atom type directly.
⋮----
}; // \@binrel{x}{y} renders like y but as mbin/mrel/mord if x is mbin/mrel/mord.
// This is equivalent to \binrel@{x}\binrel@@{y} in AMSTeX.
⋮----
}); // Build a relation or stacked op by placing one symbol on top of another
⋮----
// LaTeX applies \binrel spacing to \overset and \underset.
⋮----
mclass = "mrel"; // for \stackrel
⋮----
// TODO(kevinb): implement \\sl and \\sc
⋮----
const htmlBuilder$5 = (group, options) =>
⋮----
const mathmlBuilder$5 = (group, options) =>
⋮----
names: [// styles, except \boldsymbol defined below
"\\mathrm", "\\mathit", "\\mathbf", "\\mathnormal", // families
"\\mathbb", "\\mathcal", "\\mathfrak", "\\mathscr", "\\mathsf", "\\mathtt", // aliases, except \bm defined below
⋮----
handler: (_ref2, args) =>
⋮----
const isCharacterBox = utils.isCharacterBox(body); // amsbsy.sty's \boldsymbol uses \binrel spacing to inherit the
// argument's bin|rel|ord status
⋮----
}); // Old font changing functions
⋮----
handler: (_ref3, args) =>
⋮----
const adjustStyle = (size, originalStyle) =>
⋮----
// Figure out what style this fraction should be in based on the
// function used
⋮----
// Get display style as a default.
// If incoming style is sub/sup, use style.text() to get correct size.
⋮----
// We're in a \tfrac but incoming style is displaystyle, so:
⋮----
const htmlBuilder$6 = (group, options) =>
⋮----
// Fractions are handled in the TeXbook on pages 444-445, rules 15(a-e).
⋮----
// \cfrac inserts a \strut into the numerator.
// Get \strut dimensions from TeXbook page 353.
⋮----
} // Rule 15b
⋮----
// Rule 15c
⋮----
// Rule 15d
⋮----
} // Since we manually change the style sometimes (with \dfrac or \tfrac),
// account for the possible size change here.
⋮----
frac.depth *= newOptions.sizeMultiplier / options.sizeMultiplier; // Rule 15e
⋮----
rightDelim = buildCommon.makeSpan([]); // zero width for \cfrac
⋮----
const mathmlBuilder$6 = (group, options) =>
⋮----
names: ["\\cfrac", "\\dfrac", "\\frac", "\\tfrac", "\\dbinom", "\\binom", "\\tbinom", "\\\\atopfrac", // can’t be entered directly
⋮----
}); // Infix generalized fractions -- these are not rendered directly, but replaced
// immediately by one of the variants above.
⋮----
const denom = args[5]; // Look into the parse nodes to get the desired delimiters.
⋮----
// \genfrac acts differently than \above.
// \genfrac treats an empty size group as a signal to use a
// standard bar size. \above would see size = 0 and omit the bar.
⋮----
} // Find out if we want displaystyle, textstyle, etc.
⋮----
}); // \above is an infix fraction that also defines a fraction bar size.
⋮----
handler(_ref4, args)
⋮----
handler: (_ref5, args) =>
⋮----
// NOTE: Unlike most `htmlBuilder`s, this one handles not only "horizBrace", but
const htmlBuilder$7 = (grp, options) =>
⋮----
const style = options.style; // Pull out the `ParseNode<"horizBrace">` if `grp` is a "supsub" node.
⋮----
// Ref: LaTeX source2e: }}}}\limits}
// i.e. LaTeX treats the brace similar to an op and passes it
// with \limits, so we need to assign supsub style.
⋮----
} // Build the base group
⋮----
const body = buildGroup(group.base, options.havingBaseStyle(Style$1.DISPLAY)); // Create the stretchy element
⋮----
const braceBody = stretchy.svgSpan(group, options); // Generate the vlist, with the appropriate kerns        ┏━━━━━━━━┓
// This first vlist contains the content and the brace:   equation
⋮----
}, options); // $FlowFixMe: Replace this with passing "svg-align" into makeVList.
⋮----
}, options); // $FlowFixMe: Replace this with passing "svg-align" into makeVList.
⋮----
// To write the supsub, wrap the first vlist in another vlist:
// They can't all go in the same vlist, because the note might be
// wider than the equation. We want the equation to control the
// brace width.
//      note          long note           long note
//   ┏━━━━━━━━┓   or    ┏━━━┓     not    ┏━━━━━━━━━┓
//    equation           eqn                 eqn
⋮----
const mathmlBuilder$7 = (group, options) =>
⋮----
}; // Horizontal stretchy braces
⋮----
// str is a number with no unit specified.
// default unit is bp, per graphix package.
⋮----
// sign + magnitude, cast to number
⋮----
handler: (_ref, args, optArgs) =>
⋮----
}; // sorta character sized.
⋮----
const attributeStr = assertNodeType(optArgs[0], "raw").string; // Parser.js does not parse key/value pairs. We get a string.
⋮----
// No alt given. Use the file name. Strip away the path.
⋮----
// Horizontal spacing commands
⋮----
const mathFunction = funcName[1] === 'm'; // \mkern, \mskip
⋮----
// !mathFunction
⋮----
// Horizontal overlap functions
⋮----
// mathllap, mathrlap, mathclap
⋮----
// ref: https://www.math.lsu.edu/~aperlis/publications/mathclap/
inner = buildCommon.makeSpan([], [buildGroup(group.body, options)]); // wrap, since CSS will center a .clap > .inner > span
⋮----
let node = buildCommon.makeSpan([group.alignment], [inner, fix], options); // At this point, we have correctly set horizontal alignment of the
// two items involved in the lap.
// Next, use a strut to set the height of the HTML bounding box.
// Otherwise, a tall argument may be misplaced.
// This code resolved issue #1153
⋮----
node.children.unshift(strut); // Next, prevent vertical misplacement when next to something tall.
// This code resolves issue #1234
⋮----
// mathllap, mathrlap, mathclap
⋮----
}); // Check for extra closing math delimiters
⋮----
// Doesn't matter what this is.
⋮----
const chooseMathStyle = (group, options) =>
⋮----
// For an operator with limits, assemble the base, sup, and sub into a span.
const assembleSupSub = (base, supGroup, subGroup, options, style, slant, baseShift) =>
⋮----
let sup; // We manually have to handle the superscripts and subscripts. This,
// aside from the kern calculations, is copied from supsub.
⋮----
} // Build the final group as a vlist of the possible subscript, base,
// and possible superscript.
⋮----
const top = base.height - baseShift; // Shift the limits by the slant of the symbol. Note
// that we are supposed to shift the limits by 1/2 of the slant,
// but since we are centering the limits adding a full slant of
// margin will shift by 1/2 that.
⋮----
// This case probably shouldn't occur (this would mean the
// supsub was sending us a group with no superscript or
// subscript) but be safe.
⋮----
// Limits, symbols
// Most operators have a large successor symbol, but these don't.
const noSuccessor = ["\\smallint"]; // NOTE: Unlike most `htmlBuilder`s, this one handles not only "op", but also
// "supsub" since some of them (like \int) can affect super/subscripting.
⋮----
const htmlBuilder$8 = (grp, options) =>
⋮----
// Operators are handled in the TeXbook pg. 443-444, rule 13(a).
⋮----
// If we have limits, supsub will pass us its group to handle. Pull
// out the superscript and subscript and set the group to the op in
// its base.
⋮----
// Most symbol operators get larger in displaystyle (rule 13)
⋮----
// If this is a symbol, create the symbol.
⋮----
// No font glyphs yet, so use a glyph w/o the oval.
// TODO: When font glyphs are available, delete this code.
stash = group.name.substr(1); // $FlowFixMe
⋮----
// We're in \oiint or \oiiint. Overlay the oval.
// TODO: When font glyphs are available, delete this code.
⋮----
}, options); // $FlowFixMe
⋮----
base.classes.unshift("mop"); // $FlowFixMe
⋮----
// If this is a list, compose that list.
⋮----
base.classes[0] = "mop"; // replace old mclass
⋮----
// Otherwise, this is a text operator. Build the text from the
// operator's name.
// TODO(emily): Add a space in the middle of some of these
// operators, like \limsup
⋮----
} // If content of op is a single symbol, shift it vertically.
⋮----
// We suppress the shift of the base of \overset and \underset. Otherwise,
// shift the symbol so its center lies on the axis (rule 13). It
// appears that our fonts have the centers of the symbols already
// almost on the axis, so these numbers are very small. Note we
// don't actually apply this here, but instead it is used either in
// the vlist creation or separately when there are no limits.
baseShift = (base.height - base.depth) / 2 - options.fontMetrics().axisHeight; // The slant of the symbol is just its italic correction.
// $FlowFixMe
⋮----
const mathmlBuilder$8 = (group, options) =>
⋮----
// This is a symbol. Just add the symbol.
⋮----
// This is an operator with children. Add them.
⋮----
// This is a text operator. Add all of the characters from the
// operator's name.
node = new MathNode("mi", [new TextNode(group.name.slice(1))]); // Append an <mo>&ApplyFunction;</mo>.
// ref: https://www.w3.org/TR/REC-MathML/chap3_2.html#sec3.2.4
⋮----
}); // Note: calling defineFunction with a type that's already been defined only
// works because the same htmlBuilder and mathmlBuilder are being used.
⋮----
}); // There are 2 flags for operators; whether they produce limits in
// displaystyle, and whether they are symbols and should grow in
// displaystyle. These four groups cover the four possible choices.
⋮----
}; // No limits, not symbols
⋮----
}); // Limits, not symbols
⋮----
}); // No limits, symbols
⋮----
handler(_ref5)
⋮----
// NOTE: Unlike most `htmlBuilder`s, this one handles not only
// "operatorname", but also  "supsub" since \operatorname* can
const htmlBuilder$9 = (grp, options) =>
⋮----
// Operators are handled in the TeXbook pg. 443-444, rule 13(a).
⋮----
// If we have limits, supsub will pass us its group to handle. Pull
// out the superscript and subscript and set the group to the op in
// its base.
⋮----
// $FlowFixMe: Check if the node has a string `text` property.
⋮----
}); // Consolidate function names into symbol characters.
⋮----
// Per amsopn package,
// change minus to hyphen and \ast to asterisk
⋮----
const mathmlBuilder$9 = (group, options) =>
⋮----
// The steps taken here are similar to the html version.
let expression = buildExpression$1(group.body, options.withFont("mathrm")); // Is expression a string or has it something like a fraction?
⋮----
let isAllString = true; // default
⋮----
// Do nothing yet.
⋮----
// Write a single TextNode instead of multiple nested tags.
⋮----
identifier.setAttribute("mathvariant", "normal"); // \u2061 is the same as &ApplyFunction;
// ref: https://www.w3schools.com/charsets/ref_html_entities_a.asp
⋮----
}; // \operatorname
// amsopn.dtx: \mathop{#1\kern\z@\operator@font#3}\newmcodes@
⋮----
// Overlines are handled in the TeXbook pg 443, Rule 9.
// Build the inner group in the cramped style.
const innerGroup = buildGroup(group.body, options.havingCrampedStyle()); // Create the line above the body
⋮----
const line = buildCommon.makeLineSpan("overline-line", options); // Generate the vlist, with the appropriate kerns
⋮----
const elements = buildExpression(group.body, options.withPhantom(), false); // \phantom isn't supposed to affect the elements it contains.
// See "color" for more details.
⋮----
} // See smash for comment re: use of makeVList
⋮----
}, options); // For spacing, TeX treats \smash as a math group (same spacing as ord).
⋮----
// Make an empty span for the rule
const rule = buildCommon.makeSpan(["mord", "rule"], [], options); // Calculate the shift, width, and height of the rule, and account for units
⋮----
const shift = group.shift ? calculateSize(group.shift, options) : 0; // Style the rule to the right size
⋮----
rule.style.bottom = shift + "em"; // Record the height and width
⋮----
rule.depth = -shift; // Font size is the number large enough that the browser will
// reserve at least `absHeight` space above the baseline.
// The 1.125 factor was empirically determined
⋮----
function sizingGroup(value, options, baseOptions)
⋮----
const multiplier = options.sizeMultiplier / baseOptions.sizeMultiplier; // Add size-resetting classes to the inner list and set maxFontSize
// manually. Handle nested size changes.
⋮----
// This is a nested size change: e.g., inner[i] is the "b" in
// `\Huge a \small b`. Override the old size (the `reset-` class)
// but not the new size.
⋮----
const htmlBuilder$a = (group, options) =>
⋮----
// Handle sizing operators like \Huge. Real TeX doesn't actually allow
// these functions inside of math expressions, so we do some special
// handling.
⋮----
// Figure out what size to use based on the list of functions above
⋮----
const node = new mathMLTree.MathNode("mstyle", inner); // TODO(emily): This doesn't produce the correct size for nested size
// changes, because we don't keep state of what style we're currently
// in, so we can't reset the size to normal before changing it.  Now
// that we're passing an options parameter we should be able to fix
// this.
⋮----
// smash, with optional [tb], as in AMS
⋮----
// Optional [tb] argument is engaged.
// ref: amsmath: \renewcommand{\smash}[1][tb]{%
//               def\mb@t{\ht}\def\mb@b{\dp}\def\mb@tb{\ht\z@\z@\dp}%
⋮----
const node = tbArg.body[i]; // $FlowFixMe: Not every node type has a `text` property.
⋮----
node.height = 0; // In order to influence makeVList, we have to reset the children.
⋮----
} // At this point, we've reset the TeX-like height and depth values.
// But the span still has an HTML line height.
// makeVList applies "display: table-cell", which prevents the browser
// from acting on that line height. So we'll call makeVList now.
⋮----
}, options); // For spacing, TeX treats \hphantom as a math group (same spacing as ord).
⋮----
// Square roots are handled in the TeXbook pg. 443, Rule 11.
// First, we do the same steps as in overline to build the inner group
// and line
⋮----
// Render a small surd.
⋮----
} // Some groups can return document fragments.  Handle those by wrapping
// them in a span.
⋮----
inner = buildCommon.wrapFragment(inner, options); // Calculate the minimum size for the \surd delimiter
⋮----
} // Calculate the clearance between the body and line
⋮----
const minDelimiterHeight = inner.height + inner.depth + lineClearance + theta; // Create a sqrt SVG of the required minimum size
⋮----
const delimDepth = img.height - ruleWidth; // Adjust the clearance based on the delimiter size
⋮----
} // Shift the sqrt image
⋮----
inner.style.paddingLeft = advanceWidth + "em"; // Overlay the image and the argument.
⋮----
// Handle the optional root index
// The index is always in scriptscript style
⋮----
const rootm = buildGroup(group.index, newOptions, options); // The amount the index is shifted by. This is taken from the TeX
// source, in the definition of `\r@@t`.
⋮----
const toShift = 0.6 * (body.height - body.depth); // Build a VList with the superscript shifted up correctly
⋮----
}, options); // Add a class surrounding it so we can add on the appropriate
// kerning
⋮----
// parse out the implicit body
const body = parser.parseExpression(true, breakOnTokenText); // TODO: Refactor to avoid duplicating styleMap in multiple places (e.g.
// here and in buildHTML and de-dupe the enumeration of all the styles).
// $FlowFixMe: The names above exactly match the styles.
⋮----
// Figure out what style to use by pulling out the style from
// the function name
⋮----
// Style changes are handled in the TeXbook on pg. 442, Rule 3.
⋮----
// Figure out what style we're changing to.
⋮----
/**
 * Sometimes, groups perform special rules when they have superscripts or
 * subscripts attached to them. This function lets the `supsub` group know that
 * Sometimes, groups perform special rules when they have superscripts or
 * its inner element should handle the superscripts and subscripts instead of
 * handling them itself.
 */
⋮----
// Operators handle supsubs differently when they have limits
// (e.g. `\displaystyle\sum_2^3`)
⋮----
}; // Super scripts and subscripts, whose precise placement can depend on other
// functions that precede them.
⋮----
// Superscript and subscripts are handled in the TeXbook on page
// 445-446, rules 18(a-f).
// Here is where we defer to the inner group if it should handle
// superscripts and subscripts itself.
⋮----
const metrics = options.fontMetrics(); // Rule 18a
⋮----
} // Rule 18c
⋮----
} // scriptspace is a font-size-independent size, so scale it
// appropriately for use as the marginRight.
⋮----
// Subscripts shouldn't be shifted by the base's italic correction.
// Account for that by shifting the subscript back the appropriate
// amount. Note we only do this when the base is a single symbol.
⋮----
// $FlowFixMe
⋮----
const ruleWidth = metrics.defaultRuleThickness; // Rule 18e
⋮----
// Rule 18b
⋮----
// Rule 18c, d
⋮----
} // Wrap the supsub vlist in a span.msupsub to reset text-align.
⋮----
// Is the inner group a relevant horizonal brace?
⋮----
// Delims built here should not stretch vertically.
// See delimsizing.js for stretchy delims.
⋮----
// "mathord" and "textord" ParseNodes created in Parser.js from symbol Groups in
⋮----
// TODO(kevinb) merge adjacent <mn> nodes
// do it as a post processing step
⋮----
}; // A lookup table to determine whether a spacing function/symbol should be
// treated like a regular space character.  If a symbol or command is a key
// in this table, then it should be a regular space character.  Furthermore,
// the associated value may have a `className` specifying an extra CSS class
// to add to the created `span`.
⋮----
}; // ParseNode<"spacing"> created in Parser.js from the "spacing" symbol Groups in
// src/symbols.js.
⋮----
const className = regularSpace[group.text].className || ""; // Spaces are generated by adding an actual space. Each of these
// things has an entry in the symbols table, so these will be turned
// into appropriate outputs.
⋮----
// Spaces based on just a CSS class.
⋮----
// CSS-based MathML spaces (\nobreak, \allowbreak) are ignored
⋮----
const pad = () =>
⋮----
return table; // TODO: Left-aligned tags.
// Currently, the group and options passed here do not contain
// enough info to set tag alignment. `leqno` is in Settings but it is
// not passed to Options. On the HTML side, leqno is
// set by a CSS class applied in buildTree.js. That would have worked
// in MathML if browsers supported <mlabeledtr>. Since they don't, we
// need to rewrite the way this function is called.
⋮----
const optionsWithFont = (group, options) =>
⋮----
const font = group.font; // Checks if the argument is a font family or a font style.
⋮----
names: [// Font families
"\\text", "\\textrm", "\\textsf", "\\texttt", "\\textnormal", // Font weights
"\\textbf", "\\textmd", // Font Shapes
⋮----
// Underlines are handled in the TeXbook pg 443, Rule 10.
// Build the inner group.
const innerGroup = buildGroup(group.body, options); // Create the line to go below the body
⋮----
const line = buildCommon.makeLineSpan("underline-line", options); // Generate the vlist, with the appropriate kerns
⋮----
handler(context, args, optArgs)
⋮----
// \verb and \verb* are dealt with directly in Parser.js.
// If we end up here, it's because of a failure to match the two delimiters
// in the regex in Lexer.js.  LaTeX raises the following error when \verb is
// terminated by end of line (or file).
⋮----
const body = []; // \verb enters text mode and therefore is sized like \textstyle
⋮----
/**
 * Converts verb group into body string.
 *
 * \verb* replaces each space with an open box \u2423
 * \verb replaces each space with a no-break space \xA0
 */
⋮----
const makeVerb = group
⋮----
/** Include this to ensure that all functions are defined. */
⋮----
/**
 * The Lexer class handles tokenizing the input in various ways. Since our
 * parser expects us to be able to backtrack, the lexer allows lexing from any
 * given starting point.
 *
 * Its main exposed function is the `lex` function, which takes a position to
 * lex from and a type of token to lex. It defers to the appropriate `_innerLex`
 * function.
 *
 * The various `_innerLex` functions perform the actual lexing of different
 * kinds.
 */
⋮----
/* The following tokenRegex
 * - matches typical whitespace (but not NBSP etc.) using its first group
 * - does not match any control character \x00-\x1f except whitespace
 * - does not match a bare backslash
 * - matches any ASCII character except those just mentioned
 * - does not match the BMP private use area \uE000-\uF8FF
 * - does not match bare surrogate code units
 * - matches any BMP character except for those just described
 * - matches any valid Unicode surrogate pair
 * - matches a backslash followed by one or more letters
 * - matches a backslash followed by any BMP character, including newline
 * Just because the Lexer matches something doesn't mean it's valid input:
 * If there is no matching function or symbol definition, the Parser will
 * still reject the input.
 */
⋮----
const tokenRegexString = `(${spaceRegexString}+)|` + // whitespace
"([!-\\[\\]-\u2027\u202A-\uD7FF\uF900-\uFFFF]" + // single codepoint
`${combiningDiacriticalMarkString}*` + // ...plus accents
"|[\uD800-\uDBFF][\uDC00-\uDFFF]" + // surrogate pair
`${combiningDiacriticalMarkString}*` + // ...plus accents
"|\\\\verb\\*([^]).*?\\3" + // \verb*
"|\\\\verb([^*a-zA-Z]).*?\\4" + // \verb unstarred
"|\\\\operatorname\\*" + // \operatorname*
`|${controlWordWhitespaceRegexString}` + // \macroName + spaces
`|${controlSymbolRegexString})`; // \\, \', etc.
⋮----
/** Main Lexer class */
⋮----
class Lexer
⋮----
// category codes, only supports comment characters (14) for now
⋮----
// Separate accents from characters
⋮----
"%": 14 // comment character
⋮----
setCatcode(char, code)
/**
   * This function lexes a single token.
   */
⋮----
lex()
⋮----
// comment character
⋮----
this.tokenRegex.lastIndex = input.length; // EOF
⋮----
} // Trim any trailing whitespace from control word match
⋮----
/**
 * A `Namespace` refers to a space of nameable things like macros or lengths,
 * which can be `set` either globally or local to a nested group, using an
 * undo stack similar to how TeX implements this functionality.
 * Performance-wise, `get` and local `set` take constant time, while global
 * `set` takes time proportional to the depth of group nesting.
 */
class Namespace
⋮----
/**
   * Both arguments are optional.  The first argument is an object of
   * built-in mappings which never change.  The second argument is an object
   * of initial (global-level) mappings, which will constantly change
   * according to any global/top-level `set`s done.
   */
⋮----
/**
   * Start a new nested group, affecting future local `set`s.
   */
⋮----
beginGroup()
/**
   * End current nested group, restoring values before the group began.
   */
⋮----
endGroup()
/**
   * Detect whether `name` has a definition.  Equivalent to
   * `get(name) != null`.
   */
⋮----
has(name)
/**
   * Get the current value of a name, or `undefined` if there is no value.
   *
   * Note: Do not use `if (namespace.get(...))` to detect whether a macro
   * is defined, as the definition may be the empty string which evaluates
   * to `false` in JavaScript.  Use `if (namespace.get(...) != null)` or
   * `if (namespace.has(...))`.
   */
⋮----
get(name)
/**
   * Set the current value of a name, and optionally set it globally too.
   * Local set() sets the current value and (when appropriate) adds an undo
   * operation to the undo stack.  Global set() may change the undo
   * operation at every level, so takes time linear in their number.
   */
⋮----
set(name, value, global)
⋮----
// Global set is equivalent to setting in all groups.  Simulate this
// by destroying any undos currently scheduled for this name,
// and adding an undo with the *new* value (in case it later gets
// locally reset within this environment).
⋮----
// Undo this set at end of this group (possibly to `undefined`),
// unless an undo is already in place, in which case that older
// value is the correct one.
⋮----
/**
 * Predefined macros for KaTeX.
 * This can be used to define some commands in terms of others.
 */
⋮----
function defineMacro(name, body)
⋮----
} //////////////////////////////////////////////////////////////////////
// macro tools
⋮----
// The expansion is the token itself; but that token is interpreted
// as if its meaning were ‘\relax’ if it is a control sequence that
// would ordinarily be expanded by TeX’s expansion rules.
⋮----
// TeX first reads the token that comes immediately after \expandafter,
// without expanding it; let’s call this token t. Then TeX reads the
// token that comes after t (and possibly more tokens, if that token
// has an argument), replacing it by its expansion. Finally TeX puts
// t back in front of that expansion.
⋮----
context.expandOnce(true); // expand only an expandable token
⋮----
}); // LaTeX's \@firstoftwo{#1}{#2} expands to #1, skipping #2
// TeX source: \long\def\@firstoftwo#1#2{#1}
⋮----
}); // LaTeX's \@secondoftwo{#1}{#2} expands to #2, skipping #1
// TeX source: \long\def\@secondoftwo#1#2{#2}
⋮----
}); // LaTeX's \@ifnextchar{#1}{#2}{#3} looks ahead to the next (unexpanded)
// symbol that isn't a space, consuming any spaces but not consuming the
// first nonspace character.  If that nonspace character matches #1, then
// the macro expands to #2; otherwise, it expands to #3.
⋮----
const args = context.consumeArgs(3); // symbol, if, else
⋮----
}); // LaTeX's \@ifstar{#1}{#2} looks ahead to the next (unexpanded) symbol.
// If it is `*`, then it consumes the symbol, and the macro expands to #1;
// otherwise, the macro expands to #2 (without consuming the symbol).
// TeX source: \def\@ifstar#1{\@ifnextchar *{\@firstoftwo{#1}}}
⋮----
defineMacro("\\@ifstar", "\\@ifnextchar *{\\@firstoftwo{#1}}"); // LaTeX's \TextOrMath{#1}{#2} expands to #1 in text mode, #2 in math mode
⋮----
}); // Lookup table for parsing numbers in base 8 through 16
⋮----
}; // TeX \char makes a literal character (catcode 12) using the following forms:
// (see The TeXBook, p. 43)
//   \char123  -- decimal
//   \char'123 -- octal
//   \char"123 -- hex
//   \char`x   -- character that can be written (i.e. isn't active)
//   \char`\x  -- character that cannot be written (e.g. %)
// These all refer to characters from the font, so we turn them into special
// calls to a function \@char dealt with in the Parser.
⋮----
// Parse a number in the given base, starting with first `token`.
⋮----
}); // \newcommand{\macro}[args]{definition}
// \renewcommand{\macro}[args]{definition}
// TODO: Optional arguments: \newcommand{\macro}[args][default]{definition}
⋮----
const newcommand = (context, existsOK, nonexistsOK) =>
⋮----
// TODO: Should properly expand arg, e.g., ignore {}s
⋮----
} // Final arg is the expansion of the macro
⋮----
defineMacro("\\providecommand", context => newcommand(context, true, true)); // terminal (console) tools
⋮----
const arg = context.consumeArgs(1)[0]; // eslint-disable-next-line no-console
⋮----
const arg = context.consumeArgs(1)[0]; // eslint-disable-next-line no-console
⋮----
const name = tok.text; // eslint-disable-next-line no-console
⋮----
}); //////////////////////////////////////////////////////////////////////
// Grouping
// \let\bgroup={ \let\egroup=}
⋮----
defineMacro("\\egroup", "}"); // Symbols from latex.ltx:
// \def\lq{`}
// \def\rq{'}
// \def \aa {\r a}
// \def \AA {\r A}
⋮----
defineMacro("\\AA", "\\r A"); // Copyright (C) and registered (R) symbols. Use raw symbol in MathML.
// \DeclareTextCommandDefault{\textcopyright}{\textcircled{c}}
// \DeclareTextCommandDefault{\textregistered}{\textcircled{%
//      \check@mathfonts\fontsize\sf@size\z@\math@fontsfalse\selectfont R}}
// \DeclareRobustCommand{\copyright}{%
//    \ifmmode{\nfss@text{\textcopyright}}\else\textcopyright\fi}
⋮----
defineMacro("\\textregistered", "\\html@mathml{\\textcircled{\\scriptsize R}}{\\char`®}"); // Characters omitted from Unicode range 1D400–1D7FF
⋮----
defineMacro("\u212C", "\\mathscr{B}"); // script
⋮----
defineMacro("\u212D", "\\mathfrak{C}"); // Fraktur
⋮----
defineMacro("\u2128", "\\mathfrak{Z}"); // Define \Bbbk with a macro that works in both HTML and MathML.
⋮----
defineMacro("\\Bbbk", "\\Bbb{k}"); // Unicode middle dot
// The KaTeX fonts do not contain U+00B7. Instead, \cdotp displays
// the dot at U+22C5 and gives it punct spacing.
⋮----
defineMacro("\u00b7", "\\cdotp"); // \llap and \rlap render their contents in text mode
⋮----
defineMacro("\\clap", "\\mathclap{\\textrm{#1}}"); // \not is defined by base/fontmath.ltx via
// \DeclareMathSymbol{\not}{\mathrel}{symbols}{"36}
// It's thus treated like a \mathrel, but defined by a symbol that has zero
// width but extends to the right.  We use \rlap to get that spacing.
// For MathML we write U+0338 here. buildMathML.js will then do the overlay.
⋮----
defineMacro("\\not", '\\html@mathml{\\mathrel{\\mathrlap\\@not}}{\\char"338}'); // Negated symbols from base/fontmath.ltx:
// \def\neq{\not=} \let\ne=\neq
// \DeclareRobustCommand
//   \notin{\mathrel{\m@th\mathpalette\c@ncel\in}}
// \def\c@ncel#1#2{\m@th\ooalign{$\hfil#1\mkern1mu/\hfil$\crcr$#1#2$}}
⋮----
defineMacro("\u2209", "\\notin"); // Unicode stacked relations
⋮----
defineMacro("\u225F", "\\html@mathml{\\stackrel{\\tiny?}{=}}{\\mathrel{\\char`\u225F}}"); // Misc Unicode
⋮----
defineMacro("\uFE0F", "\\textregistered"); // The KaTeX fonts have corners at codepoints that don't match Unicode.
// For MathML purposes, use the Unicode code point.
⋮----
defineMacro("\\lrcorner", "\\html@mathml{\\@lrcorner}{\\mathop{\\char\"231f}}"); //////////////////////////////////////////////////////////////////////
// LaTeX_2ε
// \vdots{\vbox{\baselineskip4\p@  \lineskiplimit\z@
// \kern6\p@\hbox{.}\hbox{.}\hbox{.}}}
// We'll call \varvdots, which gets a glyph from symbols.js.
// The zero-width rule gets us an equivalent to the vertical 6pt kern.
⋮----
defineMacro("\u22ee", "\\vdots"); //////////////////////////////////////////////////////////////////////
// amsmath.sty
// http://mirrors.concertpass.com/tex-archive/macros/latex/required/amsmath/amsmath.pdf
// Italic Greek capital letters.  AMS defines these with \DeclareMathSymbol,
// but they are equivalent to \mathit{\Letter}.
⋮----
defineMacro("\\varOmega", "\\mathit{\\Omega}"); //\newcommand{\substack}[1]{\subarray{c}#1\endsubarray}
⋮----
defineMacro("\\substack", "\\begin{subarray}{c}#1\\end{subarray}"); // \renewcommand{\colon}{\nobreak\mskip2mu\mathpunct{}\nonscript
// \mkern-\thinmuskip{:}\mskip6muplus1mu\relax}
⋮----
defineMacro("\\colon", "\\nobreak\\mskip2mu\\mathpunct{}" + "\\mathchoice{\\mkern-3mu}{\\mkern-3mu}{}{}{:}\\mskip6mu"); // \newcommand{\boxed}[1]{\fbox{\m@th$\displaystyle#1$}}
⋮----
defineMacro("\\boxed", "\\fbox{$\\displaystyle{#1}$}"); // \def\iff{\DOTSB\;\Longleftrightarrow\;}
// \def\implies{\DOTSB\;\Longrightarrow\;}
// \def\impliedby{\DOTSB\;\Longleftarrow\;}
⋮----
defineMacro("\\impliedby", "\\DOTSB\\;\\Longleftarrow\\;"); // AMSMath's automatic \dots, based on \mdots@@ macro.
⋮----
// \keybin@ checks for the following:
⋮----
// Symbols whose definition starts with \DOTSB:
⋮----
// Symbols whose definition starts with \mathbin:
⋮----
// Symbols whose definition starts with \mathrel:
⋮----
// Symbols whose definition starts with \DOTSI:
⋮----
// Symbols whose definition starts with \DOTSX:
⋮----
// TODO: If used in text mode, should expand to \textellipsis.
// However, in KaTeX, \textellipsis and \ldots behave the same
// (in text mode), and it's unlikely we'd see any of the math commands
// that affect the behavior of \dots when in text mode.  So fine for now
// (until we support \ifmmode ... \else ... \fi).
⋮----
// \rightdelim@ checks for the following:
⋮----
// \extra@ also tests for the following:
⋮----
// \extrap@ checks for the following:
⋮----
const next = context.future().text; // \dotsc uses \extra@ but not \extrap@, instead specially checking for
// ';' and '.', but doesn't check for ','.
⋮----
defineMacro("\\dotsi", "\\!\\cdots"); // amsmath doesn't actually define \dotsx, but \dots followed by a macro
// starting with \DOTSX implies \dotso, and then \extra@ detects this case
// and forces the added `\,`.
⋮----
defineMacro("\\dotsx", "\\ldots\\,"); // \let\DOTSI\relax
// \let\DOTSB\relax
// \let\DOTSX\relax
⋮----
defineMacro("\\DOTSX", "\\relax"); // Spacing, based on amsmath.sty's override of LaTeX defaults
// \DeclareRobustCommand{\tmspace}[3]{%
//   \ifmmode\mskip#1#2\else\kern#1#3\fi\relax}
⋮----
defineMacro("\\tmspace", "\\TextOrMath{\\kern#1#3}{\\mskip#1#2}\\relax"); // \renewcommand{\,}{\tmspace+\thinmuskip{.1667em}}
// TODO: math mode should use \thinmuskip
⋮----
defineMacro("\\,", "\\tmspace+{3mu}{.1667em}"); // \let\thinspace\,
⋮----
defineMacro("\\thinspace", "\\,"); // \def\>{\mskip\medmuskip}
// \renewcommand{\:}{\tmspace+\medmuskip{.2222em}}
// TODO: \> and math mode of \: should use \medmuskip = 4mu plus 2mu minus 4mu
⋮----
defineMacro("\\:", "\\tmspace+{4mu}{.2222em}"); // \let\medspace\:
⋮----
defineMacro("\\medspace", "\\:"); // \renewcommand{\;}{\tmspace+\thickmuskip{.2777em}}
// TODO: math mode should use \thickmuskip = 5mu plus 5mu
⋮----
defineMacro("\\;", "\\tmspace+{5mu}{.2777em}"); // \let\thickspace\;
⋮----
defineMacro("\\thickspace", "\\;"); // \renewcommand{\!}{\tmspace-\thinmuskip{.1667em}}
// TODO: math mode should use \thinmuskip
⋮----
defineMacro("\\!", "\\tmspace-{3mu}{.1667em}"); // \let\negthinspace\!
⋮----
defineMacro("\\negthinspace", "\\!"); // \newcommand{\negmedspace}{\tmspace-\medmuskip{.2222em}}
// TODO: math mode should use \medmuskip
⋮----
defineMacro("\\negmedspace", "\\tmspace-{4mu}{.2222em}"); // \newcommand{\negthickspace}{\tmspace-\thickmuskip{.2777em}}
// TODO: math mode should use \thickmuskip
⋮----
defineMacro("\\negthickspace", "\\tmspace-{5mu}{.277em}"); // \def\enspace{\kern.5em }
⋮----
defineMacro("\\enspace", "\\kern.5em "); // \def\enskip{\hskip.5em\relax}
⋮----
defineMacro("\\enskip", "\\hskip.5em\\relax"); // \def\quad{\hskip1em\relax}
⋮----
defineMacro("\\quad", "\\hskip1em\\relax"); // \def\qquad{\hskip2em\relax}
⋮----
defineMacro("\\qquad", "\\hskip2em\\relax"); // \tag@in@display form of \tag
⋮----
}); // \renewcommand{\bmod}{\nonscript\mskip-\medmuskip\mkern5mu\mathbin
//   {\operator@font mod}\penalty900
//   \mkern5mu\nonscript\mskip-\medmuskip}
// \newcommand{\pod}[1]{\allowbreak
//   \if@display\mkern18mu\else\mkern8mu\fi(#1)}
// \renewcommand{\pmod}[1]{\pod{{\operator@font mod}\mkern6mu#1}}
// \newcommand{\mod}[1]{\allowbreak\if@display\mkern18mu
//   \else\mkern12mu\fi{\operator@font mod}\,\,#1}
// TODO: math mode should use \medmuskip = 4mu plus 2mu minus 4mu
⋮----
defineMacro("\\mod", "\\allowbreak" + "\\mathchoice{\\mkern18mu}{\\mkern12mu}{\\mkern12mu}{\\mkern12mu}" + "{\\rm mod}\\,\\,#1"); // \pmb    --   A simulation of bold.
// The version in ambsy.sty works by typesetting three copies of the argument
// with small offsets. We use two copies. We omit the vertical offset because
// of rendering problems that makeVList encounters in Safari.
⋮----
defineMacro("\\pmb", "\\html@mathml{" + "\\@binrel{#1}{\\mathrlap{#1}\\kern0.5px#1}}" + "{\\mathbf{#1}}"); //////////////////////////////////////////////////////////////////////
// LaTeX source2e
// \\ defaults to \newline, but changes to \cr within array environment
⋮----
defineMacro("\\\\", "\\newline"); // \def\TeX{T\kern-.1667em\lower.5ex\hbox{E}\kern-.125emX\@}
// TODO: Doesn't normally work in math mode because \@ fails.  KaTeX doesn't
// support \@ yet, so that's omitted, and we add \text so that the result
// doesn't look funny in math mode.
⋮----
defineMacro("\\TeX", "\\textrm{\\html@mathml{" + "T\\kern-.1667em\\raisebox{-.5ex}{E}\\kern-.125emX" + "}{TeX}}"); // \DeclareRobustCommand{\LaTeX}{L\kern-.36em%
//         {\sbox\z@ T%
//          \vbox to\ht\z@{\hbox{\check@mathfonts
//                               \fontsize\sf@size\z@
//                               \math@fontsfalse\selectfont
//                               A}%
//                         \vss}%
//         }%
//         \kern-.15em%
//         \TeX}
// This code aligns the top of the A with the T (from the perspective of TeX's
// boxes, though visually the A appears to extend above slightly).
// We compute the corresponding \raisebox when A is rendered in \normalsize
// \scriptstyle, which has a scale factor of 0.7 (see Options.js).
⋮----
defineMacro("\\LaTeX", "\\textrm{\\html@mathml{" + `L\\kern-.36em\\raisebox{${latexRaiseA}}{\\scriptstyle A}` + "\\kern-.15em\\TeX}{LaTeX}}"); // New KaTeX logo based on tweaking LaTeX logo
⋮----
defineMacro("\\KaTeX", "\\textrm{\\html@mathml{" + `K\\kern-.17em\\raisebox{${latexRaiseA}}{\\scriptstyle A}` + "\\kern-.15em\\TeX}{KaTeX}}"); // \DeclareRobustCommand\hspace{\@ifstar\@hspacer\@hspace}
// \def\@hspace#1{\hskip  #1\relax}
// \def\@hspacer#1{\vrule \@width\z@\nobreak
//                 \hskip #1\hskip \z@skip}
⋮----
defineMacro("\\@hspacer", "\\rule{0pt}{0pt}\\hskip #1\\relax"); //////////////////////////////////////////////////////////////////////
// mathtools.sty
//\providecommand\ordinarycolon{:}
⋮----
defineMacro("\\ordinarycolon", ":"); //\def\vcentcolon{\mathrel{\mathop\ordinarycolon}}
//TODO(edemaine): Not yet centered. Fix via \raisebox or #726
⋮----
defineMacro("\\vcentcolon", "\\mathrel{\\mathop\\ordinarycolon}"); // \providecommand*\dblcolon{\vcentcolon\mathrel{\mkern-.9mu}\vcentcolon}
⋮----
defineMacro("\\dblcolon", "\\html@mathml{" + "\\mathrel{\\vcentcolon\\mathrel{\\mkern-.9mu}\\vcentcolon}}" + "{\\mathop{\\char\"2237}}"); // \providecommand*\coloneqq{\vcentcolon\mathrel{\mkern-1.2mu}=}
⋮----
defineMacro("\\coloneqq", "\\html@mathml{" + "\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}=}}" + "{\\mathop{\\char\"2254}}"); // ≔
// \providecommand*\Coloneqq{\dblcolon\mathrel{\mkern-1.2mu}=}
⋮----
defineMacro("\\Coloneqq", "\\html@mathml{" + "\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}=}}" + "{\\mathop{\\char\"2237\\char\"3d}}"); // \providecommand*\coloneq{\vcentcolon\mathrel{\mkern-1.2mu}\mathrel{-}}
⋮----
defineMacro("\\coloneq", "\\html@mathml{" + "\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}\\mathrel{-}}}" + "{\\mathop{\\char\"3a\\char\"2212}}"); // \providecommand*\Coloneq{\dblcolon\mathrel{\mkern-1.2mu}\mathrel{-}}
⋮----
defineMacro("\\Coloneq", "\\html@mathml{" + "\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}\\mathrel{-}}}" + "{\\mathop{\\char\"2237\\char\"2212}}"); // \providecommand*\eqqcolon{=\mathrel{\mkern-1.2mu}\vcentcolon}
⋮----
defineMacro("\\eqqcolon", "\\html@mathml{" + "\\mathrel{=\\mathrel{\\mkern-1.2mu}\\vcentcolon}}" + "{\\mathop{\\char\"2255}}"); // ≕
// \providecommand*\Eqqcolon{=\mathrel{\mkern-1.2mu}\dblcolon}
⋮----
defineMacro("\\Eqqcolon", "\\html@mathml{" + "\\mathrel{=\\mathrel{\\mkern-1.2mu}\\dblcolon}}" + "{\\mathop{\\char\"3d\\char\"2237}}"); // \providecommand*\eqcolon{\mathrel{-}\mathrel{\mkern-1.2mu}\vcentcolon}
⋮----
defineMacro("\\eqcolon", "\\html@mathml{" + "\\mathrel{\\mathrel{-}\\mathrel{\\mkern-1.2mu}\\vcentcolon}}" + "{\\mathop{\\char\"2239}}"); // \providecommand*\Eqcolon{\mathrel{-}\mathrel{\mkern-1.2mu}\dblcolon}
⋮----
defineMacro("\\Eqcolon", "\\html@mathml{" + "\\mathrel{\\mathrel{-}\\mathrel{\\mkern-1.2mu}\\dblcolon}}" + "{\\mathop{\\char\"2212\\char\"2237}}"); // \providecommand*\colonapprox{\vcentcolon\mathrel{\mkern-1.2mu}\approx}
⋮----
defineMacro("\\colonapprox", "\\html@mathml{" + "\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}\\approx}}" + "{\\mathop{\\char\"3a\\char\"2248}}"); // \providecommand*\Colonapprox{\dblcolon\mathrel{\mkern-1.2mu}\approx}
⋮----
defineMacro("\\Colonapprox", "\\html@mathml{" + "\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}\\approx}}" + "{\\mathop{\\char\"2237\\char\"2248}}"); // \providecommand*\colonsim{\vcentcolon\mathrel{\mkern-1.2mu}\sim}
⋮----
defineMacro("\\colonsim", "\\html@mathml{" + "\\mathrel{\\vcentcolon\\mathrel{\\mkern-1.2mu}\\sim}}" + "{\\mathop{\\char\"3a\\char\"223c}}"); // \providecommand*\Colonsim{\dblcolon\mathrel{\mkern-1.2mu}\sim}
⋮----
defineMacro("\\Colonsim", "\\html@mathml{" + "\\mathrel{\\dblcolon\\mathrel{\\mkern-1.2mu}\\sim}}" + "{\\mathop{\\char\"2237\\char\"223c}}"); // Some Unicode characters are implemented with macros to mathtools functions.
⋮----
defineMacro("\u2237", "\\dblcolon"); // ::
⋮----
defineMacro("\u2239", "\\eqcolon"); // -:
⋮----
defineMacro("\u2254", "\\coloneqq"); // :=
⋮----
defineMacro("\u2255", "\\eqqcolon"); // =:
⋮----
defineMacro("\u2A74", "\\Coloneqq"); // ::=
//////////////////////////////////////////////////////////////////////
// colonequals.sty
// Alternate names for mathtools's macros:
⋮----
defineMacro("\\minuscoloncolon", "\\Eqcolon"); // \colonapprox name is same in mathtools and colonequals.
⋮----
defineMacro("\\coloncolonapprox", "\\Colonapprox"); // \colonsim name is same in mathtools and colonequals.
⋮----
defineMacro("\\coloncolonsim", "\\Colonsim"); // Additional macros, implemented by analogy with mathtools definitions:
⋮----
defineMacro("\\approxcoloncolon", "\\mathrel{\\approx\\mathrel{\\mkern-1.2mu}\\dblcolon}"); // Present in newtxmath, pxfonts and txfonts
⋮----
defineMacro("\\liminf", "\\DOTSB\\operatorname*{lim\\,inf}"); //////////////////////////////////////////////////////////////////////
// MathML alternates for KaTeX glyphs in the Unicode private area
⋮----
defineMacro("\\jmath", "\\html@mathml{\\@jmath}{\u0237}"); //////////////////////////////////////////////////////////////////////
// stmaryrd and semantic
// The stmaryrd and semantic packages render the next four items by calling a
// glyph. Those glyphs do not exist in the KaTeX fonts. Hence the macros.
⋮----
defineMacro("\u27e6", "\\llbracket"); // blackboard bold [
⋮----
defineMacro("\u27e7", "\\rrbracket"); // blackboard bold ]
⋮----
defineMacro("\u2983", "\\lBrace"); // blackboard bold {
⋮----
defineMacro("\u2984", "\\rBrace"); // blackboard bold }
// TODO: Create variable sized versions of the last two items. I believe that
// will require new font glyphs.
// The stmaryrd function `\minuso` provides a "Plimsoll" symbol that
// superimposes the characters \circ and \mathminus. Used in chemistry.
⋮----
defineMacro("⦵", "\\minuso"); //////////////////////////////////////////////////////////////////////
// texvc.sty
// The texvc package contains macros available in mediawiki pages.
// We omit the functions deprecated at
// https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Deprecated_syntax
// We also omit texvc's \O, which conflicts with \text{\O}
⋮----
defineMacro("\\thetasym", "\\vartheta"); // TODO: defineMacro("\\varcoppa", "\\\mbox{\\coppa}");
⋮----
defineMacro("\\Zeta", "\\mathrm{Z}"); //////////////////////////////////////////////////////////////////////
// statmath.sty
// https://ctan.math.illinois.edu/macros/latex/contrib/statmath/statmath.pdf
⋮----
defineMacro("\\plim", "\\DOTSB\\mathop{\\operatorname{plim}}\\limits"); //////////////////////////////////////////////////////////////////////
// braket.sty
// http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/braket/braket.pdf
⋮----
defineMacro("\\Ket", "\\left|#1\\right\\rangle"); // Custom Khan Academy colors, should be moved to an optional package
⋮----
/**
 * This file contains the “gullet” where macros are expanded
 * until only non-macro tokens remain.
 */
// List of commands that act like macros but aren't defined as a macro,
// function, or symbol.  Used in `isDefined`.
⋮----
// MacroExpander.js
⋮----
// Parser.js
⋮----
// Parser.js
⋮----
// Parser.js
"\\nolimits": true // Parser.js
⋮----
class MacroExpander
⋮----
this.feed(input); // Make new global namespace
⋮----
this.stack = []; // contains tokens in REVERSE order
⋮----
/**
   * Feed a new input string to the same MacroExpander
   * (with existing macros etc.).
   */
⋮----
feed(input)
/**
   * Switches between "text" and "math" modes.
   */
⋮----
switchMode(newMode)
/**
   * Start a new group nesting within all namespaces.
   */
⋮----
/**
   * End current group nesting within all namespaces.
   */
⋮----
/**
   * Returns the topmost token on the stack, without expanding it.
   * Similar in behavior to TeX's `\futurelet`.
   */
⋮----
future()
/**
   * Remove and return the next unexpanded token.
   */
⋮----
popToken()
⋮----
this.future(); // ensure non-empty stack
⋮----
/**
   * Add a given token to the token stack.  In particular, this get be used
   * to put back a token returned from one of the other methods.
   */
⋮----
pushToken(token)
/**
   * Append an array of tokens to the token stack.
   */
⋮----
pushTokens(tokens)
/**
   * Consume all following space tokens, without expansion.
   */
⋮----
consumeSpaces()
/**
   * Consume the specified number of arguments from the token stream,
   * and return the resulting array of arguments.
   */
⋮----
consumeArgs(numArgs)
⋮----
const args = []; // obtain arguments, either single token or balanced {…} group
⋮----
this.consumeSpaces(); // ignore spaces before each argument
⋮----
arg.pop(); // remove last }
⋮----
arg.reverse(); // like above, to fit in with stack order
⋮----
/**
   * Expand the next token only once if possible.
   *
   * If the token is expanded, the resulting tokens will be pushed onto
   * the stack in reverse order and will be returned as an array,
   * also in reverse order.
   *
   * If not, the next token will be returned without removing it
   * from the stack.  This case can be detected by a `Token` return value
   * instead of an `Array` return value.
   *
   * In either case, the next token will be on the top of the stack,
   * or the stack will be empty.
   *
   * Used to implement `expandAfterFuture` and `expandNextToken`.
   *
   * At the moment, macro expansion doesn't handle delimited macros,
   * i.e. things like those defined by \def\foo#1\end{…}.
   * See the TeX book page 202ff. for details on how those should behave.
   *
   * If expandableOnly, only expandable tokens are expanded and
   * an undefined control sequence results in an error.
   */
⋮----
expandOnce(expandableOnly)
⋮----
const args = this.consumeArgs(expansion.numArgs); // paste arguments in place of the placeholders
⋮----
tokens = tokens.slice(); // make a shallow copy
⋮----
tok = tokens[--i]; // next token on stack
⋮----
// ## → #
tokens.splice(i + 1, 1); // drop first #
⋮----
// replace the placeholder with the indicated argument
⋮----
} // Concatenate expansion onto top of stack.
⋮----
/**
   * Expand the next token only once (if possible), and return the resulting
   * top token on the stack (without removing anything from the stack).
   * Similar in behavior to TeX's `\expandafter\futurelet`.
   * Equivalent to expandOnce() followed by future().
   */
⋮----
expandAfterFuture()
/**
   * Recursively expand first token, then return first non-expandable token.
   */
⋮----
expandNextToken()
⋮----
const expanded = this.expandOnce(); // expandOnce returns Token if and only if it's fully expanded.
⋮----
// \relax stops the expansion, but shouldn't get returned (a
// null return value couldn't get implemented as a function).
// the token after \noexpand is interpreted as if its meaning
// were ‘\relax’
⋮----
return this.stack.pop(); // === expanded
⋮----
} // Flow unable to figure out that this pathway is impossible.
// https://github.com/facebook/flow/issues/4808
⋮----
throw new Error(); // eslint-disable-line no-unreachable
⋮----
/**
   * Fully expand the given macro name and return the resulting list of
   * tokens, or return `undefined` if no such macro is defined.
   */
⋮----
expandMacro(name)
/**
   * Fully expand the given token stream and return the resulting list of tokens
   */
⋮----
expandTokens(tokens)
⋮----
const expanded = this.expandOnce(true); // expand only expandable tokens
// expandOnce returns Token if and only if it's fully expanded.
⋮----
// the expansion of \noexpand is the token itself
⋮----
/**
   * Fully expand the given macro name and return the result as a string,
   * or return `undefined` if no such macro is defined.
   */
⋮----
expandMacroAsText(name)
/**
   * Returns the expanded macro as a reversed array of tokens and a macro
   * argument count.  Or returns `null` if no such macro.
   */
⋮----
_getExpansion(name)
⋮----
// mainly checking for undefined here
⋮----
tokens.reverse(); // to fit in with stack using push and pop
⋮----
/**
   * Determine whether a command is currently "defined" (has some
   * functionality), meaning that it's a macro (in the current group),
   * a function, a symbol, or one of the special commands listed in
   * `implicitCommands`.
   */
⋮----
isDefined(name)
/**
   * Determine whether a command is expandable.
   */
⋮----
isExpandable(name)
⋮----
return macro != null ? typeof macro === "string" || typeof macro === "function" || !macro.unexpandable // TODO(ylem): #2085
⋮----
/* && !functions[name].primitive*/
⋮----
/* eslint no-constant-condition:0 */
⋮----
/**
 * This file contains the parser used to parse out a TeX expression from the
 * input. Since TeX isn't context-free, standard parsers don't work particularly
 * well.
 *
 * The strategy of this parser is as such:
 *
 * The main functions (the `.parse...` ones) take a position in the current
 * parse string to parse tokens from. The lexer (found in Lexer.js, stored at
 * this.gullet.lexer) also supports pulling out tokens at arbitrary places. When
 * individual tokens are needed at a position, the lexer is called to pull out a
 * token, which is then used.
 *
 * The parser has a property called "mode" indicating the mode that
 * the parser is currently in. Currently it has to be one of "math" or
 * "text", which denotes whether the current environment is a math-y
 * one or a text-y one (e.g. inside \text). Currently, this serves to
 * limit the functions which can be used in text mode.
 *
 * The main functions then return an object which contains the useful data that
 * was parsed at its given point, and a new position at the end of the parsed
 * data. The main functions can call each other and continue the parsing by
 * using the returned position as a new starting point.
 *
 * There are also extra `.handle...` functions, which pull out some reused
 * functionality into self-contained functions.
 *
 * The functions return ParseNodes.
 */
class Parser
⋮----
// Start in math mode
this.mode = "math"; // Create a new macro expander (gullet) and (indirectly via that) also a
// new lexer (mouth) for this parser (stomach, in the language of TeX)
⋮----
this.gullet = new MacroExpander(input, settings, this.mode); // Store the settings for use in parsing
⋮----
this.settings = settings; // Count leftright depth (for \middle errors)
⋮----
/**
   * Checks a result to make sure it has the right type, and throws an
   * appropriate error otherwise.
   */
⋮----
expect(text, consume)
/**
   * Discards the current lookahead token, considering it consumed.
   */
⋮----
consume()
/**
   * Return the current lookahead token, or if there isn't one (at the
   * beginning, or if the previous lookahead token was consume()d),
   * fetch the next token as the new lookahead token and return it.
   */
⋮----
fetch()
/**
   * Switches between "text" and "math" modes.
   */
⋮----
/**
   * Main parsing function, which parses an entire input.
   */
⋮----
parse()
⋮----
// Create a group namespace for the math expression.
// (LaTeX creates a new group for every $...$, $$...$$, \[...\].)
⋮----
} // Use old \color behavior (same as LaTeX's \textcolor) if requested.
// We do this within the group for the math expression, so it doesn't
// pollute settings.macros.
⋮----
} // Try to parse the input
⋮----
const parse = this.parseExpression(false); // If we succeeded, make sure there's an EOF at the end
⋮----
this.expect("EOF"); // End the group namespace for the expression
⋮----
parseExpression(breakOnInfix, breakOnTokenText)
⋮----
const body = []; // Keep adding atoms to the body until we can't parse any more atoms (either
// we reached the end, a }, or a \right)
⋮----
// Ignore spaces in math mode
⋮----
/**
   * Rewrites infix operators such as \over with corresponding commands such
   * as \frac.
   *
   * There can only be one infix operator per group.  If there's more than one
   * then the expression is ambiguous.  This can be resolved by adding {}.
   */
⋮----
handleInfixNodes(body)
⋮----
} // The greediness of a superscript or subscript
⋮----
/**
   * Handle a subscript or superscript with nice errors.
   */
handleSupSubscript(name)
⋮----
const group = this.parseGroup(name, false, Parser.SUPSUB_GREEDINESS, undefined, undefined, true); // ignore spaces before sup/subscript argument
⋮----
/**
   * Converts the textual input of an unsupported command into a text node
   * contained within a color node whose color is determined by errorColor
   */
⋮----
formatUnsupportedCmd(text)
/**
   * Parses a group with optional super/subscripts.
   */
⋮----
parseAtom(breakOnTokenText)
⋮----
// The body of an atom is an implicit group, so that things like
// \left(x\right)^2 work correctly.
const base = this.parseGroup("atom", false, null, breakOnTokenText); // In text mode, we don't have superscripts or subscripts
⋮----
} // Note that base may be empty (i.e. null) at this point.
⋮----
// Guaranteed in math mode, so eat any spaces first.
this.consumeSpaces(); // Lex the first token
⋮----
// We got a limit control
⋮----
// We got a superscript start
⋮----
// We got a subscript start
⋮----
// We got a prime
⋮----
}; // Many primes can be grouped together, so we handle this here
⋮----
this.consume(); // Keep lexing tokens until we get something that's not a prime
⋮----
// For each one, add another prime to the list
⋮----
} // If there's a superscript following the primes, combine that
// superscript in with the primes.
⋮----
} // Put everything into an ordgroup as the superscript
⋮----
// If it wasn't ^, _, or ', stop parsing super/subscripts
⋮----
} // Base must be set if superscript or subscript are set per logic above,
// but need to check here for type check to pass.
⋮----
// If we got either a superscript or subscript, create a supsub
⋮----
// Otherwise return the original body
⋮----
/**
   * Parses an entire function, including its base and all of its arguments.
   */
⋮----
parseFunction(breakOnTokenText, name, // For error reporting.
greediness)
⋮----
this.consume(); // consume command token
⋮----
/**
   * Call a function handler with a suitable context and arguments.
   */
⋮----
callFunction(name, args, optArgs, token, breakOnTokenText)
/**
   * Parses the arguments of a function or environment
   */
⋮----
parseArguments(func, // Should look like "\name" or "\begin{name}".
funcData)
⋮----
const isOptional = i < funcData.numOptionalArgs; // Ignore spaces between arguments.  As the TeXbook says:
// "After you have said ‘\def\row#1#2{...}’, you are allowed to
//  put spaces between the arguments (e.g., ‘\row x n’), because
//  TeX doesn’t use single spaces as undelimited arguments."
⋮----
const consumeSpaces = i > 0 && !isOptional || // Also consume leading spaces in math mode, as parseSymbol
// won't know what to do with them.  This can only happen with
// macros, e.g. \frac\foo\foo where \foo expands to a space symbol.
// In LaTeX, the \foo's get treated as (blank) arguments.
// In KaTeX, for now, both spaces will get consumed.
// TODO(edemaine)
⋮----
/**
   * Parses a group when the mode is changing.
   */
⋮----
parseGroupOfType(name, type, optional, greediness, consumeSpaces)
⋮----
// hbox argument type wraps the argument in the equivalent of
// \hbox, which is like \text but switching to \textstyle size.
⋮----
style: "text" // simulate \textstyle
⋮----
/**
   * Discard any space tokens, fetching the next non-space token.
   */
⋮----
/**
   * Parses a group, essentially returning the string formed by the
   * brace-enclosed tokens plus some position information.
   */
⋮----
parseStringGroup(modeName, // Used to describe the mode in error messages.
optional, raw)
⋮----
let nested = 0; // allow nested braces in raw string group
⋮----
/**
   * Parses a regex-delimited group: the largest sequence of tokens
   * whose concatenated strings match `regex`. Returns the string
   * formed by the tokens plus some position information.
   */
⋮----
parseRegexGroup(regex, modeName)
/**
   * Parses a color description.
   */
⋮----
parseColorGroup(optional)
⋮----
// We allow a 6-digit HTML color spec without a leading "#".
// This follows the xcolor package's HTML color model.
// Predefined color names are all missed by this RegEx pattern.
⋮----
/**
   * Parses a size specification, consisting of magnitude and unit.
   */
⋮----
parseSizeGroup(optional)
⋮----
// Because we've tested for what is !optional, this block won't
// affect \kern, \hspace, etc. It will capture the mandatory arguments
// to \genfrac and \above.
res.text = "0pt"; // Enable \above{}
⋮----
isBlank = true; // This is here specifically for \genfrac
⋮----
// sign + magnitude, cast to number
⋮----
/**
   * Parses an URL, checking escaped letters and allowed protocols,
   * and setting the catcode of % as an active character (as in \hyperref).
   */
⋮----
parseUrlGroup(optional, consumeSpaces)
⋮----
this.gullet.lexer.setCatcode("%", 13); // active character
⋮----
const res = this.parseStringGroup("url", optional, true); // get raw string
⋮----
this.gullet.lexer.setCatcode("%", 14); // comment character
⋮----
} // hyperref package allows backslashes alone in href, but doesn't
// generate valid links in such cases; we interpret this as
// "undefined" behaviour, and keep them as-is. Some browser will
// replace backslashes with forward slashes.
⋮----
/**
   * If `optional` is false or absent, this parses an ordinary group,
   * which is either a single nucleus (like "x") or an expression
   * in braces (like "{x+y}") or an implicit group, a group that starts
   * at the current position, and ends right before a higher explicit
   * group ends, or at EOF.
   * If `optional` is true, it parses either a bracket-delimited expression
   * (like "[x+y]") or returns null to indicate the absence of a
   * bracket-enclosed group.
   * If `mode` is present, switches to that mode while parsing the group,
   * and switches back after.
   */
⋮----
parseGroup(name, // For error reporting.
optional, greediness, breakOnTokenText, mode, consumeSpaces)
⋮----
// Switch to specified mode
⋮----
} // Consume spaces if requested, crucially *after* we switch modes,
// so that the next non-space token is parsed in the correct mode.
⋮----
} // Get first token
⋮----
let result; // Try to parse an open brace or \begingroup
⋮----
const groupEnd = Parser.endOfGroup[text]; // Start a new group namespace
⋮----
this.gullet.beginGroup(); // If we get a brace, parse an expression
⋮----
const lastToken = this.fetch(); // Check that we got a matching closing brace
⋮----
this.expect(groupEnd); // End group namespace
⋮----
// A group formed by \begingroup...\endgroup is a semi-simple group
// which doesn't affect spacing in math mode, i.e., is transparent.
// https://tex.stackexchange.com/questions/1930/when-should-one-
// use-begingroup-instead-of-bgroup
⋮----
// Return nothing for an optional group
⋮----
// If there exists a function with this name, parse the function.
// Otherwise, just return a nucleus
⋮----
} // Switch mode back
⋮----
/**
   * Form ligature-like combinations of characters for text mode.
   * This includes inputs like "--", "---", "``" and "''".
   * The result will simply replace multiple textord nodes with a single
   * character in each value by a single textord node having multiple
   * characters in its value.  The representation is still ASCII source.
   * The group will be modified in place.
   */
⋮----
formLigatures(group)
⋮----
const a = group[i]; // $FlowFixMe: Not every node type has a `text` property.
⋮----
/**
   * Parse a single symbol out of the string. Here, we handle single character
   * symbols and special functions like \verb.
   */
⋮----
parseSymbol()
⋮----
} // Lexer's tokenRegex is constructed to always have matching
// first/last characters.
⋮----
arg = arg.slice(1, -1); // remove first and last char
⋮----
} // At this point, we should have a symbol, possibly with accents.
// First expand any accented base symbol according to unicodeSymbols.
⋮----
// This behavior is not strict (XeTeX-compatible) in math mode.
⋮----
} // Strip off any combining characters
⋮----
text = '\u0131'; // dotless i, in math and text mode
⋮----
text = '\u0237'; // dotless j, in math and text mode
⋮----
} // Recognize base symbol
⋮----
// $FlowFixMe
⋮----
// $FlowFixMe
⋮----
// no symbol for e.g. ^
⋮----
} // All nonmathematical Unicode characters are rendered as if they
// are in text mode (wrapped in \text) because that's what it
// takes to render them in LaTeX.  Setting `mode: this.mode` is
// another natural choice (the user requested math mode), but
// this makes it more difficult for getCharacterMetrics() to
// distinguish Unicode characters without metrics and those for
// which we want to simulate the letter M.
⋮----
return null; // EOF, ^, _, {, }, etc.
⋮----
this.consume(); // Transform combining characters into accents
⋮----
/**
   * Parses an "expression", which is a list of atoms.
   *
   * `breakOnInfix`: Should the parsing stop when we hit infix nodes? This
   *                 happens when functions have higher precendence han infix
   *                 nodes in implicit parses.
   *
   * `breakOnTokenText`: The text of the token that the expression should end
   *                     with, or `null` if something else should end the
   *                     expression.
   */
⋮----
/**
 * Provides a single function for parsing an expression using a Parser
 * TODO(emily): Remove this
 */
⋮----
/**
 * Parses an expression using a Parser, then returns the parsed result.
 */
⋮----
const parser = new Parser(toParse, settings); // Blank out any \df@tag to avoid spurious "Duplicate \tag" errors
⋮----
let tree = parser.parse(); // If the input used \tag, it will set the \df@tag macro to the tag.
// In this case, we separately parse the tag and wrap the tree.
⋮----
/* eslint no-console:0 */
⋮----
/**
 * Parse and build an expression, and place that expression in the DOM node
 * given.
 */
⋮----
}; // KaTeX's styles don't work properly in quirks mode. Print out an error, and
// disable rendering.
⋮----
/**
 * Parse and build an expression, and return the markup for that.
 */
⋮----
/**
 * Parse an expression and return the parse tree.
 */
⋮----
/**
 * If the given error is a KaTeX ParseError and options.throwOnError is false,
 * renders the invalid LaTeX as a span with hover title giving the KaTeX
 * error message.  Otherwise, simply throws the error.
 */
⋮----
/**
 * Generates and returns the katex build tree. This is used for advanced
 * use cases (like rendering to custom output).
 */
⋮----
/**
 * Generates and returns the katex build tree, with just HTML (no MathML).
 * This is used for advanced use cases (like rendering to custom output).
 */
⋮----
/**
   * Current KaTeX version
   */
⋮----
/**
   * Renders the given LaTeX into an HTML+MathML combination, and adds
   * it as a child to the specified DOM node.
   */
⋮----
/**
   * Renders the given LaTeX into an HTML+MathML combination string,
   * for sending to the client.
   */
⋮----
/**
   * KaTeX error, usually during parsing.
   */
⋮----
/**
   * Parses the given LaTeX into KaTeX's internal parse tree structure,
   * without rendering to HTML or MathML.
   *
   * NOTE: This method is not currently recommended for public use.
   * The internal tree representation is unstable and is very likely
   * to change. Use at your own risk.
   */
⋮----
/**
   * Renders the given LaTeX into an HTML+MathML internal DOM tree
   * representation, without flattening that representation to a string.
   *
   * NOTE: This method is not currently recommended for public use.
   * The internal tree representation is unstable and is very likely
   * to change. Use at your own risk.
   */
⋮----
/**
   * Renders the given LaTeX into an HTML internal DOM tree representation,
   * without MathML and without flattening that representation to a string.
   *
   * NOTE: This method is not currently recommended for public use.
   * The internal tree representation is unstable and is very likely
   * to change. Use at your own risk.
   */
⋮----
/**
   * extends internal font metrics object with a new object
   * each key in the new object represents a font name
  */
⋮----
/**
   * adds a new symbol to builtin symbols table
   */
⋮----
/**
   * adds a new macro to builtin macro list
   */
⋮----
/**
   * Expose the dom tree node types, which can be useful for type checking nodes.
   *
   * NOTE: This method is not currently recommended for public use.
   * The internal tree representation is unstable and is very likely
   * to change. Use at your own risk.
   */

================
File: docs/static/katex/README.md
================
# [<img src="https://katex.org/img/katex-logo-black.svg" width="130" alt="KaTeX">](https://katex.org/)
[![npm](https://img.shields.io/npm/v/katex.svg)](https://www.npmjs.com/package/katex)
[![CircleCI](https://circleci.com/gh/KaTeX/KaTeX.svg?style=shield)](https://circleci.com/gh/KaTeX/KaTeX)
[![codecov](https://codecov.io/gh/KaTeX/KaTeX/branch/master/graph/badge.svg)](https://codecov.io/gh/KaTeX/KaTeX)
[![Join the chat at https://gitter.im/KaTeX/KaTeX](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/KaTeX/KaTeX?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Dependabot Status](https://api.dependabot.com/badges/status?host=github&repo=KaTeX/KaTeX)](https://dependabot.com)
[![jsDelivr](https://data.jsdelivr.com/v1/package/npm/katex/badge?style=rounded)](https://www.jsdelivr.com/package/npm/katex)
![](https://img.badgesize.io/KaTeX/KaTeX/v0.12.0/dist/katex.min.js?compression=gzip)

KaTeX is a fast, easy-to-use JavaScript library for TeX math rendering on the web.

 * **Fast:** KaTeX renders its math synchronously and doesn't need to reflow the page. See how it compares to a competitor in [this speed test](http://www.intmath.com/cg5/katex-mathjax-comparison.php).
 * **Print quality:** KaTeX's layout is based on Donald Knuth's TeX, the gold standard for math typesetting.
 * **Self contained:** KaTeX has no dependencies and can easily be bundled with your website resources.
 * **Server side rendering:** KaTeX produces the same output regardless of browser or environment, so you can pre-render expressions using Node.js and send them as plain HTML.

KaTeX is compatible with all major browsers, including Chrome, Safari, Firefox, Opera, Edge, and IE 11.

KaTeX supports much (but not all) of LaTeX and many LaTeX packages. See the [list of supported functions](https://katex.org/docs/supported.html).

Try out KaTeX [on the demo page](https://katex.org/#demo)!

## Getting started

### Starter template

```html
<!DOCTYPE html>
<!-- KaTeX requires the use of the HTML5 doctype. Without it, KaTeX may not render properly -->
<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
  </head>
  ...
</html>
```

You can also [download KaTeX](https://github.com/KaTeX/KaTeX/releases) and host it yourself.

For details on how to configure auto-render extension, refer to [the documentation](https://katex.org/docs/autorender.html).

### API

Call `katex.render` to render a TeX expression directly into a DOM element.
For example:

```js
katex.render("c = \\pm\\sqrt{a^2 + b^2}", element, {
    throwOnError: false
});
```

Call `katex.renderToString` to generate an HTML string of the rendered math,
e.g., for server-side rendering.  For example:

```js
var html = katex.renderToString("c = \\pm\\sqrt{a^2 + b^2}", {
    throwOnError: false
});
// '<span class="katex">...</span>'
```

Make sure to include the CSS and font files in both cases.
If you are doing all rendering on the server, there is no need to include the
JavaScript on the client.

The examples above use the `throwOnError: false` option, which renders invalid
inputs as the TeX source code in red (by default), with the error message as
hover text.  For other available options, see the
[API documentation](https://katex.org/docs/api.html),
[options documentation](https://katex.org/docs/options.html), and
[handling errors documentation](https://katex.org/docs/error.html).

## Demo and Documentation

Learn more about using KaTeX [on the website](https://katex.org)!

## Contributing

See [CONTRIBUTING.md](https://github.com/KaTeX/KaTeX/blob/main/CONTRIBUTING.md)

## License

KaTeX is licensed under the [MIT License](http://opensource.org/licenses/MIT).

================
File: docs/static/.nojekyll
================


================
File: docs/.eslintignore
================
node_modules
build
static
html

# FIXME
src/pages/index.js
src/theme/Footer/index.js

================
File: docs/.eslintrc
================
{
    "env": {
      "browser": true,
      "node": true
    },
    "parser": "babel-eslint",
    "rules": {
      "strict": 0,
      "no-unused-vars": ["error", { "argsIgnorePattern": "^_" }],
      "no-trailing-spaces": ["error", { "skipBlankLines": true }]
    },
    "settings": {
      "react": {
        "version": "detect", // React version. "detect" automatically picks the version you have installed.
      }
    },
    "extends": [
      "eslint:recommended",
      "plugin:react/recommended"
    ]
  }

================
File: docs/.gitignore
================
# Dependencies
/node_modules

# Production
/build

# Generated files
.docusaurus
.cache-loader
.vercel
/static/img/*.svg
/static/img/*.png
vercel.json

# use npm and package-lock.json
yarn.lock

# Misc
.DS_Store
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Documentation build artifacts
/html/
/src/tests.ok
/src/cli/usage.md
/src/.gitbook/assets/*.svg

================
File: docs/.prettierignore
================
.docusaurus
build
html
static

================
File: docs/.prettierrc.json
================
{
  "trailingComma": "all",
  "tabWidth": 2,
  "semi": true,
  "singleQuote": false,
  "proseWrap": "always",
  "printWidth": 80
}

================
File: docs/babel.config.js
================


================
File: docs/build-cli-usage.sh
================
set -e
cd "$(dirname "$0")"
# shellcheck source=ci/env.sh
source ../ci/env.sh
# Get current channel
eval "$(../ci/channel-info.sh)"
# set the output file' location
out=${1:-src/cli/usage.md}
# load the usage file's header
cat src/cli/.usage.md.header > "$out"
if [[ -n $CI ]]; then
  if [[ $CI_BRANCH != $EDGE_CHANNEL* ]] && [[ $CI_BRANCH != $BETA_CHANNEL* ]] && [[ $CI_BRANCH != $STABLE_CHANNEL* ]]; then
    echo "**NOTE:** The usage doc is only auto-generated during full production deployments of the docs"
    echo "**NOTE:** This usage doc is auto-generated during deployments" >> "$out"
    exit
  fi
fi
echo 'Building the solana cli from source...'
source ../ci/rust-version.sh stable
: "${rust_stable:=}"
usage=$(cargo -q run -p solana-cli -- -C ~/.foo --help | sed -e 's|'"$HOME"'|~|g' -e 's/[[:space:]]\+$//')
section() {
  declare mark=${2:-"###"}
  declare section=$1
  read -r name rest <<<"$section"
  printf '%s %s
' "$mark" "$name"
  printf '```text
%s
```
' "$section"
}
section "$usage" >> "$out"
usage=$(sed -e '/^ \{5,\}/d' <<<"$usage")
in_subcommands=0
while read -r subcommand rest; do
  [[ $subcommand == "SUBCOMMANDS:" ]] && in_subcommands=1 && continue
  if ((in_subcommands)); then
      section "$(cargo -q run -p solana-cli -- help "$subcommand" | sed -e 's|'"$HOME"'|~|g' -e 's/[[:space:]]\+$//')" "
  fi
done <<<"$usage">>"$out"

================
File: docs/build.sh
================
set -ex
cd "$(dirname "$0")"
source ../ci/env.sh
source ../ci/rust-version.sh
../ci/docker-run-default-image.sh docs/build-cli-usage.sh
../ci/docker-run-default-image.sh docs/convert-ascii-to-svg.sh
./set-solana-release-tag.sh
npm run build
echo $?

================
File: docs/convert-ascii-to-svg.sh
================
set -e
cd "$(dirname "$0")"
output_dir=static/img
svgbob_cli="$(command -v svgbob_cli || true)"
if [[ -z "$svgbob_cli" ]]; then
  svgbob_cli="$(command -v svgbob || true)"
  [[ -n "$svgbob_cli" ]] || ( echo "svgbob_cli binary not found" && exit 1 )
fi
mkdir -p "$output_dir"
while read -r bob_file; do
  out_file=$(basename "${bob_file%.*}".svg)
  "$svgbob_cli" "$bob_file" --output "$output_dir/$out_file"
done < <(find art/*.bob)
while read -r msc_file; do
  out_file=$(basename "${msc_file%.*}".png)
  mscgen -T png -o "$output_dir/$out_file" -i "$msc_file"
done < <(find art/*.msc)

================
File: docs/deploy.sh
================
set -eo pipefail
here=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
source "$here/../ci/env.sh"
if [[ -z $CI ]]; then
  echo "Publishing docs from local"
  if [[ -z $PROJECT_NAME ]]; then
    echo "❌ PROJECT_NAME is undefined"
    exit 1
  fi
  PROJECT_NAMES=("$PROJECT_NAME")
else
  echo "Publishing docs from CI"
  if [[ -n $CI_PULL_REQUEST ]]; then
    echo "skipping docs publish from pull requests"
    exit 0
  fi
  eval "$("$here/../ci/channel-info.sh")"
  if [[ -n $CI_TAG ]]; then
    if [[ $CI_TAG != $BETA_CHANNEL* ]]; then
      echo "skipping docs publish from non-beta tag"
      exit 0
    fi
    PROJECT_NAMES=("docs-anza-xyz" "beta-docs-anza-xyz")
  else
    case $CHANNEL in
    edge)
      PROJECT_NAMES=("edge-docs-anza-xyz")
      ;;
    beta)
      PROJECT_NAMES=("beta-docs-anza-xyz")
      ;;
    stable)
      echo "skipping docs publish from stable channel"
      exit 0
      ;;
    *)
      echo "❌ unknown channel: '$CHANNEL'"
      exit 1
      ;;
    esac
  fi
fi
echo "PROJECT_NAMES: ${PROJECT_NAMES[*]}"
if [[ -z $VERCEL_TOKEN ]]; then
  echo "❌ VERCEL_TOKEN is undefined"
  exit 1
fi
for PROJECT_NAME in "${PROJECT_NAMES[@]}"; do
  echo "Publishing docs for $PROJECT_NAME"
  CONFIG_FILE=vercel.json
  cat >"$CONFIG_FILE" <<EOF
  {
    "name": "$PROJECT_NAME",
    "scope": "$VERCEL_SCOPE",
    "redirects": [
      { "source": "/apps", "destination": "/developers" },
      { "source": "/developing/programming-model/overview", "destination": "https://solana.com/docs/programs" },
      { "source": "/apps/break", "destination": "https://solana.com/docs/programs/examples" },
      { "source": "/apps/drones", "destination": "https://solana.com/docs/programs/examples" },
      { "source": "/apps/hello-world", "destination": "https://solana.com/docs/programs/examples" },
      { "source": "/apps/javascript-api", "destination": "https://solana.com/docs/clients/javascript" },
      { "source": "/apps/programming-faq", "destination": "https://solana.com/docs/programs/faq" },
      { "source": "/apps/rent", "destination": "https://solana.com/docs/core/rent" },
      { "source": "/apps/webwallet", "destination": "https://solana.com/docs/intro/wallets" },
      { "source": "/implemented-proposals/cross-program-invocation", "destination": "https://solana.com/docs/core/cpi" },
      { "source": "/implemented-proposals/program-derived-addresses", "destination": "https://solana.com/docs/core/cpi#program-derived-addresses" },
      { "source": "/apps/sysvars", "destination": "/developing/runtime-facilities/sysvars" },
      { "source": "/apps/builtins", "destination": "/developing/runtime-facilities/programs" },
      { "source": "/apps/backwards-compatibility", "destination": "/developing/backwards-compatibility" },
      { "source": "/implemented-proposals/secp256k1_instruction", "destination": "/developing/runtime-facilities/programs#secp256k1-program" },
      { "source": "/implemented-proposals/implemented-proposals", "destination": "/implemented-proposals" },
      { "source": "/cli/install-solana-cli-tools", "destination": "/cli/install" },
      { "source": "/cli/conventions", "destination": "/cli/intro" },
      { "source": "/cli/choose-a-cluster", "destination": "/cli/examples/choose-a-cluster" },
      { "source": "/cli/delegate-stake", "destination": "/cli/examples/delegate-stake" },
      { "source": "/delegate-stake", "destination": "/cli/examples/delegate-stake" },
      { "source": "/cli/sign-offchain-message", "destination": "/cli/examples/sign-offchain-message" },
      { "source": "/cli/deploy-a-program", "destination": "/cli/examples/deploy-a-program" },
      { "source": "/cli/transfer-tokens", "destination": "/cli/examples/transfer-tokens" },
      { "source": "/offline-signing/durable-nonce", "destination": "/cli/examples/durable-nonce" },
      { "source": "/offline-signing", "destination": "/cli/examples/offline-signing" },
      { "source": "/developing/test-validator", "destination": "/cli/examples/test-validator" },
      { "source": "/wallet-guide/cli", "destination": "/cli/wallets" },
      { "source": "/wallet-guide/paper-wallet", "destination": "/cli/wallets/paper" },
      { "source": "/wallet-guide/file-system-wallet", "destination": "/cli/wallets/file-system" },
      { "source": "/wallet-guide/hardware-wallet", "destination": "/cli/wallets/hardware-wallet" },
      { "source": "/wallet-guide/hardware-wallet/ledger", "destination": "/cli/wallets/hardware-wallet/ledger" },
      { "source": "/cluster/overview", "destination": "/clusters" },
      { "source": "/cluster/bench-tps", "destination": "/clusters/benchmark" },
      { "source": "/cluster/performance-metrics", "destination": "/clusters/metrics" },
      { "source": "/running-validator", "destination": "/operations" },
      { "source": "/validator/get-started/setup-a-validator", "destination": "/operations/setup-a-validator" },
      { "source": "/validator/get-started/setup-an-rpc-node", "destination": "/operations/setup-an-rpc-node" },
      { "source": "/validator/best-practices/operations", "destination": "/operations/best-practices/general" },
      { "source": "/validator/best-practices/monitoring", "destination": "/operations/best-practices/monitoring" },
      { "source": "/validator/best-practices/security", "destination": "/operations/best-practices/security" },
      { "source": "/validator/overview/running-validator-or-rpc-node", "destination": "/operations/validator-or-rpc-node" },
      { "source": "/validator/overview/validator-prerequisites", "destination": "/operations/prerequisites" },
      { "source": "/validator/overview/validator-initiatives", "destination": "/operations/validator-initiatives" },
      { "source": "/running-validator/validator-reqs", "destination": "/operations/requirements" },
      { "source": "/running-validator/validator-troubleshoot", "destination": "/operations/guides/validator-troubleshoot" },
      { "source": "/running-validator/validator-start", "destination": "/operations/guides/validator-start" },
      { "source": "/running-validator/vote-accounts", "destination": "/operations/guides/vote-accounts" },
      { "source": "/running-validator/validator-stake", "destination": "/operations/guides/validator-stake" },
      { "source": "/running-validator/validator-monitor", "destination": "/operations/guides/validator-monitor" },
      { "source": "/running-validator/validator-info", "destination": "/operations/guides/validator-info" },
      { "source": "/running-validator/validator-failover", "destination": "/operations/guides/validator-failover" },
      { "source": "/running-validator/restart-cluster", "destination": "/operations/guides/restart-cluster" },
      { "source": "/cluster/synchronization", "destination": "/consensus/synchronization" },
      { "source": "/cluster/leader-rotation", "destination": "/consensus/leader-rotation" },
      { "source": "/cluster/fork-generation", "destination": "/consensus/fork-generation" },
      { "source": "/cluster/managing-forks", "destination": "/consensus/managing-forks" },
      { "source": "/cluster/turbine-block-propagation", "destination": "/consensus/turbine-block-propagation" },
      { "source": "/cluster/commitments", "destination": "/consensus/commitments" },
      { "source": "/cluster/vote-signing", "destination": "/consensus/vote-signing" },
      { "source": "/cluster/stake-delegation-and-rewards", "destination": "/consensus/stake-delegation-and-rewards" },
      { "source": "/developing/backwards-compatibility", "destination": "/backwards-compatibility" },
      { "source": "/validator/faq", "destination": "/faq" },
      { "source": "/developing/plugins/geyser-plugins", "destination": "/validator/geyser" },
      { "source": "/validator/overview/what-is-an-rpc-node", "destination": "/what-is-an-rpc-node" },
      { "source": "/validator/overview/what-is-a-validator", "destination": "/what-is-a-validator" },
      { "source": "/developing/runtime-facilities/:path*", "destination": "/runtime/:path*" },
      { "destination": "https://solana.com/docs/rpc/:path*", "source": "/api/:path*" },
      { "destination": "https://solana.com/docs/rpc", "source": "/developing/clients/jsonrpc-api" },
      { "destination": "https://solana.com/docs/rpc", "source": "/apps/jsonrpc-api" },
      { "destination": "https://solana.com/docs/terminology", "source": "/terminology" },
      { "destination": "https://solana.com/docs/core/rent", "source": "/developing/intro/rent" },
      { "destination": "https://solana.com/docs/core/programs", "source": "/developing/intro/programs" },
      { "destination": "https://solana.com/docs/core/accounts", "source": "/developing/programming-model/accounts" },
      { "destination": "https://solana.com/docs/core/cpi", "source": "/developing/programming-model/calling-between-programs" },
      { "destination": "https://solana.com/docs/core/runtime", "source": "/developing/programming-model/runtime" },
      { "destination": "https://solana.com/docs/core/transactions", "source": "/developing/programming-model/transactions" },
      { "destination": "https://solana.com/docs/core/transactions/fees", "source": "/developing/intro/transaction_fees" },
      { "destination": "https://solana.com/docs/core/transactions/confirmation", "source": "/developing/transaction_confirmation" },
      { "destination": "https://solana.com/docs/core/transactions/versions", "source": "/developing/versioned-transactions" },
      { "destination": "https://solana.com/docs/core/transactions/retry", "source": "/integrations/retrying-transactions" },
      { "destination": "https://solana.com/docs/intro/dev", "source": "/developing/programming-model/overview" },
      { "destination": "https://solana.com/docs/advanced/lookup-tables", "source": "/developing/lookup-tables" },
      { "destination": "https://solana.com/docs", "source": "/developers" },
      { "destination": "https://solana.com/docs/advanced/state-compression", "source": "/learn/state-compression" },
      { "destination": "https://solana.com/developers/guides/javascript/compressed-nfts", "source": "/developing/guides/compressed-nfts" },
      { "destination": "https://solana.com/docs/programs", "source": "/developing/on-chain-programs/overview" },
      { "destination": "https://solana.com/docs/programs/debugging", "source": "/developing/on-chain-programs/debugging" },
      { "destination": "https://solana.com/docs/programs/deploying", "source": "/developing/on-chain-programs/deploying" },
      { "destination": "https://solana.com/docs/programs/examples", "source": "/developing/on-chain-programs/examples" },
      { "destination": "https://solana.com/docs/programs/faq", "source": "/developing/on-chain-programs/faq" },
      { "destination": "https://solana.com/docs/programs/limitations", "source": "/developing/on-chain-programs/limitations" },
      { "destination": "https://solana.com/docs/programs/lang-rust", "source": "/developing/on-chain-programs/developing-rust" },
      { "destination": "https://solana.com/docs/programs/lang-c", "source": "/developing/on-chain-programs/developing-c" },
      { "destination": "https://solana.com/docs/clients/javascript-reference", "source": "/developing/clients/javascript-reference" },
      { "destination": "https://solana.com/docs/clients/javascript", "source": "/developing/clients/javascript-api" },
      { "destination": "https://solana.com/docs/clients/rust", "source": "/developing/clients/rust-api" },
      { "destination": "https://solana.com/docs/intro/dev", "source": "/getstarted/overview" },
      { "destination": "https://solana.com/developers/guides/getstarted/hello-world-in-your-browser", "source": "/getstarted/hello-world" },
      { "destination": "https://solana.com/developers/guides/getstarted/setup-local-development", "source": "/getstarted/local" },
      { "destination": "https://solana.com/developers/guides/getstarted/local-rust-hello-world", "source": "/getstarted/rust" },
      { "destination": "https://solana.com/docs/core/clusters", "source": "/clusters/rpc-endpoints" },
      { "destination": "https://solana.com/docs/economics/staking", "source": "/staking" },
      { "destination": "https://solana.com/docs/economics/staking/:path*", "source": "/staking/:path*" },
      { "destination": "https://solana.com/docs/economics/inflation/:path*", "source": "/inflation/:path*" },
      { "destination": "https://solana.com/docs/more/exchange", "source": "/integrations/exchange" },
      { "destination": "https://solana.com/docs/intro/transaction_fees", "source": "/transaction_fees" },
      { "destination": "https://solana.com/docs/intro/economics", "source": "/storage_rent_economics" },
      { "destination": "https://solana.com/docs/intro/economics", "source": "/economics_overview" },
      { "destination": "https://solana.com/docs/intro/history", "source": "/history" },
      { "destination": "https://solana.com/docs/intro/wallets", "source": "/wallet-guide/support" },
      { "destination": "https://solana.com/docs/intro/wallets", "source": "/wallet-guide" },
      { "destination": "https://solana.com/docs/intro", "source": "/introduction" }
    ]
  }
EOF
  vercel deploy . --local-config="$CONFIG_FILE" --yes --token "$VERCEL_TOKEN" --prod
done

================
File: docs/docusaurus.config.js
================


================
File: docs/offline-cmd-md-links.sh
================
CLI_USAGE_RELPATH="../cli/usage.md"
SED_OMIT_NONMATCHING=$'\nt\nd'
SED_CMD="s:^#### solana-(.*):* [\`\\1\`](${CLI_USAGE_RELPATH}#solana-\\1):${SED_OMIT_NONMATCHING}"
OFFLINE_CMDS=$(grep -E '#### solana-|--signer ' src/cli/usage.md | grep -B1 -- --signer | sed -Ee "$SED_CMD")
grep -vE '\b(pay)\b' <<<"$OFFLINE_CMDS"

================
File: docs/package.json
================
{
  "name": "solana-docs",
  "version": "0.0.0",
  "private": true,
  "scripts": {
    "start": "docusaurus start",
    "build": "docusaurus build",
    "serve": "docusaurus serve",
    "clear": "docusaurus clear",
    "help": "docusaurus --help",
    "swizzle": "docusaurus swizzle",
    "deploy": "docusaurus deploy",
    "format": "prettier --check \"**/*.{js,jsx,json,md,scss}\"",
    "format:fix": "prettier --write \"**/*.{js,jsx,json,md,scss}\"",
    "lint": "set -ex; eslint .",
    "lint:fix": "npm run lint -- --fix"
  },
  "dependencies": {
    "@docusaurus/core": "^2.4.3",
    "@docusaurus/plugin-google-gtag": "^2.4.3",
    "@docusaurus/preset-classic": "^2.4.3",
    "@docusaurus/theme-search-algolia": "^2.4.3",
    "babel-eslint": "^10.1.0",
    "clsx": "^1.2.1",
    "eslint": "^7.32.0",
    "eslint-plugin-react": "^7.37.4",
    "postcss": "^8.5.3",
    "postcss-loader": "^4.3.0",
    "prettier": "^2.8.8",
    "react": "^16.14.0",
    "react-dom": "^16.14.0",
    "rehype-katex": "^4.0.0",
    "remark-math": "^3.0.1"
  },
  "overrides": {
    "got@<11.8.5": "^11.8.5",
    "katex@^0.12": "^0.16.21",
    "minimatch@^3.0.0": "^3.0.5",
    "path-to-regexp@^1.7.0": "^1.9.0",
    "serve-handler@^6.1.0": "^6.1.6",
    "trim@^0.0.1": "^0.0.3",
    "wait-on@^6.0.0": "^8.0.3"
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  }
}

================
File: docs/README.md
================
# Solana Validator Docs Readme

This validator's documentation is built using [Docusaurus v2](https://v2.docusaurus.io/) with `npm`.
Static content delivery is handled using `vercel`.

> Note: The documentation within this repo is specifically focused on the
> Agave validator client maintained by Anza. Solana protocol documentation, which applies
> to all Solana validator implementations, is maintained within the
> [`developer-content`](https://github.com/solana-foundation/developer-content/)
> repo. Those protocol docs are managed by the Solana Foundation within their
> GitHub organization and are publicly accessible via
> [solana.com/docs](https://solana.com/docs)

## Local Development

To set up the Solana Validator Docs site locally:

- install dependencies using `npm`
- build locally via `./build.sh`
- run the local development server
- make your changes and updates as needed

> Note: After cloning this repo to your local machine, all the local development commands are run from within this `docs` directory.

### Install dependencies

Install the site's dependencies via `npm`:

```bash
npm install
```

### Build locally

The build script generates static content into the `build` directory and can be served using any static content hosting service.

```bash
./build.sh
```

Running this build script requires **Docker**, and will auto fetch the [solanalabs/rust](https://hub.docker.com/r/solanalabs/rust) image from Docker hub to compile the desired version of the [Solana CLI](https://docs.anza.xyz/cli) from source.

This build script will also:

- generate the `cli/usage.md` document from the output of each of the Solana CLI commands and sub-commands
- convert each of the `art/*.bob` files into SVG images used throughout the docs

> Note: Running this build script is **required** before being able to run the site locally via the `npm run start` command since it will generate the `cli/usage.md` document.

If you run into errors or issues with this step, see [Common Issues](#common-issues) below. See also [CI Build Flow](#ci-build-flow) for more details on production deployments of the docs.

### Local development server

This command starts the Docusaurus local development server and opens up a browser window.

```bash
npm run start
```

> Note: Most changes are reflected live without having to restart the server or refresh the page. However, some changes may require a manual refresh of the page or a restart of the development server (via the command above).

## CI Build Flow

The docs are built and published in GitHub Actions with the `docs.yml` workflow. On each PR, the docs are built but not published.

In each post-commit build, docs are built and published using `vercel` to their respective domain depending on the build branch.

- Master branch docs are published to `edge.docs.anza.xyz`
- Beta branch docs are published to `beta.docs.anza.xyz`
- Latest release tag docs are published to `docs.anza.xyz`

## Common Issues

### Bad sidebars file (or `cli/usage` not found)

```bash
Error: Bad sidebars file.
These sidebar document ids do not exist:
- cli/usage,
```

If you have NOT successfully run the build script, then the `cli/usage.md` file will not exist within your local repo (since it is in `.gitignore`). Not having this doc file, will result in the error message above.

If the Rust toolchain (specifically `cargo`) is installed on your system, you can specifically build the `cli/usage.md` document via:

```bash
./build-cli-usage.sh
```

Or using Docker and the normal build script, you can perform a full production build of the docs to generate this file:

```bash
./build.sh
```

### Permission denied for the Docker socket

```bash
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post
```

Running docs build script (`./build.sh`) required the use of Docker.\*\*\*\*

Ensuring you have Docker installed on your system and it is running.

You may also try running either of these build scripts (and by association, Docker) with elevation permissions via `sudo`:

```bash
sudo ./build.sh
# or
sudo ./build-cli-usage.sh
```

### Multiple SVG images not found

```bash
Error: Image static/img/***.svg used in src/***.md not found.
```

During the build process of the docs (specifically within the `./convert-ascii-to-svg.sh` script run by `./build.sh`), each of the `art/*.bob` files are converted to SVG images and saved to the `static/img` directory.

To correct this issue, use the steps above to [build the docs locally](#build-locally).

> Note: While not generating and saving these SVG images within your local repo will **NOT** prevent you from running the local development server, it will result in numerous output errors in your terminal.

================
File: docs/set-solana-release-tag.sh
================
set -e
cd "$(dirname "$0")"
eval "$(../ci/channel-info.sh)"
if [[ -n $BETA_CHANNEL_LATEST_TAG ]]; then
  LATEST_AGAVE_RELEASE_VERSION=$BETA_CHANNEL_LATEST_TAG
else
  LATEST_AGAVE_RELEASE_VERSION=$STABLE_CHANNEL_LATEST_TAG
fi
VERSION_FOR_DOCS_RS="${LATEST_AGAVE_RELEASE_VERSION:1}"
set -x
if [[ -n $CI ]]; then
  sed --version || { echo "Error: Incompatible version of sed, use gnu sed"; exit 1; }
  find src/ -name \*.md -exec sed -i "s/LATEST_AGAVE_RELEASE_VERSION/$LATEST_AGAVE_RELEASE_VERSION/g" {} \;
  find src/ -name \*.md -exec sed -i "s/VERSION_FOR_DOCS_RS/$VERSION_FOR_DOCS_RS/g" {} \;
fi

================
File: docs/sidebars.js
================


================
File: dos/src/cli.rs
================
pub struct DosClientParameters {
⋮----
pub struct TransactionParams {
⋮----
pub enum Mode {
⋮----
pub enum DataType {
⋮----
pub enum TransactionType {
⋮----
fn addr_parser(addr: &str) -> Result<SocketAddr, &'static str> {
⋮----
Ok(v) => Ok(v),
Err(_) => Err("failed to parse address"),
⋮----
fn pubkey_parser(pubkey: &str) -> Result<Pubkey, &'static str> {
⋮----
Err(_) => Err("failed to parse pubkey"),
⋮----
fn validate_input(params: &DosClientParameters) {
⋮----
eprintln!("unsupported data type");
exit(1);
⋮----
eprintln!(
⋮----
pub fn build_cli_parameters() -> DosClientParameters {
⋮----
validate_input(&cmd_params);
⋮----
mod tests {
⋮----
fn test_cli_parse_rpc_no_data_input() {
let result = DosClientParameters::try_parse_from(vec![
⋮----
assert!(result.is_err());
assert_eq!(
⋮----
fn test_cli_parse_rpc_data_input() {
let entrypoint_addr: SocketAddr = "127.0.0.1:8001".parse().unwrap();
⋮----
let pubkey_str: String = pubkey.to_string();
let params = DosClientParameters::try_parse_from(vec![
⋮----
.unwrap();
⋮----
fn test_cli_parse_dos_valid_signatures() {
⋮----
fn test_cli_parse_dos_transfer() {
⋮----
fn test_cli_parse_dos_create_account() {
⋮----
fn test_cli_parse_dos_conflicting_sign_instruction() {

================
File: dos/src/lib.rs
================
pub mod cli;

================
File: dos/src/main.rs
================
fn compute_rate_per_second(count: usize) -> usize {
⋮----
struct TransactionGenerator {
⋮----
impl TransactionGenerator {
fn new(transaction_params: TransactionParams) -> Self {
⋮----
.checked_sub(Duration::from_secs(100))
.unwrap(),
⋮----
fn generate<T: 'static + TpsClient + Send + Sync>(
⋮----
let client = client.as_ref().unwrap();
let destinations = destinations.unwrap();
let payer = payer.as_ref().unwrap();
self.generate_with_blockhash(payer, destinations, client)
⋮----
self.generate_without_blockhash(destinations)
⋮----
fn generate_with_blockhash<T: 'static + TpsClient + Send + Sync>(
⋮----
&& self.last_generated.elapsed().as_millis() > 1000
⋮----
self.blockhash = client.get_latest_blockhash().unwrap();
⋮----
let transaction_type = self.transaction_params.transaction_type.as_ref().unwrap();
⋮----
self.create_multi_transfer_transaction(payer, &destinations)
⋮----
self.create_account_transaction(payer, destinations[0])
⋮----
fn create_multi_transfer_transaction(&self, payer: &Keypair, to: &[&Keypair]) -> Transaction {
⋮----
let to: Vec<(Pubkey, u64)> = to.iter().map(|to| (to.pubkey(), to_transfer)).collect();
let instructions = system_instruction::transfer_many(&payer.pubkey(), to.as_slice());
let message = Message::new(&instructions, Some(&payer.pubkey()));
⋮----
tx.sign(&[payer], self.blockhash);
⋮----
fn create_account_transaction(&self, payer: &Keypair, to: &Keypair) -> Transaction {
⋮----
let instructions = vec![system_instruction::create_account(
⋮----
let signers: Vec<&Keypair> = vec![payer, to];
⋮----
fn generate_without_blockhash(
⋮----
let program_ids = vec![system_program::id(), stake::program::id()];
let instructions = vec![CompiledInstruction::new(
⋮----
let num_signatures = self.transaction_params.num_signatures.unwrap();
tx.signatures = vec![Signature::new_unique(); num_signatures];
⋮----
struct TransactionBatchMsg {
⋮----
fn create_sender_thread(
⋮----
let connection = connection_cache.get_connection(target);
let stats_timer_receiver = tick(Duration::from_millis(SAMPLE_PERIOD_MS));
let progress_timer_receiver = tick(Duration::from_secs(PROGRESS_TIMEOUT_S));
⋮----
thread::Builder::new().name("Sender".to_string()).spawn(move || {
⋮----
select! {
⋮----
}).unwrap()
⋮----
fn create_generator_thread<T: 'static + TpsClient + Send + Sync>(
⋮----
let tx_sender = tx_sender.clone();
let mut transaction_generator = transaction_generator.clone();
⋮----
// Generate n=1000 unique keypairs
// The number of chunks is described by binomial coefficient
// and hence this choice of n provides large enough number of permutations
⋮----
// 1000 is arbitrary number. In case of permutation_size > 1,
// this guarantees large enough set of unique permutations
let permutation_size = get_permutation_size(
transaction_params.num_signatures.as_ref(),
transaction_params.num_instructions.as_ref(),
⋮----
keypairs_flat = (0..num_keypairs).map(|_| Keypair::new()).collect();
⋮----
.name("Generator".to_string())
.spawn(move || {
let indexes: Vec<usize> = (0..keypairs_flat.len()).collect();
let mut it = indexes.iter().permutations(permutation_size);
⋮----
let mut permutation = it.next();
if permutation.is_none() {
// if ran out of permutations, regenerate keys
keypairs_flat.iter_mut().for_each(|v| *v = Keypair::new());
info!("Regenerate keypairs");
permutation = it.next();
⋮----
let permutation = permutation.unwrap();
Some(apply_permutation(permutation, &keypairs_flat))
⋮----
let tx = transaction_generator.generate(
payer.as_ref(),
⋮----
client.as_ref(),
⋮----
data.push(bincode::serialize(&tx).unwrap());
⋮----
measure_generate_txs.stop();
let result = tx_sender.send(TransactionBatchMsg {
⋮----
gen_time: measure_generate_txs.as_ns(),
⋮----
if result.is_err() {
// means that receiver has been dropped by sender thread
info!("Exit generator thread");
⋮----
.unwrap()
⋮----
fn get_target(
⋮----
if nodes.is_empty() {
// skip-gossip case
target = Some((solana_pubkey::new_rand(), entrypoint_addr));
⋮----
info!("************ NODE ***********");
⋮----
info!("{node:?}");
⋮----
info!("ADDR = {entrypoint_addr}");
⋮----
if node.gossip() == Some(entrypoint_addr) {
info!("{:?}", node.gossip());
⋮----
Mode::Gossip => Some((*node.pubkey(), node.gossip().unwrap())),
Mode::Tvu => Some((*node.pubkey(), node.tvu(Protocol::UDP).unwrap())),
Mode::Tpu => Some((*node.pubkey(), node.tpu(protocol).unwrap())),
⋮----
Some((*node.pubkey(), node.tpu_forwards(protocol).unwrap()))
⋮----
Mode::Repair => todo!("repair socket is not gossiped anymore!"),
⋮----
Some((*node.pubkey(), node.serve_repair(Protocol::UDP).unwrap()))
⋮----
fn get_rpc_client(
⋮----
return Ok(RpcClient::new_socket(entrypoint_addr));
⋮----
return Ok(RpcClient::new_socket(node.rpc().unwrap()));
⋮----
Err("Node with entrypoint_addr was not found")
⋮----
fn run_dos_rpc_mode_helper<F: Fn() -> bool>(iterations: usize, rpc_client_call: F) {
⋮----
if !rpc_client_call() {
⋮----
if last_log.elapsed().as_millis() > SAMPLE_PERIOD_MS as u128 {
info!(
⋮----
fn run_dos_rpc_mode(
⋮----
run_dos_rpc_mode_helper(iterations, || -> bool {
rpc_client.get_account(data_input).is_ok()
⋮----
rpc_client.get_program_accounts(data_input).is_ok()
⋮----
panic!("unsupported data type");
⋮----
fn apply_permutation<'a, T>(permutation: Vec<&usize>, items: &'a [T]) -> Vec<&'a T> {
let mut res = Vec::with_capacity(permutation.len());
⋮----
res.push(&items[*i]);
⋮----
fn create_payers<T: 'static + TpsClient + Send + Sync>(
⋮----
let res = generate_and_fund_keypairs(
client.unwrap().clone(),
⋮----
.unwrap_or_else(|e| {
eprintln!("Error could not fund keys: {e:?}");
exit(1);
⋮----
res.into_iter().map(Some).collect()
⋮----
std::iter::repeat_with(|| None).take(size).collect()
⋮----
fn get_permutation_size(num_signatures: Option<&usize>, num_instructions: Option<&usize>) -> usize {
⋮----
fn run_dos_transactions<T: 'static + TpsClient + Send + Sync>(
⋮----
// Number of payers is the number of generating threads
// Later, we will create a new payer for each thread since Keypair is not clonable
let payers: Vec<Option<Keypair>> = create_payers(
⋮----
let (tx_sender, tx_receiver) = unbounded();
let sender_thread = create_sender_thread(tx_receiver, iterations, &target, tpu_use_quic);
⋮----
.into_iter()
.map(|payer| {
create_generator_thread(
⋮----
client.clone(),
⋮----
.collect();
if let Err(err) = sender_thread.join() {
println!("join() failed with: {err:?}");
⋮----
if let Err(err) = t_generator.join() {
⋮----
fn run_dos<T: 'static + TpsClient + Send + Sync>(
⋮----
let target = get_target(
⋮----
get_rpc_client(nodes, params.entrypoint_addr).expect("Failed to get rpc client");
run_dos_rpc_mode(
⋮----
&params.data_input.unwrap(),
⋮----
let (_, target_addr) = target.expect("should have target");
info!("Targeting {target_addr}");
run_dos_transactions(
⋮----
let (target_id, target_addr) = target.expect("should have target");
⋮----
let header = RepairRequestHeader::new(keypair.pubkey(), target_id, timestamp(), 0);
⋮----
ServeRepair::repair_proto_to_bytes(&req, &keypair).unwrap()
⋮----
vec![0; params.data_size]
⋮----
info!("{tp:?}");
⋮----
create_payers(valid_blockhash, 1, client.as_ref());
let payer = payers[0].as_ref();
⋮----
get_permutation_size(tp.num_signatures.as_ref(), tp.num_instructions.as_ref());
⋮----
(0..permutation_size).map(|_| Keypair::new()).collect();
⋮----
Some(keypairs.iter().collect())
⋮----
let tx = transaction_generator.generate(payer, keypairs_chunk, client.as_ref());
info!("{tx:?}");
bincode::serialize(&tx).unwrap()
⋮----
_ => panic!("Unsupported data_type detected"),
⋮----
let socket = bind_to_unspecified().unwrap();
⋮----
thread_rng().fill(&mut data[..]);
⋮----
let res = socket.send_to(&data, target_addr);
if res.is_err() {
⋮----
fn main() {
⋮----
let mut cmd_params = build_cli_parameters();
if !cmd_params.skip_gossip && cmd_params.shred_version.is_none() {
cmd_params.shred_version = Some(
⋮----
.unwrap_or_else(|err| {
eprintln!("Failed to get shred version: {err}");
⋮----
info!("Finding cluster entry: {:?}", cmd_params.entrypoint_addr);
⋮----
let (gossip_nodes, validators) = discover_peers(
⋮----
&vec![cmd_params.entrypoint_addr],
⋮----
cmd_params.shred_version.unwrap(),
⋮----
eprintln!(
⋮----
let client = get_client(&validators, Arc::new(connection_cache));
(gossip_nodes, Some(client))
⋮----
(vec![], None)
⋮----
info!("done found {} nodes", nodes.len());
⋮----
run_dos(&nodes, 0, Some(Arc::new(quic_client)), cmd_params);
⋮----
run_dos(&nodes, 0, Some(Arc::new(udp_client)), cmd_params);
⋮----
pub mod test {
⋮----
fn run_dos_no_client(nodes: &[ContactInfo], iterations: usize, params: DosClientParameters) {
⋮----
fn test_dos() {
⋮----
timestamp(),
⋮----
let entrypoint_addr = nodes[0].gossip().unwrap();
run_dos_no_client(
⋮----
shred_version: Some(42),
⋮----
data_input: Some(Pubkey::default()),
⋮----
fn test_dos_random() {
⋮----
assert_eq!(cluster.validators.len(), num_nodes);
let nodes = cluster.get_node_pubkeys();
let node = cluster.get_contact_info(&nodes[0]).unwrap().clone();
⋮----
entrypoint_addr: cluster.entry_point_info.gossip().unwrap(),
⋮----
fn test_dos_without_blockhash() {
⋮----
.build_validator_tpu_quic_client(cluster.entry_point_info.pubkey())
⋮----
panic!("Could not create TpuClient with Quic Cache {err:?}");
⋮----
run_dos(
⋮----
Some(client.clone()),
⋮----
num_signatures: Some(8),
⋮----
Some(client),
⋮----
fn run_dos_with_blockhash_and_payer(tpu_use_quic: bool) {
⋮----
let faucet_pubkey = faucet_keypair.pubkey();
let faucet_addr = run_local_faucet_with_unique_port_for_tests(faucet_keypair);
⋮----
faucet_addr: Some(faucet_addr),
⋮----
node_stakes: vec![999_990; num_nodes],
⋮----
validator_configs: make_identical_validator_configs(
⋮----
cluster.transfer(&cluster.funding_keypair, &faucet_pubkey, 100_000_000);
⋮----
transaction_type: Some(TransactionType::Transfer),
num_instructions: Some(1),
⋮----
num_instructions: Some(8),
⋮----
transaction_type: Some(TransactionType::AccountCreation),
⋮----
fn test_dos_with_blockhash_and_payer() {
run_dos_with_blockhash_and_payer( false)
⋮----
fn test_dos_with_blockhash_and_payer_and_quic() {
run_dos_with_blockhash_and_payer( true)

================
File: dos/Cargo.toml
================
[package]
name = "solana-dos"
publish = false
description = "Tool to send various requests to cluster in order to evaluate the effect on performance"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
workspace = "../dev-bins"

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = []
dummy-for-ci-check = []
frozen-abi = []

[dependencies]
agave-logger = { workspace = true }
bincode = { workspace = true }
clap = { version = "3.1.5", features = ["derive", "cargo"] }
crossbeam-channel = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
rand = { workspace = true }
serde = { workspace = true }
solana-bench-tps = { workspace = true, features = ["agave-unstable-api"] }
solana-client = { workspace = true }
solana-connection-cache = { workspace = true }
solana-core = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-net-utils = { workspace = true }
solana-perf = { workspace = true }
solana-pubkey = { workspace = true }
solana-quic-client = { workspace = true }
solana-rpc = { workspace = true }
solana-rpc-client = { workspace = true }
solana-signature = { workspace = true, features = ["rand"] }
solana-signer = { workspace = true }
solana-stake-interface = { workspace = true }
solana-system-interface = { workspace = true }
solana-time-utils = { workspace = true }
solana-tps-client = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-version = { workspace = true }

[dev-dependencies]
solana-core = { workspace = true, features = ["dev-context-only-utils"] }
solana-faucet = { workspace = true, features = ["dev-context-only-utils"] }
solana-local-cluster = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }

================
File: download-utils/src/lib.rs
================
pub use solana_file_download::DownloadProgressRecord;
⋮----
pub fn download_genesis_if_missing(
⋮----
if !genesis_package.exists() {
let tmp_genesis_path = genesis_package.parent().unwrap().join("tmp-genesis");
let tmp_genesis_package = tmp_genesis_path.join(DEFAULT_GENESIS_ARCHIVE);
⋮----
download_file(
&format!("http://{rpc_addr}/{DEFAULT_GENESIS_ARCHIVE}"),
⋮----
Ok(tmp_genesis_package)
⋮----
Err("genesis already exists".to_string())
⋮----
pub fn download_snapshot_archive(
⋮----
fs::create_dir_all(&snapshot_archives_remote_dir).unwrap();
⋮----
if destination_path.is_file() {
return Ok(());
⋮----
match download_file(
&format!(
⋮----
Ok(()) => return Ok(()),
Err(err) => info!("{err}"),
⋮----
Err(format!(

================
File: download-utils/Cargo.toml
================
[package]
name = "solana-download-utils"
description = "Solana Download Utils"
documentation = "https://docs.rs/solana-download-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_download_utils"

[features]
agave-unstable-api = []

[dependencies]
agave-snapshots = { workspace = true }
log = { workspace = true }
solana-clock = { workspace = true }
solana-file-download = { workspace = true }
solana-genesis-config = { workspace = true }
solana-runtime = { workspace = true }

[dev-dependencies]
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }

================
File: entry/benches/entry_sigverify.rs
================
extern crate test;
⋮----
fn bench_cpusigverify(bencher: &mut Bencher) {
⋮----
.map(|_| {
let transaction = test_tx();
entry::next_entry_mut(&mut Hash::default(), 0, vec![transaction])
⋮----
let message_hash = versioned_tx.verify_and_hash_message()?;
⋮----
Ok(sanitized_tx)
⋮----
bencher.iter(|| {
⋮----
entry::verify_transactions(entries.clone(), &thread_pool, Arc::new(verify_transaction));

================
File: entry/src/entry.rs
================
pub type EntrySender = Sender<Vec<Entry>>;
pub type EntryReceiver = Receiver<Vec<Entry>>;
⋮----
pub fn init_poh() {
init(OsStr::new("libpoh-simd.so"));
⋮----
fn init(name: &OsStr) {
⋮----
info!("Loading {name:?}");
INIT_HOOK.call_once(|| {
⋮----
perf_libs_path.to_str().unwrap_or("").to_string(),
⋮----
path = perf_libs_path.join(name);
path.as_os_str()
⋮----
Ok(api) => _ = API.set(api),
Err(err) => error!("Unable to load {lib_name:?}: {err}"),
⋮----
pub fn api() -> Option<&'static Container<Api<'static>>> {
⋮----
if std::env::var("TEST_PERF_LIBS").is_ok() {
init_poh()
⋮----
API.get()
⋮----
pub struct Api<'a> {
⋮----
/// Each Entry contains three pieces of data. The `num_hashes` field is the number
/// of hashes performed since the previous entry.  The `hash` field is the result
⋮----
/// of hashes performed since the previous entry.  The `hash` field is the result
/// of hashing `hash` from the previous entry `num_hashes` times.  The `transactions`
⋮----
/// of hashing `hash` from the previous entry `num_hashes` times.  The `transactions`
/// field points to Transactions that took place shortly before `hash` was generated.
⋮----
/// field points to Transactions that took place shortly before `hash` was generated.
///
⋮----
///
/// If you multiply `num_hashes` by the amount of time it takes to generate a new hash, you
⋮----
/// If you multiply `num_hashes` by the amount of time it takes to generate a new hash, you
/// get a duration estimate since the last `Entry`. Since processing power increases
⋮----
/// get a duration estimate since the last `Entry`. Since processing power increases
/// over time, one should expect the duration `num_hashes` represents to decrease proportionally.
⋮----
/// over time, one should expect the duration `num_hashes` represents to decrease proportionally.
/// An upper bound on Duration can be estimated by assuming each hash was generated by the
⋮----
/// An upper bound on Duration can be estimated by assuming each hash was generated by the
/// world's fastest processor at the time the entry was recorded. Or said another way, it
⋮----
/// world's fastest processor at the time the entry was recorded. Or said another way, it
#[derive(Serialize, Deserialize, Debug, Default, PartialEq, Eq, Clone, SchemaWrite, SchemaRead)]
pub struct Entry {
⋮----
pub struct EntrySummary {
⋮----
fn from(entry: &Entry) -> Self {
⋮----
num_transactions: entry.transactions.len() as u64,
⋮----
pub enum EntryType<Tx: TransactionWithMeta> {
⋮----
impl Entry {
pub fn new(prev_hash: &Hash, mut num_hashes: u64, transactions: Vec<Transaction>) -> Self {
if num_hashes == 0 && !transactions.is_empty() {
⋮----
let transactions = transactions.into_iter().map(Into::into).collect::<Vec<_>>();
let hash = next_hash(prev_hash, num_hashes, &transactions);
⋮----
pub fn new_mut(
⋮----
pub fn new_tick(num_hashes: u64, hash: &Hash) -> Self {
⋮----
transactions: vec![],
⋮----
pub fn verify(&self, start_hash: &Hash) -> bool {
let ref_hash = next_hash(start_hash, self.num_hashes, &self.transactions);
⋮----
warn!(
⋮----
pub fn is_tick(&self) -> bool {
self.transactions.is_empty()
⋮----
pub fn hash_transactions(transactions: &[VersionedTransaction]) -> Hash {
⋮----
.iter()
.flat_map(|tx| tx.signatures.iter())
.collect();
⋮----
if let Some(root_hash) = merkle_tree.get_root() {
⋮----
pub fn next_hash(
⋮----
if num_hashes == 0 && transactions.is_empty() {
⋮----
poh.hash(num_hashes.saturating_sub(1));
if transactions.is_empty() {
poh.tick().unwrap().hash
⋮----
poh.record(hash_transactions(transactions)).unwrap().hash
⋮----
pub struct EntryVerificationState {
⋮----
pub struct EntrySigVerificationState<Tx: TransactionWithMeta> {
⋮----
pub fn entries(&mut self) -> Option<Vec<EntryType<Tx>>> {
self.entries.take()
⋮----
pub fn status(&self) -> bool {
⋮----
impl EntryVerificationState {
⋮----
pub fn poh_duration_us(&self) -> u64 {
⋮----
pub fn verify_transactions<Tx: TransactionWithMeta + Send + Sync>(
⋮----
thread_pool.install(|| {
⋮----
.into_par_iter()
.map(|entry| {
if entry.transactions.is_empty() {
Ok(EntryType::Tick(entry.hash))
⋮----
Ok(EntryType::Transactions(
⋮----
.map(verify.as_ref())
⋮----
.collect()
⋮----
pub fn start_verify_transactions<Tx: TransactionWithMeta + Send + Sync + 'static>(
⋮----
start_verify_transactions_cpu(entries, skip_verification, thread_pool, verify)
⋮----
fn start_verify_transactions_cpu<Tx: TransactionWithMeta + Send + Sync + 'static>(
⋮----
move |versioned_tx| verify(versioned_tx, mode)
⋮----
let entries = verify_transactions(entries, thread_pool, Arc::new(verify_func))?;
Ok(EntrySigVerificationState {
⋮----
entries: Some(entries),
⋮----
fn compare_hashes(computed_hash: Hash, ref_entry: &Entry) -> bool {
let actual = if !ref_entry.transactions.is_empty() {
let tx_hash = hash_transactions(&ref_entry.transactions);
⋮----
poh.record(tx_hash).unwrap().hash
⋮----
pub trait EntrySlice {
⋮----
impl EntrySlice for [Entry] {
fn verify(&self, start_hash: &Hash, thread_pool: &ThreadPool) -> EntryVerificationState {
self.verify_cpu(start_hash, thread_pool)
⋮----
fn verify_cpu_generic(
⋮----
let entry_pairs = genesis.par_iter().chain(self).zip(self);
let res = thread_pool.install(|| {
entry_pairs.all(|(x0, x1)| {
let r = x1.verify(&x0.hash);
⋮----
let poh_duration_us = now.elapsed().as_micros() as u64;
⋮----
fn verify_cpu_x86_simd(
⋮----
use solana_hash::HASH_BYTES;
⋮----
let aligned_len = self.len().div_ceil(simd_len) * simd_len;
let mut hashes_bytes = vec![0u8; HASH_BYTES * aligned_len];
⋮----
.chain(self)
.enumerate()
.for_each(|(i, entry)| {
if i < self.len() {
⋮----
hashes_bytes[start..end].copy_from_slice(&entry.hash.to_bytes());
⋮----
let mut hashes_chunked: Vec<_> = hashes_bytes.chunks_mut(simd_len * HASH_BYTES).collect();
⋮----
.map(|entry| entry.num_hashes.saturating_sub(1))
⋮----
num_hashes.resize(aligned_len, 0);
let num_hashes: Vec<_> = num_hashes.chunks(simd_len).collect();
⋮----
.par_iter_mut()
.zip(num_hashes)
⋮----
.all(|(i, (chunk, num_hashes))| {
⋮----
(api().unwrap().poh_verify_many_simd_avx2)(
chunk.as_mut_ptr(),
num_hashes.as_ptr(),
⋮----
(api().unwrap().poh_verify_many_simd_avx512skx)(
⋮----
panic!("unsupported simd len: {simd_len}");
⋮----
let entry_end = std::cmp::min(entry_start + simd_len, self.len());
⋮----
.all(|(j, ref_entry)| {
⋮----
.map(Hash::new_from_array)
.unwrap();
compare_hashes(hash, ref_entry)
⋮----
fn verify_cpu(&self, start_hash: &Hash, thread_pool: &ThreadPool) -> EntryVerificationState {
⋮----
is_x86_feature_detected!("avx2"),
is_x86_feature_detected!("avx512f"),
⋮----
if api().is_some() {
if has_avx512 && self.len() >= 128 {
self.verify_cpu_x86_simd(start_hash, 16, thread_pool)
} else if has_avx2 && self.len() >= 48 {
self.verify_cpu_x86_simd(start_hash, 8, thread_pool)
⋮----
self.verify_cpu_generic(start_hash, thread_pool)
⋮----
fn verify_tick_hash_count(&self, tick_hash_count: &mut u64, hashes_per_tick: u64) -> bool {
⋮----
*tick_hash_count = tick_hash_count.saturating_add(entry.num_hashes);
if entry.is_tick() {
⋮----
fn tick_count(&self) -> u64 {
self.iter().filter(|e| e.is_tick()).count() as u64
⋮----
pub fn next_entry_mut(start: &mut Hash, num_hashes: u64, transactions: Vec<Transaction>) -> Entry {
⋮----
pub fn create_ticks(num_ticks: u64, hashes_per_tick: u64, mut hash: Hash) -> Vec<Entry> {
repeat_with(|| next_entry_mut(&mut hash, hashes_per_tick, vec![]))
.take(num_ticks as usize)
⋮----
pub fn next_entry(prev_hash: &Hash, num_hashes: u64, transactions: Vec<Transaction>) -> Entry {
⋮----
next_versioned_entry(prev_hash, num_hashes, transactions)
⋮----
pub fn next_versioned_entry(
⋮----
assert!(num_hashes > 0 || transactions.is_empty());
⋮----
hash: next_hash(prev_hash, num_hashes, &transactions),
⋮----
pub fn thread_pool_for_tests() -> ThreadPool {
⋮----
.num_threads(4)
.thread_name(|i| format!("solEntryTest{i:02}"))
.build()
.expect("new rayon threadpool")
⋮----
pub fn thread_pool_for_benches() -> ThreadPool {
⋮----
.num_threads(num_cpus::get())
.thread_name(|i| format!("solEntryBnch{i:02}"))
⋮----
mod tests {
⋮----
fn create_random_ticks(num_ticks: u64, max_hashes_per_tick: u64, mut hash: Hash) -> Vec<Entry> {
repeat_with(|| {
let hashes_per_tick = rng().random_range(1..max_hashes_per_tick);
next_entry_mut(&mut hash, hashes_per_tick, vec![])
⋮----
fn test_entry_verify() {
⋮----
let one = hash(zero.as_ref());
assert!(Entry::new_tick(0, &zero).verify(&zero));
assert!(!Entry::new_tick(0, &zero).verify(&one));
assert!(next_entry(&zero, 1, vec![]).verify(&zero));
assert!(!next_entry(&zero, 1, vec![]).verify(&one));
⋮----
fn test_verify_transactions<Tx: TransactionWithMeta + Send + Sync + 'static>(
⋮----
let verify = verify.clone();
⋮----
verify(versioned_tx, verification_mode)
⋮----
verify_transactions(entries.clone(), thread_pool, Arc::new(verify_func));
cpu_verify_result.is_ok()
⋮----
fn test_entry_transaction_verify() {
⋮----
// First, verify entries
⋮----
let tx0 = system_transaction::transfer(&keypair, &keypair.pubkey(), 0, zero);
let tx1 = system_transaction::transfer(&keypair, &keypair.pubkey(), 1, zero);
let e0 = Entry::new(&zero, 0, vec![tx0, tx1]);
assert!(e0.verify(&zero));
let tx2 = system_transaction::transfer(&keypair, &keypair.pubkey(), 2, zero);
let tx3 = system_transaction::transfer(&keypair, &keypair.pubkey(), 3, zero);
let e1 = Entry::new(&zero, 0, vec![tx2, tx3]);
assert!(e1.verify(&zero));
let es = vec![e0, e1];
let thread_pool = ThreadPoolBuilder::new().build().unwrap();
// Next, verify entry slice
⋮----
let message_hash = versioned_tx.verify_and_hash_message()?;
⋮----
sanitized_tx.verify()?;
Ok(sanitized_tx)
⋮----
assert!(test_verify_transactions(
⋮----
fn test_transaction_reorder_attack() {
⋮----
let mut e0 = Entry::new(&zero, 0, vec![tx0.clone(), tx1.clone()]);
⋮----
// Next, swap two transactions and ensure verification fails.
e0.transactions[0] = tx1.into(); // <-- attack
e0.transactions[1] = tx0.into();
assert!(!e0.verify(&zero));
⋮----
fn test_transaction_signing() {
let thread_pool = thread_pool_for_tests();
use solana_signature::Signature;
⋮----
// Verify entry with 2 transactions
let mut e0 = [Entry::new(&zero, 0, vec![tx0, tx1])];
assert!(e0.verify(&zero, &thread_pool).status());
// Clear signature of the first transaction, see that it does not verify
⋮----
assert!(!e0.verify(&zero, &thread_pool).status());
// restore original signature
⋮----
// Resize signatures and see verification fails.
let len = e0[0].transactions[0].signatures.len();
⋮----
.resize(len - 1, Signature::default());
⋮----
// Pass an entry with no transactions
let e0 = [Entry::new(&zero, 0, vec![])];
⋮----
fn test_next_entry() {
⋮----
let tick = next_entry(&zero, 1, vec![]);
assert_eq!(tick.num_hashes, 1);
assert_ne!(tick.hash, zero);
let tick = next_entry(&zero, 0, vec![]);
assert_eq!(tick.num_hashes, 0);
assert_eq!(tick.hash, zero);
⋮----
let entry0 = next_entry(&zero, 1, vec![tx0.clone()]);
assert_eq!(entry0.num_hashes, 1);
assert_eq!(entry0.hash, next_hash(&zero, 1, &[tx0.into()]));
⋮----
fn test_next_entry_panic() {
⋮----
let tx = system_transaction::transfer(&keypair, &keypair.pubkey(), 0, zero);
next_entry(&zero, 0, vec![tx]);
⋮----
fn test_verify_slice1() {
⋮----
// base case
assert!(vec![][..].verify(&zero, &thread_pool).status());
// singleton case 1
assert!(vec![Entry::new_tick(0, &zero)][..]
⋮----
// singleton case 2, bad
assert!(!vec![Entry::new_tick(0, &zero)][..]
⋮----
// inductive step
assert!(vec![next_entry(&zero, 0, vec![]); 2][..]
⋮----
let mut bad_ticks = vec![next_entry(&zero, 0, vec![]); 2];
⋮----
// inductive step, bad
assert!(!bad_ticks.verify(&zero, &thread_pool).status());
⋮----
fn test_verify_slice_with_hashes1() {
⋮----
let two = hash(one.as_ref());
⋮----
assert!(vec![][..].verify(&one, &thread_pool).status());
⋮----
assert!(vec![Entry::new_tick(1, &two)][..]
⋮----
assert!(!vec![Entry::new_tick(1, &two)][..]
⋮----
let mut ticks = vec![next_entry(&one, 1, vec![])];
ticks.push(next_entry(&ticks.last().unwrap().hash, 1, vec![]));
⋮----
assert!(ticks.verify(&one, &thread_pool).status());
let mut bad_ticks = vec![next_entry(&one, 1, vec![])];
bad_ticks.push(next_entry(&bad_ticks.last().unwrap().hash, 1, vec![]));
⋮----
assert!(!bad_ticks.verify(&one, &thread_pool).status());
⋮----
fn test_verify_slice_with_hashes_and_transactions() {
⋮----
let tx0 = system_transaction::transfer(&alice_keypair, &bob_keypair.pubkey(), 1, one);
let tx1 = system_transaction::transfer(&bob_keypair, &alice_keypair.pubkey(), 1, one);
⋮----
assert!(vec![next_entry(&one, 1, vec![tx0.clone()])][..]
⋮----
assert!(!vec![next_entry(&one, 1, vec![tx0.clone()])][..]
⋮----
let mut ticks = vec![next_entry(&one, 1, vec![tx0.clone()])];
ticks.push(next_entry(
&ticks.last().unwrap().hash,
⋮----
vec![tx1.clone()],
⋮----
let mut bad_ticks = vec![next_entry(&one, 1, vec![tx0])];
bad_ticks.push(next_entry(&bad_ticks.last().unwrap().hash, 1, vec![tx1]));
⋮----
fn test_verify_tick_hash_count() {
⋮----
transactions: vec![tx.clone()],
⋮----
transactions: vec![tx],
⋮----
// empty batch should succeed if hashes_per_tick hasn't been reached
⋮----
let mut entries = vec![];
assert!(entries.verify_tick_hash_count(&mut tick_hash_count, hashes_per_tick));
assert_eq!(tick_hash_count, 0);
⋮----
assert!(!entries.verify_tick_hash_count(&mut tick_hash_count, hashes_per_tick));
assert_eq!(tick_hash_count, hashes_per_tick);
⋮----
entries = vec![max_hash_tx_entry.clone()];
assert!(entries.verify_tick_hash_count(&mut tick_hash_count, 0));
⋮----
entries = vec![partial_tick_entry.clone()];
⋮----
assert_eq!(tick_hash_count, hashes_per_tick - 1);
⋮----
entries = vec![no_hash_tx_entry, full_tick_entry.clone()];
⋮----
assert!(!entries.verify_tick_hash_count(&mut tick_hash_count, hashes_per_tick - 1));
⋮----
entries = vec![partial_tx_entry];
⋮----
entries = vec![full_tx_entry.clone(), no_hash_tick_entry];
⋮----
entries = vec![full_tx_entry.clone(), single_hash_tick_entry.clone()];
⋮----
assert_eq!(tick_hash_count, hashes_per_tick + 1);
⋮----
entries = vec![full_tx_entry];
⋮----
entries = vec![single_hash_tx_entry.clone(), partial_tick_entry];
⋮----
.map(|_| single_hash_tx_entry.clone())
⋮----
entries = [tx_entries, vec![single_hash_tick_entry]].concat();
⋮----
entries = vec![full_tick_entry.clone(), max_hash_tick_entry];
⋮----
assert_eq!(tick_hash_count, u64::MAX);
⋮----
entries = vec![max_hash_tx_entry, full_tick_entry];
⋮----
fn test_poh_verify_fuzz() {
⋮----
let num_ticks = rng().random_range(1..100);
info!("create {num_ticks} ticks:");
let mut entries = create_random_ticks(num_ticks, 100, Hash::default());
time.stop();
⋮----
if rng().random_ratio(1, 2) {
⋮----
let modify_idx = rng().random_range(0..num_ticks) as usize;
entries[modify_idx].hash = hash(&[1, 2, 3]);
⋮----
info!("done.. {time}");
⋮----
.verify(&Hash::default(), &thread_pool_for_tests())
.status();
assert_eq!(res, !modified);
⋮----
info!("{time} {res}");
⋮----
fn test_hash_transactions() {
let mut transactions: Vec<_> = [test_tx(), test_tx(), test_tx()]
.into_iter()
.map(VersionedTransaction::from)
⋮----
let hash1 = hash_transactions(&transactions);
transactions.swap(0, 1);
let hash2 = hash_transactions(&transactions);
assert_ne!(hash1, hash2);

================
File: entry/src/lib.rs
================
pub mod entry;
pub mod poh;
mod wincode;
extern crate log;

================
File: entry/src/poh.rs
================
pub struct Poh {
⋮----
pub struct PohEntry {
⋮----
impl Poh {
pub fn new(hash: Hash, hashes_per_tick: Option<u64>) -> Self {
⋮----
pub fn new_with_slot_info(hash: Hash, hashes_per_tick: Option<u64>, tick_number: u64) -> Self {
let hashes_per_tick = hashes_per_tick.unwrap_or(LOW_POWER_MODE);
assert!(hashes_per_tick > 1);
⋮----
pub fn reset(&mut self, hash: Hash, hashes_per_tick: Option<u64>) {
⋮----
pub fn hashes_per_tick(&self) -> u64 {
⋮----
pub fn target_poh_time(&self, target_ns_per_tick: u64) -> Instant {
assert!(self.hashes_per_tick > 0);
⋮----
pub fn hash(&mut self, max_num_hashes: u64) -> bool {
⋮----
self.hash = hash(self.hash.as_ref());
⋮----
assert!(self.remaining_hashes_until_tick > 0);
⋮----
pub fn record(&mut self, mixin: Hash) -> Option<PohEntry> {
⋮----
self.hash = hashv(&[self.hash.as_ref(), mixin.as_ref()]);
⋮----
Some(PohEntry {
⋮----
pub fn record_batches(&mut self, mixins: &[Hash], entries: &mut Vec<PohEntry>) -> bool {
let num_mixins = mixins.len() as u64;
debug_assert_ne!(num_mixins, 0, "mixins.len() == 0");
⋮----
entries.clear();
entries.reserve(mixins.len());
⋮----
entries.extend(mixins.iter().map(|mixin| {
⋮----
pub fn tick(&mut self) -> Option<PohEntry> {
⋮----
pub fn remaining_hashes_in_slot(&self, ticks_per_slot: u64) -> u64 {
debug_assert!(ticks_per_slot.is_power_of_two() && ticks_per_slot > 0);
⋮----
.saturating_sub((self.tick_number & (ticks_per_slot.wrapping_sub(1))).wrapping_add(1))
.wrapping_mul(self.hashes_per_tick)
.wrapping_add(self.remaining_hashes_until_tick)
⋮----
pub fn compute_hash_time(hashes_sample_size: u64) -> Duration {
info!("Running {hashes_sample_size} hashes...");
⋮----
v = hash(v.as_ref());
⋮----
start.elapsed()
⋮----
pub fn compute_hashes_per_tick(duration: Duration, hashes_sample_size: u64) -> u64 {
let elapsed_ms = compute_hash_time(hashes_sample_size).as_millis() as u64;
duration.as_millis() as u64 * hashes_sample_size / elapsed_ms
⋮----
mod tests {
⋮----
fn verify(initial_hash: Hash, entries: &[(PohEntry, Option<Hash>)]) -> bool {
⋮----
assert_ne!(entry.num_hashes, 0);
⋮----
current_hash = hash(current_hash.as_ref());
⋮----
Some(mixin) => hashv(&[current_hash.as_ref(), mixin.as_ref()]),
None => hash(current_hash.as_ref()),
⋮----
fn test_target_poh_time() {
⋮----
assert_eq!(poh.target_poh_time(target_ns_per_tick), poh.slot_start_time);
⋮----
assert_eq!(
⋮----
let mut poh = Poh::new(zero, Some(5));
⋮----
fn test_target_poh_time_hashes_per_tick() {
⋮----
let poh = Poh::new(zero, Some(0));
⋮----
poh.target_poh_time(target_ns_per_tick);
⋮----
fn test_poh_verify() {
⋮----
let one = hash(zero.as_ref());
let two = hash(one.as_ref());
let one_with_zero = hashv(&[zero.as_ref(), zero.as_ref()]);
⋮----
assert!(verify(
⋮----
assert!(!verify(
⋮----
fn test_poh_verify_assert() {
verify(
⋮----
fn test_poh_tick() {
let mut poh = Poh::new(Hash::default(), Some(2));
assert_eq!(poh.remaining_hashes_until_tick, 2);
assert!(poh.tick().is_none());
assert_eq!(poh.remaining_hashes_until_tick, 1);
assert_matches!(poh.tick(), Some(PohEntry { num_hashes: 2, .. }));
⋮----
fn test_poh_tick_large_batch() {
⋮----
assert!(poh.hash(1_000_000));
⋮----
assert_eq!(poh.remaining_hashes_in_slot(2), 3);
poh.tick();
⋮----
assert_eq!(poh.remaining_hashes_in_slot(2), 2);
⋮----
fn test_poh_tick_too_soon() {
⋮----
assert_eq!(poh.remaining_hashes_in_slot(2), 4);
⋮----
fn test_poh_record_not_permitted_at_final_hash() {
let mut poh = Poh::new(Hash::default(), Some(10));
assert!(poh.hash(9));
⋮----
assert_eq!(poh.remaining_hashes_in_slot(2), 11);
assert!(poh.record(Hash::default()).is_none());
assert_matches!(poh.tick(), Some(PohEntry { num_hashes: 10, .. }));
assert_matches!(
⋮----
assert_eq!(poh.remaining_hashes_until_tick, 9);
assert_eq!(poh.remaining_hashes_in_slot(2), 9);
⋮----
fn test_poh_record_batches() {
⋮----
assert!(!poh.hash(4));
⋮----
assert!(poh.record_batches(&dummy_hashes[..3], &mut entries,));
assert_eq!(entries.len(), 3);
assert_eq!(entries[0].num_hashes, 5);
assert_eq!(entries[1].num_hashes, 1);
assert_eq!(entries[2].num_hashes, 1);
assert_eq!(poh.remaining_hashes_until_tick, 3);
assert_eq!(poh.remaining_hashes_in_slot(2), 13);
assert!(!poh.record_batches(&dummy_hashes[..4], &mut entries,));
assert!(!poh.record_batches(&dummy_hashes[..3], &mut entries,));
assert!(poh.record_batches(&dummy_hashes[..2], &mut entries,));
assert_eq!(entries.len(), 2);
assert_eq!(entries[0].num_hashes, 1);

================
File: entry/src/wincode.rs
================
struct MessageHeader {
⋮----
struct CompiledInstruction {
⋮----
struct LegacyMessage {
⋮----
struct MessageAddressTableLookup {
⋮----
struct V0Message {
⋮----
pub(crate) struct VersionedTransaction {
⋮----
struct VersionedMsg;
impl SchemaWrite for VersionedMsg {
type Src = solana_message::VersionedMessage;
⋮----
fn size_of(src: &Self::Src) -> WriteResult<usize> {
⋮----
solana_message::VersionedMessage::V0(message) => Ok(1 + V0Message::size_of(message)?),
⋮----
fn write(writer: &mut Writer, src: &Self::Src) -> WriteResult<()> {
⋮----
type Dst = solana_message::VersionedMessage;
fn read(reader: &mut Reader, dst: &mut MaybeUninit<Self::Dst>) -> ReadResult<()> {
// From `solana_message`:
//
// If the first bit is set, the remaining 7 bits will be used to determine
// which message version is serialized starting from version `0`. If the first
// is bit is not set, all bytes are used to encode the legacy `Message`
// format.
⋮----
dst.write(solana_message::VersionedMessage::V0(msg));
Ok(())
⋮----
_ => Err(invalid_tag_encoding(version as usize)),
⋮----
// We've already read the variant byte which, in the legacy case, represents
⋮----
let msg = unsafe { msg.assume_init() };
dst.write(solana_message::VersionedMessage::Legacy(msg));
⋮----
mod tests {
⋮----
fn strat_byte_vec(max_len: usize) -> impl Strategy<Value = Vec<u8>> {
⋮----
fn strat_repeated_byte_vec(max_len: usize) -> impl Strategy<Value = Vec<u8>> {
(any::<u8>(), 0..=max_len).prop_map(|(b, len)| vec![b; len])
⋮----
fn strat_signature() -> impl Strategy<Value = Signature> {
any::<[u8; SIGNATURE_BYTES]>().prop_map(Signature::from)
⋮----
fn strat_address() -> impl Strategy<Value = Address> {
any::<[u8; ADDRESS_BYTES]>().prop_map(Address::new_from_array)
⋮----
fn strat_hash() -> impl Strategy<Value = Hash> {
any::<[u8; HASH_BYTES]>().prop_map(Hash::new_from_array)
⋮----
fn strat_message_header() -> impl Strategy<Value = MessageHeader> {
(0u8..128, any::<u8>(), any::<u8>()).prop_map(|(a, b, c)| MessageHeader {
⋮----
fn strat_compiled_instruction() -> impl Strategy<Value = CompiledInstruction> {
(any::<u8>(), strat_byte_vec(128), strat_byte_vec(128)).prop_map(
⋮----
fn strat_address_table_lookup() -> impl Strategy<Value = MessageAddressTableLookup> {
(strat_address(), strat_byte_vec(128), strat_byte_vec(128)).prop_map(
⋮----
fn strat_legacy_message() -> impl Strategy<Value = LegacyMessage> {
⋮----
strat_message_header(),
proptest::collection::vec(strat_address(), 0..=8),
strat_hash(),
proptest::collection::vec(strat_compiled_instruction(), 0..=8),
⋮----
.prop_map(|(header, account_keys, recent_blockhash, instructions)| {
⋮----
fn strat_v0_message() -> impl Strategy<Value = v0::Message> {
⋮----
proptest::collection::vec(strat_compiled_instruction(), 0..=4),
proptest::collection::vec(strat_address_table_lookup(), 0..=4),
⋮----
.prop_map(
⋮----
fn strat_versioned_message() -> impl Strategy<Value = VersionedMessage> {
prop_oneof![
⋮----
fn strat_versioned_transaction() -> impl Strategy<Value = VersionedTransaction> {
⋮----
proptest::collection::vec(strat_signature(), 0..=8),
strat_versioned_message(),
⋮----
.prop_map(|(signatures, message)| VersionedTransaction {
⋮----
fn strat_entry() -> impl Strategy<Value = Entry> {
⋮----
proptest::collection::vec(strat_versioned_transaction(), 0..=4),
⋮----
.prop_map(|(num_hashes, hash, transactions)| Entry {
⋮----
fn strat_entries() -> impl Strategy<Value = Vec<Entry>> {
proptest::collection::vec(strat_entry(), 0..=4)
⋮----
proptest! {

================
File: entry/Cargo.toml
================
[package]
name = "solana-entry"
description = "Solana Entry"
documentation = "https://docs.rs/solana-poh"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_entry"

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
bincode = { workspace = true }
crossbeam-channel = { workspace = true }
dlopen2 = { workspace = true }
log = { workspace = true }
num_cpus = { workspace = true }
rayon = { workspace = true }
serde = { workspace = true }
solana-address = { workspace = true }
solana-hash = { workspace = true }
solana-measure = { workspace = true }
solana-merkle-tree = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-packet = { workspace = true }
solana-perf = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-short-vec = { workspace = true }
solana-signature = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
wincode = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
agave-reserved-account-keys = { workspace = true }
assert_matches = { workspace = true }
proptest = { workspace = true }
rand = { workspace = true }
solana-entry = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-keypair = { workspace = true }
solana-message = { workspace = true }
solana-perf = { workspace = true, features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-system-transaction = { workspace = true }
solana-transaction = { workspace = true, features = ["verify"] }

[[bench]]
name = "entry_sigverify"

================
File: f
================
#!/usr/bin/env bash
# Builds jito-solana in a docker container.
# Useful for running on machines that might not have cargo installed but can run docker (Flatcar Linux).
# run `./f true` to compile with debug flags

set -eux

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" &>/dev/null && pwd)"

GIT_SHA="$(git rev-parse --short HEAD)"

echo "Git hash: $GIT_SHA"

DEBUG_FLAGS=${1-false}

DOCKER_BUILDKIT=1 docker build \
  --build-arg debug=$DEBUG_FLAGS \
  --build-arg ci_commit=$GIT_SHA \
  -t jitolabs/build-solana \
  -f dev/Dockerfile . \
  --progress=plain

# Creates a temporary container, copies solana-validator built inside container there and
# removes the temporary container.
docker rm temp || true
docker container create --name temp jitolabs/build-solana
mkdir -p $SCRIPT_DIR/docker-output
# Outputs the solana-validator binary to $SOLANA/docker-output/solana-validator
docker container cp temp:/solana/docker-output $SCRIPT_DIR/
docker rm temp

================
File: faucet/src/bin/faucet.rs
================
async fn main() {
⋮----
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.short("k")
.long("keypair")
.value_name("PATH")
.takes_value(true)
.required(true)
.default_value(&default_keypair)
.help("File from which to read the faucet's keypair"),
⋮----
.long("slice")
.value_name("SECS")
⋮----
.help("Time slice over which to limit requests to faucet"),
⋮----
.long("per-time-cap")
.alias("cap")
.value_name("NUM")
⋮----
.help("Request limit for time slice, in SOL"),
⋮----
.long("per-request-cap")
⋮----
.help("Request limit for a single request, in SOL"),
⋮----
.long("allow-ip")
.value_name("IP_ADDRESS")
⋮----
.multiple(true)
.help(
⋮----
.get_matches();
let faucet_keypair = read_keypair_file(matches.value_of("keypair").unwrap())
.expect("failed to read client keypair");
let time_slice = value_of(&matches, "slice");
let per_time_cap = lamports_of_sol(&matches, "per_time_cap");
let per_request_cap = lamports_of_sol(&matches, "per_request_cap");
let allowed_ips: HashSet<_> = values_t!(matches.values_of("allowed_ip"), IpAddr)
.unwrap_or_default()
.into_iter()
.collect();
⋮----
let faucet_addr = socketaddr!(Ipv4Addr::UNSPECIFIED, FAUCET_PORT);
⋮----
let faucet1 = faucet.clone();
⋮----
let time = faucet1.lock().unwrap().time_slice;
⋮----
debug!("clearing ip cache");
faucet1.lock().unwrap().clear_caches();
⋮----
run_faucet(faucet, faucet_addr, None).await;

================
File: faucet/src/faucet_mock.rs
================
pub fn request_airdrop_transaction(
⋮----
Err(Error::other("Airdrop failed"))
⋮----
let tx = transfer(&key, &to, lamports, blockhash);
Ok(tx)

================
File: faucet/src/faucet.rs
================
macro_rules! socketaddr {
⋮----
const ERROR_RESPONSE: [u8; 2] = 0u16.to_le_bytes();
⋮----
pub enum FaucetError {
⋮----
pub enum FaucetRequest {
⋮----
pub enum FaucetTransaction {
⋮----
pub struct Faucet {
⋮----
impl Faucet {
pub fn new(
⋮----
pub fn new_with_allowed_ips(
⋮----
let time_slice = Duration::new(time_input.unwrap_or(TIME_SLICE), 0);
if let Some((per_request_cap, per_time_cap)) = per_request_cap.zip(per_time_cap) {
⋮----
warn!(
⋮----
pub fn check_time_request_limit<T: LimitByTime + std::fmt::Display>(
⋮----
let new_total = to.check_cache(self, request_amount);
to.datapoint_info(request_amount, new_total);
⋮----
return Err(FaucetError::PerTimeCapExceeded(
build_balance_message(request_amount, false, false),
to.to_string(),
build_balance_message(new_total, false, false),
build_balance_message(cap, false, false),
⋮----
Ok(())
⋮----
pub fn clear_caches(&mut self) {
self.ip_cache.clear();
self.address_cache.clear();
⋮----
pub fn build_airdrop_transaction(
⋮----
trace!("build_airdrop_transaction: {req:?}");
⋮----
let mint_pubkey = self.faucet_keypair.pubkey();
info!(
⋮----
let memo = format!(
⋮----
accounts: vec![],
data: memo.as_bytes().to_vec(),
⋮----
let message = Message::new(&[memo_instruction], Some(&mint_pubkey));
return Ok(FaucetTransaction::Memo((
⋮----
if !ip.is_loopback() && !self.allowed_ips.contains(&ip) {
self.check_time_request_limit(lamports, ip)?;
⋮----
self.check_time_request_limit(lamports, to)?;
let transfer_instruction = transfer(&mint_pubkey, &to, lamports);
let message = Message::new(&[transfer_instruction], Some(&mint_pubkey));
Ok(FaucetTransaction::Airdrop(Transaction::new(
⋮----
pub fn process_faucet_request(
⋮----
let req: FaucetRequest = deserialize(bytes)?;
info!("Airdrop transaction requested...{req:?}");
let res = self.build_airdrop_transaction(req, ip);
⋮----
info!("Airdrop transaction granted");
⋮----
warn!("Memo transaction returned: {memo}");
⋮----
(response_vec.len() as u16).to_le_bytes().to_vec();
response_vec_with_length.extend_from_slice(&response_vec);
Ok(response_vec_with_length)
⋮----
warn!("Airdrop transaction failed: {err}");
Err(err)
⋮----
impl Drop for Faucet {
fn drop(&mut self) {
⋮----
pub fn request_airdrop_transaction(
⋮----
stream.set_read_timeout(Some(Duration::new(10, 0)))?;
⋮----
let req = serialize(&req).expect("serialize faucet request");
stream.write_all(&req)?;
⋮----
stream.read_exact(&mut buffer).map_err(|err| {
info!("request_airdrop_transaction: buffer length read_exact error: {err:?}");
⋮----
return Err(FaucetError::TransactionDataTooLarge(transaction_length));
⋮----
return Err(FaucetError::NoDataReceived);
⋮----
let mut buffer = vec![0; transaction_length];
⋮----
info!("request_airdrop_transaction: buffer read_exact error: {err:?}");
⋮----
let transaction: Transaction = deserialize(&buffer)?;
Ok(transaction)
⋮----
pub fn run_local_faucet_with_port(
⋮----
let faucet_addr = socketaddr!(Ipv4Addr::UNSPECIFIED, port);
⋮----
let runtime = Runtime::new().unwrap();
runtime.block_on(run_faucet(faucet, faucet_addr, Some(sender)));
⋮----
pub struct LocalFaucetConfig {
⋮----
pub fn run_local_faucet_with_config(
⋮----
let faucet_addr = socketaddr!(config.address, config.port);
⋮----
pub fn run_local_faucet_for_tests(
⋮----
let (sender, receiver) = unbounded();
run_local_faucet_with_config(
⋮----
.recv()
.expect("run_local_faucet_for_tests")
.expect("faucet_addr")
⋮----
pub fn run_local_faucet_with_unique_port_for_tests(keypair: Keypair) -> SocketAddr {
run_local_faucet_for_tests(
⋮----
pub fn run_local_faucet(faucet_keypair: Keypair, per_time_cap: Option<u64>) -> SocketAddr {
⋮----
run_local_faucet_with_port(faucet_keypair, sender, None, per_time_cap, None, 0);
⋮----
.expect("run_local_faucet")
⋮----
pub async fn run_faucet(
⋮----
.send(
⋮----
.as_ref()
.map(|listener| listener.local_addr().unwrap())
.map_err(|err| {
format!(
⋮----
.unwrap();
⋮----
error!("Faucet failed to start: {err}");
⋮----
info!("Faucet started. Listening on: {faucet_addr}");
⋮----
let faucet = faucet.clone();
match listener.accept().await {
⋮----
if let Err(e) = process(stream, faucet).await {
info!("failed to process request; error = {e:?}");
⋮----
Err(e) => debug!("failed to accept socket; error = {e:?}"),
⋮----
async fn process(
⋮----
let mut request = vec![
⋮----
while stream.read_exact(&mut request).await.is_ok() {
trace!("{request:?}");
⋮----
match stream.peer_addr() {
⋮----
info!("{:?}", e.into_inner());
ERROR_RESPONSE.to_vec()
⋮----
let ip = peer_addr.ip();
info!("Request IP: {ip:?}");
match faucet.lock().unwrap().process_faucet_request(&request, ip) {
⋮----
trace!("Airdrop response_bytes: {response_bytes:?}");
⋮----
info!("Error in request: {e}");
⋮----
stream.write_all(&response).await?;
⋮----
pub trait LimitByTime {
⋮----
impl LimitByTime for IpAddr {
fn check_cache(&self, faucet: &mut Faucet, request_amount: u64) -> u64 {
⋮----
.entry(*self)
.and_modify(|total| *total = total.saturating_add(request_amount))
.or_insert(request_amount)
⋮----
fn datapoint_info(&self, request_amount: u64, new_total: u64) {
datapoint_info!(
⋮----
impl LimitByTime for Pubkey {
⋮----
mod tests {
⋮----
fn test_check_time_request_limit() {
⋮----
let mut faucet = Faucet::new(keypair, None, Some(2), None);
let ip = socketaddr!([203, 0, 113, 1], 1234).ip();
assert!(faucet.check_time_request_limit(1, ip).is_ok());
⋮----
assert!(faucet.check_time_request_limit(1, ip).is_err());
⋮----
assert!(faucet.check_time_request_limit(1, address).is_ok());
⋮----
assert!(faucet.check_time_request_limit(1, address).is_err());
⋮----
fn test_clear_caches() {
⋮----
let ip = socketaddr!(Ipv4Addr::LOCALHOST, 0).ip();
assert_eq!(faucet.ip_cache.len(), 0);
faucet.check_time_request_limit(1, ip).unwrap();
assert_eq!(faucet.ip_cache.len(), 1);
faucet.clear_caches();
⋮----
assert!(faucet.ip_cache.is_empty());
⋮----
assert_eq!(faucet.address_cache.len(), 0);
faucet.check_time_request_limit(1, address).unwrap();
assert_eq!(faucet.address_cache.len(), 1);
⋮----
assert!(faucet.address_cache.is_empty());
⋮----
fn test_faucet_default_init() {
⋮----
let per_time_cap: Option<u64> = Some(200);
let per_request_cap: Option<u64> = Some(100);
⋮----
assert_eq!(faucet.time_slice, Duration::new(TIME_SLICE, 0));
assert_eq!(faucet.per_time_cap, per_time_cap);
assert_eq!(faucet.per_request_cap, per_request_cap);
⋮----
fn test_faucet_build_airdrop_transaction() {
⋮----
let mint_pubkey = mint.pubkey();
⋮----
faucet.build_airdrop_transaction(request, ip).unwrap()
⋮----
let message = tx.message();
assert_eq!(tx.signatures.len(), 1);
assert_eq!(
⋮----
assert_eq!(message.recent_blockhash, blockhash);
assert_eq!(message.instructions.len(), 1);
⋮----
deserialize(&message.instructions[0].data).unwrap();
assert_eq!(instruction, SystemInstruction::Transfer { lamports: 2 });
⋮----
panic!("airdrop should succeed");
⋮----
faucet = Faucet::new(mint, None, Some(2), None);
let _tx = faucet.build_airdrop_transaction(request, ip).unwrap();
let tx = faucet.build_airdrop_transaction(request, ip);
assert!(tx.is_err());
⋮----
let _tx0 = faucet.build_airdrop_transaction(request, ip).unwrap();
⋮----
let _tx1 = faucet.build_airdrop_transaction(request1, ip).unwrap();
let tx0 = faucet.build_airdrop_transaction(request, ip);
assert!(tx0.is_err());
let tx1 = faucet.build_airdrop_transaction(request1, ip);
assert!(tx1.is_err());
⋮----
let ip = socketaddr!([203, 0, 113, 1], 0).ip();
⋮----
allowed_ips.insert(ip);
faucet = Faucet::new_with_allowed_ips(mint, None, Some(2), None, allowed_ips);
⋮----
let mut faucet = Faucet::new(mint, None, None, Some(1));
⋮----
let parsed_memo = std::str::from_utf8(&message.instructions[0].data).unwrap();
⋮----
assert_eq!(parsed_memo, expected_memo);
assert_eq!(memo, expected_memo);
⋮----
panic!("airdrop attempt should result in memo tx");
⋮----
fn test_process_faucet_request() {
⋮----
let blockhash = Hash::new_from_array(to.to_bytes());
⋮----
let req = serialize(&req).unwrap();
⋮----
let expected_instruction = transfer(&keypair.pubkey(), &to, lamports);
let message = Message::new(&[expected_instruction], Some(&keypair.pubkey()));
⋮----
let expected_bytes = serialize(&expected_tx).unwrap();
let mut expected_vec_with_length = (expected_bytes.len() as u16).to_le_bytes().to_vec();
expected_vec_with_length.extend_from_slice(&expected_bytes);
⋮----
let response = faucet.process_faucet_request(&req, ip);
let response_vec = response.unwrap().to_vec();
assert_eq!(expected_vec_with_length, response_vec);
let bad_bytes = "bad bytes".as_bytes();
assert!(faucet.process_faucet_request(bad_bytes, ip).is_err());

================
File: faucet/src/lib.rs
================
pub mod faucet;
pub mod faucet_mock;

================
File: faucet/tests/local-faucet.rs
================
fn test_local_faucet() {
⋮----
let blockhash = Hash::new_from_array(to.to_bytes());
let create_instruction = transfer(&keypair.pubkey(), &to, lamports);
let message = Message::new(&[create_instruction], Some(&keypair.pubkey()));
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(keypair);
let result = request_airdrop_transaction(&faucet_addr, &to, lamports, blockhash);
assert_eq!(expected_tx, result.unwrap());

================
File: faucet/.gitignore
================
/target/
/farf/

================
File: faucet/Cargo.toml
================
[package]
name = "solana-faucet"
description = "Solana Faucet"
documentation = "https://docs.rs/solana-faucet"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_faucet"

[[bin]]
name = "solana-faucet"
path = "src/bin/faucet.rs"

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
agave-logger = { workspace = true }
bincode = { workspace = true }
clap = { workspace = true }
crossbeam-channel = { workspace = true }
log = { workspace = true }
serde = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-cli-output = { workspace = true }
solana-hash = "=3.1.0"
solana-instruction = "=3.0.0"
solana-keypair = "=3.0.1"
solana-message = "=3.0.1"
solana-metrics = { workspace = true }
solana-net-utils = { workspace = true }
solana-packet = "=3.0.0"
solana-pubkey = { version = "=3.0.0", features = ["rand"] }
solana-signer = "=3.0.0"
solana-system-interface = "=2.0"
solana-system-transaction = "=3.0.0"
solana-transaction = "=3.0.2"
solana-version = { workspace = true }
spl-memo-interface = { version = "=2.0.0" }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[dev-dependencies]
solana-faucet = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }

================
File: feature-set/src/lib.rs
================
pub struct FeatureSet {
⋮----
impl Default for FeatureSet {
fn default() -> Self {
⋮----
inactive: AHashSet::from_iter((*FEATURE_NAMES).keys().cloned()),
⋮----
impl FeatureSet {
pub fn new(active: AHashMap<Pubkey, u64>, inactive: AHashSet<Pubkey>) -> Self {
⋮----
pub fn active(&self) -> &AHashMap<Pubkey, u64> {
⋮----
pub fn active_mut(&mut self) -> &mut AHashMap<Pubkey, u64> {
⋮----
pub fn inactive(&self) -> &AHashSet<Pubkey> {
⋮----
pub fn inactive_mut(&mut self) -> &mut AHashSet<Pubkey> {
⋮----
pub fn is_active(&self, feature_id: &Pubkey) -> bool {
self.active.contains_key(feature_id)
⋮----
pub fn activated_slot(&self, feature_id: &Pubkey) -> Option<u64> {
self.active.get(feature_id).copied()
⋮----
pub fn activate(&mut self, feature_id: &Pubkey, slot: u64) {
self.inactive.remove(feature_id);
self.active.insert(*feature_id, slot);
⋮----
pub fn deactivate(&mut self, feature_id: &Pubkey) {
self.active.remove(feature_id);
self.inactive.insert(*feature_id);
⋮----
pub fn full_inflation_features_enabled(&self) -> AHashSet<Pubkey> {
⋮----
.iter()
.filter_map(|pair| {
if self.is_active(&pair.vote_id) && self.is_active(&pair.enable_id) {
Some(pair.enable_id)
⋮----
if self.is_active(&full_inflation::devnet_and_testnet::id()) {
hash_set.insert(full_inflation::devnet_and_testnet::id());
⋮----
pub fn all_enabled() -> Self {
⋮----
active: AHashMap::from_iter((*FEATURE_NAMES).keys().cloned().map(|key| (key, 0))),
⋮----
pub fn new_warmup_cooldown_rate_epoch(&self, epoch_schedule: &EpochSchedule) -> Option<u64> {
self.activated_slot(&reduce_stake_warmup_cooldown::id())
.map(|slot| epoch_schedule.get_epoch(slot))
⋮----
pub fn runtime_features(&self) -> SVMFeatureSet {
⋮----
.is_active(&move_precompile_verification_to_svm::id()),
⋮----
.is_active(&stricter_abi_and_runtime_constraints::id()),
account_data_direct_mapping: self.is_active(&account_data_direct_mapping::id()),
⋮----
.is_active(&enable_bpf_loader_set_authority_checked_ix::id()),
enable_loader_v4: self.is_active(&enable_loader_v4::id()),
deplete_cu_meter_on_vm_failure: self.is_active(&deplete_cu_meter_on_vm_failure::id()),
abort_on_invalid_curve: self.is_active(&abort_on_invalid_curve::id()),
blake3_syscall_enabled: self.is_active(&blake3_syscall_enabled::id()),
curve25519_syscall_enabled: self.is_active(&curve25519_syscall_enabled::id()),
⋮----
.is_active(&disable_deploy_of_alloc_free_syscall::id()),
disable_fees_sysvar: self.is_active(&disable_fees_sysvar::id()),
disable_sbpf_v0_execution: self.is_active(&disable_sbpf_v0_execution::id()),
⋮----
.is_active(&enable_alt_bn128_compression_syscall::id()),
enable_alt_bn128_syscall: self.is_active(&enable_alt_bn128_syscall::id()),
enable_big_mod_exp_syscall: self.is_active(&enable_big_mod_exp_syscall::id()),
enable_get_epoch_stake_syscall: self.is_active(&enable_get_epoch_stake_syscall::id()),
enable_poseidon_syscall: self.is_active(&enable_poseidon_syscall::id()),
⋮----
.is_active(&enable_sbpf_v1_deployment_and_execution::id()),
⋮----
.is_active(&enable_sbpf_v2_deployment_and_execution::id()),
⋮----
.is_active(&enable_sbpf_v3_deployment_and_execution::id()),
get_sysvar_syscall_enabled: self.is_active(&get_sysvar_syscall_enabled::id()),
last_restart_slot_sysvar: self.is_active(&last_restart_slot_sysvar::id()),
reenable_sbpf_v0_execution: self.is_active(&reenable_sbpf_v0_execution::id()),
⋮----
.is_active(&remaining_compute_units_syscall_enabled::id()),
⋮----
.is_active(&remove_bpf_loader_incorrect_program_id::id()),
⋮----
.is_active(&move_stake_and_move_lamports_ixs::id()),
⋮----
.is_active(&stake_raise_minimum_delegation_to_1_sol::id()),
deprecate_legacy_vote_ixs: self.is_active(&deprecate_legacy_vote_ixs::id()),
⋮----
.is_active(&mask_out_rent_epoch_in_vm_serialization::id()),
⋮----
.is_active(&simplify_alt_bn128_syscall_error_codes::id()),
⋮----
.is_active(&fix_alt_bn128_multiplication_input_length::id()),
increase_tx_account_lock_limit: self.is_active(&increase_tx_account_lock_limit::id()),
enable_extend_program_checked: self.is_active(&enable_extend_program_checked::id()),
⋮----
.is_active(&formalize_loaded_transaction_data_size::id()),
⋮----
.is_active(&disable_zk_elgamal_proof_program::id()),
⋮----
.is_active(&reenable_zk_elgamal_proof_program::id()),
raise_cpi_nesting_limit_to_8: self.is_active(&raise_cpi_nesting_limit_to_8::id()),
⋮----
.is_active(&provide_instruction_data_offset_in_vm_r2::id()),
increase_cpi_account_info_limit: self.is_active(&increase_cpi_account_info_limit::id()),
vote_state_v4: self.is_active(&vote_state_v4::id()),
poseidon_enforce_padding: self.is_active(&poseidon_enforce_padding::id()),
⋮----
.is_active(&fix_alt_bn128_pairing_length_check::id()),
alt_bn128_little_endian: self.is_active(&alt_bn128_little_endian::id()),
⋮----
pub mod deprecate_rewards_sysvar {
⋮----
pub mod pico_inflation {
⋮----
pub mod full_inflation {
pub mod devnet_and_testnet {
⋮----
pub mod mainnet {
pub mod certusone {
pub mod vote {
⋮----
pub mod enable {
⋮----
pub mod secp256k1_program_enabled {
⋮----
pub mod spl_token_v2_multisig_fix {
⋮----
pub mod no_overflow_rent_distribution {
⋮----
pub mod filter_stake_delegation_accounts {
⋮----
pub mod require_custodian_for_locked_stake_authorize {
⋮----
pub mod spl_token_v2_self_transfer_fix {
⋮----
pub mod warp_timestamp_again {
⋮----
pub mod check_init_vote_data {
⋮----
pub mod secp256k1_recover_syscall_enabled {
⋮----
pub mod system_transfer_zero_check {
⋮----
pub mod blake3_syscall_enabled {
⋮----
pub mod dedupe_config_program_signers {
⋮----
pub mod verify_tx_signatures_len {
⋮----
pub mod vote_stake_checked_instructions {
⋮----
pub mod rent_for_sysvars {
⋮----
pub mod libsecp256k1_0_5_upgrade_enabled {
⋮----
pub mod tx_wide_compute_cap {
⋮----
pub mod spl_token_v2_set_authority_fix {
⋮----
pub mod merge_nonce_error_into_system_error {
⋮----
pub mod disable_fees_sysvar {
⋮----
pub mod stake_merge_with_unmatched_credits_observed {
⋮----
pub mod zk_token_sdk_enabled {
⋮----
pub mod curve25519_syscall_enabled {
⋮----
pub mod curve25519_restrict_msm_length {
⋮----
pub mod versioned_tx_message_enabled {
⋮----
pub mod libsecp256k1_fail_on_bad_count {
⋮----
pub mod libsecp256k1_fail_on_bad_count2 {
⋮----
pub mod instructions_sysvar_owned_by_sysvar {
⋮----
pub mod stake_program_advance_activating_credits_observed {
⋮----
pub mod credits_auto_rewind {
⋮----
pub mod demote_program_write_locks {
⋮----
pub mod ed25519_program_enabled {
⋮----
pub mod return_data_syscall_enabled {
⋮----
pub mod reduce_required_deploy_balance {
⋮----
pub mod sol_log_data_syscall_enabled {
⋮----
pub mod stakes_remove_delegation_if_inactive {
⋮----
pub mod do_support_realloc {
⋮----
pub mod prevent_calling_precompiles_as_programs {
⋮----
pub mod optimize_epoch_boundary_updates {
⋮----
pub mod remove_native_loader {
⋮----
pub mod send_to_tpu_vote_port {
⋮----
pub mod requestable_heap_size {
⋮----
pub mod disable_fee_calculator {
⋮----
pub mod add_compute_budget_program {
⋮----
pub mod nonce_must_be_writable {
⋮----
pub mod spl_token_v3_3_0_release {
⋮----
pub mod leave_nonce_on_success {
⋮----
pub mod reject_empty_instruction_without_program {
⋮----
pub mod fixed_memcpy_nonoverlapping_check {
⋮----
pub mod reject_non_rent_exempt_vote_withdraws {
⋮----
pub mod evict_invalid_stakes_cache_entries {
⋮----
pub mod allow_votes_to_directly_update_vote_state {
⋮----
pub mod max_tx_account_locks {
⋮----
pub mod require_rent_exempt_accounts {
⋮----
pub mod filter_votes_outside_slot_hashes {
⋮----
pub mod update_syscall_base_costs {
⋮----
pub mod stake_deactivate_delinquent_instruction {
⋮----
pub mod vote_withdraw_authority_may_change_authorized_voter {
⋮----
pub mod spl_associated_token_account_v1_0_4 {
⋮----
pub mod reject_vote_account_close_unless_zero_credit_epoch {
⋮----
pub mod add_get_processed_sibling_instruction_syscall {
⋮----
pub mod bank_transaction_count_fix {
⋮----
pub mod disable_bpf_deprecated_load_instructions {
⋮----
pub mod disable_bpf_unresolved_symbols_at_runtime {
⋮----
pub mod record_instruction_in_transaction_context_push {
⋮----
pub mod syscall_saturated_math {
⋮----
pub mod check_physical_overlapping {
⋮----
pub mod limit_secp256k1_recovery_id {
⋮----
pub mod disable_deprecated_loader {
⋮----
pub mod check_slice_translation_size {
⋮----
pub mod stake_split_uses_rent_sysvar {
⋮----
pub mod add_get_minimum_delegation_instruction_to_stake_program {
⋮----
pub mod error_on_syscall_bpf_function_hash_collisions {
⋮----
pub mod reject_callx_r10 {
⋮----
pub mod drop_redundant_turbine_path {
⋮----
pub mod executables_incur_cpi_data_cost {
⋮----
pub mod fix_recent_blockhashes {
⋮----
pub mod update_rewards_from_cached_accounts {
⋮----
pub mod partitioned_epoch_rewards_superfeature {
⋮----
pub mod spl_token_v3_4_0 {
⋮----
pub mod spl_associated_token_account_v1_1_0 {
⋮----
pub mod default_units_per_instruction {
⋮----
pub mod stake_allow_zero_undelegated_amount {
⋮----
pub mod require_static_program_ids_in_transaction {
⋮----
pub mod stake_raise_minimum_delegation_to_1_sol {
⋮----
pub mod stake_minimum_delegation_for_rewards {
⋮----
pub mod add_set_compute_unit_price_ix {
⋮----
pub mod disable_deploy_of_alloc_free_syscall {
⋮----
pub mod include_account_index_in_rent_error {
⋮----
pub mod add_shred_type_to_shred_seed {
⋮----
pub mod warp_timestamp_with_a_vengeance {
⋮----
pub mod separate_nonce_from_blockhash {
⋮----
pub mod enable_durable_nonce {
⋮----
pub mod vote_state_update_credit_per_dequeue {
⋮----
pub mod quick_bail_on_panic {
⋮----
pub mod nonce_must_be_authorized {
⋮----
pub mod nonce_must_be_advanceable {
⋮----
pub mod vote_authorize_with_seed {
⋮----
pub mod preserve_rent_epoch_for_rent_exempt_accounts {
⋮----
pub mod enable_bpf_loader_extend_program_ix {
⋮----
pub mod enable_early_verification_of_account_modifications {
⋮----
pub mod skip_rent_rewrites {
⋮----
pub mod prevent_crediting_accounts_that_end_rent_paying {
⋮----
pub mod cap_bpf_program_instruction_accounts {
⋮----
pub mod loosen_cpi_size_restriction {
⋮----
pub mod use_default_units_in_fee_calculation {
⋮----
pub mod compact_vote_state_updates {
⋮----
pub mod incremental_snapshot_only_incremental_hash_calculation {
⋮----
pub mod disable_cpi_setting_executable_and_rent_epoch {
⋮----
pub mod on_load_preserve_rent_epoch_for_rent_exempt_accounts {
⋮----
pub mod account_hash_ignore_slot {
⋮----
pub mod set_exempt_rent_epoch_max {
⋮----
pub mod relax_authority_signer_check_for_lookup_table_creation {
⋮----
pub mod stop_sibling_instruction_search_at_parent {
⋮----
pub mod vote_state_update_root_fix {
⋮----
pub mod cap_accounts_data_allocations_per_transaction {
⋮----
pub mod epoch_accounts_hash {
⋮----
pub mod remove_deprecated_request_unit_ix {
⋮----
pub mod disable_rehash_for_rent_epoch {
⋮----
pub mod increase_tx_account_lock_limit {
⋮----
pub mod limit_max_instruction_trace_length {
⋮----
pub mod check_syscall_outputs_do_not_overlap {
⋮----
pub mod enable_bpf_loader_set_authority_checked_ix {
⋮----
pub mod enable_alt_bn128_syscall {
⋮----
pub mod simplify_alt_bn128_syscall_error_codes {
⋮----
pub mod enable_alt_bn128_compression_syscall {
⋮----
pub mod fix_alt_bn128_multiplication_input_length {
⋮----
pub mod enable_program_redeployment_cooldown {
⋮----
pub mod commission_updates_only_allowed_in_first_half_of_epoch {
⋮----
pub mod enable_turbine_fanout_experiments {
⋮----
pub mod disable_turbine_fanout_experiments {
⋮----
pub mod move_serialized_len_ptr_in_cpi {
⋮----
pub mod update_hashes_per_tick {
⋮----
pub mod enable_big_mod_exp_syscall {
⋮----
pub mod disable_builtin_loader_ownership_chains {
⋮----
pub mod cap_transaction_accounts_data_size {
⋮----
pub mod remove_congestion_multiplier_from_fee_calculation {
⋮----
pub mod enable_request_heap_frame_ix {
⋮----
pub mod prevent_rent_paying_rent_recipients {
⋮----
pub mod delay_visibility_of_program_deployment {
⋮----
pub mod apply_cost_tracker_during_replay {
⋮----
pub mod stricter_abi_and_runtime_constraints {
⋮----
pub mod account_data_direct_mapping {
⋮----
pub mod add_set_tx_loaded_accounts_data_size_instruction {
⋮----
pub mod switch_to_new_elf_parser {
⋮----
pub mod round_up_heap_size {
⋮----
pub mod remove_bpf_loader_incorrect_program_id {
⋮----
pub mod include_loaded_accounts_data_size_in_fee_calculation {
⋮----
pub mod native_programs_consume_cu {
⋮----
pub mod simplify_writable_program_account_check {
⋮----
pub mod stop_truncating_strings_in_syscalls {
⋮----
pub mod clean_up_delegation_errors {
⋮----
pub mod vote_state_add_vote_latency {
⋮----
pub mod checked_arithmetic_in_fee_validation {
⋮----
pub mod last_restart_slot_sysvar {
⋮----
pub mod reduce_stake_warmup_cooldown {
⋮----
pub mod revise_turbine_epoch_stakes {
⋮----
pub mod enable_poseidon_syscall {
⋮----
pub mod timely_vote_credits {
⋮----
pub mod remaining_compute_units_syscall_enabled {
⋮----
pub mod enable_loader_v4 {
⋮----
pub mod require_rent_exempt_split_destination {
⋮----
pub mod better_error_codes_for_tx_lamport_check {
⋮----
pub mod update_hashes_per_tick2 {
⋮----
pub mod update_hashes_per_tick3 {
⋮----
pub mod update_hashes_per_tick4 {
⋮----
pub mod update_hashes_per_tick5 {
⋮----
pub mod update_hashes_per_tick6 {
⋮----
pub mod validate_fee_collector_account {
⋮----
pub mod disable_rent_fees_collection {
⋮----
pub mod enable_zk_transfer_with_fee {
⋮----
pub mod drop_legacy_shreds {
⋮----
pub mod allow_commission_decrease_at_any_time {
⋮----
pub mod add_new_reserved_account_keys {
⋮----
pub mod consume_blockstore_duplicate_proofs {
⋮----
pub mod index_erasure_conflict_duplicate_proofs {
⋮----
pub mod merkle_conflict_duplicate_proofs {
⋮----
pub mod disable_bpf_loader_instructions {
⋮----
pub mod enable_zk_proof_from_account {
⋮----
pub mod cost_model_requested_write_lock_cost {
⋮----
pub mod enable_gossip_duplicate_proof_ingestion {
⋮----
pub mod chained_merkle_conflict_duplicate_proofs {
⋮----
pub mod enable_chained_merkle_shreds {
⋮----
pub mod remove_rounding_in_fee_calculation {
⋮----
pub mod enable_tower_sync_ix {
⋮----
pub mod deprecate_unused_legacy_vote_plumbing {
⋮----
pub mod reward_full_priority_fee {
⋮----
pub mod get_sysvar_syscall_enabled {
⋮----
pub mod abort_on_invalid_curve {
⋮----
pub mod migrate_feature_gate_program_to_core_bpf {
⋮----
pub mod vote_only_full_fec_sets {
⋮----
pub mod migrate_config_program_to_core_bpf {
⋮----
pub mod enable_get_epoch_stake_syscall {
⋮----
pub mod migrate_address_lookup_table_program_to_core_bpf {
⋮----
pub mod zk_elgamal_proof_program_enabled {
⋮----
pub mod verify_retransmitter_signature {
⋮----
pub mod move_stake_and_move_lamports_ixs {
⋮----
pub mod ed25519_precompile_verify_strict {
⋮----
pub mod vote_only_retransmitter_signed_fec_sets {
⋮----
pub mod move_precompile_verification_to_svm {
⋮----
pub mod enable_transaction_loading_failure_fees {
⋮----
pub mod enable_turbine_extended_fanout_experiments {
⋮----
pub mod deprecate_legacy_vote_ixs {
⋮----
pub mod disable_sbpf_v0_execution {
⋮----
pub mod reenable_sbpf_v0_execution {
⋮----
pub mod enable_sbpf_v1_deployment_and_execution {
⋮----
pub mod enable_sbpf_v2_deployment_and_execution {
⋮----
pub mod enable_sbpf_v3_deployment_and_execution {
⋮----
pub mod remove_accounts_executable_flag_checks {
⋮----
pub mod disable_account_loader_special_case {
⋮----
pub mod enable_secp256r1_precompile {
⋮----
pub mod accounts_lt_hash {
⋮----
pub mod snapshots_lt_hash {
⋮----
pub mod remove_accounts_delta_hash {
⋮----
pub mod migrate_stake_program_to_core_bpf {
⋮----
pub mod deplete_cu_meter_on_vm_failure {
⋮----
pub mod reserve_minimal_cus_for_builtin_instructions {
⋮----
pub mod raise_block_limits_to_50m {
⋮----
pub mod drop_unchained_merkle_shreds {
⋮----
pub mod relax_intrabatch_account_locks {
⋮----
pub mod create_slashing_program {
⋮----
pub mod disable_partitioned_rent_collection {
⋮----
pub mod enable_vote_address_leader_schedule {
⋮----
pub mod require_static_nonce_account {
⋮----
pub mod raise_block_limits_to_60m {
⋮----
pub mod mask_out_rent_epoch_in_vm_serialization {
⋮----
pub mod enshrine_slashing_program {
⋮----
pub mod enable_extend_program_checked {
⋮----
pub mod formalize_loaded_transaction_data_size {
⋮----
pub mod alpenglow {
⋮----
pub mod disable_zk_elgamal_proof_program {
⋮----
pub mod reenable_zk_elgamal_proof_program {
⋮----
pub mod raise_block_limits_to_100m {
⋮----
pub mod raise_account_cu_limit {
⋮----
pub mod raise_cpi_nesting_limit_to_8 {
⋮----
pub mod enforce_fixed_fec_set {
⋮----
pub mod provide_instruction_data_offset_in_vm_r2 {
⋮----
pub mod static_instruction_limit {
⋮----
pub mod discard_unexpected_data_complete_shreds {
⋮----
pub mod vote_state_v4 {
⋮----
pub mod stake_program_buffer {
⋮----
pub mod switch_to_chacha8_turbine {
⋮----
pub mod increase_cpi_account_info_limit {
⋮----
pub mod deprecate_rent_exemption_threshold {
⋮----
pub mod poseidon_enforce_padding {
⋮----
pub mod fix_alt_bn128_pairing_length_check {
⋮----
pub mod replace_spl_token_with_p_token {
use super::Pubkey;
⋮----
pub mod alt_bn128_little_endian {
⋮----
.cloned()
.collect()
⋮----
let mut feature_ids = FEATURE_NAMES.keys().collect::<Vec<_>>();
feature_ids.sort();
⋮----
hasher.hash(feature.as_ref());
⋮----
hasher.result()
⋮----
pub struct FullInflationFeaturePair {
⋮----
mod test {
⋮----
fn test_full_inflation_features_enabled_devnet_and_testnet() {
⋮----
assert!(feature_set.full_inflation_features_enabled().is_empty());
⋮----
.insert(full_inflation::devnet_and_testnet::id(), 42);
assert_eq!(
⋮----
fn test_full_inflation_features_enabled() {
⋮----
.insert(full_inflation::mainnet::certusone::vote::id(), 42);
⋮----
.insert(full_inflation::mainnet::certusone::enable::id(), 42);

================
File: feature-set/Cargo.toml
================
[package]
name = "agave-feature-set"
description = "Solana runtime feature declarations"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
readme = false

[features]
agave-unstable-api = []
frozen-abi = ["dep:solana-frozen-abi", "dep:solana-frozen-abi-macro"]

[dependencies]
ahash = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-hash = { workspace = true }
solana-pubkey = { workspace = true, default-features = false }
solana-sha256-hasher = { workspace = true }
solana-svm-feature-set = { workspace = true }

[lints]
workspace = true

================
File: fee/src/lib.rs
================
pub struct FeeFeatures {
⋮----
fn from(feature_set: &FeatureSet) -> Self {
⋮----
enable_secp256r1_precompile: feature_set.is_active(&enable_secp256r1_precompile::ID),
⋮----
pub fn calculate_fee(
⋮----
calculate_fee_details(
⋮----
.total_fee()
⋮----
pub fn calculate_fee_details(
⋮----
calculate_signature_fee(
⋮----
pub fn calculate_signature_fee(
⋮----
.saturating_add(num_ed25519_signatures)
.saturating_add(num_secp256k1_signatures)
.saturating_add(
u64::from(enable_secp256r1_precompile).wrapping_mul(num_secp256r1_signatures),
⋮----
signature_count.saturating_mul(lamports_per_signature)
⋮----
pub struct SignatureCounts {
⋮----
fn from(message: &Tx) -> Self {
⋮----
num_transaction_signatures: message.num_transaction_signatures(),
num_ed25519_signatures: message.num_ed25519_signatures(),
num_secp256k1_signatures: message.num_secp256k1_signatures(),
num_secp256r1_signatures: message.num_secp256r1_signatures(),
⋮----
mod tests {
⋮----
fn test_calculate_signature_fee() {
⋮----
assert_eq!(

================
File: fee/Cargo.toml
================
[package]
name = "solana-fee"
description = "Solana fee calculation"
documentation = "https://docs.rs/solana-fee"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
solana-fee-structure = { workspace = true }
solana-svm-transaction = { workspace = true }

================
File: fetch-core-bpf.sh
================
set -e
here=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
source "$here"/fetch-programs.sh
PREFIX="core-bpf"
programs=()
add_core_bpf_program_to_fetch() {
  declare name=$1
  declare version=$2
  declare address=$3
  declare loader=$4
  so_name="solana_${name//-/_}_program.so"
  declare download_url="https://github.com/solana-program/$name/releases/download/program%40v$version/$so_name"
  programs+=("$name $version $address $loader $download_url")
}
add_core_bpf_program_to_fetch address-lookup-table 3.0.0 AddressLookupTab1e1111111111111111111111111 BPFLoaderUpgradeab1e11111111111111111111111
add_core_bpf_program_to_fetch config 3.0.0 Config1111111111111111111111111111111111111 BPFLoaderUpgradeab1e11111111111111111111111
add_core_bpf_program_to_fetch feature-gate 0.0.1 Feature111111111111111111111111111111111111 BPFLoaderUpgradeab1e11111111111111111111111
add_core_bpf_program_to_fetch stake 1.0.0 Stake11111111111111111111111111111111111111 BPFLoaderUpgradeab1e11111111111111111111111
fetch_programs "$PREFIX" "${programs[@]}"

================
File: fetch-perf-libs.sh
================
function installPerfLibSymlinks() {
  for dir in target/{debug,release}/{,deps/}; do
    mkdir -p "$dir"
    ln -sfT "$1" "${dir}perf-libs"
  done
}
if [[ -n "$SOLANA_PERF_LIBS_PATH" ]]; then
  mkdir -p target
  ln -sfT "$SOLANA_PERF_LIBS_PATH" target/perf-libs
  installPerfLibSymlinks "$SOLANA_PERF_LIBS_PATH"
  exit 0
fi
PERF_LIBS_VERSION=v0.19.3
VERSION=$PERF_LIBS_VERSION-1
set -e
cd "$(dirname "$0")"
if [[ $VERSION != "$(cat target/perf-libs/.version 2> /dev/null)" ]]; then
  if [[ $(uname) != Linux ]]; then
    echo Note: Performance libraries are only available for Linux
    exit 0
  fi
  if [[ $(uname -m) != x86_64 ]]; then
    echo Note: Performance libraries are only available for x86_64 architecture
    exit 0
  fi
  rm -rf target/perf-libs
  mkdir -p target/perf-libs
  (
    set -x
    cd target/perf-libs
    if [[ -r ~/.cache/solana-perf-$PERF_LIBS_VERSION.tgz ]]; then
      cp ~/.cache/solana-perf-$PERF_LIBS_VERSION.tgz solana-perf.tgz
    else
      curl -L --retry 5 --retry-delay 2 --retry-connrefused -o solana-perf.tgz \
        https://github.com/solana-labs/solana-perf-libs/releases/download/$PERF_LIBS_VERSION/solana-perf.tgz
    fi
    tar zxvf solana-perf.tgz
    if [[ ! -r ~/.cache/solana-perf-$PERF_LIBS_VERSION.tgz ]]; then
      # Save it for next time
      mkdir -p ~/.cache
      mv solana-perf.tgz ~/.cache/solana-perf-$PERF_LIBS_VERSION.tgz
    fi
    echo "$VERSION" > .version
  )
  installPerfLibSymlinks ../perf-libs
fi
exit 0

================
File: fetch-programs.sh
================
upgradeableLoader=BPFLoaderUpgradeab1e11111111111111111111111
fetch_program() {
  declare prefix=$1
  declare name=$2
  declare version=$3
  declare address=$4
  declare loader=$5
  declare download_url=$6
  declare so=$prefix-$name-$version.so
  if [[ $loader == "$upgradeableLoader" ]]; then
    genesis_args+=(--upgradeable-program "$address" "$loader" "$so" none)
  else
    genesis_args+=(--bpf-program "$address" "$loader" "$so")
  fi
  if [[ -r $so ]]; then
    return
  fi
  if [[ -r ~/.cache/solana-$prefix/$so ]]; then
    cp ~/.cache/solana-"$prefix"/"$so" "$so"
  else
    echo "Downloading $name $version"
    (
      set -x
      curl -L --retry 5 --retry-delay 2 --retry-connrefused -o "$so" "$download_url"
    )
    mkdir -p ~/.cache/solana-"$prefix"
    cp "$so" ~/.cache/solana-"$prefix"/"$so"
  fi
}
fetch_programs() {
  declare prefix=$1
  shift
  declare -a programs=("$@")
  for program in "${programs[@]}"; do
    fetch_program "$prefix" $program
  done
  echo "${genesis_args[@]}" > "$prefix"-genesis-args.sh
  echo
  echo "Available $prefix programs:"
  ls -l "$prefix"-*.so
  echo
  echo "solana-genesis command-line arguments ($prefix-genesis-args.sh):"
  cat "$prefix"-genesis-args.sh
}

================
File: fetch-spl.sh
================
set -e
here=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
source "$here"/fetch-programs.sh
PREFIX="spl"
programs=()
add_spl_program_to_fetch() {
  declare name=$1
  declare version=$2
  declare address=$3
  declare loader=$4
  declare repo=$5
  case $repo in
  "jito")
    so_name="$name.so"
    download_url="https://github.com/jito-foundation/jito-programs/releases/download/v$version/$so_name"
    ;;
  "solana")
    so_name="${PREFIX}_${name//-/_}.so"
    download_url="https://github.com/solana-program/$name/releases/download/program@v$version/$so_name"
    ;;
  *)
    echo "Unsupported repo: $repo"
    return 1
    ;;
  esac
  programs+=("$name $version $address $loader $download_url")
}
add_spl_program_to_fetch token 3.5.0 TokenkegQfeZyiNwAJbNbGKPFXCWuBvf9Ss623VQ5DA BPFLoader2111111111111111111111111111111111 solana
add_spl_program_to_fetch token-2022 8.0.0 TokenzQdBNbLqP5VEhdkAS6EPFLC1PHnBqCXEpPxuEb BPFLoaderUpgradeab1e11111111111111111111111 solana
add_spl_program_to_fetch memo  1.0.0 Memo1UhkJRfHyvLMcVucJwxXeuD728EqVDDwQDxFMNo BPFLoader1111111111111111111111111111111111 solana
add_spl_program_to_fetch memo  3.0.0 MemoSq4gqABAXKb96qnH8TysNcWxMyWCqXgDLGmfcHr BPFLoader2111111111111111111111111111111111 solana
add_spl_program_to_fetch associated-token-account 1.1.2 ATokenGPvbdGVxr1b2hvZbsiqW5xWH25efTNsLJA8knL BPFLoader2111111111111111111111111111111111 solana
add_spl_program_to_fetch feature-proposal 1.0.0 Feat1YXHhH6t1juaWF74WLcfv4XoNocjXA6sPWHNgAse BPFLoader2111111111111111111111111111111111 solana
add_spl_program_to_fetch jito_tip_payment 0.1.10 T1pyyaTNZsKv2WcRAB8oVnk93mLJw2XzjtVYqCsaHqt BPFLoaderUpgradeab1e11111111111111111111111 jito
add_spl_program_to_fetch jito_tip_distribution 0.1.10 4R3gSG8BpU4t19KYj8CfnbtRpnT8gtk4dvTHxVRwc2r7 BPFLoaderUpgradeab1e11111111111111111111111 jito
fetch_programs "$PREFIX" "${programs[@]}"

================
File: fs/src/io_uring/dir_remover.rs
================
pub struct RingDirRemover {
⋮----
impl RingDirRemover {
pub fn new() -> io::Result<RingDirRemover> {
let ring = IoUring::builder().setup_sqpoll(1000).build(1024)?;
ring.submitter().register_iowq_max_workers(&mut [12, 0])?;
Ok(Self::with_ring(ring))
⋮----
fn with_ring(ring: IoUring) -> RingDirRemover {
⋮----
pub fn remove_dir_all(&mut self, path: impl Into<PathBuf>) -> io::Result<()> {
self.remove(path, true)
⋮----
pub fn remove_dir_contents(&mut self, path: impl Into<PathBuf>) -> io::Result<()> {
self.remove(path, false)
⋮----
fn remove(&mut self, path: impl Into<PathBuf>, remove_root: bool) -> io::Result<()> {
let path = path.into();
let root_path = path.clone();
⋮----
stack.push_back(path);
while let Some(dir_path) = stack.pop_front() {
⋮----
let dir_fd = fd.as_raw_fd();
let dir_key = self.ring.context_mut().dirs.insert(Directory::new(fd));
⋮----
let dir = &mut self.ring.context_mut().dirs[dir_key];
let is_dir = entry.file_type()?.is_dir();
⋮----
stack.push_back(entry.path());
⋮----
let op = UnlinkOp::new(dir_key, dir_fd, entry.file_name());
self.ring.push(Op::Unlink(op))?;
⋮----
dir.set_finished_scanning();
if dir.scanned_and_unlinked() {
let fd = dir.fd.take();
⋮----
.push(Op::Close(CloseOp::new(dir_key, fd.into_raw_fd())))?;
⋮----
self.ring.drain()?;
⋮----
if dir.file_type()?.is_dir() {
fs::remove_dir_all(dir.path())?;
⋮----
Ok(())
⋮----
pub struct State {
⋮----
pub struct Directory {
⋮----
impl Directory {
pub fn new(fd: OwnedFd) -> Directory {
⋮----
fd: Some(fd),
⋮----
fn set_finished_scanning(&mut self) {
⋮----
fn scanned_and_unlinked(&self) -> bool {
⋮----
struct UnlinkOp {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
f.debug_struct("UnlinkOp")
.field("dir_key", &self.dir_key)
.field("dir_fd", &self.dir_fd)
// Safety: the path is guaranteed to be null terminated
.field("path", &self.path.as_c_str())
.finish()
⋮----
impl UnlinkOp {
fn new(dir_key: usize, dir_fd: RawFd, file_name: OsString) -> UnlinkOp {
⋮----
path: CString::new(file_name.into_vec()).unwrap(),
⋮----
fn entry(&mut self) -> squeue::Entry {
opcode::UnlinkAt::new(types::Fd(self.dir_fd), self.path.as_ptr() as _).build()
⋮----
fn complete(
⋮----
let dir = &mut comp.context_mut().dirs[self.dir_key];
⋮----
// This was the last file to be removed, we can now close the parent
// directory fd.
//
// Safety: the entry doesn't hold any pointers
if let Some(fd) = dir.fd.take() {
comp.push(Op::Close(CloseOp::new(self.dir_key, fd.into_raw_fd())));
⋮----
struct CloseOp {
⋮----
impl CloseOp {
fn new(dir_key: usize, fd: RawFd) -> Self {
⋮----
opcode::Close::new(types::Fd(self.fd)).build()
⋮----
let _ = comp.context_mut().dirs.remove(self.dir_key);
⋮----
enum Op {
⋮----
Op::Unlink(op) => op.entry(),
Op::Close(op) => op.entry(),
⋮----
Op::Unlink(op) => op.complete(comp, res),
Op::Close(op) => op.complete(comp, res),

================
File: fs/src/io_uring/file_creator.rs
================
type BacklogVec = SmallVec<[PendingWrite; 8 * 1024 * 1024 / DEFAULT_WRITE_SIZE]>;
⋮----
const CHECK_PROGRESS_AFTER_SUBMIT_TIMEOUT: Option<Duration> = Some(Duration::from_millis(10));
pub struct IoUringFileCreator<'a, B = LargeBuffer> {
⋮----
pub fn with_buffer_capacity<F: FnMut(PathBuf) + 'a>(
⋮----
/// Create a new `IoUringFileCreator` using provided `buffer` and `file_complete`
    /// to notify caller when file contents are already persisted.
⋮----
/// to notify caller when file contents are already persisted.
    ///
⋮----
///
    /// `buffer` is the internal buffer used for writing scheduled file contents.
⋮----
/// `buffer` is the internal buffer used for writing scheduled file contents.
    /// It must be at least `write_capacity` long. The creator will execute multiple
⋮----
/// It must be at least `write_capacity` long. The creator will execute multiple
    /// `write_capacity` sized writes in parallel to empty the work queue of files to create.
⋮----
/// `write_capacity` sized writes in parallel to empty the work queue of files to create.
    pub fn with_buffer<F: FnMut(PathBuf) + 'a>(
⋮----
pub fn with_buffer<F: FnMut(PathBuf) + 'a>(
⋮----
let ring_qsize = (buffer.as_mut().len() / write_capacity / 2).max(1) as u32;
let ring = IoUring::builder().build(ring_qsize)?;
ring.submitter()
.register_iowq_max_workers(&mut [MAX_IOWQ_WORKERS, 1])?;
⋮----
fn with_buffer_and_ring<F: FnMut(PathBuf) + 'a>(
⋮----
let buffer = backing_buffer.as_mut();
// Take prefix of buffer that is aligned to write_capacity
assert!(buffer.len() >= write_capacity);
let write_aligned_buf_len = buffer.len() / write_capacity * write_capacity;
⋮----
// Safety: buffers contain unsafe pointers to `buffer`, but we make sure they are
// dropped before `backing_buffer` is dropped.
⋮----
let state = FileCreatorState::new(buffers.collect(), file_complete);
⋮----
// Safety: kernel holds unsafe pointers to `buffer`, struct field declaration order
// guarantees that the ring is destroyed before `_backing_buffer` is dropped.
⋮----
// Fixed file descriptor slots. OpenAt will update them to valid fds. Length of registered
// slots must match the `state.files` slab whose indices are used as fd slot indices.
let fds = vec![-1; MAX_OPEN_FILES];
ring.register_files(&fds)?;
Ok(Self {
⋮----
impl<B> FileCreator for IoUringFileCreator<'_, B> {
fn schedule_create_at_dir(
⋮----
let file_key = self.open(path, mode, parent_dir_handle)?;
self.write_and_close(contents, file_key)
⋮----
fn file_complete(&mut self, path: PathBuf) {
(self.ring.context_mut().file_complete)(path)
⋮----
fn drain(&mut self) -> io::Result<()> {
let res = self.ring.drain();
self.ring.context().log_stats();
⋮----
/// Schedule opening file at `path` with `mode` permissions.
    ///
⋮----
///
    /// Returns key that can be used for scheduling writes for it.
⋮----
/// Returns key that can be used for scheduling writes for it.
    fn open(&mut self, path: PathBuf, mode: u32, dir_handle: Arc<File>) -> io::Result<usize> {
⋮----
fn open(&mut self, path: PathBuf, mode: u32, dir_handle: Arc<File>) -> io::Result<usize> {
⋮----
let path_cstring = Pin::new(file.path_cstring());
let file_key = self.wait_add_file(file)?;
⋮----
self.ring.push(op)?;
Ok(file_key)
⋮----
fn wait_add_file(&mut self, file: PendingFile) -> io::Result<usize> {
⋮----
self.ring.process_completions()?;
if self.ring.context().files.len() < self.ring.context().files.capacity() {
⋮----
.submit_and_wait(1, CHECK_PROGRESS_AFTER_SUBMIT_TIMEOUT)?;
⋮----
let file_key = self.ring.context_mut().files.insert(file);
⋮----
fn write_and_close(&mut self, mut src: impl Read, file_key: usize) -> io::Result<()> {
⋮----
let buf = self.wait_free_buf()?;
let state = self.ring.context_mut();
let file = state.files.get_mut(file_key).unwrap();
// Safety: the buffer points to the valid memory backed by `self._backing_buffer`.
// It's obtained from the queue of free buffers and is written to exclusively
let mut_slice = unsafe { slice::from_raw_parts_mut(buf.as_mut_ptr(), buf.len()) };
let len = src.read(mut_slice)?;
⋮----
state.buffers.push_front(buf);
if file.is_completed() {
⋮----
.push(FileCreatorOp::Close(CloseOp::new(file_key)))?;
⋮----
self.ring.push(FileCreatorOp::Write(op))?;
⋮----
file.backlog.push((buf, offset, len));
⋮----
Ok(())
⋮----
fn wait_free_buf(&mut self) -> io::Result<FixedIoBuffer> {
⋮----
if let Some(buf) = state.buffers.pop_front() {
return Ok(buf);
⋮----
struct FileCreatorState<'a> {
⋮----
/// Externally provided callback to be called on paths of files that were written
    file_complete: Box<dyn FnMut(PathBuf) + 'a>,
⋮----
fn new(buffers: VecDeque<FixedIoBuffer>, file_complete: impl FnMut(PathBuf) + 'a) -> Self {
⋮----
/// Returns write backlog that needs to be submitted to IO ring
    fn mark_file_opened(&mut self, file_key: usize) -> BacklogVec {
⋮----
fn mark_file_opened(&mut self, file_key: usize) -> BacklogVec {
let file = self.files.get_mut(file_key).unwrap();
⋮----
if self.buffers.len() * 2 > self.buffers.capacity() {
⋮----
/// Returns true if all of the writes are now done
    fn mark_write_completed(
⋮----
fn mark_write_completed(
⋮----
self.buffers.push_front(buf);
⋮----
fn mark_file_closed(&mut self, file_key: usize) {
let _ = self.files.remove(file_key);
⋮----
fn log_stats(&self) {
self.stats.log();
⋮----
struct FileCreatorStats {
/// Count of cases when more than half of buffers are free (files are written
    /// faster than submitted - consider less buffers or speeding up submission)
⋮----
/// faster than submitted - consider less buffers or speeding up submission)
    large_buf_headroom_count: u32,
/// Count of cases when we run out of free buffers (files are not written fast
    /// enough - consider more buffers or tuning write bandwidth / patterns)
⋮----
/// enough - consider more buffers or tuning write bandwidth / patterns)
    no_buf_count: u32,
/// Sum of all outstanding write sizes at moments of encountering no free buf
    no_buf_sum_submitted_write_sizes: usize,
⋮----
impl FileCreatorStats {
fn log(&self) {
⋮----
.checked_div(self.no_buf_count as usize)
.unwrap_or_default();
⋮----
struct OpenOp {
⋮----
impl OpenOp {
fn entry(&mut self) -> squeue::Entry {
let at_dir_fd = types::Fd(self.dir_handle.as_raw_fd());
opcode::OpenAt::new(at_dir_fd, self.path_cstring.as_ptr() as _)
.flags(O_CREAT | O_TRUNC | O_NOFOLLOW | O_WRONLY | O_NOATIME)
.mode(self.mode)
.file_index(Some(
types::DestinationSlot::try_from_slot_target(self.file_key as u32).unwrap(),
⋮----
.build()
⋮----
fn complete(
⋮----
let backlog = ring.context_mut().mark_file_opened(self.file_key);
⋮----
ring.context_mut().submitted_writes_size += len;
ring.push(FileCreatorOp::Write(op));
⋮----
struct CloseOp {
⋮----
impl<'a> CloseOp {
fn new(file_key: usize) -> Self {
⋮----
opcode::Close::new(types::Fixed(self.file_key as u32)).build()
⋮----
ring.context_mut().mark_file_closed(self.file_key);
⋮----
struct WriteOp {
⋮----
impl<'a> WriteOp {
⋮----
unsafe { buf.as_mut_ptr().byte_add(*buf_offset) },
⋮----
buf.io_buf_index()
.expect("should have a valid fixed buffer"),
⋮----
.offset(*offset as u64)
.ioprio(IO_PRIO_BE_HIGHEST)
⋮----
// Fail fast if no progress. FS should report an error (e.g. `StorageFull`) if the
// condition isn't transient, but it's hard to verify without extra tracking.
Ok(0) => return Err(io::ErrorKind::WriteZero.into()),
⋮----
Err(err) => return Err(err),
⋮----
ring.push(FileCreatorOp::Write(WriteOp {
⋮----
return Ok(());
⋮----
.context_mut()
.mark_write_completed(*file_key, total_written, buf)
⋮----
ring.push(FileCreatorOp::Close(CloseOp::new(*file_key)));
⋮----
enum FileCreatorOp {
⋮----
Self::Open(op) => op.entry(),
Self::Close(op) => op.entry(),
Self::Write(op) => op.entry(),
⋮----
Self::Open(op) => op.complete(ring, res),
Self::Close(op) => op.complete(ring, res),
Self::Write(op) => op.complete(ring, res),
⋮----
type PendingWrite = (FixedIoBuffer, usize, usize);
⋮----
struct PendingFile {
⋮----
impl PendingFile {
fn from_path(path: PathBuf) -> Self {
⋮----
fn path_cstring(&self) -> CString {
let os_str = self.path.file_name().expect("path must contain filename");
CString::new(os_str.as_encoded_bytes()).expect("path mustn't contain interior NULs")
⋮----
fn is_completed(&self) -> bool {

================
File: fs/src/io_uring/memory.rs
================
pub enum LargeBuffer {
⋮----
impl Deref for LargeBuffer {
type Target = [u8];
fn deref(&self) -> &Self::Target {
⋮----
Self::Vec(buf) => buf.as_slice(),
Self::HugeTable(mem) => mem.deref(),
⋮----
impl DerefMut for LargeBuffer {
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
Self::Vec(buf) => buf.as_mut_slice(),
Self::HugeTable(mem) => mem.deref_mut(),
⋮----
fn as_mut(&mut self) -> &mut [u8] {
⋮----
Self::Vec(vec) => vec.as_mut_slice(),
⋮----
impl LargeBuffer {
pub fn new(size: usize) -> Self {
⋮----
let size = size.next_power_of_two();
⋮----
Self::Vec(vec![0; size])
⋮----
struct AllocError;
pub struct PageAlignedMemory {
⋮----
impl PageAlignedMemory {
fn alloc_huge_table(memory_size: usize) -> Result<Self, AllocError> {
⋮----
debug_assert!(memory_size.is_power_of_two());
debug_assert!(page_size.is_power_of_two());
let aligned_size = memory_size.next_multiple_of(page_size);
⋮----
return Err(AllocError);
⋮----
Ok(Self {
ptr: NonNull::new(ptr as *mut u8).ok_or(AllocError)?,
⋮----
fn page_size() -> usize {
⋮----
impl Drop for PageAlignedMemory {
fn drop(&mut self) {
⋮----
libc::munmap(self.ptr.as_ptr() as *mut libc::c_void, self.len);
⋮----
impl Deref for PageAlignedMemory {
⋮----
unsafe { slice::from_raw_parts(self.ptr.as_ptr(), self.len) }
⋮----
impl DerefMut for PageAlignedMemory {
⋮----
unsafe { slice::from_raw_parts_mut(self.ptr.as_ptr(), self.len) }
⋮----
pub(super) struct FixedIoBuffer {
⋮----
impl FixedIoBuffer {
pub const fn empty() -> Self {
⋮----
pub unsafe fn split_buffer_chunks(
⋮----
assert!(
⋮----
let buf_start = buffer.as_ptr() as usize;
buffer.chunks_exact_mut(chunk_size).map(move |buf| {
let io_buf_index = (buf.as_ptr() as usize - buf_start) / FIXED_BUFFER_LEN;
⋮----
ptr: buf.as_mut_ptr(),
size: buf.len(),
io_buf_index: Some(io_buf_index as u16),
⋮----
pub fn len(&self) -> usize {
⋮----
pub unsafe fn as_mut_ptr(&self) -> *mut u8 {
⋮----
pub fn io_buf_index(&self) -> Option<u16> {
⋮----
pub fn into_shrinked(self, size: usize) -> Self {
assert!(size <= self.size);
⋮----
pub unsafe fn register<S, E: RingOp<S>>(
⋮----
.chunks(FIXED_BUFFER_LEN)
.map(|buf| libc::iovec {
iov_base: buf.as_ptr() as _,
iov_len: buf.len(),
⋮----
unsafe { ring.register_buffers(&iovecs) }
⋮----
fn as_ref(&self) -> &[u8] {

================
File: fs/src/io_uring/mod.rs
================
pub mod dir_remover;
pub mod file_creator;
pub mod memory;
pub mod sequential_file_reader;

================
File: fs/src/io_uring/sequential_file_reader.rs
================
pub struct SequentialFileReader<B> {
⋮----
pub fn with_capacity(buf_size: usize, path: impl AsRef<Path>) -> io::Result<Self> {
⋮----
struct SequentialFileReaderState {
⋮----
pub fn with_buffer(
⋮----
let buf_capacity = buffer.as_mut().len();
⋮----
let ring_squeue_size = (max_inflight_ops / 2).max(1);
⋮----
.setup_cqsize(max_inflight_ops)
.build(ring_squeue_size)?;
ring.submitter()
.register_iowq_max_workers(&mut [MAX_IOWQ_WORKERS, 1])?;
⋮----
fn with_buffer_and_ring(
⋮----
let buffer = backing_buffer.as_mut();
assert!(buffer.len() >= read_capacity, "buffer too small");
let read_aligned_buf_len = buffer.len() / read_capacity * read_capacity;
⋮----
.read(true)
.custom_flags(libc::O_NOATIME)
.open(path)?;
⋮----
.map(ReadBufState::Uninit)
.collect();
⋮----
for i in 0..reader.inner.context().buffers.len() {
reader.start_reading_buf(i)?;
⋮----
reader.inner.submit()?;
Ok(reader)
⋮----
fn start_reading_buf(&mut self, index: usize) -> io::Result<()> {
⋮----
} = &mut self.inner.context_mut();
⋮----
fd: file.as_raw_fd(),
⋮----
self.inner.push(op)?;
⋮----
_ => unreachable!("called start_reading_buf on a non-empty buffer"),
⋮----
Ok(())
⋮----
impl<B: AsMut<[u8]>> Read for SequentialFileReader<B> {
fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {
let available = self.fill_buf()?;
if available.is_empty() {
return Ok(0);
⋮----
let bytes_to_read = available.len().min(buf.len());
buf[..bytes_to_read].copy_from_slice(&available[..bytes_to_read]);
self.consume(bytes_to_read);
Ok(bytes_to_read)
⋮----
impl<B: AsMut<[u8]>> BufRead for SequentialFileReader<B> {
fn fill_buf(&mut self) -> io::Result<&[u8]> {
⋮----
let state = self.inner.context_mut();
let num_buffers = state.buffers.len();
⋮----
if !cursor.fill_buf()?.is_empty() {
⋮----
return Ok(&[]);
⋮----
let buf = cursor.into_inner();
debug_assert!(buf.len() == state.read_capacity);
⋮----
self.start_reading_buf(index)?;
⋮----
ReadBufState::Uninit(_) => unreachable!("should be initialized"),
⋮----
self.inner.process_completions()?;
let state = self.inner.context();
⋮----
ReadBufState::Reading => self.inner.submit()?,
⋮----
ReadBufState::Full(cursor) => Ok(cursor.fill_buf()?),
_ => unreachable!(),
⋮----
fn consume(&mut self, amt: usize) {
⋮----
ReadBufState::Full(cursor) => cursor.consume(amt),
_ => assert_eq!(amt, 0),
⋮----
enum ReadBufState {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
⋮----
.debug_struct("Uninit")
.field("io_buf_index", &buf.io_buf_index())
.finish(),
Self::Reading => write!(f, "Reading"),
⋮----
.debug_struct("Full")
.field("io_buf_index", &cursor.get_ref().io_buf_index())
⋮----
struct ReadOp {
⋮----
/// This is the offset inside the buffer. It's typically 0, but can be non-zero if a previous
    buf_off: usize,
⋮----
f.debug_struct("ReadOp")
.field("fd", &self.fd)
.field("buf_off", &self.buf_off)
.field("io_buf_index", &self.buf.io_buf_index())
.field("file_off", &self.file_off)
.field("read_len", &self.read_len)
.field("reader_buf_index", &self.reader_buf_index)
.finish()
⋮----
fn entry(&mut self) -> squeue::Entry {
⋮----
debug_assert!(*buf_off + *read_len <= buf.len());
⋮----
unsafe { buf.as_mut_ptr().byte_add(*buf_off) },
⋮----
buf.io_buf_index()
.expect("should have a valid fixed buffer"),
⋮----
.offset(*file_off as u64)
.ioprio(IO_PRIO_BE_HIGHEST)
.build()
.flags(squeue::Flags::ASYNC)
⋮----
fn complete(
⋮----
let reader_state = completion.context_mut();
⋮----
reader_state.eof_buf_index = Some(*reader_buf_index);
⋮----
completion.push(op);
⋮----
ReadBufState::Full(Cursor::new(buf.into_shrinked(total_read_len)));
⋮----
mod tests {
⋮----
fn check_reading_file(file_size: usize, backing_buffer_size: usize, read_capacity: usize) {
let pattern: Vec<u8> = (0..251).collect();
let mut temp_file = NamedTempFile::new().unwrap();
for _ in 0..file_size / pattern.len() {
io::Write::write_all(&mut temp_file, &pattern).unwrap();
⋮----
io::Write::write_all(&mut temp_file, &pattern[..file_size % pattern.len()]).unwrap();
let buf = vec![0; backing_buffer_size];
⋮----
SequentialFileReader::with_buffer(temp_file.path(), buf, read_capacity).unwrap();
⋮----
reader.read_to_end(&mut all_read_data).unwrap();
assert_eq!(all_read_data.len(), file_size);
for (i, byte) in all_read_data.iter().enumerate() {
assert_eq!(*byte, pattern[i % pattern.len()], "Mismatch - pos {i}");
⋮----
fn test_reading_small_file() {
check_reading_file(2500, 4096, 1024);
check_reading_file(2500, 4096, 2048);
check_reading_file(2500, 4096, 4096);
⋮----
fn test_reading_file_in_chunks() {
check_reading_file(25_000, 16384, 1024);
check_reading_file(25_000, 4096, 1024);
check_reading_file(25_000, 4096, 2048);
check_reading_file(25_000, 4096, 4096);
⋮----
fn test_reading_large_file() {
check_reading_file(250_000, 32768, 1024);
check_reading_file(250_000, 16384, 1024);
check_reading_file(250_000, 4096, 1024);
check_reading_file(250_000, 4096, 2048);
check_reading_file(250_000, 4096, 4096);

================
File: fs/src/buffered_reader.rs
================
struct Stack<const N: usize>([MaybeUninit<u8>; N]);
⋮----
const fn new() -> Self {
Self([MaybeUninit::uninit(); N])
⋮----
fn capacity(&self) -> usize {
⋮----
unsafe fn as_slice(&self) -> &[u8] {
unsafe { slice::from_raw_parts(self.0.as_ptr() as *const u8, N) }
⋮----
unsafe fn as_mut_slice(&mut self) -> &mut [u8] {
unsafe { slice::from_raw_parts_mut(self.0.as_mut_ptr() as *mut u8, N) }
⋮----
pub trait FileBufRead<'a>: BufRead {
/// Activate the given `file` as source of reads of this reader.
    ///
⋮----
///
    /// Resets the internal buffer to an empty state and sets the file offset to 0.
⋮----
/// Resets the internal buffer to an empty state and sets the file offset to 0.
    ///
⋮----
///
    /// `read_limit` provides a pre-defined limit on the number of bytes that can be read
⋮----
/// `read_limit` provides a pre-defined limit on the number of bytes that can be read
    /// from the file (unless EOF is reached).
⋮----
/// from the file (unless EOF is reached).
    fn set_file(&mut self, file: &'a File, read_limit: usize) -> io::Result<()>;
⋮----
pub trait RequiredLenBufRead: BufRead {
⋮----
pub trait RequiredLenBufFileRead<'a>: RequiredLenBufRead + FileBufRead<'a> {}
⋮----
/// read a file a large buffer at a time and provide access to a slice in that buffer
pub struct BufferedReader<'a, const N: usize> {
⋮----
pub struct BufferedReader<'a, const N: usize> {
⋮----
pub const fn new() -> Self {
⋮----
pub fn with_file(mut self, file: &'a File, read_limit: usize) -> Self {
self.do_set_file(file, read_limit);
⋮----
fn do_set_file(&mut self, file: &'a File, read_limit: usize) {
self.file = Some(file);
⋮----
fn set_file(&mut self, file: &'a File, read_limit: usize) -> io::Result<()> {
⋮----
Ok(())
⋮----
fn get_file_offset(&self) -> usize {
if self.buf_valid_bytes.is_empty() {
⋮----
fn read_more_bytes(&mut self) -> io::Result<()> {
debug_assert!(self.buf_valid_bytes.len() <= self.file_offset_of_next_read);
self.file_last_offset = self.file_offset_of_next_read - self.buf_valid_bytes.len();
⋮----
return Err(io::Error::new(io::ErrorKind::BrokenPipe, "no open file"));
⋮----
read_more_buffer(
⋮----
unsafe { self.buf.as_mut_slice() },
⋮----
fn valid_slice(&self) -> &[u8] {
unsafe { &self.buf.as_slice()[self.buf_valid_bytes.clone()] }
⋮----
fn read(&mut self, mut buf: &mut [u8]) -> io::Result<usize> {
let available_len = self.buf_valid_bytes.len();
⋮----
// Copy already read data to buf.
let available_valid_data = self.valid_slice();
if available_len >= buf.len() {
buf.copy_from_slice(&available_valid_data[..buf.len()]);
self.consume(buf.len());
return Ok(buf.len());
⋮----
// Only part of the buffer can be filled.
buf[..available_len].copy_from_slice(available_valid_data);
⋮----
// Read directly from file into space still left in the buf.
⋮----
let bytes_read = read_into_buffer(
⋮----
// Buffer was successfully filled, drop buffered data and move offset.
self.consume(filled_len);
Ok(filled_len)
⋮----
/// `BufferedReader` implements a more permissive API compared to `BufRead`
/// by allowing `consume` to advance beyond the end of the buffer returned by `fill_buf`.
⋮----
/// by allowing `consume` to advance beyond the end of the buffer returned by `fill_buf`.
impl<const N: usize> BufRead for BufferedReader<'_, N> {
⋮----
impl<const N: usize> BufRead for BufferedReader<'_, N> {
fn fill_buf(&mut self) -> io::Result<&[u8]> {
⋮----
self.read_more_bytes()?;
⋮----
Ok(self.valid_slice())
⋮----
fn consume(&mut self, amt: usize) {
if self.buf_valid_bytes.len() >= amt {
⋮----
let additional_amount_to_skip = amt - self.buf_valid_bytes.len();
⋮----
impl<const N: usize> RequiredLenBufRead for BufferedReader<'_, N> {
fn fill_buf_required(&mut self, required_len: usize) -> io::Result<&[u8]> {
if self.buf_valid_bytes.len() < required_len {
⋮----
if required_len > self.buf.capacity() {
return Err(io::Error::new(
⋮----
/// A buffered reader that wraps `BufRead` instance and implements `RequiredLenBufRead`.
///
⋮----
///
/// It uses auxiliary overflow buffer when `fill_buf` returns slice that doesn't satisfy
⋮----
/// It uses auxiliary overflow buffer when `fill_buf` returns slice that doesn't satisfy
pub struct BufReaderWithOverflow<R> {
⋮----
pub struct BufReaderWithOverflow<R> {
⋮----
pub fn new(reader: R, overflow_min_capacity: usize, overflow_max_capacity: usize) -> Self {
⋮----
fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {
let available_len = self.overflow_buf.len();
⋮----
self.reader.read(buf)
⋮----
assert!(
⋮----
buf[..available_len].copy_from_slice(&self.overflow_buf);
self.overflow_buf.clear();
if buf.len() > available_len {
let bytes_read = self.reader.read(&mut buf[available_len..])?;
Ok(available_len + bytes_read)
⋮----
Ok(available_len)
⋮----
impl<R: BufRead> BufRead for BufReaderWithOverflow<R> {
⋮----
if self.overflow_buf.is_empty() {
self.reader.fill_buf()
⋮----
Ok(self.overflow_buf.as_slice())
⋮----
fn consume(&mut self, mut amt: usize) {
let overflow_len = self.overflow_buf.len();
⋮----
.checked_sub(overflow_len)
.expect("should consume all previously required bytes");
⋮----
self.reader.consume(amt);
⋮----
self.reader.set_file(file, read_limit)
⋮----
self.reader.get_file_offset() - self.overflow_buf.len()
⋮----
impl<R: BufRead> RequiredLenBufRead for BufReaderWithOverflow<R> {
⋮----
let buf = self.reader.fill_buf()?;
if buf.len() >= required_len {
return self.reader.fill_buf();
⋮----
if required_len > self.overflow_buf.capacity() {
⋮----
.next_power_of_two()
.clamp(self.overflow_min_capacity, self.overflow_max_capacity);
⋮----
.reserve_exact(target_capacity - available_len);
⋮----
unsafe { self.overflow_buf.set_len(required_len) };
⋮----
.read_exact(&mut self.overflow_buf[available_len..])
.inspect_err(|_| self.overflow_buf.clear())?;
⋮----
pub fn large_file_buf_reader(path: &Path, buf_size: usize) -> io::Result<impl BufRead + use<>> {
⋮----
assert!(agave_io_uring::io_uring_supported());
⋮----
let buf_size = buf_size.max(DEFAULT_READ_SIZE);
⋮----
use std::io::BufReader;
⋮----
Ok(BufReader::with_capacity(buf_size, file))
⋮----
mod tests {
⋮----
fn rand_bytes<const N: usize>() -> [u8; N] {
use rand::Rng;
⋮----
fn test_buffered_reader() {
⋮----
let mut sample_file = tempfile().unwrap();
⋮----
sample_file.write_all(&bytes).unwrap();
⋮----
BufferedReader::<BUFFER_SIZE>::new().with_file(&sample_file, file_len_valid);
let offset = reader.get_file_offset();
let slice = reader.fill_buf_required(default_min_read).unwrap();
⋮----
assert_eq!(offset, expected_offset);
assert_eq!(slice.len(), BUFFER_SIZE);
assert_eq!(slice, &bytes[0..BUFFER_SIZE]);
⋮----
reader.consume(advance);
⋮----
assert_eq!(
⋮----
let slice = reader.fill_buf_required(required_len).unwrap();
⋮----
assert_eq!(slice.len(), expected_slice_len);
⋮----
fn test_buffered_reader_with_extra_data_in_file() {
⋮----
let mut reader = BufferedReader::<BUFFER_SIZE>::new().with_file(&sample_file, valid_len);
⋮----
let slice = reader.fill_buf_required(default_min_read_size).unwrap();
⋮----
fn test_buffered_reader_partial_consume() {
⋮----
assert_eq!(slice.len(), required_len);
⋮----
fn test_buffered_reader_partial_consume_with_move() {
⋮----
let slice = reader.fill_buf_required(required_data_len).unwrap();
⋮----
assert_eq!(slice.len(), required_data_len);
⋮----
fn test_fill_buf_required_or_overflow() {
⋮----
BufferedReader::<BUFFER_SIZE>::new().with_file(&sample_file, FILE_SIZE),
⋮----
assert_eq!(&slice[..required_len], &bytes[..required_len]);
reader.consume(required_len);
⋮----
assert_eq!(slice, &bytes[8..8 + required_len]);
⋮----
let result = reader.fill_buf_required(required_len);
assert_eq!(result.unwrap_err().kind(), io::ErrorKind::UnexpectedEof);
⋮----
let offset_before = reader.get_file_offset();
⋮----
assert_eq!(slice.len(), 0);
let offset_after = reader.get_file_offset();
assert_eq!(offset_before, offset_after);
⋮----
fn test_overflow_reader_read_and_fill_buf() {
⋮----
let buf = reader.fill_buf().unwrap();
assert_eq!(buf, &bytes[0..BUFFER_SIZE]);
reader.consume(8);
⋮----
assert_eq!(reader.read(&mut buf).unwrap(), 8);
assert_eq!(buf, &bytes[8..BUFFER_SIZE]);
⋮----
let buf = reader.fill_buf_required(32).unwrap();
assert_eq!(buf, &bytes[BUFFER_SIZE..BUFFER_SIZE + 32]);
⋮----
assert_eq!(reader.read(&mut buf).unwrap(), 48);
assert_eq!(buf, &bytes[BUFFER_SIZE..BUFFER_SIZE + 48]);
assert_eq!(reader.read(&mut buf).unwrap(), 0);

================
File: fs/src/dirs.rs
================
pub fn remove_dir_all(path: impl Into<PathBuf> + AsRef<Path>) -> io::Result<()> {
⋮----
assert!(io_uring_supported());
⋮----
return remover.remove_dir_all(path);
⋮----
pub fn remove_dir_contents(path: impl AsRef<Path>) {
let path = path.as_ref();
⋮----
if let Err(e) = remover.remove_dir_contents(path) {
⋮----
remove_dir_contents_slow(path)
⋮----
fn remove_dir_contents_slow(path: impl AsRef<Path>) {
⋮----
for entry in dir_entries.flatten() {
let sub_path = entry.path();
let result = if sub_path.is_dir() {

================
File: fs/src/file_io.rs
================
pub fn read_more_buffer(
⋮----
buffer.copy_within(valid_bytes.clone(), 0);
let bytes_read = read_into_buffer(
⋮----
&mut buffer[valid_bytes.len()..],
⋮----
*valid_bytes = 0..(valid_bytes.len() + bytes_read);
Ok(())
⋮----
fn arch_read_at(file: &File, buffer: &mut [u8], offset: u64) -> std::io::Result<usize> {
use std::os::unix::prelude::FileExt;
file.read_at(buffer, offset)
⋮----
use std::os::windows::fs::FileExt;
file.seek_read(buffer, offset)
⋮----
fn arch_write_at(file: &File, buffer: &[u8], offset: u64) -> io::Result<usize> {
⋮----
file.write_at(buffer, offset)
⋮----
file.seek_write(buffer, offset)
⋮----
pub fn write_buffer_to_file(file: &File, mut buffer: &[u8], mut offset: u64) -> io::Result<()> {
while !buffer.is_empty() {
let wrote_len = arch_write_at(file, buffer, offset)?;
⋮----
return Err(io::ErrorKind::WriteZero.into());
⋮----
pub fn read_into_buffer(
⋮----
return Ok(0);
⋮----
while buffer_offset < buffer.len() {
match arch_read_at(file, &mut buffer[buffer_offset..], offset as u64) {
⋮----
if err.kind() == std::io::ErrorKind::Interrupted {
⋮----
return Err(err);
⋮----
Ok(total_bytes_read)
⋮----
pub trait FileCreator {
⋮----
pub fn file_creator<'a>(
⋮----
return Ok(Box::new(io_uring_creator));
⋮----
Ok(Box::new(SyncIoFileCreator::new(buf_size, file_complete)))
⋮----
pub struct SyncIoFileCreator<'a> {
⋮----
fn new(_buf_size: usize, file_complete: impl FnMut(PathBuf) + 'a) -> Self {
⋮----
pub fn set_path_permissions(path: &Path, mode: u32) -> io::Result<()> {
⋮----
use std::os::unix::fs::PermissionsExt;
⋮----
let mut current_perm = fs::metadata(path)?.permissions();
current_perm.set_readonly(mode & 0o200 == 0);
⋮----
impl FileCreator for SyncIoFileCreator<'_> {
fn schedule_create_at_dir(
⋮----
options.create(true).truncate(true).write(true);
⋮----
let mut file_buf = BufWriter::new(options.open(&path)?);
⋮----
file_buf.flush()?;
⋮----
set_path_permissions(&path, mode)?;
self.file_complete(path);
⋮----
fn file_complete(&mut self, path: PathBuf) {
⋮----
fn drain(&mut self) -> io::Result<()> {
⋮----
mod tests {
⋮----
fn test_read_into_buffer() {
let mut sample_file = tempfile().unwrap();
⋮----
let bytes: Vec<u8> = (0..file_size as u8).collect();
sample_file.write_all(&bytes).unwrap();
⋮----
let mut buffer_len = buffer.len();
⋮----
read_into_buffer(&sample_file, valid_len, start_offset, &mut buffer).unwrap();
assert_eq!(num_bytes_read, buffer_len);
assert_eq!(bytes, buffer);
⋮----
buffer_len = buffer.len();
⋮----
assert_eq!(num_bytes_read, valid_len);
assert_eq!(bytes, buffer[0..valid_len]);
assert_eq!(buffer[valid_len..buffer_len], [0; 32]);
⋮----
assert_eq!(bytes[0..valid_len], buffer[0..valid_len]);
assert_eq!(buffer[valid_len..buffer_len], bytes[valid_len..buffer_len]);
⋮----
assert_eq!(num_bytes_read, valid_len - start_offset);
assert_eq!(buffer[0..num_bytes_read], bytes[start_offset..buffer_len]);
assert_eq!(buffer[num_bytes_read..buffer_len], [0; 8])
⋮----
fn test_read_more_buffer() {
⋮----
let buffer_len = buffer.len();
⋮----
let mut valid_bytes_len = valid_bytes.len();
⋮----
read_more_buffer(
⋮----
.unwrap();
assert_eq!(offset, buffer_len - valid_bytes_len);
assert_eq!(valid_bytes, 0..buffer_len);
assert_eq!(buffer[0..valid_bytes_len], [0xFFu8; 8]);
assert_eq!(
⋮----
valid_bytes_len = valid_bytes.len();
⋮----
assert_eq!(offset, file_size);
assert_eq!(valid_bytes, 0..24);
⋮----
fn read_file_to_string(path: &PathBuf) -> String {
String::from_utf8(fs::read(path).expect("Failed to read file"))
.expect("Failed to decode file contents")
⋮----
fn test_create_writes_contents() -> io::Result<()> {
⋮----
let file_path = temp_dir.path().join("test.txt");
⋮----
let mut creator = file_creator(2 << 20, |path| {
callback_invoked_path.replace(path);
⋮----
let dir = Arc::new(File::open(temp_dir.path())?);
creator.schedule_create_at_dir(
file_path.clone(),
⋮----
creator.drain()?;
drop(creator);
assert_eq!(read_file_to_string(&file_path), contents);
assert_eq!(callback_invoked_path, Some(file_path));
⋮----
fn test_multiple_file_creations() -> io::Result<()> {
⋮----
let mut creator = file_creator(2 << 20, |path: PathBuf| {
let contents = read_file_to_string(&path);
assert!(contents.starts_with("File "));
⋮----
let file_path = temp_dir.path().join(format!("file_{i}.txt"));
let data = format!("File {i}");
⋮----
dir.clone(),
⋮----
assert_eq!(callback_counter, 5);

================
File: fs/src/lib.rs
================
pub mod buffered_reader;
pub mod dirs;
pub mod file_io;
mod io_uring;

================
File: fs/Cargo.toml
================
[package]
name = "agave-fs"
description = "Agave file system utils"
documentation = "https://docs.rs/agave-fs"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "agave_fs"

[features]
agave-unstable-api = []

[dependencies]
libc = { workspace = true }
log = { workspace = true }
slab = { workspace = true }
smallvec = { workspace = true }

[target.'cfg(target_os = "linux")'.dependencies]
agave-io-uring = { workspace = true }
io-uring = { workspace = true }

[dev-dependencies]
rand = { workspace = true }
tempfile = { workspace = true }
test-case = { workspace = true }

================
File: genesis/src/address_generator.rs
================
use solana_pubkey::Pubkey;
⋮----
pub struct AddressGenerator {
⋮----
impl AddressGenerator {
pub fn new(base_pubkey: &Pubkey, program_id: &Pubkey) -> Self {
⋮----
pub fn nth(&self, nth: usize) -> Pubkey {
Pubkey::create_with_seed(&self.base_pubkey, &format!("{nth}"), &self.program_id).unwrap()
⋮----
pub fn next(&mut self) -> Pubkey {
⋮----
self.nth(nth)

================
File: genesis/src/genesis_accounts.rs
================
withdrawer: Some("59SLqk4ete5QttM1WmjfMA7uNJnJVFLQqXJSy9rvuj7c"),
⋮----
withdrawer: Some("ERnx3Csgu3LjrGGrCeCUZzuHguRu6XabT1kufSB1NDWi"),
⋮----
withdrawer: Some("5eKcGy7ZCPJdQSQGVnfmT7kGz6MKPMKaNaMEYJbmwhuT"),
⋮----
withdrawer: Some("2pKqwFKfKj2nGrknPNDSP8vXGYrgAjd28fT6yLew8sT3"),
⋮----
withdrawer: Some("Hw3sP6PreBtFCnwXbNvUypMhty62GXibjfiZ1zHBXFk6"),
⋮----
withdrawer: Some("9j3WzBSZRHrD2DbzFTUVVi81QX6boVvUTpGWcSiMwD5W"),
⋮----
withdrawer: Some("EJyZGbQ1PmpcWxfqGME6SUNHfurh1zggDqCT7rV9xLzL"),
⋮----
withdrawer: Some("JBGnGdLyo7V2z9hz51mnnbyDp9sBACtw5WYH9YRG8n7e"),
⋮----
withdrawer: Some("43XAfG3AFiF1ockdh7xp91fpFyZkbWSZq9ZFBCGUVV41"),
⋮----
withdrawer: Some("7s2GVwFo8VSrCwX9Tztt42ueiEaUtJ6zCEHU8XGvuf5E"),
⋮----
withdrawer: Some("23PJYLS1WFLqhXnXq2Hobc17DbvZaoinoTZYLyGRT8E2"),
⋮----
withdrawer: Some("6bFjx3egMjVsGKFb445564a4bwgibwbUB2tVFsJcdPv7"),
⋮----
withdrawer: Some("jXMEkVQQpoqebVMGN7DfpvdRLwJDEkoVNrwPVphNm7i"),
⋮----
withdrawer: Some("BxmwgfnyAqZnqRCJGdsEea35pcc92GFTcyGeSj4RNfJJ"),
⋮----
withdrawer: Some("Aj3K933zdRQhYEJi2Yjz8hJWXN3Z3hrKJQtPtE8VmUnq"),
⋮----
withdrawer: Some("HWzeqw1Yk5uiLgT2uGUim5ocFJNCwYUFbeCtDVpx9yUb"),
⋮----
withdrawer: Some("6mudxxoe5VyXXNXsJ3NSGSTGESfG2t86PBCQGbouHpXX"),
⋮----
withdrawer: Some("ASJpWZAxY96kbciLqzb7sg45gsH32yPzGcxjn7HPcARn"),
⋮----
withdrawer: Some("9oaCkokBBhgBsgyg4sL7fMJyQseaJb1TbADZeoPdpWdc"),
⋮----
withdrawer: Some("4YnNnycEZXCkuVs2hDthdNxMD4E8wc7ZPgyAK7Lm1uZc"),
⋮----
withdrawer: Some("C7WS9ic7KN9XNcLsNoMvzTvbzURM3rFGDEQN7qJMWNLn"),
⋮----
withdrawer: Some("FdGYQdiRky8NZzN9wZtczTBcWLYYRXrJ3LMDhqDPn5rM"),
⋮----
withdrawer: Some("EDwSQShtUWQtmFfN9SpUUd6hgonL7tRdxngAsNKv9Pe6"),
⋮----
withdrawer: Some("9BgvWHerNACjnx6ZpK51k2LEsnwBP3gFwWDzhKkHKH1m"),
⋮----
withdrawer: Some("8CUUMKYNGxdgYio5CLHRHyzMEhhVRMcqefgE6dLqnVRK"),
⋮----
withdrawer: Some("92viKFftk1dJjqJwreFqT2qHXxjSUuEE9VyHvTdY1mpY"),
⋮----
withdrawer: Some("7kgfDmgbEfypBujqn4tyApjf8H7ZWuaL3F6Ah9vQHzgR"),
⋮----
fn add_stakes(
⋮----
.iter()
.map(|staker_info| create_and_add_stakes(genesis_config, staker_info, unlock_info, None))
⋮----
pub fn add_genesis_stake_accounts(genesis_config: &mut GenesisConfig, mut issued_lamports: u64) {
⋮----
issued_lamports += add_stakes(
⋮----
) + add_stakes(
⋮----
) + add_stakes(genesis_config, GRANTS_STAKER_INFOS, &UNLOCKS_ALL_DAY_ZERO)
+ add_stakes(
⋮----
create_and_add_stakes(
⋮----
lamports: (500_000_000 * LAMPORTS_PER_SOL).saturating_sub(issued_lamports),
withdrawer: Some("3FFaheyqtyAXZSYxDzsr5CVKvJuvZD1WE1VEsBtDbRqB"),
⋮----
mod tests {
⋮----
fn test_add_genesis_stake_accounts() {
⋮----
for (cluster_type, expected_lamports) in clusters_and_expected_lamports.iter() {
⋮----
add_genesis_stake_accounts(&mut genesis_config, 0);
⋮----
.values()
.map(|account| account.lamports)
⋮----
assert_eq!(*expected_lamports, lamports);

================
File: genesis/src/lib.rs
================
pub mod address_generator;
pub mod genesis_accounts;
pub mod stakes;
pub mod unlocks;
⋮----
pub struct Base64Account {
⋮----
pub struct ValidatorAccountsFile {
⋮----
pub struct StakedValidatorAccountInfo {

================
File: genesis/src/main.rs
================
pub enum AccountFileFormat {
⋮----
fn pubkey_from_str(key_str: &str) -> Result<Pubkey, Box<dyn error::Error>> {
Pubkey::from_str(key_str).or_else(|_| {
⋮----
Keypair::try_from(bytes.as_ref()).map_err(|e| std::io::Error::other(e.to_string()))?;
Ok(keypair.pubkey())
⋮----
pub fn load_genesis_accounts(file: &str, genesis_config: &mut GenesisConfig) -> io::Result<u64> {
⋮----
.map_err(|err| io::Error::other(format!("{err:?}")))?;
⋮----
let pubkey = pubkey_from_str(key.as_str())
.map_err(|err| io::Error::other(format!("Invalid pubkey/keypair {key}: {err:?}")))?;
let owner_program_id = Pubkey::from_str(account_details.owner.as_str()).map_err(|err| {
io::Error::other(format!(
⋮----
account.set_data_from_slice(
⋮----
.decode(account_details.data.as_str())
.map_err(|err| {
⋮----
account.set_executable(account_details.executable);
lamports += account.lamports();
genesis_config.add_account(pubkey, account);
⋮----
Ok(lamports)
⋮----
pub fn load_validator_accounts(
⋮----
.map_err(|err| io::Error::other(format!("{err:?}")))?
⋮----
pubkey_from_str(account_details.identity_account.as_str()).map_err(|err| {
⋮----
pubkey_from_str(account_details.vote_account.as_str()).map_err(|err| {
⋮----
pubkey_from_str(account_details.stake_account.as_str()).map_err(|err| {
⋮----
let bls_pubkeys: Vec<BLSPubkey> = account_details.bls_pubkey.map_or(Ok(vec![]), |s| {
⋮----
.map(|pk| vec![pk])
.map_err(|err| io::Error::other(format!("Invalid BLS pubkey: {err}")))
⋮----
add_validator_accounts(
⋮----
&mut pubkeys.iter(),
&mut bls_pubkeys.iter(),
⋮----
Ok(())
⋮----
fn check_rpc_genesis_hash(
⋮----
if let Some(genesis_hash) = cluster_type.get_genesis_hash() {
let rpc_genesis_hash = rpc_client.get_genesis_hash()?;
⋮----
return Err(format!(
⋮----
.into());
⋮----
fn features_to_deactivate_for_cluster(
⋮----
let mut features_to_deactivate = pubkeys_of(matches, "deactivate_feature").unwrap_or_default();
⋮----
return Ok(features_to_deactivate);
⋮----
// if we're here, the cluster type must be one of "mainnet-beta", "testnet", or "devnet"
assert!(matches!(
⋮----
let json_rpc_url = normalize_to_url_if_moniker(
⋮----
.value_of("json_rpc_url")
.unwrap_or(matches.value_of("cluster_type").unwrap()),
⋮----
check_rpc_genesis_hash(cluster_type, &rpc_client)?;
⋮----
.keys()
.cloned()
⋮----
.chunks(MAX_MULTIPLE_ACCOUNTS)
⋮----
.get_multiple_accounts(feature_ids)
.map_err(|err| format!("Failed to fetch: {err}"))?
.into_iter()
.zip(feature_ids)
.for_each(|(maybe_account, feature_id)| {
⋮----
.as_ref()
.and_then(feature::from_account)
.and_then(|feature| feature.activated_at)
.is_none()
⋮----
features_to_deactivate.push(*feature_id);
⋮----
Ok(features_to_deactivate)
⋮----
fn add_validator_accounts(
⋮----
rent_exempt_check(
⋮----
rent.minimum_balance(StakeStateV2::size_of()),
⋮----
let Some(identity_pubkey) = pubkeys_iter.next() else {
⋮----
let vote_pubkey = pubkeys_iter.next().unwrap();
let stake_pubkey = pubkeys_iter.next().unwrap();
genesis_config.add_account(
⋮----
bls_pubkeys_iter.next().map(bls_pubkey_to_compressed_bytes);
⋮----
rent.minimum_balance(VoteStateV4::size_of()).max(1),
⋮----
authorized_pubkey.unwrap_or(identity_pubkey),
⋮----
genesis_config.add_account(*vote_pubkey, vote_account);
⋮----
fn rent_exempt_check(stake_lamports: u64, exempt: u64) -> io::Result<()> {
⋮----
Err(io::Error::other(format!(
⋮----
fn main() -> Result<(), Box<dyn error::Error>> {
⋮----
&fee_rate_governor.target_lamports_per_signature.to_string(),
&fee_rate_governor.target_signatures_per_slot.to_string(),
&fee_rate_governor.burn_percent.to_string(),
⋮----
&rent.lamports_per_byte_year.to_string(),
&rent.exemption_threshold.to_string(),
&rent.burn_percent.to_string(),
⋮----
.max(rent.minimum_balance(VoteStateV4::size_of()))
.to_string();
⋮----
.max(rent.minimum_balance(StakeStateV2::size_of()))
⋮----
let default_ticks_per_slot = &clock::DEFAULT_TICKS_PER_SLOT.to_string();
⋮----
let default_genesis_archive_unpacked_size = MAX_GENESIS_ARCHIVE_UNPACKED_SIZE.to_string();
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.long("creation-time")
.value_name("RFC3339 DATE TIME")
.validator(is_rfc3339_datetime)
.takes_value(true)
.help(
⋮----
.short("b")
.long("bootstrap-validator")
.value_name("IDENTITY_PUBKEY VOTE_PUBKEY STAKE_PUBKEY")
⋮----
.validator(is_pubkey_or_keypair)
.number_of_values(3)
.multiple(true)
.required(true)
.help("The bootstrap validator's identity, vote and stake pubkeys"),
⋮----
.long("bootstrap-validator-bls-pubkey")
.value_name("BLS_PUBKEY")
⋮----
.required(false)
.help("The bootstrap validator's bls pubkey"),
⋮----
.short("l")
.long("ledger")
.value_name("DIR")
⋮----
.help("Use directory as persistent ledger location"),
⋮----
.short("t")
.long("faucet-lamports")
.value_name("LAMPORTS")
⋮----
.help("Number of lamports to assign to the faucet"),
⋮----
.short("m")
.long("faucet-pubkey")
.value_name("PUBKEY")
⋮----
.requires("faucet_lamports")
.default_value(&default_faucet_pubkey)
.help("Path to file containing the faucet's pubkey"),
⋮----
.long("bootstrap-stake-authorized-pubkey")
.value_name("BOOTSTRAP STAKE AUTHORIZED PUBKEY")
⋮----
.long("bootstrap-validator-lamports")
⋮----
.default_value(default_bootstrap_validator_lamports)
.help("Number of lamports to assign to the bootstrap validator"),
⋮----
.long("bootstrap-validator-stake-lamports")
⋮----
.default_value(default_bootstrap_validator_stake_lamports)
.help("Number of lamports to assign to the bootstrap validator's stake account"),
⋮----
.long("target-lamports-per-signature")
⋮----
.default_value(default_target_lamports_per_signature)
⋮----
.long("lamports-per-byte-year")
⋮----
.default_value(default_lamports_per_byte_year)
⋮----
.long("rent-exemption-threshold")
.value_name("NUMBER")
⋮----
.default_value(default_rent_exemption_threshold)
⋮----
.long("rent-burn-percentage")
⋮----
.default_value(default_rent_burn_percentage)
.help("percentage of collected rent to burn")
.validator(is_valid_percentage),
⋮----
.long("fee-burn-percentage")
⋮----
.default_value(default_fee_burn_percentage)
.help("percentage of collected fee to burn")
⋮----
.long("vote-commission-percentage")
⋮----
.default_value("100")
.help("percentage of vote commission")
⋮----
.long("target-signatures-per-slot")
⋮----
.default_value(default_target_signatures_per_slot)
⋮----
.long("target-tick-duration")
.value_name("MILLIS")
⋮----
.help("The target tick rate of the cluster in milliseconds"),
⋮----
.long("hashes-per-tick")
.value_name("NUM_HASHES|\"auto\"|\"sleep\"")
⋮----
.default_value("auto")
⋮----
.long("ticks-per-slot")
.value_name("TICKS")
⋮----
.default_value(default_ticks_per_slot)
.help("The number of ticks in a slot"),
⋮----
.long("slots-per-epoch")
.value_name("SLOTS")
.validator(is_slot)
⋮----
.help("The number of slots in an epoch"),
⋮----
.long("enable-warmup-epochs")
⋮----
.long("primordial-accounts-file")
.value_name("FILENAME")
⋮----
.help("The location of pubkey for primordial accounts and balance"),
⋮----
.long("validator-accounts-file")
⋮----
.long("cluster-type")
.possible_values(&ClusterType::STRINGS)
⋮----
.default_value(default_cluster_type)
.help("Selects the features that will be enabled for the cluster"),
⋮----
.long("deactivate-feature")
⋮----
.value_name("FEATURE_PUBKEY")
.validator(is_pubkey)
⋮----
.long("max-genesis-archive-unpacked-size")
⋮----
.default_value(&default_genesis_archive_unpacked_size)
.help("maximum total uncompressed file size of created genesis archive"),
⋮----
.long("bpf-program")
.value_name("ADDRESS LOADER SBF_PROGRAM.SO")
⋮----
.help("Install a SBF program at the given address"),
⋮----
.long("upgradeable-program")
.value_name("ADDRESS UPGRADEABLE_LOADER SBF_PROGRAM.SO UPGRADE_AUTHORITY")
⋮----
.number_of_values(4)
⋮----
.long("inflation")
⋮----
.possible_values(&["pico", "full", "none"])
.help("Selects inflation"),
⋮----
.short("u")
.long("url")
.value_name("URL_OR_MONIKER")
⋮----
.global(true)
.validator(is_url_or_moniker)
⋮----
.long("alpenglow")
.help("Whether we use Alpenglow consensus."),
⋮----
.get_matches();
let ledger_path = PathBuf::from(matches.value_of("ledger_path").unwrap());
⋮----
lamports_per_byte_year: value_t_or_exit!(matches, "lamports_per_byte_year", u64),
exemption_threshold: value_t_or_exit!(matches, "rent_exemption_threshold", f64),
burn_percent: value_t_or_exit!(matches, "rent_burn_percentage", u8),
⋮----
let bootstrap_validator_pubkeys = pubkeys_of(&matches, "bootstrap_validator").unwrap();
assert_eq!(bootstrap_validator_pubkeys.len() % 3, 0);
⋮----
bls_pubkeys_of(&matches, "bootstrap_validator_bls_pubkey");
⋮----
assert_eq!(
⋮----
let mut v = bootstrap_validator_pubkeys.clone();
v.sort();
v.dedup();
if v.len() != bootstrap_validator_pubkeys.len() {
eprintln!("Error: --bootstrap-validator pubkeys cannot be duplicated");
⋮----
value_t_or_exit!(matches, "bootstrap_validator_lamports", u64);
⋮----
value_t_or_exit!(matches, "bootstrap_validator_stake_lamports", u64);
⋮----
pubkey_of(&matches, "bootstrap_stake_authorized_pubkey");
let faucet_lamports = value_t!(matches, "faucet_lamports", u64).unwrap_or(0);
let faucet_pubkey = pubkey_of(&matches, "faucet_pubkey");
let ticks_per_slot = value_t_or_exit!(matches, "ticks_per_slot", u64);
⋮----
value_t_or_exit!(matches, "target_lamports_per_signature", u64),
value_t_or_exit!(matches, "target_signatures_per_slot", u64),
⋮----
fee_rate_governor.burn_percent = value_t_or_exit!(matches, "fee_burn_percentage", u8);
⋮----
target_tick_duration: if matches.is_present("target_tick_duration") {
Duration::from_micros(value_t_or_exit!(matches, "target_tick_duration", u64))
⋮----
let cluster_type = cluster_type_of(&matches, "cluster_type").unwrap();
let features_to_deactivate = features_to_deactivate_for_cluster(&cluster_type, &matches)
.unwrap_or_else(|e| {
eprintln!("{e}");
⋮----
match matches.value_of("hashes_per_tick").unwrap() {
⋮----
compute_hashes_per_tick(poh_config.target_tick_duration, 1_000_000);
poh_config.hashes_per_tick = Some(hashes_per_tick / 2);
⋮----
poh_config.hashes_per_tick = Some(clock::DEFAULT_HASHES_PER_TICK);
⋮----
poh_config.hashes_per_tick = Some(value_t_or_exit!(matches, "hashes_per_tick", u64));
⋮----
let slots_per_epoch = if matches.value_of("slots_per_epoch").is_some() {
value_t_or_exit!(matches, "slots_per_epoch", u64)
⋮----
matches.is_present("enable_warmup_epochs"),
⋮----
if let Ok(raw_inflation) = value_t!(matches, "inflation", String) {
let inflation = match raw_inflation.as_str() {
⋮----
_ => unreachable!(),
⋮----
let commission = value_t_or_exit!(matches, "vote_commission_percentage", u8);
let rent = genesis_config.rent.clone();
let is_alpenglow = matches.is_present("alpenglow");
⋮----
&mut bootstrap_validator_pubkeys.iter(),
&mut bootstrap_validator_bls_pubkeys.unwrap_or_default().iter(),
⋮----
bootstrap_stake_authorized_pubkey.as_ref(),
⋮----
if let Some(creation_time) = unix_timestamp_from_rfc3339_datetime(&matches, "creation_time") {
⋮----
add_genesis_stake_config_account(&mut genesis_config);
add_genesis_epoch_rewards_account(&mut genesis_config);
⋮----
if !features_to_deactivate.is_empty() {
⋮----
if let Some(files) = matches.values_of("primordial_accounts_file") {
⋮----
load_genesis_accounts(file, &mut genesis_config)?;
⋮----
if let Some(files) = matches.values_of("validator_accounts_file") {
⋮----
load_validator_accounts(file, commission, &rent, &mut genesis_config)?;
⋮----
value_t_or_exit!(matches, "max_genesis_archive_unpacked_size", u64);
⋮----
.values()
.map(|account| account.lamports)
⋮----
add_genesis_stake_accounts(&mut genesis_config, issued_lamports - faucet_lamports);
⋮----
address.parse::<Pubkey>().unwrap_or_else(|err| {
eprintln!("Error: invalid {input_type} {address}: {err}");
⋮----
let mut program_data = vec![];
⋮----
.and_then(|mut file| file.read_to_end(&mut program_data))
.unwrap_or_else(|err| {
eprintln!("Error: failed to read {program}: {err}");
⋮----
if let Some(values) = matches.values_of("bpf_program") {
for (address, loader, program) in values.tuples() {
let address = parse_address(address, "address");
let loader = parse_address(loader, "loader");
let program_data = parse_program_data(program);
⋮----
lamports: genesis_config.rent.minimum_balance(program_data.len()),
⋮----
if let Some(values) = matches.values_of("upgradeable_program") {
for (address, loader, program, upgrade_authority) in values.tuples() {
⋮----
let program_data_elf = parse_program_data(program);
⋮----
upgrade_authority.parse::<Pubkey>().unwrap_or_else(|_| {
read_keypair_file(upgrade_authority)
.map(|keypair| keypair.pubkey())
⋮----
eprintln!(
⋮----
Pubkey::find_program_address(&[address.as_ref()], &loader);
⋮----
upgrade_authority_address: Some(upgrade_authority_address),
⋮----
.unwrap();
program_data.extend_from_slice(&program_data_elf);
⋮----
create_new_ledger(
⋮----
println!("{genesis_config}");
⋮----
mod tests {
⋮----
fn test_append_primordial_accounts_to_genesis() {
assert!(load_genesis_accounts("unknownfile", &mut GenesisConfig::default()).is_err());
⋮----
genesis_accounts.insert(
solana_pubkey::new_rand().to_string(),
⋮----
owner: solana_pubkey::new_rand().to_string(),
⋮----
let serialized = serde_yaml::to_string(&genesis_accounts).unwrap();
⋮----
let mut file = File::create(path).unwrap();
file.write_all(b"---\n").unwrap();
file.write_all(&serialized.into_bytes()).unwrap();
load_genesis_accounts(
⋮----
.expect("test_append_primordial_accounts_to_genesis.yml");
remove_file(path).unwrap();
⋮----
assert_eq!(genesis_config.accounts.len(), genesis_accounts.len());
for (pubkey_str, b64_account) in genesis_accounts.iter() {
let pubkey = pubkey_str.parse().unwrap();
⋮----
genesis_accounts1.insert(
⋮----
let serialized = serde_yaml::to_string(&genesis_accounts1).unwrap();
⋮----
let pubkey = &pubkey_str.parse().unwrap();
⋮----
for (pubkey_str, b64_account) in genesis_accounts1.iter() {
⋮----
let account_keypairs: Vec<_> = (0..3).map(|_| Keypair::new()).collect();
⋮----
genesis_accounts2.insert(
serde_json::to_string(&account_keypairs[0].to_bytes().to_vec()).unwrap(),
⋮----
serde_json::to_string(&account_keypairs[1].to_bytes().to_vec()).unwrap(),
⋮----
serde_json::to_string(&account_keypairs[2].to_bytes().to_vec()).unwrap(),
⋮----
let serialized = serde_yaml::to_string(&genesis_accounts2).unwrap();
⋮----
.expect("genesis");
⋮----
account_keypairs.iter().for_each(|keypair| {
let keypair_str = serde_json::to_string(&keypair.to_bytes().to_vec()).unwrap();
let pubkey = keypair.pubkey();
⋮----
fn test_genesis_account_struct_compatibility() {
⋮----
let tmpfile = tempfile::NamedTempFile::new().unwrap();
let path = tmpfile.path();
⋮----
file.write_all(yaml_string_pubkey.as_bytes()).unwrap();
⋮----
load_genesis_accounts(path.to_str().unwrap(), &mut genesis_config).expect("genesis");
⋮----
assert_eq!(genesis_config.accounts.len(), 4);
⋮----
file.write_all(yaml_string_keypair.as_bytes()).unwrap();
⋮----
assert_eq!(genesis_config.accounts.len(), 3);
⋮----
fn test_append_validator_accounts_to_genesis(add_bls_pubkey: bool) {
assert!(load_validator_accounts(
⋮----
Some(BLSKeypair::new().public.to_string())
⋮----
let validator_accounts = vec![
⋮----
let serialized = serde_yaml::to_string(&validator_accounts).unwrap();
⋮----
file.write_all(b"validator_accounts:\n").unwrap();
file.write_all(serialized.as_bytes()).unwrap();
load_validator_accounts(filename, 100, &Rent::default(), &mut genesis_config)
.expect("Failed to load validator accounts");
⋮----
let expected_accounts_len = validator_accounts.len() * accounts_per_validator;
⋮----
assert_eq!(genesis_config.accounts.len(), expected_accounts_len);
for b64_account in validator_accounts.iter() {
let identity_pk = b64_account.identity_account.parse().unwrap();
⋮----
let vote_pk = b64_account.vote_account.parse().unwrap();
let vote_data = genesis_config.accounts[&vote_pk].data.clone();
let vote_state = VoteStateV4::deserialize(&vote_data, &vote_pk).unwrap();
assert_eq!(vote_state.node_pubkey, identity_pk);
assert_eq!(vote_state.authorized_withdrawer, identity_pk);
⋮----
assert_eq!(authorized_voters.first().unwrap().1, &identity_pk);
⋮----
assert!(b64_account.bls_pubkey.is_none());
assert!(vote_state.bls_pubkey_compressed.is_none());
⋮----
let stake_pk = b64_account.stake_account.parse().unwrap();
⋮----
let stake_data = genesis_config.accounts[&stake_pk].data.clone();
⋮----
borsh1::try_from_slice_unchecked::<StakeStateV2>(&stake_data).unwrap();
assert!(
⋮----
assert_eq!(meta.authorized.staker, identity_pk);
assert_eq!(meta.authorized.withdrawer, identity_pk);
assert_eq!(stake.delegation.voter_pubkey, vote_pk);
⋮----
&Rent::default().minimum_balance(stake_account.data().len());
⋮----
assert_eq!(stake_flags, stake::stake_flags::StakeFlags::empty());

================
File: genesis/src/stakes.rs
================
pub struct StakerInfo {
⋮----
// lamports required to run staking operations for one year
//  the staker account needs carry enough
//  lamports to cover TX fees (delegation) for one year,
//  and we support one delegation per epoch
fn calculate_staker_fees(genesis_config: &GenesisConfig, years: f64) -> u64 {
⋮----
* genesis_config.epoch_schedule.get_epoch(years_as_slots(
⋮----
/// create stake accounts for lamports with at most stake_granularity in each
///  account
⋮----
///  account
pub fn create_and_add_stakes(
⋮----
pub fn create_and_add_stakes(
⋮----
// information about this staker for this group of stakes
⋮----
// description of how the stakes' lockups will expire
⋮----
let granularity = granularity.unwrap_or(u64::MAX);
⋮----
.expect("invalid staker");
⋮----
.unwrap_or(staker_info.staker)
⋮----
.expect("invalid custodian");
⋮----
let staker_rent_reserve = genesis_config.rent.minimum_balance(0).max(1);
let staker_fees = calculate_staker_fees(genesis_config, 1.0);
⋮----
.entry(authorized.staker)
.or_insert_with(|| {
⋮----
let stake_rent_reserve = genesis_config.rent.minimum_balance(StakeStateV2::size_of());
⋮----
let lamports = unlock.amount(stakes_lamports);
⋮----
for _ in 0..(lamports / granularity).saturating_sub(1) {
genesis_config.add_account(
address_generator.next(),
create_lockup_stake_account(
⋮----
create_lockup_stake_account(&authorized, &lockup, &genesis_config.rent, remainder),
⋮----
mod tests {
⋮----
fn create_and_check_stakes(
⋮----
assert_eq!(
⋮----
assert_eq!(genesis_config.accounts.len(), len);
⋮----
assert!(genesis_config
⋮----
fn test_create_stakes() {
⋮----
let reserve = rent.minimum_balance(StakeStateV2::size_of());
let staker_reserve = rent.minimum_balance(0);
⋮----
create_and_check_stakes(
⋮----
rent: rent.clone(),

================
File: genesis/src/unlocks.rs
================
pub struct UnlockInfo {
⋮----
pub struct Unlocks {
⋮----
impl Unlocks {
pub fn new(
⋮----
let cliff_slot = years_as_slots(cliff_year, tick_duration, ticks_per_slot) as u64;
let cliff_epoch = epoch_schedule.get_epoch(cliff_slot);
⋮----
years_as_slots(cliff_year + unlock_years, tick_duration, ticks_per_slot) as u64;
let unlock_epochs = epoch_schedule.get_epoch(first_unlock_slot) - cliff_epoch;
⋮----
pub fn from_epochs(
⋮----
impl Iterator for Unlocks {
type Item = Unlock;
fn next(&mut self) -> Option<Self::Item> {
⋮----
Some(Unlock {
⋮----
pub struct Unlock {
⋮----
impl Unlock {
pub fn amount(&self, total: u64) -> u64 {
⋮----
mod tests {
⋮----
fn test_make_lockups() {
⋮----
assert_eq!(

================
File: genesis/.gitignore
================
/target/
/farf/

================
File: genesis/Cargo.toml
================
[package]
name = "solana-genesis"
documentation = "https://docs.rs/solana-genesis"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "solana_genesis"

[[bin]]
name = "solana-genesis"
path = "src/main.rs"

[features]
default = ["agave-unstable-api"]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
agave-snapshots = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
clap = { workspace = true }
itertools = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
serde_yaml = { workspace = true }
solana-account = "=3.2.0"
solana-bls-signatures = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-clock = "=3.0.0"
solana-cluster-type = "=3.0.0"
solana-commitment-config = "=3.0.0"
solana-entry = { workspace = true }
solana-epoch-schedule = "=3.0.0"
solana-feature-gate-interface = "=3.0.0"
solana-fee-calculator = "=3.0.0"
solana-genesis-config = "=3.0.0"
solana-genesis-utils = { workspace = true }
solana-inflation = "=3.0.0"
solana-keypair = "=3.0.1"
solana-ledger = { workspace = true }
solana-loader-v3-interface = "6.1.0"
solana-native-token = "=3.0.0"
solana-poh-config = "=3.0.0"
solana-pubkey = { version = "=3.0.0", default-features = false }
solana-rent = "=3.0.0"
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-runtime = { workspace = true }
solana-sdk-ids = "=3.0.0"
solana-signer = "=3.0.0"
solana-stake-interface = { version = "=2.0.1", features = ["borsh"] }
solana-time-utils = "3.0.0"
solana-version = { workspace = true }
solana-vote-program = { workspace = true }
tempfile = { workspace = true }

[dev-dependencies]
solana-borsh = { workspace = true }
solana-genesis = { path = ".", features = ["agave-unstable-api"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-vote = { workspace = true }
test-case = { workspace = true }

================
File: genesis/README.md
================
# Genesis

## There are a few ways to bake validator accounts and delegated stake into genesis:
### 1) Through bootstrap validator cli args:
```
--bootstrap-validator <IDENTITY_PUBKEY> <VOTE_PUBKEY> <STAKE_PUBKEY>
--bootstrap-validator-lamports <LAMPORTS>
--bootstrap-validator-stake-lamports <LAMPORTS>
```
Note: you can pass in `--bootstrap-validator ...` multiple times, but the lamports associated with `--bootstrap-validator-lamports` and `--bootstrap-validator-stake-lamports` will apply to all `--bootstrap-validator` arguments.
For example:
```
cargo run --bin solana-genesis --
    --bootstrap-validator <IDENTITY_PUBKEY_0> <VOTE_PUBKEY_0> <STAKE_PUBKEY_0>
    --bootstrap-validator <IDENTITY_PUBKEY_1> <VOTE_PUBKEY_1> <STAKE_PUBKEY_1>
    ...
    --bootstrap-validator <IDENTITY_PUBKEY_N> <VOTE_PUBKEY_N> <STAKE_PUBKEY_N>
    --bootstrap-validator-stake-lamports 10000000000
    --bootstrap-validator-lamports 100000000000
```
All validator accounts will receive the same number of stake and account lamports

### 2) Through the primordial accounts file flag:
The primordial accounts file can be used to add accounts of any type to genesis. A user can define all account data and metadata. `data` field must be BASE64 encoded.
```
--primordial-accounts-file <PATH_TO_PRIMORDIAL_ACCOUNTS_YAML>
```
The primordial accounts file has the following format:
```
---
<IDENTITY_PUBKEY_0>:
  balance: <LAMPORTS_0>
  owner: <OWNER_PUBKEY_0>
  data: <BAS64_ENCODED_DATA_0>
  executable: false
<IDENTITY_PUBKEY_1>:
  balance: <LAMPORTS_1>
  owner: <OWNER_PUBKEY_1>
  data: <BAS64_ENCODED_DATA_1>
  executable: true
...
<IDENTITY_PUBKEY_N>:
  balance: <LAMPORTS_N>
  owner: <OWNER_PUBKEY_N>
  data: <BAS64_ENCODED_DATA_N>
  executable: true
```
The `data` portion of the yaml file holds BASE64 encoded data about the account, which can be vote or stake account information.

### 3) Through the validator accounts file flag:
The main goal with the validator accounts file is to:
- Bake validator stakes into genesis with different stake and account distributions
- Remove the overhead of forcing the user to serialize and deserialize validator stake and vote account state, as required by a primordial accounts file.
```
--validator-accounts-file <PATH_TO_VALIDATOR_ACCOUNTS_YAML>
```
The validator accounts file has the following format:
```
validator_accounts:
- balance_lamports: <BALANCE_LAMPORTS_0>
  stake_lamports: <STAKE_LAMPORTS_0>
  identity_account: <IDENTITY_PUBKEY_0>
  vote_account: <VOTE_PUBKEY_0>
  stake_account: <STAKE_PUBKEY_0>
- balance_lamports: <BALANCE_LAMPORTS_1>
  stake_lamports: <STAKE_LAMPORTS_1>
  identity_account: <IDENTITY_PUBKEY_1>
  vote_account: <VOTE_PUBKEY_1>
  stake_account: <STAKE_PUBKEY_1>
...
- balance_lamports: <BALANCE_LAMPORTS_N>
  stake_lamports: <STAKE_LAMPORTS_N>
  identity_account: <IDENTITY_PUBKEY_N>
  vote_account: <VOTE_PUBKEY_N>
  stake_account: <STAKE_PUBKEY_N>
```

================
File: genesis-utils/src/lib.rs
================
mod open;
fn check_genesis_hash(
⋮----
let genesis_hash = genesis_config.hash();
⋮----
return Err(format!(
⋮----
Ok(())
⋮----
fn load_local_genesis(
⋮----
.map_err(|err| format!("Failed to load genesis config: {err}"))?;
check_genesis_hash(&existing_genesis, expected_genesis_hash)?;
Ok(existing_genesis)
⋮----
fn get_genesis_config(
⋮----
return load_local_genesis(ledger_path, expected_genesis_hash);
⋮----
let genesis_package = ledger_path.join(DEFAULT_GENESIS_ARCHIVE);
⋮----
download_genesis_if_missing(rpc_addr, &genesis_package, use_progress_bar)
⋮----
unpack_genesis_archive(
⋮----
.map_err(|err| format!("Failed to unpack downloaded genesis config: {err}"))?;
⋮----
.map_err(|err| format!("Failed to load downloaded genesis config: {err}"))?;
check_genesis_hash(&downloaded_genesis, expected_genesis_hash)?;
⋮----
.map_err(|err| format!("Unable to rename: {err:?}"))?;
Ok(downloaded_genesis)
⋮----
load_local_genesis(ledger_path, expected_genesis_hash)
⋮----
fn set_and_verify_expected_genesis_hash(
⋮----
if expected_genesis_hash.is_none() {
info!("Expected genesis hash set to {genesis_hash}");
*expected_genesis_hash = Some(genesis_hash);
⋮----
let expected_genesis_hash = expected_genesis_hash.unwrap();
⋮----
.get_genesis_hash()
.map_err(|err| format!("Failed to get genesis hash: {err}"))?;
⋮----
pub fn download_then_check_genesis_hash(
⋮----
let genesis_config = get_genesis_config(
⋮----
set_and_verify_expected_genesis_hash(genesis_config, expected_genesis_hash, rpc_client)

================
File: genesis-utils/src/open.rs
================
pub enum OpenGenesisConfigError {
⋮----
pub fn open_genesis_config(
⋮----
Ok(genesis_config) => Ok(genesis_config),
⋮----
let genesis_package = ledger_path.join(DEFAULT_GENESIS_ARCHIVE);
unpack_genesis_archive(
⋮----
GenesisConfig::load(ledger_path).map_err(OpenGenesisConfigError::Load)

================
File: genesis-utils/Cargo.toml
================
[package]
name = "solana-genesis-utils"
description = "Solana Genesis Utils"
documentation = "https://docs.rs/solana-download-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_genesis_utils"

[features]
agave-unstable-api = []

[dependencies]
agave-snapshots = { workspace = true }
log = { workspace = true }
solana-download-utils = { workspace = true }
solana-genesis-config = { workspace = true }
solana-hash = { workspace = true }
solana-rpc-client = { workspace = true }
thiserror = { workspace = true }

================
File: geyser-plugin-interface/src/geyser_plugin_interface.rs
================
pub struct ReplicaAccountInfo<'a> {
/// The Pubkey for the account
    pub pubkey: &'a [u8],
⋮----
/// This account's data contains a loaded program (and is now read-only)
    pub executable: bool,
⋮----
/// A global monotonically increasing atomic number, which can be used
    /// to tell the order of the account update. For example, when an
⋮----
/// to tell the order of the account update. For example, when an
    /// account is updated in the same slot multiple times, the update
⋮----
/// account is updated in the same slot multiple times, the update
    /// with higher write_version should supersede the one with lower
⋮----
/// with higher write_version should supersede the one with lower
    /// write_version.
⋮----
/// write_version.
    pub write_version: u64,
⋮----
/// Information about an account being updated
/// (extended with transaction signature doing this update)
⋮----
/// (extended with transaction signature doing this update)
pub struct ReplicaAccountInfoV2<'a> {
⋮----
pub struct ReplicaAccountInfoV2<'a> {
⋮----
/// The lamports for the account
    pub lamports: u64,
/// The Pubkey of the owner program account
    pub owner: &'a [u8],
⋮----
/// First signature of the transaction caused this account modification
    pub txn_signature: Option<&'a Signature>,
⋮----
pub struct ReplicaAccountInfoV3<'a> {
⋮----
/// Reference to transaction causing this account modification
    pub txn: Option<&'a SanitizedTransaction>,
⋮----
pub enum ReplicaAccountInfoVersions<'a> {
⋮----
/// Information about a transaction
#[derive(Clone, Debug)]
⋮----
pub struct ReplicaTransactionInfo<'a> {
⋮----
/// Indicates if the transaction is a simple vote transaction.
    pub is_vote: bool,
/// The sanitized transaction.
    pub transaction: &'a SanitizedTransaction,
⋮----
/// Information about a transaction, including index in block
#[derive(Clone, Debug)]
⋮----
pub struct ReplicaTransactionInfoV2<'a> {
⋮----
/// The transaction's index in the block
    pub index: usize,
⋮----
pub struct ReplicaTransactionInfoV3<'a> {
/// The transaction signature, used for identifying the transaction.
    pub signature: &'a Signature,
⋮----
/// The versioned transaction.
    pub transaction: &'a VersionedTransaction,
⋮----
pub enum ReplicaTransactionInfoVersions<'a> {
⋮----
pub struct ReplicaEntryInfo<'a> {
⋮----
/// The number of executed transactions in the Entry
    pub executed_transaction_count: u64,
⋮----
pub struct ReplicaEntryInfoV2<'a> {
⋮----
/// The index-in-block of the first executed transaction in this Entry
    pub starting_transaction_index: usize,
⋮----
/// A wrapper to future-proof ReplicaEntryInfo handling. To make a change to the structure of
/// ReplicaEntryInfo, add an new enum variant wrapping a newer version, which will force plugin
⋮----
/// ReplicaEntryInfo, add an new enum variant wrapping a newer version, which will force plugin
/// implementations to handle the change.
⋮----
/// implementations to handle the change.
#[repr(u32)]
pub enum ReplicaEntryInfoVersions<'a> {
⋮----
pub struct ReplicaBlockInfo<'a> {
⋮----
/// Extending ReplicaBlockInfo by sending the executed_transaction_count.
#[derive(Clone, Debug)]
⋮----
pub struct ReplicaBlockInfoV2<'a> {
⋮----
/// Extending ReplicaBlockInfo by sending the entries_count.
#[derive(Clone, Debug)]
⋮----
pub struct ReplicaBlockInfoV3<'a> {
⋮----
/// Extending ReplicaBlockInfo by sending RewardsAndNumPartitions.
#[derive(Clone, Debug)]
⋮----
pub struct ReplicaBlockInfoV4<'a> {
⋮----
pub enum ReplicaBlockInfoVersions<'a> {
⋮----
pub enum GeyserPluginError {
⋮----
pub enum SlotStatus {
⋮----
impl SlotStatus {
pub fn as_str(&self) -> &'static str {
⋮----
pub type Result<T> = std::result::Result<T, GeyserPluginError>;
/// Defines a Geyser plugin, to stream data from the runtime.
/// Geyser plugins must describe desired behavior for load and unload,
⋮----
/// Geyser plugins must describe desired behavior for load and unload,
/// as well as how they will handle streamed data.
⋮----
/// as well as how they will handle streamed data.
pub trait GeyserPlugin: Any + Send + Sync + std::fmt::Debug {
⋮----
pub trait GeyserPlugin: Any + Send + Sync + std::fmt::Debug {
/// The callback to allow the plugin to setup the logging configuration using the logger
    /// and log level specified by the validator. Will be called first on load/reload, before any other
⋮----
/// and log level specified by the validator. Will be called first on load/reload, before any other
    /// callback, and only called once.
⋮----
/// callback, and only called once.
    /// # Examples
⋮----
/// # Examples
    ///
⋮----
///
    /// ```
⋮----
/// ```
    /// use agave_geyser_plugin_interface::geyser_plugin_interface::{GeyserPlugin,
⋮----
/// use agave_geyser_plugin_interface::geyser_plugin_interface::{GeyserPlugin,
    /// GeyserPluginError, Result};
⋮----
/// GeyserPluginError, Result};
    ///
⋮----
///
    /// #[derive(Debug)]
⋮----
/// #[derive(Debug)]
    /// struct SamplePlugin;
⋮----
/// struct SamplePlugin;
    /// impl GeyserPlugin for SamplePlugin {
⋮----
/// impl GeyserPlugin for SamplePlugin {
    ///     fn setup_logger(&self, logger: &'static dyn log::Log, level: log::LevelFilter) -> Result<()> {
⋮----
///     fn setup_logger(&self, logger: &'static dyn log::Log, level: log::LevelFilter) -> Result<()> {
    #[allow(unused_variables)]
fn setup_logger(&self, logger: &'static dyn log::Log, level: log::LevelFilter) -> Result<()> {
Ok(())
⋮----
fn on_load(&mut self, _config_file: &str, _is_reload: bool) -> Result<()> {
⋮----
fn on_unload(&mut self) {}
⋮----
fn update_account(
⋮----
fn notify_end_of_startup(&self) -> Result<()> {
⋮----
fn update_slot_status(
⋮----
fn notify_transaction(
⋮----
fn notify_entry(&self, entry: ReplicaEntryInfoVersions) -> Result<()> {
⋮----
fn notify_block_metadata(&self, blockinfo: ReplicaBlockInfoVersions) -> Result<()> {
⋮----
fn account_data_notifications_enabled(&self) -> bool {
⋮----
fn account_data_snapshot_notifications_enabled(&self) -> bool {
⋮----
fn transaction_notifications_enabled(&self) -> bool {
⋮----
fn entry_notifications_enabled(&self) -> bool {

================
File: geyser-plugin-interface/src/lib.rs
================
pub mod geyser_plugin_interface;

================
File: geyser-plugin-interface/Cargo.toml
================
[package]
name = "agave-geyser-plugin-interface"
description = "The Solana Geyser plugin interface."
documentation = "https://docs.rs/agave-geyser-plugin-interface"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
log = { workspace = true, features = ["std"] }
solana-clock = { workspace = true }
solana-hash = { workspace = true }
solana-signature = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-status = { workspace = true }
thiserror = { workspace = true }

================
File: geyser-plugin-interface/README.md
================
<p align="center">
  <a href="https://solana.com">
    <img alt="Solana" src="https://i.imgur.com/IKyzQ6T.png" width="250" />
  </a>
</p>

# Solana Geyser Plugin Interface

This crate enables a plugin to be added into the Solana Validator runtime to
take actions at the time of account updates or block and transaction processing;
for example, saving the account state to an external database. The plugin must
implement the `GeyserPlugin` trait. Please see the details of the
`geyser_plugin_interface.rs` for the interface definition.

The plugin should produce a `cdylib` dynamic library, which must expose a `C`
function `_create_plugin()` that instantiates the implementation of the
interface.

The https://github.com/solana-labs/solana-accountsdb-plugin-postgres repository
provides an example of how to create a plugin which saves the accounts data into
an external PostgreSQL database.

More information about Solana is available in the [Solana documentation](https://solana.com/docs).

Still have questions?  Ask us on [Stack Exchange](https://sola.na/sse)

================
File: geyser-plugin-manager/src/accounts_update_notifier.rs
================
pub(crate) struct AccountsUpdateNotifierImpl {
⋮----
impl AccountsUpdateNotifierInterface for AccountsUpdateNotifierImpl {
fn snapshot_notifications_enabled(&self) -> bool {
⋮----
fn notify_account_update(
⋮----
self.accountinfo_from_shared_account_data(account, txn, pubkey, write_version);
self.notify_plugins_of_account_update(account_info, slot, false);
⋮----
fn notify_account_restore_from_snapshot(
⋮----
let mut account = self.accountinfo_from_account_for_geyser(account);
⋮----
self.notify_plugins_of_account_update(account, slot, true);
⋮----
fn notify_end_of_restore_from_snapshot(&self) {
let plugin_manager = self.plugin_manager.read().unwrap();
if plugin_manager.plugins.is_empty() {
⋮----
for plugin in plugin_manager.plugins.iter() {
match plugin.notify_end_of_startup() {
⋮----
error!(
⋮----
trace!(
⋮----
impl AccountsUpdateNotifierImpl {
pub fn new(
⋮----
fn accountinfo_from_shared_account_data<'a>(
⋮----
pubkey: pubkey.as_ref(),
lamports: account.lamports(),
owner: account.owner().as_ref(),
executable: account.executable(),
rent_epoch: account.rent_epoch(),
data: account.data(),
⋮----
fn accountinfo_from_account_for_geyser<'a>(
⋮----
pubkey: account.pubkey.as_ref(),
⋮----
fn notify_plugins_of_account_update(
⋮----
match plugin.update_account(

================
File: geyser-plugin-manager/src/block_metadata_notifier_interface.rs
================
pub trait BlockMetadataNotifier {
⋮----
pub type BlockMetadataNotifierArc = Arc<dyn BlockMetadataNotifier + Sync + Send>;

================
File: geyser-plugin-manager/src/block_metadata_notifier.rs
================
pub(crate) struct BlockMetadataNotifierImpl {
⋮----
impl BlockMetadataNotifier for BlockMetadataNotifierImpl {
fn notify_block_metadata(
⋮----
let plugin_manager = self.plugin_manager.read().unwrap();
if plugin_manager.plugins.is_empty() {
⋮----
for plugin in plugin_manager.plugins.iter() {
⋮----
match plugin.notify_block_metadata(block_info) {
⋮----
error!(
⋮----
trace!(
⋮----
impl BlockMetadataNotifierImpl {
fn build_rewards(rewards: &KeyedRewardsAndNumPartitions) -> RewardsAndNumPartitions {
⋮----
.iter()
.map(|(pubkey, reward)| Reward {
pubkey: pubkey.to_string(),
⋮----
reward_type: Some(reward.reward_type),
⋮----
.collect(),
⋮----
fn build_replica_block_info<'a>(
⋮----
pub fn new(plugin_manager: Arc<RwLock<GeyserPluginManager>>) -> Self {

================
File: geyser-plugin-manager/src/entry_notifier.rs
================
pub(crate) struct EntryNotifierImpl {
⋮----
impl EntryNotifier for EntryNotifierImpl {
fn notify_entry<'a>(
⋮----
let plugin_manager = self.plugin_manager.read().unwrap();
if plugin_manager.plugins.is_empty() {
⋮----
for plugin in plugin_manager.plugins.iter() {
if !plugin.entry_notifications_enabled() {
⋮----
match plugin.notify_entry(ReplicaEntryInfoVersions::V0_0_2(&entry_info)) {
⋮----
error!(
⋮----
trace!("Successfully notified entry to plugin {}", plugin.name());
⋮----
impl EntryNotifierImpl {
pub fn new(plugin_manager: Arc<RwLock<GeyserPluginManager>>) -> Self {
⋮----
fn build_replica_entry_info(
⋮----
hash: entry.hash.as_ref(),

================
File: geyser-plugin-manager/src/geyser_plugin_manager.rs
================
pub struct LoadedGeyserPlugin {
⋮----
impl LoadedGeyserPlugin {
pub fn new(library: Library, plugin: Box<dyn GeyserPlugin>, name: Option<String>) -> Self {
⋮----
name: name.unwrap_or_else(|| plugin.name().to_owned()),
⋮----
pub fn name(&self) -> &str {
⋮----
impl Deref for LoadedGeyserPlugin {
type Target = Box<dyn GeyserPlugin>;
fn deref(&self) -> &Self::Target {
⋮----
impl DerefMut for LoadedGeyserPlugin {
fn deref_mut(&mut self) -> &mut Self::Target {
⋮----
pub struct GeyserPluginManager {
⋮----
impl GeyserPluginManager {
pub fn unload(&mut self) {
for mut plugin in self.plugins.drain(..) {
info!("Unloading plugin for {:?}", plugin.name());
plugin.on_unload();
⋮----
pub fn account_data_notifications_enabled(&self) -> bool {
⋮----
if plugin.account_data_notifications_enabled() {
⋮----
pub fn account_data_snapshot_notifications_enabled(&self) -> bool {
⋮----
if plugin.account_data_snapshot_notifications_enabled() {
⋮----
pub fn transaction_notifications_enabled(&self) -> bool {
⋮----
if plugin.transaction_notifications_enabled() {
⋮----
pub fn entry_notifications_enabled(&self) -> bool {
⋮----
if plugin.entry_notifications_enabled() {
⋮----
pub(crate) fn list_plugins(&self) -> JsonRpcResult<Vec<String>> {
Ok(self.plugins.iter().map(|p| p.name().to_owned()).collect())
⋮----
pub(crate) fn load_plugin(
⋮----
load_plugin_from_config(geyser_plugin_config_file.as_ref()).map_err(|e| {
⋮----
message: format!("Failed to load plugin: {e}"),
⋮----
.iter()
.any(|plugin| plugin.name().eq(new_plugin.name()))
⋮----
return Err(jsonrpc_core::Error {
⋮----
message: format!(
⋮----
setup_logger_for_plugin(&*new_plugin.plugin)?;
⋮----
.on_load(new_config_file, false)
.map_err(|on_load_err| jsonrpc_core::Error {
⋮----
let name = new_plugin.name().to_string();
self.plugins.push(new_plugin);
Ok(name)
⋮----
pub(crate) fn unload_plugin(&mut self, name: &str) -> JsonRpcResult<()> {
⋮----
.position(|plugin| plugin.name().eq(name))
⋮----
return Err(jsonrpc_core::error::Error {
⋮----
self._drop_plugin(idx);
Ok(())
⋮----
pub(crate) fn reload_plugin(&mut self, name: &str, config_file: &str) -> JsonRpcResult<()> {
⋮----
load_plugin_from_config(config_file.as_ref()).map_err(|err| jsonrpc_core::Error {
⋮----
message: err.to_string(),
⋮----
match new_plugin.on_load(new_parsed_config_file, true) {
⋮----
fn _drop_plugin(&mut self, idx: usize) {
let mut current_plugin = self.plugins.remove(idx);
let name = current_plugin.name().to_string();
current_plugin.on_unload();
info!("Unloaded plugin {name} at idx {idx}");
⋮----
fn setup_logger_for_plugin(new_plugin: &dyn GeyserPlugin) -> Result<(), jsonrpc_core::Error> {
⋮----
.setup_logger(log::logger(), log::max_level())
.map_err(|setup_logger_err| jsonrpc_core::Error {
⋮----
pub enum GeyserPluginManagerRequest {
⋮----
pub enum GeyserPluginManagerError {
⋮----
pub(crate) fn load_plugin_from_config(
⋮----
type PluginConstructor = unsafe fn() -> *mut dyn GeyserPlugin;
use libloading::Symbol;
⋮----
return Err(GeyserPluginManagerError::CannotOpenConfigFile(format!(
⋮----
if let Err(err) = file.read_to_string(&mut contents) {
return Err(GeyserPluginManagerError::CannotReadConfigFile(format!(
⋮----
return Err(GeyserPluginManagerError::InvalidConfigFileFormat(format!(
⋮----
.as_str()
.ok_or(GeyserPluginManagerError::LibPathNotSet)?;
⋮----
if libpath.is_relative() {
let config_dir = geyser_plugin_config_file.parent().ok_or_else(|| {
GeyserPluginManagerError::CannotOpenConfigFile(format!(
⋮----
libpath = config_dir.join(libpath);
⋮----
let plugin_name = result["name"].as_str().map(|s| s.to_owned());
⋮----
.as_os_str()
.to_str()
.ok_or(GeyserPluginManagerError::InvalidPluginPath)?;
⋮----
.map_err(|e| GeyserPluginManagerError::PluginLoadError(e.to_string()))?;
⋮----
.get(b"_create_plugin")
⋮----
let plugin_raw = constructor();
⋮----
Ok((
⋮----
if geyser_plugin_config_file.ends_with(TESTPLUGIN_CONFIG) {
Ok(tests::dummy_plugin_and_library(
⋮----
} else if geyser_plugin_config_file.ends_with(TESTPLUGIN2_CONFIG) {
⋮----
Err(GeyserPluginManagerError::CannotOpenConfigFile(
geyser_plugin_config_file.to_str().unwrap().to_string(),
⋮----
mod tests {
⋮----
pub(super) fn dummy_plugin_and_library<P: GeyserPlugin>(
⋮----
let library = libloading::os::windows::Library::this().unwrap();
⋮----
pub(super) struct TestPlugin;
impl GeyserPlugin for TestPlugin {
fn name(&self) -> &'static str {
⋮----
pub(super) struct TestPlugin2;
impl GeyserPlugin for TestPlugin2 {
⋮----
fn test_geyser_reload() {
⋮----
let mut plugin_manager_lock = plugin_manager.write().unwrap();
let reload_result = plugin_manager_lock.reload_plugin(DUMMY_NAME, DUMMY_CONFIG);
assert_eq!(
⋮----
let (mut plugin, config) = dummy_plugin_and_library(TestPlugin, DUMMY_CONFIG);
plugin.on_load(config, false).unwrap();
plugin_manager_lock.plugins.push(plugin);
assert_eq!(plugin_manager_lock.plugins[0].name(), DUMMY_NAME);
plugin_manager_lock.plugins[0].name();
⋮----
let reload_result = plugin_manager_lock.reload_plugin(WRONG_NAME, DUMMY_CONFIG);
⋮----
let reload_result = plugin_manager_lock.reload_plugin(DUMMY_NAME, TESTPLUGIN2_CONFIG);
assert!(reload_result.is_ok());
let plugins = plugin_manager_lock.list_plugins().unwrap();
assert!(plugins.iter().any(|name| name.eq(ANOTHER_DUMMY_NAME)));
assert!(!plugins.iter().any(|name| name.eq(DUMMY_NAME)));
⋮----
fn test_plugin_list() {
⋮----
let (mut plugin, config) = dummy_plugin_and_library(TestPlugin, TESTPLUGIN_CONFIG);
⋮----
let (mut plugin, config) = dummy_plugin_and_library(TestPlugin2, TESTPLUGIN2_CONFIG);
⋮----
assert!(plugins.iter().any(|name| name.eq(DUMMY_NAME)));
⋮----
fn test_plugin_load_unload() {
⋮----
let load_result = plugin_manager_lock.load_plugin(TESTPLUGIN_CONFIG);
assert!(load_result.is_ok());
assert_eq!(plugin_manager_lock.plugins.len(), 1);
let unload_result = plugin_manager_lock.unload_plugin(DUMMY_NAME);
assert!(unload_result.is_ok());
assert_eq!(plugin_manager_lock.plugins.len(), 0);

================
File: geyser-plugin-manager/src/geyser_plugin_service.rs
================
pub struct GeyserPluginService {
⋮----
impl GeyserPluginService {
pub fn new(
⋮----
pub fn new_with_receiver(
⋮----
info!("Starting GeyserPluginService from config files: {geyser_plugin_config_files:?}");
⋮----
plugin_manager.account_data_notifications_enabled() || geyser_plugin_always_enabled;
⋮----
plugin_manager.account_data_snapshot_notifications_enabled();
⋮----
plugin_manager.transaction_notifications_enabled() || geyser_plugin_always_enabled;
⋮----
plugin_manager.entry_notifications_enabled() || geyser_plugin_always_enabled;
⋮----
plugin_manager.clone(),
⋮----
Some(Arc::new(accounts_update_notifier))
⋮----
let transaction_notifier = TransactionNotifierImpl::new(plugin_manager.clone());
Some(Arc::new(transaction_notifier))
⋮----
let entry_notifier = EntryNotifierImpl::new(plugin_manager.clone());
Some(Arc::new(entry_notifier))
⋮----
let slot_status_notifier = SlotStatusNotifierImpl::new(plugin_manager.clone());
⋮----
Some(SlotStatusObserver::new(
⋮----
slot_status_notifier.clone(),
⋮----
Some(Arc::new(BlockMetadataNotifierImpl::new(
⋮----
Some(slot_status_notifier),
⋮----
let plugin_manager = plugin_manager.clone();
⋮----
info!("Started GeyserPluginService");
Ok(GeyserPluginService {
⋮----
fn load_plugin(
⋮----
.load_plugin(geyser_plugin_config_file)
.map_err(|e| GeyserPluginServiceError::FailedToLoadPlugin(e.into()))?;
Ok(())
⋮----
pub fn get_accounts_update_notifier(&self) -> Option<AccountsUpdateNotifier> {
self.accounts_update_notifier.clone()
⋮----
pub fn get_transaction_notifier(&self) -> Option<TransactionNotifierArc> {
self.transaction_notifier.clone()
⋮----
pub fn get_entry_notifier(&self) -> Option<EntryNotifierArc> {
self.entry_notifier.clone()
⋮----
pub fn get_block_metadata_notifier(&self) -> Option<BlockMetadataNotifierArc> {
self.block_metadata_notifier.clone()
⋮----
pub fn get_slot_status_notifier(&self) -> Option<SlotStatusNotifier> {
self.slot_status_notifier.clone()
⋮----
pub fn join(self) -> thread::Result<()> {
⋮----
slot_status_observer.join()?;
⋮----
self.plugin_manager.write().unwrap().unload();
⋮----
fn start_manager_rpc_handler(
⋮----
.name("SolGeyserPluginRpc".to_string())
.spawn(move || loop {
if let Ok(request) = request_receiver.recv_timeout(Duration::from_secs(5)) {
⋮----
let plugin_list = plugin_manager.read().unwrap().list_plugins();
⋮----
.send(plugin_list)
.expect("Admin rpc service will be waiting for response");
⋮----
.write()
.unwrap()
.reload_plugin(name, config_file);
⋮----
.send(reload_result)
⋮----
plugin_manager.write().unwrap().load_plugin(config_file);
⋮----
.send(load_result)
⋮----
let unload_result = plugin_manager.write().unwrap().unload_plugin(name);
⋮----
.send(unload_result)
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
.unwrap();
⋮----
pub enum GeyserPluginServiceError {

================
File: geyser-plugin-manager/src/lib.rs
================
pub mod accounts_update_notifier;
pub mod block_metadata_notifier;
pub mod block_metadata_notifier_interface;
pub mod entry_notifier;
pub mod geyser_plugin_manager;
pub mod geyser_plugin_service;
pub mod slot_status_notifier;
pub mod slot_status_observer;
pub mod transaction_notifier;
pub use geyser_plugin_manager::GeyserPluginManagerRequest;

================
File: geyser-plugin-manager/src/slot_status_notifier.rs
================
pub struct SlotStatusNotifierImpl {
⋮----
impl SlotStatusNotifierInterface for SlotStatusNotifierImpl {
fn notify_slot_confirmed(&self, slot: Slot, parent: Option<Slot>) {
self.notify_slot_status(slot, parent, SlotStatus::Confirmed);
⋮----
fn notify_slot_processed(&self, slot: Slot, parent: Option<Slot>) {
self.notify_slot_status(slot, parent, SlotStatus::Processed);
⋮----
fn notify_slot_rooted(&self, slot: Slot, parent: Option<Slot>) {
self.notify_slot_status(slot, parent, SlotStatus::Rooted);
⋮----
fn notify_first_shred_received(&self, slot: Slot) {
self.notify_slot_status(slot, None, SlotStatus::FirstShredReceived);
⋮----
fn notify_completed(&self, slot: Slot) {
self.notify_slot_status(slot, None, SlotStatus::Completed);
⋮----
fn notify_created_bank(&self, slot: Slot, parent: Slot) {
self.notify_slot_status(slot, Some(parent), SlotStatus::CreatedBank);
⋮----
fn notify_slot_dead(&self, slot: Slot, parent: Slot, error: String) {
self.notify_slot_status(slot, Some(parent), SlotStatus::Dead(error));
⋮----
impl SlotStatusNotifierImpl {
pub fn new(plugin_manager: Arc<RwLock<GeyserPluginManager>>) -> Self {
⋮----
pub fn notify_slot_status(&self, slot: Slot, parent: Option<Slot>, slot_status: SlotStatus) {
let plugin_manager = self.plugin_manager.read().unwrap();
if plugin_manager.plugins.is_empty() {
⋮----
for plugin in plugin_manager.plugins.iter() {
match plugin.update_slot_status(slot, parent, &slot_status) {
⋮----
error!(
⋮----
trace!(

================
File: geyser-plugin-manager/src/slot_status_observer.rs
================
pub(crate) struct SlotStatusObserver {
⋮----
impl SlotStatusObserver {
pub fn new(
⋮----
bank_notification_receiver_service: Some(Self::run_bank_notification_receiver(
⋮----
exit_updated_slot_server.clone(),
⋮----
pub fn join(&mut self) -> thread::Result<()> {
self.exit_updated_slot_server.store(true, Ordering::Relaxed);
⋮----
.take()
.map(JoinHandle::join)
.unwrap()
⋮----
fn run_bank_notification_receiver(
⋮----
.name("solBankNotif".to_string())
.spawn(move || {
while !exit.load(Ordering::Relaxed) {
if let Ok(slot) = bank_notification_receiver.recv() {
⋮----
.read()
⋮----
.notify_slot_confirmed(slot, None);
⋮----
.notify_slot_processed(slot, Some(parent));
⋮----
.notify_slot_rooted(slot, Some(parent));

================
File: geyser-plugin-manager/src/transaction_notifier.rs
================
pub(crate) struct TransactionNotifierImpl {
⋮----
impl TransactionNotifier for TransactionNotifierImpl {
fn notify_transaction(
⋮----
let plugin_manager = self.plugin_manager.read().unwrap();
if plugin_manager.plugins.is_empty() {
⋮----
for plugin in plugin_manager.plugins.iter() {
if !plugin.transaction_notifications_enabled() {
⋮----
match plugin.notify_transaction(
⋮----
error!(
⋮----
trace!(
⋮----
impl TransactionNotifierImpl {
pub fn new(plugin_manager: Arc<RwLock<GeyserPluginManager>>) -> Self {
⋮----
fn build_replica_transaction_info<'a>(

================
File: geyser-plugin-manager/Cargo.toml
================
[package]
name = "solana-geyser-plugin-manager"
description = "The Solana Geyser plugin manager."
documentation = "https://docs.rs/solana-geyser-plugin-manager"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
agave-geyser-plugin-interface = { workspace = true }
bs58 = { workspace = true }
crossbeam-channel = { workspace = true }
json5 = { workspace = true }
jsonrpc-core = { workspace = true }
libloading = { workspace = true }
log = { workspace = true }
serde_json = { workspace = true }
solana-account = { workspace = true }
solana-accounts-db = { workspace = true }
solana-clock = { workspace = true }
solana-entry = { workspace = true }
solana-hash = { workspace = true }
solana-ledger = { workspace = true }
solana-pubkey = { workspace = true }
solana-rpc = { workspace = true }
solana-runtime = { workspace = true }
solana-signature = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-status = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true }





================================================================
End of Codebase
================================================================
