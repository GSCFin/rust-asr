This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
.buildkite/
  hooks/
    post-checkout
    post-command
    pre-command
  scripts/
    build-stable.sh
    build-stable.test.sh
    common.sh
    common.test.sh
    func-assert-eq.sh
    test-all.sh
    trigger-github-actions-windows-build.sh
  pipeline-upload.sh
  solana-private.sh
.config/
  nextest.toml
.github/
  ISSUE_TEMPLATE/
    0-community.md
    1-core-contributor.md
    2-feature-gate.yml
  scripts/
    add-team-to-ghsa.sh
    check-changelog.sh
    downstream-project-spl-common.sh
    downstream-project-spl-install-deps.sh
    install-all-deps.sh
    install-openssl.sh
    install-proto.sh
    purge-ubuntu-runner.sh
  workflows/
    add-team-to-ghsa.yml
    benchmark.yml
    cargo.yml
    changelog-label.yml
    client-targets.yml
    crate-check.yml
    dependabot-pr.yml
    docs.yml
    downstream-project-anchor.yml
    downstream-project-spl-nightly.yml
    downstream-project-spl.yml
    error-reporting.yml
    label-actions.yml
    publish-windows-tarball.yml
    rebase.yaml
    release.yml
    verify-packets.yml
  CODEOWNERS
  dependabot.yml
  label-actions.yml
  PULL_REQUEST_TEMPLATE.md
  RELEASE_TEMPLATE.md
account-decoder/
  src/
    lib.rs
    parse_account_data.rs
    parse_address_lookup_table.rs
    parse_bpf_loader.rs
    parse_config.rs
    parse_nonce.rs
    parse_stake.rs
    parse_sysvar.rs
    parse_token_extension.rs
    parse_token.rs
    parse_vote.rs
    validator_info.rs
  Cargo.toml
account-decoder-client-types/
  src/
    lib.rs
    token.rs
  Cargo.toml
accounts-cluster-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
accounts-db/
  benches/
    accounts_index.rs
    accounts.rs
    bench_accounts_file.rs
    bench_hashing.rs
    bench_lock_accounts.rs
    bench_serde.rs
    read_only_accounts_cache.rs
    utils.rs
  src/
    account_storage/
      stored_account_info.rs
    accounts_db/
      accounts_db_config.rs
      geyser_plugin_utils.rs
      stats.rs
      tests.rs
    accounts_index/
      account_map_entry.rs
      accounts_index_storage.rs
      bucket_map_holder.rs
      in_mem_accounts_index.rs
      iter.rs
      roots_tracker.rs
      secondary.rs
      stats.rs
    append_vec/
      meta.rs
      test_utils.rs
    rolling_bit_field/
      iterators.rs
    tiered_storage/
      byte_block.rs
      error.rs
      file.rs
      footer.rs
      hot.rs
      index.rs
      meta.rs
      mmap_utils.rs
      owners.rs
      readable.rs
      test_utils.rs
    account_info.rs
    account_locks.rs
    account_storage_reader.rs
    account_storage.rs
    accounts_cache.rs
    accounts_db.rs
    accounts_file.rs
    accounts_hash.rs
    accounts_index.rs
    accounts_update_notifier_interface.rs
    accounts.rs
    active_stats.rs
    ancestors.rs
    ancient_append_vecs.rs
    append_vec.rs
    blockhash_queue.rs
    contains.rs
    is_loadable.rs
    is_zero_lamport.rs
    lib.rs
    obsolete_accounts.rs
    partitioned_rewards.rs
    pubkey_bins.rs
    read_only_accounts_cache.rs
    rolling_bit_field.rs
    sorted_storages.rs
    stake_rewards.rs
    storable_accounts.rs
    tiered_storage.rs
    utils.rs
    waitable_condvar.rs
  store-histogram/
    src/
      main.rs
    Cargo.toml
  store-tool/
    src/
      main.rs
    Cargo.toml
  tests/
    read_only_accounts_cache.rs
  Cargo.toml
bam-banking-bench/
  src/
    main.rs
    mock_bam_server.rs
  .gitignore
  Cargo.toml
bam-local-cluster/
  examples/
    example_config.toml
  src/
    cluster_manager.rs
    config.rs
    lib.rs
    main.rs
  Cargo.toml
  README.md
banking-bench/
  src/
    main.rs
  .gitignore
  Cargo.toml
banking-stage-ingress-types/
  src/
    lib.rs
  Cargo.toml
banks-client/
  src/
    error.rs
    lib.rs
  Cargo.toml
banks-interface/
  src/
    lib.rs
  Cargo.toml
banks-server/
  src/
    banks_server.rs
    lib.rs
  Cargo.toml
bench-streamer/
  src/
    main.rs
  .gitignore
  Cargo.toml
bench-tps/
  src/
    bench.rs
    cli.rs
    keypairs.rs
    lib.rs
    log_transaction_service.rs
    main.rs
    perf_utils.rs
    rpc_with_retry_utils.rs
    send_batch.rs
  tests/
    fixtures/
      spl_instruction_padding.so
    bench_tps.rs
  .gitignore
  Cargo.toml
bench-vote/
  src/
    main.rs
  Cargo.toml
bloom/
  benches/
    bloom.rs
  src/
    bloom.rs
    lib.rs
  Cargo.toml
bucket_map/
  src/
    bucket_api.rs
    bucket_item.rs
    bucket_map.rs
    bucket_stats.rs
    bucket_storage.rs
    bucket.rs
    index_entry.rs
    lib.rs
    restart.rs
  tests/
    bucket_map.rs
  Cargo.toml
builtins/
  src/
    core_bpf_migration.rs
    lib.rs
    prototype.rs
  Cargo.toml
builtins-default-costs/
  src/
    lib.rs
  Cargo.toml
bundle/
  src/
    lib.rs
  Cargo.toml
cargo-registry/
  src/
    client.rs
    crate_handler.rs
    main.rs
    response_builder.rs
    sparse_index.rs
  Cargo.toml
ci/
  bench/
    common.sh
    part1.sh
    part2.sh
  common/
    limit-threads.sh
    shared-functions.sh
  coverage/
    common.sh
    part-1.sh
    part-2.sh
    part-3.sh
  docker/
    build.sh
    Dockerfile
    env.sh
    README.md
  downstream-projects/
    common.sh
    func-openbook-dex.sh
    func-spl.sh
    run-all.sh
    run-openbook-dex.sh
    run-spl.sh
  semver_bash/
    LICENSE
    README.md
    semver_test.sh
    semver.sh
  stable/
    common.sh
    run-all.sh
    run-local-cluster-partially.sh
    run-localnet.sh
    run-partition.sh
  xtask/
    src/
      commands/
        bump_version.rs
        hello.rs
      commands.rs
      common.rs
      main.rs
    Cargo.toml
  _
  .gitignore
  buildkite-pipeline.sh
  buildkite-secondary.yml
  buildkite-solana-private.sh
  channel_restriction.sh
  channel-info.sh
  check-channel-version.sh
  check-crates.sh
  check-install-all.sh
  crate-version.sh
  do-audit.sh
  docker-run-default-image.sh
  docker-run.sh
  env.sh
  format-url.sh
  hoover.sh
  intercept.sh
  localnet-sanity.sh
  nits.sh
  order-crates-for-publishing.py
  platform-tools-info.sh
  publish-crate.sh
  publish-installer.sh
  publish-metrics-dashboard.sh
  publish-tarball.sh
  run-local.sh
  run-sanity.sh
  rust-version.sh
  shellcheck.sh
  test-checks.sh
  test-coverage.sh
  test-dev-context-only-utils.sh
  test-downstream-builds.sh
  test-frozen-abi.sh
  test-miri.sh
  test-sanity.sh
  test-shuttle.sh
  test-stable.sh
  test-verify-packets-gossip.sh
  test.sh
  upload-benchmark.sh
  upload-ci-artifact.sh
  upload-github-release-asset.sh
clap-utils/
  src/
    compute_budget.rs
    compute_unit_price.rs
    fee_payer.rs
    input_parsers.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
clap-v3-utils/
  src/
    input_parsers/
      mod.rs
      signer.rs
    keygen/
      derivation_path.rs
      mnemonic.rs
      mod.rs
    compute_budget.rs
    fee_payer.rs
    input_validators.rs
    keypair.rs
    lib.rs
    memo.rs
    nonce.rs
    offline.rs
  Cargo.toml
cli/
  src/
    address_lookup_table.rs
    checks.rs
    clap_app.rs
    cli.rs
    cluster_query.rs
    compute_budget.rs
    feature.rs
    inflation.rs
    lib.rs
    main.rs
    memo.rs
    nonce.rs
    program_v4.rs
    program.rs
    spend_utils.rs
    stake.rs
    test_utils.rs
    validator_info.rs
    vote.rs
    wallet.rs
  tests/
    fixtures/
      alt_bn128.so
      build.sh
      noop_large.so
      noop.so
    address_lookup_table.rs
    cluster_query.rs
    nonce.rs
    program.rs
    request_airdrop.rs
    stake.rs
    transfer.rs
    validator_info.rs
    vote.rs
  .gitignore
  Cargo.toml
cli-config/
  src/
    config_input.rs
    config.rs
    lib.rs
  Cargo.toml
cli-output/
  src/
    cli_output.rs
    cli_version.rs
    display.rs
    lib.rs
  Cargo.toml
client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    connection_cache.rs
    lib.rs
    send_and_confirm_transactions_in_parallel.rs
    tpu_client.rs
    transaction_executor.rs
  .gitignore
  Cargo.toml
client-test/
  tests/
    client.rs
    send_and_confirm_transactions_in_parallel.rs
  .gitignore
  Cargo.toml
compute-budget/
  src/
    compute_budget_limits.rs
    compute_budget.rs
    lib.rs
  Cargo.toml
compute-budget-instruction/
  benches/
    process_compute_budget_instructions.rs
  src/
    builtin_programs_filter.rs
    compute_budget_instruction_details.rs
    compute_budget_program_id_filter.rs
    instructions_processor.rs
    lib.rs
  Cargo.toml
connection-cache/
  src/
    nonblocking/
      client_connection.rs
      mod.rs
    client_connection.rs
    connection_cache_stats.rs
    connection_cache.rs
    lib.rs
  Cargo.toml
core/
  benches/
    banking_stage.rs
    banking_trace.rs
    consensus.rs
    consumer.rs
    gen_keys.rs
    proto_to_packet.rs
    receive_and_buffer_utils.rs
    receive_and_buffer.rs
    scheduler.rs
    shredder.rs
    sigverify_stage.rs
  src/
    banking_stage/
      transaction_scheduler/
        bam_receive_and_buffer.rs
        bam_scheduler.rs
        bam_utils.rs
        batch_id_generator.rs
        greedy_scheduler.rs
        in_flight_tracker.rs
        mod.rs
        prio_graph_scheduler.rs
        receive_and_buffer.rs
        scheduler_common.rs
        scheduler_controller.rs
        scheduler_error.rs
        scheduler_metrics.rs
        scheduler.rs
        transaction_priority_id.rs
        transaction_state_container.rs
        transaction_state.rs
      committer.rs
      consume_worker.rs
      consumer.rs
      decision_maker.rs
      latest_validator_vote_packet.rs
      leader_slot_metrics.rs
      leader_slot_timing_metrics.rs
      progress_tracker.rs
      qos_service.rs
      read_write_account_set.rs
      scheduler_messages.rs
      tpu_to_pack.rs
      unified_scheduler.rs
      vote_packet_receiver.rs
      vote_storage.rs
      vote_worker.rs
    block_creation_loop/
      stats.rs
    bundle_stage/
      bundle_account_locker.rs
      bundle_consumer.rs
      bundle_packet_deserializer.rs
      bundle_stage_leader_metrics.rs
      bundle_storage.rs
    cluster_slots_service/
      cluster_slots.rs
      slot_supporters.rs
    consensus/
      fork_choice.rs
      heaviest_subtree_fork_choice.rs
      latest_validator_votes_for_frozen_banks.rs
      progress_map.rs
      tower_storage.rs
      tower_vote_state.rs
      tower1_14_11.rs
      tower1_7_14.rs
      tree_diff.rs
      vote_stake_tracker.rs
    forwarding_stage/
      packet_container.rs
    proxy/
      auth.rs
      block_engine_stage.rs
      fetch_stage_manager.rs
      mod.rs
      relayer_stage.rs
    repair/
      ancestor_hashes_service.rs
      cluster_slot_state_verifier.rs
      duplicate_repair_status.rs
      malicious_repair_handler.rs
      mod.rs
      outstanding_requests.rs
      packet_threshold.rs
      quic_endpoint.rs
      repair_generic_traversal.rs
      repair_handler.rs
      repair_response.rs
      repair_service.rs
      repair_weight.rs
      repair_weighted_traversal.rs
      request_response.rs
      result.rs
      serve_repair_service.rs
      serve_repair.rs
      standard_repair_handler.rs
    snapshot_packager_service/
      snapshot_gossip_manager.rs
    tip_manager/
      tip_distribution.rs
      tip_payment.rs
    admin_rpc_post_init.rs
    bam_connection.rs
    bam_dependencies.rs
    bam_manager.rs
    banking_simulation.rs
    banking_stage.rs
    banking_trace.rs
    block_creation_loop.rs
    bundle_sigverify_stage.rs
    bundle_stage.rs
    bundle.rs
    cluster_info_vote_listener.rs
    cluster_slots_service.rs
    commitment_service.rs
    completed_data_sets_service.rs
    consensus.rs
    cost_update_service.rs
    drop_bank_service.rs
    fetch_stage.rs
    forwarding_stage.rs
    gen_keys.rs
    lib.rs
    mock_alpenglow_consensus.rs
    next_leader.rs
    optimistic_confirmation_verifier.rs
    packet_bundle.rs
    replay_stage.rs
    resource_limits.rs
    result.rs
    sample_performance_service.rs
    scheduler_bindings_server.rs
    shred_fetch_stage.rs
    sigverify_stage.rs
    sigverify.rs
    snapshot_packager_service.rs
    staked_nodes_updater_service.rs
    stats_reporter_service.rs
    system_monitor_service.rs
    tip_manager.rs
    tpu_entry_notifier.rs
    tpu.rs
    tvu.rs
    unfrozen_gossip_verified_vote_hashes.rs
    validator.rs
    vortexor_receiver_adapter.rs
    vote_simulator.rs
    voting_service.rs
    warm_quic_cache_service.rs
    window_service.rs
  tests/
    bam_connection.rs
    fork-selection.rs
    scheduler_cost_adjustment.rs
    snapshots.rs
    unified_scheduler.rs
  .gitignore
  Cargo.toml
cost-model/
  benches/
    cost_model.rs
    cost_tracker.rs
  src/
    block_cost_limits.rs
    cost_model.rs
    cost_tracker_post_analysis.rs
    cost_tracker.rs
    lib.rs
    transaction_cost.rs
  Cargo.toml
curves/
  curve25519/
    src/
      curve_syscall_traits.rs
      edwards.rs
      errors.rs
      lib.rs
      ristretto.rs
      scalar.rs
    .gitignore
    Cargo.toml
dev/
  Dockerfile
dev-bins/
  .config/
    nextest.toml
  Cargo.toml
docker-solana/
  .gitignore
  build.sh
  Dockerfile
  README.md
docs/
  art/
    fork-generation.bob
    forks-pruned.bob
    forks-pruned2.bob
    forks.bob
    passive-staking-callflow.msc
    retransmit_stage.bob
    runtime.bob
    sdk-tools.bob
    spv-bank-hash.bob
    spv-block-merkle.bob
    tpu.bob
    transaction.bob
    tvu.bob
    validator-proposal.bob
    validator.bob
  components/
    Card.jsx
    HomeCtaLinks.jsx
  src/
    cli/
      examples/
        _category_.json
        choose-a-cluster.md
        delegate-stake.md
        deploy-a-program.md
        durable-nonce.md
        offline-signing.md
        sign-offchain-message.md
        test-validator.md
        transfer-tokens.md
      wallets/
        hardware/
          _category_.json
          index.md
          ledger.md
        _category_.json
        file-system.md
        index.md
        paper.md
      .usage.md.header
      index.md
      install.md
      intro.md
    clusters/
      available.md
      benchmark.md
      index.md
      metrics.md
      testnet.md
    consensus/
      commitments.md
      fork-generation.md
      leader-rotation.md
      managing-forks.md
      stake-delegation-and-rewards.md
      synchronization.md
      turbine-block-propagation.md
      vote-signing.md
    css/
      custom.css
    implemented-proposals/
      ed_overview/
        ed_validation_client_economics/
          ed_vce_overview.md
          ed_vce_state_validation_protocol_based_rewards.md
          ed_vce_state_validation_transaction_fees.md
          ed_vce_validation_stake_delegation.md
        ed_economic_sustainability.md
        ed_mvp.md
        ed_overview.md
        ed_references.md
        ed_storage_rent_economics.md
      abi-management.md
      bank-timestamp-correction.md
      commitment.md
      durable-tx-nonces.md
      epoch_accounts_hash.md
      index.md
      installer.md
      instruction_introspection.md
      leader-leader-transition.md
      leader-validator-transition.md
      persistent-account-storage.md
      readonly-accounts.md
      reliable-vote-transmission.md
      rent.md
      repair-service.md
      rpc-transaction-history.md
      snapshot-verification.md
      staking-rewards.md
      testing-programs.md
      tower-bft.md
      transaction-fees.md
      validator-timestamp-oracle.md
    operations/
      best-practices/
        _category_.json
        general.md
        monitoring.md
        security.md
      guides/
        _category_.json
        restart-cluster.md
        validator-failover.md
        validator-info.md
        validator-monitor.md
        validator-stake.md
        validator-start.md
        validator-troubleshoot.md
        vote-accounts.md
      _category_.json
      index.md
      prerequisites.md
      requirements.md
      setup-a-validator.md
      setup-an-rpc-node.md
      validator-or-rpc-node.md
    pages/
      styles.module.css
    proposals/
      accepted-design-proposals.md
      accounts-db-replication.md
      bankless-leader.md
      block-confirmation.md
      cluster-test-framework.md
      comprehensive-compute-fees.md
      embedding-move.md
      fee_transaction_priority.md
      handle-duplicate-block.md
      interchain-transaction-verification.md
      ledger-replication-to-implement.md
      log_data.md
      off-chain-message-signing.md
      optimistic_confirmation.md
      optimistic-confirmation-and-slashing.md
      optimistic-transaction-propagation-signal.md
      partitioned-inflationary-rewards-distribution.md
      program-instruction-macro.md
      return-data.md
      rip-curl.md
      simple-payment-and-state-verification.md
      slashing.md
      snapshot-verification.md
      tick-verification.md
      timely-vote-credits.md
      validator-proposal.md
      versioned-transactions.md
      vote-signing-to-implement.md
    runtime/
      zk-docs/
        ciphertext_ciphertext_equality.pdf
        ciphertext_commitment_equality.pdf
        ciphertext_validity.pdf
        percentage_with_cap.pdf
        pubkey_proof.pdf
        twisted_elgamal.pdf
        zero_proof.pdf
      programs.md
      sysvars.md
      zk-elgamal-proof.md
    theme/
      Footer/
        index.js
        styles.module.css
    validator/
      anatomy.md
      blockstore.md
      geyser.md
      gossip.md
      runtime.md
      tpu.md
      tvu.md
    architecture.md
    backwards-compatibility.md
    faq.md
    index.mdx
    proposals.md
    what-is-a-validator.md
    what-is-an-rpc-node.md
  static/
    img/
      favicon.ico
    katex/
      contrib/
        auto-render.js
        auto-render.min.js
        auto-render.mjs
        copy-tex.css
        copy-tex.js
        copy-tex.min.css
        copy-tex.min.js
        copy-tex.mjs
        mathtex-script-type.js
        mathtex-script-type.min.js
        mathtex-script-type.mjs
        mhchem.js
        mhchem.min.js
        mhchem.mjs
        render-a11y-string.js
        render-a11y-string.min.js
        render-a11y-string.mjs
      fonts/
        KaTeX_AMS-Regular.ttf
        KaTeX_AMS-Regular.woff
        KaTeX_AMS-Regular.woff2
        KaTeX_Caligraphic-Bold.ttf
        KaTeX_Caligraphic-Bold.woff
        KaTeX_Caligraphic-Bold.woff2
        KaTeX_Caligraphic-Regular.ttf
        KaTeX_Caligraphic-Regular.woff
        KaTeX_Caligraphic-Regular.woff2
        KaTeX_Fraktur-Bold.ttf
        KaTeX_Fraktur-Bold.woff
        KaTeX_Fraktur-Bold.woff2
        KaTeX_Fraktur-Regular.ttf
        KaTeX_Fraktur-Regular.woff
        KaTeX_Fraktur-Regular.woff2
        KaTeX_Main-Bold.ttf
        KaTeX_Main-Bold.woff
        KaTeX_Main-Bold.woff2
        KaTeX_Main-BoldItalic.ttf
        KaTeX_Main-BoldItalic.woff
        KaTeX_Main-BoldItalic.woff2
        KaTeX_Main-Italic.ttf
        KaTeX_Main-Italic.woff
        KaTeX_Main-Italic.woff2
        KaTeX_Main-Regular.ttf
        KaTeX_Main-Regular.woff
        KaTeX_Main-Regular.woff2
        KaTeX_Math-BoldItalic.ttf
        KaTeX_Math-BoldItalic.woff
        KaTeX_Math-BoldItalic.woff2
        KaTeX_Math-Italic.ttf
        KaTeX_Math-Italic.woff
        KaTeX_Math-Italic.woff2
        KaTeX_SansSerif-Bold.ttf
        KaTeX_SansSerif-Bold.woff
        KaTeX_SansSerif-Bold.woff2
        KaTeX_SansSerif-Italic.ttf
        KaTeX_SansSerif-Italic.woff
        KaTeX_SansSerif-Italic.woff2
        KaTeX_SansSerif-Regular.ttf
        KaTeX_SansSerif-Regular.woff
        KaTeX_SansSerif-Regular.woff2
        KaTeX_Script-Regular.ttf
        KaTeX_Script-Regular.woff
        KaTeX_Script-Regular.woff2
        KaTeX_Size1-Regular.ttf
        KaTeX_Size1-Regular.woff
        KaTeX_Size1-Regular.woff2
        KaTeX_Size2-Regular.ttf
        KaTeX_Size2-Regular.woff
        KaTeX_Size2-Regular.woff2
        KaTeX_Size3-Regular.ttf
        KaTeX_Size3-Regular.woff
        KaTeX_Size3-Regular.woff2
        KaTeX_Size4-Regular.ttf
        KaTeX_Size4-Regular.woff
        KaTeX_Size4-Regular.woff2
        KaTeX_Typewriter-Regular.ttf
        KaTeX_Typewriter-Regular.woff
        KaTeX_Typewriter-Regular.woff2
      katex.css
      katex.js
      katex.min.css
      katex.min.js
      katex.mjs
      README.md
    .nojekyll
  .eslintignore
  .eslintrc
  .gitignore
  .prettierignore
  .prettierrc.json
  babel.config.js
  build-cli-usage.sh
  build.sh
  convert-ascii-to-svg.sh
  deploy.sh
  docusaurus.config.js
  offline-cmd-md-links.sh
  package.json
  README.md
  set-solana-release-tag.sh
  sidebars.js
dos/
  src/
    cli.rs
    lib.rs
    main.rs
  Cargo.toml
download-utils/
  src/
    lib.rs
  Cargo.toml
entry/
  benches/
    entry_sigverify.rs
  src/
    entry.rs
    lib.rs
    poh.rs
    wincode.rs
  Cargo.toml
faucet/
  src/
    bin/
      faucet.rs
    faucet_mock.rs
    faucet.rs
    lib.rs
  tests/
    local-faucet.rs
  .gitignore
  Cargo.toml
feature-set/
  src/
    lib.rs
  Cargo.toml
fee/
  src/
    lib.rs
  Cargo.toml
fs/
  src/
    io_uring/
      dir_remover.rs
      file_creator.rs
      memory.rs
      mod.rs
      sequential_file_reader.rs
    buffered_reader.rs
    dirs.rs
    file_io.rs
    lib.rs
  Cargo.toml
genesis/
  src/
    address_generator.rs
    genesis_accounts.rs
    lib.rs
    main.rs
    stakes.rs
    unlocks.rs
  .gitignore
  Cargo.toml
  README.md
genesis-utils/
  src/
    lib.rs
    open.rs
  Cargo.toml
geyser-plugin-interface/
  src/
    geyser_plugin_interface.rs
    lib.rs
  Cargo.toml
  README.md
geyser-plugin-manager/
  src/
    accounts_update_notifier.rs
    block_metadata_notifier_interface.rs
    block_metadata_notifier.rs
    entry_notifier.rs
    geyser_plugin_manager.rs
    geyser_plugin_service.rs
    lib.rs
    slot_status_notifier.rs
    slot_status_observer.rs
    transaction_notifier.rs
  Cargo.toml
gossip/
  benches/
    crds_gossip_pull.rs
    crds_shards.rs
    crds.rs
    weighted_shuffle.rs
  src/
    cluster_info_metrics.rs
    cluster_info.rs
    contact_info.rs
    crds_data.rs
    crds_entry.rs
    crds_filter.rs
    crds_gossip_error.rs
    crds_gossip_pull.rs
    crds_gossip_push.rs
    crds_gossip.rs
    crds_shards.rs
    crds_value.rs
    crds.rs
    deprecated.rs
    duplicate_shred_handler.rs
    duplicate_shred_listener.rs
    duplicate_shred.rs
    epoch_slots.rs
    epoch_specs.rs
    gossip_error.rs
    gossip_service.rs
    legacy_contact_info.rs
    lib.rs
    node.rs
    ping_pong.rs
    protocol.rs
    push_active_set.rs
    received_cache.rs
    restart_crds_values.rs
    tlv.rs
    weighted_shuffle.rs
    wire_format_tests.rs
  tests/
    crds_gossip.rs
    gossip.rs
  .gitignore
  Cargo.toml
gossip-bin/
  src/
    main.rs
  Cargo.toml
install/
  src/
    bin/
      agave-install-init.rs
    build_env.rs
    command.rs
    config.rs
    defaults.rs
    lib.rs
    main.rs
    stop_process.rs
    update_manifest.rs
  .gitignore
  agave-install-init.sh
  build.rs
  Cargo.toml
  install-help.sh
io-uring/
  src/
    lib.rs
    ring.rs
    slab.rs
  Cargo.toml
jito-protos/
  src/
    lib.rs
  build.rs
  Cargo.toml
keygen/
  src/
    keygen.rs
  .gitignore
  Cargo.toml
lattice-hash/
  benches/
    bench_lt_hash.rs
  src/
    lib.rs
    lt_hash.rs
  Cargo.toml
ledger/
  benches/
    blockstore.rs
    make_shreds_from_entries.rs
    protobuf.rs
  proptest-regressions/
    blockstore_meta.txt
  src/
    blockstore/
      blockstore_purge.rs
      column.rs
      error.rs
    leader_schedule/
      identity_keyed.rs
      vote_keyed.rs
    shred/
      common.rs
      merkle_tree.rs
      merkle.rs
      payload.rs
      shred_code.rs
      shred_data.rs
      stats.rs
      traits.rs
      wire.rs
    ancestor_iterator.rs
    bank_forks_utils.rs
    bigtable_delete.rs
    bigtable_upload_service.rs
    bigtable_upload.rs
    bit_vec.rs
    block_error.rs
    blockstore_cleanup_service.rs
    blockstore_db.rs
    blockstore_meta.rs
    blockstore_metric_report_service.rs
    blockstore_metrics.rs
    blockstore_options.rs
    blockstore_processor.rs
    blockstore.rs
    entry_notifier_interface.rs
    entry_notifier_service.rs
    genesis_utils.rs
    leader_schedule_cache.rs
    leader_schedule_utils.rs
    leader_schedule.rs
    lib.rs
    next_slots_iterator.rs
    rooted_slot_iterator.rs
    shred.rs
    shredder.rs
    sigverify_shreds.rs
    slot_stats.rs
    staking_utils.rs
    token_balances.rs
    transaction_address_lookup_table_scanner.rs
    transaction_balances.rs
    use_snapshot_archives_at_startup.rs
    wire_format_tests.rs
  tests/
    blockstore.rs
    shred.rs
  .gitignore
  Cargo.toml
ledger-tool/
  src/
    args.rs
    bigtable.rs
    blockstore.rs
    error.rs
    ledger_path.rs
    ledger_utils.rs
    main.rs
    output.rs
    program.rs
  tests/
    basic.rs
  .gitignore
  Cargo.toml
local-cluster/
  src/
    cluster_tests.rs
    cluster.rs
    integration_tests.rs
    lib.rs
    local_cluster_snapshot_utils.rs
    local_cluster.rs
    validator_configs.rs
  tests/
    local_cluster.rs
  .gitignore
  Cargo.toml
logger/
  src/
    lib.rs
  Cargo.toml
measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  .gitignore
  Cargo.toml
merkle-tree/
  src/
    lib.rs
    merkle_tree.rs
  .gitignore
  Cargo.toml
metrics/
  benches/
    metrics.rs
  scripts/
    grafana-provisioning/
      dashboards/
        cluster-monitor.json
        dashboard.yml
    .gitignore
    adjust-dashboard-for-channel.py
    enable.sh
    grafana.ini
    influxdb.conf
    README.md
    start.sh
    status.sh
    stop.sh
    test.sh
  src/
    counter.rs
    datapoint.rs
    lib.rs
    metrics.rs
  .gitignore
  Cargo.toml
  README.md
multinode-demo/
  bench-tps.sh
  bootstrap-validator.sh
  common.sh
  delegate-stake.sh
  faucet.sh
  setup-from-mainnet-beta.sh
  setup-from-testnet.sh
  setup.sh
  validator-x.sh
  validator.sh
net/
  remote/
    cleanup.sh
    README.md
    remote-client.sh
    remote-deploy-update.sh
    remote-node-wait-init.sh
    remote-node.sh
    remote-sanity.sh
  scripts/
    azure-provider.sh
    colo_nodes
    colo-node-onacquire.sh
    colo-node-onfree.sh
    colo-provider.sh
    colo-utils.sh
    create-solana-user.sh
    disable-background-upgrades.sh
    ec2-provider.sh
    ec2-security-group-config.json
    gce-provider.sh
    gce-self-destruct.sh
    install-ag.sh
    install-at.sh
    install-certbot.sh
    install-docker.sh
    install-earlyoom.sh
    install-iftop.sh
    install-jq.sh
    install-libssl.sh
    install-perf.sh
    install-rsync.sh
    localtime.sh
    mount-additional-disk.sh
    network-config.sh
    remove-docker-interface.sh
    rsync-retry.sh
  .gitignore
  common.sh
  gce.sh
  init-metrics.sh
  net.sh
  scp.sh
  ssh.sh
net-utils/
  benches/
    token_bucket.rs
  src/
    ip_echo_client.rs
    ip_echo_server.rs
    lib.rs
    multihomed_sockets.rs
    socket_addr_space.rs
    sockets.rs
    token_bucket.rs
    tooling_for_tests.rs
  .gitignore
  Cargo.toml
notifier/
  src/
    lib.rs
  .gitignore
  Cargo.toml
perf/
  benches/
    dedup.rs
    discard.rs
    recycler.rs
    reset.rs
    shrink.rs
    sigverify.rs
  src/
    data_budget.rs
    deduper.rs
    discard.rs
    lib.rs
    packet.rs
    perf_libs.rs
    recycled_vec.rs
    recycler_cache.rs
    recycler.rs
    sigverify.rs
    test_tx.rs
    thread.rs
  build.rs
  Cargo.toml
platform-tools-sdk/
  cargo-build-sbf/
    src/
      main.rs
      post_processing.rs
      toolchain.rs
      utils.rs
    tests/
      crates/
        fail/
          src/
            lib.rs
          Cargo.toml
        noop/
          src/
            lib.rs
          Cargo.toml
        package-metadata/
          src/
            lib.rs
          Cargo.toml
        workspace-metadata/
          src/
            lib.rs
          Cargo.toml
      crates.rs
    .gitignore
    Cargo.toml
  cargo-test-sbf/
    src/
      main.rs
    Cargo.toml
  gen-headers/
    src/
      main.rs
    Cargo.toml
  sbf/
    c/
      inc/
        sol/
          inc/
            alt_bn128_compression.inc
            alt_bn128.inc
            assert.inc
            big_mod_exp.inc
            blake3.inc
            compute_units.inc
            cpi.inc
            keccak.inc
            last_restart_slot.inc
            log.inc
            poseidon.inc
            pubkey.inc
            return_data.inc
            secp256k1.inc
            sha.inc
          alt_bn128_compression.h
          alt_bn128.h
          assert.h
          big_mod_exp.h
          blake3.h
          compute_units.h
          constants.h
          cpi.h
          deserialize_deprecated.h
          deserialize.h
          entrypoint.h
          keccak.h
          last_restart_slot.h
          log.h
          poseidon.h
          pubkey.h
          return_data.h
          secp256k1.h
          sha.h
          string.h
          types.h
        sys/
          param.h
        deserialize_deprecated.h
        solana_sdk.h
        stdio.h
        stdlib.h
        string.h
        wchar.h
      README.md
      sbf.ld
      sbf.mk
    scripts/
      dump.sh
      install.sh
      objcopy.sh
      package.sh
      strip.sh
    .gitignore
    env.sh
poh/
  benches/
    poh_verify.rs
    poh.rs
    transaction_recorder.rs
  src/
    lib.rs
    poh_controller.rs
    poh_recorder.rs
    poh_service.rs
    record_channels.rs
    transaction_recorder.rs
  .gitignore
  Cargo.toml
poh-bench/
  src/
    main.rs
  Cargo.toml
poseidon/
  src/
    legacy.rs
    lib.rs
  Cargo.toml
precompiles/
  benches/
    ed25519_instructions.rs
    secp256k1_instructions.rs
    secp256r1_instructions.rs
  src/
    ed25519.rs
    lib.rs
    secp256k1.rs
    secp256r1.rs
  Cargo.toml
program-binaries/
  src/
    programs/
      core_bpf_address_lookup_table-3.0.0.so
      core_bpf_config-3.0.0.so
      core_bpf_feature_gate-0.0.1.so
      core_bpf_stake-1.0.1.so
      spl_associated_token_account-1.1.1.so
      spl_memo-1.0.0.so
      spl_memo-3.0.0.so
      spl_token_2022-8.0.0.so
      spl_token-3.5.0.so
      spl-jito_tip_distribution-0.1.10.so
      spl-jito_tip_distribution-0.1.4.so
      spl-jito_tip_distribution-0.1.7.so
      spl-jito_tip_payment-0.1.10.so
      spl-jito_tip_payment-0.1.4.so
      spl-jito_tip_payment-0.1.7.so
    lib.rs
  Cargo.toml
program-runtime/
  src/
    cpi.rs
    execution_budget.rs
    invoke_context.rs
    lib.rs
    loaded_programs.rs
    mem_pool.rs
    memory.rs
    serialization.rs
    stable_log.rs
    sysvar_cache.rs
  Cargo.toml
program-test/
  src/
    lib.rs
  tests/
    fixtures/
      noop_program.so
    bpf.rs
    builtins.rs
    compute_units.rs
    core_bpf.rs
    cpi.rs
    fuzz.rs
    genesis_accounts.rs
    lamports.rs
    panic.rs
    realloc.rs
    return_data.rs
    setup.rs
    spl.rs
    sysvar_last_restart_slot.rs
    sysvar.rs
    warp.rs
  Cargo.toml
programs/
  bpf_loader/
    benches/
      bpf_loader_upgradeable.rs
      serialization.rs
    src/
      lib.rs
    test_elfs/
      out/
        noop_aligned.so
        noop_unaligned.so
        sbpfv0_verifier_err.so
        sbpfv3_return_err.so
        sbpfv3_return_ok.so
      src/
        noop_aligned/
          noop_aligned.c
        noop_unaligned/
          noop_unaligned.c
      makefile
    Cargo.toml
  bpf-loader-tests/
    tests/
      common.rs
      extend_program_ix.rs
    Cargo.toml
    noop.so
  compute-budget/
    src/
      lib.rs
    Cargo.toml
  compute-budget-bench/
    benches/
      compute_budget.rs
    Cargo.toml
  ed25519-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  loader-v4/
    src/
      lib.rs
    Cargo.toml
  sbf/
    benches/
      bpf_loader.rs
    c/
      src/
        alloc/
          alloc.c
        alt_bn128/
          alt_bn128.c
        alt_bn128_compression/
          alt_bn128.c
        bench_alu/
          bench_alu.c
          test_bench_alu.c
        big_mod_exp/
          big_mod_exp.c
        deprecated_loader/
          deprecated_loader.c
        dup_accounts/
          dup_accounts.c
        error_handling/
          error_handling.c
        float/
          float.c
        invoke/
          invoke.c
        invoked/
          instruction.h
          invoked.c
        log_data/
          log_data.c
        move_funds/
          move_funds.c
        multiple_static/
          multiple_static.c
        noop/
          noop.c
        noop++/
          noop++.cc
        panic/
          panic.c
        poseidon/
          poseidon.c
        read_program/
          read_program.c
        relative_call/
          relative_call.c
        remaining_compute_units/
          remaining_compute_units.c
        return_data/
          return_data.c
        sanity/
          sanity.c
        sanity++/
          sanity++.cc
        sbf_to_sbf/
          entrypoint.c
          helper.c
          helper.h
        secp256k1_recover/
          secp256k1_recover.c
        ser/
          ser.c
        sha/
          sha.c
        stdlib/
          stdlib.c
        struct_pass/
          struct_pass.c
        struct_ret/
          struct_ret.c
        tuner/
          tuner.c
        tuner-variable-iterations/
          tuner-variable-iterations.c
      .gitignore
    rust/
      128bit/
        src/
          lib.rs
        Cargo.toml
      128bit_dep/
        src/
          lib.rs
        Cargo.toml
      account_mem/
        src/
          lib.rs
        Cargo.toml
      account_mem_deprecated/
        src/
          lib.rs
        Cargo.toml
      alloc/
        src/
          lib.rs
        Cargo.toml
      alt_bn128/
        src/
          lib.rs
        Cargo.toml
      alt_bn128_compression/
        src/
          lib.rs
        Cargo.toml
      big_mod_exp/
        src/
          lib.rs
        Cargo.toml
      call_args/
        src/
          lib.rs
        Cargo.toml
      call_depth/
        src/
          lib.rs
        Cargo.toml
      caller_access/
        src/
          lib.rs
        Cargo.toml
      curve25519/
        src/
          lib.rs
        Cargo.toml
      custom_heap/
        src/
          lib.rs
        Cargo.toml
      dep_crate/
        src/
          lib.rs
        Cargo.toml
      deprecated_loader/
        src/
          lib.rs
        Cargo.toml
      divide_by_zero/
        src/
          lib.rs
        Cargo.toml
      dup_accounts/
        src/
          lib.rs
        Cargo.toml
      error_handling/
        src/
          lib.rs
        Cargo.toml
      external_spend/
        src/
          lib.rs
        Cargo.toml
      get_minimum_delegation/
        src/
          lib.rs
        Cargo.toml
      inner_instruction_alignment_check/
        src/
          lib.rs
        Cargo.toml
      instruction_introspection/
        src/
          lib.rs
        Cargo.toml
      invoke/
        src/
          lib.rs
        Cargo.toml
      invoke_and_error/
        src/
          lib.rs
        Cargo.toml
      invoke_and_ok/
        src/
          lib.rs
        Cargo.toml
      invoke_and_return/
        src/
          lib.rs
        Cargo.toml
      invoke_dep/
        src/
          lib.rs
        Cargo.toml
      invoked/
        src/
          lib.rs
        Cargo.toml
      invoked_dep/
        src/
          lib.rs
        Cargo.toml
      iter/
        src/
          lib.rs
        Cargo.toml
      log_data/
        src/
          lib.rs
        Cargo.toml
      many_args/
        src/
          helper.rs
          lib.rs
        Cargo.toml
      many_args_dep/
        src/
          lib.rs
        Cargo.toml
      mem/
        src/
          lib.rs
        Cargo.toml
      mem_dep/
        src/
          lib.rs
        Cargo.toml
      membuiltins/
        src/
          lib.rs
        Cargo.toml
      noop/
        src/
          lib.rs
        Cargo.toml
      panic/
        src/
          lib.rs
        Cargo.toml
      param_passing/
        src/
          lib.rs
        Cargo.toml
      param_passing_dep/
        src/
          lib.rs
        Cargo.toml
      poseidon/
        src/
          lib.rs
        Cargo.toml
      r2_instruction_data_pointer/
        src/
          lib.rs
        Cargo.toml
      rand/
        src/
          lib.rs
        Cargo.toml
      realloc/
        src/
          lib.rs
        Cargo.toml
      realloc_dep/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke/
        src/
          lib.rs
        Cargo.toml
      realloc_invoke_dep/
        src/
          lib.rs
        Cargo.toml
      remaining_compute_units/
        src/
          lib.rs
        Cargo.toml
      ro_account_modify/
        src/
          lib.rs
        Cargo.toml
      ro_modify/
        src/
          lib.rs
        Cargo.toml
      sanity/
        src/
          lib.rs
        Cargo.toml
      secp256k1_recover/
        src/
          lib.rs
        Cargo.toml
      sha/
        src/
          lib.rs
        Cargo.toml
      sibling_inner_instructions/
        src/
          lib.rs
        Cargo.toml
      sibling_instructions/
        src/
          lib.rs
        Cargo.toml
      simulation/
        src/
          lib.rs
        Cargo.toml
      spoof1/
        src/
          lib.rs
        Cargo.toml
      spoof1_system/
        src/
          lib.rs
        Cargo.toml
      syscall-get-epoch-stake/
        src/
          lib.rs
        Cargo.toml
      sysvar/
        src/
          lib.rs
        Cargo.toml
      upgradeable/
        src/
          lib.rs
        Cargo.toml
      upgraded/
        src/
          lib.rs
        Cargo.toml
    tests/
      programs.rs
      simulation.rs
      syscall_get_epoch_stake.rs
      sysvar.rs
    .gitignore
    Cargo.toml
    Makefile
  system/
    benches/
      system.rs
    src/
      lib.rs
      system_instruction.rs
      system_processor.rs
    Cargo.toml
  vote/
    benches/
      process_vote.rs
      vote_instructions.rs
    src/
      vote_state/
        handler.rs
        mod.rs
      lib.rs
      vote_processor.rs
    Cargo.toml
  zk-elgamal-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
  zk-elgamal-proof-tests/
    tests/
      process_transaction.rs
    Cargo.toml
  zk-token-proof/
    benches/
      verify_proofs.rs
    src/
      lib.rs
    Cargo.toml
pubsub-client/
  src/
    nonblocking/
      mod.rs
      pubsub_client.rs
    lib.rs
    pubsub_client.rs
  Cargo.toml
quic-client/
  src/
    nonblocking/
      mod.rs
      quic_client.rs
    lib.rs
    quic_client.rs
  tests/
    quic_client.rs
  Cargo.toml
rayon-threadlimit/
  src/
    lib.rs
  .gitignore
  Cargo.toml
rbpf-cli/
  src/
    main.rs
  Cargo.toml
remote-wallet/
  src/
    ledger_error.rs
    ledger.rs
    lib.rs
    locator.rs
    remote_keypair.rs
    remote_wallet.rs
  Cargo.toml
  README.md
reserved-account-keys/
  src/
    lib.rs
  Cargo.toml
rpc/
  src/
    rpc/
      account_resolver.rs
    cluster_tpu_info.rs
    filter.rs
    lib.rs
    max_slots.rs
    optimistically_confirmed_bank_tracker.rs
    parsed_token_accounts.rs
    rpc_cache.rs
    rpc_completed_slots_service.rs
    rpc_health.rs
    rpc_pubsub_service.rs
    rpc_pubsub.rs
    rpc_service.rs
    rpc_subscription_tracker.rs
    rpc_subscriptions.rs
    rpc.rs
    slot_status_notifier.rs
    transaction_notifier_interface.rs
    transaction_status_service.rs
  .gitignore
  Cargo.toml
rpc-client/
  src/
    nonblocking/
      mod.rs
      rpc_client.rs
    http_sender.rs
    lib.rs
    mock_sender.rs
    rpc_client.rs
    rpc_sender.rs
    spinner.rs
  Cargo.toml
rpc-client-api/
  src/
    bundles.rs
    client_error.rs
    custom_error.rs
    lib.rs
    response.rs
  Cargo.toml
rpc-client-nonce-utils/
  src/
    nonblocking/
      blockhash_query.rs
      mod.rs
    blockhash_query.rs
    lib.rs
  Cargo.toml
rpc-client-types/
  src/
    config.rs
    error_object.rs
    filter.rs
    lib.rs
    request.rs
    response.rs
  Cargo.toml
rpc-test/
  tests/
    nonblocking.rs
    rpc.rs
  .gitignore
  Cargo.toml
runtime/
  benches/
    accounts.rs
    prioritization_fee_cache.rs
    status_cache.rs
  src/
    accounts_background_service/
      pending_snapshot_packages.rs
      stats.rs
    bank/
      builtins/
        core_bpf_migration/
          error.rs
          mod.rs
          source_buffer.rs
          target_bpf_v2.rs
          target_builtin.rs
          target_core_bpf.rs
        mod.rs
      partitioned_epoch_rewards/
        calculation.rs
        distribution.rs
        epoch_rewards_hasher.rs
        mod.rs
        sysvar.rs
      accounts_lt_hash.rs
      address_lookup_table.rs
      bank_hash_details.rs
      check_transactions.rs
      fee_distribution.rs
      metrics.rs
      recent_blockhashes_account.rs
      serde_snapshot.rs
      sysvar_cache.rs
      tests.rs
    inflation_rewards/
      mod.rs
      points.rs
    serde_snapshot/
      obsolete_accounts.rs
      status_cache.rs
      storage.rs
      tests.rs
      types.rs
      utils.rs
    snapshot_package/
      compare.rs
    snapshot_utils/
      snapshot_storage_rebuilder.rs
    stakes/
      serde_stakes.rs
    account_saver.rs
    accounts_background_service.rs
    bank_client.rs
    bank_forks.rs
    bank_hash_cache.rs
    bank_utils.rs
    bank.rs
    commitment.rs
    dependency_tracker.rs
    epoch_stakes.rs
    genesis_utils.rs
    installed_scheduler_pool.rs
    lib.rs
    loader_utils.rs
    non_circulating_supply.rs
    prioritization_fee_cache.rs
    prioritization_fee.rs
    read_optimized_dashmap.rs
    rent_collector.rs
    runtime_config.rs
    serde_snapshot.rs
    snapshot_bank_utils.rs
    snapshot_controller.rs
    snapshot_minimizer.rs
    snapshot_package.rs
    snapshot_utils.rs
    stake_account.rs
    stake_history.rs
    stake_utils.rs
    stake_weighted_timestamp.rs
    stakes.rs
    static_ids.rs
    status_cache.rs
    transaction_batch.rs
    vote_sender_types.rs
  .gitignore
  Cargo.toml
runtime-transaction/
  benches/
    get_signature_details.rs
  src/
    runtime_transaction/
      sdk_transactions.rs
      transaction_view.rs
    instruction_data_len.rs
    instruction_meta.rs
    lib.rs
    runtime_transaction.rs
    signature_details.rs
    transaction_meta.rs
    transaction_with_meta.rs
  Cargo.toml
scheduler-bindings/
  src/
    lib.rs
  Cargo.toml
scheduling-utils/
  src/
    handshake/
      client.rs
      mod.rs
      server.rs
      shared.rs
      tests.rs
    error.rs
    lib.rs
    pubkeys_ptr.rs
    responses_region.rs
    thread_aware_account_locks.rs
    transaction_ptr.rs
  Cargo.toml
scripts/
  agave-build-lists.sh
  agave-install-deploy.sh
  agave-install-update-manifest-keypair.sh
  build-agave-xdp-ebpf.sh
  build-downstream-anchor-projects.sh
  cargo-clippy-nightly.sh
  cargo-clippy.sh
  cargo-for-all-lock-files.sh
  cargo-install-all.sh
  check-dev-context-only-utils.sh
  configure-metrics.sh
  confirm-cargo-version-numbers-before-bump.sh
  coverage.sh
  create-release-tarball.sh
  elf-hash-symbol.sh
  fd-monitor.sh
  generate-target-triple.sh
  iftop.sh
  increment-cargo-version.sh
  metrics-write-datapoint.sh
  net-shaper.sh
  net-stats.sh
  oom-monitor.sh
  oom-score-adj.sh
  patch-crates.sh
  patch-spl-crates-for-anchor.sh
  perf-plot.py
  perf-stats.py
  read-cargo-variable.sh
  reserve-cratesio-package-name.sh
  run.sh
  sed-i-all-rs-files-for-rust-analyzer.sh
  spl-token-cli-version.sh
  system-stats.sh
  ulimit-n.sh
  wallet-sanity.sh
sdk/
  README.md
send-transaction-service/
  src/
    lib.rs
    send_transaction_service_stats.rs
    send_transaction_service.rs
    test_utils.rs
    tpu_info.rs
    transaction_client.rs
  Cargo.toml
snapshots/
  src/
    archive_format.rs
    archive.rs
    error.rs
    hardened_unpack.rs
    kind.rs
    lib.rs
    paths.rs
    snapshot_archive_info.rs
    snapshot_config.rs
    snapshot_hash.rs
    snapshot_interval.rs
    snapshot_version.rs
    unarchive.rs
  Cargo.toml
stake-accounts/
  src/
    arg_parser.rs
    args.rs
    main.rs
    stake_accounts.rs
  Cargo.toml
storage-bigtable/
  build-proto/
    src/
      main.rs
    .gitignore
    build.sh
    Cargo.toml
    README.md
  proto/
    google.api.rs
    google.bigtable.v2.rs
    google.protobuf.rs
    google.rpc.rs
  src/
    access_token.rs
    bigtable.rs
    compression.rs
    lib.rs
    pki-goog-roots.pem
    root_ca_certificate.rs
  Cargo.toml
  init-bigtable.sh
  README.md
storage-proto/
  proto/
    confirmed_block.proto
    entries.proto
    transaction_by_addr.proto
  src/
    convert.rs
    lib.rs
  build.rs
  Cargo.toml
  README.md
streamer/
  examples/
    swqos.rs
  src/
    nonblocking/
      connection_rate_limiter.rs
      mod.rs
      qos.rs
      quic.rs
      recvmmsg.rs
      sendmmsg.rs
      simple_qos.rs
      stream_throttle.rs
      swqos.rs
      testing_utilities.rs
    evicting_sender.rs
    lib.rs
    msghdr.rs
    packet.rs
    quic.rs
    recvmmsg.rs
    sendmmsg.rs
    streamer.rs
  tests/
    recvmmsg.rs
  Cargo.toml
svm/
  doc/
    diagrams/
      context.svg
      context.tex
    spec.md
  src/
    account_loader.rs
    account_overrides.rs
    lib.rs
    message_processor.rs
    nonce_info.rs
    program_loader.rs
    rent_calculator.rs
    rollback_accounts.rs
    transaction_account_state_info.rs
    transaction_balances.rs
    transaction_commit_result.rs
    transaction_error_metrics.rs
    transaction_execution_result.rs
    transaction_processing_callback.rs
    transaction_processing_result.rs
    transaction_processor.rs
  tests/
    example-programs/
      clock-sysvar/
        src/
          lib.rs
        Cargo.toml
        clock_sysvar_program.so
      hello-solana/
        src/
          lib.rs
        Cargo.toml
        hello_solana_program.so
      simple-transfer/
        src/
          lib.rs
        Cargo.toml
        simple_transfer_program.so
      transfer-from-account/
        src/
          lib.rs
        Cargo.toml
        transfer_from_account_program.so
      write-to-account/
        src/
          lib.rs
        Cargo.toml
        write_to_account_program.so
    concurrent_tests.rs
    integration_test.rs
    mock_bank.rs
  Cargo.toml
svm-callback/
  src/
    lib.rs
  Cargo.toml
svm-feature-set/
  src/
    lib.rs
  Cargo.toml
svm-log-collector/
  src/
    lib.rs
  Cargo.toml
svm-measure/
  src/
    lib.rs
    macros.rs
    measure.rs
  Cargo.toml
svm-rent-calculator/
  src/
    lib.rs
    rent_state.rs
    svm_rent_calculator.rs
  Cargo.toml
svm-test-harness/
  bin/
    test_exec_instr.rs
  src/
    fixture/
      account_state.rs
      error.rs
      feature_set.rs
      instr_context.rs
      instr_effects.rs
      mod.rs
    file.rs
    fuzz.rs
    instr.rs
    lib.rs
    program_cache.rs
    sysvar_cache.rs
  .gitignore
  build.rs
  Cargo.toml
  Makefile
svm-timings/
  src/
    lib.rs
  Cargo.toml
svm-transaction/
  src/
    svm_message/
      sanitized_message.rs
      sanitized_transaction.rs
    svm_transaction/
      sanitized_transaction.rs
    instruction.rs
    lib.rs
    message_address_table_lookup.rs
    svm_message.rs
    svm_transaction.rs
    tests.rs
  Cargo.toml
svm-type-overrides/
  src/
    lib.rs
  Cargo.toml
syscalls/
  gen-syscall-list/
    src/
      main.rs
    build.rs
    Cargo.toml
  src/
    cpi.rs
    lib.rs
    logging.rs
    mem_ops.rs
    sysvar.rs
  Cargo.toml
test-validator/
  src/
    lib.rs
  Cargo.toml
thread-manager/
  examples/
    common/
      mod.rs
    core_contention_basics.rs
    core_contention_contending_set.toml
    core_contention_dedicated_set.toml
    core_contention_sweep.rs
  src/
    lib.rs
    native_thread_runtime.rs
    policy.rs
    rayon_runtime.rs
    tokio_runtime.rs
  Cargo.toml
  README.md
tls-utils/
  src/
    config.rs
    crypto_provider.rs
    lib.rs
    quic_client_certificate.rs
    skip_client_verification.rs
    skip_server_verification.rs
    tls_certificates.rs
  Cargo.toml
  README
tokens/
  src/
    arg_parser.rs
    args.rs
    commands.rs
    db.rs
    lib.rs
    main.rs
    spl_token.rs
    stake.rs
    token_display.rs
  tests/
    commands.rs
  .gitignore
  Cargo.toml
  README.md
tps-client/
  src/
    bank_client.rs
    lib.rs
    rpc_client.rs
    tpu_client.rs
    utils.rs
  Cargo.toml
tpu-client/
  src/
    nonblocking/
      mod.rs
      tpu_client.rs
    lib.rs
    tpu_client.rs
  .gitignore
  Cargo.toml
tpu-client-next/
  src/
    node_address_service/
      leader_tpu_cache_service.rs
      recent_leader_slots.rs
      slot_event.rs
      slot_receiver.rs
      slot_update_service.rs
    quic_networking/
      error.rs
    client_builder.rs
    connection_worker.rs
    connection_workers_scheduler.rs
    leader_updater.rs
    lib.rs
    logging.rs
    metrics.rs
    node_address_service.rs
    quic_networking.rs
    send_transaction_stats.rs
    transaction_batch.rs
    websocket_node_address_service.rs
    workers_cache.rs
  tests/
    connection_workers_scheduler_test.rs
  Cargo.toml
transaction-context/
  src/
    instruction_accounts.rs
    instruction.rs
    lib.rs
    transaction_accounts.rs
    vm_slice.rs
  Cargo.toml
transaction-dos/
  src/
    main.rs
  .gitignore
  Cargo.toml
transaction-metrics-tracker/
  src/
    lib.rs
  Cargo.toml
transaction-status/
  benches/
    extract_memos.rs
  src/
    parse_token/
      extension/
        confidential_mint_burn.rs
        confidential_transfer_fee.rs
        confidential_transfer.rs
        cpi_guard.rs
        default_account_state.rs
        group_member_pointer.rs
        group_pointer.rs
        interest_bearing_mint.rs
        memo_transfer.rs
        metadata_pointer.rs
        mint_close_authority.rs
        mod.rs
        pausable.rs
        permanent_delegate.rs
        reallocate.rs
        scaled_ui_amount.rs
        token_group.rs
        token_metadata.rs
        transfer_fee.rs
        transfer_hook.rs
    extract_memos.rs
    lib.rs
    parse_accounts.rs
    parse_address_lookup_table.rs
    parse_associated_token.rs
    parse_bpf_loader.rs
    parse_instruction.rs
    parse_stake.rs
    parse_system.rs
    parse_token.rs
    parse_vote.rs
    token_balances.rs
  Cargo.toml
transaction-status-client-types/
  src/
    lib.rs
    option_serializer.rs
  Cargo.toml
transaction-view/
  benches/
    bytes.rs
    transaction_view.rs
  src/
    address_table_lookup_frame.rs
    bytes.rs
    instructions_frame.rs
    lib.rs
    message_header_frame.rs
    resolved_transaction_view.rs
    result.rs
    sanitize.rs
    signature_frame.rs
    static_account_keys_frame.rs
    transaction_data.rs
    transaction_frame.rs
    transaction_version.rs
    transaction_view.rs
  Cargo.toml
turbine/
  benches/
    cluster_info.rs
    cluster_nodes.rs
  src/
    broadcast_stage/
      broadcast_duplicates_run.rs
      broadcast_fake_shreds_run.rs
      broadcast_metrics.rs
      broadcast_utils.rs
      fail_entry_verification_broadcast_run.rs
      standard_broadcast_run.rs
    addr_cache.rs
    broadcast_stage.rs
    cluster_nodes.rs
    lib.rs
    quic_endpoint.rs
    retransmit_stage.rs
    sigverify_shreds.rs
    xdp.rs
  Cargo.toml
udp-client/
  src/
    nonblocking/
      mod.rs
      udp_client.rs
    lib.rs
    udp_client.rs
  Cargo.toml
unified-scheduler-logic/
  src/
    lib.rs
  Cargo.toml
unified-scheduler-pool/
  src/
    lib.rs
    sleepless_testing.rs
  Cargo.toml
validator/
  src/
    bin/
      solana-test-validator.rs
    cli/
      thread_args.rs
    commands/
      authorized_voter/
        mod.rs
      bam/
        mod.rs
      block_engine/
        mod.rs
      contact_info/
        mod.rs
      exit/
        mod.rs
      manage_block_production/
        mod.rs
      monitor/
        mod.rs
      plugin/
        mod.rs
      relayer/
        mod.rs
      repair_shred_from_peer/
        mod.rs
      repair_whitelist/
        mod.rs
      run/
        args/
          account_secondary_indexes.rs
          blockstore_options.rs
          json_rpc_config.rs
          pub_sub_config.rs
          rpc_bigtable_config.rs
          rpc_bootstrap_config.rs
          send_transaction_config.rs
        args.rs
        execute.rs
        mod.rs
      set_identity/
        mod.rs
      set_log_filter/
        mod.rs
      set_public_address/
        mod.rs
      shred/
        mod.rs
      staked_nodes_overrides/
        mod.rs
      wait_for_restart_window/
        mod.rs
      mod.rs
    admin_rpc_service.rs
    bootstrap.rs
    cli.rs
    dashboard.rs
    lib.rs
    main.rs
  tests/
    cli.rs
  .gitignore
  Cargo.toml
  solana-test-validator
verified-packet-receiver/
  src/
    lib.rs
    receiver.rs
  Cargo.toml
  Readme.md
version/
  src/
    legacy.rs
    lib.rs
  .gitignore
  build.rs
  Cargo.toml
vortexor/
  src/
    cli.rs
    lib.rs
    main.rs
    rpc_load_balancer.rs
    sender.rs
    stake_updater.rs
    vortexor.rs
  tests/
    vortexor.rs
  Cargo.toml
  README.md
vote/
  benches/
    vote_account.rs
  src/
    vote_state_view/
      field_frames.rs
      frame_v1_14_11.rs
      frame_v3.rs
      frame_v4.rs
      list_view.rs
    lib.rs
    vote_account.rs
    vote_parser.rs
    vote_state_view.rs
    vote_transaction.rs
  Cargo.toml
votor/
  src/
    consensus_pool/
      certificate_builder.rs
      parent_ready_tracker.rs
      slot_stake_counters.rs
      stats.rs
      vote_pool.rs
    consensus_pool_service/
      stats.rs
    event_handler/
      stats.rs
    timer_manager/
      stats.rs
      timers.rs
    commitment.rs
    common.rs
    consensus_metrics.rs
    consensus_pool_service.rs
    consensus_pool.rs
    event_handler.rs
    event.rs
    lib.rs
    root_utils.rs
    staked_validators_cache.rs
    timer_manager.rs
    vote_history_storage.rs
    vote_history.rs
    voting_service.rs
    voting_utils.rs
    votor.rs
  Cargo.toml
votor-messages/
  src/
    consensus_message.rs
    lib.rs
    vote.rs
  Cargo.toml
watchtower/
  src/
    main.rs
  .gitignore
  Cargo.toml
  README.md
web3.js/
  README.md
wen-restart/
  proto/
    wen_restart.proto
  src/
    heaviest_fork_aggregate.rs
    last_voted_fork_slots_aggregate.rs
    lib.rs
    wen_restart.rs
  build.rs
  Cargo.toml
xdp/
  src/
    device.rs
    lib.rs
    netlink.rs
    packet.rs
    program.rs
    route.rs
    socket.rs
    tx_loop.rs
    umem.rs
  Cargo.toml
xdp-ebpf/
  src/
    bin/
      agave-xdp-prog.rs
    lib.rs
  agave-xdp-prog
  Cargo.toml
  README
zk-keygen/
  README.md
zk-sdk/
  README.md
zk-token-sdk/
  src/
    encryption/
      auth_encryption.rs
      decode_u32_precomputation_for_G.bincode
      discrete_log.rs
      elgamal.rs
      grouped_elgamal.rs
      mod.rs
      pedersen.rs
    instruction/
      batched_grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      batched_range_proof/
        batched_range_proof_u128.rs
        batched_range_proof_u256.rs
        batched_range_proof_u64.rs
        mod.rs
      grouped_ciphertext_validity/
        handles_2.rs
        handles_3.rs
        mod.rs
      transfer/
        encryption.rs
        mod.rs
        with_fee.rs
        without_fee.rs
      ciphertext_ciphertext_equality.rs
      ciphertext_commitment_equality.rs
      errors.rs
      fee_sigma.rs
      mod.rs
      pubkey_validity.rs
      range_proof.rs
      withdraw.rs
      zero_balance.rs
    range_proof/
      errors.rs
      generators.rs
      inner_product.rs
      mod.rs
      util.rs
    sigma_proofs/
      batched_grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      grouped_ciphertext_validity_proof/
        handles_2.rs
        handles_3.rs
        mod.rs
      ciphertext_ciphertext_equality_proof.rs
      ciphertext_commitment_equality_proof.rs
      errors.rs
      fee_proof.rs
      mod.rs
      pubkey_proof.rs
      zero_balance_proof.rs
    zk_token_elgamal/
      pod/
        auth_encryption.rs
        elgamal.rs
        grouped_elgamal.rs
        instruction.rs
        mod.rs
        pedersen.rs
        range_proof.rs
        sigma_proofs.rs
      convert.rs
      decryption.rs
      mod.rs
      ops.rs
    errors.rs
    lib.rs
    macros.rs
    transcript.rs
    zk_token_proof_instruction.rs
    zk_token_proof_program.rs
    zk_token_proof_state.rs
  .gitignore
  Cargo.toml
  README.md
.codecov.yml
.dockerignore
.gitignore
.gitmodules
.mergify.yml
bootstrap
cargo
cargo-build-sbf
cargo-test-sbf
Cargo.toml
CHANGELOG.md
clippy.toml
CONTRIBUTING.md
deploy_programs
f
fetch-core-bpf.sh
fetch-perf-libs.sh
fetch-programs.sh
fetch-spl.sh
legal.md
LICENSE
privacy.md
README.md
RELEASE.md
rust-toolchain.toml
rustfmt.toml
s
SECURITY.md
start
start_multi
vercel.json

================================================================
Files
================================================================

================
File: .buildkite/hooks/post-checkout
================
#!/usr/bin/env bash

CI_BUILD_START=$(date +%s)
export CI_BUILD_START

source ci/env.sh

#
# Kill any running docker containers, which are potentially left over from the
# previous CI job
#
(
  echo "+++ Killing stale docker containers"
  while read -r line; do
    read -r id image _ <<<"$line"

    if [[ $image =~ "anzaxyz/ci" ]]; then
      if docker kill "$id" >/dev/null; then
        echo "kill $id $image"
      fi
    fi

  done < <(docker ps | tail -n +2)
)

# Processes from previously aborted CI jobs seem to loiter, unclear why as one
# would expect the buildkite-agent to clean up all child processes of the
# aborted CI job.
# But as a workaround for now manually kill some known loiterers.  These
# processes will all have the `init` process as their PPID:
(
  victims=
  for name in bash cargo docker solana; do
    victims="$victims $(pgrep -u "$(id -u)" -P 1 -d \  $name)"
  done
  for victim in $victims; do
    echo "Killing pid $victim"
    kill -9 "$victim" || true
  done
)

================
File: .buildkite/hooks/post-command
================
#!/usr/bin/env bash
set -e

source ci/upload-ci-artifact.sh

#shellcheck source=ci/common/shared-functions.sh
source ci/common/shared-functions.sh

#
# Add job_stats data point
#
if [[ -z $CI_BUILD_START ]]; then
  echo Error: CI_BUILD_START empty
else
  # make sure console outputs are uploaded as soon as possible, if any
  (
    shopt -s nullglob
    for console_log in ./intercepted-console-*; do
      gzip -f "$console_log"
      upload-ci-artifact "$console_log.gz"
    done
  )

  CI_BUILD_DURATION=$(( $(date +%s) - CI_BUILD_START + 1 ))

  CI_LABEL=${BUILDKITE_LABEL:-build label missing}

  PR=false
  if [[ $BUILDKITE_BRANCH =~ pull/* ]]; then
    PR=true
  fi

  SUCCESS=true
  if [[ $BUILDKITE_COMMAND_EXIT_STATUS != 0 ]]; then
    SUCCESS=false
  fi

  if [[ -n $BUILDKITE ]]; then
    if need_to_upload_test_result; then
      if [[ -f "results.json" ]]; then
        # extract lines which start with '{'
        awk '/{.*/' results.json >sanitized-results.json

        if [[ -n "$BUILDKITE_ANALYTICS_TOKEN" ]]; then
          echo "~~~ Uploading test results to Buildkite Analytics"
          buildkite-test-collector <sanitized-results.json
        else
          echo "~~~ Ignore uploading to Buildkite Analytics"
        fi

        if [[ -n "$DATADOG_API_KEY" ]]; then
          echo "~~~ Uploading test results to Datadog"
          cargo2junit >results.xml <sanitized-results.json || true
          if [[ -f "results.xml" ]]; then
            datadog-ci junit upload --service solana results.xml
          fi
        else
          echo "~~~ Ignore Uploading to Datadog"
        fi
      fi

      if [[ -f "target/nextest/ci/junit.xml" ]]; then
        if [[ -n "$BUILDKITE_ANALYTICS_TOKEN" ]]; then
          echo "~~~ Uploading test results to Buildkite Analytics"
          curl \
            -X POST \
            -H "Authorization: Token token=\"$BUILDKITE_ANALYTICS_TOKEN\"" \
            -F "data=@target/nextest/ci/junit.xml" \
            -F "format=junit" \
            -F "run_env[CI]=buildkite" \
            -F "run_env[key]=$BUILDKITE_BUILD_ID" \
            -F "run_env[url]=$BUILDKITE_BUILD_URL" \
            -F "run_env[branch]=$BUILDKITE_BRANCH" \
            -F "run_env[commit_sha]=$BUILDKITE_COMMIT" \
            -F "run_env[number]=$BUILDKITE_BUILD_NUMBER" \
            -F "run_env[job_id]=$BUILDKITE_JOB_ID" \
            -F "run_env[message]=$BUILDKITE_MESSAGE" \
            https://analytics-api.buildkite.com/v1/uploads
          echo # add a break line for previous command
        else
          echo "~~~ Ignore uploading to Buildkite Analytics"
        fi

        if [[ -n "$DATADOG_API_KEY" ]]; then
          echo "~~~ Uploading test results to Datadog"
          datadog-ci junit upload --service solana target/nextest/ci/junit.xml
        else
          echo "~~~ Ignore Uploading to Datadog"
        fi
      fi
    fi
  fi

  point_tags="pipeline=$BUILDKITE_PIPELINE_SLUG,job=$CI_LABEL,pr=$PR,success=$SUCCESS"
  point_tags="${point_tags// /\\ }"  # Escape spaces

  point_fields="duration=$CI_BUILD_DURATION"
  point_fields="${point_fields// /\\ }"  # Escape spaces

  point="job_stats,$point_tags $point_fields"

  scripts/metrics-write-datapoint.sh "$point" || true
fi

================
File: .buildkite/hooks/pre-command
================
#!/usr/bin/env bash
set -e

# Ensure the pattern "+++ ..." never occurs when |set -x| is set, as buildkite
# interprets this as the start of a log group.
# Ref: https://buildkite.com/docs/pipelines/managing-log-output
export PS4="++"

#
# Restore target/ from the previous CI build on this machine
#
eval "$(ci/channel-info.sh)"
export EDGE_CHANNEL
export BETA_CHANNEL
export BETA_CHANNEL_LATEST_TAG
export STABLE_CHANNEL
export STABLE_CHANNEL_LATEST_TAG
export CHANNEL
export CHANNEL_LATEST_TAG

eval "$(ci/platform-tools-info.sh)"
source "ci/rust-version.sh"
HOST_RUST_VERSION="$rust_stable"
pattern='^[0-9]+\.[0-9]+\.[0-9]+$'
if [[ ${HOST_RUST_VERSION} =~ ${pattern} ]]; then
    HOST_RUST_VERSION="${rust_stable%.*}"
fi

export SBF_TOOLS_VERSION

SCCACHE_KEY_PREFIX="${rust_stable}_${rust_nightly}_${SBF_TOOLS_VERSION}"
export SCCACHE_KEY_PREFIX

SCCACHE_S3_KEY_PREFIX="$SCCACHE_KEY_PREFIX"
export SCCACHE_S3_KEY_PREFIX

SCCACHE_GCS_KEY_PREFIX="$SCCACHE_KEY_PREFIX"
export SCCACHE_GCS_KEY_PREFIX

================
File: .buildkite/scripts/build-stable.sh
================
set -e
here=$(dirname "$0")
source "$here"/common.sh
agent="${1-solana}"
parallelism=5
partitions=()
for i in $(seq 1 $parallelism); do
  partitions+=("$(
    cat <<EOF
{
  "name": "partition-$i",
  "command": "ci/docker-run-default-image.sh ci/stable/run-partition.sh $i $parallelism",
  "timeout_in_minutes": 25,
  "agent": "$agent",
  "retry": 3
}
EOF
  )")
done
# add dev-bins
partitions+=(
  "$(
    cat <<EOF
{
  "name": "dev-bins",
  "command": "ci/docker-run-default-image.sh cargo nextest run --profile ci --manifest-path ./dev-bins/Cargo.toml",
  "timeout_in_minutes": 35,
  "agent": "$agent"
}
EOF
  )")
parallelism=10
local_cluster_partitions=()
for i in $(seq 1 $parallelism); do
  local_cluster_partitions+=("$(
    cat <<EOF
{
  "name": "local-cluster-$i",
  "command": "ci/docker-run-default-image.sh ci/stable/run-local-cluster-partially.sh $i $parallelism",
  "timeout_in_minutes": 15,
  "agent": "$agent",
  "retry": 3
}
EOF
  )")
done
localnet=$(
  cat <<EOF
{
  "name": "localnet",
  "command": "ci/docker-run-default-image.sh ci/stable/run-localnet.sh",
  "timeout_in_minutes": 30,
  "agent": "$agent"
}
EOF
)
group "stable" "${partitions[@]}"
group "local-cluster" "${local_cluster_partitions[@]}"
group "localnet" "$localnet"

================
File: .buildkite/scripts/build-stable.test.sh
================
set -e
here=$(dirname "$0")
source "$here"/func-assert-eq.sh
want=$(
  cat <<'EOF'
  - group: "stable"
    steps:
      - name: "partitions"
        command: ". ci/rust-version.sh; ci/docker-run.sh $$rust_stable_docker_image ci/stable/run-partition.sh"
        timeout_in_minutes: 30
        agents:
          queue: "solana"
        parallelism: 3
        retry:
          automatic:
            - limit: 3
      - name: "localnet"
        command: ". ci/rust-version.sh; ci/docker-run.sh $$rust_stable_docker_image ci/stable/run-localnet.sh"
        timeout_in_minutes: 30
        agents:
          queue: "solana"
EOF
)
got=$(source "$here"/build-stable.sh)
assert_eq "test build stable steps" "$want" "$got"

================
File: .buildkite/scripts/common.sh
================
export INDENT_LEVEL=2
indent() {
  local indent=${1:-"$INDENT_LEVEL"}
  sed "s/^/$(printf ' %.0s' $(seq 1 "$indent"))/"
}
group() {
  # shellcheck disable=SC2016 # don't want these expressions expanded
  local name="${1:?'buildkite `group` generator requires a `name`'}"
  if [[ $# -lt 2 ]]; then
    echo "no steps provided for buildkite group \`$name\`, omitting from pipeline" 1>&2
    return
  fi
  cat <<EOF | indent
- group: "$name"
  steps:
EOF
  shift
  INDENT_LEVEL=$((INDENT_LEVEL + 4))
  for params in "$@"; do
    step "$params"
  done
  INDENT_LEVEL=$((INDENT_LEVEL - 4))
}
step() {
  local params="$1"
  local name
  name="$(echo "$params" | jq -r '.name')"
  local command
  command="$(echo "$params" | jq -r '.command')"
  local timeout_in_minutes
  timeout_in_minutes="$(echo "$params" | jq -r '.timeout_in_minutes')"
  local agent
  agent="$(echo "$params" | jq -r '.agent')"
  local parallelism
  parallelism="$(echo "$params" | jq -r '.parallelism')"
  maybe_parallelism="DELETE_THIS_LINE"
  if [ "$parallelism" != "null" ]; then
    maybe_parallelism=$(
      cat <<EOF | indent 2
parallelism: $parallelism
EOF
    )
  fi
  local retry
  retry="$(echo "$params" | jq -r '.retry')"
  maybe_retry="DELETE_THIS_LINE"
  if [ "$retry" != "null" ]; then
    maybe_retry=$(
      cat <<EOF | indent 2
retry:
  automatic:
    - limit: $retry
EOF
    )
  fi
  cat <<EOF | indent | sed '/DELETE_THIS_LINE/d'
- name: "$name"
  command: "$command"
  timeout_in_minutes: $timeout_in_minutes
  agents:
    queue: "$agent"
$maybe_parallelism
$maybe_retry
EOF
}

================
File: .buildkite/scripts/common.test.sh
================
set -e
here=$(dirname "$0")
source "$here"/func-assert-eq.sh
source "$here"/common.sh
(
  want=$(
    cat <<EOF | indent
- name: "test"
  command: "start.sh"
  timeout_in_minutes: 10
  agents:
    queue: "agent"
EOF
  )
  got=$(step '{ "name": "test", "command": "start.sh", "timeout_in_minutes": 10, "agent": "agent"}')
  assert_eq "basic setup" "$want" "$got"
)
(
  want=$(
    cat <<EOF | indent
- name: "test"
  command: "start.sh"
  timeout_in_minutes: 10
  agents:
    queue: "agent"
  parallelism: 3
EOF
  )
  got=$(step '{ "name": "test", "command": "start.sh", "timeout_in_minutes": 10, "agent": "agent", "parallelism": 3}')
  assert_eq "basic setup + parallelism" "$want" "$got"
)
(
  want=$(
    cat <<EOF | indent
- name: "test"
  command: "start.sh"
  timeout_in_minutes: 10
  agents:
    queue: "agent"
  retry:
    automatic:
      - limit: 3
EOF
  )
  got=$(step '{ "name": "test", "command": "start.sh", "timeout_in_minutes": 10, "agent": "agent", "retry": 3}')
  assert_eq "basic setup + retry" "$want" "$got"
)
(
  want=$(
    cat <<EOF | indent
- name: "test"
  command: "start.sh"
  timeout_in_minutes: 10
  agents:
    queue: "agent"
  parallelism: 3
  retry:
    automatic:
      - limit: 3
EOF
  )
  got=$(step '{ "name": "test", "command": "start.sh", "timeout_in_minutes": 10, "agent": "agent", "parallelism": 3, "retry": 3}')
  assert_eq "basic setup + parallelism + retry" "$want" "$got"
)

================
File: .buildkite/scripts/func-assert-eq.sh
================
assert_eq() {
  local test_name=$1
  local want=$2
  local got=$3
  if [[ "$want" = "$got" ]]; then
    echo "✅ $test_name"
  else
    cat <<EOF
❌ $test_name
$(diff -u <(echo "$want") <(echo "$got"))
EOF
  fi
}

================
File: .buildkite/scripts/test-all.sh
================
here=$(dirname "$0")
find "$here" -name '*.test.sh' -exec {} \;

================
File: .buildkite/scripts/trigger-github-actions-windows-build.sh
================
set -e
echo "branch: $BUILDKITE_BRANCH"
echo "commit: $BUILDKITE_COMMIT"
curl -L \
  -X POST \
  -H "Accept: application/vnd.github+json" \
  -H "Authorization: Bearer $GITHUB_TOKEN" \
  -H "X-GitHub-Api-Version: 2022-11-28" \
  https://api.github.com/repos/anza-xyz/agave/actions/workflows/publish-windows-tarball.yml/dispatches \
  -d '{"ref":"'"$BUILDKITE_BRANCH"'","inputs":{"commit":"'"$BUILDKITE_COMMIT"'"}}'

================
File: .buildkite/pipeline-upload.sh
================
set -e
cd "$(dirname "$0")"/..
source ci/_
if [[ $BUILDKITE_BRANCH == gh-readonly-queue* ]]; then
  # github merge queue
  cat <<EOF | tee /dev/tty | buildkite-agent pipeline upload
priority: 10
steps:
  - name: "sanity"
    command: "ci/docker-run-default-image.sh ci/test-sanity.sh"
    timeout_in_minutes: 5
    agents:
      queue: "check"
  - name: "checks"
    command: "ci/docker-run-default-image.sh ci/test-checks.sh"
    timeout_in_minutes: 30
    agents:
      queue: "check"
EOF
else
  _ ci/buildkite-pipeline.sh pipeline.yml
  echo +++ pipeline
  cat pipeline.yml
  _ buildkite-agent pipeline upload pipeline.yml
fi

================
File: .buildkite/solana-private.sh
================
set -e
cd "$(dirname "$0")"/..
source ci/_
_ ci/buildkite-solana-private.sh pipeline.yml
echo +++ pipeline
cat pipeline.yml
_ buildkite-agent pipeline upload pipeline.yml

================
File: .codecov.yml
================
comment:
  behavior: default
  layout: diff
  require_changes: false
coverage:
  precision: 1
  range:
  - 50.0
  - 100.0
  round: down
  status:
    patch: false
    project: false
github_checks: false

================
File: .config/nextest.toml
================
[store]
dir = "target/nextest"

[test-groups]
build-sbf = { max-threads = 1 }

[profile.ci]
failure-output = "immediate-final"
slow-timeout = { period = "60s", terminate-after = 1 }
retries = { backoff = "fixed", count = 3, delay = "1s" }

[profile.ci.junit]
path = "junit.xml"

[[profile.ci.overrides]]
filter = "package(solana-zk-elgamal-proof-program-tests) & test(/^test_batched_range_proof_u256$/)"
threads-required = "num-cpus"

[[profile.ci.overrides]]
filter = "package(solana-turbine) | package(solana-gossip) | package(solana-perf)"
retries = 0

[[profile.ci.overrides]]
filter = "package(solana-gossip) & test(/^test_star_network_push_star_200/)"
threads-required = "num-cpus"

[[profile.ci.overrides]]
filter = "package(solana-gossip) & test(/^test_star_network_push_ring_200/)"
threads-required = "num-cpus"

[[profile.ci.overrides]]
filter = "package(solana-gossip) & test(/^gossip_ring/)"
threads-required = "num-cpus"

[[profile.ci.overrides]]
filter = "package(solana-gossip) & test(/^cluster_info::tests::new_with_external_ip_test_random/)"
threads-required = "num-cpus"

[[profile.ci.overrides]]
filter = "package(solana-cargo-build-sbf)"
test-group = "build-sbf"

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_kill_partition_switch_threshold_progress$/)'
slow-timeout = { period = "60s", terminate-after = 10 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_kill_partition_switch_threshold_no_progress$/)'
slow-timeout = { period = "60s", terminate-after = 10 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_run_test_load_program_accounts_partition_root$/)'
slow-timeout = { period = "60s", terminate-after = 10 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_fork_choice_refresh_old_votes$/)'
slow-timeout = { period = "60s", terminate-after = 5 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_cluster_partition_1_1_1$/)'
slow-timeout = { period = "60s", terminate-after = 4 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_wait_for_max_stake$/)'
slow-timeout = { period = "60s", terminate-after = 4 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_snapshots_restart_validity$/)'
slow-timeout = { period = "60s", terminate-after = 4 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_slot_hash_expiry$/)'
slow-timeout = { period = "60s", terminate-after = 4 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_kill_heaviest_partition$/)'
slow-timeout = { period = "60s", terminate-after = 4 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster) & test(/^test_boot_from_local_state$/)'
slow-timeout = { period = "60s", terminate-after = 3 }

[[profile.ci.overrides]]
filter = 'package(solana-local-cluster)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.ci.overrides]]
filter = 'package(solana-cargo-build-sbf)'
slow-timeout = { period = "60s", terminate-after = 5 }

[[profile.ci.overrides]]
filter = 'package(solana-ledger) & test(/^shred::merkle::test::test_merkle_tree_round_trip/)'
slow-timeout = { period = "60s", terminate-after = 3 }

[[profile.ci.overrides]]
filter = 'package(solana-core) & test(/^banking_stage::consumer::tests::test_bank_nonce_update_blockhash_queried_before_transaction_record/)'
slow-timeout = { period = "60s", terminate-after = 8 }

[[profile.ci.overrides]]
filter = 'package(solana-core) & test(/^banking_stage::consumer::tests::test_bank_process_and_record_transactions/)'
slow-timeout = { period = "60s", terminate-after = 8 }

[[profile.ci.overrides]]
filter = 'package(solana-core) & test(/^test_slots_to_snapshot/)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.ci.overrides]]
filter = 'package(solana-core) & test(/^test_snapshots_have_expected_epoch_accounts_hash/)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.ci.overrides]]
filter = 'package(solana-client-test) & test(/^test_send_and_confirm_transactions_in_parallel_with_tpu_client/)'
slow-timeout = { period = "60s", terminate-after = 3 }

[[profile.ci.overrides]]
filter = 'package(solana-runtime) & test(/^bank_forks::tests::test_bank_forks_new_rw_arc_memory_leak/)'
slow-timeout = { period = "60s", terminate-after = 3 }

[[profile.ci.overrides]]
filter = 'package(solana-runtime) & test(/^bank::tests::test_race_register_tick_freeze/)'
slow-timeout = { period = "60s", terminate-after = 3 }

[[profile.ci.overrides]]
filter = 'package(solana-svm) & test(/^execute_fixtures/)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.ci.overrides]]
filter = "package(solana-accounts-cluster-bench)"
threads-required = "num-cpus"

================
File: .dockerignore
================
.dockerignore
.git/
.github/
.gitignore
.idea/
README.md
Dockerfile
f
target/

================
File: .github/ISSUE_TEMPLATE/0-community.md
================
---
name: Community Issue
about: Create a report describing a problem and a proposed solution
title: ''
labels: ["community"]
assignees: ''
---

#### Problem
<!--
  The GitHub issue tracker exists to track issues
  that affect the development of Solana itself.

  If you need technical support using Solana, building
  an app, or running a validator, don't open an issue here.

  Instead, post your question to the Solana Stack Exchange:
  https://solana.stackexchange.com/questions/ask
-->
<!-- If reporting a crash, degraded performance, etc, please include the software version(s) you are using. -->

#### Proposed Solution

================
File: .github/ISSUE_TEMPLATE/1-core-contributor.md
================
---
name: Core Contributor Issue
about: Create a report describing a problem and a proposed solution
title: ''
assignees: ''
---

#### Problem
<!--
  This template should only be used by core contributors. If you
  are not a core contributor, please use the "Community Issue" template
  to ensure that your issue can be triaged appropriately.
-->

#### Proposed Solution

================
File: .github/ISSUE_TEMPLATE/2-feature-gate.yml
================
name: Feature Gate Tracker
description: Track the development and status of an on-chain feature
title: "Feature Gate: "
labels: ["feature-gate"]
body:
  - type: markdown
    attributes:
      value: >
        Steps to add a new feature are outlined below. Note that these steps only cover
        the process of getting a feature into the core Solana code.
        - For features that are unambiguously good (ie bug fixes), these steps are sufficient.
        - For features that should go up for community vote (ie fee structure changes), more
        information on the additional steps to follow can be found at:
        <https://spl.solana.com/feature-proposal
        1. Generate a new keypair with `solana-keygen new --outfile feature.json --no-passphrase`
            - Keypairs should be held by core contributors only. If you're a non-core contirbutor going
            through these steps, the PR process will facilitate a keypair holder being picked. That
            person will generate the keypair, provide pubkey for PR, and ultimately enable the feature.
        2. Add a public module for the feature, specifying keypair pubkey as the id with
        `solana_sdk_macro::declare_id!()` within the module. Additionally, add an entry to `FEATURE_NAMES` map.
        3. Add desired logic to check for and switch on feature availability.
  - type: input
    id: simd
    attributes:
      label: SIMD
      description: Solana IMprovement Document (SIMD)
      placeholder: Link to the https://github.com/solana-foundation/solana-improvement-documents document for this feature
    validations:
      required: true
  - type: textarea
    id: description
    attributes:
      label: Description
      placeholder: Describe why the new feature gate is needed and any necessary conditions for its activation
    validations:
      required: true
  - type: input
    id: id
    attributes:
      label: Feature ID
      description: The public key of the feature account
    validations:
      required: true
  - type: dropdown
    id: activation-method
    attributes:
      label: Activation Method
      options:
        - Single Core Contributor
        - Staked Validator Vote
    validations:
      required: true
  - type: textarea
    id: deployment
    attributes:
      label: Deployment Considerations
      placeholder: Describe any considerations for public-cluster deployment, including needed tests and metrics to be monitored
    validations:
      required: true
  - type: input
    id: beta-version
    attributes:
      label: Minimum Beta Version
      placeholder: Edit this response when feature has landed in a beta release
    validations:
      required: false
  - type: input
    id: stable-version
    attributes:
      label: Minimum Stable Version
      placeholder: Edit this response when feature has landed in a stable release
    validations:
      required: false
  - type: input
    id: testnet
    attributes:
      label: Testnet Activation Epoch
      placeholder: Edit this response when feature is activated on this cluster
    validations:
      required: false
  - type: input
    id: devnet
    attributes:
      label: Devnet Activation Epoch
      placeholder: Edit this response when feature is activated on this cluster
    validations:
      required: false
  - type: input
    id: mainnet-beta
    attributes:
      label: Mainnet-Beta Activation Epoch
      placeholder: Edit this response when feature is activated on this cluster
    validations:
      required: false

================
File: .github/scripts/add-team-to-ghsa.sh
================
set -euof pipefail
team_to_add_slug="security-incident-response"
github_org="anza-xyz"
github_repo="agave"
ghsa_json=$(gh api \
    -H "Accept: application/vnd.github+json" \
    -H "X-GitHub-Api-Version: 2022-11-28"   \
    /repos/$github_org/$github_repo/security-advisories?per_page=100 --paginate )
ghsa_without_team=$( jq -r '[ .[] | select(.state != "closed" and all(.collaborating_teams.[]; .slug != "'"$team_to_add_slug"'")) | .ghsa_id ] | sort | .[] ' <<< "$ghsa_json" )
if [[ -z $ghsa_without_team ]]; then
    echo "All GHSAs already have $team_to_add_slug. Exiting..."
    exit 0
fi
while IFS= read -r ghsa_id; do
    original_collaborating_team_slugs=$( jq -r '[ .[] | select(.ghsa_id == "'"$ghsa_id"'") | .collaborating_teams ] | "-f collaborating_teams[]=" + .[][].slug ' <<< "$ghsa_json" )
    gh api \
        --method PATCH \
        -H "Accept: application/vnd.github+json" \
        -H "X-GitHub-Api-Version: 2022-11-28" \
        "/repos/$github_org/$github_repo/security-advisories/$ghsa_id" \
        -f "collaborating_teams[]=$team_to_add_slug" $original_collaborating_team_slugs \
        > /dev/null 2>&1
done <<< "$ghsa_without_team"

================
File: .github/scripts/check-changelog.sh
================
set -uo pipefail
CHANGELOG_FILE="CHANGELOG.md"
echo "Checking: git diff --exit-code origin/${BASE_REF} -- ${CHANGELOG_FILE}"
if git diff --exit-code "origin/${BASE_REF}" -- "${CHANGELOG_FILE}"; then
    >&2 echo "Error: this pull request requires an entry in $CHANGELOG_FILE, but no entry was found"
    exit 1
fi

================
File: .github/scripts/downstream-project-spl-common.sh
================
set -e
here="$(dirname "${BASH_SOURCE[0]}")"
program="$1"
source "$here"/../../ci/downstream-projects/common.sh
set -x
rm -rf "${program}"
git clone https://github.com/solana-program/"${program}".git
cp "$SOLANA_DIR"/rust-toolchain.toml "${program}"/
cd "${program}" || exit 1
echo "HEAD: $(git rev-parse HEAD)"
project_used_solana_version=$(sed -nE 's/solana = \"(.*)\"/\1/p' <"Cargo.toml")
echo "used solana version: $project_used_solana_version"
if semverGT "$project_used_solana_version" "$SOLANA_VER"; then
  echo "skip"
  export SKIP_SPL_DOWNSTREAM_PROJECT_TEST=1
  return
fi
update_solana_dependencies . "$SOLANA_VER"
patch_crates_io_solana Cargo.toml "$SOLANA_DIR"

================
File: .github/scripts/downstream-project-spl-install-deps.sh
================
set -e
sudo apt-get update
sudo apt-get install libudev-dev libclang-dev protobuf-compiler -y

================
File: .github/scripts/install-all-deps.sh
================
set -e
here="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
os_name="$1"
source "$here/install-openssl.sh" "$os_name"
source "$here/install-proto.sh" "$os_name"
case "$os_name" in
"Windows") ;;
"macOS")
  brew install llvm
  LIBCLANG_PATH="$(brew --prefix llvm)/lib"
  export LIBCLANG_PATH
  ;;
"Linux")
  if grep "Alpine" /etc/os-release ; then
    apk update
    apk add \
      build-base \
      clang-libclang \
      eudev-dev \
      hidapi-dev \
      linux-headers \
      llvm-dev \
      musl-dev \
      perl
  else
    sudo apt update
    sudo apt install -y libclang-dev
  fi
  ;;
*)
  echo "Unknown Operating System"
  ;;
esac

================
File: .github/scripts/install-openssl.sh
================
set -e
os_name="$1"
case "$os_name" in
"Windows")
  cat > vcpkg.json <<EOL
{
  "dependencies": ["openssl"],
  "overrides": [
    {
      "name": "openssl",
      "version": "3.4.1"
    }
  ],
  "builtin-baseline": "5ee5eee0d3e9c6098b24d263e9099edcdcef6631"
}
EOL
  vcpkg install --triplet x64-windows-static-md
  rm vcpkg.json
  export OPENSSL_LIB_DIR="$PWD/vcpkg_installed/x64-windows-static-md/lib"
  export OPENSSL_INCLUDE_DIR="$PWD/vcpkg_installed/x64-windows-static-md/include"
  ;;
"macOS") ;;
"Linux")
  if grep "Alpine" /etc/os-release ; then
    apk add openssl-dev openssl-libs-static
  fi
  ;;
*)
  echo "Unknown Operating System"
  ;;
esac

================
File: .github/scripts/install-proto.sh
================
set -e
os_name="$1"
case "$os_name" in
"Windows")
  choco install protoc
  export PROTOC='C:\ProgramData\chocolatey\lib\protoc\tools\bin\protoc.exe'
  ;;
"macOS")
  brew install protobuf
  ;;
"Linux")
  if grep "Alpine" /etc/os-release ; then
    apk add protoc
  fi
  ;;
*)
  echo "Unknown Operating System"
  ;;
esac

================
File: .github/scripts/purge-ubuntu-runner.sh
================
sudo rm -rf /usr/share/dotnet
sudo rm -rf /usr/share/swift
sudo rm -rf /usr/share/mysql
sudo rm -rf /usr/share/az_*
sudo rm -rf /usr/share/postgresql-common
sudo rm -rf /opt/ghc
sudo rm -rf /opt/az
sudo rm -rf /opt/pipx
sudo rm -rf /opt/microsoft
sudo rm -rf /opt/google
sudo rm -rf /opt/hostedtoolcache
sudo rm -rf /usr/local/lib/android
sudo rm -rf /usr/local/lib/heroku

================
File: .github/workflows/add-team-to-ghsa.yml
================
name: Add Security Team to GHSAs
on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"
jobs:
  add-team-to-ghsa:
    if: github.repository == 'anza-xyz/agave'
    runs-on: ubuntu-24.04
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          ref: master
      - name: Run script
        shell: bash
        env:
          GH_TOKEN: ${{ secrets.GHSA_ADD_SECURITY_INCIDENT_RESPONSE }}
        run: |
          .github/scripts/add-team-to-ghsa.sh

================
File: .github/workflows/benchmark.yml
================
name: Benchmark
on:
  push:
    branches:
      - master
jobs:
  benchmark:
    if: github.repository == 'anza-xyz/agave'
    name: benchmark
    runs-on: benchmark
    strategy:
      fail-fast: false
      matrix:
        test:
          - {
              name: "solana-runtime",
              commands: ["cargo +$rust_nightly bench -p solana-runtime"],
            }
          - {
              name: "solana-gossip",
              commands: ["cargo bench -p solana-gossip -- --output-format bencher --noplot"],
            }
          - {
              name: "solana-poh",
              commands: ["cargo +$rust_nightly bench -p solana-poh"],
            }
          - {
              name: "solana-core",
              commands: ["cargo +$rust_nightly bench -p solana-core"],
            }
          - {
              name: "sbf",
              before_command: "make -C programs/sbf all",
              commands:
                [
                  "cargo +$rust_nightly bench --manifest-path programs/sbf/Cargo.toml --features=sbf_c",
                ],
            }
          - {
              name: "solana-accounts-db",
              commands:
                [
                  "cargo +$rust_nightly bench -p solana-accounts-db --bench accounts_index",
                  "cargo +$rust_nightly bench -p solana-accounts-db --bench accounts",
                  "cargo +$rust_nightly bench -p solana-accounts-db --bench append_vec",
                  "cargo +$rust_nightly bench -p solana-accounts-db --bench bench_accounts_file -- --output-format bencher",
                  "cargo +$rust_nightly bench -p solana-accounts-db --bench bench_hashing -- --output-format bencher",
                  "cargo +$rust_nightly bench -p solana-accounts-db --bench bench_serde -- --output-format bencher",
                ],
            }
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - name: Before Command
        if: ${{ matrix.test.before_command != '' }}
        run: |
          ${{ matrix.test.before_command }}
      - name: Command
        run: |
          source ci/rust-version.sh nightly
          echo '${{ toJson(matrix.test.commands) }}' | jq -r '.[]' | while read command; do
            eval $command | tee -a benchmark
          done
      - name: Upload Result
        run: |
          TEST_SUITE="${{ matrix.test.name }}" \
          COMMIT_HASH="$(git rev-parse HEAD)" \
          INFLUX_HOST="${{ secrets.BENCHMARK_INFLUX_HOST }}" \
          INFLUX_DB="${{ secrets.BENCHMARK_INFLUX_DB }}" \
          INFLUX_USER="${{ secrets.BENCHMARK_INFLUX_USER }}" \
          INFLUX_PASSWORD="${{ secrets.BENCHMARK_INFLUX_PASSWORD }}" \
          INFLUX_MEASUREMENT="${{ secrets.BENCHMARK_INFLUX_MEASUREMENT }}" \
          ./ci/upload-benchmark.sh benchmark

================
File: .github/workflows/cargo.yml
================
name: Cargo
on:
  push:
    branches:
      - master
      - v[0-9]+.[0-9]+
  pull_request:
    branches:
      - master
      - v[0-9]+.[0-9]+
    paths:
      - "**.rs"
      - "**/Cargo.toml"
      - "**/Cargo.lock"
      - ".github/scripts/install-all-deps.sh"
      - ".github/scripts/install-openssl.sh"
      - ".github/scripts/install-proto.sh"
      - ".github/scripts/cargo-clippy-before-script.sh"
      - ".github/workflows/cargo.yml"
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true
env:
  SHELL: /bin/bash
  SCCACHE_GHA_ENABLED: "true"
  RUSTC_WRAPPER: "sccache"
jobs:
  clippy-nightly:
    if: github.repository == 'anza-xyz/agave'
    strategy:
      matrix:
        os:
          - os: macos-latest
          - os: ubuntu-latest
            container: docker.io/rust:1-alpine
            rustflags: -C target-feature=-crt-static
          - os: windows-latest
    runs-on: ${{ matrix.os.os }}
    container:
      image: ${{ matrix.os.container }}
    env:
      RUSTFLAGS: ${{ matrix.os.rustflags }}
    steps:
      - if: ${{ contains(matrix.os.container, 'alpine') }}
        run: |
          apk update
          apk add bash git
      - uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.10.0"
      - name: Set Perl environment variables
        if: runner.os == 'Windows'
        run: |
          echo "PERL=$((where.exe perl)[0])" | Out-File -FilePath $env:GITHUB_ENV -Append -Encoding utf8
          echo "OPENSSL_SRC_PERL=$((where.exe perl)[0])" | Out-File -FilePath $env:GITHUB_ENV -Append -Encoding utf8
      - shell: bash
        run: |
          git config --global --add safe.directory "$(pwd)"
          source .github/scripts/install-all-deps.sh ${{ runner.os }}
          source ci/rust-version.sh nightly
          rustup component add clippy --toolchain "$rust_nightly"
          scripts/cargo-clippy-nightly.sh

================
File: .github/workflows/changelog-label.yml
================
name: Require changelog entry
on:
  pull_request:
    types: [opened, synchronize, reopened, labeled, unlabeled]
jobs:
  check-changelog:
    if: contains(github.event.pull_request.labels.*.name, 'changelog')
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v6
      with:
        fetch-depth: 0
        submodules: 'recursive'
    - name: Check if changes to CHANGELOG.md
      shell: bash
      env:
        BASE_REF: ${{ github.event.pull_request.base.ref }}
      run: .github/scripts/check-changelog.sh

================
File: .github/workflows/client-targets.yml
================
name: client_targets
on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master
    paths:
      - "client/**"
      - ".github/workflows/client-targets.yml"
      - "ci/rust-version.sh"
      - "**/Cargo.toml"
      - "**/Cargo.lock"
env:
  CARGO_TERM_COLOR: always
jobs:
  android:
    if: github.repository == 'anza-xyz/agave'
    strategy:
      matrix:
        os:
          - ubuntu-22.04
        target:
          - x86_64-linux-android
          - aarch64-linux-android
          - i686-linux-android
          - armv7-linux-androideabi
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - name: Setup environment for Android NDK
        run: |
          echo "RANLIB=$ANDROID_NDK_ROOT/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-ranlib" >> $GITHUB_ENV
      - run: cargo install cargo-ndk@2.12.2
      - name: Setup Rust
        run: |
          source ci/rust-version.sh stable
          rustup target add --toolchain "$rust_stable" ${{ matrix.target }}
      - name: Stable build
        run: ./cargo stable ndk --target ${{ matrix.target }} build -p solana-client
  ios:
    if: github.repository == 'anza-xyz/agave'
    strategy:
      matrix:
        os:
          - macos-15
        target:
          - aarch64-apple-ios
          - x86_64-apple-ios
          - aarch64-apple-darwin
          - x86_64-apple-darwin
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - name: Setup Rust
        run: |
          source ci/rust-version.sh stable
          rustup target add --toolchain "$rust_stable" ${{ matrix.target }}
      - name: Stable build
        run: ./cargo stable build --target ${{ matrix.target }} -p solana-client
  error_reporting:
    needs:
      - android
      - ios
    if: failure() && github.event_name == 'push'
    uses: ./.github/workflows/error-reporting.yml
    secrets:
      WEBHOOK: ${{ secrets.SLACK_ERROR_REPORTING_WEBHOOK }}

================
File: .github/workflows/crate-check.yml
================
name: crate-check
on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master
    paths:
      - "**/Cargo.toml"
      - ".github/workflows/crate-check.yml"
      - "./ci/check-crates.sh"
jobs:
  check:
    name: crate check
    if: github.repository == 'anza-xyz/agave'
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
          submodules: 'recursive'
      - name: Get commit range (push)
        if: ${{ github.event_name == 'push' }}
        run: |
          echo "COMMIT_RANGE=${{ github.event.before }}..$GITHUB_SHA" >> $GITHUB_ENV
      - name: Get commit range (pull_request)
        if: ${{ github.event_name == 'pull_request' }}
        run: |
          echo "COMMIT_RANGE=${{ github.event.pull_request.base.sha }}..${{ github.event.pull_request.head.sha }}" >> $GITHUB_ENV
      - name: Setup Rust
        shell: bash
        run: |
          source ci/rust-version.sh stable
          rustup default $rust_stable
      - name: Install toml-cli
        shell: bash
        run: |
          cargo install toml-cli
      - run: |
          ci/check-crates.sh
  error_reporting:
    needs:
      - check
    if: failure() && github.event_name == 'push'
    uses: ./.github/workflows/error-reporting.yml
    secrets:
      WEBHOOK: ${{ secrets.SLACK_ERROR_REPORTING_WEBHOOK }}

================
File: .github/workflows/dependabot-pr.yml
================
name: Dependabot PR
on:
  pull_request:
    branches:
      - master
jobs:
  dependabot:
    timeout-minutes: 10
    runs-on: ubuntu-latest
    if: github.triggering_actor == 'dependabot[bot]'
    steps:
      - name: checkout
        uses: actions/checkout@v6
        with:
          ref: ${{ github.event.pull_request.head.ref }}
          token: ${{ secrets.PAT }}
      - name: update code
        run: |
          input="${{ github.event.pull_request.title }}"
          regex="[Bb]ump (.*) from (.*) to (.*)"
          if [[ ! "$input" =~ $regex ]]; then
            echo "unexpected pattern"
            exit 1
          fi
          crate_name="${BASH_REMATCH[1]}"
          old_version="${BASH_REMATCH[2]}"
          new_version="${BASH_REMATCH[3]}"
          echo "crate_name: $crate_name, old_version: $old_version, new_version: $new_version"
          ./scripts/cargo-for-all-lock-files.sh -- update -p $crate_name:$old_version --precise $new_version || \
          ./scripts/cargo-for-all-lock-files.sh -- update -p $crate_name --precise $new_version || \
          ./scripts/cargo-for-all-lock-files.sh -- tree
      - name: push
        run: |
          if [ -z "$(git status --porcelain)" ]; then
            echo "✅ Nothing to update"
          else
            git config user.email "49699333+dependabot[bot]@users.noreply.github.com"
            git config user.name "dependabot[bot]"
            git add **/Cargo.lock
            git commit -am "Update all Cargo files"
            git push
          fi

================
File: .github/workflows/docs.yml
================
name: docs
on:
  push:
    branches:
      - master
      - v[0-9]+.[0-9]+
    tags:
      - v[0-9]+.[0-9]+.[0-9]+
  pull_request:
    branches:
      - master
      - v[0-9]+.[0-9]+
jobs:
  check:
    if: github.repository == 'anza-xyz/agave'
    outputs:
      continue: ${{ steps.check.outputs.need_to_build }}
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0
          submodules: 'recursive'
      - name: Get commit range (push)
        if: ${{ github.event_name == 'push' }}
        run: |
          echo "COMMIT_RANGE=${{ github.event.before }}..$GITHUB_SHA" >> $GITHUB_ENV
      - name: Get commit range (pull_request)
        if: ${{ github.event_name == 'pull_request' }}
        run: |
          echo "COMMIT_RANGE=${{ github.event.pull_request.base.sha }}..${{ github.event.pull_request.head.sha }}" >> $GITHUB_ENV
      - name: Get file status
        run: |
          set +e
          git diff --name-only $COMMIT_RANGE | grep \
            -e '.github/workflows/docs.yml' \
            -e 'docs/**'
          echo "FILE_CHANGED=$?" >> $GITHUB_ENV
      - name: Check
        id: check
        shell: bash
        run: |
          source ci/env.sh
          eval "$(ci/channel-info.sh)"
          TAG=$CI_TAG
          echo "TAG: $TAG"
          echo "CHANNEL: $CHANNEL"
          echo "FILE_CHANGED: $FILE_CHANGED"
          echo need_to_build="$(
            if [ "$TAG" != '' ]
            then
              echo 1
            elif [ $FILE_CHANGED = 0 ] && ( [ "$CHANNEL" = "beta" ] || [ "$CHANNEL" = "edge" ] )
            then
              echo 1
            else
              echo 0
            fi
          )" >> $GITHUB_OUTPUT
  build:
    needs:
      - check
    if: >
      github.repository == 'anza-xyz/agave' &&
      needs.check.outputs.continue == 1
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - name: Setup Node
        uses: actions/setup-node@v6
        with:
          node-version: 24
      - name: Build
        working-directory: docs
        run: |
          npm install
          ./build.sh
      - name: Upload artifacts
        if: ${{ github.event_name == 'push' }}
        uses: actions/upload-artifact@v5
        with:
          name: docs-build
          path: ./docs/build
          retention-days: 1
  deploy:
    needs:
      - build
    if: >
      github.repository == 'anza-xyz/agave' &&
      github.event_name == 'push'
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v6
      - name: Setup Node
        uses: actions/setup-node@v6
        with:
          node-version: 24
      - name: Download artifacts
        uses: actions/download-artifact@v6
        with:
          name: docs-build
          path: ./docs/build
      - name: Deploy
        working-directory: docs
        run: ./deploy.sh
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_SCOPE: ${{ secrets.VERCEL_SCOPE }}
  error_reporting:
    needs:
      - check
      - build
      - deploy
    if: failure() && github.event_name == 'push'
    uses: ./.github/workflows/error-reporting.yml
    secrets:
      WEBHOOK: ${{ secrets.SLACK_ERROR_REPORTING_WEBHOOK }}

================
File: .github/workflows/downstream-project-anchor.yml
================
name: Downstream Project - Anchor
on:
  push:
    branches:
      - master
      - v[0-9]+.[0-9]+
  pull_request:
    branches:
      - master
      - v[0-9]+.[0-9]+
    paths:
      - "**.rs"
      - "Cargo.toml"
      - "Cargo.lock"
      - "cargo-build-sbf"
      - "cargo-test-sbf"
      - "scripts/build-downstream-anchor-projects.sh"
      - "scripts/patch-spl-crates-for-anchor.sh"
      - ".github/scripts/purge-ubuntu-runner.sh"
      - ".github/scripts/downstream-project-spl-install-deps.sh"
      - ".github/workflows/downstream-project-anchor.yml"
  workflow_call:
    inputs:
      branch:
        required: false
        type: string
        default: "master"
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true
env:
  SHELL: /bin/bash
  SCCACHE_GHA_ENABLED: "true"
  RUSTC_WRAPPER: "sccache"
jobs:
  test:
    if: false
    runs-on: ubuntu-latest
    strategy:
      matrix:
        version: [ "master" ]
    steps:
      - uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - shell: bash
        run: |
          .github/scripts/purge-ubuntu-runner.sh
      - uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.10.0"
      - shell: bash
        run: |
          source .github/scripts/downstream-project-spl-install-deps.sh
          ./scripts/build-downstream-anchor-projects.sh ${{ matrix.version }}

================
File: .github/workflows/downstream-project-spl-nightly.yml
================
name: Downstream Project - SPL (Nightly)
on:
  schedule:
    - cron: "0 3 * * *"
jobs:
  main:
    if: github.repository == 'anza-xyz/agave'
    strategy:
      fail-fast: false
      matrix:
        branch:
          - master
    uses: ./.github/workflows/downstream-project-spl.yml
    with:
      branch: ${{ matrix.branch }}
  error_reporting:
    needs:
      - main
    if: failure()
    uses: ./.github/workflows/error-reporting.yml
    secrets:
      WEBHOOK: ${{ secrets.SLACK_ERROR_REPORTING_WEBHOOK }}

================
File: .github/workflows/downstream-project-spl.yml
================
name: Downstream Project - SPL
on:
  push:
    branches:
      - master
      - v[0-9]+.[0-9]+
  pull_request:
    branches:
      - master
      - v[0-9]+.[0-9]+
    paths:
      - "**.rs"
      - "rust-toolchain.toml"
      - "Cargo.toml"
      - "Cargo.lock"
      - "platform-tools-sdk/cargo-build-sbf"
      - "platform-tools-sdk/cargo-test-sbf"
      - "ci/downstream-projects/run-spl.sh"
      - ".github/workflows/downstream-project-spl.yml"
      - ".github/scripts/downstream-project-spl-common.sh"
      - ".github/scripts/downstream-project-spl-install-deps.sh"
  workflow_call:
    inputs:
      branch:
        required: false
        type: string
        default: "master"
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true
env:
  SHELL: /bin/bash
  SCCACHE_GHA_ENABLED: "true"
  RUSTC_WRAPPER: "sccache"
jobs:
  check:
    if: false
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      matrix:
        programs:
          - associated-token-account
          - feature-proposal
          - instruction-padding
          - memo
          - record
          - single-pool
          - slashing
          - stake-pool
          - token-2022
    steps:
      - uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - shell: bash
        run: |
          .github/scripts/purge-ubuntu-runner.sh
      - uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.10.0"
      - shell: bash
        run: |
          source .github/scripts/downstream-project-spl-install-deps.sh
          source .github/scripts/downstream-project-spl-common.sh "${{ matrix.programs }}"
          if [ -n "$SKIP_SPL_DOWNSTREAM_PROJECT_TEST" ]; then
            exit 0
          fi
          cargo check
  test_cli:
    if: false
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      matrix:
        programs:
          - single-pool
          - token-2022
    steps:
      - uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - shell: bash
        run: |
          .github/scripts/purge-ubuntu-runner.sh
      - uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.10.0"
      - shell: bash
        run: |
          source .github/scripts/downstream-project-spl-install-deps.sh
          source .github/scripts/downstream-project-spl-common.sh "${{ matrix.programs }}"
          if [ -n "$SKIP_SPL_DOWNSTREAM_PROJECT_TEST" ]; then
            exit 0
          fi
          $CARGO_BUILD_SBF --manifest-path program/Cargo.toml
          cargo test --manifest-path clients/cli/Cargo.toml
  cargo-test-sbf:
    if: false
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      matrix:
        programs:
          - associated-token-account
          - feature-proposal
          - instruction-padding
          - memo
          - record
          - single-pool
          - slashing
          - stake-pool
          - token-2022
    steps:
      - uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - shell: bash
        run: |
          .github/scripts/purge-ubuntu-runner.sh
      - uses: mozilla-actions/sccache-action@v0.0.9
        with:
          version: "v0.10.0"
      - name: Install dependencies
        shell: bash
        run: |
          source .github/scripts/downstream-project-spl-install-deps.sh
          source .github/scripts/downstream-project-spl-common.sh "${{ matrix.programs }}"
          if [ -n "$SKIP_SPL_DOWNSTREAM_PROJECT_TEST" ]; then
            exit 0
          fi
      - name: Test SBPFv0
        shell: bash
        run: |
          source ci/downstream-projects/common.sh
          cd "${{ matrix.programs }}"
          $CARGO_BUILD_SBF --arch v0 --manifest-path program/Cargo.toml
          RUSTFLAGS="-Awarnings" SBF_OUT_DIR="../target/deploy"  cargo test --features test-sbf --manifest-path program/Cargo.toml
      - name: Test SBPFv1
        shell: bash
        run: |
          source ci/downstream-projects/common.sh
          cd "${{ matrix.programs }}"
          rm -rf target/deploy target/sbpf*
          $CARGO_BUILD_SBF --arch v1 --manifest-path program/Cargo.toml
          RUSTFLAGS="-Awarnings" SBF_OUT_DIR="../target/deploy"  cargo test --features test-sbf --manifest-path program/Cargo.toml
      - name: Test SBPFv2
        shell: bash
        run: |
          source ci/downstream-projects/common.sh
          cd "${{ matrix.programs }}"
          rm -rf target/deploy target/sbpf*
          $CARGO_BUILD_SBF --arch v2 --manifest-path program/Cargo.toml
          RUSTFLAGS="-Awarnings" SBF_OUT_DIR="../target/deploy"  cargo test --features test-sbf --manifest-path program/Cargo.toml

================
File: .github/workflows/error-reporting.yml
================
name: error-reporting
on:
  workflow_call:
    secrets:
      WEBHOOK:
        required: true
jobs:
  slack:
    runs-on: ubuntu-22.04
    steps:
      - env:
          COMMIT_MESSAGE: ${{ github.event.head_commit.message }}
          COMMIT_AUTHOR_NAME: ${{ github.event.head_commit.author.name }}
        run: |
          curl -H "Content-Type: application/json" \
            -X POST ${{ secrets.WEBHOOK }} \
            -d '{
              "attachments": [
                {
                  "color": "#AC514C",
                  "text":
                    "*${{ github.repository }} (${{ github.workflow }})*\n'"$COMMIT_MESSAGE"' - _'"$COMMIT_AUTHOR_NAME"'_\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Build>",
                }
              ]
            }'

================
File: .github/workflows/label-actions.yml
================
name: "Issue Label Actions"
on:
  issues:
    types: [labeled, unlabeled]
permissions:
  contents: read
  issues: write
jobs:
  action:
    runs-on: ubuntu-latest
    steps:
      - uses: dessant/label-actions@v4

================
File: .github/workflows/publish-windows-tarball.yml
================
name: Publish Windows Tarball
on:
  workflow_dispatch:
    inputs:
      commit:
        type: string
        required: true
        description: commit
jobs:
  windows-build:
    runs-on: windows-2022
    outputs:
      tag: ${{ steps.build.outputs.tag }}
      channel: ${{ steps.build.outputs.channel }}
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          ref: master
          fetch-depth: 0
          submodules: 'recursive'
      - name: Set Perl environment variables
        run: |
          echo "PERL=$((where.exe perl)[0])" | Out-File -FilePath $env:GITHUB_ENV -Append -Encoding utf8
          echo "OPENSSL_SRC_PERL=$((where.exe perl)[0])" | Out-File -FilePath $env:GITHUB_ENV -Append -Encoding utf8
      - name: Build
        id: build
        shell: bash
        env:
          COMMIT: ${{ github.event.inputs.commit }}
        run: |
          source .github/scripts/install-all-deps.sh ${{ runner.os }}
          if [[ -z ${{ env.COMMIT }} ]]; then
            echo "Required parameter env.COMMIT is empty."
            exit 1
          fi
          git checkout ${{ env.COMMIT }}
          source ci/env.sh
          echo "tag=$CI_TAG" >> $GITHUB_OUTPUT
          eval "$(ci/channel-info.sh)"
          echo "channel=$CHANNEL" >> $GITHUB_OUTPUT
          echo "::notice::commit: ${{ env.COMMIT }}"
          echo "::notice::tag: $CI_TAG"
          echo "::notice::channel: $CHANNEL"
          ci/publish-tarball.sh
      - name: Prepare Upload Files
        if: ${{ steps.build.outputs.channel != '' || steps.build.outputs.tag != '' }}
        shell: bash
        run: |
          FOLDER_NAME=${{ steps.build.outputs.tag || steps.build.outputs.channel }}
          mkdir -p "windows-release/$FOLDER_NAME"
          cp -v "solana-release-x86_64-pc-windows-msvc.tar.bz2" "windows-release/$FOLDER_NAME/"
          cp -v "solana-release-x86_64-pc-windows-msvc.yml" "windows-release/$FOLDER_NAME/"
          cp -v "agave-install-init-x86_64-pc-windows-msvc"* "windows-release/$FOLDER_NAME"
      - name: Upload Artifacts
        if: ${{ steps.build.outputs.channel != '' || steps.build.outputs.tag != '' }}
        uses: actions/upload-artifact@v5
        with:
          name: windows-artifact
          path: windows-release/
  windows-gcs-upload:
    if: ${{ needs.windows-build.outputs.channel != '' || needs.windows-build.outputs.tag != '' }}
    needs: [windows-build]
    runs-on: ubuntu-22.04
    steps:
      - name: Download
        uses: actions/download-artifact@v6
        with:
          name: windows-artifact
          path: ./windows-release
      - name: Setup crediential
        uses: "google-github-actions/auth@v3"
        with:
          credentials_json: "${{ secrets.GCS_RELEASE_BUCKET_WRITER_CREDIENTIAL }}"
      - name: Upload files to GCS
        run: |
          gcloud storage cp --recursive windows-release/* gs://anza-release/
  windows-gh-release:
    if: ${{ needs.windows-build.outputs.tag != '' }}
    needs: [windows-build]
    runs-on: ubuntu-22.04
    steps:
      - name: Download
        uses: actions/download-artifact@v6
        with:
          name: windows-artifact
          path: ./windows-release/
      - name: Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ needs.windows-build.outputs.tag }}
          files: |
            windows-release/${{ needs.windows-build.outputs.tag }}/*

================
File: .github/workflows/rebase.yaml
================
name: "Rebase jito-solana from upstream anza-xyz/agave"
on:
  schedule:
    - cron: "00 19 * * 1-5"
jobs:
  rebase:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - branch: master
            upstream_branch: master
            upstream_repo: https://github.com/anza-xyz/agave.git
          - branch: v3.1
            upstream_branch: v3.1
            upstream_repo: https://github.com/anza-xyz/agave.git
          - branch: v3.0
            upstream_branch: v3.0
            upstream_repo: https://github.com/anza-xyz/agave.git
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ matrix.branch }}
          submodules: recursive
          fetch-depth: 0
          token: ${{ secrets.JITO_SOLANA_RELEASE_TOKEN }}
      - name: Add upstream
        run: git remote add upstream ${{ matrix.upstream_repo }}
      - name: Fetch upstream
        run: git fetch upstream
      - name: Fetch origin
        run: git fetch origin
      - name: Set REBASE_BRANCH
        run: echo "REBASE_BRANCH=ci/nightly/${{ matrix.branch }}/$(date +'%Y-%m-%d-%H-%M')" >> $GITHUB_ENV
      - name: echo $REBASE_BRANCH
        run: echo $REBASE_BRANCH
      - name: Create rebase branch
        run: git checkout -b $REBASE_BRANCH
      - name: Setup email
        run: |
          git config --global user.email "infra@jito.wtf"
          git config --global user.name "Jito Infrastructure"
      - name: Rebase
        id: rebase
        run: git rebase upstream/${{ matrix.upstream_branch }}
      - name: Send warning for rebase error
        if: failure() && steps.rebase.outcome == 'failure'
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "Nightly rebase on branch ${{ matrix.branch }}\nStatus: Rebase failed to apply cleanly"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      - name: Check if rebase applied
        id: check_rebase_applied
        run: |
          PRE_REBASE_SHA=$(git rev-parse ${{ matrix.branch }})
          POST_REBASE_SHA=$(git rev-parse HEAD)
          if [ "$PRE_REBASE_SHA" = "$POST_REBASE_SHA" ]; then
            echo "No rebase was applied, exiting..."
            exit 1
          else
            echo "Rebase applied successfully."
          fi
      - name: Send warning for rebase error
        if: failure() && steps.check_rebase_applied.outcome == 'failure'
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "Nightly rebase on branch ${{ matrix.branch }}\nStatus: Rebase not needed"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      - name: Set REBASE_SHA
        run: echo "REBASE_SHA=$(git rev-parse HEAD)" >> $GITHUB_ENV
      - name: Push changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ env.REBASE_BRANCH }}
      - name: Wait for buildkite to start build
        run: sleep 300
      - name: Wait for buildkite to finish
        id: wait_for_buildkite
        timeout-minutes: 300
        run: |
          while true; do
            response=$(curl -s -f -H "Authorization: Bearer ${{ secrets.BUILDKITE_TOKEN }}" "https://api.buildkite.com/v2/organizations/jito/pipelines/jito-solana/builds?commit=${{ env.REBASE_SHA }}")
            if [ $? -ne 0 ]; then
              echo "Curl request failed."
              exit 1
            fi
            state=$(echo $response | jq --exit-status -r '.[0].state')
            echo "Current build state: $state"
            case $state in
              "passed"|"finished")
                echo "Build finished successfully."
                exit 0
              ;;
              "canceled"|"canceling"|"not_run")
                echo "Build failed or was cancelled."
                exit 2
              ;;
            esac
          sleep 30
          done
      - name: Send failure update
        uses: slackapi/slack-github-action@v1.25.0
        if: failure() && steps.wait_for_buildkite.outcome == 'failure'
        with:
          payload: |
            {
              "text": "Nightly rebase on branch ${{ matrix.branch }}\nStatus: CI failed\nBranch: ${{ env.REBASE_BRANCH}}\nBuild: https://buildkite.com/jito/jito-solana/builds?commit=${{ env.REBASE_SHA }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      - name: Fetch the latest remote changes
        run: git fetch origin ${{ matrix.branch }}
      - name: Check if origin HEAD has changed from the beginning of the workflow
        run: |
          LOCAL_SHA=$(git rev-parse ${{ matrix.branch }})
          ORIGIN_SHA=$(git rev-parse origin/${{ matrix.branch }})
          if [ "$ORIGIN_SHA" != "$LOCAL_SHA" ]; then
            echo "The remote HEAD of ${{ matrix.branch }} does not match the local HEAD of ${{ matrix.branch }} at the beginning of CI."
            echo "origin sha: $ORIGIN_SHA"
            echo "local sha: $LOCAL_SHA"
            exit 1
          else
            echo "The remote HEAD matches the local REBASE_SHA at the beginning of CI. Proceeding."
          fi
      - name: Reset ${{ matrix.branch }} to ${{ env.REBASE_BRANCH }}
        run: |
          git checkout ${{ matrix.branch }}
          git reset --hard ${{ env.REBASE_BRANCH }}
      - name: Push rebased %{{ matrix.branch }}
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.JITO_SOLANA_RELEASE_TOKEN }}
          branch: ${{ matrix.branch }}
          force: true
      - name: Send success update
        uses: slackapi/slack-github-action@v1.25.0
        with:
          payload: |
            {
              "text": "Nightly rebase on branch ${{ matrix.branch }}\nStatus: CI success, rebased, and pushed\nBranch: ${{ env.REBASE_BRANCH}}\nBuild: https://buildkite.com/jito/jito-solana/builds?commit=${{ env.REBASE_SHA }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

================
File: .github/workflows/release.yml
================
name: Release
on:
  push:
    tags:
      - "*"
jobs:
  trigger-buildkite-pipeline:
    if: github.repository == 'jito-foundation/jito-solana'
    runs-on: ubuntu-latest
    steps:
      - name: Trigger a Buildkite Build
        uses: "buildkite/trigger-pipeline-action@v2.4.1"
        with:
          buildkite_api_access_token: ${{ secrets.TRIGGER_BK_BUILD_TOKEN }}
          pipeline: "jito/agave-secondary"
          branch: "${{ github.ref_name }}"
          build_env_vars: '{"TRIGGERED_BUILDKITE_TAG": "${{ github.ref_name }}"}'
          commit: "HEAD"
          message: ":github: Triggered from a GitHub Action"
  draft-release:
    if: github.repository == 'jito-foundation/jito-solana'
    runs-on: ubuntu-latest
    steps:
      - name: Create Release
        uses: actions/github-script@v8
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            github.rest.repos.createRelease({
              owner: context.repo.owner,
              repo: context.repo.repo,
              tag_name: '${{ github.ref_name }}',
              name: 'Release ${{ github.ref_name }}',
              body: '🚧',
              draft: false,
              prerelease: true
            })

================
File: .github/workflows/verify-packets.yml
================
name: Verify Packets
on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master
    paths:
      - .github/workflows/verify-packets.yml
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true
jobs:
  gossip:
    timeout-minutes: 30
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          submodules: 'recursive'
      - name: Install required packages
        run: |
          sudo apt update
          sudo apt install -y \
            libclang-dev \
            libprotobuf-dev \
            libssl-dev \
            libudev-dev \
            pkg-config \
            zlib1g-dev \
            llvm \
            clang \
            cmake \
            make \
            protobuf-compiler \
            git-lfs
      - name: Run packet verify
        run: |
          ./ci/test-verify-packets-gossip.sh

================
File: .github/CODEOWNERS
================
# Please keep this file sorted
/bloom/ @anza-xyz/networking
/compute-budget-instruction/ @anza-xyz/fees
/core/src/repair/ @anza-xyz/networking
/fee/ @anza-xyz/fees
/gossip/ @anza-xyz/networking
/log-collector/ @anza-xyz/svm
/net-utils/ @anza-xyz/networking
/program-runtime/ @anza-xyz/svm
/programs/bpf_loader/ @anza-xyz/svm
/programs/loader-v4/ @anza-xyz/svm
/runtime-transaction/ @anza-xyz/tx-metadata
/svm-callback/ @anza-xyz/svm
/svm-conformance/ @anza-xyz/svm
/svm-transaction/ @anza-xyz/svm
/svm/ @anza-xyz/svm
/tls-utils/ @anza-xyz/networking
/transaction-context/ @anza-xyz/svm
/transaction-view/ @anza-xyz/tx-metadata
/turbine/ @anza-xyz/networking
/xdp/ @anza-xyz/networking

================
File: .github/dependabot.yml
================
version: 2
updates:
  - package-ecosystem: cargo
    directories:
      - "/"
      - "/dev-bins"
      - "/ci/xtask"
      - "/programs/sbf"
    schedule:
      interval: daily
      time: "01:00"
      timezone: America/Los_Angeles
    open-pull-requests-limit: 6
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: daily
      time: "01:00"
      timezone: America/Los_Angeles
    open-pull-requests-limit: 3

================
File: .github/label-actions.yml
================
question:
  issues:
    comment: >
      Hi @{issue-author},
      Thanks for your question!
      We want to make sure to keep signal strong in the GitHub issue tracker &ndash; to make sure
      that it remains the best place to track issues that affect the development of Solana itself.
      Questions like yours deserve a purpose-built Q&A forum. Unless there exists evidence that
      this is a bug with Solana itself, please post your question to the Solana Stack Exchange
      using this link: https://solana.stackexchange.com/questions/ask
      ---
      _This
      [automated message](https://github.com/anza-xyz/agave/blob/master/.github/label-actions.yml)
      is a result of having added the &lsquo;question&rsquo; tag_.
    close: true

================
File: .github/PULL_REQUEST_TEMPLATE.md
================
#### Problem


#### Summary of Changes


Fixes #
<!-- OPTIONAL: Feature Gate Issue: # -->
<!-- Don't forget to add the "feature-gate" label -->

================
File: .github/RELEASE_TEMPLATE.md
================
# Release v0.X.Y <milestone name>

fun blurb about the name, what's in the release

## Major Features And Improvements

* bulleted
* list of features and improvements

## Breaking Changes

* bulleted
* list
* of
* protocol changes/breaks
* API breaks
* CLI changes
* etc.

## Bug Fixes and Other Changes

* can be pulled from commit log, or synthesized

## Thanks to our Contributors

This release contains contributions from many people at Solana, as well as:

  pull from commit log

================
File: .gitignore
================
farf/
target/
/solana-release/
/solana-release.tar.bz2
/solana-metrics/
/solana-metrics.tar.bz2
**/target/
/test-ledger/

**/*.rs.bk
.cargo

/config/

.cache

# log files
*.log
log-*.txt
log-*/
!log-collector/
!log-analyzer/*
!log-utils/

# intellij files
.idea/
/solana.iml
/.vscode/

# fetch-core-bpf.sh artifacts
/core-bpf-genesis-args.sh
/core-bpf-*.so

# fetch-spl.sh artifacts
/spl-genesis-args.sh
/spl-*.so
/jito_*.so

.DS_Store
# scripts that may be generated by cargo *-bpf commands
**/cargo-*-bpf-child-script-*.sh

.env
docker-output/

================
File: .gitmodules
================
[submodule "jito-protos/protos"]
	path = jito-protos/protos
	url = https://github.com/jito-labs/mev-protos.git
[submodule "jito-protos/bam-protos"]
	path = jito-protos/bam-protos
	url = https://github.com/jito-labs/bam-protos.git

================
File: .mergify.yml
================
pull_request_rules:
  - name: label changes from community
    conditions:
      - author≠@anza
      - author≠@monorepo-maintainers
      - author≠@monorepo-write
      - author≠@monorepo-triage
      - author≠mergify[bot]
      - author≠dependabot[bot]
      - author≠github-actions[bot]
    actions:
      label:
        add:
          - community
          - need:merge-assist
  - name: request review for community changes
    conditions:
      - author≠@anza
      - author≠@monorepo-maintainers
      - author≠@monorepo-write
      - author≠@monorepo-triage
      - author≠mergify[bot]
      - author≠dependabot[bot]
      - author≠github-actions[bot]
      - "#approved-reviews-by=0"
      - "#commented-reviews-by=0"
      - "#changes-requested-reviews-by=0"
      - "#review-requested=0"
    actions:
      request_reviews:
        teams:
          - "@jito-labs/engineering-team"
  - name: label changes from monorepo-triage
    conditions:
      - author≠@anza
      - author≠mergify[bot]
      - author≠dependabot[bot]
      - author≠github-actions[bot]
      - author≠@monorepo-maintainers
      - author≠@monorepo-write
      - author=@monorepo-triage
    actions:
      label:
        add:
          - need:merge-assist
  - name: automatic merge (squash) on CI success
    conditions:
      - and:
        - status-success=buildkite/agave
        - status-success=ci-gate
        - label=automerge
        - label!=no-automerge
        - or:
          - -files~=^docs/
          - status-success=build & deploy docs
        - or:
          - -files~=(\.rs|Cargo\.toml|Cargo\.lock|\.github/scripts/cargo-clippy-before-script\.sh|\.github/workflows/cargo\.yml)$
          - or:
            - check-success=clippy-nightly (macos-latest)
            - check-success=clippy-nightly (macos-latest-large)
        - or:
          - -files~=(\.rs|Cargo\.toml|Cargo\.lock|cargo-build-sbf|cargo-test-sbf|ci/downstream-projects/run-spl\.sh|\.github/workflows/downstream-project-spl\.yml)$
          - and:
            - status-success=check (associated-token-account)
            - status-success=check (feature-proposal)
            - status-success=check (instruction-padding)
            - status-success=check (memo)
            - status-success=check (record)
            - status-success=check (single-pool)
            - status-success=check (slashing)
            - status-success=check (stake-pool)
            - status-success=check (token-2022)
            - status-success=test_cli (single-pool)
            - status-success=test_cli (token-2022)
            - status-success=cargo-test-sbf (associated-token-account)
            - status-success=cargo-test-sbf (feature-proposal)
            - status-success=cargo-test-sbf (instruction-padding)
            - status-success=cargo-test-sbf (memo)
            - status-success=cargo-test-sbf (record)
            - status-success=cargo-test-sbf (single-pool)
            - status-success=cargo-test-sbf (slashing)
            - status-success=cargo-test-sbf (stake-pool)
            - status-success=cargo-test-sbf (token-2022)
        - or:
          - -files~=(Cargo\.toml|.github/workflows/crate-check\.yml|ci/check-crates\.sh)$
          - check-success=crate check
    actions:
      merge:
        method: squash
  - name: remove automerge label on CI failure
    conditions:
      - and:
        - label=automerge
        - "#status-failure!=0"
        - -merged
    actions:
      label:
        remove:
          - automerge
      comment:
        message: automerge label removed due to a CI failure
  - name: v3.0 feature-gate backport
    conditions:
      - label=v3.0
      - label=feature-gate
    actions:
      backport:
        assignees: &BackportAssignee
          - "{{ merged_by|replace('mergify[bot]', label|select('equalto', 'community')|first|default(author)|replace('community', '@anza-xyz/community-pr-subscribers')) }}"
        title: "{{ destination_branch }}: {{ title }} (backport of #{{ number }})"
        ignore_conflicts: true
        labels:
          - feature-gate
        branches:
          - v3.0
  - name: v3.0 non-feature-gate backport
    conditions:
      - label=v3.0
      - label!=feature-gate
    actions:
      backport:
        assignees: *BackportAssignee
        title: "{{ destination_branch }}: {{ title }} (backport of #{{ number }})"
        ignore_conflicts: true
        branches:
          - v3.0
  - name: v3.0 backport warning comment
    conditions:
      - label=v3.0
    actions:
      comment:
        message: >
          Backports to the stable branch are to be avoided unless absolutely
          necessary for fixing bugs, security issues, and perf regressions.
          Changes intended for backport should be structured such that a
          minimum effective diff can be committed separately from any
          refactoring, plumbing, cleanup, etc that are not strictly
          necessary to achieve the goal. Any of the latter should go only
          into master and ride the normal stabilization schedule.
  - name: v3.1 feature-gate backport
    conditions:
      - label=v3.1
      - label=feature-gate
    actions:
      backport:
        assignees: *BackportAssignee
        title: "{{ destination_branch }}: {{ title }} (backport of #{{ number }})"
        ignore_conflicts: true
        labels:
          - feature-gate
        branches:
          - v3.1
  - name: v3.1 non-feature-gate backport
    conditions:
      - label=v3.1
      - label!=feature-gate
    actions:
      backport:
        assignees: *BackportAssignee
        title: "{{ destination_branch }}: {{ title }} (backport of #{{ number }})"
        ignore_conflicts: true
        branches:
          - v3.1
  - name: v3.1 backport warning comment
    conditions:
      - label=v3.1
    actions:
      comment:
        message: >
          Backports to the beta branch are to be avoided unless absolutely
          necessary for fixing bugs, security issues, and perf regressions.
          Changes intended for backport should be structured such that a
          minimum effective diff can be committed separately from any
          refactoring, plumbing, cleanup, etc that are not strictly
          necessary to achieve the goal. Any of the latter should go only
          into master and ride the normal stabilization schedule. Exceptions
          include CI/metrics changes, CLI improvements and documentation
          updates on a case by case basis.
  - name: Reminder to update RPC clients for changes in `rpc/`
    conditions:
      - or:
        - files~=^rpc/src/rpc\.rs$
        - files~=^rpc/src/rpc_pubsub\.rs$
        - files~=^rpc-client-api/src/.*\.rs$
    actions:
      comment:
        message: |
          If this PR represents a change to the public RPC API:
          1. Make sure it includes a complementary update to `rpc-client/` ([example](https://github.com/solana-labs/solana/pull/29558/files))
          2. Open a follow-up PR to update the JavaScript client `@solana/kit` ([example](https://github.com/solana-labs/solana-web3.js/pull/2868/files))
          Thank you for keeping the RPC clients in sync with the server API @{{author}}.
  - name: Reminder to add Firedancer team to changes in `programs/`
    conditions:
      - or:
        - files~=^programs/address-lookup-table/src/.*\.rs$
        - files~=^programs/bpf_loader/src/.*\.rs$
        - files~=^programs/compute_budget/src/.*\.rs$
        - files~=^programs/config/src/.*\.rs$
        - files~=^programs/loader-v4/src/.*\.rs$
        - files~=^programs/stake/src/.*\.rs$
        - files~=^programs/system/src/.*\.rs$
        - files~=^programs/vote/src/.*\.rs$
        - files~=^programs/zk-elgamal-proof/src/.*\.rs$
    actions:
      comment:
        message: |
          The Firedancer team maintains a line-for-line reimplementation of the
          native programs, and until native programs are moved to BPF, those
          implementations must exactly match their Agave counterparts.
          If this PR represents a change to a native program implementation (not
          tests), please include a reviewer from the Firedancer team. And please
          keep refactors to a minimum.
  - name: Notify about the deprecation of zk-token-sdk
    conditions:
      - or:
        - files~=^zk-token-sdk/
    actions:
      comment:
        message: |
          For your information, the `solana-zk-token-sdk` is deprecated, and this
          directory will be removed in a future version. Please take this in mind
          when making modifications.
commands_restrictions:
  copy:
    conditions:
    - author=@jito-labs/engineering-team

================
File: account-decoder/src/lib.rs
================
pub mod parse_account_data;
pub mod parse_address_lookup_table;
pub mod parse_bpf_loader;
⋮----
pub mod parse_config;
pub mod parse_nonce;
pub mod parse_stake;
pub mod parse_sysvar;
pub mod parse_token;
pub mod parse_token_extension;
pub mod parse_vote;
pub mod validator_info;
⋮----
pub type StringAmount = String;
pub type StringDecimals = String;
⋮----
fn encode_bs58<T: ReadableAccount>(
⋮----
let slice = slice_data(account.data(), data_slice_config);
if slice.len() <= MAX_BASE58_BYTES {
bs58::encode(slice).into_string()
⋮----
"error: data too large for bs58 encoding".to_string()
⋮----
pub fn encode_ui_account<T: ReadableAccount>(
⋮----
let space = account.data().len();
⋮----
let data = encode_bs58(account, data_slice_config);
⋮----
BASE64_STANDARD.encode(slice_data(account.data(), data_slice_config)),
⋮----
let mut encoder = zstd::stream::write::Encoder::new(Vec::new(), 0).unwrap();
⋮----
.write_all(slice_data(account.data(), data_slice_config))
.and_then(|()| encoder.finish())
⋮----
Ok(zstd_data) => UiAccountData::Binary(BASE64_STANDARD.encode(zstd_data), encoding),
⋮----
parse_account_data_v3(pubkey, account.owner(), account.data(), additional_data)
⋮----
lamports: account.lamports(),
⋮----
owner: account.owner().to_string(),
executable: account.executable(),
rent_epoch: account.rent_epoch(),
space: Some(space as u64),
⋮----
pub struct UiFeeCalculator {
⋮----
fn from(fee_calculator: FeeCalculator) -> Self {
⋮----
lamports_per_signature: fee_calculator.lamports_per_signature.to_string(),
⋮----
impl Default for UiFeeCalculator {
fn default() -> Self {
⋮----
lamports_per_signature: "0".to_string(),
⋮----
fn slice_data(data: &[u8], data_slice_config: Option<UiDataSliceConfig>) -> &[u8] {
⋮----
if offset >= data.len() {
⋮----
} else if length > data.len() - offset {
⋮----
mod test {
⋮----
fn test_slice_data() {
let data = vec![1, 2, 3, 4, 5];
let slice_config = Some(UiDataSliceConfig {
⋮----
assert_eq!(slice_data(&data, slice_config), &data[..]);
⋮----
assert_eq!(slice_data(&data, slice_config), &data[1..3]);
⋮----
assert_eq!(slice_data(&data, slice_config), &[] as &[u8]);
⋮----
fn test_encode_account_when_data_exceeds_base58_byte_limit() {
let data = vec![42; MAX_BASE58_BYTES + 2];
⋮----
assert_eq!(
⋮----
assert_ne!(
⋮----
fn test_base64_zstd() {
let encoded_account = encode_ui_account(
⋮----
data: vec![0; 1024],
⋮----
assert_matches!(
⋮----
let decoded_account = encoded_account.decode::<Account>().unwrap();
assert_eq!(decoded_account.data(), &vec![0; 1024]);
let decoded_account = encoded_account.decode::<AccountSharedData>().unwrap();

================
File: account-decoder/src/parse_account_data.rs
================
pub use solana_account_decoder_client_types::ParsedAccount;
⋮----
m.insert(
⋮----
m.insert(config::id(), ParsableAccount::Config);
m.insert(system_program::id(), ParsableAccount::Nonce);
⋮----
m.insert(spl_token_interface::id(), ParsableAccount::SplToken);
m.insert(stake::id(), ParsableAccount::Stake);
m.insert(sysvar::id(), ParsableAccount::Sysvar);
m.insert(vote::id(), ParsableAccount::Vote);
⋮----
pub enum ParseAccountError {
⋮----
pub enum ParsableAccount {
⋮----
pub struct AccountAdditionalDataV3 {
⋮----
pub struct SplTokenAdditionalData {
⋮----
impl SplTokenAdditionalData {
pub fn with_decimals(decimals: u8) -> Self {
⋮----
pub struct SplTokenAdditionalDataV2 {
⋮----
fn from(v: SplTokenAdditionalData) -> Self {
⋮----
impl SplTokenAdditionalDataV2 {
⋮----
pub fn parse_account_data_v3(
⋮----
.get(program_id)
.ok_or(ParseAccountError::ProgramNotParsable)?;
let additional_data = additional_data.unwrap_or_default();
⋮----
serde_json::to_value(parse_address_lookup_table(data)?)?
⋮----
serde_json::to_value(parse_bpf_upgradeable_loader(data)?)?
⋮----
ParsableAccount::Config => serde_json::to_value(parse_config(data, pubkey)?)?,
ParsableAccount::Nonce => serde_json::to_value(parse_nonce(data)?)?,
⋮----
parse_token_v3(data, additional_data.spl_token_additional_data.as_ref())?,
⋮----
ParsableAccount::Stake => serde_json::to_value(parse_stake(data)?)?,
ParsableAccount::Sysvar => serde_json::to_value(parse_sysvar(data, pubkey)?)?,
ParsableAccount::Vote => serde_json::to_value(parse_vote(data, pubkey)?)?,
⋮----
Ok(ParsedAccount {
program: format!("{program_name:?}").to_kebab_case(),
⋮----
space: data.len() as u64,
⋮----
mod test {
⋮----
fn test_parse_account_data() {
⋮----
let data = vec![0; 4];
assert!(parse_account_data_v3(&account_pubkey, &other_program, &data, None).is_err());
⋮----
let mut vote_account_data: Vec<u8> = vec![0; VoteStateV4::size_of()];
⋮----
VoteStateV4::serialize(&versioned, &mut vote_account_data).unwrap();
let parsed = parse_account_data_v3(
⋮----
&vote_program_id(),
⋮----
.unwrap();
assert_eq!(parsed.program, "vote".to_string());
assert_eq!(parsed.space, VoteStateV4::size_of() as u64);
⋮----
let nonce_account_data = bincode::serialize(&nonce_data).unwrap();
⋮----
assert_eq!(parsed.program, "nonce".to_string());
assert_eq!(parsed.space, State::size() as u64);

================
File: account-decoder/src/parse_address_lookup_table.rs
================
pub fn parse_address_lookup_table(
⋮----
.map(|address_lookup_table| {
LookupTableAccountType::LookupTable(address_lookup_table.into())
⋮----
.or_else(|err| match err {
InstructionError::UninitializedAccount => Ok(LookupTableAccountType::Uninitialized),
_ => Err(ParseAccountError::AccountNotParsable(
⋮----
pub enum LookupTableAccountType {
⋮----
pub struct UiLookupTable {
⋮----
fn from(address_lookup_table: AddressLookupTable) -> Self {
⋮----
deactivation_slot: address_lookup_table.meta.deactivation_slot.to_string(),
last_extended_slot: address_lookup_table.meta.last_extended_slot.to_string(),
⋮----
.map(|authority| authority.to_string()),
⋮----
.iter()
.map(|address| address.to_string())
.collect(),
⋮----
mod test {
⋮----
fn test_parse_address_lookup_table() {
⋮----
authority: Some(authority),
⋮----
addresses.resize_with(num_addresses, Pubkey::new_unique);
⋮----
let lookup_table_data = AddressLookupTable::serialize_for_tests(lookup_table).unwrap();
let parsing_result = parse_address_lookup_table(&lookup_table_data).unwrap();
⋮----
assert_eq!(
⋮----
assert_eq!(ui_lookup_table.authority, Some(authority.to_string()));
assert_eq!(ui_lookup_table.addresses.len(), num_addresses);
⋮----
assert!(parse_address_lookup_table(&[]).is_err());

================
File: account-decoder/src/parse_bpf_loader.rs
================
pub fn parse_bpf_upgradeable_loader(
⋮----
let account_state: UpgradeableLoaderState = deserialize(data).map_err(|_| {
⋮----
let offset = if authority_address.is_some() {
⋮----
- serialized_size(&Pubkey::default()).unwrap() as usize
⋮----
authority: authority_address.map(|pubkey| pubkey.to_string()),
⋮----
BASE64_STANDARD.encode(&data[offset..]),
⋮----
program_data: programdata_address.to_string(),
⋮----
let offset = if upgrade_authority_address.is_some() {
⋮----
authority: upgrade_authority_address.map(|pubkey| pubkey.to_string()),
⋮----
Ok(parsed_account)
⋮----
pub enum BpfUpgradeableLoaderAccountType {
⋮----
pub struct UiBuffer {
⋮----
pub struct UiProgram {
⋮----
pub struct UiProgramData {
⋮----
mod test {
⋮----
fn test_parse_bpf_upgradeable_loader_accounts() {
⋮----
let account_data = serialize(&bpf_loader_state).unwrap();
assert_eq!(
⋮----
let program = vec![7u8; 64];
⋮----
authority_address: Some(authority),
⋮----
let mut account_data = serialize(&bpf_loader_state).unwrap();
account_data.extend_from_slice(&program);
⋮----
upgrade_authority_address: Some(authority),

================
File: account-decoder/src/parse_config.rs
================
pub fn parse_config(data: &[u8], pubkey: &Pubkey) -> Result<ConfigAccountType, ParseAccountError> {
⋮----
get_config_data(data)
.ok()
.and_then(|data| deserialize::<StakeConfig>(data).ok())
.map(|config| ConfigAccountType::StakeConfig(config.into()))
⋮----
deserialize::<ConfigKeys>(data).ok().and_then(|key_list| {
if !key_list.keys.is_empty() && key_list.keys[0].0 == validator_info::id() {
parse_config_data::<String>(data, key_list.keys).and_then(|validator_info| {
Some(ConfigAccountType::ValidatorInfo(UiConfig {
⋮----
config_data: serde_json::from_str(&validator_info.config_data).ok()?,
⋮----
parsed_account.ok_or(ParseAccountError::AccountNotParsable(
⋮----
fn parse_config_data<T>(data: &[u8], keys: Vec<(Pubkey, bool)>) -> Option<UiConfig<T>>
⋮----
let config_data: T = deserialize(get_config_data(data).ok()?).ok()?;
⋮----
.iter()
.map(|key| UiConfigKey {
pubkey: key.0.to_string(),
⋮----
.collect();
Some(UiConfig { keys, config_data })
⋮----
pub enum ConfigAccountType {
⋮----
pub struct UiConfigKey {
⋮----
pub struct UiStakeConfig {
⋮----
fn from(config: StakeConfig) -> Self {
⋮----
pub struct UiConfig<T> {
⋮----
mod test {
⋮----
fn create_config_account<T: serde::Serialize>(
⋮----
let mut data = serialize(&ConfigKeys { keys }).unwrap();
data.extend_from_slice(&serialize(config_data).unwrap());
⋮----
fn test_parse_config() {
⋮----
let stake_config_account = create_config_account(vec![], &stake_config, 10);
assert_eq!(
⋮----
info: serde_json::to_string(&json!({
⋮----
.unwrap(),
⋮----
let validator_info_config_account = create_config_account(
vec![(validator_info::id(), false), (info_pubkey, true)],
⋮----
let bad_data = vec![0; 4];
assert!(parse_config(&bad_data, &info_pubkey).is_err());

================
File: account-decoder/src/parse_nonce.rs
================
pub fn parse_nonce(data: &[u8]) -> Result<UiNonceState, ParseAccountError> {
⋮----
.map_err(|_| ParseAccountError::from(InstructionError::InvalidAccountData))?;
match nonce_versions.state() {
State::Uninitialized => Err(ParseAccountError::from(
⋮----
State::Initialized(data) => Ok(UiNonceState::Initialized(UiNonceData {
authority: data.authority.to_string(),
blockhash: data.blockhash().to_string(),
fee_calculator: data.fee_calculator.into(),
⋮----
pub enum UiNonceState {
⋮----
pub struct UiNonceData {
⋮----
mod test {
⋮----
fn test_parse_nonce() {
⋮----
let nonce_account_data = bincode::serialize(&nonce_data).unwrap();
assert_eq!(
⋮----
let bad_data = vec![0; 4];
assert!(parse_nonce(&bad_data).is_err());

================
File: account-decoder/src/parse_stake.rs
================
pub fn parse_stake(data: &[u8]) -> Result<StakeAccountType, ParseAccountError> {
let stake_state: StakeStateV2 = deserialize(data)
.map_err(|_| ParseAccountError::AccountNotParsable(ParsableAccount::Stake))?;
⋮----
meta: meta.into(),
⋮----
stake: Some(stake.into()),
⋮----
Ok(parsed_account)
⋮----
pub enum StakeAccountType {
⋮----
pub struct UiStakeAccount {
⋮----
pub struct UiMeta {
⋮----
fn from(meta: Meta) -> Self {
⋮----
rent_exempt_reserve: meta.rent_exempt_reserve.to_string(),
authorized: meta.authorized.into(),
lockup: meta.lockup.into(),
⋮----
pub struct UiLockup {
⋮----
fn from(lockup: Lockup) -> Self {
⋮----
custodian: lockup.custodian.to_string(),
⋮----
pub struct UiAuthorized {
⋮----
fn from(authorized: Authorized) -> Self {
⋮----
staker: authorized.staker.to_string(),
withdrawer: authorized.withdrawer.to_string(),
⋮----
pub struct UiStake {
⋮----
fn from(stake: Stake) -> Self {
⋮----
delegation: stake.delegation.into(),
⋮----
pub struct UiDelegation {
⋮----
fn from(delegation: Delegation) -> Self {
⋮----
voter: delegation.voter_pubkey.to_string(),
stake: delegation.stake.to_string(),
activation_epoch: delegation.activation_epoch.to_string(),
deactivation_epoch: delegation.deactivation_epoch.to_string(),
⋮----
mod test {
⋮----
fn test_parse_stake() {
⋮----
let stake_data = serialize(&stake_state).unwrap();
assert_eq!(
⋮----
let bad_data = vec![1, 2, 3, 4];
assert!(parse_stake(&bad_data).is_err());

================
File: account-decoder/src/parse_sysvar.rs
================
pub fn parse_sysvar(data: &[u8], pubkey: &Pubkey) -> Result<SysvarAccountType, ParseAccountError> {
⋮----
.ok()
.map(|clock| SysvarAccountType::Clock(clock.into()))
⋮----
deserialize(data).ok().map(SysvarAccountType::EpochSchedule)
⋮----
.map(|fees| SysvarAccountType::Fees(fees.into()))
⋮----
.map(|recent_blockhashes| {
⋮----
.iter()
.map(|entry| UiRecentBlockhashesEntry {
blockhash: entry.blockhash.to_string(),
fee_calculator: entry.fee_calculator.into(),
⋮----
.collect();
⋮----
.map(|rent| SysvarAccountType::Rent(rent.into()))
⋮----
.map(|rewards| SysvarAccountType::Rewards(rewards.into()))
⋮----
deserialize::<SlotHashes>(data).ok().map(|slot_hashes| {
⋮----
.map(|slot_hash| UiSlotHashEntry {
⋮----
hash: slot_hash.1.to_string(),
⋮----
deserialize::<SlotHistory>(data).ok().map(|slot_history| {
⋮----
bits: format!("{:?}", SlotHistoryBits(slot_history.bits)),
⋮----
deserialize::<StakeHistory>(data).ok().map(|stake_history| {
⋮----
.map(|entry| UiStakeHistoryEntry {
⋮----
stake_history: entry.1.clone(),
⋮----
.map(|last_restart_slot| {
⋮----
.map(|epoch_rewards| SysvarAccountType::EpochRewards(epoch_rewards.into()))
⋮----
parsed_account.ok_or(ParseAccountError::AccountNotParsable(
⋮----
pub enum SysvarAccountType {
⋮----
pub struct UiClock {
⋮----
fn from(clock: Clock) -> Self {
⋮----
pub struct UiFees {
⋮----
fn from(fees: Fees) -> Self {
⋮----
fee_calculator: fees.fee_calculator.into(),
⋮----
pub struct UiRent {
⋮----
fn from(rent: Rent) -> Self {
⋮----
lamports_per_byte_year: rent.lamports_per_byte_year.to_string(),
⋮----
pub struct UiRewards {
⋮----
fn from(rewards: Rewards) -> Self {
⋮----
pub struct UiRecentBlockhashesEntry {
⋮----
pub struct UiSlotHashEntry {
⋮----
pub struct UiSlotHistory {
⋮----
struct SlotHistoryBits(BitVec<u64>);
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
⋮----
if self.0.get(i) {
write!(f, "1")?;
⋮----
write!(f, "0")?;
⋮----
Ok(())
⋮----
pub struct UiStakeHistoryEntry {
⋮----
pub struct UiLastRestartSlot {
⋮----
pub struct UiEpochRewards {
⋮----
fn from(epoch_rewards: EpochRewards) -> Self {
⋮----
parent_blockhash: epoch_rewards.parent_blockhash.to_string(),
total_points: epoch_rewards.total_points.to_string(),
total_rewards: epoch_rewards.total_rewards.to_string(),
distributed_rewards: epoch_rewards.distributed_rewards.to_string(),
⋮----
mod test {
⋮----
use solana_sysvar::recent_blockhashes::IterItem;
⋮----
fn test_parse_sysvars() {
⋮----
let clock_sysvar = create_account_for_test(&Clock::default());
assert_eq!(
⋮----
let epoch_schedule_sysvar = create_account_for_test(&epoch_schedule);
⋮----
let fees_sysvar = create_account_for_test(&Fees::default());
⋮----
vec![IterItem(0, &hash, 10)].into_iter().collect();
let recent_blockhashes_sysvar = create_account_for_test(&recent_blockhashes);
⋮----
let rent_sysvar = create_account_for_test(&rent);
⋮----
let rewards_sysvar = create_account_for_test(&Rewards::default());
⋮----
slot_hashes.add(1, hash);
let slot_hashes_sysvar = create_account_for_test(&slot_hashes);
⋮----
slot_history.add(42);
let slot_history_sysvar = create_account_for_test(&slot_history);
⋮----
stake_history.add(1, stake_history_entry.clone());
let stake_history_sysvar = create_account_for_test(&stake_history);
⋮----
assert!(parse_sysvar(&stake_history_sysvar.data, &bad_pubkey).is_err());
let bad_data = vec![0; 4];
assert!(parse_sysvar(&bad_data, &sysvar::stake_history::id()).is_err());
⋮----
let last_restart_slot_account = create_account_for_test(&last_restart_slot);
⋮----
let epoch_rewards_sysvar = create_account_for_test(&epoch_rewards);

================
File: account-decoder/src/parse_token_extension.rs
================
pub fn parse_extension<S: BaseState + Pack>(
⋮----
.map(|&extension| {
UiExtension::TransferFeeConfig(convert_transfer_fee_config(extension))
⋮----
.unwrap_or(UiExtension::UnparseableExtension),
⋮----
UiExtension::TransferFeeAmount(convert_transfer_fee_amount(extension))
⋮----
UiExtension::MintCloseAuthority(convert_mint_close_authority(extension))
⋮----
UiExtension::ConfidentialTransferMint(convert_confidential_transfer_mint(extension))
⋮----
convert_confidential_transfer_fee_config(extension),
⋮----
UiExtension::ConfidentialTransferAccount(convert_confidential_transfer_account(
⋮----
convert_confidential_transfer_fee_amount(extension),
⋮----
UiExtension::DefaultAccountState(convert_default_account_state(extension))
⋮----
.map(|&extension| UiExtension::MemoTransfer(convert_memo_transfer(extension)))
⋮----
UiExtension::InterestBearingConfig(convert_interest_bearing_config(extension))
⋮----
.map(|&extension| UiExtension::CpiGuard(convert_cpi_guard(extension)))
⋮----
.map(|&extension| UiExtension::PermanentDelegate(convert_permanent_delegate(extension)))
⋮----
.map(|&extension| UiExtension::MetadataPointer(convert_metadata_pointer(extension)))
⋮----
.map(|extension| UiExtension::TokenMetadata(convert_token_metadata(extension)))
⋮----
.map(|&extension| UiExtension::TransferHook(convert_transfer_hook(extension)))
⋮----
UiExtension::TransferHookAccount(convert_transfer_hook_account(extension))
⋮----
.map(|&extension| UiExtension::GroupPointer(convert_group_pointer(extension)))
⋮----
UiExtension::GroupMemberPointer(convert_group_member_pointer(extension))
⋮----
.map(|&extension| UiExtension::TokenGroup(convert_token_group(extension)))
⋮----
.map(|&extension| UiExtension::TokenGroupMember(convert_token_group_member(extension)))
⋮----
UiExtension::ConfidentialMintBurn(convert_confidential_mint_burn(extension))
⋮----
UiExtension::ScaledUiAmountConfig(convert_scaled_ui_amount(extension))
⋮----
.map(|&extension| UiExtension::PausableConfig(convert_pausable_config(extension)))
⋮----
fn convert_transfer_fee(transfer_fee: extension::transfer_fee::TransferFee) -> UiTransferFee {
⋮----
fn convert_transfer_fee_config(
⋮----
transfer_fee_config.transfer_fee_config_authority.into();
⋮----
transfer_fee_config.withdraw_withheld_authority.into();
⋮----
.map(|pubkey| pubkey.to_string()),
withdraw_withheld_authority: withdraw_withheld_authority.map(|pubkey| pubkey.to_string()),
⋮----
older_transfer_fee: convert_transfer_fee(transfer_fee_config.older_transfer_fee),
newer_transfer_fee: convert_transfer_fee(transfer_fee_config.newer_transfer_fee),
⋮----
fn convert_transfer_fee_amount(
⋮----
fn convert_mint_close_authority(
⋮----
let authority: Option<Pubkey> = mint_close_authority.close_authority.into();
⋮----
close_authority: authority.map(|pubkey| pubkey.to_string()),
⋮----
fn convert_default_account_state(
⋮----
.unwrap_or_default();
⋮----
account_state: convert_account_state(account_state),
⋮----
fn convert_memo_transfer(memo_transfer: extension::memo_transfer::MemoTransfer) -> UiMemoTransfer {
⋮----
require_incoming_transfer_memos: memo_transfer.require_incoming_transfer_memos.into(),
⋮----
fn convert_interest_bearing_config(
⋮----
let rate_authority: Option<Pubkey> = interest_bearing_config.rate_authority.into();
⋮----
rate_authority: rate_authority.map(|pubkey| pubkey.to_string()),
⋮----
fn convert_cpi_guard(cpi_guard: extension::cpi_guard::CpiGuard) -> UiCpiGuard {
⋮----
lock_cpi: cpi_guard.lock_cpi.into(),
⋮----
fn convert_permanent_delegate(
⋮----
let delegate: Option<Pubkey> = permanent_delegate.delegate.into();
⋮----
delegate: delegate.map(|pubkey| pubkey.to_string()),
⋮----
pub fn convert_confidential_transfer_mint(
⋮----
let authority: Option<Pubkey> = confidential_transfer_mint.authority.into();
⋮----
confidential_transfer_mint.auditor_elgamal_pubkey.into();
⋮----
authority: authority.map(|pubkey| pubkey.to_string()),
auto_approve_new_accounts: confidential_transfer_mint.auto_approve_new_accounts.into(),
auditor_elgamal_pubkey: auditor_elgamal_pubkey.map(|pubkey| pubkey.to_string()),
⋮----
pub fn convert_confidential_transfer_fee_config(
⋮----
let authority: Option<Pubkey> = confidential_transfer_fee_config.authority.into();
⋮----
.into();
⋮----
.into(),
withheld_amount: format!("{}", confidential_transfer_fee_config.withheld_amount),
⋮----
fn convert_confidential_transfer_account(
⋮----
approved: confidential_transfer_account.approved.into(),
elgamal_pubkey: format!("{}", confidential_transfer_account.elgamal_pubkey),
pending_balance_lo: format!("{}", confidential_transfer_account.pending_balance_lo),
pending_balance_hi: format!("{}", confidential_transfer_account.pending_balance_hi),
available_balance: format!("{}", confidential_transfer_account.available_balance),
decryptable_available_balance: format!(
⋮----
fn convert_confidential_transfer_fee_amount(
⋮----
withheld_amount: format!("{}", confidential_transfer_fee_amount.withheld_amount),
⋮----
fn convert_metadata_pointer(
⋮----
let authority: Option<Pubkey> = metadata_pointer.authority.into();
let metadata_address: Option<Pubkey> = metadata_pointer.metadata_address.into();
⋮----
metadata_address: metadata_address.map(|pubkey| pubkey.to_string()),
⋮----
fn convert_token_metadata(token_metadata: TokenMetadata) -> UiTokenMetadata {
let update_authority: Option<Pubkey> = token_metadata.update_authority.into();
⋮----
update_authority: update_authority.map(|pubkey| pubkey.to_string()),
mint: token_metadata.mint.to_string(),
⋮----
fn convert_transfer_hook(transfer_hook: extension::transfer_hook::TransferHook) -> UiTransferHook {
let authority: Option<Pubkey> = transfer_hook.authority.into();
let program_id: Option<Pubkey> = transfer_hook.program_id.into();
⋮----
program_id: program_id.map(|pubkey| pubkey.to_string()),
⋮----
fn convert_transfer_hook_account(
⋮----
transferring: transfer_hook.transferring.into(),
⋮----
fn convert_group_pointer(group_pointer: extension::group_pointer::GroupPointer) -> UiGroupPointer {
let authority: Option<Pubkey> = group_pointer.authority.into();
let group_address: Option<Pubkey> = group_pointer.group_address.into();
⋮----
group_address: group_address.map(|pubkey| pubkey.to_string()),
⋮----
fn convert_group_member_pointer(
⋮----
let authority: Option<Pubkey> = member_pointer.authority.into();
let member_address: Option<Pubkey> = member_pointer.member_address.into();
⋮----
member_address: member_address.map(|pubkey| pubkey.to_string()),
⋮----
fn convert_token_group(token_group: TokenGroup) -> UiTokenGroup {
let update_authority: Option<Pubkey> = token_group.update_authority.into();
⋮----
mint: token_group.mint.to_string(),
size: token_group.size.into(),
max_size: token_group.max_size.into(),
⋮----
fn convert_token_group_member(member: TokenGroupMember) -> UiTokenGroupMember {
⋮----
mint: member.mint.to_string(),
group: member.group.to_string(),
member_number: member.member_number.into(),
⋮----
fn convert_confidential_mint_burn(
⋮----
confidential_supply: confidential_mint_burn.confidential_supply.to_string(),
decryptable_supply: confidential_mint_burn.decryptable_supply.to_string(),
supply_elgamal_pubkey: confidential_mint_burn.supply_elgamal_pubkey.to_string(),
pending_burn: confidential_mint_burn.pending_burn.to_string(),
⋮----
fn convert_scaled_ui_amount(
⋮----
let authority: Option<Pubkey> = scaled_ui_amount_config.authority.into();
let multiplier: f64 = scaled_ui_amount_config.multiplier.into();
⋮----
let new_multiplier: f64 = scaled_ui_amount_config.new_multiplier.into();
⋮----
multiplier: multiplier.to_string(),
⋮----
new_multiplier: new_multiplier.to_string(),
⋮----
fn convert_pausable_config(
⋮----
let authority: Option<Pubkey> = pausable_config.authority.into();
⋮----
paused: pausable_config.paused.into(),

================
File: account-decoder/src/parse_token.rs
================
pub fn parse_token_v3(
⋮----
let additional_data = additional_data.as_ref().ok_or_else(|| {
⋮----
"no mint_decimals provided to parse spl-token account".to_string(),
⋮----
let extension_types = account.get_extension_types().unwrap_or_default();
⋮----
.iter()
.map(|extension_type| parse_extension::<Account>(extension_type, &account))
.collect();
return Ok(TokenAccountType::Account(UiTokenAccount {
mint: account.base.mint.to_string(),
owner: account.base.owner.to_string(),
token_amount: token_amount_to_ui_amount_v3(account.base.amount, additional_data),
⋮----
COption::Some(pubkey) => Some(pubkey.to_string()),
⋮----
state: convert_account_state(account.base.state),
is_native: account.base.is_native(),
⋮----
Some(token_amount_to_ui_amount_v3(reserve, additional_data))
⋮----
delegated_amount: if account.base.delegate.is_none() {
⋮----
Some(token_amount_to_ui_amount_v3(
⋮----
let extension_types = mint.get_extension_types().unwrap_or_default();
⋮----
.map(|extension_type| parse_extension::<Mint>(extension_type, &mint))
⋮----
return Ok(TokenAccountType::Mint(UiMint {
⋮----
supply: mint.base.supply.to_string(),
⋮----
if data.len() == Multisig::get_packed_len() {
⋮----
.map_err(|_| ParseAccountError::AccountNotParsable(ParsableAccount::SplToken))?;
Ok(TokenAccountType::Multisig(UiMultisig {
⋮----
.filter_map(|pubkey| {
⋮----
Some(pubkey.to_string())
⋮----
.collect(),
⋮----
Err(ParseAccountError::AccountNotParsable(
⋮----
pub fn convert_account_state(state: AccountState) -> UiAccountState {
⋮----
pub fn token_amount_to_ui_amount_v3(
⋮----
interest_bearing_config.amount_to_ui_amount(amount, decimals, unix_timestamp);
⋮----
.as_ref()
.and_then(|x| f64::from_str(x).ok()),
ui_amount_string.unwrap_or("".to_string()),
⋮----
scaled_ui_amount_config.amount_to_ui_amount(amount, decimals, unix_timestamp);
⋮----
.checked_pow(decimals as u32)
.map(|dividend| amount as f64 / dividend as f64);
(ui_amount, real_number_string_trimmed(amount, decimals))
⋮----
amount: amount.to_string(),
⋮----
pub fn get_token_account_mint(data: &[u8]) -> Option<Pubkey> {
⋮----
.then(|| Pubkey::try_from(data.get(..32)?).ok())
.flatten()
⋮----
mod test {
⋮----
fn test_parse_token() {
⋮----
let mut account_data = vec![0; Account::get_packed_len()];
let mut account = Account::unpack_unchecked(&account_data).unwrap();
⋮----
Account::pack(account, &mut account_data).unwrap();
assert!(parse_token_v3(&account_data, None).is_err());
assert_eq!(
⋮----
let mut mint_data = vec![0; Mint::get_packed_len()];
let mut mint = Mint::unpack_unchecked(&mint_data).unwrap();
⋮----
Mint::pack(mint, &mut mint_data).unwrap();
⋮----
let mut multisig_data = vec![0; Multisig::get_packed_len()];
⋮----
let mut multisig = Multisig::unpack_unchecked(&multisig_data).unwrap();
⋮----
Multisig::pack(multisig, &mut multisig_data).unwrap();
⋮----
let bad_data = vec![0; 4];
assert!(parse_token_v3(&bad_data, None).is_err());
⋮----
fn test_get_token_account_mint() {
⋮----
fn test_ui_token_amount_real_string() {
assert_eq!(&real_number_string(1, 0), "1");
assert_eq!(&real_number_string_trimmed(1, 0), "1");
⋮----
token_amount_to_ui_amount_v3(1, &SplTokenAdditionalDataV2::with_decimals(0));
⋮----
assert_eq!(token_amount.ui_amount, Some(1.0));
assert_eq!(&real_number_string(10, 0), "10");
assert_eq!(&real_number_string_trimmed(10, 0), "10");
⋮----
token_amount_to_ui_amount_v3(10, &SplTokenAdditionalDataV2::with_decimals(0));
⋮----
assert_eq!(token_amount.ui_amount, Some(10.0));
assert_eq!(&real_number_string(1, 9), "0.000000001");
assert_eq!(&real_number_string_trimmed(1, 9), "0.000000001");
⋮----
token_amount_to_ui_amount_v3(1, &SplTokenAdditionalDataV2::with_decimals(9));
⋮----
assert_eq!(token_amount.ui_amount, Some(0.000000001));
assert_eq!(&real_number_string(1_000_000_000, 9), "1.000000000");
assert_eq!(&real_number_string_trimmed(1_000_000_000, 9), "1");
let token_amount = token_amount_to_ui_amount_v3(
⋮----
assert_eq!(&real_number_string(1_234_567_890, 3), "1234567.890");
assert_eq!(&real_number_string_trimmed(1_234_567_890, 3), "1234567.89");
⋮----
assert_eq!(token_amount.ui_amount, Some(1234567.89));
⋮----
assert_eq!(token_amount.ui_amount, None);
⋮----
fn test_ui_token_amount_with_interest() {
⋮----
initialization_timestamp: 0.into(),
pre_update_average_rate: 500.into(),
last_update_timestamp: INT_SECONDS_PER_YEAR.into(),
current_rate: 500.into(),
⋮----
interest_bearing_config: Some((config, INT_SECONDS_PER_YEAR)),
⋮----
let token_amount = token_amount_to_ui_amount_v3(ONE, &additional_data);
assert!(token_amount
⋮----
assert!((token_amount.ui_amount.unwrap() - 1.0512710963760241f64).abs() < f64::EPSILON);
let token_amount = token_amount_to_ui_amount_v3(TEN, &additional_data);
⋮----
assert!((token_amount.ui_amount.unwrap() - 10.512710963760242f64).abs() < f64::EPSILON);
⋮----
pre_update_average_rate: 32767.into(),
last_update_timestamp: 0.into(),
current_rate: 32767.into(),
⋮----
interest_bearing_config: Some((config, INT_SECONDS_PER_YEAR * 1_000)),
⋮----
let token_amount = token_amount_to_ui_amount_v3(u64::MAX, &additional_data);
assert_eq!(token_amount.ui_amount, Some(f64::INFINITY));
assert_eq!(token_amount.ui_amount_string, "inf");
⋮----
fn test_ui_token_amount_with_multiplier() {
⋮----
new_multiplier: 2f64.into(),
⋮----
scaled_ui_amount_config: Some((config, 0)),
⋮----
assert_eq!(token_amount.ui_amount_string, "2");
assert!(token_amount.ui_amount_string.starts_with("2"));
assert!((token_amount.ui_amount.unwrap() - 2.0).abs() < f64::EPSILON);
⋮----
assert!(token_amount.ui_amount_string.starts_with("20"));
assert!((token_amount.ui_amount.unwrap() - 20.0).abs() < f64::EPSILON);
⋮----
new_multiplier: f64::INFINITY.into(),
⋮----
fn test_ui_token_amount_real_string_zero() {
assert_eq!(&real_number_string(0, 0), "0");
assert_eq!(&real_number_string_trimmed(0, 0), "0");
⋮----
token_amount_to_ui_amount_v3(0, &SplTokenAdditionalDataV2::with_decimals(0));
⋮----
assert_eq!(token_amount.ui_amount, Some(0.0));
assert_eq!(&real_number_string(0, 9), "0.000000000");
assert_eq!(&real_number_string_trimmed(0, 9), "0");
⋮----
token_amount_to_ui_amount_v3(0, &SplTokenAdditionalDataV2::with_decimals(9));
⋮----
assert_eq!(&real_number_string(0, 25), "0.0000000000000000000000000");
assert_eq!(&real_number_string_trimmed(0, 25), "0");
⋮----
token_amount_to_ui_amount_v3(0, &SplTokenAdditionalDataV2::with_decimals(20));
⋮----
fn test_parse_token_account_with_extensions() {
⋮----
.unwrap();
let mut account_data = vec![0; account_size];
⋮----
StateWithExtensionsMut::<Account>::unpack_uninitialized(&mut account_data).unwrap();
⋮----
account_state.pack_base();
account_state.init_account_type().unwrap();
⋮----
let memo_transfer = account_state.init_extension::<MemoTransfer>(true).unwrap();
memo_transfer.require_incoming_transfer_memos = true.into();
⋮----
fn test_parse_token_mint_with_extensions() {
⋮----
let mut mint_data = vec![0; mint_size];
⋮----
StateWithExtensionsMut::<Mint>::unpack_uninitialized(&mut mint_data).unwrap();
⋮----
mint_state.pack_base();
mint_state.init_account_type().unwrap();
⋮----
OptionalNonZeroPubkey::try_from(Some(owner_pubkey)).unwrap();

================
File: account-decoder/src/parse_vote.rs
================
pub fn parse_vote(data: &[u8], vote_pubkey: &Pubkey) -> Result<VoteAccountType, ParseAccountError> {
⋮----
VoteStateV4::deserialize(data, vote_pubkey).map_err(ParseAccountError::from)?;
⋮----
.iter()
.map(|(epoch, credits, previous_credits)| UiEpochCredits {
⋮----
credits: credits.to_string(),
previous_credits: previous_credits.to_string(),
⋮----
.collect();
let votes = vote_state.votes.iter().map(UiLandedVote::from).collect();
⋮----
.map(|(epoch, authorized_voter)| UiAuthorizedVoters {
⋮----
authorized_voter: authorized_voter.to_string(),
⋮----
Ok(VoteAccountType::Vote(UiVoteState {
node_pubkey: vote_state.node_pubkey.to_string(),
authorized_withdrawer: vote_state.authorized_withdrawer.to_string(),
⋮----
inflation_rewards_collector: vote_state.inflation_rewards_collector.to_string(),
block_revenue_collector: vote_state.block_revenue_collector.to_string(),
⋮----
pending_delegator_rewards: vote_state.pending_delegator_rewards.to_string(),
⋮----
.map(|bytes| bs58::encode(bytes).into_string()),
⋮----
pub enum VoteAccountType {
⋮----
pub struct UiVoteState {
⋮----
struct UiLockout {
⋮----
fn from(lockout: &Lockout) -> Self {
⋮----
slot: lockout.slot(),
confirmation_count: lockout.confirmation_count(),
⋮----
struct UiLandedVote {
⋮----
fn from(landed_vote: &LandedVote) -> Self {
⋮----
struct UiAuthorizedVoters {
⋮----
struct UiPriorVoters {
⋮----
struct UiEpochCredits {
⋮----
mod test {
⋮----
fn test_parse_vote() {
⋮----
let mut vote_account_data: Vec<u8> = vec![0; VoteStateV4::size_of()];
let versioned = VoteStateVersions::new_v4(vote_state.clone());
VoteStateV4::serialize(&versioned, &mut vote_account_data).unwrap();
⋮----
node_pubkey: Pubkey::default().to_string(),
authorized_withdrawer: Pubkey::default().to_string(),
⋮----
votes: vec![],
⋮----
authorized_voters: vec![],
prior_voters: vec![],
epoch_credits: vec![],
⋮----
assert_eq!(
⋮----
let bad_data = vec![0; 4];
assert!(parse_vote(&bad_data, &vote_pubkey).is_err());
⋮----
fn test_ui_landed_vote_flatten() {
⋮----
let json = serde_json::to_value(&ui_landed_vote).unwrap();
assert_eq!(json["latency"], 5);
assert_eq!(json["slot"], 12345);
assert_eq!(json["confirmationCount"], 10);
assert!(json.get("lockout").is_none());
⋮----
let deserialized: UiLandedVote = serde_json::from_str(json_str).unwrap();
assert_eq!(deserialized, ui_landed_vote);

================
File: account-decoder/src/validator_info.rs
================
pub struct ValidatorInfo {

================
File: account-decoder/Cargo.toml
================
[package]
name = "solana-account-decoder"
description = "Solana account decoder"
documentation = "https://docs.rs/solana-account-decoder"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
Inflector = { workspace = true }
base64 = { workspace = true }
bincode = { workspace = true }
bs58 = { workspace = true }
bv = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account = { workspace = true }
solana-account-decoder-client-types = { workspace = true, features = ["zstd"] }
solana-address-lookup-table-interface = { workspace = true, features = [
    "bincode",
    "bytemuck",
] }
solana-clock = { workspace = true }
solana-config-interface = { workspace = true, features = ["bincode"] }
solana-epoch-schedule = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-instruction = { workspace = true }
solana-loader-v3-interface = { workspace = true, features = ["serde"] }
solana-nonce = { workspace = true, features = ["serde"] }
solana-program-option = { workspace = true }
solana-program-pack = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-slot-hashes = { workspace = true }
solana-slot-history = { workspace = true }
solana-stake-interface = { workspace = true, features = ["bincode", "sysvar"] }
solana-sysvar = { workspace = true }
solana-vote-interface = { workspace = true, features = ["bincode"] }
spl-generic-token = { workspace = true }
spl-token-2022-interface = { workspace = true }
spl-token-group-interface = { workspace = true }
spl-token-interface = { workspace = true }
spl-token-metadata-interface = { workspace = true }
thiserror = { workspace = true }
zstd = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
solana-account = { workspace = true, features = ["bincode"] }
solana-hash = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
spl-pod = { workspace = true }

[lints]
workspace = true

================
File: account-decoder-client-types/src/lib.rs
================
use std::io::Read;
⋮----
pub mod token;
⋮----
pub struct UiAccount {
⋮----
pub enum UiAccountData {
⋮----
impl UiAccountData {
pub fn decode(&self) -> Option<Vec<u8>> {
⋮----
UiAccountData::LegacyBinary(blob) => bs58::decode(blob).into_vec().ok(),
⋮----
UiAccountEncoding::Base58 => bs58::decode(blob).into_vec().ok(),
UiAccountEncoding::Base64 => BASE64_STANDARD.decode(blob).ok(),
⋮----
BASE64_STANDARD.decode(blob).ok().and_then(|zstd_data| {
let mut data = vec![];
zstd::stream::read::Decoder::new(zstd_data.as_slice())
.and_then(|mut reader| reader.read_to_end(&mut data))
.map(|_| data)
.ok()
⋮----
pub enum UiAccountEncoding {
⋮----
impl UiAccount {
pub fn decode<T: WritableAccount>(&self) -> Option<T> {
let data = self.data.decode()?;
Some(T::create(
⋮----
Pubkey::from_str(&self.owner).ok()?,
⋮----
pub struct ParsedAccount {
⋮----
pub struct UiDataSliceConfig {

================
File: account-decoder-client-types/src/token.rs
================
pub struct UiTokenAmount {
⋮----
impl UiTokenAmount {
pub fn real_number_string(&self) -> String {
real_number_string(
u64::from_str(&self.amount).unwrap_or_default(),
⋮----
pub fn real_number_string_trimmed(&self) -> String {
if !self.ui_amount_string.is_empty() {
self.ui_amount_string.clone()
⋮----
real_number_string_trimmed(
⋮----
pub fn real_number_string(amount: u64, decimals: u8) -> String {
⋮----
let mut s = format!("{:01$}", amount, decimals + 1);
s.insert(s.len() - decimals, '.');
⋮----
amount.to_string()
⋮----
pub fn real_number_string_trimmed(amount: u64, decimals: u8) -> String {
let mut s = real_number_string(amount, decimals);
⋮----
let zeros_trimmed = s.trim_end_matches('0');
s = zeros_trimmed.trim_end_matches('.').to_string();
⋮----
pub enum TokenAccountType {
⋮----
pub struct UiTokenAccount {
⋮----
pub enum UiAccountState {
⋮----
pub enum UiExtension {
⋮----
pub struct UiTokenGroupMember {
⋮----
pub struct UiTokenGroup {
⋮----
pub struct UiMetadataPointer {
⋮----
pub struct UiTransferHook {
⋮----
pub struct UiTransferHookAccount {
⋮----
pub struct UiConfidentialTransferMint {
⋮----
pub struct UiConfidentialTransferFeeAmount {
⋮----
pub struct UiConfidentialTransferFeeConfig {
⋮----
pub struct UiPermanentDelegate {
⋮----
pub struct UiCpiGuard {
⋮----
pub struct UiInterestBearingConfig {
⋮----
pub struct UiMemoTransfer {
⋮----
pub struct UiDefaultAccountState {
⋮----
pub struct UiConfidentialTransferAccount {
⋮----
pub struct UiMintCloseAuthority {
⋮----
pub struct UiTransferFeeAmount {
⋮----
pub struct UiTransferFeeConfig {
⋮----
pub struct UiTransferFee {
⋮----
pub struct UiGroupMemberPointer {
⋮----
pub struct UiGroupPointer {
⋮----
pub struct UiTokenMetadata {
⋮----
pub struct UiConfidentialMintBurn {
⋮----
pub struct UiMint {
⋮----
pub struct UiMultisig {
⋮----
pub struct UiScaledUiAmountConfig {
⋮----
pub struct UiPausableConfig {

================
File: account-decoder-client-types/Cargo.toml
================
[package]
name = "solana-account-decoder-client-types"
description = "Core RPC client types for solana-account-decoder"
documentation = "https://docs.rs/solana-account-decoder-client-types"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]
all-features = true
rustdoc-args = ["--cfg=docsrs"]

[features]
agave-unstable-api = []
zstd = ["dep:zstd"]

[dependencies]
base64 = { workspace = true }
bs58 = { workspace = true, features = ["std"] }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account = { workspace = true }
solana-pubkey = { workspace = true }
zstd = { workspace = true, optional = true }

================
File: accounts-cluster-bench/src/main.rs
================
pub fn poll_slot_height(client: &RpcClient) -> Slot {
⋮----
let response = client.get_slot_with_commitment(CommitmentConfig::confirmed());
⋮----
warn!("get_slot_height failure: {response:?}. remaining retries {num_retries}");
⋮----
panic!("failed to get_slot_height(), rpc node down?")
⋮----
sleep(Duration::from_millis(100));
⋮----
pub fn poll_get_latest_blockhash(client: &RpcClient) -> Option<Hash> {
⋮----
let response = client.get_latest_blockhash();
⋮----
return Some(blockhash);
⋮----
warn!("get_latest_blockhash failure: {response:?}. remaining retries {num_retries}");
⋮----
panic!("failed to get_latest_blockhash(), rpc node down?")
⋮----
pub fn poll_get_fee_for_message(client: &RpcClient, message: &mut Message) -> (Option<u64>, Hash) {
⋮----
let response = client.get_fee_for_message(message);
⋮----
return (Some(fee), message.recent_blockhash);
⋮----
warn!("get_fee_for_message failure: {response:?}. remaining retries {num_retries}");
let blockhash = poll_get_latest_blockhash(client).expect("blockhash");
⋮----
panic!("failed to get_fee_for_message(), rpc node down?")
⋮----
fn airdrop_lamports(client: &RpcClient, id: &Keypair, desired_balance: u64) -> bool {
let starting_balance = client.get_balance(&id.pubkey()).unwrap_or(0);
info!("starting balance {starting_balance}");
⋮----
info!(
⋮----
let blockhash = client.get_latest_blockhash().unwrap();
⋮----
client.request_airdrop_with_blockhash(&id.pubkey(), airdrop_amount, &blockhash)
⋮----
panic!(
⋮----
let current_balance = client.get_balance(&id.pubkey()).unwrap_or_else(|e| {
panic!("airdrop error {e}");
⋮----
info!("current balance {current_balance}...");
⋮----
struct SeedTracker {
⋮----
struct TransactionSignatureTracker(Arc<RwLock<VecDeque<Signature>>>);
impl TransactionSignatureTracker {
fn get_random(&self) -> Option<Signature> {
let signatures = self.read().unwrap();
if signatures.is_empty() {
⋮----
let random_index = rng().random_range(0..signatures.len());
let random_signature = signatures.get(random_index);
random_signature.cloned()
⋮----
fn track_transactions(&self, transactions: &[Transaction]) {
let mut lock = self.write().unwrap();
for signature in transactions.iter().map(Transaction::get_signature) {
lock.push_back(*signature);
⋮----
impl Deref for TransactionSignatureTracker {
type Target = Arc<RwLock<VecDeque<Signature>>>;
fn deref(&self) -> &Self::Target {
⋮----
fn make_create_message(
⋮----
let space = if mint.is_some() {
⋮----
maybe_space.unwrap_or_else(|| rng().random_range(0..1000))
⋮----
.flat_map(|_| {
let program_id = if mint.is_some() {
⋮----
let seed = max_created_seed.fetch_add(1, Ordering::Relaxed).to_string();
⋮----
Pubkey::create_with_seed(&base_keypair.pubkey(), &seed, &program_id).unwrap();
let mut instructions = vec![system_instruction::create_account_with_seed(
⋮----
instructions.push(
⋮----
&base_keypair.pubkey(),
⋮----
.unwrap(),
⋮----
&[&base_keypair.pubkey()],
⋮----
.collect();
Message::new(&instructions, Some(&keypair.pubkey()))
⋮----
fn make_close_message(
⋮----
.filter_map(|_| {
⋮----
let max_created_seed = max_created.load(Ordering::Relaxed);
let max_closed_seed = max_closed.load(Ordering::Relaxed);
⋮----
let seed = max_closed.fetch_add(1, Ordering::Relaxed).to_string();
⋮----
Some(
⋮----
&keypair.pubkey(),
⋮----
Some(system_instruction::transfer_with_seed(
⋮----
pub enum RpcBench {
⋮----
pub enum RpcParseError {
⋮----
impl FromStr for RpcBench {
type Err = RpcParseError;
fn from_str(s: &str) -> Result<Self, Self::Err> {
⋮----
"account-info" => Ok(RpcBench::AccountInfo),
"block" => Ok(RpcBench::Block),
"blocks" => Ok(RpcBench::Blocks),
"first-available-block" => Ok(RpcBench::FirstAvailableBlock),
"slot" => Ok(RpcBench::Slot),
"supply" => Ok(RpcBench::Supply),
"multiple-accounts" => Ok(RpcBench::MultipleAccounts),
"token-accounts-by-delegate" => Ok(RpcBench::TokenAccountsByDelegate),
"token-accounts-by-owner" => Ok(RpcBench::TokenAccountsByOwner),
"token-supply" => Ok(RpcBench::TokenSupply),
"transaction" => Ok(RpcBench::Transaction),
"transaction-parsed" => Ok(RpcBench::TransactionParsed),
"version" => Ok(RpcBench::Version),
_ => Err(RpcParseError::InvalidOption),
⋮----
fn process_get_multiple_accounts(
⋮----
let start = max_closed.load(Ordering::Relaxed);
let end = max_created.load(Ordering::Relaxed);
⋮----
.map(|seed| {
Pubkey::create_with_seed(base_keypair_pubkey, &seed.to_string(), program_id)
.unwrap()
⋮----
match client.get_multiple_accounts(&addresses) {
⋮----
rpc_time.stop();
for account in accounts.into_iter().flatten() {
if rng().random_ratio(1, 10_000) {
⋮----
stats.total_success_time_us += rpc_time.as_us();
⋮----
stats.total_errors_time_us += rpc_time.as_us();
⋮----
if last_error.elapsed().as_secs() > 2 {
info!("error: {e:?}");
⋮----
debug!("error: {e:?}");
⋮----
fn process_get_transaction(
⋮----
let Some(signature) = transaction_signature_tracker.get_random() else {
info!("transaction: No transactions have yet been made; skipping");
⋮----
match client.get_transaction(&signature, encoding) {
⋮----
measure.stop();
⋮----
stats.total_success_time_us += measure.as_us();
⋮----
stats.total_errors_time_us += measure.as_us();
⋮----
info!("get_transaction error: {:?}", &e);
⋮----
struct RpcBenchStats {
⋮----
fn run_rpc_bench_loop(
⋮----
fn flush_stats(
⋮----
if exit.load(Ordering::Relaxed) {
flush_stats(&iters, &mut last_print, &rpc_bench, &mut stats, &thread);
⋮----
let start: u64 = max_closed.load(Ordering::Relaxed);
let end: u64 = max_created.load(Ordering::Relaxed);
⋮----
if seed_range.is_empty() {
info!("get_account_info: No accounts have yet been created; skipping");
⋮----
let seed = rng().random_range(seed_range).to_string();
⋮----
Pubkey::create_with_seed(base_keypair_pubkey, &seed, program_id).unwrap();
⋮----
match client.get_account(&account_pubkey) {
⋮----
info!("get_account_info error: {e:?}");
⋮----
let slot_height = slot_height.load(Ordering::Relaxed);
⋮----
match client.get_block_with_config(
⋮----
commitment: Some(CommitmentConfig::confirmed()),
⋮----
info!("get_block error: {e:?}");
⋮----
match client.get_blocks_with_commitment(
slot_height.saturating_sub(MAX_GET_CONFIRMED_BLOCKS_RANGE),
Some(slot_height),
⋮----
info!("get_blocks error: {e:?}");
⋮----
match client.get_first_available_block() {
⋮----
info!("get_first_available_block error: {e:?}");
⋮----
match client.get_slot() {
⋮----
info!("get_slot error: {e:?}");
⋮----
match client.get_token_supply(&mint.unwrap()) {
⋮----
info!("get_token_supply error: {e:?}");
⋮----
process_get_multiple_accounts(
⋮----
match client.get_program_accounts(program_id) {
⋮----
if rng().random_ratio(1, 100) {
info!("accounts: {} first: {:?}", accounts.len(), accounts.first());
⋮----
info!("get-program-accounts error: {e:?}");
⋮----
let filter = TokenAccountsFilter::Mint(*mint.as_ref().unwrap());
match client.get_token_accounts_by_delegate(base_keypair_pubkey, filter) {
⋮----
info!("get-token-accounts-by-delegate error: {e:?}");
⋮----
match client.get_token_accounts_by_owner(base_keypair_pubkey, filter) {
⋮----
info!("get-token-accounts-by-owner error: {e:?}");
⋮----
.get_token_supply_with_commitment(&mint.unwrap(), CommitmentConfig::confirmed())
⋮----
info!("get-token-supply error: {e:?}");
⋮----
process_get_transaction(
⋮----
match client.get_version() {
⋮----
if last_print.elapsed().as_secs() > 3 {
⋮----
fn make_rpc_bench_threads(
⋮----
.into_iter()
.flat_map(|rpc_bench| {
(0..num_rpc_bench_threads).map(move |thread| {
let client = client.clone();
let start_bench = start_bench_barrier.clone();
let exit = exit.clone();
let max_closed = seed_tracker.max_closed.clone();
let max_created = seed_tracker.max_created.clone();
let slot_height = slot_height.clone();
let transaction_signature_tracker = transaction_signature_tracker.clone();
⋮----
.name(format!("rpc-bench-{thread}"))
.spawn(move || {
start_bench.wait();
run_rpc_bench_loop(
⋮----
.collect()
⋮----
fn run_accounts_bench(
⋮----
assert!(num_instructions > 0);
info!("Targeting {}", client.url());
⋮----
let mut blockhash = poll_get_latest_blockhash(&client).expect("blockhash");
let slot_height = Arc::new(AtomicU64::new(poll_slot_height(&client)));
⋮----
.iter()
.map(|keypair| client.get_balance(&keypair.pubkey()).unwrap_or(0))
⋮----
let min_balance = maybe_lamports.unwrap_or_else(|| {
let space = maybe_space.unwrap_or(default_max_lamports);
⋮----
.get_minimum_balance_for_rent_exemption(space as usize)
.expect("min balance")
⋮----
TransactionSignatureTracker(Arc::new(RwLock::new(VecDeque::with_capacity(5000))));
info!("Starting balance(s): {balances:?}");
let executor = TransactionExecutor::new_with_rpc_client(client.clone());
// Create and close messages both require 2 signatures, fake a 2 signature message to calculate fees
⋮----
vec![AccountMeta::new(Pubkey::new_unique(), true)],
⋮----
let mut start_bench_barrier = Some(Arc::new(Barrier::new(
// In order to unlock the benchmark threads, `wait()` must be called on each thread and then
// once from this thread, after the first pass through the account creation loop.
⋮----
let base_keypair_pubkey = base_keypair.pubkey();
⋮----
make_rpc_bench_threads(
⋮----
start_bench_barrier.as_ref().unwrap(),
⋮----
if latest_blockhash.elapsed().as_millis() > 10_000 {
blockhash = poll_get_latest_blockhash(&client).expect("blockhash");
slot_height.store(poll_slot_height(&client), Ordering::Relaxed);
⋮----
let (fee, blockhash) = poll_get_fee_for_message(&client, &mut message);
let fee = fee.expect("get_fee_for_message");
⋮----
for (i, balance) in balances.iter_mut().enumerate() {
if *balance < lamports || last_balance.elapsed().as_millis() > 2000 {
if let Ok(b) = client.get_balance(&payer_keypairs[i].pubkey()) {
⋮----
info!("Balance {balance} is less than needed: {lamports}, doing airdrop...");
if !airdrop_lamports(&client, payer_keypairs[i], lamports * 100_000) {
warn!("failed airdrop, exiting");
⋮----
// Create accounts
let sigs_len = executor.num_outstanding();
⋮----
if num_to_create >= payer_keypairs.len() {
info!("creating {num_to_create} new");
let chunk_size = num_to_create / payer_keypairs.len();
⋮----
for (i, keypair) in payer_keypairs.iter().enumerate() {
⋮----
.into_par_iter()
.map(|_| {
let message = make_create_message(
⋮----
seed_tracker.max_created.clone(),
⋮----
let signers: Vec<&Keypair> = vec![keypair, &base_keypair];
⋮----
balances[i] = balances[i].saturating_sub(lamports * txs.len() as u64);
info!("txs: {}", txs.len());
transaction_signature_tracker.track_transactions(&txs);
let new_ids = executor.push_transactions(txs);
info!("ids: {}", new_ids.len());
tx_sent_count += new_ids.len();
total_accounts_created += num_instructions * new_ids.len();
⋮----
let max_closed_seed = seed_tracker.max_closed.load(Ordering::Relaxed);
// Close every account we've created with seed between max_closed_seed..expected_closed
⋮----
let message = make_close_message(
⋮----
mint.is_some(),
⋮----
let signers: Vec<&Keypair> = vec![payer_keypairs[0], &base_keypair];
⋮----
balances[0] = balances[0].saturating_sub(fee * txs.len() as u64);
info!("close txs: {}", txs.len());
⋮----
info!("close ids: {}", new_ids.len());
⋮----
total_accounts_closed += new_ids.len() as u64;
⋮----
let _ = executor.drain_cleared();
⋮----
if last_log.elapsed().as_millis() > 3000
⋮----
info!("{iterations} iterations reached");
⋮----
if executor.num_outstanding() >= batch_size {
sleep(Duration::from_millis(500));
⋮----
executor.close();
⋮----
let max_created_seed = seed_tracker.max_created.load(Ordering::Relaxed);
⋮----
let num_to_close = min(
⋮----
if num_to_close >= payer_keypairs.len() {
info!("closing {num_to_close} accounts");
let chunk_size = num_to_close / payer_keypairs.len();
info!("{chunk_size:?} chunk_size");
⋮----
if message.instructions.is_empty() {
⋮----
Some(Transaction::new(&signers, message, blockhash))
⋮----
balances[i] = balances[i].saturating_sub(fee * txs.len() as u64);
⋮----
total_accounts_closed += (num_instructions * new_ids.len()) as u64;
⋮----
if last_log.elapsed().as_millis() > 3000 || max_closed_seed >= max_created_seed {
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
t.join().unwrap();
⋮----
fn main() {
⋮----
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg({
⋮----
.short("C")
.long("config")
.value_name("FILEPATH")
.takes_value(true)
.help("Configuration file to use");
⋮----
arg.default_value(config_file)
⋮----
.arg(
⋮----
.short("u")
.long("url")
.value_name("URL_OR_MONIKER")
⋮----
.validator(is_url_or_moniker)
.conflicts_with("entrypoint")
.help(
⋮----
.long("entrypoint")
⋮----
.value_name("HOST:PORT")
.conflicts_with("json_rpc_url")
.help("RPC entrypoint address. Usually <ip>:8899"),
⋮----
.long("faucet")
⋮----
.hidden(hidden_unless_forced())
.help("Faucet entrypoint address. Usually <ip>:9900"),
⋮----
.long("space")
⋮----
.value_name("BYTES")
.conflicts_with("mint")
.help("Size of accounts to create"),
⋮----
.long("lamports")
⋮----
.value_name("LAMPORTS")
.help("How many lamports to fund each account"),
⋮----
.long("identity")
⋮----
.multiple(true)
.value_name("FILE")
.help("keypair file"),
⋮----
.long("batch-size")
⋮----
.help("Number of transactions to send per batch"),
⋮----
.long("close-frequency")
⋮----
.long("num-instructions")
⋮----
.value_name("NUM_INSTRUCTIONS")
.help("Number of accounts to create on each transaction"),
⋮----
.long("iterations")
⋮----
.value_name("NUM_ITERATIONS")
.help("Number of iterations to make. 0 = unlimited iterations."),
⋮----
.long("max-accounts")
⋮----
.value_name("NUM_ACCOUNTS")
⋮----
.long("check-gossip")
.help("Just use entrypoint address directly"),
⋮----
.long("shred-version")
⋮----
.value_name("VERSION")
.requires("check_gossip")
.help("The shred version to use for gossip discovery"),
⋮----
.long("mint")
⋮----
.value_name("MINT_ADDRESS")
.help("Mint address to initialize account"),
⋮----
.long("reclaim-accounts")
.takes_value(false)
.help("Reclaim accounts after session ends; incompatible with --iterations 0"),
⋮----
.long("num-rpc-bench-threads")
⋮----
.value_name("NUM_THREADS")
.help("Spawn this many RPC benching threads for each type passed by --rpc-bench"),
⋮----
.long("rpc-bench")
⋮----
.value_name("RPC_BENCH_TYPE(S)")
⋮----
.requires_ifs(&[("supply", "mint"), ("token-accounts-by-owner", "mint")])
.requires_ifs(&[
⋮----
.help("Spawn a thread which calls a specific RPC method in a loop to benchmark it"),
⋮----
.get_matches();
let skip_gossip = !matches.is_present("check_gossip");
let space = value_t!(matches, "space", u64).ok();
let lamports = value_t!(matches, "lamports", u64).ok();
let batch_size = value_t!(matches, "batch_size", usize).unwrap_or(4);
let close_nth_batch = value_t!(matches, "close_nth_batch", u64).unwrap_or(0);
let iterations = value_t!(matches, "iterations", usize).unwrap_or(10);
let max_accounts = value_t!(matches, "max_accounts", usize).ok();
let num_instructions = value_t!(matches, "num_instructions", usize).unwrap_or(1);
⋮----
eprintln!("bad num_instructions: {num_instructions}");
exit(1);
⋮----
let rpc_benches = values_t!(matches, "rpc_bench", String)
.map(|benches| {
⋮----
.map(|bench| RpcBench::from_str(&bench).unwrap())
⋮----
.ok();
let num_rpc_bench_threads = if rpc_benches.is_none() {
⋮----
value_t!(matches, "num_rpc_bench_threads", usize).unwrap_or(1)
⋮----
let mint = pubkey_of(&matches, "mint");
let payer_keypairs: Vec<_> = values_t_or_exit!(matches, "identity", String)
⋮----
.map(|keypair_string| {
read_keypair_file(keypair_string)
.unwrap_or_else(|_| panic!("bad keypair {keypair_string:?}"))
⋮----
let mut payer_keypair_refs: Vec<&Keypair> = vec![];
for keypair in payer_keypairs.iter() {
payer_keypair_refs.push(keypair);
⋮----
let client = if let Some(addr) = matches.value_of("entrypoint") {
let entrypoint_addr = solana_net_utils::parse_host_port(addr).unwrap_or_else(|e| {
eprintln!("failed to parse entrypoint address: {e}");
exit(1)
⋮----
if let Ok(version) = value_t!(matches, "shred_version", u16) {
Some(version)
⋮----
solana_net_utils::get_cluster_shred_version(&entrypoint_addr).unwrap_or_else(
⋮----
eprintln!("Failed to get shred version: {err}");
⋮----
info!("Finding cluster entry: {entrypoint_addr:?}");
let (gossip_nodes, _validators) = discover_peers(
⋮----
&vec![entrypoint_addr],
⋮----
shred_version.unwrap(),
⋮----
.unwrap_or_else(|err| {
eprintln!("Failed to discover {entrypoint_addr} node: {err:?}");
⋮----
info!("done found {} nodes", gossip_nodes.len());
gossip_nodes[0].rpc().unwrap()
⋮----
info!("Using {entrypoint_addr:?} as the RPC address");
⋮----
let config = if let Some(config_file) = matches.value_of("config_file") {
solana_cli_config::Config::load(config_file).unwrap_or_default()
⋮----
matches.value_of("json_rpc_url").unwrap_or(""),
⋮----
run_accounts_bench(
⋮----
matches.is_present("reclaim_accounts"),
⋮----
pub mod test {
⋮----
fn initialize_and_add_secondary_indexes(validator_config: &mut ValidatorConfig) {
⋮----
if account_indexes.is_none() {
*account_indexes = Some(AccountSecondaryIndexes::default());
⋮----
add_secondary_indexes(account_indexes.as_mut().unwrap());
add_secondary_indexes(&mut validator_config.rpc_config.account_indexes);
⋮----
fn add_secondary_indexes(indexes: &mut AccountSecondaryIndexes) {
indexes.indexes.insert(AccountIndex::SplTokenOwner);
indexes.indexes.insert(AccountIndex::SplTokenMint);
indexes.indexes.insert(AccountIndex::ProgramId);
⋮----
fn test_accounts_cluster_bench() {
⋮----
initialize_and_add_secondary_indexes(&mut validator_config);
⋮----
node_stakes: vec![100; num_nodes],
validator_configs: make_identical_validator_configs(&validator_config, num_nodes),
⋮----
let rpc_addr = cluster.entry_point_info.rpc().unwrap();
⋮----
let pre_txs = client.get_transaction_count().unwrap();
⋮----
client.clone(),
⋮----
Some(vec![RpcBench::ProgramAccounts]),
⋮----
let post_txs = client.get_transaction_count().unwrap();
start.stop();
info!("{start} pre {pre_txs} post {post_txs}");
⋮----
fn test_halt_accounts_creation_at_max() {
⋮----
Some(90),
⋮----
fn test_create_then_reclaim_spl_token_accounts() {
⋮----
let mint_pubkey = mint_keypair.pubkey();
let faucet_addr = run_local_faucet_for_tests(
⋮----
Some(faucet_addr),
⋮----
test_validator.rpc_url(),
⋮----
let latest_blockhash = rpc_client.get_latest_blockhash().unwrap();
⋮----
.request_airdrop_with_blockhash(&funder.pubkey(), LAMPORTS_PER_SOL, &latest_blockhash)
.unwrap();
⋮----
.confirm_transaction_with_spinner(
⋮----
.get_minimum_balance_for_rent_exemption(spl_mint_len)
⋮----
&funder.pubkey(),
&spl_mint_keypair.pubkey(),
⋮----
Some(&funder.pubkey()),
⋮----
.send_and_confirm_transaction(&transaction)
⋮----
.get_minimum_balance_for_rent_exemption(account_len)
⋮----
Some(account_len as u64),
⋮----
Some(minimum_balance),
⋮----
Some(spl_mint_keypair.pubkey()),
⋮----
info!("{start}");

================
File: accounts-cluster-bench/.gitignore
================
/farf/

================
File: accounts-cluster-bench/Cargo.toml
================
[package]
name = "solana-accounts-cluster-bench"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
agave-logger = { workspace = true }
clap = { workspace = true }
log = { workspace = true }
rand = { workspace = true }
rayon = { workspace = true }
solana-account-decoder = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true, features = ["serde", "bincode", "blake3"] }
solana-net-utils = { workspace = true }
solana-program-pack = { workspace = true }
solana-pubkey = { workspace = true }
solana-rpc-client = { workspace = true, features = ["default"] }
solana-rpc-client-api = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true, features = ["bincode"] }
solana-transaction = { workspace = true }
solana-transaction-status = { workspace = true }
solana-version = { workspace = true }
spl-generic-token = { workspace = true }
spl-token-interface = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dependencies]
jemallocator = { workspace = true }

[dev-dependencies]
solana-accounts-db = { workspace = true }
solana-core = { workspace = true, features = ["dev-context-only-utils"] }
solana-faucet = { workspace = true, features = ["dev-context-only-utils"] }
solana-local-cluster = { workspace = true, features = ["dev-context-only-utils"] }
solana-native-token = { workspace = true }
solana-poh-config = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-test-validator = { workspace = true }

================
File: accounts-db/benches/accounts_index.rs
================
extern crate test;
⋮----
fn bench_accounts_index(bencher: &mut Bencher) {
⋮----
.map(|_| solana_pubkey::new_rand())
.collect();
⋮----
for pubkey in pubkeys.iter().take(NUM_PUBKEYS) {
index.upsert(
⋮----
bencher.iter(|| {
⋮----
let pubkey = rng().random_range(0..NUM_PUBKEYS);
⋮----
reclaims.clear();
⋮----
index.add_root(root);

================
File: accounts-db/benches/accounts.rs
================
extern crate test;
⋮----
fn new_accounts_db(account_paths: Vec<PathBuf>) -> AccountsDb {
⋮----
fn bench_delete_dependencies(bencher: &mut Bencher) {
⋮----
let accounts_db = new_accounts_db(vec![PathBuf::from("accounts_delete_deps")]);
⋮----
let zero_account = AccountSharedData::new(0, 0, AccountSharedData::default().owner());
⋮----
let account = AccountSharedData::new(i + 1, 0, AccountSharedData::default().owner());
⋮----
.store_for_tests((i, [(&pubkey, &account)].as_slice()));
⋮----
.store_for_tests((i, [(&old_pubkey, &zero_account)].as_slice()));
⋮----
accounts.accounts_db.add_root_and_flush_write_cache(i);
⋮----
bencher.iter(|| {
accounts.accounts_db.clean_accounts_for_tests();
⋮----
fn store_accounts_with_possible_contention<F>(bench_name: &str, bencher: &mut Bencher, reader_f: F)
⋮----
let accounts_db = new_accounts_db(vec![PathBuf::from(
⋮----
.take(num_keys)
.collect();
⋮----
let storable_accounts: Vec<_> = pubkeys.iter().zip(accounts_data.iter()).collect();
accounts.store_accounts_par((slot, storable_accounts.as_slice()), None);
accounts.add_root(slot);
⋮----
.flush_accounts_cache_slot_for_tests(slot);
⋮----
let accounts = accounts.clone();
let pubkeys = pubkeys.clone();
⋮----
.name(format!("reader{i:02}"))
.spawn(move || {
reader_f(&accounts, &pubkeys);
⋮----
.unwrap();
⋮----
.take(num_new_keys)
⋮----
let new_storable_accounts: Vec<_> = new_pubkeys.iter().zip(accounts_data.iter()).collect();
// Write to a different slot than the one being read from. Because
// there's a new account pubkey being written to every time, will
accounts.store_accounts_par((slot + 1, new_storable_accounts.as_slice()), None);
⋮----
fn bench_concurrent_read_write(bencher: &mut Bencher) {
store_accounts_with_possible_contention(
⋮----
let i = rng.random_range(0..pubkeys.len());
⋮----
.load_without_fixed_root(&Ancestors::default(), &pubkeys[i])
.unwrap(),
⋮----
fn bench_concurrent_scan_write(bencher: &mut Bencher) {
store_accounts_with_possible_contention("concurrent_scan_write", bencher, |accounts, _| loop {
⋮----
.load_by_program(
⋮----
AccountSharedData::default().owner(),
⋮----
fn bench_dashmap_single_reader_with_n_writers(bencher: &mut Bencher) {
⋮----
map.insert(i, i);
⋮----
let map = map.clone();
⋮----
.name("readers".to_string())
.spawn(move || loop {
test::black_box(map.entry(5).or_insert(2));
⋮----
test::black_box(map.get(&5).unwrap().value());
⋮----
fn bench_rwlock_hashmap_single_reader_with_n_writers(bencher: &mut Bencher) {
⋮----
map.write().unwrap().insert(i, i);
⋮----
test::black_box(map.write().unwrap().get(&5));
⋮----
test::black_box(map.read().unwrap().get(&5));
⋮----
fn setup_bench_dashmap_iter() -> (Arc<Accounts>, DashMap<Pubkey, (AccountSharedData, Hash)>) {
⋮----
.map(|num_keys| num_keys.parse::<usize>().unwrap())
.unwrap_or_else(|_| 10000);
⋮----
dashmap.insert(
⋮----
AccountSharedData::new(1, 0, AccountSharedData::default().owner()),
⋮----
fn bench_dashmap_par_iter(bencher: &mut Bencher) {
let (accounts, dashmap) = setup_bench_dashmap_iter();
⋮----
test::black_box(accounts.accounts_db.thread_pool_foreground.install(|| {
⋮----
.par_iter()
.map(|cached_account| (*cached_account.key(), cached_account.value().1))
⋮----
fn bench_dashmap_iter(bencher: &mut Bencher) {
let (_accounts, dashmap) = setup_bench_dashmap_iter();
⋮----
.iter()
⋮----
fn bench_load_largest_accounts(b: &mut Bencher) {
let accounts_db = new_accounts_db(Vec::new());
⋮----
let lamports = rng.random();
⋮----
.store_for_tests((0, [(&pubkey, &account)].as_slice()));
⋮----
accounts.accounts_db.add_root_and_flush_write_cache(0);
let ancestors = Ancestors::from(vec![0]);
⋮----
b.iter(|| {
accounts.load_largest_accounts(
⋮----
fn bench_sort_and_remove_dups(b: &mut Bencher) {
fn generate_sample_account_from_storage(i: u8) -> AccountFromStorage {
⋮----
std::iter::repeat_with(|| generate_sample_account_from_storage(rng.random::<u8>()))
.take(1000)
⋮----
b.iter(|| AccountsDb::sort_and_remove_dups(&mut accounts.clone()));
⋮----
fn bench_sort_and_remove_dups_no_dups(b: &mut Bencher) {
⋮----
accounts.shuffle(&mut rng);

================
File: accounts-db/benches/bench_accounts_file.rs
================
mod utils;
⋮----
fn bench_write_accounts_file(c: &mut Criterion, storage_access: StorageAccess) {
let mut group = c.benchmark_group(format!("write_accounts_file_{storage_access:?}"));
⋮----
let temp_dir = tempfile::tempdir().unwrap();
⋮----
group.throughput(Throughput::Elements(accounts_count as u64));
⋮----
.take(accounts_count)
.collect();
⋮----
.iter()
.map(|(pubkey, account)| (pubkey, account))
⋮----
let storable_accounts = (Slot::MAX, accounts_refs.as_slice());
group.bench_function(BenchmarkId::new("append_vec", accounts_count), |b| {
b.iter_batched_ref(
⋮----
let path = temp_dir.path().join(format!("append_vec_{accounts_count}"));
let file_size = accounts.len() * (space + append_vec::STORE_META_OVERHEAD);
⋮----
let res = append_vec.append_accounts(&storable_accounts, 0).unwrap();
let accounts_written_count = res.offsets.len();
assert_eq!(accounts_written_count, accounts_count);
⋮----
group.bench_function(BenchmarkId::new("hot_storage", accounts_count), |b| {
⋮----
.path()
.join(format!("hot_storage_{accounts_count}"));
⋮----
HotStorageWriter::new(path).unwrap()
⋮----
let res = hot_storage.write_accounts(&storable_accounts, 0).unwrap();
⋮----
fn bench_write_accounts_file_file_io(c: &mut Criterion) {
bench_write_accounts_file(c, StorageAccess::File);
⋮----
fn bench_write_accounts_file_mmap(c: &mut Criterion) {
bench_write_accounts_file(
⋮----
fn bench_scan_pubkeys(c: &mut Criterion) {
let mut group = c.benchmark_group("scan_pubkeys");
⋮----
let append_vec_path = temp_dir.path().join(format!("append_vec_{accounts_count}"));
⋮----
.map(|(_, account)| append_vec::aligned_stored_size(account.data().len()))
.sum();
⋮----
.append_accounts(&(Slot::MAX, storable_accounts.as_slice()), 0)
.unwrap();
assert_eq!(stored_accounts_info.offsets.len(), accounts_count);
append_vec.flush().unwrap();
⋮----
append_vec.path(),
append_vec.len(),
⋮----
.unwrap()
⋮----
AppendVec::new_from_file(append_vec.path(), append_vec.len(), StorageAccess::File)
⋮----
let mut hot_storage_writer = HotStorageWriter::new(&hot_storage_path).unwrap();
⋮----
.write_accounts(&(Slot::MAX, storable_accounts.as_slice()), 0)
⋮----
hot_storage_writer.flush().unwrap();
let hot_storage_file = TieredReadableFile::new(&hot_storage_path).unwrap();
let hot_storage_reader = HotStorageReader::new(hot_storage_file).unwrap();
group.bench_function(BenchmarkId::new("append_vec_mmap", accounts_count), |b| {
b.iter(|| {
⋮----
append_vec_mmap.scan_pubkeys(|_| count += 1).unwrap();
assert_eq!(count, accounts_count);
⋮----
group.bench_function(BenchmarkId::new("append_vec_file", accounts_count), |b| {
⋮----
append_vec_file.scan_pubkeys(|_| count += 1).unwrap();
⋮----
hot_storage_reader.scan_pubkeys(|_| count += 1).unwrap();
⋮----
fn bench_get_account_shared_data(c: &mut Criterion) {
let mut group = c.benchmark_group("get_account_shared_data");
⋮----
let storable_accounts: Vec<_> = utils::accounts(255, &[data_size], &[1]).take(1).collect();
let append_vec_path = temp_dir.path().join(format!("append_vec_{data_size}"));
⋮----
assert_eq!(stored_accounts_info.offsets.len(), 1);
⋮----
group.bench_function(
⋮----
b.iter_with_large_drop(|| {
_ = append_vec_mmap.get_account_shared_data(0).unwrap();
⋮----
.get_stored_account_callback(0, |account| {
create_account_shared_data(&account)
⋮----
_ = append_vec_file.get_account_shared_data(0).unwrap();
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: accounts-db/benches/bench_hashing.rs
================
fn bench_hash_account(c: &mut Criterion) {
⋮----
let mut group = c.benchmark_group("hash_account");
⋮----
let num_bytes = META_SIZE.checked_add(data_size).unwrap();
group.throughput(Throughput::Bytes(num_bytes as u64));
⋮----
group.bench_function(BenchmarkId::new("lattice", data_size), |b| {
b.iter(|| AccountsDb::lt_hash_account(&account, &address));
⋮----
criterion_group!(benches, bench_hash_account);
criterion_main!(benches);

================
File: accounts-db/benches/bench_lock_accounts.rs
================
fn create_test_transactions(lock_count: usize, read_conflicts: bool) -> Vec<SanitizedTransaction> {
let shared_pubkeys: Vec<_> = (0..lock_count).map(|_| Pubkey::new_unique()).collect();
let mut transactions = vec![];
⋮----
let mut account_metas = vec![];
⋮----
account_metas.push(account_meta);
⋮----
transactions.push(SanitizedTransaction::from_transaction_for_tests(
⋮----
fn bench_entry_lock_accounts(c: &mut Criterion) {
let mut group = c.benchmark_group("bench_lock_accounts");
⋮----
iproduct!(BATCH_SIZES, LOCK_COUNTS, [false, true], [false, true])
⋮----
let name = format!(
⋮----
let transactions = create_test_transactions(lock_count, read_conflicts);
group.throughput(Throughput::Elements(transactions.len() as u64));
let transaction_batches: Vec<_> = transactions.chunks(batch_size).collect();
let batch_results = vec![Ok(()); batch_size].into_iter();
group.bench_function(name.as_str(), move |b| {
b.iter(|| {
⋮----
let results = accounts.lock_accounts(
black_box(batch.iter()),
batch_results.clone(),
⋮----
accounts.unlock_accounts(batch.iter().zip(&results));
⋮----
criterion_group!(benches, bench_entry_lock_accounts);
criterion_main!(benches);

================
File: accounts-db/benches/bench_serde.rs
================
fn bench_account_serialize(c: &mut Criterion) {
let mut group = c.benchmark_group("serde_account_serialize");
⋮----
let num_bytes = bincode::serialized_size(&account).unwrap();
group.throughput(Throughput::Bytes(num_bytes));
⋮----
data: vec![0; data_size],
⋮----
group.bench_function(BenchmarkId::new("data_as_seq", data_size), |b| {
b.iter_batched(
⋮----
|buffer| bincode::serialize_into(buffer, &account_with_data_as_seq).unwrap(),
⋮----
group.bench_function(BenchmarkId::new("data_as_bytes", data_size), |b| {
⋮----
|buffer| bincode::serialize_into(buffer, &account_with_data_as_bytes).unwrap(),
⋮----
fn bench_account_deserialize(c: &mut Criterion) {
let mut group = c.benchmark_group("serde_account_deserialize");
⋮----
let serialized_account = bincode::serialize(&account).unwrap();
let num_bytes = serialized_account.len() as u64;
⋮----
.unwrap();
⋮----
bincode::deserialize::<AccountWithDataAsSeq>(serialized_account.as_slice())
.unwrap()
⋮----
bincode::deserialize::<AccountWithDataAsBytes>(serialized_account.as_slice())
⋮----
struct AccountWithDataAsSeq {
⋮----
const _: () = assert!(mem::size_of::<AccountWithDataAsSeq>() == mem::size_of::<Account>());
⋮----
struct AccountWithDataAsBytes {
⋮----
const _: () = assert!(mem::size_of::<AccountWithDataAsBytes>() == mem::size_of::<Account>());
criterion_group!(benches, bench_account_serialize, bench_account_deserialize);
criterion_main!(benches);

================
File: accounts-db/benches/read_only_accounts_cache.rs
================
mod utils;
⋮----
fn bench_read_only_accounts_cache(c: &mut Criterion) {
let mut group = c.benchmark_group("read_only_accounts_cache");
⋮----
.collect();
⋮----
.iter()
.map(|(pubkey, _)| pubkey.to_owned())
⋮----
for (pubkey, account) in accounts.iter() {
cache.store(*pubkey, slot, account.clone());
⋮----
.map(|i| {
⋮----
let pubkeys = pubkeys.clone();
⋮----
.name(format!("reader{i:02}"))
.spawn({
⋮----
while !stop_threads.load(Ordering::Relaxed) {
let pubkey = pubkeys.choose(&mut rng).unwrap();
black_box(cache.load(*pubkey, slot));
⋮----
.unwrap()
⋮----
let accounts = accounts.clone();
⋮----
.name(format!("writer{i:02}"))
⋮----
let mut rng = SmallRng::seed_from_u64(100_u64.saturating_add(i as u64));
⋮----
let (pubkey, account) = accounts.choose(&mut rng).unwrap();
⋮----
group.bench_function(BenchmarkId::new("store", num_readers_writers), |b| {
b.iter_custom(|iters| {
⋮----
for (pubkey, account) in accounts.iter().cycle().take(iters as usize) {
⋮----
total_time = total_time.saturating_add(start.elapsed());
⋮----
group.bench_function(BenchmarkId::new("load", num_readers_writers), |b| {
⋮----
for (pubkey, _) in accounts.iter().cycle().take(iters as usize) {
⋮----
start.elapsed()
⋮----
stop_threads.store(true, Ordering::Relaxed);
⋮----
reader_handle.join().unwrap();
⋮----
writer_handle.join().unwrap();
⋮----
fn bench_read_only_accounts_cache_eviction(
⋮----
let mut group = c.benchmark_group(group_name);
⋮----
let pubkey = pubkeys.choose(&mut rng).unwrap().to_owned();
⋮----
black_box(cache.load(pubkey, slot));
⋮----
let accounts = utils::accounts(0, DATA_SIZES, WEIGHTS).take(iters as usize);
⋮----
cache.store(pubkey, slot, account);
⋮----
fn bench_read_only_accounts_cache_eviction_lo_hi(c: &mut Criterion) {
bench_read_only_accounts_cache_eviction(
⋮----
fn bench_read_only_accounts_cache_eviction_hi(c: &mut Criterion) {
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: accounts-db/benches/utils.rs
================
pub fn accounts<'a>(
⋮----
let distribution = WeightedIndex::new(weights).unwrap();
⋮----
let index = distribution.sample(&mut rng);
⋮----
let owner: [u8; 32] = rng.random();
⋮----
rent.minimum_balance(data_size),
⋮----
pub fn accounts_with_size_limit<'a>(
⋮----
sum = sum.saturating_add(data_size);
⋮----
Some((

================
File: accounts-db/src/account_storage/stored_account_info.rs
================
pub struct StoredAccountInfo<'storage> {
⋮----
pub fn pubkey(&self) -> &Pubkey {
⋮----
/// Constructs a new StoredAccountInfo from a StoredAccountInfoWithoutData and data
    ///
⋮----
///
    /// Use this ctor when `other_stored_account` is going out of scope, *but not* the underlying
⋮----
/// Use this ctor when `other_stored_account` is going out of scope, *but not* the underlying
    /// `'storage`.  This facilitates incremental improvements towards not reading account data
⋮----
/// `'storage`.  This facilitates incremental improvements towards not reading account data
    pub fn new_from<'other>(
⋮----
pub fn new_from<'other>(
⋮----
assert_eq!(other_stored_account.data_len, data.len());
⋮----
impl IsZeroLamport for StoredAccountInfo<'_> {
fn is_zero_lamport(&self) -> bool {
⋮----
impl ReadableAccount for StoredAccountInfo<'_> {
fn lamports(&self) -> u64 {
⋮----
fn owner(&self) -> &Pubkey {
⋮----
fn data(&self) -> &[u8] {
⋮----
fn executable(&self) -> bool {
⋮----
fn rent_epoch(&self) -> Epoch {
⋮----
pub struct StoredAccountInfoWithoutData<'storage> {
⋮----
pub fn new_from<'other>(other_stored_account: &'other StoredAccountInfo<'storage>) -> Self {
// Note that we must use the pubkey/owner fields directly so that we can get the `'storage`
⋮----
data_len: other_stored_account.data.len(),
⋮----
impl IsZeroLamport for StoredAccountInfoWithoutData<'_> {

================
File: accounts-db/src/accounts_db/accounts_db_config.rs
================
pub struct AccountsDbConfig {
⋮----
index: Some(ACCOUNTS_INDEX_CONFIG_FOR_TESTING),
⋮----
index: Some(ACCOUNTS_INDEX_CONFIG_FOR_BENCHMARKS),

================
File: accounts-db/src/accounts_db/geyser_plugin_utils.rs
================
impl AccountsDb {
pub fn notify_account_at_accounts_update(
⋮----
accounts_update_notifier.notify_account_update(
⋮----
pub mod tests {
⋮----
pub fn set_geyser_plugin_notifier(&mut self, notifier: Option<AccountsUpdateNotifier>) {
⋮----
struct GeyserTestPlugin {
⋮----
impl AccountsUpdateNotifierInterface for GeyserTestPlugin {
fn snapshot_notifications_enabled(&self) -> bool {
⋮----
fn notify_account_update(
⋮----
self.accounts_notified.entry(*pubkey).or_default().push((
⋮----
account.clone(),
⋮----
fn notify_account_restore_from_snapshot(
⋮----
.entry(*account.pubkey)
.or_default()
.push((slot, write_version, create_account_shared_data(account)));
⋮----
fn notify_end_of_restore_from_snapshot(&self) {
self.is_startup_done.store(true, Ordering::Relaxed);
⋮----
fn test_notify_account_restore_from_snapshot(mark_obsolete_accounts: MarkObsoleteAccounts) {
⋮----
// Account with key1 is updated twice in two different slots, should get notified twice
// Need to add root and flush write cache for each slot to ensure accounts are written
// to correct slots. Cache flush can skip writes if accounts have already been written to
// a newer slot
⋮----
let storage0 = accounts_db.create_and_insert_store(slot0, /*size*/ 4_096, "");
⋮----
.write_accounts(&(slot0, [(&key1, &account)].as_slice()), /*skip*/ 0);
⋮----
let storage1 = accounts_db.create_and_insert_store(slot1, /*size*/ 4_096, "");
⋮----
.write_accounts(&(slot1, [(&key1, &account)].as_slice()), /*skip*/ 0);
// Account with key2 is updated in a single slot, should get notified once
⋮----
let storage2 = accounts_db.create_and_insert_store(slot2, /*size*/ 4_096, "");
⋮----
.write_accounts(&(slot2, [(&key2, &account)].as_slice()), /*skip*/ 0);
// Do the notification
⋮----
accounts_db.set_geyser_plugin_notifier(Some(notifier.clone()));
accounts_db.generate_index(None, false);
// Ensure key1 was notified twice in different slots
⋮----
let notified_key1 = notifier.accounts_notified.get(&key1).unwrap();
assert_eq!(notified_key1.len(), 2);
// Since index generation goes through storages in parallel, there's not a
let mut notified_key1_values = notified_key1.value().clone();
notified_key1_values.sort_unstable_by_key(|k| k.0);
⋮----
assert_eq!(*slot, slot0);
assert_eq!(*write_version, 0);
⋮----
assert_eq!(*slot, slot1);
⋮----
let notified_key2 = notifier.accounts_notified.get(&key2).unwrap();
assert_eq!(notified_key2.len(), 1);
⋮----
assert_eq!(*slot, slot2);
⋮----
assert!(notifier.is_startup_done.load(Ordering::Relaxed));
⋮----
fn test_notify_account_at_accounts_update() {
⋮----
accounts.set_geyser_plugin_notifier(Some(notifier.clone()));
⋮----
AccountSharedData::new(account1_lamports1, 1, AccountSharedData::default().owner());
⋮----
accounts.store_for_tests((slot0, &[(&key1, &account1)][..]));
⋮----
AccountSharedData::new(account2_lamports, 1, AccountSharedData::default().owner());
accounts.store_for_tests((slot0, &[(&key2, &account2)][..]));
⋮----
let account1 = AccountSharedData::new(account1_lamports2, 1, account1.owner());
accounts.store_for_tests((slot1, &[(&key1, &account1)][..]));
⋮----
AccountSharedData::new(account3_lamports, 1, AccountSharedData::default().owner());
accounts.store_for_tests((slot1, &[(&key3, &account3)][..]));
assert_eq!(notifier.accounts_notified.get(&key1).unwrap().len(), 2);
assert_eq!(
⋮----
assert_eq!(notifier.accounts_notified.get(&key1).unwrap()[0].0, slot0);
⋮----
assert_eq!(notifier.accounts_notified.get(&key1).unwrap()[1].0, slot1);
assert_eq!(notifier.accounts_notified.get(&key2).unwrap().len(), 1);
⋮----
assert_eq!(notifier.accounts_notified.get(&key2).unwrap()[0].0, slot0);
assert_eq!(notifier.accounts_notified.get(&key3).unwrap().len(), 1);
⋮----
assert_eq!(notifier.accounts_notified.get(&key3).unwrap()[0].0, slot1);

================
File: accounts-db/src/accounts_db/stats.rs
================
pub struct AccountsStats {
⋮----
pub struct PurgeStats {
⋮----
impl PurgeStats {
pub fn report(&self, metric_name: &'static str, report_interval_ms: Option<u64>) {
⋮----
.map(|report_interval_ms| self.last_report.should_update(report_interval_ms))
.unwrap_or(true);
⋮----
datapoint_info!(
⋮----
pub struct StoreAccountsTiming {
⋮----
impl StoreAccountsTiming {
pub fn accumulate(&mut self, other: &Self) {
⋮----
pub struct FlushStats {
⋮----
impl FlushStats {
⋮----
.accumulate(&other.store_accounts_timing);
⋮----
pub struct LatestAccountsIndexRootsStats {
⋮----
impl LatestAccountsIndexRootsStats {
pub fn update(&self, accounts_index_roots_stats: &AccountsIndexRootsStats) {
⋮----
self.roots_len.store(value, Ordering::Relaxed);
⋮----
self.uncleaned_roots_len.store(value, Ordering::Relaxed);
⋮----
self.roots_range.store(value, Ordering::Relaxed);
⋮----
self.rooted_cleaned_count.fetch_add(
⋮----
self.unrooted_cleaned_count.fetch_add(
⋮----
self.clean_unref_from_storage_us.fetch_add(
⋮----
self.clean_dead_slot_us.fetch_add(
⋮----
pub fn report(&self) {
⋮----
// Don't need to reset since this tracks the latest updates, not a cumulative total
⋮----
pub struct CleanAccountsStats {
⋮----
impl CleanAccountsStats {
⋮----
self.purge_stats.report("clean_purge_slots_stats", None);
self.latest_accounts_index_roots_stats.report();
⋮----
pub struct ShrinkAncientStats {
⋮----
pub struct ShrinkStatsSub {
⋮----
impl ShrinkStatsSub {
⋮----
pub struct ShrinkStats {
⋮----
impl ShrinkStats {
⋮----
if self.last_report.should_update(1000) {
⋮----
impl ShrinkAncientStats {
⋮----
pub struct ObsoleteAccountsStats {
⋮----
fn sum<I>(iter: I) -> Self
⋮----
iter.fold(Self::default(), |mut accumulated_stats, item| {

================
File: accounts-db/src/accounts_db/tests.rs
================
fn linear_ancestors(end_slot: u64) -> Ancestors {
let mut ancestors: Ancestors = vec![(0, 0)].into_iter().collect();
⋮----
ancestors.insert(i, (i - 1) as usize);
⋮----
impl AccountsDb {
fn get_storage_for_slot(&self, slot: Slot) -> Option<Arc<AccountStorageEntry>> {
self.storage.get_slot_storage_entry(slot)
⋮----
fn is_zero_lamport(&self, index: usize) -> bool {
self.1[index].1.lamports() == 0
⋮----
fn data_len(&self, index: usize) -> usize {
self.1[index].1.data().len()
⋮----
fn account<Ret>(
⋮----
callback(self.1[index].1.into())
⋮----
fn pubkey(&self, index: usize) -> &Pubkey {
⋮----
fn slot(&self, index: usize) -> Slot {
// note that this could be different than 'target_slot()' PER account
⋮----
fn target_slot(&self) -> Slot {
⋮----
fn len(&self) -> usize {
self.1.len()
⋮----
fn contains_multiple_slots(&self) -> bool {
let len = self.len();
⋮----
let slot = self.slot(0);
// true if any item has a different slot than the first item
(1..len).any(|i| slot != self.slot(i))
⋮----
fn create_loadable_account_with_fields(
⋮----
data: name.as_bytes().to_vec(),
⋮----
fn create_loadable_account_for_test(name: &str) -> AccountSharedData {
create_loadable_account_with_fields(name, DUMMY_INHERITABLE_ACCOUNT_FIELDS)
⋮----
impl AccountStorageEntry {
fn add_account(&self, num_bytes: usize) {
self.add_accounts(1, num_bytes)
⋮----
/// Helper macro to define accounts_db_test for both `AppendVec` and `HotStorage`.
/// This macro supports creating both regular tests and tests that should panic.
⋮----
/// This macro supports creating both regular tests and tests that should panic.
/// Usage:
⋮----
/// Usage:
///   For regular test, use the following syntax.
⋮----
///   For regular test, use the following syntax.
///     define_accounts_db_test!(TEST_NAME, |accounts_db| { TEST_BODY }); // regular test
⋮----
///     define_accounts_db_test!(TEST_NAME, |accounts_db| { TEST_BODY }); // regular test
///   For test that should panic, use the following syntax.
⋮----
///   For test that should panic, use the following syntax.
///     define_accounts_db_test!(TEST_NAME, panic = "PANIC_MSG", |accounts_db| { TEST_BODY });
⋮----
///     define_accounts_db_test!(TEST_NAME, panic = "PANIC_MSG", |accounts_db| { TEST_BODY });
macro_rules! define_accounts_db_test {
⋮----
macro_rules! define_accounts_db_test {
⋮----
pub(crate) use define_accounts_db_test;
fn run_generate_index_duplicates_within_slot_test(db: AccountsDb, reverse: bool) {
⋮----
let append_vec = db.create_and_insert_store(slot0, 1000, "test");
⋮----
account_small.set_data(vec![1]);
account_small.set_lamports(1);
⋮----
account_big.set_data(vec![5; 10]);
account_big.set_lamports(2);
assert_ne!(
⋮----
// same account twice with different data lens
// Rules are the last one of each pubkey is the one that ends up in the index.
let mut data = vec![(&pubkey, &account_big), (&pubkey, &account_small)];
⋮----
data = data.into_iter().rev().collect();
⋮----
// construct append vec with account to generate an index from
append_vec.accounts.write_accounts(&storable_accounts, 0);
assert!(!db.accounts_index.contains(&pubkey));
⋮----
let storage = db.get_storage_for_slot(slot0).unwrap();
⋮----
db.generate_index_for_slot(
⋮----
storage.slot(),
storage.id(),
⋮----
define_accounts_db_test!(
⋮----
fn test_generate_index_for_single_ref_zero_lamport_slot() {
⋮----
let result = db.generate_index(None, false);
let slot_list_len = db.accounts_index.get_and_then(&pubkey, |entry| {
(false, entry.unwrap().slot_list_lock_read_len())
⋮----
assert_eq!(slot_list_len, 1);
assert_eq!(append_vec.alive_bytes(), aligned_stored_size(0));
assert_eq!(append_vec.accounts_count(), 1);
assert_eq!(append_vec.count(), 1);
assert_eq!(result.accounts_data_len, 0);
assert_eq!(1, append_vec.num_zero_lamport_single_ref_accounts());
assert_eq!(
⋮----
fn generate_sample_account_from_storage(i: u8) -> AccountFromStorage {
// offset has to be 8 byte aligned
⋮----
/// Reserve ancient storage size is not supported for TiredStorage
#[test]
fn test_sort_and_remove_dups() {
// empty
let mut test1 = vec![];
let expected = test1.clone();
⋮----
assert_eq!(test1, expected);
⋮----
// just 0
let mut test1 = vec![generate_sample_account_from_storage(0)];
⋮----
// 0, 1
let mut test1 = vec![
⋮----
// 1, 0. sort should reverse
let mut test2 = vec![
⋮----
assert_eq!(test2, expected);
⋮----
// 0 twice so it gets removed
⋮----
let mut expected = test1.clone();
expected.truncate(1); // get rid of 1st duplicate
test1.first_mut().unwrap().data_len = 2342342; // this one should be ignored, so modify the data_len so it will fail the compare below if it is used
⋮----
// insert another good one before or after the 2 bad ones
test1.insert(insert_other_good, generate_sample_account_from_storage(1));
// other good one should always be last since it is sorted after
expected.push(generate_sample_account_from_storage(1));
⋮----
.into_iter()
.map(generate_sample_account_from_storage)
⋮----
test1.iter_mut().take(3).for_each(|entry| {
entry.data_len = 2342342; // this one should be ignored, so modify the data_len so it will fail the compare below if it is used
⋮----
fn test_sort_and_remove_dups_random() {
⋮----
std::iter::repeat_with(|| generate_sample_account_from_storage(rng.random::<u8>()))
.take(1000)
.collect();
let mut accounts1 = accounts.clone();
⋮----
// Use BTreeMap to calculate sort and remove dups alternatively.
⋮----
for account in accounts.iter() {
if map.insert(*account.pubkey(), *account).is_some() {
⋮----
let accounts2: Vec<_> = map.into_values().collect();
assert_eq!(accounts1, accounts2);
assert_eq!(num_dups1, num_dups2);
⋮----
pub(crate) fn append_single_account_with_default_hash(
⋮----
let slot = storage.slot();
⋮----
.write_accounts(&storable_accounts, 0)
.unwrap();
⋮----
// updates 'alive_bytes' on the storage
storage.add_account(stored_accounts_info.size);
⋮----
StorageLocation::AppendVec(storage.id(), stored_accounts_info.offsets[0]),
account.lamports() == 0,
⋮----
index.upsert(
⋮----
fn append_sample_data_to_storage(
⋮----
account_data_size.unwrap_or(48) as usize,
AccountSharedData::default().owner(),
⋮----
append_single_account_with_default_hash(storage, pubkey, &acc, mark_alive, None);
⋮----
fn sample_storage_with_entries_id_fill_percentage(
⋮----
let (_temp_dirs, paths) = get_temp_accounts_paths(1).unwrap();
let file_size = account_data_size.unwrap_or(123) * 100 / fill_percentage;
let size_aligned: usize = aligned_stored_size(file_size as usize);
⋮----
(1024 * 1024).max(size_aligned),
⋮----
append_sample_data_to_storage(&arc, pubkey, mark_alive, account_data_size);
⋮----
fn sample_storage_with_entries_id(
⋮----
sample_storage_with_entries_id_fill_percentage(
⋮----
define_accounts_db_test!(test_accountsdb_add_root, |db| {
⋮----
define_accounts_db_test!(test_accountsdb_latest_ancestor, |db| {
⋮----
define_accounts_db_test!(test_accountsdb_latest_ancestor_with_root, |db| {
⋮----
define_accounts_db_test!(test_accountsdb_root_one_slot, |db| {
⋮----
define_accounts_db_test!(test_accountsdb_add_root_many, |db| {
⋮----
define_accounts_db_test!(test_accountsdb_count_stores, |db| {
⋮----
define_accounts_db_test!(test_accounts_unsquashed, |db0| {
⋮----
fn test_flush_slots_with_reclaim_old_slots() {
⋮----
let mut pubkeys = vec![];
⋮----
let mut slot_pubkeys = vec![];
⋮----
accounts.store_for_tests((slot, [(&pubkey, &account)].as_slice()));
slot_pubkeys.push(pubkey);
⋮----
pubkeys.push(slot_pubkeys);
accounts.add_root_and_flush_write_cache(slot);
⋮----
for (slot, slot_pubkeys) in pubkeys.iter().enumerate() {
for pubkey in slot_pubkeys.iter().take(5 - slot) {
⋮----
accounts.store_for_tests((new_slot, [(pubkey, &account)].as_slice()));
⋮----
.slot_cache(new_slot)
.unwrap()
.iter()
.map(|iter_item| {
let pubkey = *iter_item.key();
let account = iter_item.value().account.clone();
⋮----
let storage = accounts.create_and_insert_store(new_slot, 4096, "test_flush_slots");
accounts.accounts_index.add_root(new_slot);
accounts._store_accounts_frozen(
⋮----
assert!(accounts.accounts_cache.remove_slot(new_slot).is_some());
assert!(accounts.storage.get_slot_storage_entry(0).is_none());
⋮----
assert!(accounts.storage.get_slot_storage_entry(slot).is_some());
let storage = accounts.storage.get_slot_storage_entry(slot).unwrap();
⋮----
assert!(accounts.storage.get_slot_storage_entry(new_slot).is_some());
⋮----
fn run_test_remove_unrooted_slot(is_cached: bool, db: AccountsDb) {
⋮----
let ancestors = vec![(unrooted_slot, 1)].into_iter().collect();
assert!(!db.accounts_index.contains(&key));
⋮----
db.store_for_tests((unrooted_slot, &[(&key, &account0)][..]));
assert!(db.accounts_cache.contains(unrooted_slot));
⋮----
let storage = db.create_and_insert_store(unrooted_slot, file_size, "");
db.store_accounts_frozen(
(unrooted_slot, [(&key, &account0)].as_slice()),
⋮----
assert!(db.storage.get_slot_storage_entry(unrooted_slot).is_some());
⋮----
assert!(!db.accounts_index.is_alive_root(unrooted_slot));
assert!(db.accounts_index.contains(&key));
db.assert_load_account(unrooted_slot, key, 1);
// Purge the slot
db.remove_unrooted_slots(&[(unrooted_slot, unrooted_bank_id)]);
assert!(db.load_without_fixed_root(&ancestors, &key).is_none());
assert!(db.accounts_cache.slot_cache(unrooted_slot).is_none());
assert!(db.storage.get_slot_storage_entry(unrooted_slot).is_none());
⋮----
// Test we can store for the same slot again and get the right information
⋮----
db.store_for_tests((unrooted_slot, [(&key, &account0)].as_slice()));
db.assert_load_account(unrooted_slot, key, 2);
⋮----
define_accounts_db_test!(test_remove_unrooted_slot_cached, |db| {
⋮----
define_accounts_db_test!(test_remove_unrooted_slot_storage, |db| {
⋮----
fn update_accounts(accounts: &AccountsDb, pubkeys: &[Pubkey], slot: Slot, range: usize) {
⋮----
let idx = rng().random_range(0..range);
let ancestors = vec![(slot, 0)].into_iter().collect();
if let Some((mut account, _)) = accounts.load_without_fixed_root(&ancestors, &pubkeys[idx])
⋮----
account.checked_add_lamports(1).unwrap();
accounts.store_for_tests((slot, [(&pubkeys[idx], &account)].as_slice()));
if account.is_zero_lamport() {
⋮----
assert!(accounts
⋮----
lamports: account.lamports(),
⋮----
assert_eq!(default_account, account);
⋮----
fn test_account_one() {
let (_accounts_dirs, paths) = get_temp_accounts_paths(1).unwrap();
⋮----
let mut pubkeys: Vec<Pubkey> = vec![];
db.create_account(&mut pubkeys, 0, 1, 0, 0);
let ancestors = vec![(0, 0)].into_iter().collect();
let account = db.load_without_fixed_root(&ancestors, &pubkeys[0]).unwrap();
⋮----
assert_eq!((default_account, 0), account);
⋮----
fn test_account_many() {
let (_accounts_dirs, paths) = get_temp_accounts_paths(2).unwrap();
⋮----
db.create_account(&mut pubkeys, 0, 100, 0, 0);
db.check_accounts(&pubkeys, 0, 100, 1);
⋮----
fn test_account_update() {
⋮----
accounts.create_account(&mut pubkeys, 0, 100, 0, 0);
update_accounts(&accounts, &pubkeys, 0, 99);
accounts.add_root_and_flush_write_cache(0);
accounts.check_storage(0, 100, 100);
⋮----
fn test_account_grow_many() {
let (_accounts_dir, paths) = get_temp_accounts_paths(2).unwrap();
⋮----
let mut keys = vec![];
⋮----
accounts.store_for_tests((0, [(&key, &account)].as_slice()));
keys.push(key);
⋮----
for (i, key) in keys.iter().enumerate() {
⋮----
let mut all_slots = vec![];
for slot_storage in accounts.storage.iter() {
all_slots.push(slot_storage.0)
⋮----
*append_vec_histogram.entry(slot).or_insert(0) += 1;
⋮----
for count in append_vec_histogram.values() {
assert!(*count >= 2);
⋮----
fn test_account_grow() {
⋮----
accounts.store_for_tests((0, [(&pubkey1, &account1)].as_slice()));
⋮----
let store = &accounts.storage.get_slot_storage_entry(0).unwrap();
assert_eq!(store.count(), 1);
⋮----
accounts.store_for_tests((0, [(&pubkey2, &account2)].as_slice()));
⋮----
assert_eq!(accounts.storage.len(), 1);
⋮----
assert_eq!(store.count(), 2);
⋮----
// lots of writes, but they are all duplicates
⋮----
fn test_lazy_gc_slot() {
⋮----
// Only run this test with mark obsolete accounts disabled as garbage collection
// is not lazy with mark obsolete accounts enabled
⋮----
let account = AccountSharedData::new(1, 0, AccountSharedData::default().owner());
//store an account
accounts.store_for_tests((0, [(&pubkey, &account)].as_slice()));
⋮----
.get_with_and_then(
⋮----
Some(&ancestors),
⋮----
|(_slot, account_info)| account_info.store_id(),
⋮----
//slot is still there, since gc is lazy
assert_eq!(accounts.storage.get_slot_storage_entry(0).unwrap().id(), id);
//store causes clean
accounts.store_for_tests((1, [(&pubkey, &account)].as_slice()));
//slot is gone
accounts.print_accounts_stats("pre-clean");
accounts.add_root_and_flush_write_cache(1);
assert!(accounts.storage.get_slot_storage_entry(0).is_some());
accounts.clean_accounts_for_tests();
⋮----
let ancestors = vec![(1, 1)].into_iter().collect();
⋮----
fn test_clean_zero_lamport_and_dead_slot() {
⋮----
let account = AccountSharedData::new(1, 1, AccountSharedData::default().owner());
let zero_lamport_account = AccountSharedData::new(0, 0, AccountSharedData::default().owner());
accounts.store_for_tests((0, [(&pubkey1, &account)].as_slice()));
accounts.store_for_tests((0, [(&pubkey2, &account)].as_slice()));
let ancestors = vec![(0, 1)].into_iter().collect();
⋮----
assert_eq!(slot1, 0);
assert_eq!(slot1, slot2);
assert_eq!(account_info1.storage_location(), StorageLocation::Cached);
⋮----
accounts.store_for_tests((1, [(&pubkey1, &account)].as_slice()));
accounts.store_for_tests((2, [(&pubkey1, &zero_lamport_account)].as_slice()));
⋮----
accounts.add_root_and_flush_write_cache(2);
⋮----
assert!(accounts.storage.get_slot_storage_entry(1).is_none());
assert_eq!(accounts.alive_account_count_in_slot(1), 0);
⋮----
fn test_clean_dead_slot_with_obsolete_accounts() {
⋮----
accounts.set_latest_full_snapshot_slot(2);
accounts.store_for_tests((0, [(&pubkey, &account), (&pubkey2, &account)].as_slice()));
accounts.store_for_tests((
⋮----
[(&pubkey, &account), (&pubkey2, &zero_lamport_account)].as_slice(),
⋮----
accounts.store_for_tests((2, [(&pubkey, &account)].as_slice()));
⋮----
assert!(accounts.storage.get_slot_storage_entry(1).is_some());
let slot = accounts.storage.get_slot_storage_entry(1).unwrap();
⋮----
accounts.assert_ref_count(&pubkey, 1);
⋮----
fn test_remove_zero_lamport_multi_ref_accounts_panic() {
⋮----
let one_lamport_account = AccountSharedData::new(1, 0, AccountSharedData::default().owner());
⋮----
accounts.store_for_tests((slot, [(&pubkey_zero, &one_lamport_account)].as_slice()));
accounts.add_root(1);
accounts.flush_rooted_accounts_cache(Some(slot), false);
accounts.store_for_tests((slot + 1, [(&pubkey_zero, &zero_lamport_account)].as_slice()));
accounts.add_root(2);
accounts.flush_rooted_accounts_cache(Some(slot + 1), false);
accounts.remove_zero_lamport_single_ref_accounts_after_shrink(
⋮----
fn test_remove_zero_lamport_single_ref_accounts_after_shrink() {
⋮----
AccountSharedData::new(0, 0, AccountSharedData::default().owner());
⋮----
[(&pubkey_zero, &zero_lamport_account), (&pubkey2, &account)].as_slice(),
⋮----
.store_for_tests((slot + 1, [(&pubkey_zero, &zero_lamport_account)].as_slice()));
⋮----
accounts.add_root(slot + 1);
accounts.flush_rooted_accounts_cache(None, false);
⋮----
accounts.accounts_index.get_and_then(&pubkey_zero, |entry| {
⋮----
assert_eq!(entry.unwrap().ref_count(), expected_ref_count, "{pass}");
⋮----
assert_eq!(entry.unwrap().slot_list_lock_read_len(), expected_slot_list);
⋮----
accounts.accounts_index.get_and_then(&pubkey2, |entry| {
assert!(entry.is_some());
⋮----
let zero_lamport_single_ref_pubkeys = if pass < 2 { vec![&pubkey_zero] } else { vec![] };
⋮----
assert!(entry.is_none(), "{pass}");
⋮----
assert_eq!(entry.unwrap().slot_list_lock_read_len(), 1);
⋮----
assert_eq!(entry.unwrap().slot_list_lock_read_len(), 2);
⋮----
.slot_list_read_lock()
⋮----
.map(|(s, _)| s)
.cloned()
⋮----
assert_eq!(slots, vec![slot, slot + 1]);
⋮----
unreachable!("Shouldn't reach here.")
⋮----
assert!(entry.is_some(), "{pass}");
⋮----
fn test_shrink_zero_lamport_single_ref_account() {
⋮----
for latest_full_snapshot_slot in [None, Some(0), Some(1), Some(2)] {
⋮----
.get_slot_storage_entry(slot)
⋮----
.fetch_sub(aligned_stored_size(0), Ordering::Release);
⋮----
accounts.set_latest_full_snapshot_slot(latest_full_snapshot_slot);
⋮----
accounts.shrink_slot_forced(slot);
assert!(
⋮----
let expected_alive_count = if latest_full_snapshot_slot.unwrap_or(Slot::MAX) < slot {
⋮----
fn test_clean_multiple_zero_lamport_decrements_index_ref_count() {
⋮----
accounts.set_latest_full_snapshot_slot(0);
accounts.store_for_tests((0, [(&pubkey1, &zero_lamport_account)].as_slice()));
accounts.store_for_tests((0, [(&pubkey2, &zero_lamport_account)].as_slice()));
accounts.store_for_tests((1, [(&pubkey1, &zero_lamport_account)].as_slice()));
⋮----
accounts.assert_ref_count(&pubkey1, 3);
accounts.assert_ref_count(&pubkey2, 1);
⋮----
assert!(accounts.storage.get_slot_storage_entry(2).is_some());
accounts.assert_ref_count(&pubkey1, 1);
accounts.assert_ref_count(&pubkey2, 0);
⋮----
assert!(accounts.storage.get_slot_storage_entry(2).is_none());
accounts.assert_ref_count(&pubkey1, 0);
⋮----
fn test_clean_zero_lamport_and_old_roots() {
⋮----
accounts.store_for_tests((1, [(&pubkey, &zero_lamport_account)].as_slice()));
⋮----
assert_eq!(accounts.alive_account_count_in_slot(0), 0);
⋮----
assert!(!accounts.accounts_index.contains_with(&pubkey, None, None));
⋮----
fn test_clean_old_with_normal_account(mark_obsolete_accounts: MarkObsoleteAccounts) {
⋮----
assert_eq!(accounts.alive_account_count_in_slot(1), 1);
⋮----
assert_eq!(accounts.alive_account_count_in_slot(0), 1);
⋮----
fn test_clean_old_with_zero_lamport_account(mark_obsolete_accounts: MarkObsoleteAccounts) {
⋮----
let normal_account = AccountSharedData::new(1, 0, AccountSharedData::default().owner());
let zero_account = AccountSharedData::new(0, 0, AccountSharedData::default().owner());
accounts.store_for_tests((0, [(&pubkey1, &normal_account)].as_slice()));
accounts.store_for_tests((1, [(&pubkey1, &zero_account)].as_slice()));
accounts.store_for_tests((0, [(&pubkey2, &normal_account)].as_slice()));
accounts.store_for_tests((1, [(&pubkey2, &normal_account)].as_slice()));
⋮----
assert_eq!(accounts.alive_account_count_in_slot(1), 2);
accounts.print_accounts_stats("");
// With obsolete accounts enabled, slot 0 is cleaned during flush
⋮----
// even if rooted, old state isn't cleaned up
assert_eq!(accounts.alive_account_count_in_slot(0), 2);
⋮----
//Old state behind zero-lamport account is cleaned up
⋮----
fn test_clean_old_with_both_normal_and_zero_lamport_accounts(
⋮----
account_indexes: spl_token_mint_index_enabled(),
⋮----
// Set up account to be added to secondary index
⋮----
let mut account_data_with_mint = vec![0; spl_generic_token::token::Account::get_packed_len()];
account_data_with_mint[..PUBKEY_BYTES].clone_from_slice(&(mint_key.to_bytes()));
⋮----
let mut normal_account = AccountSharedData::new(1, 0, AccountSharedData::default().owner());
normal_account.set_owner(spl_generic_token::token::id());
normal_account.set_data(account_data_with_mint.clone());
let mut zero_account = AccountSharedData::new(0, 0, AccountSharedData::default().owner());
zero_account.set_owner(spl_generic_token::token::id());
zero_account.set_data(account_data_with_mint);
⋮----
accounts.store_for_tests((2, [(&pubkey2, &normal_account)].as_slice()));
//simulate slots are rooted after while
⋮----
//even if rooted, old state isn't cleaned up
⋮----
assert_eq!(accounts.alive_account_count_in_slot(2), 1);
// Secondary index should still find both pubkeys
⋮----
.index_scan_accounts(
⋮----
found_accounts.insert(*key);
⋮----
assert_eq!(found_accounts.len(), 2);
assert!(found_accounts.contains(&pubkey1));
assert!(found_accounts.contains(&pubkey2));
⋮----
accounts.account_indexes.keys = Some(AccountSecondaryIndexesIncludeExclude {
⋮----
keys: [mint_key].iter().cloned().collect::<HashSet<Pubkey>>(),
⋮----
// Secondary index can't be used - do normal scan: should still find both pubkeys
⋮----
found_accounts.insert(*account.unwrap().0);
⋮----
assert!(!used_index);
⋮----
// Secondary index can now be used since it isn't marked as excluded
⋮----
assert!(used_index);
⋮----
//both zero lamport and normal accounts are cleaned up
⋮----
// The only store to slot 1 was a zero lamport account, should
// be purged by zero-lamport cleaning logic because slot 1 is
// rooted
⋮----
// `pubkey1`, a zero lamport account, should no longer exist in accounts index
// because it has been removed by the clean
assert!(!accounts.accounts_index.contains_with(&pubkey1, None, None));
// Secondary index should have purged `pubkey1` as well
let mut found_accounts = vec![];
⋮----
|key, _| found_accounts.push(*key),
⋮----
assert_eq!(found_accounts, vec![pubkey2]);
⋮----
fn test_clean_max_slot_zero_lamport_account(mark_obsolete_accounts: MarkObsoleteAccounts) {
⋮----
// store an account, make it a zero lamport account
// in slot 1
⋮----
accounts.store_for_tests((1, [(&pubkey, &zero_account)].as_slice()));
// simulate slots are rooted after while
⋮----
// Clean is performed as part of flush with obsolete accounts marked, so explicit clean isn't needed
⋮----
// Only clean up to account 0, should not purge slot 0 based on
// updates in later slots in slot 1
⋮----
accounts.clean_accounts(Some(0), false, &EpochSchedule::default());
⋮----
assert!(accounts.accounts_index.contains_with(&pubkey, None, None));
// Now the account can be cleaned up
accounts.clean_accounts(Some(1), false, &EpochSchedule::default());
⋮----
// The zero lamport account, should no longer exist in accounts index
// because it has been removed
⋮----
fn assert_no_stores(accounts: &AccountsDb, slot: Slot) {
let store = accounts.storage.get_slot_storage_entry(slot);
assert!(store.is_none());
⋮----
fn test_accounts_db_purge_keep_live() {
⋮----
let owner = *AccountSharedData::default().owner();
⋮----
// If there is no latest full snapshot, zero lamport accounts can be cleaned and removed
// immediately. Set latest full snapshot slot to zero to avoid cleaning zero lamport accounts
⋮----
// Step A
⋮----
accounts.store_for_tests((current_slot, [(&pubkey, &account)].as_slice()));
// Store another live account to slot 1 which will prevent any purge
// since the store count will not be zero
accounts.store_for_tests((current_slot, [(&pubkey2, &account2)].as_slice()));
accounts.add_root_and_flush_write_cache(current_slot);
⋮----
.get_with_and_then(&pubkey, None, None, false, |(slot, account_info)| {
⋮----
.get_with_and_then(&pubkey2, None, None, false, |(slot, account_info)| {
⋮----
assert_eq!(slot1, current_slot);
⋮----
assert_eq!(account_info1.store_id(), account_info2.store_id());
// Step B
⋮----
accounts.store_for_tests((current_slot, [(&pubkey, &zero_lamport_account)].as_slice()));
⋮----
accounts.assert_load_account(current_slot, pubkey, zero_lamport);
⋮----
accounts.print_accounts_stats("pre_purge");
⋮----
accounts.print_accounts_stats("post_purge");
let (slot_list_len, index_slot) = accounts.accounts_index.get_and_then(&pubkey, |entry| {
let slot_list = entry.unwrap().slot_list_read_lock();
(false, (slot_list.len(), slot_list[0].0))
⋮----
assert_eq!(index_slot, zero_lamport_slot);
accounts.assert_ref_count(&pubkey, 2);
accounts.check_storage(1, 1, 2);
accounts.check_storage(2, 1, 1);
⋮----
fn test_accounts_db_purge1() {
⋮----
accounts.add_root(0);
⋮----
let ancestors = linear_ancestors(current_slot);
info!("ancestors: {ancestors:?}");
let hash = accounts.calculate_accounts_lt_hash_at_startup_from_index(&ancestors, current_slot);
⋮----
assert!(!accounts.accounts_index.contains(&pubkey));
assert_no_stores(&accounts, 1);
assert_no_stores(&accounts, 2);
⋮----
fn test_store_account_stress() {
⋮----
db.add_root(slot);
⋮----
.map(|_| {
let db = db.clone();
⋮----
.name("account-writers".to_string())
.spawn(move || {
⋮----
let account_bal = rng().random_range(1..99);
account.set_lamports(account_bal);
db.store_for_tests((slot, [(&pubkey, &account)].as_slice()));
⋮----
.load_without_fixed_root(&Ancestors::default(), &pubkey)
.unwrap_or_else(|| {
panic!("Could not fetch stored account {pubkey}, iter {i}")
⋮----
assert_eq!(slot, slot);
assert_eq!(account.lamports(), account_bal);
⋮----
t.join().unwrap();
⋮----
fn test_accountsdb_scan_accounts() {
⋮----
db.store_for_tests((0, [(&key0, &account0)].as_slice()));
⋮----
db.store_for_tests((1, [(&key1, &account1)].as_slice()));
⋮----
db.scan_accounts(
⋮----
accounts.push(account);
⋮----
.expect("should scan accounts");
assert_eq!(accounts, vec![account0]);
let ancestors = vec![(1, 1), (0, 0)].into_iter().collect();
⋮----
assert_eq!(accounts.len(), 2);
⋮----
fn test_cleanup_key_not_removed() {
⋮----
db.print_accounts_stats("pre");
let slots: HashSet<Slot> = vec![1].into_iter().collect();
⋮----
let _ = db.purge_keys_exact(purge_keys);
⋮----
db.store_for_tests((2, [(&key1, &account2)].as_slice()));
db.print_accounts_stats("post");
let ancestors = vec![(2, 0)].into_iter().collect();
⋮----
fn test_store_large_account() {
⋮----
db.store_for_tests((0, [(&key, &account)].as_slice()));
⋮----
let ret = db.load_without_fixed_root(&ancestors, &key).unwrap();
assert_eq!(ret.0.data().len(), data_len);
⋮----
fn test_hash_stored_account() {
⋮----
let account = create_account_shared_data(&stored_account);
let expected_account_hash = LtHashChecksum([
⋮----
fn test_verify_bank_capitalization() {
⋮----
let ancestors = vec![(some_slot, 0)].into_iter().collect();
db.store_for_tests((some_slot, [(&key, &account)].as_slice()));
⋮----
db.add_root_and_flush_write_cache(some_slot);
⋮----
db.store_for_tests((
⋮----
&create_loadable_account_for_test("foo"),
⋮----
.as_slice(),
⋮----
fn test_storage_finder() {
⋮----
db.create_and_insert_store(1, 8192, "test_storage_finder");
db.store_for_tests((1, [(&key, &account)].as_slice()));
⋮----
fn test_get_snapshot_storages_empty() {
⋮----
assert!(db.get_storages(..=0).0.is_empty());
⋮----
fn test_get_snapshot_storages_only_older_than_or_equal_to_snapshot_slot() {
⋮----
db.store_for_tests((base_slot, [(&key, &account)].as_slice()));
db.add_root_and_flush_write_cache(base_slot);
assert!(db.get_storages(..=before_slot).0.is_empty());
assert_eq!(1, db.get_storages(..=base_slot).0.len());
assert_eq!(1, db.get_storages(..=after_slot).0.len());
⋮----
fn test_get_snapshot_storages_only_non_empty() {
⋮----
db.storage.remove(&base_slot, false);
assert!(db.get_storages(..=after_slot).0.is_empty());
⋮----
fn test_get_snapshot_storages_only_roots() {
⋮----
fn test_get_snapshot_storages_exclude_empty() {
⋮----
.get_slot_storage_entry(0)
⋮----
.remove_accounts(0, 1);
⋮----
fn test_get_snapshot_storages_with_base_slot() {
⋮----
db.store_for_tests((slot, [(&key, &account)].as_slice()));
db.add_root_and_flush_write_cache(slot);
assert_eq!(0, db.get_storages(slot + 1..=slot + 1).0.len());
assert_eq!(1, db.get_storages(slot..=slot + 1).0.len());
⋮----
fn do_full_clean_refcount(mut accounts: AccountsDb, store1_first: bool, store_size: u64) {
let pubkey1 = Pubkey::from_str("My11111111111111111111111111111111111111111").unwrap();
let pubkey2 = Pubkey::from_str("My22211111111111111111111111111111111111111").unwrap();
let pubkey3 = Pubkey::from_str("My33311111111111111111111111111111111111111").unwrap();
⋮----
accounts.store_for_tests((current_slot, [(&pubkey1, &account)].as_slice()));
accounts.store_for_tests((current_slot, [(&pubkey2, &account)].as_slice()));
⋮----
info!("post A");
accounts.print_accounts_stats("Post-A");
⋮----
assert_eq!(0, accounts.alive_account_count_in_slot(current_slot));
⋮----
accounts.store_for_tests((current_slot, [(&pubkey1, &account2)].as_slice()));
⋮----
assert_eq!(1, accounts.alive_account_count_in_slot(current_slot));
accounts.assert_ref_count(&pubkey1, 2);
⋮----
accounts.print_accounts_stats("Post-B pre-clean");
⋮----
info!("post B");
accounts.print_accounts_stats("Post-B");
⋮----
accounts.store_for_tests((current_slot, [(&pubkey1, &account3)].as_slice()));
accounts.store_for_tests((current_slot, [(&pubkey2, &account3)].as_slice()));
accounts.store_for_tests((current_slot, [(&pubkey3, &account4)].as_slice()));
⋮----
info!("post C");
accounts.print_accounts_stats("Post-C");
⋮----
accounts.store_for_tests((current_slot, [(&pubkey1, &zero_lamport_account)].as_slice()));
accounts.store_for_tests((current_slot, [(&pubkey2, &zero_lamport_account)].as_slice()));
accounts.store_for_tests((current_slot, [(&pubkey3, &zero_lamport_account)].as_slice()));
let snapshot_stores = accounts.get_storages(..=current_slot).0;
let total_accounts: usize = snapshot_stores.iter().map(|s| s.accounts_count()).sum();
assert!(!snapshot_stores.is_empty());
assert!(total_accounts > 0);
info!("post D");
accounts.print_accounts_stats("Post-D");
⋮----
accounts.print_accounts_stats("Post-D clean");
let total_accounts_post_clean: usize = snapshot_stores.iter().map(|s| s.accounts_count()).sum();
assert_eq!(total_accounts, total_accounts_post_clean);
⋮----
accounts.assert_ref_count(&pubkey3, 0);
⋮----
define_accounts_db_test!(test_full_clean_refcount_no_first_4m, |accounts| {
⋮----
define_accounts_db_test!(test_full_clean_refcount_no_first_4k, |accounts| {
⋮----
define_accounts_db_test!(test_full_clean_refcount_first_4k, |accounts| {
⋮----
fn test_clean_stored_dead_slots_empty() {
⋮----
dead_slots.insert(10);
accounts.clean_stored_dead_slots(&dead_slots, None, &HashSet::default());
⋮----
fn test_shrink_all_slots_none() {
⋮----
accounts.shrink_candidate_slots(&epoch_schedule);
⋮----
accounts.shrink_all_slots(*startup, &EpochSchedule::default(), None);
⋮----
fn test_shrink_candidate_slots() {
⋮----
.map(|_| solana_pubkey::new_rand())
⋮----
accounts.store_for_tests((current_slot, [(pubkey, &account)].as_slice()));
⋮----
accounts.shrink_candidate_slots(&EpochSchedule::default());
⋮----
accounts.shrink_all_slots(false, &EpochSchedule::default(), None);
⋮----
fn test_shrink_candidate_slots_with_dead_ancient_account() {
⋮----
.map(|data_size| {
⋮----
.map(|(pubkey, account)| (pubkey, account))
⋮----
db.store_for_tests((starting_ancient_slot, accounts.as_slice()));
db.add_root_and_flush_write_cache(starting_ancient_slot);
let storage = db.get_storage_for_slot(starting_ancient_slot).unwrap();
let ancient_accounts = db.get_unique_accounts_from_storage(&storage);
assert_eq!(ancient_accounts.stored_accounts.len(), 3);
⋮----
.min_by(|a, b| a.data_len.cmp(&b.data_len))
⋮----
let modified_account_owner = *AccountSharedData::default().owner();
⋮----
let ancient_append_vec_offset = db.ancient_append_vec_offset.unwrap().abs();
⋮----
[(&modified_account_pubkey, &modified_account)].as_slice(),
⋮----
db.add_root_and_flush_write_cache(current_slot);
db.clean_accounts_for_tests();
db.shrink_ancient_slots(&epoch_schedule);
⋮----
let created_accounts = db.get_unique_accounts_from_storage(&storage);
assert_eq!(created_accounts.stored_accounts.len(), 3);
db.shrink_candidate_slots(&epoch_schedule);
⋮----
.scan_pubkeys(|pubkey| {
assert_ne!(pubkey, &modified_account_pubkey);
⋮----
.expect("must scan accounts storage");
⋮----
fn test_select_candidates_by_total_usage_no_candidates() {
⋮----
db.select_candidates_by_total_usage(&candidates, DEFAULT_ACCOUNTS_SHRINK_RATIO);
assert_eq!(0, selected_candidates.len());
assert_eq!(0, next_candidates.len());
⋮----
fn test_select_candidates_by_total_usage_3_way_split_condition(storage_access: StorageAccess) {
⋮----
db.storage.insert(store1_slot, Arc::clone(&store1));
store1.alive_bytes.store(0, Ordering::Release);
candidates.insert(store1_slot);
⋮----
db.storage.insert(store2_slot, Arc::clone(&store2));
⋮----
.store(store_file_size as usize / 2, Ordering::Release);
candidates.insert(store2_slot);
⋮----
db.storage.insert(store3_slot, Arc::clone(&store3));
⋮----
.store(store_file_size as usize, Ordering::Release);
candidates.insert(store3_slot);
// Set the target alive ratio to 0.6 so that we can just get rid of store1, the remaining two stores
// alive ratio can be > the target ratio: the actual ratio is 0.75 because of 150 alive bytes / 200 total bytes.
// The target ratio is also set to larger than store2's alive ratio: 0.5 so that it would be added
// to the candidates list for next round.
⋮----
db.select_candidates_by_total_usage(&candidates, target_alive_ratio);
assert_eq!(1, selected_candidates.len());
assert!(selected_candidates.contains(&store1_slot));
assert_eq!(1, next_candidates.len());
assert!(next_candidates.contains(&store2_slot));
⋮----
fn test_select_candidates_by_total_usage_2_way_split_condition(storage_access: StorageAccess) {
// three candidates, 2 are selected for shrink, one is ignored
⋮----
// Set the target ratio to default (0.8), both store1 and store2 must be selected and store3 is ignored.
⋮----
assert_eq!(2, selected_candidates.len());
⋮----
assert!(selected_candidates.contains(&store2_slot));
⋮----
fn test_select_candidates_by_total_usage_all_clean(storage_access: StorageAccess) {
// 2 candidates, they must be selected to achieve the target alive ratio
⋮----
.store(store_file_size as usize / 4, Ordering::Release);
⋮----
// Set the target ratio to default (0.8), both stores from the two different slots must be selected.
⋮----
fn test_delete_dependencies() {
⋮----
accounts_index.upsert(
⋮----
accounts_index.add_root(0);
accounts_index.add_root(1);
accounts_index.add_root(2);
accounts_index.add_root(3);
let num_bins = accounts_index.bins();
⋮----
.take(num_bins)
⋮----
let (rooted_entries, ref_count) = accounts_index.get_and_then(key, |entry| {
let slot_list_lock = entry.unwrap().slot_list_read_lock();
let rooted = accounts_index.get_rooted_entries(slot_list_lock.as_ref(), None);
(false, (rooted, entry.unwrap().ref_count()))
⋮----
let index = accounts_index.bin_calculator.bin_from_pubkey(key);
⋮----
candidates_bin.insert(
⋮----
for candidates_bin in candidates.iter() {
⋮----
) in candidates_bin.iter()
⋮----
info!(" purge {key} ref_count {ref_count} =>");
⋮----
info!("  {x:?}");
⋮----
store_counts.insert(0, (0, HashSet::from_iter(vec![key0])));
store_counts.insert(1, (0, HashSet::from_iter(vec![key0, key1])));
store_counts.insert(2, (0, HashSet::from_iter(vec![key1, key2])));
store_counts.insert(3, (1, HashSet::from_iter(vec![key2])));
⋮----
accounts.calc_delete_dependencies(&candidates, &mut store_counts, None);
let mut stores: Vec<_> = store_counts.keys().cloned().collect();
stores.sort_unstable();
⋮----
info!(
⋮----
// if the store count doesn't exist for this id, then it is implied to be > 0
assert!(store_counts
⋮----
fn test_account_balance_for_capitalization_sysvar() {
⋮----
assert_eq!(normal_sysvar.lamports(), 1);
⋮----
fn test_account_balance_for_capitalization_native_program() {
let normal_native_program = create_loadable_account_for_test("foo");
assert_eq!(normal_native_program.lamports(), 1);
⋮----
fn test_store_overhead() {
⋮----
let store = accounts.storage.get_slot_storage_entry(0).unwrap();
let total_len = store.accounts.len();
info!("total: {total_len}");
assert_eq!(total_len, STORE_META_OVERHEAD);
⋮----
fn test_store_clean_after_shrink() {
⋮----
accounts.store_for_tests((0, &[(&pubkey1, &account)][..]));
⋮----
accounts.store_for_tests((0, &[(&pubkey2, &account)][..]));
⋮----
accounts.store_for_tests((1, &[(&pubkey1, &zero_account)][..]));
⋮----
accounts.flush_accounts_cache(true, None);
⋮----
accounts.print_accounts_stats("post-clean");
⋮----
fn test_wrapping_storage_id() {
⋮----
db.next_id.store(AccountsFileId::MAX, Ordering::Release);
⋮----
let keys = (0..slots).map(|_| Pubkey::new_unique()).collect::<Vec<_>>();
keys.iter().enumerate().for_each(|(slot, key)| {
⋮----
db.store_for_tests((slot, [(key, &zero_lamport_account)].as_slice()));
⋮----
assert_eq!(slots - 1, db.next_id.load(Ordering::Acquire));
⋮----
keys.iter().for_each(|key| {
assert!(db.load_without_fixed_root(&ancestors, key).is_some());
⋮----
fn test_reuse_storage_id() {
⋮----
fn test_zero_lamport_new_root_not_cleaned() {
⋮----
db.store_for_tests((0, [(&account_key, &zero_lamport_account)].as_slice()));
db.store_for_tests((1, [(&account_key, &zero_lamport_account)].as_slice()));
db.add_root_and_flush_write_cache(0);
db.add_root_and_flush_write_cache(1);
db.clean_accounts(Some(0), false, &EpochSchedule::default());
⋮----
fn test_store_load_cached() {
⋮----
db.store_for_tests((slot, &[(&key, &account0)][..]));
assert!(db
⋮----
let ancestors = vec![(slot + 1, 1)].into_iter().collect();
⋮----
let ancestors = vec![(slot, 1)].into_iter().collect();
⋮----
fn test_store_flush_load_cached() {
⋮----
db.mark_slot_frozen(slot);
db.flush_accounts_cache(true, None);
⋮----
fn test_flush_accounts_cache() {
⋮----
db.store_for_tests((unrooted_slot, &[(&unrooted_key, &account0)][..]));
db.store_for_tests((root5, &[(&key5, &account0)][..]));
db.store_for_tests((root6, &[(&key6, &account0)][..]));
⋮----
db.mark_slot_frozen(*slot);
⋮----
db.add_root(root5);
db.add_root(root6);
⋮----
assert!(db.accounts_index.contains(&unrooted_key));
assert_eq!(db.accounts_cache.num_slots(), 1);
assert!(db.accounts_cache.slot_cache(unrooted_slot).is_some());
⋮----
fn max_cache_slots() -> usize {
⋮----
fn test_flush_accounts_cache_if_needed() {
run_test_flush_accounts_cache_if_needed(0, 2 * max_cache_slots());
run_test_flush_accounts_cache_if_needed(2 * max_cache_slots(), 0);
run_test_flush_accounts_cache_if_needed(max_cache_slots() - 1, 0);
run_test_flush_accounts_cache_if_needed(0, max_cache_slots() - 1);
run_test_flush_accounts_cache_if_needed(max_cache_slots(), 0);
run_test_flush_accounts_cache_if_needed(0, max_cache_slots());
run_test_flush_accounts_cache_if_needed(2 * max_cache_slots(), 2 * max_cache_slots());
run_test_flush_accounts_cache_if_needed(max_cache_slots() - 1, max_cache_slots() - 1);
run_test_flush_accounts_cache_if_needed(max_cache_slots(), max_cache_slots());
⋮----
fn run_test_flush_accounts_cache_if_needed(num_roots: usize, num_unrooted: usize) {
⋮----
db.write_cache_limit_bytes = Some(max_cache_slots() as u64);
⋮----
let num_slots = 2 * max_cache_slots();
⋮----
db.store_for_tests((i as Slot, &[(&key, &account0)][..]));
⋮----
db.mark_slot_frozen(i as Slot);
⋮----
db.add_root(i as Slot);
⋮----
db.flush_accounts_cache(false, None);
⋮----
if total_slots <= max_cache_slots() {
assert_eq!(db.accounts_cache.num_slots(), total_slots);
⋮----
let expected_size = std::cmp::min(num_unrooted, max_cache_slots());
⋮----
for (slot, key) in (0..num_slots as Slot).zip(keys) {
⋮----
vec![(slot, 1)].into_iter().collect()
⋮----
fn test_read_only_accounts_cache() {
⋮----
let slot1_account = AccountSharedData::new(1, 1, AccountSharedData::default().owner());
db.store_for_tests((0, &[(&account_key, &zero_lamport_account)][..]));
db.store_for_tests((1, &[(&account_key, &slot1_account)][..]));
db.add_root(0);
db.add_root(1);
⋮----
db.add_root(2);
assert_eq!(db.read_only_accounts_cache.cache_len(), 0);
⋮----
.load_with_fixed_root(&Ancestors::default(), &account_key)
.map(|(account, _)| account)
⋮----
assert_eq!(account.lamports(), 1);
assert_eq!(db.read_only_accounts_cache.cache_len(), 1);
⋮----
db.store_for_tests((2, &[(&account_key, &zero_lamport_account)][..]));
⋮----
.map(|(account, _)| account);
assert!(account.is_none());
⋮----
fn test_load_with_read_only_accounts_cache() {
⋮----
.load_account_with(&Ancestors::default(), &account_key, false)
⋮----
assert_eq!(slot, 1);
⋮----
.load_account_with(&Ancestors::default(), &account_key, true)
⋮----
let account = db.load_account_with(&Ancestors::default(), &account_key, false);
⋮----
db.read_only_accounts_cache.reset_for_tests();
⋮----
let account = db.load_account_with(&Ancestors::default(), &account_key, true);
⋮----
let slot2_account = AccountSharedData::new(2, 1, AccountSharedData::default().owner());
db.store_for_tests((2, &[(&account_key, &slot2_account)][..]));
⋮----
assert_eq!(account.lamports(), 2);
⋮----
assert_eq!(slot, 2);
⋮----
fn test_flush_cache_clean() {
⋮----
.do_load(
⋮----
Some(0),
⋮----
assert_eq!(account.0.lamports(), 0);
⋮----
fn test_flush_cache_dont_clean_zero_lamport_account(mark_obsolete_accounts: MarkObsoleteAccounts) {
⋮----
db.set_latest_full_snapshot_slot(0);
⋮----
AccountSharedData::new(original_lamports, 1, AccountSharedData::default().owner());
⋮----
db.store_for_tests((0, &[(&zero_lamport_account_key, &slot0_account)][..]));
db.store_for_tests((0, &[(&other_account_key, &slot0_account)][..]));
⋮----
assert!(db.storage.get_slot_storage_entry(0).is_some());
db.store_for_tests((1, &[(&zero_lamport_account_key, &zero_lamport_account)][..]));
db.store_for_tests((2, &[(&zero_lamport_account_key, &zero_lamport_account)][..]));
⋮----
db.assert_ref_count(&zero_lamport_account_key, 2);
⋮----
db.assert_ref_count(&zero_lamport_account_key, 1);
⋮----
db.assert_ref_count(&other_account_key, 1);
⋮----
fn test_flush_cache_populates_uncleaned_pubkeys() {
⋮----
accounts_db.store_for_tests((slot, [(pubkey, account)].as_slice()));
assert_eq!(accounts_db.get_len_of_slots_with_uncleaned_pubkeys(), 0);
accounts_db.add_root_and_flush_write_cache(slot);
assert_eq!(accounts_db.get_len_of_slots_with_uncleaned_pubkeys(), 1);
accounts_db.clean_accounts_for_tests();
⋮----
struct ScanTracker {
⋮----
impl ScanTracker {
fn exit(self) -> thread::Result<()> {
self.exit.store(true, Ordering::Relaxed);
self.t_scan.join()
⋮----
fn setup_scan(
⋮----
let exit_ = exit.clone();
⋮----
let ready_ = ready.clone();
⋮----
.name("scan".to_string())
⋮----
ready_.store(true, Ordering::Relaxed);
⋮----
if exit_.load(Ordering::Relaxed) {
⋮----
sleep(Duration::from_millis(10));
⋮----
while !ready.load(Ordering::Relaxed) {
⋮----
fn test_scan_flush_accounts_cache_then_clean_drop() {
⋮----
db.store_for_tests((1, &[(&account_key2, &slot1_account)][..]));
⋮----
db.add_root(max_scan_root);
let scan_ancestors: Arc<Ancestors> = Arc::new(vec![(0, 1), (1, 1)].into_iter().collect());
⋮----
let scan_tracker = setup_scan(db.clone(), scan_ancestors.clone(), bank_id, account_key2);
⋮----
db.add_root(new_root);
⋮----
db.flush_accounts_cache(true, Some(new_root));
⋮----
assert!(db.accounts_cache.slot_cache(1).is_some());
⋮----
assert_eq!(account.0.lamports(), zero_lamport_account.lamports());
⋮----
Some(max_scan_root),
⋮----
assert_eq!(account.0.lamports(), slot1_account.lamports());
scan_tracker.exit().unwrap();
⋮----
db.purge_slot(1, bank_id, false);
⋮----
fn get_and_assert_single_storage(&self, slot: Slot) -> Arc<AccountStorageEntry> {
self.storage.get_slot_storage_entry(slot).unwrap()
⋮----
define_accounts_db_test!(test_alive_bytes, |accounts_db| {
⋮----
fn setup_accounts_db_cache_clean(
⋮----
let slots: Vec<_> = (0..num_slots as Slot).collect();
⋮----
.take(num_slots)
⋮----
if scan_slot.is_some() {
accounts_db.store_for_tests(
⋮----
accounts_db.store_for_tests((
⋮----
accounts_db.add_root(*slot as Slot);
if Some(*slot) == scan_slot {
let ancestors = Arc::new(vec![(stall_slot, 1), (*slot, 1)].into_iter().collect());
⋮----
scan_tracker = Some(setup_scan(
accounts_db.clone(),
⋮----
accounts_db.accounts_cache.remove_slot(stall_slot);
if accounts_db.accounts_cache.num_slots() <= max_cache_slots() {
accounts_db.flush_accounts_cache(false, None);
assert_eq!(accounts_db.accounts_cache.num_slots(), num_slots);
⋮----
fn test_accounts_db_cache_clean_dead_slots() {
⋮----
let (accounts_db, keys, mut slots, _) = setup_accounts_db_cache_clean(num_slots, None, None);
⋮----
assert_eq!(*slots.last().unwrap(), last_dead_slot);
⋮----
slots.push(alive_slot);
⋮----
accounts_db.add_root(alive_slot);
⋮----
assert!(accounts_db
⋮----
accounts_db.flush_accounts_cache(true, None);
assert_eq!(accounts_db.accounts_cache.num_slots(), 0);
⋮----
if let ScanStorageResult::Stored(slot_accounts) = accounts_db.scan_account_storage(
⋮----
|_| Some(0),
⋮----
slot_accounts.insert(*stored_account.pubkey());
⋮----
assert_eq!(slot_accounts.len(), keys.len());
⋮----
assert!(slot_accounts.is_empty());
⋮----
panic!("Expected slot to be in storage, not cache");
⋮----
fn test_accounts_db_cache_clean() {
let (accounts_db, keys, slots, _) = setup_accounts_db_cache_clean(10, None, None);
⋮----
if let ScanStorageResult::Stored(slot_account) = accounts_db.scan_account_storage(
⋮----
*slot_account = *stored_account.pubkey();
⋮----
assert_eq!(slot_account, keys[*slot as usize]);
⋮----
panic!("Everything should have been flushed")
⋮----
fn run_test_accounts_db_cache_clean_max_root(
⋮----
assert!(requested_flush_root < (num_slots as Slot));
⋮----
setup_accounts_db_cache_clean(num_slots, scan_root, Some(max_cache_slots() as u64));
let is_cache_at_limit = num_slots - requested_flush_root as usize - 1 > max_cache_slots();
accounts_db.flush_accounts_cache(true, Some(requested_flush_root));
⋮----
assert_eq!(accounts_db.accounts_cache.num_slots(), 0,);
⋮----
let slot_accounts = accounts_db.scan_account_storage(
⋮----
assert!(*slot > requested_flush_root);
Some(*loaded_account.pubkey())
⋮----
assert!(*slot <= requested_flush_root);
⋮----
slot_accounts.into_iter().collect::<HashSet<Pubkey>>()
⋮----
if *slot >= requested_flush_root || *slot >= scan_root.unwrap_or(Slot::MAX) {
⋮----
assert_eq!(slot_accounts, expected_accounts);
⋮----
fn test_accounts_db_cache_clean_max_root() {
⋮----
run_test_accounts_db_cache_clean_max_root(10, requested_flush_root, None);
⋮----
fn test_accounts_db_cache_clean_max_root_with_scan() {
⋮----
run_test_accounts_db_cache_clean_max_root(
⋮----
Some(requested_flush_root - 1),
⋮----
Some(requested_flush_root + 1),
⋮----
fn test_accounts_db_cache_clean_max_root_with_cache_limit_hit() {
⋮----
max_cache_slots() + requested_flush_root as usize + 2,
⋮----
fn test_accounts_db_cache_clean_max_root_with_cache_limit_hit_and_scan() {
⋮----
fn run_flush_rooted_accounts_cache(should_clean: bool) {
⋮----
let (accounts_db, keys, slots, _) = setup_accounts_db_cache_clean(num_slots, None, None);
accounts_db.flush_rooted_accounts_cache(None, should_clean);
⋮----
let ScanStorageResult::Stored(slot_accounts) = accounts_db.scan_account_storage(
⋮----
slot_account.insert(*stored_account.pubkey());
⋮----
panic!("All roots should have been flushed to storage");
⋮----
let expected_accounts = if !should_clean || slot == slots.last().unwrap() {
⋮----
fn test_flush_rooted_accounts_cache_with_clean() {
run_flush_rooted_accounts_cache(true);
⋮----
fn test_flush_rooted_accounts_cache_without_clean() {
run_flush_rooted_accounts_cache(false);
⋮----
fn test_shrink_unref() {
⋮----
let account1 = AccountSharedData::new(1, 0, AccountSharedData::default().owner());
db.store_for_tests((0, [(&account_key1, &account1)].as_slice()));
db.store_for_tests((0, [(&account_key2, &account1)].as_slice()));
⋮----
db.store_for_tests((1, &[(&account_key1, &account1)][..]));
⋮----
db.flush_rooted_accounts_cache(None, false);
db.clean_accounts(Some(1), false, &EpochSchedule::default());
⋮----
let mut shrink_candidate_slots = db.shrink_candidate_slots.lock().unwrap();
shrink_candidate_slots.insert(0);
⋮----
db.store_for_tests((2, &[(&account_key2, &account1)][..]));
⋮----
db.get_and_assert_single_storage(0);
db.clean_accounts(Some(2), false, &EpochSchedule::default());
assert_no_storages_at_slot(&db, 0);
db.assert_ref_count(&account_key1, 1);
⋮----
fn test_clean_drop_dead_zero_lamport_single_ref_accounts() {
⋮----
let one_account = AccountSharedData::new(1, 0, AccountSharedData::default().owner());
⋮----
accounts_db.store_for_tests((slot, &[(&key1, &one_account)][..]));
accounts_db.add_root(slot);
⋮----
accounts_db.store_for_tests((slot, &[(&key1, &zero_account)][..]));
⋮----
accounts_db.clean_accounts(Some(1), false, &epoch_schedule);
assert!(accounts_db.storage.get_slot_storage_entry(0).is_none());
assert!(accounts_db.storage.get_slot_storage_entry(1).is_none());
⋮----
fn test_clean_drop_dead_storage_handle_zero_lamport_single_ref_accounts() {
⋮----
let account0 = AccountSharedData::new(0, 0, AccountSharedData::default().owner());
⋮----
db.store_for_tests((1, &[(&account_key1, &account0)][..]));
db.store_for_tests((1, &[(&account_key2, &account1)][..]));
⋮----
assert!(db.storage.get_slot_storage_entry(0).is_none());
⋮----
assert!(db.shrink_candidate_slots.lock().unwrap().contains(&1));
⋮----
fn test_shrink_unref_handle_zero_lamport_single_ref_accounts() {
⋮----
assert!(db.dirty_stores.contains_key(&1));
assert!(!db.shrink_candidate_slots.lock().unwrap().contains(&1));
⋮----
db.get_and_assert_single_storage(1);
⋮----
assert_no_storages_at_slot(&db, 1);
db.assert_ref_count(&account_key2, 1);
db.get_and_assert_single_storage(2);
⋮----
define_accounts_db_test!(test_partial_clean, |db| {
⋮----
fn start_load_thread(
⋮----
.name("account-do-load".to_string())
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
// Meddle load_limit to cover all branches of implementation.
// There should absolutely no behavioral difference; the load_limit triggered
// slow branch should only affect the performance.
// Ordering::Relaxed is ok because of no data dependencies; the modified field is
// completely free-standing cfg(test) control-flow knob.
⋮----
.store(rng().random_range(0..10) as u64, Ordering::Relaxed);
// Load should never be unable to find this key
⋮----
// slot + 1 == account.lamports because of the account-cache-flush thread
⋮----
fn test_load_account_and_cache_flush_race() {
⋮----
pubkey.as_ref(),
&AccountSharedData::new(1, 0, AccountSharedData::default().owner()),
⋮----
let exit = exit.clone();
let pubkey = pubkey.clone();
let mut account = AccountSharedData::new(1, 0, AccountSharedData::default().owner());
⋮----
.name("account-cache-flush".to_string())
⋮----
account.set_lamports(slot + 1);
db.store_for_tests((slot, &[(pubkey.as_ref(), &account)][..]));
⋮----
sleep(Duration::from_millis(RACY_SLEEP_MS));
⋮----
let t_do_load = start_load_thread(
⋮----
exit.clone(),
⋮----
sleep(Duration::from_secs(RACE_TIME));
exit.store(true, Ordering::Relaxed);
t_flush_accounts_cache.join().unwrap();
t_do_load.join().map_err(std::panic::resume_unwind).unwrap()
⋮----
fn do_test_load_account_and_shrink_race(with_retry: bool) {
⋮----
// Store an account
⋮----
account.set_lamports(lamports);
db.store_for_tests((slot, [(pubkey.as_ref(), &account)].as_slice()));
// Set the slot as a root so account loads will see the contents of this slot
⋮----
.name("account-shrink".to_string())
.spawn(move || loop {
⋮----
// Simulate adding shrink candidates from clean_accounts()
db.shrink_candidate_slots.lock().unwrap().insert(slot);
⋮----
t_shrink_accounts.join().unwrap();
⋮----
fn test_load_account_and_shrink_race_with_retry() {
do_test_load_account_and_shrink_race(true);
⋮----
fn test_load_account_and_shrink_race_without_retry() {
do_test_load_account_and_shrink_race(false);
⋮----
fn test_cache_flush_delayed_remove_unrooted_race() {
⋮----
// Start up a thread to flush the accounts cache
⋮----
// Wait for the signal to start a trial
if flush_trial_start_receiver.recv().is_err() {
⋮----
db.flush_slot_cache(10);
flush_done_sender.send(()).unwrap();
⋮----
// Start up a thread remove the slot
⋮----
.name("account-remove".to_string())
⋮----
if remove_trial_start_receiver.recv().is_err() {
⋮----
db.remove_unrooted_slots(&[(slot, bank_id)]);
remove_done_sender.send(()).unwrap();
⋮----
db.store_for_tests((slot, &[(&pubkey, &account)][..]));
// Wait for both threads to finish
flush_trial_start_sender.send(()).unwrap();
remove_trial_start_sender.send(()).unwrap();
let _ = flush_done_receiver.recv();
let _ = remove_done_receiver.recv();
⋮----
drop(flush_trial_start_sender);
drop(remove_trial_start_sender);
t_flush_cache.join().unwrap();
t_remove.join().unwrap();
⋮----
fn test_cache_flush_remove_unrooted_race_multiple_slots() {
⋮----
if new_trial_start_receiver.recv().is_err() {
⋮----
db.flush_slot_cache(slot);
⋮----
// Simulate spurious wake-up that can happen, but is too rare to
// otherwise depend on in tests.
db.remove_unrooted_slots_synchronization.signal.notify_all();
⋮----
// Run multiple trials. Has the added benefit of rewriting the same slots after we've
⋮----
.map(|slot| {
⋮----
all_slots.shuffle(&mut rand::rng());
⋮----
new_trial_start_sender.send(()).unwrap();
for chunks in slots_to_dump.chunks(slots_to_dump.len() / 2) {
db.remove_unrooted_slots(chunks);
⋮----
assert_no_storages_at_slot(&db, *slot);
assert!(db.accounts_cache.slot_cache(*slot).is_none());
⋮----
assert!(!db.accounts_index.contains(&account_in_slot));
⋮----
flush_done_receiver.recv().unwrap();
⋮----
db.remove_unrooted_slots(&[(*slot, *bank_id)]);
⋮----
drop(new_trial_start_sender);
⋮----
t_spurious_signal.join().unwrap();
⋮----
fn test_collect_uncleaned_slots_up_to_slot() {
⋮----
db.uncleaned_pubkeys.insert(slot1, vec![pubkey1]);
db.uncleaned_pubkeys.insert(slot2, vec![pubkey2]);
db.uncleaned_pubkeys.insert(slot3, vec![pubkey3]);
let mut uncleaned_slots1 = db.collect_uncleaned_slots_up_to_slot(slot1);
let mut uncleaned_slots2 = db.collect_uncleaned_slots_up_to_slot(slot2);
let mut uncleaned_slots3 = db.collect_uncleaned_slots_up_to_slot(slot3);
uncleaned_slots1.sort_unstable();
uncleaned_slots2.sort_unstable();
uncleaned_slots3.sort_unstable();
assert_eq!(uncleaned_slots1, [slot1]);
assert_eq!(uncleaned_slots2, [slot1, slot2]);
assert_eq!(uncleaned_slots3, [slot1, slot2, slot3]);
⋮----
fn test_remove_uncleaned_slots_and_collect_pubkeys_up_to_slot() {
⋮----
db.store_for_tests((slot1, [(&pubkey1, &account1)].as_slice()));
db.store_for_tests((slot2, [(&pubkey2, &account2)].as_slice()));
db.store_for_tests((slot3, [(&pubkey3, &account3)].as_slice()));
db.add_root(slot2);
db.add_root(slot3);
⋮----
let num_bins = db.accounts_index.bins();
⋮----
db.remove_uncleaned_slots_up_to_slot_and_move_pubkeys(slot3, &candidates);
⋮----
.any(|bin| bin.read().unwrap().contains(pubkey))
⋮----
assert!(candidates_contain(&pubkey1));
assert!(candidates_contain(&pubkey2));
assert!(candidates_contain(&pubkey3));
⋮----
fn test_shrink_productive(storage_access: StorageAccess) {
⋮----
store.add_account(file_size as usize);
assert!(!AccountsDb::is_shrinking_productive(&store));
⋮----
store.add_account(file_size as usize / 2);
store.add_account(file_size as usize / 4);
store.remove_accounts(file_size as usize / 4, 1);
assert!(AccountsDb::is_shrinking_productive(&store));
⋮----
fn test_is_candidate_for_shrink(storage_access: StorageAccess) {
⋮----
panic!("Expect the default to be TotalSpace")
⋮----
.store(store_file_size as usize - 1, Ordering::Release);
assert!(accounts.is_candidate_for_shrink(&entry));
⋮----
assert!(!accounts.is_candidate_for_shrink(&entry));
⋮----
.store(file_size_shrink_limit + 1, Ordering::Release);
⋮----
define_accounts_db_test!(test_calculate_storage_count_and_alive_bytes, |accounts| {
⋮----
fn test_calculate_storage_count_and_alive_bytes_obsolete_account(
⋮----
accounts.accounts_index.set_startup(Startup::Startup);
⋮----
assert!(account_sizes.len() >= num_accounts_to_mark_obsolete);
⋮----
.map(|size| {
⋮----
AccountSharedData::new(1, size, AccountSharedData::default().owner()),
⋮----
let storage = accounts.create_and_insert_store(slot0, 10_000, "");
⋮----
.write_accounts(&(slot0, &account_list[..]), 0);
let offsets = offsets.unwrap().offsets;
let data_lens = storage.accounts.get_account_data_lens(&offsets);
let mut offsets: Vec<_> = offsets.into_iter().zip(data_lens).collect();
// Randomize the accounts that get marked obsolete
⋮----
offsets.shuffle(&mut rng);
⋮----
offsets.split_at(num_accounts_to_mark_obsolete);
⋮----
.write()
⋮----
.mark_accounts_obsolete(accounts_to_mark_obsolete.iter().cloned(), slot0 + 1);
⋮----
let info = accounts.generate_index_for_slot(&mut reader, &storage, 0, 0, &storage_info);
⋮----
assert_eq!(storage_info.len(), 1);
for entry in storage_info.iter() {
// Sum up the stored size of all non obsolete accounts
⋮----
.map(|(_, data_len)| storage.accounts.calculate_stored_size(*data_len))
.sum();
⋮----
accounts.accounts_index.set_startup(Startup::Normal);
⋮----
define_accounts_db_test!(test_set_storage_count_and_alive_bytes, |accounts| {
// make sure we have storage 0
⋮----
// fake out the store count to avoid the assert
⋮----
// count needs to be <= approx stored count in store.
// approx stored count is 1 in store since we added a single account.
⋮----
// populate based on made up hash data
⋮----
define_accounts_db_test!(test_purge_alive_unrooted_slots_after_clean, |accounts| {
// Key shared between rooted and nonrooted slot
⋮----
// Key to keep the storage entry for the unrooted slot alive
⋮----
// Store accounts with greater than 0 lamports
⋮----
// Simulate adding dirty pubkeys on bank freeze. Note this is
// not a rooted slot
// On the next *rooted* slot, update the `shared_key` account to zero lamports
⋮----
// Simulate adding dirty pubkeys on bank freeze, set root
⋮----
// The later rooted zero-lamport update to `shared_key` cannot be cleaned
// because it is kept alive by the unrooted slot.
⋮----
// Simulate purge_slot() all from AccountsBackgroundService
⋮----
// Now clean should clean up the remaining key
⋮----
/// asserts that not only are there 0 append vecs, but there is not even an entry in the storage map for 'slot'
fn assert_no_storages_at_slot(db: &AccountsDb, slot: Slot) {
⋮----
fn assert_no_storages_at_slot(db: &AccountsDb, slot: Slot) {
assert!(db.storage.get_slot_storage_entry(slot).is_none());
⋮----
// Test to make sure `clean_accounts()` works properly with `latest_full_snapshot_slot`
//
// Basically:
⋮----
// - slot 1: set Account1's balance to non-zero
// - slot 2: set Account1's balance to a different non-zero amount
// - slot 3: set Account1's balance to zero
// - call `clean_accounts()` with `max_clean_root` set to 2
//     - ensure Account1 has *not* been purged
//     - ensure the store from slot 1 is cleaned up
// - call `clean_accounts()` with `latest_full_snapshot_slot` set to 2
⋮----
// - call `clean_accounts()` with `latest_full_snapshot_slot` set to 3
//     - ensure Account1 *has* been purged
⋮----
fn test_filter_zero_lamport_clean_for_incremental_snapshots() {
⋮----
struct TestParameters {
⋮----
key_set.insert(pubkey);
⋮----
store_counts.insert(slot, (store_count, key_set));
⋮----
candidates[0].insert(
⋮----
accounts_db.set_latest_full_snapshot_slot(latest_full_snapshot_slot);
⋮----
accounts_db.filter_zero_lamport_clean_for_incremental_snapshots(
⋮----
// Scenario 1: last full snapshot is NONE
// In this scenario incremental snapshots are OFF, so always purge
⋮----
do_test(TestParameters {
⋮----
max_clean_root: Some(slot),
⋮----
// Scenario 2: last full snapshot is GREATER THAN zero lamport account slot
// In this scenario always purge, and just test the various permutations of
// `should_filter_for_incremental_snapshots` based on `max_clean_root`.
⋮----
let latest_full_snapshot_slot = Some(slot + 1);
⋮----
max_clean_root: latest_full_snapshot_slot.map(|s| s + 1),
⋮----
// Scenario 3: last full snapshot is EQUAL TO zero lamport account slot
// In this scenario always purge, as it's the same as Scenario 2.
⋮----
let latest_full_snapshot_slot = Some(slot);
⋮----
// Scenario 4: last full snapshot is LESS THAN zero lamport account slot
// In this scenario do *not* purge, except when `should_filter_for_incremental_snapshots`
// is false
⋮----
let latest_full_snapshot_slot = Some(slot - 1);
⋮----
/// test 'unref' parameter 'pubkeys_removed_from_accounts_index'
fn test_unref_pubkeys_removed_from_accounts_index() {
⋮----
fn test_unref_pubkeys_removed_from_accounts_index() {
⋮----
pubkeys_removed_from_accounts_index.insert(pk1);
⋮----
// pk1 in slot1, purge it
⋮----
purged_slot_pubkeys.insert((slot1, pk1));
⋮----
db.accounts_index.upsert(
⋮----
db.unref_accounts(
⋮----
db.assert_ref_count(&pk1, expected);
⋮----
fn test_unref_accounts() {
⋮----
assert!(purged_stored_account_slots.is_empty());
⋮----
db.assert_ref_count(&pk1, 0);
⋮----
// pk1 and pk2 both in slot1 and slot2, so each has refcount of 2
⋮----
// purge pk1 from both 1 and 2 and pk2 from slot 1
let purges = vec![(slot1, pk1), (slot1, pk2), (slot2, pk1)];
purges.into_iter().for_each(|(slot, pk)| {
purged_slot_pubkeys.insert((slot, pk));
⋮----
for (pk, slots) in [(pk1, vec![slot1, slot2]), (pk2, vec![slot1])] {
let result = purged_stored_account_slots.remove(&pk).unwrap();
assert_eq!(result, slots.into_iter().collect::<IntSet<_>>());
⋮----
db.assert_ref_count(&pk2, 1);
⋮----
define_accounts_db_test!(test_many_unrefs, |db| {
⋮----
// make sure we have > 1 batch. Bigger numbers cost more in test time here.
⋮----
// put the pubkey into the acct idx in 'n' slots
⋮----
// unref all 'n' slots
⋮----
define_accounts_db_test!(test_mark_dirty_dead_stores_empty, |db| {
⋮----
fn test_mark_dirty_dead_stores_no_shrink_in_progress() {
// None for shrink_in_progress, 1 existing store at the slot
// There should be no more append vecs at that slot after the call to mark_dirty_dead_stores.
// This tests the case where this slot was combined into an ancient append vec from an older slot and
// there is no longer an append vec at this slot.
⋮----
let existing_store = db.create_and_insert_store(slot, size, "test");
let old_id = existing_store.id();
let dead_storages = db.mark_dirty_dead_stores(slot, add_dirty_stores, None, false);
⋮----
assert_eq!(dead_storages.len(), 1);
assert_eq!(dead_storages.first().unwrap().id(), old_id);
⋮----
assert_eq!(1, db.dirty_stores.len());
let dirty_store = db.dirty_stores.get(&slot).unwrap();
assert_eq!(dirty_store.id(), old_id);
⋮----
assert!(db.dirty_stores.is_empty());
⋮----
assert!(db.storage.is_empty_entry(slot));
⋮----
fn test_mark_dirty_dead_stores() {
⋮----
let old_store = db.create_and_insert_store(slot, size, "test");
let old_id = old_store.id();
let shrink_in_progress = db.get_store_for_shrink(slot, 100);
⋮----
db.mark_dirty_dead_stores(slot, add_dirty_stores, Some(shrink_in_progress), false);
assert!(db.storage.get_slot_storage_entry(slot).is_some());
⋮----
define_accounts_db_test!(test_add_uncleaned_pubkeys_after_shrink, |db| {
⋮----
fn test_sweep_get_oldest_non_ancient_slot_max() {
⋮----
ancient_append_vec_offset: Some(ancient_append_vec_offset as i64),
⋮----
assert_eq!(0, db.get_oldest_non_ancient_slot(&epoch_schedule));
⋮----
db.add_root(max_root_inclusive);
⋮----
fn test_sweep_get_oldest_non_ancient_slot() {
⋮----
ancient_append_vec_offset: Some(ancient_append_vec_offset),
⋮----
fn test_sweep_get_oldest_non_ancient_slot2() {
⋮----
let ancient_append_vec_offset = db.ancient_append_vec_offset.unwrap();
assert_ne!(ancient_append_vec_offset, 0);
⋮----
db.add_root(completed_slot);
⋮----
-((epoch_schedule.slots_per_epoch as i64).saturating_sub(1)),
⋮----
define_accounts_db_test!(test_get_sorted_potential_ancient_slots, |db| {
⋮----
fn test_shrink_collect_simple() {
⋮----
let max_num_accounts = *account_counts.iter().max().unwrap();
⋮----
pubkey_opposite_zero_lamports = Some(&pubkeys[account_count]);
⋮----
pubkey_opposite_alive = Some(&pubkeys[account_count]);
⋮----
debug!(
⋮----
for pubkey in pubkeys.iter().take(account_count) {
let old_lamports = account.lamports();
if Some(pubkey) == pubkey_opposite_zero_lamports {
account.set_lamports(u64::from(old_lamports == 0));
⋮----
db.store_for_tests((slot5, [(pubkey, &account)].as_slice()));
account.set_lamports(old_lamports);
⋮----
&& Some(pubkey) == pubkey_opposite_alive
⋮----
to_purge.push(*pubkey);
⋮----
db.add_root_and_flush_write_cache(slot5);
to_purge.iter().for_each(|pubkey| {
db.accounts_index.purge_exact(
⋮----
[slot5].into_iter().collect::<HashSet<_>>(),
⋮----
let storage = db.get_storage_for_slot(slot5).unwrap();
⋮----
.get_unique_accounts_from_storage_for_shrink(
⋮----
vec![*pubkey_opposite_alive.unwrap()]
⋮----
vec![]
⋮----
.filter(|p| Some(p) != pubkey_opposite_alive.as_ref())
.sorted()
⋮----
expect_single_opposite_alive_account.clone()
⋮----
assert_eq!(shrink_collect.slot, slot5);
⋮----
// zero lamport accounts store size=0 data
⋮----
assert_eq!(shrink_collect.alive_total_bytes, 0);
⋮----
// expected_capacity is determined by what size append vec gets created when the write cache is flushed to an append vec.
⋮----
(account_count * aligned_stored_size(space)) as u64;
⋮----
// zero lamport accounts always write space = 0
⋮----
assert_eq!(shrink_collect.capacity, expected_capacity);
assert_eq!(shrink_collect.total_starting_accounts, account_count);
⋮----
fn test_shrink_collect_with_obsolete_accounts() {
⋮----
.take(account_count)
⋮----
100, // lamports
128, // space
⋮----
for (i, pubkey) in pubkeys.iter().enumerate() {
⋮----
// Mark third account as zero lamport
// These will be removed during shrink
account.set_lamports(0);
zero_lamport_pubkeys.push(*pubkey);
⋮----
// Regular accounts that should be kept
account.set_lamports(200);
regular_pubkeys.push(*pubkey);
⋮----
db.store_for_tests((slot, [(pubkey, &account)].as_slice()));
⋮----
// Flush the cache
⋮----
let storage = db.get_and_assert_single_storage(slot);
⋮----
// Mark Some accounts obsolete. These will include zero lamport and non zero lamport accounts
⋮----
// Lookup the pubkey in the database and find the AccountInfo
⋮----
.get_with_and_then(pubkey, None, None, false, |account_info| {
db.remove_dead_accounts(
[account_info].iter(),
⋮----
obsolete_pubkeys.push(*pubkey);
⋮----
// Purge accounts via clean and ensure that they will be unreffed.
⋮----
[slot].into_iter().collect::<HashSet<_>>(),
⋮----
unref_pubkeys.push(*pubkey);
⋮----
db.get_unique_accounts_from_storage_for_shrink(&storage, &ShrinkStats::default());
⋮----
assert_eq!(shrink_collect.slot, slot);
⋮----
define_accounts_db_test!(test_combine_ancient_slots_empty, |db| {
⋮----
fn test_combine_ancient_slots_simple() {
_ = get_one_ancient_append_vec_and_others(0);
⋮----
fn get_all_accounts_from_storages<'a>(
⋮----
.flat_map(|storage| {
⋮----
.scan_accounts(&mut reader, |_offset, account| {
vec.push((*account.pubkey(), create_account_shared_data(&account)));
⋮----
.scan_pubkeys(|k| {
compare.push(*k);
⋮----
assert_eq!(compare, vec.iter().map(|(k, _)| *k).collect::<Vec<_>>());
⋮----
pub(crate) fn get_all_accounts(
⋮----
.filter_map(|slot| {
let storage = db.storage.get_slot_storage_entry(slot);
storage.map(|storage| get_all_accounts_from_storages(std::iter::once(&storage)))
⋮----
.flatten()
⋮----
pub(crate) fn compare_all_accounts(
⋮----
let mut two_indexes = (0..two.len()).collect::<Vec<_>>();
one.iter().for_each(|(pubkey, account)| {
for i in 0..two_indexes.len() {
⋮----
if !accounts_equal(account, &two[two_indexes[i]].1) {
⋮----
two_indexes.remove(i);
⋮----
.map(|(_pubkey, account)| account.lamports())
⋮----
pub fn get_account_from_account_from_storage(
⋮----
.get_slot_storage_entry_shrinking_in_progress_ok(slot)
⋮----
.get_account_shared_data(account.index_info.offset())
⋮----
fn populate_index(db: &AccountsDb, slots: Range<Slot>) {
slots.into_iter().for_each(|slot| {
if let Some(storage) = db.get_storage_for_slot(slot) {
⋮----
.scan_accounts(&mut reader, |offset, account| {
⋮----
StorageLocation::AppendVec(storage.id(), offset),
account.is_zero_lamport(),
⋮----
account.pubkey(),
⋮----
pub(crate) fn remove_account_for_tests(storage: &AccountStorageEntry, num_bytes: usize) {
storage.remove_accounts(num_bytes, 1);
⋮----
pub(crate) fn create_storages_and_update_index(
⋮----
let local_tf = (tf.is_none()).then(|| {
⋮----
let tf = tf.unwrap_or_else(|| local_tf.as_ref().unwrap());
⋮----
.map(|storage| storage.1.id())
.max()
.unwrap_or(999);
⋮----
let storage = sample_storage_with_entries_id(
⋮----
db.storage_access(),
⋮----
insert_store(db, Arc::clone(&storage));
⋮----
let storage = db.get_storage_for_slot(starting_slot).unwrap();
⋮----
assert_eq!(created_accounts.stored_accounts.len(), 1);
⋮----
populate_index(db, starting_slot..(starting_slot + (num_slots as Slot) + 1));
⋮----
pub(crate) fn create_db_with_storages_and_index(
⋮----
create_storages_and_update_index(&db, None, slot1, num_slots, alive, account_data_size);
⋮----
fn get_one_ancient_append_vec_and_others_with_account_size(
⋮----
create_db_with_storages_and_index(alive, num_normal_slots + 1, account_data_size);
let storage = db.get_storage_for_slot(slot1).unwrap();
⋮----
db.combine_ancient_slots_packed(vec![slot1], CAN_RANDOMLY_SHRINK_FALSE);
let after_store = db.get_storage_for_slot(slot1).unwrap();
⋮----
} = db.get_unique_accounts_from_storage(&after_store);
assert!(created_accounts.capacity <= after_capacity);
⋮----
assert_eq!(after_stored_accounts.len(), 1);
⋮----
fn get_one_ancient_append_vec_and_others(num_normal_slots: usize) -> (AccountsDb, Slot) {
get_one_ancient_append_vec_and_others_with_account_size(num_normal_slots, None)
⋮----
fn test_handle_dropped_roots_for_ancient() {
⋮----
db.handle_dropped_roots_for_ancient(std::iter::empty::<Slot>());
⋮----
let dropped_roots = vec![slot0];
db.accounts_index.add_root(slot0);
assert!(db.accounts_index.is_alive_root(slot0));
db.handle_dropped_roots_for_ancient(dropped_roots.into_iter());
assert!(!db.accounts_index.is_alive_root(slot0));
⋮----
fn insert_store(db: &AccountsDb, append_vec: Arc<AccountStorageEntry>) {
db.storage.insert(append_vec.slot(), append_vec);
⋮----
fn test_handle_dropped_roots_for_ancient_assert(storage_access: StorageAccess) {
⋮----
insert_store(&db, entry);
⋮----
/// Test that `clean` reclaims old accounts when cleaning old storages
///
⋮----
///
/// When `clean` constructs candidates from old storages, pubkeys in these storages may have other
⋮----
/// When `clean` constructs candidates from old storages, pubkeys in these storages may have other
/// newer versions of the accounts in other newer storages *not* explicitly marked to be visited by
⋮----
/// newer versions of the accounts in other newer storages *not* explicitly marked to be visited by
/// `clean`.  In this case, `clean` should still reclaim the old versions of these accounts.
⋮----
/// `clean`.  In this case, `clean` should still reclaim the old versions of these accounts.
#[test]
fn test_clean_old_storages_with_reclaims_rooted() {
// Test is testing clean behaviour that is specific to obsolete accounts disabled
// Only run in obsolete accounts disabled mode
⋮----
// store `pubkey` into multiple slots, and also store another unique pubkey
// to prevent the whole storage from being marked as dead by `clean`.
⋮----
[(&pubkey, &account), (&Pubkey::new_unique(), &account)].as_slice(),
⋮----
accounts_db.uncleaned_pubkeys.remove(&slot);
// ensure this slot is *not* in the dirty_stores nor uncleaned_pubkeys, because we want to
// test cleaning *old* storages, i.e. when they aren't explicitly marked for cleaning
assert!(!accounts_db.dirty_stores.contains_key(&slot));
assert!(!accounts_db.uncleaned_pubkeys.contains_key(&slot));
⋮----
// add `old_slot` to the dirty stores list to mimic it being picked up as old
⋮----
.get_slot_storage_entry_shrinking_in_progress_ok(old_slot)
⋮----
accounts_db.dirty_stores.insert(old_slot, old_storage);
// ensure the slot list for `pubkey` has both the old and new slots
⋮----
.get_bin(&pubkey)
.slot_list_mut(&pubkey, |slot_list| slot_list.clone_list())
⋮----
assert_eq!(slot_list.len(), slots.len());
assert!(slot_list.iter().map(|(slot, _)| slot).eq(slots.iter()));
// `clean` should now reclaim the account in `old_slot`, even though `new_slot` is not
// explicitly being cleaned
⋮----
// ensure we've reclaimed the account in `old_slot`
⋮----
assert_eq!(slot_list.len(), 1);
assert!(slot_list
⋮----
/// Test that `clean` respects rooted vs unrooted slots w.r.t. reclaims
///
⋮----
///
/// When an account is in multiple slots, and the latest is unrooted, `clean` should *not* reclaim
⋮----
/// When an account is in multiple slots, and the latest is unrooted, `clean` should *not* reclaim
/// all the rooted versions.
⋮----
/// all the rooted versions.
#[test]
fn test_clean_old_storages_with_reclaims_unrooted() {
⋮----
// only `old_slot` should be rooted, not `new_slot`
accounts_db.add_root_and_flush_write_cache(old_slot);
assert!(accounts_db.accounts_index.is_alive_root(old_slot));
assert!(!accounts_db.accounts_index.is_alive_root(new_slot));
// ensure `old_slot` is in uncleaned_pubkeys (but not dirty_stores) so it'll be cleaned
assert!(accounts_db.uncleaned_pubkeys.contains_key(&old_slot));
assert!(!accounts_db.dirty_stores.contains_key(&old_slot));
// and `new_slot` should be in neither
assert!(!accounts_db.uncleaned_pubkeys.contains_key(&new_slot));
assert!(!accounts_db.dirty_stores.contains_key(&new_slot));
⋮----
// `clean` should *not* reclaim the account in `old_slot` because `new_slot` is not a root
⋮----
// ensure we have NOT reclaimed the account in `old_slot`
⋮----
/// Ensure the calculating capitalization produces the correct value
#[test]
fn test_calculate_capitalization_simple() {
⋮----
/// Ensure that calculating capitalization panics of there is an overflow
/// while summing balance within a single slot.
⋮----
/// while summing balance within a single slot.
#[test]
⋮----
fn test_calculate_capitalization_overflow_intra_slot() {
⋮----
accounts_db.store_for_tests((0, [(&Pubkey::new_unique(), &account)].as_slice()));
⋮----
accounts_db.calculate_capitalization_at_startup_from_index(&Ancestors::from(vec![0]), 0);
⋮----
fn test_calculate_capitalization_overflow_inter_slot() {
⋮----
accounts_db.store_for_tests((1, [(&Pubkey::new_unique(), &account)].as_slice()));
accounts_db.calculate_capitalization_at_startup_from_index(&Ancestors::from(vec![0, 1]), 1);
⋮----
fn test_mark_obsolete_accounts_at_startup_none() {
⋮----
let pubkeys_with_duplicates_by_bin = vec![];
⋮----
accounts_db.mark_obsolete_accounts_at_startup(slots, pubkeys_with_duplicates_by_bin);
⋮----
fn test_mark_obsolete_accounts_at_startup_purge_slot() {
⋮----
accounts_db.store_for_tests((0, [(&pubkey1, &account), (&pubkey2, &account)].as_slice()));
accounts_db.flush_accounts_cache_slot_for_tests(0);
accounts_db.store_for_tests((1, [(&pubkey1, &account)].as_slice()));
accounts_db.flush_accounts_cache_slot_for_tests(1);
accounts_db.store_for_tests((2, [(&pubkey1, &account)].as_slice()));
accounts_db.flush_accounts_cache_slot_for_tests(2);
let pubkeys_with_duplicates_by_bin = vec![vec![pubkey1]];
⋮----
assert!(accounts_db.storage.get_slot_storage_entry(0).is_some());
⋮----
accounts_db.assert_ref_count(&pubkey1, 1);
assert_eq!(obsolete_stats.accounts_marked_obsolete, 2);
⋮----
fn test_mark_obsolete_accounts_at_startup_multiple_bins() {
⋮----
[(&pubkey1, &account), (&pubkey2, &account)].as_slice(),
⋮----
accounts_db.flush_accounts_cache_slot_for_tests(slot);
⋮----
let pubkeys_with_duplicates_by_bin = vec![vec![pubkey1], vec![pubkey2]];
⋮----
accounts_db.mark_obsolete_accounts_at_startup(2, pubkeys_with_duplicates_by_bin);
⋮----
assert!(accounts_db.storage.get_slot_storage_entry(1).is_some());
⋮----
accounts_db.assert_ref_count(&pubkey2, 1);
⋮----
assert_eq!(obsolete_stats.slots_removed, 1);
⋮----
fn test_batch_insert_zero_lamport_single_ref_account_offsets() {
⋮----
let storage = accounts.create_and_insert_store(1, 100, "test");
let offsets1 = vec![10, 20, 30];
let count1 = storage.batch_insert_zero_lamport_single_ref_account_offsets(&offsets1);
assert_eq!(count1, 3, "Should insert all 3 new offsets");
assert_eq!(storage.num_zero_lamport_single_ref_accounts(), 3);
let offsets2 = vec![20, 30, 40, 50];
let count2 = storage.batch_insert_zero_lamport_single_ref_account_offsets(&offsets2);
assert_eq!(count2, 2, "Should insert only 2 new offsets (40, 50)");
assert_eq!(storage.num_zero_lamport_single_ref_accounts(), 5);
let offsets3 = vec![10, 20];
let count3 = storage.batch_insert_zero_lamport_single_ref_account_offsets(&offsets3);
assert_eq!(count3, 0, "Should not insert any duplicates");
⋮----
let empty_offsets: Vec<usize> = vec![];
let count4 = storage.batch_insert_zero_lamport_single_ref_account_offsets(&empty_offsets);
assert_eq!(count4, 0, "Should handle empty slice");
⋮----
let offsets5 = vec![10, 60, 20, 70, 30, 80, 40];
let count5 = storage.batch_insert_zero_lamport_single_ref_account_offsets(&offsets5);
assert_eq!(count5, 3, "Should insert only 3 new offsets (60, 70, 80)");
assert_eq!(storage.num_zero_lamport_single_ref_accounts(), 8);

================
File: accounts-db/src/accounts_index/account_map_entry.rs
================
pub struct AccountMapEntry<T> {
⋮----
const _: () = assert!(size_of::<AccountMapEntry<AccountInfo>>() == 48);
⋮----
pub fn new(slot_list: SlotList<T>, ref_count: RefCount, meta: AccountMapEntryMeta) -> Self {
⋮----
pub(super) fn empty_for_tests() -> Self {
⋮----
pub fn ref_count(&self) -> RefCount {
self.ref_count.load(Ordering::Acquire)
⋮----
pub fn addref(&self) {
let previous = self.ref_count.fetch_add(1, Ordering::Release);
assert_ne!(previous, RefCount::MAX);
self.set_dirty(true);
⋮----
pub fn unref(&self) -> RefCount {
self.unref_by_count(1)
⋮----
pub fn unref_by_count(&self, count: RefCount) -> RefCount {
let previous = self.ref_count.fetch_sub(count, Ordering::Release);
⋮----
assert!(
⋮----
pub fn dirty(&self) -> bool {
self.meta.dirty.load(Ordering::Acquire)
⋮----
pub fn set_dirty(&self, value: bool) {
self.meta.dirty.store(value, Ordering::Release)
⋮----
pub fn clear_dirty(&self) -> bool {
⋮----
.compare_exchange(true, false, Ordering::AcqRel, Ordering::Relaxed)
.is_ok()
⋮----
pub fn age(&self) -> Age {
self.meta.age.load(Ordering::Acquire)
⋮----
pub fn set_age(&self, value: Age) {
self.meta.age.store(value, Ordering::Release)
⋮----
pub fn try_exchange_age(&self, next_age: Age, expected_age: Age) {
let _ = self.meta.age.compare_exchange(
⋮----
pub fn slot_list_lock_read_len(&self) -> usize {
self.slot_list.read().unwrap().len()
⋮----
pub fn slot_list_read_lock(&self) -> SlotListReadGuard<'_, T> {
SlotListReadGuard(self.slot_list.read().unwrap())
⋮----
/// Acquire a write lock on the slot list and return accessor for modifying it
    ///
⋮----
///
    /// Do not call any locking function (`slot_list_*lock*`) on the same `AccountMapEntry` until accessor
⋮----
/// Do not call any locking function (`slot_list_*lock*`) on the same `AccountMapEntry` until accessor
    /// they return is dropped.
⋮----
/// they return is dropped.
    pub fn slot_list_write_lock(&self) -> SlotListWriteGuard<'_, T> {
⋮----
pub fn slot_list_write_lock(&self) -> SlotListWriteGuard<'_, T> {
SlotListWriteGuard(self.slot_list.write().unwrap())
⋮----
pub struct SlotListReadGuard<'a, T>(RwLockReadGuard<'a, SlotList<T>>);
impl<T> Deref for SlotListReadGuard<'_, T> {
type Target = [(Slot, T)];
fn deref(&self) -> &Self::Target {
self.0.as_slice()
⋮----
pub fn clone_list(&self) -> SlotList<T>
⋮----
self.0.iter().copied().collect()
⋮----
pub struct SlotListWriteGuard<'a, T>(RwLockWriteGuard<'a, SlotList<T>>);
⋮----
/// Append element to the end of slot list
    pub fn push(&mut self, item: (Slot, T)) {
⋮----
pub fn push(&mut self, item: (Slot, T)) {
self.0.push(item);
⋮----
/// Retains only the elements specified by the predicate.
    ///
⋮----
///
    /// Returns number of preserved elements (size of the slot list after processing).
⋮----
/// Returns number of preserved elements (size of the slot list after processing).
    pub fn retain_and_count<F>(&mut self, f: F) -> usize
⋮----
pub fn retain_and_count<F>(&mut self, f: F) -> usize
⋮----
self.0.retain(f);
self.0.len()
⋮----
/// Clears the list, removing all elements.
    #[cfg(test)]
pub fn clear(&mut self) {
self.0.clear();
⋮----
pub fn assign(&mut self, value: impl IntoIterator<Item = (Slot, T)>) {
*self.0 = value.into_iter().collect();
⋮----
impl<T> Deref for SlotListWriteGuard<'_, T> {
⋮----
pub struct AccountMapEntryMeta {
⋮----
impl AccountMapEntryMeta {
pub fn new_dirty<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>>(
⋮----
age: AtomicAge::new(storage.future_age_to_flush(is_cached)),
⋮----
pub fn new_clean<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>>(
⋮----
age: AtomicAge::new(storage.future_age_to_flush(false)),
⋮----
pub enum PreAllocatedAccountMapEntry<T: IndexValue> {
⋮----
impl<T: IndexValue> IsZeroLamport for PreAllocatedAccountMapEntry<T> {
fn is_zero_lamport(&self) -> bool {
⋮----
entry.slot_list_read_lock()[0].1.is_zero_lamport()
⋮----
PreAllocatedAccountMapEntry::Raw(raw) => raw.1.is_zero_lamport(),
⋮----
fn from(source: PreAllocatedAccountMapEntry<T>) -> (Slot, T) {
⋮----
PreAllocatedAccountMapEntry::Entry(entry) => entry.slot_list_read_lock()[0],
⋮----
pub fn new<U: DiskIndexValue + From<T> + Into<T>>(
⋮----
fn allocate<U: DiskIndexValue + From<T> + Into<T>>(
⋮----
let is_cached = account_info.is_cached();
⋮----
pub fn into_account_map_entry<U: DiskIndexValue + From<T> + Into<T>>(

================
File: accounts-db/src/accounts_index/accounts_index_storage.rs
================
pub struct AccountsIndexStorage<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> {
⋮----
impl<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> Debug for AccountsIndexStorage<T, U> {
fn fmt(&self, _f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
Ok(())
⋮----
/// low-level managing the bg threads
struct BgThreads {
⋮----
struct BgThreads {
⋮----
impl Drop for BgThreads {
fn drop(&mut self) {
self.exit.store(true, Ordering::Relaxed);
self.wait.notify_all();
if let Some(handles) = self.handles.take() {
⋮----
.into_iter()
.for_each(|handle| handle.join().unwrap());
⋮----
impl BgThreads {
fn new<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>>(
⋮----
let is_disk_index_enabled = storage.is_disk_index_enabled();
⋮----
threads.get()
⋮----
// no disk index, so only need 1 thread to report stats
⋮----
// stop signal used for THIS batch of bg threads
⋮----
let handles = Some(
⋮----
.map(|idx| {
// the first thread we start is special
⋮----
let local_exit = local_exit.clone();
let system_exit = exit.clone();
let in_mem_ = in_mem.to_vec();
// note that using rayon here causes us to exhaust # rayon threads and many tests running in parallel deadlock
⋮----
.name(format!("solIdxFlusher{idx:02}"))
.spawn(move || {
storage_.background(
vec![local_exit, system_exit],
⋮----
.unwrap()
⋮----
.collect(),
⋮----
/// startup=true causes:
    ///      in mem to act in a way that flushes to disk asap
⋮----
///      in mem to act in a way that flushes to disk asap
    /// startup=false is 'normal' operation
⋮----
/// startup=false is 'normal' operation
    pub(crate) fn set_startup(&self, startup: Startup) {
⋮----
pub(crate) fn set_startup(&self, startup: Startup) {
⋮----
self.storage.set_startup(is_startup);
⋮----
pub fn get_startup_remaining_items_to_flush_estimate(&self) -> usize {
⋮----
.as_ref()
.map(|_| self.storage.stats.get_remaining_items_to_flush_estimate())
.unwrap_or_default()
⋮----
pub fn new(bins: usize, config: &AccountsIndexConfig, exit: Arc<AtomicBool>) -> Self {
⋮----
.unwrap_or_else(accounts_index::default_num_flush_threads);
let storage = Arc::new(BucketMapHolder::new(bins, config, num_flush_threads.get()));
⋮----
.map(|bin| Arc::new(InMemAccountsIndex::new(&storage, bin, num_initial_accounts)))
.collect();

================
File: accounts-db/src/accounts_index/bucket_map_holder.rs
================
pub type Age = u8;
pub type AtomicAge = AtomicU8;
const _: () = assert!(std::mem::size_of::<Age>() == std::mem::size_of::<AtomicAge>());
⋮----
pub struct BucketMapHolder<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> {
⋮----
impl<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> Debug for BucketMapHolder<T, U> {
fn fmt(&self, _f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
Ok(())
⋮----
/// is the accounts index using disk as a backing store
    pub fn is_disk_index_enabled(&self) -> bool {
⋮----
pub fn is_disk_index_enabled(&self) -> bool {
self.disk.is_some()
⋮----
pub fn increment_age(&self) {
// since we are about to change age, there are now 0 buckets that have been flushed at this age
// this should happen before the age.fetch_add
// Otherwise, as soon as we increment the age, a thread could race us and flush before we swap this out since it detects the age has moved forward and a bucket will be eligible for flushing.
let previous = self.count_buckets_flushed.swap(0, Ordering::AcqRel);
// fetch_add is defined to wrap.
// That's what we want. 0..255, then back to 0.
self.age.fetch_add(1, Ordering::Release);
self.future_age_to_flush.fetch_add(1, Ordering::Release);
⋮----
.fetch_add(1, Ordering::Release);
assert!(
⋮----
self.wait_dirty_or_aged.notify_all();
⋮----
pub fn future_age_to_flush(&self, is_cached: bool) -> Age {
⋮----
.load(Ordering::Acquire)
⋮----
fn has_age_interval_elapsed(&self) -> bool {
self.age_timer.should_update(self.age_interval_ms())
⋮----
pub fn get_startup(&self) -> bool {
self.startup.load(Ordering::Relaxed)
⋮----
pub fn set_startup(&self, value: bool) {
⋮----
self.wait_for_idle();
⋮----
self.startup.store(value, Ordering::Relaxed)
⋮----
pub fn wait_for_idle(&self) {
assert!(self.get_startup());
if !self.is_disk_index_enabled() {
⋮----
let start_age = self.current_age();
⋮----
.wait_timeout(Duration::from_millis(self.age_interval_ms()));
if self.current_age().wrapping_sub(start_age) > 1 {
⋮----
pub fn current_age(&self) -> Age {
self.age.load(Ordering::Acquire)
⋮----
pub fn bucket_flushed_at_current_age(&self, can_advance_age: bool) {
let count_buckets_flushed = 1 + self.count_buckets_flushed.fetch_add(1, Ordering::AcqRel);
⋮----
self.maybe_advance_age_internal(
self.all_buckets_flushed_at_current_age_internal(count_buckets_flushed),
⋮----
pub fn all_buckets_flushed_at_current_age(&self) -> bool {
self.all_buckets_flushed_at_current_age_internal(self.count_buckets_flushed())
⋮----
fn all_buckets_flushed_at_current_age_internal(&self, count_buckets_flushed: usize) -> bool {
⋮----
pub fn count_buckets_flushed(&self) -> usize {
self.count_buckets_flushed.load(Ordering::Acquire)
⋮----
pub fn maybe_advance_age(&self) -> bool {
self.maybe_advance_age_internal(self.all_buckets_flushed_at_current_age())
⋮----
fn maybe_advance_age_internal(&self, all_buckets_flushed_at_current_age: bool) -> bool {
if all_buckets_flushed_at_current_age && self.has_age_interval_elapsed() {
self.increment_age();
⋮----
pub fn new(bins: usize, config: &AccountsIndexConfig, threads: usize) -> Self {
⋮----
.unwrap_or(DEFAULT_AGE_TO_STAY_IN_CACHE);
⋮----
bucket_config.drives = config.drives.as_ref().cloned();
⋮----
.as_ref()
.and_then(|drives| drives.first())
.map(|drive| drive.join("accounts_index_restart"));
⋮----
IndexLimit::Minimal => Some(BucketMap::new(bucket_config)),
⋮----
pub fn next_bucket_to_flush(&self) -> usize {
⋮----
.fetch_update(Ordering::AcqRel, Ordering::Acquire, |bucket| {
Some((bucket + 1) % self.bins)
⋮----
.unwrap()
⋮----
fn age_interval_ms(&self) -> u64 {
⋮----
fn throttling_wait_ms_internal(
⋮----
let remaining_ms = (interval_ms * target_percent / 100).saturating_sub(elapsed_ms);
let remaining_bins = (self.bins as u64).saturating_sub(bins_flushed);
⋮----
Some(1)
⋮----
fn throttling_wait_ms(&self) -> Option<u64> {
let interval_ms = self.age_interval_ms();
let elapsed_ms = self.age_timer.elapsed_ms();
let bins_flushed = self.count_buckets_flushed() as u64;
self.throttling_wait_ms_internal(interval_ms, elapsed_ms, bins_flushed)
⋮----
fn should_thread_sleep(&self) -> bool {
let bins_flushed = self.count_buckets_flushed();
⋮----
let active = self.stats.active_threads.load(Ordering::Relaxed);
bins_flushed.saturating_add(active as usize) >= self.bins
⋮----
pub fn background(
⋮----
let bins = in_mem.len();
let flush = self.is_disk_index_enabled();
⋮----
self.wait_dirty_or_aged.wait_timeout(Duration::from_millis(
self.stats.remaining_until_next_interval(),
⋮----
m.stop();
⋮----
.fetch_add(m.as_us(), Ordering::Relaxed);
} else if self.should_thread_sleep() || throttling_wait_ms.is_some() {
⋮----
.remaining_until_next_interval(self.age_interval_ms()),
⋮----
wait = wait.max(1);
⋮----
.fetch_add(throttling_wait_ms * 1000, Ordering::Relaxed);
⋮----
.wait_timeout(Duration::from_millis(wait));
⋮----
self.maybe_advance_age();
⋮----
if exit.iter().any(|exit| exit.load(Ordering::Relaxed)) {
⋮----
self.stats.active_threads.fetch_add(1, Ordering::Relaxed);
⋮----
let index = self.next_bucket_to_flush();
in_mem[index].flush(can_advance_age);
⋮----
self.stats.report_stats(self);
if self.all_buckets_flushed_at_current_age() {
⋮----
throttling_wait_ms = self.throttling_wait_ms();
if throttling_wait_ms.is_some() {
⋮----
self.stats.active_threads.fetch_sub(1, Ordering::Relaxed);
⋮----
pub mod tests {
⋮----
fn test_next_bucket_to_flush() {
⋮----
.map(|_| AtomicUsize::default())
⋮----
(0..threads).into_par_iter().for_each(|_| {
(0..iterations).for_each(|_| {
let bin = test.next_bucket_to_flush();
visited[bin].fetch_add(1, Ordering::Relaxed);
⋮----
visited.iter().enumerate().for_each(|(bin, visited)| {
assert_eq!(visited.load(Ordering::Relaxed), expected, "bin: {bin}")
⋮----
fn test_ages() {
⋮----
assert_eq!(0, test.current_age());
assert_eq!(test.ages_to_stay_in_cache, test.future_age_to_flush(false));
assert_eq!(Age::MAX, test.future_age_to_flush(true));
(0..bins).for_each(|_| {
test.bucket_flushed_at_current_age(false);
⋮----
test.increment_age();
assert_eq!(1, test.current_age());
assert_eq!(
⋮----
assert_eq!(0, test.future_age_to_flush(true));
⋮----
fn test_age_increment() {
⋮----
assert_eq!(test.current_age(), (age % 256) as Age);
⋮----
assert!(!test.all_buckets_flushed_at_current_age());
⋮----
.fetch_add(bins, Ordering::Release);
⋮----
fn test_throttle() {
⋮----
let interval_ms = test.age_interval_ms();
⋮----
let result = test.throttling_wait_ms_internal(interval_ms, elapsed_ms, bins_flushed);
assert_eq!(result, None);
⋮----
assert_eq!(result, Some(1));
⋮----
fn test_disk_index_enabled() {
⋮----
assert!(test.is_disk_index_enabled());
⋮----
fn test_age_time() {
⋮----
test.bucket_flushed_at_current_age(true);
⋮----
while now.elapsed().as_millis() < (time as u128) * 100 {
if test.maybe_advance_age() {
⋮----
if test.current_age() >= expected {
⋮----
fn test_age_broad() {
⋮----
assert_eq!(test.current_age(), 0);
⋮----
test.maybe_advance_age();
assert_eq!(test.current_age(), 1);

================
File: accounts-db/src/accounts_index/in_mem_accounts_index.rs
================
pub struct StartupStats {
⋮----
pub struct InMemAccountsIndex<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> {
⋮----
impl<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> Debug for InMemAccountsIndex<T, U> {
fn fmt(&self, _f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
Ok(())
⋮----
/// An entry was inserted into the index; did it already exist in the index?
#[derive(Debug)]
pub enum InsertNewEntryResults {
⋮----
/// An entry was inserted into the index that previously existed; where did it previously exist?
#[derive(Debug)]
pub enum ExistedLocation {
⋮----
struct StartupInfoDuplicates<T: IndexValue> {
/// entries that were found to have duplicate index entries.
    /// When all entries have been inserted, these can be resolved and held in memory.
⋮----
/// When all entries have been inserted, these can be resolved and held in memory.
    duplicates: Vec<(Slot, Pubkey, T)>,
/// pubkeys that were already added to disk and later found to be duplicates,
    duplicates_put_on_disk: HashSet<(Slot, Pubkey)>,
/// (slot, pubkey) pairs that are found to be duplicates when we are
    /// starting from in-memory only index. This field is used only when disk
⋮----
/// starting from in-memory only index. This field is used only when disk
    /// index is disabled.
⋮----
/// index is disabled.
    duplicates_from_in_memory_only: Vec<(Slot, Pubkey)>,
⋮----
struct StartupInfo<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> {
/// entries to add next time we are flushing to disk
    insert: Mutex<Vec<(Pubkey, (Slot, U))>>,
/// pubkeys with more than 1 entry
    duplicates: Mutex<StartupInfoDuplicates<T>>,
⋮----
pub fn new(
⋮----
let lowest_pubkey = bin_calc.lowest_pubkey_from_bin(bin);
let highest_pubkey = bin_calc.highest_pubkey_from_bin(bin);
⋮----
.as_ref()
.map(|disk| disk.get_bucket_from_index(bin))
.cloned(),
⋮----
// initialize this to max, to make it clear we have not flushed at age 0, the starting age
⋮----
// Spread out the scanning across all ages within the window.
// This causes us to scan 1/N of the bins each 'Age'
⋮----
rng().random_range(0..num_ages_to_distribute_flushes),
⋮----
fn get_should_age(&self, age: Age) -> bool {
let last_age_flushed = self.last_age_flushed();
⋮----
fn set_has_aged(&self, age: Age, can_advance_age: bool) {
self.last_age_flushed.store(age, Ordering::Release);
self.storage.bucket_flushed_at_current_age(can_advance_age);
⋮----
fn last_age_flushed(&self) -> Age {
self.last_age_flushed.load(Ordering::Acquire)
⋮----
pub fn keys(&self) -> Vec<Pubkey> {
Self::update_stat(&self.stats().keys, 1);
let mut keys: HashSet<_> = self.map_internal.read().unwrap().keys().cloned().collect();
if let Some(disk) = self.bucket.as_ref() {
let disk_keys = disk.keys();
keys.reserve(disk_keys.len());
⋮----
keys.insert(key);
⋮----
keys.into_iter().collect()
⋮----
fn load_from_disk(&self, pubkey: &Pubkey) -> Option<(SlotList<U>, RefCount)> {
self.bucket.as_ref().and_then(|disk| {
⋮----
let entry_disk = disk.read_value(pubkey);
⋮----
Self::update_time_stat(&self.stats().load_disk_found_us, m);
Self::update_stat(&self.stats().load_disk_found_count, 1);
⋮----
Self::update_time_stat(&self.stats().load_disk_missing_us, m);
Self::update_stat(&self.stats().load_disk_missing_count, 1);
⋮----
entry_disk.map(|(slot_list, ref_count)| {
⋮----
fn load_account_entry_from_disk(&self, pubkey: &Pubkey) -> Option<AccountMapEntry<T>> {
let entry_disk = self.load_from_disk(pubkey)?;
let entry_cache = self.disk_to_cache_entry(entry_disk.0, entry_disk.1);
debug_assert!(!entry_cache.dirty());
Some(entry_cache)
⋮----
pub(super) fn get_only_in_mem<RT>(
⋮----
let map = self.map_internal.read().unwrap();
let result = map.get(pubkey);
m.stop();
callback(if let Some(entry) = result {
⋮----
self.set_age_to_future(entry, false);
⋮----
Some(entry)
⋮----
drop(map);
⋮----
let stats = self.stats();
⋮----
Self::update_stat(time, m.as_us());
⋮----
fn set_age_to_future(&self, entry: &AccountMapEntry<T>, is_cached: bool) {
entry.set_age(self.storage.future_age_to_flush(is_cached));
⋮----
pub(super) fn get_internal_inner<RT>(
⋮----
// SAFETY: The entry Arc is not passed to `callback`, so
// it cannot live beyond this function call.
self.get_only_in_mem(pubkey, true, |entry| {
⋮----
callback(Some(entry)).1
⋮----
// not in cache, look on disk
⋮----
let disk_entry = self.load_account_entry_from_disk(pubkey);
if disk_entry.is_none() {
return callback(None).1;
⋮----
let disk_entry = disk_entry.unwrap();
let mut map = self.map_internal.write().unwrap();
let capacity_pre = map.capacity();
let entry = map.entry(*pubkey);
⋮----
Entry::Occupied(occupied) => callback(Some(occupied.get())).1,
⋮----
debug_assert!(!disk_entry.dirty());
let (add_to_cache, rt) = callback(Some(&disk_entry));
// We are holding a write lock to the in-memory map.
// This pubkey is not in the in-memory map.
// If the entry is now dirty, then it must be put in the cache or the modifications will be lost.
if add_to_cache || disk_entry.dirty() {
stats.inc_mem_count();
vacant.insert(Box::new(disk_entry));
⋮----
let capacity_post = map.capacity();
⋮----
stats.update_in_mem_capacity(capacity_pre, capacity_post);
⋮----
fn remove_if_slot_list_empty_value(&self, is_empty: bool) -> bool {
⋮----
self.stats().inc_delete();
⋮----
fn delete_disk_key(&self, pubkey: &Pubkey) {
⋮----
disk.delete_key(pubkey)
⋮----
/// return false if the entry is in the index (disk or memory) and has a slot list len > 0
    /// return true in all other cases, including if the entry is NOT in the index at all
⋮----
/// return true in all other cases, including if the entry is NOT in the index at all
    fn remove_if_slot_list_empty_entry(
⋮----
fn remove_if_slot_list_empty_entry(
⋮----
.remove_if_slot_list_empty_value(occupied.get().slot_list_lock_read_len() == 0);
⋮----
// note there is a potential race here that has existed.
// if someone else holds the arc,
//  then they think the item is still in the index and can make modifications.
// We have to have a write lock to the map here, which means nobody else can get
//  the arc, but someone may already have retrieved a clone of it.
// account index in_mem flushing is one such possibility
self.delete_disk_key(occupied.key());
self.stats().dec_mem_count();
occupied.remove();
⋮----
let entry_disk = self.load_from_disk(vacant.key());
⋮----
// on disk
if self.remove_if_slot_list_empty_value(entry_disk.0.is_empty()) {
// not in cache, but on disk, so just delete from disk
self.delete_disk_key(vacant.key());
⋮----
// could insert into cache here, but not required for correctness and value is unclear
⋮----
None => true, // not in cache or on disk, but slot list is 'empty' and entry is not in index, so return true
⋮----
pub fn remove_if_slot_list_empty(&self, pubkey: Pubkey) -> bool {
⋮----
let entry = map.entry(pubkey);
⋮----
let found = matches!(entry, Entry::Occupied(_));
let result = self.remove_if_slot_list_empty_entry(entry);
⋮----
self.stats()
.update_in_mem_capacity(capacity_pre, capacity_post);
self.update_entry_stats(m, found);
⋮----
pub fn slot_list_mut<RT>(
⋮----
self.get_internal_inner(pubkey, |entry| {
⋮----
entry.map(|entry| {
let result = user_fn(entry.slot_list_write_lock());
entry.set_dirty(true);
⋮----
fn cache_entry_at_slot(current: &AccountMapEntry<T>, new_value: (Slot, T)) {
let mut slot_list = current.slot_list_write_lock();
⋮----
.iter()
.any(|(existing_slot, _)| *existing_slot == slot)
⋮----
slot_list.push((slot, new_entry));
⋮----
current.set_dirty(true);
⋮----
pub fn upsert(
⋮----
let (slot, account_info) = new_value.into();
let is_cached = account_info.is_cached();
self.get_or_create_index_entry_for_pubkey(pubkey, |entry| {
⋮----
self.set_age_to_future(entry, true);
⋮----
self.set_age_to_future(entry, slot_list_length > 1);
⋮----
pub fn get_or_create_index_entry_for_pubkey(
⋮----
self.get_only_in_mem(pubkey, false, |entry| {
⋮----
callback(entry);
⋮----
let current = occupied.get_mut();
callback(current);
⋮----
let disk_entry = self.load_account_entry_from_disk(vacant.key());
⋮----
self.stats().inc_insert();
⋮----
callback(&new_value);
assert_ne!(
⋮----
assert!(new_value.dirty());
vacant.insert(Box::new(new_value));
⋮----
Self::update_stat(&self.stats().updates_in_mem, 1);
⋮----
fn update_entry_stats(&self, stopped_measure: Measure, found: bool) {
⋮----
Self::update_stat(time, stopped_measure.as_us());
⋮----
fn lock_and_update_slot_list(
⋮----
match ref_count_change.cmp(&0) {
⋮----
assert_eq!(ref_count_change, 1);
current.addref();
⋮----
current.unref_by_count(ref_count_change.unsigned_abs());
⋮----
fn update_slot_list(
⋮----
assert!(!account_info.is_cached());
let old_slot = other_slot.unwrap_or(slot);
⋮----
let mut final_len = slot_list.retain_and_count(|cur_item| {
⋮----
assert!(!found_slot);
let is_cur_account_cached = cur_account_info.is_cached();
⋮----
reclaims.push(reclaim_item);
⋮----
assert!(is_cur_account_cached);
⋮----
reclaims.push(*cur_item);
⋮----
slot_list.push((slot, account_info));
⋮----
fn disk_to_cache_entry(
⋮----
.into_iter()
.map(|(slot, info)| (slot, info.into()))
.collect(),
⋮----
pub fn startup_insert_only(
⋮----
assert!(self.storage.get_startup());
assert!(self.bucket.is_some());
let mut insert = self.startup_info.insert.lock().unwrap();
⋮----
insert.extend(items.map(|(k, v)| (k, (slot, v.into()))));
⋮----
.fetch_add(m.end_as_us(), Ordering::Relaxed);
⋮----
pub fn startup_update_duplicates_from_in_memory_only(&self, items: Vec<(Slot, Pubkey)>) {
⋮----
assert!(self.bucket.is_none());
let mut duplicates = self.startup_info.duplicates.lock().unwrap();
duplicates.duplicates_from_in_memory_only.extend(items);
⋮----
pub fn insert_new_entry_if_missing_with_lock(
⋮----
let (slot, account_info) = new_entry.into();
let slot_list = occupied.get().slot_list_read_lock();
if slot_list.len() == 1 {
other_slot = Some(slot_list[0].0);
⋮----
drop(slot_list);
⋮----
occupied.get(),
⋮----
let new_entry = new_entry.into_account_map_entry(&self.storage);
assert!(new_entry.dirty());
vacant.insert(new_entry);
⋮----
pub fn flush(&self, can_advance_age: bool) {
⋮----
self.flush_internal(&flush_guard, can_advance_age)
⋮----
pub const fn size_of_uninitialized() -> usize {
⋮----
pub const fn size_of_single_entry() -> usize {
⋮----
fn should_evict_based_on_age(
⋮----
current_age.wrapping_sub(entry.age()) <= ages_flushing_now
⋮----
fn try_make_entry_for_flush(
⋮----
if entry.ref_count() != 1 {
⋮----
let was_dirty = entry.clear_dirty();
⋮----
let slot_list = entry.slot_list_read_lock();
⋮----
if slot_list.len() != 1 {
⋮----
if slot_list_elem.1.is_cached() {
⋮----
fn gather_possible_evictions<'a>(
⋮----
// not planning to evict this item from memory within 'ages_flushing_now' ages
⋮----
if v.ref_count() != 1 {
⋮----
if v.dirty() {
candidates_to_flush.push(*k);
⋮----
candidates_to_evict.push(*k);
⋮----
CandidatesToFlush(candidates_to_flush),
CandidatesToEvict(candidates_to_evict),
⋮----
fn flush_scan(
⋮----
Self::gather_possible_evictions(map.iter(), current_age, ages_flushing_now);
⋮----
Self::update_time_stat(&self.stats().flush_scan_us, m);
⋮----
fn write_startup_info_to_disk(&self) {
let insert = std::mem::take(&mut *self.startup_info.insert.lock().unwrap());
if insert.is_empty() {
⋮----
let map_internal = self.map_internal.read().unwrap();
assert!(
⋮----
drop(map_internal);
⋮----
let disk = self.bucket.as_ref().unwrap();
let mut count = insert.len() as u64;
for (i, duplicate_entry) in disk.batch_insert_non_duplicates(&insert) {
⋮----
duplicates.duplicates.push((entry.0, *k, entry.1.into()));
⋮----
.insert((duplicate_entry.0, *k));
⋮----
self.stats().inc_insert_count(count);
⋮----
pub fn populate_and_retrieve_duplicate_keys_from_startup(&self) -> Vec<(Slot, Pubkey)> {
assert!(self.startup_info.insert.lock().unwrap().is_empty());
let mut duplicate_items = self.startup_info.duplicates.lock().unwrap();
⋮----
drop(duplicate_items);
⋮----
let storage = self.storage.as_ref();
⋮----
.chain(duplicates.into_iter().map(|(slot, key, info)| {
⋮----
match self.insert_new_entry_if_missing_with_lock(key, entry) {
⋮----
.collect();
⋮----
stats.inc_insert_count(num_did_not_exist);
stats.add_mem_count(num_did_not_exist as usize);
⋮----
.fetch_add(num_existed_in_mem, Ordering::Relaxed);
⋮----
stats.add_mem_count(num_existed_on_disk as usize);
⋮----
.fetch_add(num_existed_on_disk, Ordering::Relaxed);
⋮----
pub fn startup_take_duplicates_from_in_memory_only(&self) -> Vec<(Slot, Pubkey)> {
⋮----
fn flush_internal(&self, flush_guard: &FlushGuard, can_advance_age: bool) {
let current_age = self.storage.current_age();
let iterate_for_age = self.get_should_age(current_age);
let startup = self.storage.get_startup();
⋮----
self.write_startup_info_to_disk();
⋮----
assert_eq!(current_age, self.storage.current_age());
self.set_has_aged(current_age, can_advance_age);
⋮----
debug_assert!(!startup);
⋮----
debug_assert!(iterate_for_age);
⋮----
.fetch_sub(1, Ordering::AcqRel);
⋮----
.store(self.num_ages_to_distribute_flushes, Ordering::Release);
⋮----
Self::update_stat(&self.stats().buckets_scanned, 1);
⋮----
self.flush_scan(current_age, flush_guard, ages_flushing_now);
⋮----
.filter_map(|key| {
⋮----
let map_read_guard = self.map_internal.read().unwrap();
let entry = map_read_guard.get(&key)?;
⋮----
self.try_make_entry_for_flush(entry, current_age, ages_flushing_now);
flush_stats.flush_should_evict_us += mse.end_as_us();
drop(map_read_guard);
flush_stats.flush_read_lock_us += lock_measure.end_as_us();
⋮----
let disk_entry = [(slot, account_info.into())];
⋮----
let disk_resize = disk.try_write(&key, (&disk_entry,  1));
⋮----
disk.grow(err);
flush_stats.flush_grow_us += m.end_as_us();
⋮----
Some(key)
⋮----
flush_stats.flush_update_us = flush_update_measure.end_as_us();
flush_stats.update_to_stats(self.stats());
⋮----
self.evict_from_cache(&flushed_keys_to_evict, current_age, ages_flushing_now);
self.evict_from_cache(&candidates_to_evict.0, current_age, ages_flushing_now);
Self::update_time_stat(&self.stats().flush_evict_us, m);
⋮----
fn evict_from_cache(&self, evictions: &[Pubkey], current_age: Age, ages_flushing_now: Age) {
if evictions.is_empty() {
⋮----
for evictions in evictions.chunks(50) {
⋮----
if let Entry::Occupied(occupied) = map.entry(*k) {
let v = occupied.get();
if v.dirty()
⋮----
stats.sub_mem_count(evicted);
⋮----
pub fn stats(&self) -> &Stats {
⋮----
fn update_stat(stat: &AtomicU64, value: u64) {
⋮----
stat.fetch_add(value, Ordering::Relaxed);
⋮----
pub fn update_time_stat(stat: &AtomicU64, mut m: Measure) {
⋮----
let value = m.as_us();
⋮----
pub(crate) fn capacity_for_startup(&self) -> usize {
self.map_internal.read().unwrap().capacity()
⋮----
struct DiskFlushStats {
⋮----
impl DiskFlushStats {
fn new() -> Self {
⋮----
fn update_to_stats(&self, stats: &Stats) {
⋮----
struct FlushGuard<'a> {
⋮----
fn lock(flushing: &'a AtomicBool) -> Option<Self> {
let already_flushing = flushing.swap(true, Ordering::AcqRel);
// Eager evaluation here would result in dropping Self and clearing flushing flag
⋮----
(!already_flushing).then(|| Self { flushing })
⋮----
impl Drop for FlushGuard<'_> {
fn drop(&mut self) {
self.flushing.store(false, Ordering::Release);
⋮----
struct CandidatesToFlush(Vec<Pubkey>);
⋮----
struct CandidatesToEvict(Vec<Pubkey>);
⋮----
enum ShouldFlush<T> {
⋮----
enum ReasonToNotFlush {
⋮----
mod tests {
⋮----
fn new_for_test<T: IndexValue>() -> InMemAccountsIndex<T, T> {
⋮----
fn new_disk_buckets_for_test<T: IndexValue>() -> InMemAccountsIndex<T, T> {
⋮----
assert!(bucket.storage.is_disk_index_enabled());
⋮----
fn test_get_or_create_index_entry_for_pubkey_insert_new() {
⋮----
accounts_index.get_or_create_index_entry_for_pubkey(&pubkey, |entry| {
assert_eq!(entry.slot_list_lock_read_len(), 0);
assert_eq!(entry.ref_count(), 0);
assert!(entry.dirty());
⋮----
assert!(callback_called);
⋮----
accounts_index.get_only_in_mem(&pubkey, false, |entry| {
found = entry.is_some();
⋮----
assert!(found);
⋮----
fn test_get_or_create_index_entry_for_pubkey_existing_in_mem() {
⋮----
.write()
.unwrap()
.insert(pubkey, entry);
⋮----
assert_eq!(entry.slot_list_lock_read_len(), 1);
assert_eq!(entry.ref_count(), 1);
⋮----
fn test_get_or_create_index_entry_for_pubkey_existing_on_disk() {
⋮----
.try_write(&pubkey, disk_entry)
.unwrap();
⋮----
assert!(!found);
⋮----
assert!(!entry.dirty());
⋮----
fn test_get_or_create_index_entry_for_pubkey_empty_slot_list_assertion() {
⋮----
accounts_index.get_or_create_index_entry_for_pubkey(&pubkey, |_entry| {
⋮----
fn test_update_slot_list_other_populate_reclaims() {
⋮----
for other_slot in [Some(new_slot), Some(unique_other_slot), None] {
⋮----
let mut slot_list = entry.slot_list_write_lock();
assert_eq!(
⋮----
assert_eq!(slot_list.clone_list(), SlotList::from([at_new_slot]));
assert!(reclaims.is_empty());
⋮----
let expected_reclaims = ReclaimsSlotList::from(slot_list.as_ref());
let other_slot = Some(unique_other_slot);
⋮----
assert_eq!(reclaims, expected_reclaims);
⋮----
.extend((0..3).map(|i| (ignored_slot + i, ignored_value + i)));
possible_initial_slot_list_contents.push(at_new_slot);
possible_initial_slot_list_contents.push((unique_other_slot, other_value));
⋮----
for initial_slot_list_len in 0..=possible_initial_slot_list_contents.len() {
⋮----
(0..possible_initial_slot_list_contents.len()).permutations(initial_slot_list_len)
⋮----
Some(new_slot),
Some(unique_other_slot),
Some(missing_other_slot),
⋮----
if other_slot.is_some()
&& new_slot != other_slot.unwrap()
&& slot_list.contains(&(new_slot, info))
⋮----
.map(|i| possible_initial_slot_list_contents[*i])
⋮----
let mut expected = slot_list.clone_list();
let original = slot_list.clone_list();
⋮----
let mut slot_list = slot_list.clone_list();
⋮----
expected.retain(|(slot, info)| {
let retain = slot != &new_slot && Some(*slot) != other_slot;
⋮----
expected_reclaims.push((*slot, *info));
⋮----
expected.push((new_slot, info));
let expected_result = 1 - expected_reclaims.len() as i32;
⋮----
expected_reclaims.sort_unstable();
reclaims.sort_unstable();
⋮----
slot_list.sort_unstable();
expected.sort_unstable();
⋮----
assert_eq!(attempts, 652);
⋮----
fn test_gather_possible_evictions() {
⋮----
.map(|age| {
⋮----
entry.set_age(age);
⋮----
entry.set_dirty(false);
⋮----
map_dirty.iter().chain(&map_clean),
⋮----
assert_eq!(candidates_to_flush.0.len(), 1 + ages_flushing_now as usize);
assert_eq!(candidates_to_evict.0.len(), 1 + ages_flushing_now as usize);
candidates_to_flush.0.iter().for_each(|key| {
let entry = map_dirty.get(key).unwrap();
⋮----
assert_eq!(*entry.slot_list_read_lock(), slot_list_dirty);
⋮----
candidates_to_evict.0.iter().for_each(|key| {
let entry = map_clean.get(key).unwrap();
⋮----
assert_eq!(*entry.slot_list_read_lock(), slot_list_clean);
⋮----
fn test_try_make_entry_for_flush() {
⋮----
let entry_for_flush = bucket.try_make_entry_for_flush(&entry, 0, 0);
assert_eq!(entry_for_flush, ShouldFlush::Yes((slot, account_info)));
⋮----
assert_eq!(entry_for_flush, ShouldFlush::No(ReasonToNotFlush::Clean),);
⋮----
let entry_for_flush = bucket.try_make_entry_for_flush(&entry, 1, 0);
assert_eq!(entry_for_flush, ShouldFlush::No(ReasonToNotFlush::Age),);
⋮----
assert_eq!(entry_for_flush, ShouldFlush::No(ReasonToNotFlush::RefCount),);
⋮----
fn test_age() {
⋮----
assert!(test.get_should_age(test.storage.current_age()));
assert_eq!(test.storage.count_buckets_flushed(), 0);
test.set_has_aged(0, true);
assert!(!test.get_should_age(test.storage.current_age()));
assert_eq!(test.storage.count_buckets_flushed(), 1);
⋮----
assert!(!test.storage.all_buckets_flushed_at_current_age());
test.storage.bucket_flushed_at_current_age(true);
⋮----
assert!(test.storage.all_buckets_flushed_at_current_age());
test.storage.increment_age();
assert_eq!(test.storage.current_age(), 1);
⋮----
fn test_update_slot_list_other_reclaim_old_slots() {
⋮----
.extend((0..3).map(|i| (reclaimed_slot + i, reclaimed_value + i)));
⋮----
assert_eq!(attempts, 219202);
⋮----
fn test_update_slot_list_new_slot_duplicate_panic(slot_to_replace: u64) {
⋮----
Some(slot_to_replace),
⋮----
fn test_flush_guard() {
⋮----
assert!(flush_guard.is_some());
assert!(flushing_active.load(Ordering::Acquire));
⋮----
assert!(flush_guard2.is_none());
⋮----
assert!(!flushing_active.load(Ordering::Acquire));
⋮----
fn test_remove_if_slot_list_empty_entry() {
⋮----
let mut map = test.map_internal.write().unwrap();
⋮----
let entry = map.entry(unknown_key);
assert_matches!(entry, Entry::Vacant(_));
⋮----
assert!(test.remove_if_slot_list_empty_entry(entry));
⋮----
map.insert(key, val);
let entry = map.entry(key);
assert_matches!(entry, Entry::Occupied(_));
⋮----
val.slot_list_write_lock().push((1, 1));
⋮----
assert!(!test.remove_if_slot_list_empty_entry(entry));
⋮----
fn test_lock_and_update_slot_list() {
⋮----
assert_eq!(test.slot_list_lock_read_len(), len);
assert_eq!(len, 1);
⋮----
assert_eq!(len, 2);
⋮----
fn test_new_with_num_initial_accounts(num_initial_accounts: Option<usize>) {
⋮----
total_capacity += accounts_index.map_internal.read().unwrap().capacity();
⋮----
assert!(total_capacity > num_initial_accounts);
⋮----
assert_eq!(total_capacity, 0);

================
File: accounts-db/src/accounts_index/iter.rs
================
pub struct AccountsIndexPubkeyIterator<'a, T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> {
⋮----
pub fn new<R>(
⋮----
let (start_bin, end_bin_inclusive) = index.bin_start_end_inclusive(range);
⋮----
start_bound: range.start_bound(),
end_bound: range.end_bound(),
⋮----
end_bin_inclusive: index.account_maps.len().saturating_sub(1),
⋮----
impl<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> Iterator
⋮----
type Item = Vec<Pubkey>;
fn next(&mut self) -> Option<Self::Item> {
⋮----
while self.items.len() < ITER_BATCH_SIZE {
⋮----
.keys()
.into_iter()
.filter(|k| range.contains(&k))
⋮----
items.sort_unstable();
⋮----
self.items.append(&mut items);
⋮----
(!self.items.is_empty()).then(|| std::mem::take(&mut self.items))
⋮----
pub enum AccountsIndexPubkeyIterOrder {
⋮----
mod tests {
⋮----
fn test_account_index_iter_batched() {
⋮----
.take(num_pubkeys)
⋮----
index.upsert(
⋮----
let mut iter = index.iter(None::<&Range<Pubkey>>, iter_order);
let x = iter.next().unwrap();
assert_eq!(x.len(), 2 * ITER_BATCH_SIZE);
assert_eq!(
⋮----
assert_eq!(iter.items.len(), 0);
assert!(iter.next().is_none());
⋮----
fn test_accounts_iter_finished() {
⋮----
index.add_root(0);
let mut iter = index.iter(None::<&Range<Pubkey>>, AccountsIndexPubkeyIterOrder::Sorted);

================
File: accounts-db/src/accounts_index/roots_tracker.rs
================
use crate::rolling_bit_field::RollingBitField;
⋮----
pub struct RootsTracker {
⋮----
impl Default for RootsTracker {
fn default() -> Self {
⋮----
impl RootsTracker {
pub fn new(max_width: u64) -> Self {

================
File: accounts-db/src/accounts_index/secondary.rs
================
pub struct AccountSecondaryIndexes {
⋮----
impl AccountSecondaryIndexes {
pub fn is_empty(&self) -> bool {
self.indexes.is_empty()
⋮----
pub fn contains(&self, index: &AccountIndex) -> bool {
self.indexes.contains(index)
⋮----
pub fn include_key(&self, key: &Pubkey) -> bool {
⋮----
Some(options) => options.exclude ^ options.keys.contains(key),
⋮----
pub struct AccountSecondaryIndexesIncludeExclude {
⋮----
pub enum AccountIndex {
⋮----
pub enum IndexKey {
⋮----
type SecondaryReverseIndexEntry = RwLock<Vec<Pubkey>>;
pub trait SecondaryIndexEntry: Debug {
⋮----
struct SecondaryIndexStats {
⋮----
pub struct RwLockSecondaryIndexEntry {
⋮----
impl SecondaryIndexEntry for RwLockSecondaryIndexEntry {
fn insert_if_not_exists(&self, key: &Pubkey, inner_keys_count: &AtomicU64) {
if self.account_keys.read().unwrap().contains(key) {
⋮----
let was_newly_inserted = self.account_keys.write().unwrap().insert(*key);
⋮----
inner_keys_count.fetch_add(1, Ordering::Relaxed);
⋮----
fn remove_inner_key(&self, key: &Pubkey) -> bool {
self.account_keys.write().unwrap().remove(key)
⋮----
fn is_empty(&self) -> bool {
self.account_keys.read().unwrap().is_empty()
⋮----
fn keys(&self) -> Vec<Pubkey> {
self.account_keys.read().unwrap().iter().cloned().collect()
⋮----
fn len(&self) -> usize {
self.account_keys.read().unwrap().len()
⋮----
pub struct SecondaryIndex<SecondaryIndexEntryType: SecondaryIndexEntry + Default + Sync + Send> {
⋮----
// Map from index keys to index values
⋮----
pub fn new(metrics_name: &'static str) -> Self {
⋮----
pub fn insert(&self, key: &Pubkey, inner_key: &Pubkey) {
⋮----
.get(key)
.unwrap_or_else(|| self.index.entry(*key).or_default().downgrade());
pubkeys_map.insert_if_not_exists(inner_key, &self.stats.num_inner_keys);
⋮----
let outer_keys = self.reverse_index.get(inner_key).unwrap_or_else(|| {
⋮----
.entry(*inner_key)
.or_insert(RwLock::new(Vec::with_capacity(1)))
.downgrade()
⋮----
let should_insert = !outer_keys.read().unwrap().contains(key);
⋮----
let mut w_outer_keys = outer_keys.write().unwrap();
if !w_outer_keys.contains(key) {
w_outer_keys.push(*key);
⋮----
if self.stats.last_report.should_update(1000) {
datapoint_info!(
⋮----
fn remove_index_entries(&self, outer_key: &Pubkey, removed_inner_key: &Pubkey) {
⋮----
.get_mut(outer_key)
.expect("If we're removing a key, then it must have an entry in the map");
assert!(inner_key_map.value().remove_inner_key(removed_inner_key));
inner_key_map.is_empty()
⋮----
if let Occupied(key_entry) = self.index.entry(*outer_key) {
if key_entry.get().is_empty() {
key_entry.remove();
⋮----
pub fn remove_by_inner_key(&self, inner_key: &Pubkey) {
⋮----
if let Some((_, outer_keys_set)) = self.reverse_index.remove(inner_key) {
for removed_outer_key in outer_keys_set.into_inner().unwrap().into_iter() {
removed_outer_keys.insert(removed_outer_key);
⋮----
self.remove_index_entries(outer_key, inner_key);
⋮----
.fetch_sub(removed_outer_keys.len() as u64, Ordering::Relaxed);
⋮----
pub fn get(&self, key: &Pubkey) -> Vec<Pubkey> {
if let Some(inner_keys_map) = self.index.get(key) {
inner_keys_map.keys()
⋮----
vec![]
⋮----
pub fn log_contents(&self) {
⋮----
.iter()
.map(|entry| (entry.value().len(), *entry.key()))
⋮----
entries.sort_unstable();
⋮----
.rev()
.take(20)
.for_each(|(v, k)| info!("owner: {k}, accounts: {v}"));

================
File: accounts-db/src/accounts_index/stats.rs
================
pub struct HeldInMemStats {
⋮----
pub struct Stats {
⋮----
impl Stats {
pub fn new(bins: usize) -> Stats {
⋮----
pub fn inc_insert(&self) {
self.inc_insert_count(1);
⋮----
pub fn inc_insert_count(&self, count: u64) {
self.inserts.fetch_add(count, Ordering::Relaxed);
self.count.fetch_add(count as usize, Ordering::Relaxed);
⋮----
pub fn inc_delete(&self) {
self.deletes.fetch_add(1, Ordering::Relaxed);
self.count.fetch_sub(1, Ordering::Relaxed);
⋮----
pub fn inc_mem_count(&self) {
self.add_mem_count(1);
⋮----
pub fn dec_mem_count(&self) {
self.sub_mem_count(1);
⋮----
pub fn add_mem_count(&self, count: usize) {
self.count_in_mem.fetch_add(count, Ordering::Relaxed);
⋮----
pub fn sub_mem_count(&self, count: usize) {
self.count_in_mem.fetch_sub(count, Ordering::Relaxed);
⋮----
pub fn update_in_mem_capacity(&self, pre: usize, post: usize) {
match post.cmp(&pre) {
⋮----
.fetch_add(post - pre, Ordering::Relaxed);
⋮----
.fetch_sub(pre - post, Ordering::Relaxed);
⋮----
fn ms_per_age<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>>(
⋮----
let age_now = storage.current_age();
let ages_flushed = storage.count_buckets_flushed() as u64;
let last_age = self.last_age.swap(age_now, Ordering::Relaxed) as u64;
let last_ages_flushed = self.last_ages_flushed.swap(ages_flushed, Ordering::Relaxed);
⋮----
let age_delta = age_now.saturating_sub(last_age);
⋮----
let bin_delta = ages_flushed.saturating_sub(last_ages_flushed);
⋮----
pub fn remaining_until_next_interval(&self) -> u64 {
⋮----
.remaining_until_next_interval(STATS_INTERVAL_MS)
⋮----
fn get_stats(mut data: Vec<usize>) -> (usize, usize, usize, usize) {
if data.is_empty() {
⋮----
data.sort_unstable();
⋮----
*data.first().unwrap(),
*data.last().unwrap(),
data.iter().sum(),
data[data.len() / 2],
⋮----
fn calc_percent(ms: u64, elapsed_ms: u64) -> f32 {
⋮----
pub fn total_count(&self) -> usize {
self.count.load(Ordering::Relaxed)
⋮----
pub fn get_remaining_items_to_flush_estimate(&self) -> usize {
let in_mem = self.count_in_mem.load(Ordering::Relaxed) as u64;
let held_in_mem = self.held_in_mem.slot_list_cached.load(Ordering::Relaxed)
+ self.held_in_mem.slot_list_len.load(Ordering::Relaxed)
+ self.held_in_mem.ref_count.load(Ordering::Relaxed)
+ self.held_in_mem.age.load(Ordering::Relaxed);
in_mem.saturating_sub(held_in_mem) as usize
⋮----
pub fn report_stats<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>>(
⋮----
let elapsed_ms = self.last_time.elapsed_ms();
⋮----
if !self.last_time.should_update(STATS_INTERVAL_MS) {
⋮----
let ms_per_age = self.ms_per_age(storage, elapsed_ms);
let disk = storage.disk.as_ref();
⋮----
.map(|disk| {
⋮----
.map(|i| disk.get_bucket_from_index(i as usize).bucket_len() as usize)
⋮----
.unwrap_or_default();
⋮----
let startup = storage.get_startup();
let was_startup = self.last_was_startup.swap(startup, Ordering::Relaxed);
let count_in_mem = self.count_in_mem.load(Ordering::Relaxed);
let capacity_in_mem = self.capacity_in_mem.load(Ordering::Relaxed);
⋮----
if storage.is_disk_index_enabled() {
⋮----
datapoint_info!(
⋮----
let held_in_mem_clean = self.held_in_mem.clean.swap(0, Ordering::Relaxed);
let held_in_mem_age = self.held_in_mem.age.swap(0, Ordering::Relaxed);
let held_in_mem_ref_count = self.held_in_mem.ref_count.swap(0, Ordering::Relaxed);
⋮----
self.held_in_mem.slot_list_len.swap(0, Ordering::Relaxed);
⋮----
self.held_in_mem.slot_list_cached.swap(0, Ordering::Relaxed);

================
File: accounts-db/src/append_vec/meta.rs
================
pub struct StoredMeta {
⋮----
pub struct AccountMeta {
⋮----
fn from(account: &'a T) -> Self {
⋮----
lamports: account.lamports(),
owner: *account.owner(),
executable: account.executable(),
rent_epoch: account.rent_epoch(),
⋮----
fn from(account: Option<&'a T>) -> Self {
⋮----
pub struct StoredAccountMeta<'append_vec> {
⋮----
pub fn pubkey(&self) -> &'append_vec Pubkey {
⋮----
pub fn stored_size(&self) -> usize {
⋮----
pub fn offset(&self) -> usize {
⋮----
pub fn data(&self) -> &'append_vec [u8] {
⋮----
pub fn data_len(&self) -> usize {
⋮----
pub fn meta(&self) -> &StoredMeta {
⋮----
impl IsZeroLamport for StoredAccountMeta<'_> {
fn is_zero_lamport(&self) -> bool {
self.lamports() == 0
⋮----
impl<'append_vec> ReadableAccount for StoredAccountMeta<'append_vec> {
fn lamports(&self) -> u64 {
⋮----
fn data(&self) -> &'append_vec [u8] {
self.data()
⋮----
fn owner(&self) -> &'append_vec Pubkey {
⋮----
fn executable(&self) -> bool {
⋮----
fn rent_epoch(&self) -> Epoch {
⋮----
/// [`StoredAccountMeta`] without account data or account hash.
pub struct StoredAccountNoData<'append_vec> {
⋮----
pub struct StoredAccountNoData<'append_vec> {
⋮----
pub fn lamports(&self) -> u64 {
⋮----
pub fn owner(&self) -> &'append_vec Pubkey {
⋮----
pub fn sanitize(&self) -> bool {
self.sanitize_executable() && self.sanitize_lamports()
⋮----
pub fn data_len(&self) -> u64 {
⋮----
pub fn executable(&self) -> bool {
⋮----
pub fn rent_epoch(&self) -> Epoch {
⋮----
pub fn sanitize_executable(&self) -> bool {
self.ref_executable_byte() & !1 == 0
⋮----
pub fn is_default_account(&self) -> bool {
⋮----
pub fn sanitize_lamports(&self) -> bool {
self.account_meta.lamports != 0 || self.is_default_account()
⋮----
pub fn ref_executable_byte(&self) -> &u8 {
⋮----
let executable_byte: &u8 = unsafe { &*(executable_bool_ptr.cast()) };
⋮----
impl IsZeroLamport for StoredAccountNoData<'_> {
⋮----
mod tests {
⋮----
fn test_stored_readable_account() {
⋮----
data: data.clone(),
⋮----
assert!(accounts_equal(&account, &stored_account));

================
File: accounts-db/src/append_vec/test_utils.rs
================
pub struct TempFile {
⋮----
impl Drop for TempFile {
fn drop(&mut self) {
⋮----
pub fn get_append_vec_dir() -> String {
std::env::var("FARF_DIR").unwrap_or_else(|_| "farf/append_vec_tests".to_string())
⋮----
pub fn get_append_vec_path(path: &str) -> TempFile {
let out_dir = get_append_vec_dir();
⋮----
.sample_iter(&Alphanumeric)
.map(char::from)
.take(30)
.collect();
let dir = format!("{out_dir}/{rand_string}");
⋮----
buf.push(format!("{dir}/{path}"));
std::fs::create_dir_all(dir).expect("Create directory failed");
⋮----
pub fn create_test_account(sample: usize) -> (Pubkey, AccountSharedData) {
⋮----
account.set_data_from_slice(&vec![data_len as u8; data_len]);
⋮----
pub fn create_test_account_with(data_len: usize) -> (Pubkey, AccountSharedData) {

================
File: accounts-db/src/rolling_bit_field/iterators.rs
================
pub struct RollingBitFieldOnesIter<'a> {
⋮----
pub fn new(rolling_bit_field: &'a RollingBitField) -> Self {
⋮----
excess_iter: rolling_bit_field.excess.iter(),
⋮----
impl Iterator for RollingBitFieldOnesIter<'_> {
type Item = u64;
fn next(&mut self) -> Option<Self::Item> {
// Iterate over the excess first
if let Some(excess) = self.excess_iter.next() {
return Some(*excess);
⋮----
// Then iterate over the bit vec
⋮----
// If there are no more bits in the range, then we've iterated over everything and are done
let bit = self.bit_range.next()?;
if self.rolling_bit_field.contains_assume_in_range(&bit) {
break Some(bit);
⋮----
mod tests {
⋮----
fn test_rolling_bit_field_ones_iter(num_bits: u64, mut expected: Vec<u64>) {
⋮----
rolling_bit_field.insert(*val);
⋮----
let mut actual: Vec<_> = rolling_bit_field.iter_ones().collect();
actual.sort_unstable();
expected.sort_unstable();
assert_eq!(actual, expected);

================
File: accounts-db/src/tiered_storage/byte_block.rs
================
pub enum ByteBlockEncoder {
⋮----
pub struct ByteBlockWriter {
⋮----
impl ByteBlockWriter {
pub fn new(encoding: AccountBlockFormat) -> Self {
⋮----
.level(0)
.build(Vec::new())
.unwrap(),
⋮----
pub fn raw_len(&self) -> usize {
⋮----
pub fn write_pod<T: bytemuck::NoUninit>(&mut self, value: &T) -> io::Result<usize> {
unsafe { self.write_type(value) }
⋮----
pub unsafe fn write_type<T>(&mut self, value: &T) -> io::Result<usize> {
⋮----
let ptr = ptr::from_ref(value).cast();
⋮----
self.write(slice)?;
Ok(size)
⋮----
pub fn write_optional_fields(
⋮----
size += self.write_pod(&rent_epoch)?;
⋮----
debug_assert_eq!(size, opt_fields.size());
⋮----
pub fn write(&mut self, buf: &[u8]) -> io::Result<()> {
⋮----
ByteBlockEncoder::Raw(cursor) => cursor.write_all(buf)?,
ByteBlockEncoder::Lz4(lz4_encoder) => lz4_encoder.write_all(buf)?,
⋮----
self.len += buf.len();
Ok(())
⋮----
pub fn finish(self) -> io::Result<Vec<u8>> {
⋮----
ByteBlockEncoder::Raw(cursor) => Ok(cursor.into_inner()),
⋮----
let (compressed_block, result) = lz4_encoder.finish();
⋮----
Ok(compressed_block)
⋮----
pub struct ByteBlockReader;
pub fn read_pod<T: bytemuck::AnyBitPattern>(byte_block: &[u8], offset: usize) -> Option<&T> {
unsafe { read_type(byte_block, offset) }
⋮----
pub unsafe fn read_type<T>(byte_block: &[u8], offset: usize) -> Option<&T> {
let (next, overflow) = offset.overflowing_add(std::mem::size_of::<T>());
if overflow || next > byte_block.len() {
⋮----
let ptr = byte_block[offset..].as_ptr().cast();
debug_assert!((ptr as usize).is_multiple_of(std::mem::align_of::<T>()));
Some(unsafe { &*ptr })
⋮----
impl ByteBlockReader {
pub fn decode(encoding: AccountBlockFormat, input: &[u8]) -> io::Result<Vec<u8>> {
⋮----
let mut decoder = lz4::Decoder::new(input).unwrap();
let mut output = vec![];
decoder.read_to_end(&mut output)?;
Ok(output)
⋮----
AccountBlockFormat::AlignedRaw => panic!("the input buffer is already decoded"),
⋮----
mod tests {
⋮----
fn read_type_unaligned<T>(buffer: &[u8], offset: usize) -> (T, usize) {
⋮----
let (next, overflow) = offset.overflowing_add(size);
assert!(!overflow && next <= buffer.len());
⋮----
let ptr = data.as_ptr().cast();
⋮----
fn write_single(format: AccountBlockFormat) {
⋮----
writer.write_pod(&value).unwrap();
assert_eq!(writer.raw_len(), mem::size_of::<u32>());
let buffer = writer.finish().unwrap();
⋮----
ByteBlockReader::decode(format, &buffer).unwrap()
⋮----
assert_eq!(decoded_buffer.len(), mem::size_of::<u32>());
⋮----
assert_eq!(value, value_from_buffer);
⋮----
assert_eq!(next, mem::size_of::<u32>());
⋮----
fn test_write_single_raw_format() {
write_single(AccountBlockFormat::AlignedRaw);
⋮----
fn test_write_single_encoded_format() {
write_single(AccountBlockFormat::Lz4);
⋮----
struct TestMetaStruct {
⋮----
fn write_multiple(format: AccountBlockFormat) {
⋮----
let test_metas: Vec<TestMetaStruct> = vec![
⋮----
writer.write_type(&test_metas[0]).unwrap();
writer.write_type(&test_data1).unwrap();
writer.write_type(&test_metas[1]).unwrap();
writer.write_type(&test_data2).unwrap();
writer.write_type(&test_metas[2]).unwrap();
writer.write_type(&test_data3).unwrap();
⋮----
assert_eq!(
⋮----
assert_eq!(test_metas[0], meta1_from_buffer);
⋮----
assert_eq!(test_metas[1], meta2_from_buffer);
⋮----
assert_eq!(test_metas[2], meta3_from_buffer);
⋮----
fn test_write_multiple_raw_format() {
write_multiple(AccountBlockFormat::AlignedRaw);
⋮----
fn test_write_multiple_lz4_format() {
write_multiple(AccountBlockFormat::Lz4);
⋮----
fn write_optional_fields(format: AccountBlockFormat) {
⋮----
let mut opt_fields_vec = vec![];
⋮----
for rent_epoch in [None, Some(test_epoch)] {
some_count += rent_epoch.iter().count();
opt_fields_vec.push(AccountMetaOptionalFields { rent_epoch });
⋮----
writer.write_optional_fields(opt_fields).unwrap();
expected_size += opt_fields.size();
⋮----
assert_eq!(decoded_buffer.len(), expected_size);
⋮----
let rent_epoch = read_pod::<Epoch>(&decoded_buffer, offset).unwrap();
assert_eq!(*rent_epoch, expected_rent_epoch);
⋮----
assert_eq!(some_count, verified_count);
⋮----
fn test_write_optional_fields_raw_format() {
write_optional_fields(AccountBlockFormat::AlignedRaw);
⋮----
fn test_write_optional_fields_lz4_format() {
write_optional_fields(AccountBlockFormat::Lz4);

================
File: accounts-db/src/tiered_storage/error.rs
================
pub enum TieredStorageError {

================
File: accounts-db/src/tiered_storage/file.rs
================
pub struct TieredStorageMagicNumber(pub u64);
const _: () = assert!(std::mem::size_of::<TieredStorageMagicNumber>() == 8);
impl Default for TieredStorageMagicNumber {
fn default() -> Self {
Self(FILE_MAGIC_NUMBER)
⋮----
pub struct TieredReadableFile(pub File);
impl TieredReadableFile {
pub fn new(file_path: impl AsRef<Path>) -> TieredStorageResult<Self> {
let file = Self(
⋮----
.read(true)
.create(false)
.open(&file_path)?,
⋮----
file.check_magic_number()?;
Ok(file)
⋮----
pub fn new_writable(file_path: impl AsRef<Path>) -> io::Result<Self> {
Ok(Self(
⋮----
.create_new(true)
.write(true)
.open(file_path)?,
⋮----
fn check_magic_number(&self) -> TieredStorageResult<()> {
self.seek_from_end(-(std::mem::size_of::<TieredStorageMagicNumber>() as i64))?;
⋮----
self.read_pod(&mut magic_number)?;
⋮----
return Err(TieredStorageError::MagicNumberMismatch(
⋮----
Ok(())
⋮----
pub fn read_pod<T: NoUninit + AnyBitPattern>(&self, value: &mut T) -> io::Result<()> {
unsafe { self.read_type(value) }
⋮----
pub unsafe fn read_type<T>(&self, value: &mut T) -> io::Result<()> {
let ptr = ptr::from_mut(value).cast();
⋮----
self.read_bytes(bytes)
⋮----
pub fn seek(&self, offset: u64) -> io::Result<u64> {
(&self.0).seek(SeekFrom::Start(offset))
⋮----
pub fn seek_from_end(&self, offset: i64) -> io::Result<u64> {
(&self.0).seek(SeekFrom::End(offset))
⋮----
pub fn read_bytes(&self, buffer: &mut [u8]) -> io::Result<()> {
(&self.0).read_exact(buffer)
⋮----
pub struct TieredWritableFile(pub BufWriter<File>);
impl TieredWritableFile {
pub fn new(file_path: impl AsRef<Path>) -> io::Result<Self> {
Ok(Self(BufWriter::new(
⋮----
pub fn write_pod<T: NoUninit>(&mut self, value: &T) -> io::Result<usize> {
unsafe { self.write_type(value) }
⋮----
pub unsafe fn write_type<T>(&mut self, value: &T) -> io::Result<usize> {
let ptr = ptr::from_ref(value).cast();
⋮----
self.write_bytes(bytes)
⋮----
pub fn seek(&mut self, offset: u64) -> io::Result<u64> {
self.0.seek(SeekFrom::Start(offset))
⋮----
pub fn seek_from_end(&mut self, offset: i64) -> io::Result<u64> {
self.0.seek(SeekFrom::End(offset))
⋮----
pub fn write_bytes(&mut self, bytes: &[u8]) -> io::Result<usize> {
self.0.write_all(bytes)?;
Ok(bytes.len())
⋮----
impl Drop for TieredWritableFile {
fn drop(&mut self) {
let result = self.0.flush();
⋮----
panic!("failed to flush TieredWritableFile on drop: {err}");
⋮----
mod tests {
⋮----
fn generate_test_file_with_number(path: impl AsRef<Path>, number: u64) {
let mut file = TieredWritableFile::new(path).unwrap();
file.write_pod(&number).unwrap();
⋮----
fn test_new() {
let temp_dir = TempDir::new().unwrap();
let path = temp_dir.path().join("test_new");
generate_test_file_with_number(&path, FILE_MAGIC_NUMBER);
assert!(TieredReadableFile::new(&path).is_ok());
⋮----
fn test_magic_number_mismatch() {
⋮----
let path = temp_dir.path().join("test_magic_number_mismatch");
generate_test_file_with_number(&path, !FILE_MAGIC_NUMBER);
assert!(matches!(

================
File: accounts-db/src/tiered_storage/footer.rs
================
pub enum AccountMetaFormat {
⋮----
pub enum AccountBlockFormat {
⋮----
pub struct TieredStorageFooter {
⋮----
const _: () = assert!(
⋮----
impl Default for TieredStorageFooter {
fn default() -> Self {
⋮----
impl TieredStorageFooter {
pub fn new_from_path(path: impl AsRef<Path>) -> TieredStorageResult<Self> {
⋮----
pub fn write_footer_block(&self, file: &mut TieredWritableFile) -> TieredStorageResult<usize> {
⋮----
bytes_written += unsafe { file.write_type(self)? };
bytes_written += file.write_pod(&TieredStorageMagicNumber::default())?;
Ok(bytes_written)
⋮----
pub fn new_from_footer_block(file: &TieredReadableFile) -> TieredStorageResult<Self> {
file.seek_from_end(-(FOOTER_TAIL_SIZE as i64))?;
⋮----
file.read_pod(&mut footer_version)?;
⋮----
return Err(TieredStorageError::InvalidFooterVersion(footer_version));
⋮----
file.read_pod(&mut footer_size)?;
⋮----
return Err(TieredStorageError::InvalidFooterSize(
⋮----
file.read_pod(&mut magic_number)?;
⋮----
return Err(TieredStorageError::MagicNumberMismatch(
⋮----
file.seek_from_end(-(footer_size as i64))?;
unsafe { file.read_type(&mut footer)? };
⋮----
Ok(footer)
⋮----
pub fn new_from_mmap(mmap: &Mmap) -> TieredStorageResult<&TieredStorageFooter> {
let offset = mmap.len().saturating_sub(FOOTER_TAIL_SIZE);
⋮----
return Err(TieredStorageError::InvalidFooterVersion(*footer_version));
⋮----
let footer_offset = mmap.len().saturating_sub(footer_size as usize);
⋮----
fn sanitize(footer: &Self) -> Result<(), SanitizeFooterError> {
⋮----
.map_err(SanitizeFooterError::InvalidAccountMetaFormat)?;
⋮----
.map_err(SanitizeFooterError::InvalidOwnersBlockFormat)?;
⋮----
.map_err(SanitizeFooterError::InvalidIndexBlockFormat)?;
⋮----
.map_err(SanitizeFooterError::InvalidAccountBlockFormat)?;
Ok(())
⋮----
pub enum SanitizeFooterError {
⋮----
mod tests {
⋮----
fn test_footer() {
let path = get_append_vec_path("test_file_footer");
⋮----
let mut file = TieredWritableFile::new(&path.path).unwrap();
expected_footer.write_footer_block(&mut file).unwrap();
⋮----
let footer = TieredStorageFooter::new_from_path(&path.path).unwrap();
assert_eq!(expected_footer, footer);
⋮----
fn test_footer_layout() {
assert_eq!(offset_of!(TieredStorageFooter, account_meta_format), 0x00);
assert_eq!(offset_of!(TieredStorageFooter, owners_block_format), 0x02);
assert_eq!(offset_of!(TieredStorageFooter, index_block_format), 0x04);
assert_eq!(offset_of!(TieredStorageFooter, account_block_format), 0x06);
assert_eq!(offset_of!(TieredStorageFooter, account_entry_count), 0x08);
assert_eq!(
⋮----
assert_eq!(offset_of!(TieredStorageFooter, account_block_size), 0x10);
assert_eq!(offset_of!(TieredStorageFooter, owner_count), 0x18);
assert_eq!(offset_of!(TieredStorageFooter, owner_entry_size), 0x1C);
assert_eq!(offset_of!(TieredStorageFooter, index_block_offset), 0x20);
assert_eq!(offset_of!(TieredStorageFooter, owners_block_offset), 0x28);
assert_eq!(offset_of!(TieredStorageFooter, min_account_address), 0x30);
assert_eq!(offset_of!(TieredStorageFooter, max_account_address), 0x50);
assert_eq!(offset_of!(TieredStorageFooter, hash), 0x70);
assert_eq!(offset_of!(TieredStorageFooter, format_version), 0x90);
assert_eq!(offset_of!(TieredStorageFooter, footer_size), 0x98);
⋮----
fn test_sanitize() {
⋮----
assert!(result.is_ok());
⋮----
assert!(matches!(

================
File: accounts-db/src/tiered_storage/hot.rs
================
fn new_hot_footer() -> TieredStorageFooter {
⋮----
const MAX_HOT_OWNER_OFFSET: OwnerOffset = OwnerOffset((1 << 29) - 1);
⋮----
fn padding_bytes(data_len: usize) -> u8 {
⋮----
struct HotMetaPackedFields {
⋮----
const _: () = assert!(std::mem::size_of::<HotMetaPackedFields>() == 4);
⋮----
pub struct HotAccountOffset(u32);
const _: () = assert!(std::mem::size_of::<HotAccountOffset>() == 4);
impl AccountOffset for HotAccountOffset {}
impl HotAccountOffset {
pub fn new(offset: usize) -> TieredStorageResult<Self> {
⋮----
return Err(TieredStorageError::OffsetOutOfBounds(
⋮----
if !offset.is_multiple_of(HOT_ACCOUNT_ALIGNMENT) {
return Err(TieredStorageError::OffsetAlignmentError(
⋮----
Ok(HotAccountOffset((offset / HOT_ACCOUNT_ALIGNMENT) as u32))
⋮----
fn offset(&self) -> usize {
⋮----
pub struct HotAccountMeta {
⋮----
const _: () = assert!(std::mem::size_of::<HotAccountMeta>() == 8 + 4 + 4);
impl TieredAccountMeta for HotAccountMeta {
fn new() -> Self {
⋮----
fn with_lamports(mut self, lamports: u64) -> Self {
⋮----
fn with_account_data_padding(mut self, padding: u8) -> Self {
⋮----
panic!("padding exceeds MAX_HOT_PADDING");
⋮----
self.packed_fields.set_padding(padding);
⋮----
fn with_owner_offset(mut self, owner_offset: OwnerOffset) -> Self {
⋮----
panic!("owner_offset exceeds MAX_HOT_OWNER_OFFSET");
⋮----
self.packed_fields.set_owner_offset(owner_offset.0);
⋮----
fn with_account_data_size(self, _account_data_size: u64) -> Self {
⋮----
fn with_flags(mut self, flags: &AccountMetaFlags) -> Self {
⋮----
fn lamports(&self) -> u64 {
⋮----
fn account_data_padding(&self) -> u8 {
self.packed_fields.padding()
⋮----
fn owner_offset(&self) -> OwnerOffset {
OwnerOffset(self.packed_fields.owner_offset())
⋮----
fn flags(&self) -> &AccountMetaFlags {
⋮----
fn supports_shared_account_block() -> bool {
⋮----
fn rent_epoch(&self, account_block: &[u8]) -> Option<Epoch> {
self.flags()
.has_rent_epoch()
.then(|| {
let offset = self.optional_fields_offset(account_block)
+ AccountMetaOptionalFields::rent_epoch_offset(self.flags());
byte_block::read_pod::<Epoch>(account_block, offset).copied()
⋮----
.flatten()
⋮----
fn final_rent_epoch(&self, account_block: &[u8]) -> Epoch {
self.rent_epoch(account_block)
.unwrap_or(if self.lamports() != 0 {
⋮----
fn optional_fields_offset(&self, account_block: &[u8]) -> usize {
⋮----
.len()
.saturating_sub(AccountMetaOptionalFields::size_from_flags(&self.flags))
⋮----
fn account_data_size(&self, account_block: &[u8]) -> usize {
self.optional_fields_offset(account_block)
.saturating_sub(self.account_data_padding() as usize)
⋮----
fn account_data<'a>(&self, account_block: &'a [u8]) -> &'a [u8] {
&account_block[..self.account_data_size(account_block)]
⋮----
/// The struct that offers read APIs for accessing a hot account.
#[derive(PartialEq, Eq, Debug)]
pub struct HotAccount<'accounts_file, M: TieredAccountMeta> {
⋮----
/// The address of the account
    pub address: &'accounts_file Pubkey,
⋮----
/// The index for accessing the account inside its belonging AccountsFile
    pub index: IndexOffset,
/// The account block that contains this account.  Note that this account
    /// block may be shared with other accounts.
⋮----
/// block may be shared with other accounts.
    pub account_block: &'accounts_file [u8],
⋮----
pub fn address(&self) -> &'accounts_file Pubkey {
⋮----
/// Returns the index to this account in its AccountsFile.
    pub fn index(&self) -> IndexOffset {
⋮----
pub fn index(&self) -> IndexOffset {
⋮----
/// Returns the data associated to this account.
    pub fn data(&self) -> &'accounts_file [u8] {
⋮----
pub fn data(&self) -> &'accounts_file [u8] {
self.meta.account_data(self.account_block)
⋮----
pub fn stored_size(&self) -> usize {
stored_size(self.meta.account_data_size(self.account_block))
⋮----
impl<'accounts_file, M: TieredAccountMeta> ReadableAccount for HotAccount<'accounts_file, M> {
⋮----
self.meta.lamports()
⋮----
fn owner(&self) -> &'accounts_file Pubkey {
⋮----
/// Returns true if the data associated to this account is executable.
    fn executable(&self) -> bool {
⋮----
fn executable(&self) -> bool {
self.meta.flags().executable()
⋮----
/// Returns the epoch that this account will next owe rent by parsing
    /// the specified account block.  RENT_EXEMPT_RENT_EPOCH will be returned
⋮----
/// the specified account block.  RENT_EXEMPT_RENT_EPOCH will be returned
    /// if the account is rent-exempt.
⋮----
/// if the account is rent-exempt.
    ///
⋮----
///
    /// For a zero-lamport account, Epoch::default() will be returned to
⋮----
/// For a zero-lamport account, Epoch::default() will be returned to
    /// default states of an AccountSharedData.
⋮----
/// default states of an AccountSharedData.
    fn rent_epoch(&self) -> Epoch {
⋮----
fn rent_epoch(&self) -> Epoch {
self.meta.final_rent_epoch(self.account_block)
⋮----
/// Returns the data associated to this account.
    fn data(&self) -> &'accounts_file [u8] {
⋮----
fn data(&self) -> &'accounts_file [u8] {
self.data()
⋮----
pub struct HotStorageReader {
⋮----
impl HotStorageReader {
pub fn new(file: TieredReadableFile) -> TieredStorageResult<Self> {
let mmap = unsafe { MmapOptions::new().map(&file.0)? };
⋮----
Ok(Self { mmap, footer })
⋮----
pub fn len(&self) -> usize {
self.mmap.len()
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub fn capacity(&self) -> u64 {
self.len() as u64
⋮----
pub fn footer(&self) -> &TieredStorageFooter {
⋮----
pub fn num_accounts(&self) -> usize {
⋮----
fn get_account_meta_from_offset(
⋮----
let offset = account_offset.offset();
assert!(
⋮----
Ok(meta)
⋮----
pub(super) fn get_account_offset(
⋮----
fn get_account_address(&self, index: IndexOffset) -> TieredStorageResult<&Pubkey> {
⋮----
.get_account_address(&self.mmap, &self.footer, index)
⋮----
fn get_owner_address(&self, owner_offset: OwnerOffset) -> TieredStorageResult<&Pubkey> {
⋮----
.get_owner_address(&self.mmap, &self.footer, owner_offset)
⋮----
fn get_account_block_size(
⋮----
let account_meta_offset = account_offset.offset();
⋮----
if index_offset.0.saturating_add(1) == self.footer.account_entry_count {
⋮----
self.get_account_offset(IndexOffset(index_offset.0.saturating_add(1)))?
.offset()
⋮----
Ok(account_block_ending_offset
.saturating_sub(account_meta_offset)
.saturating_sub(std::mem::size_of::<HotAccountMeta>()))
⋮----
fn get_account_block(
⋮----
let (data, _) = get_slice(
⋮----
account_offset.offset() + std::mem::size_of::<HotAccountMeta>(),
self.get_account_block_size(account_offset, index_offset)?,
⋮----
Ok(data)
⋮----
pub fn get_stored_account_without_data_callback<Ret>(
⋮----
return Ok(None);
⋮----
let account_offset = self.get_account_offset(index_offset)?;
let meta = self.get_account_meta_from_offset(account_offset)?;
let account_block = self.get_account_block(account_offset, index_offset)?;
⋮----
pubkey: self.get_account_address(index_offset)?,
lamports: meta.lamports(),
owner: self.get_owner_address(meta.owner_offset())?,
data_len: meta.account_data_size(account_block),
executable: meta.flags().executable(),
rent_epoch: meta.final_rent_epoch(account_block),
⋮----
Ok(Some(callback(stored_account)))
⋮----
pub fn get_stored_account_callback<Ret>(
⋮----
data: meta.account_data(account_block),
⋮----
pub fn get_account_shared_data(
⋮----
let lamports = meta.lamports();
let data = meta.account_data(account_block).to_vec();
let owner = *self.get_owner_address(meta.owner_offset())?;
let executable = meta.flags().executable();
let rent_epoch = meta.final_rent_epoch(account_block);
Ok(Some(AccountSharedData::create(
⋮----
pub fn scan_pubkeys(&self, mut callback: impl FnMut(&Pubkey)) -> TieredStorageResult<()> {
⋮----
let address = self.get_account_address(IndexOffset(i))?;
callback(address);
⋮----
Ok(())
⋮----
pub(crate) fn calculate_stored_size(data_len: usize) -> usize {
stored_size(data_len)
⋮----
pub(crate) fn get_account_data_lens(
⋮----
let mut result = Vec::with_capacity(sorted_offsets.len());
⋮----
let index_offset = IndexOffset(AccountInfo::get_reduced_offset(offset));
⋮----
let data_len = meta.account_data_size(account_block);
result.push(data_len);
⋮----
Ok(result)
⋮----
pub fn scan_accounts_without_data(
⋮----
self.get_stored_account_without_data_callback(IndexOffset(i), |account| {
callback(AccountInfo::reduced_offset_to_offset(i), account)
⋮----
pub fn scan_accounts(
⋮----
self.get_stored_account_callback(IndexOffset(i), |account| {
⋮----
pub fn data_for_archive(&self) -> &[u8] {
self.mmap.as_ref()
⋮----
fn stored_size(data_len: usize) -> usize {
⋮----
fn write_optional_fields(
⋮----
size += file.write_pod(&rent_epoch)?;
⋮----
debug_assert_eq!(size, opt_fields.size());
Ok(size)
⋮----
pub struct HotStorageWriter {
⋮----
impl HotStorageWriter {
pub fn new(file_path: impl AsRef<Path>) -> TieredStorageResult<Self> {
Ok(Self {
⋮----
fn write_account(
⋮----
flags.set_executable(executable);
let padding_len = padding_bytes(account_data.len());
⋮----
.with_lamports(lamports)
.with_owner_offset(owner_offset)
.with_account_data_size(account_data.len() as u64)
.with_account_data_padding(padding_len)
.with_flags(&flags);
⋮----
stored_size += self.storage.write_pod(&meta)?;
stored_size += self.storage.write_bytes(account_data)?;
⋮----
.write_bytes(&PADDING_BUFFER[0..(padding_len as usize)])?;
stored_size += write_optional_fields(&mut self.storage, &optional_fields)?;
Ok(stored_size)
⋮----
pub fn write_accounts<'a>(
⋮----
let mut footer = new_hot_footer();
let mut index = vec![];
⋮----
let len = accounts.len();
let total_input_accounts = len.saturating_sub(skip);
⋮----
address: *account.pubkey(),
⋮----
address_range.update(account.pubkey());
⋮----
account.lamports(),
account.owner(),
account.data(),
account.executable(),
(account.rent_epoch() != RENT_EXEMPT_RENT_EPOCH)
.then_some(account.rent_epoch()),
⋮----
let owner_offset = owners_table.insert(owner);
⋮----
self.write_account(lamports, owner_offset, data, executable, rent_epoch)?;
offsets.push(index.len());
index.push(index_entry);
⋮----
assert!(cursor % HOT_BLOCK_ALIGNMENT == 0);
⋮----
.write_index_block(&mut self.storage, &index)?;
⋮----
assert_eq!(cursor % HOT_BLOCK_ALIGNMENT, 4);
cursor += self.storage.write_pod(&0u32)?;
⋮----
footer.owner_count = owners_table.len() as u32;
⋮----
.write_owners_block(&mut self.storage, &owners_table)?;
⋮----
cursor += footer.write_footer_block(&mut self.storage)?;
Ok(StoredAccountsInfo {
⋮----
pub fn flush(&mut self) -> TieredStorageResult<()> {
⋮----
.flush()
.map_err(TieredStorageError::FlushHotWriter)
⋮----
mod tests {
⋮----
struct WriteTestFileInfo {
⋮----
fn write_test_file(num_accounts: usize, num_owners: usize) -> WriteTestFileInfo {
let temp_dir = TempDir::new().unwrap();
let file_path = temp_dir.path().join("test");
⋮----
.take(num_owners)
.collect();
⋮----
.take(num_accounts)
⋮----
.map(|i| vec![i as u8; rng.random_range(0..4096)])
⋮----
.map(|i| {
⋮----
.with_lamports(rng.random())
.with_owner_offset(OwnerOffset(rng.random_range(0..num_owners) as u32))
.with_account_data_padding(padding_bytes(datas[i].len()))
⋮----
let mut file = TieredWritableFile::new(&file_path).unwrap();
⋮----
.iter()
.zip(datas.iter())
.zip(addresses.iter())
.map(|((meta, data), address)| {
⋮----
current_offset += file.write_pod(meta).unwrap();
current_offset += file.write_bytes(data).unwrap();
⋮----
.write_bytes(&padding_buffer[0..padding_bytes(data.len()) as usize])
.unwrap();
⋮----
offset: HotAccountOffset::new(prev_offset).unwrap(),
⋮----
.write_index_block(&mut file, &index_writer_entries)
⋮----
owners.iter().for_each(|owner_address| {
owners_table.insert(owner_address);
⋮----
.write_owners_block(&mut file, &owners_table)
⋮----
footer.write_footer_block(&mut file).unwrap();
⋮----
fn test_hot_account_meta_layout() {
assert_eq!(offset_of!(HotAccountMeta, lamports), 0x00);
assert_eq!(offset_of!(HotAccountMeta, packed_fields), 0x08);
assert_eq!(offset_of!(HotAccountMeta, flags), 0x0C);
assert_eq!(std::mem::size_of::<HotAccountMeta>(), 16);
⋮----
fn test_packed_fields() {
⋮----
packed_fields.set_padding(TEST_PADDING);
packed_fields.set_owner_offset(TEST_OWNER_OFFSET);
assert_eq!(packed_fields.padding(), TEST_PADDING);
assert_eq!(packed_fields.owner_offset(), TEST_OWNER_OFFSET);
⋮----
fn test_packed_fields_max_values() {
⋮----
packed_fields.set_padding(MAX_HOT_PADDING);
packed_fields.set_owner_offset(MAX_HOT_OWNER_OFFSET.0);
assert_eq!(packed_fields.padding(), MAX_HOT_PADDING);
assert_eq!(packed_fields.owner_offset(), MAX_HOT_OWNER_OFFSET.0);
⋮----
fn test_hot_meta_max_values() {
⋮----
.with_account_data_padding(MAX_HOT_PADDING)
.with_owner_offset(MAX_HOT_OWNER_OFFSET);
assert_eq!(meta.account_data_padding(), MAX_HOT_PADDING);
assert_eq!(meta.owner_offset(), MAX_HOT_OWNER_OFFSET);
⋮----
fn test_max_hot_account_offset() {
assert_matches!(HotAccountOffset::new(0), Ok(_));
assert_matches!(HotAccountOffset::new(MAX_HOT_ACCOUNT_OFFSET), Ok(_));
⋮----
fn test_max_hot_account_offset_out_of_bounds() {
assert_matches!(
⋮----
fn test_max_hot_account_offset_alignment_error() {
⋮----
fn test_hot_meta_padding_exceeds_limit() {
HotAccountMeta::new().with_account_data_padding(MAX_HOT_PADDING + 1);
⋮----
fn test_hot_meta_owner_offset_exceeds_limit() {
HotAccountMeta::new().with_owner_offset(OwnerOffset(MAX_HOT_OWNER_OFFSET.0 + 1));
⋮----
fn test_hot_account_meta() {
⋮----
const TEST_OWNER_OFFSET: OwnerOffset = OwnerOffset(0x1fef_1234);
⋮----
rent_epoch: Some(TEST_RENT_EPOCH),
⋮----
.with_lamports(TEST_LAMPORTS)
.with_account_data_padding(TEST_PADDING)
.with_owner_offset(TEST_OWNER_OFFSET)
⋮----
assert_eq!(meta.lamports(), TEST_LAMPORTS);
assert_eq!(meta.account_data_padding(), TEST_PADDING);
assert_eq!(meta.owner_offset(), TEST_OWNER_OFFSET);
assert_eq!(*meta.flags(), flags);
⋮----
fn test_hot_account_meta_full() {
⋮----
.with_lamports(TEST_LAMPORT)
.with_account_data_padding(padding.len().try_into().unwrap())
.with_owner_offset(OwnerOffset(OWNER_OFFSET))
⋮----
writer.write_pod(&expected_meta).unwrap();
⋮----
writer.write_type(&account_data).unwrap();
writer.write_type(&padding).unwrap();
⋮----
writer.write_optional_fields(&optional_fields).unwrap();
let buffer = writer.finish().unwrap();
let meta = byte_block::read_pod::<HotAccountMeta>(&buffer, 0).unwrap();
assert_eq!(expected_meta, *meta);
assert!(meta.flags().has_rent_epoch());
assert_eq!(meta.account_data_padding() as usize, padding.len());
⋮----
assert_eq!(
⋮----
assert_eq!(account_data.len(), meta.account_data_size(account_block));
assert_eq!(account_data, meta.account_data(account_block));
assert_eq!(meta.rent_epoch(account_block), optional_fields.rent_epoch);
⋮----
fn test_hot_storage_footer() {
⋮----
let path = temp_dir.path().join("test_hot_storage_footer");
⋮----
let mut file = TieredWritableFile::new(&path).unwrap();
expected_footer.write_footer_block(&mut file).unwrap();
⋮----
let file = TieredReadableFile::new(&path).unwrap();
let hot_storage = HotStorageReader::new(file).unwrap();
assert_eq!(expected_footer, *hot_storage.footer());
⋮----
fn test_hot_storage_get_account_meta_from_offset() {
⋮----
.map(|_| {
⋮----
.with_lamports(rng.random_range(0..u64::MAX))
.with_owner_offset(OwnerOffset(rng.random_range(0..NUM_ACCOUNTS)))
⋮----
.map(|meta| {
⋮----
HotAccountOffset::new(prev_offset).unwrap()
⋮----
for (offset, expected_meta) in account_offsets.iter().zip(hot_account_metas.iter()) {
let meta = hot_storage.get_account_meta_from_offset(*offset).unwrap();
assert_eq!(meta, expected_meta);
⋮----
assert_eq!(&footer, hot_storage.footer());
⋮----
fn test_get_account_meta_from_offset_out_of_bounds() {
⋮----
.path()
.join("test_get_account_meta_from_offset_out_of_bounds");
⋮----
let offset = HotAccountOffset::new(footer.index_block_offset as usize).unwrap();
hot_storage.get_account_meta_from_offset(offset).unwrap();
⋮----
fn test_hot_storage_get_account_offset_and_address() {
⋮----
.join("test_hot_storage_get_account_offset_and_address");
⋮----
.take(NUM_ACCOUNTS as usize)
⋮----
.map(|address| AccountIndexWriterEntry {
⋮----
rng.random_range(0..u32::MAX) as usize * HOT_ACCOUNT_ALIGNMENT,
⋮----
.unwrap(),
⋮----
for (i, index_writer_entry) in index_writer_entries.iter().enumerate() {
⋮----
.get_account_offset(IndexOffset(i as u32))
⋮----
assert_eq!(account_offset, index_writer_entry.offset);
⋮----
.get_account_address(IndexOffset(i as u32))
⋮----
assert_eq!(account_address, &index_writer_entry.address);
⋮----
fn test_hot_storage_get_owner_address() {
⋮----
let path = temp_dir.path().join("test_hot_storage_get_owner_address");
⋮----
.take(NUM_OWNERS)
⋮----
addresses.iter().for_each(|owner_address| {
⋮----
for (i, address) in addresses.iter().enumerate() {
⋮----
fn test_get_stored_account_without_data_callback() {
⋮----
let test_info = write_test_file(NUM_ACCOUNTS, NUM_OWNERS);
let file = TieredReadableFile::new(&test_info.file_path).unwrap();
⋮----
.get_stored_account_without_data_callback(IndexOffset(i as u32), |stored_account| {
assert_eq!(stored_account.lamports, test_info.metas[i].lamports());
assert_eq!(stored_account.data_len, test_info.datas[i].len());
⋮----
assert_eq!(*stored_account.pubkey(), test_info.addresses[i]);
⋮----
.unwrap()
⋮----
assert!(matches!(
⋮----
fn test_get_stored_account_callback() {
⋮----
.get_stored_account_callback(IndexOffset(i as u32), |stored_account| {
assert_eq!(stored_account.lamports(), test_info.metas[i].lamports());
assert_eq!(stored_account.data().len(), test_info.datas[i].len());
assert_eq!(stored_account.data(), test_info.datas[i]);
⋮----
fn test_get_account_shared_data() {
⋮----
let index_offset = IndexOffset(i as u32);
⋮----
.get_account_shared_data(index_offset)
⋮----
assert_eq!(account.lamports(), test_info.metas[i].lamports());
assert_eq!(account.data().len(), test_info.datas[i].len());
assert_eq!(account.data(), test_info.datas[i]);
⋮----
fn test_hot_storage_writer_twice_on_same_path() {
⋮----
.join("test_hot_storage_writer_twice_on_same_path");
assert_matches!(HotStorageWriter::new(&path), Ok(_));
assert_matches!(HotStorageWriter::new(&path), Err(_));
⋮----
fn test_write_account_and_index_blocks() {
⋮----
.map(|size| create_test_account(*size))
⋮----
let path = temp_dir.path().join("test_write_account_and_index_blocks");
⋮----
let mut writer = HotStorageWriter::new(&path).unwrap();
let stored_accounts_info = writer.write_accounts(&storable_accounts, 0).unwrap();
writer.flush().unwrap();
⋮----
let num_accounts = account_data_sizes.len();
⋮----
storable_accounts.account_default_if_zero_lamport(i, |account| {
verify_test_account(
⋮----
&account.take_account(),
account.pubkey(),
⋮----
.get_stored_account_callback(IndexOffset(offset as u32), |stored_account| {
storable_accounts.account_default_if_zero_lamport(offset, |account| {
⋮----
.scan_accounts(|_offset, stored_account| {
⋮----
verify_test_account(&stored_account, &account.take_account(), account.pubkey());
⋮----
let footer = hot_storage.footer();
⋮----
assert!(!hot_storage.is_empty());
assert_eq!(expected_size, hot_storage.len());

================
File: accounts-db/src/tiered_storage/index.rs
================
pub struct AccountIndexWriterEntry<Offset: AccountOffset> {
⋮----
pub trait AccountOffset: Clone + Copy + Pod + Zeroable {}
⋮----
pub struct IndexOffset(pub u32);
const _: () = assert!(std::mem::size_of::<IndexOffset>() == 4);
⋮----
pub enum IndexBlockFormat {
⋮----
const _: () = assert!(std::mem::size_of::<IndexBlockFormat>() == 2);
impl IndexBlockFormat {
pub fn write_index_block(
⋮----
bytes_written += file.write_pod(&index_entry.address)?;
⋮----
bytes_written += file.write_pod(&index_entry.offset)?;
⋮----
Ok(bytes_written)
⋮----
pub fn get_account_address<'a>(
⋮----
debug_assert!(index_offset.0 < footer.account_entry_count);
⋮----
debug_assert!(
⋮----
Ok(address)
⋮----
pub fn get_account_offset<Offset: AccountOffset>(
⋮----
Ok(*account_offset)
⋮----
pub fn entry_size<Offset: AccountOffset>(&self) -> usize {
⋮----
mod tests {
⋮----
fn test_address_and_offset_indexer() {
⋮----
let temp_dir = TempDir::new().unwrap();
let path = temp_dir.path().join("test_address_and_offset_indexer");
⋮----
.take(ENTRY_COUNT)
.collect();
⋮----
.iter()
.map(|address| AccountIndexWriterEntry {
⋮----
rng.random_range(0..u32::MAX) as usize * HOT_ACCOUNT_ALIGNMENT,
⋮----
.unwrap(),
⋮----
let mut file = TieredWritableFile::new(&path).unwrap();
⋮----
.write_index_block(&mut file, &index_entries)
.unwrap();
⋮----
.read(true)
.create(false)
.open(&path)
⋮----
let mmap = unsafe { MmapOptions::new().map(&file).unwrap() };
for (i, index_entry) in index_entries.iter().enumerate() {
⋮----
.get_account_offset::<HotAccountOffset>(&mmap, &footer, IndexOffset(i as u32))
⋮----
assert_eq!(index_entry.offset, account_offset);
⋮----
.get_account_address(&mmap, &footer, IndexOffset(i as u32))
⋮----
assert_eq!(index_entry.address, *address);
⋮----
fn test_get_account_address_out_of_bounds() {
⋮----
.path()
.join("test_get_account_address_out_of_bounds");
⋮----
footer.write_footer_block(&mut file).unwrap();
⋮----
.get_account_address(&mmap, &footer, IndexOffset(footer.account_entry_count))
⋮----
fn test_get_account_address_exceeds_index_block_boundary() {
⋮----
.join("test_get_account_address_exceeds_index_block_boundary");
⋮----
.get_account_address(&mmap, &footer, IndexOffset(2))
⋮----
fn test_get_account_offset_out_of_bounds() {
⋮----
.join("test_get_account_offset_out_of_bounds");
⋮----
IndexOffset(footer.account_entry_count),
⋮----
fn test_get_account_offset_exceeds_index_block_boundary() {
⋮----
.join("test_get_account_offset_exceeds_index_block_boundary");
⋮----
.get_account_offset::<HotAccountOffset>(&mmap, &footer, IndexOffset(2))

================
File: accounts-db/src/tiered_storage/meta.rs
================
pub struct AccountMetaFlags {
⋮----
const _: () = assert!(std::mem::size_of::<AccountMetaFlags>() == 4);
pub trait TieredAccountMeta: Sized {
⋮----
impl AccountMetaFlags {
pub fn new_from(optional_fields: &AccountMetaOptionalFields) -> Self {
⋮----
flags.set_has_rent_epoch(optional_fields.rent_epoch.is_some());
flags.set_executable(false);
⋮----
pub struct AccountMetaOptionalFields {
⋮----
impl AccountMetaOptionalFields {
pub fn size(&self) -> usize {
self.rent_epoch.map_or(0, |_| std::mem::size_of::<Epoch>())
⋮----
pub fn size_from_flags(flags: &AccountMetaFlags) -> usize {
⋮----
if flags.has_rent_epoch() {
⋮----
pub fn rent_epoch_offset(_flags: &AccountMetaFlags) -> usize {
⋮----
pub struct AccountAddressRange {
⋮----
impl Default for AccountAddressRange {
fn default() -> Self {
⋮----
impl AccountAddressRange {
pub fn update(&mut self, address: &Pubkey) {
⋮----
pub mod tests {
⋮----
fn test_account_meta_flags_new() {
⋮----
assert!(!flags.has_rent_epoch());
assert_eq!(flags.reserved(), 0u32);
assert_eq!(
⋮----
fn verify_flags_serialization(flags: &AccountMetaFlags) {
assert_eq!(AccountMetaFlags::from_bytes(flags.into_bytes()), *flags);
⋮----
fn test_account_meta_flags_set() {
⋮----
flags.set_has_rent_epoch(true);
assert!(flags.has_rent_epoch());
assert!(!flags.executable());
verify_flags_serialization(&flags);
flags.set_executable(true);
⋮----
assert!(flags.executable());
⋮----
fn update_and_verify_flags(opt_fields: &AccountMetaOptionalFields) {
⋮----
assert_eq!(flags.has_rent_epoch(), opt_fields.rent_epoch.is_some());
⋮----
fn test_optional_fields_update_flags() {
⋮----
for rent_epoch in [None, Some(test_epoch)] {
update_and_verify_flags(&AccountMetaOptionalFields { rent_epoch });
⋮----
fn test_optional_fields_size() {
⋮----
fn test_optional_fields_offset() {
⋮----
let derived_size = if rent_epoch.is_some() {
⋮----
fn test_pubkey_range_update_single() {
⋮----
address_range.update(&address);
assert_eq!(address_range.min, address);
assert_eq!(address_range.max, address);
⋮----
fn test_pubkey_range_update_multiple() {
⋮----
addresses.push(address);
⋮----
.iter()
.for_each(|address| address_range.update(address));
assert_eq!(address_range.min, addresses[min_index]);
assert_eq!(address_range.max, addresses[max_index]);

================
File: accounts-db/src/tiered_storage/mmap_utils.rs
================
pub fn get_pod<T: bytemuck::AnyBitPattern>(mmap: &Mmap, offset: usize) -> io::Result<(&T, usize)> {
⋮----
pub unsafe fn get_type<T>(mmap: &Mmap, offset: usize) -> io::Result<(&T, usize)> {
let (data, next) = get_slice(mmap, offset, std::mem::size_of::<T>())?;
let ptr = data.as_ptr().cast();
debug_assert!((ptr as usize).is_multiple_of(std::mem::align_of::<T>()));
Ok((unsafe { &*ptr }, next))
⋮----
pub fn get_slice(mmap: &Mmap, offset: usize, size: usize) -> io::Result<(&[u8], usize)> {
let (next, overflow) = offset.overflowing_add(size);
if overflow || next > mmap.len() {
error!(
⋮----
return Err(std::io::Error::new(
⋮----
let next = u64_align!(next);
let ptr = data.as_ptr();
Ok((unsafe { std::slice::from_raw_parts(ptr, size) }, next))

================
File: accounts-db/src/tiered_storage/owners.rs
================
pub struct OwnerOffset(pub u32);
⋮----
pub enum OwnersBlockFormat {
⋮----
impl OwnersBlockFormat {
pub fn write_owners_block(
⋮----
bytes_written += file.write_pod(address)?;
⋮----
Ok(bytes_written)
⋮----
pub fn get_owner_address<'a>(
⋮----
Ok(pubkey)
⋮----
/// The in-memory representation of owners block for write.
/// It manages a set of unique addresses of account owners.
⋮----
/// It manages a set of unique addresses of account owners.
#[derive(Debug, Default)]
pub struct OwnersTable {
⋮----
/// OwnersBlock is persisted as a consecutive bytes of pubkeys without any
/// meta-data.  For each account meta, it has a owner_offset field to
⋮----
/// meta-data.  For each account meta, it has a owner_offset field to
/// access its owner's address in the OwnersBlock.
⋮----
/// access its owner's address in the OwnersBlock.
impl OwnersTable {
⋮----
impl OwnersTable {
pub fn insert(&mut self, pubkey: &Pubkey) -> OwnerOffset {
let (offset, _existed) = self.owners_set.insert_full(*pubkey);
OwnerOffset(offset as u32)
⋮----
pub fn len(&self) -> usize {
self.owners_set.len()
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
mod tests {
⋮----
fn test_owners_block() {
let temp_dir = TempDir::new().unwrap();
let path = temp_dir.path().join("test_owners_block");
⋮----
.take(NUM_OWNERS as usize)
.collect();
⋮----
let mut file = TieredWritableFile::new(&path).unwrap();
⋮----
addresses.iter().for_each(|owner_address| {
owners_table.insert(owner_address);
⋮----
.write_owners_block(&mut file, &owners_table)
.unwrap();
footer.write_footer_block(&mut file).unwrap();
⋮----
let file = OpenOptions::new().read(true).open(path).unwrap();
let mmap = unsafe { MmapOptions::new().map(&file).unwrap() };
for (i, address) in addresses.iter().enumerate() {
assert_eq!(
⋮----
fn test_owners_table() {
⋮----
.take(NUM_OWNERS)
⋮----
assert_eq!(owners_table.insert(address), OwnerOffset(i as u32));
⋮----
let cloned_addresses = addresses.clone();
for (i, address) in cloned_addresses.iter().enumerate() {
⋮----
assert_eq!(owners_table.owners_set.len(), addresses.len());

================
File: accounts-db/src/tiered_storage/readable.rs
================
pub enum TieredStorageReader {
⋮----
impl TieredStorageReader {
pub fn new_from_path(path: impl AsRef<Path>) -> TieredStorageResult<Self> {
⋮----
AccountMetaFormat::Hot => Ok(Self::Hot(HotStorageReader::new(file)?)),
⋮----
pub fn len(&self) -> usize {
⋮----
Self::Hot(hot) => hot.len(),
⋮----
pub fn is_empty(&self) -> bool {
⋮----
Self::Hot(hot) => hot.is_empty(),
⋮----
pub fn capacity(&self) -> u64 {
⋮----
Self::Hot(hot) => hot.capacity(),
⋮----
pub fn footer(&self) -> &TieredStorageFooter {
⋮----
Self::Hot(hot) => hot.footer(),
⋮----
pub fn num_accounts(&self) -> usize {
⋮----
Self::Hot(hot) => hot.num_accounts(),
⋮----
pub fn get_account_shared_data(
⋮----
Self::Hot(hot) => hot.get_account_shared_data(index_offset),
⋮----
pub fn get_stored_account_without_data_callback<Ret>(
⋮----
Self::Hot(hot) => hot.get_stored_account_without_data_callback(index_offset, callback),
⋮----
pub fn get_stored_account_callback<Ret>(
⋮----
Self::Hot(hot) => hot.get_stored_account_callback(index_offset, callback),
⋮----
pub fn scan_pubkeys(&self, callback: impl FnMut(&Pubkey)) -> TieredStorageResult<()> {
⋮----
Self::Hot(hot) => hot.scan_pubkeys(callback),
⋮----
pub fn scan_accounts_without_data(
⋮----
Self::Hot(hot) => hot.scan_accounts_without_data(callback),
⋮----
pub fn scan_accounts(
⋮----
Self::Hot(hot) => hot.scan_accounts(callback),
⋮----
pub(crate) fn calculate_stored_size(&self, data_len: usize) -> usize {
⋮----
pub(crate) fn get_account_data_lens(
⋮----
Self::Hot(hot) => hot.get_account_data_lens(sorted_offsets),
⋮----
pub fn data_for_archive(&self) -> &[u8] {
⋮----
Self::Hot(hot) => hot.data_for_archive(),

================
File: accounts-db/src/tiered_storage/test_utils.rs
================
pub(super) fn create_test_account(seed: u64) -> (Pubkey, AccountSharedData) {
⋮----
data: std::iter::repeat_n(data_byte, seed as usize).collect(),
owner: [owner_byte; 32].into(),
executable: !seed.is_multiple_of(2),
rent_epoch: if !seed.is_multiple_of(3) {
⋮----
pub(super) fn verify_test_account(
⋮----
(acc.lamports(), acc.owner(), acc.data(), acc.executable());
assert_eq!(stored_account.lamports(), lamports);
assert_eq!(stored_account.data().len(), data.len());
assert_eq!(stored_account.data(), data);
assert_eq!(stored_account.executable(), executable);
assert_eq!(stored_account.owner(), owner);
assert_eq!(stored_account.pubkey(), address);
⋮----
pub(super) fn verify_test_account_with_footer(
⋮----
verify_test_account(stored_account, account, address);
assert!(footer.min_account_address <= *address);
assert!(footer.max_account_address >= *address);

================
File: accounts-db/src/account_info.rs
================
pub type Offset = usize;
⋮----
pub enum StorageLocation {
⋮----
impl StorageLocation {
pub fn is_offset_equal(&self, other: &StorageLocation) -> bool {
⋮----
matches!(other, StorageLocation::Cached)
⋮----
pub fn is_store_id_equal(&self, other: &StorageLocation) -> bool {
⋮----
pub type OffsetReduced = u32;
⋮----
pub struct PackedOffsetAndFlags {
⋮----
pub struct AccountInfo {
⋮----
const _: () = assert!(size_of::<AccountInfo>() == 8);
impl IsZeroLamport for AccountInfo {
fn is_zero_lamport(&self) -> bool {
self.account_offset_and_flags.is_zero_lamport()
⋮----
impl IsCached for AccountInfo {
fn is_cached(&self) -> bool {
self.account_offset_and_flags.offset_reduced() == CACHED_OFFSET
⋮----
impl IndexValue for AccountInfo {}
impl DiskIndexValue for AccountInfo {}
impl IsCached for StorageLocation {
⋮----
matches!(self, StorageLocation::Cached)
⋮----
impl AccountInfo {
pub fn new(storage_location: StorageLocation, is_zero_lamport: bool) -> Self {
⋮----
assert_ne!(
⋮----
packed_offset_and_flags.set_offset_reduced(Self::get_reduced_offset(offset));
assert_eq!(
⋮----
packed_offset_and_flags.set_offset_reduced(CACHED_OFFSET);
⋮----
packed_offset_and_flags.set_is_zero_lamport(is_zero_lamport);
⋮----
pub fn get_reduced_offset(offset: usize) -> OffsetReduced {
⋮----
pub fn store_id(&self) -> AccountsFileId {
assert!(!self.is_cached());
⋮----
pub fn offset(&self) -> Offset {
Self::reduced_offset_to_offset(self.account_offset_and_flags.offset_reduced())
⋮----
pub fn reduced_offset_to_offset(reduced_offset: OffsetReduced) -> Offset {
⋮----
pub fn storage_location(&self) -> StorageLocation {
if self.is_cached() {
⋮----
StorageLocation::AppendVec(self.store_id, self.offset())
⋮----
mod test {
⋮----
fn test_limits() {
⋮----
assert!(info.offset() == offset);
⋮----
fn test_illegal_offset() {
⋮----
fn test_alignment() {

================
File: accounts-db/src/account_locks.rs
================
use qualifier_attr::qualifiers;
⋮----
pub struct AccountLocks {
⋮----
impl AccountLocks {
pub fn try_lock_accounts<'a>(
⋮----
self.can_lock_accounts(keys.clone())?;
self.lock_accounts(keys);
Ok(())
⋮----
pub fn try_lock_transaction_batch<'a>(
⋮----
validated_batch_keys.iter_mut().for_each(|validated_keys| {
if let Ok(keys) = validated_keys.as_ref() {
if let Err(e) = self.can_lock_accounts(keys.clone()) {
*validated_keys = Err(e);
⋮----
.into_iter()
.map(|available_keys| available_keys.map(|keys| self.lock_accounts(keys)))
.collect()
⋮----
pub fn unlock_accounts<'a>(&mut self, keys: impl Iterator<Item = (&'a Pubkey, bool)>) {
⋮----
self.unlock_write(k);
⋮----
self.unlock_readonly(k);
⋮----
fn can_lock_accounts<'a>(
⋮----
if !self.can_write_lock(key) {
return Err(TransactionError::AccountInUse);
⋮----
} else if !self.can_read_lock(key) {
⋮----
fn lock_accounts<'a>(&mut self, keys: impl Iterator<Item = (&'a Pubkey, bool)>) {
⋮----
self.lock_write(key);
⋮----
self.lock_readonly(key);
⋮----
fn is_locked_readonly(&self, key: &Pubkey) -> bool {
self.readonly_locks.get(key).is_some_and(|count| *count > 0)
⋮----
fn is_locked_write(&self, key: &Pubkey) -> bool {
self.write_locks.get(key).is_some_and(|count| *count > 0)
⋮----
fn can_read_lock(&self, key: &Pubkey) -> bool {
!self.is_locked_write(key)
⋮----
fn can_write_lock(&self, key: &Pubkey) -> bool {
!self.is_locked_readonly(key) && !self.is_locked_write(key)
⋮----
fn lock_readonly(&mut self, key: &Pubkey) {
*self.readonly_locks.entry(*key).or_default() += 1;
⋮----
fn lock_write(&mut self, key: &Pubkey) {
*self.write_locks.entry(*key).or_default() += 1;
⋮----
fn unlock_readonly(&mut self, key: &Pubkey) {
if let hash_map::Entry::Occupied(mut occupied_entry) = self.readonly_locks.entry(*key) {
let count = occupied_entry.get_mut();
⋮----
occupied_entry.remove_entry();
⋮----
debug_assert!(
⋮----
fn unlock_write(&mut self, key: &Pubkey) {
if let hash_map::Entry::Occupied(mut occupied_entry) = self.write_locks.entry(*key) {
⋮----
pub fn validate_account_locks(
⋮----
if account_keys.len() > tx_account_lock_limit {
Err(TransactionError::TooManyAccountLocks)
} else if has_duplicates(account_keys) {
Err(TransactionError::AccountLoadedTwice)
⋮----
thread_local! {
⋮----
fn has_duplicates(account_keys: AccountKeys) -> bool {
⋮----
if account_keys.len() >= USE_ACCOUNT_LOCK_SET_SIZE {
HAS_DUPLICATES_SET.with_borrow_mut(|set| {
let has_duplicates = account_keys.iter().any(|key| !set.insert(*key));
set.clear();
⋮----
for (idx, key) in account_keys.iter().enumerate() {
for jdx in idx + 1..account_keys.len() {
⋮----
mod tests {
⋮----
fn test_account_locks() {
⋮----
let result = account_locks.try_lock_accounts([(&key1, true), (&key2, false)].into_iter());
assert!(result.is_ok());
let result = account_locks.try_lock_accounts([(&key1, true)].into_iter());
assert_eq!(result, Err(TransactionError::AccountInUse));
let result = account_locks.try_lock_accounts([(&key2, true)].into_iter());
⋮----
let result = account_locks.try_lock_accounts([(&key1, false)].into_iter());
⋮----
let result = account_locks.try_lock_accounts([(&key2, false)].into_iter());
⋮----
account_locks.unlock_accounts([(&key1, true), (&key2, false)].into_iter());
assert!(!account_locks.is_locked_write(&key1));
assert!(account_locks.is_locked_readonly(&key2));
account_locks.unlock_accounts([(&key2, false)].into_iter());
assert!(!account_locks.is_locked_readonly(&key2));
⋮----
fn test_validate_account_locks_valid_no_dynamic() {
⋮----
assert!(validate_account_locks(account_keys, MAX_TX_ACCOUNT_LOCKS).is_ok());
⋮----
fn test_validate_account_locks_too_many_no_dynamic() {
⋮----
assert_eq!(
⋮----
fn test_validate_account_locks_duplicate_no_dynamic() {
⋮----
fn test_validate_account_locks_valid_dynamic() {
⋮----
writable: vec![Pubkey::new_unique()],
readonly: vec![Pubkey::new_unique()],
⋮----
let account_keys = AccountKeys::new(static_keys, Some(&dynamic_keys));
⋮----
fn test_validate_account_locks_too_many_dynamic() {
⋮----
fn test_validate_account_locks_duplicate_dynamic() {
⋮----
readonly: vec![duplicate_key],
⋮----
fn test_has_duplicates_small() {
let mut keys = (0..16).map(|_| Pubkey::new_unique()).collect::<Vec<_>>();
⋮----
assert!(!has_duplicates(account_keys));
⋮----
assert!(has_duplicates(account_keys));
⋮----
fn test_has_duplicates_large() {
let mut keys = (0..64).map(|_| Pubkey::new_unique()).collect::<Vec<_>>();

================
File: accounts-db/src/account_storage_reader.rs
================
pub struct AccountStorageReader<'a> {
⋮----
pub fn new(storage: &'a AccountStorageEntry, snapshot_slot: Option<Slot>) -> io::Result<Self> {
let internals = storage.accounts.internals_for_archive();
let num_total_bytes = storage.accounts.len();
let num_alive_bytes = num_total_bytes - storage.get_obsolete_bytes(snapshot_slot);
⋮----
.obsolete_accounts_read_lock()
.filter_obsolete_accounts(snapshot_slot)
.collect();
// Tiered storage is not compatible with obsolete accounts at this time
if matches!(storage.accounts, AccountsFile::TieredStorage(_)) {
assert!(
⋮----
// Convert the length to the size
⋮----
.iter_mut()
.for_each(|(_offset, len)| {
*len = storage.accounts.calculate_stored_size(*len);
⋮----
.sort_unstable_by(|(a_offset, _), (b_offset, _)| b_offset.cmp(a_offset));
⋮----
InternalsForArchive::FileIo(path) => Some(File::open(path)?),
⋮----
Ok(Self {
⋮----
pub fn len(&self) -> usize {
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
impl Read for AccountStorageReader<'_> {
fn read(&mut self, buf: &mut [u8]) -> io::Result<usize> {
⋮----
let buf_len = buf.len();
⋮----
let next_obsolete_account = self.sorted_obsolete_accounts.last();
⋮----
self.current_offset += obsolete_size.min(self.num_total_bytes - obsolete_start);
self.sorted_obsolete_accounts.pop();
⋮----
let bytes_left_in_buffer = buf_len.saturating_sub(total_read);
⋮----
obsolete_start.saturating_sub(self.current_offset)
⋮----
self.num_total_bytes.saturating_sub(self.current_offset)
⋮----
let bytes_to_read = bytes_left_in_buffer.min(bytes_to_read_from_file);
⋮----
.read(&mut buf[total_read..][..bytes_to_read])?,
⋮----
.as_mut()
.expect("File is opened during initialization");
file.seek(SeekFrom::Start(self.current_offset as u64))?;
file.read(&mut buf[total_read..][..bytes_to_read])?
⋮----
Ok(total_read)
⋮----
mod tests {
⋮----
fn create_storage_for_storage_reader(
⋮----
let (temp_dirs, paths) = get_temp_accounts_paths(1).unwrap();
⋮----
fn test_account_storage_reader_tiered_storage_one_obsolete_account_should_panic(
⋮----
create_storage_for_storage_reader(0, AccountsFileProvider::HotStorage, storage_access);
⋮----
storage.accounts.write_accounts(&(slot, &accounts[..]), 0);
⋮----
let mut size = storage.accounts.get_account_data_lens(&[0]);
⋮----
.obsolete_accounts()
.write()
.unwrap()
.mark_accounts_obsolete(vec![(offset, size.pop().unwrap())].into_iter(), 0);
_ = AccountStorageReader::new(&storage, None).unwrap();
⋮----
fn test_account_storage_reader_no_obsolete_accounts(
⋮----
let (storage, _temp_dirs) = create_storage_for_storage_reader(0, provider, storage_access);
⋮----
let reader = AccountStorageReader::new(&storage, None).unwrap();
assert_eq!(reader.len(), storage.accounts.len());
⋮----
fn test_account_storage_reader_with_obsolete_accounts(
⋮----
create_storage_for_storage_reader(0, AccountsFileProvider::AppendVec, storage_access);
⋮----
.take(total_accounts)
⋮----
.into_iter()
.map(|account| (Pubkey::new_unique(), account))
⋮----
.write_accounts(&(slot, &accounts_to_append[..]), 0);
⋮----
info!("Generated seed: {seed}");
⋮----
.map(|offsets| {
⋮----
.choose_multiple(&mut rng, number_of_accounts_to_remove)
.cloned()
⋮----
.unwrap_or_default();
assert_eq!(obsolete_account_offset.len(), number_of_accounts_to_remove);
⋮----
.get_account_data_lens(&obsolete_account_offset);
⋮----
.mark_accounts_obsolete(obsolete_account_offset.into_iter().zip(data_lens), 0);
⋮----
.reopen_as_readonly(storage_access)
.unwrap_or(storage);
⋮----
StorageAccess::File => assert!(matches!(
⋮----
StorageAccess::Mmap => assert!(matches!(
⋮----
let mut reader = AccountStorageReader::new(&storage, None).unwrap();
let current_len = storage.accounts.len() - storage.get_obsolete_bytes(None);
assert_eq!(reader.len(), current_len);
let temp_dir = tempfile::tempdir().unwrap();
let temp_file_path = temp_dir.path().join("output_file");
let mut output_file = File::create(&temp_file_path).unwrap();
let bytes_written = io::copy(&mut reader, &mut output_file).unwrap();
assert_eq!(bytes_written as usize, reader.len());
drop(output_file);
⋮----
.unwrap();
assert_eq!(
⋮----
assert_eq!(new_storage.accounts.len(), reader.len());
⋮----
fn test_account_storage_reader_filter_by_slot(storage_access: StorageAccess) {
⋮----
create_storage_for_storage_reader(10, AccountsFileProvider::AppendVec, storage_access);
⋮----
.as_ref()
.and_then(|offsets| offsets.offsets.iter().max().cloned())
⋮----
.choose_multiple(&mut rng, total_accounts - 1)
⋮----
if !obsolete_account_offset.contains(&max_offset) {
if let Some(random_index) = obsolete_account_offset.choose_mut(&mut rng) {
⋮----
obsolete_account_offset.into_iter().for_each(|offset| {
let mut size = storage.accounts.get_account_data_lens(&[offset]);
⋮----
.mark_accounts_obsolete(
vec![(offset, size.pop().unwrap())].into_iter(),
⋮----
let mut reader = AccountStorageReader::new(&storage, Some(snapshot_slot)).unwrap();
⋮----
storage.accounts.len() - storage.get_obsolete_bytes(Some(snapshot_slot));

================
File: accounts-db/src/account_storage.rs
================
pub mod stored_account_info;
pub type AccountStorageMap = DashMap<Slot, Arc<AccountStorageEntry>>;
⋮----
pub struct AccountStorage {
⋮----
impl AccountStorage {
pub(crate) fn get_account_storage_entry(
⋮----
self.map.get(&slot).and_then(|entry| {
(entry.value().id() == store_id).then_some(Arc::clone(entry.value()))
⋮----
lookup_in_map()
.or_else(|| {
⋮----
.read()
.unwrap()
.get(&slot)
.and_then(|entry| (entry.id() == store_id).then(|| Arc::clone(entry)))
⋮----
.or_else(lookup_in_map)
⋮----
pub(crate) fn no_shrink_in_progress(&self) -> bool {
self.shrink_in_progress_map.read().unwrap().is_empty()
⋮----
pub fn get_slot_storage_entry(&self, slot: Slot) -> Option<Arc<AccountStorageEntry>> {
assert!(
⋮----
self.get_slot_storage_entry_shrinking_in_progress_ok(slot)
⋮----
pub(super) fn all_storages(&self) -> Vec<Arc<AccountStorageEntry>> {
assert!(self.no_shrink_in_progress());
⋮----
.iter()
.map(|item| Arc::clone(item.value()))
.collect()
⋮----
pub(crate) fn replace_storage_with_equivalent(
⋮----
assert_eq!(storage.slot(), slot);
if let Some(mut existing_storage) = self.map.get_mut(&slot) {
assert_eq!(slot, existing_storage.value().slot());
*existing_storage.value_mut() = storage;
⋮----
pub(crate) fn get_slot_storage_entry_shrinking_in_progress_ok(
⋮----
self.map.get(&slot).map(|entry| Arc::clone(entry.value()))
⋮----
pub(crate) fn all_slots(&self) -> Vec<Slot> {
⋮----
self.map.iter().map(|iter_item| *iter_item.key()).collect()
⋮----
pub(crate) fn is_empty_entry(&self, slot: Slot) -> bool {
⋮----
self.map.get(&slot).is_none()
⋮----
pub fn initialize(&mut self, all_storages: AccountStorageMap) {
assert!(self.map.is_empty());
⋮----
pub(crate) fn remove(
⋮----
assert!(shrink_can_be_active || self.shrink_in_progress_map.read().unwrap().is_empty());
self.map.remove(slot).map(|(_, storage)| storage)
⋮----
pub(crate) fn iter(&self) -> AccountStorageIter<'_> {
⋮----
pub(crate) fn insert(&self, slot: Slot, store: Arc<AccountStorageEntry>) {
⋮----
assert!(self.map.insert(slot, store).is_none());
⋮----
/// called when shrinking begins on a slot and append vec.
    /// When 'ShrinkInProgress' is dropped by caller, the old store will be replaced with 'new_store' in the storage map.
⋮----
/// When 'ShrinkInProgress' is dropped by caller, the old store will be replaced with 'new_store' in the storage map.
    pub(crate) fn shrinking_in_progress(
⋮----
pub(crate) fn shrinking_in_progress(
⋮----
.expect("no pre-existing storage for shrinking slot")
.value(),
⋮----
// insert 'new_store' into 'shrink_in_progress_map'
⋮----
pub(crate) fn len(&self) -> usize {
self.map.len()
⋮----
pub fn get_if(
⋮----
.filter_map(|entry| {
let slot = entry.key();
let storage = entry.value();
predicate(slot, storage).then(|| (*slot, Arc::clone(storage)))
⋮----
pub struct AccountStorageIter<'a> {
⋮----
pub fn new(storage: &'a AccountStorage) -> Self {
⋮----
iter: storage.map.iter(),
⋮----
impl Iterator for AccountStorageIter<'_> {
type Item = (Slot, Arc<AccountStorageEntry>);
fn next(&mut self) -> Option<Self::Item> {
if let Some(entry) = self.iter.next() {
⋮----
let store = entry.value();
return Some((*slot, Arc::clone(store)));
⋮----
pub struct ShrinkInProgress<'a> {
⋮----
impl Drop for ShrinkInProgress<'_> {
fn drop(&mut self) {
assert_eq!(
⋮----
// The new store can be removed from 'shrink_in_progress_map'
assert!(self
⋮----
pub fn new_storage(&self) -> &Arc<AccountStorageEntry> {
⋮----
pub(crate) fn old_storage(&self) -> &Arc<AccountStorageEntry> {
⋮----
/// Wrapper over slice of `Arc<AccountStorageEntry>` that provides an ordered access to storages.
///
⋮----
///
/// A few strategies are available for ordering storages:
⋮----
/// A few strategies are available for ordering storages:
/// - `with_small_to_large_ratio`: interleaving small and large storage file sizes
⋮----
/// - `with_small_to_large_ratio`: interleaving small and large storage file sizes
/// - `with_random_order`: orders storages randomly
⋮----
/// - `with_random_order`: orders storages randomly
pub struct AccountStoragesOrderer<'a> {
⋮----
pub struct AccountStoragesOrderer<'a> {
⋮----
/// Create balancing orderer that interleaves storages with small and large file sizes.
    ///
⋮----
///
    /// Storages are returned in cycles based on `small_to_large_ratio` - `ratio.0` small storages
⋮----
/// Storages are returned in cycles based on `small_to_large_ratio` - `ratio.0` small storages
    /// preceding `ratio.1` large storages.
⋮----
/// preceding `ratio.1` large storages.
    pub fn with_small_to_large_ratio(
⋮----
pub fn with_small_to_large_ratio(
⋮----
let len_range = 0..storages.len();
let mut indices: Vec<_> = len_range.clone().collect();
indices.sort_unstable_by_key(|i| storages[*i].capacity());
indices.iter_mut().for_each(|i| {
*i = select_from_range_with_start_end_rates(len_range.clone(), *i, small_to_large_ratio)
⋮----
indices: indices.into_boxed_slice(),
⋮----
pub fn with_random_order(storages: &'a [Arc<AccountStorageEntry>]) -> Self {
let mut indices: Vec<usize> = (0..storages.len()).collect();
indices.shuffle(&mut rand::rng());
⋮----
pub fn entries_len(&self) -> usize {
self.indices.len()
⋮----
/// Returns the original index, into the storages slice, at `position`
    ///
⋮----
///
    /// # Panics
⋮----
/// # Panics
    ///
⋮----
///
    /// Caller must ensure `position` is in range, else will panic.
⋮----
/// Caller must ensure `position` is in range, else will panic.
    pub fn original_index(&'a self, position: usize) -> usize {
⋮----
pub fn original_index(&'a self, position: usize) -> usize {
⋮----
pub fn iter(&'a self) -> impl ExactSizeIterator<Item = &'a AccountStorageEntry> + 'a {
self.indices.iter().map(|i| self.storages[*i].as_ref())
⋮----
pub fn par_iter(&'a self) -> impl IndexedParallelIterator<Item = &'a AccountStorageEntry> + 'a {
self.indices.par_iter().map(|i| self.storages[*i].as_ref())
⋮----
pub fn into_concurrent_consumer(self) -> AccountStoragesConcurrentConsumer<'a> {
⋮----
type Output = AccountStorageEntry;
fn index(&self, position: usize) -> &Self::Output {
let original_index = self.original_index(position);
self.storages[original_index].as_ref()
⋮----
pub struct AccountStoragesConcurrentConsumer<'a> {
⋮----
pub fn new(orderer: AccountStoragesOrderer<'a>) -> Self {
⋮----
/// Takes the next `AccountStorageEntry` moving shared consume position
    /// until the end of the entries source is reached.
⋮----
/// until the end of the entries source is reached.
    pub fn next(&'a self) -> Option<NextItem<'a>> {
⋮----
pub fn next(&'a self) -> Option<NextItem<'a>> {
let position = self.current_position.fetch_add(1, Ordering::Relaxed);
if position < self.orderer.entries_len() {
// SAFETY: We have ensured `position` is in range.
let original_index = self.orderer.original_index(position);
⋮----
Some(NextItem {
⋮----
/// Value returned from calling `AccountStoragesConcurrentConsumer::next()`
#[derive(Debug)]
pub struct NextItem<'a> {
⋮----
/// Select the `nth` (`0 <= nth < range.len()`) value from a `range`, choosing values alternately
/// from its start or end according to a `start_rate : end_rate` ratio.
⋮----
/// from its start or end according to a `start_rate : end_rate` ratio.
///
⋮----
///
/// For every `start_rate` values selected from the start, `end_rate` values are selected from the end.
⋮----
/// For every `start_rate` values selected from the start, `end_rate` values are selected from the end.
/// The resulting sequence alternates in a balanced and interleaved fashion between the range's start and end.
⋮----
/// The resulting sequence alternates in a balanced and interleaved fashion between the range's start and end.
fn select_from_range_with_start_end_rates(
⋮----
fn select_from_range_with_start_end_rates(
⋮----
let range_len = range.len();
⋮----
let cycle_num = nth.checked_div(cycle).expect("rates sum must be positive");
⋮----
pub(crate) mod tests {
⋮----
fn test_shrink_in_progress(storage_access: StorageAccess) {
⋮----
assert!(storage.get_account_storage_entry(slot, id).is_none());
⋮----
// 2 append vecs with same id, but different sizes
⋮----
storage.map.insert(slot, entry);
// look in map
⋮----
// look in shrink_in_progress_map
⋮----
.write()
⋮----
.insert(slot, entry2);
⋮----
// remove from map
storage.map.remove(&slot).unwrap();
⋮----
fn get_test_storage_with_id(
⋮----
// add a map store
⋮----
fn get_test_storage(&self, storage_access: StorageAccess) -> Arc<AccountStorageEntry> {
self.get_test_storage_with_id(0, storage_access)
⋮----
fn test_get_slot_storage_entry_fail(storage_access: StorageAccess) {
⋮----
.insert(0, storage.get_test_storage(storage_access));
storage.get_slot_storage_entry(0);
⋮----
fn test_all_slots_fail(storage_access: StorageAccess) {
⋮----
storage.all_slots();
⋮----
fn test_initialize_fail(storage_access: StorageAccess) {
⋮----
storage.initialize(AccountStorageMap::default());
⋮----
fn test_remove_fail(storage_access: StorageAccess) {
⋮----
storage.remove(&0, false);
⋮----
fn test_iter_fail(storage_access: StorageAccess) {
⋮----
storage.iter();
⋮----
fn test_insert_fail(storage_access: StorageAccess) {
⋮----
let sample = storage.get_test_storage(storage_access);
⋮----
.insert(0, sample.clone());
storage.insert(0, sample);
⋮----
fn test_shrinking_in_progress_fail3(storage_access: StorageAccess) {
⋮----
storage.map.insert(0, sample.clone());
⋮----
storage.shrinking_in_progress(0, sample);
⋮----
fn test_shrinking_in_progress_fail4(storage_access: StorageAccess) {
⋮----
let sample_to_shrink = storage.get_test_storage(storage_access);
⋮----
storage.map.insert(0, sample_to_shrink);
let _shrinking_in_progress = storage.shrinking_in_progress(0, sample.clone());
⋮----
fn test_shrinking_in_progress_second_call(storage_access: StorageAccess) {
⋮----
let sample_to_shrink = storage.get_test_storage_with_id(id_to_shrink, storage_access);
⋮----
storage.map.insert(slot, sample_to_shrink);
let shrinking_in_progress = storage.shrinking_in_progress(slot, sample.clone());
assert!(storage.map.contains_key(&slot));
assert_eq!(id_to_shrink, storage.map.get(&slot).unwrap().id());
⋮----
drop(shrinking_in_progress);
⋮----
assert_eq!(id_shrunk, storage.map.get(&slot).unwrap().id());
assert!(storage.shrink_in_progress_map.read().unwrap().is_empty());
storage.shrinking_in_progress(slot, sample);
⋮----
fn test_shrinking_in_progress_fail1(storage_access: StorageAccess) {
⋮----
fn test_shrinking_in_progress_fail2(storage_access: StorageAccess) {
⋮----
fn test_missing(storage_access: StorageAccess) {
⋮----
let id = sample.id();
⋮----
let slot = sample.slot();
⋮----
assert!(storage
⋮----
storage.map.insert(slot, sample.clone());
assert!(storage.get_account_storage_entry(slot, id).is_some());
⋮----
.insert(slot, Arc::clone(&sample));
⋮----
storage.map.remove(&slot);
⋮----
fn test_get_if(storage_access: StorageAccess) {
⋮----
assert!(storage.get_if(|_, _| true).is_empty());
⋮----
storage.map.insert(slot, entry.into());
⋮----
// look 'em up
⋮----
let found = storage.get_if(|slot, _| *slot == id as Slot);
assert!(found
⋮----
assert!(storage.get_if(|_, _| false).is_empty());
assert_eq!(storage.get_if(|_, _| true).len(), ids.len());
⋮----
fn test_get_if_fail(storage_access: StorageAccess) {
⋮----
storage.get_if(|_, _| true);
⋮----
fn test_select_range_with_start_end_rates() {
⋮----
.map(|i| select_from_range_with_start_end_rates(1..11, i, (2, 1)))
.collect();
assert_eq!(interleaved, vec![1, 2, 10, 3, 4, 9, 5, 6, 8, 7]);
⋮----
.map(|i| select_from_range_with_start_end_rates(1..11, i, (1, 1)))
⋮----
assert_eq!(interleaved, vec![1, 10, 2, 9, 3, 8, 4, 7, 5, 6]);
⋮----
.map(|i| select_from_range_with_start_end_rates(1..10, i, (2, 1)))
⋮----
assert_eq!(interleaved, vec![1, 2, 9, 3, 4, 8, 5, 6, 7]);
⋮----
.map(|i| select_from_range_with_start_end_rates(1..10, i, (1, 2)))
⋮----
assert_eq!(interleaved, vec![1, 9, 8, 2, 7, 6, 3, 5, 4]);
⋮----
.map(|i| select_from_range_with_start_end_rates(1..14, i, (2, 3)))
⋮----
assert_eq!(interleaved, vec![1, 2, 13, 12, 11, 3, 4, 10, 9, 8, 5, 6, 7]);

================
File: accounts-db/src/accounts_cache.rs
================
pub struct SlotCache {
⋮----
impl Drop for SlotCache {
fn drop(&mut self) {
⋮----
.fetch_sub(*self.size.get_mut(), Ordering::Relaxed);
⋮----
.fetch_sub(*self.accounts_count.get_mut(), Ordering::Relaxed);
⋮----
impl SlotCache {
pub fn report_slot_store_metrics(&self) {
datapoint_info!(
⋮----
pub fn insert(&self, pubkey: &Pubkey, account: AccountSharedData) -> Arc<CachedAccount> {
let data_len = account.data().len() as u64;
⋮----
if let Some(old) = self.cache.insert(*pubkey, item.clone()) {
self.same_account_writes.fetch_add(1, Ordering::Relaxed);
⋮----
.fetch_add(data_len, Ordering::Relaxed);
let old_len = old.account.data().len() as u64;
let grow = data_len.saturating_sub(old_len);
⋮----
self.size.fetch_add(grow, Ordering::Relaxed);
self.total_size.fetch_add(grow, Ordering::Relaxed);
⋮----
let shrink = old_len.saturating_sub(data_len);
⋮----
self.size.fetch_sub(shrink, Ordering::Relaxed);
self.total_size.fetch_sub(shrink, Ordering::Relaxed);
⋮----
self.size.fetch_add(data_len, Ordering::Relaxed);
self.total_size.fetch_add(data_len, Ordering::Relaxed);
⋮----
self.accounts_count.fetch_add(1, Ordering::Relaxed);
self.total_accounts_count.fetch_add(1, Ordering::Relaxed);
⋮----
pub fn get_cloned(&self, pubkey: &Pubkey) -> Option<Arc<CachedAccount>> {
⋮----
.get(pubkey)
.map(|account_ref| account_ref.value().clone())
⋮----
pub fn mark_slot_frozen(&self) {
self.is_frozen.store(true, Ordering::Release);
⋮----
pub fn is_frozen(&self) -> bool {
self.is_frozen.load(Ordering::Acquire)
⋮----
pub fn total_bytes(&self) -> u64 {
self.unique_account_writes_size.load(Ordering::Relaxed)
+ self.same_account_writes_size.load(Ordering::Relaxed)
⋮----
impl Deref for SlotCache {
type Target = DashMap<Pubkey, Arc<CachedAccount>, PubkeyHasherBuilder>;
fn deref(&self) -> &Self::Target {
⋮----
pub struct CachedAccount {
⋮----
impl CachedAccount {
pub fn pubkey(&self) -> &Pubkey {
⋮----
pub struct AccountsCache {
⋮----
impl AccountsCache {
pub fn new_inner(&self) -> Arc<SlotCache> {
⋮----
pub fn size(&self) -> u64 {
self.total_size.load(Ordering::Relaxed)
⋮----
pub fn report_size(&self) {
⋮----
pub fn store(
⋮----
let slot_cache = self.slot_cache(slot).unwrap_or_else(||
⋮----
.entry(slot)
.or_insert_with(|| self.new_inner())
.clone());
slot_cache.insert(pubkey, account)
⋮----
pub fn load(&self, slot: Slot, pubkey: &Pubkey) -> Option<Arc<CachedAccount>> {
self.slot_cache(slot)
.and_then(|slot_cache| slot_cache.get_cloned(pubkey))
⋮----
pub fn remove_slot(&self, slot: Slot) -> Option<Arc<SlotCache>> {
self.cache.remove(&slot).map(|(_, slot_cache)| slot_cache)
⋮----
pub fn slot_cache(&self, slot: Slot) -> Option<Arc<SlotCache>> {
self.cache.get(&slot).map(|result| result.value().clone())
⋮----
pub fn add_root(&self, root: Slot) {
self.maybe_unflushed_roots.write().unwrap().insert(root);
⋮----
pub fn clear_roots(&self, max_root: Option<Slot>) -> BTreeSet<Slot> {
let mut w_maybe_unflushed_roots = self.maybe_unflushed_roots.write().unwrap();
⋮----
let greater_than_max_root = w_maybe_unflushed_roots.split_off(&(max_root + 1));
⋮----
pub fn cached_frozen_slots(&self) -> Vec<Slot> {
⋮----
.iter()
.filter_map(|item| {
let (slot, slot_cache) = item.pair();
slot_cache.is_frozen().then_some(*slot)
⋮----
.collect()
⋮----
pub fn contains(&self, slot: Slot) -> bool {
self.cache.contains_key(&slot)
⋮----
pub fn num_slots(&self) -> usize {
self.cache.len()
⋮----
pub fn fetch_max_flush_root(&self) -> Slot {
self.max_flushed_root.load(Ordering::Acquire)
⋮----
pub fn set_max_flush_root(&self, root: Slot) {
self.max_flushed_root.fetch_max(root, Ordering::Release);
⋮----
pub mod tests {
⋮----
pub fn remove_slots_le(&self, max_root: Slot) -> Vec<(Slot, Arc<SlotCache>)> {
let mut removed_slots = vec![];
self.cache.retain(|slot, slot_cache| {
⋮----
removed_slots.push((*slot, slot_cache.clone()))
⋮----
fn test_remove_slots_le() {
⋮----
assert!(cache.remove_slots_le(1).is_empty());
⋮----
cache.store(
⋮----
let removed = cache.remove_slots_le(0);
assert_eq!(removed.len(), 1);
assert_eq!(removed[0].0, inserted_slot);
⋮----
fn test_cached_frozen_slots() {
⋮----
assert!(cache.cached_frozen_slots().is_empty());
⋮----
cache.slot_cache(inserted_slot).unwrap().mark_slot_frozen();
assert_eq!(cache.cached_frozen_slots(), vec![inserted_slot]);

================
File: accounts-db/src/accounts_db.rs
================
mod accounts_db_config;
mod geyser_plugin_utils;
pub mod stats;
pub mod tests;
⋮----
use qualifier_attr::qualifiers;
⋮----
pub(crate) enum ScanAccountStorageData {
⋮----
pub(crate) struct AliveAccounts<'a> {
/// slot the accounts are currently stored in
    pub(crate) slot: Slot,
⋮----
pub(crate) struct ShrinkCollectAliveSeparatedByRefs<'a> {
/// accounts where ref_count = 1
    pub(crate) one_ref: AliveAccounts<'a>,
⋮----
/// account where ref_count > 1, and this slot is NOT the highest alive entry in the index for the pubkey
    pub(crate) many_refs_old_alive: AliveAccounts<'a>,
⋮----
pub(crate) trait ShrinkCollectRefs<'a>: Sync + Send {
⋮----
fn collect(&mut self, mut other: Self) {
self.bytes = self.bytes.saturating_add(other.bytes);
self.accounts.append(&mut other.accounts);
⋮----
fn with_capacity(capacity: usize, slot: Slot) -> Self {
⋮----
fn add(
⋮----
self.accounts.push(account);
self.bytes = self.bytes.saturating_add(account.stored_size());
⋮----
fn len(&self) -> usize {
self.accounts.len()
⋮----
fn alive_bytes(&self) -> usize {
⋮----
fn alive_accounts(&self) -> &Vec<&'a AccountFromStorage> {
⋮----
fn collect(&mut self, other: Self) {
self.one_ref.collect(other.one_ref);
⋮----
.collect(other.many_refs_this_is_newest_alive);
self.many_refs_old_alive.collect(other.many_refs_old_alive);
⋮----
} else if slot_list.len() == 1
⋮----
.iter()
.any(|(slot_list_slot, _info)| slot_list_slot > &self.many_refs_old_alive.slot)
⋮----
other.add(ref_count, account, slot_list);
⋮----
.len()
.saturating_add(self.many_refs_old_alive.len())
.saturating_add(self.many_refs_this_is_newest_alive.len())
⋮----
.alive_bytes()
.saturating_add(self.many_refs_old_alive.alive_bytes())
.saturating_add(self.many_refs_this_is_newest_alive.alive_bytes())
⋮----
unimplemented!("illegal use");
⋮----
pub enum StoreReclaims {
/// normal reclaim mode
    Default,
/// do not return reclaims from accounts index upsert
    Ignore,
⋮----
/// specifies how to return zero lamport accounts from a load
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum LoadZeroLamports {
/// return None if loaded account has zero lamports
    None,
/// return Some(account with zero lamports) if loaded account has zero lamports
    /// This used to be the only behavior.
⋮----
/// This used to be the only behavior.
    /// Note that this is non-deterministic if clean is running asynchronously.
⋮----
/// Note that this is non-deterministic if clean is running asynchronously.
    /// If a zero lamport account exists in the index, then Some is returned.
⋮----
/// If a zero lamport account exists in the index, then Some is returned.
    /// Once it is cleaned from the index, None is returned.
⋮----
/// Once it is cleaned from the index, None is returned.
    #[cfg(feature = "dev-context-only-utils")]
⋮----
pub(crate) struct ShrinkCollect<'a, T: ShrinkCollectRefs<'a>> {
⋮----
/// total size in storage of all alive accounts
    pub(crate) alive_total_bytes: usize,
⋮----
/// true if all alive accounts are zero lamports
    pub(crate) all_are_zero_lamports: bool,
⋮----
struct LoadAccountsIndexForShrink<'a, T: ShrinkCollectRefs<'a>> {
/// all alive accounts
    alive_accounts: T,
/// pubkeys that are going to be unref'd in the accounts index after we are
    pubkeys_to_unref: Vec<&'a Pubkey>,
/// pubkeys that are the last remaining zero lamport instance of an account
    zero_lamport_single_ref_pubkeys: Vec<&'a Pubkey>,
⋮----
pub struct AccountFromStorage {
⋮----
impl IsZeroLamport for AccountFromStorage {
fn is_zero_lamport(&self) -> bool {
self.index_info.is_zero_lamport()
⋮----
impl AccountFromStorage {
pub fn pubkey(&self) -> &Pubkey {
⋮----
pub fn stored_size(&self) -> usize {
aligned_stored_size(self.data_len as usize)
⋮----
pub fn data_len(&self) -> usize {
⋮----
pub(crate) fn new(offset: Offset, account: &StoredAccountInfoWithoutData) -> Self {
⋮----
account.is_zero_lamport(),
⋮----
pubkey: *account.pubkey(),
⋮----
pub struct GetUniqueAccountsResult {
⋮----
pub struct AccountsAddRootTiming {
⋮----
const ANCIENT_APPEND_VEC_DEFAULT_OFFSET: Option<i64> = Some(100_000);
⋮----
pub enum AccountShrinkThreshold {
⋮----
impl Default for AccountShrinkThreshold {
fn default() -> AccountShrinkThreshold {
⋮----
pub enum ScanStorageResult<R, B> {
⋮----
pub struct IndexGenerationInfo {
⋮----
struct SlotIndexGenerationInfo {
⋮----
pub struct DuplicatesLtHash(pub LtHash);
impl Default for DuplicatesLtHash {
fn default() -> Self {
Self(LtHash::identity())
⋮----
struct SlotLtHash(pub LtHash);
impl Default for SlotLtHash {
⋮----
struct GenerateIndexTimings {
⋮----
struct StorageSizeAndCount {
⋮----
type StorageSizeAndCountMap =
⋮----
impl GenerateIndexTimings {
pub fn report(&self, startup_stats: &StartupStats) {
datapoint_info!(
⋮----
impl IsZeroLamport for AccountSharedData {
⋮----
self.lamports() == 0
⋮----
impl IsZeroLamport for Account {
⋮----
pub type AtomicAccountsFileId = AtomicU32;
pub type AccountsFileId = u32;
type AccountSlots = HashMap<Pubkey, IntSet<Slot>>;
type SlotOffsets = IntMap<Slot, IntSet<Offset>>;
type ReclaimResult = (AccountSlots, SlotOffsets);
type PubkeysRemovedFromAccountsIndex = HashSet<Pubkey>;
type ShrinkCandidates = IntSet<Slot>;
⋮----
pub enum LoadHint {
⋮----
pub enum LoadedAccountAccessor<'a> {
// StoredAccountInfo can't be held directly here due to its lifetime dependency on
⋮----
fn check_and_get_loaded_account_shared_data(&mut self) -> AccountSharedData {
⋮----
.get_account_shared_data(*offset)
.expect(
⋮----
_ => self.check_and_get_loaded_account(|loaded_account| loaded_account.take_account()),
⋮----
fn check_and_get_loaded_account<T>(
⋮----
panic!(
⋮----
self.get_loaded_account(callback).unwrap()
⋮----
self.get_loaded_account(callback).expect(
⋮----
fn get_loaded_account<T>(
⋮----
let cached_account = cached_account.take().expect(
⋮----
Some(callback(LoadedAccount::Cached(cached_account)))
⋮----
.as_ref()
.and_then(|(storage_entry, offset)| {
⋮----
.get_stored_account_callback(*offset, |account| {
callback(LoadedAccount::Stored(account))
⋮----
pub enum LoadedAccount<'a> {
⋮----
LoadedAccount::Stored(stored_account) => stored_account.pubkey(),
LoadedAccount::Cached(cached_account) => cached_account.pubkey(),
⋮----
pub fn take_account(&self) -> AccountSharedData {
⋮----
LoadedAccount::Stored(stored_account) => create_account_shared_data(stored_account),
⋮----
Cow::Owned(cached_account) => cached_account.account.clone(),
Cow::Borrowed(cached_account) => cached_account.account.clone(),
⋮----
pub fn is_cached(&self) -> bool {
⋮----
self.data().len()
⋮----
impl ReadableAccount for LoadedAccount<'_> {
fn lamports(&self) -> u64 {
⋮----
LoadedAccount::Stored(stored_account) => stored_account.lamports(),
LoadedAccount::Cached(cached_account) => cached_account.account.lamports(),
⋮----
fn data(&self) -> &[u8] {
⋮----
LoadedAccount::Stored(stored_account) => stored_account.data(),
LoadedAccount::Cached(cached_account) => cached_account.account.data(),
⋮----
fn owner(&self) -> &Pubkey {
⋮----
LoadedAccount::Stored(stored_account) => stored_account.owner(),
LoadedAccount::Cached(cached_account) => cached_account.account.owner(),
⋮----
fn executable(&self) -> bool {
⋮----
LoadedAccount::Stored(stored_account) => stored_account.executable(),
LoadedAccount::Cached(cached_account) => cached_account.account.executable(),
⋮----
fn rent_epoch(&self) -> Epoch {
⋮----
LoadedAccount::Stored(stored_account) => stored_account.rent_epoch(),
LoadedAccount::Cached(cached_account) => cached_account.account.rent_epoch(),
⋮----
fn to_account_shared_data(&self) -> AccountSharedData {
self.take_account()
⋮----
struct CleanKeyTimings {
⋮----
/// number of ancient append vecs that were scanned because they were dirty when clean started
    dirty_ancient_stores: usize,
⋮----
/// Persistent storage structure holding the accounts
#[derive(Debug)]
pub struct AccountStorageEntry {
⋮----
/// storage holding the accounts
    pub accounts: AccountsFile,
/// The number of alive accounts in this storage
    count: AtomicUsize,
⋮----
/// offsets to accounts that are zero lamport single ref stored in this
    /// storage. These are still alive. But, shrink will be able to remove them.
⋮----
/// storage. These are still alive. But, shrink will be able to remove them.
    ///
⋮----
///
    /// NOTE: It's possible that one of these zero lamport single ref accounts
⋮----
/// NOTE: It's possible that one of these zero lamport single ref accounts
    zero_lamport_single_ref_offsets: RwLock<IntSet<Offset>>,
⋮----
impl AccountStorageEntry {
pub fn new(
⋮----
let path = Path::new(path).join(tail);
let accounts = provider.new_writable(path, file_size, storage_access);
⋮----
fn reopen_as_readonly(&self, storage_access: StorageAccess) -> Option<Self> {
⋮----
self.accounts.reopen_as_readonly().map(|accounts| Self {
⋮----
count: AtomicUsize::new(self.count()),
alive_bytes: AtomicUsize::new(self.alive_bytes()),
⋮----
self.zero_lamport_single_ref_offsets.read().unwrap().clone(),
⋮----
obsolete_accounts: RwLock::new(self.obsolete_accounts.read().unwrap().clone()),
⋮----
pub fn new_existing(
⋮----
pub fn count(&self) -> usize {
self.count.load(Ordering::Acquire)
⋮----
pub fn alive_bytes(&self) -> usize {
self.alive_bytes.load(Ordering::Acquire)
⋮----
pub fn obsolete_accounts_for_snapshots(&self, slot: Slot) -> ObsoleteAccounts {
self.obsolete_accounts_read_lock()
.obsolete_accounts_for_snapshots(slot)
⋮----
pub(crate) fn obsolete_accounts_read_lock(&self) -> RwLockReadGuard<'_, ObsoleteAccounts> {
self.obsolete_accounts.read().unwrap()
⋮----
/// Returns the number of bytes that were marked obsolete as of the passed
    /// in slot or earlier. If slot is None, then slot will be assumed to be the
⋮----
/// in slot or earlier. If slot is None, then slot will be assumed to be the
    /// max root, and all obsolete bytes will be returned.
⋮----
/// max root, and all obsolete bytes will be returned.
    pub fn get_obsolete_bytes(&self, slot: Option<Slot>) -> usize {
⋮----
pub fn get_obsolete_bytes(&self, slot: Option<Slot>) -> usize {
⋮----
.obsolete_accounts_read_lock()
.filter_obsolete_accounts(slot)
.map(|(offset, data_len)| {
⋮----
.calculate_stored_size(data_len)
.min(self.accounts.len() - offset)
⋮----
.sum();
⋮----
/// Return true if offset is "new" and inserted successfully. Otherwise,
    /// return false if the offset exists already.
⋮----
/// return false if the offset exists already.
    fn insert_zero_lamport_single_ref_account_offset(&self, offset: usize) -> bool {
⋮----
fn insert_zero_lamport_single_ref_account_offset(&self, offset: usize) -> bool {
⋮----
self.zero_lamport_single_ref_offsets.write().unwrap();
zero_lamport_single_ref_offsets.insert(offset)
⋮----
/// Insert offsets into the zero lamport single ref account offset set.
    /// Return the number of new offsets that were inserted.
⋮----
/// Return the number of new offsets that were inserted.
    fn batch_insert_zero_lamport_single_ref_account_offsets(&self, offsets: &[Offset]) -> u64 {
⋮----
fn batch_insert_zero_lamport_single_ref_account_offsets(&self, offsets: &[Offset]) -> u64 {
⋮----
if zero_lamport_single_ref_offsets.insert(*offset) {
⋮----
/// Return the number of zero_lamport_single_ref accounts in the storage.
    fn num_zero_lamport_single_ref_accounts(&self) -> usize {
⋮----
fn num_zero_lamport_single_ref_accounts(&self) -> usize {
self.zero_lamport_single_ref_offsets.read().unwrap().len()
⋮----
/// Return the "alive_bytes" minus "zero_lamport_single_ref_accounts bytes".
    fn alive_bytes_exclude_zero_lamport_single_ref_accounts(&self) -> usize {
⋮----
fn alive_bytes_exclude_zero_lamport_single_ref_accounts(&self) -> usize {
⋮----
.dead_bytes_due_to_zero_lamport_single_ref(self.num_zero_lamport_single_ref_accounts());
self.alive_bytes().saturating_sub(zero_lamport_dead_bytes)
⋮----
/// Returns the number of bytes used in this storage
    pub fn written_bytes(&self) -> u64 {
⋮----
pub fn written_bytes(&self) -> u64 {
self.accounts.len() as u64
⋮----
/// Returns the number of bytes, not accounts, this storage can hold
    pub fn capacity(&self) -> u64 {
⋮----
pub fn capacity(&self) -> u64 {
self.accounts.capacity()
⋮----
pub fn has_accounts(&self) -> bool {
self.count() > 0
⋮----
pub fn slot(&self) -> Slot {
⋮----
pub fn id(&self) -> AccountsFileId {
⋮----
pub fn flush(&self) -> Result<(), AccountsFileError> {
self.accounts.flush()
⋮----
fn add_accounts(&self, num_accounts: usize, num_bytes: usize) {
self.count.fetch_add(num_accounts, Ordering::Release);
self.alive_bytes.fetch_add(num_bytes, Ordering::Release);
⋮----
/// Removes `num_bytes` and `num_accounts` from the storage,
    /// and returns the remaining number of accounts.
⋮----
/// and returns the remaining number of accounts.
    fn remove_accounts(&self, num_bytes: usize, num_accounts: usize) -> usize {
⋮----
fn remove_accounts(&self, num_bytes: usize, num_accounts: usize) -> usize {
let prev_alive_bytes = self.alive_bytes.fetch_sub(num_bytes, Ordering::Release);
let prev_count = self.count.fetch_sub(num_accounts, Ordering::Release);
// enforce invariant that we're not removing too many bytes or accounts
assert!(
⋮----
pub fn path(&self) -> &Path {
self.accounts.path()
⋮----
pub fn get_temp_accounts_paths(count: u32) -> io::Result<(Vec<TempDir>, Vec<PathBuf>)> {
let temp_dirs: io::Result<Vec<TempDir>> = (0..count).map(|_| TempDir::new()).collect();
⋮----
.map(|temp_dir| {
⋮----
.map(|(run_dir, _snapshot_dir)| run_dir)
⋮----
.collect();
⋮----
Ok((temp_dirs, paths))
⋮----
struct CleaningInfo {
⋮----
pub enum MarkObsoleteAccounts {
⋮----
type CleaningCandidates = (Box<[RwLock<HashMap<Pubkey, CleaningInfo>>]>, Option<Slot>);
⋮----
struct RemoveUnrootedSlotsSynchronization {
⋮----
type AccountInfoAccountsIndex = AccountsIndex<AccountInfo, AccountInfo>;
⋮----
pub struct AccountsDb {
⋮----
pub fn quarter_thread_count() -> usize {
⋮----
pub fn default_num_foreground_threads() -> usize {
get_thread_count()
⋮----
fn example() -> Self {
⋮----
accounts_db.store_for_tests((some_slot, [(&key, &account)].as_slice()));
accounts_db.add_root_and_flush_write_cache(0);
⋮----
impl AccountsDb {
⋮----
pub fn new_with_config(
⋮----
let accounts_index_config = accounts_db_config.index.unwrap_or_default();
⋮----
let base_working_path = accounts_db_config.base_working_path.clone();
⋮----
let base_working_temp_dir = TempDir::new().unwrap();
let base_working_path = base_working_temp_dir.path().to_path_buf();
(base_working_path, Some(base_working_temp_dir))
⋮----
let (paths, temp_paths) = if paths.is_empty() {
let (temp_dirs, temp_paths) = get_temp_accounts_paths(DEFAULT_NUM_DIRS).unwrap();
(temp_paths, Some(temp_dirs))
⋮----
.clone()
.unwrap_or_else(|| paths.clone());
let read_cache_size = accounts_db_config.read_cache_limit_bytes.unwrap_or((
⋮----
.unwrap_or(Self::DEFAULT_READ_ONLY_CACHE_EVICT_SAMPLE_SIZE);
⋮----
.map(Into::into)
.unwrap_or_else(default_num_foreground_threads);
⋮----
.num_threads(num_foreground_threads)
.thread_name(|i| format!("solAcctsDbFg{i:02}"))
.stack_size(ACCOUNTS_STACK_SIZE)
.build()
.expect("new rayon threadpool");
⋮----
.unwrap_or_else(quarter_thread_count);
⋮----
.thread_name(|i| format!("solAcctsDbBg{i:02}"))
.num_threads(num_background_threads)
⋮----
.or(ANCIENT_APPEND_VEC_DEFAULT_OFFSET),
⋮----
.unwrap_or(DEFAULT_ANCIENT_STORAGE_IDEAL_SIZE),
⋮----
.unwrap_or(DEFAULT_MAX_ANCIENT_STORAGES),
account_indexes: accounts_db_config.account_indexes.unwrap_or_default(),
⋮----
for path in new.paths.iter() {
std::fs::create_dir_all(path).expect("Create directory failed.");
⋮----
pub fn file_size(&self) -> u64 {
⋮----
pub fn get_base_working_path(&self) -> PathBuf {
self.base_working_path.clone()
⋮----
pub fn has_accounts_update_notifier(&self) -> bool {
self.accounts_update_notifier.is_some()
⋮----
fn next_id(&self) -> AccountsFileId {
let next_id = self.next_id.fetch_add(1, Ordering::AcqRel);
⋮----
fn new_storage_entry(&self, slot: Slot, path: &Path, size: u64) -> AccountStorageEntry {
⋮----
self.next_id(),
⋮----
fn collect_reclaims(
⋮----
let one_epoch_old = self.get_oldest_non_ancient_slot(epoch_schedule);
⋮----
let removed_from_index = self.accounts_index.clean_rooted_entries(
⋮----
.lock()
.unwrap()
.insert(*pubkey);
⋮----
if !reclaims.is_empty() {
⋮----
.filter_map(|(slot, _)| (slot < &one_epoch_old).then_some(1))
⋮----
ancient_account_cleans.fetch_add(old_reclaims, Ordering::Relaxed);
⋮----
clean_rooted.stop();
⋮----
.fetch_add(clean_rooted.as_us(), Ordering::Relaxed);
⋮----
fn clean_accounts_older_than_root(
⋮----
if reclaims.is_empty() {
⋮----
let (reclaim_result, reclaim_us) = measure_us!(self.handle_reclaims(
⋮----
.fetch_add(reclaim_us, Ordering::Relaxed);
⋮----
fn calc_delete_dependencies(
⋮----
for (bin_index, bin) in candidates.iter().enumerate() {
for (pubkey, cleaning_info) in bin.iter() {
⋮----
let all_stores_being_deleted = slot_list.len() as RefCount == *ref_count;
⋮----
if let Some(count) = store_counts.get(slot).map(|s| s.0) {
debug!("calc_delete_dependencies() slot: {slot}, count len: {count}");
⋮----
failed_slot = Some(*slot);
⋮----
debug!(
⋮----
if !already_counted.contains(slot) {
pending_stores.insert(*slot);
⋮----
while !pending_stores.is_empty() {
let slot = pending_stores.iter().next().cloned().unwrap();
if Some(slot) == min_slot {
if let Some(failed_slot) = failed_slot.take() {
info!(
⋮----
pending_stores.remove(&slot);
if !already_counted.insert(slot) {
⋮----
if let Some(store_count) = store_counts.remove(&slot) {
⋮----
self.accounts_index.bin_calculator.bin_from_pubkey(key);
⋮----
for (slot, _account_info) in &bin.get(key).unwrap().slot_list {
⋮----
update_pending_stores(bin);
⋮----
update_pending_stores(&candidates[candidates_bin_index]);
⋮----
pub fn purge_keys_exact<C>(
⋮----
measure_us!(for (pubkey, slots_set) in pubkey_to_slot_set.into_iter() {
⋮----
let (pubkeys_removed_from_accounts_index, handle_dead_keys_us) = measure_us!(self
⋮----
.fetch_add(purge_exact_count, Ordering::Relaxed);
⋮----
.fetch_add(handle_dead_keys_us, Ordering::Relaxed);
⋮----
.fetch_add(purge_exact_us, Ordering::Relaxed);
⋮----
fn max_clean_root(&self, proposed_clean_root: Option<Slot>) -> Option<Slot> {
⋮----
self.accounts_index.min_ongoing_scan_root(),
⋮----
(Some(min_scan_root), None) => Some(min_scan_root),
(None, Some(proposed_clean_root)) => Some(proposed_clean_root),
⋮----
Some(std::cmp::min(min_scan_root, proposed_clean_root))
⋮----
fn get_oldest_non_ancient_slot(&self, epoch_schedule: &EpochSchedule) -> Slot {
self.get_oldest_non_ancient_slot_from_slot(
⋮----
self.accounts_index.max_root_inclusive(),
⋮----
fn get_oldest_non_ancient_slot_from_slot(
⋮----
-((epoch_schedule.slots_per_epoch as i64).saturating_sub(1)),
⋮----
result.min(max_root_inclusive)
⋮----
fn collect_uncleaned_slots_up_to_slot(&self, max_slot_inclusive: Slot) -> Vec<Slot> {
⋮----
.filter_map(|entry| {
let slot = *entry.key();
(slot <= max_slot_inclusive).then_some(slot)
⋮----
.collect()
⋮----
fn remove_uncleaned_slots_up_to_slot_and_move_pubkeys(
⋮----
let uncleaned_slots = self.collect_uncleaned_slots_up_to_slot(max_slot_inclusive);
for uncleaned_slot in uncleaned_slots.into_iter() {
⋮----
self.uncleaned_pubkeys.remove(&uncleaned_slot)
⋮----
removed_pubkeys.sort_by(|a, b| {
⋮----
.bin_from_pubkey(a)
.cmp(&self.accounts_index.bin_calculator.bin_from_pubkey(b))
⋮----
if let Some(first_removed_pubkey) = removed_pubkeys.first() {
⋮----
.bin_from_pubkey(first_removed_pubkey);
let mut candidates_bin = candidates[prev_bin].write().unwrap();
⋮----
.bin_from_pubkey(&removed_pubkey);
⋮----
candidates_bin = candidates[curr_bin].write().unwrap();
⋮----
candidates_bin.insert(
⋮----
fn count_pubkeys(candidates: &[RwLock<HashMap<Pubkey, CleaningInfo>>]) -> u64 {
⋮----
.map(|x| x.read().unwrap().len())
⋮----
fn construct_candidate_clean_keys(
⋮----
let oldest_non_ancient_slot = self.get_oldest_non_ancient_slot(epoch_schedule);
⋮----
let max_root_inclusive = self.accounts_index.max_root_inclusive();
let max_slot_inclusive = max_clean_root_inclusive.unwrap_or(max_root_inclusive);
let mut dirty_stores = Vec::with_capacity(self.dirty_stores.len());
⋮----
self.dirty_stores.retain(|slot, store| {
⋮----
min_dirty_slot = min_dirty_slot.map(|min| min.min(*slot)).or(Some(*slot));
dirty_stores.push((*slot, store.clone()));
⋮----
let dirty_stores_len = dirty_stores.len();
let num_bins = self.accounts_index.bins();
⋮----
.take(num_bins)
⋮----
let index = self.accounts_index.bin_calculator.bin_from_pubkey(&pubkey);
let mut candidates_bin = candidates[index].write().unwrap();
⋮----
.entry(pubkey)
.or_default()
⋮----
let chunk_size = 1.max(dirty_stores_len.saturating_div(rayon::current_num_threads()));
⋮----
.par_chunks(chunk_size)
.map(|dirty_store_chunk| {
let mut oldest_dirty_slot = max_slot_inclusive.saturating_add(1);
dirty_store_chunk.iter().for_each(|(slot, store)| {
⋮----
dirty_ancient_stores.fetch_add(1, Ordering::Relaxed);
⋮----
oldest_dirty_slot = oldest_dirty_slot.min(*slot);
⋮----
.scan_accounts_without_data(|_offset, account| {
let pubkey = *account.pubkey();
let is_zero_lamport = account.is_zero_lamport();
insert_candidate(pubkey, is_zero_lamport);
⋮----
.expect("must scan accounts storage");
⋮----
.min()
.unwrap_or(&max_slot_inclusive.saturating_add(1));
⋮----
dirty_store_routine();
⋮----
self.thread_pool_background.install(|| {
⋮----
trace!(
⋮----
dirty_store_processing_time.stop();
timings.dirty_store_processing_us += dirty_store_processing_time.as_us();
timings.dirty_ancient_stores = dirty_ancient_stores.load(Ordering::Relaxed);
⋮----
self.remove_uncleaned_slots_up_to_slot_and_move_pubkeys(max_slot_inclusive, &candidates);
collect_delta_keys.stop();
timings.collect_delta_keys_us += collect_delta_keys.as_us();
⋮----
let latest_full_snapshot_slot = self.latest_full_snapshot_slot();
⋮----
.retain(|(slot, pubkey)| {
⋮----
insert_candidate(*pubkey, true);
⋮----
fn exhaustively_verify_refcounts(&self, max_slot_inclusive: Option<Slot>) {
⋮----
max_slot_inclusive.unwrap_or_else(|| self.accounts_index.max_root_inclusive());
info!("exhaustively verifying refcounts as of slot: {max_slot_inclusive}");
⋮----
let mut storages = self.storage.all_storages();
storages.retain(|s| s.slot() <= max_slot_inclusive);
storages.par_iter().for_each_init(
⋮----
let slot = storage.slot();
⋮----
.scan_accounts(reader.as_mut(), |_offset, account| {
let pk = account.pubkey();
match pubkey_refcount.entry(*pk) {
⋮----
if !occupied_entry.get().iter().any(|s| s == &slot) {
occupied_entry.get_mut().push(slot);
⋮----
vacant_entry.insert(vec![slot]);
⋮----
.expect("must scan accounts storage")
⋮----
let total = pubkey_refcount.len();
⋮----
let threads = quarter_thread_count();
⋮----
(0..=threads).into_par_iter().for_each(|attempt| {
⋮----
.skip(attempt * per_batch)
.take(per_batch)
.for_each(|entry| {
if failed.load(Ordering::Relaxed) {
⋮----
.get_and_then(entry.key(), |index_entry| {
⋮----
match (index_entry.ref_count() as usize).cmp(&entry.value().len()) {
⋮----
let slot_list = index_entry.slot_list_read_lock();
⋮----
.filter(|(slot, _)| slot > &max_slot_inclusive)
.count();
if ((index_entry.ref_count() as usize) - num_too_new)
> entry.value().len()
⋮----
failed.store(true, Ordering::Relaxed);
error!(
⋮----
panic!("exhaustively_verify_refcounts failed");
⋮----
pub fn clean_accounts(
⋮----
self.exhaustively_verify_refcounts(max_clean_root_inclusive);
⋮----
.install(|| self.exhaustively_verify_refcounts(max_clean_root_inclusive));
⋮----
let _guard = self.active_stats.activate(ActiveStatItem::Clean);
⋮----
let max_clean_root_inclusive = self.max_clean_root(max_clean_root_inclusive);
self.report_store_stats();
⋮----
.activate(ActiveStatItem::CleanConstructCandidates);
⋮----
let (mut candidates, min_dirty_slot) = self.construct_candidate_clean_keys(
⋮----
measure_construct_candidates.stop();
drop(active_guard);
⋮----
candidates.par_iter().for_each(|candidates_bin| {
⋮----
let mut candidates_bin = candidates_bin.write().unwrap();
candidates_bin.retain(|candidate_pubkey, candidate_info| {
⋮----
self.accounts_index.scan(
⋮----
let index_in_slot_list = self.accounts_index.latest_slot(
⋮----
if account_info.is_zero_lamport() {
⋮----
self.accounts_index.get_rooted_entries(
⋮----
if slot_list.len() > 1
⋮----
<= max_clean_root_inclusive.unwrap_or(Slot::MAX)
⋮----
let reclaims_new = self.collect_reclaims(
⋮----
if !reclaims_new.is_empty() {
reclaims.lock().unwrap().extend(reclaims_new);
⋮----
!candidate_info.slot_list.is_empty()
⋮----
found_not_zero_accum.fetch_add(found_not_zero, Ordering::Relaxed);
not_found_on_fork_accum.fetch_add(not_found_on_fork, Ordering::Relaxed);
missing_accum.fetch_add(missing, Ordering::Relaxed);
useful_accum.fetch_add(useful, Ordering::Relaxed);
purges_old_accounts_count.fetch_add(purges_old_accounts_local, Ordering::Relaxed);
⋮----
.activate(ActiveStatItem::CleanScanCandidates);
⋮----
do_clean_scan();
⋮----
self.thread_pool_background.install(do_clean_scan);
⋮----
accounts_scan.stop();
⋮----
.iter_mut()
.map(|candidates_bin| mem::take(candidates_bin.get_mut().unwrap()))
⋮----
let retained_keys_count: usize = candidates.iter().map(HashMap::len).sum();
let reclaims = reclaims.into_inner().unwrap();
⋮----
pubkeys_removed_from_accounts_index.into_inner().unwrap();
let active_guard = self.active_stats.activate(ActiveStatItem::CleanOldAccounts);
⋮----
self.clean_accounts_older_than_root(&reclaims, &pubkeys_removed_from_accounts_index);
clean_old_rooted.stop();
⋮----
.activate(ActiveStatItem::CleanCollectStoreCounts);
⋮----
for candidates_bin in candidates.iter_mut() {
for (pubkey, cleaning_info) in candidates_bin.iter_mut() {
⋮----
debug_assert!(!slot_list.is_empty(), "candidate slot_list can't be empty");
if purged_account_slots.contains_key(pubkey) {
*ref_count = self.accounts_index.ref_count_from_storage(pubkey);
⋮----
slot_list.retain(|(slot, account_info)| {
⋮----
.get(pubkey)
.map(|slots_removed| slots_removed.contains(slot))
.unwrap_or(false);
⋮----
.get(slot)
.map(|store_removed| store_removed.contains(&account_info.offset()))
⋮----
if let Some(store_count) = store_counts.get_mut(slot) {
⋮----
store_count.1.insert(*pubkey);
⋮----
key_set.insert(*pubkey);
⋮----
.get_account_storage_entry(*slot, account_info.store_id())
.map(|store| store.count())
⋮----
store_counts.insert(*slot, (count, key_set));
⋮----
store_counts_time.stop();
⋮----
.activate(ActiveStatItem::CleanCalcDeleteDeps);
⋮----
self.calc_delete_dependencies(&candidates, &mut store_counts, min_dirty_slot);
calc_deps_time.stop();
⋮----
.activate(ActiveStatItem::CleanFilterZeroLamport);
⋮----
self.filter_zero_lamport_clean_for_incremental_snapshots(
⋮----
purge_filter.stop();
⋮----
let active_guard = self.active_stats.activate(ActiveStatItem::CleanReclaims);
⋮----
.into_iter()
.filter_map(|(pubkey, cleaning_info)| {
⋮----
(!slot_list.is_empty()).then_some((
⋮----
.map(|(slot, _)| *slot)
⋮----
pubkey_to_slot_set.append(&mut bin_set);
⋮----
self.purge_keys_exact(pubkey_to_slot_set);
pubkeys_removed_from_accounts_index.extend(pubkeys_removed_from_accounts_index2);
⋮----
self.handle_reclaims(
reclaims.iter(),
⋮----
reclaims_time.stop();
⋮----
measure_all.stop();
self.clean_accounts_stats.report();
⋮----
fn handle_reclaims<'a, I>(
⋮----
self.remove_dead_accounts(reclaims, expected_single_dead_slot, mark_accounts_obsolete);
⋮----
assert!(dead_slots.len() <= 1);
if dead_slots.len() == 1 {
assert!(dead_slots.contains(&expected_single_dead_slot));
⋮----
!matches!(mark_accounts_obsolete, MarkAccountsObsolete::Yes(_));
self.process_dead_slots(
⋮----
Some(&mut reclaim_result.0),
⋮----
fn filter_zero_lamport_clean_for_incremental_snapshots(
⋮----
let should_filter_for_incremental_snapshots = max_clean_root_inclusive.unwrap_or(Slot::MAX)
> latest_full_snapshot_slot.unwrap_or(Slot::MAX);
⋮----
bin.retain(|pubkey, cleaning_info| {
⋮----
for (slot, _account_info) in slot_list.iter() {
if let Some(store_count) = store_counts.get(slot) {
⋮----
.max_by_key(|(slot, _account_info)| slot)
.unwrap();
assert!(account_info.is_zero_lamport());
let cannot_purge = *slot > latest_full_snapshot_slot.unwrap();
⋮----
.insert((*slot, *pubkey));
⋮----
fn process_dead_slots(
⋮----
if dead_slots.is_empty() {
⋮----
self.clean_stored_dead_slots(
⋮----
self.remove_dead_slots_metadata(dead_slots.iter());
clean_dead_slots.stop();
⋮----
self.purge_dead_slots_from_storage(dead_slots.iter(), purge_stats);
purge_removed_slots.stop();
⋮----
let mut list = self.shrink_candidate_slots.lock().unwrap();
⋮----
list.remove(slot);
⋮----
fn load_accounts_index_for_shrink<'a, T: ShrinkCollectRefs<'a>>(
⋮----
let count = accounts.len();
⋮----
accounts.iter().map(|account| account.pubkey()),
⋮----
if stored_account.is_zero_lamport()
⋮----
.map(|latest_full_snapshot_slot| {
⋮----
.unwrap_or(true)
⋮----
zero_lamport_single_ref_pubkeys.push(pubkey);
self.add_uncleaned_pubkeys_after_shrink(
⋮----
[*pubkey].into_iter(),
⋮----
all_are_zero_lamports &= stored_account.is_zero_lamport();
alive_accounts.add(ref_count, stored_account, slot_list);
⋮----
let is_alive = slot_list.iter().any(|(slot, _acct_info)| {
⋮----
pubkeys_to_unref.push(pubkey);
⋮----
do_populate_accounts_for_shrink(ref_count, slot_list);
⋮----
do_populate_accounts_for_shrink(ref_count, &slot_list);
⋮----
assert_eq!(index, std::cmp::min(accounts.len(), count));
⋮----
.fetch_add(index_scan_returned_some_count, Ordering::Relaxed);
⋮----
.fetch_add(index_scan_returned_none_count, Ordering::Relaxed);
stats.alive_accounts.fetch_add(alive, Ordering::Relaxed);
stats.dead_accounts.fetch_add(dead, Ordering::Relaxed);
⋮----
pub fn get_unique_accounts_from_storage(
⋮----
let capacity = store.capacity();
let mut stored_accounts = Vec::with_capacity(store.count());
⋮----
.scan_accounts_without_data(|offset, account| {
⋮----
stored_accounts.push(AccountFromStorage {
⋮----
pub fn set_storage_access(&mut self, storage_access: StorageAccess) {
⋮----
fn sort_and_remove_dups(accounts: &mut Vec<AccountFromStorage>) -> usize {
accounts.sort_by(|a, b| a.pubkey().cmp(b.pubkey()));
let len0 = accounts.len();
if accounts.len() > 1 {
⋮----
while curr < accounts.len() {
if accounts[curr].pubkey() != accounts[last].pubkey() {
⋮----
accounts.truncate(last + 1);
⋮----
len0 - accounts.len()
⋮----
pub(crate) fn get_unique_accounts_from_storage_for_shrink(
⋮----
measure_us!(self.get_unique_accounts_from_storage(store));
⋮----
.fetch_add(storage_read_elapsed_us, Ordering::Relaxed);
⋮----
.fetch_add(result.num_duplicated_accounts as u64, Ordering::Relaxed);
⋮----
pub(crate) fn shrink_collect<'a: 'b, 'b, T: ShrinkCollectRefs<'b>>(
⋮----
let slot = store.slot();
⋮----
// Get a set of all obsolete offsets
// Slot is not needed, as all obsolete accounts can be considered
// dead for shrink. Zero lamport accounts are not marked obsolete
⋮----
.filter_obsolete_accounts(None)
.map(|(offset, _)| offset)
⋮----
// Filter all the accounts that are marked obsolete
let total_starting_accounts = stored_accounts.len();
stored_accounts.retain(|account| !obsolete_offsets.contains(&account.index_info.offset()));
let len = stored_accounts.len();
⋮----
alive_total_bytes: 0, // will be updated after `alive_accounts` is populated
⋮----
.fetch_add(len as u64, Ordering::Relaxed);
⋮----
.fetch_add((total_starting_accounts - len) as u64, Ordering::Relaxed);
⋮----
.fetch_add(*num_duplicated_accounts as u64, Ordering::Relaxed);
⋮----
.par_chunks(SHRINK_COLLECT_CHUNK_SIZE)
.for_each(|stored_accounts| {
⋮----
} = self.load_accounts_index_for_shrink(stored_accounts, stats, slot);
// collect
let mut shrink_collect = shrink_collect.lock().unwrap();
shrink_collect.alive_accounts.collect(alive_accounts);
⋮----
.append(&mut pubkeys_to_unref);
⋮----
.append(&mut zero_lamport_single_ref_pubkeys);
⋮----
index_read_elapsed.stop();
let mut shrink_collect = shrink_collect.into_inner().unwrap();
let alive_total_bytes = shrink_collect.alive_accounts.alive_bytes();
⋮----
.fetch_add(index_read_elapsed.as_us(), Ordering::Relaxed);
stats.accounts_removed.fetch_add(
total_starting_accounts - shrink_collect.alive_accounts.len(),
⋮----
stats.bytes_removed.fetch_add(
capacity.saturating_sub(alive_total_bytes as u64),
⋮----
.fetch_add(alive_total_bytes as u64, Ordering::Relaxed);
⋮----
/// These accounts were found during shrink of `slot` to be slot_list=[slot] and ref_count == 1 and lamports = 0.
    /// This means this slot contained the only account data for this pubkey and it is zero lamport.
⋮----
/// This means this slot contained the only account data for this pubkey and it is zero lamport.
    /// Thus, we did NOT treat this as an alive account, so we did NOT copy the zero lamport account to the new
⋮----
/// Thus, we did NOT treat this as an alive account, so we did NOT copy the zero lamport account to the new
    /// storage. So, the account will no longer be alive or exist at `slot`.
⋮----
/// storage. So, the account will no longer be alive or exist at `slot`.
    /// So, first, remove the ref count since this newly shrunk storage will no longer access it.
⋮----
/// So, first, remove the ref count since this newly shrunk storage will no longer access it.
    /// Second, remove `slot` from the index entry's slot list. If the slot list is now empty, then the
⋮----
/// Second, remove `slot` from the index entry's slot list. If the slot list is now empty, then the
    fn remove_zero_lamport_single_ref_accounts_after_shrink(
⋮----
fn remove_zero_lamport_single_ref_accounts_after_shrink(
⋮----
stats.purged_zero_lamports.fetch_add(
zero_lamport_single_ref_pubkeys.len() as u64,
⋮----
zero_lamport_single_ref_pubkeys.iter().cloned(),
⋮----
Some(AccountsIndexScanResult::UnrefAssert0)
⋮----
Some(AccountsIndexScanResult::UnrefLog0)
⋮----
zero_lamport_single_ref_pubkeys.iter().for_each(|k| {
_ = self.purge_keys_exact([(**k, slot)]);
⋮----
pub(crate) fn remove_old_stores_shrink<'a, T: ShrinkCollectRefs<'a>>(
⋮----
// handle the zero lamport alive accounts before calling clean
// We have to update the index entries for these zero lamport pubkeys before we remove the storage in `mark_dirty_dead_stores`
// that contained the accounts.
self.remove_zero_lamport_single_ref_accounts_after_shrink(
⋮----
// Purge old, overwritten storage entries
// This has the side effect of dropping `shrink_in_progress`, which removes the old storage completely. The
// index has to be correct before we drop the old storage.
let dead_storages = self.mark_dirty_dead_stores(
⋮----
// If all accounts are zero lamports, then we want to mark the entire OLD append vec as dirty.
// otherwise, we'll call 'add_uncleaned_pubkeys_after_shrink' just on the unref'd keys below.
⋮----
let dead_storages_len = dead_storages.len();
⋮----
shrink_collect.pubkeys_to_unref.iter().cloned().cloned(),
⋮----
let (_, drop_storage_entries_elapsed) = measure_us!(drop(dead_storages));
time.stop();
⋮----
.fetch_add(dead_storages_len as u64, Ordering::Relaxed);
⋮----
.fetch_add(drop_storage_entries_elapsed, Ordering::Relaxed);
⋮----
.fetch_add(time.as_us(), Ordering::Relaxed);
⋮----
pub(crate) fn unref_shrunk_dead_accounts<'a>(
⋮----
if slot_list.len() == 1 && ref_count == 2 {
if let Some((slot_alive, acct_info)) = slot_list.first() {
if acct_info.is_zero_lamport() && !acct_info.is_cached() {
self.zero_lamport_single_ref_found(
⋮----
acct_info.offset(),
⋮----
warn!(
⋮----
datapoint_warn!(
⋮----
pub(crate) fn zero_lamport_single_ref_found(&self, slot: Slot, offset: Offset) {
⋮----
.get_slot_storage_entry_shrinking_in_progress_ok(slot)
⋮----
if store.insert_zero_lamport_single_ref_account_offset(offset) {
⋮----
.fetch_add(1, Ordering::Relaxed);
if store.num_zero_lamport_single_ref_accounts() == store.count() {
self.dirty_stores.entry(slot).or_insert(store);
⋮----
&& self.is_candidate_for_shrink(&store)
⋮----
let is_new = self.shrink_candidate_slots.lock().unwrap().insert(slot);
⋮----
fn shrink_storage(&self, store: Arc<AccountStorageEntry>) {
⋮----
if self.accounts_cache.contains(slot) {
⋮----
self.get_unique_accounts_from_storage_for_shrink(&store, &self.shrink_stats);
debug!("do_shrink_slot_store: slot: {slot}");
⋮----
// This shouldn't happen if alive_bytes is accurate.
⋮----
self.dirty_stores.insert(slot, store.clone());
⋮----
self.unref_shrunk_dead_accounts(shrink_collect.pubkeys_to_unref.iter().cloned(), slot);
let total_accounts_after_shrink = shrink_collect.alive_accounts.len();
⋮----
measure_us!(self.get_store_for_shrink(slot, shrink_collect.alive_total_bytes as u64));
stats_sub.create_and_insert_store_elapsed_us = Saturating(time_us);
let accounts = [(slot, &shrink_collect.alive_accounts.alive_accounts()[..])];
⋮----
stats_sub.store_accounts_timing = self.store_accounts_frozen(
⋮----
shrink_in_progress.new_storage(),
⋮----
rewrite_elapsed.stop();
stats_sub.rewrite_elapsed_us = Saturating(rewrite_elapsed.as_us());
self.shrink_candidate_slots.lock().unwrap().remove(&slot);
self.remove_old_stores_shrink(
⋮----
Some(shrink_in_progress),
⋮----
self.reopen_storage_as_readonly_shrinking_in_progress_ok(slot);
⋮----
self.shrink_stats.report();
⋮----
pub(crate) fn update_shrink_stats(
⋮----
shrink_stats.create_and_insert_store_elapsed.fetch_add(
⋮----
shrink_stats.store_accounts_elapsed.fetch_add(
⋮----
shrink_stats.update_index_elapsed.fetch_add(
⋮----
shrink_stats.handle_reclaims_elapsed.fetch_add(
⋮----
.fetch_add(stats_sub.rewrite_elapsed_us.0, Ordering::Relaxed);
⋮----
.fetch_add(stats_sub.unpackable_slots_count.0 as u64, Ordering::Relaxed);
shrink_stats.newest_alive_packed_count.fetch_add(
⋮----
pub fn mark_dirty_dead_stores(
⋮----
dead_storages.push(store.clone());
⋮----
not_retaining_store(shrink_in_progress.old_storage());
} else if let Some(store) = self.storage.remove(&slot, shrink_can_be_active) {
not_retaining_store(&store);
⋮----
pub(crate) fn reopen_storage_as_readonly_shrinking_in_progress_ok(&self, slot: Slot) {
⋮----
if let Some(new_storage) = storage.reopen_as_readonly(self.storage_access) {
assert_eq!(storage.id(), new_storage.id());
assert_eq!(storage.accounts.len(), new_storage.accounts.len());
⋮----
.replace_storage_with_equivalent(slot, Arc::new(new_storage));
⋮----
pub fn get_store_for_shrink(&self, slot: Slot, size: u64) -> ShrinkInProgress<'_> {
let shrunken_store = self.create_store(slot, size, "shrink", self.shrink_paths.as_slice());
self.storage.shrinking_in_progress(slot, shrunken_store)
⋮----
// Reads all accounts in given slot's AppendVecs and filter only to alive,
fn shrink_slot_forced(&self, slot: Slot) {
debug!("shrink_slot_forced: slot: {slot}");
⋮----
self.shrink_storage(store)
⋮----
fn all_slots_in_storage(&self) -> Vec<Slot> {
self.storage.all_slots()
⋮----
fn select_candidates_by_total_usage(
⋮----
struct StoreUsageInfo {
⋮----
let mut store_usage: Vec<StoreUsageInfo> = Vec::with_capacity(shrink_slots.len());
⋮----
let Some(store) = self.storage.get_slot_storage_entry(*slot) else {
⋮----
let alive_bytes = store.alive_bytes();
⋮----
total_bytes += store.capacity();
let alive_ratio = alive_bytes as f64 / store.capacity() as f64;
store_usage.push(StoreUsageInfo {
⋮----
store: store.clone(),
⋮----
store_usage.sort_by(|a, b| {
⋮----
.partial_cmp(&b.alive_ratio)
.unwrap_or(std::cmp::Ordering::Equal)
⋮----
shrink_slots_next_batch.insert(usage.slot);
⋮----
let current_store_size = store.capacity();
let after_shrink_size = store.alive_bytes() as u64;
let bytes_saved = current_store_size.saturating_sub(after_shrink_size);
⋮----
shrink_slots.insert(usage.slot, Arc::clone(store));
⋮----
fn get_roots_less_than(&self, slot: Slot) -> Vec<Slot> {
⋮----
.read()
⋮----
.get_all_less_than(slot)
⋮----
fn get_sorted_potential_ancient_slots(&self, oldest_non_ancient_slot: Slot) -> Vec<Slot> {
let mut ancient_slots = self.get_roots_less_than(oldest_non_ancient_slot);
ancient_slots.sort_unstable();
⋮----
pub fn shrink_ancient_slots(&self, epoch_schedule: &EpochSchedule) {
if self.ancient_append_vec_offset.is_none() {
⋮----
let sorted_slots = self.get_sorted_potential_ancient_slots(oldest_non_ancient_slot);
self.combine_ancient_slots_packed(sorted_slots, can_randomly_shrink);
⋮----
pub(crate) fn handle_dropped_roots_for_ancient(
⋮----
dropped_roots.for_each(|slot| {
self.accounts_index.clean_dead_slot(slot);
assert!(self.storage.remove(&slot, false).is_none());
debug_assert!(
⋮----
fn add_uncleaned_pubkeys_after_shrink(
⋮----
let mut uncleaned_pubkeys = self.uncleaned_pubkeys.entry(slot).or_default();
uncleaned_pubkeys.extend(pubkeys);
⋮----
pub fn shrink_candidate_slots(&self, epoch_schedule: &EpochSchedule) -> usize {
⋮----
std::mem::take(&mut *self.shrink_candidate_slots.lock().unwrap());
⋮----
.store(shrink_candidates_slots.len() as u64, Ordering::Relaxed);
let candidates_count = shrink_candidates_slots.len();
let ((mut shrink_slots, shrink_slots_next_batch), select_time_us) = measure_us!({
⋮----
if shrink_slots.len() < SHRINK_INSERT_ANCIENT_THRESHOLD {
let mut ancients = self.best_ancient_slots_to_shrink.write().unwrap();
while let Some((slot, capacity)) = ancients.pop_front() {
if let Some(store) = self.storage.get_slot_storage_entry(slot) {
if !shrink_slots.contains(&slot)
&& capacity == store.capacity()
⋮----
let ancient_bytes_added_to_shrink = store.alive_bytes() as u64;
shrink_slots.insert(slot, store);
⋮----
.fetch_add(ancient_bytes_added_to_shrink, Ordering::Relaxed);
⋮----
if shrink_slots.is_empty()
⋮----
.map(|s| s.is_empty())
⋮----
let _guard = (!shrink_slots.is_empty())
.then_some(|| self.active_stats.activate(ActiveStatItem::Shrink));
let num_selected = shrink_slots.len();
let (_, shrink_all_us) = measure_us!({
⋮----
let mut shrink_slots = self.shrink_candidate_slots.lock().unwrap();
pended_counts = shrink_slots_next_batch.len();
⋮----
shrink_slots.insert(slot);
⋮----
pub fn shrink_all_slots(
⋮----
let _guard = self.active_stats.activate(ActiveStatItem::Shrink);
⋮----
let mut slots = self.all_slots_in_storage();
⋮----
slots.retain(|slot| slot < &newest_slot_skip_shrink_inclusive);
⋮----
if self.dirty_stores.len() > DIRTY_STORES_CLEANING_THRESHOLD {
⋮----
self.clean_accounts(latest_full_snapshot_slot, is_startup, epoch_schedule);
⋮----
slots.chunks(OUTER_CHUNK_SIZE).for_each(|chunk| {
chunk.par_chunks(inner_chunk_size).for_each(|slots| {
⋮----
self.shrink_slot_forced(*slot);
⋮----
maybe_clean();
⋮----
self.shrink_slot_forced(slot);
⋮----
pub fn scan_accounts<F>(
⋮----
self.accounts_index.scan_accounts(
⋮----
self.get_account_accessor(slot, pubkey, &account_info.storage_location());
⋮----
_ => account_accessor.get_loaded_account(|loaded_account| {
(pubkey, loaded_account.take_account(), slot)
⋮----
scan_func(account_slot)
⋮----
Ok(())
⋮----
pub fn index_scan_accounts<F>(
⋮----
if !self.account_indexes.include_key(key) {
⋮----
self.scan_accounts(ancestors, bank_id, scan_func, config)?;
return Ok(used_index);
⋮----
self.accounts_index.index_scan_accounts(
⋮----
.get_account_accessor(slot, pubkey, &account_info.storage_location())
.get_loaded_account(|loaded_account| {
⋮----
Ok(used_index)
⋮----
pub(crate) fn scan_account_storage<R, B>(
⋮----
Option<&'storage [u8]>, // account data
⋮----
self.scan_cache_storage_fallback(slot, cache_map_func, |retval, storage| {
⋮----
storage.scan_accounts_without_data(|_offset, account_without_data| {
storage_scan_func(retval, &account_without_data, None);
⋮----
storage.scan_accounts(&mut reader, |_offset, account| {
⋮----
storage_scan_func(retval, &account_without_data, Some(account.data));
⋮----
/// Scan the cache with a fallback to storage for a specific slot.
    pub fn scan_cache_storage_fallback<R, B>(
⋮----
pub fn scan_cache_storage_fallback<R, B>(
⋮----
if let Some(slot_cache) = self.accounts_cache.slot_cache(slot) {
// If we see the slot in the cache, then all the account information
// is in this cached slot
if slot_cache.len() > SCAN_SLOT_PAR_ITER_THRESHOLD {
ScanStorageResult::Cached(self.thread_pool_foreground.install(|| {
⋮----
.par_iter()
.filter_map(|cached_account| {
cache_map_func(&LoadedAccount::Cached(Cow::Borrowed(
cached_account.value(),
⋮----
.collect(),
⋮----
// If the slot is not in the cache, then all the account information must have
// been flushed. This is guaranteed because we only remove the rooted slot from
// the cache *after* we've finished flushing in `flush_slot_cache`.
⋮----
storage_fallback_func(&mut retval, &storage.accounts);
⋮----
pub fn load(
⋮----
self.do_load(ancestors, pubkey, None, load_hint, LoadZeroLamports::None)
⋮----
pub fn load_account_into_read_cache(&self, ancestors: &Ancestors, pubkey: &Pubkey) {
self.do_load_with_populate_read_cache(
⋮----
pub fn load_with_fixed_root(
⋮----
self.load(ancestors, pubkey, LoadHint::FixedMaxRoot)
⋮----
fn read_index_for_accessor_or_load_slow<'a>(
⋮----
self.accounts_index.get_with_and_then(
⋮----
Some(ancestors),
⋮----
let storage_location = account_info.storage_location();
⋮----
.then(|| self.get_account_accessor(slot, pubkey, &storage_location));
⋮----
fn retry_to_get_account_accessor<'a>(
⋮----
// Happy drawing time! :)
//
// Reader                               | Accessed data source for cached/stored
// -------------------------------------+----------------------------------
// R1 read_index_for_accessor_or_load_slow()| cached/stored: index
//          |                           |
//        <(store_id, offset, ..)>      |
//          V                           |
// R2 retry_to_get_account_accessor()/  | cached: map of caches & entry for (slot, pubkey)
//        get_account_accessor()        | stored: map of stores
⋮----
//        <Accessor>                    |
⋮----
// R3 check_and_get_loaded_account()/   | cached: N/A (note: basically noop unwrap)
//        get_loaded_account()          | stored: store's entry for slot
⋮----
sleep(Duration::from_millis(self.load_delay));
⋮----
let account_accessor = self.get_account_accessor(slot, pubkey, &storage_location);
⋮----
return Some((account_accessor, slot));
⋮----
assert!(num_acceptable_failed_iterations <= 1);
⋮----
let load_limit = self.load_limit.load(Ordering::Relaxed);
⋮----
let message = format!(
⋮----
datapoint_warn!("accounts_db-do_load_warn", ("warn", message, String));
⋮----
.read_index_for_accessor_or_load_slow(
⋮----
if new_slot == slot && new_storage_location.is_store_id_equal(&storage_location) {
⋮----
.get_and_then(pubkey, |entry| -> (_, ()) {
⋮----
assert!(!new_storage_location.is_cached(), "{message}");
assert_eq!(load_hint, LoadHint::Unspecified, "{message}");
panic!("{message}");
⋮----
return Some((
maybe_account_accessor.expect("must be some if clone_in_lock=true"),
⋮----
fn do_load(
⋮----
pub fn load_account_with(
⋮----
self.read_index_for_accessor_or_load_slow(ancestors, pubkey, None, false)?;
let in_write_cache = storage_location.is_cached();
⋮----
let result = self.read_only_accounts_cache.load(*pubkey, slot);
⋮----
if account.is_zero_lamport() {
⋮----
return Some((account, slot));
⋮----
let (mut account_accessor, slot) = self.retry_to_get_account_accessor(
⋮----
let in_write_cache = matches!(account_accessor, LoadedAccountAccessor::Cached(_));
let account = account_accessor.check_and_get_loaded_account_shared_data();
⋮----
.store(*pubkey, slot, account.clone());
⋮----
Some((account, slot))
⋮----
fn do_load_with_populate_read_cache(
⋮----
assert!(max_root.is_none());
let starting_max_root = self.accounts_index.max_root_inclusive();
⋮----
self.read_index_for_accessor_or_load_slow(ancestors, pubkey, max_root, false)?;
⋮----
if load_zero_lamports == LoadZeroLamports::None && account.is_zero_lamport() {
⋮----
if self.read_only_accounts_cache.in_cache(pubkey, slot) {
⋮----
let ending_max_root = self.accounts_index.max_root_inclusive();
⋮----
fn get_account_accessor<'a>(
⋮----
let maybe_cached_account = self.accounts_cache.load(slot, pubkey).map(Cow::Owned);
⋮----
.get_account_storage_entry(slot, *store_id)
.map(|account_storage_entry| (account_storage_entry, *offset));
⋮----
fn create_store(
⋮----
let path_index = rng().random_range(0..paths.len());
let store = Arc::new(self.new_storage_entry(slot, Path::new(&paths[path_index]), size));
⋮----
fn create_and_insert_store(
⋮----
self.create_and_insert_store_with_paths(slot, size, from, &self.paths)
⋮----
fn create_and_insert_store_with_paths(
⋮----
let store = self.create_store(slot, size, from, paths);
let store_for_index = store.clone();
self.insert_store(slot, store_for_index);
⋮----
fn insert_store(&self, slot: Slot, store: Arc<AccountStorageEntry>) {
self.storage.insert(slot, store)
⋮----
pub fn enable_bank_drop_callback(&self) {
⋮----
.store(true, Ordering::Release);
⋮----
pub fn purge_slot(&self, slot: Slot, bank_id: BankId, is_serialized_with_abs: bool) {
if self.is_bank_drop_callback_enabled.load(Ordering::Acquire) && !is_serialized_with_abs {
⋮----
.remove(&bank_id)
⋮----
self.purge_slots(std::iter::once(&slot));
⋮----
pub fn purge_slots_from_cache_and_store<'a>(
⋮----
if let Some(slot_cache) = self.accounts_cache.slot_cache(*remove_slot) {
⋮----
total_removed_cached_bytes += slot_cache.total_bytes();
self.purge_slot_cache(*remove_slot, &slot_cache);
remove_cache_elapsed.stop();
remove_cache_elapsed_across_slots += remove_cache_elapsed.as_us();
assert!(self.accounts_cache.remove_slot(*remove_slot).is_some());
⋮----
self.purge_slot_storage(*remove_slot, purge_stats);
⋮----
.fetch_add(remove_cache_elapsed_across_slots, Ordering::Relaxed);
⋮----
.fetch_add(num_cached_slots_removed, Ordering::Relaxed);
⋮----
.fetch_add(total_removed_cached_bytes, Ordering::Relaxed);
⋮----
fn purge_dead_slots_from_storage<'a>(
⋮----
// Check all slots `removed_slots` are no longer "relevant" roots.
// Note that the slots here could have been rooted slots, but if they're passed here
⋮----
assert!(self
⋮----
safety_checks_elapsed.stop();
⋮----
.fetch_add(safety_checks_elapsed.as_us(), Ordering::Relaxed);
⋮----
let mut all_removed_slot_storages = vec![];
⋮----
if let Some(store) = self.storage.remove(remove_slot, false) {
total_removed_stored_bytes += store.accounts.capacity();
all_removed_slot_storages.push(store);
⋮----
remove_storage_entries_elapsed.stop();
let num_stored_slots_removed = all_removed_slot_storages.len();
⋮----
drop(all_removed_slot_storages);
drop_storage_entries_elapsed.stop();
⋮----
.fetch_add(remove_storage_entries_elapsed.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(drop_storage_entries_elapsed.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(num_stored_slots_removed, Ordering::Relaxed);
⋮----
.fetch_add(total_removed_stored_bytes, Ordering::Relaxed);
⋮----
.fetch_add(num_stored_slots_removed as u64, Ordering::Relaxed);
⋮----
fn purge_slot_cache(&self, purged_slot: Slot, slot_cache: &SlotCache) {
let pubkeys = slot_cache.iter().map(|account| *account.key());
self.purge_slot_cache_pubkeys(purged_slot, pubkeys, true);
⋮----
fn purge_slot_cache_pubkeys(
⋮----
let (reclaims, _) = self.purge_keys_exact(pubkeys.into_iter().map(|key| {
⋮----
assert_eq!(reclaims.len(), num_purged_keys);
⋮----
self.remove_dead_slots_metadata(std::iter::once(&purged_slot));
⋮----
fn purge_slot_storage(&self, remove_slot: Slot, purge_stats: &PurgeStats) {
⋮----
.get_slot_storage_entry_shrinking_in_progress_ok(remove_slot)
⋮----
.scan_pubkeys(|pk| {
stored_keys.insert((*pk, remove_slot));
⋮----
scan_storages_elapsed.stop();
⋮----
.fetch_add(scan_storages_elapsed.as_us(), Ordering::Relaxed);
⋮----
let (reclaims, pubkeys_removed_from_accounts_index) = self.purge_keys_exact(stored_keys);
purge_accounts_index_elapsed.stop();
⋮----
.fetch_add(purge_accounts_index_elapsed.as_us(), Ordering::Relaxed);
⋮----
let expected_dead_slot = Some(remove_slot);
⋮----
handle_reclaims_elapsed.stop();
⋮----
.fetch_add(handle_reclaims_elapsed.as_us(), Ordering::Relaxed);
⋮----
fn purge_slots<'a>(&self, slots: impl Iterator<Item = &'a Slot> + Clone) {
⋮----
.filter(|slot| !self.accounts_index.is_alive_root(**slot));
⋮----
self.purge_slots_from_cache_and_store(non_roots, &self.external_purge_slots_stats);
⋮----
.report("external_purge_slots_stats", Some(1000));
⋮----
pub fn remove_unrooted_slots(&self, remove_slots: &[(Slot, BankId)]) {
⋮----
.get_rooted_from_list(remove_slots.iter().map(|(slot, _)| slot));
⋮----
let mut currently_contended_slots = slots_under_contention.lock().unwrap();
⋮----
.filter_map(|(remove_slot, _)| {
let is_being_flushed = !currently_contended_slots.insert(*remove_slot);
is_being_flushed.then_some(remove_slot)
⋮----
.cloned()
⋮----
if !remaining_contended_flush_slots.is_empty() {
currently_contended_slots = signal.wait(currently_contended_slots).unwrap();
⋮----
remaining_contended_flush_slots.retain(|flush_slot| {
!currently_contended_slots.insert(*flush_slot)
⋮----
let mut locked_removed_bank_ids = self.accounts_index.removed_bank_ids.lock().unwrap();
for (_slot, remove_bank_id) in remove_slots.iter() {
locked_removed_bank_ids.insert(*remove_bank_id);
⋮----
self.purge_slots_from_cache_and_store(
remove_slots.iter().map(|(slot, _)| slot),
⋮----
remove_unrooted_purge_stats.report("remove_unrooted_slots_purge_slots_stats", None);
⋮----
assert!(currently_contended_slots.remove(remove_slot));
⋮----
pub fn lt_hash_account(account: &impl ReadableAccount, pubkey: &Pubkey) -> AccountLtHash {
if account.lamports() == 0 {
⋮----
AccountLtHash(lt_hash)
⋮----
fn hash_account_helper(account: &impl ReadableAccount, pubkey: &Pubkey) -> blake3::Hasher {
⋮----
buffer.extend_from_slice(&account.lamports().to_le_bytes());
let data = account.data();
if data.len() > DATA_SIZE {
hasher.update(&buffer);
buffer.clear();
hasher.update(data);
⋮----
buffer.extend_from_slice(data);
⋮----
buffer.push(account.executable().into());
buffer.extend_from_slice(account.owner().as_ref());
buffer.extend_from_slice(pubkey.as_ref());
⋮----
pub fn mark_slot_frozen(&self, slot: Slot) {
⋮----
slot_cache.mark_slot_frozen();
slot_cache.report_slot_store_metrics();
⋮----
self.accounts_cache.report_size();
⋮----
fn should_aggressively_flush_cache(&self) -> bool {
⋮----
.unwrap_or(WRITE_CACHE_LIMIT_BYTES_DEFAULT)
< self.accounts_cache.size()
⋮----
pub fn flush_accounts_cache(&self, force_flush: bool, requested_flush_root: Option<Slot>) {
⋮----
assert!(requested_flush_root.is_some());
if !force_flush && !self.should_aggressively_flush_cache() {
⋮----
let _guard = self.active_stats.activate(ActiveStatItem::Flush);
⋮----
.flush_rooted_accounts_cache(
⋮----
flush_roots_elapsed.stop();
⋮----
if self.should_aggressively_flush_cache() {
self.flush_rooted_accounts_cache(None, false)
⋮----
flush_stats.accumulate(&flush_stats_aggressively);
⋮----
let max_flushed_root = self.accounts_cache.fetch_max_flush_root();
⋮----
let mut old_slots = self.accounts_cache.cached_frozen_slots();
old_slots.sort_unstable();
excess_slot_count = old_slots.len();
⋮----
old_slots.into_iter().for_each(|old_slot| {
⋮----
if let Some(stats) = self.flush_slot_cache(old_slot) {
flush_stats.accumulate(&stats);
⋮----
fn flush_rooted_accounts_cache(
⋮----
.then(|| {
self.max_clean_root(requested_flush_root)
⋮----
.flatten();
⋮----
Some(move |&pubkey: &Pubkey| {
written_accounts.insert(pubkey)
⋮----
let flushed_roots: BTreeSet<Slot> = self.accounts_cache.clear_roots(requested_flush_root);
⋮----
for &root in flushed_roots.iter().rev() {
⋮----
self.flush_slot_cache_with_clean(root, should_flush_f.as_mut(), max_clean_root)
⋮----
if let Some(&root) = flushed_roots.last() {
self.accounts_cache.set_max_flush_root(root);
⋮----
let num_new_roots = flushed_roots.len();
⋮----
fn do_flush_slot_cache(
⋮----
let iter_items: Vec<_> = slot_cache.iter().collect();
let mut pubkeys: Vec<Pubkey> = vec![];
if should_flush_f.is_some() {
⋮----
.filter_map(|iter_item| {
let key = iter_item.key();
let account = &iter_item.value().account;
⋮----
.as_mut()
.map(|should_flush_f| should_flush_f(key))
.unwrap_or(true);
⋮----
aligned_stored_size(account.data().len()) as u64;
⋮----
Some((key, account))
⋮----
pubkeys.push(*key);
⋮----
let is_dead_slot = accounts.is_empty();
self.purge_slot_cache_pubkeys(slot, pubkeys, is_dead_slot);
⋮----
&& should_flush_f.is_some()
⋮----
let flushed_store = self.create_and_insert_store(
⋮----
let (store_accounts_timing_inner, store_accounts_total_inner_us) = measure_us!(self
⋮----
flush_stats.store_accounts_total_us = Saturating(store_accounts_total_inner_us);
assert!(self.storage.get_slot_storage_entry(slot).is_some());
⋮----
assert!(self.accounts_cache.remove_slot(slot).is_some());
⋮----
self.uncleaned_pubkeys.entry(slot).or_default().extend(
⋮----
.filter(|(_pubkey, account)| account.is_zero_lamport())
.map(|(pubkey, _account)| pubkey),
⋮----
.entry(slot)
⋮----
.extend(accounts.into_iter().map(|(pubkey, _account)| *pubkey));
⋮----
fn flush_slot_cache(&self, slot: Slot) -> Option<FlushStats> {
self.flush_slot_cache_with_clean(slot, None::<&mut fn(&_) -> bool>, None)
⋮----
fn flush_slot_cache_with_clean(
⋮----
.insert(slot)
⋮----
let flush_stats = self.accounts_cache.slot_cache(slot).map(|slot_cache| {
⋮----
self.do_flush_slot_cache(slot, &slot_cache, should_flush_f, max_clean_root)
⋮----
.notify_all();
⋮----
fn report_store_stats(&self) {
⋮----
for (slot, store) in self.storage.iter() {
⋮----
total_alive_bytes += store.alive_bytes();
⋮----
pub fn calculate_accounts_lt_hash_at_startup_from_index(
⋮----
.fold(
⋮----
for pubkey in accounts_index_bin.keys() {
⋮----
.get_with_and_then(
⋮----
Some(startup_slot),
⋮----
(!account_info.is_zero_lamport()).then(|| {
self.get_account_accessor(
⋮----
&account_info.storage_location(),
⋮----
accumulator_lt_hash.mix_in(&account_lt_hash.0);
⋮----
.reduce(LtHash::identity, |mut accum, elem| {
accum.mix_in(&elem);
⋮----
AccountsLtHash(lt_hash)
⋮----
pub fn calculate_capitalization_at_startup_from_index(
⋮----
.map(|accounts_index_bin| {
⋮----
.keys()
⋮----
.map(|pubkey| {
⋮----
loaded_account.lamports()
⋮----
.flatten()
.unwrap_or(0)
⋮----
.try_fold(0, u64::checked_add)
⋮----
.try_reduce(|| 0, u64::checked_add)
.expect("capitalization cannot overflow")
⋮----
fn apply_offset_to_slot(slot: Slot, offset: i64) -> Slot {
⋮----
slot.saturating_add(offset as u64)
⋮----
slot.saturating_sub(offset.unsigned_abs())
⋮----
pub fn get_pubkeys_for_slot(&self, slot: Slot) -> Vec<Pubkey> {
let scan_result = self.scan_cache_storage_fallback(
⋮----
|loaded_account| Some(*loaded_account.pubkey()),
⋮----
.scan_pubkeys(|pubkey| {
accum.insert(*pubkey);
⋮----
ScanStorageResult::Stored(stored_result) => stored_result.into_iter().collect(),
⋮----
pub fn get_pubkey_account_for_slot(&self, slot: Slot) -> Vec<(Pubkey, AccountSharedData)> {
let scan_result = self.scan_account_storage(
⋮----
Some((*loaded_account.pubkey(), loaded_account.take_account()))
⋮----
let data = data.unwrap();
⋮----
accum.insert(*loaded_account.pubkey(), loaded_account.take_account());
⋮----
fn update_index<'a>(
⋮----
let target_slot = accounts.target_slot();
let len = std::cmp::min(accounts.len(), infos.len());
⋮----
assert!(target_slot <= self.accounts_index.max_root_inclusive());
⋮----
(start..end).for_each(|i| {
⋮----
accounts.account(i, |account| {
let old_slot = accounts.slot(i);
self.accounts_index.upsert(
⋮----
account.pubkey(),
⋮----
if matches!(
⋮----
let chunk_size = std::cmp::max(1, len / quarter_thread_count());
⋮----
thread_pool.install(|| {
⋮----
.into_par_iter()
.map(|batch| {
⋮----
update(start, end)
⋮----
.filter(|reclaims| !reclaims.is_empty())
⋮----
let reclaims = update(0, len);
⋮----
vec![]
⋮----
vec![reclaims]
⋮----
fn should_not_shrink(alive_bytes: u64, total_bytes: u64) -> bool {
⋮----
fn is_shrinking_productive(store: &AccountStorageEntry) -> bool {
let alive_count = store.count();
let total_bytes = store.capacity();
let alive_bytes = store.alive_bytes_exclude_zero_lamport_single_ref_accounts() as u64;
⋮----
pub(crate) fn is_candidate_for_shrink(&self, store: &AccountStorageEntry) -> bool {
⋮----
fn remove_dead_accounts<'a, I>(
⋮----
assert!(self.storage.no_shrink_in_progress());
⋮----
// No cached accounts should make it here
assert!(!account_info.is_cached());
⋮----
.entry(*slot)
⋮----
.insert(account_info.offset());
⋮----
assert_eq!(reclaimed_offsets.len(), 1);
assert!(reclaimed_offsets.contains_key(&expected_slot));
⋮----
.fetch_add(reclaimed_offsets.len() as u64, Ordering::Relaxed);
reclaimed_offsets.iter().for_each(|(slot, offsets)| {
if let Some(store) = self.storage.get_slot_storage_entry(*slot) {
assert_eq!(
⋮----
let remaining_accounts = if offsets.len() == store.count() {
// all remaining alive accounts in the storage are being removed, so the entire storage/slot is dead
store.remove_accounts(store.alive_bytes(), offsets.len())
⋮----
// not all accounts are being removed, so figure out sizes of accounts we are removing and update the alive bytes and alive account count
let (remaining_accounts, us) = measure_us!({
⋮----
// sort so offsets are in order. This improves efficiency of loading the accounts.
⋮----
.fetch_add(us, Ordering::Relaxed);
⋮----
// Check if we have removed all accounts from the storage
// This may be different from the check above as this
// can be multithreaded
⋮----
self.dirty_stores.insert(*slot, store);
dead_slots.insert(*slot);
⋮----
// Checking that this single storage entry is ready for shrinking,
// should be a sufficient indication that the slot is ready to be shrunk
// because slots should only have one storage entry, namely the one that was
// created by `flush_slot_cache()`.
new_shrink_candidates.insert(*slot);
⋮----
measure.stop();
⋮----
.fetch_add(measure.as_us(), Ordering::Relaxed);
⋮----
let mut shrink_candidate_slots = self.shrink_candidate_slots.lock().unwrap();
⋮----
shrink_candidate_slots.insert(slot);
⋮----
drop(shrink_candidate_slots);
⋮----
dead_slots.retain(|slot| {
if let Some(slot_store) = self.storage.get_slot_storage_entry(*slot) {
if slot_store.count() != 0 {
⋮----
fn remove_dead_slots_metadata<'a>(&'a self, dead_slots_iter: impl Iterator<Item = &'a Slot>) {
self.clean_dead_slots_from_accounts_index(dead_slots_iter);
⋮----
fn unref_pubkeys<'a>(
⋮----
(0..batches).into_par_iter().for_each(|batch| {
⋮----
.skip(skip)
.take(UNREF_ACCOUNTS_BATCH_SIZE)
.filter(|pubkey| {
⋮----
pubkeys_removed_from_accounts_index.contains(pubkey);
⋮----
fn unref_accounts(
⋮----
self.unref_pubkeys(
purged_slot_pubkeys.iter().map(|(_slot, pubkey)| pubkey),
purged_slot_pubkeys.len(),
⋮----
.insert(slot);
⋮----
fn clean_dead_slots_from_accounts_index<'a>(
⋮----
.map(|slot| {
if self.accounts_index.clean_dead_slot(*slot) {
⋮----
accounts_index_root_stats.clean_dead_slot_us += measure.as_us();
if self.log_dead_slots.load(Ordering::Relaxed) {
⋮----
trace!("remove_dead_slots_metadata: dead_slots: {dead_slots:?}");
⋮----
.update_roots_stats(&mut accounts_index_root_stats);
⋮----
.update(&accounts_index_root_stats);
⋮----
/// pubkeys_removed_from_accounts_index - These keys have already been removed from the accounts index
    ///    and should not be unref'd. If they exist in the accounts index, they are NEW.
⋮----
///    and should not be unref'd. If they exist in the accounts index, they are NEW.
    fn clean_stored_dead_slots(
⋮----
fn clean_stored_dead_slots(
⋮----
let mut stores = vec![];
for slot in dead_slots.iter() {
if let Some(slot_storage) = self.storage.get_slot_storage_entry(*slot) {
stores.push(slot_storage);
⋮----
.map(|store| {
⋮----
let mut pubkeys = Vec::with_capacity(store.count());
⋮----
if !obsolete_accounts.contains(&(offset, account.data_len)) {
pubkeys.push((slot, *account.pubkey));
⋮----
self.unref_accounts(
⋮----
measure_unref.stop();
accounts_index_root_stats.clean_unref_from_storage_us += measure_unref.as_us();
⋮----
pub(crate) fn store_accounts_unfrozen<'a>(
⋮----
if accounts.is_empty() {
⋮----
(0..accounts.len()).for_each(|index| {
total_data += accounts.data_len(index);
⋮----
.fetch_add(total_data as u64, Ordering::Relaxed);
⋮----
let infos = self.write_accounts_to_cache(accounts.target_slot(), &accounts, transactions);
store_accounts_time.stop();
⋮----
.fetch_add(store_accounts_time.as_us(), Ordering::Relaxed);
⋮----
self.update_index(
⋮----
update_index_time.stop();
⋮----
.fetch_add(update_index_time.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(accounts.len() as u64, Ordering::Relaxed);
self.report_store_timings();
⋮----
pub fn store_accounts_frozen<'a>(
⋮----
self._store_accounts_frozen(
⋮----
fn _store_accounts_frozen<'a>(
⋮----
let slot = accounts.target_slot();
⋮----
if self.read_only_accounts_cache.can_slot_be_in_cache(slot) {
⋮----
.remove_assume_not_present(accounts.pubkey(index));
⋮----
let infos = self.write_accounts_to_storage(slot, storage, &accounts);
⋮----
self.mark_zero_lamport_single_ref_accounts(&infos, storage, reclaim_handling);
⋮----
let reclaims = self.update_index(
⋮----
let reclaims_len = reclaims.iter().map(|r| r.len()).sum::<usize>();
⋮----
.fetch_add(reclaims_len as u64, Ordering::Relaxed);
⋮----
reclaims.iter().flatten(),
⋮----
handle_reclaims_time.stop();
handle_reclaims_elapsed = handle_reclaims_time.as_us();
self.stats.num_obsolete_slots_removed.fetch_add(
purge_stats.num_stored_slots_removed.load(Ordering::Relaxed),
⋮----
self.stats.num_obsolete_bytes_removed.fetch_add(
⋮----
.load(Ordering::Relaxed),
⋮----
.fetch_add(handle_reclaims_elapsed, Ordering::Relaxed);
⋮----
store_accounts_elapsed: store_accounts_time.as_us(),
update_index_elapsed: update_index_time.as_us(),
⋮----
fn write_accounts_to_cache<'a, 'b>(
⋮----
let mut current_write_version = if self.accounts_update_notifier.is_some() {
⋮----
.fetch_add(accounts_and_meta_to_store.len() as u64, Ordering::AcqRel)
⋮----
(0..accounts_and_meta_to_store.len())
.map(|index| {
let txn = txs.map(|txs| *txs.get(index).expect("txs must be present if provided"));
accounts_and_meta_to_store.account_default_if_zero_lamport(index, |account| {
let account_shared_data = account.take_account();
let pubkey = account.pubkey();
⋮----
AccountInfo::new(StorageLocation::Cached, account.is_zero_lamport());
self.notify_account_at_accounts_update(
⋮----
current_write_version = current_write_version.saturating_add(1);
self.accounts_cache.store(slot, pubkey, account_shared_data);
⋮----
fn write_accounts_to_storage<'a>(
⋮----
let mut infos: Vec<AccountInfo> = Vec::with_capacity(accounts_and_meta_to_store.len());
⋮----
while infos.len() < accounts_and_meta_to_store.len() {
⋮----
.write_accounts(accounts_and_meta_to_store, infos.len());
append_accounts.stop();
total_append_accounts_us += append_accounts.as_us();
⋮----
// See if an account overflows the storage in the slot.
let data_len = accounts_and_meta_to_store.data_len(infos.len());
⋮----
if data_len > storage.accounts.remaining_bytes() {
⋮----
self.create_and_insert_store(slot, special_store_size, "large create");
⋮----
let store_id = storage.id();
for (i, offset) in stored_accounts_info.offsets.iter().enumerate() {
infos.push(AccountInfo::new(
⋮----
accounts_and_meta_to_store.is_zero_lamport(i),
⋮----
storage.add_accounts(
stored_accounts_info.offsets.len(),
⋮----
.fetch_add(total_append_accounts_us, Ordering::Relaxed);
⋮----
/// Marks zero lamport single reference accounts in the storage during store_accounts
    fn mark_zero_lamport_single_ref_accounts(
⋮----
fn mark_zero_lamport_single_ref_accounts(
⋮----
// If the reclaim handling is `ReclaimOldSlots`, then all zero lamport accounts are single
// ref accounts and they need to be inserted into the storages zero lamport single ref
// accounts list
// For other values of reclaim handling, there are no zero lamport single ref accounts
// so nothing needs to be done in this function
⋮----
storage.insert_zero_lamport_single_ref_account_offset(account_info.offset());
⋮----
// If any zero lamport accounts were added, the storage may be valid for shrinking
⋮----
&& self.is_candidate_for_shrink(storage)
⋮----
.insert(storage.slot);
⋮----
add_zero_lamport_accounts.stop();
⋮----
.fetch_add(add_zero_lamport_accounts.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(num_zero_lamport_accounts_added, Ordering::Relaxed);
⋮----
fn report_store_timings(&self) {
if self.stats.last_store_report.should_update(1000) {
let read_cache_stats = self.read_only_accounts_cache.get_and_reset_stats();
⋮----
pub fn add_root(&self, slot: Slot) -> AccountsAddRootTiming {
⋮----
self.accounts_index.add_root(slot);
index_time.stop();
⋮----
self.accounts_cache.add_root(slot);
cache_time.stop();
⋮----
index_us: index_time.as_us(),
cache_us: cache_time.as_us(),
⋮----
/// Returns storages for `requested_slots`
    pub fn get_storages(
⋮----
pub fn get_storages(
⋮----
.get_if(|slot, storage| requested_slots.contains(slot) && storage.has_accounts())
.into_vec()
⋮----
.unzip();
let duration = start.elapsed();
debug!("get_snapshot_storages: {duration:?}");
⋮----
/// Returns the latest full snapshot slot
    pub fn latest_full_snapshot_slot(&self) -> Option<Slot> {
⋮----
pub fn latest_full_snapshot_slot(&self) -> Option<Slot> {
self.latest_full_snapshot_slot.read()
⋮----
/// Sets the latest full snapshot slot to `slot`
    pub fn set_latest_full_snapshot_slot(&self, slot: Slot) {
⋮----
pub fn set_latest_full_snapshot_slot(&self, slot: Slot) {
*self.latest_full_snapshot_slot.lock_write() = Some(slot);
⋮----
fn generate_index_for_slot<'a>(
⋮----
if storage.accounts.get_account_data_lens(&[0]).is_empty() {
⋮----
let mut zero_lamport_pubkeys = vec![];
let mut zero_lamport_offsets = vec![];
⋮----
let mut keyed_account_infos = vec![];
⋮----
.filter(|notifier| notifier.snapshot_notifications_enabled());
⋮----
.scan_accounts(reader, |offset, account| {
if obsolete_accounts.contains(&offset) {
⋮----
let data_len = account.data.len();
stored_size_alive += storage.accounts.calculate_stored_size(data_len);
let is_account_zero_lamport = account.is_zero_lamport();
⋮----
zero_lamport_offsets.push(offset);
⋮----
zero_lamport_pubkeys.push(*account.pubkey);
⋮----
keyed_account_infos.push((
⋮----
if !self.account_indexes.is_empty() {
self.accounts_index.update_secondary_indexes(
⋮----
let account_lt_hash = Self::lt_hash_account(&account, account.pubkey());
slot_lt_hash.0.mix_in(&account_lt_hash.0);
⋮----
debug_assert!(geyser_notifier.snapshot_notifications_enabled());
⋮----
pubkey: account.pubkey(),
lamports: account.lamports(),
owner: account.owner(),
executable: account.executable(),
rent_epoch: account.rent_epoch(),
data: account.data(),
⋮----
geyser_notifier.notify_account_restore_from_snapshot(
⋮----
.insert_new_if_missing_into_primary_index(slot, keyed_account_infos);
⋮----
let mut info = storage_info.entry(store_id).or_default();
⋮----
if !zero_lamport_pubkeys.is_empty() {
⋮----
.insert(slot, zero_lamport_pubkeys.clone());
assert!(old.is_none());
⋮----
storage.batch_insert_zero_lamport_single_ref_account_offsets(&zero_lamport_offsets);
⋮----
pub fn generate_index(
⋮----
storages.sort_unstable_by_key(|storage| storage.slot);
⋮----
storages.truncate(limit);
⋮----
let num_storages = storages.len();
self.accounts_index.set_startup(Startup::Startup);
⋮----
struct IndexGenerationAccumulator {
⋮----
impl IndexGenerationAccumulator {
const fn new() -> Self {
⋮----
fn accumulate(&mut self, other: Self) {
⋮----
self.zero_lamport_pubkeys.extend(other.zero_lamport_pubkeys);
⋮----
self.all_zeros_slots.extend(other.all_zeros_slots);
⋮----
self.lt_hash.mix_in(&other.lt_hash);
⋮----
AccountStoragesOrderer::with_random_order(&storages).into_concurrent_consumer();
⋮----
.map(|i| {
⋮----
.name(format!("solGenIndex{i:02}"))
.spawn_scoped(s, || {
⋮----
while let Some(next_item) = storages_orderer.next() {
self.maybe_throttle_index_generation();
⋮----
let slot_info = self.generate_index_for_slot(
⋮----
.extend(slot_info.zero_lamport_pubkeys);
⋮----
thread_accum.all_zeros_slots.push((
⋮----
thread_accum.lt_hash.mix_in(&slot_info.slot_lt_hash.0);
⋮----
num_processed.fetch_add(1, Ordering::Relaxed);
⋮----
.expect("spawn threads");
⋮----
.name("solGenIndexLog".to_string())
⋮----
if exit_logger.load(Ordering::Relaxed) {
⋮----
let num_processed = num_processed.load(Ordering::Relaxed);
⋮----
info!("generating index: processed all slots");
⋮----
.expect("spawn thread");
⋮----
let Ok(thread_accum) = thread_handle.join() else {
exit_logger.store(true, Ordering::Relaxed);
panic!("index generation failed");
⋮----
total_accum.accumulate(thread_accum);
⋮----
logger_thread_handle.join().expect("join thread");
⋮----
let index_stats = self.accounts_index.stats();
index_stats.inc_insert_count(total_accum.num_did_not_exist);
index_stats.add_mem_count(total_accum.num_did_not_exist as usize);
⋮----
.fetch_add(total_accum.num_existed_in_mem, Ordering::Relaxed);
⋮----
index_stats.add_mem_count(total_accum.num_existed_on_disk as usize);
⋮----
.fetch_add(total_accum.num_existed_on_disk, Ordering::Relaxed);
⋮----
geyser_notifier.notify_end_of_restore_from_snapshot();
⋮----
info!("Verifying index...");
⋮----
storages.par_iter().for_each(|storage| {
⋮----
let key = account.pubkey();
self.accounts_index.get_and_then(key, |entry| {
let index_entry = entry.unwrap();
⋮----
for (slot2, account_info2) in slot_list.iter() {
⋮----
assert_eq!(&ai, account_info2);
⋮----
assert_eq!(1, count);
⋮----
info!("Verifying index... Done in {:?}", start.elapsed());
⋮----
self.accounts_index.set_startup(Startup::Normal);
m.stop();
let index_flush_us = m.as_us();
let populate_duplicate_keys_us = measure_us!({
⋮----
let unique_pubkeys_by_bin = unique_pubkeys_by_bin.into_inner().unwrap();
⋮----
index_time: index_time.as_us(),
⋮----
total_duplicate_slot_keys: total_duplicate_slot_keys.load(Ordering::Relaxed),
⋮----
struct DuplicatePubkeysVisitedInfo {
⋮----
impl DuplicatePubkeysVisitedInfo {
fn reduce(mut self, other: Self) -> Self {
⋮----
.mix_in(&other.duplicates_lt_hash.0);
⋮----
let (num_zero_lamport_single_refs, visit_zero_lamports_us) = measure_us!(
⋮----
.par_chunks(4096)
.fold(DuplicatePubkeysVisitedInfo::default, |accum, pubkeys| {
⋮----
) = self.visit_duplicate_pubkeys_during_startup(pubkeys);
⋮----
.reduce(
⋮----
visit_duplicate_accounts_timer.stop();
timings.visit_duplicate_accounts_time_us = visit_duplicate_accounts_timer.as_us();
⋮----
total_accum.lt_hash.mix_out(&duplicates_lt_hash.0);
⋮----
info!("accounts data len: {}", total_accum.accounts_data_len);
⋮----
self.dirty_stores.insert(slot, storage);
⋮----
self.accounts_index.add_root(storage.slot());
⋮----
self.set_storage_count_and_alive_bytes(storage_info, &mut timings);
⋮----
let slot_marked_obsolete = storages.last().unwrap().slot();
⋮----
self.mark_obsolete_accounts_at_startup(slot_marked_obsolete, unique_pubkeys_by_bin);
mark_obsolete_accounts_time.stop();
timings.mark_obsolete_accounts_us = mark_obsolete_accounts_time.as_us();
⋮----
total_time.stop();
timings.total_time_us = total_time.as_us();
timings.report(self.accounts_index.get_startup_stats());
self.accounts_index.log_secondary_indexes();
⋮----
.map(|bin| bin.capacity_for_startup())
⋮----
.stats()
⋮----
.store(index_capacity, Ordering::Relaxed);
⋮----
calculated_accounts_lt_hash: AccountsLtHash(total_accum.lt_hash),
⋮----
fn mark_obsolete_accounts_at_startup(
⋮----
.map(|pubkeys_by_bin| {
⋮----
.clean_and_unref_rooted_entries_by_bin(pubkeys_by_bin);
⋮----
accounts_marked_obsolete: reclaims.len() as u64,
slots_removed: stats.total_removed_storage_entries.load(Ordering::Relaxed)
⋮----
fn maybe_throttle_index_generation(&self) {
if !self.accounts_index.is_disk_index_enabled() {
⋮----
.get_startup_remaining_items_to_flush_estimate()
⋮----
sleep(Duration::from_millis(10));
⋮----
fn visit_zero_lamport_pubkeys_during_startup(&self, mut pubkeys: Vec<Pubkey>) -> u64 {
⋮----
let orig_len = pubkeys.len();
pubkeys.sort_unstable();
pubkeys.dedup();
let uniq_len = pubkeys.len();
⋮----
pubkeys.iter(),
⋮----
let (slot_list, ref_count) = slots_refs.unwrap();
⋮----
assert_eq!(slot_list.len(), 1);
let (slot_alive, account_info) = slot_list.first().unwrap();
⋮----
.entry(*slot_alive)
⋮----
.push(account_info.offset());
⋮----
count += store.batch_insert_zero_lamport_single_ref_account_offsets(&offsets);
⋮----
if self.shrink_candidate_slots.lock().unwrap().insert(slot) {
⋮----
.fetch_add(count, Ordering::Relaxed);
⋮----
.fetch_add(dead_stores, Ordering::Relaxed);
⋮----
.fetch_add(shrink_stores, Ordering::Relaxed);
⋮----
.fetch_add(non_shrink_stores, Ordering::Relaxed);
⋮----
fn visit_duplicate_pubkeys_during_startup(
⋮----
if slot_list.len() > 1 {
let max = slot_list.iter().map(|(slot, _)| slot).max().unwrap();
slot_list.iter().for_each(|(slot, account_info)| {
⋮----
.get_account_storage_entry(*slot, account_info.store_id());
⋮----
maybe_storage_entry.map(|entry| (entry, account_info.offset())),
⋮----
accessor.check_and_get_loaded_account(|loaded_account| {
let data_len = loaded_account.data_len();
if loaded_account.lamports() > 0 {
⋮----
duplicates_lt_hash.0.mix_in(&account_lt_hash.0);
⋮----
fn set_storage_count_and_alive_bytes(
⋮----
for (_slot, store) in self.storage.iter() {
let id = store.id();
assert_eq!(store.alive_bytes(), 0);
if let Some(entry) = stored_sizes_and_counts.get(&id) {
⋮----
let prev_count = store.count.swap(entry.count, Ordering::Release);
assert_eq!(prev_count, 0);
⋮----
.store(entry.stored_size, Ordering::Release);
⋮----
trace!("id: {id} clearing count");
store.count.store(0, Ordering::Release);
⋮----
storage_size_storages_time.stop();
timings.storage_size_storages_us = storage_size_storages_time.as_us();
⋮----
pub fn print_accounts_stats(&self, label: &str) {
self.print_index(label);
self.print_count_and_status(label);
⋮----
fn print_index(&self, label: &str) {
let mut alive_roots: Vec<_> = self.accounts_index.all_alive_roots();
⋮----
alive_roots.sort();
info!("{label}: accounts_index alive_roots: {alive_roots:?}");
self.accounts_index.account_maps.iter().for_each(|map| {
for pubkey in map.keys() {
self.accounts_index.get_and_then(&pubkey, |account_entry| {
⋮----
let list_r = account_entry.slot_list_read_lock();
info!(" key: {} ref_count: {}", pubkey, account_entry.ref_count(),);
info!("      slots: {list_r:?}");
⋮----
pub fn print_count_and_status(&self, label: &str) {
let mut slots: Vec<_> = self.storage.all_slots();
⋮----
slots.sort();
info!("{}: count_and status for {} slots:", label, slots.len());
⋮----
let entry = self.storage.get_slot_storage_entry(*slot).unwrap();
⋮----
enum HandleReclaims<'a> {
⋮----
enum MarkAccountsObsolete {
⋮----
pub enum UpdateIndexThreadSelection {
⋮----
fn accounts_count(&self) -> usize {
⋮----
.scan_pubkeys(|_| {
⋮----
pub(crate) fn obsolete_accounts(&self) -> &RwLock<ObsoleteAccounts> {
⋮----
pub fn default_for_tests() -> Self {
⋮----
pub fn new_single_for_tests() -> Self {
⋮----
pub fn new_single_for_tests_with_provider_and_config(
⋮----
pub fn new_for_tests(paths: Vec<PathBuf>) -> Self {
⋮----
fn new_for_tests_with_provider_and_config(
⋮----
pub fn get_len_of_slots_with_uncleaned_pubkeys(&self) -> usize {
self.uncleaned_pubkeys.len()
⋮----
pub fn storage_access(&self) -> StorageAccess {
⋮----
pub fn clean_accounts_for_tests(&self) {
self.clean_accounts(None, false, &EpochSchedule::default())
⋮----
pub fn flush_accounts_cache_slot_for_tests(&self, slot: Slot) {
self.flush_slot_cache(slot);
⋮----
pub fn add_root_and_flush_write_cache(&self, slot: Slot) {
self.add_root(slot);
self.flush_root_write_cache(slot);
⋮----
pub fn load_without_fixed_root(
⋮----
self.do_load(
⋮----
pub fn assert_load_account(&self, slot: Slot, pubkey: Pubkey, expected_lamports: u64) {
let ancestors = vec![(slot, 0)].into_iter().collect();
let (account, slot) = self.load_without_fixed_root(&ancestors, &pubkey).unwrap();
assert_eq!((account.lamports(), slot), (expected_lamports, slot));
⋮----
pub fn assert_not_load_account(&self, slot: Slot, pubkey: Pubkey) {
⋮----
let load = self.load_without_fixed_root(&ancestors, &pubkey);
assert!(load.is_none(), "{load:?}");
⋮----
pub fn check_accounts(&self, pubkeys: &[Pubkey], slot: Slot, num: usize, count: usize) {
⋮----
let idx = rng().random_range(0..num);
let account = self.load_without_fixed_root(&ancestors, &pubkeys[idx]);
let account1 = Some((
⋮----
AccountSharedData::default().owner(),
⋮----
assert_eq!(account, account1);
⋮----
pub fn store_for_tests<'a>(&self, accounts: impl StorableAccounts<'a>) {
self.store_accounts_unfrozen(
⋮----
pub fn modify_accounts(&self, pubkeys: &[Pubkey], slot: Slot, num: usize, count: usize) {
⋮----
self.store_for_tests((slot, [(&pubkeys[idx], &account)].as_slice()));
⋮----
pub fn check_storage(&self, slot: Slot, alive_count: usize, total_count: usize) {
let store = self.storage.get_slot_storage_entry(slot).unwrap();
assert_eq!(store.count(), alive_count);
assert_eq!(store.accounts_count(), total_count);
⋮----
pub fn create_account(
⋮----
AccountSharedData::new((t + 1) as u64, space, AccountSharedData::default().owner());
pubkeys.push(pubkey);
assert!(self.load_without_fixed_root(&ancestors, &pubkey).is_none());
self.store_for_tests((slot, [(&pubkey, &account)].as_slice()));
⋮----
pub fn assert_ref_count(&self, pubkey: &Pubkey, expected_ref_count: RefCount) {
⋮----
MarkObsoleteAccounts::Enabled => expected_ref_count.min(1),
⋮----
pub fn alive_account_count_in_slot(&self, slot: Slot) -> usize {
⋮----
.get_slot_storage_entry(slot)
.map(|storage| storage.count())
⋮----
.saturating_add(
⋮----
.slot_cache(slot)
.map(|slot_cache| slot_cache.len())
.unwrap_or_default(),
⋮----
pub fn flush_root_write_cache(&self, root: Slot) {
⋮----
self.flush_accounts_cache(true, Some(root));
⋮----
pub fn all_account_count_in_accounts_file(&self, slot: Slot) -> usize {
let store = self.storage.get_slot_storage_entry(slot);
⋮----
store.accounts_count()

================
File: accounts-db/src/accounts_file.rs
================
macro_rules! u64_align {
⋮----
pub type Result<T> = std::result::Result<T, AccountsFileError>;
⋮----
pub enum AccountsFileError {
⋮----
pub enum StorageAccess {
⋮----
pub enum AccountsFile {
⋮----
impl AccountsFile {
⋮----
pub fn new_from_file(
⋮----
Ok((Self::AppendVec(av), num_accounts))
⋮----
pub fn new_for_startup(
⋮----
Ok(Self::AppendVec(av))
⋮----
pub(crate) fn reopen_as_readonly(&self) -> Option<Self> {
⋮----
Self::AppendVec(av) => av.reopen_as_readonly_file_io().map(Self::AppendVec),
⋮----
pub(crate) fn dead_bytes_due_to_zero_lamport_single_ref(&self, count: usize) -> usize {
⋮----
Self::AppendVec(av) => av.dead_bytes_due_to_zero_lamport_single_ref(count),
Self::TieredStorage(ts) => ts.dead_bytes_due_to_zero_lamport_single_ref(count),
⋮----
pub fn flush(&self) -> Result<()> {
⋮----
Self::AppendVec(av) => av.flush()?,
⋮----
Ok(())
⋮----
pub fn remaining_bytes(&self) -> u64 {
⋮----
Self::AppendVec(av) => av.remaining_bytes(),
Self::TieredStorage(ts) => ts.capacity().saturating_sub(ts.len() as u64),
⋮----
pub fn len(&self) -> usize {
⋮----
Self::AppendVec(av) => av.len(),
Self::TieredStorage(ts) => ts.len(),
⋮----
pub fn is_empty(&self) -> bool {
⋮----
Self::AppendVec(av) => av.is_empty(),
Self::TieredStorage(ts) => ts.is_empty(),
⋮----
pub fn capacity(&self) -> u64 {
⋮----
Self::AppendVec(av) => av.capacity(),
Self::TieredStorage(ts) => ts.capacity(),
⋮----
pub fn file_name(slot: Slot, id: AccountsFileId) -> String {
format!("{slot}.{id}")
⋮----
pub fn get_stored_account_without_data_callback<Ret>(
⋮----
Self::AppendVec(av) => av.get_stored_account_without_data_callback(offset, callback),
⋮----
let index_offset = IndexOffset(AccountInfo::get_reduced_offset(offset));
ts.reader()?
.get_stored_account_without_data_callback(index_offset, callback)
.ok()?
⋮----
pub fn get_stored_account_callback<Ret>(
⋮----
Self::AppendVec(av) => av.get_stored_account_callback(offset, callback),
⋮----
.get_stored_account_callback(index_offset, callback)
⋮----
pub(crate) fn get_account_shared_data(&self, offset: usize) -> Option<AccountSharedData> {
⋮----
Self::AppendVec(av) => av.get_account_shared_data(offset),
⋮----
ts.reader()?.get_account_shared_data(index_offset).ok()?
⋮----
pub fn path(&self) -> &Path {
⋮----
Self::AppendVec(av) => av.path(),
Self::TieredStorage(ts) => ts.path(),
⋮----
pub fn scan_accounts_without_data(
⋮----
Self::AppendVec(av) => av.scan_accounts_without_data(callback)?,
⋮----
if let Some(reader) = ts.reader() {
reader.scan_accounts_without_data(callback)?;
⋮----
pub(crate) fn scan_accounts<'a>(
⋮----
Self::AppendVec(av) => av.scan_accounts(reader, callback)?,
⋮----
reader.scan_accounts(callback)?;
⋮----
/// Calculate the amount of storage required for an account with the passed
    /// in data_len
⋮----
/// in data_len
    pub(crate) fn calculate_stored_size(&self, data_len: usize) -> usize {
⋮----
pub(crate) fn calculate_stored_size(&self, data_len: usize) -> usize {
⋮----
.reader()
.expect("Reader must be initialized as stored size is specific to format")
.calculate_stored_size(data_len),
⋮----
/// for each offset in `sorted_offsets`, get the data size
    pub(crate) fn get_account_data_lens(&self, sorted_offsets: &[usize]) -> Vec<usize> {
⋮----
pub(crate) fn get_account_data_lens(&self, sorted_offsets: &[usize]) -> Vec<usize> {
⋮----
Self::AppendVec(av) => av.get_account_data_lens(sorted_offsets),
⋮----
.and_then(|reader| reader.get_account_data_lens(sorted_offsets).ok())
.unwrap_or_default(),
⋮----
/// iterate over all pubkeys
    pub fn scan_pubkeys(&self, callback: impl FnMut(&Pubkey)) -> Result<()> {
⋮----
pub fn scan_pubkeys(&self, callback: impl FnMut(&Pubkey)) -> Result<()> {
⋮----
Self::AppendVec(av) => av.scan_pubkeys(callback)?,
⋮----
reader.scan_pubkeys(callback)?;
⋮----
/// Copy each account metadata, account and hash to the internal buffer.
    /// If there is no room to write the first entry, None is returned.
⋮----
/// If there is no room to write the first entry, None is returned.
    /// Otherwise, returns the starting offset of each account metadata.
⋮----
/// Otherwise, returns the starting offset of each account metadata.
    /// Plus, the final return value is the offset where the next entry would be appended.
⋮----
/// Plus, the final return value is the offset where the next entry would be appended.
    /// So, return.len() is 1 + (number of accounts written)
⋮----
/// So, return.len() is 1 + (number of accounts written)
    /// After each account is appended, the internal `current_len` is updated
⋮----
/// After each account is appended, the internal `current_len` is updated
    /// and will be available to other threads.
⋮----
/// and will be available to other threads.
    pub fn write_accounts<'a>(
⋮----
pub fn write_accounts<'a>(
⋮----
Self::AppendVec(av) => av.append_accounts(accounts, skip),
// Note: The conversion here is needed as the AccountsDB currently
// assumes all offsets are multiple of 8 while TieredStorage uses
// IndexOffset that is equivalent to AccountInfo::reduced_offset.
⋮----
.write_accounts(accounts, skip, &HOT_FORMAT)
.map(|mut stored_accounts_info| {
stored_accounts_info.offsets.iter_mut().for_each(|offset| {
⋮----
.ok(),
⋮----
/// Returns the way to access this accounts file when archiving
    pub fn internals_for_archive(&self) -> InternalsForArchive<'_> {
⋮----
pub fn internals_for_archive(&self) -> InternalsForArchive<'_> {
⋮----
Self::AppendVec(av) => av.internals_for_archive(),
⋮----
ts.reader()
.expect("must be a reader when archiving")
.data_for_archive(),
⋮----
pub enum AccountsFileProvider {
⋮----
impl AccountsFileProvider {
pub fn new_writable(
⋮----
pub enum InternalsForArchive<'a> {
/// Accessing the internals is done via Mmap
    Mmap(&'a [u8]),
⋮----
pub struct StoredAccountsInfo {

================
File: accounts-db/src/accounts_hash.rs
================
use solana_lattice_hash::lt_hash::LtHash;
⋮----
pub struct AccountLtHash(pub LtHash);
pub const ZERO_LAMPORT_ACCOUNT_LT_HASH: AccountLtHash = AccountLtHash(LtHash::identity());
⋮----
pub struct AccountsLtHash(pub LtHash);

================
File: accounts-db/src/accounts_index.rs
================
mod account_map_entry;
mod accounts_index_storage;
mod bucket_map_holder;
pub(crate) mod in_mem_accounts_index;
mod iter;
mod roots_tracker;
mod secondary;
mod stats;
⋮----
pub const FLUSH_THREADS_TESTING: NonZeroUsize = NonZeroUsize::new(1).unwrap();
⋮----
bins: Some(BINS_FOR_TESTING),
num_flush_threads: Some(FLUSH_THREADS_TESTING),
⋮----
bins: Some(BINS_FOR_BENCHMARKS),
⋮----
pub type ScanResult<T> = Result<T, ScanError>;
pub type SlotList<T> = SmallVec<[(Slot, T); 1]>;
pub type ReclaimsSlotList<T> = Vec<(Slot, T)>;
pub type RefCount = u32;
pub type AtomicRefCount = AtomicU32;
⋮----
pub(crate) struct InsertNewIfMissingIntoPrimaryIndexInfo {
⋮----
pub enum ScanFilter {
⋮----
pub enum UpsertReclaim {
⋮----
pub struct ScanConfig {
⋮----
impl Default for ScanConfig {
fn default() -> Self {
⋮----
impl ScanConfig {
pub fn new(scan_order: ScanOrder) -> Self {
⋮----
pub fn abort(&self) {
if let Some(abort) = self.abort.as_ref() {
abort.store(true, Ordering::Relaxed)
⋮----
pub fn recreate_with_abort(&self) -> Self {
⋮----
abort: Some(self.abort.clone().unwrap_or_default()),
⋮----
pub fn is_aborted(&self) -> bool {
⋮----
abort.load(Ordering::Relaxed)
⋮----
pub enum ScanOrder {
⋮----
pub trait IsCached {
⋮----
pub trait IndexValue: 'static + IsCached + IsZeroLamport + DiskIndexValue {}
pub trait DiskIndexValue:
⋮----
pub enum ScanError {
⋮----
enum ScanTypes<R: RangeBounds<Pubkey>> {
⋮----
pub enum IndexLimit {
⋮----
pub struct AccountsIndexConfig {
⋮----
impl Default for AccountsIndexConfig {
⋮----
pub fn default_num_flush_threads() -> NonZeroUsize {
NonZeroUsize::new(std::cmp::max(2, num_cpus::get() / 4)).expect("non-zero system threads")
⋮----
pub struct AccountsIndexRootsStats {
⋮----
pub enum AccountsIndexScanResult {
⋮----
pub struct AccountsIndex<T: IndexValue, U: DiskIndexValue + From<T> + Into<T>> {
⋮----
pub fn default_for_tests() -> Self {
⋮----
pub fn new(config: &AccountsIndexConfig, exit: Arc<AtomicBool>) -> Self {
⋮----
fn bin_from_pubkey(&self, pubkey: &Pubkey) -> usize {
self.bin_calculator.bin_from_pubkey(pubkey)
⋮----
fn bin_start_end_inclusive<R>(&self, range: &R) -> (usize, usize)
⋮----
let start_bin = match range.start_bound() {
Bound::Included(start) => self.bin_from_pubkey(start),
⋮----
let start_bin = self.bin_from_pubkey(start);
⋮----
let end_bin_inclusive = match range.end_bound() {
Bound::Included(end) => self.bin_from_pubkey(end),
⋮----
let end_bin = self.bin_from_pubkey(end);
⋮----
end_bin.saturating_sub(1)
⋮----
Bound::Unbounded => self.account_maps.len().saturating_sub(1),
⋮----
fn allocate_accounts_index(
⋮----
let bins = config.bins.unwrap_or(BINS_DEFAULT);
⋮----
.map(|bin| Arc::clone(&storage.in_mem[bin]))
.collect();
⋮----
fn iter<'a, R>(
⋮----
pub fn is_disk_index_enabled(&self) -> bool {
self.storage.storage.is_disk_index_enabled()
⋮----
fn min_ongoing_scan_root_from_btree(ongoing_scan_roots: &BTreeMap<Slot, u64>) -> Option<Slot> {
ongoing_scan_roots.keys().next().cloned()
⋮----
fn do_checked_scan_accounts<F, R>(
⋮----
let locked_removed_bank_ids = self.removed_bank_ids.lock().unwrap();
if locked_removed_bank_ids.contains(&scan_bank_id) {
return Err(ScanError::SlotRemoved {
slot: ancestors.max_slot(),
⋮----
self.active_scans.fetch_add(1, Ordering::Relaxed);
⋮----
// This lock is also grabbed by clean_accounts(), so clean
// has at most cleaned up to the current `max_root` (since
// clean only happens *after* BankForks::set_root() which sets
// the `max_root`)
⋮----
.write()
.unwrap();
// `max_root()` grabs a lock while
// the `ongoing_scan_roots` lock is held,
// make sure inverse doesn't happen to avoid
let max_root_inclusive = self.max_root_inclusive();
⋮----
.fetch_max(current, Ordering::Relaxed);
⋮----
*w_ongoing_scan_roots.entry(max_root_inclusive).or_default() += 1;
⋮----
let ancestors = if ancestors.contains_key(&max_root) {
⋮----
self.do_scan_accounts(metric_name, ancestors, func, range, Some(max_root), config);
⋮----
self.do_scan_secondary_index(
⋮----
Some(max_root),
⋮----
self.active_scans.fetch_sub(1, Ordering::Relaxed);
let mut ongoing_scan_roots = self.ongoing_scan_roots.write().unwrap();
let count = ongoing_scan_roots.get_mut(&max_root).unwrap();
⋮----
ongoing_scan_roots.remove(&max_root);
⋮----
.lock()
.unwrap()
.contains(&scan_bank_id);
⋮----
Err(ScanError::SlotRemoved {
⋮----
Ok(())
⋮----
fn do_scan_accounts<F, R>(
⋮----
// TODO: expand to use mint index to find the `pubkey_list` below more efficiently
// instead of scanning the entire range
⋮----
for pubkeys in self.iter(range.as_ref(), returns_items) {
iterator_timer.stop();
iterator_elapsed += iterator_timer.as_us();
⋮----
self.get_and_then(&pubkey, |entry| {
⋮----
let list_r = &list.slot_list_read_lock();
read_lock_timer.stop();
read_lock_elapsed += read_lock_timer.as_us();
⋮----
if let Some(index) = self.latest_slot(Some(ancestors), list_r, max_root) {
latest_slot_timer.stop();
latest_slot_elapsed += latest_slot_timer.as_us();
⋮----
func(&pubkey, (&list_r[index].1, list_r[index].0));
load_account_timer.stop();
load_account_elapsed += load_account_timer.as_us();
⋮----
if config.is_aborted() {
⋮----
total_elapsed_timer.stop();
if !metric_name.is_empty() {
datapoint_info!(
⋮----
fn do_scan_secondary_index<
⋮----
for pubkey in index.get(index_key) {
⋮----
self.get_with_and_then(
⋮----
Some(ancestors),
⋮----
|(slot, account_info)| func(&pubkey, (&account_info, slot)),
⋮----
/// Gets the index's entry for `pubkey` and applies `callback` to it
    pub fn get_and_then<R>(
⋮----
pub fn get_and_then<R>(
⋮----
self.get_bin(pubkey).get_internal_inner(pubkey, callback)
⋮----
pub(crate) fn get_with_and_then<R>(
⋮----
self.get_and_then(pubkey, |entry| {
let callback_result = entry.and_then(|entry| {
self.get_account_info_with_and_then(entry, ancestors, max_root, callback)
⋮----
pub(crate) fn get_account_info_with_and_then<R>(
⋮----
let slot_list = entry.slot_list_read_lock();
self.latest_slot(ancestors, &slot_list, max_root)
.map(|found_index| callback(slot_list[found_index]))
⋮----
pub fn contains(&self, pubkey: &Pubkey) -> bool {
self.get_and_then(pubkey, |entry| (false, entry.is_some()))
⋮----
pub(crate) fn contains_with(
⋮----
self.get_with_and_then(pubkey, ancestors, max_root, false, |_| ())
.is_some()
⋮----
fn slot_list_mut<RT>(
⋮----
let read_lock = self.get_bin(pubkey);
read_lock.slot_list_mut(pubkey, user_fn)
⋮----
pub fn handle_dead_keys(
⋮----
if !dead_keys.is_empty() {
for key in dead_keys.iter() {
let w_index = self.get_bin(key);
if w_index.remove_if_slot_list_empty(*key) {
pubkeys_removed_from_accounts_index.insert(*key);
self.purge_secondary_indexes_by_inner_key(key, account_indexes);
⋮----
pub(crate) fn scan_accounts<F>(
⋮----
self.do_checked_scan_accounts(
⋮----
/// call func with every pubkey and index visible from a given set of ancestors
    pub(crate) fn index_scan_accounts<F>(
⋮----
pub(crate) fn index_scan_accounts<F>(
⋮----
// Pass "" not to log metrics, so RPC doesn't get spammy
⋮----
pub fn get_rooted_entries(
⋮----
let max_inclusive = max_inclusive.unwrap_or(Slot::MAX);
let lock = &self.roots_tracker.read().unwrap().alive_roots;
⋮----
.iter()
.filter(|(slot, _)| *slot <= max_inclusive && lock.contains(slot))
.cloned()
.collect()
⋮----
/// returns true if, after this fn call:
    /// accounts index entry for `pubkey` has an empty slot list
⋮----
/// accounts index entry for `pubkey` has an empty slot list
    /// or `pubkey` does not exist in accounts index
⋮----
/// or `pubkey` does not exist in accounts index
    pub(crate) fn purge_exact(
⋮----
pub(crate) fn purge_exact(
⋮----
self.slot_list_mut(pubkey, |mut slot_list| {
slot_list.retain_and_count(|(slot, item)| {
let should_purge = slots_to_purge.contains(slot);
⋮----
reclaims.push((*slot, *item));
⋮----
.unwrap_or(true)
⋮----
pub fn min_ongoing_scan_root(&self) -> Option<Slot> {
Self::min_ongoing_scan_root_from_btree(&self.ongoing_scan_roots.read().unwrap())
⋮----
// Given a SlotList `L`, a list of ancestors and a maximum slot, find the latest element
// in `L`, where the slot `S` is an ancestor or root, and if `S` is a root, then `S <= max_root`
pub(crate) fn latest_slot(
⋮----
if !ancestors.is_empty() {
for (i, (slot, _t)) in slot_list.iter().rev().enumerate() {
if (rv.is_none() || *slot > current_max) && ancestors.contains_key(slot) {
rv = Some(i);
⋮----
let max_root_inclusive = max_root_inclusive.unwrap_or(Slot::MAX);
⋮----
if (rv.is_none() || *slot > current_max) && *slot <= max_root_inclusive {
⋮----
None => self.roots_tracker.read().unwrap(),
⋮----
if lock.alive_roots.contains(slot) {
⋮----
tracker = Some(lock);
⋮----
rv.map(|index| slot_list.len() - 1 - index)
⋮----
pub(crate) fn stats(&self) -> &Stats {
⋮----
/// get stats related to startup
    pub(crate) fn get_startup_stats(&self) -> &StartupStats {
⋮----
pub(crate) fn get_startup_stats(&self) -> &StartupStats {
⋮----
pub(crate) fn set_startup(&self, value: Startup) {
self.storage.set_startup(value);
⋮----
pub fn get_startup_remaining_items_to_flush_estimate(&self) -> usize {
self.storage.get_startup_remaining_items_to_flush_estimate()
⋮----
/// Scan AccountsIndex for a given iterator of Pubkeys.
    ///
⋮----
///
    /// This fn takes 4 arguments.
⋮----
/// This fn takes 4 arguments.
    ///  - an iterator of pubkeys to scan
⋮----
///  - an iterator of pubkeys to scan
    ///  - callback fn to run for each pubkey in the accounts index
⋮----
///  - callback fn to run for each pubkey in the accounts index
    ///  - avoid_callback_result. If it is Some(default), then callback is ignored and
⋮----
///  - avoid_callback_result. If it is Some(default), then callback is ignored and
    ///    default is returned instead.
⋮----
///    default is returned instead.
    ///  - provide_entry_in_callback. If true, populate the ref of the Arc of the
⋮----
///  - provide_entry_in_callback. If true, populate the ref of the Arc of the
    ///    index entry to `callback` fn. Otherwise, provide None.
⋮----
///    index entry to `callback` fn. Otherwise, provide None.
    ///
⋮----
///
    /// The `callback` fn must return `AccountsIndexScanResult`, which is
⋮----
/// The `callback` fn must return `AccountsIndexScanResult`, which is
    /// used to indicates whether the AccountIndex Entry should be added to
⋮----
/// used to indicates whether the AccountIndex Entry should be added to
    /// in-memory cache. The `callback` fn takes in 3 arguments:
⋮----
/// in-memory cache. The `callback` fn takes in 3 arguments:
    ///   - the first an immutable ref of the pubkey,
⋮----
///   - the first an immutable ref of the pubkey,
    ///   - the second an option of the SlotList and RefCount
⋮----
///   - the second an option of the SlotList and RefCount
    ///   - the third an option of the AccountMapEntry, which is only populated
⋮----
///   - the third an option of the AccountMapEntry, which is only populated
    ///     when `provide_entry_in_callback` is true. Otherwise, it will be
⋮----
///     when `provide_entry_in_callback` is true. Otherwise, it will be
    ///     None.
⋮----
///     None.
    pub(crate) fn scan<'a, F, I>(
⋮----
pub(crate) fn scan<'a, F, I>(
⋮----
let mut last_bin = self.bins(); // too big, won't match
pubkeys.into_iter().for_each(|pubkey| {
let bin = self.bin_calculator.bin_from_pubkey(pubkey);
⋮----
// cannot reuse lock since next pubkey is in a different bin than previous one
lock = Some(&self.account_maps[bin]);
⋮----
let result = if let Some(result) = avoid_callback_result.as_ref() {
⋮----
let slot_list = locked_entry.slot_list_read_lock();
callback(pubkey, Some((slot_list.as_ref(), locked_entry.ref_count())))
⋮----
locked_entry.unref();
⋮----
assert_eq!(
⋮----
let old_ref = locked_entry.unref();
⋮----
info!(
⋮----
datapoint_warn!(
⋮----
avoid_callback_result.unwrap_or_else(|| callback(pubkey, None));
⋮----
lock.as_ref()
⋮----
.get_internal_inner(pubkey, internal_callback);
⋮----
.get_only_in_mem(pubkey, false, |mut entry| {
if entry.is_some() && matches!(filter, ScanFilter::OnlyAbnormalTest)
⋮----
let local_entry = entry.unwrap();
if local_entry.ref_count() == 1
&& local_entry.slot_list_lock_read_len() == 1
⋮----
internal_callback(entry);
entry.is_some()
⋮----
if !found && matches!(filter, ScanFilter::OnlyAbnormalWithVerify) {
lock.as_ref().unwrap().get_internal_inner(pubkey, |entry| {
assert!(entry.is_some(), "{pubkey}, entry: {entry:?}");
let entry = entry.unwrap();
assert_eq!(entry.ref_count(), 1, "{pubkey}");
assert_eq!(entry.slot_list_lock_read_len(), 1, "{pubkey}");
⋮----
fn get_newest_root_in_slot_list(
⋮----
.map(|(slot, _)| slot)
.filter(|slot| max_allowed_root_inclusive.is_none_or(|max_root| **slot <= max_root))
.filter(|slot| alive_roots.contains(slot))
.max()
.copied()
.unwrap_or(0)
⋮----
fn update_spl_token_secondary_indexes<G: spl_generic_token::token::GenericTokenAccount>(
⋮----
if account_indexes.contains(&AccountIndex::SplTokenOwner) {
⋮----
if account_indexes.include_key(owner_key) {
self.spl_token_owner_index.insert(owner_key, pubkey);
⋮----
if account_indexes.contains(&AccountIndex::SplTokenMint) {
⋮----
if account_indexes.include_key(mint_key) {
self.spl_token_mint_index.insert(mint_key, pubkey);
⋮----
pub fn get_index_key_size(&self, index: &AccountIndex, index_key: &Pubkey) -> Option<usize> {
⋮----
AccountIndex::ProgramId => self.program_id_index.index.get(index_key).map(|x| x.len()),
⋮----
.get(index_key)
.map(|x| x.len()),
⋮----
pub(crate) fn log_secondary_indexes(&self) {
if !self.program_id_index.index.is_empty() {
info!("secondary index: {:?}", AccountIndex::ProgramId);
self.program_id_index.log_contents();
⋮----
if !self.spl_token_mint_index.index.is_empty() {
info!("secondary index: {:?}", AccountIndex::SplTokenMint);
self.spl_token_mint_index.log_contents();
⋮----
if !self.spl_token_owner_index.index.is_empty() {
info!("secondary index: {:?}", AccountIndex::SplTokenOwner);
self.spl_token_owner_index.log_contents();
⋮----
pub(crate) fn update_secondary_indexes(
⋮----
if account_indexes.is_empty() {
⋮----
let account_owner = account.owner();
let account_data = account.data();
if account_indexes.contains(&AccountIndex::ProgramId)
&& account_indexes.include_key(account_owner)
⋮----
self.program_id_index.insert(account_owner, pubkey);
⋮----
pub(crate) fn get_bin(&self, pubkey: &Pubkey) -> &InMemAccountsIndex<T, U> {
&self.account_maps[self.bin_calculator.bin_from_pubkey(pubkey)]
⋮----
pub fn bins(&self) -> usize {
self.account_maps.len()
⋮----
pub(crate) fn insert_new_if_missing_into_primary_index(
⋮----
let use_disk = self.storage.storage.is_disk_index_enabled();
⋮----
let bins = self.bins();
let random_bin_offset = rng().random_range(0..bins);
⋮----
items.sort_unstable_by(|(pubkey_a, _), (pubkey_b, _)| {
((bin_calc.bin_from_pubkey(pubkey_a) + random_bin_offset) % bins)
.cmp(&((bin_calc.bin_from_pubkey(pubkey_b) + random_bin_offset) % bins))
.then_with(|| pubkey_a.cmp(pubkey_b))
⋮----
let storage = self.storage.storage.as_ref();
while !items.is_empty() {
let mut start_index = items.len() - 1;
⋮----
let pubkey_bin = bin_calc.bin_from_pubkey(last_pubkey);
⋮----
assert_ne!(
⋮----
if bin_calc.bin_from_pubkey(next_pubkey) != pubkey_bin {
⋮----
let r_account_maps = self.account_maps[pubkey_bin].as_ref();
count += items.len() - start_index;
let items = items.drain(start_index..);
⋮----
r_account_maps.startup_insert_only(slot, items);
⋮----
let mut duplicates_from_in_memory = vec![];
items.for_each(|(pubkey, account_info)| {
⋮----
match r_account_maps.insert_new_entry_if_missing_with_lock(pubkey, new_entry) {
⋮----
duplicates_from_in_memory.push((other_slot, pubkey));
⋮----
duplicates_from_in_memory.push((slot, pubkey));
⋮----
.startup_update_duplicates_from_in_memory_only(duplicates_from_in_memory);
⋮----
insert_time.stop();
⋮----
insert_time.as_us(),
⋮----
pub(crate) fn populate_and_retrieve_duplicate_keys_from_startup(
⋮----
(0..self.bins())
.into_par_iter()
.map(|pubkey_bin| {
⋮----
if self.storage.storage.is_disk_index_enabled() {
r_account_maps.populate_and_retrieve_duplicate_keys_from_startup()
⋮----
r_account_maps.startup_take_duplicates_from_in_memory_only()
⋮----
.for_each(f);
⋮----
pub fn upsert(
⋮----
let map = self.get_bin(pubkey);
map.upsert(pubkey, new_item, Some(old_slot), reclaims, reclaim);
self.update_secondary_indexes(pubkey, account, account_indexes);
⋮----
pub fn ref_count_from_storage(&self, pubkey: &Pubkey) -> RefCount {
⋮----
map.get_internal_inner(pubkey, |entry| {
⋮----
entry.map(|entry| entry.ref_count()).unwrap_or_default(),
⋮----
fn purge_secondary_indexes_by_inner_key(
⋮----
if account_indexes.contains(&AccountIndex::ProgramId) {
self.program_id_index.remove_by_inner_key(inner_key);
⋮----
self.spl_token_owner_index.remove_by_inner_key(inner_key);
⋮----
self.spl_token_mint_index.remove_by_inner_key(inner_key);
⋮----
fn purge_older_root_entries(
⋮----
if slot_list.len() <= 1 {
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
let roots_tracker = &self.roots_tracker.read().unwrap();
⋮----
max_clean_root_inclusive.unwrap_or_else(|| roots_tracker.alive_roots.max_inclusive())
⋮----
slot_list.retain_and_count(|(slot, value)| {
⋮----
) && !value.is_cached();
⋮----
reclaims.push((*slot, *value));
⋮----
pub fn clean_rooted_entries(
⋮----
.slot_list_mut(pubkey, |mut slot_list| {
is_slot_list_empty = self.purge_older_root_entries(
⋮----
.is_none();
⋮----
let w_maps = self.get_bin(pubkey);
removed = w_maps.remove_if_slot_list_empty(*pubkey);
⋮----
fn clean_and_unref_slot_list_on_startup(
⋮----
let mut slot_list = entry.slot_list_write_lock();
⋮----
.map(|(slot, _account)| *slot)
⋮----
.expect("Slot list has entries");
⋮----
assert!(!value.is_cached(), "Unsafe to reclaim cached entries");
⋮----
entry.unref_by_count(reclaim_count);
⋮----
entry.set_dirty(true);
⋮----
.last()
.expect("Slot list should have at least one entry after cleaning")
⋮----
pub fn clean_and_unref_rooted_entries_by_bin(
⋮----
let map = match pubkeys_by_bin.first() {
Some(pubkey) => self.get_bin(pubkey),
⋮----
let entry = entry.expect("Expected entry to exist in accounts index");
self.clean_and_unref_slot_list_on_startup(entry, &mut reclaims);
⋮----
fn can_purge_older_entries(
⋮----
pub fn get_rooted_from_list<'a>(&self, slots: impl Iterator<Item = &'a Slot>) -> Vec<Slot> {
let roots_tracker = self.roots_tracker.read().unwrap();
⋮----
.filter_map(|s| {
if roots_tracker.alive_roots.contains(s) {
Some(*s)
⋮----
pub fn is_alive_root(&self, slot: Slot) -> bool {
⋮----
.read()
⋮----
.contains(&slot)
⋮----
pub fn add_root(&self, slot: Slot) {
self.roots_added.fetch_add(1, Ordering::Relaxed);
let mut w_roots_tracker = self.roots_tracker.write().unwrap();
assert!(
⋮----
w_roots_tracker.alive_roots.insert(slot);
⋮----
pub fn max_root_inclusive(&self) -> Slot {
⋮----
.max_inclusive()
⋮----
pub fn clean_dead_slot(&self, slot: Slot) -> bool {
⋮----
if !w_roots_tracker.alive_roots.remove(&slot) {
⋮----
drop(w_roots_tracker);
self.roots_removed.fetch_add(1, Ordering::Relaxed);
⋮----
pub(crate) fn update_roots_stats(&self, stats: &mut AccountsIndexRootsStats) {
⋮----
stats.roots_len = Some(roots_tracker.alive_roots.len());
stats.roots_range = Some(roots_tracker.alive_roots.range_width());
⋮----
pub fn all_alive_roots(&self) -> Vec<Slot> {
let tracker = self.roots_tracker.read().unwrap();
tracker.alive_roots.get_all()
⋮----
pub fn purge_roots(&self, pubkey: &Pubkey) -> (SlotList<T>, bool) {
⋮----
let reclaims = self.get_rooted_entries(&slot_list, None);
let is_empty = slot_list.retain_and_count(|(slot, _)| !self.is_alive_root(*slot)) == 0;
⋮----
pub(crate) enum Startup {
⋮----
pub mod tests {
⋮----
pub enum SecondaryIndexTypes<'a> {
⋮----
pub fn spl_token_mint_index_enabled() -> AccountSecondaryIndexes {
⋮----
account_indexes.insert(AccountIndex::SplTokenMint);
⋮----
pub fn spl_token_owner_index_enabled() -> AccountSecondaryIndexes {
⋮----
account_indexes.insert(AccountIndex::SplTokenOwner);
⋮----
fn create_spl_token_mint_secondary_index_state() -> (usize, usize, AccountSecondaryIndexes) {
⋮----
(0, PUBKEY_BYTES, spl_token_mint_index_enabled())
⋮----
fn create_spl_token_owner_secondary_index_state() -> (usize, usize, AccountSecondaryIndexes) {
⋮----
spl_token_owner_index_enabled(),
⋮----
fn test_get_empty() {
⋮----
assert!(!index.contains_with(key, Some(&ancestors), None));
assert!(!index.contains_with(key, None, None));
⋮----
.scan_accounts(
⋮----
.expect("scan should succeed");
assert_eq!(num, 0);
⋮----
fn test_secondary_index_include_exclude() {
⋮----
assert!(!index.contains(&AccountIndex::ProgramId));
index.indexes.insert(AccountIndex::ProgramId);
assert!(index.contains(&AccountIndex::ProgramId));
assert!(index.include_key(&pk1));
assert!(index.include_key(&pk2));
⋮----
index.keys = Some(AccountSecondaryIndexesIncludeExclude {
keys: [pk1].iter().cloned().collect::<HashSet<_>>(),
⋮----
assert!(!index.include_key(&pk2));
⋮----
assert!(!index.include_key(&pk1));
⋮----
keys: [pk1, pk2].iter().cloned().collect::<HashSet<_>>(),
⋮----
fn test_insert_no_ancestors() {
⋮----
index.upsert(
⋮----
assert!(gc.is_empty());
⋮----
assert!(!index.contains_with(&key, Some(&ancestors), None));
assert!(!index.contains_with(&key, None, None));
⋮----
type AccountInfoTest = f64;
impl IndexValue for AccountInfoTest {}
impl DiskIndexValue for AccountInfoTest {}
impl IsCached for AccountInfoTest {
fn is_cached(&self) -> bool {
⋮----
impl IsZeroLamport for AccountInfoTest {
fn is_zero_lamport(&self) -> bool {
⋮----
fn test_insert_duplicates() {
⋮----
ancestors.insert(slot, 0);
⋮----
let items = vec![(*pubkey, account_info), (*pubkey, account_info2)];
index.set_startup(Startup::Startup);
let (_, _result) = index.insert_new_if_missing_into_primary_index(slot, items);
⋮----
fn test_insert_new_with_lock_no_ancestors() {
⋮----
let items = vec![(*pubkey, account_info)];
⋮----
let expected_len = items.len();
let (_, result) = index.insert_new_if_missing_into_primary_index(slot, items);
assert_eq!(result.count, expected_len);
index.set_startup(Startup::Normal);
⋮----
assert!(!index.contains_with(pubkey, Some(&ancestors), None));
assert!(!index.contains_with(pubkey, None, None));
⋮----
assert!(index.contains_with(pubkey, Some(&ancestors), None));
assert_eq!(index.ref_count_from_storage(pubkey), 1);
⋮----
assert_eq!(num, 1);
⋮----
fn get_pre_allocated<T: IndexValue>(
⋮----
let (slot2, account_info2) = entry.into();
⋮----
fn test_clean_and_unref_rooted_entries_by_bin_empty() {
⋮----
let pubkeys_by_bin: Vec<Pubkey> = vec![];
let reclaims = index.clean_and_unref_rooted_entries_by_bin(&pubkeys_by_bin);
assert!(reclaims.is_empty());
⋮----
fn test_clean_and_unref_rooted_entries_by_bin_single_entry() {
⋮----
let reclaims = index.clean_and_unref_rooted_entries_by_bin(&[pubkey]);
assert_eq!(reclaims.len(), 0);
⋮----
fn test_clean_and_unref_rooted_entries_by_bin_with_reclaim() {
⋮----
assert_eq!(reclaims, ReclaimsSlotList::from([(slot1, account_info1)]));
⋮----
fn test_clean_and_unref_rooted_entries_by_bin_multiple_pubkeys() {
⋮----
while pubkeys.len() < 10 {
⋮----
if index.bin_calculator.bin_from_pubkey(&new_pubkey) == bin_index {
pubkeys.push(new_pubkey);
⋮----
for (i, pubkey) in pubkeys.iter().enumerate() {
⋮----
expected_reclaims.push((slot - 1, true));
⋮----
let mut reclaims = index.clean_and_unref_rooted_entries_by_bin(&pubkeys);
reclaims.sort_unstable();
expected_reclaims.sort_unstable();
assert!(!reclaims.is_empty());
assert_eq!(reclaims, expected_reclaims);
⋮----
fn test_new_entry() {
⋮----
let new_entry = get_pre_allocated(
⋮----
.into_account_map_entry(&index.storage.storage);
assert_eq!(new_entry.ref_count(), 0);
assert_eq!(new_entry.slot_list_lock_read_len(), 1);
⋮----
assert_eq!(new_entry.ref_count(), 1);
⋮----
fn test_batch_insert() {
⋮----
let items = vec![(key0, account_infos[0]), (key1, account_infos[1])];
⋮----
let (_, result) = index.insert_new_if_missing_into_primary_index(slot0, items);
⋮----
for (i, key) in [key0, key1].iter().enumerate() {
index.get_and_then(key, |entry| {
assert_eq!(entry.unwrap().ref_count(), 1);
⋮----
fn test_new_entry_code_paths_helper<T: IndexValue>(
⋮----
if is_cached && upsert_method.is_none() {
⋮----
let items = vec![(key, account_infos[0])];
⋮----
index.get_and_then(&key, |entry| {
⋮----
assert_eq!(entry.ref_count(), RefCount::from(!is_cached));
assert_eq!(slot_list.as_ref(), &[(slot0, account_infos[0])]);
⋮----
assert_eq!(slot_list.as_ref(), new_entry.slot_list_read_lock().as_ref(),);
⋮----
let items = vec![(key, account_infos[1])];
⋮----
let (_, result) = index.insert_new_if_missing_into_primary_index(slot1, items);
⋮----
upsert_method == Some(UpsertReclaim::ReclaimOldSlots) && !is_cached;
⋮----
assert!(!gc.is_empty());
assert_eq!(gc.len(), 1);
assert_eq!(gc[0], (slot0, account_infos[0]));
⋮----
index.populate_and_retrieve_duplicate_keys_from_startup(|_slot_keys| {});
let last_item = index.get_and_then(&key, |entry| {
⋮----
assert_eq!(entry.ref_count(), 1);
assert_eq!(slot_list.as_ref(), &[(slot1, account_infos[1])],);
⋮----
assert_eq!(entry.ref_count(), if is_cached { 0 } else { 2 });
⋮----
(false, *slot_list.last().unwrap())
⋮----
assert_eq!(last_item, new_entry.into());
⋮----
fn test_new_entry_and_update_code_paths(
⋮----
test_new_entry_code_paths_helper([1.0, 2.0], true, upsert_method, use_disk);
⋮----
test_new_entry_code_paths_helper([1, 2], false, upsert_method, use_disk);
⋮----
fn test_insert_with_lock_no_ancestors() {
⋮----
assert_eq!(0, account_maps_stats_len(&index));
⋮----
let r_account_maps = index.get_bin(&key);
r_account_maps.upsert(
⋮----
assert_eq!(1, account_maps_stats_len(&index));
⋮----
let (stored_slot, value) = entry.unwrap().slot_list_read_lock()[0];
assert_eq!(stored_slot, slot);
assert_eq!(value, account_info);
⋮----
assert!(index.contains_with(&key, Some(&ancestors), None));
⋮----
fn test_insert_wrong_ancestors() {
⋮----
let ancestors = vec![(1, 1)].into_iter().collect();
⋮----
fn test_insert_ignore_reclaims() {
⋮----
assert!(!value.is_cached());
⋮----
reclaims.clear();
⋮----
assert!(value.is_cached());
⋮----
fn test_insert_with_ancestors() {
⋮----
let ancestors = vec![(0, 0)].into_iter().collect();
⋮----
.get_with_and_then(
⋮----
Some(&ancestors),
⋮----
assert_eq!(slot, 0);
assert!(account_info);
⋮----
assert!(found_key);
⋮----
fn setup_accounts_index_keys(num_pubkeys: usize) -> (AccountsIndex<bool, bool>, Vec<Pubkey>) {
⋮----
.take(num_pubkeys.saturating_sub(1))
⋮----
pubkeys.push(Pubkey::default());
⋮----
index.add_root(root_slot);
⋮----
fn run_test_scan_accounts(num_pubkeys: usize) {
let (index, _) = setup_accounts_index_keys(num_pubkeys);
⋮----
scanned_keys.insert(*pubkey);
⋮----
assert_eq!(scanned_keys.len(), num_pubkeys);
⋮----
fn test_scan_accounts() {
run_test_scan_accounts(0);
run_test_scan_accounts(1);
run_test_scan_accounts(ITER_BATCH_SIZE * 10);
run_test_scan_accounts(ITER_BATCH_SIZE * 10 - 1);
run_test_scan_accounts(ITER_BATCH_SIZE * 10 + 1);
⋮----
fn test_is_alive_root() {
⋮----
assert!(!index.is_alive_root(0));
index.add_root(0);
assert!(index.is_alive_root(0));
⋮----
fn test_insert_with_root() {
⋮----
.get_with_and_then(&key, None, None, false, |(slot, account_info)| {
⋮----
fn test_clean_first() {
⋮----
index.add_root(1);
index.clean_dead_slot(0);
assert!(index.is_alive_root(1));
⋮----
fn test_clean_last() {
⋮----
index.clean_dead_slot(1);
assert!(!index.is_alive_root(1));
⋮----
fn test_update_last_wins() {
⋮----
assert_eq!(account_info, 1);
⋮----
assert_eq!(gc, ReclaimsSlotList::from([(0, 1)]));
⋮----
assert_eq!(account_info, 0);
⋮----
fn test_update_new_slot() {
⋮----
let ancestors = vec![(1, 0)].into_iter().collect();
⋮----
assert_eq!(slot, 1);
assert!(!account_info);
⋮----
fn test_update_gc_purged_slot() {
⋮----
index.add_root(3);
⋮----
assert_eq!(gc, ReclaimsSlotList::new());
⋮----
assert_eq!(slot, 3);
⋮----
assert_eq!(index, (&true, 3));
⋮----
fn test_upsert_reclaims() {
⋮----
CacheableIndexValueTest(true),
⋮----
CacheableIndexValueTest(false),
⋮----
let slot_list_len = index.get_and_then(&key, |entry| {
(false, entry.unwrap().slot_list_lock_read_len())
⋮----
assert_eq!(slot_list_len, 1);
⋮----
fn account_maps_stats_len<T: IndexValue>(index: &AccountsIndex<T, T>) -> usize {
index.storage.storage.stats.total_count()
⋮----
fn test_purge() {
⋮----
let purges = index.purge_roots(&key);
assert_eq!(purges, (SlotList::new(), false));
⋮----
assert_eq!(purges, (SlotList::from([(1, 10)]), true));
⋮----
fn test_latest_slot() {
let slot_slice = vec![(0, true), (5, true), (3, true), (7, true)];
⋮----
assert!(index.latest_slot(None, &slot_slice, None).is_none());
index.add_root(5);
assert_eq!(index.latest_slot(None, &slot_slice, None).unwrap(), 1);
assert_eq!(index.latest_slot(None, &slot_slice, Some(5)).unwrap(), 1);
assert!(index.latest_slot(None, &slot_slice, Some(4)).is_none());
let ancestors = vec![(3, 1), (7, 1)].into_iter().collect();
⋮----
fn make_empty_token_account_data() -> Vec<u8> {
⋮----
let mut data = vec![0; spl_generic_token::token::Account::get_packed_len()];
⋮----
fn run_test_purge_exact_secondary_index<
⋮----
let slots = vec![1, 2, 5, 9];
⋮----
let mut account_data = make_empty_token_account_data();
account_data[key_start..key_end].clone_from_slice(&(index_key.to_bytes()));
⋮----
account_data.to_vec(),
⋮----
assert_eq!(secondary_index.index.get(&index_key).unwrap().len(), 1);
⋮----
index.purge_exact(
⋮----
slots.into_iter().collect::<HashSet<Slot>>(),
⋮----
let _ = index.handle_dead_keys(&[account_key], secondary_indexes);
assert!(secondary_index.index.is_empty());
assert!(secondary_index.reverse_index.is_empty());
⋮----
fn test_reclaim_older_items_in_slot_list() {
⋮----
assert_eq!(slot_list_len, reclaim_slot as usize);
⋮----
assert_eq!(slot_list_len, (reclaim_slot + 1) as usize);
⋮----
assert_eq!(gc.len(), reclaim_slot as usize);
for (slot, value) in gc.iter() {
assert!(*slot < reclaim_slot);
assert_eq!(*value, *slot);
⋮----
let ancestors = vec![(reclaim_slot, 0)].into_iter().collect();
⋮----
assert_eq!(slot, reclaim_slot);
assert_eq!(account_info, account_value);
⋮----
let ancestors = vec![((reclaim_slot + 1), 0)].into_iter().collect();
⋮----
assert_eq!(slot, reclaim_slot + 1);
assert_eq!(account_info, account_value + 1);
⋮----
fn test_reclaim_do_not_reclaim_cached_other_slot() {
⋮----
assert_eq!(entry.slot_list_lock_read_len(), 2);
⋮----
assert_eq!(gc[0], (0, CacheableIndexValueTest(false)));
⋮----
fn test_purge_exact_spl_token_mint_secondary_index() {
let (key_start, key_end, secondary_indexes) = create_spl_token_mint_secondary_index_state();
⋮----
run_test_purge_exact_secondary_index(
⋮----
fn test_purge_exact_spl_token_owner_secondary_index() {
⋮----
create_spl_token_owner_secondary_index_state();
⋮----
fn test_purge_older_root_entries() {
⋮----
assert!(!index.purge_older_root_entries(&mut slot_list, &mut reclaims, None));
⋮----
slot_list.assign([(1, true), (2, true), (5, true), (9, true)]);
⋮----
assert_eq!(reclaims, ReclaimsSlotList::from([(1, true), (2, true)]));
⋮----
slot_list.assign([(1 as Slot, true), (2, true), (5, true), (9, true)]);
index.add_root(6);
⋮----
assert!(!index.purge_older_root_entries(&mut slot_list, &mut reclaims, Some(6)));
⋮----
assert!(!index.purge_older_root_entries(&mut slot_list, &mut reclaims, Some(5)));
⋮----
assert!(!index.purge_older_root_entries(&mut slot_list, &mut reclaims, Some(2)));
⋮----
assert!(!index.purge_older_root_entries(&mut slot_list, &mut reclaims, Some(1)));
⋮----
assert!(!index.purge_older_root_entries(&mut slot_list, &mut reclaims, Some(7)));
⋮----
fn check_secondary_index_mapping_correct<SecondaryIndexEntryType>(
⋮----
assert_eq!(secondary_index.index.len(), secondary_index_keys.len());
let account_key_map = secondary_index.get(secondary_index_key);
assert_eq!(account_key_map.len(), 1);
assert_eq!(account_key_map, vec![*account_key]);
⋮----
let secondary_index_key_map = secondary_index.reverse_index.get(account_key).unwrap();
⋮----
fn run_test_spl_token_secondary_indexes<
⋮----
let mut secondary_indexes = secondary_indexes.clone();
⋮----
&AccountSharedData::create(0, account_data.to_vec(), Pubkey::default(), false, 0),
⋮----
&AccountSharedData::create(0, account_data[1..].to_vec(), *token_id, false, 0),
⋮----
index.update_secondary_indexes(
⋮----
&AccountSharedData::create(0, account_data.to_vec(), *token_id, false, 0),
⋮----
check_secondary_index_mapping_correct(secondary_index, &[index_key], &account_key);
⋮----
assert!(!secondary_index.index.is_empty());
assert!(!secondary_index.reverse_index.is_empty());
secondary_indexes.keys = Some(AccountSecondaryIndexesIncludeExclude {
keys: [index_key].iter().cloned().collect::<HashSet<_>>(),
⋮----
secondary_index.index.clear();
secondary_index.reverse_index.clear();
⋮----
keys: [].iter().cloned().collect::<HashSet<_>>(),
⋮----
index.slot_list_mut(&account_key, |mut slot_list| slot_list.clear());
let _ = index.handle_dead_keys(&[account_key], &secondary_indexes);
⋮----
fn test_spl_token_mint_secondary_index() {
⋮----
for token_id in &spl_token_ids() {
run_test_spl_token_secondary_indexes(
⋮----
fn test_spl_token_owner_secondary_index() {
⋮----
fn run_test_secondary_indexes_same_slot_and_forks<
⋮----
let mut account_data1 = make_empty_token_account_data();
⋮----
.clone_from_slice(&(secondary_key1.to_bytes()));
let mut account_data2 = make_empty_token_account_data();
⋮----
.clone_from_slice(&(secondary_key2.to_bytes()));
⋮----
&AccountSharedData::create(0, account_data1.to_vec(), *token_id, false, 0),
⋮----
&AccountSharedData::create(0, account_data2.to_vec(), *token_id, false, 0),
⋮----
check_secondary_index_mapping_correct(
⋮----
assert_eq!(secondary_index.get(&secondary_key1), vec![account_key]);
index.add_root(later_slot);
index.slot_list_mut(&account_key, |mut slot_list| {
index.purge_older_root_entries(&mut slot_list, &mut ReclaimsSlotList::new(), None)
⋮----
index.purge_exact(&account_key, later_slot, &mut reclaims);
⋮----
fn test_spl_token_mint_secondary_index_same_slot_and_forks() {
let (key_start, key_end, account_index) = create_spl_token_mint_secondary_index_state();
⋮----
run_test_secondary_indexes_same_slot_and_forks(
⋮----
fn test_rwlock_secondary_index_same_slot_and_forks() {
let (key_start, key_end, account_index) = create_spl_token_owner_secondary_index_state();
⋮----
impl IndexValue for bool {}
impl IndexValue for u64 {}
impl DiskIndexValue for bool {}
impl DiskIndexValue for u64 {}
impl IsCached for bool {
⋮----
impl IsCached for u64 {
⋮----
impl IsZeroLamport for bool {
⋮----
impl IsZeroLamport for u64 {
⋮----
struct CacheableIndexValueTest(bool);
impl IndexValue for CacheableIndexValueTest {}
impl DiskIndexValue for CacheableIndexValueTest {}
impl IsCached for CacheableIndexValueTest {
⋮----
impl IsZeroLamport for CacheableIndexValueTest {
⋮----
fn test_get_newest_root_in_slot_list() {
⋮----
let roots_tracker = &index.roots_tracker.read().unwrap();
⋮----
index.add_root(slot2);
⋮----
let slot_list = vec![(slot2, true)];
⋮----
fn upsert_simple_test(&self, key: &Pubkey, slot: Slot, value: T) {
⋮----
let reclaim_method = if self.is_alive_root(slot) {
⋮----
self.upsert(
⋮----
pub fn clear_roots(&self) {
self.roots_tracker.write().unwrap().alive_roots.clear()
⋮----
fn test_unref() {
⋮----
index.upsert_simple_test(&key, slot1, value);
⋮----
assert_eq!(entry.unref(), 1);
assert_eq!(entry.ref_count(), 0);
⋮----
fn test_illegal_unref() {
⋮----
entry.unref();
⋮----
fn test_clean_rooted_entries_return() {
⋮----
assert!(index.clean_rooted_entries(&key_unknown, &mut gc, None));
⋮----
assert!(!index.clean_rooted_entries(&key, &mut gc, None));
assert!(!index.clean_rooted_entries(&key, &mut gc, Some(slot1)));
⋮----
assert!(index.clean_rooted_entries(&key, &mut gc, Some(slot2)));
assert_eq!(gc, ReclaimsSlotList::from([(slot1, value)]));
⋮----
index.add_root(slot1);
assert!(!index.clean_rooted_entries(&key, &mut gc, Some(slot2)));
index.upsert_simple_test(&key, slot2, value);
⋮----
let account_map_entry = entry.unwrap();
let slot_list = account_map_entry.slot_list_read_lock();
assert_eq!(2, slot_list.len());
assert_eq!(&[(slot1, value), (slot2, value)], slot_list.as_ref());
⋮----
gc.clear();
index.clean_dead_slot(slot2);
⋮----
assert!(index.clean_rooted_entries(&key, &mut gc, Some(slot3)));
assert_eq!(gc, ReclaimsSlotList::from([(slot2, value)]));
⋮----
fn test_handle_dead_keys_return() {
⋮----
fn test_start_end_bin() {
⋮----
assert_eq!(index.bins(), BINS_FOR_TESTING);
⋮----
let (start, end) = index.bin_start_end_inclusive(&range);
assert_eq!(start, 0);
assert_eq!(end, BINS_FOR_TESTING - 1);
⋮----
assert_eq!(end, 0);
let range = (Included(key), Excluded(key));
⋮----
let range = (Excluded(key), Excluded(key));
⋮----
let bins = index.bins();
assert_eq!(start, bins - 1);
assert_eq!(end, bins - 1);
⋮----
assert_eq!(start, bins);
⋮----
fn test_illegal_bins() {
⋮----
config.bins = Some(3);
⋮----
fn test_scan_config() {
⋮----
assert_eq!(config.scan_order, scan_order);
assert!(config.abort.is_none());
assert!(!config.is_aborted());
config.abort();
⋮----
assert_eq!(config.scan_order, ScanOrder::Sorted);
⋮----
assert_eq!(config.scan_order, ScanOrder::Unsorted);
⋮----
let config = config.recreate_with_abort();
assert!(config.abort.is_some());
⋮----
assert!(config.is_aborted());

================
File: accounts-db/src/accounts_update_notifier_interface.rs
================
pub trait AccountsUpdateNotifierInterface: std::fmt::Debug {
⋮----
/// Notified when all accounts have been notified when restoring from a snapshot.
    fn notify_end_of_restore_from_snapshot(&self);
⋮----
pub type AccountsUpdateNotifier = Arc<dyn AccountsUpdateNotifierInterface + Sync + Send>;
/// Account type with only the fields necessary for Geyser
#[derive(Debug, Clone)]
pub struct AccountForGeyser<'a> {
⋮----
impl ReadableAccount for AccountForGeyser<'_> {
fn lamports(&self) -> u64 {
⋮----
fn data(&self) -> &[u8] {
⋮----
fn owner(&self) -> &Pubkey {
⋮----
fn executable(&self) -> bool {
⋮----
fn rent_epoch(&self) -> Epoch {

================
File: accounts-db/src/accounts.rs
================
pub type PubkeyAccountSlot = (Pubkey, AccountSharedData, Slot);
pub struct TransactionAccountLocksIterator<'a, T: SVMMessage> {
⋮----
pub fn new(transaction: &'a T) -> Self {
⋮----
pub fn accounts_with_is_writable(
⋮----
.account_keys()
.iter()
.enumerate()
.map(|(index, key)| (key, self.transaction.is_writable(index)))
⋮----
/// This structure handles synchronization for db
#[derive(Debug)]
pub struct Accounts {
/// Single global AccountsDb
    pub accounts_db: Arc<AccountsDb>,
/// set of read-only and writable accounts which are currently
    /// being processed by banking/replay threads
⋮----
/// being processed by banking/replay threads
    pub(crate) account_locks: Mutex<AccountLocks>,
⋮----
pub enum AccountAddressFilter {
Exclude, // exclude all addresses matching the filter
Include, // only include addresses matching the filter
⋮----
impl Accounts {
pub fn new(accounts_db: Arc<AccountsDb>) -> Self {
⋮----
/// Return loaded addresses and the deactivation slot.
    /// If the table hasn't been deactivated, the deactivation slot is `u64::MAX`.
⋮----
/// If the table hasn't been deactivated, the deactivation slot is `u64::MAX`.
    pub fn load_lookup_table_addresses(
⋮----
pub fn load_lookup_table_addresses(
⋮----
self.load_lookup_table_addresses_into(
⋮----
.map(|deactivation_slot| (loaded_addresses, deactivation_slot))
⋮----
pub fn load_lookup_table_addresses_into(
⋮----
.load_with_fixed_root(ancestors, address_table_lookup.account_key)
.map(|(account, _rent)| account)
.ok_or(AddressLookupError::LookupTableAccountNotFound)?;
if table_account.owner() == &address_lookup_table::program::id() {
let current_slot = ancestors.max_slot();
let lookup_table = AddressLookupTable::deserialize(table_account.data())
.map_err(|_ix_err| AddressLookupError::InvalidAccountData)?;
let writable_addresses = lookup_table.lookup_iter(
⋮----
let readonly_addresses = lookup_table.lookup_iter(
⋮----
.reserve(address_table_lookup.writable_indexes.len());
⋮----
.reserve(address_table_lookup.readonly_indexes.len());
⋮----
.push(address.ok_or(AddressLookupError::InvalidLookupIndex)?);
⋮----
Ok(lookup_table.meta.deactivation_slot)
⋮----
Err(AddressLookupError::InvalidAccountOwner)
⋮----
fn load_slow(
⋮----
self.accounts_db.load(ancestors, pubkey, load_hint)
⋮----
pub fn load_with_fixed_root(
⋮----
self.load_slow(ancestors, pubkey, LoadHint::FixedMaxRoot)
⋮----
pub fn load_with_fixed_root_do_not_populate_read_cache(
⋮----
self.load_slow(
⋮----
pub fn load_without_fixed_root(
⋮----
self.load_slow(ancestors, pubkey, LoadHint::Unspecified)
⋮----
pub fn scan_slot<F, B>(&self, slot: Slot, func: F) -> Vec<B>
⋮----
let scan_result = self.accounts_db.scan_account_storage(
⋮----
func(loaded_account)
⋮----
let data = data.unwrap();
⋮----
let loaded_account_pubkey = *loaded_account.pubkey();
if let Some(val) = func(&loaded_account) {
accum.insert(loaded_account_pubkey, val);
⋮----
ScanStorageResult::Stored(stored_result) => stored_result.into_values().collect(),
⋮----
pub fn load_by_program_slot(
⋮----
self.scan_slot(slot, |stored_account| {
⋮----
.map(|program_id| program_id == stored_account.owner())
.unwrap_or(true)
.then(|| (*stored_account.pubkey(), stored_account.take_account()))
⋮----
pub fn load_largest_accounts(
⋮----
return Ok(vec![]);
⋮----
self.accounts_db.scan_accounts(
⋮----
if account.lamports() == 0 {
⋮----
let contains_address = filter_by_address.contains(pubkey);
⋮----
if account_balances.len() == num {
⋮----
.peek()
.expect("BinaryHeap::peek should succeed when len > 0");
if *entry >= (account.lamports(), *pubkey) {
⋮----
account_balances.pop();
⋮----
account_balances.push(Reverse((account.lamports(), *pubkey)));
⋮----
Ok(account_balances
.into_sorted_vec()
.into_iter()
.map(|Reverse((balance, pubkey))| (pubkey, balance))
.collect())
⋮----
fn load_while_filtering<F: Fn(&AccountSharedData) -> bool>(
⋮----
.filter(|(_, account, _)| account.is_loadable() && filter(account))
.map(|(pubkey, account, _slot)| (*pubkey, account))
⋮----
collector.push(mapped_account_tuple)
⋮----
pub fn load_by_program(
⋮----
.scan_accounts(
⋮----
account.owner() == program_id
⋮----
.map(|_| collector)
⋮----
pub fn load_by_program_with_filter<F: Fn(&AccountSharedData) -> bool>(
⋮----
account.owner() == program_id && filter(account)
⋮----
fn calc_scan_result_size(account: &AccountSharedData) -> usize {
account.data().len()
⋮----
fn accumulate_and_check_scan_result_size(
⋮----
if let Some(byte_limit_for_scan) = byte_limit_for_scan.as_ref() {
⋮----
sum.fetch_add(added, Ordering::Relaxed)
.saturating_add(added)
⋮----
fn maybe_abort_scan(
⋮----
if config.is_aborted() {
⋮----
"The accumulated scan results exceeded the limit".to_string(),
⋮----
pub fn load_by_index_key_with_filter<F: Fn(&AccountSharedData) -> bool>(
⋮----
let config = config.recreate_with_abort();
⋮----
.index_scan_accounts(
⋮----
let use_account = filter(account);
⋮----
config.abort();
⋮----
.map(|_| collector);
⋮----
pub fn account_indexes_include_key(&self, key: &Pubkey) -> bool {
self.accounts_db.account_indexes.include_key(key)
⋮----
pub fn load_all(
⋮----
some_account_tuple.filter(|(_, account, _)| account.is_loadable())
⋮----
collector.push((*pubkey, account, slot))
⋮----
pub fn scan_all<F>(
⋮----
.scan_accounts(ancestors, bank_id, scan_func, &ScanConfig::new(scan_order))
⋮----
pub fn lock_accounts<'a>(
⋮----
// Validate the account locks, then get keys and is_writable if successful validation.
// We collect to fully evaluate before taking the account_locks mutex.
⋮----
.zip(results)
.map(|(tx, result)| {
⋮----
.and_then(|_| validate_account_locks(tx.account_keys(), tx_account_lock_limit))
.map(|_| TransactionAccountLocksIterator::new(tx).accounts_with_is_writable())
⋮----
let account_locks = &mut self.account_locks.lock().unwrap();
⋮----
account_locks.try_lock_transaction_batch(validated_batch_keys)
⋮----
.map(|result_validated_tx_keys| match result_validated_tx_keys {
Ok(validated_tx_keys) => account_locks.try_lock_accounts(validated_tx_keys),
Err(e) => Err(e),
⋮----
.collect()
⋮----
/// Once accounts are unlocked, new transactions that modify that state can enter the pipeline
    pub fn unlock_accounts<'a, Tx: SVMMessage + 'a>(
⋮----
pub fn unlock_accounts<'a, Tx: SVMMessage + 'a>(
⋮----
if !txs_and_results.clone().any(|(_, res)| res.is_ok()) {
⋮----
let mut account_locks = self.account_locks.lock().unwrap();
debug!("bank unlock accounts");
⋮----
if res.is_ok() {
⋮----
account_locks.unlock_accounts(tx_account_locks.accounts_with_is_writable());
⋮----
/// Store `accounts` into the DB
    ///
⋮----
///
    /// This version updates the accounts index sequentially,
⋮----
/// This version updates the accounts index sequentially,
    /// using the same thread that calls the fn itself.
⋮----
/// using the same thread that calls the fn itself.
    pub fn store_accounts_seq<'a>(
⋮----
pub fn store_accounts_seq<'a>(
⋮----
self.accounts_db.store_accounts_unfrozen(
⋮----
///
    /// This version updates the accounts index in parallel,
⋮----
/// This version updates the accounts index in parallel,
    /// using the foreground AccountsDb thread pool.
⋮----
/// using the foreground AccountsDb thread pool.
    pub fn store_accounts_par<'a>(
⋮----
pub fn store_accounts_par<'a>(
⋮----
pub fn add_root(&self, slot: Slot) -> AccountsAddRootTiming {
self.accounts_db.add_root(slot)
⋮----
mod tests {
⋮----
fn new_sanitized_tx<T: Signers>(
⋮----
fn sanitized_tx_from_metas(accounts: Vec<AccountMeta>) -> SanitizedTransaction {
⋮----
data: vec![],
⋮----
SanitizedTransaction::new_for_tests(sanitized_message, vec![Signature::new_unique()], false)
⋮----
fn test_load_lookup_table_addresses_account_not_found() {
let ancestors = vec![(0, 0)].into_iter().collect();
⋮----
writable_indexes: vec![],
readonly_indexes: vec![],
⋮----
assert_eq!(
⋮----
fn test_load_lookup_table_addresses_invalid_account_owner() {
⋮----
invalid_table_account.set_lamports(1);
accounts.store_for_tests(0, &invalid_table_key, &invalid_table_account);
accounts.add_root_and_flush_write_cache(0);
⋮----
fn test_load_lookup_table_addresses_invalid_account_data() {
⋮----
fn test_load_lookup_table_addresses() {
let ancestors = vec![(1, 1), (0, 0)].into_iter().collect();
⋮----
let table_addresses = vec![Pubkey::new_unique(), Pubkey::new_unique()];
⋮----
addresses: Cow::Owned(table_addresses.clone()),
⋮----
table_state.serialize_for_tests().unwrap(),
⋮----
accounts.store_for_tests(0, &table_key, &table_account);
⋮----
writable_indexes: vec![0],
readonly_indexes: vec![1],
⋮----
fn test_load_by_program_slot() {
⋮----
accounts.store_for_tests(0, &pubkey0, &account0);
⋮----
accounts.store_for_tests(0, &pubkey1, &account1);
⋮----
accounts.store_for_tests(0, &pubkey2, &account2);
⋮----
let loaded = accounts.load_by_program_slot(0, Some(&Pubkey::from([2; 32])));
assert_eq!(loaded.len(), 2);
let loaded = accounts.load_by_program_slot(0, Some(&Pubkey::from([3; 32])));
assert_eq!(loaded, vec![(pubkey2, account2)]);
let loaded = accounts.load_by_program_slot(0, Some(&Pubkey::from([4; 32])));
assert_eq!(loaded, vec![]);
⋮----
fn test_lock_accounts_with_duplicates(relax_intrabatch_account_locks: bool) {
⋮----
account_keys: vec![keypair.pubkey(), keypair.pubkey()],
⋮----
let tx = new_sanitized_tx(&[&keypair], message, Hash::default());
let results = accounts.lock_accounts(
[tx].iter(),
[Ok(())].into_iter(),
⋮----
assert_eq!(results[0], Err(TransactionError::AccountLoadedTwice));
⋮----
fn test_lock_accounts_with_too_many_accounts(relax_intrabatch_account_locks: bool) {
⋮----
.map(|_| Pubkey::new_unique())
.collect();
account_keys[0] = keypair.pubkey();
⋮----
let txs = [new_sanitized_tx(&[&keypair], message, Hash::default())];
⋮----
txs.iter(),
vec![Ok(()); txs.len()].into_iter(),
⋮----
assert_eq!(results, vec![Ok(())]);
accounts.unlock_accounts(txs.iter().zip(&results));
⋮----
assert_eq!(results[0], Err(TransactionError::TooManyAccountLocks));
⋮----
fn test_accounts_locks(relax_intrabatch_account_locks: bool) {
⋮----
accounts.store_for_tests(0, &keypair0.pubkey(), &account0);
accounts.store_for_tests(0, &keypair1.pubkey(), &account1);
accounts.store_for_tests(0, &keypair2.pubkey(), &account2);
accounts.store_for_tests(0, &keypair3.pubkey(), &account3);
let instructions = vec![CompiledInstruction::new(2, &(), vec![0, 1])];
⋮----
vec![keypair0.pubkey(), keypair1.pubkey(), native_loader::id()],
⋮----
let tx = new_sanitized_tx(&[&keypair0], message, Hash::default());
let results0 = accounts.lock_accounts(
[tx.clone()].iter(),
⋮----
assert_eq!(results0, vec![Ok(())]);
assert!(accounts
⋮----
vec![keypair2.pubkey(), keypair1.pubkey(), native_loader::id()],
⋮----
let tx0 = new_sanitized_tx(&[&keypair2], message, Hash::default());
⋮----
vec![keypair1.pubkey(), keypair3.pubkey(), native_loader::id()],
⋮----
let tx1 = new_sanitized_tx(&[&keypair1], message, Hash::default());
⋮----
let results1 = accounts.lock_accounts(
⋮----
accounts.unlock_accounts(iter::once(&tx).zip(&results0));
accounts.unlock_accounts(txs.iter().zip(&results1));
⋮----
let tx = new_sanitized_tx(&[&keypair1], message, Hash::default());
let results2 = accounts.lock_accounts(
⋮----
assert!(!accounts
⋮----
fn test_accounts_locks_multithreaded(relax_intrabatch_account_locks: bool) {
⋮----
let readonly_tx = new_sanitized_tx(&[&keypair0], readonly_message, Hash::default());
⋮----
vec![keypair1.pubkey(), keypair2.pubkey(), native_loader::id()],
⋮----
let writable_tx = new_sanitized_tx(&[&keypair1], writable_message, Hash::default());
let counter_clone = counter.clone();
let accounts_clone = accounts_arc.clone();
let exit_clone = exit.clone();
⋮----
let txs = [writable_tx.clone()];
let results = accounts_clone.clone().lock_accounts(
⋮----
for result in results.iter() {
if result.is_ok() {
counter_clone.clone().fetch_add(1, Ordering::Release);
⋮----
accounts_clone.unlock_accounts(txs.iter().zip(&results));
if exit_clone.clone().load(Ordering::Relaxed) {
⋮----
let txs = [readonly_tx.clone()];
let results = accounts_arc.clone().lock_accounts(
⋮----
if results[0].is_ok() {
let counter_value = counter_clone.clone().load(Ordering::Acquire);
⋮----
assert_eq!(counter_value, counter_clone.clone().load(Ordering::Acquire));
⋮----
accounts_arc.unlock_accounts(txs.iter().zip(&results));
⋮----
exit.store(true, Ordering::Relaxed);
⋮----
fn test_demote_program_write_locks(relax_intrabatch_account_locks: bool) {
⋮----
assert!(results0[0].is_ok());
⋮----
pub fn store_for_tests(&self, slot: Slot, pubkey: &Pubkey, account: &AccountSharedData) {
⋮----
.store_for_tests((slot, [(pubkey, account)].as_slice()))
⋮----
pub fn add_root_and_flush_write_cache(&self, slot: Slot) {
self.add_root(slot);
self.accounts_db.flush_accounts_cache_slot_for_tests(slot);
⋮----
fn test_accounts_locks_with_results(relax_intrabatch_account_locks: bool) {
⋮----
vec![keypair1.pubkey(), keypair0.pubkey(), native_loader::id()],
⋮----
let tx0 = new_sanitized_tx(&[&keypair1], message, Hash::default());
⋮----
vec![keypair2.pubkey(), keypair0.pubkey(), native_loader::id()],
⋮----
let tx1 = new_sanitized_tx(&[&keypair2], message, Hash::default());
⋮----
vec![keypair3.pubkey(), keypair0.pubkey(), native_loader::id()],
⋮----
let tx2 = new_sanitized_tx(&[&keypair3], message, Hash::default());
⋮----
let qos_results = vec![
⋮----
qos_results.into_iter(),
⋮----
fn test_accounts_locks_intrabatch_conflicts(relax_intrabatch_account_locks: bool) {
⋮----
accounts_db.store_for_tests((
⋮----
.as_slice(),
⋮----
let r_tx = sanitized_tx_from_metas(vec![AccountMeta {
⋮----
let w_tx = sanitized_tx_from_metas(vec![AccountMeta {
⋮----
let accounts = Accounts::new(accounts_db.clone());
⋮----
[w_tx.clone()].iter(),
⋮----
[r_tx.clone()].iter(),
⋮----
assert_eq!(results, vec![Err(TransactionError::AccountInUse)]);
⋮----
[w_tx.clone(), r_tx.clone()].iter(),
[Ok(()), Ok(())].into_iter(),
⋮----
assert_eq!(results, vec![Ok(()), Ok(())]);
⋮----
assert_eq!(results, vec![Ok(()), Err(TransactionError::AccountInUse)]);
⋮----
fn huge_clean() {
⋮----
let zero_account = AccountSharedData::new(0, 0, AccountSharedData::default().owner());
info!("storing..");
⋮----
let account = AccountSharedData::new(i + 1, 0, AccountSharedData::default().owner());
accounts.store_for_tests(i, &pubkey, &account);
accounts.store_for_tests(i, &old_pubkey, &zero_account);
⋮----
accounts.add_root_and_flush_write_cache(i);
⋮----
info!("  store {i}");
⋮----
info!("done..cleaning..");
accounts.accounts_db.clean_accounts_for_tests();
⋮----
fn test_load_largest_accounts() {
⋮----
let mut keys = vec![];
⋮----
keys.push(Pubkey::new_unique());
⋮----
keys.sort();
let pubkey2 = keys.pop().unwrap();
let pubkey1 = keys.pop().unwrap();
let pubkey0 = keys.pop().unwrap();
⋮----
let all_pubkeys: HashSet<_> = vec![pubkey0, pubkey1, pubkey2].into_iter().collect();
⋮----
assert!(pubkey1 > pubkey0);
⋮----
let exclude1: HashSet<_> = vec![pubkey1].into_iter().collect();
⋮----
let include1_2: HashSet<_> = vec![pubkey1, pubkey2].into_iter().collect();
⋮----
fn zero_len_account_size() -> usize {
⋮----
fn test_calc_scan_result_size() {
⋮----
fn test_maybe_abort_scan() {
assert!(Accounts::maybe_abort_scan(ScanResult::Ok(vec![]), &ScanConfig::default()).is_ok());
assert!(Accounts::maybe_abort_scan(
⋮----
let config = ScanConfig::new(ScanOrder::Sorted).recreate_with_abort();
assert!(Accounts::maybe_abort_scan(ScanResult::Ok(vec![]), &config).is_ok());
⋮----
assert!(Accounts::maybe_abort_scan(ScanResult::Ok(vec![]), &config).is_err());
⋮----
fn test_accumulate_and_check_scan_result_size() {
⋮----
(AccountSharedData::default(), zero_len_account_size(), false),
⋮----
zero_len_account_size(),
⋮----
zero_len_account_size() + 3,
⋮----
assert!(Accounts::accumulate_and_check_scan_result_size(
⋮----
assert!(!Accounts::accumulate_and_check_scan_result_size(
⋮----
fn test_batched_locking() {
⋮----
let txs = vec![tx0, tx1, tx2];
let qos_results = vec![Ok(()), Ok(()), Ok(())];

================
File: accounts-db/src/active_stats.rs
================
pub struct ActiveStats {
⋮----
pub enum ActiveStatItem {
⋮----
pub struct ActiveStatGuard<'a> {
⋮----
impl Drop for ActiveStatGuard<'_> {
fn drop(&mut self) {
self.stats.update_and_log(self.item, |stat| {
stat.fetch_sub(1, Ordering::Relaxed).wrapping_sub(1)
⋮----
impl ActiveStats {
⋮----
/// create a stack object to set the state to increment stat initially and decrement on drop
    pub fn activate(&self, stat: ActiveStatItem) -> ActiveStatGuard<'_> {
⋮----
pub fn activate(&self, stat: ActiveStatItem) -> ActiveStatGuard<'_> {
self.update_and_log(stat, |stat| {
stat.fetch_add(1, Ordering::Relaxed).wrapping_add(1)
⋮----
fn update_and_log(&self, item: ActiveStatItem, modify_stat: impl Fn(&AtomicUsize) -> usize) {
⋮----
let value = modify_stat(stat);
⋮----
ActiveStatItem::Clean => datapoint_info!("accounts_db_active", ("clean", value, i64)),
ActiveStatItem::CleanConstructCandidates => datapoint_info!(
⋮----
datapoint_info!("accounts_db_active", ("clean_scan_candidates", value, i64))
⋮----
datapoint_info!("accounts_db_active", ("clean_old_accounts", value, i64))
⋮----
datapoint_info!(
⋮----
datapoint_info!("accounts_db_active", ("clean_calc_delete_deps", value, i64))
⋮----
ActiveStatItem::CleanFilterZeroLamport => datapoint_info!(
⋮----
datapoint_info!("accounts_db_active", ("clean_reclaims", value, i64))
⋮----
datapoint_info!("accounts_db_active", ("squash_ancient", value, i64))
⋮----
datapoint_info!("accounts_db_active", ("shrink", value, i64))
⋮----
ActiveStatItem::Flush => datapoint_info!("accounts_db_active", ("flush", value, i64)),

================
File: accounts-db/src/ancestors.rs
================
pub type AncestorsForSerialization = HashMap<Slot, usize>;
⋮----
pub struct Ancestors {
⋮----
impl Debug for Ancestors {
fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
write!(f, "{:?}", self.keys())
⋮----
impl Default for Ancestors {
fn default() -> Self {
⋮----
fn from(mut source: Vec<Slot>) -> Ancestors {
source.sort_unstable();
⋮----
source.into_iter().for_each(|slot| {
result.ancestors.insert(slot);
⋮----
fn from(source: &HashMap<Slot, usize>) -> Ancestors {
let vec = source.keys().copied().collect::<Vec<_>>();
⋮----
fn from(source: &Ancestors) -> HashMap<Slot, usize> {
let mut result = HashMap::with_capacity(source.len());
source.keys().iter().for_each(|slot| {
result.insert(*slot, 0);
⋮----
impl Ancestors {
pub fn keys(&self) -> Vec<Slot> {
self.ancestors.get_all()
⋮----
pub fn remove(&mut self, slot: &Slot) {
self.ancestors.remove(slot);
⋮----
pub fn contains_key(&self, slot: &Slot) -> bool {
self.ancestors.contains(slot)
⋮----
pub fn len(&self) -> usize {
self.ancestors.len()
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub fn min_slot(&self) -> Slot {
self.ancestors.min().unwrap_or_default()
⋮----
pub fn max_slot(&self) -> Slot {
self.ancestors.max_exclusive().saturating_sub(1)
⋮----
fn from_iter<I>(iter: I) -> Self
⋮----
data.push(i);
⋮----
fn from(source: Vec<(Slot, usize)>) -> Ancestors {
Ancestors::from(source.into_iter().map(|(slot, _)| slot).collect::<Vec<_>>())
⋮----
pub fn insert(&mut self, slot: Slot, _size: usize) {
self.ancestors.insert(slot);
⋮----
pub mod tests {
⋮----
fn test_ancestors_permutations() {
⋮----
while hash.len() < width {
⋮----
hash.insert(slot, 0);
ancestors.insert(slot, 0);
⋮----
compare_ancestors(&hash, &ancestors);
⋮----
if hash.contains(&slot) {
⋮----
time.stop();
⋮----
if ancestors.contains_key(&slot) {
⋮----
time2.stop();
info!(
⋮----
assert_eq!(count, count2);
⋮----
fn compare_ancestors(hashset: &HashMap<u64, usize>, ancestors: &Ancestors) {
assert_eq!(hashset.len(), ancestors.len());
assert_eq!(hashset.is_empty(), ancestors.is_empty());
⋮----
for item in hashset.iter() {
⋮----
assert!(ancestors.contains_key(key));
⋮----
assert_eq!(ancestors.contains_key(&slot), hashset.contains(&slot));
⋮----
fn test_ancestors_smaller() {
⋮----
hash.insert(slot);
slots.push((slot, 0));

================
File: accounts-db/src/ancient_append_vecs.rs
================
struct PackedAncientStorageTuning {
⋮----
struct SlotInfo {
⋮----
struct AncientSlotInfos {
⋮----
impl AncientSlotInfos {
fn add(
⋮----
let alive_bytes = storage.alive_bytes() as u64;
⋮----
let capacity = storage.accounts.capacity();
⋮----
} else if can_randomly_shrink && rng().random_range(0..10000) == 0 {
⋮----
self.shrink_indexes.push(self.all_infos.len());
⋮----
self.all_infos.push(SlotInfo {
⋮----
fn filter_ancient_slots(
⋮----
self.choose_storages_to_shrink(tuning);
self.filter_by_smallest_capacity(tuning, stats);
⋮----
fn sort_shrink_indexes_by_bytes_saved(&mut self) {
self.shrink_indexes.sort_unstable_by(|l, r| {
⋮----
let aligned_capacity = u64_align!(item.capacity as usize) as u64;
⋮----
datapoint_warn!(
⋮----
item.capacity.saturating_sub(item.alive_bytes)
⋮----
amount_shrunk(r).cmp(&amount_shrunk(l))
⋮----
fn clear_should_shrink_after_cutoff(&mut self, tuning: &PackedAncientStorageTuning) {
let mut bytes_to_shrink_due_to_ratio = Saturating(0);
⋮----
(self.total_alive_bytes_shrink.0 * tuning.percent_of_alive_shrunk_data / 100).min(
⋮----
self.best_slots_to_shrink = VecDeque::with_capacity(self.shrink_indexes.len());
⋮----
.push_back((info.slot, info.capacity));
⋮----
fn choose_storages_to_shrink(&mut self, tuning: &PackedAncientStorageTuning) {
self.sort_shrink_indexes_by_bytes_saved();
self.clear_should_shrink_after_cutoff(tuning);
⋮----
fn truncate_to_max_storages(
⋮----
self.shrink_indexes.clear();
let total_storages = self.all_infos.len();
let mut cumulative_bytes = Saturating(0u64);
⋮----
for (i, info) in self.all_infos.iter().enumerate() {
⋮----
cumulative_bytes.0.div_ceil(tuning.ideal_storage_size.get()) as usize;
⋮----
self.all_infos.truncate(i);
⋮----
.fetch_add(bytes_from_must_shrink, Ordering::Relaxed);
⋮----
.fetch_add(bytes_from_smallest_storages, Ordering::Relaxed);
⋮----
.fetch_add(bytes_from_newest_storages, Ordering::Relaxed);
⋮----
fn filter_by_smallest_capacity(
⋮----
self.all_infos.clear();
⋮----
self.all_infos.sort_unstable_by(|l, r| {
⋮----
.cmp(&l.is_high_slot)
.then_with(|| r.should_shrink.cmp(&l.should_shrink))
.then_with(|| l.capacity.cmp(&r.capacity))
⋮----
self.truncate_to_max_storages(tuning, stats);
⋮----
struct WriteAncientAccounts<'a> {
/// 'ShrinkInProgress' instances created by starting a shrink operation
    shrinks_in_progress: HashMap<Slot, ShrinkInProgress<'a>>,
⋮----
/// specify what to do with slots with accounts with many refs
enum IncludeManyRefSlots {
⋮----
enum IncludeManyRefSlots {
/// include them in packing
    Include,
// skip them. ie. don't include them until sufficient slots of single refs have been created
⋮----
impl AccountsDb {
pub(crate) fn combine_ancient_slots_packed(
⋮----
ideal_storage_size: NonZeroU64::new(get_ancient_append_vec_capacity()).unwrap(),
⋮----
max_resulting_storages: NonZeroU64::new(10).unwrap(),
⋮----
let _guard = self.active_stats.activate(ActiveStatItem::SquashAncient);
⋮----
let (_, total_us) = measure_us!(self.combine_ancient_slots_packed_internal(
⋮----
.fetch_add(total_us, Ordering::Relaxed);
self.shrink_ancient_stats.report();
⋮----
fn many_ref_accounts_can_be_moved(
⋮----
.iter()
.map(|alive| alive.bytes)
⋮----
// nothing required, so no problem moving nothing
⋮----
if target_slots_sorted.len() < required_ideal_packed {
⋮----
.len()
.saturating_sub(required_ideal_packed);
⋮----
.all(|many| many.slot <= highest_slot)
⋮----
fn combine_ancient_slots_packed_internal(
⋮----
.store(*sorted_slots.first().unwrap_or(&0), Ordering::Relaxed);
⋮----
.fetch_add(sorted_slots.len() as u64, Ordering::Relaxed);
⋮----
self.collect_sort_filter_ancient_slots(sorted_slots, &mut tuning);
⋮----
.store(tuning.ideal_storage_size.into(), Ordering::Relaxed);
⋮----
&mut *self.best_ancient_slots_to_shrink.write().unwrap(),
⋮----
if ancient_slot_infos.all_infos.is_empty() {
return; // nothing to do
⋮----
.get_unique_accounts_from_storage_for_combining_ancient_slots(
⋮----
let mut accounts_to_combine = self.calc_accounts_to_combine(
⋮----
.iter_mut()
.filter_map(|alive| {
⋮----
(!newest_alive.accounts.is_empty()).then_some(newest_alive)
⋮----
// Sort highest slot to lowest slot. This way, we will put the multi ref accounts with the highest slots in the highest
// packed slot.
many_refs_newest.sort_unstable_by(|a, b| b.slot.cmp(&a.slot));
metrics.newest_alive_packed_count += many_refs_newest.len();
⋮----
datapoint_info!("shrink_ancient_stats", ("high_slot", 1, i64));
⋮----
// for the accounts which are one ref and can be put anywhere, we want to put the accounts from the LARGEST storages at the end.
// This causes us to keep the accounts we're re-packing from already existing ancient storages together with other normal one ref accounts.
⋮----
.sort_unstable_by(|a, b| a.capacity.cmp(&b.capacity));
⋮----
many_refs_newest.iter().chain(
⋮----
.map(|shrink_collect| &shrink_collect.alive_accounts.one_ref),
⋮----
if pack.len() > accounts_to_combine.target_slots_sorted.len() {
⋮----
.for_each(|combine| {
self.unref_shrunk_dead_accounts(
combine.pubkeys_to_unref.iter().cloned(),
⋮----
let write_ancient_accounts = self.write_packed_storages(&accounts_to_combine, pack);
self.finish_combine_ancient_slots_packed_internal(
⋮----
fn collect_sort_filter_ancient_slots(
⋮----
let mut ancient_slot_infos = self.calc_ancient_slot_info(slots, tuning);
⋮----
(ancient_slot_infos.total_alive_bytes.0 * 2 / tuning.max_ancient_slots.max(1) as u64)
.max(self.ancient_storage_ideal_size),
⋮----
.unwrap();
ancient_slot_infos.filter_ancient_slots(tuning, &self.shrink_ancient_stats);
⋮----
fn write_ancient_accounts<'a, 'b: 'a>(
⋮----
let target_slot = accounts_to_write.target_slot();
⋮----
measure_us!(self.get_store_for_shrink(target_slot, bytes));
let (store_accounts_timing, rewrite_elapsed_us) = measure_us!(self.store_accounts_frozen(
⋮----
write_ancient_accounts.metrics.accumulate(&ShrinkStatsSub {
⋮----
rewrite_elapsed_us: Saturating(rewrite_elapsed_us),
create_and_insert_store_elapsed_us: Saturating(create_and_insert_store_elapsed_us),
⋮----
.insert(target_slot, shrink_in_progress);
⋮----
fn calc_ancient_slot_info(
⋮----
let len = slots.len();
⋮----
let max_slot = slots.iter().max().cloned().unwrap_or_default();
let high_slot_boundary = max_slot.saturating_sub(HIGH_SLOT_OFFSET);
⋮----
if let Some(storage) = self.storage.get_slot_storage_entry(*slot) {
let is_candidate_for_shrink = self.is_candidate_for_shrink(&storage);
if infos.add(
⋮----
is_high_slot(*slot),
⋮----
.filter(|info| info.should_shrink)
.map(|info| {
total_dead_bytes += info.capacity.saturating_sub(info.alive_bytes);
⋮----
.count()
.saturating_sub(randoms as usize);
⋮----
.fetch_add(should_shrink_count as u64, Ordering::Relaxed);
⋮----
.fetch_add(total_dead_bytes, Ordering::Relaxed);
⋮----
.fetch_add(total_alive_bytes, Ordering::Relaxed);
⋮----
.fetch_add(randoms, Ordering::Relaxed);
⋮----
fn write_packed_storages<'a, 'b>(
⋮----
// ok if we have more slots, but NOT ok if we have fewer slots than we have contents
assert!(accounts_to_combine.target_slots_sorted.len() >= packed_contents.len());
// write packed storages containing contents from many original slots
// iterate slots in highest to lowest
⋮----
.rev()
.zip(packed_contents)
⋮----
// keep track of how many slots were shrunk away
⋮----
.fetch_add(
⋮----
.saturating_sub(packer.len()) as u64,
⋮----
self.thread_pool_background.install(|| {
packer.par_iter().for_each(|(target_slot, pack)| {
⋮----
self.write_one_packed_storage(
⋮----
let mut write = write_ancient_accounts.lock().unwrap();
⋮----
.extend(write_ancient_accounts_local.shrinks_in_progress);
⋮----
.accumulate(&write_ancient_accounts_local.metrics);
⋮----
let mut write_ancient_accounts = write_ancient_accounts.into_inner().unwrap();
// write new storages where contents were unable to move because ref_count > 1
self.write_ancient_accounts_to_same_slot_multiple_refs(
accounts_to_combine.accounts_keep_slots.values(),
⋮----
/// for each slot in 'ancient_slots', collect all accounts in that slot
    fn get_unique_accounts_from_storage_for_combining_ancient_slots<'a>(
⋮----
fn get_unique_accounts_from_storage_for_combining_ancient_slots<'a>(
⋮----
let mut accounts_to_combine = Vec::with_capacity(ancient_slots.len());
⋮----
let unique_accounts = self.get_unique_accounts_from_storage_for_shrink(
⋮----
accounts_to_combine.push((info, unique_accounts));
⋮----
/// finish shrink operation on slots where a new storage was created
    /// drop root and storage for all original slots whose contents were combined into other storages
⋮----
/// drop root and storage for all original slots whose contents were combined into other storages
    fn finish_combine_ancient_slots_packed_internal(
⋮----
fn finish_combine_ancient_slots_packed_internal(
⋮----
let mut dropped_roots = Vec::with_capacity(accounts_to_combine.accounts_to_combine.len());
⋮----
let shrink_in_progress = write_ancient_accounts.shrinks_in_progress.remove(&slot);
⋮----
if shrink_in_progress.is_none() {
dropped_roots.push(slot);
⋮----
self.remove_old_stores_shrink(
⋮----
self.shrink_candidate_slots.lock().unwrap().remove(&slot);
⋮----
self.reopen_storage_as_readonly_shrinking_in_progress_ok(slot);
⋮----
self.handle_dropped_roots_for_ancient(dropped_roots.into_iter());
metrics.accumulate(&write_ancient_accounts.metrics);
⋮----
fn calc_accounts_to_combine<'a>(
⋮----
accounts_per_storage.sort_unstable_by(|a, b| b.0.slot.cmp(&a.0.slot));
⋮----
let len = accounts_per_storage.len();
⋮----
.map(|(info, unique_accounts)| {
⋮----
.map(|a| a.alive_total_bytes)
⋮----
for (i, shrink_collect) in accounts_to_combine.iter_mut().enumerate() {
// If 0 < alive_bytes < `ideal_storage_size`, then `min_resulting_packed_slots` = 0.
// We obviously require 1 packed slot if we have at least 1 alive byte.
// We want ceiling, so we add 1.
⋮----
alive_bytes.saturating_sub(1) as u64 / u64::from(tuning.ideal_storage_size) + 1;
// assert that iteration is in descending slot order since the code below relies on this.
⋮----
assert!(last_slot > shrink_collect.slot);
⋮----
last_slot = Some(shrink_collect.slot);
⋮----
.is_empty()
⋮----
if many_refs_old_alive.accounts.is_empty() {
// if THIS slot can be used as a target slot, then even if we have multi refs
// this is ok.
required_packed_slots = required_packed_slots.saturating_sub(1);
⋮----
if (target_slots_sorted.len() as u64) >= required_packed_slots {
// we have prepared to pack enough normal target slots, that form now on we can safely pack
// any 'many ref' slots.
⋮----
.fetch_add(1, Ordering::Relaxed);
alive_bytes = alive_bytes.saturating_sub(shrink_collect.alive_total_bytes);
remove.push(i);
⋮----
if !many_refs_old_alive.accounts.is_empty() {
many_refs_old_alive_count += many_refs_old_alive.accounts.len();
many_refs_old_alive.accounts.iter().for_each(|account| {
⋮----
if shrink_collect.pubkeys_to_unref.is_empty()
&& shrink_collect.alive_accounts.one_ref.accounts.is_empty()
⋮----
.insert(shrink_collect.slot, std::mem::take(many_refs_old_alive));
⋮----
target_slots_sorted.push(shrink_collect.slot);
⋮----
let unpackable_slots_count = remove.len();
for i in remove.iter().rev() {
accounts_to_combine.remove(*i);
⋮----
target_slots_sorted.sort_unstable();
⋮----
.fetch_add(accounts_keep_slots.len() as u64, Ordering::Relaxed);
⋮----
.fetch_add(many_refs_old_alive_count as u64, Ordering::Relaxed);
⋮----
fn write_one_packed_storage<'a, 'b: 'a>(
⋮----
.fetch_add(packed.bytes, Ordering::Relaxed);
⋮----
self.write_ancient_accounts(*bytes_total, accounts_to_write, write_ancient_accounts)
⋮----
/// For each slot and alive accounts in 'accounts_to_combine'
    fn write_ancient_accounts_to_same_slot_multiple_refs<'a, 'b: 'a>(
⋮----
fn write_ancient_accounts_to_same_slot_multiple_refs<'a, 'b: 'a>(
⋮----
accounts: vec![(alive_accounts.slot, &alive_accounts.accounts[..])],
⋮----
self.write_one_packed_storage(&packed, alive_accounts.slot, write_ancient_accounts);
⋮----
/// hold all alive accounts to be shrunk and/or combined
#[derive(Debug, Default)]
struct AccountsToCombine<'a> {
⋮----
/// all the rest of alive accounts that can move slots and should be combined
    /// This includes all accounts with ref_count = 1 from the slots in 'accounts_keep_slots'.
⋮----
/// This includes all accounts with ref_count = 1 from the slots in 'accounts_keep_slots'.
    accounts_to_combine: Vec<ShrinkCollect<'a, ShrinkCollectAliveSeparatedByRefs<'a>>>,
⋮----
struct PackedAncientStorage<'a> {
/// accounts to move into this storage, along with the slot the accounts are currently stored in
    accounts: Vec<(Slot, &'a [&'a AccountFromStorage])>,
/// total bytes required to hold 'accounts'
    bytes: u64,
⋮----
fn pack(
⋮----
let ideal_size: u64 = ideal_size.into();
⋮----
let mut current_alive_accounts = accounts_to_combine.next();
// starting at first entry in current_alive_accounts
⋮----
// 0 bytes written so far from the current set of accounts
let mut partial_bytes_written = Saturating(0);
// pack a new storage each iteration of this outer loop
⋮----
// walk through each set of alive accounts to pack the current new storage up to ideal_size
⋮----
while !full && current_alive_accounts.is_some() {
let alive_accounts = current_alive_accounts.unwrap();
if partial_inner_index >= alive_accounts.accounts.len() {
// current_alive_accounts have all been written, so advance to next set from accounts_to_combine
current_alive_accounts = accounts_to_combine.next();
// reset partial progress since we're starting over with a new set of alive accounts
⋮----
partial_bytes_written = Saturating(0);
⋮----
alive_accounts.bytes.saturating_sub(partial_bytes_written.0);
⋮----
bytes_total.saturating_add(bytes_remaining_this_slot);
⋮----
partial_inner_index_max_exclusive = alive_accounts.accounts.len();
⋮----
while partial_inner_index_max_exclusive < alive_accounts.accounts.len() {
⋮----
let account_size = account.stored_size();
let new_size = bytes_total.saturating_add(account_size);
⋮----
accounts_to_write.push((
⋮----
if accounts_to_write.is_empty() {
⋮----
result.push(PackedAncientStorage {
⋮----
pub const fn get_ancient_append_vec_capacity() -> u64 {
⋮----
use crate::append_vec::MAXIMUM_APPEND_VEC_FILE_SIZE;
const _: () = assert!(
⋮----
pub mod tests {
⋮----
fn get_sample_storages(
⋮----
let (db, slot1) = create_db_with_storages_and_index(alive, slots, account_data_size);
⋮----
.filter_map(|slot| db.storage.get_slot_storage_entry((slot as Slot) + slot1))
⋮----
.map(|storage| SlotInfo {
⋮----
slot: storage.slot(),
⋮----
.collect();
⋮----
fn unique_to_accounts<'a>(
⋮----
one.flat_map(|result| {
result.stored_accounts.iter().map(|result| {
⋮----
*result.pubkey(),
get_account_from_account_from_storage(result, db, slot),
⋮----
.collect()
⋮----
pub(crate) fn compare_all_vec_accounts<'a>(
⋮----
compare_all_accounts(
&unique_to_accounts(one, db, slot),
&unique_to_accounts(two, db, slot),
⋮----
fn test_write_packed_storages_empty() {
let (db, _storages, _slots, _infos) = get_sample_storages(0, None);
⋮----
db.write_packed_storages(&AccountsToCombine::default(), Vec::default());
assert!(write_ancient_accounts.shrinks_in_progress.is_empty());
⋮----
fn test_write_packed_storages_too_few_slots() {
let (db, storages, slots, _infos) = get_sample_storages(1, None);
⋮----
.first()
.unwrap()
⋮----
.get_stored_account_without_data_callback(offset, |account| {
⋮----
let packed_contents = vec![PackedAncientStorage {
⋮----
db.write_packed_storages(&accounts_to_combine, packed_contents);
⋮----
fn test_write_ancient_accounts_to_same_slot_multiple_refs_empty() {
⋮----
db.write_ancient_accounts_to_same_slot_multiple_refs(
AccountsToCombine::default().accounts_keep_slots.values(),
⋮----
fn test_pack_ancient_storages_one_account_per_storage() {
⋮----
(get_ancient_append_vec_capacity(), 1.min(num_slots)),
⋮----
let (db, storages, slots, _infos) = get_sample_storages(num_slots, None);
⋮----
.map(|store| db.get_unique_accounts_from_storage(store))
⋮----
.zip(slots_vec.iter().cloned())
.map(|(accounts, slot)| AliveAccounts {
accounts: accounts.stored_accounts.iter().collect::<Vec<_>>(),
⋮----
.map(|account| aligned_stored_size(account.data_len()))
.sum(),
⋮----
accounts_to_combine.iter(),
NonZeroU64::new(ideal_size).unwrap(),
⋮----
let storages_needed = result.len();
assert_eq!(storages_needed, expected_storages);
⋮----
fn test_pack_ancient_storages_one_partial() {
// n slots
// m accounts per slot
// divide into different ideal sizes so that we combine multiple slots sometimes and combine partial slots
⋮----
.and_then(|storage| storage.accounts.get_account_shared_data(0))
.unwrap_or_default();
// add some accounts to each storage so we can make partial progress
⋮----
.map(|storage| {
⋮----
.map(|_| {
⋮----
let mut account = account_template.clone();
account.set_lamports(lamports);
⋮----
append_single_account_with_default_hash(
⋮----
Some(&db.accounts_index),
⋮----
.map(|store| (store.slot(), db.get_unique_accounts_from_storage(store)))
⋮----
let original_results_all_accounts = vec_unique_to_accounts(&original_results, &db);
let slots_vec = slots.clone().collect::<Vec<_>>();
⋮----
.map(|((_slot, accounts), slot)| AliveAccounts {
⋮----
assert_eq!(
⋮----
&packed_to_compare(&result, &db)[..],
⋮----
fn packed_to_compare(
⋮----
.flat_map(|packed| {
packed.accounts.iter().flat_map(|(slot, stored_metas)| {
stored_metas.iter().map(|stored_meta| {
⋮----
*stored_meta.pubkey(),
get_account_from_account_from_storage(stored_meta, db, *slot),
⋮----
fn test_pack_ancient_storages_varying() {
⋮----
// different number of accounts in each slot
// each account has different size
⋮----
// compare at end that all accounts are in result exactly once
⋮----
get_ancient_append_vec_capacity(),
⋮----
// random # of extra accounts here
let total_accounts_per_storage = rng().random_range(0..total_accounts_per_storage);
⋮----
account.set_data((0..data_size).map(|x| (x % 256) as u8).collect());
⋮----
let largest_account_size = aligned_stored_size(data_size) as u64;
// all packed storages should be close to ideal size
result.iter().enumerate().for_each(|(i, packed)| {
if i + 1 < result.len() && ideal_size > largest_account_size {
// cannot assert this on the last packed storage - it may be small
// cannot assert this when the ideal size is too small to hold the largest account size
assert!(
⋮----
assert!(packed.bytes > 0, "packed size of zero");
⋮----
result.iter().for_each(|packed| {
⋮----
enum TestWriteMultipleRefs {
⋮----
fn test_finish_combine_ancient_slots_packed_internal() {
// n storages
// 1 account each
// all accounts have 1 ref
// nothing shrunk, so all storages and roots should be removed
// or all slots shrunk so no roots or storages should be removed
⋮----
let (mut db, storages, slots, infos) = get_sample_storages(num_slots, None);
db.set_storage_access(storage_access);
⋮----
.zip(
⋮----
.map(|store| db.get_unique_accounts_from_storage(store)),
⋮----
let accounts_to_combine = db.calc_accounts_to_combine(
⋮----
&default_tuning(),
⋮----
slots.clone().for_each(|slot| {
db.add_root(slot);
let storage = db.storage.get_slot_storage_entry(slot);
assert!(storage.is_some());
⋮----
db.shrink_candidate_slots.lock().unwrap().insert(slot);
⋮----
.read()
⋮----
.get_all();
assert_eq!(roots, slots.clone().collect::<Vec<_>>());
⋮----
// make it look like each of the slots was shrunk
⋮----
.insert(slot, db.get_store_for_shrink(slot, 1));
⋮----
db.finish_combine_ancient_slots_packed_internal(
⋮----
assert!(!db.shrink_candidate_slots.lock().unwrap().contains(&slot));
⋮----
slots.for_each(|slot| {
⋮----
assert!(!storage.unwrap().has_accounts());
⋮----
assert!(storage.is_none());
⋮----
fn test_calc_accounts_to_combine_many_refs() {
⋮----
// all accounts have 1 ref or all accounts have 2 refs
⋮----
let alive_bytes_per_slot = aligned_stored_size(data_size as usize) as u64;
// pack 2.5 ancient slots into 1 packed slot ideally
⋮----
ideal_storage_size: NonZeroU64::new(alive_bytes_per_slot * 2 + 1).unwrap(),
..default_tuning()
⋮----
get_sample_storages(num_slots, Some(data_size));
infos.iter_mut().for_each(|a| {
⋮----
storages = storages.into_iter().rev().collect();
infos = infos.into_iter().rev().collect();
⋮----
original_results.iter().for_each(|results| {
results.stored_accounts.iter().for_each(|account| {
db.accounts_index.get_and_then(account.pubkey(), |entry| {
(false, entry.unwrap().addref())
⋮----
.zip(original_results.into_iter())
⋮----
// In this test setup, 2.5 regular slots fits into 1 ancient slot.
// When there are two_refs and when slots < 3, all regular slots can fit into one ancient slots.
// Therefore, we should have all slots that can be combined for slots < 3.
// However, when slots >=3, we need more than one ancient slots. The pack algorithm will need to first
// find at least [ceiling(num_slots/2.5) - 1] slots that's don't have many_refs before we can pack slots with many_refs.
⋮----
.saturating_sub(1))
.for_each(|i| {
⋮----
assert!(slots[i] < slots[i + 1]);
⋮----
fn test_calc_accounts_to_combine_simple() {
⋮----
let alive_bytes_per_account = aligned_stored_size(data_size as usize) as u64;
⋮----
ideal_storage_size: NonZeroU64::new(alive_bytes_per_account).unwrap(),
⋮----
.for_each(|a| a.alive_bytes += alive_bytes_per_account);
⋮----
slots_vec = slots.rev().collect::<Vec<_>>();
⋮----
.get_and_then(account.pubkey(), |entry| {
⋮----
storages.iter().for_each(|storage| {
⋮----
assert!(db.accounts_index.purge_exact(
⋮----
assert!(!accounts_to_combine
⋮----
slots_vec.iter().cloned().rev().collect::<Vec<_>>()
⋮----
slots_vec.clone()
⋮----
vec![1]
⋮----
vec![]
⋮----
assert!(accounts_to_combine.accounts_keep_slots.is_empty());
assert!(accounts_to_combine.accounts_to_combine.iter().all(
⋮----
db.write_packed_storages(
⋮----
fn test_calc_accounts_to_combine_older_dup() {
⋮----
let (db, mut storages, slots, infos) = get_sample_storages(num_slots + 1, None);
⋮----
let storage = storages.first().unwrap().clone();
let ignored_storage = storages.pop().unwrap();
⋮----
get_account_from_account_from_storage(account_with_2_refs, &db, slot1);
let pk_with_2_refs = account_with_2_refs.pubkey();
let mut account_with_1_ref = account_shared_data_with_2_refs.clone();
account_with_1_ref.checked_add_lamports(1).unwrap();
⋮----
assert_eq!(original_results.first().unwrap().stored_accounts.len(), 2);
⋮----
assert_eq!(accounts_to_combine.accounts_to_combine.len(), num_slots);
⋮----
.keys()
.cloned()
⋮----
accounts_keep.sort_unstable();
assert_eq!(accounts_keep, slots_vec);
assert!(accounts_to_combine.target_slots_sorted.is_empty());
assert_eq!(accounts_to_combine.accounts_keep_slots.len(), num_slots);
⋮----
assert_eq!(accounts_to_combine.accounts_to_combine.len(), 1);
⋮----
.map(|account| get_account_from_account_from_storage(account, &db, slot1))
⋮----
assert!(accounts_to_combine
⋮----
db.write_packed_storages(&accounts_to_combine, packed_contents)
⋮----
assert_eq!(write_ancient_accounts.shrinks_in_progress.len(), num_slots);
⋮----
shrinks_in_progress.sort_unstable_by(|a, b| a.0.cmp(b.0));
⋮----
let shrink_in_progress = shrinks_in_progress.first().unwrap().1;
⋮----
.new_storage()
⋮----
.scan_accounts(&mut reader, |_offset, _| {
⋮----
.expect("must scan accounts storage");
assert_eq!(count, 1);
⋮----
.get_stored_account_callback(0, |account| {
assert_eq!(account.pubkey(), pk_with_2_refs);
create_account_shared_data(&account)
⋮----
assert_eq!(account, account_shared_data_with_2_refs);
⋮----
fn test_calc_accounts_to_combine_opposite() {
⋮----
let (db, storages, slots, infos) = get_sample_storages(num_slots, None);
⋮----
_ = account_with_1_ref.checked_add_lamports(1);
⋮----
.get_and_then(account.pubkey(), |entry| (true, entry.unwrap().addref()));
⋮----
assert_eq!(accounts_to_combine.target_slots_sorted, slots_vec);
assert!(accounts_keep.is_empty());
assert!(!accounts_to_combine.target_slots_sorted.is_empty());
⋮----
let storage = db.storage.get_slot_storage_entry(slot1).unwrap();
⋮----
(*account.pubkey(), create_account_shared_data(&account))
⋮----
.scan_accounts(&mut reader, |_, _| {
⋮----
assert_eq!(count, 2);
assert_eq!(accounts_shrunk_same_slot.0, *pk_with_2_refs);
assert_eq!(accounts_shrunk_same_slot.1, account_shared_data_with_2_refs);
⋮----
fn test_get_unique_accounts_from_storage_for_combining_ancient_slots() {
⋮----
let (db, storages, slots, mut infos) = get_sample_storages(num_slots, None);
⋮----
db.get_unique_accounts_from_storage_for_combining_ancient_slots(&infos);
let all_accounts = get_all_accounts(&db, slots.clone());
assert_eq!(all_accounts.len(), num_slots);
compare_all_vec_accounts(
original_results.iter(),
results.iter().map(|(_, accounts)| accounts),
⋮----
&unique_to_accounts(
⋮----
info.storage.id(),
⋮----
fn test_get_ancient_append_vec_capacity() {
assert_eq!(get_ancient_append_vec_capacity(), 128 * 1024 * 1024);
⋮----
fn get_one_packed_ancient_append_vec_and_others(
⋮----
let (db, slot1) = create_db_with_storages_and_index(alive, num_normal_slots + 1, None);
⋮----
let created_accounts = db.get_unique_accounts_from_storage(&storage);
db.combine_ancient_slots_packed(vec![slot1], CAN_RANDOMLY_SHRINK_FALSE);
assert!(db.storage.get_slot_storage_entry(slot1).is_some());
let after_store = db.storage.get_slot_storage_entry(slot1).unwrap();
⋮----
} = db.get_unique_accounts_from_storage(&after_store);
assert_eq!(created_accounts.capacity, after_capacity);
assert_eq!(created_accounts.stored_accounts.len(), 1);
assert_eq!(after_stored_accounts.len(), 1);
⋮----
fn assert_storage_info(info: &SlotInfo, storage: &AccountStorageEntry, should_shrink: bool) {
assert_eq!(storage.id(), info.storage.id());
assert_eq!(storage.slot(), info.slot);
assert_eq!(storage.capacity(), info.capacity);
assert_eq!(storage.alive_bytes(), info.alive_bytes as usize);
assert_eq!(should_shrink, info.should_shrink);
⋮----
enum TestCollectInfo {
⋮----
fn test_calc_ancient_slot_info_one_alive_only() {
⋮----
for data_size in [None, Some(1_040_000)] {
let (db, slot1) = create_db_with_storages_and_index(alive, slots, data_size);
⋮----
let alive_bytes_expected = storage.alive_bytes();
⋮----
let is_candidate_for_shrink = db.is_candidate_for_shrink(&storage);
⋮----
infos.add(
⋮----
NonZeroU64::new(get_ancient_append_vec_capacity()).unwrap(),
⋮----
infos = db.calc_ancient_slot_info(vec![slot1], &tuning);
⋮----
infos = db.collect_sort_filter_ancient_slots(vec![slot1], &mut tuning);
⋮----
assert_eq!(infos.all_infos.len(), 1, "{method:?}");
let should_shrink = db.is_candidate_for_shrink(&storage);
assert_storage_info(infos.all_infos.first().unwrap(), &storage, should_shrink);
⋮----
assert_eq!(infos.total_alive_bytes.0, alive_bytes_expected as u64);
⋮----
assert!(infos.shrink_indexes.is_empty());
⋮----
assert_eq!(infos.total_alive_bytes_shrink.0, 0);
⋮----
fn test_calc_ancient_slot_info_one_dead() {
⋮----
let (db, slot1) = create_db_with_storages_and_index(alive, slots, None);
⋮----
assert!(infos.all_infos.is_empty());
⋮----
assert_eq!(infos.total_alive_bytes.0, 0);
⋮----
fn test_calc_ancient_slot_info_several() {
⋮----
.map(|slot| db.storage.get_slot_storage_entry(*slot).unwrap())
⋮----
.map(|storage| storage.alive_bytes() as u64)
⋮----
let infos = db.calc_ancient_slot_info(slot_vec.clone(), &tuning);
⋮----
assert_eq!(infos.all_infos.len(), slots);
⋮----
.zip(infos.all_infos.iter())
.for_each(|(storage, info)| {
let should_shrink = db.is_candidate_for_shrink(storage);
assert_storage_info(info, storage, should_shrink);
⋮----
assert_eq!(infos.total_alive_bytes.0, alive_bytes_expected);
⋮----
fn test_calc_ancient_slot_info_one_alive_one_dead() {
⋮----
create_db_with_storages_and_index(true , slots, data_size);
assert_eq!(slot1, 1);
assert_eq!(alives[slot1 as usize], slot1_is_alive);
⋮----
let slot = storage.slot();
⋮----
remove_account_for_tests(storage, storage.written_bytes() as usize);
⋮----
.filter(|storage| alives[storage.slot() as usize])
⋮----
db.calc_ancient_slot_info(slot_vec.clone(), &tuning)
⋮----
.unwrap(),
⋮----
db.collect_sort_filter_ancient_slots(slot_vec.clone(), &mut tuning)
⋮----
assert_eq!(infos.all_infos.len(), 1, "method: {method:?}");
alive_storages.iter().zip(infos.all_infos.iter()).for_each(
⋮----
assert_eq!(infos.total_alive_bytes_shrink.0, alive_bytes_expected);
⋮----
fn create_test_infos(count: usize) -> AncientSlotInfos {
let (db, slot1) = create_db_with_storages_and_index(true , 1, None);
⋮----
.map(|index| SlotInfo {
⋮----
.collect(),
shrink_indexes: (0..count).collect(),
⋮----
enum TestSmallestCapacity {
⋮----
fn test_filter_by_smallest_capacity_empty() {
⋮----
let ideal_storage_size_large = get_ancient_append_vec_capacity();
let mut infos = create_test_infos(1);
⋮----
ideal_storage_size: NonZeroU64::new(ideal_storage_size_large).unwrap(),
⋮----
infos.shrink_indexes.clear();
infos.filter_ancient_slots(&tuning, &ShrinkAncientStats::default());
⋮----
infos.filter_by_smallest_capacity(&tuning, &ShrinkAncientStats::default());
⋮----
fn test_filter_by_smallest_capacity_sort() {
⋮----
let mut infos = create_test_infos(7);
⋮----
.enumerate()
.for_each(|(i, info)| info.capacity = 1 + i as u64);
⋮----
infos.all_infos.last_mut().unwrap().capacity = 0;
⋮----
infos.all_infos.last_mut().unwrap().alive_bytes = ideal_storage_size_large;
⋮----
fn test_filter_by_smallest_capacity_high_slot_more() {
let tuning = default_tuning();
let num_high_slots = tuning.max_resulting_storages.get() * 2;
⋮----
let mut infos = create_test_infos(num_ancient_storages as usize);
⋮----
.sort_unstable_by_key(|slot_info| slot_info.slot);
⋮----
.take(num_high_slots as usize)
.for_each(|slot_info| {
⋮----
.filter_map(|slot_info| slot_info.is_high_slot.then_some(slot_info.slot))
⋮----
infos.all_infos.shuffle(&mut rng());
⋮----
.map(|slot_info| slot_info.slot)
⋮----
assert_eq!(infos.all_infos.len() as u64, num_high_slots);
assert_eq!(slots_actual, slots_expected);
⋮----
fn test_filter_by_smallest_capacity_high_slot_less() {
⋮----
let num_high_slots = tuning.max_resulting_storages.get() / 2;
⋮----
assert!(high_slots
⋮----
fn test(filter: bool, infos: &mut AncientSlotInfos, tuning: &PackedAncientStorageTuning) {
⋮----
infos.filter_by_smallest_capacity(tuning, &ShrinkAncientStats::default());
⋮----
infos.truncate_to_max_storages(tuning, &ShrinkAncientStats::default());
⋮----
fn test_truncate_to_max_storages() {
⋮----
test(filter, &mut infos, &tuning);
assert_eq!(infos.all_infos.len(), usize::from(!filter));
⋮----
ideal_storage_size: NonZeroU64::new(ideal_storage_size).unwrap(),
⋮----
let mut infos = create_test_infos(2);
⋮----
assert_eq!(infos.all_infos.len(), 2);
⋮----
let mut infos = create_test_infos(5);
⋮----
fn test_calc_ancient_slot_info_one_shrink_one_not() {
⋮----
.map(|shrink| (!shrink).then_some(1_040_000))
⋮----
create_db_with_storages_and_index(true , 1, data_sizes[1]);
create_storages_and_update_index(
⋮----
assert_eq!(shrinks[slot1 as usize], slot1_shrink);
⋮----
.map(|slot| {
let storage = db.storage.get_slot_storage_entry(*slot).unwrap();
assert_eq!(*slot, storage.slot());
⋮----
assert_eq!(infos.all_infos.len(), 2, "{method:?}");
⋮----
assert!(infos
⋮----
fn default_tuning() -> PackedAncientStorageTuning {
⋮----
ideal_storage_size: NonZeroU64::new(1).unwrap(),
⋮----
fn test_clear_should_shrink_after_cutoff_empty() {
⋮----
infos.clear_should_shrink_after_cutoff(&tuning);
⋮----
enum TestWriteAncient {
⋮----
pub fn build_refs_accounts_from_storage_with_slot(
⋮----
.map(|(slot, accounts)| (*slot, accounts.iter().collect()))
⋮----
pub fn build_refs_accounts_from_storage_with_slot2<'a>(
⋮----
.map(|(slot, accounts)| (*slot, &accounts[..]))
⋮----
fn test_write_ancient_accounts() {
for data_size in [None, Some(10_000_000)] {
⋮----
// invalid combination when num_slots > 0, but required to hit num_slots=0, combine_into=0
⋮----
get_sample_storages(num_slots, data_size);
let initial_accounts = get_all_accounts(&db, slots.clone());
⋮----
.scan_accounts_without_data(|offset, account| {
accounts.push(AccountFromStorage::new(offset, &account));
⋮----
(storage.slot(), accounts)
⋮----
// reshape the data
⋮----
build_refs_accounts_from_storage_with_slot(&accounts_byval);
⋮----
build_refs_accounts_from_storage_with_slot2(&accounts_byval2);
let target_slot = slots.clone().nth(combine_into).unwrap_or(slots.start);
⋮----
.map(|storage| storage.written_bytes())
⋮----
TestWriteAncient::AncientAccounts => db.write_ancient_accounts(
⋮----
db.write_one_packed_storage(
⋮----
// target slots are supposed to be read in reverse order, so test that
target_slots_sorted: vec![
Slot::MAX, // this asserts if it gets used
⋮----
.write_packed_storages(&accounts_to_combine, vec![packed]);
⋮----
let one = result.drain().collect::<Vec<_>>();
assert_eq!(1, one.len());
assert_eq!(target_slot, one.first().unwrap().0);
⋮----
// make sure the single new append vec contains all the same accounts
⋮----
one.first()
⋮----
.scan_accounts(&mut reader, |_offset, meta| {
two.push((*meta.pubkey(), create_account_shared_data(&meta)));
⋮----
compare_all_accounts(&initial_accounts, &two[..]);
⋮----
let all_accounts = get_all_accounts(&db, target_slot..(target_slot + 1));
compare_all_accounts(&initial_accounts, &all_accounts);
⋮----
enum TestShouldShrink {
⋮----
fn test_clear_should_shrink_after_cutoff_simple() {
⋮----
.for_each(|(i, info)| {
⋮----
infos.all_infos = infos.all_infos.into_iter().rev().collect();
⋮----
infos.total_alive_bytes_shrink = Saturating(
⋮----
.map(|info| info.alive_bytes)
⋮----
// 0 so that we combine everything with regard to the overall # of slots limit
⋮----
// irrelevant for what this test is trying to test, but necessary to avoid minimums
ideal_storage_size: NonZeroU64::new(get_ancient_append_vec_capacity())
⋮----
infos.choose_storages_to_shrink(&tuning);
⋮----
// filter_ancient_slots modifies in several ways and doesn't retain the values to compare
⋮----
.filter_map(|info| info.should_shrink.then_some(()))
.count();
⋮----
fn test_sort_shrink_indexes_by_bytes_saved() {
⋮----
storage: storage.clone(),
⋮----
all_infos: vec![info1, info2],
shrink_indexes: vec![0, 1],
⋮----
infos.sort_shrink_indexes_by_bytes_saved();
⋮----
assert!(first_capacity >= second_capacity);
⋮----
fn test_combine_ancient_slots_packed_internal() {
⋮----
let (db, slot1) = create_db_with_storages_and_index(alive, num_slots, None);
⋮----
db.combine_ancient_slots_packed_internal(
(0..num_slots).map(|slot| (slot as Slot) + slot1).collect(),
⋮----
let storage = db.storage.get_slot_storage_entry(slot1);
⋮----
let mut expected_slots = (max_ancient_slots / 2).min(num_slots);
⋮----
let all_accounts = get_all_accounts(&db, slot1..(slot1 + num_slots as Slot));
compare_all_accounts(&original_results_all_accounts, &all_accounts);
⋮----
&vec_unique_to_accounts(&results, &db),
&get_all_accounts(&db, slot1..(slot1 + num_slots as Slot)),
⋮----
fn vec_unique_to_accounts(
⋮----
one.iter()
.flat_map(|(slot, result)| {
⋮----
get_account_from_account_from_storage(result, db, *slot),
⋮----
fn test_combine_packed_ancient_slots_simple() {
⋮----
_ = get_one_packed_ancient_append_vec_and_others(alive, 0);
⋮----
fn combine_ancient_slots_packed_for_tests(db: &AccountsDb, sorted_slots: Vec<Slot>) {
⋮----
db.combine_ancient_slots_packed_internal(sorted_slots, tuning, &mut stats_sub);
⋮----
fn test_shrink_packed_ancient() {
⋮----
create_storages_and_update_index(&db, None, initial_slot, MAX_RECYCLE_STORES, true, None);
⋮----
.filter_map(|slot| db.storage.get_slot_storage_entry(slot))
⋮----
combine_ancient_slots_packed_for_tests(&db, (initial_slot..=max_slot_inclusive).collect());
⋮----
let mut storages = vec![];
⋮----
create_storages_and_update_index(&db, None, ancient_slot, num_normal_slots, true, None);
⋮----
storages.extend(
⋮----
.clone()
.filter_map(|slot| db.storage.get_slot_storage_entry(slot)),
⋮----
let initial_accounts = get_all_accounts(&db, range);
⋮----
&get_all_accounts(&db, ancient_slot..(max_slot_inclusive + 1)),
⋮----
combine_ancient_slots_packed_for_tests(
⋮----
(ancient_slot..=max_slot_inclusive).collect(),
⋮----
.for_each(|storage| assert_eq!(Arc::strong_count(storage), 1));
⋮----
create_storages_and_update_index(&db, None, next_slot, num_normal_slots, true, None);
⋮----
storages = vec![];
⋮----
let initial_accounts_all = get_all_accounts(&db, range_all.clone());
let initial_accounts = get_all_accounts(&db, range.clone());
⋮----
&get_all_accounts(&db, range_all.clone()),
⋮----
compare_all_accounts(&initial_accounts, &get_all_accounts(&db, range.clone()));
combine_ancient_slots_packed_for_tests(&db, range.clone().collect());
compare_all_accounts(&initial_accounts_all, &get_all_accounts(&db, range_all));
compare_all_accounts(&initial_accounts, &get_all_accounts(&db, range));
⋮----
fn test_shrink_collect_alive_add() {
⋮----
let (_db, storages, _slots, _infos) = get_sample_storages(num_slots, data_size);
⋮----
.get_stored_account_without_data_callback(offset, |stored_account| {
⋮----
let slot_list = vec![];
alive_accounts.add(1, &account, &slot_list);
assert!(!alive_accounts.one_ref.accounts.is_empty());
assert!(alive_accounts.many_refs_old_alive.accounts.is_empty());
assert!(alive_accounts
⋮----
let slot_list = vec![(
⋮----
alive_accounts.add(2, &account, &slot_list);
assert!(alive_accounts.one_ref.accounts.is_empty());
⋮----
assert!(!alive_accounts
⋮----
let slot_list = vec![
⋮----
assert!(!alive_accounts.many_refs_old_alive.accounts.is_empty());
⋮----
panic!("unexpected");
⋮----
fn test_many_ref_accounts_can_be_moved() {
⋮----
ideal_storage_size: NonZeroU64::new(1000).unwrap(),
⋮----
let many_refs_newest = vec![];
let target_slots_sorted = vec![];
assert!(AccountsDb::many_ref_accounts_can_be_moved(
⋮----
let many_refs_newest = vec![AliveAccounts {
⋮----
assert!(!AccountsDb::many_ref_accounts_can_be_moved(
⋮----
let target_slots_sorted = vec![slot];
⋮----
let target_slots_sorted = vec![slot, slot + 1];
⋮----
let target_slots_sorted = vec![slot - 1, slot];
⋮----
fn test_shrink_ancient_expected_unref() {
⋮----
.map(|_| solana_pubkey::new_rand())
⋮----
pubkeys_to_unref.iter().for_each(|k| {
⋮----
db.accounts_index.upsert(
⋮----
expected_ref_counts_before_unref.insert(*k, 2);
expected_ref_counts_after_unref.insert(*k, 1);
⋮----
pubkeys_to_unref: pubkeys_to_unref.iter().collect(),
⋮----
db.accounts_index.scan(
shrink_collect.pubkeys_to_unref.iter().cloned(),
⋮----
assert!(expected_ref_counts_before_unref.is_empty());
db.unref_shrunk_dead_accounts(shrink_collect.pubkeys_to_unref.iter().cloned(), 0);
⋮----
assert!(expected_ref_counts_after_unref.is_empty());
⋮----
fn test_ideal_storage_size_updated_before_used() {
⋮----
create_db_with_storages_and_index(true , num_slots, Some(data_size));
⋮----
create_storages_and_update_index(&db, None, non_ancient_slot, 1, true, Some(data_size));
⋮----
slot_vec.push(non_ancient_slot);
let infos = db.collect_sort_filter_ancient_slots(slot_vec.clone(), &mut tuning);
let ideal_storage_size = tuning.ideal_storage_size.get();
let max_resulting_storages = tuning.max_resulting_storages.get();
⋮----
assert_eq!(infos.all_infos.len(), expected_all_infos_len as usize);

================
File: accounts-db/src/append_vec.rs
================
mod meta;
pub mod test_utils;
⋮----
const _: () = assert!(
⋮----
pub fn aligned_stored_size(data_len: usize) -> usize {
u64_align!(STORE_META_OVERHEAD + data_len)
⋮----
fn aligned_stored_size_checked(data_len: usize) -> Option<usize> {
Some(u64_align!(stored_size_checked(data_len)?))
⋮----
fn stored_size_checked(data_len: usize) -> Option<usize> {
STORE_META_OVERHEAD.checked_add(data_len)
⋮----
pub type Result<T> = std::result::Result<T, AppendVecError>;
⋮----
pub enum AppendVecError {
⋮----
pub(crate) struct ValidSlice<'a>(&'a [u8]);
⋮----
pub(crate) fn new(data: &'a [u8]) -> Self {
Self(data)
⋮----
pub(crate) fn len(&self) -> usize {
self.0.len()
⋮----
/// offsets to help navigate the persisted format of `AppendVec`
#[derive(Debug)]
struct AccountOffsets {
/// offset to the end of the &[u8] data
    offset_to_end_of_data: usize,
⋮----
enum AppendVecFileBacking {
/// A file-backed block of memory that is used to store the data for each appended item.
    Mmap(MmapMut),
/// This was opened as a read only file
    File(File),
⋮----
/// Validates and serializes appends (when `append_guard` is called) such that only
/// writable AppendVec is updated and only from a single thread at a time.
⋮----
/// writable AppendVec is updated and only from a single thread at a time.
#[derive(Debug)]
enum ReadWriteState {
⋮----
/// A lock used to serialize append operations.
        append_lock: Mutex<()>,
⋮----
impl ReadWriteState {
fn new(allow_writes: bool) -> Self {
⋮----
fn append_guard(&self) -> MutexGuard<'_, ()> {
⋮----
Self::ReadOnly => panic!("append not allowed in read-only state"),
Self::Writable { append_lock } => append_lock.lock().unwrap(),
⋮----
pub struct AppendVec {
⋮----
pub struct AppendVecStat {
⋮----
impl Drop for AppendVec {
fn drop(&mut self) {
APPEND_VEC_STATS.files_open.fetch_sub(1, Ordering::Relaxed);
if *self.is_dirty.get_mut() {
APPEND_VEC_STATS.files_dirty.fetch_sub(1, Ordering::Relaxed);
⋮----
.fetch_sub(1, Ordering::Relaxed);
⋮----
if self.remove_file_on_drop.load(Ordering::Acquire) {
if let Err(_err) = remove_file(&self.path) {
inc_new_counter_info!("append_vec_drop_fail", 1);
⋮----
impl AppendVec {
pub fn new(
⋮----
let file = file.into();
⋮----
AppendVec::sanitize_len_and_size(initial_len, size).unwrap();
⋮----
let _ignored = remove_file(&file);
⋮----
.read(true)
.write(true)
.create(create)
.open(&file)
.map_err(|e| {
panic!(
⋮----
.unwrap();
data.seek(SeekFrom::Start((size - 1) as u64)).unwrap();
data.write_all(&[0]).unwrap();
data.rewind().unwrap();
data.flush().unwrap();
⋮----
let mmap = mmap.unwrap_or_else(|err| {
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
APPEND_VEC_STATS.files_open.fetch_add(1, Ordering::Relaxed);
⋮----
fn sanitize_len_and_size(current_len: usize, file_size: usize) -> Result<()> {
⋮----
Err(AppendVecError::FileSizeTooSmall(file_size))
⋮----
.map(|max| file_size > max)
.unwrap_or(true)
⋮----
Err(AppendVecError::FileSizeTooLarge(file_size))
⋮----
Err(AppendVecError::OffsetOutOfBounds(current_len, file_size))
⋮----
Ok(())
⋮----
pub fn dead_bytes_due_to_zero_lamport_single_ref(&self, count: usize) -> usize {
aligned_stored_size(0) * count
⋮----
pub fn flush(&self) -> Result<()> {
let should_flush = self.is_dirty.swap(false, Ordering::AcqRel);
⋮----
mmap.flush()?;
⋮----
file.sync_all()?;
⋮----
pub fn reset(&self) {
let _lock = self.read_write_state.append_guard();
self.current_len.store(0, Ordering::Release);
⋮----
pub(crate) fn reopen_as_readonly_file_io(&self) -> Option<Self> {
if matches!(self.read_write_state, ReadWriteState::ReadOnly)
&& matches!(self.backing, AppendVecFileBacking::File(_))
⋮----
self.remove_file_on_drop.store(false, Ordering::Release);
⋮----
AppendVec::new_from_file_unchecked(self.path.clone(), self.len(), StorageAccess::File)
.ok()?;
if self.is_dirty.swap(false, Ordering::AcqRel) {
*new.is_dirty.get_mut() = true;
⋮----
Some(new)
⋮----
pub fn remaining_bytes(&self) -> u64 {
self.capacity()
.saturating_sub(u64_align!(self.len()) as u64)
⋮----
pub fn len(&self) -> usize {
self.current_len.load(Ordering::Acquire)
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub fn capacity(&self) -> u64 {
⋮----
pub fn new_from_file(
⋮----
let path = path.into();
⋮----
let num_accounts = new.sanitize_layout_and_length()?;
Ok((new, num_accounts))
⋮----
pub fn new_for_startup(
⋮----
|| (u64_align!(current_len) as u64 == new.file_size)
⋮----
Ok(new)
⋮----
info!(
⋮----
let _num_accounts = new.sanitize_layout_and_length()?;
⋮----
pub fn new_from_file_unchecked(
⋮----
let file_size = std::fs::metadata(&path)?.len();
⋮----
.write(is_writable)
.create(false)
.open(&path)?;
⋮----
return Ok(AppendVec {
⋮----
if result.is_err() {
⋮----
Ok(AppendVec {
⋮----
pub fn new_for_store_tool(path: impl Into<PathBuf>) -> Result<Self> {
⋮----
fn sanitize_layout_and_length(&self) -> Result<usize> {
⋮----
self.scan_stored_accounts_no_data(|account| {
if !matches || !account.sanitize() {
⋮----
last_offset = account.offset() + account.stored_size();
⋮----
let aligned_current_len = u64_align!(self.current_len.load(Ordering::Acquire));
⋮----
return Err(AppendVecError::IncorrectLayout(self.path.clone()));
⋮----
Ok(num_accounts)
⋮----
fn get_slice(slice: ValidSlice<'_>, offset: usize, size: usize) -> Option<(&[u8], usize)> {
// SAFETY: Wrapping math is safe here because if `end` does wrap, the Range
// parameter to `.get()` will be invalid, and `.get()` will correctly return None.
let end = offset.wrapping_add(size);
⋮----
.get(offset..end)
.map(|subslice| (subslice, u64_align!(end)))
⋮----
/// Copy `len` bytes from `src` to the first 64-byte boundary after position `offset` of
    /// the internal buffer. Then update `offset` to the first byte after the copied data.
⋮----
/// the internal buffer. Then update `offset` to the first byte after the copied data.
    fn append_ptr(&self, offset: &mut usize, src: *const u8, len: usize) -> io::Result<()> {
⋮----
fn append_ptr(&self, offset: &mut usize, src: *const u8, len: usize) -> io::Result<()> {
let pos = u64_align!(*offset);
⋮----
//UNSAFE: This mut append is safe because only 1 thread can append at a time
//Mutex<()> guarantees exclusive write access to the memory occupied in
//the range.
⋮----
let dst = data.as_ptr() as *mut _;
⋮----
// Safety: caller should ensure the passed pointer and length are valid.
⋮----
write_buffer_to_file(file, data, pos as u64)?;
⋮----
/// Copy each value in `vals`, in order, to the first 64-byte boundary after position `offset`.
    /// If there is sufficient space, then update `offset` and the internal `current_len` to the
⋮----
/// If there is sufficient space, then update `offset` and the internal `current_len` to the
    /// first byte after the copied data and return the starting position of the copied data.
⋮----
/// first byte after the copied data and return the starting position of the copied data.
    /// Otherwise return None and leave `offset` unchanged.
⋮----
/// Otherwise return None and leave `offset` unchanged.
    fn append_ptrs_locked(
⋮----
fn append_ptrs_locked(
⋮----
end = u64_align!(end);
⋮----
return Ok(None);
⋮----
self.append_ptr(offset, val.0, val.1)?
⋮----
self.current_len.store(*offset, Ordering::Release);
Ok(Some(pos))
⋮----
/// Return a reference to the type at `offset` if its data doesn't overrun the internal buffer.
    fn get_type<T>(slice: ValidSlice<'_>, offset: usize) -> Option<(&T, usize)> {
⋮----
fn get_type<T>(slice: ValidSlice<'_>, offset: usize) -> Option<(&T, usize)> {
⋮----
let ptr = data.as_ptr().cast();
//UNSAFE: The cast is safe because the slice is aligned and fits into the memory
//and the lifetime of the &T is tied to self, which holds the underlying memory map
Some((unsafe { &*ptr }, next))
⋮----
/// MmapMut could have more capacity than `len()` knows is valid.
    /// Return the subset of `mmap` that is known to be valid.
⋮----
/// Return the subset of `mmap` that is known to be valid.
    /// This allows comparisons against the slice len.
⋮----
/// This allows comparisons against the slice len.
    fn get_valid_slice_from_mmap<'a>(&self, mmap: &'a MmapMut) -> ValidSlice<'a> {
⋮----
fn get_valid_slice_from_mmap<'a>(&self, mmap: &'a MmapMut) -> ValidSlice<'a> {
ValidSlice(&mmap[..self.len()])
⋮----
pub fn get_stored_account_without_data_callback<Ret>(
⋮----
self.get_stored_account_no_data_callback(offset, |stored_account| {
⋮----
pubkey: stored_account.pubkey(),
lamports: stored_account.lamports(),
owner: stored_account.owner(),
data_len: stored_account.data_len() as usize,
executable: stored_account.executable(),
rent_epoch: stored_account.rent_epoch(),
⋮----
callback(account)
⋮----
pub fn get_stored_account_callback<Ret>(
⋮----
self.get_stored_account_meta_callback(offset, |stored_account_meta| {
⋮----
pubkey: stored_account_meta.pubkey(),
lamports: stored_account_meta.lamports(),
owner: stored_account_meta.owner(),
data: stored_account_meta.data(),
executable: stored_account_meta.executable(),
rent_epoch: stored_account_meta.rent_epoch(),
⋮----
pub fn get_stored_account_meta_callback<Ret>(
⋮----
let slice = self.get_valid_slice_from_mmap(mmap);
⋮----
Some(callback(StoredAccountMeta {
⋮----
let bytes_read = read_into_buffer(file, self.len(), offset, unsafe {
slice::from_raw_parts_mut(buf.as_mut_ptr() as *mut u8, buf.len())
⋮----
let valid_bytes = ValidSlice(unsafe {
slice::from_raw_parts(buf.as_ptr() as *const u8, bytes_read)
⋮----
Some(if remaining_bytes_for_data >= data_len as usize {
⋮----
assert!(data_len <= MAX_PERMITTED_DATA_LENGTH, "{data_len}");
⋮----
let bytes_read = read_into_buffer(file, self.len(), offset + next, unsafe {
slice::from_raw_parts_mut(data.as_mut_ptr() as *mut u8, data_len as usize)
⋮----
let data = unsafe { data.assume_init() };
let stored_size = aligned_stored_size(data_len as usize);
⋮----
fn get_stored_account_no_data_callback<Ret>(
⋮----
let stored_size = aligned_stored_size_checked(meta.data_len as usize)?;
Some(callback(StoredAccountNoData {
⋮----
pub fn get_account_shared_data(&self, offset: usize) -> Option<AccountSharedData> {
⋮----
.get_stored_account_meta_callback(offset, |account| {
create_account_shared_data(&account)
⋮----
read_into_buffer(file, self.len(), offset, unsafe { &mut *buf.as_mut_ptr() })
⋮----
let slice = data.spare_capacity_mut();
⋮----
slice::from_raw_parts_mut(slice.as_mut_ptr() as *mut u8, data_len as usize)
⋮----
unsafe { data.set_len(data_len as usize) };
⋮----
pub fn get_account_test(
⋮----
let data_len = self.get_account_data_lens(&[offset]);
⋮----
.iter()
.map(|len| AppendVec::calculate_stored_size(*len))
.sum();
let result = self.get_stored_account_meta_callback(offset, |r_callback| {
let r2 = self.get_account_shared_data(offset);
assert!(solana_account::accounts_equal(
⋮----
assert_eq!(sizes, r_callback.stored_size());
let pubkey = r_callback.meta().pubkey;
Some((pubkey, create_account_shared_data(&r_callback)))
⋮----
if result.is_none() {
assert!(self
⋮----
assert!(self.get_account_shared_data(offset).is_none());
assert_eq!(sizes, 0);
⋮----
result.flatten()
⋮----
pub fn path(&self) -> &Path {
self.path.as_path()
⋮----
fn next_account_offset(start_offset: usize, stored_meta: &StoredMeta) -> AccountOffsets {
⋮----
.checked_add(stored_meta.data_len as usize)
.expect("stored size cannot overflow");
⋮----
pub fn scan_accounts_without_data(
⋮----
self.scan_stored_accounts_no_data(|stored_account| {
let offset = stored_account.offset();
⋮----
callback(offset, account);
⋮----
pub(crate) fn scan_accounts<'a>(
⋮----
self.scan_accounts_stored_meta(reader, |stored_account_meta| {
let offset = stored_account_meta.offset();
⋮----
/// Iterate over all accounts and call `callback` with each account.
    ///
⋮----
///
    /// Prefer scan_accounts() when possible, as it does not contain file format
⋮----
/// Prefer scan_accounts() when possible, as it does not contain file format
    /// implementation details, and thus potentially can read less and be faster.
⋮----
/// implementation details, and thus potentially can read less and be faster.
    #[allow(clippy::blocks_in_conditions)]
fn scan_accounts_stored_meta<'a>(
⋮----
offset += account.stored_size();
if account.is_zero_lamport() && account.pubkey() == &Pubkey::default() {
⋮----
callback(account);
⋮----
.unwrap_or_default()
⋮----
reader.set_file(file, self.len())?;
⋮----
let offset = reader.get_file_offset();
let bytes = match reader.fill_buf_required(min_buf_len) {
⋮----
Err(err) if err.kind() == std::io::ErrorKind::UnexpectedEof => break,
Err(err) => return Err(AppendVecError::Io(err)),
⋮----
let (meta, next) = Self::get_type::<StoredMeta>(bytes, 0).unwrap();
let (account_meta, next) = Self::get_type::<AccountMeta>(bytes, next).unwrap();
⋮----
let (_hash, next) = Self::get_type::<ObsoleteAccountHash>(bytes, next).unwrap();
⋮----
let leftover = bytes.len() - next;
⋮----
let stored_size = aligned_stored_size(data_len);
⋮----
reader.consume(stored_size);
⋮----
pub fn scan_accounts_stored_meta_for_store_tool(
⋮----
let mut reader = new_scan_accounts_reader();
self.scan_accounts_stored_meta(&mut reader, callback)
⋮----
pub(crate) fn calculate_stored_size(data_len: usize) -> usize {
aligned_stored_size(data_len)
⋮----
pub(crate) fn get_account_data_lens(&self, sorted_offsets: &[usize]) -> Vec<usize> {
let self_len = self.len();
let mut account_sizes = Vec::with_capacity(sorted_offsets.len());
⋮----
account_sizes.push(stored_meta.data_len as usize);
⋮----
let Some(bytes_read) = read_into_buffer(file, self_len, offset, unsafe {
⋮----
buffer.as_mut_ptr() as *mut u8,
⋮----
.ok() else {
⋮----
let bytes = ValidSlice(unsafe {
slice::from_raw_parts(buffer.as_ptr() as *const u8, bytes_read)
⋮----
pub fn scan_pubkeys(&self, mut callback: impl FnMut(&Pubkey)) -> Result<()> {
⋮----
callback(account.pubkey());
⋮----
fn scan_stored_accounts_no_data(
⋮----
let Some(stored_size) = stored_size_checked(stored_meta.data_len as usize)
⋮----
let stored_size = u64_align!(stored_size);
callback(StoredAccountNoData {
⋮----
let mut reader = BufferedReader::<BUFFER_SIZE>::new().with_file(file, self_len);
⋮----
let bytes = match reader.fill_buf_required(REQUIRED_READ_LEN) {
⋮----
let (stored_meta, next) = Self::get_type::<StoredMeta>(bytes, 0).unwrap();
let (account_meta, _) = Self::get_type::<AccountMeta>(bytes, next).unwrap();
⋮----
pub fn append_accounts<'a>(
⋮----
let mut offset = self.len();
let len = accounts.len();
⋮----
accounts.account_default_if_zero_lamport(i, |account| {
⋮----
lamports: account.lamports(),
owner: *account.owner(),
rent_epoch: account.rent_epoch(),
executable: account.executable(),
⋮----
pubkey: *account.pubkey(),
data_len: account.data().len() as u64,
⋮----
let stored_meta_ptr = ptr::from_ref(&stored_meta).cast();
let account_meta_ptr = ptr::from_ref(&account_meta).cast();
let hash_ptr = ObsoleteAccountHash::ZEROED.0.as_ptr();
let data_ptr = account.data().as_ptr();
⋮----
.append_ptrs_locked(&mut offset, &ptrs)
.expect("must append data to append_vec")
⋮----
offsets.push(start_offset)
⋮----
if !offsets.is_empty() {
let was_dirty = self.is_dirty.swap(true, Ordering::AcqRel);
⋮----
APPEND_VEC_STATS.files_dirty.fetch_add(1, Ordering::Relaxed);
⋮----
(!offsets.is_empty()).then(|| {
offsets.push(u64_align!(offset));
let size = offsets.windows(2).map(|offset| offset[1] - offset[0]).sum();
offsets.pop();
⋮----
pub(crate) fn internals_for_archive(&self) -> InternalsForArchive<'_> {
⋮----
AppendVecFileBacking::File(_file) => InternalsForArchive::FileIo(self.path()),
// note this returns the entire mmap slice, even bytes that we consider invalid
⋮----
/// Create a reusable buffered reader tuned for scanning storages with account data.
pub(crate) fn new_scan_accounts_reader<'a>() -> impl RequiredLenBufFileRead<'a> {
⋮----
pub(crate) fn new_scan_accounts_reader<'a>() -> impl RequiredLenBufFileRead<'a> {
// 128KiB covers a reasonably large distribution of typical account sizes.
// In a recent sample, 99.98% of accounts' data lengths were less than or equal to 128KiB.
⋮----
struct ObsoleteAccountHash([u8; 32]);
impl ObsoleteAccountHash {
const ZEROED: Self = Self([0; 32]);
⋮----
pub mod tests {
⋮----
fn append_account_test(&self, data: &(Pubkey, AccountSharedData)) -> Option<usize> {
⋮----
self.append_accounts(&storable_accounts, 0)
.map(|res| res.offsets[0])
⋮----
fn test_account_meta_default() {
⋮----
assert_eq!(&def1, &def2);
⋮----
let def2 = AccountMeta::from(Some(&AccountSharedData::default()));
⋮----
fn test_account_meta_non_default() {
⋮----
let def2 = AccountMeta::from(&AccountSharedData::from(def2_account.clone()));
⋮----
let def2 = AccountMeta::from(Some(&AccountSharedData::from(def2_account)));
⋮----
fn test_append_vec_new_bad_size(storage_access: StorageAccess) {
let path = get_append_vec_path("test_append_vec_new_bad_size");
⋮----
fn test_append_vec_new_from_file_bad_size(storage_access: StorageAccess) {
let file = get_append_vec_path("test_append_vec_new_from_file_bad_size");
⋮----
.create_new(true)
.open(path)
.expect("create a test file for mmap");
⋮----
assert_matches!(result, Err(ref message) if message.to_string().contains("too small file size 0 for AppendVec"));
⋮----
fn test_append_vec_sanitize_len_and_size_too_small() {
⋮----
fn test_append_vec_sanitize_len_and_size_maximum() {
⋮----
assert_matches!(result, Ok(_));
⋮----
fn test_append_vec_sanitize_len_and_size_too_large() {
⋮----
assert_matches!(result, Err(ref message) if message.to_string().contains("too large file size 17179869185 for AppendVec"));
⋮----
fn test_append_vec_sanitize_len_and_size_full_and_same_as_current_len() {
⋮----
fn test_append_vec_sanitize_len_and_size_larger_current_len() {
⋮----
assert_matches!(result, Err(ref message) if message.to_string().contains("is larger than file size (1048576)"));
⋮----
fn test_append_vec_one(storage_access: StorageAccess) {
let path = get_append_vec_path("test_append");
⋮----
let account = create_test_account(0);
let index = av.append_account_test(&account).unwrap();
assert_eq!(av.get_account_test(index).unwrap(), account);
truncate_and_test(av, index);
⋮----
fn truncate_and_test(av: AppendVec, index: usize) {
⋮----
av.current_len.fetch_sub(1, Ordering::Relaxed);
assert_eq!(av.get_account_test(index), None);
⋮----
fn test_append_vec_one_with_data(storage_access: StorageAccess) {
⋮----
let account = create_test_account(data_len);
⋮----
assert_eq!(
⋮----
fn test_remaining_bytes(storage_access: StorageAccess) {
⋮----
assert_eq!(av.capacity(), sz64);
assert_eq!(av.remaining_bytes(), sz64);
⋮----
av.append_account_test(&account).unwrap();
⋮----
assert_eq!(av.remaining_bytes(), sz64 - (STORE_META_OVERHEAD as u64));
assert_eq!(av.len(), av_len);
let account = create_test_account(1);
⋮----
let alignment_bytes = u64_align!(av_len) - av_len;
assert_eq!(alignment_bytes, 7);
assert_eq!(av.remaining_bytes(), sz64 - u64_align!(av_len) as u64);
⋮----
fn test_append_vec_data(storage_access: StorageAccess) {
let path = get_append_vec_path("test_append_data");
⋮----
let account = create_test_account(5);
⋮----
let account1 = create_test_account(6);
let index1 = av.append_account_test(&account1).unwrap();
⋮----
assert_eq!(av.get_account_test(index1).unwrap(), account1);
⋮----
fn accounts_count(&self) -> usize {
⋮----
self.scan_stored_accounts_no_data(|_| {
⋮----
.expect("must scan accounts storage");
⋮----
fn rand_exhaustive_append_vec(
⋮----
let mut rng = rng();
⋮----
let pubkey = Pubkey::new_from_array(rng.random());
let owner = Pubkey::new_from_array(rng.random());
let mut account = AccountSharedData::new(rng.random(), data_len, &owner);
let data = std::iter::from_fn(|| Some(rng.random::<u8>()))
.take(data_len)
⋮----
account.set_data(data);
⋮----
let account = create_account(data_len);
let size = aligned_stored_size(account.1.data().len());
⋮----
test_accounts.push(account);
⋮----
let path = get_append_vec_path("test_scan_accounts_stored_meta_correctness");
⋮----
.append_accounts(&(slot, test_accounts.as_slice()), 0)
⋮----
av.flush().unwrap();
⋮----
fn test_scan_accounts_correctness() {
⋮----
let (av_mmap, _, test_accounts, path) = rand_exhaustive_append_vec(num_accounts);
let av_file = AppendVec::new_from_file(&path.path, av_mmap.len(), StorageAccess::File)
.unwrap()
⋮----
av.scan_accounts_stored_meta(&mut reader, |v| {
⋮----
let recovered = create_account_shared_data(&v);
assert_eq!(&recovered, account);
assert_eq!(v.pubkey(), pubkey);
⋮----
assert_eq!(index, num_accounts);
⋮----
av.scan_stored_accounts_no_data(|stored_account| {
⋮----
assert_eq!(stored_account.pubkey(), pubkey);
assert_eq!(stored_account.lamports(), account.lamports());
assert_eq!(stored_account.owner(), account.owner());
assert_eq!(stored_account.data_len(), account.data().len() as u64);
assert_eq!(stored_account.executable(), account.executable());
assert_eq!(stored_account.rent_epoch(), account.rent_epoch());
⋮----
fn test_scan_useless_accounts() {
⋮----
rand_exhaustive_append_vec(num_accounts);
let av_current_len = av_mmap.len();
⋮----
let slice = av_mmap.get_valid_slice_from_mmap(mmap);
⋮----
AppendVec::get_type::<StoredMeta>(slice, stored_meta_offset).unwrap();
⋮----
AppendVec::get_type::<AccountMeta>(slice, account_meta_offset).unwrap();
assert_eq!(stored_meta.pubkey, test_accounts[num_new_accounts].0);
⋮----
..stored_meta.clone()
⋮----
..account_meta.clone()
⋮----
.append_ptr(
⋮----
ptr::from_ref(&new_stored_meta).cast(),
⋮----
ptr::from_ref(&new_account_meta).cast(),
⋮----
av_mmap.flush().unwrap();
⋮----
panic!("append vec must be mmap");
⋮----
av.scan_accounts_stored_meta(&mut reader, |stored_account| {
⋮----
let recovered = create_account_shared_data(&stored_account);
⋮----
assert_eq!(recovered, *account);
⋮----
assert_eq!(index, num_new_accounts);
⋮----
fn test_append_vec_append_many(storage_access: StorageAccess) {
let path = get_append_vec_path("test_append_many");
⋮----
let mut indexes = vec![];
⋮----
let mut sizes = vec![];
⋮----
let account = create_test_account(sample + 1);
sizes.push(aligned_stored_size(account.1.data().len()));
let pos = av.append_account_test(&account).unwrap();
assert_eq!(av.get_account_test(pos).unwrap(), account);
indexes.push(pos);
⋮----
.get_account_data_lens(indexes.as_slice())
⋮----
assert_eq!(sizes.iter().sum::<usize>(), stored_size);
⋮----
trace!("append time: {} ms", now.elapsed().as_millis());
⋮----
let sample = rng().random_range(0..indexes.len());
⋮----
assert_eq!(av.get_account_test(indexes[sample]).unwrap(), account);
⋮----
trace!("random read time: {} ms", now.elapsed().as_millis());
assert_eq!(indexes.len(), size);
assert_eq!(indexes[0], 0);
assert_eq!(av.accounts_count(), size);
⋮----
assert_eq!(recovered, account.1);
⋮----
trace!("sequential read time: {} ms", now.elapsed().as_millis());
⋮----
fn test_new_from_file_crafted_zero_lamport_account(storage_access: StorageAccess) {
let file = get_append_vec_path("test_append_bytes");
⋮----
let f = std::fs::File::create(path).unwrap();
⋮----
writer.write_all(append_vec_data.as_slice()).unwrap();
⋮----
assert_matches!(result, Err(ref message) if message.to_string().contains("incorrect layout/length/data"));
⋮----
fn test_new_from_file_crafted_data_len(storage_access: StorageAccess) {
let file = get_append_vec_path("test_new_from_file_crafted_data_len");
⋮----
av.append_account_test(&create_test_account(10)).unwrap();
⋮----
av.len()
⋮----
assert!(av.is_ok());
⋮----
let mut file = OpenOptions::new().write(true).open(path).unwrap();
file.seek(SeekFrom::Start(ACCOUNT_0_DATA_LEN_OFFSET))
⋮----
file.write_all(&crafted_data_len.to_ne_bytes()).unwrap();
file.flush().unwrap();
⋮----
fn test_append_vec_reset(storage_access: StorageAccess) {
let file = get_append_vec_path("test_append_vec_reset");
⋮----
assert!(!av.is_empty());
av.reset();
assert_eq!(av.len(), 0);
⋮----
fn test_append_vec_flush(storage_access: StorageAccess) {
let file = get_append_vec_path("test_append_vec_flush");
⋮----
AppendVec::new_from_file(path, accounts_len, storage_access).unwrap();
⋮----
assert_eq!(num_account, 1);
⋮----
fn test_append_vec_reopen_as_readonly(storage_access: StorageAccess) {
⋮----
av.reopen_as_readonly_file_io()
.expect("appendable AppendVec should always re-open as read-only"),
⋮----
ro_av.len()
⋮----
let (av, _) = AppendVec::new_from_file(path, accounts_len, storage_access).unwrap();
let reopen = av.reopen_as_readonly_file_io();
⋮----
assert!(reopen.is_none());
⋮----
assert!(reopen.is_some());
⋮----
fn test_new_from_file_too_large_data_len(storage_access: StorageAccess) {
let file = get_append_vec_path("test_new_from_file_too_large_data_len");
⋮----
file.write_all(&too_large_data_len.to_ne_bytes()).unwrap();
⋮----
fn test_new_from_file_crafted_executable(storage_access: StorageAccess) {
let file = get_append_vec_path("test_new_from_crafted_executable");
⋮----
let mut executable_account = create_test_account(10);
executable_account.1.set_executable(true);
av.append_account_test(&executable_account).unwrap()
⋮----
av.get_stored_account_no_data_callback(0, |account| {
assert_eq!(*account.ref_executable_byte(), 0);
⋮----
av.get_stored_account_no_data_callback(offset_1, |account| {
assert_eq!(*account.ref_executable_byte(), 1);
⋮----
file.seek(SeekFrom::Start(ACCOUNT_0_EXECUTABLE_OFFSET))
⋮----
file.write_all(&[crafted_executable]).unwrap();
⋮----
fn test_type_layout() {
assert_eq!(offset_of!(StoredMeta, write_version_obsolete), 0x00);
assert_eq!(offset_of!(StoredMeta, data_len), 0x08);
assert_eq!(offset_of!(StoredMeta, pubkey), 0x10);
assert_eq!(mem::size_of::<StoredMeta>(), 0x30);
assert_eq!(offset_of!(AccountMeta, lamports), 0x00);
assert_eq!(offset_of!(AccountMeta, rent_epoch), 0x08);
assert_eq!(offset_of!(AccountMeta, owner), 0x10);
assert_eq!(offset_of!(AccountMeta, executable), 0x30);
assert_eq!(mem::size_of::<AccountMeta>(), 0x38);
⋮----
fn test_get_account_shared_data_from_truncated_file(storage_access: StorageAccess) {
let file = get_append_vec_path("test_get_account_shared_data_from_truncated_file");
⋮----
let account = create_test_account_with(data_len);
⋮----
aligned_stored_size(data_len),
⋮----
let account = av.get_account_shared_data(0);
assert!(account.is_none());
let result = av.get_stored_account_meta_callback(0, |_| true);
assert!(result.is_none());
⋮----
fn test_get_account_sizes(storage_access: StorageAccess) {
⋮----
.take(NUM_ACCOUNTS)
.collect();
⋮----
let mut accounts = Vec::with_capacity(pubkeys.len());
let mut stored_sizes = Vec::with_capacity(pubkeys.len());
⋮----
let lamports = rng.random();
let data_len = rng.random_range(0..MAX_PERMITTED_DATA_LENGTH) as usize;
⋮----
accounts.push(account);
stored_sizes.push(aligned_stored_size(data_len));
⋮----
let total_stored_size = stored_sizes.iter().sum();
let temp_file = get_append_vec_path("test_get_account_sizes");
⋮----
let storable_accounts: Vec<_> = std::iter::zip(&pubkeys, &accounts).collect();
⋮----
.append_accounts(&(slot, storable_accounts.as_slice()), 0)
⋮----
append_vec.flush().unwrap();
⋮----
AppendVec::new_from_file(&temp_file.path, total_stored_size, storage_access).unwrap();
⋮----
.get_account_data_lens(account_offsets.as_slice())
⋮----
assert_eq!(account_sizes, total_stored_size);
⋮----
fn test_scan_helper(
⋮----
total_stored_size += aligned_stored_size(data_len);
⋮----
let temp_file = get_append_vec_path("test_scan");
⋮----
let total_stored_size = modify_fn(&temp_file.path, total_stored_size);
⋮----
.unwrap(),
⋮----
check_fn(&append_vec, &pubkeys, &account_offsets, &accounts);
⋮----
fn test_scan_pubkeys_helper(
⋮----
test_scan_helper(
⋮----
.scan_pubkeys(|pubkey| {
assert_eq!(pubkey, pubkeys.get(i).unwrap());
⋮----
assert_eq!(i, pubkeys.len());
⋮----
fn test_scan_pubkeys(storage_access: StorageAccess) {
test_scan_pubkeys_helper(storage_access, |_, size| size);
⋮----
fn test_scan_pubkeys_incomplete_data(storage_access: StorageAccess) {
test_scan_pubkeys_helper(storage_access, |path, size| {
⋮----
.append(true)
⋮----
f.write_all(&[0xFF]).unwrap();
⋮----
fn test_scan_pubkeys_missing_account_data(storage_access: StorageAccess) {
⋮----
f.write_all(stored_meta_slice).unwrap();
f.write_all(account_meta_slice).unwrap();
⋮----
fn test_scan_stored_accounts_no_data_helper(
⋮----
.scan_stored_accounts_no_data(|stored_account| {
let pubkey = pubkeys.get(i).unwrap();
let account = accounts.get(i).unwrap();
let offset = account_offsets.get(i).unwrap();
⋮----
assert_eq!(stored_account.offset(), *offset);
⋮----
assert_eq!(i, accounts.len());
⋮----
fn test_scan_stored_accounts_no_data(storage_access: StorageAccess) {
test_scan_stored_accounts_no_data_helper(storage_access, |_, size| size);
⋮----
fn test_scan_stored_accounts_no_data_incomplete_data(storage_access: StorageAccess) {
test_scan_stored_accounts_no_data_helper(storage_access, |path, size| {
⋮----
fn test_scan_stored_accounts_no_data_missing_account_data(storage_access: StorageAccess) {
⋮----
fn test_scan_accounts_stored_meta_helper(
⋮----
.scan_accounts_stored_meta(&mut reader, |stored_account| {
⋮----
assert!(accounts_equal(&stored_account, account));
⋮----
fn test_scan_accounts_stored_meta(storage_access: StorageAccess) {
test_scan_accounts_stored_meta_helper(storage_access, |_, size| size);
⋮----
fn test_scan_accounts_stored_meta_incomplete_meta_data(storage_access: StorageAccess) {
test_scan_accounts_stored_meta_helper(storage_access, |path, size| {
⋮----
fn test_scan_accounts_stored_meta_missing_account_data(storage_access: StorageAccess) {
⋮----
fn test_is_dirty(begins_dirty: bool, storage_access: StorageAccess) {
let file = get_append_vec_path("test_is_dirty");
⋮----
*av1.remove_file_on_drop.get_mut() = false;
assert!(!*av1.is_dirty.get_mut());
⋮----
av1.append_account_test(&create_test_account(10)).unwrap();
⋮----
assert_eq!(*av1.is_dirty.get_mut(), begins_dirty);
let mut av2 = av1.reopen_as_readonly_file_io().unwrap();
*av2.remove_file_on_drop.get_mut() = false;
⋮----
assert_eq!(*av2.is_dirty.get_mut(), begins_dirty);
assert!(av2.flush().is_ok());
assert!(!*av2.is_dirty.get_mut());
assert!(av1.flush().is_ok());

================
File: accounts-db/src/blockhash_queue.rs
================
use solana_sysvar::recent_blockhashes;
⋮----
pub struct HashInfo {
⋮----
impl HashInfo {
pub fn lamports_per_signature(&self) -> u64 {
⋮----
pub struct BlockhashQueue {
⋮----
impl Default for BlockhashQueue {
fn default() -> Self {
⋮----
impl BlockhashQueue {
pub fn new(max_age: usize) -> Self {
⋮----
pub fn last_hash(&self) -> Hash {
self.last_hash.expect("no hash has been set")
⋮----
pub fn get_lamports_per_signature(&self, hash: &Hash) -> Option<u64> {
⋮----
.get(hash)
.map(|hash_age| hash_age.fee_calculator.lamports_per_signature)
⋮----
pub fn is_hash_valid_for_age(&self, hash: &Hash, max_age: usize) -> bool {
self.get_hash_info_if_valid(hash, max_age).is_some()
⋮----
pub fn get_hash_info_if_valid(&self, hash: &Hash, max_age: usize) -> Option<&HashInfo> {
self.hashes.get(hash).filter(|info| {
⋮----
pub fn get_hash_age(&self, hash: &Hash) -> Option<u64> {
⋮----
.map(|info| self.last_hash_index - info.hash_index)
⋮----
pub fn genesis_hash(&mut self, hash: &Hash, lamports_per_signature: u64) {
self.hashes.insert(
⋮----
timestamp: timestamp(),
⋮----
self.last_hash = Some(*hash);
⋮----
fn is_hash_index_valid(last_hash_index: u64, max_age: usize, hash_index: u64) -> bool {
⋮----
pub fn register_hash(&mut self, hash: &Hash, lamports_per_signature: u64) {
⋮----
if self.hashes.len() >= self.max_age {
self.hashes.retain(|_, info| {
⋮----
pub fn get_recent_blockhashes(&self) -> impl Iterator<Item = recent_blockhashes::IterItem<'_>> {
(self.hashes).iter().map(|(k, v)| {
⋮----
pub fn get_max_age(&self) -> usize {
⋮----
mod tests {
⋮----
use solana_sysvar::recent_blockhashes::IterItem;
⋮----
fn test_register_hash() {
⋮----
assert!(!hash_queue.is_hash_valid_for_age(&last_hash, max_age));
hash_queue.register_hash(&last_hash, 0);
assert!(hash_queue.is_hash_valid_for_age(&last_hash, max_age));
assert_eq!(hash_queue.last_hash_index, 1);
⋮----
fn test_reject_old_last_hash() {
⋮----
let last_hash = hash(&serialize(&0).unwrap());
⋮----
let last_hash = hash(&serialize(&i).unwrap());
⋮----
// Assert we're no longer able to use the oldest hash.
⋮----
assert!(!hash_queue.is_hash_valid_for_age(&last_hash, 0));
let last_valid_hash = hash(&serialize(&1).unwrap());
assert!(hash_queue.is_hash_valid_for_age(&last_valid_hash, max_age));
assert!(!hash_queue.is_hash_valid_for_age(&last_valid_hash, 0));
⋮----
fn test_queue_init_blockhash() {
⋮----
assert_eq!(last_hash, hash_queue.last_hash());
assert!(hash_queue.is_hash_valid_for_age(&last_hash, 0));
⋮----
fn test_get_recent_blockhashes() {
⋮----
let recent_blockhashes = blockhash_queue.get_recent_blockhashes();
assert_eq!(recent_blockhashes.count(), 0);
⋮----
let hash = hash(&serialize(&i).unwrap());
blockhash_queue.register_hash(&hash, 0);
⋮----
assert!(blockhash_queue.is_hash_valid_for_age(hash, MAX_RECENT_BLOCKHASHES));
⋮----
fn test_len() {
⋮----
assert_eq!(hash_queue.hashes.len(), 0);
⋮----
hash_queue.register_hash(&Hash::new_unique(), 0);
⋮----
assert_eq!(hash_queue.hashes.len(), MAX_AGE);
⋮----
assert_eq!(hash_queue.hashes.len(), MAX_AGE + 1);
⋮----
fn test_get_hash_age() {
⋮----
hash_list.resize_with(MAX_AGE + 1, Hash::new_unique);
⋮----
assert!(hash_queue.get_hash_age(hash).is_none());
⋮----
hash_queue.register_hash(hash, 0);
⋮----
for (age, hash) in hash_list.iter().rev().enumerate() {
assert_eq!(hash_queue.get_hash_age(hash), Some(age as u64));
⋮----
assert!(hash_queue.get_hash_age(&hash_list[0]).is_none());
⋮----
fn test_is_hash_valid_for_age() {
⋮----
assert!(!hash_queue.is_hash_valid_for_age(hash, MAX_AGE));
⋮----
assert!(hash_queue.is_hash_valid_for_age(hash, MAX_AGE));
⋮----
assert!(hash_queue.is_hash_valid_for_age(&hash_list[MAX_AGE], 0));
assert!(!hash_queue.is_hash_valid_for_age(&hash_list[MAX_AGE - 1], 0));
⋮----
fn test_get_hash_info_if_valid() {
⋮----
assert!(hash_queue.get_hash_info_if_valid(hash, MAX_AGE).is_none());
⋮----
assert_eq!(
⋮----
assert!(hash_queue

================
File: accounts-db/src/contains.rs
================
pub trait Contains<'a, T: Eq + Hash> {
⋮----
type Item = &'a T;
type Iter = std::collections::hash_map::Keys<'a, T, U>;
fn contains(&self, key: &T) -> bool {
⋮----
fn contains_iter(&'a self) -> Self::Iter {
self.keys()
⋮----
type Iter = std::collections::hash_set::Iter<'a, T>;
⋮----
self.iter()
⋮----
type Iter = std::iter::Once<&'a T>;

================
File: accounts-db/src/is_loadable.rs
================
use crate::is_zero_lamport::IsZeroLamport;
pub trait IsLoadable {
⋮----
impl<T: IsZeroLamport> IsLoadable for T {
fn is_loadable(&self) -> bool {
!self.is_zero_lamport()

================
File: accounts-db/src/is_zero_lamport.rs
================
pub trait IsZeroLamport {

================
File: accounts-db/src/lib.rs
================
pub mod account_info;
pub mod account_locks;
pub mod account_storage;
pub mod account_storage_reader;
pub mod accounts;
mod accounts_cache;
pub mod accounts_db;
pub mod accounts_file;
pub mod accounts_hash;
pub mod accounts_index;
pub mod accounts_update_notifier_interface;
mod active_stats;
pub mod ancestors;
mod ancient_append_vecs;
⋮----
pub mod append_vec;
⋮----
mod append_vec;
pub mod blockhash_queue;
pub mod contains;
pub mod is_loadable;
mod is_zero_lamport;
mod obsolete_accounts;
pub mod partitioned_rewards;
pub mod pubkey_bins;
⋮----
pub mod read_only_accounts_cache;
⋮----
mod read_only_accounts_cache;
mod rolling_bit_field;
pub mod sorted_storages;
pub mod stake_rewards;
pub mod storable_accounts;
pub mod tiered_storage;
pub mod utils;
pub mod waitable_condvar;
⋮----
extern crate solana_metrics;
⋮----
extern crate solana_frozen_abi_macro;

================
File: accounts-db/src/obsolete_accounts.rs
================
pub struct ObsoleteAccountItem {
⋮----
pub struct ObsoleteAccounts {
⋮----
impl ObsoleteAccounts {
pub fn mark_accounts_obsolete(
⋮----
self.accounts.reserve(newly_obsolete_accounts.len());
⋮----
self.accounts.push(ObsoleteAccountItem {
⋮----
pub fn filter_obsolete_accounts(
⋮----
.iter()
.filter(move |obsolete_account| slot.is_none_or(|s| obsolete_account.slot <= s))
.map(|obsolete_account| (obsolete_account.offset, obsolete_account.data_len))
⋮----
pub fn obsolete_accounts_for_snapshots(&self, slot: Slot) -> ObsoleteAccounts {
⋮----
.filter(|account| account.slot <= slot)
.cloned()
.collect();
⋮----
mod tests {
⋮----
fn test_mark_accounts_obsolete() {
⋮----
let new_accounts = vec![(10, 100), (20, 200), (30, 300)];
⋮----
obsolete_accounts.mark_accounts_obsolete(new_accounts.into_iter(), slot);
let expected_accounts = vec![(10, 100), (20, 200), (30, 300)];
⋮----
.map(|item| (item.offset, item.data_len))
⋮----
assert_eq!(actual_accounts, expected_accounts);
⋮----
fn test_filter_obsolete_accounts() {
⋮----
let new_accounts = vec![(10, 100, 40), (20, 200, 42), (30, 300, 44)]
.into_iter()
.map(|(offset, data_len, slot)| ObsoleteAccountItem {
⋮----
new_accounts.into_iter().for_each(|item| {
⋮----
.mark_accounts_obsolete([(item.offset, item.data_len)].into_iter(), item.slot)
⋮----
.filter_obsolete_accounts(Some(42))
⋮----
assert_eq!(filtered_accounts, vec![(10, 100), (20, 200)]);
let filtered_accounts: Vec<_> = obsolete_accounts.filter_obsolete_accounts(None).collect();
assert_eq!(filtered_accounts, vec![(10, 100), (20, 200), (30, 300)]);
⋮----
fn test_obsolete_accounts_for_snapshots() {
⋮----
new_accounts.iter().for_each(|item| {
⋮----
let obsolete_accounts_for_snapshots = obsolete_accounts.obsolete_accounts_for_snapshots(42);
⋮----
.filter(|account| account.slot <= 42)
⋮----
assert_eq!(obsolete_accounts_for_snapshots.accounts, expected_accounts);
⋮----
obsolete_accounts.obsolete_accounts_for_snapshots(100);
assert_eq!(obsolete_accounts_for_snapshots.accounts, new_accounts);

================
File: accounts-db/src/partitioned_rewards.rs
================
pub struct PartitionedEpochRewardsConfig {
⋮----
impl Default for PartitionedEpochRewardsConfig {
fn default() -> Self {
⋮----
impl PartitionedEpochRewardsConfig {
pub fn new_for_test(stake_account_stores_per_block: u64) -> Self {

================
File: accounts-db/src/pubkey_bins.rs
================
use solana_pubkey::Pubkey;
⋮----
pub struct PubkeyBinCalculator24 {
⋮----
impl PubkeyBinCalculator24 {
⋮----
const fn num_bits<T>() -> usize {
⋮----
pub(crate) fn log_2(x: u32) -> u32 {
assert!(x > 0);
Self::num_bits::<u32>() as u32 - x.leading_zeros() - 1
⋮----
pub fn new(bins: usize) -> Self {
assert!(bins > 0);
⋮----
assert!(bins <= max_plus_1);
assert!(bins.is_power_of_two());
⋮----
pub fn bins(&self) -> usize {
⋮----
pub fn bin_from_pubkey(&self, pubkey: &Pubkey) -> usize {
let as_ref = pubkey.as_ref();
⋮----
pub(crate) fn lowest_pubkey_from_bin(&self, mut bin: usize) -> Pubkey {
assert!(bin < self.bins());
⋮----
pubkey.as_mut()[0] = ((bin / 256 / 256) & 0xff) as u8;
pubkey.as_mut()[1] = ((bin / 256) & 0xff) as u8;
pubkey.as_mut()[2] = (bin & 0xff) as u8;
⋮----
pub(crate) fn highest_pubkey_from_bin(&self, mut bin: usize) -> Pubkey {
⋮----
pub mod tests {
⋮----
fn test_pubkey_bins_log2() {
assert_eq!(PubkeyBinCalculator24::num_bits::<u8>(), 8);
assert_eq!(PubkeyBinCalculator24::num_bits::<u32>(), 32);
⋮----
assert_eq!(PubkeyBinCalculator24::log_2(2u32.pow(i)), i);
⋮----
fn test_pubkey_lowest_highest_bin4() {
⋮----
assert_eq!(calc.lowest_pubkey_from_bin(0), expected_lowest);
assert_eq!(calc.highest_pubkey_from_bin(0), expected_highest);
⋮----
assert_eq!(calc.lowest_pubkey_from_bin(1), expected_lowest,);
assert_eq!(calc.highest_pubkey_from_bin(1), expected_highest);
⋮----
assert_eq!(calc.lowest_pubkey_from_bin(2), expected_lowest);
assert_eq!(calc.highest_pubkey_from_bin(2), expected_highest);
⋮----
assert_eq!(calc.lowest_pubkey_from_bin(3), expected_lowest);
assert_eq!(calc.highest_pubkey_from_bin(3), expected_highest);
⋮----
fn test_pubkey_bins() {
⋮----
let bins = 2u32.pow(i);
⋮----
assert_eq!(calc.shift_bits, 24 - i, "i: {i}");
⋮----
assert_eq!(
⋮----
assert_eq!(calc.bins(), bins as usize);
⋮----
fn test_pubkey_bins_pubkeys() {
⋮----
let bins = 2usize.pow(i);
⋮----
pk.as_mut()[0] = 0;
assert_eq!(0, calc.bin_from_pubkey(&pk));
pk.as_mut()[0] = 0xff;
assert_eq!(bins - 1, calc.bin_from_pubkey(&pk));
⋮----
pk.as_mut()[0] = (bin << shift_bits) as u8;
⋮----
pk.as_mut()[0] = ((bin << shift_bits) - 1) as u8;
assert_eq!(bin - 1, calc.bin_from_pubkey(&pk));
⋮----
pk.as_mut()[1] = 0;
⋮----
pk.as_mut()[1] = 0xff;
⋮----
pk.as_mut()[0] = (target / 256) as u8;
pk.as_mut()[1] = (target % 256) as u8;
⋮----
pk.as_mut()[2] = 0xff;
⋮----
pk.as_mut()[0] = (target / 256 / 256) as u8;
pk.as_mut()[1] = ((target / 256) % 256) as u8;
pk.as_mut()[2] = (target % 256) as u8;
⋮----
fn test_pubkey_bins_illegal_bins3() {
⋮----
fn test_pubkey_bins_illegal_bins2() {
⋮----
fn test_pubkey_bins_illegal_bins() {

================
File: accounts-db/src/read_only_accounts_cache.rs
================
type ReadOnlyCacheKey = Pubkey;
⋮----
struct ReadOnlyAccountCacheEntry {
⋮----
pub struct ReadOnlyCacheStats {
⋮----
struct AtomicReadOnlyCacheStats {
⋮----
pub(crate) struct ReadOnlyAccountsCache {
⋮----
impl ReadOnlyAccountsCache {
⋮----
pub(crate) fn new(
⋮----
assert!(max_data_size_lo <= max_data_size_hi);
assert!(evict_sample_size > 0);
⋮----
evictor_exit_flag.clone(),
⋮----
data_size.clone(),
⋮----
cache.clone(),
stats.clone(),
⋮----
pub(crate) fn in_cache(&self, pubkey: &Pubkey, slot: Slot) -> bool {
if let Some(entry) = self.cache.get(pubkey) {
⋮----
pub(crate) fn load(&self, pubkey: Pubkey, slot: Slot) -> Option<AccountSharedData> {
let (account, load_us) = measure_us!({
⋮----
self.stats.load_us.fetch_add(load_us, Ordering::Relaxed);
⋮----
fn account_size(account: &AccountSharedData) -> usize {
CACHE_ENTRY_SIZE + account.data().len()
⋮----
pub(crate) fn store(&self, pubkey: Pubkey, slot: Slot, account: AccountSharedData) {
self.store_with_timestamp(pubkey, slot, account, self.timestamp())
⋮----
fn store_with_timestamp(
⋮----
self.highest_slot_stored.fetch_max(slot, Ordering::Release);
⋮----
self.data_size.fetch_add(account_size, Ordering::Relaxed);
match self.cache.entry(pubkey) {
⋮----
entry.insert(ReadOnlyAccountCacheEntry::new(account, slot, timestamp));
⋮----
let entry = entry.get_mut();
⋮----
self.data_size.fetch_sub(account_size, Ordering::Relaxed);
⋮----
entry.last_update_time.store(timestamp, Ordering::Relaxed);
⋮----
let store_us = measure_store.end_as_us();
self.stats.store_us.fetch_add(store_us, Ordering::Relaxed);
⋮----
/// true if any pubkeys could have ever been stored into the cache at `slot`
    pub(crate) fn can_slot_be_in_cache(&self, slot: Slot) -> bool {
⋮----
pub(crate) fn can_slot_be_in_cache(&self, slot: Slot) -> bool {
self.highest_slot_stored.load(Ordering::Acquire) >= slot
⋮----
/// remove entry if it exists.
    /// Assume the entry does not exist for performance.
⋮----
/// Assume the entry does not exist for performance.
    pub(crate) fn remove_assume_not_present(&self, pubkey: &Pubkey) -> Option<AccountSharedData> {
⋮----
pub(crate) fn remove_assume_not_present(&self, pubkey: &Pubkey) -> Option<AccountSharedData> {
// get read lock first to see if the entry exists
⋮----
.contains_key(pubkey)
.then(|| self.remove(pubkey))
.flatten()
⋮----
pub(crate) fn remove(&self, pubkey: &Pubkey) -> Option<AccountSharedData> {
Self::do_remove(pubkey, &self.cache, &self.data_size).map(|entry| entry.account)
⋮----
fn do_remove(
⋮----
let (_, entry) = cache.remove(key)?;
⋮----
data_size.fetch_sub(account_size, Ordering::Relaxed);
Some(entry)
⋮----
pub(crate) fn cache_len(&self) -> usize {
self.cache.len()
⋮----
pub(crate) fn data_size(&self) -> usize {
self.data_size.load(Ordering::Relaxed)
⋮----
pub(crate) fn get_and_reset_stats(&self) -> ReadOnlyCacheStats {
let hits = self.stats.hits.swap(0, Ordering::Relaxed);
let misses = self.stats.misses.swap(0, Ordering::Relaxed);
let evicts = self.stats.evicts.swap(0, Ordering::Relaxed);
let load_us = self.stats.load_us.swap(0, Ordering::Relaxed);
let store_us = self.stats.store_us.swap(0, Ordering::Relaxed);
let evict_us = self.stats.evict_us.swap(0, Ordering::Relaxed);
⋮----
.swap(0, Ordering::Relaxed);
⋮----
fn spawn_evictor(
⋮----
.name("solAcctReadCache".to_string())
.spawn(move || {
info!("AccountsReadCacheEvictor has started");
let mut rng = rng();
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
.fetch_add(1, Ordering::Relaxed);
if data_size.load(Ordering::Relaxed) <= max_data_size_hi {
⋮----
let (num_evicts, evict_us) = measure_us!(Self::evict(
⋮----
stats.evicts.fetch_add(num_evicts, Ordering::Relaxed);
stats.evict_us.fetch_add(evict_us, Ordering::Relaxed);
⋮----
info!("AccountsReadCacheEvictor has stopped");
⋮----
.expect("spawn accounts read cache evictor thread")
⋮----
fn evict<R>(
⋮----
while data_size.load(Ordering::Relaxed) > target_data_size {
⋮----
.shards()
.choose(rng)
.expect("number of shards should be greater than zero");
let shard = shard.read();
for (key, entry) in shard.iter().choose_multiple(rng, remaining_samples) {
let last_update_time = entry.get().last_update_time.load(Ordering::Relaxed);
⋮----
key_to_evict = Some(key.to_owned());
⋮----
remaining_samples = remaining_samples.saturating_sub(1);
⋮----
let key = key_to_evict.expect("eviction sample should not be empty");
⋮----
callback(&key, _entry);
⋮----
num_evicts = num_evicts.saturating_add(1);
⋮----
fn timestamp(&self) -> u64 {
self.timer.elapsed().as_nanos() as u64
⋮----
pub fn evict_in_foreground<R, C>(
⋮----
impl Drop for ReadOnlyAccountsCache {
fn drop(&mut self) {
self.evictor_exit_flag.store(true, Ordering::Relaxed);
⋮----
.join()
.expect("join accounts read cache evictor thread");
⋮----
impl ReadOnlyAccountCacheEntry {
fn new(account: AccountSharedData, slot: Slot, timestamp: u64) -> Self {
⋮----
mod tests {
⋮----
pub fn reset_for_tests(&self) {
self.cache.clear();
self.data_size.store(0, Ordering::Relaxed);
⋮----
fn test_accountsdb_sizeof() {
assert!(std::mem::size_of::<Arc<u64>>() == std::mem::size_of::<Arc<u8>>());
assert!(std::mem::size_of::<Arc<u64>>() == std::mem::size_of::<Arc<[u8; 32]>>());
⋮----
fn test_read_only_accounts_cache_random(evict_sample_size: usize) {
⋮----
let slots: Vec<Slot> = repeat_with(|| rng.random_range(0..1000)).take(5).collect();
let pubkeys: Vec<Pubkey> = repeat_with(|| {
⋮----
rng.fill(&mut arr[..]);
⋮----
.take(35)
.collect();
⋮----
if rng.random_bool(0.1) {
let element = cache.cache.iter().choose(&mut rng).unwrap();
let (pubkey, entry) = element.pair();
⋮----
let account = cache.load(*pubkey, slot).unwrap();
let (other, other_slot, index) = hash_map.get_mut(pubkey).unwrap();
assert_eq!(account, *other);
assert_eq!(slot, *other_slot);
⋮----
let mut data = vec![0u8; DATA_SIZE];
rng.fill(&mut data[..]);
⋮----
lamports: rng.random(),
⋮----
executable: rng.random(),
rent_epoch: rng.random(),
⋮----
let slot = *slots.choose(&mut rng).unwrap();
let pubkey = *pubkeys.choose(&mut rng).unwrap();
hash_map.insert(pubkey, (account.clone(), slot, ix));
cache.store(pubkey, slot, account);
cache.evict_in_foreground(evict_sample_size, &mut rng, |_, _| {});
⋮----
assert_eq!(cache.cache_len(), 17);
assert_eq!(hash_map.len(), 35);
for entry in cache.cache.iter() {
let pubkey = entry.key();
let ReadOnlyAccountCacheEntry { account, slot, .. } = entry.value();
⋮----
.get(pubkey)
.expect("account to be present in the map");
assert_eq!(account, local_account);
assert_eq!(slot, local_slot);
⋮----
fn test_evict_in_background(evict_sample_size: usize) {
⋮----
cache.store(pubkey, i as Slot, account);
⋮----
assert_eq!(cache.cache_len(), MAX_ENTRIES);
assert_eq!(cache.data_size(), MAX_CACHE_SIZE);
assert_eq!(cache.stats.evicts.load(Ordering::Relaxed), 0);
⋮----
while cache.stats.evicts.load(Ordering::Relaxed) == 0 {
assert!(

================
File: accounts-db/src/rolling_bit_field.rs
================
mod iterators;
⋮----
pub struct RollingBitField {
⋮----
fn eq(&self, other: &Self) -> bool {
self.len() == other.len() && {
for item in self.get_all() {
if !other.contains(&item) {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
⋮----
bits.push_str(&format!("{prev}"));
⋮----
while index < self.bits.len() {
⋮----
bits.push_str(&format!(";{index}"));
⋮----
if index < self.bits.len() {
bits.push_str(&format!(", {prev}"));
⋮----
bits.push_str(&format!(";{count}"));
⋮----
bits.push(']');
// The order of the `count` and `bits` fields is changed on
// purpose so that possibly very long output of `bits` doesn't
f.debug_struct("RollingBitField")
.field("max_width", &self.max_width)
.field("min", &self.min)
.field("max_exclusive", &self.max_exclusive)
.field("count", &self.count)
.field("bits", &bits)
.field("excess", &self.excess)
.finish()
⋮----
impl RollingBitField {
pub fn new(max_width: u64) -> Self {
assert!(max_width > 0);
assert!(max_width.is_power_of_two());
⋮----
fn get_address(&self, key: &u64) -> u64 {
⋮----
pub fn range_width(&self) -> u64 {
⋮----
pub fn min(&self) -> Option<u64> {
if self.is_empty() {
⋮----
} else if self.excess.is_empty() {
Some(self.min)
⋮----
let excess_min = self.excess.iter().min().copied();
if self.all_items_in_excess() {
⋮----
Some(std::cmp::min(self.min, excess_min.unwrap_or(u64::MAX)))
⋮----
pub fn insert(&mut self, key: u64) {
let mut bits_empty = self.count == 0 || self.all_items_in_excess();
⋮----
if self.excess.insert(key) {
⋮----
let new_width = new_max.saturating_sub(self.min);
⋮----
let inserted = self.excess.insert(self.min);
assert!(inserted);
⋮----
let address = self.get_address(&key);
self.bits.set(address, false);
self.purge(&key);
⋮----
let value = self.bits.get(address);
⋮----
self.bits.set(address, true);
⋮----
assert!(
⋮----
pub fn remove(&mut self, key: &u64) -> bool {
⋮----
let address = self.get_address(key);
let get = self.bits.get(address);
⋮----
self.purge(key);
⋮----
let remove = self.excess.remove(key);
⋮----
fn all_items_in_excess(&self) -> bool {
self.excess.len() == self.count
⋮----
fn purge(&mut self, key: &u64) {
if self.count > 0 && !self.all_items_in_excess() {
⋮----
if self.contains_assume_in_range(&key) {
⋮----
fn contains_assume_in_range(&self, key: &u64) -> bool {
⋮----
self.bits.get(address)
⋮----
pub fn contains(&self, key: &u64) -> bool {
⋮----
self.contains_assume_in_range(key)
⋮----
self.excess.contains(key)
⋮----
pub fn len(&self) -> usize {
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub fn max_exclusive(&self) -> u64 {
⋮----
pub fn max_inclusive(&self) -> u64 {
self.max_exclusive.saturating_sub(1)
⋮----
pub fn get_all_less_than(&self, max_slot_exclusive: Slot) -> Vec<u64> {
⋮----
self.excess.iter().for_each(|slot| {
⋮----
all.push(*slot)
⋮----
all.push(key);
⋮----
pub fn get_prior(&self, max_slot_exclusive: Slot) -> Option<Slot> {
let mut slot = max_slot_exclusive.saturating_sub(1);
self.min().and_then(|min| {
⋮----
if self.contains(&slot) {
return Some(slot);
⋮----
slot = slot.saturating_sub(1);
⋮----
pub fn get_all(&self) -> Vec<u64> {
⋮----
self.excess.iter().for_each(|slot| all.push(*slot));
⋮----
pub fn iter_ones(&self) -> RollingBitFieldOnesIter<'_> {
⋮----
pub mod tests {
⋮----
pub fn clear(&mut self) {
⋮----
fn test_get_all_less_than() {
⋮----
assert!(bitfield.get_all_less_than(0).is_empty());
bitfield.insert(0);
⋮----
assert_eq!(bitfield.get_all_less_than(1), vec![0]);
bitfield.insert(1);
⋮----
bitfield.insert(last_item_not_in_excess);
assert!(bitfield.excess.is_empty());
assert_eq!(
⋮----
bitfield.insert(first_item_in_excess);
assert!(bitfield.excess.contains(&0));
⋮----
bitfield.insert(len * 2);
let mut less = bitfield.get_all_less_than(len * 2);
less.sort_unstable();
⋮----
let mut less = bitfield.get_all_less_than(len * 2 + 1);
⋮----
fn test_bitfield_delete_non_excess() {
⋮----
assert_eq!(bitfield.min(), None);
⋮----
assert_eq!(bitfield.min(), Some(0));
⋮----
bitfield.insert(too_big);
assert!(bitfield.contains(&0));
assert!(bitfield.contains(&too_big));
assert_eq!(bitfield.len(), 2);
assert_eq!(bitfield.excess.len(), 1);
assert_eq!(bitfield.min, too_big);
⋮----
assert_eq!(bitfield.max_exclusive, too_big + 1);
// delete the thing that is NOT in excess
bitfield.remove(&too_big);
assert_eq!(bitfield.min, too_big + 1);
⋮----
bitfield.insert(too_big_times_2);
⋮----
assert!(bitfield.contains(&too_big_times_2));
⋮----
assert_eq!(bitfield.min(), bitfield.excess.iter().min().copied());
assert_eq!(bitfield.min, too_big_times_2);
assert_eq!(bitfield.max_exclusive, too_big_times_2 + 1);
bitfield.remove(&0);
bitfield.remove(&too_big_times_2);
assert!(bitfield.is_empty());
⋮----
bitfield.insert(other);
assert!(bitfield.contains(&other));
⋮----
assert_eq!(bitfield.min, other);
assert_eq!(bitfield.max_exclusive, other + 1);
⋮----
fn test_bitfield_insert_excess() {
⋮----
// delete the thing that IS in excess
// this does NOT affect min/max
⋮----
// re-add to excess
⋮----
fn test_bitfield_permutations() {
⋮----
while hash.len() < width {
⋮----
hash.insert(slot);
bitfield.insert(slot);
⋮----
compare(&hash, &bitfield);
⋮----
if hash.contains(&slot) {
⋮----
time.stop();
⋮----
if bitfield.contains(&slot) {
⋮----
time2.stop();
info!(
⋮----
assert_eq!(count, count2);
⋮----
fn test_bitfield_power_2() {
⋮----
fn test_bitfield_0() {
⋮----
fn setup_empty(width: u64) -> RollingBitFieldTester {
⋮----
struct RollingBitFieldTester {
⋮----
impl RollingBitFieldTester {
fn insert(&mut self, slot: u64) {
self.bitfield.insert(slot);
self.hash_set.insert(slot);
assert!(self.bitfield.contains(&slot));
compare(&self.hash_set, &self.bitfield);
⋮----
fn remove(&mut self, slot: &u64) -> bool {
let result = self.bitfield.remove(slot);
assert_eq!(result, self.hash_set.remove(slot));
assert!(!self.bitfield.contains(slot));
self.compare();
⋮----
fn compare(&self) {
⋮----
fn setup_wide(width: u64, start: u64) -> RollingBitFieldTester {
let mut tester = setup_empty(width);
tester.compare();
tester.insert(start);
tester.insert(start + 1);
⋮----
fn test_bitfield_insert_wide() {
⋮----
let mut tester = setup_wide(width, start);
⋮----
let all = tester.bitfield.get_all();
// higher than max range by 1
tester.insert(slot);
⋮----
assert!(bitfield.contains(&slot));
⋮----
assert_eq!(bitfield.count, 3);
⋮----
fn test_bitfield_insert_wide_before() {
⋮----
let mut bitfield = setup_wide(width, start).bitfield;
⋮----
// assert here - would make min too low, causing too wide of a range
⋮----
assert_eq!(1, bitfield.excess.len());
assert_eq!(3, bitfield.count);
⋮----
fn test_bitfield_insert_wide_before_ok() {
⋮----
let slot = start + 2 - width; // this item would make our width exactly equal to what is allowed, but it is also inserting prior to min
⋮----
fn test_bitfield_contains_wide_no_assert() {
⋮----
let bitfield = setup_wide(width, start).bitfield;
⋮----
assert!(!bitfield.contains(&slot));
⋮----
// too large
⋮----
// too small, before min
⋮----
fn test_bitfield_remove_wide() {
⋮----
assert!(!tester.remove(&slot));
⋮----
fn test_bitfield_excess2() {
⋮----
// insert 1st slot
⋮----
assert!(tester.bitfield.excess.is_empty());
// insert a slot before the previous one. this is 'excess' since we don't use this pattern in normal operation
⋮----
tester.insert(slot2);
assert_eq!(tester.bitfield.excess.len(), 1);
tester.remove(&slot);
assert!(tester.bitfield.contains(&slot2));
⋮----
tester.remove(&slot2);
assert!(tester.bitfield.contains(&slot));
⋮----
fn test_bitfield_excess() {
⋮----
for width in [16, 4194304].iter() {
⋮----
for start in [0, width * 5].iter().cloned() {
for recreate in [false, true].iter().cloned() {
⋮----
for reverse_slots in [false, true].iter().cloned() {
⋮----
let recreated = setup_empty(width);
⋮----
let slot_use = maybe_reverse(slot);
tester.insert(slot_use);
⋮----
assert!(!tester.bitfield.contains(&(start - width * 2)));
⋮----
assert!(!tester.bitfield.contains(&(start + width * 2)));
⋮----
assert_eq!(tester.bitfield.len(), len);
assert_eq!(tester.bitfield.count, len);
⋮----
assert!(tester.remove(&slot_use));
⋮----
assert!(tester.bitfield.is_empty());
assert_eq!(tester.bitfield.count, 0);
⋮----
fn test_bitfield_remove_wide_before() {
⋮----
fn compare_internal(hashset: &HashSet<u64>, bitfield: &RollingBitField) {
assert_eq!(hashset.len(), bitfield.len());
assert_eq!(hashset.is_empty(), bitfield.is_empty());
if !bitfield.is_empty() {
⋮----
for item in bitfield.get_all() {
assert!(hashset.contains(&item));
if !bitfield.excess.contains(&item) {
⋮----
assert_eq!(bitfield.min(), Some(overall_min));
assert_eq!(bitfield.get_all().len(), hashset.len());
if bitfield.excess.len() != bitfield.len() {
let width = if bitfield.is_empty() {
⋮----
fn compare(hashset: &HashSet<u64>, bitfield: &RollingBitField) {
compare_internal(hashset, bitfield);
let clone = bitfield.clone();
compare_internal(hashset, &clone);
assert!(clone.eq(bitfield));
assert_eq!(clone, *bitfield);
⋮----
fn test_bitfield_functionality() {
⋮----
let max_bitfield_width = 2u64.pow(power);
⋮----
let mut tester = setup_empty(max_bitfield_width);
⋮----
while tester.hash_set.len() < width {
⋮----
assert!(tester.remove(&slot));
⋮----
for item in all.iter() {
assert!(tester.remove(item));
assert!(!tester.remove(item));
⋮----
fn bitfield_insert_and_test(bitfield: &mut RollingBitField, slot: Slot) {
let len = bitfield.len();
let old_all = bitfield.get_all();
let (new_min, new_max) = if bitfield.is_empty() {
⋮----
assert_eq!(bitfield.min, new_min);
assert_eq!(bitfield.max_exclusive, new_max);
assert_eq!(bitfield.len(), len + 1);
assert!(!bitfield.is_empty());
⋮----
assert!(bitfield.contains_assume_in_range(&(slot + bitfield.max_width)));
let get_all = bitfield.get_all();
⋮----
.into_iter()
.for_each(|slot| assert!(get_all.contains(&slot)));
assert!(get_all.contains(&slot));
assert!(get_all.len() == len + 1);
⋮----
fn test_bitfield_clear() {
⋮----
assert_eq!(bitfield.len(), 0);
⋮----
bitfield_insert_and_test(&mut bitfield, 0);
bitfield.clear();
⋮----
assert!(bitfield.get_all().is_empty());
bitfield_insert_and_test(&mut bitfield, 1);
⋮----
bitfield_insert_and_test(&mut bitfield, 4);
⋮----
fn test_bitfield_wrapping() {
⋮----
assert_eq!(bitfield.get_all(), vec![0]);
bitfield_insert_and_test(&mut bitfield, 2);
assert_eq!(bitfield.get_all(), vec![0, 2]);
bitfield_insert_and_test(&mut bitfield, 3);
bitfield.insert(3);
assert_eq!(bitfield.get_all(), vec![0, 2, 3]);
assert!(bitfield.remove(&0));
assert!(!bitfield.remove(&0));
assert_eq!(bitfield.min, 2);
assert_eq!(bitfield.max_exclusive, 4);
⋮----
assert_eq!(bitfield.get_all(), vec![2, 3]);
bitfield.insert(4);
⋮----
assert_eq!(bitfield.max_exclusive, 5);
assert_eq!(bitfield.len(), 3);
assert_eq!(bitfield.get_all(), vec![2, 3, 4]);
assert!(bitfield.remove(&2));
assert_eq!(bitfield.min, 3);
⋮----
assert_eq!(bitfield.get_all(), vec![3, 4]);
assert!(bitfield.remove(&3));
assert_eq!(bitfield.min, 4);
⋮----
assert_eq!(bitfield.len(), 1);
assert_eq!(bitfield.get_all(), vec![4]);
assert!(bitfield.remove(&4));
⋮----
bitfield_insert_and_test(&mut bitfield, 8);
assert!(bitfield.remove(&8));
⋮----
bitfield_insert_and_test(&mut bitfield, 9);
assert!(bitfield.remove(&9));
⋮----
fn test_bitfield_smaller() {
⋮----
while hash_set.len() < width {
⋮----
hash_set.insert(slot);
⋮----
if hash_set.contains(&slot) {
⋮----
fn test_debug_formatter() {
⋮----
bitfield.insert(4095);
⋮----
bitfield.insert(2);

================
File: accounts-db/src/sorted_storages.rs
================
pub struct SortedStorages<'a> {
/// range of slots where storages exist (likely sparse)
    range: Range<Slot>,
/// the actual storages
    /// A HashMap allows sparse storage and fast lookup of Slot -> Storage.
⋮----
/// A HashMap allows sparse storage and fast lookup of Slot -> Storage.
    /// We expect ~432k slots.
⋮----
/// We expect ~432k slots.
    storages: HashMap<Slot, &'a Arc<AccountStorageEntry>>,
⋮----
pub fn empty() -> Self {
⋮----
pub fn iter_range<R>(&'a self, range: &R) -> SortedStoragesIter<'a>
⋮----
fn get(&self, slot: Slot) -> Option<&Arc<AccountStorageEntry>> {
self.storages.get(&slot).copied()
⋮----
pub fn range_width(&self) -> Slot {
⋮----
pub fn range(&self) -> &Range<Slot> {
⋮----
pub fn max_slot_inclusive(&self) -> Slot {
self.range.end.saturating_sub(1)
⋮----
pub fn slot_count(&self) -> usize {
self.storages.len()
⋮----
pub fn storage_count(&self) -> usize {
⋮----
pub fn new(source: &'a [Arc<AccountStorageEntry>]) -> Self {
let slots = source.iter().map(|storage| {
storage.slot() // this must be unique. Will be enforced in new_with_slots
⋮----
Self::new_with_slots(source.iter().zip(slots), None, None)
⋮----
/// create [`SortedStorages`] from `source` iterator.
    /// `source` contains a [`Arc<AccountStorageEntry>`] and its associated slot
⋮----
/// `source` contains a [`Arc<AccountStorageEntry>`] and its associated slot
    /// `source` does not have to be sorted in any way, but is assumed to not have duplicate slot #s
⋮----
/// `source` does not have to be sorted in any way, but is assumed to not have duplicate slot #s
    pub fn new_with_slots(
⋮----
pub fn new_with_slots(
⋮----
adjust_min_max(slot);
⋮----
let source_ = source.clone();
⋮----
source_.for_each(|(_, slot)| {
⋮----
time.stop();
⋮----
source.for_each(|(original_storages, slot)| {
assert!(
⋮----
time2.stop();
debug!("SortedStorages, times: {}, {}", time.as_us(), time2.as_us());
⋮----
pub struct SortedStoragesIter<'a> {
/// range for the iterator to iterate over (start_inclusive..end_exclusive)
    range: Range<Slot>,
/// the data to return per slot
    storages: &'a SortedStorages<'a>,
/// the slot to be returned the next time 'Next' is called
    next_slot: Slot,
⋮----
impl<'a> Iterator for SortedStoragesIter<'a> {
type Item = (Slot, Option<&'a Arc<AccountStorageEntry>>);
fn next(&mut self) -> Option<Self::Item> {
⋮----
// iterator is still in range. Storage may or may not exist at this slot, but the iterator still needs to return the slot
⋮----
Some((slot, self.storages.get(slot)))
⋮----
// iterator passed the end of the range, so from now on it returns None
⋮----
pub fn new<R: RangeBounds<Slot>>(
⋮----
let storage_range = storages.range();
let next_slot = match range.start_bound() {
⋮----
let end_exclusive_slot = match range.end_bound() {
⋮----
mod tests {
⋮----
pub fn new_debug(
⋮----
storages.insert(*slot, *storage);
⋮----
pub fn new_for_tests(storages: &[&'a Arc<AccountStorageEntry>], slots: &[Slot]) -> Self {
assert_eq!(storages.len(), slots.len());
⋮----
storages.iter().cloned().zip(slots.iter().cloned()),
⋮----
fn test_sorted_storages_range_iter() {
⋮----
assert!(storages.is_none());
⋮----
assert_eq!(
⋮----
let s1 = create_sample_store(1);
⋮----
let store2 = create_sample_store(2);
let store4 = create_sample_store(4);
⋮----
fn test_sorted_storages_new_with_slots() {
let store = create_sample_store(1);
⋮----
[(&store, end), (&store, start)].iter().cloned(),
Some(min),
Some(max),
⋮----
assert_eq!(storages.storages.len(), 2);
assert_eq!(storages.range, start..end + 1);
⋮----
assert_eq!(storages.range, start..max + 1);
⋮----
assert_eq!(storages.range, min..end + 1);
⋮----
assert_eq!(storages.range, min..max + 1);
⋮----
fn test_sorted_storages_duplicate_slots() {
⋮----
fn test_sorted_storages_none() {
⋮----
assert_eq!(result.range, Range::default());
assert_eq!(result.slot_count(), 0);
assert_eq!(result.storages.len(), 0);
assert!(result.get(0).is_none());
⋮----
fn test_sorted_storages_1() {
⋮----
assert_eq!(result.slot_count(), 1);
assert_eq!(result.storages.len(), 1);
assert_eq!(result.get(slot).unwrap().id(), store.id());
⋮----
fn create_sample_store(id: AccountsFileId) -> Arc<AccountStorageEntry> {
⋮----
let (_temp_dirs, paths) = crate::accounts_db::get_temp_accounts_paths(1).unwrap();
⋮----
fn test_sorted_storages_2() {
⋮----
assert_eq!(result.slot_count(), 2);
assert_eq!(result.storages.len(), 2);
⋮----
assert!(result.get(3).is_none());
assert!(result.get(5).is_none());
assert!(result.get(6).is_none());
assert!(result.get(8).is_none());
assert_eq!(result.get(slots[0]).unwrap().id(), store.id());
assert_eq!(result.get(slots[1]).unwrap().id(), store2.id());

================
File: accounts-db/src/stake_rewards.rs
================
pub struct StakeReward {
⋮----
impl StakeReward {
pub fn get_stake_reward(&self) -> i64 {
⋮----
impl IsZeroLamport for StakeReward {
fn is_zero_lamport(&self) -> bool {
self.stake_account.lamports() == 0
⋮----
fn account<Ret>(
⋮----
callback((&self.1[index].stake_pubkey, &entry.stake_account).into())
⋮----
fn is_zero_lamport(&self, index: usize) -> bool {
self.1[index].is_zero_lamport()
⋮----
fn data_len(&self, index: usize) -> usize {
self.1[index].stake_account.data().len()
⋮----
fn pubkey(&self, index: usize) -> &Pubkey {
⋮----
fn slot(&self, _index: usize) -> Slot {
self.target_slot()
⋮----
fn target_slot(&self) -> Slot {
⋮----
fn len(&self) -> usize {
self.1.len()
⋮----
pub fn new_random() -> Self {
⋮----
&validator_voting_keypair.pubkey(),
⋮----
let reward_lamports: i64 = rng.random_range(1..200);
let validator_stake_account = create_stake_account(
&validator_staking_keypair.pubkey(),
⋮----
commission: Some(0),
⋮----
pub fn credit(&mut self, amount: u64) {
⋮----
self.stake_account.checked_add_lamports(amount).unwrap();
⋮----
fn create_stake_account(
⋮----
vote_state::VoteStateV4::deserialize(vote_account.data(), voter_pubkey).unwrap();
let credits_observed = vote_state.credits();
let rent_exempt_reserve = rent.minimum_balance(stake_account.data().len());
⋮----
.checked_sub(rent_exempt_reserve)
.expect("lamports >= rent_exempt_reserve");
⋮----
.set_state(&StakeStateV2::Stake(meta, stake, StakeFlags::empty()))
.expect("set_state");

================
File: accounts-db/src/storable_accounts.rs
================
pub enum AccountForStorage<'a> {
⋮----
fn from(source: (&'a Pubkey, &'a AccountSharedData)) -> Self {
⋮----
fn from(source: &'a StoredAccountInfo<'a>) -> Self {
⋮----
impl IsZeroLamport for AccountForStorage<'_> {
fn is_zero_lamport(&self) -> bool {
self.lamports() == 0
⋮----
pub fn pubkey(&self) -> &'a Pubkey {
⋮----
AccountForStorage::StoredAccountInfo(account) => account.pubkey(),
⋮----
pub fn take_account(&self) -> AccountSharedData {
⋮----
AccountForStorage::AddressAndAccount((_pubkey, account)) => (*account).clone(),
AccountForStorage::StoredAccountInfo(account) => create_account_shared_data(*account),
⋮----
impl ReadableAccount for AccountForStorage<'_> {
fn lamports(&self) -> u64 {
⋮----
AccountForStorage::AddressAndAccount((_pubkey, account)) => account.lamports(),
AccountForStorage::StoredAccountInfo(account) => account.lamports(),
⋮----
fn data(&self) -> &[u8] {
⋮----
AccountForStorage::AddressAndAccount((_pubkey, account)) => account.data(),
AccountForStorage::StoredAccountInfo(account) => account.data(),
⋮----
fn owner(&self) -> &Pubkey {
⋮----
AccountForStorage::AddressAndAccount((_pubkey, account)) => account.owner(),
AccountForStorage::StoredAccountInfo(account) => account.owner(),
⋮----
fn executable(&self) -> bool {
⋮----
AccountForStorage::AddressAndAccount((_pubkey, account)) => account.executable(),
AccountForStorage::StoredAccountInfo(account) => account.executable(),
⋮----
fn rent_epoch(&self) -> Epoch {
⋮----
AccountForStorage::AddressAndAccount((_pubkey, account)) => account.rent_epoch(),
AccountForStorage::StoredAccountInfo(account) => account.rent_epoch(),
⋮----
fn to_account_shared_data(&self) -> AccountSharedData {
self.take_account()
⋮----
pub struct StorableAccountsCacher {
⋮----
pub trait StorableAccounts<'a>: Sync {
/// account at 'index'
    fn account<Ret>(
⋮----
fn account_default_if_zero_lamport<Ret>(
⋮----
if self.is_zero_lamport(index) {
callback(AccountForStorage::AddressAndAccount((
self.pubkey(index),
⋮----
self.account(index, callback)
⋮----
fn is_empty(&self) -> bool {
self.len() == 0
⋮----
fn contains_multiple_slots(&self) -> bool {
⋮----
fn account<Ret>(
⋮----
callback((self.1[index].0, self.1[index].1).into())
⋮----
fn is_zero_lamport(&self, index: usize) -> bool {
self.1[index].1.is_zero_lamport()
⋮----
fn data_len(&self, index: usize) -> usize {
self.1[index].1.data().len()
⋮----
fn pubkey(&self, index: usize) -> &Pubkey {
⋮----
fn slot(&self, _index: usize) -> Slot {
// per-index slot is not unique per slot when per-account slot is not included in the source data
self.target_slot()
⋮----
fn target_slot(&self) -> Slot {
⋮----
fn len(&self) -> usize {
self.1.len()
⋮----
callback((&self.1[index].0, &self.1[index].1).into())
⋮----
pub struct StorableAccountsBySlot<'a> {
⋮----
/// each element is (source slot, accounts moving FROM source slot)
    slots_and_accounts: &'a [(Slot, &'a [&'a AccountFromStorage])],
⋮----
/// remember the last storage we looked up for a given slot
    cached_storage: RwLock<StorableAccountsCacher>,
⋮----
/// each element of slots_and_accounts is (source slot, accounts moving FROM source slot)
    pub fn new(
⋮----
pub fn new(
⋮----
let mut starting_offsets = Vec::with_capacity(slots_and_accounts.len());
⋮----
.first()
.map(|(slot, _)| *slot)
.unwrap_or_default();
⋮----
cumulative_len = cumulative_len.saturating_add(accounts.len());
starting_offsets.push(cumulative_len);
⋮----
/// given an overall index for all accounts in self: return
    /// (slots_and_accounts index, index within those accounts)
⋮----
/// (slots_and_accounts index, index within those accounts)
    /// This implementation is optimized for performance by using binary search
⋮----
/// This implementation is optimized for performance by using binary search
    /// on the starting_offsets based on the assumption that the
⋮----
/// on the starting_offsets based on the assumption that the
    /// starting_offsets are always sorted.
⋮----
/// starting_offsets are always sorted.
    fn find_internal_index(&self, index: usize) -> (usize, usize) {
⋮----
fn find_internal_index(&self, index: usize) -> (usize, usize) {
// special case for when there is only one slot - just return the first index without searching.
// This happens when we are just shrinking a single slot storage, which happens very often.
⋮----
.binary_search_by(|offset| match offset.cmp(&index) {
⋮----
Ok(offset_index) => unreachable!("we shouldn't reach here: {}", offset_index),
⋮----
let indexes = self.find_internal_index(index);
⋮----
let offset = data.index_info.offset();
⋮----
.get_stored_account_callback(offset, |account| callback((&account).into()))
.expect("account has to exist to be able to store it")
⋮----
let reader = self.cached_storage.read().unwrap();
⋮----
if let Some(storage) = reader.storage.as_ref() {
return call_callback(storage);
⋮----
.get_slot_storage_entry_shrinking_in_progress_ok(slot)
.expect("source slot has to have a storage to be able to store accounts");
let ret = call_callback(&storage);
let mut writer = self.cached_storage.write().unwrap();
⋮----
writer.storage = Some(storage);
⋮----
self.slots_and_accounts[indexes.0].1[indexes.1].is_zero_lamport()
⋮----
self.slots_and_accounts[indexes.0].1[indexes.1].data_len()
⋮----
self.slots_and_accounts[indexes.0].1[indexes.1].pubkey()
⋮----
fn slot(&self, index: usize) -> Slot {
⋮----
pub mod tests {
⋮----
/// given an overall index for all accounts in self:
        /// return (slots_and_accounts index, index within those accounts)
⋮----
/// return (slots_and_accounts index, index within those accounts)
        /// This is the baseline unoptimized implementation. It is not used in the validator. It
⋮----
/// This is the baseline unoptimized implementation. It is not used in the validator. It
        /// is used for testing an optimized version - `find_internal_index`, in the actual implementation.
⋮----
/// is used for testing an optimized version - `find_internal_index`, in the actual implementation.
        fn find_internal_index_loop(&self, index: usize) -> (usize, usize) {
⋮----
fn find_internal_index_loop(&self, index: usize) -> (usize, usize) {
// search offsets for the accounts slice that contains 'index'.
⋮----
.iter()
.enumerate()
⋮----
[offset_index.saturating_sub(1)]
⋮----
panic!("failed");
⋮----
callback(account_for_storage)
⋮----
self.1[index].is_zero_lamport()
⋮----
self.1[index].data.len()
⋮----
self.1[index].pubkey()
⋮----
/// this is no longer used. It is very tricky to get these right. There are already tests for this. It is likely worth it to leave this here for a while until everything has settled.
    impl<'a, T: ReadableAccount + Sync> StorableAccounts<'a> for (Slot, &'a [&'a (Pubkey, T)])
⋮----
self.1[index].1.lamports() == 0
⋮----
// same other slot for all accounts
⋮----
fn compare<'a>(a: &impl StorableAccounts<'a>, b: &impl StorableAccounts<'a>) {
assert_eq!(a.target_slot(), b.target_slot());
assert_eq!(a.len(), b.len());
assert_eq!(a.is_empty(), b.is_empty());
(0..a.len()).for_each(|i| {
b.account(i, |account| {
a.account(i, |account_a| {
assert_eq!(account_a.pubkey(), account.pubkey());
assert!(accounts_equal(&account_a, &account));
⋮----
fn test_contains_multiple_slots() {
⋮----
assert!(!test3.contains_multiple_slots());
⋮----
fn test_storable_accounts() {
⋮----
raw.push((pk, account.clone(), starting_slot % max_slots));
⋮----
raw2.push(StoredAccountInfo {
⋮----
lamports: raw.1.lamports(),
owner: raw.1.owner(),
data: raw.1.data(),
executable: raw.1.executable(),
rent_epoch: raw.1.rent_epoch(),
⋮----
raw4.push((raw.0, raw.1.clone()));
⋮----
.map(|account| {
⋮----
account.is_zero_lamport(),
⋮----
data_len: account.data.len() as u64,
⋮----
.collect();
⋮----
raw.iter()
.zip(
raw2.iter()
.zip(raw4.iter().zip(raw2_accounts_from_storage.iter())),
⋮----
.for_each(|(raw, (raw2, (raw4, raw2_accounts_from_storage)))| {
two.push((&raw.0, &raw.1));
three.push(raw2);
three_accounts_from_storage_byval.push(*raw2_accounts_from_storage);
four_pubkey_and_account_value.push(raw4);
⋮----
let storage = setup_sample_storage(&db, source_slot);
⋮----
.write_accounts(&(source_slot, &three[..]), 0)
⋮----
.iter_mut()
.zip(offsets.offsets.iter())
.for_each(|(account, offset)| {
⋮----
three_accounts_from_storage_byval.iter().collect::<Vec<_>>();
let accounts_with_slots = vec![(source_slot, &three_accounts_from_storage[..])];
⋮----
compare(&test2, &test3);
compare(&test2, &test4);
compare(&test2, &test_moving_slots2);
for (i, raw) in raw.iter().enumerate() {
test3.account(i, |account| {
assert_eq!(raw.0, *account.pubkey());
assert!(accounts_equal(&raw.1, &account));
⋮----
assert_eq!(raw.2, test3.slot(i));
assert_eq!(target_slot, test4.slot(i));
assert_eq!(target_slot, test2.slot(i));
assert_eq!(old_slot, test_moving_slots2.slot(i));
⋮----
assert_eq!(target_slot, test3.target_slot());
assert_eq!(target_slot, test4.target_slot());
assert_eq!(target_slot, test_moving_slots2.target_slot());
assert!(!test2.contains_multiple_slots());
assert!(!test4.contains_multiple_slots());
assert_eq!(test3.contains_multiple_slots(), entries > 1);
⋮----
fn setup_sample_storage(db: &AccountsDb, slot: Slot) -> Arc<AccountStorageEntry> {
⋮----
let (_temp_dirs, paths) = get_temp_accounts_paths(1).unwrap();
⋮----
db.storage_access(),
⋮----
db.storage.insert(slot, storage.clone());
⋮----
fn test_storable_accounts_by_slot() {
⋮----
raw.push((pk, account.clone()));
⋮----
let raw2_refs = raw2.iter().collect::<Vec<_>>();
⋮----
let remaining1 = entries.saturating_sub(entries0);
⋮----
let remaining2 = entries.saturating_sub(entries0 + entries1);
⋮----
let remaining3 = entries.saturating_sub(entries0 + entries1 + entries2);
⋮----
.filter_map(|(slot, count)| {
⋮----
(overall_index < raw2.len()).then(|| {
⋮----
raw2_accounts_from_storage[range.clone()].to_vec();
let storage = setup_sample_storage(&db, slot);
⋮----
.write_accounts(&(slot, &raw2_refs[range.clone()]), 0)
⋮----
result.iter_mut().zip(offsets.offsets.iter()).for_each(
⋮----
range.for_each(|_| expected_slots.push(slot));
⋮----
.map(|(slot, accounts)| (*slot, accounts.iter().collect::<Vec<_>>()))
⋮----
.map(|(slot, accounts)| (*slot, &accounts[..]))
⋮----
assert_eq!(99, storable.target_slot());
assert_eq!(entries0 != entries, storable.contains_multiple_slots());
(0..entries).for_each(|index| {
⋮----
storable.account(index, |account| {
⋮----
assert!(accounts_equal(&account, &raw2[index]));
assert_eq!(account.pubkey(), raw2[index].pubkey());
⋮----
assert!(called);
assert_eq!(storable.slot(index), expected_slots[index]);
⋮----
fn test_find_internal_index() {
⋮----
data_len: account.data().len() as u64,
⋮----
let n = rand::rng().random_range(1..10);
⋮----
let accounts = (0..n).map(|_| &account_from_storage).collect::<Vec<_>>();
all_accounts.push(accounts);
⋮----
slot_accounts.push((slot, &all_accounts[slot as usize][..]));
⋮----
let (slot_index, account_index) = storable_accounts.find_internal_index_loop(i);
let (slot_index2, account_index2) = storable_accounts.find_internal_index(i);
assert_eq!(slot_index, slot_index2);
assert_eq!(account_index, account_index2);

================
File: accounts-db/src/tiered_storage.rs
================
pub mod byte_block;
pub mod error;
pub mod file;
pub mod footer;
pub mod hot;
pub mod index;
pub mod meta;
pub mod mmap_utils;
pub mod owners;
pub mod readable;
mod test_utils;
⋮----
pub type TieredStorageResult<T> = Result<T, TieredStorageError>;
⋮----
pub struct TieredStorageFormat {
⋮----
pub struct TieredStorage {
⋮----
impl Drop for TieredStorage {
fn drop(&mut self) {
⋮----
if err.kind() != io::ErrorKind::NotFound {
panic!(
⋮----
impl TieredStorage {
pub fn new_writable(path: impl Into<PathBuf>) -> Self {
⋮----
already_written: false.into(),
path: path.into(),
⋮----
pub fn new_readonly(path: impl Into<PathBuf>) -> TieredStorageResult<Self> {
let path = path.into();
Ok(Self {
reader: TieredStorageReader::new_from_path(&path).map(OnceLock::from)?,
already_written: true.into(),
⋮----
pub fn path(&self) -> &Path {
self.path.as_path()
⋮----
pub fn write_accounts<'a>(
⋮----
let was_written = self.already_written.swap(true, Ordering::AcqRel);
⋮----
panic!("cannot write same tiered storage file more than once");
⋮----
let stored_accounts_info = writer.write_accounts(accounts, skip)?;
writer.flush()?;
⋮----
debug_assert!(!self.is_read_only());
⋮----
.set(TieredStorageReader::new_from_path(&self.path)?)
.unwrap();
Ok(stored_accounts_info)
⋮----
Err(TieredStorageError::UnknownFormat(self.path.to_path_buf()))
⋮----
pub fn reader(&self) -> Option<&TieredStorageReader> {
self.reader.get()
⋮----
pub fn is_read_only(&self) -> bool {
self.reader.get().is_some()
⋮----
pub fn len(&self) -> usize {
self.reader().map_or(0, |reader| reader.len())
⋮----
pub fn is_empty(&self) -> bool {
self.len() == 0
⋮----
pub fn capacity(&self) -> u64 {
self.reader()
.map_or(MAX_TIERED_STORAGE_FILE_SIZE, |reader| reader.capacity())
⋮----
pub fn dead_bytes_due_to_zero_lamport_single_ref(&self, count: usize) -> usize {
⋮----
mod tests {
⋮----
fn footer(&self) -> Option<&TieredStorageFooter> {
self.reader.get().map(|r| r.footer())
⋮----
fn write_zero_accounts(
⋮----
let storable_accounts = (slot_ignored, account_refs.as_slice());
let result = tiered_storage.write_accounts(&storable_accounts, 0, &HOT_FORMAT);
⋮----
panic!("actual: {result:?}, expected: {expected_result:?}");
⋮----
assert!(tiered_storage.is_read_only());
assert_eq!(
⋮----
fn test_new_meta_file_only() {
let temp_dir = tempdir().unwrap();
let tiered_storage_path = temp_dir.path().join("test_new_meta_file_only");
⋮----
assert!(!tiered_storage.is_read_only());
assert_eq!(tiered_storage.path(), tiered_storage_path);
assert_eq!(tiered_storage.len(), 0);
write_zero_accounts(
⋮----
Ok(StoredAccountsInfo {
offsets: vec![],
⋮----
let tiered_storage_readonly = TieredStorage::new_readonly(&tiered_storage_path).unwrap();
let footer = tiered_storage_readonly.footer().unwrap();
assert!(tiered_storage_readonly.is_read_only());
assert_eq!(tiered_storage_readonly.reader().unwrap().num_accounts(), 0);
assert_eq!(footer.account_meta_format, HOT_FORMAT.account_meta_format);
assert_eq!(footer.owners_block_format, HOT_FORMAT.owners_block_format);
assert_eq!(footer.index_block_format, HOT_FORMAT.index_block_format);
assert_eq!(footer.account_block_format, HOT_FORMAT.account_block_format);
⋮----
fn test_write_accounts_twice() {
⋮----
let tiered_storage_path = temp_dir.path().join("test_write_accounts_twice");
⋮----
Err(TieredStorageError::AttemptToUpdateReadOnly(
⋮----
fn test_remove_on_drop() {
⋮----
let tiered_storage_path = temp_dir.path().join("test_remove_on_drop");
⋮----
assert!(!tiered_storage_path.try_exists().unwrap());
⋮----
assert!(tiered_storage_path.try_exists().unwrap());
⋮----
_ = ManuallyDrop::new(TieredStorage::new_readonly(&tiered_storage_path).unwrap());
⋮----
_ = TieredStorage::new_readonly(&tiered_storage_path).unwrap();
⋮----
fn do_test_write_accounts(
⋮----
.iter()
.map(|size| create_test_account(*size))
.collect();
⋮----
let tiered_storage_path = temp_dir.path().join(path_suffix);
⋮----
_ = tiered_storage.write_accounts(&storable_accounts, 0, &format);
let reader = tiered_storage.reader().unwrap();
let num_accounts = storable_accounts.len();
assert_eq!(reader.num_accounts(), num_accounts);
⋮----
storable_accounts.account_default_if_zero_lamport(i, |account| {
expected_accounts_map.insert(*account.pubkey(), account.take_account());
⋮----
let footer = reader.footer();
⋮----
.scan_accounts(|_offset, stored_account| {
if let Some(account) = expected_accounts_map.get(stored_account.pubkey()) {
verify_test_account_with_footer(
⋮----
stored_account.pubkey(),
⋮----
verified_accounts.insert(*stored_account.pubkey());
if min_pubkey > *stored_account.pubkey() {
min_pubkey = *stored_account.pubkey();
⋮----
if max_pubkey < *stored_account.pubkey() {
max_pubkey = *stored_account.pubkey();
⋮----
assert_eq!(footer.min_account_address, min_pubkey);
assert_eq!(footer.max_account_address, max_pubkey);
assert!(!verified_accounts.is_empty());
assert_eq!(verified_accounts.len(), expected_accounts_map.len());
⋮----
fn test_write_accounts_small_accounts() {
do_test_write_accounts(
⋮----
HOT_FORMAT.clone(),
⋮----
fn test_write_accounts_one_max_len() {
⋮----
fn test_write_accounts_mixed_size() {

================
File: accounts-db/src/utils.rs
================
pub fn create_all_accounts_run_and_snapshot_dirs(
⋮----
let mut run_dirs = Vec::with_capacity(account_paths.len());
let mut snapshot_dirs = Vec::with_capacity(account_paths.len());
⋮----
let (run_dir, snapshot_dir) = create_accounts_run_and_snapshot_dirs(account_path)?;
run_dirs.push(run_dir);
snapshot_dirs.push(snapshot_dir);
⋮----
Ok((run_dirs, snapshot_dirs))
⋮----
pub fn create_accounts_run_and_snapshot_dirs(
⋮----
let run_path = account_dir.as_ref().join(ACCOUNTS_RUN_DIR);
let snapshot_path = account_dir.as_ref().join(ACCOUNTS_SNAPSHOT_DIR);
if (!run_path.is_dir()) || (!snapshot_path.is_dir()) {
if fs::remove_dir_all(&account_dir).is_err() {
⋮----
Ok((run_path, snapshot_path))
⋮----
pub fn move_and_async_delete_path_contents(path: impl AsRef<Path>) {
move_and_async_delete_path(&path);
⋮----
pub fn move_and_async_delete_path(path: impl AsRef<Path>) {
⋮----
let mut lock = IN_PROGRESS_DELETES.lock().unwrap();
if !path.as_ref().exists() {
⋮----
if lock.contains(path.as_ref()) {
⋮----
let mut path_delete = path.as_ref().to_path_buf();
path_delete.set_file_name(format!(
⋮----
warn!(
⋮----
lock.insert(path.as_ref().to_path_buf());
drop(lock);
⋮----
IN_PROGRESS_DELETES.lock().unwrap().remove(path.as_ref());
⋮----
lock.insert(path_delete.clone());
⋮----
.name("solDeletePath".to_string())
.spawn(move || {
trace!("background deleting {}...", path_delete.display());
let (result, measure_delete) = measure_time!(dirs::remove_dir_all(&path_delete));
⋮----
panic!("Failed to async delete '{}': {err}", path_delete.display());
⋮----
trace!(
⋮----
IN_PROGRESS_DELETES.lock().unwrap().remove(&path_delete);
⋮----
.expect("spawn background delete thread");
⋮----
pub fn create_and_canonicalize_directories(
⋮----
.into_iter()
.map(create_and_canonicalize_directory)
.collect()
⋮----
pub fn create_and_canonicalize_directory(directory: impl AsRef<Path>) -> io::Result<PathBuf> {
⋮----
pub fn create_account_shared_data(account: &impl ReadableAccount) -> AccountSharedData {
⋮----
account.lamports(),
account.data().to_vec(),
*account.owner(),
account.executable(),
account.rent_epoch(),
⋮----
mod tests {
⋮----
pub fn test_create_all_accounts_run_and_snapshot_dirs() {
⋮----
.map(|_| {
let tmp_dir = tempfile::TempDir::new().unwrap();
let account_path = tmp_dir.path().join("accounts");
⋮----
.unzip();
⋮----
create_all_accounts_run_and_snapshot_dirs(&account_paths).unwrap();
account_run_paths.iter().all(|path| path.is_dir());
account_snapshot_paths.iter().all(|path| path.is_dir());
let account_path_first = account_paths.first().unwrap();
⋮----
assert!(account_path_first.exists());
assert!(!account_path_first.join(ACCOUNTS_RUN_DIR).exists());
assert!(!account_path_first.join(ACCOUNTS_SNAPSHOT_DIR).exists());
_ = create_all_accounts_run_and_snapshot_dirs(&account_paths).unwrap();

================
File: accounts-db/src/waitable_condvar.rs
================
pub struct WaitableCondvar {
⋮----
impl WaitableCondvar {
pub fn notify_all(&self) {
self.event.notify_all();
⋮----
pub fn notify_one(&self) {
self.event.notify_one();
⋮----
pub fn wait_timeout(&self, timeout: Duration) -> bool {
let lock = self.mutex.lock().unwrap();
let res = self.event.wait_timeout(lock, timeout).unwrap();
if res.1.timed_out() {
⋮----
pub mod tests {
⋮----
fn test_waitable_condvar() {
⋮----
let data_ = data.clone();
⋮----
let cv_ = cv.clone();
let cv2_ = cv2.clone();
let cv2__ = cv2.clone();
⋮----
let handle = Builder::new().spawn(move || {
⋮----
while cv2_.wait_timeout(Duration::from_millis(1)) {
if !notified && data_.load(Ordering::Relaxed) {
⋮----
cv_.notify_all();
⋮----
assert!(data_.swap(false, Ordering::Relaxed));
⋮----
let handle2 = Builder::new().spawn(move || {
⋮----
assert!(!cv2__.wait_timeout(Duration::from_millis(10000)));
⋮----
assert!(cv.wait_timeout(Duration::from_millis(1)));
assert!(!data.swap(true, Ordering::Relaxed));
assert!(!cv.wait_timeout(Duration::from_millis(10000)));
cv2.notify_all();
⋮----
assert!(handle.unwrap().join().is_ok());
assert!(handle2.unwrap().join().is_ok());

================
File: accounts-db/store-histogram/src/main.rs
================
struct Bin {
⋮----
fn pad(width: usize) -> String {
" ".repeat(width)
⋮----
fn get_stars(x: usize, max: usize, width: usize) -> String {
⋮----
s = format!("{s}{}", if i <= percent { "*" } else { " " });
⋮----
fn is_parsable<T>(string: String) -> Result<(), String>
⋮----
.map(|_| ())
.map_err(|err| format!("error parsing '{string}': {err}"))
⋮----
fn calc(info: &[(usize, usize)], bin_widths: Vec<usize>, offset: i64) {
let mut info = info.to_owned();
info.sort();
let min = info.first().unwrap().0;
let max_inclusive = info.last().unwrap().0;
⋮----
eprintln!("storages: {}", info.len());
eprintln!("lowest slot: {min}");
eprintln!("highest slot: {max_inclusive}");
eprintln!("slot range: {}", max_inclusive - min + 1);
eprintln!("ancient boundary: {outside_slot}");
eprintln!(
⋮----
for i in 0..bin_widths.len() {
let next = if i == bin_widths.len() - 1 {
⋮----
bins.push(abin);
⋮----
info.into_iter().for_each(|(slot, size)| {
for bin in bins.iter_mut() {
⋮----
bin.min_size = bin.min_size.min(size);
bin.max_size = bin.max_size.max(size);
⋮----
bin_all.min_size = bin_all.min_size.min(size);
bin_all.max_size = bin_all.max_size.max(size);
⋮----
bins.retain_mut(|bin| {
⋮----
bin_max.sum_size = bin_max.sum_size.max(bin.sum_size);
bin_max.max_size = bin_max.max_size.max(bin.max_size);
bin_max.count = bin_max.count.max(bin.count);
bin_max.min_size = bin_max.min_size.max(bin.min_size);
⋮----
bin_max.avg = bin_max.avg.max(bin.avg);
⋮----
eprintln!("overall stats");
eprintln!("size {}", bin_all.sum_size);
eprintln!("count {}", bin_all.count);
eprintln!("min size {}", bin_all.min_size);
eprintln!("max size {}", bin_all.max_size);
eprintln!("avg size {}", bin_all.avg);
eprintln!("bin width {}", bins[0].slot_max - bins[0].slot_min);
for i in 0..bins.len() {
⋮----
eprintln!("...");
⋮----
eprintln!("{}", String::from_utf8(vec![b'-'; 168]).unwrap());
⋮----
let offset = format!("{:8}", bin.slot_min);
⋮----
format!("{:8}", "slot age"),
pad(1),
format!("{:10}", "count"),
⋮----
format!("{:10}", "min size"),
⋮----
format!("{:10}", "max size"),
⋮----
format!("{:10}", "sum size"),
⋮----
format!("{:10}", "avg size"),
⋮----
format!(",{:>15}", "slot min"),
format!(",{:>15}", "count"),
format!(",{:>15}", "sum size"),
format!(",{:>7}", "% size"),
format!(",{:>15}", "min size"),
format!(",{:>15}", "max size"),
format!(",{:>15}", "avg size"),
⋮----
s.iter().for_each(|s| {
s2 = format!("{s2}{s}");
⋮----
eprintln!("{s2}");
⋮----
get_stars(bin.count, bin_max.count, 10),
⋮----
get_stars(bin.min_size, bin_max.min_size, 10),
⋮----
get_stars(bin.max_size, bin_max.max_size, 10),
⋮----
get_stars(bin.sum_size, bin_max.sum_size, 10),
⋮----
get_stars(bin.avg, bin_max.avg, 10),
⋮----
format!(",{:15}", max_inclusive - bin.slot_min),
format!(",{:15}", bin.count),
format!(",{:15}", bin.sum_size),
format!(",{:6}%", bin.sum_size * 100 / bin_all.sum_size),
format!(",{:15}", bin.min_size),
format!(",{:15}", bin.max_size),
format!(",{:15}", bin.avg),
⋮----
fn normal_bin_widths() -> Vec<usize> {
let mut bin_widths = vec![0];
⋮----
bin_widths.push(b);
⋮----
bin_widths.push(432_000);
⋮----
// if b > max_range {
// break;
// }
⋮----
fn normal_ancient(offset: i64) -> Vec<usize> {
⋮----
bin_widths.push((432_000 - offset) as usize);
⋮----
fn main() {
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.index(1)
.takes_value(true)
.value_name("PATH")
.help("ledger path"),
⋮----
.long("offset")
⋮----
.value_name("SLOT-OFFSET")
.validator(is_parsable::<i64>)
.help("ancient offset"),
⋮----
.get_matches();
let ledger = value_t_or_exit!(matches, "ledger", String);
let offset = value_t!(matches, "offset", i64).unwrap_or(100_000);
let path: PathBuf = [&ledger, "accounts", "run"].iter().collect();
if path.is_dir() {
⋮----
for entry in dir.flatten() {
if let Some(name) = entry.path().file_name() {
let name = name.to_str().unwrap().split_once(".").unwrap().0;
match fs::metadata(entry.path()) {
⋮----
info.push((name.parse::<usize>().unwrap(), meta.len() as usize));
⋮----
// skip when metadata fails. This can happen when you are running this tool while a validator is running.
// It could clean something away and delete it after getting the dir but before opening the file.
⋮----
// eprintln!("{name}, {len}");
⋮----
eprintln!("======== Normal Histogram");
calc(&info, normal_bin_widths(), offset);
eprintln!("========");
eprintln!("\n======== Normal Ancient Histogram");
calc(&info, normal_ancient(offset), offset);
⋮----
panic!("couldn't read folder: {path:?}, {dir:?}");
⋮----
panic!("not a folder: {path:?}");
⋮----
pub mod tests {
⋮----
fn test_calc() {
let info = vec![
⋮----
let max = info.iter().map(|(slot, _size)| *slot).max().unwrap();
⋮----
.into_iter()
.map(|(slot, size)| (max - slot + base, size))

================
File: accounts-db/store-histogram/Cargo.toml
================
[package]
name = "agave-store-histogram"
description = "Tool to calculate account storage histogram"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
clap = { workspace = true }
solana-version = { workspace = true }

================
File: accounts-db/store-tool/src/main.rs
================
fn main() {
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.global_setting(AppSettings::ArgRequiredElseHelp)
.global_setting(AppSettings::ColoredHelp)
.global_setting(AppSettings::InferSubcommands)
.global_setting(AppSettings::UnifiedHelpMessage)
.global_setting(AppSettings::VersionlessSubcommands)
.subcommand(
⋮----
.about("Inspects an account storage file and display each account's information")
.arg(
⋮----
.index(1)
.takes_value(true)
.value_name("PATH")
.help("Account storage file to inspect"),
⋮----
.short("v")
.long("verbose")
.takes_value(false)
.help("Show additional account information"),
⋮----
.about("Searches for accounts")
⋮----
.help("Account storage directory to search"),
⋮----
.index(2)
⋮----
.value_name("PUBKEYS")
.value_delimiter(",")
.help("Search for the entries of one or more pubkeys, delimited by commas"),
⋮----
.get_matches();
let subcommand = matches.subcommand();
let subcommand_str = subcommand.0.to_string();
⋮----
(CMD_INSPECT, Some(subcommand_matches)) => cmd_inspect(&matches, subcommand_matches),
(CMD_SEARCH, Some(subcommand_matches)) => cmd_search(&matches, subcommand_matches),
_ => unreachable!(),
⋮----
.unwrap_or_else(|err| {
eprintln!("Error: '{subcommand_str}' failed: {err}");
⋮----
fn cmd_inspect(
⋮----
let path = value_t_or_exit!(subcommand_matches, "path", String);
let verbose = subcommand_matches.is_present("verbose");
do_inspect(path, verbose)
⋮----
fn cmd_search(
⋮----
let addresses = values_t_or_exit!(subcommand_matches, "addresses", Pubkey);
⋮----
do_search(path, addresses, verbose)
⋮----
fn do_inspect(file: impl AsRef<Path>, verbose: bool) -> Result<(), String> {
let storage = AppendVec::new_for_store_tool(file.as_ref()).map_err(|err| {
format!(
⋮----
let data_size_width = width10(MAX_PERMITTED_DATA_LENGTH);
let offset_width = width16(storage.capacity());
let mut num_accounts = Saturating(0usize);
let mut stored_accounts_size = Saturating(0);
let mut lamports = Saturating(0);
⋮----
.scan_accounts_stored_meta_for_store_tool(|account| {
⋮----
println!("{account:?}");
⋮----
println!(
⋮----
stored_accounts_size += account.stored_size();
lamports += account.lamports();
⋮----
.map_err(|err| {
⋮----
Ok(())
⋮----
fn do_search(
⋮----
fn get_files_in(dir: impl AsRef<Path>) -> Result<Vec<PathBuf>, io::Error> {
⋮----
let path = entry?.path();
if path.is_file() {
⋮----
files.push(path);
⋮----
Ok(files)
⋮----
let files = get_files_in(&dir).map_err(|err| {
⋮----
files.par_iter().for_each(|file| {
let Ok(storage) = AppendVec::new_for_store_tool(file).inspect_err(|err| {
eprintln!(
⋮----
let file_name = Path::new(file.file_name().expect("path is a file"));
⋮----
if addresses.contains(account.pubkey()) {
⋮----
println!("storage: {}, {account:?}", file_name.display());
⋮----
fn width10(x: u64) -> usize {
(x as f64).log10().ceil() as usize
⋮----
fn width16(x: u64) -> usize {
(x as f64).log(16.0).ceil() as usize

================
File: accounts-db/store-tool/Cargo.toml
================
[package]
name = "agave-store-tool"
description = "Tool for account storage files"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
workspace = "../../dev-bins"

[features]
agave-unstable-api = []
dev-context-only-utils = []
dummy-for-ci-check = []
frozen-abi = []

[dependencies]
ahash = { workspace = true }
clap = { workspace = true }
rayon = { workspace = true }
solana-account = { workspace = true }
solana-accounts-db = { workspace = true, features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true }
solana-system-interface = { workspace = true }
solana-version = { workspace = true }

================
File: accounts-db/tests/read_only_accounts_cache.rs
================
fn test_read_only_accounts_cache_eviction(num_accounts: (usize, usize), evict_sample_size: usize) {
⋮----
let max_cache_size = num_accounts_lo.saturating_mul(CACHE_ENTRY_SIZE.saturating_add(DATA_SIZE));
⋮----
let data = vec![0u8; DATA_SIZE];
⋮----
data: data.clone(),
⋮----
cache.store(pubkey, slot, account.clone());
⋮----
newer_half.insert(pubkey);
⋮----
assert_eq!(cache.cache_len(), num_accounts_hi);
⋮----
let mut evicted = vec![];
⋮----
cache.evict_in_foreground(evict_sample_size, &mut rng, |pubkey, entry| {
let entry = entry.unwrap();
evicts = evicts.saturating_add(1);
if newer_half.contains(pubkey) {
evicts_from_newer_half = evicts_from_newer_half.saturating_add(1);
⋮----
evicted.push((*pubkey, entry));
⋮----
assert!(!evicted.is_empty());
for (pubkey, entry) in evicted.drain(..) {
cache.store_with_timestamp(
⋮----
entry.last_update_time.load(Ordering::Relaxed),
⋮----
assert!(error_margin < 0.03);

================
File: accounts-db/Cargo.toml
================
[package]
name = "solana-accounts-db"
description = "Solana accounts db"
documentation = "https://docs.rs/solana-accounts-db"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_accounts_db"

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "dep:qualifier_attr",
    "dep:solana-keypair",
    "dep:solana-rent",
    "dep:solana-signer",
    "dep:solana-stake-interface",
    "dep:solana-vote-program",
    "solana-account/dev-context-only-utils",
    "solana-transaction/dev-context-only-utils",
]
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-fee-calculator/frozen-abi",
    "solana-vote-program/frozen-abi",
]

[dependencies]
agave-fs = { workspace = true }
ahash = { workspace = true }
bincode = { workspace = true }
blake3 = { workspace = true }
bv = { workspace = true, features = ["serde"] }
bytemuck = { workspace = true }
bytemuck_derive = { workspace = true }
crossbeam-channel = { workspace = true }
dashmap = { workspace = true, features = ["rayon", "raw-api"] }
indexmap = { workspace = true }
itertools = { workspace = true }
log = { workspace = true }
lz4 = { workspace = true }
memmap2 = { workspace = true }
modular-bitfield = { workspace = true }
num_cpus = { workspace = true }
num_enum = { workspace = true }
qualifier_attr = { workspace = true, optional = true }
rand = { workspace = true }
rayon = { workspace = true }
seqlock = { workspace = true }
serde = { workspace = true, features = ["rc"] }
smallvec = { workspace = true, features = ["const_generics"] }
solana-account = { workspace = true, features = ["serde"] }
solana-address-lookup-table-interface = { workspace = true, features = [
    "bincode",
    "bytemuck",
] }
solana-bucket-map = { workspace = true }
solana-clock = { workspace = true }
solana-epoch-schedule = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-genesis-config = { workspace = true, features = ["serde"] }
solana-hash = { workspace = true, features = ["serde"] }
solana-keypair = { workspace = true, optional = true }
solana-lattice-hash = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-nohash-hasher = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-rayon-threadlimit = { workspace = true }
solana-rent = { workspace = true, optional = true }
solana-reward-info = { workspace = true, features = ["serde"] }
solana-sha256-hasher = { workspace = true }
solana-signer = { workspace = true, optional = true }
solana-slot-hashes = { workspace = true }
solana-stake-interface = { workspace = true, optional = true }
solana-svm-transaction = { workspace = true }
solana-system-interface = { workspace = true }
solana-sysvar = { workspace = true }
solana-time-utils = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
solana-vote-program = { workspace = true, optional = true }
spl-generic-token = { workspace = true }
static_assertions = { workspace = true }
tempfile = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
agave-reserved-account-keys = { workspace = true }
assert_matches = { workspace = true }
criterion = { workspace = true }
libsecp256k1 = { workspace = true }
memoffset = { workspace = true }
rand_chacha = { workspace = true }
serde_bytes = { workspace = true }
# See order-crates-for-publishing.py for using this unusual `path = "."`
solana-accounts-db = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-compute-budget = { workspace = true }
solana-instruction = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signature = { workspace = true, features = ["rand"] }
solana-slot-history = { workspace = true }
solana-svm = { workspace = true }
static_assertions = { workspace = true }
strum = { workspace = true, features = ["derive"] }
strum_macros = { workspace = true }
test-case = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dev-dependencies]
jemallocator = { workspace = true }

[[bench]]
name = "bench_accounts_file"
harness = false

[[bench]]
name = "bench_hashing"
harness = false

[[bench]]
name = "read_only_accounts_cache"
harness = false

[[bench]]
name = "bench_serde"
harness = false

[[bench]]
name = "bench_lock_accounts"
harness = false

[lints]
workspace = true

================
File: bam-banking-bench/src/main.rs
================
mod mock_bam_server;
⋮----
fn main() {
⋮----
let matches = Command::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.long("num-keypairs")
.takes_value(true)
.help("Number of keypairs")
.default_value("1000"),
⋮----
.long("test-duration")
⋮----
.help("Test duration in seconds")
.default_value("60"),
⋮----
.get_matches();
let test_duration = matches.value_of_t::<u64>("test_duration").unwrap();
⋮----
} = create_genesis_config(mint_total);
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
let bank = bank_forks.read().unwrap().working_bank_with_scheduler();
bank.write_cost_tracker()
.unwrap()
.set_limits(u64::MAX, u64::MAX, u64::MAX);
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path()).expect("Expected to be able to open database ledger"),
⋮----
create_test_recorder(
bank.clone(),
blockstore.clone(),
⋮----
Some(leader_schedule_cache),
⋮----
let (banking_tracer, tracer_thread) = BankingTracer::new(None).unwrap();
⋮----
let (batch_sender, batch_receiver) = unbounded();
let (outbound_sender, outbound_receiver) = unbounded();
⋮----
batch_sender: batch_sender.clone(),
⋮----
outbound_receiver: outbound_receiver.clone(),
⋮----
ContactInfo::new_localhost(&keypair.pubkey(), timestamp()),
⋮----
bank_forks: bank_forks.clone(),
⋮----
let keypairs = (0..matches.value_of_t::<usize>("num_keypairs").unwrap())
.map(|_| Keypair::new())
⋮----
keypairs.iter().for_each(|k: &Keypair| {
bank.process_transaction(&system_transaction::transfer(
⋮----
&k.pubkey(),
⋮----
bank.last_blockhash(),
⋮----
.unwrap();
⋮----
let shared_leader_state = poh_recorder.read().unwrap().shared_leader_state();
⋮----
exit.clone(),
⋮----
} = banking_tracer.create_channels(false);
⋮----
poh_recorder.clone(),
⋮----
bank_forks.clone(),
⋮----
Some(bam_dependencies),
⋮----
let bank_forks = bank_forks.clone();
let poh_recorder = poh_recorder.clone();
let exit = exit.clone();
spawn(move || {
bank_setting_loop(
⋮----
sleep(Duration::from_secs(test_duration));
exit.store(true, Ordering::Relaxed);
drop(non_vote_sender);
drop(tpu_vote_sender);
drop(gossip_vote_sender);
banking_stage.join().unwrap();
debug!("waited for banking_stage");
poh_service.join().unwrap();
sleep(Duration::from_secs(1));
debug!("waited for poh_service");
⋮----
tracer_thread.join().unwrap().unwrap();
⋮----
mock_bam_server.join().unwrap();
bank_setting_thread.join().unwrap();
⋮----
fn bank_setting_loop(
⋮----
let mut bank = bank_forks.read().unwrap().working_bank_with_scheduler();
⋮----
let mut bank_transaction_count = bank.transaction_count();
⋮----
while !exit.load(Ordering::Relaxed) {
⋮----
signal_receiver.recv_timeout(Duration::from_millis(10))
⋮----
total_txs += entry.transactions.len();
⋮----
if poh_recorder.read().unwrap().bank().is_none() {
let new_bank_transaction_count = bank.transaction_count();
eprintln!(
⋮----
.write()
⋮----
.reset(bank.clone(), Some((bank.slot(), bank.slot() + 1)));
if let Some((result, _timings)) = bank.wait_for_completed_scheduler() {
assert_matches!(result, Ok(_));
⋮----
let new_slot = bank.slot() + 1;
⋮----
Bank::new_from_parent(bank.clone(), &solana_pubkey::new_rand(), new_slot);
assert_matches!(poh_recorder.read().unwrap().bank(), None);
update_bank_forks_and_poh_recorder_for_new_tpu_bank(
⋮----
bank = bank_forks.read().unwrap().working_bank_with_scheduler();
⋮----
while start.elapsed() < Duration::from_secs(1)
&& poh_recorder.read().unwrap().bank().is_none()
⋮----
assert_matches!(poh_recorder.read().unwrap().bank(), Some(_));

================
File: bam-banking-bench/src/mock_bam_server.rs
================
fn make_transfer_transaction_with_compute_unit_price(
⋮----
transfer(&from_keypair.pubkey(), to, lamports),
⋮----
Some(&from_keypair.pubkey()),
⋮----
tx.signatures = vec![Signature::new_unique(); 1];
⋮----
struct BamOutboundMessageResult {
⋮----
struct BamTransactionInfo {
⋮----
struct BamTransactionAndResult {
⋮----
struct BankStats {
⋮----
impl BankStats {
fn new(bank_slot: u64) -> Self {
⋮----
fn print_stats(&self) {
⋮----
.values()
.map(|tx_and_result| {
⋮----
.as_ref()
.unwrap()
⋮----
.duration_since(tx_and_result.transaction.time_sent)
.as_millis()
⋮----
time_diffs.sort();
let median_time_diff = time_diffs[time_diffs.len() / 2];
let average_time_diff = time_diffs.iter().sum::<u128>() / time_diffs.len() as u128;
let max_time_diff = time_diffs.iter().max().unwrap();
let min_time_diff = time_diffs.iter().min().unwrap();
⋮----
.filter(|result| {
matches!(
⋮----
.count();
println!(
⋮----
pub(crate) struct MockBamServer;
impl MockBamServer {
pub(crate) fn run(
⋮----
BankStats::new(shared_leader_state.load().working_bank().unwrap().slot());
⋮----
while !exit.load(Ordering::Relaxed) {
let Some(bank) = shared_leader_state.load().working_bank().cloned() else {
⋮----
if bank.slot() != bank_stats.bank_slot {
⋮----
bank_stats.print_stats();
bank_stats = BankStats::new(bank.slot());
⋮----
fn handle_outbound_messages(
⋮----
while let Ok(msg) = outbound_receiver.try_recv() {
⋮----
.get_mut(&result.seq_id)
.unwrap();
transaction_info.result = Some(BamOutboundMessageResult {
⋮----
panic!("unexpected message");
⋮----
fn wait_for_all_results(
⋮----
.iter()
.all(|(_, result)| result.result.is_some())
⋮----
fn send_transactions(
⋮----
for keypair in keypairs.iter() {
let tx = make_transfer_transaction_with_compute_unit_price(
⋮----
&keypair.pubkey(),
⋮----
bank.last_blockhash(),
⋮----
let packet = solana_packet::Packet::from_data(None, &tx).unwrap();
let data = packet.data(..).unwrap_or_default().to_vec();
⋮----
max_schedule_slot: bank.slot(),
packets: vec![Packet {
⋮----
batch_sender.send(atomic_txn_batch).unwrap();
bank_stats.sent_transactions_and_results.insert(

================
File: bam-banking-bench/.gitignore
================
/target/
/farf/

================
File: bam-banking-bench/Cargo.toml
================
[package]
name = "bam-banking-bench"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
agave-banking-stage-ingress-types = { workspace = true }
agave-logger = { workspace = true }
assert_matches = { workspace = true }
clap = { version = "3.1.8", features = ["derive", "cargo"] }
crossbeam-channel = { workspace = true }
jito-protos = { workspace = true }
log = { workspace = true }
rand = { workspace = true }
rayon = { workspace = true }
solana-client = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-core = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-net-utils = { workspace = true }
solana-perf = { workspace = true }
solana-poh = { workspace = true }
solana-pubkey = { workspace = true }
solana-runtime = { workspace = true }
solana-signature = { workspace = true, features = ["rand"] }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }
solana-time-utils = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-version = { workspace = true }
tokio = { workspace = true }

================
File: bam-local-cluster/examples/example_config.toml
================
bam_url = "http://127.0.0.1:50055"
tip_payment_program_id = "T1pyyaTNZsKv2WcRAB8oVnk93mLJw2XzjtVYqCsaHqt"
tip_distribution_program_id = "4R3gSG8BpU4t19KYj8CfnbtRpnT8gtk4dvTHxVRwc2r7"
faucet_address = "127.0.0.1:12345"
ledger_base_directory = "output/ledger"
validator_build_path = "/path/to/jito-solana/target/release/"
ledger_tool_build_path = "/path/to/jito-solana/dev-bins/target/release/"

[[validators]]
geyser_config = "/path/to/validator-1/geyser1.json"
node_keypair = "/path/to/validator-1/node-keypair.json"
node_pubkey = "9uFHWmQQHX2Bgz9EKmDJcHGEwGvPywkuWoLH4oaKCj7A"
vote_keypair = "/path/to/validator-1/vote-keypair.json"
vote_pubkey = "2vGoCmcm1eeMmjFiw5vkEMBe9nBTwZdiL6FTVvEvekMW"
ledger_path = "/path/to/validator-1"

[[validators]]
geyser_config = "/path/to/validator-2/geyser2.json"
node_keypair = "/path/to/validator-2/node-keypair.json"
node_pubkey = "9k7XKLbNS85FCGMo6vGNyLM1JBGE3j32tEwLW6cQS2z2"
vote_keypair = "/path/to/validator-2/vote-keypair.json"
vote_pubkey = "vCbtKZFqtMU5UZUK4A1rSv2JVeNBgpzAKEfitmAr2md"
ledger_path = "/path/to/validator-2"

[[validators]]
geyser_config = "/path/to/validator-3/geyser3.json"
node_keypair = "/path/to/validator-3/node-keypair.json"
node_pubkey = "5sVJpE9gRcbuQDjXZoA87SJQwp12avysdGdPUYsW9QVd"
vote_keypair = "/path/to/validator-3/vote-keypair.json"
vote_pubkey = "Cj8XxNaCCsWP9jMsna2fog3svsEzqhsRPhJ2YYkFG5KU"
ledger_path = "/path/to/validator-3"

[[validators]]
geyser_config = "/path/to/validator-4/geyser4.json"
node_keypair = "/path/to/validator-4/node-keypair.json"
node_pubkey = "EwjV6iZRemLb98wrjFVBoJMRfoYTmCyeeUjvdiSDxG4s"
vote_keypair = "/path/to/validator-4/vote-keypair.json"
vote_pubkey = "3Wdvr1BRH9GWdsrqbjyDWsof2UCnqszQnwWFcEJdH5LZ"
ledger_path = "/path/to/validator-4"

================
File: bam-local-cluster/src/cluster_manager.rs
================
pub struct BamValidator {
⋮----
impl BamValidator {
⋮----
fn start_process(
⋮----
let validator_binary = format!("{}/agave-validator", cluster_config.validator_build_path);
⋮----
cmd.env("RUST_LOG", "info")
.arg("--log")
.arg(log_file_path)
.arg("--ledger")
.arg(ledger_path)
.arg("--identity")
.arg(identity_path)
.arg("--vote-account")
.arg(vote_path)
.arg("--authorized-voter")
⋮----
.arg("--bind-address")
.arg("0.0.0.0")
.arg("--dynamic-port-range")
.arg(format!(
⋮----
.arg("--no-wait-for-vote-to-start-leader")
.arg("--no-os-network-limits-test")
.arg("--wait-for-supermajority")
.arg("0")
.arg("--rpc-port")
.arg(rpc_port.to_string())
.arg("--rpc-faucet-address")
.arg(&cluster_config.faucet_address)
.arg("--rpc-pubsub-enable-block-subscription")
.arg("--rpc-pubsub-enable-vote-subscription")
.arg("--account-index")
.arg("program-id")
.arg("--allow-private-addr")
.arg("--full-rpc-api")
.arg("--enable-rpc-transaction-history")
.arg("--enable-extended-tx-metadata-storage")
.arg("--expected-shred-version")
.arg(compute_shred_version(&genesis_config.hash(), None).to_string())
.arg("--bam-url")
.arg(&cluster_config.bam_url)
.arg("--tip-distribution-program-pubkey")
.arg(&cluster_config.tip_distribution_program_id)
.arg("--tip-payment-program-pubkey")
.arg(&cluster_config.tip_payment_program_id)
.arg("--merkle-root-upload-authority")
.arg("11111111111111111111111111111111")
.arg("--commission-bps")
.arg("100");
⋮----
cmd.arg("--expected-bank-hash").arg(expected_bank_hash);
⋮----
cmd.arg("--gossip-port").arg(gossip_port.to_string());
⋮----
cmd.arg("--entrypoint").arg(bootstrap_gossip);
⋮----
cmd.arg("--geyser-plugin-config").arg(geyser_config);
⋮----
info!("Starting {node_name} node with command: {cmd:?}");
let cmd_str = std::iter::once(cmd.get_program())
.chain(cmd.get_args())
.map(|s| s.to_string_lossy())
⋮----
.join(" ");
println!("CLI Command: {cmd_str}");
⋮----
.stdout(Stdio::piped())
.stderr(Stdio::piped())
.spawn()
.context(format!("Failed to start {node_name} node"))?;
let child_stdout = child.stdout.take();
let child_stderr = child.stderr.take();
let log_file_path = log_file_path.clone();
let node_name = node_name.to_string();
⋮----
let node_name = node_name.clone();
runtime.spawn(async move {
⋮----
Ok(Self {
⋮----
node_name: node_name.to_string(),
⋮----
pub fn get_bank_hash(
⋮----
let ledger_tool_binary = format!("{ledger_tool_build_path}/agave-ledger-tool");
⋮----
cmd.arg("-l")
.arg(ledger_path.to_str().unwrap())
.arg("--ignore-ulimit-nofile-error")
.arg("verify")
.arg("--print-bank-hash");
let output = cmd.output()?;
if !output.status.success() {
⋮----
error!("stdout: {stdout}");
⋮----
error!("stderr: {stderr}");
return Err(anyhow::anyhow!(
⋮----
for line in stdout.lines() {
if let Some(hash) = line.strip_prefix("Bank hash for slot 0: ") {
return Ok(hash.trim().to_string());
⋮----
Err(anyhow::anyhow!(
⋮----
async fn stream_output_stdout(stdout: std::process::ChildStdout, node_name: &str) {
⋮----
let reader = BufReader::new(tokio::process::ChildStdout::from_std(stdout).unwrap());
let mut lines = reader.lines();
while let Ok(Some(line)) = lines.next_line().await {
println!("[\x1b[35m{node_name}\x1b[0m] stdout: {line}");
⋮----
async fn stream_output_stderr(stderr: std::process::ChildStderr, node_name: &str) {
⋮----
let reader = BufReader::new(tokio::process::ChildStderr::from_std(stderr).unwrap());
⋮----
println!("[\x1b[35m{node_name}\x1b[0m] stderr: {line}");
⋮----
async fn tail_log_file(log_file_path: &PathBuf, node_name: &str) {
⋮----
if let Err(e) = file.seek(SeekFrom::End(0)).await {
eprintln!("Failed to seek log file: {e}");
⋮----
buf.clear();
match reader.read_line(&mut buf).await {
⋮----
print!("[\x1b[35m{node_name}\x1b[0m] LOG: {buf}");
⋮----
Err(e) if e.kind() == io::ErrorKind::Interrupted => continue,
⋮----
eprintln!("Error reading log file: {e}");
⋮----
pub fn is_running(&mut self) -> bool {
⋮----
.try_wait()
.map(|status| status.is_none())
.unwrap_or(false)
⋮----
pub fn kill(&mut self) -> Result<(), Box<dyn std::error::Error>> {
self.process.kill()?;
self.process.wait()?;
Ok(())
⋮----
fn create_snapshot(
⋮----
.arg(validator_ledger_path.to_str().unwrap())
⋮----
.arg("create-snapshot")
.arg("0");
⋮----
pub struct BamLocalCluster {
⋮----
impl BamLocalCluster {
pub fn new(
⋮----
vote_keypairs.push(ValidatorVoteKeypairs {
node_keypair: read_keypair_file(&validator.node_keypair)?,
vote_keypair: read_keypair_file(&validator.vote_keypair)?,
⋮----
let stakes = vec![DEFAULT_NODE_STAKE; config.validators.len()];
⋮----
let runtime = Runtime::new().expect("Could not create Tokio runtime");
⋮----
genesis_config_info.mint_keypair.insecure_clone(),
⋮----
runtime.spawn(run_faucet(faucet, faucet_address, None));
⋮----
for (i, validator_config) in config.validators.iter().enumerate() {
if skip_last_validator && (i == config.validators.len().saturating_sub(1)) {
info!(
⋮----
let ledger_path = validator_config.ledger_path.clone();
if !ledger_path.exists() {
⋮----
.into());
⋮----
create_new_ledger(
⋮----
info!("Getting bank hash for bootstrap validator");
⋮----
info!("Bank hash for slot 0: {bank_hash:?}");
expected_bank_hash = Some(bank_hash);
⋮----
let log_file_path = ledger_path.join("validator.log");
⋮----
.saturating_add(i.saturating_mul(1000) as u16);
let dynamic_port_range_end = dynamic_port_range_start.saturating_add(1000);
⋮----
&format!("validator-{}", i.saturating_add(1)),
is_bootstrap.then_some(BOOTSTRAP_GOSSIP_PORT),
⋮----
dynamic_port_range_start.saturating_add(100)
⋮----
Some(BOOTSTRAP_GOSSIP)
⋮----
expected_bank_hash.clone(),
⋮----
validators.push(validator);
⋮----
sleep(Duration::from_secs(5));
⋮----
pub fn create_genesis_config_with_vote_accounts_and_cluster_type(
⋮----
assert!(!voting_keypairs.is_empty());
assert_eq!(voting_keypairs.len(), stakes.len());
⋮----
let voting_keypair = voting_keypairs[0].borrow().vote_keypair.insecure_clone();
let validator_pubkey = voting_keypairs[0].borrow().node_keypair.pubkey();
let mut genesis_config = create_genesis_config_with_leader_ex(
⋮----
&mint_keypair.pubkey(),
⋮----
&voting_keypairs[0].borrow().vote_keypair.pubkey(),
&voting_keypairs[0].borrow().stake_keypair.pubkey(),
⋮----
vec![],
⋮----
let feature_set_keys = FEATURE_NAMES.keys().cloned().collect::<Vec<_>>();
let feature_set_keys_chunks = feature_set_keys.chunks(100);
⋮----
info!("Getting features from mainnet-beta...");
⋮----
.get_multiple_accounts(chunk)
.expect("Failed to get features from mainnet-beta");
for (pubkey, account) in chunk.iter().zip(response) {
⋮----
if feature.activated_at.is_some() {
info!("Activating feature: {:?}", FEATURE_NAMES.get(pubkey));
activate_feature(&mut genesis_config, *pubkey);
⋮----
info!("Activating remaining compute units syscall enabled");
activate_feature(
⋮----
activate_feature(&mut genesis_config, agave_feature_set::vote_state_v4::id());
⋮----
for (validator_voting_keypairs, stake) in voting_keypairs[1..].iter().zip(&stakes[1..]) {
let node_pubkey = validator_voting_keypairs.borrow().node_keypair.pubkey();
let vote_pubkey = validator_voting_keypairs.borrow().vote_keypair.pubkey();
let stake_pubkey = validator_voting_keypairs.borrow().stake_keypair.pubkey();
⋮----
let stake_account = Account::from(create_stake_account(
⋮----
genesis_config_info.genesis_config.accounts.extend(vec![
⋮----
for (pubkey, account) in spl_programs(&genesis_config_info.genesis_config.rent) {
⋮----
.add_account(pubkey, account);
⋮----
pub fn run(&self) -> Result<(), Box<dyn std::error::Error>> {
let validators_ptr = self.validators.clone();
⋮----
self.runtime.spawn(async move {
⋮----
self.runtime.block_on(async {
⋮----
async fn monitor_validators(
⋮----
if let Ok(mut validators_guard) = validators.lock() {
let valid_idx_limit = validators_guard.len().saturating_sub(1);
for (i, validator) in validators_guard.iter_mut().enumerate() {
if !validator.is_running() {
error!(
⋮----
let _ = exit_tx.send(());
⋮----
debug!(
⋮----
pub fn shutdown(self) {
info!("Shutting down cluster...");
for mut validator in self.validators.lock().unwrap().drain(..) {
if let Err(e) = validator.kill() {
error!("Failed to kill validator {}: {}", validator.node_name, e);
⋮----
fn create_stake_account(
⋮----
vote_state::VoteStateV4::deserialize(vote_account.data(), voter_pubkey).unwrap();
let credits_observed = vote_state.credits();
let rent_exempt_reserve = rent.minimum_balance(stake_account.data().len());
⋮----
.checked_sub(rent_exempt_reserve)
.expect("lamports >= rent_exempt_reserve");
⋮----
.set_state(&StakeStateV2::Stake(meta, stake, StakeFlags::empty()))
.expect("set_state");

================
File: bam-local-cluster/src/config.rs
================
pub struct LocalClusterConfig {
⋮----
pub struct CustomValidatorConfig {
⋮----
pub struct ClusterInfo {
⋮----
impl LocalClusterConfig {
pub fn from_file(path: &str) -> Result<Self, Box<dyn std::error::Error>> {
⋮----
Ok(config)
⋮----
pub fn get_bootstrap_node(&self) -> Option<&CustomValidatorConfig> {
self.validators.first()
⋮----
pub fn get_validator_nodes(&self) -> Vec<&CustomValidatorConfig> {
self.validators.iter().skip(1).collect()

================
File: bam-local-cluster/src/lib.rs
================
pub mod cluster_manager;
pub mod config;

================
File: bam-local-cluster/src/main.rs
================
fn main() -> Result<(), Box<dyn std::error::Error>> {
⋮----
.version("0.1")
.about("Spins up a local Solana cluster from a TOML config for BAM testing")
.arg(
⋮----
.long("config")
.value_name("FILE")
.help("TOML configuration file path")
.takes_value(true)
.required(true),
⋮----
.long("quiet")
.help("Quiet mode does not tail the validator log files to stdout")
.takes_value(false)
.required(false),
⋮----
.long("skip-last-validator")
.help("Skip starting the last validator in the configuration")
⋮----
.get_matches();
let is_quiet = matches.is_present("quiet");
let skip_last_validator = matches.is_present("skip-last-validator");
let config_path = matches.value_of("config").unwrap();
let config = LocalClusterConfig::from_file(config_path).expect("Failed to parse TOML");
info!("Starting cluster with config: {config:?}");
⋮----
if config.validators.len() <= 1 {
error!("Cannot skip the last validator when only one validator is configured");
return Err("Skipping the last validator requires at least two validators".into());
⋮----
info!("Skipping startup of the last validator process");
⋮----
let cluster = match BamLocalCluster::new(config.clone(), is_quiet, skip_last_validator) {
⋮----
error!("Failed to start cluster: {e}");
return Err(e);
⋮----
if let Err(e) = cluster.run() {
error!("Cluster error: {e}");
⋮----
cluster.shutdown();
Ok(())

================
File: bam-local-cluster/Cargo.toml
================
[package]
name = "bam-local-cluster"
description = "BAM LocalCluster bootstrapper for Solana testing"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
anyhow = { workspace = true }
axum = { workspace = true }
clap = { workspace = true }
log = { workspace = true }
serde = { workspace = true, features = ["derive"] }
serde_json = { workspace = true }
solana-account  = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-core = { workspace = true }
solana-faucet = { workspace = true }
solana-feature-gate-interface = { workspace = true }
solana-fee-calculator = { workspace = true }
solana-genesis-config = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true }
solana-local-cluster = { workspace = true }
solana-native-token = { workspace = true }
solana-program-test = { workspace = true }
solana-rent = { workspace = true }
solana-rpc = { workspace = true }
solana-rpc-client = { workspace = true }
solana-runtime = { workspace = true }
solana-shred-version = { workspace = true }
solana-signer = { workspace = true }
solana-stake-interface = { workspace = true }
solana-streamer = { workspace = true } 
solana-system-interface = { workspace = true }
solana-vote-program = { workspace = true }
tokio = { workspace = true, features = ["full"] }
toml = { workspace = true }

================
File: bam-local-cluster/README.md
================
# BAM Local Cluster

A tool for spinning up local Solana clusters with BAM (Block Assembly Marketplace) support for testing purposes. This tool uses subprocess-based execution to spawn `agave-validator` instances.

## Overview

BAM Local Cluster is a development tool for spinning up local Solana clusters with BAM (Block Assembly Marketplace) support. It's designed for testing BAM-related functionality in a controlled local environment.

The tool automatically handles:
- Validator process management and monitoring
- Genesis configuration with SPL programs
- Keypair generation and ledger setup
- Bootstrap node coordination
- Process health monitoring and graceful shutdown

## Quick Start

1. **Build the binaries**:
   ```bash
   # Build agave-validator
   cargo build --release --bin agave-validator
   
   # Build bam-local-cluster
   cargo build --release --bin bam-local-cluster
   ```

2. **Create a configuration file** (see `examples/example_config.toml`)

3. **Run the cluster**:
   ```bash
   RUST_LOG=info ./target/release/bam-local-cluster --config bam-local-cluster/examples/example_config.toml
   ```

## Configuration

The configuration file specifies BAM service URLs, tip program IDs, and validator settings. See `examples/example_config.toml` for a complete example.

Key configuration options:
- `bam_url`: BAM service endpoint
- `tip_payment_program_id` / `tip_distribution_program_id`: Tip manager programs
- `faucet_address`: Faucet service for airdrops
- `ledger_base_directory`: Base directory for validator ledgers
- `validator_build_path`: Build output directory (e.g., "target/debug" or "target/release") - required
- `ledger_tool_build_path`: Builder output for ledger tool (e.g., "target/debug" or "target/release") - required
- `validators`: Array of validator configurations (first is bootstrap node)



## How It Works

The tool spawns `agave-validator` processes as subprocesses, automatically handling:

1. Genesis configuration with SPL programs
2. Keypair generation and ledger setup
3. Bootstrap node startup and coordination
4. Validator process spawning and monitoring
5. Graceful shutdown on Ctrl+C or process failure

## Usage

The cluster runs until you press Ctrl+C or a validator process fails. All validator output is streamed to the console for debugging.

Use `--skip-last-validator` to omit starting the final validator (useful when running an alternate validator for testing purposes); the validator still receives stake/airdrop in genesis and remains in the leader schedule.


## Troubleshooting

Common issues:
- **Port conflicts**: Bootstrap node uses gossip port 8001 and RPC port 8899
- **Binary not found**: Ensure `agave-validator` and `agave-ledger-tool` are built in your target directory
- **Permission errors**: Make sure the ledger base directory is writable

Validator output is streamed to the console for debugging.

## Development

To modify the cluster behavior:

1. Update `src/cluster_manager.rs` for process spawning logic
2. Modify `src/config.rs` for configuration structure
3. Update `src/main.rs` for command-line interface

## License

This project is part of the Jito Solana JDS repository and follows the same license terms.

================
File: banking-bench/src/main.rs
================
fn check_txs(
⋮----
if let Ok((_bank, (entry, _tick_height))) = receiver.recv_timeout(Duration::from_millis(10))
⋮----
total += entry.transactions.len();
⋮----
if now.elapsed().as_secs() > 60 {
⋮----
if poh_recorder.read().unwrap().bank().is_none() {
⋮----
assert!(total >= ref_tx_count);
⋮----
enum WriteLockContention {
⋮----
impl WriteLockContention {
fn possible_values<'a>() -> impl Iterator<Item = clap::PossibleValue<'a>> {
⋮----
.iter()
.filter_map(|v| v.to_possible_value())
⋮----
type Err = String;
fn from_str(input: &str) -> Result<Self, String> {
⋮----
fn make_accounts_txs(
⋮----
.map(|_| pubkey::new_rand())
.collect();
⋮----
.into_par_iter()
.map(|i| {
let is_simulated_mint = is_simulated_mint_transaction(
⋮----
let mut new = make_transfer_transaction_with_compute_unit_price(
⋮----
let sig: [u8; 64] = std::array::from_fn(|_| rng().random::<u8>());
⋮----
new.signatures = vec![Signature::from(sig)];
⋮----
.collect()
⋮----
fn is_simulated_mint_transaction(
⋮----
fn make_transfer_transaction_with_compute_unit_price(
⋮----
let from_pubkey = from_keypair.pubkey();
let instructions = vec![
⋮----
let message = Message::new(&instructions, Some(&from_pubkey));
⋮----
struct PacketsPerIteration {
⋮----
impl PacketsPerIteration {
fn new(
⋮----
let transactions = make_accounts_txs(
⋮----
let packet_batches: Vec<PacketBatch> = to_packet_batches(&transactions, packets_per_batch);
assert_eq!(packet_batches.len(), batches_per_iteration);
⋮----
fn refresh_blockhash(&mut self, new_blockhash: Hash) {
for tx in self.transactions.iter_mut() {
⋮----
self.packet_batches = to_packet_batches(&self.transactions, self.packets_per_batch);
⋮----
fn main() {
⋮----
let matches = Command::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.long("iterations")
.takes_value(true)
.help("Number of test iterations"),
⋮----
.long("num-chunks")
⋮----
.value_name("SIZE")
.help("Number of transaction chunks."),
⋮----
.long("packets-per-batch")
⋮----
.help("Packets per batch"),
⋮----
.long("skip-sanity")
.takes_value(false)
.help("Skip transaction sanity execution"),
⋮----
.long("trace-banking")
⋮----
.help("Enable banking tracing"),
⋮----
.long("write-lock-contention")
⋮----
.possible_values(WriteLockContention::possible_values())
.help("Accounts that test transactions write lock"),
⋮----
.long("batches-per-iteration")
⋮----
.help("Number of batches to send in each iteration"),
⋮----
.long("block-production-method")
.value_name("METHOD")
⋮----
.possible_values(BlockProductionMethod::cli_names())
.help(BlockProductionMethod::cli_message()),
⋮----
.long("block-production-num-workers")
⋮----
.value_name("NUMBER")
.help("Number of worker threads to use for block production"),
⋮----
.long("transaction-structure")
.value_name("STRUCT")
⋮----
.possible_values(TransactionStructure::cli_names())
.help(TransactionStructure::cli_message()),
⋮----
.long("simulate-mint")
⋮----
.help("Simulate mint transactions to have higher priority"),
⋮----
.long("mint-txs-percentage")
⋮----
.requires("simulate_mint")
.help("In simulating mint, number of mint transactions out of 100."),
⋮----
.get_matches();
⋮----
.unwrap_or_default();
⋮----
.unwrap_or_else(|_| BankingStage::default_num_workers());
let num_chunks = matches.value_of_t::<usize>("num_chunks").unwrap_or(16);
⋮----
.unwrap_or(192);
let iterations = matches.value_of_t::<usize>("iterations").unwrap_or(1000);
⋮----
.unwrap_or(BankingStage::default_num_workers().get());
⋮----
.unwrap_or(WriteLockContention::None);
⋮----
.unwrap_or(99);
⋮----
} = create_genesis_config(mint_total);
let (replay_vote_sender, _replay_vote_receiver) = unbounded();
⋮----
let mut bank = bank_forks.read().unwrap().working_bank_with_scheduler();
bank.write_cost_tracker()
.unwrap()
.set_limits(u64::MAX, u64::MAX, u64::MAX);
⋮----
Some(PacketsPerIteration::new(
⋮----
genesis_config.hash(),
⋮----
matches.is_present("simulate_mint"),
⋮----
.take(num_chunks)
⋮----
.map(|packets_for_single_iteration| packets_for_single_iteration.transactions.len() as u64)
.sum();
info!("worker threads: {block_production_num_workers} txs: {total_num_transactions}");
all_packets.iter().for_each(|packets_for_single_iteration| {
⋮----
.for_each(|tx| {
⋮----
fund.signatures = vec![Signature::from(sig)];
bank.process_transaction(&fund).unwrap();
⋮----
let skip_sanity = matches.is_present("skip_sanity");
⋮----
let res = bank.process_transaction(tx);
assert!(res.is_ok(), "sanity test transactions error: {res:?}");
⋮----
bank.clear_signatures();
⋮----
bank.process_transactions(packets_for_single_iteration.transactions.iter());
⋮----
assert!(r.is_ok(), "sanity parallel execution error: {r:?}");
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
⋮----
Blockstore::open(ledger_path.path()).expect("Expected to be able to open database ledger"),
⋮----
) = create_test_recorder(
bank.clone(),
blockstore.clone(),
⋮----
Some(leader_schedule_cache),
⋮----
BankingTracer::new(matches.is_present("trace_banking").then_some((
&blockstore.banking_trace_path(),
exit.clone(),
⋮----
.unwrap();
⋮----
} = banking_tracer.create_channels(false);
⋮----
poh_recorder.clone(),
⋮----
bank_forks.clone(),
⋮----
let base_tx_count = bank.transaction_count();
⋮----
trace!("RUNNING ITERATION {current_iteration_index}");
⋮----
packets_for_this_iteration.packet_batches.iter().enumerate()
⋮----
sent += packet_batch.len();
trace!(
⋮----
.send(BankingPacketBatch::new(vec![packet_batch.clone()]))
⋮----
if bank.get_signature_status(&tx.signatures[0]).is_some() {
⋮----
sleep(Duration::from_millis(5));
⋮----
if check_txs(
⋮----
packets_for_this_iteration.transactions.len(),
⋮----
eprintln!(
⋮----
tx_total_us += now.elapsed().as_micros() as u64;
⋮----
.reset_sync(bank.clone(), Some((bank.slot(), bank.slot() + 1)))
⋮----
poh_time.stop();
⋮----
if let Some((result, _timings)) = bank.wait_for_completed_scheduler() {
assert_matches!(result, Ok(_));
⋮----
let new_slot = bank.slot() + 1;
let new_bank = Bank::new_from_parent(bank.clone(), &collector, new_slot);
new_bank_time.stop();
⋮----
assert_matches!(poh_recorder.read().unwrap().bank(), None);
update_bank_forks_and_poh_recorder_for_new_tpu_bank(
⋮----
bank = bank_forks.read().unwrap().working_bank_with_scheduler();
assert_matches!(poh_recorder.read().unwrap().bank(), Some(_));
insert_time.stop();
debug!(
⋮----
total_us += now.elapsed().as_micros() as u64;
⋮----
let last_blockhash = bank.last_blockhash();
for packets_for_single_iteration in all_packets.iter_mut() {
packets_for_single_iteration.refresh_blockhash(last_blockhash);
⋮----
.read()
⋮----
.working_bank()
.transaction_count();
debug!("processed: {txs_processed} base: {base_tx_count}");
⋮----
drop(non_vote_sender);
drop(tpu_vote_sender);
drop(gossip_vote_sender);
exit.store(true, Ordering::Relaxed);
banking_stage.join().unwrap();
debug!("waited for banking_stage");
poh_service.join().unwrap();
sleep(Duration::from_secs(1));
debug!("waited for poh_service");
⋮----
tracer_thread.join().unwrap().unwrap();

================
File: banking-bench/.gitignore
================
/target/
/farf/

================
File: banking-bench/Cargo.toml
================
[package]
name = "solana-banking-bench"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
workspace = "../dev-bins"

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = []
dummy-for-ci-check = []
frozen-abi = []

[dependencies]
agave-banking-stage-ingress-types = { workspace = true }
agave-logger = { workspace = true }
assert_matches = { workspace = true }
clap = { version = "3.1.8", features = ["derive", "cargo"] }
crossbeam-channel = { workspace = true }
log = { workspace = true }
rand = { workspace = true }
rayon = { workspace = true }
solana-client = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-core = { workspace = true, features = ["dev-context-only-utils"] }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-perf = { workspace = true }
solana-poh = { workspace = true, features = ["dev-context-only-utils"] }
solana-pubkey = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }
solana-time-utils = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-version = { workspace = true }
tokio = { workspace = true, features = ["sync"] }

================
File: banking-stage-ingress-types/src/lib.rs
================
pub type BankingPacketBatch = Arc<Vec<PacketBatch>>;
pub type BankingPacketReceiver = Receiver<BankingPacketBatch>;

================
File: banking-stage-ingress-types/Cargo.toml
================
[package]
name = "agave-banking-stage-ingress-types"
description = "Agave banking stage ingress types"
documentation = "https://docs.rs/agave-banking-stage-ingress-types"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
crossbeam-channel = { workspace = true }
solana-perf = { workspace = true }

================
File: banks-client/src/error.rs
================
pub enum BanksClientError {
⋮----
impl BanksClientError {
pub fn unwrap(&self) -> TransactionError {
⋮----
| BanksClientError::SimulationError { err, .. } => err.clone(),
_ => panic!("unexpected transport error"),
⋮----
fn from(err: BanksClientError) -> Self {
⋮----
BanksClientError::ClientError(err) => Self::other(err.to_string()),
⋮----
BanksClientError::RpcError(err) => Self::other(err.to_string()),
BanksClientError::TransactionError(err) => Self::other(err.to_string()),
BanksClientError::SimulationError { err, .. } => Self::other(err.to_string()),
⋮----
BanksClientError::ClientError(err) => Self::IoError(io::Error::other(err.to_string())),
BanksClientError::Io(err) => Self::IoError(io::Error::other(err.to_string())),
BanksClientError::RpcError(err) => Self::IoError(io::Error::other(err.to_string())),

================
File: banks-client/src/lib.rs
================
mod error;
mod transaction {
⋮----
pub trait BanksClientExt {}
⋮----
pub struct BanksClient {
⋮----
impl BanksClient {
⋮----
pub fn new<C>(
⋮----
pub async fn send_transaction_with_context(
⋮----
.send_transaction_with_context(ctx, transaction.into())
⋮----
.map_err(Into::into)
⋮----
pub async fn get_transaction_status_with_context(
⋮----
.get_transaction_status_with_context(ctx, signature)
⋮----
pub async fn get_slot_with_context(
⋮----
.get_slot_with_context(ctx, commitment)
⋮----
pub async fn get_block_height_with_context(
⋮----
.get_block_height_with_context(ctx, commitment)
⋮----
pub async fn process_transaction_with_commitment_and_context(
⋮----
.process_transaction_with_commitment_and_context(ctx, transaction.into(), commitment)
⋮----
pub async fn process_transaction_with_preflight_and_commitment_and_context(
⋮----
.process_transaction_with_preflight_and_commitment_and_context(
⋮----
transaction.into(),
⋮----
pub async fn process_transaction_with_metadata_and_context(
⋮----
.process_transaction_with_metadata_and_context(ctx, transaction.into())
⋮----
pub async fn simulate_transaction_with_commitment_and_context(
⋮----
.simulate_transaction_with_commitment_and_context(ctx, transaction.into(), commitment)
⋮----
pub async fn get_account_with_commitment_and_context(
⋮----
.get_account_with_commitment_and_context(ctx, address, commitment)
⋮----
pub async fn send_transaction(
⋮----
self.send_transaction_with_context(context::current(), transaction.into())
⋮----
pub async fn get_sysvar<T: SysvarSerialize>(&self) -> Result<T, BanksClientError> {
⋮----
.get_account(T::id())
⋮----
.ok_or(BanksClientError::ClientError("Sysvar not present"))?;
from_account::<T, _>(&sysvar).ok_or(BanksClientError::ClientError(
⋮----
pub async fn get_rent(&self) -> Result<Rent, BanksClientError> {
⋮----
pub async fn process_transaction_with_commitment(
⋮----
.process_transaction_with_commitment_and_context(ctx, transaction, commitment)
⋮----
None => Err(BanksClientError::ClientError(
⋮----
Some(transaction_result) => Ok(transaction_result?),
⋮----
pub async fn process_transaction_with_metadata(
⋮----
self.process_transaction_with_metadata_and_context(ctx, transaction.into())
⋮----
pub async fn process_transaction_with_preflight_and_commitment(
⋮----
} => Err(BanksClientError::ClientError(
⋮----
} => Err(BanksClientError::SimulationError {
⋮----
} => result.map_err(Into::into),
⋮----
pub async fn process_transaction_with_preflight(
⋮----
self.process_transaction_with_preflight_and_commitment(
⋮----
pub async fn process_transaction(
⋮----
self.process_transaction_with_commitment(transaction, CommitmentLevel::default())
⋮----
pub async fn process_transactions_with_commitment<T: Into<VersionedTransaction>>(
⋮----
let mut clients: Vec<_> = transactions.iter().map(|_| self.clone()).collect();
⋮----
.iter_mut()
.zip(transactions)
.map(|(client, transaction)| {
client.process_transaction_with_commitment(transaction, commitment)
⋮----
let statuses = join_all(futures).await;
statuses.into_iter().collect()
⋮----
pub async fn process_transactions<'a, T: Into<VersionedTransaction> + 'a>(
⋮----
self.process_transactions_with_commitment(transactions, CommitmentLevel::default())
⋮----
/// Simulate a transaction at the given commitment level
    pub async fn simulate_transaction_with_commitment(
⋮----
pub async fn simulate_transaction_with_commitment(
⋮----
self.simulate_transaction_with_commitment_and_context(
⋮----
/// Simulate a transaction at the default commitment level
    pub async fn simulate_transaction(
⋮----
pub async fn simulate_transaction(
⋮----
self.simulate_transaction_with_commitment(transaction, CommitmentLevel::default())
⋮----
/// Return the most recent rooted slot. All transactions at or below this slot
    /// are said to be finalized. The cluster will not fork to a higher slot.
⋮----
/// are said to be finalized. The cluster will not fork to a higher slot.
    pub async fn get_root_slot(&self) -> Result<Slot, BanksClientError> {
⋮----
pub async fn get_root_slot(&self) -> Result<Slot, BanksClientError> {
self.get_slot_with_context(context::current(), CommitmentLevel::default())
⋮----
/// Return the most recent rooted block height. All transactions at or below this height
    /// are said to be finalized. The cluster will not fork to a higher block height.
⋮----
/// are said to be finalized. The cluster will not fork to a higher block height.
    pub async fn get_root_block_height(&self) -> Result<Slot, BanksClientError> {
⋮----
pub async fn get_root_block_height(&self) -> Result<Slot, BanksClientError> {
self.get_block_height_with_context(context::current(), CommitmentLevel::default())
⋮----
/// Return the account at the given address at the slot corresponding to the given
    /// commitment level. If the account is not found, None is returned.
⋮----
/// commitment level. If the account is not found, None is returned.
    pub async fn get_account_with_commitment(
⋮----
pub async fn get_account_with_commitment(
⋮----
self.get_account_with_commitment_and_context(context::current(), address, commitment)
⋮----
/// Return the account at the given address at the time of the most recent root slot.
    /// If the account is not found, None is returned.
⋮----
/// If the account is not found, None is returned.
    pub async fn get_account(&self, address: Pubkey) -> Result<Option<Account>, BanksClientError> {
⋮----
pub async fn get_account(&self, address: Pubkey) -> Result<Option<Account>, BanksClientError> {
self.get_account_with_commitment(address, CommitmentLevel::default())
⋮----
/// Return the unpacked account data at the given address
    /// If the account is not found, an error is returned
⋮----
/// If the account is not found, an error is returned
    pub async fn get_packed_account_data<T: Pack>(
⋮----
pub async fn get_packed_account_data<T: Pack>(
⋮----
.get_account(address)
⋮----
.ok_or(BanksClientError::ClientError("Account not found"))?;
⋮----
.map_err(|_| BanksClientError::ClientError("Failed to deserialize account"))
⋮----
/// If the account is not found, an error is returned
    pub async fn get_account_data_with_borsh<T: BorshDeserialize>(
⋮----
pub async fn get_account_data_with_borsh<T: BorshDeserialize>(
⋮----
T::try_from_slice(&account.data).map_err(Into::into)
⋮----
/// Return the balance in lamports of an account at the given address at the slot
    /// corresponding to the given commitment level.
⋮----
/// corresponding to the given commitment level.
    pub async fn get_balance_with_commitment(
⋮----
pub async fn get_balance_with_commitment(
⋮----
Ok(self
.get_account_with_commitment_and_context(context::current(), address, commitment)
⋮----
.map(|x| x.lamports)
.unwrap_or(0))
⋮----
/// Return the balance in lamports of an account at the given address at the time
    /// of the most recent root slot.
⋮----
/// of the most recent root slot.
    pub async fn get_balance(&self, address: Pubkey) -> Result<u64, BanksClientError> {
⋮----
pub async fn get_balance(&self, address: Pubkey) -> Result<u64, BanksClientError> {
self.get_balance_with_commitment(address, CommitmentLevel::default())
⋮----
/// Return the status of a transaction with a signature matching the transaction's first
    pub async fn get_transaction_status(
⋮----
pub async fn get_transaction_status(
⋮----
self.get_transaction_status_with_context(context::current(), signature)
⋮----
pub async fn get_transaction_statuses(
⋮----
.into_iter()
.map(|signature| (self.clone(), signature))
.collect();
⋮----
.map(|(client, signature)| client.get_transaction_status(*signature));
let statuses = join_all(futs).await;
⋮----
pub async fn get_latest_blockhash(&self) -> Result<Hash, BanksClientError> {
self.get_latest_blockhash_with_commitment(CommitmentLevel::default())
⋮----
.map(|x| x.0)
.ok_or(BanksClientError::ClientError("valid blockhash not found"))
⋮----
pub async fn get_latest_blockhash_with_commitment(
⋮----
self.get_latest_blockhash_with_commitment_and_context(context::current(), commitment)
⋮----
pub async fn get_latest_blockhash_with_commitment_and_context(
⋮----
.get_latest_blockhash_with_commitment_and_context(ctx, commitment)
⋮----
pub async fn get_fee_for_message(
⋮----
self.get_fee_for_message_with_commitment_and_context(
⋮----
pub async fn get_fee_for_message_with_commitment(
⋮----
pub async fn get_fee_for_message_with_commitment_and_context(
⋮----
.get_fee_for_message_with_commitment_and_context(ctx, message, commitment)
⋮----
pub async fn start_client<C>(transport: C) -> Result<BanksClient, BanksClientError>
⋮----
Ok(BanksClient {
inner: TarpcClient::new(client::Config::default(), transport).spawn(),
⋮----
pub async fn start_tcp_client<T: ToSocketAddrs>(addr: T) -> Result<BanksClient, BanksClientError> {
⋮----
mod tests {
⋮----
fn test_banks_client_new() {
⋮----
fn test_banks_server_transfer_via_server() -> Result<(), BanksClientError> {
let genesis = create_genesis_config(10);
⋮----
let slot = bank.slot();
⋮----
let mint_pubkey = genesis.mint_keypair.pubkey();
⋮----
let message = Message::new(&[instruction], Some(&mint_pubkey));
Runtime::new()?.block_on(async {
⋮----
start_local_server(bank_forks, block_commitment_cache, Duration::from_millis(1))
⋮----
let banks_client = start_client(client_transport).await?;
let recent_blockhash = banks_client.get_latest_blockhash().await?;
⋮----
.simulate_transaction(transaction.clone())
⋮----
.unwrap();
assert!(simulation_result.result.unwrap().is_ok());
banks_client.process_transaction(transaction).await.unwrap();
assert_eq!(banks_client.get_balance(bob_pubkey).await?, 1);
Ok(())
⋮----
fn test_banks_server_transfer_via_client() -> Result<(), BanksClientError> {
⋮----
let mint_pubkey = &genesis.mint_keypair.pubkey();
⋮----
let message = Message::new(&[instruction], Some(mint_pubkey));
⋮----
.get_latest_blockhash_with_commitment(CommitmentLevel::default())
⋮----
banks_client.send_transaction(transaction).await?;
let mut status = banks_client.get_transaction_status(signature).await?;
while status.is_none() {
let root_block_height = banks_client.get_root_block_height().await?;
⋮----
sleep(Duration::from_millis(100)).await;
status = banks_client.get_transaction_status(signature).await?;
⋮----
assert!(status.unwrap().err.is_none());

================
File: banks-client/Cargo.toml
================
[package]
name = "solana-banks-client"
description = "Solana banks client"
documentation = "https://docs.rs/solana-banks-client"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_banks_client"

[features]
agave-unstable-api = []

[dependencies]
borsh = { workspace = true }
futures = { workspace = true }
solana-account = { workspace = true, features = ["bincode"] }
solana-banks-interface = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-hash = { workspace = true }
solana-message = { workspace = true }
solana-program-pack = { workspace = true }
solana-pubkey = { workspace = true }
solana-rent = { workspace = true }
solana-signature = { workspace = true }
solana-sysvar = { workspace = true, features = ["bincode"] }
solana-transaction = { workspace = true }
solana-transaction-context = { workspace = true }
solana-transaction-error = { workspace = true }
tarpc = { workspace = true, features = ["full"] }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tokio-serde = { workspace = true, features = ["bincode"] }

[dev-dependencies]
solana-banks-server = { workspace = true }
solana-pubkey = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true }

================
File: banks-interface/src/lib.rs
================
mod transaction {
⋮----
pub enum TransactionConfirmationStatus {
⋮----
pub struct TransactionStatus {
⋮----
pub struct TransactionSimulationDetails {
⋮----
pub struct TransactionMetadata {
⋮----
pub struct BanksTransactionResultWithSimulation {
⋮----
pub struct BanksTransactionResultWithMetadata {
⋮----
pub trait Banks {
⋮----
mod tests {
⋮----
fn test_banks_client_new() {

================
File: banks-interface/Cargo.toml
================
[package]
name = "solana-banks-interface"
description = "Solana banks RPC interface"
documentation = "https://docs.rs/solana-banks-interface"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_banks_interface"

[features]
agave-unstable-api = []

[dependencies]
serde = { workspace = true }
solana-account = { workspace = true, features = [ "serde" ] }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true, features = [ "serde" ] }
solana-hash = { workspace = true }
solana-message = { workspace = true, features = [ "serde" ] }
solana-pubkey = { workspace = true }
solana-signature = { workspace = true, features = [ "serde" ] }
solana-transaction = { workspace = true, features = [ "serde" ] }
solana-transaction-context = { workspace = true, features = [ "serde" ] }
solana-transaction-error = { workspace = true, features = [ "serde" ] }
tarpc = { workspace = true, features = ["full"] }

================
File: banks-server/src/banks_server.rs
================
mod transaction {
⋮----
struct BanksServer {
⋮----
impl BanksServer {
fn new(
⋮----
fn run(bank_forks: Arc<RwLock<BankForks>>, transaction_receiver: Receiver<TransactionInfo>) {
while let Ok(info) = transaction_receiver.recv() {
let mut transaction_infos = vec![info];
while let Ok(info) = transaction_receiver.try_recv() {
transaction_infos.push(info);
⋮----
.into_iter()
.map(|info| deserialize(&info.wire_transaction).unwrap())
.collect();
⋮----
let bank = bank_forks.read().unwrap().working_bank();
let lock = bank.freeze_lock();
⋮----
let _ = bank.try_process_entry_transactions(transactions);
⋮----
fn new_loopback(
⋮----
let (transaction_sender, transaction_receiver) = unbounded();
⋮----
let slot = bank.slot();
⋮----
let mut w_block_commitment_cache = block_commitment_cache.write().unwrap();
w_block_commitment_cache.set_all_slots(slot, slot);
⋮----
let server_bank_forks = bank_forks.clone();
⋮----
.name("solBankForksCli".to_string())
.spawn(move || Self::run(server_bank_forks, transaction_receiver))
.unwrap();
⋮----
fn slot(&self, commitment: CommitmentLevel) -> Slot {
⋮----
.read()
.unwrap()
.slot_with_commitment(commitment)
⋮----
fn bank(&self, commitment: CommitmentLevel) -> Arc<Bank> {
self.bank_forks.read().unwrap()[self.slot(commitment)].clone()
⋮----
async fn poll_signature_status(
⋮----
.bank(commitment)
.get_signature_status_with_blockhash(signature, blockhash);
while status.is_none() {
sleep(self.poll_signature_status_sleep_duration).await;
let bank = self.bank(commitment);
if bank.block_height() > last_valid_block_height {
⋮----
status = bank.get_signature_status_with_blockhash(signature, blockhash);
⋮----
fn simulate_transaction(
⋮----
Some(false),
⋮----
bank.get_reserved_account_keys(),
⋮----
.is_active(&agave_feature_set::static_instruction_limit::id()),
⋮----
result: Some(Err(err)),
⋮----
} = bank.simulate_transaction_unchecked(&sanitized_transaction, true);
⋮----
result: Some(result),
simulation_details: Some(simulation_details),
⋮----
impl Banks for BanksServer {
async fn send_transaction_with_context(self, _: Context, transaction: VersionedTransaction) {
let message_hash = transaction.message.hash();
let blockhash = transaction.message.recent_blockhash();
⋮----
.root_bank()
.get_blockhash_last_valid_block_height(blockhash)
⋮----
let signature = transaction.signatures.first().cloned().unwrap_or_default();
⋮----
serialize(&transaction).unwrap(),
⋮----
self.transaction_sender.send(info).unwrap();
⋮----
async fn get_transaction_status_with_context(
⋮----
let bank = self.bank(CommitmentLevel::Processed);
let (slot, status) = bank.get_signature_status_slot(&signature)?;
let r_block_commitment_cache = self.block_commitment_cache.read().unwrap();
let optimistically_confirmed_bank = self.bank(CommitmentLevel::Confirmed);
⋮----
optimistically_confirmed_bank.get_signature_status_slot(&signature);
let confirmations = if r_block_commitment_cache.root() >= slot
&& r_block_commitment_cache.highest_super_majority_root() >= slot
⋮----
.get_confirmation_count(slot)
.or(Some(0))
⋮----
Some(TransactionStatus {
⋮----
err: status.err(),
confirmation_status: if confirmations.is_none() {
Some(TransactionConfirmationStatus::Finalized)
} else if optimistically_confirmed.is_some() {
Some(TransactionConfirmationStatus::Confirmed)
⋮----
Some(TransactionConfirmationStatus::Processed)
⋮----
async fn get_slot_with_context(self, _: Context, commitment: CommitmentLevel) -> Slot {
self.slot(commitment)
⋮----
async fn get_block_height_with_context(self, _: Context, commitment: CommitmentLevel) -> u64 {
self.bank(commitment).block_height()
⋮----
async fn process_transaction_with_preflight_and_commitment_and_context(
⋮----
simulate_transaction(&self.bank(commitment), transaction.clone());
⋮----
.process_transaction_with_commitment_and_context(ctx, transaction, commitment)
⋮----
async fn simulate_transaction_with_commitment_and_context(
⋮----
simulate_transaction(&self.bank(commitment), transaction)
⋮----
async fn process_transaction_with_commitment_and_context(
⋮----
transaction.clone(),
⋮----
bank.as_ref(),
⋮----
Err(err) => return Some(Err(err)),
⋮----
if let Err(err) = sanitized_transaction.verify() {
return Some(Err(err));
⋮----
let message_hash = sanitized_transaction.message_hash();
⋮----
let signature = sanitized_transaction.signature();
⋮----
self.poll_signature_status(signature, blockhash, last_valid_block_height, commitment)
⋮----
async fn process_transaction_with_metadata_and_context(
⋮----
let bank = self.bank_forks.read().unwrap().working_bank();
match bank.process_transaction_with_metadata(transaction) {
⋮----
result: Err(error),
⋮----
metadata: Some(TransactionMetadata {
⋮----
log_messages: details.log_messages.unwrap_or_default(),
⋮----
async fn get_account_with_commitment_and_context(
⋮----
bank.get_account(&address).map(Account::from)
⋮----
async fn get_latest_blockhash_with_context(self, _: Context) -> Hash {
let bank = self.bank(CommitmentLevel::default());
bank.last_blockhash()
⋮----
async fn get_latest_blockhash_with_commitment_and_context(
⋮----
let blockhash = bank.last_blockhash();
let last_valid_block_height = bank.get_blockhash_last_valid_block_height(&blockhash)?;
Some((blockhash, last_valid_block_height))
⋮----
async fn get_fee_for_message_with_commitment_and_context(
⋮----
SanitizedMessage::try_from_legacy_message(message, bank.get_reserved_account_keys())
.ok()?;
bank.get_fee_for_message(&sanitized_message)
⋮----
pub async fn start_local_server(
⋮----
let server = server::BaseChannel::with_defaults(server_transport).execute(banks_server.serve());
⋮----
fn create_client(
⋮----
let runtime_handle = maybe_runtime.unwrap_or_else(|| {
Handle::try_current().expect("runtime handle not provided, and not inside Tokio runtime")
⋮----
let port_range = localhost_port_range_for_tests();
let bind_socket = bind_to(IpAddr::V4(Ipv4Addr::LOCALHOST), port_range.0)
.expect("Should be able to open UdpSocket for tests.");
⋮----
runtime_handle.spawn({
⋮----
let cancel = cancel.clone();
⋮----
if exit.load(Ordering::Relaxed) {
cancel.cancel();
⋮----
pub async fn start_tcp_server(
⋮----
.filter_map(|r| future::ready(r.ok()))
.map(server::BaseChannel::with_defaults)
.max_channels_per_key(1, |t| {
t.as_ref()
.peer_addr()
.map(|x| x.ip())
.unwrap_or_else(|_| Ipv4Addr::UNSPECIFIED.into())
⋮----
.map(|chan| {
let (sender, receiver) = unbounded();
let client = create_client(None, cluster_info.clone(), exit.clone());
⋮----
exit.clone(),
⋮----
bank_forks.clone(),
block_commitment_cache.clone(),
⋮----
chan.execute(server.serve())
⋮----
.buffer_unordered(10)
.for_each(|_| async {});
⋮----
Ok(())

================
File: banks-server/src/lib.rs
================
pub mod banks_server;

================
File: banks-server/Cargo.toml
================
[package]
name = "solana-banks-server"
description = "Solana banks server"
documentation = "https://docs.rs/solana-banks-server"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_banks_server"

[features]
agave-unstable-api = []

[dependencies]
agave-feature-set = { workspace = true }
bincode = { workspace = true }
crossbeam-channel = { workspace = true }
futures = { workspace = true }
solana-account = { workspace = true }
solana-banks-interface = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-message = { workspace = true }
solana-net-utils = { workspace = true }
solana-pubkey = { workspace = true }
solana-runtime = { workspace = true }
solana-runtime-transaction = { workspace = true }
solana-send-transaction-service = { workspace = true }
solana-signature = { workspace = true }
solana-svm = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
tarpc = { workspace = true, features = ["full"] }
tokio = { workspace = true, features = ["full"] }
tokio-serde = { workspace = true, features = ["bincode"] }
tokio-util = { workspace = true }

================
File: bench-streamer/src/main.rs
================
fn producer(dest_addr: &SocketAddr, exit: Arc<AtomicBool>) -> JoinHandle<usize> {
let send = bind_to_unspecified().unwrap();
⋮----
packet.buffer_mut()[..payload.len()].copy_from_slice(&payload);
packet.meta_mut().size = payload.len();
packet.meta_mut().set_socket_addr(dest_addr);
⋮----
packet_batch.resize(batch_size, packet);
spawn(move || {
⋮----
if exit.load(Ordering::Relaxed) {
⋮----
let packets_and_addrs = packet_batch.iter().map(|packet| {
let addr = packet.meta().socket_addr();
let data = packet.data(..).unwrap();
⋮----
batch_send(&send, packets_and_addrs).unwrap();
⋮----
fn sink(exit: Arc<AtomicBool>, rvs: Arc<AtomicUsize>, r: PacketBatchReceiver) -> JoinHandle<()> {
spawn(move || loop {
⋮----
if let Ok(packet_batch) = r.recv_timeout(timer) {
rvs.fetch_add(packet_batch.len(), Ordering::Relaxed);
⋮----
fn main() -> Result<()> {
let matches = Command::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.long("num-recv-sockets")
.value_name("NUM")
.takes_value(true)
.default_value("1")
.help("Use NUM receive sockets"),
⋮----
.long("num-producers")
⋮----
.default_value("4")
.help("Use this many producer threads."),
⋮----
.long("duration")
⋮----
.default_value("5")
.help("Run for this many seconds"),
⋮----
.get_matches();
let num_sockets = value_t_or_exit!(matches, "num-recv-sockets", usize);
let num_producers = value_t_or_exit!(matches, "num-producers", usize);
let duration_secs = value_t_or_exit!(matches, "duration", u64);
⋮----
let (_port, read_sockets) = multi_bind_in_range_with_config(
⋮----
.unwrap();
⋮----
.into_iter()
.map(|read_socket| {
⋮----
.set_read_timeout(Some(Duration::new(1, 0)))
⋮----
addr = read_socket.local_addr().unwrap();
let (packet_sender, packet_receiver) = unbounded();
let receiver = receiver(
"solRcvrBenStrmr".to_string(),
⋮----
exit.clone(),
⋮----
recycler.clone(),
stats.clone(),
⋮----
.unzip();
⋮----
.map(|_| producer(&addr, exit.clone()))
.collect();
⋮----
.map(|r_reader| sink(exit.clone(), rvs.clone(), r_reader))
⋮----
let start_val = rvs.load(Ordering::Relaxed);
sleep(duration);
let elapsed = start.elapsed().unwrap();
let end_val = rvs.load(Ordering::Relaxed);
let time = elapsed.as_secs() * 10_000_000_000 + u64::from(elapsed.subsec_nanos());
⋮----
println!("performance: {:?}", fcount / ftime);
exit.store(true, Ordering::Relaxed);
⋮----
t_reader.join()?;
⋮----
num_packets_sent += t_producer.join()?;
⋮----
println!("{num_packets_sent} total packets sent");
⋮----
t_sink.join()?;
⋮----
Ok(())

================
File: bench-streamer/.gitignore
================
/target/
/farf/

================
File: bench-streamer/Cargo.toml
================
[package]
name = "solana-bench-streamer"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
clap = { version = "3.1.5", features = ["cargo"] }
crossbeam-channel = { workspace = true }
solana-net-utils = { workspace = true }
solana-streamer = { workspace = true }
solana-version = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dependencies]
jemallocator = { workspace = true }

================
File: bench-tps/src/bench.rs
================
pub fn max_lamports_for_prioritization(compute_unit_price: &Option<ComputeUnitPrice>) -> u64 {
⋮----
.saturating_mul(COMPUTE_UNIT_PRICE_MULTIPLIER as u128),
⋮----
compute_unit_price.saturating_mul(TRANSFER_TRANSACTION_COMPUTE_UNIT as u128);
⋮----
.saturating_add(MICRO_LAMPORTS_PER_LAMPORT.saturating_sub(1) as u128)
.saturating_div(MICRO_LAMPORTS_PER_LAMPORT as u128);
u64::try_from(fee).unwrap_or(u64::MAX)
⋮----
fn get_transaction_loaded_accounts_data_size(enable_padding: bool) -> u32 {
⋮----
pub(crate) struct TimestampedTransaction {
⋮----
pub(crate) type SharedTransactions = Arc<RwLock<VecDeque<Vec<TimestampedTransaction>>>>;
struct KeypairChunks<'a> {
⋮----
/// Split input slice of keypairs into two sets of chunks of given size
    fn new(keypairs: &'a [Keypair], chunk_size: usize) -> Self {
⋮----
fn new(keypairs: &'a [Keypair], chunk_size: usize) -> Self {
⋮----
fn new_with_conflict_groups(
⋮----
for chunk in keypairs.chunks_exact(2 * chunk_size) {
source_keypair_chunks.push(chunk[..chunk_size].iter().collect());
dest_keypair_chunks.push(
⋮----
.flatten()
.take(chunk_size)
.collect(),
⋮----
struct TransactionChunkGenerator<'a, 'b, T: ?Sized> {
⋮----
fn new(
⋮----
nonce_keypairs.map(|nonce_keypairs| KeypairChunks::new(nonce_keypairs, chunk_size));
⋮----
fn generate(&mut self, blockhash: Option<&Hash>) -> Vec<TimestampedTransaction> {
let tx_count = self.account_chunks.source.len();
info!(
⋮----
generate_nonced_system_txs(
self.client.clone(),
⋮----
assert!(blockhash.is_some());
generate_system_txs(
⋮----
blockhash.unwrap(),
⋮----
let duration = signing_start.elapsed();
let ns = duration.as_secs() * 1_000_000_000 + u64::from(duration.subsec_nanos());
⋮----
datapoint_info!(
⋮----
fn advance(&mut self) {
self.account_chunks.dest[self.chunk_index].rotate_left(1);
⋮----
nonce_chunks.dest[self.chunk_index].rotate_left(1);
⋮----
self.chunk_index = (self.chunk_index + 1) % self.account_chunks.source.len();
⋮----
fn wait_for_target_slots_per_epoch<T>(target_slots_per_epoch: u64, client: &Arc<T>)
⋮----
info!("Waiting until epochs are {target_slots_per_epoch} slots long..");
⋮----
if let Ok(epoch_info) = client.get_epoch_info() {
⋮----
info!("Done epoch_info: {epoch_info:?}");
⋮----
sleep(Duration::from_secs(3));
⋮----
fn create_sampler_thread<T>(
⋮----
info!("Sampling TPS every {sample_period} second...");
let maxes = maxes.clone();
let client = client.clone();
⋮----
.name("solana-client-sample".to_string())
.spawn(move || {
sample_txs(exit_signal, &maxes, sample_period, &client);
⋮----
.unwrap()
⋮----
fn generate_chunked_transfers<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
// generate and send transactions for the specified duration
⋮----
while start.elapsed() < duration {
generate_txs(
⋮----
// In sustained mode, overlap the transfers with generation. This has higher average
// performance but lower peak performance in tested environments.
⋮----
// Ensure that we don't generate more transactions than we can handle.
while shared_txs.read().unwrap().len() > 2 * threads {
sleep(Duration::from_millis(1));
⋮----
while !shared_txs.read().unwrap().is_empty()
|| shared_tx_active_thread_count.load(Ordering::Relaxed) > 0
⋮----
chunk_generator.advance();
⋮----
fn create_sender_threads<T>(
⋮----
.map(|_| {
let exit_signal = exit_signal.clone();
let shared_txs = shared_txs.clone();
let shared_tx_active_thread_count = shared_tx_active_thread_count.clone();
let total_tx_sent_count = total_tx_sent_count.clone();
⋮----
let signatures_sender = signatures_sender.clone();
⋮----
.name("solana-client-sender".to_string())
⋮----
do_tx_transfers(
⋮----
.collect()
⋮----
pub fn do_bench_tps<T>(
⋮----
assert!(gen_keypairs.len() >= 2 * tx_count);
⋮----
client.clone(),
⋮----
nonce_keypairs.as_ref(),
⋮----
match client.get_transaction_count() {
⋮----
info!("Couldn't get transaction count: {err:?}");
sleep(Duration::from_secs(1));
⋮----
info!("Initial transaction count {first_tx_count}");
⋮----
let sample_thread = create_sampler_thread(&client, exit_signal.clone(), sample_period, &maxes);
⋮----
let blockhash = Arc::new(RwLock::new(get_latest_blockhash(client.as_ref())));
⋮----
let blockhash = blockhash.clone();
⋮----
let id = id.pubkey();
Some(
⋮----
.name("solana-blockhash-poller".to_string())
⋮----
poll_blockhash(&exit_signal, &blockhash, &client, &id);
⋮----
.unwrap(),
⋮----
let (log_transaction_service, signatures_sender) = create_log_transactions_service_and_sender(
⋮----
block_data_file.as_deref(),
transaction_data_file.as_deref(),
⋮----
let sender_threads = create_sender_threads(
⋮----
exit_signal.clone(),
⋮----
wait_for_target_slots_per_epoch(target_slots_per_epoch, &client);
⋮----
generate_chunked_transfers(
⋮----
exit_signal.store(true, Ordering::Relaxed);
info!("Waiting for sampler threads...");
if let Err(err) = sample_thread.join() {
info!("  join() failed with: {err:?}");
⋮----
info!("Waiting for transmit threads...");
⋮----
if let Err(err) = t.join() {
⋮----
info!("Waiting for blockhash thread...");
if let Err(err) = blockhash_thread.join() {
⋮----
info!("Waiting for log_transaction_service thread...");
if let Err(err) = log_transaction_service.join() {
⋮----
withdraw_durable_nonce_accounts(client.clone(), &gen_keypairs, &nonce_keypairs);
⋮----
let balance = client.get_balance(&id.pubkey()).unwrap_or(0);
metrics_submit_lamport_balance(balance);
compute_and_report_stats(
⋮----
&start.elapsed(),
total_tx_sent_count.load(Ordering::Relaxed),
⋮----
let r_maxes = maxes.read().unwrap();
r_maxes.first().unwrap().1.txs
⋮----
fn metrics_submit_lamport_balance(lamport_balance: u64) {
info!("Token balance: {lamport_balance}");
⋮----
fn generate_system_txs(
⋮----
source.iter().zip(dest.iter()).collect()
⋮----
dest.iter().zip(source.iter()).collect()
⋮----
.expect("ok for non-empty range");
(0..pairs.len())
⋮----
.sample(&mut rng)
.saturating_mul(COMPUTE_UNIT_PRICE_MULTIPLIER)
⋮----
ComputeUnitPrice::Fixed(compute_unit_price) => vec![*compute_unit_price; pairs.len()],
⋮----
pairs.iter().zip(compute_unit_prices.iter()).collect();
⋮----
.par_iter()
.map(|((from, to), compute_unit_price)| {
let compute_unit_price = Some(**compute_unit_price);
⋮----
transaction: transfer_with_compute_unit_price_and_padding(
⋮----
&to.pubkey(),
⋮----
timestamp: Some(timestamp()),
⋮----
.map(|(from, to)| TimestampedTransaction {
⋮----
fn transfer_with_compute_unit_price_and_padding(
⋮----
let from_pubkey = from_keypair.pubkey();
⋮----
wrap_instruction(
⋮----
vec![],
⋮----
.expect("Could not create padded instruction")
⋮----
let mut instructions = vec![];
⋮----
instructions.push(
⋮----
get_transaction_loaded_accounts_data_size(instruction_padding_config.is_some()),
⋮----
instructions.push(instruction);
if instruction_padding_config.is_some() {
instructions.push(ComputeBudgetInstruction::set_compute_unit_limit(
⋮----
instructions.extend_from_slice(&[
⋮----
let message = Message::new(&instructions, Some(&from_pubkey));
⋮----
fn get_nonce_accounts<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
// get_multiple_accounts supports maximum MAX_MULTIPLE_ACCOUNTS pubkeys in request
assert!(nonce_pubkeys.len() <= MAX_MULTIPLE_ACCOUNTS);
⋮----
match client.get_multiple_accounts(nonce_pubkeys) {
⋮----
info!("Couldn't get durable nonce account: {err:?}");
⋮----
fn get_nonce_blockhashes<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
let num_accounts = nonce_pubkeys.len();
let mut blockhashes = vec![Hash::default(); num_accounts];
⋮----
while !unprocessed.is_empty() {
⋮----
request_pubkeys.push(nonce_pubkeys[*i]);
request_indexes.push(*i);
⋮----
let num_unprocessed_before = unprocessed.len();
⋮----
.chunks(MAX_MULTIPLE_ACCOUNTS)
.flat_map(|pubkeys| get_nonce_accounts(client, pubkeys))
.collect();
for (account, index) in accounts.iter().zip(request_indexes.iter()) {
⋮----
let nonce_data = nonce_utils::data_from_account(nonce_account).unwrap();
blockhashes[*index] = nonce_data.blockhash();
unprocessed.remove(index);
⋮----
let num_unprocessed_after = unprocessed.len();
debug!(
⋮----
request_pubkeys.clear();
request_indexes.clear();
⋮----
fn nonced_transfer_with_padding(
⋮----
Some(&from_pubkey),
⋮----
&nonce_authority.pubkey(),
⋮----
fn generate_nonced_system_txs<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
let length = source.len();
⋮----
.iter()
.map(|keypair| keypair.pubkey())
⋮----
let blockhashes: Vec<Hash> = get_nonce_blockhashes(&client, &pubkeys);
⋮----
transactions.push(TimestampedTransaction {
transaction: nonced_transfer_with_padding(
⋮----
&dest[i].pubkey(),
⋮----
&source_nonce[i].pubkey(),
⋮----
let pubkeys: Vec<Pubkey> = dest_nonce.iter().map(|keypair| keypair.pubkey()).collect();
⋮----
&source[i].pubkey(),
⋮----
&dest_nonce[i].pubkey(),
⋮----
fn generate_txs<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
chunk_generator.generate(None)
⋮----
let blockhash = blockhash.read().map(|x| *x).ok();
chunk_generator.generate(blockhash.as_ref())
⋮----
let sz = transactions.len() / threads;
let chunks: Vec<_> = transactions.chunks(sz).collect();
⋮----
let mut shared_txs_wl = shared_txs.write().unwrap();
⋮----
shared_txs_wl.push_back(chunk.to_vec());
⋮----
fn get_new_latest_blockhash<T: TpsClient + ?Sized>(
⋮----
while start.elapsed().as_secs() < 5 {
if let Ok(new_blockhash) = client.get_latest_blockhash() {
⋮----
return Some(new_blockhash);
⋮----
debug!("Got same blockhash ({blockhash:?}), will retry...");
sleep(Duration::from_millis(DEFAULT_MS_PER_SLOT / 2));
⋮----
fn poll_blockhash<T: TpsClient + ?Sized>(
⋮----
let old_blockhash = *blockhash.read().unwrap();
if let Some(new_blockhash) = get_new_latest_blockhash(client, &old_blockhash) {
*blockhash.write().unwrap() = new_blockhash;
⋮----
if blockhash_last_updated.elapsed().as_secs() > 120 {
eprintln!("Blockhash is stuck");
exit(1)
} else if blockhash_last_updated.elapsed().as_secs() > 30
&& last_error_log.elapsed().as_secs() >= 1
⋮----
error!("Blockhash is not updating");
⋮----
let balance = client.get_balance(id).unwrap_or(0);
⋮----
if exit_signal.load(Ordering::Relaxed) {
⋮----
sleep(Duration::from_millis(50));
⋮----
fn do_tx_transfers<T: TpsClient + ?Sized>(
⋮----
let mut last_sent_time = timestamp();
⋮----
sleep(Duration::from_millis(thread_batch_sleep_ms as u64));
⋮----
let mut shared_txs_wl = shared_txs.write().expect("write lock in do_tx_transfers");
shared_txs_wl.pop_front()
⋮----
shared_tx_thread_count.fetch_add(1, Ordering::Relaxed);
let num_txs = txs.len();
info!("Transferring 1 unit {num_txs} times...");
⋮----
let now = timestamp();
// Transactions without durable nonce that are too old will be rejected by the cluster Don't bother
⋮----
signatures.push(tx.transaction.signatures[0]);
transactions.push(tx.transaction);
compute_unit_prices.push(tx.compute_unit_price);
⋮----
if let Err(error) = signatures_sender.send(TransactionInfoBatch {
⋮----
error!(
⋮----
if let Err(error) = client.send_batch(transactions) {
warn!("send_batch_sync in do_tx_transfers failed: {error}");
⋮----
last_sent_time = timestamp();
⋮----
shared_txs_wl.clear();
⋮----
shared_tx_thread_count.fetch_add(-1, Ordering::Relaxed);
total_tx_sent_count.fetch_add(num_txs, Ordering::Relaxed);
⋮----
fn compute_and_report_stats(
⋮----
// Compute/report stats
⋮----
info!(" Node address        |       Max TPS | Total Transactions");
info!("---------------------+---------------+--------------------");
for (sock, stats) in maxes.read().unwrap().iter() {
⋮----
let num_nodes_with_tps = maxes.read().unwrap().len() - nodes_with_zero_tps;
⋮----
info!("\nAverage max TPS: {average_max:.2}, {nodes_with_zero_tps} nodes had 0 TPS");
⋮----
pub fn generate_and_fund_keypairs<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
let rent = client.get_minimum_balance_for_rent_exemption(0)?;
⋮----
info!("Creating {keypair_count} keypairs...");
let (mut keypairs, extra) = generate_keypairs(funding_key, keypair_count as u64);
fund_keypairs(
⋮----
keypairs.truncate(keypair_count);
Ok(keypairs)
⋮----
pub fn fund_keypairs<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
info!("Get lamports...");
let first_key = keypairs[0].pubkey();
let first_keypair_balance = client.get_balance(&first_key).unwrap_or(0);
let last_key = keypairs[keypairs.len() - 1].pubkey();
let last_keypair_balance = client.get_balance(&last_key).unwrap_or(0);
⋮----
vec![AccountMeta::new(Pubkey::new_unique(), true)],
⋮----
&client.get_latest_blockhash().unwrap(),
⋮----
let max_fee = client.get_fee_for_message(&single_sig_message).unwrap();
⋮----
let total_keypairs = keypairs.len() as u64 + 1;
⋮----
let funding_key_balance = client.get_balance(&funding_key.pubkey()).unwrap_or(0);
⋮----
let latest_blockhash = get_latest_blockhash(client.as_ref());
⋮----
.request_airdrop_with_blockhash(
&funding_key.pubkey(),
⋮----
.is_err()
⋮----
return Err(TpsClientError::AirdropFailure);
⋮----
.then(|| get_transaction_loaded_accounts_data_size(enable_padding));
fund_keys(
⋮----
Ok(())
⋮----
mod tests {
⋮----
fn bank_with_all_features(
⋮----
bank.wrap_with_bank_forks_for_tests()
⋮----
fn test_bench_tps_bank_client() {
let (genesis_config, id) = create_genesis_config(10_000 * LAMPORTS_PER_SOL);
let (bank, _bank_forks) = bank_with_all_features(&genesis_config);
⋮----
generate_and_fund_keypairs(client.clone(), &config.id, keypair_count, 20, false, false)
.unwrap();
do_bench_tps(client, config, keypairs, None);
⋮----
fn test_bench_tps_fund_keys() {
⋮----
let rent = client.get_minimum_balance_for_rent_exemption(0).unwrap();
⋮----
generate_and_fund_keypairs(client.clone(), &id, keypair_count, lamports, false, false)
⋮----
assert_eq!(
⋮----
fn test_bench_tps_fund_keys_with_fees() {
let (mut genesis_config, id) = create_genesis_config(10_000 * LAMPORTS_PER_SOL);
⋮----
assert_eq!(client.get_balance(&kp.pubkey()).unwrap(), lamports + rent);
⋮----
fn test_bench_tps_create_durable_nonce() {
⋮----
let nonce_keypairs = generate_durable_nonce_accounts(client.clone(), &authority_keypairs);
⋮----
.get_minimum_balance_for_rent_exemption(State::size())
⋮----
withdraw_durable_nonce_accounts(client, &authority_keypairs, &nonce_keypairs)
⋮----
fn test_bench_tps_key_chunks_new() {
⋮----
.take(num_keypairs)
⋮----
fn test_bench_tps_key_chunks_new_with_conflict_groups() {

================
File: bench-tps/src/cli.rs
================
pub enum ExternalClientType {
⋮----
pub struct InstructionPaddingConfig {
⋮----
pub enum ComputeUnitPrice {
⋮----
pub struct Config {
⋮----
impl Eq for Config {}
impl Default for Config {
fn default() -> Config {
⋮----
pub fn build_args<'a>(version: &'_ str) -> App<'a, '_> {
App::new(crate_name!())
.about(crate_description!())
.version(version)
.arg({
⋮----
.short("C")
.long("config")
.value_name("FILEPATH")
.takes_value(true)
.global(true)
.help("Configuration file to use");
⋮----
arg.default_value(config_file)
⋮----
.arg(
⋮----
.short("u")
.long("url")
.value_name("URL_OR_MONIKER")
⋮----
.validator(is_url_or_moniker)
.help(
⋮----
.long("ws")
.value_name("URL")
⋮----
.validator(is_url)
.help("WebSocket URL for the solana cluster"),
⋮----
.short("d")
.long("faucet")
.value_name("HOST:PORT")
⋮----
.hidden(hidden_unless_forced())
.help("Deprecated. BenchTps no longer queries the faucet directly"),
⋮----
.short("i")
.long("identity")
.value_name("PATH")
⋮----
.help("Deprecated. Use --authority instead"),
⋮----
.short("a")
.long("authority")
⋮----
.short("t")
.long("threads")
.value_name("NUM")
⋮----
.help("Number of threads"),
⋮----
.long("duration")
.value_name("SECS")
⋮----
.help("Seconds to run benchmark, then exit; default is forever"),
⋮----
.arg(Arg::with_name("sustained").long("sustained").help(
⋮----
.long("tx-count")
.alias("tx_count")
⋮----
.help("Number of transactions to send per batch"),
⋮----
.long("keypair-multiplier")
⋮----
.help("Multiply by transaction count to determine number of keypairs to create"),
⋮----
.short("z")
.long("thread-batch-sleep-ms")
⋮----
.help("Per-thread-per-iteration sleep in ms"),
⋮----
.long("write-client-keys")
.value_name("FILENAME")
⋮----
.help("Generate client keys and stakes and write the list to YAML file"),
⋮----
.long("read-client-keys")
⋮----
.help("Read client keys and stakes from the YAML file"),
⋮----
.long("target-lamports-per-signature")
.value_name("LAMPORTS")
⋮----
.long("num-lamports-per-account")
⋮----
.help("Number of lamports per account."),
⋮----
.long("target-slots-per-epoch")
.value_name("SLOTS")
⋮----
.help("Wait until epochs are this many slots long."),
⋮----
.long("use-rpc-client")
.conflicts_with("tpu_client")
.takes_value(false)
.help("Submit transactions with a RpcClient"),
⋮----
.long("use-tpu-client")
.conflicts_with("rpc_client")
⋮----
.help("Submit transactions with a TpuClient"),
⋮----
.long("tpu-connection-pool-size")
⋮----
.long("compute-unit-price")
⋮----
.validator(|s| is_within_range(s, 0..))
.help("Sets constant compute-unit-price to transfer transactions"),
⋮----
.long("use-randomized-compute-unit-price")
⋮----
.conflicts_with("compute_unit_price")
.help("Sets random compute-unit-price in range [0..100] to transfer transactions"),
⋮----
.long("skip-tx-account-data-size")
⋮----
.conflicts_with("instruction_padding_data_size")
.help("Skips setting the account data size for each transaction"),
⋮----
.long("use-durable-nonce")
.help("Use durable transaction nonce instead of recent blockhash"),
⋮----
.long("instruction-padding-program-id")
.requires("instruction_padding_data_size")
⋮----
.value_name("PUBKEY")
⋮----
.long("instruction-padding-data-size")
⋮----
.long("num-conflict-groups")
⋮----
.validator(|arg| is_within_range(arg, 1..))
⋮----
.long("bind-address")
.value_name("HOST")
⋮----
.validator(solana_net_utils::is_host)
.requires("client_node_id")
.help("IP address to use with connection cache"),
⋮----
.long("client-node-id")
⋮----
.requires("json_rpc_url")
.validator(is_keypair)
⋮----
.long("commitment-config")
⋮----
.possible_values(&["processed", "confirmed", "finalized"])
.default_value("confirmed")
.help("Block commitment config for getting latest blockhash"),
⋮----
.long("block-data-file")
⋮----
.help("File to save block statistics relevant to the submitted transactions."),
⋮----
.long("transaction-data-file")
⋮----
pub fn parse_args(matches: &ArgMatches) -> Result<Config, &'static str> {
⋮----
let config = if let Some(config_file) = matches.value_of("config_file") {
solana_cli_config::Config::load(config_file).unwrap_or_default()
⋮----
matches.value_of("json_rpc_url").unwrap_or(""),
⋮----
matches.value_of("websocket_url").unwrap_or(""),
⋮----
if matches.is_present("identity") {
eprintln!("Warning: --identity is deprecated. Please use --authority");
⋮----
.value_of("authority")
.or(matches.value_of("identity"))
.unwrap_or(""),
⋮----
if let Ok(id) = read_keypair_file(id_path) {
⋮----
} else if matches.is_present("identity") || matches.is_present("authority") {
return Err("could not parse authority path");
⋮----
if matches.is_present("rpc_client") {
⋮----
if let Some(v) = matches.value_of("tpu_connection_pool_size") {
⋮----
.to_string()
⋮----
.map_err(|_| "can't parse tpu-connection-pool-size")?;
⋮----
if let Some(t) = matches.value_of("threads") {
args.threads = t.to_string().parse().map_err(|_| "can't parse threads")?;
⋮----
if let Some(duration) = matches.value_of("duration") {
⋮----
.parse()
.map_err(|_| "can't parse duration")?;
⋮----
if let Some(s) = matches.value_of("tx_count") {
args.tx_count = s.to_string().parse().map_err(|_| "can't parse tx_count")?;
⋮----
if let Some(s) = matches.value_of("keypair_multiplier") {
⋮----
.map_err(|_| "can't parse keypair-multiplier")?;
⋮----
return Err("args.keypair_multiplier must be greater than or equal to 2");
⋮----
if let Some(t) = matches.value_of("thread-batch-sleep-ms") {
⋮----
.map_err(|_| "can't parse thread-batch-sleep-ms")?;
⋮----
args.sustained = matches.is_present("sustained");
if let Some(s) = matches.value_of("write-client-keys") {
⋮----
args.client_ids_and_stake_file = s.to_string();
⋮----
if let Some(s) = matches.value_of("read-client-keys") {
assert!(!args.write_to_client_file);
⋮----
if let Some(v) = matches.value_of("target_lamports_per_signature") {
⋮----
.map_err(|_| "can't parse target-lamports-per-signature")?;
⋮----
if let Some(v) = matches.value_of("num_lamports_per_account") {
⋮----
.map_err(|_| "can't parse num-lamports-per-account")?;
⋮----
if let Some(t) = matches.value_of("target_slots_per_epoch") {
⋮----
.map_err(|_| "can't parse target-slots-per-epoch")?;
⋮----
if let Some(str) = matches.value_of("compute_unit_price") {
args.compute_unit_price = Some(ComputeUnitPrice::Fixed(
str.parse().map_err(|_| "can't parse compute-unit-price")?,
⋮----
if matches.is_present("use_randomized_compute_unit_price") {
args.compute_unit_price = Some(ComputeUnitPrice::Random);
⋮----
if matches.is_present("skip_tx_account_data_size") {
⋮----
if matches.is_present("use_durable_nonce") {
⋮----
if let Some(data_size) = matches.value_of("instruction_padding_data_size") {
⋮----
.value_of("instruction_padding_program_id")
.map(|target_str| target_str.parse().unwrap())
.unwrap_or_else(|| spl_instruction_padding_interface::ID);
⋮----
.map_err(|_| "Can't parse padded instruction data size")?;
args.instruction_padding_config = Some(InstructionPaddingConfig {
⋮----
if let Some(num_conflict_groups) = matches.value_of("num_conflict_groups") {
⋮----
.map_err(|_| "Can't parse num-conflict-groups")?;
args.num_conflict_groups = Some(parsed_num_conflict_groups);
⋮----
if let Some(addr) = matches.value_of("bind_address") {
⋮----
solana_net_utils::parse_host(addr).map_err(|_| "Failed to parse bind-address")?;
⋮----
if let Some(client_node_id_filename) = matches.value_of("client_node_id") {
let client_node_id = read_keypair_file(client_node_id_filename).map_err(|_| "")?;
args.client_node_id = Some(client_node_id);
⋮----
args.commitment_config = value_t_or_exit!(matches, "commitment_config", CommitmentConfig);
args.block_data_file = matches.value_of("block_data_file").map(|s| s.to_string());
⋮----
.value_of("transaction_data_file")
.map(|s| s.to_string());
Ok(args)
⋮----
mod tests {
⋮----
fn write_tmp_keypair(out_dir: &TempDir) -> (Keypair, String) {
⋮----
.path()
.join(format!("keypair_file-{}", keypair.pubkey()));
let keypair_file_name = file_path.into_os_string().into_string().unwrap();
write_keypair_file(&keypair, &keypair_file_name).unwrap();
⋮----
fn test_cli_parse() {
let out_dir = tempdir().unwrap();
let (keypair, keypair_file_name) = write_tmp_keypair(&out_dir);
let matches = build_args("1.0.0").get_matches_from(vec![
⋮----
let actual = parse_args(&matches).unwrap();
assert_eq!(
⋮----
let keypair = read_keypair_file(&keypair_file_name).unwrap();
⋮----
let (client_id, client_id_file_name) = write_tmp_keypair(&out_dir);

================
File: bench-tps/src/keypairs.rs
================
pub fn get_keypairs<T>(
⋮----
let file = File::open(path).unwrap();
info!("Reading {client_ids_and_stake_file}");
let accounts: HashMap<String, Base64Account> = serde_yaml::from_reader(file).unwrap();
let mut keypairs = vec![];
⋮----
.into_iter()
.for_each(|(keypair, primordial_account)| {
let bytes: Vec<u8> = serde_json::from_str(keypair.as_str()).unwrap();
keypairs.push(Keypair::try_from(bytes.as_ref()).unwrap());
⋮----
if keypairs.len() < keypair_count {
eprintln!(
⋮----
exit(1);
⋮----
keypairs.sort_by_key(|x| x.pubkey().to_string());
fund_keypairs(
⋮----
keypairs.len().saturating_sub(keypair_count) as u64,
⋮----
.unwrap_or_else(|e| {
eprintln!("Error could not fund keys: {e:?}");
⋮----
generate_and_fund_keypairs(

================
File: bench-tps/src/lib.rs
================
pub mod bench;
pub mod cli;
pub mod keypairs;
mod log_transaction_service;
mod perf_utils;
mod rpc_with_retry_utils;
pub mod send_batch;

================
File: bench-tps/src/log_transaction_service.rs
================
pub(crate) struct TransactionInfoBatch {
⋮----
pub(crate) type SignatureBatchSender = Sender<TransactionInfoBatch>;
pub(crate) struct LogTransactionService {
⋮----
pub(crate) fn create_log_transactions_service_and_sender<Client>(
⋮----
if data_file_provided(block_data_file, transaction_data_file) {
let (sender, receiver) = unbounded();
⋮----
(Some(log_tx_service), Some(sender))
⋮----
// How many blocks to process during one iteration.
// The time to process blocks is dominated by get_block calls.
// Each call takes slightly less time than slot.
⋮----
// How often process blocks.
⋮----
// Max age for transaction in the transaction map, older transactions are cleaned up and marked as timeout.
⋮----
// Map used to filter submitted transactions.
⋮----
struct TransactionSendInfo {
⋮----
type MapSignatureToTxInfo = HashMap<Signature, TransactionSendInfo>;
type SignatureBatchReceiver = Receiver<TransactionInfoBatch>;
impl LogTransactionService {
fn new<Client>(
⋮----
if !data_file_provided(block_data_file, transaction_data_file) {
panic!(
⋮----
let client = client.clone();
⋮----
.name("LogTransactionService".to_string())
.spawn(move || {
⋮----
.expect("LogTransactionService should have started successfully.");
⋮----
pub fn join(self) -> thread::Result<()> {
self.thread_handler.join()
⋮----
fn run<Client>(
⋮----
// used to request blocks data and only confirmed makes sense in this context.
⋮----
let block_processing_timer_receiver = tick(Duration::from_millis(PROCESS_BLOCKS_EVERY_MS));
let mut start_slot = get_slot_with_retry(&client, commitment).expect(
⋮----
select! {
⋮----
/// Download and process the blocks.
    /// Returns the time when the last processed block has been confirmed or now().
⋮----
/// Returns the time when the last processed block has been confirmed or now().
    fn process_blocks<Client>(
⋮----
fn process_blocks<Client>(
⋮----
encoding: Some(UiTransactionEncoding::Base64),
transaction_details: Some(TransactionDetails::Full),
rewards: Some(true),
commitment: Some(commitment),
max_supported_transaction_version: Some(0),
⋮----
.iter()
.map(|slot| client.get_block_with_config(*slot, rpc_block_config));
let num_blocks = blocks.len();
⋮----
for (block, slot) in blocks.zip(&block_slots) {
⋮----
if block_time.is_some() {
⋮----
measure_process_blocks.stop();
let time_process_blocks_us = measure_process_blocks.as_us();
info!("Time to process {num_blocks} blocks: {time_process_blocks_us}us.");
last_block_time.unwrap_or_else(Utc::now)
⋮----
fn process_block(
⋮----
.as_ref()
.expect("Rewards should be part of the block information.");
⋮----
.find(|r| r.reward_type == Some(RewardType::Fee))
.map_or("".to_string(), |x| x.pubkey.clone());
⋮----
warn!("Empty block: {slot}");
⋮----
let Some(transaction) = transaction.decode() else {
⋮----
.map_or(0, |meta| match meta.compute_units_consumed {
⋮----
total_cu_consumed = total_cu_consumed.saturating_add(cu_consumed);
⋮----
}) = signature_to_tx_info.remove(signature)
⋮----
num_bench_tps_transactions = num_bench_tps_transactions.saturating_add(1);
bench_tps_cu_consumed = bench_tps_cu_consumed.saturating_add(cu_consumed);
tx_log_writer.write(
Some(block.blockhash.clone()),
Some(slot_leader.clone()),
⋮----
Some(slot),
⋮----
meta.as_ref(),
⋮----
block_log_writer.write(
block.blockhash.clone(),
⋮----
transactions.len(),
⋮----
block.block_time.map(|time| {
Utc.timestamp_opt(time, 0)
.latest()
.expect("valid timestamp")
⋮----
fn clean_transaction_map(
⋮----
signature_to_tx_info.retain(|signature, tx_info| {
let duration_since_sent = last_block_time.signed_duration_since(tx_info.sent_at);
let is_timeout_tx = duration_since_sent.num_milliseconds() > REMOVE_TIMEOUT_TX_EVERY_MS;
⋮----
fn data_file_provided(block_data_file: Option<&str>, transaction_data_file: Option<&str>) -> bool {
block_data_file.is_some() || transaction_data_file.is_some()
⋮----
type CsvFileWriter = csv::Writer<File>;
⋮----
struct BlockData {
⋮----
struct BlockLogWriter {
⋮----
impl BlockLogWriter {
fn new(block_data_file: Option<&str>) -> Self {
let block_log_writer = block_data_file.map(|block_data_file| {
⋮----
.expect("Application should be able to create a file."),
⋮----
fn write(
⋮----
block_time: block_time.map(|time| {
⋮----
.expect("timestamp should be valid")
⋮----
let _ = block_log_writer.serialize(block_data);
⋮----
fn flush(&mut self) {
⋮----
let _ = block_log_writer.flush();
⋮----
struct TransactionData {
⋮----
struct TransactionLogWriter {
⋮----
impl TransactionLogWriter {
fn new(transaction_data_file: Option<&str>) -> Self {
let transaction_log_writer = transaction_data_file.map(|transaction_data_file| {
⋮----
signature: signature.to_string(),
sent_at: Some(sent_at),
⋮----
successful: meta.as_ref().is_some_and(|m| m.status.is_ok()),
⋮----
.and_then(|m| m.err.as_ref().map(|x| x.to_string())),
⋮----
compute_unit_price: compute_unit_price.unwrap_or(0),
⋮----
let _ = transaction_log_writer.serialize(tx_data);
⋮----
let _ = transaction_log_writer.flush();

================
File: bench-tps/src/main.rs
================
fn find_node_activated_stake(
⋮----
let vote_accounts = rpc_client.get_vote_accounts();
⋮----
error!("Failed to get vote accounts, error: {error}");
return Err(());
⋮----
let vote_accounts = vote_accounts.unwrap();
⋮----
.iter()
.map(|vote_account| vote_account.activated_stake)
.sum();
let node_id_as_str = node_id.to_string();
⋮----
.find(|&vote_account| vote_account.node_pubkey == node_id_as_str);
⋮----
Some(value) => Ok((value.activated_stake, total_active_stake)),
⋮----
error!("Failed to find stake for requested node");
Err(())
⋮----
fn create_connection_cache(
⋮----
if client_node_id.is_none() {
⋮----
json_rpc_url.to_string(),
⋮----
let client_node_id = client_node_id.unwrap();
⋮----
find_node_activated_stake(rpc_client, client_node_id.pubkey()).unwrap_or_default();
info!("Stake for specified client_node_id: {stake}, total stake: {total_stake}");
⋮----
(client_node_id.pubkey(), stake),
⋮----
Some((client_node_id, bind_address)),
Some((&staked_nodes, &client_node_id.pubkey())),
⋮----
fn create_client(
⋮----
.unwrap_or_else(|err| {
eprintln!("Could not create TpuClient {err:?}");
exit(1);
⋮----
fn main() {
⋮----
let matches = cli::build_args(solana_version::version!()).get_matches();
⋮----
eprintln!("{error}");
⋮----
info!("Generating {keypair_count} keypairs");
let (keypairs, _) = generate_keypairs(id, keypair_count as u64);
let num_accounts = keypairs.len() as u64;
⋮----
.saturating_add(max_lamports_for_prioritization(compute_unit_price));
⋮----
(NUM_SIGNATURES_FOR_TXS * max_fee).div_ceil(num_accounts) + num_lamports_per_account;
⋮----
keypairs.iter().for_each(|keypair| {
accounts.insert(
serde_json::to_string(&keypair.to_bytes().to_vec()).unwrap(),
⋮----
owner: system_program::id().to_string(),
⋮----
info!("Writing {client_ids_and_stake_file}");
let serialized = serde_yaml::to_string(&accounts).unwrap();
⋮----
let mut file = File::create(path).unwrap();
file.write_all(b"---\n").unwrap();
file.write_all(&serialized.into_bytes()).unwrap();
⋮----
let connection_cache = create_connection_cache(
⋮----
client_node_id.as_ref(),
⋮----
let client = create_client(
⋮----
info!(
⋮----
.get_account(&instruction_padding_config.program_id)
.expect(
⋮----
let keypairs = get_keypairs(
client.clone(),
⋮----
instruction_padding_config.is_some(),
⋮----
Some(generate_durable_nonce_accounts(client.clone(), &keypairs))
⋮----
do_bench_tps(client, cli_config, keypairs, nonce_keypairs);

================
File: bench-tps/src/perf_utils.rs
================
pub struct SampleStats {
⋮----
pub fn sample_txs<T>(
⋮----
.get_transaction_count_with_commitment(CommitmentConfig::processed())
.expect("transaction count");
⋮----
total_elapsed = start_time.elapsed();
let elapsed = now.elapsed();
⋮----
match client.get_transaction_count_with_commitment(CommitmentConfig::processed()) {
⋮----
info!("Couldn't get transaction count {e:?}");
sleep(Duration::from_secs(sample_period));
⋮----
info!("Expected txs({txs}) >= last_txs({last_txs})");
⋮----
let tps = sample_txs as f32 / elapsed.as_secs_f32();
⋮----
info!(
⋮----
if exit_signal.load(Ordering::Relaxed) {
⋮----
sample_stats.write().unwrap().push((client.addr(), stats));

================
File: bench-tps/src/rpc_with_retry_utils.rs
================
fn call_rpc_with_retry<Func, Data>(f: Func, retry_warning: &str) -> TpsClientResult<Data>
⋮----
match f() {
⋮----
return Ok(slot);
⋮----
return Err(error);
⋮----
warn!("{retry_warning}: {error}, retry.");
sleep(Duration::from_millis(RETRY_EVERY_MS));
⋮----
pub(crate) fn get_slot_with_retry<Client>(
⋮----
call_rpc_with_retry(
|| client.get_slot_with_commitment(commitment),
⋮----
pub(crate) fn get_blocks_with_retry<Client>(
⋮----
|| client.get_blocks_with_commitment(start_slot, end_slot, commitment),

================
File: bench-tps/src/send_batch.rs
================
pub fn get_latest_blockhash<T: TpsClient + ?Sized>(client: &T) -> Hash {
⋮----
match client.get_latest_blockhash() {
⋮----
info!("Couldn't get last blockhash: {err:?}");
sleep(Duration::from_secs(1));
⋮----
pub fn generate_keypairs(seed_keypair: &Keypair, count: u64) -> (Vec<Keypair>, u64) {
⋮----
seed.copy_from_slice(&seed_keypair.to_bytes()[..32]);
⋮----
(rnd.gen_n_keypairs(total_keys), extra)
⋮----
pub fn fund_keys<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
let mut funded: Vec<&Keypair> = vec![source];
⋮----
let mut not_funded: Vec<&Keypair> = dests.iter().collect();
while !not_funded.is_empty() {
// Build to fund list and prepare funding sources for next iteration
let mut new_funded: Vec<&Keypair> = vec![];
let mut to_fund: Vec<(&Keypair, Vec<(Pubkey, u64)>)> = vec![];
⋮----
let start = not_funded.len() - MAX_SPENDS_PER_TX as usize;
let dests: Vec<_> = not_funded.drain(start..).collect();
let spends: Vec<_> = dests.iter().map(|k| (k.pubkey(), to_lamports)).collect();
to_fund.push((f, spends));
new_funded.extend(dests.into_iter());
⋮----
to_fund.chunks(FUND_CHUNK_LEN).for_each(|chunk| {
Vec::<(&Keypair, Transaction)>::with_capacity(chunk.len()).fund(
⋮----
info!("funded: {} left: {}", new_funded.len(), not_funded.len());
⋮----
pub fn generate_durable_nonce_accounts<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
.get_minimum_balance_for_rent_exemption(State::size())
.unwrap();
⋮----
let count = authority_keypairs.len();
let (mut nonce_keypairs, _extra) = generate_keypairs(seed_keypair, count as u64);
nonce_keypairs.truncate(count);
info!("Creating {count} nonce accounts...");
⋮----
.iter()
.zip(nonce_keypairs.iter())
.map(|x| NonceCreateSigners(x.0, x.1))
.collect();
⋮----
NonceCreateContainer::with_capacity(chunk.len())
.create_accounts(&client, chunk, nonce_rent);
⋮----
pub fn withdraw_durable_nonce_accounts<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
.map(|x| NonceWithdrawSigners(x.0, x.1.pubkey()))
⋮----
to_withdraw.chunks(FUND_CHUNK_LEN).for_each(|chunk| {
NonceWithdrawContainer::with_capacity(chunk.len()).withdraw_accounts(&client, chunk);
⋮----
// Size of the chunk of transactions
// try to transfer a "few" at a time with recent blockhash
// assume 4MB network buffers, and 512 byte packets
⋮----
fn verify_funding_transfer<T: TpsClient + ?Sized>(
⋮----
for a in &tx.message().account_keys[1..] {
match client.get_balance_with_commitment(a, CommitmentConfig::processed()) {
⋮----
Err(err) => error!("failed to get balance {err:?}"),
⋮----
/// Helper trait to encapsulate common logic for sending transactions batch
///
⋮----
///
trait SendBatchTransactions<'a, T: Sliceable + Send + Sync> {
⋮----
trait SendBatchTransactions<'a, T: Sliceable + Send + Sync> {
⋮----
trait Sliceable {
⋮----
fn make<V: Send + Sync, F: Fn(&V) -> (T, Transaction) + Send + Sync>(
⋮----
let txs: Vec<(T, Transaction)> = chunk.par_iter().map(create_transaction).collect();
make_txs.stop();
debug!("make {} unsigned txs: {}us", txs.len(), make_txs.as_us());
self.extend(txs);
⋮----
fn send_transactions<C, F>(&mut self, client: &Arc<C>, to_lamports: u64, log_progress: F)
⋮----
while !self.is_empty() {
log_progress(tries, self.len());
let blockhash = get_latest_blockhash(client.as_ref());
self.sign(blockhash);
self.send(client);
⋮----
self.verify(client, to_lamports);
⋮----
info!("transactions sent in {tries} tries");
⋮----
fn sign(&mut self, blockhash: Hash) {
⋮----
self.par_iter_mut().for_each(|(k, tx)| {
tx.sign(&k.as_slice(), blockhash);
⋮----
sign_txs.stop();
debug!("sign {} txs: {}us", self.len(), sign_txs.as_us());
⋮----
fn send<C: TpsClient + ?Sized>(&self, client: &Arc<C>) {
⋮----
let batch: Vec<_> = self.iter().map(|(_keypair, tx)| tx.clone()).collect();
let result = client.send_batch(batch);
send_txs.stop();
if result.is_err() {
debug!("Failed to send batch {result:?}");
⋮----
debug!("send {} {}", self.len(), send_txs);
⋮----
fn verify<C: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
let starting_txs = self.len();
⋮----
// Only loop multiple times for small (quick) transaction batches
⋮----
let time = time.clone();
⋮----
let client = client.clone();
⋮----
.par_iter()
.filter_map(move |(k, tx)| {
let pubkey = k.get_pubkey();
if too_many_failures.load(Ordering::Relaxed) {
⋮----
let verified = if verify_funding_transfer(&client, tx, to_lamports) {
verified_txs.fetch_add(1, Ordering::Relaxed);
Some(pubkey)
⋮----
failed_verify.fetch_add(1, Ordering::Relaxed);
⋮----
let verified_txs = verified_txs.load(Ordering::Relaxed);
let failed_verify = failed_verify.load(Ordering::Relaxed);
let remaining_count = starting_txs.saturating_sub(verified_txs + failed_verify);
⋮----
too_many_failures.store(true, Ordering::Relaxed);
warn!(
⋮----
let mut time_l = time.lock().unwrap();
if time_l.elapsed().as_secs() > 2 {
info!(
⋮----
self.retain(|(k, _)| !verified_set.contains(&k.get_pubkey()));
if self.is_empty() {
⋮----
info!("Looping verification");
⋮----
sleep(Duration::from_millis(100));
⋮----
type FundingSigners<'a> = &'a Keypair;
type FundingChunk<'a> = [(FundingSigners<'a>, Vec<(Pubkey, u64)>)];
type FundingContainer<'a> = Vec<(FundingSigners<'a>, Transaction)>;
impl<'a> Sliceable for FundingSigners<'a> {
type Slice = [FundingSigners<'a>; 1];
fn as_slice(&self) -> Self::Slice {
⋮----
fn get_pubkey(&self) -> Pubkey {
self.pubkey()
⋮----
trait FundingTransactions<'a>: SendBatchTransactions<'a, FundingSigners<'a>> {
⋮----
fn fund<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
self.make(to_fund, |(k, t)| -> (FundingSigners<'a>, Transaction) {
let mut instructions = system_instruction::transfer_many(&k.pubkey(), t);
⋮----
instructions.push(
⋮----
let message = Message::new(&instructions, Some(&k.pubkey()));
⋮----
self.send_transactions(client, to_lamports, log_progress);
⋮----
// Introduce a new structure to specify Sliceable implementations
// which uses both Keypairs to sign the transaction
struct NonceCreateSigners<'a>(&'a Keypair, &'a Keypair);
type NonceCreateChunk<'a> = [NonceCreateSigners<'a>];
type NonceCreateContainer<'a> = Vec<(NonceCreateSigners<'a>, Transaction)>;
impl<'a> Sliceable for NonceCreateSigners<'a> {
type Slice = [&'a Keypair; 2];
⋮----
self.0.pubkey()
⋮----
trait NonceTransactions<'a>: SendBatchTransactions<'a, NonceCreateSigners<'a>> {
⋮----
fn create_accounts<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
self.make(to_fund, |kp| -> (NonceCreateSigners<'a>, Transaction) {
⋮----
&authority.pubkey(),
&nonce.pubkey(),
⋮----
NonceCreateSigners(authority, nonce),
Transaction::new_with_payer(&instructions, Some(&authority.pubkey())),
⋮----
self.send_transactions(client, nonce_rent, log_progress);
⋮----
struct NonceWithdrawSigners<'a>(&'a Keypair, Pubkey);
type NonceWithdrawChunk<'a> = [NonceWithdrawSigners<'a>];
type NonceWithdrawContainer<'a> = Vec<(NonceWithdrawSigners<'a>, Transaction)>;
impl<'a> Sliceable for NonceWithdrawSigners<'a> {
type Slice = [&'a Keypair; 1];
⋮----
trait NonceWithdrawTransactions<'a>: SendBatchTransactions<'a, NonceWithdrawSigners<'a>> {
⋮----
fn withdraw_accounts<T: 'static + TpsClient + Send + Sync + ?Sized>(
⋮----
self.make(
⋮----
let nonce_balance = client.get_balance(&nonce_pubkey).unwrap();
let instructions = vec![
⋮----
NonceWithdrawSigners(authority, nonce_pubkey),
⋮----
self.send_transactions(client, 0, log_progress);

================
File: bench-tps/tests/bench_tps.rs
================
fn program_account(program_data: &[u8]) -> AccountSharedData {
⋮----
lamports: Rent::default().minimum_balance(program_data.len()).min(1),
data: program_data.to_vec(),
⋮----
fn test_bench_tps_local_cluster(config: Config) {
let additional_accounts = vec![(
⋮----
let faucet_pubkey = faucet_keypair.pubkey();
let faucet_addr = run_local_faucet_for_tests(
⋮----
node_stakes: vec![999_990; NUM_NODES],
⋮----
validator_configs: make_identical_validator_configs(
⋮----
faucet_addr: Some(faucet_addr),
⋮----
cluster.transfer(&cluster.funding_keypair, &faucet_pubkey, 100_000_000);
⋮----
.build_validator_tpu_quic_client(cluster.entry_point_info.pubkey())
.unwrap_or_else(|err| {
panic!("Could not create TpuClient with Quic Cache {err:?}");
⋮----
let keypairs = generate_and_fund_keypairs(
client.clone(),
⋮----
.unwrap();
let _total = do_bench_tps(client, config, keypairs, None);
⋮----
assert!(_total > 100);
⋮----
fn test_bench_tps_test_validator(config: Config) {
⋮----
let mint_pubkey = mint_keypair.pubkey();
⋮----
.fee_rate_governor(FeeRateGovernor::new(0, 0))
.rent(Rent {
⋮----
.faucet_addr(Some(faucet_addr))
.add_program(
⋮----
.start_with_mint_address(mint_pubkey, SocketAddrSpace::Unspecified)
.expect("validator start failed");
⋮----
test_validator.rpc_url(),
⋮----
let websocket_url = test_validator.rpc_pubsub_url();
⋮----
QuicConnectionManager::new_with_connection_config(QuicConfig::new().unwrap()),
⋮----
.expect("Should build Quic Tpu Client."),
⋮----
Some(generate_durable_nonce_accounts(client.clone(), &keypairs))
⋮----
let _total = do_bench_tps(client, config, keypairs, nonce_keypairs);
⋮----
fn test_bench_tps_local_cluster_solana() {
test_bench_tps_local_cluster(Config {
⋮----
fn test_bench_tps_tpu_client() {
test_bench_tps_test_validator(Config {
⋮----
fn test_bench_tps_tpu_client_nonce() {
⋮----
fn test_bench_tps_local_cluster_with_padding() {
⋮----
instruction_padding_config: Some(InstructionPaddingConfig {
⋮----
fn test_bench_tps_tpu_client_with_padding() {

================
File: bench-tps/.gitignore
================
/target/
/config/
/config-local/
/farf/

================
File: bench-tps/Cargo.toml
================
[package]
name = "solana-bench-tps"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }
workspace = "../dev-bins"

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = ["agave-unstable-api"]
agave-unstable-api = []
dev-context-only-utils = []
dummy-for-ci-check = []
frozen-abi = []

[dependencies]
agave-logger = { workspace = true }
chrono = { workspace = true }
clap = { workspace = true }
crossbeam-channel = { workspace = true }
csv = { workspace = true }
log = { workspace = true }
rand = { workspace = true }
rayon = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
serde_yaml = { workspace = true }
solana-account = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-compute-budget-interface = { workspace = true }
solana-connection-cache = { workspace = true }
solana-core = { workspace = true, features = ["dev-context-only-utils"] }
solana-fee-calculator = { workspace = true }
solana-genesis = { workspace = true }
solana-genesis-config = { workspace = true }
solana-gossip = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-metrics = { workspace = true }
solana-native-token = { workspace = true }
solana-net-utils = { workspace = true }
solana-nonce = { workspace = true }
solana-pubkey = { workspace = true }
solana-quic-client = { workspace = true }
solana-rpc = { workspace = true }
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-rpc-client-nonce-utils = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-system-interface = { workspace = true }
solana-time-utils = { workspace = true }
solana-tps-client = { workspace = true }
solana-tpu-client = { workspace = true }
solana-transaction = { workspace = true }
solana-transaction-status = { workspace = true }
solana-version = { workspace = true }
spl-instruction-padding-interface = { version = "=1.0.0" }
thiserror = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dependencies]
jemallocator = { workspace = true }

[dev-dependencies]
agave-feature-set = { workspace = true }
serial_test = { workspace = true }
solana-faucet = { workspace = true, features = ["dev-context-only-utils"] }
solana-local-cluster = { workspace = true }
solana-rent = { workspace = true }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }
solana-sdk-ids = { workspace = true }
solana-test-validator = { workspace = true }
solana-tps-client = { workspace = true, features = ["bank-client"] }
tempfile = { workspace = true }

================
File: bench-vote/src/main.rs
================
fn sink(
⋮----
spawn(move || {
⋮----
while !exit.load(Ordering::Relaxed) {
if let Ok(packet_batch) = receiver.recv_timeout(SINK_RECEIVE_TIMEOUT) {
received_size.fetch_add(packet_batch.len(), Ordering::Relaxed);
⋮----
let count = received_size.load(Ordering::Relaxed);
if verbose && last_report.elapsed() > SINK_REPORT_INTERVAL {
⋮----
let rate = change as u64 / SINK_REPORT_INTERVAL.as_secs();
println!("Received txns count: total: {count}, rate {rate}/s");
⋮----
fn main() -> Result<()> {
let matches = App::new(crate_name!())
.about(crate_description!())
.version(solana_version::version!())
.arg(
⋮----
.short("i")
.long("identity")
.value_name("KEYPAIR")
.takes_value(true)
.validator(is_keypair_or_ask_keyword)
.help(
⋮----
.long("num-recv-sockets")
.value_name("NUM")
⋮----
.help("Use NUM receive sockets"),
⋮----
.long("num-producers")
⋮----
.help("Use this many producer threads."),
⋮----
.long("max-connections")
⋮----
.help("Maximum concurrent client connections allowed on the server side."),
⋮----
.long("max-connections-per-peer")
⋮----
.help("Maximum concurrent client connections per peer allowed on the server side."),
⋮----
.long("max-connections-per-ipaddr-per-min")
⋮----
.long("connection-pool-size")
⋮----
.help("Maximum concurrent client connections on the client side."),
⋮----
.long("server-only")
.takes_value(false)
.help("Run the bench tool as a server only."),
⋮----
.long("client-only")
⋮----
.requires("server-address")
.help("Run the bench tool as a client only."),
⋮----
.short("n")
.long("server-address")
.value_name("HOST:PORT")
⋮----
.validator(|arg| solana_net_utils::is_host_port(arg.to_string()))
⋮----
.long("use-connection-cache")
⋮----
.long("verbose")
⋮----
.help("Show verbose messages."),
⋮----
.long("use-quic")
.value_name("Boolean")
⋮----
.default_value("false")
.help("Controls if to use QUIC for sending/receiving vote transactions."),
⋮----
.get_matches();
⋮----
if let Some(n) = matches.value_of("num-recv-sockets") {
num_sockets = max(num_sockets, n.to_string().parse().expect("integer"));
⋮----
let vote_use_quic = value_t_or_exit!(matches, "use-quic", bool);
let num_producers: u64 = value_t!(matches, "num-producers", u64).unwrap_or(4);
⋮----
value_t!(matches, "max-connections", usize).unwrap_or(DEFAULT_MAX_STAKED_CONNECTIONS);
let max_connections_per_peer: usize = value_t!(matches, "max-connections-per-peer", usize)
.unwrap_or(DEFAULT_MAX_QUIC_CONNECTIONS_PER_UNSTAKED_PEER);
⋮----
value_t!(matches, "max-connections-per-ipaddr-per-min", usize).unwrap_or(1024);
⋮----
value_t!(matches, "connection-pool-size", usize).unwrap_or(256);
let use_connection_cache = matches.is_present("use-connection-cache");
let server_only = matches.is_present("server-only");
let client_only = matches.is_present("client-only");
let verbose = matches.is_present("verbose");
let destination = matches.is_present("server-address").then(|| {
⋮----
.value_of("server-address")
.expect("Server address must be set when --client-only is used");
solana_net_utils::parse_host_port(addr).expect("Expecting a valid server address")
⋮----
let port = destination.map_or(0, |addr| addr.port());
let ip_addr = destination.map_or(IpAddr::V4(Ipv4Addr::UNSPECIFIED), |addr| addr.ip());
let quic_params = vote_use_quic.then(|| {
let identity_keypair = keypair_of(&matches, "identity")
.or_else(|| {
println!("--identity is not specified, will generate a key dynamically.");
Some(Keypair::new())
⋮----
.unwrap();
⋮----
(identity_keypair.pubkey(), stake),
(Pubkey::new_unique(), total_stake.saturating_sub(stake)),
⋮----
let (port, read_sockets) = multi_bind_in_range_with_config(
⋮----
.try_into()
.unwrap(),
⋮----
let (s_reader, r_reader) = unbounded();
read_channels.push(r_reader);
let server = spawn_stake_wighted_qos_server(
⋮----
quic_params.staked_nodes.clone(),
⋮----
cancel.clone(),
⋮----
read_threads.push(server.thread);
⋮----
read.set_read_timeout(Some(SOCKET_RECEIVE_TIMEOUT)).unwrap();
⋮----
read_threads.push(receiver(
"solRcvrBenVote".to_string(),
⋮----
exit.clone(),
⋮----
recycler.clone(),
stats.clone(),
⋮----
.into_iter()
.map(|r_reader| sink(exit.clone(), received_size.clone(), r_reader, verbose))
.collect();
⋮----
println!("Running server at {destination:?}");
⋮----
Some(exit),
Some(cancel),
Some(read_threads),
Some(sink_threads),
⋮----
(None, None, None, None, destination.unwrap())
⋮----
let producer_threads = (!server_only).then(|| {
producer(
⋮----
.flatten()
.try_for_each(JoinHandle::join)?;
⋮----
exit.store(true, Ordering::Relaxed);
cancel.unwrap().cancel();
⋮----
println!("To stop the server, please press ^C");
⋮----
let elapsed = start.elapsed().unwrap();
let ftime = elapsed.as_nanos() as f64 / 1_000_000_000.0;
⋮----
println!(
⋮----
Ok(())
⋮----
enum Transporter {
⋮----
struct QuicParams {
⋮----
fn producer(
⋮----
println!("Running clients against {sock:?}");
let transporter = if use_connection_cache || quic_params.is_some() {
⋮----
Some((
⋮----
&quic_params.identity_keypair.pubkey(),
⋮----
Transporter::DirectSocket(Arc::new(bind_to_unspecified().unwrap()))
⋮----
let mut handles = vec![];
⋮----
let transporter = transporter.clone();
let identity_keypair = identity_keypair.insecure_clone();
handles.push(thread::spawn(move || {
⋮----
slots: vec![current_slot],
⋮----
&identity_keypair.pubkey(),
⋮----
let message = Message::new(&[vote_instruction], Some(&identity_keypair.pubkey()));
⋮----
let serialized_transaction = bincode::serialize(&transaction).unwrap();
⋮----
let connection = cache.get_connection(&sock);
match connection.send_data(&serialized_transaction) {
⋮----
println!("Sent transaction successfully");
⋮----
println!("Error sending transaction {ex:?}");
⋮----
match socket.send_to(&serialized_transaction, sock) {

================
File: bench-vote/Cargo.toml
================
[package]
name = "solana-bench-vote"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
agave-logger = { workspace = true }
bincode = { workspace = true }
clap = { workspace = true }
crossbeam-channel = { workspace = true }
solana-clap-utils = { workspace = true }
solana-client = { workspace = true }
solana-connection-cache = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-message = { workspace = true }
solana-net-utils = { workspace = true }
solana-pubkey = { workspace = true }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-transaction = { workspace = true }
solana-version = { workspace = true }
solana-vote-program = { workspace = true }
tokio-util = { workspace = true }

[target.'cfg(not(any(target_env = "msvc", target_os = "freebsd")))'.dependencies]
jemallocator = { workspace = true }

================
File: bloom/benches/bloom.rs
================
fn bench_bits_set(b: &mut Bencher) {
⋮----
b.iter(|| {
let idx = hasher.finish() % bits.len();
bits.set(idx, true);
hasher.write_u64(idx);
⋮----
fn bench_bits_set_hasher(b: &mut Bencher) {
⋮----
fn bench_sigs_bloom(b: &mut Bencher) {
let blockhash = hash(Hash::default().as_ref());
let keys = (0..27).map(|i| blockhash.hash_at_index(i)).collect();
⋮----
id = hash(id.as_ref());
let mut sigbytes = Vec::from(id.as_ref());
⋮----
sigbytes.extend(id.as_ref());
let sig = Signature::try_from(sigbytes).unwrap();
if sigs.contains(&sig) {
⋮----
sigs.add(&sig);
sigs.contains(&sig);
⋮----
assert_eq!(falses, 0);
⋮----
fn bench_sigs_hashmap(b: &mut Bencher) {
⋮----
sigs.insert(sig);
⋮----
fn bench_add_hash(b: &mut Bencher) {
⋮----
.take(1200)
.collect();
⋮----
bloom.add(hash_value);
⋮----
let index = rng.random_range(0..hash_values.len());
if !bloom.contains(&hash_values[index]) {
⋮----
assert_eq!(fail, 0);
⋮----
fn bench_add_hash_atomic(b: &mut Bencher) {
⋮----
let bloom: ConcurrentBloom<_> = Bloom::random(1287, 0.1, 7424).into();
⋮----
benchmark_group!(
⋮----
benchmark_main!(benches);

================
File: bloom/src/bloom.rs
================
pub trait BloomHashIndex {
⋮----
pub struct Bloom<T: BloomHashIndex> {
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
write!(
⋮----
for i in 0..std::cmp::min(MAX_PRINT_BITS, self.bits.len()) {
if self.bits.get(i) {
write!(f, "1")?;
⋮----
write!(f, "0")?;
⋮----
if self.bits.len() > MAX_PRINT_BITS {
write!(f, "..")?;
⋮----
write!(f, " }}")
⋮----
impl<T: BloomHashIndex> Sanitize for Bloom<T> {
fn sanitize(&self) -> Result<(), SanitizeError> {
// Avoid division by zero in self.pos(...).
if self.bits.is_empty() {
Err(SanitizeError::InvalidValue)
⋮----
Ok(())
⋮----
pub fn new(num_bits: usize, keys: Vec<u64>) -> Self {
⋮----
/// Create filter optimal for num size given the `FALSE_RATE`.
    ///
⋮----
///
    /// The keys are randomized for picking data out of a collision resistant hash of size
⋮----
/// The keys are randomized for picking data out of a collision resistant hash of size
    /// `keysize` bytes.
⋮----
/// `keysize` bytes.
    ///
⋮----
///
    /// See <https://hur.st/bloomfilter/>.
⋮----
/// See <https://hur.st/bloomfilter/>.
    pub fn random(num_items: usize, false_rate: f64, max_bits: usize) -> Self {
⋮----
pub fn random(num_items: usize, false_rate: f64, max_bits: usize) -> Self {
⋮----
let keys: Vec<u64> = (0..num_keys).map(|_| rand::rng().random()).collect();
⋮----
fn num_bits(num_items: f64, false_rate: f64) -> f64 {
⋮----
((n * p.ln()) / (1f64 / 2f64.powf(2f64.ln())).ln()).ceil()
⋮----
fn num_keys(num_bits: f64, num_items: f64) -> f64 {
⋮----
// infinity as usize is zero in rust 1.43 but 2^64-1 in rust 1.45; ensure it's zero here
⋮----
1f64.max(((m / n) * 2f64.ln()).round())
⋮----
fn pos(&self, key: &T, k: u64) -> u64 {
key.hash_at_index(k)
.checked_rem(self.bits.len())
.unwrap_or(0)
⋮----
pub fn clear(&mut self) {
self.bits = BitVec::new_fill(false, self.bits.len());
⋮----
pub fn add(&mut self, key: &T) {
⋮----
let pos = self.pos(key, *k);
if !self.bits.get(pos) {
self.num_bits_set = self.num_bits_set.saturating_add(1);
self.bits.set(pos, true);
⋮----
pub fn contains(&self, key: &T) -> bool {
⋮----
fn slice_hash(slice: &[u8], hash_index: u64) -> u64 {
⋮----
hasher.write(slice);
hasher.finish()
⋮----
impl<T: AsRef<[u8]>> BloomHashIndex for T {
fn hash_at_index(&self, hash_index: u64) -> u64 {
slice_hash(self.as_ref(), hash_index)
⋮----
pub struct ConcurrentBloom<T> {
⋮----
fn from(bloom: Bloom<T>) -> Self {
⋮----
num_bits: bloom.bits.len(),
⋮----
.into_boxed_slice()
.iter()
.map(|&x| AtomicU64::new(x))
.collect(),
⋮----
fn pos(&self, key: &T, hash_index: u64) -> (usize, u64) {
⋮----
.hash_at_index(hash_index)
.checked_rem(self.num_bits)
.unwrap_or(0);
let index = pos.wrapping_shr(6);
let mask = 1u64.wrapping_shl(u32::try_from(pos & 63).unwrap());
⋮----
pub fn add(&self, key: &T) -> bool {
⋮----
let (index, mask) = self.pos(key, *k);
let prev_val = self.bits[index].fetch_or(mask, Ordering::Relaxed);
⋮----
self.keys.iter().all(|k| {
⋮----
let bit = self.bits[index].load(Ordering::Relaxed) & mask;
⋮----
pub fn clear(&self) {
self.bits.iter().for_each(|bit| {
bit.store(0u64, Ordering::Relaxed);
⋮----
fn from(atomic_bloom: ConcurrentBloom<T>) -> Self {
⋮----
.into_iter()
.map(AtomicU64::into_inner)
.collect();
let num_bits_set = bits.iter().map(|x| x.count_ones() as u64).sum();
let mut bits: BitVec<u64> = bits.into();
bits.truncate(atomic_bloom.num_bits);
⋮----
pub struct ConcurrentBloomInterval<T: BloomHashIndex> {
⋮----
impl<T: BloomHashIndex> Deref for ConcurrentBloomInterval<T> {
type Target = ConcurrentBloom<T>;
fn deref(&self) -> &Self::Target {
⋮----
pub fn new(num_items: usize, false_positive_rate: f64, max_bits: usize) -> Self {
⋮----
pub fn maybe_reset(&self, reset_interval_ms: u64) {
if self.interval.should_update(reset_interval_ms) {
self.bloom.clear();
⋮----
mod test {
⋮----
fn test_bloom_filter() {
⋮----
assert_eq!(bloom.keys.len(), 0);
assert_eq!(bloom.bits.len(), 1);
⋮----
assert_eq!(bloom.keys.len(), 3);
assert_eq!(bloom.bits.len(), 48);
⋮----
assert_eq!(bloom.keys.len(), 1);
assert_eq!(bloom.bits.len(), 100);
⋮----
fn test_add_contains() {
⋮----
bloom.keys = vec![0, 1, 2, 3];
let key = hash(b"hello");
assert!(!bloom.contains(&key));
bloom.add(&key);
assert!(bloom.contains(&key));
let key = hash(b"world");
⋮----
fn test_random() {
⋮----
b1.keys.sort_unstable();
b2.keys.sort_unstable();
assert_ne!(b1.keys, b2.keys);
⋮----
fn test_filter_math() {
assert_eq!(Bloom::<Hash>::num_bits(100f64, 0.1f64) as u64, 480u64);
assert_eq!(Bloom::<Hash>::num_bits(100f64, 0.01f64) as u64, 959u64);
assert_eq!(Bloom::<Hash>::num_keys(1000f64, 50f64) as u64, 14u64);
assert_eq!(Bloom::<Hash>::num_keys(2000f64, 50f64) as u64, 28u64);
assert_eq!(Bloom::<Hash>::num_keys(2000f64, 25f64) as u64, 55u64);
assert_eq!(Bloom::<Hash>::num_keys(20f64, 1000f64) as u64, 1u64);
⋮----
fn test_debug() {
let mut b: Bloom<Hash> = Bloom::new(3, vec![100]);
b.add(&Hash::default());
assert_eq!(
⋮----
let mut b: Bloom<Hash> = Bloom::new(1000, vec![100]);
⋮----
b.add(&hash(&[1, 2]));
⋮----
fn generate_random_hash() -> Hash {
⋮----
rng.fill(&mut hash);
⋮----
fn test_atomic_bloom() {
⋮----
.take(1200)
⋮----
let bloom: ConcurrentBloom<_> = Bloom::<Hash>::random(1287, 0.1, 7424).into();
⋮----
assert_eq!(bloom.num_bits, 6168);
assert_eq!(bloom.bits.len(), 97);
hash_values.par_iter().for_each(|v| {
bloom.add(v);
⋮----
let bloom: Bloom<Hash> = bloom.into();
⋮----
assert_eq!(bloom.bits.len(), 6168);
assert!(bloom.num_bits_set > 2000);
⋮----
assert!(bloom.contains(&hash_value));
⋮----
.take(10_000)
.filter(|hash_value| bloom.contains(hash_value))
.count();
assert!(false_positive < 2_000, "false_positive: {false_positive}");
⋮----
fn test_atomic_bloom_round_trip() {
⋮----
let keys: Vec<_> = std::iter::repeat_with(|| rng.random()).take(5).collect();
let mut bloom = Bloom::<Hash>::new(9731, keys.clone());
⋮----
.take(1000)
⋮----
bloom.add(hash_value);
⋮----
assert!(num_bits_set > 2000, "# bits set: {num_bits_set}");
let bloom: ConcurrentBloom<_> = bloom.into();
assert_eq!(bloom.num_bits, 9731);
assert_eq!(bloom.bits.len(), 9731_usize.div_ceil(64));
⋮----
assert!(bloom.contains(hash_value));
⋮----
let bloom: Bloom<_> = bloom.into();
assert_eq!(bloom.num_bits_set, num_bits_set);
⋮----
assert_eq!(bloom.bits.len(), 9731);
⋮----
more_hash_values.par_iter().for_each(|v| {
⋮----
assert!(false_positive < 2000, "false_positive: {false_positive}");
⋮----
assert!(bloom.num_bits_set > num_bits_set);
assert!(
⋮----
assert_eq!(bits, bloom.bits);

================
File: bloom/src/lib.rs
================
pub mod bloom;
⋮----
extern crate solana_frozen_abi_macro;

================
File: bloom/Cargo.toml
================
[package]
name = "solana-bloom"
description = "Solana bloom filter"
documentation = "https://docs.rs/solana-bloom"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_bloom"

[features]
agave-unstable-api = []
frozen-abi = [
    "dep:solana-frozen-abi",
    "dep:solana-frozen-abi-macro",
    "solana-hash/frozen-abi",
]

[dependencies]
bv = { workspace = true, features = ["serde"] }
fnv = { workspace = true }
rand = { workspace = true }
serde = { workspace = true, features = ["rc"] }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-frozen-abi-macro = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-sanitize = { workspace = true }
solana-time-utils = { workspace = true }

[dev-dependencies]
bencher = { workspace = true }
rayon = { workspace = true }
solana-bloom = { path = ".", features = ["agave-unstable-api"] }
solana-hash = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-signature = { workspace = true, features = ["std"] }

[[bench]]
name = "bloom"
harness = false

[lints]
workspace = true

================
File: bootstrap
================
#!/usr/bin/env bash
set -eu

BANK_HASH=$(cargo run --release --bin solana-ledger-tool -- -l config/bootstrap-validator bank-hash)

# increase max file handle limit
ulimit -Hn 1000000

# if above fails, run:
# sudo bash -c 'echo "*               hard    nofile          1000000" >> /etc/security/limits.conf'

# NOTE: make sure tip-payment and tip-distribution program are deployed using the correct pubkeys
RUST_LOG=INFO,solana_core::bundle_stage=DEBUG \
  NDEBUG=1 ./multinode-demo/bootstrap-validator.sh \
  --wait-for-supermajority 0 \
  --expected-bank-hash "$BANK_HASH" \
  --block-engine-url http://127.0.0.1 \
  --relayer-url http://127.0.0.1:11226 \
  --rpc-pubsub-enable-block-subscription \
  --enable-rpc-transaction-history \
  --tip-payment-program-pubkey T1pyyaTNZsKv2WcRAB8oVnk93mLJw2XzjtVYqCsaHqt \
  --tip-distribution-program-pubkey 4R3gSG8BpU4t19KYj8CfnbtRpnT8gtk4dvTHxVRwc2r7 \
  --commission-bps 0 \
  --shred-receiver-address 127.0.0.1:1002 \
  --trust-relayer-packets \
  --trust-block-engine-packets

================
File: bucket_map/src/bucket_api.rs
================
use crate::bucket_item::BucketItem;
⋮----
type LockedBucket<T> = RwLock<Option<Bucket<T>>>;
pub struct BucketApi<T: Clone + Copy + PartialEq + 'static> {
⋮----
/// keeps track of which index file this bucket is currently using
    /// or at startup, which bucket file this bucket should initially use
⋮----
/// or at startup, which bucket file this bucket should initially use
    restartable_bucket: RestartableBucket,
⋮----
pub(crate) fn new(
⋮----
/// Get the items for bucket
    #[cfg(feature = "dev-context-only-utils")]
pub fn items(&self) -> Vec<BucketItem<T>> {
⋮----
.read()
.unwrap()
.as_ref()
.map(|bucket| bucket.items())
.unwrap_or_default()
⋮----
/// Get the Pubkeys
    pub fn keys(&self) -> Vec<Pubkey> {
⋮----
pub fn keys(&self) -> Vec<Pubkey> {
⋮----
.map_or_else(Vec::default, |bucket| bucket.keys())
⋮----
/// Get the values for Pubkey `key`
    pub fn read_value<C: for<'a> From<&'a [T]>>(&self, key: &Pubkey) -> Option<(C, RefCount)> {
⋮----
pub fn read_value<C: for<'a> From<&'a [T]>>(&self, key: &Pubkey) -> Option<(C, RefCount)> {
self.bucket.read().unwrap().as_ref().and_then(|bucket| {
⋮----
.read_value(key)
.map(|(value, ref_count)| (C::from(value), ref_count))
⋮----
pub fn bucket_len(&self) -> u64 {
self.count.load(Ordering::Relaxed)
⋮----
pub fn delete_key(&self, key: &Pubkey) {
let mut bucket = self.get_write_bucket();
if let Some(bucket) = bucket.as_mut() {
bucket.delete_key(key)
⋮----
/// allocate new bucket if not allocated yet
    fn allocate_bucket(&self, bucket: &mut RwLockWriteGuard<Option<Bucket<T>>>) {
⋮----
fn allocate_bucket(&self, bucket: &mut RwLockWriteGuard<Option<Bucket<T>>>) {
if bucket.is_none() {
**bucket = Some(Bucket::new(
⋮----
self.restartable_bucket.clone(),
⋮----
fn get_write_bucket(&self) -> RwLockWriteGuard<'_, Option<Bucket<T>>> {
let mut bucket = self.bucket.write().unwrap();
⋮----
bucket.handle_delayed_grows();
⋮----
self.allocate_bucket(&mut bucket);
⋮----
pub fn insert(&self, pubkey: &Pubkey, value: (&[T], RefCount)) {
⋮----
bucket.as_mut().unwrap().insert(pubkey, value)
⋮----
pub fn grow(&self, err: BucketMapError) {
if let Some(bucket) = self.bucket.read().unwrap().as_ref() {
bucket.grow(err)
⋮----
pub fn set_anticipated_count(&self, count: u64) {
⋮----
bucket.as_mut().unwrap().set_anticipated_count(count);
⋮----
pub fn batch_insert_non_duplicates(&self, items: &[(Pubkey, T)]) -> Vec<(usize, T)> {
⋮----
bucket.as_mut().unwrap().batch_insert_non_duplicates(items)
⋮----
pub fn update<F>(&self, key: &Pubkey, updatefn: F)
⋮----
bucket.as_mut().unwrap().update(key, updatefn)
⋮----
pub fn try_write(
⋮----
.as_mut()
⋮----
.try_write(pubkey, value.0.iter(), value.0.len(), value.1)

================
File: bucket_map/src/bucket_item.rs
================
pub struct BucketItem<T> {

================
File: bucket_map/src/bucket_map.rs
================
pub struct BucketMapConfig {
⋮----
impl BucketMapConfig {
pub fn new(max_buckets: usize) -> BucketMapConfig {
⋮----
pub struct BucketMap<T: Clone + Copy + Debug + PartialEq + 'static> {
⋮----
/// true if dropping self removes all folders.
    /// This is primarily for test environments.
⋮----
/// This is primarily for test environments.
    pub erase_drives_on_drop: bool,
⋮----
impl<T: Clone + Copy + Debug + PartialEq> Drop for BucketMap<T> {
fn drop(&mut self) {
if self.temp_dir.is_none() && self.erase_drives_on_drop {
⋮----
impl<T: Clone + Copy + Debug + PartialEq> Debug for BucketMap<T> {
fn fmt(&self, _f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
Ok(())
⋮----
pub enum BucketMapError {
⋮----
pub fn new(config: BucketMapConfig) -> Self {
assert_ne!(
⋮----
assert!(
⋮----
let max_search = config.max_search.unwrap_or(MAX_SEARCH_DEFAULT);
⋮----
if restart.is_none() {
if let Some(drives) = config.drives.as_ref() {
⋮----
let drives = config.drives.unwrap_or_else(|| {
temp_dir = Some(TempDir::new().unwrap());
vec![temp_dir.as_ref().unwrap().path().to_path_buf()]
⋮----
let restart = restart.map(|restart| Arc::new(Mutex::new(restart)));
⋮----
Restart::get_restartable_buckets(restart.as_ref(), &drives, config.max_buckets);
⋮----
.into_iter()
.map(|restartable_bucket| {
⋮----
.collect();
let log2 = |x: usize| usize::BITS - x.leading_zeros() - 1;
⋮----
max_buckets_pow2: log2(config.max_buckets) as u8,
⋮----
erase_drives_on_drop: restart.is_none(),
⋮----
fn erase_previous_drives(drives: &[PathBuf]) {
drives.iter().for_each(|folder| {
⋮----
pub fn num_buckets(&self) -> usize {
self.buckets.len()
⋮----
pub fn read_value<C: for<'a> From<&'a [T]>>(&self, key: &Pubkey) -> Option<(C, RefCount)> {
self.get_bucket(key).read_value(key)
⋮----
pub fn delete_key(&self, key: &Pubkey) {
self.get_bucket(key).delete_key(key);
⋮----
pub fn insert(&self, key: &Pubkey, value: (&[T], RefCount)) {
self.get_bucket(key).insert(key, value)
⋮----
pub fn try_insert(&self, key: &Pubkey, value: (&[T], RefCount)) -> Result<(), BucketMapError> {
self.get_bucket(key).try_write(key, value)
⋮----
pub fn update<F>(&self, key: &Pubkey, updatefn: F)
⋮----
self.get_bucket(key).update(key, updatefn)
⋮----
pub fn get_bucket(&self, key: &Pubkey) -> &Arc<BucketApi<T>> {
self.get_bucket_from_index(self.bucket_ix(key))
⋮----
pub fn get_bucket_from_index(&self, ix: usize) -> &Arc<BucketApi<T>> {
⋮----
pub fn bucket_ix(&self, key: &Pubkey) -> usize {
⋮----
let location = read_be_u64(key.as_ref());
⋮----
fn read_be_u64(input: &[u8]) -> u64 {
assert!(input.len() >= std::mem::size_of::<u64>());
u64::from_be_bytes(input[0..std::mem::size_of::<u64>()].try_into().unwrap())
⋮----
mod tests {
⋮----
fn bucket_map_test_insert() {
⋮----
index.update(&key, |_| Some((vec![0], 1)));
assert_eq!(index.read_value(&key), Some((vec![0], 1)));
⋮----
fn bucket_map_test_insert2() {
⋮----
let bucket = index.get_bucket(&key);
⋮----
index.insert(&key, (&[0], 0));
⋮----
let result = index.try_insert(&key, (&[0], 0));
assert!(result.is_err());
assert_eq!(index.read_value::<Vec<_>>(&key), None);
⋮----
bucket.grow(result.unwrap_err());
⋮----
assert!(result.is_ok());
⋮----
assert_eq!(index.read_value(&key), Some((vec![0], 0)));
⋮----
fn bucket_map_test_update2() {
⋮----
index.insert(&key, (&[0], 1));
⋮----
index.insert(&key, (&[1], 1));
assert_eq!(index.read_value(&key), Some((vec![1], 1)));
⋮----
fn bucket_map_test_update() {
⋮----
index.update(&key, |_| Some((vec![1], 1)));
⋮----
fn bucket_map_test_update_to_0_len() {
⋮----
index.update(&key, |_| Some((vec![], 1)));
assert_eq!(index.read_value(&key), Some((vec![], 1)));
index.update(&key, |_| Some((vec![], 2)));
assert_eq!(index.read_value(&key), Some((vec![], 2)));
index.update(&key, |_| Some((vec![1], 2)));
assert_eq!(index.read_value(&key), Some((vec![1], 2)));
⋮----
fn bucket_map_test_delete() {
⋮----
index.update(&key, |_| Some((vec![i], 1)));
assert_eq!(index.read_value(&key), Some((vec![i], 1)));
index.delete_key(&key);
⋮----
fn bucket_map_test_delete_2() {
⋮----
fn bucket_map_test_n_drives() {
⋮----
fn bucket_map_test_grow_read() {
⋮----
let keys: Vec<Pubkey> = (0..100).map(|_| Pubkey::new_unique()).collect();
for k in 0..keys.len() {
⋮----
let i = read_be_u64(key.as_ref());
index.update(key, |_| Some((vec![i], 1)));
assert_eq!(index.read_value(key), Some((vec![i], 1)));
for (ix, key) in keys.iter().enumerate() {
⋮----
let expected = if ix <= k { Some((vec![i], 1)) } else { None };
assert_eq!(index.read_value(key), expected);
⋮----
fn bucket_map_test_n_delete() {
⋮----
let keys: Vec<Pubkey> = (0..20).map(|_| Pubkey::new_unique()).collect();
for key in keys.iter() {
⋮----
index.delete_key(key);
assert_eq!(index.read_value::<Vec<_>>(key), None);
for key in keys.iter().skip(k + 1) {
⋮----
fn hashmap_compare() {
use std::sync::Mutex;
⋮----
.map(|max_buckets_pow2| {
⋮----
let all_keys = Mutex::new(vec![]);
⋮----
let count = rng().random_range(0..max_slot_list_len);
⋮----
.map(|x| (x as usize, x as usize ))
⋮----
let range = rng().random_range(0..100);
⋮----
rng().random_range(0..MAX_LEGAL_REFCOUNT)
⋮----
let mut keys = all_keys.lock().unwrap();
if keys.is_empty() {
⋮----
let len = keys.len();
Some(keys.remove(rng().random_range(0..len)))
⋮----
keys.push(key);
⋮----
let expected_count = hash_map.read().unwrap().len();
⋮----
.iter()
.map(|map| {
let total_entries = (0..map.num_buckets())
.map(|bucket| map.get_bucket_from_index(bucket).bucket_len() as usize)
⋮----
assert_eq!(total_entries, expected_count);
let mut r = vec![];
for bin in 0..map.num_buckets() {
r.append(&mut map.buckets[bin].items());
⋮----
let hm = hash_map.read().unwrap();
for (k, v) in hm.iter() {
for map in maps.iter_mut() {
for i in 0..map.len() {
⋮----
assert_eq!(map[i].slot_list, v.0);
assert_eq!(map[i].ref_count, v.1);
map.remove(i);
⋮----
for map in maps.iter() {
assert!(map.is_empty());
⋮----
initial = initial.saturating_sub(1);
if initial > 0 || rng().random_range(0..5) == 0 {
⋮----
to_add = rng().random_range(1..(initial / 4).max(2));
⋮----
.map(|_| {
⋮----
let mut v = gen_rand_value();
⋮----
if v.0.len() > 1 {
v.0.truncate(1);
} else if v.0.is_empty() {
⋮----
let mut new_v = gen_rand_value();
if !new_v.0.is_empty() {
v.0 = vec![new_v.0.pop().unwrap()];
⋮----
additions.clone().into_iter().for_each(|(k, v)| {
hash_map.write().unwrap().insert(k, v);
return_key(k);
⋮----
let insert = rng().random_range(0..2) == 0;
maps.iter().for_each(|map| {
let batch_insert_now = map.buckets.len() == 1
⋮----
&& rng().random_range(0..2) == 0;
⋮----
.clone()
⋮----
.map(|(k, mut v)| (k, v.0.pop().unwrap()))
⋮----
if batch_additions.len() > 1 && rng().random_range(0..2) == 0 {
⋮----
rng().random_range(0..batch_additions.len());
⋮----
batch_additions.insert(
⋮----
assert_eq!(
⋮----
map.insert(&k, (&v.0, v.1))
⋮----
map.update(&k, |current| {
assert!(current.is_none());
Some(v.clone())
⋮----
if rng().random_range(0..10) == 0 {
if let Some(k) = get_key() {
⋮----
let (v, rc) = gen_rand_value();
let v_old = hm.get(&k);
⋮----
map.insert(&k, (&v, rc))
⋮----
assert_eq!(current, v_old.map(|(v, rc)| (&v[..], *rc)), "{k}");
Some((v.clone(), rc))
⋮----
drop(hm);
hash_map.write().unwrap().insert(k, (v, rc));
⋮----
if rng().random_range(0..20) == 0 {
⋮----
let mut hm = hash_map.write().unwrap();
hm.remove(&k);
⋮----
map.delete_key(&k);
⋮----
let mut inc = rng().random_range(0..2) == 0;
⋮----
let (v, mut rc) = hm.get(&k).map(|(v, rc)| (v.to_vec(), *rc)).unwrap();
⋮----
hm.insert(k, (v.to_vec(), rc));
⋮----
map.update(&k, |current| Some((current.unwrap().0.to_vec(), rc)))
⋮----
verify();

================
File: bucket_map/src/bucket_stats.rs
================
pub struct StartupBucketStats {
⋮----
pub struct BucketStats {
⋮----
impl BucketStats {
pub fn update_max_size(&self, size: u64) {
self.max_size.fetch_max(size, Ordering::Relaxed);
⋮----
pub fn resize_grow(&self, old_size: u64, new_size: u64) {
let size_change = new_size.saturating_sub(old_size);
⋮----
.fetch_add(size_change, Ordering::Relaxed);
⋮----
pub struct BucketMapStats {

================
File: bucket_map/src/bucket_storage.rs
================
pub trait BucketOccupied: BucketCapacity {
⋮----
fn copying_entry(
⋮----
pub trait BucketCapacity {
⋮----
fn capacity_pow2(&self) -> u8 {
unimplemented!();
⋮----
pub struct BucketStorage<O: BucketOccupied> {
⋮----
pub enum BucketStorageError {
⋮----
impl<O: BucketOccupied> Drop for BucketStorage<O> {
fn drop(&mut self) {
⋮----
self.delete();
⋮----
pub(crate) enum IncludeHeader {
⋮----
pub enum Capacity {
⋮----
impl BucketCapacity for Capacity {
fn capacity(&self) -> u64 {
⋮----
panic!("illegal to ask for pow2 from random capacity");
⋮----
pub fn new_with_capacity(
⋮----
fn allocate_to_fill_page(capacity: &mut Capacity, cell_size: u64) -> u64 {
let mut bytes = capacity.capacity() * cell_size;
⋮----
assert!(
⋮----
assert_eq!(bytes_new % cell_size, 0);
⋮----
fn delete(&self) {
_ = remove_file(&self.path);
⋮----
pub fn max_search(&self) -> u64 {
⋮----
pub fn new(
⋮----
fn get_offset_to_first_data() -> u64 {
⋮----
assert_eq!(
⋮----
pub(crate) fn load_on_restart(
⋮----
.ok()
.map(|metadata| metadata.len().saturating_sub(offset) / elem_size)?;
⋮----
Some(Self {
⋮----
cell_size: elem_size.into(),
⋮----
pub(crate) fn copying_entry(&mut self, ix_new: u64, other: &Self, ix_old: u64) {
let start = self.get_start_offset_with_header(ix_new);
let start_old = other.get_start_offset_with_header(ix_old);
self.contents.copying_entry(
⋮----
pub fn is_free(&self, ix: u64) -> bool {
let start = self.get_start_offset_with_header(ix);
⋮----
self.contents.is_free(entry, ix as usize)
⋮----
pub(crate) fn try_lock(&mut self, ix: u64) -> bool {
⋮----
if self.contents.is_free(entry, ix as usize) {
self.contents.occupy(entry, ix as usize);
⋮----
pub fn occupy(&mut self, ix: u64, is_resizing: bool) -> Result<(), BucketStorageError> {
debug_assert!(ix < self.capacity(), "occupy: bad index size");
if self.try_lock(ix) {
⋮----
self.count.fetch_add(1, Ordering::Relaxed);
⋮----
Ok(())
⋮----
Err(BucketStorageError::AlreadyOccupied)
⋮----
pub fn free(&mut self, ix: u64) {
debug_assert!(ix < self.capacity(), "bad index size");
⋮----
self.contents.free(&mut self.mmap[start..], ix as usize);
self.count.fetch_sub(1, Ordering::Relaxed);
⋮----
fn get_start_offset_with_header(&self, ix: u64) -> usize {
⋮----
fn get_start_offset(&self, ix: u64, header: IncludeHeader) -> usize {
self.get_start_offset_with_header(ix)
⋮----
pub(crate) fn get_header<T>(&self, ix: u64) -> &T {
⋮----
unsafe { slice.get_unchecked(0) }
⋮----
pub(crate) fn get_header_mut<T>(&mut self, ix: u64) -> &mut T {
⋮----
unsafe { slice.get_unchecked_mut(0) }
⋮----
pub(crate) fn get<T>(&self, ix: u64) -> &T {
⋮----
pub(crate) fn get_mut<T>(&mut self, ix: u64) -> &mut T {
⋮----
pub(crate) fn get_slice<T>(&self, ix: u64, len: u64, header: IncludeHeader) -> &[T] {
debug_assert!(
⋮----
let start = self.get_start_offset(ix, header);
⋮----
debug_assert!(slice.len() >= size);
⋮----
let ptr = slice.as_ptr().cast();
debug_assert!((ptr as usize).is_multiple_of(std::mem::align_of::<T>()));
⋮----
pub(crate) fn get_slice_mut<T>(
⋮----
let ptr = slice.as_mut_ptr().cast();
⋮----
fn map_open_file(
⋮----
.read(true)
.write(true)
.create(create)
.open(&path);
⋮----
panic!(
⋮----
let mut data = data.unwrap();
⋮----
data.seek(SeekFrom::Start(create_bytes - 1)).unwrap();
data.write_all(&[0]).unwrap();
data.rewind().unwrap();
measure_new_file.stop();
⋮----
data.flush().unwrap();
⋮----
.fetch_add(measure_flush.end_as_us(), Ordering::Relaxed);
⋮----
let mmap = unsafe { MmapMut::map_mut(&data) }.unwrap_or_else(|err| {
⋮----
mmap.advise(memmap2::Advice::Random).unwrap();
measure_mmap.stop();
⋮----
.fetch_add(measure_new_file.as_us(), Ordering::Relaxed);
⋮----
.fetch_add(measure_mmap.as_us(), Ordering::Relaxed);
Some(mmap)
⋮----
fn new_map(drives: &[PathBuf], bytes: u64, stats: &BucketStats) -> (MmapMut, PathBuf, u128) {
let r = rng().random_range(0..drives.len());
⋮----
let file_random = rng().random_range(0..u128::MAX);
let pos = format!("{file_random}");
let file = drive.join(pos);
let res = Self::map_open_file(file.clone(), true, bytes, stats).unwrap();
⋮----
fn copy_contents(&mut self, old_bucket: &Self) {
⋮----
let old_cap = old_bucket.capacity();
⋮----
let increment = self.contents.capacity_pow2() - old_bucket.contents.capacity_pow2();
⋮----
(0..old_cap as usize).for_each(|i| {
if !old_bucket.is_free(i as u64) {
self.copying_entry((i * index_grow) as u64, old_bucket, i as u64);
⋮----
let start = self.get_start_offset_with_header((i * index_grow) as u64);
⋮----
.occupy(&mut self.mmap[start..], i * index_grow);
⋮----
let dst = dst_slice.as_ptr() as *mut _;
let src = src_slice.as_ptr();
⋮----
m.stop();
self.stats.resizes.fetch_add(1, Ordering::Relaxed);
self.stats.resize_us.fetch_add(m.as_us(), Ordering::Relaxed);
⋮----
pub fn update_max_size(&self) {
self.stats.update_max_size(self.capacity());
⋮----
pub fn new_resized(
⋮----
.map(|bucket| Arc::clone(&bucket.count))
.unwrap_or_default(),
⋮----
new_bucket.copy_contents(bucket);
⋮----
new_bucket.update_max_size();
⋮----
pub(crate) fn capacity_bytes(&self) -> u64 {
self.capacity() * self.cell_size
⋮----
pub fn capacity(&self) -> u64 {
self.contents.capacity()
⋮----
mod test {
⋮----
fn test_bucket_storage_index_bucket() {
let tmpdir = tempdir().unwrap();
let paths: Vec<PathBuf> = vec![tmpdir.path().to_path_buf()];
assert!(!paths.is_empty());
⋮----
assert!(storage.is_free(ix));
assert!(storage.occupy(ix, false).is_ok());
⋮----
fn test_bucket_storage_using_header() {
⋮----
assert!(storage.occupy(ix, false).is_err());
assert!(!storage.is_free(ix));
storage.free(ix);
⋮----
fn test_load_on_restart_failures() {
⋮----
assert!(BucketStorage::<IndexBucket<u64>>::load_on_restart(
⋮----
let path = tmpdir.path().join("small");
⋮----
.create(true)
.truncate(true)
.open(path.clone())
.unwrap();
_ = file.write_all(&vec![1u8; len]);
drop(file);
assert_eq!(std::fs::metadata(&path).unwrap().len(), len as u64);
⋮----
NonZeroU64::new(elem_size).unwrap(),
⋮----
stats.clone(),
count.clone(),
⋮----
if let Some(result) = result.as_ref() {
assert_eq!(result.capacity() as usize, len / elem_size as usize);
⋮----
assert_eq!(result.is_none(), len < elem_size as usize, "{len}");
⋮----
fn test_load_on_restart() {
for request in [Some(7), None] {
⋮----
let expected_capacity = storage.capacity();
(0..num_elems).for_each(|ix| {
⋮----
let len = storage.mmap.len();
(0..expected_capacity as usize).for_each(|i| {
⋮----
let path = storage.path.clone();
drop(storage);
⋮----
assert_eq!(storage.capacity(), expected_capacity);
assert_eq!(len, storage.mmap.len());
⋮----
assert_eq!(storage.mmap[i], (i % 256) as u8);
⋮----
fn test_header_bad_size() {
struct BucketBadHeader;
impl BucketCapacity for BucketBadHeader {
⋮----
impl BucketOccupied for BucketBadHeader {
fn occupy(&mut self, _element: &mut [u8], _ix: usize) {
⋮----
fn free(&mut self, _element: &mut [u8], _ix: usize) {
⋮----
fn is_free(&self, _element: &[u8], _ix: usize) -> bool {
⋮----
fn offset_to_first_data() -> usize {
⋮----
fn new(_num_elements: Capacity) -> Self {

================
File: bucket_map/src/bucket.rs
================
use crate::bucket_item::BucketItem;
⋮----
pub struct ReallocatedItems<I: BucketOccupied, D: BucketOccupied> {
⋮----
impl<I: BucketOccupied, D: BucketOccupied> Default for ReallocatedItems<I, D> {
fn default() -> Self {
⋮----
pub struct Reallocated<I: BucketOccupied, D: BucketOccupied> {
⋮----
impl<I: BucketOccupied, D: BucketOccupied> Default for Reallocated<I, D> {
⋮----
pub fn add_reallocation(&self) {
assert_eq!(
⋮----
pub fn get_reallocated(&self) -> bool {
⋮----
.compare_exchange(1, 0, Ordering::Acquire, Ordering::Relaxed)
.is_ok()
⋮----
struct DataFileEntryToFree {
⋮----
pub struct Bucket<T: Copy + PartialEq + 'static> {
⋮----
/// index
    pub index: BucketStorage<IndexBucket<T>>,
/// random offset for the index
    random: u64,
/// storage buckets to store SlotSlice up to a power of 2 in len
    pub data: Vec<BucketStorage<DataBucket>>,
⋮----
/// # entries caller expects the map to need to contain.
    /// Used as a hint for the next time we need to grow.
⋮----
/// Used as a hint for the next time we need to grow.
    anticipated_size: u64,
⋮----
/// set to true once any entries have been deleted from the index.
    /// Deletes indicate that there can be free slots and that the full search range must be searched for an entry.
⋮----
/// Deletes indicate that there can be free slots and that the full search range must be searched for an entry.
    at_least_one_entry_deleted: bool,
/// keep track of which index file this bucket is using so on restart we can try to reuse it
    restartable_bucket: RestartableBucket,
/// true if this bucket was loaded (as opposed to created blank).
    /// When populating, we want to prioritize looking for data on disk that already matches as opposed to writing new data.
⋮----
/// When populating, we want to prioritize looking for data on disk that already matches as opposed to writing new data.
    reused_file_at_startup: bool,
⋮----
pub(crate) fn new(
⋮----
let elem_size = NonZeroU64::new(std::mem::size_of::<IndexEntry<T>>() as u64).unwrap();
⋮----
.and_then(|path| {
// try to reuse the file this bucket was using last time we were running
restartable_bucket.get().and_then(|(_file_name, random)| {
⋮----
path.clone(),
⋮----
count.clone(),
⋮----
.map(|index| (index, random, true /* true = reused file */));
if result.is_none() {
// we couldn't reuse it, so delete it
⋮----
.unwrap_or_else(|| {
⋮----
elem_size.into(),
⋮----
let random = rng().random();
restartable_bucket.set_file(file_name, random);
⋮----
stats.index.resize_grow(0, index.capacity_bytes());
⋮----
data: vec![],
⋮----
pub fn keys(&self) -> Vec<Pubkey> {
let mut rv = vec![];
for i in 0..self.index.capacity() {
if self.index.is_free(i) {
⋮----
let ix: &IndexEntry<T> = self.index.get(i);
rv.push(ix.key);
⋮----
pub fn items(&self) -> Vec<BucketItem<T>> {
let mut result = Vec::with_capacity(self.index.count.load(Ordering::Relaxed) as usize);
⋮----
let ii = i % self.index.capacity();
if self.index.is_free(ii) {
⋮----
let key = ix.key(&self.index);
let (v, ref_count) = ix.read_value(&self.index, &self.data);
result.push(BucketItem {
⋮----
slot_list: v.to_vec(),
⋮----
pub fn find_index_entry(&self, key: &Pubkey) -> Option<(IndexEntryPlaceInBucket<T>, u64)> {
⋮----
fn find_index_entry_mut(
⋮----
let ix = Self::bucket_index_ix(key, random) % index.capacity();
⋮----
let capacity = index.capacity();
for i in ix..ix + index.max_search() {
⋮----
if index.is_free(ii) {
if first_free.is_none() {
first_free = Some(ii);
⋮----
if elem.key(index) == key {
m.stop();
⋮----
.fetch_add(m.as_us(), Ordering::Relaxed);
return Ok((Some(elem), ii));
⋮----
Some(ii) => Ok((None, ii)),
None => Err(BucketMapError::IndexNoSpace(index.contents.capacity())),
⋮----
fn bucket_find_index_entry(
⋮----
let ii = i % index.capacity();
⋮----
return Some((elem, ii));
⋮----
fn bucket_create_key(
⋮----
if !index.is_free(ii) {
⋮----
index.occupy(ii, is_resizing).unwrap();
IndexEntryPlaceInBucket::new(ii).init(index, key);
return Ok(ii);
⋮----
Err(BucketMapError::IndexNoSpace(index.contents.capacity()))
⋮----
pub(crate) fn read_value(&self, key: &Pubkey) -> Option<(&[T], RefCount)> {
let (elem, _) = self.find_index_entry(key)?;
Some(elem.read_value(&self.index, &self.data))
⋮----
fn index_entries(items: &[(Pubkey, T)], random: u64) -> Vec<(u64, usize)> {
let mut inserts = Vec::with_capacity(items.len());
items.iter().enumerate().for_each(|(i, (key, _v))| {
⋮----
inserts.push((ix, i));
⋮----
pub(crate) fn batch_insert_non_duplicates(&mut self, items: &[(Pubkey, T)]) -> Vec<(usize, T)> {
assert!(
⋮----
let current_len = self.index.count.load(Ordering::Relaxed);
let anticipated = items.len() as u64;
self.set_anticipated_count((anticipated).saturating_add(current_len));
⋮----
let cap = self.index.capacity();
entries.sort_unstable_by(|a, b| (a.0 % cap).cmp(&(b.0 % cap)).reverse());
⋮----
self.set_anticipated_count(0);
self.index.count.fetch_add(
items.len().saturating_sub(duplicates.len()) as u64,
⋮----
stats.entries_reused.fetch_add(
⋮----
.len()
.saturating_sub(duplicates.len())
.saturating_sub(entries_created_on_disk) as u64,
⋮----
.fetch_add(entries_created_on_disk as u64, Ordering::Relaxed);
⋮----
self.grow(error);
self.handle_delayed_grows();
⋮----
pub fn batch_insert_non_duplicates_reusing_file(
⋮----
let max_search = index.max_search();
let cap = index.capacity();
let search_end = max_search.min(cap);
⋮----
'outer: while let Some((ix_entry_raw, ix)) = reverse_sorted_entries.pop() {
⋮----
// search for an empty spot starting at `ix_entry`
⋮----
match elem.occupy_if_matches(index, v, k) {
⋮----
// pubkey is same, and it is occupied, so we found a duplicate
⋮----
elem.read_value(index, data_buckets);
// someone is already allocated with this pubkey, so we found a duplicate
duplicates.push((ix, *v_existing.first().unwrap()));
⋮----
// fall through and look at next search value
⋮----
not_found.push((ix_entry_raw, ix));
⋮----
*reverse_sorted_entries = not_found.into_iter().rev().collect();
⋮----
pub fn batch_insert_non_duplicates_internal(
⋮----
'outer: while let Some((ix_entry_raw, i)) = reverse_sorted_entries.pop() {
⋮----
if index.try_lock(ix_index) {
⋮----
// found free element and occupied it.
// Note that since we are in the startup phase where we only add and do not remove, it is NOT possible to find this same pubkey AFTER
//  the index we started searching at, or we would have found it as occupied BEFORE we were able to lock it here.
//  This precondition is not true once we are able to delete entries.
// These fields will be overwritten after allocation by callers.
// Since this part of the mmapped file could have previously been used by someone else, there can be garbage here.
elem.init(index, k);
// new data stored should be stored in IndexEntry and NOT in data file
// new data len is 1
elem.set_slot_count_enum_value(index, OccupiedEnum::OneSlotInIndex(v));
⋮----
if elem.key(index) == k {
⋮----
duplicates.push((i, *v_existing.first().unwrap()));
continue 'outer; // this 'insertion' is completed: found a duplicate entry
⋮----
reverse_sorted_entries.push((ix_entry_raw, i));
return Err(BucketMapError::IndexNoSpace(cap));
⋮----
Ok(())
⋮----
pub fn try_write(
⋮----
// num_slots > 1 because we can store num_slots = 0 or num_slots = 1 in the index entry
⋮----
if requires_data_bucket && self.data.get(best_fit_bucket as usize).is_none() {
// fail early if the data bucket we need doesn't exist - we don't want the index entry partially allocated
return Err(BucketMapError::DataNoSpace((best_fit_bucket, 0)));
⋮----
let max_search = self.index.max_search();
⋮----
self.index.occupy(elem_ix, is_resizing).unwrap();
⋮----
elem_allocate.init(&mut self.index, key);
⋮----
elem.get_slot_count_enum(&self.index)
⋮----
let bucket_ix = multiple_slots.data_bucket_ix() as usize;
let loc = multiple_slots.data_loc(&self.data[bucket_ix]);
self.data[bucket_ix].free(loc);
⋮----
elem.set_slot_count_enum_value(
⋮----
if let Some(single_element) = data.next() {
⋮----
.store(true, Ordering::Relaxed);
⋮----
return Ok(());
⋮----
.as_ref()
.and_then(|elem| elem.get_multiple_slots_mut(&mut self.index))
⋮----
let elem_loc = multiple_slots.data_loc(current_bucket);
⋮----
assert!(!current_bucket.is_free(elem_loc));
let slice: &mut [T] = current_bucket.get_slice_mut(
⋮----
multiple_slots.set_num_slots(num_slots);
slice.iter_mut().zip(data).for_each(|(dest, src)| {
⋮----
old_data_entry_to_free = Some(DataFileEntryToFree {
⋮----
let cap_power = best_bucket.contents.capacity_pow2();
let cap = best_bucket.capacity();
let pos = rng().random_range(0..cap);
⋮----
for i in pos..pos + (max_search * 10).min(cap) {
⋮----
if best_bucket.is_free(ix) {
⋮----
multiple_slots.set_storage_offset(ix);
⋮----
.set_storage_capacity_when_created_pow2(best_bucket.contents.capacity_pow2());
⋮----
best_bucket.occupy(ix, false).unwrap();
⋮----
let slice = best_bucket.get_slice_mut(ix, num_slots, IncludeHeader::NoHeader);
⋮----
elem.unwrap_or_else(|| {
⋮----
.set_slot_count_enum_value(
⋮----
return Err(BucketMapError::DataNoSpace((best_fit_bucket, cap_power)));
⋮----
self.data[bucket_ix].free(location);
⋮----
pub fn delete_key(&mut self, key: &Pubkey) {
if let Some((elem, elem_ix)) = self.find_index_entry(key) {
⋮----
let ix = multiple_slots.data_bucket_ix() as usize;
⋮----
let loc = multiple_slots.data_loc(data_bucket);
⋮----
data_bucket.free(loc);
⋮----
self.index.free(elem_ix);
⋮----
pub(crate) fn set_anticipated_count(&mut self, count: u64) {
⋮----
pub fn grow_index(&self, mut current_capacity: u64) {
if self.index.contents.capacity() == current_capacity {
⋮----
let new_capacity = (current_capacity * 110 / 100).max(anticipated_size);
⋮----
current_capacity = index.capacity();
⋮----
for ix in 0..self.index.capacity() {
if !self.index.is_free(ix) {
let elem: &IndexEntry<T> = self.index.get(ix);
⋮----
if new_ix.is_err() {
⋮----
let new_ix = new_ix.unwrap();
let new_elem: &mut IndexEntry<T> = index.get_mut(new_ix);
⋮----
index.copying_entry(new_ix, &self.index, ix);
⋮----
self.stats.index.update_max_size(index.capacity());
let mut items = self.reallocated.items.lock().unwrap();
items.index = Some(index);
self.reallocated.add_reallocation();
self.restartable_bucket.set_file(file_name, self.random);
⋮----
.fetch_add(count - 1, Ordering::Relaxed);
⋮----
self.stats.index.resizes.fetch_add(1, Ordering::Relaxed);
⋮----
pub fn apply_grow_index(&mut self, mut index: BucketStorage<IndexBucket<T>>) {
⋮----
.resize_grow(self.index.capacity_bytes(), index.capacity_bytes());
if self.restartable_bucket.restart.is_some() {
⋮----
fn elem_size() -> u64 {
⋮----
fn add_data_bucket(&mut self, bucket: BucketStorage<DataBucket>) {
self.stats.data.file_count.fetch_add(1, Ordering::Relaxed);
self.stats.data.resize_grow(0, bucket.capacity_bytes());
self.data.push(bucket);
⋮----
pub fn apply_grow_data(&mut self, ix: usize, bucket: BucketStorage<DataBucket>) {
if self.data.get(ix).is_none() {
for i in self.data.len()..ix {
self.add_data_bucket(
⋮----
self.add_data_bucket(bucket);
⋮----
.resize_grow(data_bucket.capacity_bytes(), bucket.capacity_bytes());
⋮----
pub fn grow_data(&self, data_index: u64, current_capacity_pow2: u8) {
⋮----
self.data.get(data_index as usize),
⋮----
items.data = Some((data_index, new_bucket));
⋮----
fn bucket_index_ix(key: &Pubkey, random: u64) -> u64 {
⋮----
hasher_builder.hash_one(key)
⋮----
pub(crate) fn grow(&self, err: BucketMapError) {
⋮----
self.grow_data(data_index, current_capacity_pow2);
⋮----
self.grow_index(current_capacity);
⋮----
pub fn handle_delayed_grows(&mut self) {
if self.reallocated.get_reallocated() {
let mut items = std::mem::take(&mut *self.reallocated.items.lock().unwrap());
if let Some(bucket) = items.index.take() {
self.apply_grow_index(bucket);
⋮----
let (i, new_bucket) = items.data.take().unwrap();
self.apply_grow_data(i as usize, new_bucket);
⋮----
pub fn insert(&mut self, key: &Pubkey, value: (&[T], RefCount)) {
⋮----
let Err(err) = self.try_write(key, new.iter(), new.len(), refct) else {
⋮----
self.grow(err);
⋮----
pub fn update<F>(&mut self, key: &Pubkey, mut updatefn: F)
⋮----
let current = self.read_value(key);
let new = updatefn(current);
if new.is_none() {
self.delete_key(key);
⋮----
let (new, refct) = new.unwrap();
self.insert(key, (&new, refct));
⋮----
mod tests {
⋮----
fn test_batch_insert_non_duplicates_reusing_file_many_entries() {
⋮----
.map(|l| (Pubkey::from([(l + 1) as u8; 32]), v + (l as u64)))
⋮----
let hashed_raw = hashed.clone();
let tmpdir = tempdir().unwrap();
let paths: Arc<Vec<PathBuf>> = Arc::new(vec![tmpdir.path().to_path_buf()]);
assert!(!paths.is_empty());
⋮----
paths.clone(),
⋮----
hashed.sort_unstable_by(|a, b| (a.0 % cap).cmp(&(b.0 % cap)).reverse());
hashed.windows(2).for_each(|two| {
assert_ne!(two[0].0 % cap, two[1].0 % cap, "{two:?}, cap: {cap}");
⋮----
.unwrap();
assert!(hashed.is_empty());
assert!(duplicates.is_empty());
hashed_raw.iter().for_each(|(hash, i)| {
⋮----
assert_eq!(entry.key(&index), &k);
⋮----
drop(index);
let path = paths.first().unwrap().join(file_name.to_string());
⋮----
.unwrap(),
⋮----
assert_eq!(entry.get_slot_count_enum(&index), OccupiedEnum::Free);
⋮----
hashed.clone_from(&hashed_raw);
⋮----
assert_eq!(entries_created, hashed_raw.len());
⋮----
assert_eq!(entries_created, 0);
⋮----
fn test_batch_insert_non_duplicates_reusing_file_blank_file() {
⋮----
let mut index = create_test_index(None);
⋮----
assert_eq!(entry.key(&index), &Pubkey::default());
assert_eq!(hashed, hashed_raw, "len: {len}");
⋮----
fn test_batch_insert_non_duplicates_reusing_file_insert_twice() {
⋮----
entry.init(&mut index, &raw[0].0);
entry.set_slot_count_enum_value(&mut index, OccupiedEnum::OneSlotInIndex(&raw[0].1));
⋮----
fn test_batch_insert_non_duplicates_reusing_file_insert_duplicate() {
⋮----
entry.init(&mut index, &(raw[0].0));
⋮----
entry.set_slot_count_enum_value(&mut index, OccupiedEnum::OneSlotInIndex(&non_matching_v));
⋮----
assert_eq!(duplicates, vec![(0, non_matching_v)], "len: {len}");
⋮----
fn test_batch_insert_non_duplicates_reusing_file_skip_one() {
⋮----
let other = raw.pop().unwrap();
⋮----
entry.init(&mut index, &(other.0));
⋮----
entry.set_slot_count_enum_value(&mut index, OccupiedEnum::Free);
⋮----
assert_eq!(entry.key(&index), &other.0);
⋮----
assert_eq!(entry.key(&index), &raw[0].0);
⋮----
fn test_batch_insert_non_duplicates_reusing_file_existing_zero() {
⋮----
entry.set_slot_count_enum_value(&mut index, OccupiedEnum::ZeroSlots);
⋮----
fn test_index_entries() {
⋮----
.map(|l| {
⋮----
assert_eq!(hashed.len(), len);
(0..len).for_each(|i| {
⋮----
assert_eq!(Bucket::<u64>::bucket_index_ix(&raw.0, random), hashed.0);
assert_eq!(i, hashed.1);
⋮----
fn create_test_index(max_search: Option<u8>) -> BucketStorage<IndexBucket<u64>> {
⋮----
let paths: Vec<PathBuf> = vec![tmpdir.path().to_path_buf()];
⋮----
let max_search = max_search.unwrap_or(2);
⋮----
fn batch_insert_duplicates_internal_simple() {
⋮----
let raw = (0..len).map(|l| (k, v + (l as u64))).collect::<Vec<_>>();
⋮----
assert!(Bucket::<u64>::batch_insert_non_duplicates_internal(
⋮----
assert_eq!(duplicates.len(), len as usize - 1);
assert_eq!(hashed.len(), 0);
let single_hashed_raw_inserted = hashed_raw.last().unwrap();
⋮----
single_hashed_raw_inserted.0 % index.capacity(),
⋮----
let (value, ref_count) = elem.read_value(&index, &data_buckets);
assert_eq!(ref_count, 1);
assert_eq!(value, &[raw[single_hashed_raw_inserted.1].1]);
⋮----
.iter()
.rev()
.skip(1)
.map(|(_hash, i)| (*i, raw[single_hashed_raw_inserted.1].1))
⋮----
assert_eq!(expected_duplicates, duplicates);
⋮----
fn batch_insert_non_duplicates_internal_simple() {
⋮----
let elem = IndexEntryPlaceInBucket::new(raw2.0 % index.capacity());
⋮----
assert_eq!(value, &[raw[hashed_raw[i].1].1]);
⋮----
fn batch_insert_non_duplicates_internal_same_ix_exceeds_max_search() {
⋮----
hashed.iter_mut().for_each(|v| {
⋮----
let mut index = create_test_index(Some(max_search as u8));
⋮----
assert!(if len > max_search {
⋮----
assert_eq!(hashed[0], hashed_raw[0]);
⋮----
(raw2.0 + search_required) % index.capacity(),
⋮----
fn test_occupy_if_matches() {
⋮----
let raw = vec![(k, v)];
⋮----
let elem = IndexEntryPlaceInBucket::new(single_hashed_raw_inserted.0 % index.capacity());
assert_eq!(elem.get_slot_count_enum(&index), OccupiedEnum::Free);
elem.init(&mut index, &k);
elem.set_slot_count_enum_value(&mut index, OccupiedEnum::OneSlotInIndex(&v));
⋮----
elem.set_slot_count_enum_value(&mut index, OccupiedEnum::Free);
⋮----
fn test_occupy_if_matches_panic() {
⋮----
fn batch_insert_after_delete() {
⋮----
assert_eq!(bucket.read_value(&key), None);
bucket.update(&key, |_| Some((vec![0], 0)));
bucket.delete_key(&key);
bucket.batch_insert_non_duplicates(&[]);
⋮----
fn test_bucket_index_ix_is_stable() {
⋮----
assert_eq!(ix1, 0x0CAD_75DB_E472_9589);
⋮----
assert_ne!(ix2, ix1);

================
File: bucket_map/src/index_entry.rs
================
impl BucketCapacity for BucketWithHeader {
fn capacity(&self) -> u64 {
self.capacity_pow2.capacity()
⋮----
fn capacity_pow2(&self) -> u8 {
self.capacity_pow2.capacity_pow2()
⋮----
struct DataBucketRefCountOccupiedHeader {
⋮----
pub enum OccupyIfMatches {
⋮----
pub struct BucketWithHeader {
⋮----
impl BucketOccupied for BucketWithHeader {
fn occupy(&mut self, element: &mut [u8], ix: usize) {
assert!(self.is_free(element, ix));
⋮----
entry.packed_ref_count.set_occupied(OCCUPIED_OCCUPIED);
⋮----
fn free(&mut self, element: &mut [u8], ix: usize) {
assert!(!self.is_free(element, ix));
⋮----
entry.packed_ref_count.set_occupied(OCCUPIED_FREE);
⋮----
fn is_free(&self, element: &[u8], _ix: usize) -> bool {
⋮----
entry.packed_ref_count.occupied() == OCCUPIED_FREE
⋮----
fn offset_to_first_data() -> usize {
⋮----
fn new(capacity: Capacity) -> Self {
assert!(matches!(capacity, Capacity::Pow2(_)));
⋮----
pub struct IndexBucketUsingBitVecBits<T: PartialEq + 'static> {
/// 2 bits per entry that represent a 4 state enum tag
    pub enum_tag_first_bit: BitVec,
/// second will be empty in all healthy cases because in real use, we only use enum values 0 and 2 (and we use the high bit for first)
    pub enum_tag_second_bit: BitVec,
/// number of elements allocated
    capacity: u64,
⋮----
/// set the 2 bits (first and second) in `enum_tag`
    pub(crate) fn set_bits(&mut self, ix: u64, first: bool, second: bool) {
⋮----
pub(crate) fn set_bits(&mut self, ix: u64, first: bool, second: bool) {
self.enum_tag_first_bit.set(ix, first);
if self.enum_tag_second_bit.is_empty() {
⋮----
// enum_tag_second_bit can remain empty.
// The first time someone sets the second bit, we have to allocate and check it.
⋮----
self.enum_tag_second_bit.set(ix, second);
⋮----
/// get the 2 bits (first and second) in `enum_tag`
    fn get_bits(&self, ix: u64) -> (bool, bool) {
⋮----
fn get_bits(&self, ix: u64) -> (bool, bool) {
⋮----
self.enum_tag_first_bit.get(ix),
⋮----
self.enum_tag_second_bit.get(ix)
⋮----
/// turn the tag into bits and store them
    fn set_enum_tag(&mut self, ix: u64, value: OccupiedEnumTag) {
⋮----
fn set_enum_tag(&mut self, ix: u64, value: OccupiedEnumTag) {
⋮----
self.set_bits(ix, (value & 2) == 2, (value & 1) == 1);
⋮----
/// read the bits and convert them to an enum tag
    fn get_enum_tag(&self, ix: u64) -> OccupiedEnumTag {
⋮----
fn get_enum_tag(&self, ix: u64) -> OccupiedEnumTag {
let (first, second) = self.get_bits(ix);
⋮----
impl<T: Copy + PartialEq + 'static> BucketOccupied for IndexBucketUsingBitVecBits<T> {
⋮----
self.set_enum_tag(ix as u64, OccupiedEnumTag::Free);
⋮----
fn is_free(&self, _element: &[u8], ix: usize) -> bool {
self.get_enum_tag(ix as u64) == OccupiedEnumTag::Free
⋮----
enum_tag_first_bit: BitVec::new_fill(false, capacity.capacity()),
⋮----
capacity: capacity.capacity(),
⋮----
fn copying_entry(
⋮----
self.set_enum_tag(ix_new as u64, other.get_enum_tag(ix_old as u64));
⋮----
impl<T: PartialEq> BucketCapacity for IndexBucketUsingBitVecBits<T> {
⋮----
pub type DataBucket = BucketWithHeader;
pub type IndexBucket<T> = IndexBucketUsingBitVecBits<T>;
pub struct IndexEntryPlaceInBucket<T: 'static> {
⋮----
pub struct IndexEntry<T: Clone + Copy> {
⋮----
pub(crate) struct PackedRefCount {
⋮----
pub(crate) struct MultipleSlots {
⋮----
impl MultipleSlots {
pub(crate) fn set_storage_capacity_when_created_pow2(
⋮----
.set_capacity_when_created_pow2(storage_capacity_when_created_pow2)
⋮----
pub(crate) fn set_storage_offset(&mut self, storage_offset: u64) {
⋮----
.set_offset_checked(storage_offset)
.expect("New storage offset must fit into 7 bytes!")
⋮----
fn storage_capacity_when_created_pow2(&self) -> u8 {
self.storage_cap_and_offset.capacity_when_created_pow2()
⋮----
fn storage_offset(&self) -> u64 {
self.storage_cap_and_offset.offset()
⋮----
pub(crate) fn num_slots(&self) -> Slot {
⋮----
pub(crate) fn set_num_slots(&mut self, num_slots: Slot) {
⋮----
pub(crate) fn data_bucket_ix(&self) -> u64 {
Self::data_bucket_from_num_slots(self.num_slots())
⋮----
pub(crate) fn data_bucket_from_num_slots(num_slots: Slot) -> u64 {
⋮----
(Slot::BITS - (num_slots - 1).leading_zeros()) as u64
⋮----
pub(crate) fn data_loc(&self, storage: &BucketStorage<DataBucket>) -> u64 {
self.storage_offset()
<< (storage.contents.capacity_pow2() - self.storage_capacity_when_created_pow2())
⋮----
pub fn set_ref_count(
⋮----
.set_ref_count(ref_count);
⋮----
pub fn ref_count(data_bucket: &BucketStorage<DataBucket>, data_ix: u64) -> RefCount {
⋮----
.ref_count()
⋮----
pub(crate) union SingleElementOrMultipleSlots<T: Clone + Copy> {
⋮----
enum OccupiedEnumTag {
⋮----
pub(crate) enum OccupiedEnum<'a, T> {
/// this spot is not occupied.
    /// ALL other enum values ARE occupied.
⋮----
/// ALL other enum values ARE occupied.
    Free = OccupiedEnumTag::Free as u8,
/// zero slots in the slot list
    ZeroSlots = OccupiedEnumTag::ZeroSlots as u8,
/// one slot in the slot list, it is stored in the index
    OneSlotInIndex(&'a T) = OccupiedEnumTag::OneSlotInIndex as u8,
⋮----
/// Pack the storage offset and capacity-when-created-pow2 fields into a single u64
#[bitfield(bits = 64)]
⋮----
struct PackedStorage {
⋮----
pub(crate) fn get_slot_count_enum<'a>(
⋮----
let enum_tag = index_bucket.contents.get_enum_tag(self.ix);
⋮----
/// return Some(MultipleSlots) if this item's data is stored in the data file
    pub(crate) fn get_multiple_slots_mut<'a>(
⋮----
pub(crate) fn get_multiple_slots_mut<'a>(
⋮----
Some(&mut index_entry.contents.multiple_slots)
⋮----
/// make this index entry reflect `value`
    pub(crate) fn set_slot_count_enum_value<'a>(
⋮----
pub(crate) fn set_slot_count_enum_value<'a>(
⋮----
index_bucket.contents.set_enum_tag(self.ix, tag);
⋮----
pub fn init(&self, index_bucket: &mut BucketStorage<IndexBucket<T>>, pubkey: &Pubkey) {
⋮----
pub(crate) fn occupy_if_matches(
⋮----
assert_eq!(
⋮----
.set_enum_tag(self.ix, OccupiedEnumTag::OneSlotInIndex);
⋮----
self.set_slot_count_enum_value(index_bucket, OccupiedEnum::OneSlotInIndex(data));
⋮----
pub(crate) fn read_value<'a>(
⋮----
let slot_list = match self.get_slot_count_enum(index_bucket) {
⋮----
let loc = multiple_slots.data_loc(data_bucket);
assert!(!data_bucket.is_free(loc));
⋮----
panic!("trying to read data from a free entry");
⋮----
pub fn new(ix: u64) -> Self {
⋮----
pub fn key<'a>(&self, index_bucket: &'a BucketStorage<IndexBucket<T>>) -> &'a Pubkey {
let entry: &IndexEntry<T> = index_bucket.get(self.ix);
⋮----
fn get_from_bytes<T>(item_slice: &[u8]) -> &T {
debug_assert!(std::mem::size_of::<T>() <= item_slice.len());
let item = item_slice.as_ptr().cast();
debug_assert!((item as usize).is_multiple_of(std::mem::align_of::<T>()));
⋮----
fn get_mut_from_bytes<T>(item_slice: &mut [u8]) -> &mut T {
⋮----
let item = item_slice.as_mut_ptr().cast();
⋮----
mod tests {
⋮----
fn test_api() {
⋮----
multiple_slots.set_storage_offset(offset);
⋮----
assert_eq!(multiple_slots.storage_offset(), offset);
assert_eq!(multiple_slots.storage_capacity_when_created_pow2(), 0);
⋮----
multiple_slots.set_storage_capacity_when_created_pow2(pow);
⋮----
assert_eq!(multiple_slots.storage_capacity_when_created_pow2(), pow);
⋮----
fn test_size() {
assert_eq!(std::mem::size_of::<PackedStorage>(), 1 + 7);
assert_eq!(std::mem::size_of::<IndexEntry<u64>>(), 32 + 8 + 8);
⋮----
fn test_set_storage_offset_value_too_large() {
⋮----
multiple_slots.set_storage_offset(too_big);
⋮----
fn test_data_bucket_from_num_slots() {

================
File: bucket_map/src/lib.rs
================
mod bucket;
pub mod bucket_api;
mod bucket_item;
pub mod bucket_map;
mod bucket_stats;
mod bucket_storage;
mod index_entry;
mod restart;
pub type MaxSearch = u8;
pub type RefCount = u64;

================
File: bucket_map/src/restart.rs
================
pub(crate) struct Header {
⋮----
const _: () = assert!(
⋮----
pub(crate) struct OneIndexBucket {
⋮----
pub(crate) struct Restart {
⋮----
pub(crate) struct RestartableBucket {
⋮----
impl RestartableBucket {
pub(crate) fn set_file(&self, file_name: u128, random: u64) {
if let Some(mut restart) = self.restart.as_ref().map(|restart| restart.lock().unwrap()) {
let bucket = restart.get_bucket_mut(self.index);
⋮----
pub(crate) fn get(&self) -> Option<(u128, u64)> {
self.restart.as_ref().map(|restart| {
let restart = restart.lock().unwrap();
let bucket = restart.get_bucket(self.index);
⋮----
impl Debug for RestartableBucket {
fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
write!(
⋮----
Ok(())
⋮----
impl Debug for Restart {
⋮----
let header = self.get_header();
writeln!(f, "{header:?}")?;
⋮----
impl Restart {
pub(crate) fn new(config: &BucketMapConfig) -> Option<Restart> {
⋮----
let path = config.restart_config_file.as_ref();
⋮----
_ = remove_file(path);
let mmap = Self::new_map(path, expected_len as u64).ok()?;
⋮----
let header = restart.get_header_mut();
⋮----
header.max_search = config.max_search.unwrap_or(MAX_SEARCH_DEFAULT);
(0..config.max_buckets).for_each(|index| {
let bucket = restart.get_bucket_mut(index);
⋮----
Some(restart)
⋮----
pub(crate) fn get_restart_file(config: &BucketMapConfig) -> Option<Restart> {
let path = config.restart_config_file.as_ref()?;
let metadata = std::fs::metadata(path).ok()?;
let file_len = metadata.len();
⋮----
.read(true)
.write(true)
.create(false)
.open(path)
.ok()?;
let mmap = unsafe { MmapMut::map_mut(&file).unwrap() };
⋮----
let header = restart.get_header();
⋮----
|| header.max_search != config.max_search.unwrap_or(MAX_SEARCH_DEFAULT)
⋮----
fn expected_len(max_buckets: usize) -> usize {
⋮----
fn get_all_possible_index_files_in_drives(drives: &[PathBuf]) -> HashMap<u128, PathBuf> {
⋮----
drives.iter().for_each(|drive| {
if drive.is_dir() {
⋮----
for entry in dir.flatten() {
if let Some(name) = entry.path().file_name() {
if let Some(id) = name.to_str().and_then(|str| str.parse::<u128>().ok())
⋮----
result.insert(id, entry.path());
⋮----
pub(crate) fn get_restartable_buckets(
⋮----
.map(|index| {
let path = restart.and_then(|restart| {
⋮----
let id = restart.get_bucket(index).file_name;
paths.remove(&id)
⋮----
restart: restart.cloned(),
⋮----
.collect();
paths.into_iter().for_each(|path| {
⋮----
fn new_map(file: impl AsRef<Path>, capacity: u64) -> Result<MmapMut, std::io::Error> {
⋮----
.create_new(true)
.open(file)?;
⋮----
data.seek(SeekFrom::Start(capacity - 1)).unwrap();
data.write_all(&[0]).unwrap();
data.rewind().unwrap();
⋮----
data.flush().unwrap();
Ok(unsafe { MmapMut::map_mut(&data).unwrap() })
⋮----
fn get_header(&self) -> &Header {
⋮----
fn get_header_mut(&mut self) -> &mut Header {
⋮----
fn get_bucket(&self, index: usize) -> &OneIndexBucket {
⋮----
fn get_bucket_mut(&mut self, index: usize) -> &mut OneIndexBucket {
⋮----
mod test {
⋮----
fn test_header_alignment() {
assert_eq!(
⋮----
fn test_get_restartable_buckets() {
let tmpdir = tempdir().unwrap();
let paths: Vec<PathBuf> = vec![tmpdir.path().to_path_buf()];
assert!(!paths.is_empty());
let config_file = tmpdir.path().join("config");
⋮----
drives: Some(paths.clone()),
restart_config_file: Some(config_file.clone()),
⋮----
let restart = Arc::new(Mutex::new(Restart::new(&config).unwrap()));
⋮----
assert!(files.is_empty());
⋮----
.map(|bucket| RestartableBucket {
restart: Some(restart.clone()),
⋮----
(0..config.max_buckets + 1).for_each(|i| {
⋮----
let file = tmpdir.path().join(file_name.to_string());
create_dummy_file(&file);
⋮----
restartable_buckets[i].set_file(file_name, random);
assert_eq!(Some((file_name, random)), restartable_buckets[i].get());
⋮----
let deleted_file = tmpdir.path().join((1 + config.max_buckets).to_string());
assert!(std::fs::metadata(deleted_file.clone()).is_ok());
⋮----
Some(&restart),
&Arc::new(paths.clone()),
⋮----
(0..config.max_buckets).for_each(|i| {
⋮----
assert_eq!(Some((0, 0)), restartable_buckets[i].get());
assert_eq!(None, calc_restartable_buckets[i].path);
⋮----
let expected_path = tmpdir.path().join(file_name.to_string());
assert!(std::fs::metadata(expected_path.clone()).is_ok());
⋮----
assert_eq!(Some(expected_path), calc_restartable_buckets[i].path);
⋮----
assert!(
⋮----
fn create_dummy_file(path: &Path) {
⋮----
restart_config_file: Some(path.to_path_buf()),
⋮----
assert!(Restart::new(&config).is_some());
⋮----
fn test_get_all_possible_index_files_in_drives() {
⋮----
create_dummy_file(&tmpdir.path().join("config"));
⋮----
assert_eq!(files.remove(&file_name), Some(&file).cloned());
⋮----
u128::MAX.to_string() + ".",
u128::MAX.to_string() + "0",
"-123".to_string(),
⋮----
let file = tmpdir.path().join(file_name);
⋮----
assert!(files.is_empty(), "{files:?}");
⋮----
let tmpdir2 = tempdir().unwrap();
⋮----
vec![paths.first().unwrap().clone(), tmpdir2.path().to_path_buf()];
(0..4).for_each(|i| {
⋮----
let file = parent.path().join(i.to_string());
⋮----
assert_eq!(files.len(), 2);
(0..2).for_each(|file_name| {
let path = files.remove(&file_name).unwrap();
assert_eq!(tmpdir.path().join(file_name.to_string()), path);
⋮----
assert_eq!(files.len(), 4);
⋮----
(2..4).for_each(|file_name| {
⋮----
assert_eq!(tmpdir2.path().join(file_name.to_string()), path);
⋮----
fn test_restartable_bucket_load() {
⋮----
test_default_restart(&restart, &config);
drop(restart);
⋮----
assert!(restart.is_some());
⋮----
assert!(restart.is_none());
⋮----
max_search: Some(MAX_SEARCH_DEFAULT + 1),
⋮----
restart.lock().unwrap().get_header_mut().version = HEADER_VERSION + 1;
⋮----
let path = path.unwrap();
⋮----
let mmap = Restart::new_map(path, wrong_file_len as u64).unwrap();
drop(mmap);
⋮----
assert_eq!(restart.is_none(), smaller_bigger != 1);
⋮----
fn test_restartable_bucket() {
⋮----
drives: Some(paths),
restart_config_file: Some(config_file),
⋮----
(0..=last_offset).for_each(|offset| test_set_get(&restart, buckets, offset));
⋮----
let restart = Arc::new(Mutex::new(Restart::get_restart_file(&config).unwrap()));
test_get(&restart, buckets, last_offset);
(4..6).for_each(|offset| test_set_get(&restart, buckets, offset));
⋮----
fn test_set_get(restart: &Arc<Mutex<Restart>>, buckets: usize, test_offset: usize) {
test_set(restart, buckets, test_offset);
test_get(restart, buckets, test_offset);
⋮----
fn test_set(restart: &Arc<Mutex<Restart>>, buckets: usize, test_offset: usize) {
(0..buckets).for_each(|bucket| {
⋮----
assert!(restartable_bucket.get().is_some());
⋮----
restartable_bucket.set_file(file_name, random);
assert_eq!(restartable_bucket.get(), Some((file_name, random)));
⋮----
fn test_get(restart: &Arc<Mutex<Restart>>, buckets: usize, test_offset: usize) {
⋮----
fn test_default_restart(restart: &Arc<Mutex<Restart>>, config: &BucketMapConfig) {
⋮----
assert_eq!(header.version, HEADER_VERSION);
assert_eq!(header.buckets, config.max_buckets as u64);
⋮----
assert_eq!(restartable_bucket.get(), Some((0, 0)));

================
File: bucket_map/tests/bucket_map.rs
================
fn bucket_map_test_mt() {
⋮----
let tmpdir1 = std::env::temp_dir().join("bucket_map_test_mt");
let tmpdir2 = PathBuf::from("/mnt/data/0").join("bucket_map_test_mt");
⋮----
.iter()
.filter(|x| std::fs::create_dir_all(x).is_ok())
.cloned()
.collect();
assert!(!paths.is_empty());
⋮----
drives: Some(paths.clone()),
⋮----
(0..threads).into_par_iter().for_each(|_| {
⋮----
index.update(&key, |_| Some((vec![0u64], 0)));
⋮----
let ix: u64 = index.bucket_ix(&key) as u64;
index.update(&key, |_| Some((vec![ix], 0)));
assert_eq!(index.read_value(&key), Some((vec![ix], 0)));
⋮----
timer.stop();
println!("time: {}ns per item", timer.as_ns() / (threads * items));
⋮----
for tmpdir in paths.iter() {
let folder_size = fs_extra::dir::get_size(tmpdir).unwrap();
⋮----
std::fs::remove_dir_all(tmpdir).unwrap();
⋮----
println!("overhead: {}bytes per item", total / (threads * items));

================
File: bucket_map/Cargo.toml
================
[package]
name = "solana-bucket-map"
description = "solana-bucket-map"
documentation = "https://docs.rs/solana-bucket-map"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["lib"]
name = "solana_bucket_map"

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
ahash = { workspace = true }
bv = { workspace = true, features = ["serde"] }
bytemuck = { workspace = true }
bytemuck_derive = { workspace = true }
memmap2 = { workspace = true }
modular-bitfield = { workspace = true }
num_enum = { workspace = true }
rand = { workspace = true }
solana-clock = { workspace = true }
solana-measure = { workspace = true }
solana-pubkey = { workspace = true }
tempfile = { workspace = true }

[dev-dependencies]
agave-logger = { workspace = true }
fs_extra = { workspace = true }
rayon = { workspace = true }
solana-bucket-map = { path = ".", features = ["agave-unstable-api", "dev-context-only-utils"] }
solana-pubkey = { workspace = true, features = ["rand"] }

================
File: builtins/src/core_bpf_migration.rs
================
pub enum CoreBpfMigrationTargetType {
⋮----
pub struct CoreBpfMigrationConfig {

================
File: builtins/src/lib.rs
================
pub mod core_bpf_migration;
pub mod prototype;
⋮----
macro_rules! testable_prototype {
⋮----
testable_prototype!(BuiltinPrototype {
⋮----
core_bpf_migration_config: Some(CoreBpfMigrationConfig {
⋮----
verified_build_hash: Some(buffer_accounts::slashing_program::VERIFIED_BUILD_HASH),
⋮----
mod buffer_accounts {
pub mod slashing_program {
⋮----
pub mod test_only {
⋮----
pub mod system_program {
pub mod feature {
⋮----
pub mod source_buffer {
⋮----
pub mod upgrade_authority {
⋮----
upgrade_authority_address: Some(upgrade_authority::id()),
⋮----
pub mod vote_program {
⋮----
pub mod solana_bpf_loader_deprecated_program {
⋮----
pub mod solana_bpf_loader_program {
⋮----
pub mod solana_bpf_loader_upgradeable_program {
⋮----
pub mod compute_budget_program {
⋮----
pub mod zk_token_proof_program {
⋮----
pub mod loader_v4 {
⋮----
pub mod zk_elgamal_proof_program {
⋮----
mod tests {
⋮----
fn test_testable_prototypes() {
assert_eq!(

================
File: builtins/src/prototype.rs
================
pub struct BuiltinPrototype {
⋮----
/// The program's entrypoint function.
    pub entrypoint: BuiltinFunctionWithContext,
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
let mut builder = f.debug_struct("BuiltinPrototype");
builder.field("program_id", &self.program_id);
builder.field("name", &self.name);
builder.field("enable_feature_id", &self.enable_feature_id);
builder.field("core_bpf_migration_config", &self.core_bpf_migration_config);
builder.finish()
⋮----
pub struct StatelessBuiltinPrototype {

================
File: builtins/Cargo.toml
================
[package]
name = "solana-builtins"
description = "Solana builtin programs"
documentation = "https://docs.rs/solana-builtins"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
agave-feature-set = { workspace = true }
solana-bpf-loader-program = { workspace = true }
solana-compute-budget-program = { workspace = true }
solana-hash = { workspace = true }
solana-loader-v4-program = { workspace = true }
solana-program-runtime = { workspace = true }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-system-program = { workspace = true }
solana-vote-program = { workspace = true }
solana-zk-elgamal-proof-program = { workspace = true }
solana-zk-token-proof-program = { workspace = true }

[lints]
workspace = true

================
File: builtins-default-costs/src/lib.rs
================
use qualifier_attr::field_qualifiers;
⋮----
pub struct MigratingBuiltinCost {
⋮----
pub enum BuiltinCost {
⋮----
impl BuiltinCost {
fn core_bpf_migration_feature(&self) -> Option<&Pubkey> {
⋮----
}) => Some(core_bpf_migration_feature),
⋮----
fn position(&self) -> Option<usize> {
⋮----
BuiltinCost::Migrating(MigratingBuiltinCost { position, .. }) => Some(*position),
⋮----
.iter()
.chain(NON_MIGRATING_BUILTINS_COSTS.iter())
.cloned()
.collect()
⋮----
.keys()
.for_each(|key| temp_table[key.as_ref()[0] as usize] = true);
⋮----
pub enum BuiltinMigrationFeatureIndex {
⋮----
pub fn get_builtin_migration_feature_index(program_id: &Pubkey) -> BuiltinMigrationFeatureIndex {
BUILTIN_INSTRUCTION_COSTS.get(program_id).map_or(
⋮----
builtin_cost.position().map_or(
⋮----
const fn validate_position(migrating_builtins: &[(Pubkey, BuiltinCost)]) {
⋮----
while index < migrating_builtins.len() {
⋮----
BuiltinCost::Migrating(MigratingBuiltinCost { position, .. }) => assert!(
⋮----
panic!("migration feature must exist and at correct position")
⋮----
const _: () = validate_position(MIGRATING_BUILTINS_COSTS);
pub fn get_migration_feature_id(index: usize) -> &'static Pubkey {
⋮----
.get(index)
.expect("valid index of MIGRATING_BUILTINS_COSTS")
⋮----
.core_bpf_migration_feature()
.expect("migrating builtin")
⋮----
pub fn get_migration_feature_position(feature_id: &Pubkey) -> usize {
⋮----
.position(|(_, c)| c.core_bpf_migration_feature().expect("migrating builtin") == feature_id)
.unwrap()
⋮----
mod test {
⋮----
fn test_const_builtin_cost_arrays() {
assert!(MIGRATING_BUILTINS_COSTS
⋮----
assert!(NON_MIGRATING_BUILTINS_COSTS
⋮----
fn test_get_builtin_migration_feature_index() {
assert!(matches!(
⋮----
let feature_index = get_builtin_migration_feature_index(program_id);
⋮----
panic!("expect migrating builtin")
⋮----
assert_eq!(
⋮----
fn test_get_migration_feature_id_invalid_index() {
let _ = get_migration_feature_id(MIGRATING_BUILTINS_COSTS.len() + 1);

================
File: builtins-default-costs/Cargo.toml
================
[package]
name = "solana-builtins-default-costs"
description = "Solana builtins default costs"
documentation = "https://docs.rs/solana-builtins-default-costs"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]
# Add additional builtin programs here

[lib]
crate-type = ["lib"]
name = "solana_builtins_default_costs"

[features]
agave-unstable-api = []
frozen-abi = ["dep:solana-frozen-abi", "solana-vote-program/frozen-abi"]
dev-context-only-utils = ["dep:qualifier_attr"]

[dependencies]
agave-feature-set = { workspace = true }
ahash = { workspace = true }
log = { workspace = true }
qualifier_attr = { workspace = true, optional = true }
solana-bpf-loader-program = { workspace = true }
solana-compute-budget-program = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-loader-v4-program = { workspace = true, features = ["agave-unstable-api"] }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-system-program = { workspace = true }
solana-vote-program = { workspace = true }

[dev-dependencies]
rand = "0.8.5"
static_assertions = { workspace = true }

[lints]
workspace = true

================
File: bundle/src/lib.rs
================
pub fn derive_bundle_id(transactions: &[impl TransactionWithMeta]) -> String {
⋮----
hasher.update(transactions.iter().map(|tx| tx.signature()).join(","));
format!("{:x}", hasher.finalize())

================
File: bundle/Cargo.toml
================
[package]
name = "solana-bundle"
description = "Library related to handling bundles"
documentation = "https://docs.rs/solana-bundle"
readme = "../README.md"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[lib]
crate-type = ["lib"]
name = "solana_bundle"

[features]
agave-unstable-api = []

[dependencies]
itertools = { workspace = true }
sha2 = { workspace = true }
solana-runtime-transaction = { workspace = true }

================
File: cargo
================
#!/usr/bin/env bash

if [[ -n "${NO_RUSTUP_OVERRIDE}" ]]; then
  set -x
  exec cargo "${@}"
fi

# shellcheck source=ci/rust-version.sh
here=$(dirname "$0")

toolchain=
case "$1" in
  stable)
    source "${here}"/ci/rust-version.sh stable
    # shellcheck disable=SC2054 # rust_stable is sourced from rust-version.sh
    toolchain="$rust_stable"
    shift
    ;;
  nightly)
    source "${here}"/ci/rust-version.sh nightly
    # shellcheck disable=SC2054 # rust_nightly is sourced from rust-version.sh
    toolchain="$rust_nightly"
    shift
    ;;
  *)
    source "${here}"/ci/rust-version.sh stable
    # shellcheck disable=SC2054 # rust_stable is sourced from rust-version.sh
    toolchain="$rust_stable"
    ;;
esac

set -x
exec cargo "+${toolchain}" "${@}"

================
File: cargo-build-sbf
================
#!/usr/bin/env bash

here=$(dirname "$0")

maybe_sbf_sdk="--sbf-sdk $here/platform-tools-sdk/sbf"
for a in "$@"; do
  if [[ $a = --sbf-sdk ]]; then
    maybe_sbf_sdk=
  fi
done

set -ex
if [[ ! -f "$here"/platform-tools-sdk/sbf/syscalls.txt ]]; then
  cargo build --manifest-path "$here"/syscalls/gen-syscall-list/Cargo.toml
fi
exec cargo run --manifest-path "$here"/platform-tools-sdk/cargo-build-sbf/Cargo.toml -- $maybe_sbf_sdk "$@"

================
File: cargo-registry/src/client.rs
================
pub(crate) struct Client {
⋮----
impl Client {
pub fn get_cli_config(&'_ self) -> CliConfig<'_> {
⋮----
websocket_url: self.websocket_url.clone(),
⋮----
signers: vec![&self.cli_signers[0], &self.cli_signers[1]],
⋮----
fn get_keypair(
⋮----
matches.value_of(name).unwrap_or(""),
⋮----
read_keypair_file(default_signer.path)
⋮----
fn get_clap_app<'ab, 'v>(name: &str, about: &'ab str, version: &'v str) -> App<'ab, 'v> {
⋮----
.about(about)
.version(version)
.arg(
⋮----
.long("skip-preflight")
.global(true)
.takes_value(false)
.help("Skip the preflight check when sending transactions"),
⋮----
.short("C")
.long("config")
.value_name("FILEPATH")
.takes_value(true)
⋮----
.help("Configuration file to use"),
⋮----
.short("u")
.long("url")
.value_name("URL_OR_MONIKER")
⋮----
.validator(is_url_or_moniker)
.help(
⋮----
.short("k")
.long("keypair")
.value_name("KEYPAIR")
⋮----
.help("Filepath or URL to a keypair"),
⋮----
.short("a")
.long("authority")
⋮----
.help("Filepath or URL to program authority keypair"),
⋮----
.short("p")
.long("port")
.value_name("PORT")
⋮----
.help("Cargo registry's local TCP port. The server will bind to this port and wait for requests."),
⋮----
.short("s")
.long("server-url")
⋮----
.long("commitment")
⋮----
.possible_values(&[
⋮----
.value_name("COMMITMENT_LEVEL")
.hide_possible_values(true)
⋮----
.help("Return information at the selected commitment level [possible values: processed, confirmed, finalized]"),
⋮----
.long("rpc-timeout")
.value_name("SECONDS")
⋮----
.default_value(DEFAULT_RPC_TIMEOUT_SECONDS)
⋮----
.hidden(hidden_unless_forced())
.help("Timeout value for RPC requests"),
⋮----
.long("confirm-timeout")
⋮----
.default_value(DEFAULT_CONFIRM_TX_TIMEOUT_SECONDS)
⋮----
.help("Timeout value for initial transaction status"),
⋮----
pub(crate) fn new() -> Result<Client, Box<dyn error::Error>> {
⋮----
crate_name!(),
crate_description!(),
⋮----
.get_matches();
let cli_config = if let Some(config_file) = matches.value_of("config_file") {
Config::load(config_file).unwrap_or_default()
⋮----
matches.value_of("json_rpc_url").unwrap_or(""),
⋮----
matches.value_of("websocket_url").unwrap_or(""),
⋮----
matches.value_of("commitment").unwrap_or(""),
⋮----
let rpc_timeout = value_t_or_exit!(matches, "rpc_timeout", u64);
⋮----
value_t_or_exit!(matches, "confirm_transaction_initial_timeout", u64);
⋮----
let port = value_t_or_exit!(matches, "port", u16);
⋮----
value_t!(matches, "server_url", String).unwrap_or(format!("http://0.0.0.0:{port}"));
let skip_preflight = matches.is_present("skip_preflight");
Ok(Client {
⋮----
json_rpc_url.to_string(),
⋮----
cli_signers: vec![payer_keypair, authority_keypair],
⋮----
preflight_commitment: Some(commitment.commitment),

================
File: cargo-registry/src/crate_handler.rs
================
pub(crate) type Error = Box<dyn std::error::Error + Send + Sync + 'static>;
⋮----
pub(crate) enum DependencyType {
⋮----
pub(crate) struct Dependency {
⋮----
pub(crate) struct PackageMetaData {
⋮----
impl PackageMetaData {
fn new(bytes: &Bytes) -> serde_json::Result<(PackageMetaData, usize)> {
⋮----
let end_of_meta_data = sizeof_length.saturating_add(json_length as usize);
let json_body = bytes.slice(sizeof_length..end_of_meta_data);
from_slice::<PackageMetaData>(json_body.deref()).map(|data| (data, end_of_meta_data))
⋮----
fn read_u32_length(bytes: &Bytes) -> serde_json::Result<(u32, usize)> {
⋮----
let length_le = bytes.slice(0..sizeof_length);
⋮----
u32::from_le_bytes(length_le.deref().try_into().expect("Failed to read length"));
Ok((length, sizeof_length))
⋮----
pub(crate) struct Program {
⋮----
impl Program {
#[allow(deprecated)] // Using sync wrapper for now, should migrate to async
fn deploy(&self, client: Arc<Client>, signer: &dyn Signer) -> Result<(), Error> {
if self.id != signer.pubkey() {
return Err("Signer doesn't match program ID".into());
⋮----
let mut cli_config = client.get_cli_config();
cli_config.signers.push(signer);
⋮----
.map_err(|err| format!("Unable to open program file: {err}"))?;
⋮----
file.read_to_end(&mut program_data)
.map_err(|err| format!("Unable to read program file: {err}"))?;
⋮----
let crate_len = u32::to_le_bytes(crate_tar_gz.0.len() as u32);
program_data.extend_from_slice(&crate_tar_gz.0);
program_data.extend_from_slice(&crate_len);
⋮----
process_deploy_program_sync(
client.rpc_client.clone(),
⋮----
&signer.pubkey(),
⋮----
Some(&2),
⋮----
.map_err(|e| {
error!("Failed to deploy the program: {e}");
format!("Failed to deploy the program: {e}")
⋮----
Ok(())
⋮----
fn dump(&mut self, client: Arc<Client>) -> Result<(), Error> {
info!("Fetching program {:?}", self.id);
process_dump_sync(
⋮----
&client.get_cli_config(),
Some(self.id),
⋮----
error!("Failed to fetch the program: {e}");
format!("Failed to fetch the program: {e}")
⋮----
return Err("Failed to read the program file".into());
⋮----
let data_len = data.len();
⋮----
let length_le = data.slice(data_len.saturating_sub(sizeof_length)..data_len);
⋮----
.saturating_sub(sizeof_length)
.saturating_sub(length as usize);
let crate_end = data_len.saturating_sub(sizeof_length);
self.packed_crate = PackedCrate(Bytes::copy_from_slice(&data[crate_start..crate_end]));
⋮----
pub(crate) fn crate_name_to_program_id(crate_name: &str) -> Option<Pubkey> {
let (_, id_str) = crate_name.split_once('-')?;
⋮----
.ok()
.and_then(|bytes| Pubkey::try_from(bytes).ok())
⋮----
fn program_id_to_crate_name(id: Pubkey) -> String {
format!("sol-{}", hex::encode(id.to_bytes()))
⋮----
fn from(value: &UnpackedCrate) -> Self {
⋮----
path: value.program_path.clone(),
⋮----
_tempdir: value.tempdir.clone(),
meta: value.meta.clone(),
packed_crate: value.packed_crate.clone(),
⋮----
pub(crate) struct PackedCrate(pub(crate) Bytes);
impl PackedCrate {
fn new(value: UnpackedCrate) -> Result<Self, Error> {
⋮----
archive.mode(HeaderMode::Deterministic);
⋮----
archive.append_dir_all(
format!("{}-{}/", value.meta.name, value.meta.vers),
⋮----
let data = archive.into_inner()?;
⋮----
encoder.read_to_end(&mut zipped_data)?;
Ok(PackedCrate(Bytes::from(zipped_data)))
⋮----
fn new_rebased(&self, meta: &PackageMetaData, target_base: &str) -> Result<Self, Error> {
let mut unpacked = UnpackedCrate::decompress(self.clone(), meta.clone())?;
⋮----
unpacked.meta.name = target_base.to_string();
⋮----
fs::rename(source_path, target_path.clone())
.map_err(|_| "Failed to rename the crate folder")?;
⋮----
fn version(&self) -> String {
let decoder = GzDecoder::new(self.0.as_ref());
⋮----
.entries()
⋮----
.and_then(|mut entries| entries.nth(0))
⋮----
if let Ok(path) = entry.path() {
if let Some(path_str) = path.to_str() {
if let Some((_, vers)) = path_str.rsplit_once('-') {
let mut version = vers.to_string();
if version.ends_with('/') {
version.pop();
⋮----
"0.1.0".to_string()
⋮----
pub(crate) struct UnpackedCrate {
⋮----
impl UnpackedCrate {
fn decompress(packed_crate: PackedCrate, meta: PackageMetaData) -> Result<Self, Error> {
let cksum = format!("{:x}", Sha256::digest(&packed_crate.0));
let decoder = GzDecoder::new(packed_crate.0.as_ref());
⋮----
let tempdir = tempdir()?;
archive.unpack(tempdir.path())?;
⋮----
let program_path = UnpackedCrate::make_path(&tempdir, &meta, format!("out/{lib_name}.so"))
.into_os_string()
.into_string()
.map_err(|_| "Failed to get program file path")?;
⋮----
format!("out/{lib_name}-keypair.json"),
⋮----
.map_err(|e| format!("Failed to get keypair from the file: {e}"))?;
Ok(UnpackedCrate {
⋮----
program_id: keypair.pubkey(),
keypair: Some(keypair),
⋮----
pub(crate) fn new(bytes: Bytes) -> Result<Self, Error> {
⋮----
PackageMetaData::read_u32_length(&bytes.slice(offset..))?;
let packed_crate = PackedCrate(bytes.slice(offset.saturating_add(length_size)..));
⋮----
pub(crate) fn publish(
⋮----
return Err("No signer provided for the program deployment".into());
⋮----
Program::from(self).deploy(client, signer)?;
let mut entry: IndexEntry = self.meta.clone().into();
entry.cksum.clone_from(&self.cksum);
index.insert_entry(entry)?;
info!("Successfully deployed the program");
⋮----
pub(crate) fn fetch_index(id: Pubkey, client: Arc<Client>) -> Result<IndexEntry, Error> {
⋮----
let mut entry: IndexEntry = meta.into();
entry.cksum = format!("{:x}", Sha256::digest(&packed_crate.0));
Ok(entry)
⋮----
pub(crate) fn fetch(
⋮----
program.dump(client)?;
// Decompile the program
// Generate a Cargo.toml
let mut meta = unpacked.meta.clone();
⋮----
meta.vers = program.packed_crate.version();
Ok((program.packed_crate, meta))
⋮----
PackedCrate::new(unpacked).map(|file| (file, meta))
⋮----
fn new_empty(id: Pubkey, vers: &str) -> Result<Self, Error> {
⋮----
vers: vers.to_string(),
deps: vec![],
⋮----
authors: vec![],
⋮----
keywords: vec![],
categories: vec![],
⋮----
.map_err(|_| "Failed to create the base directory for output")?;
let program_path = Self::make_path(&tempdir, &meta, format!("out/{id}.so"))
⋮----
Ok(Self {
⋮----
cksum: "".to_string(),
⋮----
fn make_path<P: AsRef<Path>>(tempdir: &TempDir, meta: &PackageMetaData, append: P) -> PathBuf {
let mut path = tempdir.path().to_path_buf();
path.push(format!("{}-{}/", meta.name, meta.vers));
path.push(append);
⋮----
fn program_library_name(tempdir: &TempDir, meta: &PackageMetaData) -> Result<String, Error> {
⋮----
.get("lib")
.and_then(|v| v.get("name"))
.and_then(|v| v.as_str())
.ok_or("Failed to get module name")?;
Ok(library_name.to_string())
⋮----
fn fixup_toml(
⋮----
toml.get_mut("package")
.and_then(|v| v.get_mut("name"))
.map(|v| *v = toml::Value::String(name.to_string()))
.ok_or("Failed to set package name")?;
fs::write(toml_orig_path, toml.to_string())?;

================
File: cargo-registry/src/main.rs
================
mod client;
mod crate_handler;
mod response_builder;
mod sparse_index;
⋮----
pub struct CargoRegistryService {}
impl CargoRegistryService {
async fn handle_publish_request(
⋮----
info!("Handling request to publish the crate");
let bytes = body::to_bytes(request.into_body()).await;
⋮----
tokio::task::spawn_blocking(move || unpacked_crate.publish(client, index))
⋮----
if result.is_ok() {
info!("Published the crate successfully. {result:?}");
⋮----
format!("Failed to publish the crate. {result:?}").as_str(),
⋮----
fn get_crate_name_and_version(path: &str) -> Option<(&str, &str, &str)> {
path.rsplit_once('/').and_then(|(remainder, version)| {
⋮----
.rsplit_once('/')
.map(|(remainder, name)| (remainder, name, version))
⋮----
fn handle_download_crate_request(
⋮----
if path.len() != PATH_PREFIX.len() {
⋮----
.and_then(|id| UnpackedCrate::fetch(id, version, client).ok());
⋮----
fn handle_yank_request(
⋮----
fn handle_unyank_request(
⋮----
fn get_crate_name(path: &str) -> Option<(&str, &str)> {
path.rsplit_once('/')
⋮----
fn handle_get_owners_request(
⋮----
fn handle_add_owners_request(
⋮----
fn handle_delete_owners_request(
⋮----
fn handle_get_crates_request(
⋮----
if path.len() >= PATH_PREFIX.len() {
⋮----
async fn handler(
⋮----
let path = request.uri().path();
if path.starts_with("/git") {
return Ok(response_builder::error_response(
⋮----
if path.starts_with(index.index_root.as_str()) {
return Ok(index.handler(request, client.clone()));
⋮----
if !path.starts_with(PATH_PREFIX) {
⋮----
let Some((path, endpoint)) = path.rsplit_once('/') else {
⋮----
Ok(match *request.method() {
⋮----
Self::handle_publish_request(request, client.clone(), index.clone()).await
⋮----
"download" => Self::handle_download_crate_request(path, &request, client.clone()),
⋮----
async fn main() {
⋮----
let client = Arc::new(Client::new().expect("Failed to get RPC Client instance"));
⋮----
let registry_service = make_service_fn(move |_| {
let client_inner = client.clone();
let index = index.clone();
⋮----
Ok::<_, Error>(service_fn(move |request| {
CargoRegistryService::handler(index.clone(), request, client_inner.clone())
⋮----
let server = Server::bind(&bind_addr).serve(registry_service);
info!("Server running on http://{bind_addr}");

================
File: cargo-registry/src/response_builder.rs
================
use log::error;
pub(crate) fn error_response(status: hyper::StatusCode, msg: &str) -> hyper::Response<hyper::Body> {
error!("{msg}");
⋮----
.status(status)
.body(hyper::Body::from(
⋮----
.to_string(),
⋮----
.unwrap()
⋮----
pub(crate) fn success_response_str(value: &str) -> hyper::Response<hyper::Body> {
⋮----
.status(hyper::StatusCode::OK)
.body(hyper::Body::from(value.to_string()))
⋮----
pub(crate) fn success_response_bytes(bytes: hyper::body::Bytes) -> hyper::Response<hyper::Body> {
⋮----
.body(hyper::Body::from(bytes))
⋮----
pub(crate) fn success_response() -> hyper::Response<hyper::Body> {
success_response_str("")
⋮----
pub(crate) fn error_not_allowed() -> hyper::Response<hyper::Body> {
error_response(hyper::StatusCode::METHOD_NOT_ALLOWED, "Unknown request")
⋮----
pub(crate) fn error_not_implemented() -> hyper::Response<hyper::Body> {
error_response(
⋮----
pub(crate) fn error_in_parsing() -> hyper::Response<hyper::Body> {
⋮----
pub(crate) fn error_incorrect_length() -> hyper::Response<hyper::Body> {

================
File: cargo-registry/src/sparse_index.rs
================
struct RegistryConfig {
⋮----
pub(crate) struct RegistryIndex {
⋮----
pub(crate) struct IndexEntryDep {
⋮----
fn from(v: Dependency) -> Self {
⋮----
kind: serde_json::to_string(&v.kind).expect("Failed to stringify dep kind"),
⋮----
pub(crate) struct IndexEntry {
⋮----
fn from(v: PackageMetaData) -> Self {
⋮----
deps: v.deps.into_iter().map(|v| v.into()).collect(),
⋮----
impl RegistryIndex {
pub(crate) fn new(root: &str, server_url: &str) -> Self {
⋮----
dl: format!("{server_url}/api/v1/crates"),
api: Some(server_url.to_string()),
⋮----
serde_json::to_string(&registry_config).expect("Failed to create registry config");
info!("Registry index is available at {server_url}{root}/");
⋮----
index_root: root.to_string(),
⋮----
pub(crate) fn handler(
⋮----
let path = request.uri().path();
let expected_root = self.index_root.as_str();
if !path.starts_with(expected_root) {
⋮----
let Some((_, path)) = path.split_once(expected_root) else {
⋮----
self.handle_crate_lookup_request(path, client)
⋮----
pub(crate) fn insert_entry(&self, entry: IndexEntry) -> Result<(), Error> {
⋮----
.write()
.map_err(|e| format!("Failed to lock the index for writing: {e}"))?;
info!("Inserting {}-{} in registry index", entry.name, entry.vers);
write_index.insert(entry.name.clone(), entry);
Ok(())
⋮----
fn get_crate_name_from_path(path: &str) -> Option<&str> {
let (path, crate_name) = path.rsplit_once('/')?;
match crate_name.len() {
⋮----
let first_char = crate_name.chars().next()?;
path == format!("/3/{first_char}")
⋮----
let (first_two_char, rest) = crate_name.split_at(2);
let (next_two_char, _) = rest.split_at(2);
path == format!("/{first_two_char}/{next_two_char}")
⋮----
.then_some(crate_name)
⋮----
fn handle_crate_lookup_request(
⋮----
info!("Looking up index for {crate_name:?}");
let Ok(read_index) = self.index.read() else {
⋮----
let response = if let Some(entry) = read_index.get(crate_name) {
Some(serde_json::to_string(entry))
⋮----
.and_then(|id| UnpackedCrate::fetch_index(id, client).ok())
.map(|entry| serde_json::to_string(&entry))
⋮----
response_builder::success_response_str(response.as_str())
⋮----
mod test {
⋮----
fn test_get_crate_name_from_path() {
assert_eq!(RegistryIndex::get_crate_name_from_path(""), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/"), None);
// Single character crate name
assert_eq!(RegistryIndex::get_crate_name_from_path("/a"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/1/a"), Some("a"));
assert_eq!(RegistryIndex::get_crate_name_from_path("/2/a"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/a/a"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/ab"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/1/ab"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/2/ab"), Some("ab"));
assert_eq!(RegistryIndex::get_crate_name_from_path("/3/ab"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/ab/ab"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/abc"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/1/abc"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/2/abc"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/3/abc"), None);
assert_eq!(
⋮----
assert_eq!(RegistryIndex::get_crate_name_from_path("/ab/abc"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/ab/c/abc"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/abcd"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/1/abcd"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/2/abcd"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/3/abcd"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/3/a/abcd"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/4/abcd"), None);
⋮----
assert_eq!(RegistryIndex::get_crate_name_from_path("/ab/cd/abc"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/abcdefgh"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/1/abcdefgh"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/2/abcdefgh"), None);
assert_eq!(RegistryIndex::get_crate_name_from_path("/3/abcdefgh"), None);
⋮----
assert_eq!(RegistryIndex::get_crate_name_from_path("/4/abcdefgh"), None);

================
File: cargo-registry/Cargo.toml
================
[package]
name = "agave-cargo-registry"
description = "Solana cargo registry"
documentation = "https://docs.rs/agave-cargo-registry"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
default = ["remote-wallet-hidraw"]
agave-unstable-api = []
dev-context-only-utils = []
remote-wallet-hidraw = ["solana-remote-wallet/linux-static-hidraw"]
remote-wallet-libusb = ["solana-remote-wallet/linux-static-libusb"]

[dependencies]
agave-logger = { workspace = true }
clap = { workspace = true }
flate2 = { workspace = true }
hex = { workspace = true }
hyper = { workspace = true, features = ["full"] }
log = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
sha2 = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli = { workspace = true }
solana-cli-config = { workspace = true }
solana-cli-output = { workspace = true }
solana-commitment-config = { workspace = true }
solana-keypair = { workspace = true }
solana-pubkey = { workspace = true }
solana-remote-wallet = { workspace = true }
solana-rpc-client = { workspace = true, features = ["default"] }
solana-rpc-client-api = { workspace = true }
solana-signer = { workspace = true }
solana-version = { workspace = true }
tar = { workspace = true }
tempfile = { workspace = true }
tokio = { workspace = true, features = ["full"] }
toml = { workspace = true }

[dev-dependencies]

================
File: cargo-test-sbf
================
#!/usr/bin/env bash

here=$(dirname "$0")

maybe_sbf_sdk="--sbf-sdk $here/platform-tools-sdk/sbf"
for a in "$@"; do
  if [[ $a = --sbf-sdk ]]; then
    maybe_sbf_sdk=
  fi
done

export CARGO_BUILD_SBF="$here"/cargo-build-sbf
set -x
exec cargo run --manifest-path "$here"/platform-tools-sdk/cargo-test-sbf/Cargo.toml -- $maybe_sbf_sdk "$@"

================
File: Cargo.toml
================
[workspace]
members = [
    "account-decoder",
    "account-decoder-client-types",
    "accounts-cluster-bench",
    "accounts-db",
    "accounts-db/store-histogram",
    "bam-banking-bench",
    "bam-local-cluster",
    "banking-stage-ingress-types",
    "banks-client",
    "banks-interface",
    "banks-server",
    "bench-streamer",
    "bench-vote",
    "bloom",
    "bucket_map",
    "builtins",
    "builtins-default-costs",
    "bundle",
    "cargo-registry",
    "clap-utils",
    "clap-v3-utils",
    "cli",
    "cli-config",
    "cli-output",
    "client",
    "client-test",
    "compute-budget",
    "compute-budget-instruction",
    "connection-cache",
    "core",
    "cost-model",
    "curves/curve25519",
    "download-utils",
    "entry",
    "faucet",
    "feature-set",
    "fee",
    "fs",
    "genesis",
    "genesis-utils",
    "geyser-plugin-interface",
    "geyser-plugin-manager",
    "gossip",
    "gossip-bin",
    "install",
    "io-uring",
    "jito-protos",
    "keygen",
    "lattice-hash",
    "ledger",
    "local-cluster",
    "logger",
    "measure",
    "merkle-tree",
    "metrics",
    "net-utils",
    "notifier",
    "perf",
    "platform-tools-sdk/cargo-build-sbf",
    "platform-tools-sdk/cargo-test-sbf",
    "platform-tools-sdk/gen-headers",
    "poh",
    "poh-bench",
    "poseidon",
    "precompiles",
    "program-binaries",
    "program-runtime",
    "program-test",
    "programs/bpf-loader-tests",
    "programs/bpf_loader",
    "programs/compute-budget",
    "programs/compute-budget-bench",
    "programs/ed25519-tests",
    "programs/loader-v4",
    "programs/system",
    "programs/vote",
    "programs/zk-elgamal-proof",
    "programs/zk-elgamal-proof-tests",
    "programs/zk-token-proof",
    "pubsub-client",
    "quic-client",
    "rayon-threadlimit",
    "rbpf-cli",
    "remote-wallet",
    "reserved-account-keys",
    "rpc",
    "rpc-client",
    "rpc-client-api",
    "rpc-client-nonce-utils",
    "rpc-client-types",
    "rpc-test",
    "runtime",
    "runtime-transaction",
    "scheduler-bindings",
    "scheduling-utils",
    "send-transaction-service",
    "snapshots",
    "stake-accounts",
    "storage-bigtable",
    "storage-bigtable/build-proto",
    "storage-proto",
    "streamer",
    "svm",
    "svm-callback",
    "svm-feature-set",
    "svm-log-collector",
    "svm-measure",
    "svm-test-harness",
    "svm-timings",
    "svm-transaction",
    "svm-type-overrides",
    "syscalls",
    "syscalls/gen-syscall-list",
    "test-validator",
    "thread-manager",
    "tls-utils",
    "tokens",
    "tps-client",
    "tpu-client",
    "tpu-client-next",
    "transaction-context",
    "transaction-dos",
    "transaction-metrics-tracker",
    "transaction-status",
    "transaction-status-client-types",
    "transaction-view",
    "turbine",
    "udp-client",
    "unified-scheduler-logic",
    "unified-scheduler-pool",
    "validator",
    "verified-packet-receiver",
    "version",
    "vortexor",
    "vote",
    "votor",
    "votor-messages",
    "watchtower",
    "wen-restart",
    "xdp",
    "xdp-ebpf",
    "zk-token-sdk",
]

exclude = [
    # solana-dos depends on bench-tps. ignore bench-tps here to avoid confusing cargo's
    # workspace resolution logic, which expects it in the top-level worspace rather than
    # dev-bins
    "bench-tps",
    "ci/xtask",
    "dev-bins",
    "programs/sbf",
    "svm/tests/example-programs",
]

resolver = "2"

[workspace.package]
version = "4.0.0-alpha.0"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
description = "Blockchain, Rebuilt for Scale"
repository = "https://github.com/anza-xyz/agave"
homepage = "https://anza.xyz/"
license = "Apache-2.0"
edition = "2021"

[workspace.lints.rust]
warnings = "deny"

# List of rust-2024-compatibility lints that are already satisfied
# See https://doc.rust-lang.org/rustc/lints/groups.html
boxed_slice_into_iter = "deny"
dependency_on_unit_never_type_fallback = "deny"
deprecated_safe_2024 = "deny"
impl_trait_overcaptures = "deny"
missing_unsafe_on_extern = "deny"
never_type_fallback_flowing_into_unsafe = "deny"
rust_2024_guarded_string_incompatible_syntax = "deny"
rust_2024_incompatible_pat = "deny"
rust_2024_prelude_collisions = "deny"
static_mut_refs = "deny"
unsafe_attr_outside_unsafe = "deny"
unsafe_op_in_unsafe_fn = "deny"

[workspace.lints.rust.unexpected_cfgs]
level = "warn"
check-cfg = [
    'cfg(target_os, values("solana"))',
    'cfg(feature, values("frozen-abi", "no-entrypoint"))',
]

# Clippy lint configuration that can not be applied in clippy.toml
[workspace.lints.clippy]
arithmetic_side_effects = "deny"
default_trait_access = "deny"
manual_let_else = "deny"
used_underscore_binding = "deny"

[workspace.dependencies]
Inflector = "0.11.4"
aes-gcm-siv = "0.11.1"
agave-banking-stage-ingress-types = { path = "banking-stage-ingress-types", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-cargo-registry = { path = "cargo-registry", version = "=4.0.0-alpha.0" }
agave-feature-set = { path = "feature-set", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-fs = { path = "fs", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-geyser-plugin-interface = { path = "geyser-plugin-interface", version = "=4.0.0-alpha.0" }
agave-io-uring = { path = "io-uring", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-logger = { path = "logger", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-precompiles = { path = "precompiles", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-reserved-account-keys = { path = "reserved-account-keys", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-scheduler-bindings = { path = "scheduler-bindings", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-scheduling-utils = { path = "scheduling-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-snapshots = { path = "snapshots", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-syscalls = { path = "syscalls", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-thread-manager = { path = "thread-manager", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-transaction-view = { path = "transaction-view", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-verified-packet-receiver = { path = "verified-packet-receiver", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-votor = { path = "votor", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-votor-messages = { path = "votor-messages", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-xdp = { path = "xdp", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
agave-xdp-ebpf = { path = "xdp-ebpf", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
ahash = "0.8.11"
anyhow = "1.0.100"
aquamarine = "0.6.0"
arbitrary = "1.4.2"
arc-swap = "1.7.1"
ark-bn254 = "0.5.0"
ark-bn254-0-4 = { package = "ark-bn254", version = "0.4.0" }
array-bytes = "=1.4.1"
arrayref = "0.3.9"
arrayvec = "0.7.6"
assert_cmd = "2.0"
assert_matches = "1.5.0"
async-lock = "3.4.1"
async-trait = "0.1.89"
atty = "0.2.11"
axum = "0.8.6"
aya = "0.13"
aya-ebpf = "0.1.1"
backoff = "0.4.0"
base64 = "0.22.1"
bencher = "0.1.5"
bincode = "1.3.3"
bitflags = { version = "2.10.0" }
bitvec = { version = "1.0.1", features = ["serde"] }
blake3 = "1.8.2"
borsh = { version = "1.5.7", features = ["derive", "unstable__schema"] }
bs58 = { version = "0.5.1", default-features = false }
bv = "0.11.1"
byte-unit = "4.0.19"
bytemuck = "1.24.0"
bytemuck_derive = "1.10.2"
bytes = "1.11"
bzip2 = "0.4.4"
caps = "0.5.6"
cargo_metadata = "0.15.4"
cfg-if = "1.0.4"
cfg_eval = "0.1.2"
chrono = { version = "0.4.42", default-features = false }
chrono-humanize = "0.2.3"
clap = "2.33.1"
console = "0.16.1"
console_error_panic_hook = "0.1.7"
console_log = "0.2.2"
const_format = "0.2.35"
core_affinity = "0.8.3"
criterion = "0.5.1"
criterion-stats = "0.3.0"
crossbeam-channel = "0.5.15"
csv = "1.4.0"
ctrlc = "3.5.0"
curve25519-dalek = { version = "4.1.3", features = ["digest", "rand_core"] }
dashmap = "5.5.3"
derivation-path = { version = "0.2.0", default-features = false }
derive-where = "1.6.0"
derive_more = { version = "2.0.1", features = ["full"] }
dialoguer = "0.12.0"
digest = "0.10.7"
dir-diff = "0.3.3"
dirs-next = "2.0.0"
dlopen2 = "0.8.0"
dyn-clone = "1.0.20"
eager = "0.1.0"
ed25519-dalek = "=1.0.1"
ed25519-dalek-bip32 = "0.2.0"
enum-iterator = "2.3.0"
env_logger = "0.11.8"
fast-math = "0.1"
fd-lock = "4.0.4"
five8_const = "0.1.4"
flate2 = "1.0.31"
fnv = "1.0.7"
fs_extra = "1.3.0"
futures = "0.3.31"
futures-util = "0.3.29"
gag = "1.0.0"
gethostname = "0.2.3"
getrandom = "0.3.4"
goauth = "0.13.1"
governor = "0.6.3"
hex = "0.4.3"
hidapi = { version = "2.6.3", default-features = false }
histogram = "0.6.9"
hmac = "0.12.1"
http = "0.2.12"
humantime = "2.3.0"
hyper = "0.14.32"
hyper-proxy = "0.9.1"
im = "15.1.0"
indexmap = "2.12.0"
indicatif = "0.18.3"
io-uring = "0.7.11"
itertools = "0.14.0"
jemallocator = { package = "tikv-jemallocator", version = "0.6.0", features = [
    "unprefixed_malloc_on_supported_platforms",
] }
jito-protos = { path = "jito-protos", version = "=4.0.0-alpha.0" }
js-sys = "0.3.82"
json5 = "0.4.1"
jsonrpc-core = "18.0.0"
jsonrpc-core-client = "18.0.0"
jsonrpc-derive = "18.0.0"
jsonrpc-http-server = "18.0.0"
jsonrpc-ipc-server = "18.0.0"
jsonrpc-pubsub = "18.0.0"
jsonrpc-server-utils = "18.0.0"
lazy-lru = "0.1.3"
lazy_static = "1.5.0"
libc = "0.2.177"
libloading = "0.7.4"
libsecp256k1 = { version = "0.6.0", default-features = false, features = [
    "std",
    "static-context",
] }
light-poseidon = "0.4.0"
light-poseidon-0-2 = { package = "light-poseidon", version = "0.2.0" }
log = "0.4.28"
lru = "0.7.7"
lz4 = "1.28.1"
memmap2 = "0.9.9"
memoffset = "0.9"
merlin = { version = "3", default-features = false }
min-max-heap = "1.3.0"
mockall = "0.13.1"
modular-bitfield = "0.13.0"
nix = "0.30.1"
num-bigint = "0.4.6"
num-derive = "0.4"
num-traits = "0.2"
num_cpus = "1.17.0"
num_enum = "0.7.4"
openssl = "0.10"
parking_lot = "0.12"
pbkdf2 = { version = "0.12.2", default-features = false }
pem = "1.1.1"
percentage = "0.1.0"
pickledb = { version = "0.5.1", default-features = false }
predicates = "3.1"
pretty-hex = "0.3.0"
pretty_assertions = "1.4.1"
prio-graph = "0.3.0"
proc-macro2 = "1.0.97"
proptest = "1.9"
prost = "0.11.9"
prost-build = "0.11.9"
prost-types = "0.11.9"
protobuf-src = "1.1.0"
protosol = "2.0.0"
qstring = "0.7.2"
qualifier_attr = { version = "0.2.2", default-features = false }
quinn = "0.11.9"
quinn-proto = "0.11.13"
quote = "1.0"
rand = "0.9.2"
rand0-7 = { package = "rand", version = "0.7" }
rand_chacha = "0.9.0"
rand_chacha0-2 = { package = "rand_chacha", version = "0.2.2" }
rayon = "1.11.0"
reed-solomon-erasure = "6.0.0"
regex = "1.12.2"
reqwest = { version = "0.12.24", default-features = false }
reqwest-middleware = "0.4.2"
rolling-file = "0.2.0"
rpassword = "7.4"
rts-alloc = { version = "1.0.0" }
rustls = { version = "0.23.35", features = ["std"], default-features = false }
scopeguard = "1.2.0"
semver = "1.0.27"
seqlock = "0.2.0"
serde = { version = "1.0.228", features = ["derive"] }
serde-big-array = "0.5.1"
serde_bytes = "0.11.19"
serde_json = "1.0.145"
serde_with = { version = "3.15.1", default-features = false }
serde_yaml = "0.9.34"
serial_test = "3.2.0"
sha2 = "0.10.9"
sha3 = "0.10.8"
shaq = { version = "1.0.0" }
shuttle = "0.7.1"
signal-hook = "0.3.18"
siphasher = "1.0.1"
slab = "0.4.11"
smallvec = { version = "1.15.1", default-features = false, features = ["union"] }
smpl_jwt = "0.7.1"
socket2 = "0.6.1"
soketto = "0.8"
solana-account = "3.2.0"
solana-account-decoder = { path = "account-decoder", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-account-decoder-client-types = { path = "account-decoder-client-types", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-account-info = "3.0.0"
solana-accounts-db = { path = "accounts-db", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-address = "1.1.0"
solana-address-lookup-table-interface = "3.0.0"
solana-atomic-u64 = "3.0.0"
solana-banks-client = { path = "banks-client", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-banks-interface = { path = "banks-interface", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-banks-server = { path = "banks-server", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-big-mod-exp = "3.0.0"
solana-bincode = "3.0.0"
solana-blake3-hasher = "3.1.0"
solana-bloom = { path = "bloom", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-bls-signatures = { version = "1.0.0", features = ["serde"] }
solana-bn254 = "3.1.2"
solana-borsh = "3.0.0"
solana-bpf-loader-program = { path = "programs/bpf_loader", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-bucket-map = { path = "bucket_map", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-builtins = { path = "builtins", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-builtins-default-costs = { path = "builtins-default-costs", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-bundle = { path = "bundle", version = "=4.0.0-alpha.0" }
solana-clap-utils = { path = "clap-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-clap-v3-utils = { path = "clap-v3-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-cli = { path = "cli", version = "=4.0.0-alpha.0" }
solana-cli-config = { path = "cli-config", version = "=4.0.0-alpha.0" }
solana-cli-output = { path = "cli-output", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-client = { path = "client", version = "=4.0.0-alpha.0" }
solana-client-traits = "3.0.0"
solana-clock = "3.0.0"
solana-cluster-type = "3.0.0"
solana-commitment-config = "3.0.0"
solana-compute-budget = { path = "compute-budget", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-compute-budget-instruction = { path = "compute-budget-instruction", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-compute-budget-interface = "3.0.0"
solana-compute-budget-program = { path = "programs/compute-budget", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-config-interface = "2.0.0"
solana-connection-cache = { path = "connection-cache", version = "=4.0.0-alpha.0", default-features = false, features = ["agave-unstable-api"] }
solana-core = { path = "core", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-cost-model = { path = "cost-model", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-cpi = "3.0.0"
solana-curve25519 = { path = "curves/curve25519", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-define-syscall = "3.0.0"
solana-derivation-path = "3.0.0"
solana-download-utils = { path = "download-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-ed25519-program = "3.0.0"
solana-entry = { path = "entry", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-epoch-info = "3.0.0"
solana-epoch-rewards = "3.0.0"
solana-epoch-rewards-hasher = "3.0.0"
solana-epoch-schedule = "3.0.0"
solana-example-mocks = "3.0.0"
solana-faucet = { path = "faucet", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-feature-gate-interface = "3.0.0"
solana-fee = { path = "fee", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-fee-calculator = "3.0.0"
solana-fee-structure = "3.0.0"
solana-file-download = "3.1.0"
solana-frozen-abi = "3.0.1"
solana-frozen-abi-macro = "3.0.1"
solana-genesis = { path = "genesis", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-genesis-config = "3.0.0"
solana-genesis-utils = { path = "genesis-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-geyser-plugin-manager = { path = "geyser-plugin-manager", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-gossip = { path = "gossip", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-hard-forks = "3.0.0"
solana-hash = "3.1.0"
solana-inflation = "3.0.0"
solana-instruction = "3.0.0"
solana-instruction-error = "2.1.0"
solana-instructions-sysvar = "3.0.0"
solana-keccak-hasher = "3.1.0"
solana-keypair = "3.0.1"
solana-last-restart-slot = "3.0.0"
solana-lattice-hash = { path = "lattice-hash", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-ledger = { path = "ledger", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-loader-v2-interface = "3.0.0"
solana-loader-v3-interface = "6.1.0"
solana-loader-v4-interface = "3.1.0"
solana-loader-v4-program = { path = "programs/loader-v4", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-local-cluster = { path = "local-cluster", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-measure = { path = "measure", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-merkle-tree = { path = "merkle-tree", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-message = "3.0.1"
solana-metrics = { path = "metrics", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-msg = "3.0.0"
solana-native-token = "3.0.0"
solana-net-utils = { path = "net-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-nohash-hasher = "0.2.1"
solana-nonce = "3.0.0"
solana-nonce-account = "3.0.0"
solana-notifier = { path = "notifier", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-offchain-message = "3.0.0"
solana-packet = "4.0.0"
solana-perf = { path = "perf", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-poh = { path = "poh", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-poh-config = "3.0.0"
solana-poseidon = { path = "poseidon", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-precompile-error = "3.0.0"
solana-presigner = "3.0.0"
solana-program = { version = "3.0.0", default-features = false }
solana-program-binaries = { path = "program-binaries", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-program-entrypoint = "3.1.0"
solana-program-error = "3.0.0"
solana-program-memory = "3.0.0"
solana-program-option = "3.0.0"
solana-program-pack = "3.0.0"
solana-program-runtime = { path = "program-runtime", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-program-test = { path = "program-test", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-pubkey = { version = "3.0.0", default-features = false }
solana-pubsub-client = { path = "pubsub-client", version = "=4.0.0-alpha.0" }
solana-quic-client = { path = "quic-client", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-quic-definitions = "3.0.0"
solana-rayon-threadlimit = { path = "rayon-threadlimit", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-remote-wallet = { path = "remote-wallet", version = "=4.0.0-alpha.0", default-features = false, features = ["agave-unstable-api"] }
solana-rent = "3.0.0"
solana-reward-info = "3.0.0"
solana-rpc = { path = "rpc", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-rpc-client = { path = "rpc-client", version = "=4.0.0-alpha.0", default-features = false }
solana-rpc-client-api = { path = "rpc-client-api", version = "=4.0.0-alpha.0" }
solana-rpc-client-nonce-utils = { path = "rpc-client-nonce-utils", version = "=4.0.0-alpha.0" }
solana-rpc-client-types = { path = "rpc-client-types", version = "=4.0.0-alpha.0" }
solana-runtime = { path = "runtime", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-runtime-transaction = { path = "runtime-transaction", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-sanitize = "3.0.1"
solana-sbpf = { version = "=0.13.1", default-features = false }
solana-sdk-ids = "3.0.0"
solana-secp256k1-program = "3.0.0"
solana-secp256k1-recover = "3.0.0"
solana-secp256r1-program = "3.0.0"
solana-seed-derivable = "3.0.0"
solana-seed-phrase = "3.0.0"
solana-send-transaction-service = { path = "send-transaction-service", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-serde = "3.0.0"
solana-serde-varint = "3.0.0"
solana-serialize-utils = "3.1.0"
solana-sha256-hasher = "3.1.0"
solana-short-vec = "3.0.0"
solana-shred-version = "3.0.0"
solana-signature = { version = "3.1.0", default-features = false }
solana-signer = "3.0.0"
solana-signer-store = "0.1.0"
solana-slot-hashes = "3.0.0"
solana-slot-history = "3.0.0"
solana-stable-layout = "3.0.0"
solana-stake-interface = { version = "2.0.1" }
solana-storage-bigtable = { path = "storage-bigtable", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-storage-proto = { path = "storage-proto", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-streamer = { path = "streamer", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm = { path = "svm", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-callback = { path = "svm-callback", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-feature-set = { path = "svm-feature-set", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-log-collector = { path = "svm-log-collector", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-measure = { path = "svm-measure", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-test-harness = { path = "svm-test-harness", version = "=4.0.0-alpha.0" }
solana-svm-timings = { path = "svm-timings", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-transaction = { path = "svm-transaction", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-svm-type-overrides = { path = "svm-type-overrides", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-system-interface = "2.0"
solana-system-program = { path = "programs/system", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-system-transaction = "3.0.0"
solana-sysvar = "3.0.0"
solana-sysvar-id = "3.0.0"
solana-test-validator = { path = "test-validator", version = "=4.0.0-alpha.0" }
solana-time-utils = "3.0.0"
solana-tls-utils = { path = "tls-utils", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-tps-client = { path = "tps-client", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-tpu-client = { path = "tpu-client", version = "=4.0.0-alpha.0", default-features = false, features = ["agave-unstable-api"] }
solana-tpu-client-next = { path = "tpu-client-next", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-transaction = "3.0.2"
solana-transaction-context = { path = "transaction-context", version = "=4.0.0-alpha.0", features = ["agave-unstable-api", "bincode"] }
solana-transaction-error = "3.0.0"
solana-transaction-metrics-tracker = { path = "transaction-metrics-tracker", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-transaction-status = { path = "transaction-status", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-transaction-status-client-types = { path = "transaction-status-client-types", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-turbine = { path = "turbine", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-udp-client = { path = "udp-client", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-unified-scheduler-logic = { path = "unified-scheduler-logic", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-unified-scheduler-pool = { path = "unified-scheduler-pool", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-validator-exit = "3.0.0"
solana-version = { path = "version", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-vote = { path = "vote", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-vote-interface = "4.0.4"
solana-vote-program = { path = "programs/vote", version = "=4.0.0-alpha.0", default-features = false, features = ["agave-unstable-api"] }
solana-wen-restart = { path = "wen-restart", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-zk-elgamal-proof-program = { path = "programs/zk-elgamal-proof", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-zk-sdk = "4.0.0"
solana-zk-token-proof-program = { path = "programs/zk-token-proof", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
solana-zk-token-sdk = { path = "zk-token-sdk", version = "=4.0.0-alpha.0", features = ["agave-unstable-api"] }
spl-associated-token-account-interface = "2.0.0"
spl-generic-token = "2.0.0"
spl-memo-interface = "2.0.0"
spl-pod = "0.7.0"
spl-token-2022-interface = "2.1.0"
spl-token-confidential-transfer-proof-extraction = "0.5.0"
spl-token-group-interface = "0.7.0"
spl-token-interface = "2.0.0"
spl-token-metadata-interface = "0.8.0"
static_assertions = "1.1.0"
stream-cancel = "0.8.2"
strum = "0.24"
strum_macros = "0.24"
subtle = "2.6.1"
symlink = "0.1.0"
syn = "2.0"
sys-info = "0.9.1"
sysctl = "0.7.1"
systemstat = "0.2.5"
tar = "0.4.44"
tarpc = "0.29.0"
tempfile = "3.23.0"
test-case = "3.3.1"
thiserror = "2.0.17"
thread-priority = "3.0.0"
tiny-bip39 = "2.0.0"
tokio = "1.48.0"
tokio-serde = "0.8"
tokio-stream = "0.1.17"
tokio-tungstenite = "0.28.0"
tokio-util = "0.7.17"
toml = "0.9.8"
tonic = "0.9.2"
tonic-build = "0.9.2"
tower = "0.5.2"
tracing = "0.1"
trait-set = "0.3.0"
trees = "0.4.2"
tungstenite = "0.28.0"
unwrap_none = "0.1.2"
uriparse = "0.6.4"
url = "2.5.7"
vec_extract_if_polyfill = "0.1.0"
wasm-bindgen = "0.2"
winapi = "0.3.8"
wincode = { version = "0.1.2", features = ["derive", "solana-short-vec"] }
winreg = "0.55"
x509-parser = "0.14.0"
zeroize = { version = "1.8", default-features = false }
zstd = "0.13.3"

[profile.release-with-debug]
inherits = "release"
debug = true
strip = false
split-debuginfo = "off"

[profile.release]
split-debuginfo = "unpacked"
lto = "thin"

[profile.release-with-lto]
inherits = "release"
lto = "fat"
codegen-units = 1

# curve25519-dalek uses the simd backend by default in v4 if possible,
# which has very slow performance on some platforms with opt-level 0,
# which is the default for dev and test builds.
# This slowdown causes certain interactions in the solana-test-validator,
# such as verifying ZK proofs in transactions, to take much more than 400ms,
# creating problems in the testing environment.
# To enable better performance in solana-test-validator during tests and dev builds,
# we override the opt-level to 3 for the crate.
[profile.dev.package.curve25519-dalek]
opt-level = 3

[patch.crates-io]
# for details, see https://github.com/anza-xyz/crossbeam/commit/fd279d707025f0e60951e429bf778b4813d1b6bf
crossbeam-epoch = { git = "https://github.com/anza-xyz/crossbeam", rev = "fd279d707025f0e60951e429bf778b4813d1b6bf" }

# We include the following crates as our dependencies above from crates.io:
#
#  * spl-associated-token-account-interface
#  * spl-instruction-padding
#  * spl-memo-interface
#  * spl-pod
#  * spl-token
#  * spl-token-2022-interface
#  * spl-token-metadata-interface
#
# They, in turn, depend on a number of crates that we also include directly
# using `path` specifications.  For example, `spl-token` depends on
# `solana-program`.  And we explicitly specify `solana-program` above as a local
# path dependency:
#
#     solana-program = { path = "../../sdk/program", version = "=1.16.0" }
#
# Unfortunately, Cargo will try to resolve the `spl-token` `solana-program`
# dependency only using what is available on crates.io.  Crates.io normally
# contains a previous version of these crates, and we end up with two versions
# of `solana-program` and `solana-zk-token-sdk` and all of their dependencies in
# our build tree.
#
# If you are developing downstream using non-crates-io solana-program (local or
# forked repo, or from github rev, eg), duplicate the following patch statements
# in your Cargo.toml. If you still hit duplicate-type errors with the patch
# statements in place, run `cargo update -p solana-program` and/or `cargo update
# -p solana-zk-token-sdk` to remove extraneous versions from your Cargo.lock
# file.
#
# There is a similar override in `programs/sbf/Cargo.toml`.  Please keep both
# comments and the overrides in sync.
solana-curve25519 = { path = "curves/curve25519" }

================
File: CHANGELOG.md
================
# Changelog

All notable changes to this project will be documented in this file.

Please follow the [guidance](#adding-to-this-changelog) at the bottom of this file when making changes
The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/).
This project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html)
and follows a [Backwards Compatibility Policy](https://docs.anza.xyz/backwards-compatibility)

Release channels have their own copy of this changelog:
* [edge - v4.0](#edge-channel)
* [beta - v3.1](https://github.com/anza-xyz/agave/blob/v3.1/CHANGELOG.md)
* [stable - v3.0](https://github.com/anza-xyz/agave/blob/v3.0/CHANGELOG.md)

<a name="edge-channel"></a>
## 4.0.0-Unreleased
### RPC
#### Breaking
* `--public-tpu-address` and `--public-tpu-forwards-address` CLI arguments and `setPublicTpuForwardsAddress`, `setPublicTpuAddress` RPC methods now specify QUIC ports, not UDP.
#### Changes
* Added `--enable-scheduler-bindings` which binds an IPC server at `<ledger-path>/scheduler_bindings.ipc` for external schedulers to connect to.
### Validator
#### Breaking
* Removed deprecated arguments
  * `--accounts-db-clean-threads`
  * `--accounts-db-hash-threads`
  * `--accounts-db-read-cache-limit-mb`
  * `--accounts-hash-cache-path`
  * `--disable-accounts-disk-index`
#### Deprecations
* Using `mmap` for `--accounts-db-access-storages-method` is now deprecated.

## 3.1.0
### RPC
#### Breaking
* A signature verification failure in `simulateTransaction()` or the preflight stage of `sendTransaction()` will now be attached to the simulation result's `err` property as `TransactionError::SignatureFailure` instead of being thrown as a JSON RPC API error (-32003). Applications that already guard against JSON RPC exceptions should expect signature verification errors to appear on the simulation result instead. Applications that already handle the materialization of `TransactionErrors` on simulation results can now expect to receive errors of type `TransactionError::SignatureFailure` at those verification sites.
#### Changes
* The `getProgramAccounts` RPC endpoint now returns JSON-RPC errors when malformed filters are provided (previously these malformed filters would be silently ignored and the RPC call would execute an unfiltered query).
* `PubsubClient` can now be constructed with the URI of an RPC (as a `str`, `String`, or `Uri`) as well as an `http::Request<()>`. The addition of `Request` allows you to set request headers when establishing a websocket connection with an RPC.
### Validator
#### Breaking
#### Deprecations
* The `--monitor` flag with `agave-validator exit` is now deprecated. Operators can use the `monitor` command after `exit` instead.
* The `--disable-accounts-disk-index` flag is now deprecated.
* All monorepo crates falling outside the
[backward compatibility policy](https://docs.anza.xyz/backwards-compatibility) are now
deprecated, signaling their inclusion in the Agave Unstable API. Enable the
`agave-unstable-api` crate feature to acknowledge use of an interface that may break
without warning. From v4.0.0 onward, symbols in these crates will be unavailable without
`agave-unstable-api` enabled.
* The `--dev-halt-at-slot` flag is now deprecated.

#### Changes
* The accounts index is now kept entirely in memory by default.

## 3.0.0

### RPC

#### Breaking
* Added a `slot` property to `EpochRewardsPeriodActiveErrorData`
* Added error data containing a `slot` property to `RpcCustomError::SlotNotEpochBoundary`

#### Changes
* The subscription server now prioritizes processing received messages before sending out responses. This ensures that new subscription requests and time-sensitive messages like `PING` opcodes take priority over notifications.

### Validator

#### Breaking
* When XDP is enabled, the validator process requires the `CAP_NET_RAW`, `CAP_NET_ADMIN`, `CAP_BPF`, and `CAP_PERFMON` capabilities. These can be configured in the systemd service file by setting `CapabilityBoundingSet=CAP_NET_RAW CAP_NET_ADMIN CAP_BPF CAP_PERFMON` under the `[Service]` section or directly on the binary with the command `sudo setcap cap_net_raw,cap_net_admin,cap_bpf,cap_perfmon=p <path/to/agave-validator>` (this command must be run each time the binary is replaced)
* Enabling XDP zero copy on systems configured with LACP bond requires manually passing  `--experimental-retransmit-xdp-interface <real-interface>` (e.g.: `eno17395np0` not `bond0`), as zero copy is only available on physical interfaces.
* Require increased `memlock` limits - recommended setting is `LimitMEMLOCK=2000000000` in systemd service configuration. Lack of sufficient limit (on Linux) will cause startup error.
* Remove deprecated arguments
  * `--accounts-index-memory-limit-mb`
  * `--accountsdb-repl-bind-address`, `--accountsdb-repl-port`, `--accountsdb-repl-threads`, `--enable-accountsdb-repl`
  * `--disable-quic-servers`, `--enable-quic-servers`
  * `--etcd-cacert-file`, `--etcd-cert-file`, `--etcd-domain-name`, `--etcd-endpoint`, `--etcd-key-file`, `--tower-storage`
  * `--no-check-vote-account`
  * `--no-rocksdb-compaction`, `--rocksdb-compaction-interval-slots`, `--rocksdb-max-compaction-jitter-slots`
  * `--replay-slots-concurrently`
    * Use `--replay-forks-threads` with a value of `4` to match preexisting behavior
  * `--rpc-pubsub-max-connections`, `--rpc-pubsub-max-fragment-size`, `--rpc-pubsub-max-in-buffer-capacity`, `--rpc-pubsub-max-out-buffer-capacity`, `--enable-cpi-and-log-storage`, `--minimal-rpc-api`
  * `--skip-poh-verify`
* Deprecated snapshot archive formats have been removed and are no longer loadable.
* Using `--snapshot-interval-slots 0` to disable generating snapshots has been removed. Use `--no-snapshots` instead.
* Validator will now bind all ports within provided `--dynamic-port-range`, including the client ports. A range of at least 25 ports is recommended to avoid failures to bind during startup.
* Agave and agave-ledger-tool can no longer operate with legacy shreds. Legacy shreds have not been in circulation since the activation of https://explorer.solana.com/address/GV49KKQdBNaiv2pgqhS2Dy3GWYJGXMTVYbYkdk91orRy. This change may break operations with old ledgers that may still contain legacy shreds.

#### Changes
* `--transaction-structure view` is now the default.
* The default full snapshot interval is now 100,000 slots.
* `SOLANA_BANKING_THREADS` environment variable is no longer supported. Use `--block-prouduction-num-workers` instead.
* By default, `agave-validator exit` will now wait for the validator process to terminate before returning. The `--wait-for-exit` flag has been deprecated, but operators can still opt out with the new `--no-wait-for-exit` flag.

## 2.3.0

### Validator

#### Breaking
* ABI of `TimedTracedEvent` changed, since `PacketBatch` became an enum, which carries different packet batch types. (#5646)

#### Changes
* Account notifications for Geyser are no longer deduplicated when restoring from a snapshot.
* Add `--no-snapshots` to disable generating snapshots.
* `--block-production-method central-scheduler-greedy` is now the default.
* The default full snapshot interval is now 50,000 slots.
* Graceful exit (via `agave-validtor exit`) is required in order to boot from local state. Refer to the help of `--use-snapshot-archives-at-startup` for more information about booting from local state.

#### Deprecations
* Using `--snapshot-interval-slots 0` to disable generating snapshots is now deprecated.
* Using `blockstore-processor` for `--block-verification-method` is now deprecated.

### Platform Tools SDK

#### Changes
* `cargo-build-sbf` and `cargo-test-sbf` now accept `v0`, `v1`, `v2` and `v3` for the `--arch` argument. These parameters specify the SBPF version to build for.
* SBFPv1 and SBPFv2 are also available for Anza's C compiler toolchain.
* SBPFv3 will be only available for the Rust toolchain. The C toolchain will no longer be supported for SBPFv3 onwards.
* `cargo-build-sbf` now supports the `--optimize-size` argument, which reduces program size, potentially at the cost of increased CU usage.

#### Breaking
* Although the solana rust toolchain still supports the `sbf-solana-solana` target, the new `cargo-build-sbf` version target defaults to `sbpf-solana-solana`. The generated programs will be available on `target/deploy` and `target/sbpf-solana-solana/release`.
* If the `sbf-solana-solana` target folder is still necessary, use `cargo +solana build --triple sbf-solana-solana --release`.
* The target triple changes as well for the new SBPF versions. Triples will be `sbpfv1-solana-solana` for version `v1`, `sbpfv2-solana-solana` for `v2`, and `sbpfv3-solana-solana` for `v3`. Generated programs are available on both the `target/deploy` folder and the `target/<triple>/release` folder. The binary in `target/deploy` has smaller size, since we strip unnecessary sections from the one available in `target/<triple>/release`.
* `cargo-build-sbf` no longer automatically enables the `program` feature to the `solana-sdk` dependency. This feature allowed `solana-sdk` to work in on-chain programs. Users must enable the `program` feature explicitly or use `solana-program` instead. This new behavior only breaks programs using `solana-sdk` v1.3 and earlier.

### CLI

#### Changes
* `withdraw-stake` now accepts the `AVAILABLE` keyword for the amount, allowing withdrawal of unstaked lamports (#4483)
* `solana-test-validator` will now bind to localhost (127.0.0.1) by default rather than all interfaces to improve security. Provide `--bind-address 0.0.0.0` to bind to all interfaces to restore the previous default behavior.

### RPC

#### Changes
* `simulateTransaction` now includes `loadedAccountsDataSize` in its result. `loadedAccountsDataSize` is the total number of bytes loaded for all accounts in the simulated transaction.

## 2.2.0

### CLI

#### Changes
* Add global `--skip-preflight` option for skipping preflight checks on all transactions sent through RPC. This flag, along with `--use-rpc`, can improve success rate with program deployments using the public RPC nodes.
* Add new command `solana feature revoke` for revoking pending feature activations. When a feature is activated, `solana feature revoke <feature-keypair> <cluster>` can be used to deallocate and reassign the account to the System program, undoing the operation. This can only be done before the feature becomes active.

### Validator

#### Breaking
* Blockstore Index column format change
  * The Blockstore Index column format has been updated. The column format written in v2.2 is compatible with v2.1, but incompatible with v2.0 and older.
* Snapshot format change
  * The snapshot format has been modified to implement SIMD-215. Since only adjacent versions are guaranteed to maintain snapshot compatibility, this means snapshots created with v2.2 are compatible with v2.1 and incompatible with v2.0 and older.

#### Changes
* Add new variant to `--block-production-method` for `central-scheduler-greedy`. This is a simplified scheduler that has much better performance than the more strict `central-scheduler` variant.
* Unhide `--accounts-db-access-storages-method` for agave-validator and agave-ledger-tool and change default to `file`
* Remove tracer stats from banking-trace. `banking-trace` directory should be cleared when restarting on v2.2 for first time. It will not break if not cleared, but the file will be a mix of new/old format. (#4043)
* Add `--snapshot-zstd-compression-level` to set the compression level when archiving snapshots with zstd.

#### Deprecations
* Deprecate `--tower-storage` and all `--etcd-*` arguments

### SDK

#### Changes
* `cargo-build-sbf`: add `--skip-tools-install` flag to avoid downloading platform tools and `--no-rustup-override` flag to not use rustup when invoking `cargo`. Useful for immutable environments like Nix.

## 2.1.0
* Breaking:
  * SDK:
    * `cargo-build-bpf` and `cargo-test-bpf` have been deprecated for two years and have now been definitely removed.
       Use `cargo-build-sbf` and `cargo-test-sbf` instead.
    * dependency: `curve25519-dalek` upgraded to new major version 4 (#1693). This causes breakage when mixing v2.0 and v2.1 Solana crates, so be sure to use all of one or the other. Please use only crates compatible with v2.1.
  * Stake:
    * removed the unreleased `redelegate` instruction processor and CLI commands (#2213)
  * Banks-client:
    * relax functions to use `&self` instead of `&mut self` (#2591)
  * `agave-validator`:
    * Remove the deprecated value of `fifo` for `--rocksdb-shred-compaction` (#3451)
* Changes
  * SDK:
    * removed the `respan` macro. This was marked as "internal use only" and was no longer used internally.
    * add `entrypoint_no_alloc!`, a more performant program entrypoint that avoids allocations, saving 20-30 CUs per unique account
    * `cargo-build-sbf`: a workspace or package-level Cargo.toml may specify `tools-version` for overriding the default platform tools version when building on-chain programs. For example:
```toml
[package.metadata.solana]
tools-version = "1.43"
```
or
```toml
[workspace.metadata.solana]
tools-version = "1.43"
```
The order of precedence for the chosen tools version goes: `--tools-version` argument, package version, workspace version, and finally default version.
  * `package-metadata`: specify a program's id in Cargo.toml for easy consumption by downstream users and tools using `solana-package-metadata` (#1806). For example:
```toml
[package.metadata.solana]
program-id = "MyProgram1111111111111111111111111111111111"
```
Can be consumed in the program crate:
```rust
solana_package_metadata::declare_id_with_package_metadata!("solana.program-id");
```
This is equivalent to writing:
```rust
solana_pubkey::declare_id!("MyProgram1111111111111111111111111111111111");
```
  * `agave-validator`: Update PoH speed check to compare against current hash rate from a Bank (#2447)
  * `solana-test-validator`: Add `--clone-feature-set` flag to mimic features from a target cluster (#2480)
  * `solana-genesis`: the `--cluster-type` parameter now clones the feature set from the target cluster (#2587)
  * `unified-scheduler` as default option for `--block-verification-method` (#2653)
  * warn that `thread-local-multi-iterator` option for `--block-production-method` is deprecated (#3113)

## 2.0.0
* Breaking
  * SDK:
    * Support for Borsh v0.9 removed, please use v1 or v0.10 (#1440)
    * `Copy` is no longer derived on `Rent` and `EpochSchedule`, please switch to using `clone()` (solana-labs#32767)
    * `solana-sdk`: deprecated symbols removed
    * `solana-program`: deprecated symbols removed
  * RPC: obsolete and deprecated v1 endpoints are removed. These endpoints are:
    confirmTransaction, getSignatureStatus, getSignatureConfirmation, getTotalSupply,
    getConfirmedSignaturesForAddress, getConfirmedBlock, getConfirmedBlocks, getConfirmedBlocksWithLimit,
    getConfirmedTransaction, getConfirmedSignaturesForAddress2, getRecentBlockhash, getFees,
    getFeeCalculatorForBlockhash, getFeeRateGovernor, getSnapshotSlot getStakeActivation
  * Deprecated methods are removed from `RpcClient` and `RpcClient::nonblocking`
  * `solana-client`: deprecated re-exports removed; please import `solana-connection-cache`, `solana-quic-client`, or `solana-udp-client` directly
  * Deprecated arguments removed from `agave-validator`:
    * `--enable-rpc-obsolete_v1_7` (#1886)
    * `--accounts-db-caching-enabled` (#2063)
    * `--accounts-db-index-hashing` (#2063)
    * `--no-accounts-db-index-hashing` (#2063)
    * `--incremental-snapshots` (#2148)
    * `--halt-on-known-validators-accounts-hash-mismatch` (#2157)
* Changes
  * `central-scheduler` as default option for `--block-production-method` (#34891)
  * `solana-rpc-client-api`: `RpcFilterError` depends on `base64` version 0.22, so users may need to upgrade to `base64` version 0.22
  * Changed default value for `--health-check-slot-distance` from 150 to 128
  * CLI: Can specify `--with-compute-unit-price`, `--max-sign-attempts`, and `--use-rpc` during program deployment
  * RPC's `simulateTransaction` now returns an extra `replacementBlockhash` field in the response
    when the `replaceRecentBlockhash` config param is `true` (#380)
  * SDK: `cargo test-sbf` accepts `--tools-version`, just like `build-sbf` (#1359)
  * CLI: Can specify `--full-snapshot-archive-path` (#1631)
  * transaction-status: The SPL Token `amountToUiAmount` instruction parses the amount into a string instead of a number (#1737)
  * Implemented partitioned epoch rewards as per [SIMD-0118](https://github.com/solana-foundation/solana-improvement-documents/blob/fae25d5a950f43bd787f1f5d75897ef1fdd425a7/proposals/0118-partitioned-epoch-reward-distribution.md). Feature gate: #426. Specific changes include:
    * EpochRewards sysvar expanded and made persistent (#428, #572)
    * Stake Program credits now allowed during distribution (#631)
    * Updated type in Bank::epoch_rewards_status (#1277)
    * Partitions are recalculated on boot from snapshot (#1159)
    * `epoch_rewards_status` removed from snapshot (#1274)
  * Added `unified-scheduler` option for `--block-verification-method` (#1668)
  * Deprecate the `fifo` option for `--rocksdb-shred-compaction` (#1882)
    * `fifo` will remain supported in v2.0 with plans to fully remove in v2.1

## 1.18.0
* Changes
  * Added a github check to support `changelog` label
  * The default for `--use-snapshot-archives-at-startup` is now `when-newest` (#33883)
    * The default for `solana-ledger-tool`, however, remains `always` (#34228)
  * Added `central-scheduler` option for `--block-production-method` (#33890)
  * Updated to Borsh v1
  * Added allow_commission_decrease_at_any_time feature which will allow commission on a vote account to be
    decreased even in the second half of epochs when the commission_updates_only_allowed_in_first_half_of_epoch
    feature would have prevented it
  * Updated local ledger storage so that the RPC endpoint
    `getSignaturesForAddress` always returns signatures in block-inclusion order
  * RPC's `simulateTransaction` now returns `innerInstructions` as `json`/`jsonParsed` (#34313).
  * Bigtable upload now includes entry summary data for each slot, stored in a
    new `entries` table
  * Forbid multiple values for the `--signer` CLI flag, forcing users to specify multiple occurrences of `--signer`, one for each signature
  * New program deployments default to the exact size of a program, instead of
    double the size. Program accounts must be extended with `solana program extend`
    before an upgrade if they need to accommodate larger programs.
  * Interface for `gossip_service::get_client()` has changed. `gossip_service::get_multi_client()` has been removed.
  * CLI: Can specify `--with-compute-unit-price`, `--max-sign-attempts`, and `--use-rpc` during program deployment
* Upgrade Notes
  * `solana-program` and `solana-sdk` default to support for Borsh v1, with
limited backward compatibility for v0.10 and v0.9. Please upgrade to Borsh v1.
  * Operators running their own bigtable instances need to create the `entries`
    table before upgrading their warehouse nodes

## 1.17.0
* Changes
  * Added a changelog.
  * Added `--use-snapshot-archives-at-startup` for faster validator restarts
* Upgrade Notes

## Adding to this Changelog
### Audience
* Entries in this log are intended to be easily understood by contributors,
consensus validator operators, rpc operators, and dapp developers.

### Noteworthy
* A change is noteworthy if it:
  * Adds a feature gate, or
  * Implements a SIMD, or
  * Modifies a public API, or
  * Changes normal validator / rpc run configurations, or
  * Changes command line arguments, or
  * Fixes a bug that has received public attention, or
  * Significantly improves performance, or
  * Is authored by an external contributor.

### Instructions
* Update this log in the same pull request that implements the change. If the
change is spread over several pull requests update this log in the one that
makes the feature code complete.
* Add notes to the [Unreleased] section in each branch that you merge to.
  * Add a description of your change to the Changes section.
  * Add Upgrade Notes if the change is likely to require:
    * validator or rpc operators to update their configs, or
    * dapp or client developers to make changes.
* Link to any relevant feature gate issues or SIMDs.
* If you add entries on multiple branches use the same wording if possible.
This simplifies the process of diffing between versions of the log.

================
File: ci/bench/common.sh
================
here="$(dirname "$0")"
#shellcheck source=ci/_
source "$here"/../_
source "$here"/../upload-ci-artifact.sh
source "$here"/../rust-version.sh nightly
export RUST_BACKTRACE=1
export UPLOAD_METRICS=""
export TARGET_BRANCH=$CI_BRANCH
if [[ -z $CI_BRANCH ]] || [[ -n $CI_PULL_REQUEST ]]; then
  TARGET_BRANCH=$EDGE_CHANNEL
else
  UPLOAD_METRICS="upload"
fi
export BENCH_FILE=bench_output.log
export BENCH_ARTIFACT=current_bench_results.log
_ cargo +"$rust_nightly" build --release

================
File: ci/bench/part1.sh
================
set -eo pipefail
here="$(dirname "$0")"
#shellcheck source=ci/bench/common.sh
source "$here"/common.sh
_ cargo +"$rust_nightly" bench --manifest-path core/Cargo.toml ${V:+--verbose} \
  -- -Z unstable-options --format=json | tee -a "$BENCH_FILE"
_ cargo +"$rust_nightly" bench --manifest-path gossip/Cargo.toml ${V:+--verbose} \
  -- -Z unstable-options --format=json | tee -a "$BENCH_FILE"
_ cargo +"$rust_nightly" bench --manifest-path poh/Cargo.toml ${V:+--verbose} \
  -- -Z unstable-options --format=json | tee -a "$BENCH_FILE"
_ cargo +"$rust_nightly" bench --manifest-path turbine/Cargo.toml ${V:+--verbose} \
  -- -Z unstable-options --format=json | tee -a "$BENCH_FILE"

================
File: ci/bench/part2.sh
================
set -eo pipefail
here="$(dirname "$0")"
#shellcheck source=ci/bench/common.sh
source "$here"/common.sh
_ cargo +"$rust_nightly" bench --manifest-path runtime/Cargo.toml ${V:+--verbose} \
  -- -Z unstable-options --format=json | tee -a "$BENCH_FILE"
(
  _ cargo build --manifest-path=keygen/Cargo.toml
  export PATH="$PWD/target/debug":$PATH
  _ make -C programs/sbf all
  _ cargo +"$rust_nightly" bench --manifest-path programs/sbf/Cargo.toml ${V:+--verbose} --features=sbf_c \
    -- -Z unstable-options --format=json --nocapture | tee -a "$BENCH_FILE"
)
_ cargo +"$rust_nightly" run --release --manifest-path banking-bench/Cargo.toml ${V:+--verbose} | tee -a "$BENCH_FILE"
_ cargo +"$rust_nightly" bench --manifest-path programs/zk-elgamal-proof/Cargo.toml ${V:+--verbose} | tee -a "$BENCH_FILE"
_ cargo +"$rust_nightly" bench --manifest-path precompiles/Cargo.toml ${V:+--verbose} | tee -a "$BENCH_FILE"

================
File: ci/common/limit-threads.sh
================
set -e
if [[ -f "/proc/meminfo" ]]; then
  JOBS=$(grep MemTotal /proc/meminfo | awk '{printf "%.0f", ($2 / (4 * 1024 * 1024))}')
else
  JOBS=$(sysctl hw.memsize | awk '{printf "%.0f", ($2 / (4 * 1024**3))}')
fi
NPROC=$(nproc)
JOBS=$((JOBS > NPROC ? NPROC : JOBS))
export NPROC
export JOBS

================
File: ci/common/shared-functions.sh
================
need_to_upload_test_result() {
  local branches=(
    "$EDGE_CHANNEL"
    "$BETA_CHANNEL"
    "$STABLE_CHANNEL"
  )
  for n in "${branches[@]}"; do
    if [[ "$CI_BRANCH" == "$n" ]]; then
      return 0
    fi
  done
  return 1
}
exit_if_error() {
  if [[ "$1" -ne 0 ]]; then
    exit "$1"
  fi
}

================
File: ci/coverage/common.sh
================
set -euo pipefail
export PART_1_PACKAGES=(
  solana-ledger
)
export PART_2_PACKAGES=(
  solana-accounts-db
  solana-runtime
  solana-perf
  solana-core
  solana-wen-restart
  solana-gossip
)

================
File: ci/coverage/part-1.sh
================
set -euo pipefail
git_root=$(git rev-parse --show-toplevel)
source "$git_root"/ci/coverage/common.sh
packages=()
for package in "${PART_1_PACKAGES[@]}"; do
  packages+=(--package "$package")
done
echo "--- coverage: root (part 1)"
"$git_root"/ci/test-coverage.sh \
  --features frozen-abi \
  --features dev-context-only-utils \
  --lib \
  "${packages[@]}"

================
File: ci/coverage/part-2.sh
================
set -euo pipefail
git_root=$(git rev-parse --show-toplevel)
source "$git_root"/ci/coverage/common.sh
packages=()
for package in "${PART_2_PACKAGES[@]}"; do
  packages+=(--package "$package")
done
echo "--- coverage: root (part 2)"
"$git_root"/ci/test-coverage.sh \
  --features dev-context-only-utils \
  --lib \
  "${packages[@]}"

================
File: ci/coverage/part-3.sh
================
set -euo pipefail
git_root=$(git rev-parse --show-toplevel)
source "$git_root"/ci/coverage/common.sh
exclude_packages=()
for package in "${PART_1_PACKAGES[@]}"; do
  exclude_packages+=(--exclude "$package")
done
for package in "${PART_2_PACKAGES[@]}"; do
  exclude_packages+=(--exclude "$package")
done
exclude_packages+=(--exclude solana-local-cluster)
echo "--- coverage: coverage (part 3)"
"$git_root"/ci/test-coverage.sh \
  --features frozen-abi \
  --features dev-context-only-utils \
  --workspace \
  --lib \
  "${exclude_packages[@]}"
cargo clean
echo "--- coverage: dev-bins"
"$git_root"/ci/test-coverage.sh \
  --features dev-context-only-utils \
  --manifest-path "$git_root"/dev-bins/Cargo.toml \
  --workspace \
  --lib
cargo clean
echo "--- coverage: xtask"
"$git_root"/ci/test-coverage.sh --manifest-path "$git_root"/ci/xtask/Cargo.toml

================
File: ci/docker/build.sh
================
set -e
here="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$here/env.sh"
platform=()
if [[ $(uname -m) = arm64 ]]; then
  platform+=(--platform linux/amd64)
fi
echo "build image: ${CI_DOCKER_IMAGE:?}"
docker build "${platform[@]}" \
  -f "$here/Dockerfile" \
  --build-arg "BASE_IMAGE=${CI_DOCKER_ARG_BASE_IMAGE}" \
  --build-arg "RUST_VERSION=${CI_DOCKER_ARG_RUST_VERSION}" \
  --build-arg "RUST_NIGHTLY_VERSION=${CI_DOCKER_ARG_RUST_NIGHTLY_VERSION}" \
  --build-arg "NODE_MAJOR=${CI_DOCKER_ARG_NODE_MAJOR}" \
  --build-arg "SCCACHE_VERSION=${CI_DOCKER_ARG_SCCACHE_VERSION}" \
  --build-arg "GRCOV_VERSION=${CI_DOCKER_ARG_GRCOV_VERSION}" \
  -t "$CI_DOCKER_IMAGE" .
docker push "$CI_DOCKER_IMAGE"

================
File: ci/docker/Dockerfile
================
ARG BASE_IMAGE=
FROM ${BASE_IMAGE}

ARG \
  RUST_VERSION= \
  RUST_NIGHTLY_VERSION= \
  NODE_MAJOR= \
  SCCACHE_VERSION= \
  GRCOV_VERSION=

SHELL ["/bin/bash", "-o", "pipefail", "-c"]

ENV \
  DEBIAN_FRONTEND=noninteractive \
  TZ=UTC

# rust
ENV \
  RUSTUP_HOME=/usr/local/rustup \
  CARGO_HOME=/usr/local/cargo \
  PATH="$PATH:/usr/local/cargo/bin"

RUN \
  if [ -z "$RUST_VERSION" ]; then echo "ERROR: The RUST_VERSION argument is required!" && exit 1; fi && \
  if [ -z "$RUST_NIGHTLY_VERSION" ]; then echo "ERROR: The RUST_NIGHTLY_VERSION argument is required!" && exit 1; fi && \
  if [ -z "$NODE_MAJOR" ]; then echo "ERROR: The NODE_MAJOR argument is required!" && exit 1; fi && \
  if [ -z "$SCCACHE_VERSION" ]; then echo "ERROR: The SCCACHE_VERSION argument is required!" && exit 1; fi && \
  if [ -z "$GRCOV_VERSION" ]; then echo "ERROR: The GRCOV_VERSION argument is required!" && exit 1; fi && \
  apt-get update && \
  apt-get install --no-install-recommends -y \
  # basic
  tzdata \
  apt-transport-https \
  sudo \
  build-essential \
  git \
  vim \
  jq \
  ca-certificates \
  curl \
  gnupg \
  lld \
  cmake \
  openssh-client \
  # docs
  mscgen \
  # solana compiling
  libssl-dev \
  libudev-dev \
  pkg-config \
  zlib1g-dev \
  llvm \
  clang \
  cmake \
  make \
  libprotobuf-dev \
  protobuf-compiler \
  libclang-dev \
  && \
  # buildkite
  curl -fsSL https://keys.openpgp.org/vks/v1/by-fingerprint/32A37959C2FA5C3C99EFBC32A79206696452D198 | gpg --dearmor -o /usr/share/keyrings/buildkite-agent-archive-keyring.gpg && \
  echo "deb [signed-by=/usr/share/keyrings/buildkite-agent-archive-keyring.gpg] https://apt.buildkite.com/buildkite-agent stable main" | tee /etc/apt/sources.list.d/buildkite-agent.list && \
  apt-get update && \
  apt-get install -y buildkite-agent && \
  # gh
  curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && \
  sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg && \
  echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && \
  apt-get update && \
  apt-get install -y gh && \
  # rust
  curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs/ | sh -s -- --no-modify-path --profile minimal --default-toolchain $RUST_VERSION -y && \
  rustup component add rustfmt && \
  rustup component add clippy && \
  rustup install $RUST_NIGHTLY_VERSION && \
  rustup component add clippy --toolchain=$RUST_NIGHTLY_VERSION && \
  rustup component add rustfmt --toolchain=$RUST_NIGHTLY_VERSION && \
  rustup component add miri --toolchain=$RUST_NIGHTLY_VERSION && \
  rustup component add llvm-tools-preview --toolchain=$RUST_NIGHTLY_VERSION && \
  rustup target add wasm32-unknown-unknown && \
  cargo install cargo-audit && \
  # uncomment once the dcou-parition related patch is upstreamed...
  # cargo install cargo-hack && \
  cargo install --git https://github.com/anza-xyz/cargo-hack.git --rev 5e59c3ec6c661c02601487c0d4b2a2649fe06c9f cargo-hack && \
  cargo install cargo-sort@^2 && \
  cargo install mdbook && \
  cargo install mdbook-linkcheck && \
  cargo install svgbob_cli && \
  cargo install wasm-pack && \
  cargo install rustfilt && \
  rustup show && \
  rustc --version && \
  cargo --version && \
  chmod -R a+w $CARGO_HOME $RUSTUP_HOME && \
  rm -rf $CARGO_HOME/registry && \
  # sccache
  curl -LOsS "https://github.com/mozilla/sccache/releases/download/$SCCACHE_VERSION/sccache-$SCCACHE_VERSION-x86_64-unknown-linux-musl.tar.gz" && \
  tar -xzf "sccache-$SCCACHE_VERSION-x86_64-unknown-linux-musl.tar.gz" && \
  mv "sccache-$SCCACHE_VERSION-x86_64-unknown-linux-musl"/sccache "$CARGO_HOME/bin/" && \
  rm "sccache-$SCCACHE_VERSION-x86_64-unknown-linux-musl.tar.gz" && \
  rm -rf "sccache-$SCCACHE_VERSION-x86_64-unknown-linux-musl" && \
  # nextest
  curl -LsSf https://get.nexte.st/latest/linux | tar zxf - -C "$CARGO_HOME/bin" && \
  # nodejs
  sudo mkdir -p /etc/apt/keyrings && \
  curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg && \
  echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list && \
  sudo apt-get update && \
  sudo apt-get install -y nodejs && \
  # setup path
  mkdir /.cache && \
  chmod -R a+w /.cache && \
  mkdir /.config && \
  chmod -R a+w /.config && \
  mkdir /.npm && \
  chmod -R a+w /.npm && \
  # grcov
  curl -LOsS "https://github.com/mozilla/grcov/releases/download/$GRCOV_VERSION/grcov-x86_64-unknown-linux-musl.tar.bz2" && \
  tar -xf grcov-x86_64-unknown-linux-musl.tar.bz2 && \
  mv ./grcov $CARGO_HOME/bin && \
  rm grcov-x86_64-unknown-linux-musl.tar.bz2 && \
  # codecov
  curl -Os https://uploader.codecov.io/latest/linux/codecov && \
  chmod +x codecov && \
  mv codecov /usr/bin && \
  # clean lists
  rm -rf /var/lib/apt/lists/*

================
File: ci/docker/env.sh
================
ci_docker_env_sh_here="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
NO_INSTALL=1 source "${ci_docker_env_sh_here}/../rust-version.sh"
if [[ -z "${rust_stable}" || -z "${rust_nightly}" ]]; then
  echo "Error: rust_stable or rust_nightly is empty. Please check rust-version.sh." >&2
  exit 1
fi
export CI_DOCKER_ARG_BASE_IMAGE=ubuntu:22.04
export CI_DOCKER_ARG_RUST_VERSION="${rust_stable}"
export CI_DOCKER_ARG_RUST_NIGHTLY_VERSION="${rust_nightly}"
export CI_DOCKER_ARG_NODE_MAJOR=24
export CI_DOCKER_ARG_SCCACHE_VERSION=v0.9.1
export CI_DOCKER_ARG_GRCOV_VERSION=v0.8.18
hash_vars=(
  "$(cat "${ci_docker_env_sh_here}/Dockerfile")"
  "${CI_DOCKER_ARG_BASE_IMAGE}"
  "${CI_DOCKER_ARG_RUST_VERSION}"
  "${CI_DOCKER_ARG_RUST_NIGHTLY_VERSION}"
  "${CI_DOCKER_ARG_NODE_MAJOR}"
  "${CI_DOCKER_ARG_SCCACHE_VERSION}"
  "${CI_DOCKER_ARG_GRCOV_VERSION}"
)
hash_input=$(IFS="_"; echo "${hash_vars[*]}")
ci_docker_hash=$(echo -n "${hash_input}" | sha256sum | head -c 8)
CI_DOCKER_SANITIZED_BASE_IMAGE="${CI_DOCKER_ARG_BASE_IMAGE//:/-}"
export CI_DOCKER_IMAGE="anzaxyz/ci:${CI_DOCKER_SANITIZED_BASE_IMAGE}_rust-${CI_DOCKER_ARG_RUST_VERSION}_${CI_DOCKER_ARG_RUST_NIGHTLY_VERSION}_${ci_docker_hash}"

================
File: ci/docker/README.md
================
Docker image containing rust, rust nightly and some preinstalled packages used in CI

This image is manually maintained:

#### CLI

1. Edit
   1. `ci/rust-version.sh` for rust and rust nightly version
   2. `ci/docker/env.sh` for some environment variables
   3. `ci/docker/Dockerfile` for some other packages
2. Ensure you're a member of the [Anza Docker Hub Organization](https://hub.docker.com/u/anzaxyz) and already `docker login`
3. Run `ci/docker/build.sh` to build/publish the new image

================
File: ci/downstream-projects/common.sh
================
set -e
source ci/_
source ci/semver_bash/semver.sh
source scripts/patch-crates.sh
source scripts/read-cargo-variable.sh
SOLANA_VER=$(readCargoVariable version Cargo.toml)
export SOLANA_VER
export SOLANA_DIR=$PWD
export CARGO="$SOLANA_DIR"/cargo
export CARGO_BUILD_SBF="$SOLANA_DIR"/cargo-build-sbf
export CARGO_TEST_SBF="$SOLANA_DIR"/cargo-test-sbf
mkdir -p target/downstream-projects
cd target/downstream-projects

================
File: ci/downstream-projects/func-openbook-dex.sh
================
openbook_dex() {
  (
    set -x
    rm -rf openbook-dex
    git clone https://github.com/openbook-dex/program.git openbook-dex
    cp "$SOLANA_DIR"/rust-toolchain.toml openbook-dex/
    cd openbook-dex || exit 1
    update_solana_dependencies . "$SOLANA_VER"
    patch_crates_io_solana Cargo.toml "$SOLANA_DIR"
    cat >> Cargo.toml <<EOF
anchor-lang = { git = "https://github.com/coral-xyz/anchor.git", branch = "master" }
EOF
    patch_crates_io_solana dex/Cargo.toml "$SOLANA_DIR"
    cat >> dex/Cargo.toml <<EOF
anchor-lang = { git = "https://github.com/coral-xyz/anchor.git", branch = "master" }
[workspace]
exclude = [
    "crank",
    "permissioned",
]
EOF
    cargo build
    $CARGO_BUILD_SBF \
      --manifest-path dex/Cargo.toml --no-default-features --features program
    cargo test \
      --manifest-path dex/Cargo.toml --no-default-features --features program
  )
}

================
File: ci/downstream-projects/func-spl.sh
================
spl() {
  (
    PROGRAMS=(
      instruction-padding/program
      token/transfer-hook/example
      token/program
      token/program-2022
      token/program-2022-test
      associated-token-account/program
      token-upgrade/program
      feature-proposal/program
      governance/addin-mock/program
      governance/program
      name-service/program
      stake-pool/program
      single-pool/program
    )
    set -x
    rm -rf spl
    git clone https://github.com/solana-labs/solana-program-library.git spl
    cp "$SOLANA_DIR"/rust-toolchain.toml spl/
    cd spl || exit 1
    project_used_solana_version=$(sed -nE 's/solana-sdk = \"[>=<~]*(.*)\"/\1/p' <"token/program/Cargo.toml")
    echo "used solana version: $project_used_solana_version"
    if semverGT "$project_used_solana_version" "$SOLANA_VER"; then
      echo "skip"
      return
    fi
    ./patch.crates-io.sh "$SOLANA_DIR"
    for program in "${PROGRAMS[@]}"; do
      $CARGO_TEST_SBF --manifest-path "$program"/Cargo.toml
    done
    cargo build
    cargo test
  )
}

================
File: ci/downstream-projects/run-all.sh
================
set -e
here="$(dirname "$0")"
#shellcheck source=ci/downstream-projects/func-spl.sh
source "$here"/func-spl.sh
source "$here"/func-openbook-dex.sh
source "$here"/common.sh
_ spl
_ openbook_dex

================
File: ci/downstream-projects/run-openbook-dex.sh
================
set -e
here="$(dirname "$0")"
#shellcheck source=ci/downstream-projects/func-openbook-dex.sh
source "$here"/func-openbook-dex.sh
source "$here"/common.sh
_ openbook_dex

================
File: ci/downstream-projects/run-spl.sh
================
set -e
here="$(dirname "$0")"
#shellcheck source=ci/downstream-projects/func-spl.sh
source "$here"/func-spl.sh
source "$here"/common.sh
_ spl

================
File: ci/semver_bash/LICENSE
================
Copyright (c) 2013, Ray Bejjani
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met: 

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer. 
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution. 

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

The views and conclusions contained in the software and documentation are those
of the authors and should not be interpreted as representing official policies, 
either expressed or implied, of the FreeBSD Project.

================
File: ci/semver_bash/README.md
================
semver_bash is a bash parser for semantic versioning
====================================================

[Semantic Versioning](http://semver.org/) is a set of guidelines that help keep
version and version management sane. This is a bash based parser to help manage
a project's versions. Use it from a Makefile or any scripts you use in your
project.

Usage
-----
semver_bash can be used from the command line as:  

    $ ./semver.sh "3.2.1" "3.2.1-alpha"  
    3.2.1 -> M: 3 m:2 p:1 s:  
    3.2.1-alpha -> M: 3 m:2 p:1 s:-alpha  
    3.2.1 == 3.2.1-alpha -> 1.  
    3.2.1 < 3.2.1-alpha -> 1.  
    3.2.1 > 3.2.1-alpha -> 0.


Alternatively, you can source it from within a script:

    . ./semver.sh  
    
    local MAJOR=0  
    local MINOR=0  
    local PATCH=0  
    local SPECIAL=""
    
    semverParseInto "1.2.3" MAJOR MINOR PATCH SPECIAL  
    semverParseInto "3.2.1" MAJOR MINOR PATCH SPECIAL

================
File: ci/semver_bash/semver_test.sh
================
. ./semver.sh
semverTest() {
local A=R1.3.2
local B=R2.3.2
local C=R1.4.2
local D=R1.3.3
local E=R1.3.2a
local F=R1.3.2b
local G=R1.2.3
local MAJOR=0
local MINOR=0
local PATCH=0
local SPECIAL=""
semverParseInto $A MAJOR MINOR PATCH SPECIAL
echo "$A -> M:$MAJOR m:$MINOR p:$PATCH s:$SPECIAL. Expect M:1 m:3 p:2 s:"
semverParseInto $E MAJOR MINOR PATCH SPECIAL
echo "$E -> M:$MAJOR m:$MINOR p:$PATCH s:$SPECIAL. Expect M:1 m:3 p:2 s:a"
echo "Equality comparisons"
semverEQ $A $A
echo "$A == $A -> $?. Expect 0."
semverLT $A $A
echo "$A < $A -> $?. Expect 1."
semverGT $A $A
echo "$A > $A -> $?. Expect 1."
echo "Major number comparisons"
semverEQ $A $B
echo "$A == $B -> $?. Expect 1."
semverLT $A $B
echo "$A < $B -> $?. Expect 0."
semverGT $A $B
echo "$A > $B -> $?. Expect 1."
semverEQ $B $A
echo "$B == $A -> $?. Expect 1."
semverLT $B $A
echo "$B < $A -> $?. Expect 1."
semverGT $B $A
echo "$B > $A -> $?. Expect 0."
echo "Minor number comparisons"
semverEQ $A $C
echo "$A == $C -> $?. Expect 1."
semverLT $A $C
echo "$A < $C -> $?. Expect 0."
semverGT $A $C
echo "$A > $C -> $?. Expect 1."
semverEQ $C $A
echo "$C == $A -> $?. Expect 1."
semverLT $C $A
echo "$C < $A -> $?. Expect 1."
semverGT $C $A
echo "$C > $A -> $?. Expect 0."
echo "patch number comparisons"
semverEQ $A $D
echo "$A == $D -> $?. Expect 1."
semverLT $A $D
echo "$A < $D -> $?. Expect 0."
semverGT $A $D
echo "$A > $D -> $?. Expect 1."
semverEQ $D $A
echo "$D == $A -> $?. Expect 1."
semverLT $D $A
echo "$D < $A -> $?. Expect 1."
semverGT $D $A
echo "$D > $A -> $?. Expect 0."
echo "special section vs no special comparisons"
semverEQ $A $E
echo "$A == $E -> $?. Expect 1."
semverLT $A $E
echo "$A < $E -> $?. Expect 1."
semverGT $A $E
echo "$A > $E -> $?. Expect 0."
semverEQ $E $A
echo "$E == $A -> $?. Expect 1."
semverLT $E $A
echo "$E < $A -> $?. Expect 0."
semverGT $E $A
echo "$E > $A -> $?. Expect 1."
echo "special section vs special comparisons"
semverEQ $E $F
echo "$E == $F -> $?. Expect 1."
semverLT $E $F
echo "$E < $F -> $?. Expect 0."
semverGT $E $F
echo "$E > $F -> $?. Expect 1."
semverEQ $F $E
echo "$F == $E -> $?. Expect 1."
semverLT $F $E
echo "$F < $E -> $?. Expect 1."
semverGT $F $E
echo "$F > $E -> $?. Expect 0."
echo "Minor and patch number comparisons"
semverEQ $A $G
echo "$A == $G -> $?. Expect 1."
semverLT $A $G
echo "$A < $G -> $?. Expect 1."
semverGT $A $G
echo "$A > $G -> $?. Expect 0."
semverEQ $G $A
echo "$G == $A -> $?. Expect 1."
semverLT $G $A
echo "$G < $A -> $?. Expect 0."
semverGT $G $A
echo "$G > $A -> $?. Expect 1."
}
semverTest

================
File: ci/semver_bash/semver.sh
================
function semverParseInto() {
    local RE='[^0-9]*\([0-9]*\)[.]\([0-9]*\)[.]\([0-9]*\)\([\.0-9A-Za-z-]*\)'
    eval $2=`echo $1 | sed -e "s#$RE#\1#"`
    eval $3=`echo $1 | sed -e "s#$RE#\2#"`
    eval $4=`echo $1 | sed -e "s#$RE#\3#"`
    eval $5=`echo $1 | sed -e "s#$RE#\4#"`
}
function semverEQ() {
    local MAJOR_A=0
    local MINOR_A=0
    local PATCH_A=0
    local SPECIAL_A=0
    local MAJOR_B=0
    local MINOR_B=0
    local PATCH_B=0
    local SPECIAL_B=0
    semverParseInto $1 MAJOR_A MINOR_A PATCH_A SPECIAL_A
    semverParseInto $2 MAJOR_B MINOR_B PATCH_B SPECIAL_B
    if [ $MAJOR_A -ne $MAJOR_B ]; then
        return 1
    fi
    if [ $MINOR_A -ne $MINOR_B ]; then
        return 1
    fi
    if [ $PATCH_A -ne $PATCH_B ]; then
        return 1
    fi
    if [[ "_$SPECIAL_A" != "_$SPECIAL_B" ]]; then
        return 1
    fi
    return 0
}
function semverLT() {
    local MAJOR_A=0
    local MINOR_A=0
    local PATCH_A=0
    local SPECIAL_A=0
    local MAJOR_B=0
    local MINOR_B=0
    local PATCH_B=0
    local SPECIAL_B=0
    semverParseInto $1 MAJOR_A MINOR_A PATCH_A SPECIAL_A
    semverParseInto $2 MAJOR_B MINOR_B PATCH_B SPECIAL_B
    if [ $MAJOR_A -lt $MAJOR_B ]; then
        return 0
    fi
    if [[ $MAJOR_A -le $MAJOR_B  && $MINOR_A -lt $MINOR_B ]]; then
        return 0
    fi
    if [[ $MAJOR_A -le $MAJOR_B  && $MINOR_A -le $MINOR_B && $PATCH_A -lt $PATCH_B ]]; then
        return 0
    fi
    if [[ "_$SPECIAL_A"  == "_" ]] && [[ "_$SPECIAL_B"  == "_" ]] ; then
        return 1
    fi
    if [[ "_$SPECIAL_A"  == "_" ]] && [[ "_$SPECIAL_B"  != "_" ]] ; then
        return 1
    fi
    if [[ "_$SPECIAL_A"  != "_" ]] && [[ "_$SPECIAL_B"  == "_" ]] ; then
        return 0
    fi
    if [[ "_$SPECIAL_A" < "_$SPECIAL_B" ]]; then
        return 0
    fi
    return 1
}
function semverGT() {
    semverEQ $1 $2
    local EQ=$?
    semverLT $1 $2
    local LT=$?
    if [ $EQ -ne 0 ] && [ $LT -ne 0 ]; then
        return 0
    else
        return 1
    fi
}
if [ "___semver.sh" == "___`basename $0`" ]; then
MAJOR=0
MINOR=0
PATCH=0
SPECIAL=""
semverParseInto $1 MAJOR MINOR PATCH SPECIAL
echo "$1 -> M: $MAJOR m:$MINOR p:$PATCH s:$SPECIAL"
semverParseInto $2 MAJOR MINOR PATCH SPECIAL
echo "$2 -> M: $MAJOR m:$MINOR p:$PATCH s:$SPECIAL"
semverEQ $1 $2
echo "$1 == $2 -> $?."
semverLT $1 $2
echo "$1 < $2 -> $?."
semverGT $1 $2
echo "$1 > $2 -> $?."
fi

================
File: ci/stable/common.sh
================
set -e
export RUST_BACKTRACE=1
export RUSTFLAGS="-D warnings"
source ci/_

================
File: ci/stable/run-all.sh
================
set -eo pipefail
here="$(dirname "$0")"
#shellcheck source=ci/common/shared-functions.sh
source "$here"/../common/shared-functions.sh
source "$here"/../common/limit-threads.sh
source "$here"/common.sh
_ ci/intercept.sh cargo test --jobs "$JOBS" --workspace --tests --verbose -- --nocapture

================
File: ci/stable/run-local-cluster-partially.sh
================
set -e
CURRENT=$1
: "${CURRENT:?}"
TOTAL=$2
: "${TOTAL:?}"
if [ "$CURRENT" -gt "$TOTAL" ]; then
  echo "Error: The value of CURRENT (\$1) cannot be greater than the value of TOTAL (\$2)."
  exit 1
fi
here="$(dirname "$0")"
#shellcheck source=ci/common/shared-functions.sh
source "$here"/../common/shared-functions.sh
source "$here"/common.sh
_ cargo nextest run \
  --profile ci \
  --package solana-local-cluster \
  --test local_cluster \
  --partition hash:"$CURRENT/$TOTAL" \
  --test-threads=1 \
  --no-tests=warn

================
File: ci/stable/run-localnet.sh
================
set -e
here="$(dirname "$0")"
export RUST_LOG="solana_metrics=warn,info,$RUST_LOG"
echo --- ci/localnet-sanity.sh
"$here"/../localnet-sanity.sh -x
echo --- ci/run-sanity.sh
"$here"/../run-sanity.sh -x

================
File: ci/stable/run-partition.sh
================
set -eo pipefail
CURRENT=$1
: "${CURRENT:?}"
TOTAL=$2
: "${TOTAL:?}"
if [ "$CURRENT" -gt "$TOTAL" ]; then
  echo "Error: The value of CURRENT (\$1) cannot be greater than the value of TOTAL (\$2)."
  exit 1
fi
here="$(dirname "$0")"
#shellcheck source=ci/common/shared-functions.sh
source "$here"/../common/shared-functions.sh
source "$here"/../common/limit-threads.sh
source "$here"/common.sh
ARGS=(
  --profile ci
  --workspace
  --tests
  --jobs "$JOBS"
  --partition hash:"$CURRENT/$TOTAL"
  --verbose
  --exclude solana-local-cluster
  --no-tests=warn
)
_ cargo nextest run "${ARGS[@]}"

================
File: ci/xtask/src/commands/bump_version.rs
================
pub struct CommandArgs {
⋮----
pub enum BumpLevel {
⋮----
fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
⋮----
write!(f, "{s}")
⋮----
pub fn run(args: CommandArgs) -> Result<()> {
// get the current version
⋮----
crate::common::get_current_version().context("failed to get current version")?;
⋮----
// bump the version
let new_version = bump_version(&args.level, &current_version);
// get all crates
let all_crates = crate::common::get_all_crates().context("failed to get all crates")?;
// update all cargo.toml
⋮----
crate::common::find_all_cargo_tomls().context("failed to find all cargo.toml files")?;
info!("found {} cargo.toml files", all_cargo_tomls.len());
⋮----
info!("processing {}", cargo_toml.display());
// parse the cargo.toml file into a DocumentMut
⋮----
.context(format!("failed to read {}", cargo_toml.display()))?;
⋮----
.context(format!("failed to parse {}", cargo_toml.display()))?;
// check if workspace.package.version is the same as the current version
⋮----
.get("workspace")
.and_then(|workspace| workspace.get("package"))
.and_then(|package| package.get("version"))
.and_then(|version| version.as_str())
⋮----
if workspace_package_version_str == current_version.to_string() {
doc["workspace"]["package"]["version"] = value(new_version.to_string());
info!("  bumped workspace.package.version from {current_version} to {new_version}",);
⋮----
// check if package.version is the same as the current version
⋮----
.get("package")
⋮----
if package_version_str == current_version.to_string() {
doc["package"]["version"] = value(new_version.to_string());
info!("  bumped package.version from {current_version} to {current_version}",);
⋮----
// Update versions in [workspace.dependencies] if they match `current_version`
⋮----
.and_then(|ws| ws.get("dependencies"))
.and_then(|deps| deps.as_table())
⋮----
// Avoid borrowing `doc` while iterating
let keys: Vec<String> = dependencies.iter().map(|(k, _)| k.to_string()).collect();
⋮----
if all_crates.contains(&name) {
⋮----
.get(&name)
.and_then(|v| v.get("version"))
.and_then(|v| v.as_str())
⋮----
if !version.contains(&current_version.to_string()) {
⋮----
let old_version = version.to_string();
⋮----
.replace(&current_version.to_string(), &new_version.to_string());
doc["workspace"]["dependencies"][&name]["version"] = value(&new_version);
info!(
⋮----
// write the updated document back to the file
debug!("writing {}", cargo_toml.display());
fs::write(&cargo_toml, doc.to_string())
.context(format!("failed to write {}", cargo_toml.display()))?;
⋮----
// update all Cargo.lock files
⋮----
crate::common::find_all_cargo_locks().context("failed to find all Cargo.lock files")?;
info!("found {} Cargo.lock files", all_cargo_locks.len());
⋮----
let dir = cargo_lock.parent().context(format!(
⋮----
info!("running `cargo tree` in {}", dir.display());
⋮----
.arg("tree")
.current_dir(dir)
.output()
.context(format!("failed to run `cargo tree` in {}", dir.display()))?;
if !output.status.success() {
return Err(anyhow!("{}", String::from_utf8_lossy(&output.stderr)));
⋮----
Ok(())
⋮----
pub fn bump_version(level: &BumpLevel, current: &Version) -> Version {
let mut new_version = current.clone();
⋮----
new_version.major = new_version.major.saturating_add(1);
⋮----
new_version.minor = new_version.minor.saturating_add(1);
⋮----
new_version.patch = new_version.patch.saturating_add(1);
⋮----
mod tests {
⋮----
fn test_bump_version() {
⋮----
assert_eq!(

================
File: ci/xtask/src/commands/hello.rs
================
pub fn run() -> Result<()> {
debug!("DEBUG MODE");
info!("Hello!");
Ok(())

================
File: ci/xtask/src/commands.rs
================
pub mod bump_version;
pub mod hello;

================
File: ci/xtask/src/common.rs
================
pub fn get_git_root_path() -> Result<PathBuf> {
⋮----
.args(["rev-parse", "--show-toplevel"])
.output()
.map_err(|e| anyhow!("failed to get git root path, error: {e}"))?;
let root = String::from_utf8_lossy(&output.stdout).trim().to_string();
Ok(PathBuf::from(root))
⋮----
pub fn find_files_by_name(filename: &str) -> Result<Vec<PathBuf>> {
let git_root = get_git_root_path()?;
let mut results = vec![];
⋮----
.into_iter()
.filter_entry(|entry| {
⋮----
.path()
.components()
.any(|c| c.as_os_str() == "target" || c.as_os_str() == ".git")
⋮----
.filter_map(Result::ok)
.filter(|e| e.file_name() == filename)
⋮----
results.push(entry.path().to_path_buf());
⋮----
Ok(results)
⋮----
pub fn find_all_cargo_tomls() -> Result<Vec<PathBuf>> {
find_files_by_name("Cargo.toml")
⋮----
pub fn find_all_cargo_locks() -> Result<Vec<PathBuf>> {
find_files_by_name("Cargo.lock")
⋮----
pub fn get_all_crates() -> Result<Vec<String>> {
let cargo_tomls = find_all_cargo_tomls()?;
let mut crates = vec![];
⋮----
.get("package")
.and_then(|package| package.get("name"))
.and_then(|name| name.as_str())
⋮----
crates.push(name.to_string());
⋮----
Ok(crates)
⋮----
pub fn get_current_version() -> Result<String> {
⋮----
let cargo_toml = git_root.join("Cargo.toml");
⋮----
.get("workspace")
.and_then(|workspace| workspace.get("package"))
.and_then(|package| package.get("version"))
.and_then(|version| version.as_str())
⋮----
return Err(anyhow!("failed to get version from Cargo.toml"));
⋮----
Ok(version.to_string())
⋮----
mod tests {
⋮----
fn test_get_git_root_path() {
let temp_dir = tempfile::tempdir().unwrap();
std::env::set_current_dir(temp_dir.path()).unwrap();
Command::new("git").args(["init"]).output().unwrap();
let root_path = get_git_root_path().unwrap();
let canonicalized_root_path = fs::canonicalize(root_path).unwrap();
let canonicalized_temp_dir_path = fs::canonicalize(temp_dir.path()).unwrap();
assert_eq!(canonicalized_root_path, canonicalized_temp_dir_path);
⋮----
fn test_workspace_functions() {
let root_dir = tempfile::tempdir().unwrap();
let root_dir_path = root_dir.path();
std::env::set_current_dir(root_dir_path).unwrap();
⋮----
root_dir_path.join("Cargo.toml"),
⋮----
.unwrap();
fs::write(root_dir_path.join("Cargo.lock"), "").unwrap();
fs::create_dir_all(root_dir_path.join("foo")).unwrap();
⋮----
root_dir_path.join("foo/Cargo.toml"),
⋮----
fs::write(root_dir_path.join("foo/Cargo.lock"), "").unwrap();
fs::create_dir_all(root_dir_path.join("bar")).unwrap();
⋮----
root_dir_path.join("bar/Cargo.toml"),
⋮----
fs::write(root_dir_path.join("bar/Cargo.lock"), "").unwrap();
// test find_files_by_name for Cargo.toml
⋮----
let files = find_all_cargo_tomls().unwrap();
assert_eq!(files.len(), 3);
⋮----
fs::canonicalize(root_dir_path.join("Cargo.toml")).unwrap(),
fs::canonicalize(root_dir_path.join("foo/Cargo.toml")).unwrap(),
fs::canonicalize(root_dir_path.join("bar/Cargo.toml")).unwrap(),
⋮----
.iter()
.cloned()
.collect();
let actual_files: HashSet<_> = files.iter().cloned().collect();
assert_eq!(expected_files, actual_files);
⋮----
let files = find_all_cargo_locks().unwrap();
⋮----
fs::canonicalize(root_dir_path.join("Cargo.lock")).unwrap(),
fs::canonicalize(root_dir_path.join("foo/Cargo.lock")).unwrap(),
fs::canonicalize(root_dir_path.join("bar/Cargo.lock")).unwrap(),
⋮----
let crates = get_all_crates().unwrap();
assert_eq!(crates.len(), 2);
⋮----
["foo", "bar"].iter().map(|s| s.to_string()).collect();
let actual_crates: HashSet<String> = crates.iter().map(|s| s.to_string()).collect();
assert_eq!(expected_crates, actual_crates);
⋮----
let version = get_current_version().unwrap();
assert_eq!(version, "3.1.0");

================
File: ci/xtask/src/main.rs
================
mod commands;
mod common;
⋮----
struct Xtask {
⋮----
enum Commands {
⋮----
pub struct GlobalOptions {
⋮----
async fn main() {
if let Err(err) = try_main().await {
error!("Error: {err}");
for (i, cause) in err.chain().skip(1).enumerate() {
error!("  {}: {}", i.saturating_add(1), cause);
⋮----
async fn try_main() -> Result<()> {
⋮----
Ok(())

================
File: ci/xtask/Cargo.toml
================
[package]
name = "xtask"
version = "0.1.0"
authors = ["Anza Maintainers <maintainers@anza.xyz>"]
description = "Blockchain, Rebuilt for Scale"
repository = "https://github.com/anza-xyz/agave"
homepage = "https://anza.xyz/"
license = "Apache-2.0"
edition = "2021"
publish = false

# Prevents auto-detection of parent workspace (ie. git worktrees).
[workspace]

[features]
agave-unstable-api = []
dummy-for-ci-check = []
frozen-abi = []

[dependencies]
anyhow = "1.0.100"
clap = { version = "4.5.31", features = ["derive"] }
env_logger = "0.11.8"
log = "0.4.28"
semver = "1.0.27"
tokio = { version = "1.48.0", features = ["full"] }
toml_edit = { version = "0.23.7", features = ["serde"] }
walkdir = "2.5.0"

[dev-dependencies]
pretty_assertions = "1.4.1"
serial_test = "3.2.0"
tempfile = "3.23.0"

================
File: ci/_
================
# Buildkite log management helper
#
# See https://buildkite.com/docs/pipelines/managing-log-output
#
# |source| me
#

base_dir=$(realpath --strip "$(dirname "$0")/..")

_() {
  if [[ $(pwd) = $base_dir ]]; then
    echo "--- $*" > /dev/stderr
  else
    echo "--- $* (wd: $(pwd))" > /dev/stderr
  fi
  "$@"
}

================
File: ci/.gitignore
================
/node_modules/
/package-lock.json
/snapcraft.credentials

================
File: ci/buildkite-pipeline.sh
================
set -e
cd "$(dirname "$0")"/..
output_file=${1:-/dev/stderr}
if [[ -n $CI_PULL_REQUEST ]]; then
  # filter pr number from ci branch.
  [[ $CI_BRANCH =~ pull/([0-9]+)/head ]]
  pr_number=${BASH_REMATCH[1]}
  echo "get affected files from PR: $pr_number"
  if [[ $BUILDKITE_REPO =~ ^https:\/\/github\.com\/([^\/]+)\/([^\/\.]+) ]]; then
    owner="${BASH_REMATCH[1]}"
    repo="${BASH_REMATCH[2]}"
  elif [[ $BUILDKITE_REPO =~ ^git@github\.com:([^\/]+)\/([^\/\.]+) ]]; then
    owner="${BASH_REMATCH[1]}"
    repo="${BASH_REMATCH[2]}"
  else
    echo "couldn't parse owner and repo. use defaults"
    owner="anza-xyz"
    repo="agave"
  fi
  query='
  query($owner: String!, $repo: String!, $pr: Int!, $endCursor: String) {
    repository(owner: $owner, name: $repo) {
      pullRequest(number: $pr) {
        files(first: 100, after: $endCursor) {
          pageInfo{ hasNextPage, endCursor }
          nodes {
            path
          }
        }
      }
    }
  }'
  readarray -t affected_files < <(
    gh api graphql \
      -f query="$query" \
      -F pr="$pr_number" \
      -F owner="$owner" \
      -F repo="$repo" \
      --paginate \
      --jq '.data.repository.pullRequest.files.nodes.[].path'
  )
  if [[ ${
    echo "Unable to determine the files affected by this PR"
    exit 1
  fi
else
  affected_files=()
fi
annotate() {
  if [[ -n $BUILDKITE ]]; then
    buildkite-agent annotate "$@"
  fi
}
mandatory_affected_files=()
mandatory_affected_files+=(^ci/buildkite-pipeline.sh)
mandatory_affected_files+=(^ci/docker-rust/Dockerfile)
mandatory_affected_files+=(^ci/docker-rust-nightly/Dockerfile)
affects() {
  if [[ -z $CI_PULL_REQUEST ]]; then
    return 0
  fi
  for pattern in "${mandatory_affected_files[@]}" "$@"; do
    if [[ ${pattern:0:1} = "!" ]]; then
      for file in "${affected_files[@]}"; do
        if [[ ! $file =~ ${pattern:1} ]]; then
          return 0
        fi
      done
    else
      for file in "${affected_files[@]}"; do
        if [[ $file =~ $pattern ]]; then
          return 0
        fi
      done
    fi
  done
  return 1
}
affects_other_than() {
  if [[ -z $CI_PULL_REQUEST ]]; then
    return 0
  fi
  for file in "${affected_files[@]}"; do
    declare matched=false
    for pattern in "$@"; do
        if [[ $file =~ $pattern ]]; then
          matched=true
        fi
    done
    if ! $matched; then
      return 0
    fi
  done
  return 1
}
start_pipeline() {
  echo "# $*" > "$output_file"
  echo "steps:" >> "$output_file"
}
command_step() {
  cat >> "$output_file" <<EOF
  - name: "$1"
    command: "$2"
    timeout_in_minutes: $3
    artifact_paths: "log-*.txt"
    agents:
      queue: "${4:-solana}"
EOF
}
trigger_secondary_step() {
  cat  >> "$output_file" <<"EOF"
  - name: "Trigger Build on agave-secondary"
    trigger: "agave-secondary"
    branches: "!pull/*"
    async: true
    soft_fail: true
    build:
      message: "${BUILDKITE_MESSAGE}"
      commit: "${BUILDKITE_COMMIT}"
      branch: "${BUILDKITE_BRANCH}"
      env:
        TRIGGERED_BUILDKITE_TAG: "${BUILDKITE_TAG}"
EOF
}
wait_step() {
  echo "  - wait" >> "$output_file"
}
all_test_steps() {
  command_step checks1 "ci/docker-run-default-image.sh ci/test-checks.sh" 20 check
  command_step dcou-1-of-3 "ci/docker-run-default-image.sh ci/test-dev-context-only-utils.sh --partition 1/3" 20 check
  command_step dcou-2-of-3 "ci/docker-run-default-image.sh ci/test-dev-context-only-utils.sh --partition 2/3" 20 check
  command_step dcou-3-of-3 "ci/docker-run-default-image.sh ci/test-dev-context-only-utils.sh --partition 3/3" 20 check
  command_step miri "ci/docker-run-default-image.sh ci/test-miri.sh" 5 check
  command_step frozen-abi "ci/docker-run-default-image.sh ci/test-frozen-abi.sh" 30 check
  wait_step
  .buildkite/scripts/build-stable.sh >> "$output_file"
  if affects \
             .rs$ \
             Cargo.lock$ \
             Cargo.toml$ \
             ^ci/rust-version.sh \
             ^ci/test-docs.sh \
      ; then
    command_step doctest "ci/docker-run-default-image.sh ci/test-docs.sh" 15
  else
    annotate --style info --context test-docs \
      "Docs skipped as no .rs files were modified"
  fi
  wait_step
  if affects \
             .rs$ \
             Cargo.lock$ \
             Cargo.toml$ \
             ^ci/rust-version.sh \
             ^ci/test-stable-sbf.sh \
             ^ci/test-stable.sh \
             ^ci/test-local-cluster.sh \
             ^core/build.rs \
             ^fetch-perf-libs.sh \
             ^platform-tools-sdk/ \
             ^programs/ \
             cargo-build-sbf$ \
             cargo-test-sbf$ \
      ; then
    cat >> "$output_file" <<"EOF"
  - command: "ci/docker-run-default-image.sh ci/test-stable-sbf.sh"
    name: "stable-sbf"
    timeout_in_minutes: 35
    agents:
      queue: "solana"
EOF
  else
    annotate --style info \
      "Stable-SBF skipped as no relevant files were modified"
  fi
  if affects \
             .rs$ \
             Cargo.lock$ \
             Cargo.toml$ \
             ^ci/rust-version.sh \
      ; then
    command_step shuttle "ci/docker-run-default-image.sh ci/test-shuttle.sh" 10
  else
    annotate --style info \
      "test-shuttle skipped as no relevant files were modified"
  fi
  if affects \
             .rs$ \
             Cargo.lock$ \
             Cargo.toml$ \
             ^ci/rust-version.sh \
             ^ci/test-coverage.sh \
             ^scripts/coverage.sh \
      ; then
    cat >> "$output_file" <<"EOF"
  - group: "coverage"
    steps:
      - command: "ci/docker-run-default-image.sh ci/coverage/part-1.sh"
        name: "coverage-1"
        timeout_in_minutes: 60
        agents:
          queue: "solana"
      - command: "ci/docker-run-default-image.sh ci/coverage/part-2.sh"
        name: "coverage-2"
        timeout_in_minutes: 60
        agents:
          queue: "solana"
      - command: "ci/docker-run-default-image.sh ci/coverage/part-3.sh"
        name: "coverage-3"
        timeout_in_minutes: 60
        agents:
          queue: "solana"
EOF
  else
    annotate --style info --context test-coverage \
      "Coverage skipped as no .rs files were modified"
  fi
}
pull_or_push_steps() {
  command_step sanity "ci/test-sanity.sh" 5 check
  wait_step
  if affects \
              .sh$ \
              ^.buildkite/hooks \
      ; then
    command_step shellcheck "ci/shellcheck.sh" 5 check
    wait_step
  fi
  if affects .toml$ && affects .lock$ && ! affects_other_than .toml$ .lock$; then
    optional_old_version_number=$(git diff origin/"$BUILDKITE_PULL_REQUEST_BASE_BRANCH"..HEAD validator/Cargo.toml | \
      grep -e "^-version" | sed  's/-version = "\(.*\)"/\1/')
    echo "optional_old_version_number: ->$optional_old_version_number<-"
    new_version_number=$(grep -e  "^version = " validator/Cargo.toml | sed 's/version = "\(.*\)"/\1/')
    echo "new_version_number: ->$new_version_number<-"
    diff_other_than_version_bump=$(git diff origin/"$BUILDKITE_PULL_REQUEST_BASE_BRANCH"..HEAD | \
      grep -vE "^ |^@@ |^--- |^\+\+\+ |^index |^diff |^-( \")?solana.*$optional_old_version_number|^\+( \")?solana.*$new_version_number|^-version|^\+version"|cat)
    echo "diff_other_than_version_bump: ->$diff_other_than_version_bump<-"
    if [ -z "$diff_other_than_version_bump" ]; then
      echo "Diff only contains version bump."
      command_step checks "ci/docker-run-default-image.sh ci/test-checks.sh" 20
      exit 0
    fi
  fi
  if affects_other_than ^.mergify .md$ ^docs/ ^.gitbook; then
    all_test_steps
  fi
}
if [[ -n $BUILDKITE_TAG ]]; then
  start_pipeline "Tag pipeline for $BUILDKITE_TAG"
  annotate --style info --context release-tag \
    "https://github.com/jito-foundation/jito-solana/releases/$BUILDKITE_TAG"
  trigger_secondary_step
  exit 0
fi
if [[ $BUILDKITE_BRANCH =~ ^pull ]]; then
  echo "+++ Affected files in this PR"
  for file in "${affected_files[@]}"; do
    echo "- $file"
  done
  start_pipeline "Pull request pipeline for $BUILDKITE_BRANCH"
  annotate --style info --context pr-backlink \
    "Github Pull Request: https://github.com/jito-foundation/jito-solana/$BUILDKITE_BRANCH"
  pull_or_push_steps
  exit 0
fi
start_pipeline "Push pipeline for ${BUILDKITE_BRANCH:-?unknown branch?}"
pull_or_push_steps
wait_step
trigger_secondary_step
exit 0

================
File: ci/buildkite-secondary.yml
================
steps:
  - name: "cargo audit"
    command: "ci/docker-run-default-image.sh ci/do-audit.sh"
    agents:
      queue: "release-build"
    timeout_in_minutes: 10
  - wait
  - name: "publish tarball (x86_64-unknown-linux-gnu)"
    command: "ci/publish-tarball.sh"
    agents:
      queue: "release-build"
    timeout_in_minutes: 60
  - name: "publish installer"
    command: "ci/publish-installer.sh"
    agents:
      queue: "release-build"
    timeout_in_minutes: 5

================
File: ci/buildkite-solana-private.sh
================
set -e
cd "$(dirname "$0")"/..
output_file=${1:-/dev/stderr}
if [[ -n $CI_PULL_REQUEST ]]; then
  # filter pr number from ci branch.
  [[ $CI_BRANCH =~ pull/([0-9]+)/head ]]
  pr_number=${BASH_REMATCH[1]}
  echo "get affected files from PR: $pr_number"
  readarray -t affected_files < <(gh pr diff --name-only "$pr_number")
  if [[ ${
    echo "Unable to determine the files affected by this PR"
    exit 1
  fi
else
  affected_files=()
fi
annotate() {
  if [[ -n $BUILDKITE ]]; then
    buildkite-agent annotate "$@"
  fi
}
affects() {
  if [[ -z $CI_PULL_REQUEST ]]; then
    return 0
  fi
  for pattern in ^ci/docker-rust/Dockerfile ^ci/docker-rust-nightly/Dockerfile "$@"; do
    if [[ ${pattern:0:1} = "!" ]]; then
      for file in "${affected_files[@]}"; do
        if [[ ! $file =~ ${pattern:1} ]]; then
          return 0
        fi
      done
    else
      for file in "${affected_files[@]}"; do
        if [[ $file =~ $pattern ]]; then
          return 0
        fi
      done
    fi
  done
  return 1
}
affects_other_than() {
  if [[ -z $CI_PULL_REQUEST ]]; then
    return 0
  fi
  for file in "${affected_files[@]}"; do
    declare matched=false
    for pattern in "$@"; do
        if [[ $file =~ $pattern ]]; then
          matched=true
        fi
    done
    if ! $matched; then
      return 0
    fi
  done
  return 1
}
start_pipeline() {
  echo "# $*" > "$output_file"
  echo "steps:" >> "$output_file"
}
command_step() {
  cat >> "$output_file" <<EOF
  - name: "$1"
    command: "$2"
    timeout_in_minutes: $3
    artifact_paths: "log-*.txt"
    agents:
      queue: "default"
EOF
}
wait_step() {
  echo "  - wait" >> "$output_file"
}
all_test_steps() {
  command_step checks "ci/docker-run-default-image.sh ci/test-checks.sh" 20
  wait_step
  .buildkite/scripts/build-stable.sh default >> "$output_file"
  if affects \
             .rs$ \
             ^ci/rust-version.sh \
             ^ci/test-docs.sh \
      ; then
    command_step doctest "ci/docker-run-default-image.sh ci/test-docs.sh" 15
  else
    annotate --style info --context test-docs \
      "Docs skipped as no .rs files were modified"
  fi
  wait_step
  if affects \
             .rs$ \
             Cargo.lock$ \
             Cargo.toml$ \
             ^ci/rust-version.sh \
             ^ci/test-stable-sbf.sh \
             ^ci/test-stable.sh \
             ^ci/test-local-cluster.sh \
             ^core/build.rs \
             ^fetch-perf-libs.sh \
             ^platform-tools-sdk/ \
             ^programs/ \
      ; then
    cat >> "$output_file" <<"EOF"
  - command: "ci/docker-run-default-image.sh ci/test-stable-sbf.sh"
    name: "stable-sbf"
    timeout_in_minutes: 35
    agents:
      queue: "default"
EOF
  else
    annotate --style info \
      "Stable-SBF skipped as no relevant files were modified"
  fi
  if affects \
             .rs$ \
             Cargo.lock$ \
             Cargo.toml$ \
             ^ci/rust-version.sh \
             ^ci/test-coverage.sh \
             ^scripts/coverage.sh \
      ; then
    command_step coverage "ci/docker-run-default-image.sh ci/test-coverage.sh" 80
  else
    annotate --style info --context test-coverage \
      "Coverage skipped as no .rs files were modified"
  fi
}
pull_or_push_steps() {
  command_step sanity "ci/test-sanity.sh" 5
  wait_step
  if affects .sh$; then
    command_step shellcheck "ci/shellcheck.sh" 5
    wait_step
  fi
  if affects_other_than ^.mergify .md$ ^docs/ ^.gitbook; then
    all_test_steps
  fi
}
if [[ $BUILDKITE_BRANCH =~ ^pull ]]; then
  echo "+++ Affected files in this PR"
  for file in "${affected_files[@]}"; do
    echo "- $file"
  done
  start_pipeline "Pull request pipeline for $BUILDKITE_BRANCH"
  annotate --style info --context pr-backlink \
    "Github Pull Request: https://github.com/jito-foundation/jito-solana/$BUILDKITE_BRANCH"
  pull_or_push_steps
  exit 0
fi
start_pipeline "Push pipeline for ${BUILDKITE_BRANCH:-?unknown branch?}"
pull_or_push_steps
wait_step
exit 0

================
File: ci/channel_restriction.sh
================
set -ex
[[ -n $CI_TAG ]] && exit 0
eval "$(ci/channel-info.sh)"
for acceptable_channel in "$@"; do
  if [[ "$CHANNEL" == "$acceptable_channel" ]]; then
    exit 0
  fi
done
echo "Not running from one of the following channels: $*"
exit 1

================
File: ci/channel-info.sh
================
here="$(dirname "$0")"
# shellcheck source=ci/semver_bash/semver.sh
source "$here"/semver_bash/semver.sh
remote=https://github.com/jito-foundation/jito-solana.git
tags=( \
  $(git ls-remote --tags $remote \
    | cut -c52- \
    | grep '^v[[:digit:]][[:digit:]]*\.[[:digit:]][[:digit:]]*.[[:digit:]][[:digit:]]*$' \
    | cut -c2- \
  ) \
)
heads=( \
  $(git ls-remote --heads $remote \
    | cut -c53- \
    | grep '^v[[:digit:]][[:digit:]]*\.[[:digit:]][[:digit:]]*$' \
    | cut -c2- \
  ) \
)
beta=
for head in "${heads[@]}"; do
  if [[ -n $beta ]]; then
    if semverLT "$head.0" "$beta.0"; then
      continue
    fi
  fi
  beta=$head
done
stable=
for head in "${heads[@]}"; do
  if [[ $head = "$beta" ]]; then
    continue
  fi
  if [[ -n $stable ]]; then
    if semverLT "$head.0" "$stable.0"; then
      continue
    fi
  fi
  stable=$head
done
for tag in "${tags[@]}"; do
  if [[ -n $beta && $tag = $beta* ]]; then
    if [[ -n $beta_tag ]]; then
      if semverLT "$tag" "$beta_tag"; then
        continue
      fi
    fi
    beta_tag=$tag
  fi
  if [[ -n $stable && $tag = $stable* ]]; then
    if [[ -n $stable_tag ]]; then
      if semverLT "$tag" "$stable_tag"; then
        continue
      fi
    fi
    stable_tag=$tag
  fi
done
EDGE_CHANNEL=master
BETA_CHANNEL=${beta:+v$beta}
STABLE_CHANNEL=${stable:+v$stable}
BETA_CHANNEL_LATEST_TAG=${beta_tag:+v$beta_tag}
STABLE_CHANNEL_LATEST_TAG=${stable_tag:+v$stable_tag}
if [[ -n $CI_BASE_BRANCH ]]; then
  BRANCH="$CI_BASE_BRANCH"
elif [[ -n $CI_BRANCH ]]; then
  BRANCH="$CI_BRANCH"
fi
if [[ -z "$CHANNEL" ]]; then
  if [[ $BRANCH = "$STABLE_CHANNEL" ]]; then
    CHANNEL=stable
  elif [[ $BRANCH = "$EDGE_CHANNEL" ]]; then
    CHANNEL=edge
  elif [[ $BRANCH = "$BETA_CHANNEL" ]]; then
    CHANNEL=beta
  fi
fi
if [[ $CHANNEL = beta ]]; then
  CHANNEL_LATEST_TAG="$BETA_CHANNEL_LATEST_TAG"
elif [[ $CHANNEL = stable ]]; then
  CHANNEL_LATEST_TAG="$STABLE_CHANNEL_LATEST_TAG"
fi
echo EDGE_CHANNEL="$EDGE_CHANNEL"
echo BETA_CHANNEL="$BETA_CHANNEL"
echo BETA_CHANNEL_LATEST_TAG="$BETA_CHANNEL_LATEST_TAG"
echo STABLE_CHANNEL="$STABLE_CHANNEL"
echo STABLE_CHANNEL_LATEST_TAG="$STABLE_CHANNEL_LATEST_TAG"
echo CHANNEL="$CHANNEL"
echo CHANNEL_LATEST_TAG="$CHANNEL_LATEST_TAG"
exit 0

================
File: ci/check-channel-version.sh
================
if [[ -z $CI_TAG ]]; then
  echo "--- channel version check"
  (
    eval "$(ci/channel-info.sh)"
    if [[ -n $CHANNEL_LATEST_TAG ]]; then
      source scripts/read-cargo-variable.sh
      version=$(readCargoVariable version Cargo.toml)
      echo "latest channel tag: $CHANNEL_LATEST_TAG"
      echo "current version: v$version"
      if [[ $CHANNEL_LATEST_TAG = v$version ]]; then
        echo -e "\033[31mError:\033[0m A release has been tagged since your feature branch was last rebased. <current version> must be greater than <latest channel tag>..
        Possible solutions (in the order they should be tried):
        1. rebase your feature branch on the base branch
        2. merge the PR: \"Bump Version to ...\" once it has passed ci/checks, then rebase
        3. ask for help in #devops on discord"
        exit 1
      fi
    else
      echo "Skipped. CHANNEL_LATEST_TAG (CHANNEL=$CHANNEL) unset"
    fi
  )
fi

================
File: ci/check-crates.sh
================
if [[ -z $COMMIT_RANGE ]]; then
  echo "COMMIT_RANGE should be provided"
  exit 1
fi
if ! command -v toml &>/dev/null; then
  echo "not found toml-cli"
  cargo install toml-cli
fi
declare skip_patterns=(
  "Cargo.toml"
  "programs/sbf"
)
declare -A verified_crate_owners=(
  ["anza-team"]=1
)
readarray -t files <<<"$(git diff "$COMMIT_RANGE" --diff-filter=ACMR --name-only | grep Cargo.toml)"
printf "%s\n" "${files[@]}"
error_count=0
for file in "${files[@]}"; do
  read -r crate_name package_publish workspace < <(toml get "$file" . | jq -r '(.package.name | tostring)+" "+(.package.publish | tostring)+" "+(.workspace | tostring)')
  if [ "$crate_name" == "solana-bundle" ]; then
    continue
  fi
  echo "=== $crate_name ($file) ==="
  if [[ $package_publish = 'false' ]]; then
    echo -e "⏩ skip (package_publish: $package_publish)\n"
    continue
  fi
  if [[ "$workspace" != "null" ]]; then
    echo -e "⏩ skip (is a workspace root)\n"
    continue
  fi
  for skip_pattern in "${skip_patterns[@]}"; do
    if [[ $file =~ ^$skip_pattern ]]; then
      echo -e "⏩ skip (match skip patterns)\n"
      continue 2
    fi
  done
  IFS=$'\t' read -r lic licf desc home repo < <(toml get "$file" . | jq -r "
    (.package.license | tojson)\
    +\"\t\"+(.package.license_file | tojson)\
    +\"\t\"+(.package.description | tojson)\
    +\"\t\"+(.package.homepage | tojson)\
    +\"\t\"+(.package.repository | tojson)\
  ")
  declare missing_metadata=()
  if [ "$lic" = "null" ] && [ "$licf" = "null" ]; then
    missing_metadata+=( "license" )
  else
    echo "✅ license"
  fi
  if [ "$desc" = "null" ]; then
    missing_metadata+=( "description" )
  else
    echo "✅ description"
  fi
  if [ "$home" = "null" ]; then
    missing_metadata+=( "homepage" )
  else
    echo "✅ homepage"
  fi
  if [ "$repo" = "null" ]; then
    missing_metadata+=( "repository" )
  else
    echo "✅ repository"
  fi
  if [ ${
    echo "❌ $crate_name is missing the following metadata fields: ${missing_metadata[*]}"
    exit 1
  fi
  response=$(curl -s https://crates.io/api/v1/crates/"$crate_name"/owners)
  errors=$(echo "$response" | jq .errors)
  if [[ $errors != "null" ]]; then
    details=$(echo "$response" | jq .errors | jq -r ".[0].detail")
    if [[ $details = *"does not exist"* ]]; then
      ((error_count++))
      echo "❌ new crate $crate_name not found on crates.io. you can either
1. mark it as not for publication in its Cargo.toml
    [package]
    ...
    publish = false
or
2. make a dummy publication.
  example:
  scripts/reserve-cratesio-package-name.sh \
    --token <GRIMES_CRATESIO_TOKEN> \
    lib solana-new-lib-crate
  see also: scripts/reserve-cratesio-package-name.sh --help
"
    else
      ((error_count++))
      echo "❌ $response"
    fi
  else
    readarray -t owners <<<"$(echo "$response" | jq .users | jq -r ".[] | .login")"
    verified_owner_count=0
    unverified_owner_count=0
    for owner in "${owners[@]}"; do
      if [[ -z $owner ]]; then
        continue
      fi
      owner_id="$(echo "$owner" | awk '{print $1}')"
      if [[ ${verified_crate_owners[$owner_id]} ]]; then
        ((verified_owner_count++))
        echo "✅ $owner"
      else
        ((unverified_owner_count++))
        echo "❌ $owner"
      fi
    done
    if [[ ($unverified_owner_count -gt 0) ]]; then
      ((error_count++))
      echo "error: found unverified owner(s)"
    elif [[ ($verified_owner_count -le 0) ]]; then
      ((error_count++))
      echo "error: there are no verified owners"
    fi
  fi
  echo ""
done
if [ "$error_count" -eq 0 ]; then
  echo "success"
  exit 0
else
  exit 1
fi

================
File: ci/check-install-all.sh
================
source scripts/spl-token-cli-version.sh
if [[ -z $splTokenCliVersion ]]; then
    echo "On the stable channel, splTokenCliVersion must be set in scripts/spl-token-cli-version.sh"
    exit 1
fi

================
File: ci/crate-version.sh
================
set -e
Cargo_toml=$1
[[ -n $Cargo_toml ]] || {
  echo "Usage: $0 path/to/Cargo.toml"
  exit 0
}
[[ -r $Cargo_toml ]] || {
  echo "Error: unable to read $Cargo_toml"
  exit 1
}
while read -r name equals value _; do
  if [[ $name = version && $equals = = ]]; then
    echo "${value//\"/}"
    exit 0
  fi
done < <(cat "$Cargo_toml")
echo Unable to locate version in Cargo.toml 1>&2
exit 1

================
File: ci/do-audit.sh
================
set -e
here="$(dirname "$0")"
src_root="$(readlink -f "${here}/..")"
cd "${src_root}"
# `cargo-audit` doesn't give us a way to do this nicely, so hammer it is...
dep_tree_filter="grep -Ev '│|└|├|─'"
while [[ -n $1 ]]; do
  if [[ $1 = "--display-dependency-trees" ]]; then
    dep_tree_filter="cat"
    shift
  fi
done
cargo_audit_ignores=(
  --ignore RUSTSEC-2022-0093
  --ignore RUSTSEC-2024-0421
  --ignore RUSTSEC-2024-0344
  --ignore RUSTSEC-2024-0376
  --ignore RUSTSEC-2025-0009
)
scripts/cargo-for-all-lock-files.sh audit "${cargo_audit_ignores[@]}" | $dep_tree_filter
exit "${PIPESTATUS[0]}"

================
File: ci/docker-run-default-image.sh
================
set -e
here="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$here/docker/env.sh"
exec "$here/docker-run.sh" "${CI_DOCKER_IMAGE:?}" "$@"

================
File: ci/docker-run.sh
================
set -e
usage() {
  echo "Usage: $0 [--nopull] [docker image name] [command]"
  echo
  echo Runs command in the specified docker image with
  echo a CI-appropriate environment.
  echo
  echo "--nopull   Skip the dockerhub image update"
  echo "--shell    Skip command and enter an interactive shell"
  echo
}
cd "$(dirname "$0")/.."
INTERACTIVE=false
if [[ $1 = --shell ]]; then
  INTERACTIVE=true
  shift
fi
NOPULL=false
if [[ $1 = --nopull ]]; then
  NOPULL=true
  shift
fi
IMAGE="$1"
if [[ -z "$IMAGE" ]]; then
  echo Error: image not defined
  exit 1
fi
$NOPULL || docker pull "$IMAGE"
shift
ARGS=(
  --workdir /solana
  --volume "$PWD:/solana"
  --rm
)
if [[ -n $CI ]]; then
  if [[ -n $BUILDKITE ]]; then
    if [[ $BUILDKITE_RETRY_COUNT -ge 2 ]]; then
      echo "--- $0 ... (with sccache being DISABLED due to many (${BUILDKITE_RETRY_COUNT}) retries)"
    else
      echo "--- $0 ... (with sccache enabled with prefix: $SCCACHE_KEY_PREFIX)"
      ARGS+=(
        --env "RUSTC_WRAPPER=/usr/local/cargo/bin/sccache"
      )
      mkdir -p "$HOME/.cache/sccache-for-docker"
      CONTAINER_HOME="/"
      ARGS+=(
        --volume "$HOME/.cache/sccache-for-docker:$CONTAINER_HOME/.cache/sccache"
      )
      if [ -n "$AWS_ACCESS_KEY_ID" ]; then
        ARGS+=(
          --env AWS_ACCESS_KEY_ID
          --env AWS_SECRET_ACCESS_KEY
          --env SCCACHE_BUCKET
          --env SCCACHE_REGION
          --env SCCACHE_S3_KEY_PREFIX
        )
      fi
      if [ -n "$SCCACHE_GCS_KEY_PATH" ]; then
        ARGS+=(
          --env SCCACHE_GCS_KEY_PATH
          --volume "$SCCACHE_GCS_KEY_PATH:$SCCACHE_GCS_KEY_PATH"
          --env SCCACHE_GCS_BUCKET
          --env SCCACHE_GCS_RW_MODE
          --env SCCACHE_GCS_KEY_PREFIX
        )
      fi
    fi
    ARGS+=(--security-opt seccomp=unconfined)
    ARGS+=(--ulimit memlock=-1:-1)
  fi
fi
if [[ -z "$SOLANA_DOCKER_RUN_NOSETUID" ]]; then
  ARGS+=(--user "$(id -u):$(id -g)")
fi
if [[ -n $SOLANA_ALLOCATE_TTY ]]; then
  ARGS+=(--interactive --tty)
fi
ARGS+=(
  --env BUILDKITE
  --env BUILDKITE_AGENT_ACCESS_TOKEN
  --env BUILDKITE_JOB_ID
  --env BUILDKITE_PARALLEL_JOB
  --env BUILDKITE_PARALLEL_JOB_COUNT
  --env CI
  --env CI_BRANCH
  --env CI_BASE_BRANCH
  --env CI_TAG
  --env CI_BUILD_ID
  --env CI_COMMIT
  --env CI_JOB_ID
  --env CI_PULL_REQUEST
  --env CI_REPO_SLUG
  --env CRATES_IO_TOKEN
)
CODECOV_ENVS=$(CI=true bash <(while ! curl -sS --retry 5 --retry-delay 2 --retry-connrefused --fail https://codecov.io/env; do sleep 10; done))
if $INTERACTIVE; then
  if [[ -n $1 ]]; then
    echo
    echo "Note: '$*' ignored due to --shell argument"
    echo
  fi
  set -x
  exec docker run --interactive --tty "${ARGS[@]}" $CODECOV_ENVS "$IMAGE" bash
fi
set -x
exec docker run "${ARGS[@]}" $CODECOV_ENVS -t "$IMAGE" "$@"

================
File: ci/env.sh
================
if [[ -n $CI ]]; then
  export CI=1
  if [[ -n $BUILDKITE ]]; then
    export CI_BRANCH=$BUILDKITE_BRANCH
    export CI_BUILD_ID=$BUILDKITE_BUILD_ID
    if [[ $BUILDKITE_COMMIT = HEAD ]]; then
      BUILDKITE_COMMIT="$(git rev-parse HEAD)"
    fi
    export CI_COMMIT=$BUILDKITE_COMMIT
    export CI_JOB_ID=$BUILDKITE_JOB_ID
    if [[ $CI_BRANCH =~ pull/* ]]; then
      export CI_BASE_BRANCH=$BUILDKITE_PULL_REQUEST_BASE_BRANCH
      export CI_PULL_REQUEST=true
    else
      export CI_BASE_BRANCH=$BUILDKITE_BRANCH
      export CI_PULL_REQUEST=
    fi
    if [[ -n $BUILDKITE_TRIGGERED_FROM_BUILD_PIPELINE_SLUG ]]; then
      export CI_REPO_SLUG=$BUILDKITE_ORGANIZATION_SLUG/$BUILDKITE_TRIGGERED_FROM_BUILD_PIPELINE_SLUG
    else
      export CI_REPO_SLUG=$BUILDKITE_ORGANIZATION_SLUG/$BUILDKITE_PIPELINE_SLUG
    fi
    if [[ -n $TRIGGERED_BUILDKITE_TAG ]]; then
      export CI_TAG=$TRIGGERED_BUILDKITE_TAG
    else
      export CI_TAG=$BUILDKITE_TAG
    fi
  elif [[ $GITHUB_ACTION ]]; then
    export CI_BUILD_ID=$GITHUB_RUN_ID
    export CI_JOB_ID=$GITHUB_RUN_NUMBER
    export CI_REPO_SLUG=$GITHUB_REPOSITORY
    export CI_BRANCH=$GITHUB_REF_NAME
    CI_COMMIT=$(git rev-parse HEAD)
    export CI_COMMIT
    CI_TAG=$(git tag --points-at HEAD)
    export CI_TAG
    if [[ $GITHUB_BASE_REF ]]; then
      export CI_BASE_BRANCH=$GITHUB_BASE_REF
      export CI_PULL_REQUEST=true
    fi
  fi
else
  export CI=
  export CI_BRANCH=
  export CI_BASE_BRANCH=
  export CI_BUILD_ID=
  export CI_COMMIT=
  export CI_JOB_ID=
  export CI_PULL_REQUEST=
  export CI_REPO_SLUG=
  export CI_TAG=
fi
cat <<EOF
CI=$CI
CI_BRANCH=$CI_BRANCH
CI_BASE_BRANCH=$CI_BASE_BRANCH
CI_BUILD_ID=$CI_BUILD_ID
CI_COMMIT=$CI_COMMIT
CI_JOB_ID=$CI_JOB_ID
CI_PULL_REQUEST=$CI_PULL_REQUEST
CI_REPO_SLUG=$CI_REPO_SLUG
CI_TAG=$CI_TAG
EOF

================
File: ci/format-url.sh
================
if [[ $
  echo "Usage: $0 url"
  exit 1
fi
if [[ -z $BUILDKITE ]]; then
  echo "$1"
else
  URL="$(echo "$1" | sed 's/;/%3b/g')" # Escape ;
  printf '\033]1339;url='
  echo -n "$URL"
  printf '\a\n'
fi

================
File: ci/hoover.sh
================
echo --- Delete all exited containers first
(
  set -x
  exited=$(docker ps -aq --no-trunc --filter "status=exited")
  if [[ -n "$exited" ]]; then
    docker rm $exited
  fi
)
echo --- Delete untagged images
(
  set -x
  untagged=$(docker images | grep '<none>'| awk '{ print $3 }')
  if [[ -n "$untagged" ]]; then
    docker rmi $untagged
  fi
)
echo --- Delete all dangling images
(
  set -x
  dangling=$(docker images --filter 'dangling=true' -q --no-trunc | sort | uniq)
  if [[ -n "$dangling" ]]; then
    docker rmi $dangling
  fi
)
echo --- Remove unused docker networks
(
  set -x
  docker network prune -f
)
echo "--- Delete /tmp files older than 1 day owned by $(id -un)"
(
  set -x
  find /tmp -maxdepth 1 -user "$(id -un)" -mtime +1 -print0 | xargs -0 rm -rf
)
echo --- Deleting stale buildkite agent build directories
if [[ ! -d ../../../../builds/$BUILDKITE_AGENT_NAME ]]; then
  echo Warning: Skipping flush of stale agent build directories
  echo "  PWD=$PWD"
else
  (
    for keepDir in "$BUILDKITE_PIPELINE_SLUG" \
                   "$BUILDKITE_ORGANIZATION_SLUG" \
                   "$BUILDKITE_AGENT_NAME"; do
      cd .. || exit 1
      for dir in *; do
        if [[ -d $dir && $dir != "$keepDir" ]]; then
          echo "Removing $dir"
          rm -rf "${dir:?}"/
        fi
      done
    done
  )
fi
echo --- System Status
(
  set -x
  docker images
  docker ps
  docker network ls
  df -h
)
exit 0

================
File: ci/intercept.sh
================
set -e
if [[ (-n $CI || -n $FORCE_INTERCEPT) && -z $NO_INTERCEPT ]]; then
  : "${INTERCEPT_OUTPUT:="./intercepted-console-$(date '+%Yy%mm%dd%Hh%Mm%Ss%Nns').log"}"
  echo "$0: Intercepting stderr into $INTERCEPT_OUTPUT, along side tee-d stdout."
  # we don't care about being racy here as was before; so disable shellcheck
  # shellcheck disable=SC2094
  if "$@" 2>> "$INTERCEPT_OUTPUT" 1>> >(tee -a "$INTERCEPT_OUTPUT"); then
    exit_code=0
  else
    exit_code=$?
    echo "$0: command failed; please see $INTERCEPT_OUTPUT in artifacts"
  fi
  exit "$exit_code"
else
  "$@"
fi

================
File: ci/localnet-sanity.sh
================
set -e
skipSetup=false
iterations=1
restartInterval=never
rollingRestart=false
extraNodes=0
walletRpcPort=:8899
usage() {
  exitcode=0
  if [[ -n "$1" ]]; then
    exitcode=1
    echo "Error: $*"
  fi
  cat <<EOF
usage: $0 [options...]
Start a local cluster and run sanity on it
  options:
   -i [number] - Number of times to run sanity (default: $iterations)
   -k [number] - Restart the cluster after this number of sanity iterations (default: $restartInterval)
   -R          - Restart the cluster by incrementially stopping and restarting
                 nodes (at the cadence specified by -k).  When disabled all
                 nodes will be first killed then restarted (default: $rollingRestart)
   -b          - Disable leader rotation
   -x          - Add an extra validator (may be supplied multiple times)
   -r          - Select the RPC endpoint hosted by a node that starts as
                 a validator node.  If unspecified the RPC endpoint hosted by
                 the bootstrap validator will be used.
   -c          - Reuse existing node/ledger configuration from a previous sanity
                 run
EOF
  exit $exitcode
}
cd "$(dirname "$0")"/..
while getopts "ch?i:k:brxR" opt; do
  case $opt in
  h | \?)
    usage
    ;;
  c)
    skipSetup=true
    ;;
  i)
    iterations=$OPTARG
    ;;
  k)
    restartInterval=$OPTARG
    ;;
  x)
    extraNodes=$((extraNodes + 1))
    ;;
  r)
    walletRpcPort=":18899"
    ;;
  R)
    rollingRestart=true
    ;;
  *)
    usage "Error: unhandled option: $opt"
    ;;
  esac
done
source ci/upload-ci-artifact.sh
source scripts/configure-metrics.sh
source multinode-demo/common.sh --prebuild
nodes=(
  "multinode-demo/bootstrap-validator.sh \
    --no-restart \
    --init-complete-file init-complete-node0.log \
    --dynamic-port-range 8000-8200"
  "multinode-demo/validator.sh \
    --no-restart \
    --dynamic-port-range 8200-8400
    --init-complete-file init-complete-node1.log \
    --rpc-port 18899"
)
if [[ extraNodes -gt 0 ]]; then
  for i in $(seq 1 $extraNodes); do
    portStart=$((8400 + i * 200))
    portEnd=$((portStart + 200))
    nodes+=(
      "multinode-demo/validator.sh \
        --no-restart \
        --dynamic-port-range $portStart-$portEnd
        --label dyn$i \
        --init-complete-file init-complete-node$((1 + i)).log"
    )
  done
fi
numNodes=$((2 + extraNodes))
pids=()
logs=()
getNodeLogFile() {
  declare nodeIndex=$1
  declare cmd=$2
  declare baseCmd
  baseCmd=$(basename "${cmd// */}" .sh)
  echo "log-$baseCmd-$nodeIndex.txt"
}
startNode() {
  declare nodeIndex=$1
  declare cmd=$2
  echo "--- Start $cmd"
  declare log
  log=$(getNodeLogFile "$nodeIndex" "$cmd")
  rm -f "$log"
  $cmd > "$log" 2>&1 &
  declare pid=$!
  pids+=("$pid")
  echo "pid: $pid"
  echo "log: $log"
}
waitForNodeToInit() {
  declare initCompleteFile=$1
  while [[ ! -r $initCompleteFile ]]; do
    if [[ $SECONDS -ge 300 ]]; then
      echo "^^^ +++"
      echo "Error: $initCompleteFile not found in $SECONDS seconds"
      exit 1
    fi
    echo "Waiting for $initCompleteFile ($SECONDS)..."
    sleep 2
  done
  echo "Found $initCompleteFile"
}
initCompleteFiles=()
waitForAllNodesToInit() {
  echo "--- ${#initCompleteFiles[@]} nodes booting"
  SECONDS=
  for initCompleteFile in "${initCompleteFiles[@]}"; do
    waitForNodeToInit "$initCompleteFile"
  done
  echo "All nodes finished booting in $SECONDS seconds"
}
startNodes() {
  declare addLogs=false
  if [[ ${
    addLogs=true
  fi
  initCompleteFiles=()
  maybeExpectedGenesisHash=
  for i in $(seq 0 $((${
    if [[ "$i" -eq 1 ]]; then
      SECONDS=
      waitForNodeToInit "$initCompleteFile"
      (
        set -x
        $solana_cli --keypair config/bootstrap-validator/identity.json \
          --url http://127.0.0.1:8899 genesis-hash
      ) | tee genesis-hash.log
      maybeExpectedGenesisHash="--expected-genesis-hash $(tail -n1 genesis-hash.log)"
    fi
    declare cmd=${nodes[$i]}
    declare initCompleteFile="init-complete-node$i.log"
    rm -f "$initCompleteFile"
    initCompleteFiles+=("$initCompleteFile")
    startNode "$i" "$cmd $maybeExpectedGenesisHash"
    if $addLogs; then
      logs+=("$(getNodeLogFile "$i" "$cmd")")
    fi
  done
  waitForAllNodesToInit
}
killNode() {
  declare pid=$1
  set +e
  if kill "$pid"; then
    echo "Waiting for $pid to exit..."
    wait "$pid"
    echo "$pid exited with $?"
  fi
  set -e
}
killNodes() {
  [[ ${
  echo "--- RPC exit"
  $agave_validator --ledger "$SOLANA_CONFIG_DIR"/bootstrap-validator exit --force || true
  $agave_validator --ledger "$SOLANA_CONFIG_DIR"/validator exit --force || true
  sleep 2
  echo "--- Killing nodes: ${pids[*]}"
  for pid in "${pids[@]}"; do
    killNode "$pid"
  done
  echo "done killing nodes"
  pids=()
}
rollingNodeRestart() {
  if [[ ${
    echo "^^^ +++"
    echo "Error: log/nodes array length mismatch"
    exit 1
  fi
  if [[ ${
    echo "^^^ +++"
    echo "Error: pids/nodes array length mismatch"
    exit 1
  fi
  declare oldPids=("${pids[@]}")
  for i in $(seq 0 $((${
    declare pid=${oldPids[$i]}
    declare cmd=${nodes[$i]}
    if [[ $i -eq 0 ]]; then
      [[ "$cmd" = "multinode-demo/faucet.sh" ]]
      pids+=("$pid")
    else
      echo "--- Restarting $pid: $cmd"
      killNode "$pid"
      echo "(sleeping for 20 seconds)"
      sleep 20
      declare initCompleteFile="init-complete-node$i.log"
      rm -f "$initCompleteFile"
      initCompleteFiles+=("$initCompleteFile")
      startNode "$i" "$cmd"
    fi
  done
  declare oldPidsList
  oldPidsList="$(printf ":%s" "${oldPids[@]}"):"
  declare newPids=("${pids[0]}") # 0 = faucet pid
  for pid in "${pids[@]}"; do
    [[ $oldPidsList =~ :$pid: ]] || {
      newPids+=("$pid")
    }
  done
  pids=("${newPids[@]}")
  waitForAllNodesToInit
}
verifyLedger() {
  for ledger in bootstrap-validator validator; do
    echo "--- $ledger ledger verification"
    (
      set -x
      $solana_ledger_tool --ledger "$SOLANA_CONFIG_DIR"/$ledger verify
    ) || flag_error
  done
}
shutdown() {
  exitcode=$?
  killNodes
  set +e
  echo "--- Upload artifacts"
  for log in "${logs[@]}"; do
    upload-ci-artifact "$log"
    tail "$log"
  done
  exit $exitcode
}
trap shutdown EXIT INT
set -e
declare iteration=1
flag_error() {
  echo "Failed (iteration: $iteration/$iterations)"
  echo "^^^ +++"
  exit 1
}
if ! $skipSetup; then
  clear_config_dir "$SOLANA_CONFIG_DIR"
  multinode-demo/setup.sh --hashes-per-tick sleep
else
  verifyLedger
fi
startNodes
lastTransactionCount=
while [[ $iteration -le $iterations ]]; do
  echo "--- Node count ($iteration)"
  (
    set -x
    client_keypair=/tmp/client-id.json-$$
    $solana_keygen new --no-passphrase -fso $client_keypair || exit $?
    $solana_gossip --allow-private-addr spy -n 127.0.0.1:8001 --num-nodes-exactly $numNodes || exit $?
    rm -rf $client_keypair
  ) || flag_error
  echo "--- RPC API: bootstrap-validator getTransactionCount ($iteration)"
  (
    set -x
    curl --retry 5 --retry-delay 2 --retry-connrefused \
      -X POST -H 'Content-Type: application/json' \
      -d '{"jsonrpc":"2.0","id":1, "method":"getTransactionCount"}' \
      -o log-transactionCount.txt \
      http://localhost:8899
    cat log-transactionCount.txt
  ) || flag_error
  echo "--- RPC API: validator getTransactionCount ($iteration)"
  (
    set -x
    curl --retry 5 --retry-delay 2 --retry-connrefused \
      -X POST -H 'Content-Type: application/json' \
      -d '{"jsonrpc":"2.0","id":1, "method":"getTransactionCount"}' \
      http://localhost:18899
  ) || flag_error
  transactionCount=$(sed -e 's/{"jsonrpc":"2.0","result":\([0-9]*\),"id":1}/\1/' log-transactionCount.txt)
  if [[ -n $lastTransactionCount ]]; then
    echo "--- Transaction count check: $lastTransactionCount < $transactionCount"
    if [[ $lastTransactionCount -ge $transactionCount ]]; then
      echo "Error: Transaction count is not advancing"
      echo "* lastTransactionCount: $lastTransactionCount"
      echo "* transactionCount: $transactionCount"
      flag_error
    fi
  fi
  lastTransactionCount=$transactionCount
  echo "--- Wallet sanity ($iteration)"
  (
    set -x
    timeout 60s scripts/wallet-sanity.sh --url http://127.0.0.1"$walletRpcPort"
  ) || flag_error
  iteration=$((iteration + 1))
  if [[ $restartInterval != never && $((iteration % restartInterval)) -eq 0 ]]; then
    if $rollingRestart; then
      rollingNodeRestart
    else
      killNodes
      verifyLedger
      startNodes
    fi
  fi
done
killNodes
verifyLedger
echo +++
echo "Ok ($iterations iterations)"
exit 0

================
File: ci/nits.sh
================
set -e
cd "$(dirname "$0")/.."
source ci/_
# Logging hygiene: Please don't print from --lib, use the `log` crate instead
declare prints=(
  'print!'
  'println!'
  'eprint!'
  'eprintln!'
  'dbg!'
)
# Parts of the tree that are expected to be print free
declare print_free_tree=(
  ':core/src/**.rs'
  ':^core/src/validator.rs'
  ':faucet/src/**.rs'
  ':ledger/src/**.rs'
  ':metrics/src/**.rs'
  ':net-utils/src/**.rs'
  ':platform-tools-sdk/**.rs'
  ':platform-tools-sdk/sbf/rust/rust-utils/**.rs'
  ':^platform-tools-sdk/cargo-build-sbf/**.rs'
  ':runtime/src/**.rs'
  ':programs/**.rs'
  ':^**bin**.rs'
  ':^**bench**.rs'
  ':^**test**.rs'
  ':^**/build.rs'
)
if _ git --no-pager grep -n "${prints[@]/
    exit 1
fi
if _ git --no-pager grep -F '.hidden(true)' -- '*.rs'; then
    echo 'use ".hidden(hidden_unless_forced())" instead'
    exit 1
fi
declare useGithubIssueInsteadOf=(
  X\XX
  T\BD
  F\IXME
)
if _ git --no-pager grep -n --max-depth=0 "${useGithubIssueInsteadOf[@]/#/-e }" -- '*.rs' '*.sh' '*.md'; then
    exit 1
fi
_ git --no-pager grep -n --max-depth=0 "-e TODO" -- '*.rs' '*.sh' '*.md' || true
echo "^^^ +++"

================
File: ci/order-crates-for-publishing.py
================
real_file = os.path.realpath(__file__)
ci_path = os.path.dirname(real_file)
src_root = os.path.dirname(ci_path)
def load_metadata()
⋮----
cmd = f'{src_root}/cargo metadata --no-deps --format-version=1'
⋮----
def is_self_dev_dep_with_dev_context_only_utils(package, dependency, wrong_self_dev_dependencies)
⋮----
no_explicit_version = '*'
is_special_cased = False
⋮----
is_special_cased = True
⋮----
def is_path_dev_dep(dependency)
def should_add(package, dependency, wrong_self_dev_dependencies)
⋮----
related_to_solana = dependency['name'].startswith(('solana','agave'))
self_dev_dep_with_dev_context_only_utils = is_self_dev_dep_with_dev_context_only_utils(
⋮----
def get_packages()
⋮----
metadata = load_metadata()
manifest_path = dict()
dependency_graph = dict()
wrong_self_dev_dependencies = list()
⋮----
circular_dependencies = set()
⋮----
sorted_dependency_graph = []
max_iterations = pow(len(dependency_graph),2)
⋮----
deleted_packages = []
⋮----
dependency_graph = {p: d for p, d in dependency_graph.items() if not p in deleted_packages }

================
File: ci/platform-tools-info.sh
================
here="$(dirname "$0")"
SBF_TOOLS_VERSION=unknown
cargo_build_sbf_main="${here}/../platform-tools-sdk/cargo-build-sbf/src/toolchain.rs"
if [[ -f "${cargo_build_sbf_main}" ]]; then
    version=$(sed -e 's/^.*DEFAULT_PLATFORM_TOOLS_VERSION.*=\s*"\(v[0-9.]\+\)".*/\1/;t;d' "${cargo_build_sbf_main}")
    if [[ ${version} != '' ]]; then
        SBF_TOOLS_VERSION="${version}"
    else
        echo '--- unable to parse SBF_TOOLS_VERSION'
    fi
else
    echo "--- '${cargo_build_sbf_main}' not present"
fi
echo SBF_TOOLS_VERSION="${SBF_TOOLS_VERSION}"

================
File: ci/publish-crate.sh
================
set -e
cd "$(dirname "$0")/.."
source ci/semver_bash/semver.sh
source ci/rust-version.sh stable
# shellcheck disable=SC2086
is_crate_version_uploaded() {
  name=$1
  version=$2
  curl https://crates.io/api/v1/crates/${name}/${version} | \
  python3 -c "import sys,json; print('version' in json.load(sys.stdin));"
}
# Only package/publish if this is a tagged release
[[ -n $CI_TAG ]] || {
  echo CI_TAG unset, skipped
  exit 0
}
semverParseInto "$CI_TAG" MAJOR MINOR PATCH SPECIAL
expectedCrateVersion="$MAJOR.$MINOR.$PATCH$SPECIAL"
[[ -n "$CRATES_IO_TOKEN" ]] || {
  echo CRATES_IO_TOKEN undefined
  exit 1
}
workspace_cargo_tomls=(Cargo.toml programs/sbf/Cargo.toml)
for cargo_toml in "${workspace_cargo_tomls[@]}"; do
  if ! grep -q "^version = \"$expectedCrateVersion\"$" "$cargo_toml"; then
    echo "Error: Cargo.toml version is not $expectedCrateVersion"
    exit 1
  fi
done
Cargo_tomls=$(ci/order-crates-for-publishing.py)
for Cargo_toml in $Cargo_tomls; do
  echo "--- $Cargo_toml"
  if ! grep -q "^version = { workspace = true }$" "$Cargo_toml"; then
    echo "Warn: $Cargo_toml doesn't use the inherited version"
    grep -q "^version = \"$expectedCrateVersion\"$" "$Cargo_toml" || {
      echo "Error: $Cargo_toml version is not $expectedCrateVersion"
      exit 1
    }
  fi
  crate_name=$(grep -m 1 '^name = ' "$Cargo_toml" | cut -f 3 -d ' ' | tr -d \")
  if grep -q "^publish = false" "$Cargo_toml"; then
    echo "$crate_name is marked as unpublishable"
    continue
  fi
  if [[ $(is_crate_version_uploaded "$crate_name" "$expectedCrateVersion") = True ]] ; then
    echo "${crate_name} version ${expectedCrateVersion} is already on crates.io"
    continue
  fi
  (
    set -x
    crate=$(dirname "$Cargo_toml")
    cargoCommand="cargo publish --token $CRATES_IO_TOKEN"
    numRetries=10
    for ((i = 1; i <= numRetries; i++)); do
      echo "Attempt ${i} of ${numRetries}"
      if ci/docker-run-default-image.sh bash -exc "cd $crate; $cargoCommand"; then
        break
      fi
      if [ "$i" -lt "$numRetries" ]; then
        sleep 3
      else
        echo "couldn't publish '$crate_name'"
        exit 1
      fi
    done
  )
done
exit 0

================
File: ci/publish-installer.sh
================
set -e
cd "$(dirname "$0")/.."
# check does it need to publish
if [[ -n $DO_NOT_PUBLISH_TAR ]]; then
  echo "Skipping publishing install wrapper"
  exit 0
fi
eval "$(ci/channel-info.sh)"
if [[ -n "$CI_TAG" ]]; then
  CHANNEL_OR_TAG=$CI_TAG
else
  CHANNEL_OR_TAG=$CHANNEL
fi
if [[ -z $CHANNEL_OR_TAG ]]; then
  echo +++ Unable to determine channel or tag to publish into, exiting.
  exit 0
fi
source ci/upload-ci-artifact.sh
cat >release.jito.wtf-install <<EOF
SOLANA_RELEASE=$CHANNEL_OR_TAG
SOLANA_INSTALL_INIT_ARGS=$CHANNEL_OR_TAG
SOLANA_DOWNLOAD_ROOT=https://release.jito.wtf
EOF
cat install/agave-install-init.sh >>release.jito.wtf-install
echo --- GCS: "install"
upload-gcs-artifact "/solana/release.jito.wtf-install" "gs://jito-release/$CHANNEL_OR_TAG/install"
echo --- AWS S3 Store: "install"
upload-s3-artifact "/solana/release.jito.wtf-install" "s3://release.jito.wtf/$CHANNEL_OR_TAG/install"
echo Published to:
ci/format-url.sh https://release.jito.wtf/"$CHANNEL_OR_TAG"/install

================
File: ci/publish-metrics-dashboard.sh
================
set -e
cd "$(dirname "$0")/.."
if [[ -z $BUILDKITE ]]; then
  echo BUILDKITE not defined
  exit 1
fi
if [[ -z $PUBLISH_CHANNEL ]]; then
  PUBLISH_CHANNEL=$(buildkite-agent meta-data get "channel" --default "")
fi
if [[ -z $PUBLISH_CHANNEL ]]; then
  (
    cat <<EOF
steps:
  - block: "Select Dashboard"
    fields:
      - select: "Channel"
        key: "channel"
        options:
          - label: "stable"
            value: "stable"
          - label: "edge"
            value: "edge"
          - label: "beta"
            value: "beta"
  - command: "ci/$(basename "$0")"
EOF
  ) | buildkite-agent pipeline upload
  exit 0
fi
ci/channel-info.sh
eval "$(ci/channel-info.sh)"
case $PUBLISH_CHANNEL in
edge)
  CHANNEL_BRANCH=$EDGE_CHANNEL
  ;;
beta)
  CHANNEL_BRANCH=$BETA_CHANNEL
  ;;
stable)
  # Set to whatever branch 'testnet' is on.
  CHANNEL_BRANCH=$BETA_CHANNEL
  ;;
*)
  echo "Error: Invalid PUBLISH_CHANNEL=$PUBLISH_CHANNEL"
  exit 1
  ;;
esac
if [[ $CI_BRANCH != "$CHANNEL_BRANCH" ]]; then
  (
    cat <<EOF
steps:
  - trigger: "$BUILDKITE_PIPELINE_SLUG"
    async: true
    build:
      message: "$BUILDKITE_MESSAGE"
      branch: "$CHANNEL_BRANCH"
      env:
        PUBLISH_CHANNEL: "$PUBLISH_CHANNEL"
EOF
  ) | buildkite-agent pipeline upload
  exit 0
fi
set -x
exec metrics/publish-metrics-dashboard.sh "$PUBLISH_CHANNEL"

================
File: ci/publish-tarball.sh
================
set -e
cd "$(dirname "$0")/.."
DRYRUN=
if [[ -z $CI_BRANCH ]]; then
  DRYRUN="echo"
  CHANNEL=unknown
fi
eval "$(ci/channel-info.sh)"
TAG=
if [[ -n "$CI_TAG" ]]; then
  CHANNEL_OR_TAG=$CI_TAG
  TAG="$CI_TAG"
else
  CHANNEL_OR_TAG=$CHANNEL
fi
if [[ -z $CHANNEL_OR_TAG ]]; then
  echo +++ Unable to determine channel or tag to publish into, exiting.
  exit 0
fi
source scripts/generate-target-triple.sh
TARGET="$BUILD_TARGET_TRIPLE"
if [[ $TARGET == *windows* ]]; then
  (
    set -x
    git --version
    git config core.symlinks true
    find . -type l -delete
    git reset --hard
    sed -i 's/^crossbeam-epoch/#crossbeam-epoch/' Cargo.toml
  )
fi
RELEASE_BASENAME="${RELEASE_BASENAME:=solana-release}"
TARBALL_BASENAME="${TARBALL_BASENAME:="$RELEASE_BASENAME"}"
echo --- Creating release tarball
scripts/create-release-tarball.sh --build-dir "$RELEASE_BASENAME" \
                                  --channel-or-tag "$TAG" \
                                  --target "$TARGET" \
                                  --tarball-basename "$TARBALL_BASENAME"
MAYBE_TARBALLS=
if [[ $TARGET == *linux* ]]; then
  (
    set -x
    platform-tools-sdk/sbf/scripts/package.sh
    [[ -f sbf-sdk.tar.bz2 ]]
  )
  MAYBE_TARBALLS="sbf-sdk.tar.bz2"
fi
source ci/upload-ci-artifact.sh
for file in "${TARBALL_BASENAME}"-$TARGET.tar.bz2 "${TARBALL_BASENAME}"-$TARGET.yml agave-install-init-"$TARGET"* $MAYBE_TARBALLS; do
  if [[ -n $DO_NOT_PUBLISH_TAR ]]; then
    upload-ci-artifact "$file"
    echo "Skipped $file due to DO_NOT_PUBLISH_TAR"
    continue
  fi
  if [[ -n $BUILDKITE ]]; then
    echo --- GCS Store: "$file"
    upload-gcs-artifact "/solana/$file" gs://jito-release/"$CHANNEL_OR_TAG"/"$file"
    echo --- AWS S3 Store: "$file"
    upload-s3-artifact "/solana/$file" s3://release.jito.wtf/"$CHANNEL_OR_TAG"/"$file"
    echo Published to:
    $DRYRUN ci/format-url.sh https://release.jito.wtf/"$CHANNEL_OR_TAG"/"$file"
    if [[ -n $TAG ]]; then
      ci/upload-github-release-asset.sh "$file"
    fi
  elif [[ -n $GITHUB_ACTIONS ]]; then
    mkdir -p github-action-s3-upload/"$CHANNEL_OR_TAG"
    cp -v "$file" github-action-s3-upload/"$CHANNEL_OR_TAG"/
    if [[ -n $TAG ]]; then
      mkdir -p github-action-release-upload/
      cp -v "$file" github-action-release-upload/
    fi
  fi
done
echo --- ok

================
File: ci/run-local.sh
================
cd "$(dirname "$0")/.."
export CI_LOCAL_RUN=true
set -e
steps=()
steps+=(test-sanity)
steps+=(shellcheck)
steps+=(test-checks)
steps+=(test-coverage)
steps+=(test-stable)
steps+=(test-stable-sbf)
steps+=(test-stable-perf)
steps+=(test-downstream-builds)
steps+=(test-bench)
steps+=(test-local-cluster)
steps+=(test-local-cluster-flakey)
steps+=(test-local-cluster-slow-1)
steps+=(test-local-cluster-slow-2)
step_index=0
if [[ -n "$1" ]]; then
  start_step="$1"
  while [[ $step_index -lt ${
    step="${steps[$step_index]}"
    if [[ "$step" = "$start_step" ]]; then
      break
    fi
    step_index=$((step_index + 1))
  done
  if [[ $step_index -eq ${
    echo "unexpected start step: \"$start_step\"" 1>&2
    exit 1
  else
    echo "** starting at step: \"$start_step\" **"
    echo
  fi
fi
while [[ $step_index -lt ${
  step="${steps[$step_index]}"
  cmd="ci/${step}.sh"
  $cmd
  step_index=$((step_index + 1))
done

================
File: ci/run-sanity.sh
================
set -e
cd "$(dirname "$0")/.."
# shellcheck source=multinode-demo/common.sh
source multinode-demo/common.sh
if [[ -z $CI ]]; then
  # Build eagerly if needed for local development. Otherwise, odd timing error occurs...
  $solana_keygen --version
  $solana_genesis --version
  $solana_faucet --version
  $solana_cli --version
  $agave_validator --version
  $solana_ledger_tool --version
fi
rm -rf config/run/init-completed config/ledger
# Sanity-check that agave-validator can successfully terminate itself without relying on
# process::exit() by extending the timeout...
# Also the banking_tracer thread needs some extra time to flush due to
# unsynchronized and buffered IO.
validator_timeout="${SOLANA_VALIDATOR_EXIT_TIMEOUT:-120}"
SOLANA_RUN_SH_VALIDATOR_ARGS="${SOLANA_RUN_SH_VALIDATOR_ARGS} --full-snapshot-interval-slots 200" \
  SOLANA_VALIDATOR_EXIT_TIMEOUT="$validator_timeout" \
  timeout "$validator_timeout" ./scripts/run.sh &
pid=$!
attempts=20
while [[ ! -f config/run/init-completed ]]; do
  sleep 1
  if ((--attempts == 0)); then
     echo "Error: validator failed to boot"
     exit 1
  else
    echo "Checking init"
  fi
done
snapshot_slot=50
latest_slot=0
while [[ $latest_slot -le $((snapshot_slot + 1)) ]]; do
  sleep 1
  echo "Checking slot"
  latest_slot=$($solana_cli --url http://localhost:8899 slot --commitment processed)
done
$agave_validator --ledger config/ledger exit --force || true
wait $pid
for method in blockstore-processor unified-scheduler
do
  rm -rf config/snapshot-ledger
  $solana_ledger_tool create-snapshot --ledger config/ledger "$snapshot_slot" config/snapshot-ledger
  cp config/ledger/genesis.tar.bz2 config/snapshot-ledger
  $solana_ledger_tool copy --ledger config/ledger \
    --target-ledger config/snapshot-ledger --starting-slot "$snapshot_slot" --ending-slot "$latest_slot"
  set -x
  $solana_ledger_tool --ledger config/snapshot-ledger slot "$latest_slot" --verbose --verbose \
    |& grep -q "Log Messages:$" && exit 1
  $solana_ledger_tool verify --abort-on-invalid-block \
    --ledger config/snapshot-ledger --block-verification-method "$method" \
    --enable-rpc-transaction-history --enable-extended-tx-metadata-storage
  $solana_ledger_tool --ledger config/snapshot-ledger slot "$latest_slot" --verbose --verbose \
    |& grep -q "Log Messages:$"
  set +x
done
first_simulated_slot=$((latest_slot / 2))
purge_slot=$((first_simulated_slot + latest_slot / 4))
echo "First simulated slot: ${first_simulated_slot}"
$solana_ledger_tool purge --ledger config/ledger "$purge_slot"
$solana_ledger_tool simulate-block-production --ledger config/ledger \
  --first-simulated-slot $first_simulated_slot
$solana_ledger_tool verify --abort-on-invalid-block \
  --ledger config/ledger --enable-hash-overrides --halt-at-slot "$snapshot_slot"

================
File: ci/rust-version.sh
================
if [[ -n ${RUST_STABLE_VERSION:-} ]]; then
  stable_version="$RUST_STABLE_VERSION"
else
  base="$(dirname "${BASH_SOURCE[0]}")"
  source "$base/../scripts/read-cargo-variable.sh"
  stable_version=$(readCargoVariable channel "$base/../rust-toolchain.toml")
fi
if [[ -n ${RUST_NIGHTLY_VERSION:-} ]]; then
  nightly_version="$RUST_NIGHTLY_VERSION"
else
  nightly_version=2025-08-02
fi
export rust_stable="$stable_version"
export rust_nightly=nightly-"$nightly_version"
if [[ -n ${NO_INSTALL:-} ]]; then
  return
fi
[[ -z $1 ]] || (
  rustup_install() {
    declare toolchain=$1
    if ! cargo +"$toolchain" -V > /dev/null; then
      echo "$0: Missing toolchain? Installing...: $toolchain" >&2
      rustup install "$toolchain" --no-self-update
      cargo +"$toolchain" -V
    fi
  }
  set -e
  cd "$(dirname "${BASH_SOURCE[0]}")"
  case $1 in
  stable)
     rustup_install "$rust_stable"
     ;;
  nightly)
     rustup_install "$rust_nightly"
    ;;
  all)
     rustup_install "$rust_stable"
     rustup_install "$rust_nightly"
    ;;
  *)
    echo "$0: Note: ignoring unknown argument: $1" >&2
    ;;
  esac
)

================
File: ci/shellcheck.sh
================
set -e
cd "$(dirname "$0")/.."
(
  set -x
  git ls-files -- '.buildkite/hooks' '*.sh' ':(exclude)ci/semver_bash' \
    | xargs ci/docker-run.sh koalaman/shellcheck:v0.8.0 --color=always --external-sources --shell=bash
)
echo --- ok

================
File: ci/test-checks.sh
================
set -e
cd "$(dirname "$0")/.."
source ci/_
source ci/rust-version.sh stable
source ci/rust-version.sh nightly
eval "$(ci/channel-info.sh)"
cargoNightly="$(readlink -f "./cargo") nightly"
if ! $cargoNightly hack --version >/dev/null 2>&1; then
  cat >&2 <<EOF
ERROR: cargo hack failed.
       install 'cargo hack' with 'cargo install cargo-hack'
EOF
fi
echo --- build environment
(
  set -x
  rustup run "$rust_stable" rustc --version --verbose
  rustup run "$rust_nightly" rustc --version --verbose
  cargo --version --verbose
  $cargoNightly --version --verbose
  cargo clippy --version --verbose
  $cargoNightly clippy --version --verbose
  $cargoNightly miri --version --verbose
  $cargoNightly hack --version --verbose
  cargo audit --version
  grcov --version
  sccache --version
  wasm-pack --version
  cargo nextest --version --verbose
  $cargoNightly nextest --version --verbose
)
export RUST_BACKTRACE=1
export RUSTFLAGS="-D warnings -A incomplete_features"
if [[ $CI_BASE_BRANCH = "$EDGE_CHANNEL" ]]; then
  if _ scripts/cargo-for-all-lock-files.sh "+${rust_nightly}" check \
    --locked --workspace --all-targets --features dummy-for-ci-check,frozen-abi; then
    true
  else
    check_status=$?
    echo "$0: Some Cargo.lock might be outdated; sync them (or just be a compilation error?)" >&2
    echo "$0: protip: $ ./scripts/cargo-for-all-lock-files.sh [--ignore-exit-code] ... \\" >&2
    echo "$0:   [tree (for outdated Cargo.lock sync)|check (for compilation error)|update -p foo --precise x.y.z (for your Cargo.toml update)] ..." >&2
    exit "$check_status"
  fi
else
  echo "Note: cargo-for-all-lock-files.sh skipped because $CI_BASE_BRANCH != $EDGE_CHANNEL"
fi
_ ci/order-crates-for-publishing.py
_ scripts/cargo-clippy.sh
if [[ -n $CI ]]; then
  _ scripts/cargo-for-all-lock-files.sh -- "+${rust_nightly}" sort --workspace --check > /dev/null
else
  _ scripts/cargo-for-all-lock-files.sh -- "+${rust_nightly}" sort --workspace --check
fi
_ scripts/check-dev-context-only-utils.sh tree
_ scripts/cargo-for-all-lock-files.sh -- "+${rust_nightly}" fmt --all -- --check
_ ci/do-audit.sh
if [[ -n $CI ]] && [[ $CHANNEL = "stable" ]]; then
  _ ci/check-install-all.sh
fi
echo --- ok

================
File: ci/test-coverage.sh
================
set -e
here=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
if [[ -z $CI ]]; then
  echo "This script is used by CI environment. \
Use \`scripts/coverage.sh\` directly if you only want to obtain the coverage report"
  exit 1
fi
SHORT_CI_COMMIT=${CI_COMMIT:0:9}
COMMIT_HASH=$SHORT_CI_COMMIT "$here/../scripts/coverage.sh" "$@"
echo "--- codecov.io report"
if [[ -z "$CODECOV_TOKEN" ]]; then
  echo "^^^ +++"
  echo CODECOV_TOKEN undefined, codecov.io upload skipped
else
  codecov -t "${CODECOV_TOKEN}" --dir "$here/../target/cov/${SHORT_CI_COMMIT}"
  if [[ -n "$BUILDKITE" ]]; then
    buildkite-agent annotate --style success --context codecov.io \
      "CodeCov report: https://codecov.io/github/anza-xyz/agave/commit/$CI_COMMIT"
  fi
fi

================
File: ci/test-dev-context-only-utils.sh
================
set -eo pipefail
source ./ci/_
(unset RUSTC_WRAPPER; cargo install --force --git https://github.com/anza-xyz/cargo-hack.git --rev 5e59c3ec6c661c02601487c0d4b2a2649fe06c9f cargo-hack)
unset SCCACHE_GCS_KEY_PATH SCCACHE_GCS_BUCKET SCCACHE_GCS_RW_MODE SCCACHE_GCS_KEY_PREFIX
export SCCACHE_CACHE_SIZE="200G"
export CARGO_INCREMENTAL=0
_ sccache --show-stats
scripts/check-dev-context-only-utils.sh check-all-targets "$@"
scripts/check-dev-context-only-utils.sh check-bins-and-lib "$@" --features agave-unstable-api
_ sccache --stop-server

================
File: ci/test-downstream-builds.sh
================
cd "$(dirname "$0")/.."
export CI_LOCAL_RUN=true
set -ex
ci/downstream-projects/run-all.sh

================
File: ci/test-frozen-abi.sh
================
set -euo pipefail
here=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
source "$here/rust-version.sh" nightly
packages=$(cargo +"$rust_nightly" metadata --no-deps --format-version=1 | jq -r '.packages[] | select(.features | has("frozen-abi")) | .name')
for package in $packages; do
  cmd="cargo +$rust_nightly test -p $package --features frozen-abi --lib -- test_abi_ --nocapture"
  echo "--- $cmd"
  $cmd
done

================
File: ci/test-miri.sh
================
set -eo pipefail
source ci/_
source ci/rust-version.sh nightly
_ cargo "+${rust_nightly}" miri test -p solana-unified-scheduler-logic
_ cargo "+${rust_nightly}" miri test --target s390x-unknown-linux-gnu -p solana-vote -- "vote_state_view" --skip "arbitrary"
_ cargo "+${rust_nightly}" miri test -p solana-vote -- "vote_state_view" --skip "arbitrary"
(! _ cargo "+${rust_nightly}" miri test -p solana-unified-scheduler-logic -- \
  --ignored --exact "utils::tests::test_ub_illegally_created_multiple_tokens")
(! _ cargo "+${rust_nightly}" miri test -p solana-unified-scheduler-logic -- \
  --ignored --exact "utils::tests::test_ub_illegally_shared_token_cell")

================
File: ci/test-sanity.sh
================
set -e
cd "$(dirname "$0")/.."
source ci/_
echo --- git diff --check
if [[ -n "$CI_BASE_BRANCH" ]]; then
  branch="$CI_BASE_BRANCH"
  remote=origin
else
  IFS='/' read -r remote branch < <(git rev-parse --abbrev-ref --symbolic-full-name '@{u}' 2>/dev/null) || true
  if [[ -z "$branch" ]]; then
    branch="$remote"
    remote=
  fi
fi
if [[ -n "$remote" ]] && ! git remote | grep --quiet "^$remote\$" 2>/dev/null; then
  echo "WARNING: Remote \`$remote\` not configured for this working directory. Assuming it is actually part of the branch name"
  branch="$remote"/"$branch"
  remote=
fi
if [[ -z "$branch" || -z "$remote" ]]; then
  msg="Cannot determine remote target branch. Set one with \`git branch --set-upstream-to=TARGET\`"
  if [[ -n "$CI" ]]; then
    echo "ERROR: $msg" 1>&2
    exit 1
  else
    echo "WARNING: $msg" 1>&2
  fi
fi
if [[ -n "$remote" ]]; then
  echo "Checking remote \`$remote\` for updates to target branch \`$branch\`"
  git fetch --quiet "$remote" "$branch"
  target="$remote"/"$branch"
else
  echo "WARNING: Target branch \`$branch\` appears to be local. No remote updates will be considered."
  target="$branch"
fi
(
  set -x
  git diff "$target" --check --oneline
)
_ ci/check-channel-version.sh
_ ci/nits.sh
scripts/increment-cargo-version.sh check
if ! [ -v SOLANA_CI_ALLOW_STALE_CARGO_LOCK ] ; then
(
  _ scripts/cargo-for-all-lock-files.sh tree >/dev/null
  set +e
  if ! _ git diff --exit-code; then
    cat <<EOF 1>&2
Error: Uncommitted Cargo.lock changes.
Run './scripts/cargo-for-all-lock-files.sh tree' and commit the result.
EOF
    exit 1
  fi
)
fi
(
  if git diff "$target" | grep -v '+++' | grep '^+.*solana[-_]sdk[: =]'; then
    cat <<'EOF' 1>&2
Error: solana sdk crate dependencies (re)introduced.
This crate is DEPRECATED. Please use the standalone crates for the corresponding modules
EOF
    exit 1
  fi
)
echo --- ok

================
File: ci/test-shuttle.sh
================
set -eo pipefail
source ci/_
cargo nextest run --profile ci  --manifest-path="svm/Cargo.toml" --features="shuttle-test" --test concurrent_tests --release --jobs 1
cargo nextest run --release --profile ci  --manifest-path="runtime/Cargo.toml" --features="shuttle-test" shuttle_tests
cargo nextest run --release --profile ci --manifest-path="poh/Cargo.toml" --features="shuttle-test" shuttle_tests

================
File: ci/test-stable.sh
================
set -eo pipefail
cd "$(dirname "$0")/.."
cargo="$(readlink -f "./cargo")"
source ci/_
annotate() {
  ${BUILDKITE:-false} && {
    buildkite-agent annotate "$@"
  }
}
cargo_build_sbf_sanity() {
  cargo_build_sbf="$(realpath ./cargo-build-sbf)"
  pushd programs/sbf
  # Generate the sanity programs list
  if [ ! -f target/sanity_programs.txt ]; then
    cargo test --features="sbf_rust,sbf_sanity_list" --test programs test_program_sbf_sanity
  fi
  mapfile -t rust_programs < <(cat target/sanity_programs.txt)
  pushd rust
  for program in "${rust_programs[@]}"
  do
    pushd "$program"
    $cargo_build_sbf --arch "$1"
    popd
  done
  popd
  SBF_OUT_DIR=target/deploy cargo test --features=sbf_rust --test programs test_program_sbf_sanity
  popd
}
testName=$(basename "$0" .sh)
source ci/rust-version.sh stable
export RUST_BACKTRACE=1
export RUSTFLAGS="-D warnings"
source scripts/ulimit-n.sh
source ci/common/limit-threads.sh
eval "$(ci/channel-info.sh)"
source ci/common/shared-functions.sh
echo "Executing $testName"
case $testName in
test-stable)
  _ ci/intercept.sh cargo test --jobs "$JOBS" --all --tests --exclude solana-local-cluster ${V:+--verbose} -- --nocapture
  ;;
test-stable-sbf)
  test -d target/debug/sbf && find target/debug/sbf -name '*.d' -delete
  test -d target/release/sbf && find target/release/sbf -name '*.d' -delete
  "$cargo" install rustfilt
  _ "$cargo" build --manifest-path=keygen/Cargo.toml
  export PATH="$PWD/target/debug":$PATH
  cargo_build_sbf="$(realpath ./cargo-build-sbf)"
  "$cargo_build_sbf" --version
  _ platform-tools-sdk/sbf/scripts/install.sh
  _ make -C programs/sbf test-v0
  _ make -C programs/sbf clean-all
  _ cargo_build_sbf_sanity "v0"
  _ make -C programs/sbf clean-all test-v1
  _ make -C programs/sbf clean-all
  _ cargo_build_sbf_sanity "v1"
  _ make -C programs/sbf clean-all test-v2
  _ make -C programs/sbf clean-all
  _ cargo_build_sbf_sanity "v2"
  exit 0
  ;;
test-docs)
  _ cargo test --jobs "$JOBS" --all --doc --exclude solana-local-cluster ${V:+--verbose} -- --nocapture
  exit 0
  ;;
*)
  echo "Error: Unknown test: $testName"
  ;;
esac
(
  export CARGO_TOOLCHAIN=+"$rust_stable"
  export RUST_LOG="solana_metrics=warn,info,$RUST_LOG"
  echo --- ci/localnet-sanity.sh
  ci/localnet-sanity.sh -x
  echo --- ci/run-sanity.sh
  ci/run-sanity.sh -x
)

================
File: ci/test-verify-packets-gossip.sh
================
set -e
here=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
if ! git lfs --version &>/dev/null; then
  echo "Git LFS is not installed. Please install Git LFS to proceed."
  exit 1
fi
rm -rf "$here"/solana-packets
git clone https://github.com/anza-xyz/solana-packets.git "$here"/solana-packets
GOSSIP_WIRE_FORMAT_PACKETS="$here/solana-packets/GOSSIP_PACKETS" cargo test --package solana-gossip -- wire_format_tests::tests::test_gossip_wire_format --exact --show-output

================
File: ci/test.sh
================
script_dir_by_bash_source=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
script_dir_by_0=$(cd "$(dirname "$0")" && pwd)
echo "script_dir_by_bash_source = $script_dir_by_bash_source"
echo "script_dir_by_0 = $script_dir_by_0"

================
File: ci/upload-benchmark.sh
================
set -e
usage() {
  cat <<EOF >&2
USAGE:
    $0 <BENCHMARK_FILEPATH>
REQUIRED ENVIRONMENTS:
    INFLUX_HOST           Hostname or IP address of the InfluxDB server
    INFLUX_DB             Name of the InfluxDB database
    INFLUX_USER           Username for InfluxDB
    INFLUX_PASSWORD       Password for InfluxDB
    INFLUX_MEASUREMENT    Measurement for InfluxDB
OPTIONAL ENVIRONMENTS:
    COMMIT_HASH   Commit hash of the benchmark file
    TEST_SUITE    The group name for all tests in the benchmark file
    DRY_RUN       Dry run
ARGS:
    <BENCHMARK_FILEPATH>    The output file generated by running
                            \`cargo bench -- -Z unstable-options --format=json\`
                            contains the benchmark results in JSON format
EOF
}
print_error_and_exit() {
  local msg="$1"
  echo "error: $msg" >&2
  echo ""
  usage
  exit 1
}
check_env() {
  local var_name="$1"
  if [ -z "${!var_name}" ]; then
    print_error_and_exit "Environment variable $var_name is required"
  fi
}
filepath="$1"
if [ ! -f "$filepath" ]; then
  print_error_and_exit "invalid <BENCHMARK_FILEPATH>"
fi
if [ -z "$COMMIT_HASH" ]; then
  COMMIT_HASH=$(uuidgen)
fi
if [ -z "$TEST_SUITE" ]; then
  TEST_SUITE="$(basename "${BENCHMARK_FILEPATH}")-$(date +%s)"
fi
if [ -z "$DRY_RUN" ]; then
  required_env_vars=(
    "INFLUX_HOST"
    "INFLUX_DB"
    "INFLUX_USER"
    "INFLUX_PASSWORD"
    "INFLUX_MEASUREMENT"
  )
  for var in "${required_env_vars[@]}"; do
    check_env "$var"
  done
fi
while IFS= read -r line; do
  if [[ $line =~ ^test\ (.*)\ \.\.\.\ bench:\ *([0-9,\.]+)\ ns\/iter\ \(\+\/-\ *([0-9,\.]+)\) ]]; then
    test_name="${BASH_REMATCH[1]}"
    ns_iter="${BASH_REMATCH[2]}"
    plus_minus="${BASH_REMATCH[3]}"
    ns_iter=$(echo "$ns_iter" | tr -d ',' | cut -d'.' -f1)
    plus_minus=$(echo "$plus_minus" | tr -d ',' | cut -d'.' -f1)
    datapoint="${INFLUX_MEASUREMENT},commit=${COMMIT_HASH},test_suite=${TEST_SUITE},name=${test_name} median=${ns_iter}i,deviation=${plus_minus}i"
    echo "datapoint: $datapoint"
    if [[ -z "$DRY_RUN" ]]; then
      curl -s -X POST "${INFLUX_HOST}/write?db=${INFLUX_DB}" --data-binary "$datapoint"
    fi
  fi
done <"$filepath"

================
File: ci/upload-ci-artifact.sh
================
upload-ci-artifact() {
  echo "--- artifact: $1"
  if [[ -r "$1" ]]; then
    ls -l "$1"
    if ${BUILDKITE:-false}; then
      (
        set -x
        buildkite-agent artifact upload "$1"
      )
    fi
  else
    echo ^^^ +++
    echo "$1 not found"
  fi
}
upload-gcs-artifact() {
  echo "--- artifact: $1 to $2"
  docker run --rm \
    -v "$GCS_RELEASE_BUCKET_WRITER_CREDIENTIAL:/application_default_credentials.json" \
    -v "$PWD:/solana" \
    -e CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE=/application_default_credentials.json \
    gcr.io/google.com/cloudsdktool/google-cloud-cli:latest \
    gcloud storage cp "$1" "$2"
}

================
File: ci/upload-github-release-asset.sh
================
set -e
if [[ -z $1 ]]; then
  echo No files specified
  exit 1
fi
if [[ -z $GITHUB_TOKEN ]]; then
  echo Error: GITHUB_TOKEN not defined
  exit 1
fi
if [[ -z $CI_TAG ]]; then
  echo Error: CI_TAG not defined
  exit 1
fi
CI_REPO_SLUG=jito-foundation/jito-solana
releaseId=$( \
  curl -s "https://api.github.com/repos/$CI_REPO_SLUG/releases/tags/$CI_TAG" \
  | grep -m 1 \"id\": \
  | sed -ne 's/^[^0-9]*\([0-9]*\),$/\1/p' \
)
echo "Github release id for $CI_TAG is $releaseId"
for file in "$@"; do
  echo "--- Uploading $file to tag $CI_TAG of $CI_REPO_SLUG"
  curl \
    --verbose \
    --data-binary @"$file" \
    -H "Authorization: token $GITHUB_TOKEN" \
    -H "Content-Type: application/octet-stream" \
    "https://uploads.github.com/repos/$CI_REPO_SLUG/releases/$releaseId/assets?name=$(basename "$file")"
  echo
done

================
File: clap-utils/src/compute_budget.rs
================
pub fn compute_unit_price_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(COMPUTE_UNIT_PRICE_ARG.long)
.takes_value(true)
.value_name("COMPUTE-UNIT-PRICE")
.validator(is_parsable::<u64>)
.help(COMPUTE_UNIT_PRICE_ARG.help)
⋮----
pub fn compute_unit_limit_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(COMPUTE_UNIT_LIMIT_ARG.long)
⋮----
.value_name("COMPUTE-UNIT-LIMIT")
.validator(is_parsable::<u32>)
.help(COMPUTE_UNIT_LIMIT_ARG.help)
⋮----
pub enum ComputeUnitLimit {

================
File: clap-utils/src/compute_unit_price.rs
================


================
File: clap-utils/src/fee_payer.rs
================
pub fn fee_payer_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(FEE_PAYER_ARG.long)
.takes_value(true)
.value_name("KEYPAIR")
.validator(input_validators::is_valid_signer)
.help(FEE_PAYER_ARG.help)

================
File: clap-utils/src/input_parsers.rs
================
pub fn values_of<T>(matches: &ArgMatches<'_>, name: &str) -> Option<Vec<T>>
⋮----
.values_of(name)
.map(|xs| xs.map(|x| x.parse::<T>().unwrap()).collect())
⋮----
// Return a parsed value from matches at `name`
pub fn value_of<T>(matches: &ArgMatches<'_>, name: &str) -> Option<T>
⋮----
if let Some(value) = matches.value_of(name) {
value.parse::<T>().ok()
⋮----
pub fn unix_timestamp_from_rfc3339_datetime(
⋮----
matches.value_of(name).and_then(|value| {
⋮----
.ok()
.map(|date_time| date_time.timestamp())
⋮----
// Return the keypair for an argument with filename `name` or None if not present.
pub fn keypair_of(matches: &ArgMatches<'_>, name: &str) -> Option<Keypair> {
⋮----
let skip_validation = matches.is_present(SKIP_SEED_PHRASE_VALIDATION_ARG.name);
keypair_from_seed_phrase(name, skip_validation, true, None, true).ok()
⋮----
read_keypair_file(value).ok()
⋮----
pub fn keypairs_of(matches: &ArgMatches<'_>, name: &str) -> Option<Vec<Keypair>> {
matches.values_of(name).map(|values| {
⋮----
.filter_map(|value| {
⋮----
.collect()
⋮----
// Return a pubkey for an argument that can itself be parsed into a pubkey,
// or is a filename that can be read as a keypair
pub fn pubkey_of(matches: &ArgMatches<'_>, name: &str) -> Option<Pubkey> {
value_of(matches, name).or_else(|| keypair_of(matches, name).map(|keypair| keypair.pubkey()))
⋮----
pub fn pubkeys_of(matches: &ArgMatches<'_>, name: &str) -> Option<Vec<Pubkey>> {
⋮----
.map(|value| {
value.parse::<Pubkey>().unwrap_or_else(|_| {
read_keypair_file(value)
.expect("read_keypair_file failed")
.pubkey()
⋮----
pub fn bls_pubkeys_of(matches: &ArgMatches<'_>, name: &str) -> Option<Vec<BLSPubkey>> {
⋮----
BLSPubkey::from_str(value).unwrap_or_else(|_| {
panic!("Failed to parse BLS public key from value: {value}")
⋮----
pub fn pubkeys_sigs_of(matches: &ArgMatches<'_>, name: &str) -> Option<Vec<(Pubkey, Signature)>> {
⋮----
.map(|pubkey_signer_string| {
let mut signer = pubkey_signer_string.split('=');
let key = Pubkey::from_str(signer.next().unwrap()).unwrap();
let sig = Signature::from_str(signer.next().unwrap()).unwrap();
⋮----
// Return a signer from matches at `name`
⋮----
pub fn signer_of(
⋮----
if let Some(location) = matches.value_of(name) {
let signer = signer_from_path(matches, location, name, wallet_manager)?;
let signer_pubkey = signer.pubkey();
Ok((Some(signer), Some(signer_pubkey)))
⋮----
Ok((None, None))
⋮----
pub fn pubkey_of_signer(
⋮----
Ok(Some(pubkey_from_path(
⋮----
Ok(None)
⋮----
pub fn pubkeys_of_multiple_signers(
⋮----
if let Some(pubkey_matches) = matches.values_of(name) {
let mut pubkeys: Vec<Pubkey> = vec![];
⋮----
pubkeys.push(pubkey_from_path(matches, signer, name, wallet_manager)?);
⋮----
Ok(Some(pubkeys))
⋮----
pub fn resolve_signer(
⋮----
resolve_signer_from_path(
⋮----
matches.value_of(name).unwrap(),
⋮----
/// Convert a SOL amount string to lamports.
///
⋮----
///
/// Accepts plain or decimal strings ("50", "0.03", ".5", "1.").
⋮----
/// Accepts plain or decimal strings ("50", "0.03", ".5", "1.").
/// Any decimal places beyond 9 are truncated.
⋮----
/// Any decimal places beyond 9 are truncated.
pub fn lamports_of_sol(matches: &ArgMatches<'_>, name: &str) -> Option<u64> {
⋮----
pub fn lamports_of_sol(matches: &ArgMatches<'_>, name: &str) -> Option<u64> {
⋮----
let (sol, lamports) = value.split_once('.').unwrap_or((value, ""));
let sol = if sol.is_empty() {
⋮----
sol.parse::<u64>().ok()?
⋮----
let lamports = if lamports.is_empty() {
⋮----
format!("{lamports:0<9}")[..9].parse().ok()?
⋮----
Some(
⋮----
.saturating_mul(sol)
.saturating_add(lamports),
⋮----
pub fn cluster_type_of(matches: &ArgMatches<'_>, name: &str) -> Option<ClusterType> {
value_of(matches, name)
⋮----
pub fn commitment_of(matches: &ArgMatches<'_>, name: &str) -> Option<CommitmentConfig> {
⋮----
.value_of(name)
.map(|value| CommitmentConfig::from_str(value).unwrap_or_default())
⋮----
// Parse a cpu range in standard cpuset format, eg:
//
// 0-4,9
// 0-2,7,12-14
pub fn parse_cpu_ranges(data: &str) -> Result<Vec<usize>, io::Error> {
data.split(',')
.map(|range| {
⋮----
.split('-')
.map(|s| s.parse::<usize>().map_err(|ParseIntError { .. }| range));
let start = iter.next().unwrap()?; // str::split always returns at least one element.
let end = match iter.next() {
⋮----
if iter.next().is_some() {
return Err(range);
⋮----
Ok(start..=end)
⋮----
.try_fold(Vec::new(), |mut cpus, range| {
let range = range.map_err(|range| io::Error::new(io::ErrorKind::InvalidData, range))?;
cpus.extend(range);
Ok(cpus)
⋮----
mod tests {
⋮----
fn app<'ab, 'v>() -> App<'ab, 'v> {
⋮----
.arg(
⋮----
.long("multiple")
.takes_value(true)
.multiple(true),
⋮----
.arg(Arg::with_name("single").takes_value(true).long("single"))
.arg(Arg::with_name("unit").takes_value(true).long("unit"))
⋮----
fn tmp_file_path(name: &str, pubkey: &Pubkey) -> String {
use std::env;
let out_dir = env::var("FARF_DIR").unwrap_or_else(|_| "farf".to_string());
format!("{out_dir}/tmp/{name}-{pubkey}")
⋮----
fn test_values_of() {
let matches = app().get_matches_from(vec!["test", "--multiple", "50", "--multiple", "39"]);
assert_eq!(values_of(&matches, "multiple"), Some(vec![50, 39]));
assert_eq!(values_of::<u64>(&matches, "single"), None);
⋮----
let matches = app().get_matches_from(vec![
⋮----
assert_eq!(
⋮----
fn test_value_of() {
let matches = app().get_matches_from(vec!["test", "--single", "50"]);
assert_eq!(value_of(&matches, "single"), Some(50));
assert_eq!(value_of::<u64>(&matches, "multiple"), None);
⋮----
let matches = app().get_matches_from(vec!["test", "--single", &pubkey.to_string()]);
assert_eq!(value_of(&matches, "single"), Some(pubkey));
⋮----
fn test_keypair_of() {
⋮----
let outfile = tmp_file_path("test_keypair_of.json", &keypair.pubkey());
let _ = write_keypair_file(&keypair, &outfile).unwrap();
let matches = app().get_matches_from(vec!["test", "--single", &outfile]);
⋮----
assert!(keypair_of(&matches, "multiple").is_none());
let matches = app().get_matches_from(vec!["test", "--single", "random_keypair_file.json"]);
assert!(keypair_of(&matches, "single").is_none());
fs::remove_file(&outfile).unwrap();
⋮----
fn test_pubkey_of() {
⋮----
let outfile = tmp_file_path("test_pubkey_of.json", &keypair.pubkey());
⋮----
assert_eq!(pubkey_of(&matches, "single"), Some(keypair.pubkey()));
assert_eq!(pubkey_of(&matches, "multiple"), None);
⋮----
app().get_matches_from(vec!["test", "--single", &keypair.pubkey().to_string()]);
⋮----
assert_eq!(pubkey_of(&matches, "single"), None);
⋮----
fn test_pubkeys_of() {
⋮----
let outfile = tmp_file_path("test_pubkeys_of.json", &keypair.pubkey());
⋮----
fn test_pubkeys_sigs_of() {
⋮----
let sig1 = Keypair::new().sign_message(&[0u8]);
let sig2 = Keypair::new().sign_message(&[1u8]);
let signer1 = format!("{key1}={sig1}");
let signer2 = format!("{key2}={sig2}");
⋮----
app().get_matches_from(vec!["test", "--multiple", &signer1, "--multiple", &signer2]);
⋮----
fn test_lamports_of_sol_origin() {
use solana_native_token::sol_str_to_lamports;
⋮----
matches.value_of(name).and_then(sol_str_to_lamports)
⋮----
assert_eq!(lamports_of_sol(&matches, "single"), Some(50_000_000_000));
assert_eq!(lamports_of_sol(&matches, "multiple"), None);
let matches = app().get_matches_from(vec!["test", "--single", "1.5"]);
assert_eq!(lamports_of_sol(&matches, "single"), Some(1_500_000_000));
⋮----
let matches = app().get_matches_from(vec!["test", "--single", "0.03"]);
assert_eq!(lamports_of_sol(&matches, "single"), Some(30_000_000));
let matches = app().get_matches_from(vec!["test", "--single", ".03"]);
⋮----
let matches = app().get_matches_from(vec!["test", "--single", "1."]);
assert_eq!(lamports_of_sol(&matches, "single"), Some(1_000_000_000));
let matches = app().get_matches_from(vec!["test", "--single", ".0"]);
assert_eq!(lamports_of_sol(&matches, "single"), Some(0));
let matches = app().get_matches_from(vec!["test", "--single", "."]);
assert_eq!(lamports_of_sol(&matches, "single"), None);
let matches = app().get_matches_from(vec!["test", "--single", "1.000000015"]);
assert_ne!(lamports_of_sol(&matches, "single"), Some(1_000_000_015));
let matches = app().get_matches_from(vec!["test", "--single", "0.0157"]);
assert_ne!(lamports_of_sol(&matches, "single"), Some(15_700_000));
let matches = app().get_matches_from(vec!["test", "--single", "0.5025"]);
assert_ne!(lamports_of_sol(&matches, "single"), Some(502_500_000));
⋮----
fn test_bls_pubkeys_of() {
⋮----
fn test_lamports_of_sol() {
⋮----
assert_eq!(lamports_of_sol(&matches, "single"), Some(1_000_000_015));
⋮----
assert_eq!(lamports_of_sol(&matches, "single"), Some(15_700_000));
⋮----
assert_eq!(lamports_of_sol(&matches, "single"), Some(502_500_000));
let matches = app().get_matches_from(vec!["test", "--single", "0.1234567891"]);
assert_eq!(lamports_of_sol(&matches, "single"), Some(123_456_789));
let matches = app().get_matches_from(vec!["test", "--single", "0.1234567899"]);
⋮----
let matches = app().get_matches_from(vec!["test", "--single", "1.000.4567899"]);
⋮----
let matches = app().get_matches_from(vec!["test", "--single", "6,998"]);
⋮----
let matches = app().get_matches_from(vec!["test", "--single", "6,998.00"]);

================
File: clap-utils/src/input_validators.rs
================
fn is_parsable_generic<U, T>(string: T) -> Result<(), String>
⋮----
.as_ref()
⋮----
.map(|_| ())
.map_err(|err| format!("error parsing '{string}': {err}"))
⋮----
pub fn is_parsable<T>(string: String) -> Result<(), String>
⋮----
pub fn is_within_range<T, R>(string: String, range: R) -> Result<(), String>
⋮----
if !range.contains(&input) {
Err(format!("input '{input:?}' out of range {range:?}"))
⋮----
Ok(())
⋮----
Err(err) => Err(format!("error parsing '{string}': {err}")),
⋮----
pub fn is_pubkey<T>(string: T) -> Result<(), String>
⋮----
pub fn is_hash<T>(string: T) -> Result<(), String>
⋮----
pub fn is_keypair<T>(string: T) -> Result<(), String>
⋮----
read_keypair_file(string.as_ref())
⋮----
.map_err(|err| format!("{err}"))
⋮----
pub fn is_keypair_or_ask_keyword<T>(string: T) -> Result<(), String>
⋮----
if string.as_ref() == ASK_KEYWORD {
return Ok(());
⋮----
pub fn is_prompt_signer_source<T>(string: T) -> Result<(), String>
⋮----
match parse_signer_source(string.as_ref())
.map_err(|err| format!("{err}"))?
⋮----
SignerSourceKind::Prompt => Ok(()),
_ => Err(format!(
⋮----
pub fn is_pubkey_or_keypair<T>(string: T) -> Result<(), String>
⋮----
is_pubkey(string.as_ref()).or_else(|_| is_keypair(string))
⋮----
pub fn is_valid_pubkey<T>(string: T) -> Result<(), String>
⋮----
SignerSourceKind::Filepath(path) => is_keypair(path),
_ => Ok(()),
⋮----
pub fn is_valid_signer<T>(string: T) -> Result<(), String>
⋮----
is_valid_pubkey(string)
⋮----
pub fn is_pubkey_sig<T>(string: T) -> Result<(), String>
⋮----
let mut signer = string.as_ref().split('=');
⋮----
.next()
.ok_or_else(|| "Malformed signer string".to_string())?,
⋮----
Ok(_) => Ok(()),
Err(err) => Err(format!("{err}")),
⋮----
pub fn is_url<T>(string: T) -> Result<(), String>
⋮----
match url::Url::parse(string.as_ref()) {
⋮----
if url.has_host() {
⋮----
Err("no host provided".to_string())
⋮----
pub fn is_url_or_moniker<T>(string: T) -> Result<(), String>
⋮----
match url::Url::parse(&normalize_to_url_if_moniker(string.as_ref())) {
⋮----
pub fn normalize_to_url_if_moniker<T: AsRef<str>>(url_or_moniker: T) -> String {
match url_or_moniker.as_ref() {
⋮----
.to_string()
⋮----
pub fn is_epoch<T>(epoch: T) -> Result<(), String>
⋮----
pub fn is_slot<T>(slot: T) -> Result<(), String>
⋮----
pub fn is_pow2<T>(bins: T) -> Result<(), String>
⋮----
bins.as_ref()
⋮----
.map_err(|e| format!("Unable to parse, provided: {bins}, err: {e}"))
.and_then(|v| {
if !v.is_power_of_two() {
Err(format!("Must be a power of 2: {v}"))
⋮----
pub fn is_port<T>(port: T) -> Result<(), String>
⋮----
pub fn is_valid_percentage<T>(percentage: T) -> Result<(), String>
⋮----
.map_err(|e| format!("Unable to parse input percentage, provided: {percentage}, err: {e}"))
⋮----
Err(format!(
⋮----
pub fn is_amount<T>(amount: T) -> Result<(), String>
⋮----
if amount.as_ref().parse::<u64>().is_ok() || amount.as_ref().parse::<f64>().is_ok() {
⋮----
pub fn is_amount_or_all<T>(amount: T) -> Result<(), String>
⋮----
if amount.as_ref().parse::<u64>().is_ok()
|| amount.as_ref().parse::<f64>().is_ok()
|| amount.as_ref() == "ALL"
⋮----
pub fn is_amount_or_all_or_available<T>(amount: T) -> Result<(), String>
⋮----
|| amount.as_ref() == "AVAILABLE"
⋮----
pub fn is_rfc3339_datetime<T>(value: T) -> Result<(), String>
⋮----
DateTime::parse_from_rfc3339(value.as_ref())
⋮----
.map_err(|e| format!("{e}"))
⋮----
pub fn is_derivation<T>(value: T) -> Result<(), String>
⋮----
let value = value.as_ref().replace('\'', "");
let mut parts = value.split('/');
let account = parts.next().unwrap();
⋮----
.map_err(|e| format!("Unable to parse derivation, provided: {account}, err: {e}"))
.and_then(|_| {
if let Some(change) = parts.next() {
change.parse::<u32>().map_err(|e| {
format!("Unable to parse derivation, provided: {change}, err: {e}")
⋮----
Ok(0)
⋮----
pub fn is_structured_seed<T>(value: T) -> Result<(), String>
⋮----
.split_once(':')
.ok_or("Seed must contain ':' as delimiter")
.unwrap();
if prefix.is_empty() || value.is_empty() {
Err(String::from("Seed prefix or value is empty"))
⋮----
"string" | "pubkey" | "hex" | "u8" => Ok(()),
⋮----
let len = prefix.len();
⋮----
Err(format!("Wrong prefix length {len} {prefix}:{value}"))
⋮----
let type_size = &prefix[1..len.saturating_sub(2)];
let byte_order = &prefix[len.saturating_sub(2)..len];
⋮----
Err(format!("Wrong prefix sign {sign} {prefix}:{value}"))
⋮----
pub fn is_derived_address_seed<T>(value: T) -> Result<(), String>
⋮----
let value = value.as_ref();
if value.len() > MAX_SEED_LEN {
⋮----
pub fn validate_maximum_full_snapshot_archives_to_retain<T>(value: T) -> Result<(), String>
⋮----
if value.eq("0") {
Err(String::from(
⋮----
pub fn validate_maximum_incremental_snapshot_archives_to_retain<T>(value: T) -> Result<(), String>
⋮----
pub fn validate_cpu_ranges<T>(value: T, err_prefix: &str) -> Result<(), String>
⋮----
parse_cpu_ranges(value.as_ref())
⋮----
.map_err(|e| format!("{err_prefix} {e}"))
⋮----
pub fn is_non_zero(value: impl AsRef<str>) -> Result<(), String> {
⋮----
Err(String::from("cannot be zero"))
⋮----
mod tests {
⋮----
fn test_is_derivation() {
assert_eq!(is_derivation("2"), Ok(()));
assert_eq!(is_derivation("0"), Ok(()));
assert_eq!(is_derivation("65537"), Ok(()));
assert_eq!(is_derivation("0/2"), Ok(()));
assert_eq!(is_derivation("0'/2'"), Ok(()));
assert!(is_derivation("a").is_err());
assert!(is_derivation("4294967296").is_err());
assert!(is_derivation("a/b").is_err());
assert!(is_derivation("0/4294967296").is_err());

================
File: clap-utils/src/keypair.rs
================
pub struct SignOnly {
⋮----
impl SignOnly {
pub fn has_all_signers(&self) -> bool {
self.absent_signers.is_empty() && self.bad_signers.is_empty()
⋮----
pub fn presigner_of(&self, pubkey: &Pubkey) -> Option<Presigner> {
presigner_from_pubkey_sigs(pubkey, &self.present_signers)
⋮----
pub type CliSigners = Vec<Box<dyn Signer>>;
pub type SignerIndex = usize;
pub struct CliSignerInfo {
⋮----
impl CliSignerInfo {
pub fn index_of(&self, pubkey: Option<Pubkey>) -> Option<usize> {
⋮----
.iter()
.position(|signer| signer.pubkey() == pubkey)
⋮----
Some(0)
⋮----
pub fn index_of_or_none(&self, pubkey: Option<Pubkey>) -> Option<usize> {
⋮----
pub fn signers_for_message(&self, message: &Message) -> Vec<&dyn Signer> {
⋮----
.filter_map(|k| {
if message.signer_keys().contains(&&k.pubkey()) {
Some(k.as_ref())
⋮----
.collect()
⋮----
pub struct DefaultSigner {
⋮----
impl DefaultSigner {
pub fn new<AN: AsRef<str>, P: AsRef<str>>(arg_name: AN, path: P) -> Self {
let arg_name = arg_name.as_ref().to_string();
let path = path.as_ref().to_string();
⋮----
fn path(&self) -> Result<&str, Box<dyn std::error::Error>> {
if !self.is_path_checked.borrow().deref() {
parse_signer_source(&self.path)
.and_then(|s| {
⋮----
std::fs::metadata(path).map(|_| ()).map_err(|e| e.into())
⋮----
Ok(())
⋮----
.map_err(|_| {
std::io::Error::other(format!(
⋮----
*self.is_path_checked.borrow_mut() = true;
⋮----
Ok(&self.path)
⋮----
pub fn generate_unique_signers(
⋮----
let mut unique_signers = vec![];
// Determine if the default signer is needed
if bulk_signers.iter().any(|signer| signer.is_none()) {
let default_signer = self.signer_from_path(matches, wallet_manager)?;
unique_signers.push(default_signer);
⋮----
for signer in bulk_signers.into_iter().flatten() {
if !unique_signers.iter().any(|s| s == &signer) {
unique_signers.push(signer);
⋮----
Ok(CliSignerInfo {
⋮----
/// Loads the default [Signer] from one of several possible sources.
    ///
⋮----
///
    /// The `path` is not strictly a file system path, but is interpreted as
⋮----
/// The `path` is not strictly a file system path, but is interpreted as
    /// various types of _signing source_, depending on its format, one of which
⋮----
/// various types of _signing source_, depending on its format, one of which
    /// is a path to a keypair file. Some sources may require user interaction
⋮----
/// is a path to a keypair file. Some sources may require user interaction
    /// in the course of calling this function.
⋮----
/// in the course of calling this function.
    ///
⋮----
///
    /// This simply delegates to the [`signer_from_path`] free function, passing
⋮----
/// This simply delegates to the [`signer_from_path`] free function, passing
    /// it the `DefaultSigner`s `path` and `arg_name` fields as the `path` and
⋮----
/// it the `DefaultSigner`s `path` and `arg_name` fields as the `path` and
    /// `keypair_name` arguments.
⋮----
/// `keypair_name` arguments.
    ///
⋮----
///
    /// See the [`signer_from_path`] free function for full documentation of how
⋮----
/// See the [`signer_from_path`] free function for full documentation of how
    /// this function interprets its arguments.
⋮----
/// this function interprets its arguments.
    ///
⋮----
///
    /// # Examples
⋮----
/// # Examples
    ///
⋮----
///
    /// ```no_run
⋮----
/// ```no_run
    /// use clap::{App, Arg, value_t_or_exit};
⋮----
/// use clap::{App, Arg, value_t_or_exit};
    /// use solana_clap_utils::keypair::DefaultSigner;
⋮----
/// use solana_clap_utils::keypair::DefaultSigner;
    /// use solana_clap_utils::offline::OfflineArgs;
⋮----
/// use solana_clap_utils::offline::OfflineArgs;
    ///
⋮----
///
    /// let clap_app = App::new("my-program")
⋮----
/// let clap_app = App::new("my-program")
    ///     // The argument we'll parse as a signer "path"
⋮----
///     // The argument we'll parse as a signer "path"
    pub fn signer_from_path(
⋮----
pub fn signer_from_path(
⋮----
signer_from_path(matches, self.path()?, &self.arg_name, wallet_manager)
⋮----
pub fn signer_from_path_with_config(
⋮----
signer_from_path_with_config(
⋮----
self.path()?,
⋮----
pub(crate) struct SignerSource {
⋮----
impl SignerSource {
fn new(kind: SignerSourceKind) -> Self {
⋮----
fn new_legacy(kind: SignerSourceKind) -> Self {
⋮----
pub(crate) enum SignerSourceKind {
⋮----
fn as_ref(&self) -> &str {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
let s: &str = self.as_ref();
write!(f, "{s}")
⋮----
pub(crate) enum SignerSourceError {
⋮----
pub(crate) fn parse_signer_source<S: AsRef<str>>(
⋮----
let source = source.as_ref();
⋮----
while let Some(trimmed) = source.strip_prefix('\'') {
source = if let Some(trimmed) = trimmed.strip_suffix('\'') {
⋮----
source.replace('\\', "/")
⋮----
source.to_string()
⋮----
match uriparse::URIReference::try_from(source.as_str()) {
Err(_) => Err(SignerSourceError::UnrecognizedSource),
⋮----
if let Some(scheme) = uri.scheme() {
let scheme = scheme.as_str().to_ascii_lowercase();
match scheme.as_str() {
SIGNER_SOURCE_PROMPT => Ok(SignerSource {
⋮----
SIGNER_SOURCE_FILEPATH => Ok(SignerSource::new(SignerSourceKind::Filepath(
uri.path().to_string(),
⋮----
SIGNER_SOURCE_USB => Ok(SignerSource {
⋮----
SIGNER_SOURCE_STDIN => Ok(SignerSource::new(SignerSourceKind::Stdin)),
⋮----
// On Windows, an absolute path's drive letter will be parsed as the URI
if scheme.len() == 1 {
return Ok(SignerSource::new(SignerSourceKind::Filepath(source)));
⋮----
Err(SignerSourceError::UnrecognizedSource)
⋮----
match source.as_str() {
STDOUT_OUTFILE_TOKEN => Ok(SignerSource::new(SignerSourceKind::Stdin)),
ASK_KEYWORD => Ok(SignerSource::new_legacy(SignerSourceKind::Prompt)),
_ => match Pubkey::from_str(source.as_str()) {
Ok(pubkey) => Ok(SignerSource::new(SignerSourceKind::Pubkey(pubkey))),
Err(_) => std::fs::metadata(source.as_str())
.map(|_| SignerSource::new(SignerSourceKind::Filepath(source)))
.map_err(|err| err.into()),
⋮----
pub fn presigner_from_pubkey_sigs(
⋮----
signers.iter().find_map(|(signer, sig)| {
⋮----
Some(Presigner::new(signer, sig))
⋮----
pub struct SignerFromPathConfig {
⋮----
signer_from_path_with_config(matches, path, keypair_name, wallet_manager, &config)
⋮----
} = parse_signer_source(path)?;
⋮----
let skip_validation = matches.is_present(SKIP_SEED_PHRASE_VALIDATION_ARG.name);
Ok(Box::new(keypair_from_seed_phrase(
⋮----
SignerSourceKind::Filepath(path) => match read_keypair_file(&path) {
Err(e) => Err(std::io::Error::other(format!(
⋮----
.into()),
Ok(file) => Ok(Box::new(file)),
⋮----
Ok(Box::new(read_keypair(&mut stdin)?))
⋮----
if wallet_manager.is_none() {
*wallet_manager = maybe_wallet_manager()?;
⋮----
Ok(Box::new(generate_remote_keypair(
⋮----
derivation_path.unwrap_or_default(),
⋮----
matches.is_present("confirm_key"),
⋮----
Err(RemoteWalletError::NoDeviceFound.into())
⋮----
let presigner = pubkeys_sigs_of(matches, SIGNER_ARG.name)
.as_ref()
.and_then(|presigners| presigner_from_pubkey_sigs(&pubkey, presigners));
⋮----
Ok(Box::new(presigner))
} else if config.allow_null_signer || matches.is_present(SIGN_ONLY_ARG.name) {
Ok(Box::new(NullSigner::new(&pubkey)))
⋮----
Err(std::io::Error::other(format!(
⋮----
.into())
⋮----
pub fn pubkey_from_path(
⋮----
let SignerSource { kind, .. } = parse_signer_source(path)?;
⋮----
SignerSourceKind::Pubkey(pubkey) => Ok(pubkey),
_ => Ok(signer_from_path(matches, path, keypair_name, wallet_manager)?.pubkey()),
⋮----
pub fn resolve_signer_from_path(
⋮----
keypair_from_seed_phrase(
⋮----
.map(|_| None)
⋮----
Ok(_) => Ok(Some(path.to_string())),
⋮----
read_keypair(&mut stdin).map(|_| None)
⋮----
let path = generate_remote_keypair(
⋮----
.map(|keypair| keypair.path)?;
Ok(Some(path))
⋮----
_ => Ok(Some(path.to_string())),
⋮----
/// Prompts user for a passphrase and then asks for confirmirmation to check for mistakes
pub fn prompt_passphrase(prompt: &str) -> Result<String, Box<dyn error::Error>> {
⋮----
pub fn prompt_passphrase(prompt: &str) -> Result<String, Box<dyn error::Error>> {
let passphrase = prompt_password(prompt)?;
if !passphrase.is_empty() {
⋮----
return Err("Passphrases did not match".into());
⋮----
Ok(passphrase)
⋮----
/// Loads a [Keypair] from one of several possible sources.
///
⋮----
///
/// The `path` is not strictly a file system path, but is interpreted as various
⋮----
/// The `path` is not strictly a file system path, but is interpreted as various
/// types of _signing source_, depending on its format, one of which is a path
⋮----
/// types of _signing source_, depending on its format, one of which is a path
/// to a keypair file. Some sources may require user interaction in the course
⋮----
/// to a keypair file. Some sources may require user interaction in the course
/// of calling this function.
⋮----
/// of calling this function.
///
⋮----
///
/// This is the same as [`signer_from_path`] except that it only supports
⋮----
/// This is the same as [`signer_from_path`] except that it only supports
/// signing sources that can result in a [Keypair]: prompt for seed phrase,
⋮----
/// signing sources that can result in a [Keypair]: prompt for seed phrase,
/// keypair file, and stdin.
⋮----
/// keypair file, and stdin.
///
⋮----
///
/// If `confirm_pubkey` is `true` then after deriving the pubkey, the user will
⋮----
/// If `confirm_pubkey` is `true` then after deriving the pubkey, the user will
/// be prompted to confirm that the pubkey is as expected.
⋮----
/// be prompted to confirm that the pubkey is as expected.
///
⋮----
///
/// See [`signer_from_path`] for full documentation of how this function
⋮----
/// See [`signer_from_path`] for full documentation of how this function
/// interprets its arguments.
⋮----
/// interprets its arguments.
///
⋮----
///
/// # Examples
⋮----
/// # Examples
///
⋮----
///
/// ```no_run
⋮----
/// ```no_run
/// use clap::{App, Arg, value_t_or_exit};
⋮----
/// use clap::{App, Arg, value_t_or_exit};
/// use solana_clap_utils::keypair::keypair_from_path;
⋮----
/// use solana_clap_utils::keypair::keypair_from_path;
///
⋮----
///
/// let clap_app = App::new("my-program")
⋮----
/// let clap_app = App::new("my-program")
///     // The argument we'll parse as a signer "path"
⋮----
///     // The argument we'll parse as a signer "path"
pub fn keypair_from_path(
⋮----
pub fn keypair_from_path(
⋮----
Ok(keypair_from_seed_phrase(
⋮----
Ok(file) => Ok(file),
⋮----
Ok(read_keypair(&mut stdin)?)
⋮----
_ => Err(std::io::Error::other(format!(
⋮----
pub fn keypair_from_seed_phrase(
⋮----
let seed_phrase = prompt_password(format!("[{keypair_name}] seed phrase: "))?;
let seed_phrase = seed_phrase.trim();
let passphrase_prompt = format!(
⋮----
let passphrase = prompt_passphrase(&passphrase_prompt)?;
⋮----
keypair_from_seed_phrase_and_passphrase(seed_phrase, &passphrase)?
⋮----
let seed = generate_seed_from_seed_phrase_and_passphrase(seed_phrase, &passphrase);
keypair_from_seed_and_derivation_path(&seed, derivation_path)?
⋮----
let sanitized = sanitize_seed_phrase(seed_phrase);
⋮----
return Ok(mnemonic);
⋮----
Err("Can't get mnemonic from seed phrases")
⋮----
let mnemonic = parse_language_fn()?;
⋮----
keypair_from_seed(seed.as_bytes())?
⋮----
keypair_from_seed_and_derivation_path(seed.as_bytes(), derivation_path)?
⋮----
let pubkey = keypair.pubkey();
print!("Recovered pubkey `{pubkey:?}`. Continue? (y/n): ");
let _ignored = stdout().flush();
⋮----
stdin().read_line(&mut input).expect("Unexpected input");
if input.to_lowercase().trim() != "y" {
println!("Exiting");
exit(1);
⋮----
Ok(keypair)
⋮----
fn sanitize_seed_phrase(seed_phrase: &str) -> String {
⋮----
.split_whitespace()
⋮----
.join(" ")
⋮----
mod tests {
⋮----
fn test_sanitize_seed_phrase() {
⋮----
assert_eq!(
⋮----
fn test_signer_info_signers_for_message() {
⋮----
&[transfer(&source.pubkey(), &recipient, 42)],
Some(&fee_payer.pubkey()),
⋮----
let signers = vec![
⋮----
let msg_signers = signer_info.signers_for_message(&message);
let signer_pubkeys = msg_signers.iter().map(|s| s.pubkey()).collect::<Vec<_>>();
let expect = vec![
⋮----
assert_eq!(signer_pubkeys, expect);
⋮----
fn test_parse_signer_source() {
assert_matches!(
⋮----
let stdin = "stdin:".to_string();
⋮----
assert!(
⋮----
let file0 = NamedTempFile::new().unwrap();
let path = file0.path();
assert!(path.is_absolute());
let absolute_path_str = path.to_str().unwrap();
let file1 = NamedTempFile::new_in(std::env::current_dir().unwrap()).unwrap();
let path = file1.path().file_name().unwrap().to_str().unwrap();
⋮----
assert!(path.is_relative());
let relative_path_str = path.to_str().unwrap();
⋮----
let usb = "usb://ledger".to_string();
⋮----
assert_matches!(parse_signer_source(usb).unwrap(), SignerSource {
⋮----
let usb = "usb://ledger?key=0/0".to_string();
⋮----
let expected_derivation_path = Some(DerivationPath::new_bip44(Some(0), Some(0)));
⋮----
let junk = "sometextthatisnotapubkeyorfile".to_string();
assert!(Pubkey::from_str(&junk).is_err());
⋮----
let prompt = "prompt:".to_string();
⋮----
fn signer_from_path_with_file() -> Result<(), Box<dyn std::error::Error>> {
⋮----
let dir = dir.path();
let keypair_path = dir.join("id.json");
let keypair_path_str = keypair_path.to_str().expect("utf-8");
⋮----
write_keypair_file(&keypair, &keypair_path)?;
let args = vec!["program", keypair_path_str];
⋮----
.arg(
⋮----
.required(true)
.help("The signing keypair"),
⋮----
.offline_args();
let clap_matches = clap_app.get_matches_from(args);
let keypair_str = value_t_or_exit!(clap_matches, "keypair", String);
let wallet_manager = initialize_wallet_manager()?;
let signer = signer_from_path(
⋮----
&mut Some(wallet_manager),
⋮----
assert_eq!(keypair.pubkey(), signer.pubkey());

================
File: clap-utils/src/lib.rs
================
use thiserror::Error;
pub struct ArgConstant<'a> {
⋮----
pub struct DisplayError(Box<dyn std::error::Error>);
impl DisplayError {
pub fn new_as_boxed(inner: Box<dyn std::error::Error>) -> Box<Self> {
DisplayError(inner).into()
⋮----
fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
write!(fmt, "{}", self.0)
⋮----
pub fn hidden_unless_forced() -> bool {
std::env::var("SOLANA_NO_HIDDEN_CLI_ARGS").is_err()
⋮----
pub mod compute_budget;
pub mod compute_unit_price;
pub mod fee_payer;
pub mod input_parsers;
pub mod input_validators;
pub mod keypair;
pub mod memo;
pub mod nonce;
pub mod offline;

================
File: clap-utils/src/memo.rs
================
pub fn memo_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(MEMO_ARG.long)
.takes_value(true)
.value_name("MEMO")
.help(MEMO_ARG.help)

================
File: clap-utils/src/nonce.rs
================
fn nonce_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(NONCE_ARG.long)
.takes_value(true)
.value_name("PUBKEY")
.requires(BLOCKHASH_ARG.name)
.validator(is_valid_pubkey)
.help(NONCE_ARG.help)
⋮----
pub fn nonce_authority_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(NONCE_AUTHORITY_ARG.long)
⋮----
.value_name("KEYPAIR")
.validator(is_valid_signer)
.help(NONCE_AUTHORITY_ARG.help)
⋮----
pub trait NonceArgs {
⋮----
impl NonceArgs for App<'_, '_> {
fn nonce_args(self, global: bool) -> Self {
self.arg(nonce_arg().global(global)).arg(
nonce_authority_arg()
.requires(NONCE_ARG.name)
.global(global),

================
File: clap-utils/src/offline.rs
================
pub fn blockhash_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(BLOCKHASH_ARG.long)
.takes_value(true)
.value_name("BLOCKHASH")
.validator(is_hash)
.help(BLOCKHASH_ARG.help)
⋮----
pub fn sign_only_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(SIGN_ONLY_ARG.long)
.takes_value(false)
.requires(BLOCKHASH_ARG.name)
.help(SIGN_ONLY_ARG.help)
⋮----
fn signer_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(SIGNER_ARG.long)
⋮----
.value_name("PUBKEY=SIGNATURE")
.validator(is_pubkey_sig)
⋮----
.multiple(true)
.number_of_values(1)
.help(SIGNER_ARG.help)
⋮----
pub fn dump_transaction_message<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(DUMP_TRANSACTION_MESSAGE.long)
⋮----
.requires(SIGN_ONLY_ARG.name)
.help(DUMP_TRANSACTION_MESSAGE.help)
⋮----
pub trait ArgsConfig {
fn blockhash_arg<'a, 'b>(&self, arg: Arg<'a, 'b>) -> Arg<'a, 'b> {
⋮----
fn sign_only_arg<'a, 'b>(&self, arg: Arg<'a, 'b>) -> Arg<'a, 'b> {
⋮----
fn signer_arg<'a, 'b>(&self, arg: Arg<'a, 'b>) -> Arg<'a, 'b> {
⋮----
fn dump_transaction_message_arg<'a, 'b>(&self, arg: Arg<'a, 'b>) -> Arg<'a, 'b> {
⋮----
pub trait OfflineArgs {
⋮----
impl OfflineArgs for App<'_, '_> {
fn offline_args_config(self, config: &dyn ArgsConfig) -> Self {
self.arg(config.blockhash_arg(blockhash_arg()))
.arg(config.sign_only_arg(sign_only_arg()))
.arg(config.signer_arg(signer_arg()))
.arg(config.dump_transaction_message_arg(dump_transaction_message()))
⋮----
fn offline_args(self) -> Self {
struct NullArgsConfig {}
impl ArgsConfig for NullArgsConfig {}
self.offline_args_config(&NullArgsConfig {})

================
File: clap-utils/Cargo.toml
================
[package]
name = "solana-clap-utils"
description = "Solana utilities for the clap"
documentation = "https://docs.rs/solana-clap-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "solana_clap_utils"

[features]
agave-unstable-api = []

[dependencies]
chrono = { workspace = true, features = ["default"] }
clap = "2.33.0"
rpassword = { workspace = true }
solana-bls-signatures = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-derivation-path = { workspace = true }
solana-hash = { workspace = true }
solana-keypair = { workspace = true, features = ["seed-derivable"] }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-presigner = { workspace = true }
solana-pubkey = { workspace = true }
solana-remote-wallet = { workspace = true }
solana-seed-phrase = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
thiserror = { workspace = true }
tiny-bip39 = { workspace = true }
uriparse = { workspace = true }
url = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-system-interface = { workspace = true, features = ["bincode"] }
tempfile = { workspace = true }

================
File: clap-v3-utils/src/input_parsers/mod.rs
================
pub mod signer;
⋮----
pub fn values_of<T>(matches: &ArgMatches, name: &str) -> Option<Vec<T>>
⋮----
.values_of(name)
.map(|xs| xs.map(|x| x.parse::<T>().unwrap()).collect())
⋮----
pub fn value_of<T>(matches: &ArgMatches, name: &str) -> Option<T>
⋮----
.value_of(name)
.and_then(|value| value.parse::<T>().ok())
⋮----
pub fn unix_timestamp_from_rfc3339_datetime(
⋮----
matches.value_of(name).and_then(|value| {
⋮----
.ok()
.map(|date_time| date_time.timestamp())
⋮----
pub fn lamports_of_sol(matches: &ArgMatches, name: &str) -> Option<u64> {
matches.value_of(name).and_then(sol_str_to_lamports)
⋮----
pub fn cluster_type_of(matches: &ArgMatches, name: &str) -> Option<ClusterType> {
value_of(matches, name)
⋮----
pub fn commitment_of(matches: &ArgMatches, name: &str) -> Option<CommitmentConfig> {
⋮----
.map(|value| CommitmentConfig::from_str(value).unwrap_or_default())
⋮----
pub fn parse_url(arg: &str) -> Result<String, String> {
⋮----
.map_err(|err| err.to_string())
.and_then(|url| {
url.has_host()
.then_some(arg.to_string())
.ok_or("no host provided".to_string())
⋮----
pub fn parse_url_or_moniker(arg: &str) -> Result<String, String> {
parse_url(&normalize_to_url_if_moniker(arg))
⋮----
pub fn parse_pow2(arg: &str) -> Result<usize, String> {
⋮----
.map_err(|e| format!("Unable to parse, provided: {arg}, err: {e}"))
.and_then(|v| {
v.is_power_of_two()
.then_some(v)
.ok_or(format!("Must be a power of 2: {v}"))
⋮----
pub fn parse_percentage(arg: &str) -> Result<u8, String> {
⋮----
.map_err(|e| format!("Unable to parse input percentage, provided: {arg}, err: {e}"))
⋮----
(v <= 100).then_some(v).ok_or(format!(
⋮----
pub enum Amount {
⋮----
impl Amount {
pub fn parse(arg: &str) -> Result<Amount, String> {
⋮----
Ok(Amount::All)
⋮----
Self::parse_decimal(arg).or(Self::parse_raw(arg)
.map_err(|_| format!("Unable to parse input amount, provided: {arg}")))
⋮----
pub fn parse_decimal(arg: &str) -> Result<Amount, String> {
⋮----
.map(Amount::Decimal)
.map_err(|_| format!("Unable to parse input amount, provided: {arg}"))
⋮----
pub fn parse_raw(arg: &str) -> Result<Amount, String> {
⋮----
.map(Amount::Raw)
⋮----
pub fn parse_decimal_or_all(arg: &str) -> Result<Amount, String> {
⋮----
Self::parse_decimal(arg).map_err(|_| {
format!("Unable to parse input amount as float or 'ALL' keyword, provided: {arg}")
⋮----
pub fn to_raw_amount(&self, decimals: u8) -> Self {
⋮----
Amount::Raw((amount * 10_usize.pow(decimals as u32) as f64) as u64)
⋮----
pub fn sol_to_lamport(&self) -> Amount {
⋮----
self.to_raw_amount(NATIVE_SOL_DECIMALS)
⋮----
pub enum RawTokenAmount {
⋮----
pub fn parse_rfc3339_datetime(arg: &str) -> Result<String, String> {
⋮----
.map(|_| arg.to_string())
.map_err(|e| format!("{e}"))
⋮----
pub fn parse_derivation(arg: &str) -> Result<String, String> {
let value = arg.replace('\'', "");
let mut parts = value.split('/');
let account = parts.next().unwrap();
⋮----
.map_err(|e| format!("Unable to parse derivation, provided: {account}, err: {e}"))
.and_then(|_| {
if let Some(change) = parts.next() {
change.parse::<u32>().map_err(|e| {
format!("Unable to parse derivation, provided: {change}, err: {e}")
⋮----
Ok(0)
⋮----
Ok(arg.to_string())
⋮----
pub fn parse_structured_seed(arg: &str) -> Result<String, String> {
⋮----
.split_once(':')
.ok_or("Seed must contain ':' as delimiter")
.unwrap();
if prefix.is_empty() || value.is_empty() {
Err(String::from("Seed prefix or value is empty"))
⋮----
"string" | "pubkey" | "hex" | "u8" => Ok(arg.to_string()),
⋮----
let len = prefix.len();
⋮----
Err(format!("Wrong prefix length {len} {prefix}:{value}"))
⋮----
let type_size = &prefix[1..len.saturating_sub(2)];
let byte_order = &prefix[len.saturating_sub(2)..len];
⋮----
Err(format!("Wrong prefix sign {sign} {prefix}:{value}"))
⋮----
Err(format!(
⋮----
pub fn parse_derived_address_seed(arg: &str) -> Result<String, String> {
(arg.len() <= MAX_SEED_LEN)
⋮----
.ok_or(format!(
⋮----
pub fn keypair_of(matches: &ArgMatches, name: &str) -> Option<Keypair> {
if let Some(value) = matches.value_of(name) {
⋮----
let skip_validation = matches.is_present(SKIP_SEED_PHRASE_VALIDATION_ARG.name);
keypair_from_seed_phrase(name, skip_validation, true, None, true).ok()
⋮----
read_keypair_file(value).ok()
⋮----
pub fn keypairs_of(matches: &ArgMatches, name: &str) -> Option<Vec<Keypair>> {
matches.values_of(name).map(|values| {
⋮----
.filter_map(|value| {
⋮----
.collect()
⋮----
pub fn pubkey_of(matches: &ArgMatches, name: &str) -> Option<Pubkey> {
value_of(matches, name).or_else(|| keypair_of(matches, name).map(|keypair| keypair.pubkey()))
⋮----
pub fn pubkeys_of(matches: &ArgMatches, name: &str) -> Option<Vec<Pubkey>> {
⋮----
.map(|value| {
value.parse::<Pubkey>().unwrap_or_else(|_| {
read_keypair_file(value)
.expect("read_keypair_file failed")
.pubkey()
⋮----
mod tests {
⋮----
fn app<'ab>() -> Command<'ab> {
⋮----
.arg(
⋮----
.long("multiple")
.takes_value(true)
.action(ArgAction::Append)
.multiple_values(true),
⋮----
.arg(Arg::new("single").takes_value(true).long("single"))
.arg(Arg::new("unit").takes_value(true).long("unit"))
⋮----
fn test_values_of() {
let matches = app().get_matches_from(vec!["test", "--multiple", "50", "--multiple", "39"]);
assert_eq!(values_of(&matches, "multiple"), Some(vec![50, 39]));
assert_eq!(values_of::<u64>(&matches, "single"), None);
⋮----
let matches = app().get_matches_from(vec![
⋮----
assert_eq!(
⋮----
fn test_value_of() {
let matches = app().get_matches_from(vec!["test", "--single", "50"]);
assert_eq!(value_of(&matches, "single"), Some(50));
assert_eq!(value_of::<u64>(&matches, "multiple"), None);
⋮----
let matches = app().get_matches_from(vec!["test", "--single", &pubkey.to_string()]);
assert_eq!(value_of(&matches, "single"), Some(pubkey));
⋮----
fn test_parse_pubkey() {
let command = Command::new("test").arg(
⋮----
.long("pubkey")
⋮----
.value_parser(clap::value_parser!(Pubkey)),
⋮----
.clone()
.try_get_matches_from(vec!["test", "--pubkey", "11111111111111111111111111111111"])
⋮----
.try_get_matches_from(vec!["test", "--pubkey", "this_is_an_invalid_arg"])
.unwrap_err();
assert_eq!(matches_error.kind, clap::error::ErrorKind::ValueValidation);
⋮----
fn test_parse_hash() {
⋮----
.long("hash")
⋮----
.value_parser(clap::value_parser!(Hash)),
⋮----
.try_get_matches_from(vec!["test", "--hash", "11111111111111111111111111111111"])
⋮----
.try_get_matches_from(vec!["test", "--hash", "this_is_an_invalid_arg"])
⋮----
fn test_parse_token_decimal() {
⋮----
.long("amount")
⋮----
.value_parser(Amount::parse_decimal),
⋮----
.try_get_matches_from(vec!["test", "--amount", "11223344"])
⋮----
.try_get_matches_from(vec!["test", "--amount", "0.11223344"])
⋮----
.try_get_matches_from(vec!["test", "--amount", "this_is_an_invalid_arg"])
⋮----
.try_get_matches_from(vec!["test", "--amount", "all"])
⋮----
fn test_parse_token_decimal_or_all() {
⋮----
.value_parser(Amount::parse_decimal_or_all),
⋮----
.try_get_matches_from(vec!["test", "--amount", "ALL"])
⋮----
assert_eq!(*matches.get_one::<Amount>("amount").unwrap(), Amount::All,);
⋮----
fn test_sol_to_lamports() {
⋮----
let test_cases = vec![
⋮----
.try_get_matches_from(vec!["test", "--amount", arg])
⋮----
fn test_derivation() {
⋮----
.long("derivation")
⋮----
.value_parser(parse_derivation),
⋮----
let test_arguments = vec![
⋮----
.try_get_matches_from(vec!["test", "--derivation", arg])
⋮----
assert_eq!(matches.get_one::<String>("derivation").unwrap(), arg);
⋮----
fn test_unix_timestamp_from_rfc3339_datetime() {
⋮----
.long("timestamp")
⋮----
.value_parser(clap::value_parser!(UnixTimestamp)),
⋮----
.try_get_matches_from(vec!["test", "--timestamp", "1234"])
⋮----
.try_get_matches_from(vec!["test", "--timestamp", "this_is_an_invalid_arg"])
⋮----
fn test_cluster_type() {
⋮----
.long("cluster")
⋮----
.value_parser(clap::value_parser!(ClusterType)),
⋮----
.try_get_matches_from(vec!["test", "--cluster", "testnet"])
⋮----
.try_get_matches_from(vec!["test", "--cluster", "this_is_an_invalid_arg"])
⋮----
fn test_commitment_config() {
⋮----
.long("commitment")
⋮----
.value_parser(clap::value_parser!(CommitmentConfig)),
⋮----
.try_get_matches_from(vec!["test", "--commitment", "finalized"])
⋮----
.try_get_matches_from(vec!["test", "--commitment", "this_is_an_invalid_arg"])

================
File: clap-v3-utils/src/input_parsers/signer.rs
================
pub enum SignerSourceError {
⋮----
pub enum SignerSourceKind {
⋮----
fn as_ref(&self) -> &str {
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
let s: &str = self.as_ref();
write!(f, "{s}")
⋮----
pub struct SignerSource {
⋮----
impl SignerSource {
fn new(kind: SignerSourceKind) -> Self {
⋮----
fn new_legacy(kind: SignerSourceKind) -> Self {
⋮----
pub fn try_get_keypair(
⋮----
keypair_from_source(matches, source, name, true).map(Some)
⋮----
Ok(None)
⋮----
pub fn try_get_keypairs(
⋮----
.filter_map(|source| keypair_from_source(matches, source, name, true).ok())
.collect();
Ok(Some(keypairs))
⋮----
pub fn try_get_signer(
⋮----
let signer = signer_from_source(matches, source, name, wallet_manager)?;
let signer_pubkey = signer.pubkey();
Ok(Some((signer, signer_pubkey)))
⋮----
pub fn try_get_signers(
⋮----
.filter_map(|source| {
let signer = signer_from_source(matches, source, name, wallet_manager).ok()?;
⋮----
Some((signer, signer_pubkey))
⋮----
Ok(Some(signers))
⋮----
pub fn try_get_pubkey(
⋮----
pubkey_from_source(matches, source, name, wallet_manager).map(Some)
⋮----
pub fn try_get_pubkeys(
⋮----
.filter_map(|source| pubkey_from_source(matches, source, name, wallet_manager).ok())
⋮----
Ok(Some(pubkeys))
⋮----
pub fn try_resolve(
⋮----
resolve_signer_from_source(matches, source, name, wallet_manager)
⋮----
pub fn parse<S: AsRef<str>>(source: S) -> Result<Self, SignerSourceError> {
let source = source.as_ref();
⋮----
while let Some(trimmed) = source.strip_prefix('\'') {
source = if let Some(trimmed) = trimmed.strip_suffix('\'') {
⋮----
source.replace('\\', "/")
⋮----
source.to_string()
⋮----
match uriparse::URIReference::try_from(source.as_str()) {
Err(_) => Err(SignerSourceError::UnrecognizedSource),
⋮----
if let Some(scheme) = uri.scheme() {
let scheme = scheme.as_str().to_ascii_lowercase();
match scheme.as_str() {
SIGNER_SOURCE_PROMPT => Ok(SignerSource {
⋮----
SIGNER_SOURCE_FILEPATH => Ok(SignerSource::new(
SignerSourceKind::Filepath(uri.path().to_string()),
⋮----
SIGNER_SOURCE_USB => Ok(SignerSource {
⋮----
SIGNER_SOURCE_STDIN => Ok(SignerSource::new(SignerSourceKind::Stdin)),
⋮----
// On Windows, an absolute path's drive letter will be parsed as the URI
if scheme.len() == 1 {
return Ok(SignerSource::new(SignerSourceKind::Filepath(source)));
⋮----
Err(SignerSourceError::UnrecognizedSource)
⋮----
match source.as_str() {
STDOUT_OUTFILE_TOKEN => Ok(SignerSource::new(SignerSourceKind::Stdin)),
ASK_KEYWORD => Ok(SignerSource::new_legacy(SignerSourceKind::Prompt)),
_ => match Pubkey::from_str(source.as_str()) {
Ok(pubkey) => Ok(SignerSource::new(SignerSourceKind::Pubkey(pubkey))),
Err(_) => std::fs::metadata(source.as_str())
.map(|_| SignerSource::new(SignerSourceKind::Filepath(source)))
.map_err(|err| err.into()),
⋮----
pub struct SignerSourceParserBuilder {
⋮----
impl SignerSourceParserBuilder {
pub fn allow_all(mut self) -> Self {
⋮----
pub fn allow_prompt(mut self) -> Self {
⋮----
pub fn allow_file_path(mut self) -> Self {
⋮----
pub fn allow_usb(mut self) -> Self {
⋮----
pub fn allow_stdin(mut self) -> Self {
⋮----
pub fn allow_pubkey(mut self) -> Self {
⋮----
pub fn allow_legacy(mut self) -> Self {
⋮----
pub fn build(self) -> ValueParser {
⋮----
return Err(SignerSourceError::UnsupportedSource);
⋮----
SignerSourceKind::Prompt if self.allow_prompt => Ok(signer_source),
SignerSourceKind::Filepath(_) if self.allow_file_path => Ok(signer_source),
SignerSourceKind::Usb(_) if self.allow_usb => Ok(signer_source),
SignerSourceKind::Stdin if self.allow_stdin => Ok(signer_source),
SignerSourceKind::Pubkey(_) if self.allow_pubkey => Ok(signer_source),
_ => Err(SignerSourceError::UnsupportedSource),
⋮----
pub fn try_keypair_of(
⋮----
extract_keypair(matches, name, value)
⋮----
pub fn try_keypairs_of(
⋮----
Ok(matches.try_get_many::<String>(name)?.map(|values| {
⋮----
.filter_map(|value| extract_keypair(matches, name, value).ok().flatten())
.collect()
⋮----
fn extract_keypair(
⋮----
let skip_validation = matches.try_contains_id(SKIP_SEED_PHRASE_VALIDATION_ARG.name)?;
keypair_from_seed_phrase(name, skip_validation, true, None, true).map(Some)
⋮----
read_keypair_file(path).map(Some)
⋮----
pub fn try_pubkey_of(
⋮----
Ok(Some(*pubkey))
⋮----
Ok(try_keypair_of(matches, name)?.map(|keypair| keypair.pubkey()))
⋮----
pub fn try_pubkeys_of(
⋮----
let mut pubkeys = Vec::with_capacity(pubkey_strings.len());
⋮----
pubkeys.push(pubkey_string.parse::<Pubkey>()?);
⋮----
pub fn pubkeys_sigs_of(matches: &ArgMatches, name: &str) -> Option<Vec<(Pubkey, Signature)>> {
matches.values_of(name).map(|values| {
⋮----
.map(|pubkey_signer_string| {
let mut signer = pubkey_signer_string.split('=');
let key = Pubkey::from_str(signer.next().unwrap()).unwrap();
let sig = Signature::from_str(signer.next().unwrap()).unwrap();
⋮----
pub fn try_pubkeys_sigs_of(
⋮----
let mut pubkey_sig_pairs = Vec::with_capacity(pubkey_signer_strings.len());
⋮----
.split_once('=')
.ok_or("failed to parse `pubkey=signature` pair")?;
⋮----
pubkey_sig_pairs.push((pubkey, sig));
⋮----
Ok(Some(pubkey_sig_pairs))
⋮----
pub fn signer_of(
⋮----
let signer = signer_from_path(matches, location, name, wallet_manager)?;
⋮----
Ok((Some(signer), Some(signer_pubkey)))
⋮----
Ok((None, None))
⋮----
pub fn pubkey_of_signer(
⋮----
Ok(Some(pubkey_from_path(
⋮----
pub fn pubkeys_of_multiple_signers(
⋮----
let mut pubkeys: Vec<Pubkey> = vec![];
⋮----
pubkeys.push(pubkey_from_path(matches, signer, name, wallet_manager)?);
⋮----
pub fn resolve_signer(
⋮----
resolve_signer_from_path(
⋮----
matches.try_get_one::<String>(name)?.unwrap(),
⋮----
pub struct PubkeySignature {
⋮----
impl FromStr for PubkeySignature {
type Err = String;
fn from_str(s: &str) -> Result<Self, Self::Err> {
let mut signer = s.split('=');
⋮----
.next()
.ok_or_else(|| String::from("Malformed signer string"))?;
let pubkey = Pubkey::from_str(pubkey).map_err(|err| format!("{err}"))?;
⋮----
let signature = Signature::from_str(signature).map_err(|err| format!("{err}"))?;
Ok(Self { pubkey, signature })
⋮----
mod tests {
⋮----
fn test_parse_signer_source() {
assert_matches!(
⋮----
let stdin = "stdin:".to_string();
⋮----
assert!(
⋮----
let file0 = NamedTempFile::new().unwrap();
let path = file0.path();
assert!(path.is_absolute());
let absolute_path_str = path.to_str().unwrap();
let file1 = NamedTempFile::new_in(std::env::current_dir().unwrap()).unwrap();
let path = file1.path().file_name().unwrap().to_str().unwrap();
⋮----
assert!(path.is_relative());
let relative_path_str = path.to_str().unwrap();
⋮----
let usb = "usb://ledger".to_string();
⋮----
assert_matches!(SignerSource::parse(usb).unwrap(), SignerSource {
⋮----
let usb = "usb://ledger?key=0/0".to_string();
⋮----
let expected_derivation_path = Some(DerivationPath::new_bip44(Some(0), Some(0)));
⋮----
let junk = "sometextthatisnotapubkeyorfile".to_string();
assert!(Pubkey::from_str(&junk).is_err());
⋮----
let prompt = "prompt:".to_string();
⋮----
fn app<'ab>() -> Command<'ab> {
⋮----
.arg(
⋮----
.long("multiple")
.takes_value(true)
.action(ArgAction::Append)
.multiple_values(true),
⋮----
.arg(Arg::new("single").takes_value(true).long("single"))
.arg(Arg::new("unit").takes_value(true).long("unit"))
⋮----
fn tmp_file_path(name: &str, pubkey: &Pubkey) -> String {
use std::env;
let out_dir = env::var("FARF_DIR").unwrap_or_else(|_| "farf".to_string());
format!("{out_dir}/tmp/{name}-{pubkey}")
⋮----
fn test_keypair_of() {
⋮----
let outfile = tmp_file_path("test_keypair_of.json", &keypair.pubkey());
let _ = write_keypair_file(&keypair, &outfile).unwrap();
let matches = app().get_matches_from(vec!["test", "--single", &outfile]);
assert_eq!(
⋮----
assert!(keypair_of(&matches, "multiple").is_none());
let matches = app().get_matches_from(vec!["test", "--single", "random_keypair_file.json"]);
assert!(keypair_of(&matches, "single").is_none());
fs::remove_file(&outfile).unwrap();
⋮----
fn test_pubkey_of() {
⋮----
let outfile = tmp_file_path("test_pubkey_of.json", &keypair.pubkey());
⋮----
assert_eq!(pubkey_of(&matches, "single"), Some(keypair.pubkey()));
assert_eq!(pubkey_of(&matches, "multiple"), None);
⋮----
app().get_matches_from(vec!["test", "--single", &keypair.pubkey().to_string()]);
⋮----
assert_eq!(pubkey_of(&matches, "single"), None);
⋮----
fn test_pubkeys_of() {
⋮----
let outfile = tmp_file_path("test_pubkeys_of.json", &keypair.pubkey());
⋮----
let matches = app().get_matches_from(vec![
⋮----
fn test_pubkeys_sigs_of() {
⋮----
let sig1 = Keypair::new().sign_message(&[0u8]);
let sig2 = Keypair::new().sign_message(&[1u8]);
let signer1 = format!("{key1}={sig1}");
let signer2 = format!("{key2}={sig2}");
⋮----
app().get_matches_from(vec!["test", "--multiple", &signer1, "--multiple", &signer2]);
⋮----
fn test_parse_pubkey_signature() {
let command = Command::new("test").arg(
⋮----
.long("pubkeysig")
⋮----
.value_parser(clap::value_parser!(PubkeySignature)),
⋮----
.clone()
.try_get_matches_from(vec![
⋮----
.unwrap();
⋮----
pubkey: Pubkey::from_str("11111111111111111111111111111111").unwrap(),
signature: Signature::from_str("4TpFuec1u4BZfxgHg2VQXwvBHANZuNSJHmgrU34GViLAM5uYZ8t7uuhWMHN4k9r41B2p9mwnHjPGwTmTxyvCZw63").unwrap(),
⋮----
.try_get_matches_from(vec!["test", "--pubkeysig", "this_is_an_invalid_arg"])
.unwrap_err();
assert_eq!(matches_error.kind, clap::error::ErrorKind::ValueValidation);
⋮----
fn test_parse_keypair_source() {
⋮----
.long("keypair")
⋮----
.value_parser(
⋮----
.allow_file_path()
.build(),
⋮----
let path_str = path.to_str().unwrap();
⋮----
.try_get_matches_from(vec!["test", "--keypair", path_str])
⋮----
let signer_source = matches.get_one::<SignerSource>("keypair").unwrap();
assert!(matches!(signer_source, SignerSource {
⋮----
.try_get_matches_from(vec!["test", "--keypair", "-"])
⋮----
.try_get_matches_from(vec!["test", "--keypair", "usb://ledger"])
⋮----
fn test_parse_keypair_or_ask_keyword_source() {
⋮----
.allow_prompt()
.allow_legacy()
⋮----
.try_get_matches_from(vec!["test", "--keypair", "ASK"])
⋮----
.try_get_matches_from(vec!["test", "--keypair", "prompt:"])
⋮----
fn test_parse_prompt_signer_source() {
⋮----
fn test_parse_pubkey_or_keypair_signer_source() {
⋮----
.long("signer")
⋮----
.allow_pubkey()
⋮----
.try_get_matches_from(vec!["test", "--signer", &pubkey.to_string()])
⋮----
let signer_source = matches.get_one::<SignerSource>("signer").unwrap();
assert!(matches!(
⋮----
.try_get_matches_from(vec!["test", "--signer", path_str])
⋮----
.try_get_matches_from(vec!["test", "--signer", "-"])
⋮----
.try_get_matches_from(vec!["test", "--signer", "usb://ledger"])

================
File: clap-v3-utils/src/keygen/derivation_path.rs
================
pub fn derivation_path_arg<'a>() -> Arg<'a> {
⋮----
.long("derivation-path")
.value_name("DERIVATION_PATH")
.takes_value(true)
.min_values(0)
.max_values(1)
.help(
⋮----
pub fn acquire_derivation_path(
⋮----
if matches.try_contains_id("derivation_path")? {
Ok(Some(DerivationPath::from_absolute_path_str(
⋮----
.map(|path| path.as_str())
.unwrap_or(DEFAULT_DERIVATION_PATH),
⋮----
Ok(None)

================
File: clap-v3-utils/src/keygen/mnemonic.rs
================
pub fn word_count_arg<'a>() -> Arg<'a> {
⋮----
.long(WORD_COUNT_ARG.long)
.value_parser(PossibleValuesParser::new(POSSIBLE_WORD_COUNTS))
.default_value("12")
.value_name("NUMBER")
.takes_value(true)
.help(WORD_COUNT_ARG.help)
⋮----
pub fn try_get_word_count(matches: &ArgMatches) -> Result<Option<usize>, Box<dyn error::Error>> {
Ok(matches
⋮----
.map(|count| match count.as_str() {
⋮----
_ => unreachable!(),
⋮----
pub fn language_arg<'a>() -> Arg<'a> {
⋮----
.long(LANGUAGE_ARG.long)
.value_parser(PossibleValuesParser::new(POSSIBLE_LANGUAGES))
.default_value("english")
.value_name("LANGUAGE")
⋮----
.help(LANGUAGE_ARG.help)
⋮----
pub fn no_passphrase_arg<'a>() -> Arg<'a> {
⋮----
.long(NO_PASSPHRASE_ARG.long)
.alias("no-passphrase")
.help(NO_PASSPHRASE_ARG.help)
⋮----
pub fn acquire_language(matches: &ArgMatches) -> Language {
⋮----
match matches.get_one::<String>(language_name).unwrap().as_str() {
⋮----
pub fn try_get_language(matches: &ArgMatches) -> Result<Option<Language>, Box<dyn error::Error>> {
⋮----
.map(|language| match language.as_str() {
⋮----
pub fn no_passphrase_and_message() -> (String, String) {
(NO_PASSPHRASE.to_string(), "".to_string())
⋮----
pub fn acquire_passphrase_and_message(
⋮----
if matches.try_contains_id(NO_PASSPHRASE_ARG.name)? {
Ok(no_passphrase_and_message())
⋮----
match prompt_passphrase(PROMPT) {
⋮----
println!();
Ok((passphrase, " and your BIP39 passphrase".to_string()))
⋮----
Err(e) => Err(e),

================
File: clap-v3-utils/src/keygen/mod.rs
================
pub mod derivation_path;
pub mod mnemonic;
⋮----
pub fn no_outfile_arg<'a>() -> Arg<'a> {
⋮----
.long(NO_OUTFILE_ARG.long)
.help(NO_OUTFILE_ARG.help)
⋮----
pub trait KeyGenerationCommonArgs {
⋮----
impl KeyGenerationCommonArgs for Command<'_> {
fn key_generation_common_args(self) -> Self {
self.arg(word_count_arg())
.arg(language_arg())
.arg(no_passphrase_arg())
⋮----
pub fn check_for_overwrite(
⋮----
let force = matches.try_contains_id("force")?;
if !force && Path::new(outfile).exists() {
let err_msg = format!("Refusing to overwrite {outfile} without --force flag");
return Err(err_msg.into());
⋮----
Ok(())

================
File: clap-v3-utils/src/compute_budget.rs
================
pub fn compute_unit_price_arg<'a>() -> Arg<'a> {
⋮----
.long(COMPUTE_UNIT_PRICE_ARG.long)
.takes_value(true)
.value_name("COMPUTE-UNIT-PRICE")
.value_parser(value_parser!(u64))
.help(COMPUTE_UNIT_PRICE_ARG.help)
⋮----
pub fn compute_unit_limit_arg<'a>() -> Arg<'a> {
⋮----
.long(COMPUTE_UNIT_LIMIT_ARG.long)
⋮----
.value_name("COMPUTE-UNIT-LIMIT")
.value_parser(value_parser!(u32))
.help(COMPUTE_UNIT_LIMIT_ARG.help)

================
File: clap-v3-utils/src/fee_payer.rs
================
pub fn fee_payer_arg<'a>() -> Arg<'a> {
⋮----
.long(FEE_PAYER_ARG.long)
.takes_value(true)
.value_name("KEYPAIR")
.validator(|s| input_validators::is_valid_signer(s))
.help(FEE_PAYER_ARG.help)

================
File: clap-v3-utils/src/input_validators.rs
================
fn is_parsable_generic<U, T>(string: T) -> Result<(), String>
⋮----
.as_ref()
⋮----
.map(|_| ())
.map_err(|err| format!("error parsing '{string}': {err}"))
⋮----
pub fn is_parsable<T>(string: &str) -> Result<(), String>
⋮----
pub fn is_within_range<T, R>(string: String, range: R) -> Result<(), String>
⋮----
if !range.contains(&input) {
Err(format!("input '{input:?}' out of range {range:?}"))
⋮----
Ok(())
⋮----
Err(err) => Err(format!("error parsing '{string}': {err}")),
⋮----
pub fn is_pubkey(string: &str) -> Result<(), String> {
⋮----
pub fn is_hash<T>(string: T) -> Result<(), String>
⋮----
pub fn is_keypair<T>(string: T) -> Result<(), String>
⋮----
read_keypair_file(string.as_ref())
⋮----
.map_err(|err| format!("{err}"))
⋮----
pub fn is_keypair_or_ask_keyword<T>(string: T) -> Result<(), String>
⋮----
if string.as_ref() == ASK_KEYWORD {
return Ok(());
⋮----
pub fn is_prompt_signer_source(string: &str) -> Result<(), String> {
⋮----
.map_err(|err| format!("{err}"))?
⋮----
SignerSourceKind::Prompt => Ok(()),
_ => Err(format!(
⋮----
pub fn is_pubkey_or_keypair<T>(string: T) -> Result<(), String>
⋮----
is_pubkey(string.as_ref()).or_else(|_| is_keypair(string))
⋮----
pub fn is_valid_pubkey<T>(string: T) -> Result<(), String>
⋮----
match SignerSource::parse(string.as_ref())
⋮----
SignerSourceKind::Filepath(path) => is_keypair(path),
_ => Ok(()),
⋮----
pub fn is_valid_signer<T>(string: T) -> Result<(), String>
⋮----
is_valid_pubkey(string)
⋮----
pub fn is_pubkey_sig<T>(string: T) -> Result<(), String>
⋮----
let mut signer = string.as_ref().split('=');
⋮----
.next()
.ok_or_else(|| "Malformed signer string".to_string())?,
⋮----
Ok(_) => Ok(()),
Err(err) => Err(format!("{err}")),
⋮----
pub fn is_url<T>(string: T) -> Result<(), String>
⋮----
match url::Url::parse(string.as_ref()) {
⋮----
if url.has_host() {
⋮----
Err("no host provided".to_string())
⋮----
pub fn is_url_or_moniker<T>(string: T) -> Result<(), String>
⋮----
let normalized = normalize_to_url_if_moniker(string.as_ref());
⋮----
pub fn normalize_to_url_if_moniker<T: AsRef<str>>(url_or_moniker: T) -> String {
match url_or_moniker.as_ref() {
⋮----
.to_string()
⋮----
pub fn is_epoch<T>(epoch: T) -> Result<(), String>
⋮----
pub fn is_slot<T>(slot: T) -> Result<(), String>
⋮----
pub fn is_pow2<T>(bins: T) -> Result<(), String>
⋮----
bins.as_ref()
⋮----
.map_err(|e| format!("Unable to parse, provided: {bins}, err: {e}"))
.and_then(|v| {
if !v.is_power_of_two() {
Err(format!("Must be a power of 2: {v}"))
⋮----
pub fn is_port<T>(port: T) -> Result<(), String>
⋮----
pub fn is_valid_percentage<T>(percentage: T) -> Result<(), String>
⋮----
.map_err(|e| format!("Unable to parse input percentage, provided: {percentage}, err: {e}"))
⋮----
Err(format!(
⋮----
pub fn is_amount<T>(amount: T) -> Result<(), String>
⋮----
if amount.as_ref().parse::<u64>().is_ok() || amount.as_ref().parse::<f64>().is_ok() {
⋮----
pub fn is_amount_or_all<T>(amount: T) -> Result<(), String>
⋮----
if amount.as_ref().parse::<u64>().is_ok()
|| amount.as_ref().parse::<f64>().is_ok()
|| amount.as_ref() == "ALL"
⋮----
pub fn is_rfc3339_datetime<T>(value: T) -> Result<(), String>
⋮----
DateTime::parse_from_rfc3339(value.as_ref())
⋮----
.map_err(|e| format!("{e}"))
⋮----
pub fn is_derivation<T>(value: T) -> Result<(), String>
⋮----
let value = value.as_ref().replace('\'', "");
let mut parts = value.split('/');
let account = parts.next().unwrap();
⋮----
.map_err(|e| format!("Unable to parse derivation, provided: {account}, err: {e}"))
.and_then(|_| {
if let Some(change) = parts.next() {
change.parse::<u32>().map_err(|e| {
format!("Unable to parse derivation, provided: {change}, err: {e}")
⋮----
Ok(0)
⋮----
pub fn is_structured_seed<T>(value: T) -> Result<(), String>
⋮----
.split_once(':')
.ok_or("Seed must contain ':' as delimiter")
.unwrap();
if prefix.is_empty() || value.is_empty() {
Err(String::from("Seed prefix or value is empty"))
⋮----
"string" | "pubkey" | "hex" | "u8" => Ok(()),
⋮----
let len = prefix.len();
⋮----
Err(format!("Wrong prefix length {len} {prefix}:{value}"))
⋮----
let type_size = &prefix[1..len.saturating_sub(2)];
let byte_order = &prefix[len.saturating_sub(2)..len];
⋮----
Err(format!("Wrong prefix sign {sign} {prefix}:{value}"))
⋮----
pub fn is_derived_address_seed<T>(value: T) -> Result<(), String>
⋮----
let value = value.as_ref();
if value.len() > MAX_SEED_LEN {

================
File: clap-v3-utils/src/keypair.rs
================
pub struct SignOnly {
⋮----
impl SignOnly {
pub fn has_all_signers(&self) -> bool {
self.absent_signers.is_empty() && self.bad_signers.is_empty()
⋮----
pub fn presigner_of(&self, pubkey: &Pubkey) -> Option<Presigner> {
presigner_from_pubkey_sigs(pubkey, &self.present_signers)
⋮----
pub type CliSigners = Vec<Box<dyn Signer>>;
pub type SignerIndex = usize;
pub struct CliSignerInfo {
⋮----
impl CliSignerInfo {
pub fn index_of(&self, pubkey: Option<Pubkey>) -> Option<usize> {
⋮----
.iter()
.position(|signer| signer.pubkey() == pubkey)
⋮----
Some(0)
⋮----
pub fn index_of_or_none(&self, pubkey: Option<Pubkey>) -> Option<usize> {
⋮----
pub fn signers_for_message(&self, message: &Message) -> Vec<&dyn Signer> {
⋮----
.filter_map(|k| {
if message.signer_keys().contains(&&k.pubkey()) {
Some(k.as_ref())
⋮----
.collect()
⋮----
pub struct DefaultSigner {
⋮----
impl DefaultSigner {
pub fn new<AN: AsRef<str>, P: AsRef<str>>(arg_name: AN, path: P) -> Self {
let arg_name = arg_name.as_ref().to_string();
let path = path.as_ref().to_string();
⋮----
fn path(&self) -> Result<&str, Box<dyn std::error::Error>> {
if !self.is_path_checked.borrow().deref() {
⋮----
.and_then(|s| {
⋮----
std::fs::metadata(path).map(|_| ()).map_err(|e| e.into())
⋮----
Ok(())
⋮----
.map_err(|_| {
std::io::Error::other(format!(
⋮----
*self.is_path_checked.borrow_mut() = true;
⋮----
Ok(&self.path)
⋮----
pub fn generate_unique_signers(
⋮----
let mut unique_signers = vec![];
if bulk_signers.iter().any(|signer| signer.is_none()) {
let default_signer = self.signer_from_path(matches, wallet_manager)?;
unique_signers.push(default_signer);
⋮----
for signer in bulk_signers.into_iter().flatten() {
if !unique_signers.iter().any(|s| s == &signer) {
unique_signers.push(signer);
⋮----
Ok(CliSignerInfo {
⋮----
pub fn signer_from_path(
⋮----
signer_from_path(matches, self.path()?, &self.arg_name, wallet_manager)
⋮----
pub fn signer_from_path_with_config(
⋮----
signer_from_path_with_config(
⋮----
self.path()?,
⋮----
pub fn presigner_from_pubkey_sigs(
⋮----
signers.iter().find_map(|(signer, sig)| {
⋮----
Some(Presigner::new(signer, sig))
⋮----
pub struct SignerFromPathConfig {
⋮----
signer_from_path_with_config(matches, path, keypair_name, wallet_manager, &config)
⋮----
pub fn signer_from_source(
⋮----
signer_from_source_with_config(matches, source, keypair_name, wallet_manager, &config)
⋮----
signer_from_source_with_config(matches, &source, keypair_name, wallet_manager, config)
⋮----
pub fn signer_from_source_with_config(
⋮----
let skip_validation = matches.try_contains_id(SKIP_SEED_PHRASE_VALIDATION_ARG.name)?;
Ok(Box::new(keypair_from_seed_phrase(
⋮----
derivation_path.clone(),
⋮----
SignerSourceKind::Filepath(path) => match read_keypair_file(path) {
Err(e) => Err(std::io::Error::other(format!(
⋮----
.into()),
Ok(file) => Ok(Box::new(file)),
⋮----
Ok(Box::new(read_keypair(&mut stdin)?))
⋮----
if wallet_manager.is_none() {
*wallet_manager = maybe_wallet_manager()?;
⋮----
let confirm_key = matches.try_contains_id("confirm_key").unwrap_or(false);
Ok(Box::new(generate_remote_keypair(
locator.clone(),
derivation_path.clone().unwrap_or_default(),
⋮----
Err(RemoteWalletError::NoDeviceFound.into())
⋮----
let presigner = try_pubkeys_sigs_of(matches, SIGNER_ARG.name)
.ok()
.flatten()
.as_ref()
.and_then(|presigners| presigner_from_pubkey_sigs(pubkey, presigners));
⋮----
Ok(Box::new(presigner))
⋮----
|| matches.try_contains_id(SIGN_ONLY_ARG.name).unwrap_or(false)
⋮----
Ok(Box::new(NullSigner::new(pubkey)))
⋮----
Err(std::io::Error::other(format!(
⋮----
.into())
⋮----
pub fn pubkey_from_path(
⋮----
pubkey_from_source(matches, &source, keypair_name, wallet_manager)
⋮----
pub fn pubkey_from_source(
⋮----
SignerSourceKind::Pubkey(pubkey) => Ok(pubkey),
_ => Ok(signer_from_source(matches, source, keypair_name, wallet_manager)?.pubkey()),
⋮----
pub fn resolve_signer_from_path(
⋮----
resolve_signer_from_source(matches, &source, keypair_name, wallet_manager)
⋮----
pub fn resolve_signer_from_source(
⋮----
keypair_from_seed_phrase(
⋮----
.map(|_| None)
⋮----
Ok(_) => Ok(Some(path.to_string())),
⋮----
read_keypair(&mut stdin).map(|_| None)
⋮----
let path = generate_remote_keypair(
⋮----
.map(|keypair| keypair.path)?;
Ok(Some(path))
⋮----
SignerSourceKind::Pubkey(pubkey) => Ok(Some(pubkey.to_string())),
⋮----
/// Prompts user for a passphrase and then asks for confirmirmation to check for mistakes
pub fn prompt_passphrase(prompt: &str) -> Result<String, Box<dyn error::Error>> {
⋮----
pub fn prompt_passphrase(prompt: &str) -> Result<String, Box<dyn error::Error>> {
let passphrase = prompt_password(prompt)?;
if !passphrase.is_empty() {
⋮----
return Err("Passphrases did not match".into());
⋮----
Ok(passphrase)
⋮----
/// Loads a [Keypair] from one of several possible sources.
///
⋮----
///
/// The `path` is not strictly a file system path, but is interpreted as various
⋮----
/// The `path` is not strictly a file system path, but is interpreted as various
/// types of _signing source_, depending on its format, one of which is a path
⋮----
/// types of _signing source_, depending on its format, one of which is a path
/// to a keypair file. Some sources may require user interaction in the course
⋮----
/// to a keypair file. Some sources may require user interaction in the course
/// of calling this function.
⋮----
/// of calling this function.
///
⋮----
///
/// This is the same as [`signer_from_path`] except that it only supports
⋮----
/// This is the same as [`signer_from_path`] except that it only supports
/// signing sources that can result in a [Keypair]: prompt for seed phrase,
⋮----
/// signing sources that can result in a [Keypair]: prompt for seed phrase,
/// keypair file, and stdin.
⋮----
/// keypair file, and stdin.
///
⋮----
///
/// If `confirm_pubkey` is `true` then after deriving the pubkey, the user will
⋮----
/// If `confirm_pubkey` is `true` then after deriving the pubkey, the user will
/// be prompted to confirm that the pubkey is as expected.
⋮----
/// be prompted to confirm that the pubkey is as expected.
///
⋮----
///
/// See [`signer_from_path`] for full documentation of how this function
⋮----
/// See [`signer_from_path`] for full documentation of how this function
/// interprets its arguments.
⋮----
/// interprets its arguments.
///
⋮----
///
/// # Examples
⋮----
/// # Examples
///
⋮----
///
/// ```no_run
⋮----
/// ```no_run
/// use clap::{Arg, Command};
⋮----
/// use clap::{Arg, Command};
/// use solana_clap_v3_utils::keypair::keypair_from_path;
⋮----
/// use solana_clap_v3_utils::keypair::keypair_from_path;
///
⋮----
///
/// let clap_app = Command::new("my-program")
⋮----
/// let clap_app = Command::new("my-program")
///     // The argument we'll parse as a signer "path"
⋮----
///     // The argument we'll parse as a signer "path"
pub fn keypair_from_path(
⋮----
pub fn keypair_from_path(
⋮----
let keypair = encodable_key_from_path(path, keypair_name, skip_validation)?;
⋮----
confirm_encodable_keypair_pubkey(&keypair, "pubkey");
⋮----
Ok(keypair)
⋮----
pub fn keypair_from_source(
⋮----
let keypair = encodable_key_from_source(source, keypair_name, skip_validation)?;
⋮----
pub fn elgamal_keypair_from_path(
⋮----
let elgamal_keypair = encodable_key_from_path(path, elgamal_keypair_name, skip_validation)?;
⋮----
confirm_encodable_keypair_pubkey(&elgamal_keypair, "ElGamal pubkey");
⋮----
Ok(elgamal_keypair)
⋮----
pub fn elgamal_keypair_from_source(
⋮----
let elgamal_keypair = encodable_key_from_source(source, elgamal_keypair_name, skip_validation)?;
⋮----
fn confirm_encodable_keypair_pubkey<K: EncodableKeypair>(keypair: &K, pubkey_label: &str) {
let pubkey = keypair.encodable_pubkey().to_string();
println!("Recovered {pubkey_label} `{pubkey:?}`. Continue? (y/n): ");
let _ignored = stdout().flush();
⋮----
stdin().read_line(&mut input).expect("Unexpected input");
if input.to_lowercase().trim() != "y" {
println!("Exiting");
exit(1);
⋮----
pub fn ae_key_from_path(
⋮----
encodable_key_from_path(path, key_name, skip_validation)
⋮----
pub fn ae_key_from_source(
⋮----
encodable_key_from_source(source, key_name, skip_validation)
⋮----
fn encodable_key_from_path<K: EncodableKey + SeedDerivable>(
⋮----
encodable_key_from_source(&source, keypair_name, skip_validation)
⋮----
fn encodable_key_from_source<K: EncodableKey + SeedDerivable>(
⋮----
SignerSourceKind::Prompt => Ok(encodable_key_from_seed_phrase(
⋮----
Ok(file) => Ok(file),
⋮----
Ok(K::read(&mut stdin)?)
⋮----
_ => Err(std::io::Error::other(format!(
⋮----
pub fn keypair_from_seed_phrase(
⋮----
encodable_key_from_seed_phrase(keypair_name, skip_validation, derivation_path, legacy)?;
⋮----
pub fn elgamal_keypair_from_seed_phrase(
⋮----
let elgamal_keypair: ElGamalKeypair = encodable_key_from_seed_phrase(
⋮----
pub fn ae_key_from_seed_phrase(
⋮----
encodable_key_from_seed_phrase(keypair_name, skip_validation, derivation_path, legacy)
⋮----
fn encodable_key_from_seed_phrase<K: EncodableKey + SeedDerivable>(
⋮----
let seed_phrase = prompt_password(format!("[{key_name}] seed phrase: "))?;
let seed_phrase = seed_phrase.trim();
let passphrase_prompt = format!(
⋮----
let passphrase = prompt_passphrase(&passphrase_prompt)?;
⋮----
let seed = generate_seed_from_seed_phrase_and_passphrase(seed_phrase, &passphrase);
⋮----
let sanitized = sanitize_seed_phrase(seed_phrase);
⋮----
return Ok(mnemonic);
⋮----
Err("Can't get mnemonic from seed phrases")
⋮----
let mnemonic = parse_language_fn()?;
⋮----
K::from_seed(seed.as_bytes())?
⋮----
K::from_seed_and_derivation_path(seed.as_bytes(), derivation_path)?
⋮----
Ok(key)
⋮----
fn sanitize_seed_phrase(seed_phrase: &str) -> String {
⋮----
.split_whitespace()
⋮----
.join(" ")
⋮----
mod tests {
⋮----
fn test_sanitize_seed_phrase() {
⋮----
assert_eq!(
⋮----
fn test_signer_info_signers_for_message() {
⋮----
&[transfer(&source.pubkey(), &recipient, 42)],
Some(&fee_payer.pubkey()),
⋮----
let signers = vec![
⋮----
let msg_signers = signer_info.signers_for_message(&message);
let signer_pubkeys = msg_signers.iter().map(|s| s.pubkey()).collect::<Vec<_>>();
let expect = vec![
⋮----
assert_eq!(signer_pubkeys, expect);
⋮----
fn signer_from_path_with_file() -> Result<(), Box<dyn std::error::Error>> {
⋮----
let dir = dir.path();
let keypair_path = dir.join("id.json");
let keypair_path_str = keypair_path.to_str().expect("utf-8");
⋮----
write_keypair_file(&keypair, &keypair_path)?;
let args = vec!["program", keypair_path_str];
⋮----
.arg(
⋮----
.required(true)
.help("The signing keypair"),
⋮----
.offline_args();
let clap_matches = clap_app.get_matches_from(args);
let keypair_str: String = clap_matches.value_of_t_or_exit("keypair");
let wallet_manager = initialize_wallet_manager()?;
let signer = signer_from_path(
⋮----
&mut Some(wallet_manager),
⋮----
assert_eq!(keypair.pubkey(), signer.pubkey());
⋮----
fn test_signer_from_source_can_parse_null_signer() {
⋮----
let matches = clap_app.get_matches_from(Vec::<&str>::new());
⋮----
signer_from_source_with_config(&matches, &source, "test_key", &mut None, &config)
.unwrap();
assert_eq!(signer.pubkey(), pubkey);
⋮----
fn test_signer_from_source_pubkey_error_on_missing_sig() {
⋮----
signer_from_source_with_config(&matches, &source, "test_key", &mut None, &config);
assert!(result.is_err());
let err_string = result.err().unwrap().to_string();
assert!(err_string.contains(&format!("missing signature for supplied pubkey: {pubkey}")));
⋮----
fn test_signer_from_source_pubkey_presigner_match() {
⋮----
let pubkey = keypair.pubkey();
⋮----
let signature = keypair.sign_message(message);
⋮----
let signer_arg = format!("{pubkey}={signature}");
let clap_app = Command::new("test").arg(
⋮----
.long(SIGNER_ARG.long)
.value_name("PUBKEY=SIGNATURE"),
⋮----
let matches = clap_app.get_matches_from(vec!["test", "--signer", &signer_arg]);
⋮----
let signer = signer_from_source_with_config(
⋮----
fn test_signer_from_source_pubkey_presigner_no_match() {
⋮----
let result = signer_from_source_with_config(
⋮----
assert!(err_string.contains(&format!(
⋮----
fn test_signer_from_source_pubkey_sign_only_match() {
⋮----
let clap_app = Command::new("test_sign_only_match").offline_args();
let matches = clap_app.get_matches_from(vec![

================
File: clap-v3-utils/src/lib.rs
================
use thiserror::Error;
pub struct ArgConstant<'a> {
⋮----
pub struct DisplayError(Box<dyn std::error::Error>);
impl DisplayError {
pub fn new_as_boxed(inner: Box<dyn std::error::Error>) -> Box<Self> {
DisplayError(inner).into()
⋮----
fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
write!(fmt, "{}", self.0)
⋮----
pub mod compute_budget;
pub mod fee_payer;
pub mod input_parsers;
pub mod input_validators;
pub mod keygen;
pub mod keypair;
pub mod memo;
pub mod nonce;
pub mod offline;

================
File: clap-v3-utils/src/memo.rs
================
pub fn memo_arg<'a>() -> Arg<'a> {
⋮----
.long(MEMO_ARG.long)
.takes_value(true)
.value_name("MEMO")
.help(MEMO_ARG.help)

================
File: clap-v3-utils/src/nonce.rs
================
fn nonce_arg<'a>() -> Arg<'a> {
⋮----
.long(NONCE_ARG.long)
.takes_value(true)
.value_name("PUBKEY")
.validator(|s| is_valid_pubkey(s))
.help(NONCE_ARG.help)
⋮----
pub fn nonce_authority_arg<'a>() -> Arg<'a> {
⋮----
.long(NONCE_AUTHORITY_ARG.long)
⋮----
.value_name("KEYPAIR")
.validator(|s| is_valid_signer(s))
.help(NONCE_AUTHORITY_ARG.help)
⋮----
pub trait NonceArgs {
⋮----
impl NonceArgs for Command<'_> {
fn nonce_args(self, global: bool) -> Self {
self.arg(nonce_arg().global(global)).arg(
nonce_authority_arg()
.requires(NONCE_ARG.name)
.global(global),

================
File: clap-v3-utils/src/offline.rs
================
pub fn blockhash_arg<'a>() -> Arg<'a> {
⋮----
.long(BLOCKHASH_ARG.long)
.takes_value(true)
.value_name("BLOCKHASH")
.value_parser(value_parser!(Hash))
.help(BLOCKHASH_ARG.help)
⋮----
pub fn sign_only_arg<'a>() -> Arg<'a> {
⋮----
.long(SIGN_ONLY_ARG.long)
.takes_value(false)
.requires(BLOCKHASH_ARG.name)
.help(SIGN_ONLY_ARG.help)
⋮----
fn signer_arg<'a>() -> Arg<'a> {
⋮----
.long(SIGNER_ARG.long)
⋮----
.value_name("PUBKEY=SIGNATURE")
.value_parser(value_parser!(PubkeySignature))
⋮----
.action(ArgAction::Append)
.multiple_values(false)
.help(SIGNER_ARG.help)
⋮----
pub fn dump_transaction_message<'a>() -> Arg<'a> {
⋮----
.long(DUMP_TRANSACTION_MESSAGE.long)
⋮----
.requires(SIGN_ONLY_ARG.name)
.help(DUMP_TRANSACTION_MESSAGE.help)
⋮----
pub trait ArgsConfig {
fn blockhash_arg<'a>(&self, arg: Arg<'a>) -> Arg<'a> {
⋮----
fn sign_only_arg<'a>(&self, arg: Arg<'a>) -> Arg<'a> {
⋮----
fn signer_arg<'a>(&self, arg: Arg<'a>) -> Arg<'a> {
⋮----
fn dump_transaction_message_arg<'a>(&self, arg: Arg<'a>) -> Arg<'a> {
⋮----
pub trait OfflineArgs {
⋮----
impl OfflineArgs for Command<'_> {
fn offline_args_config(self, config: &dyn ArgsConfig) -> Self {
self.arg(config.blockhash_arg(blockhash_arg()))
.arg(config.sign_only_arg(sign_only_arg()))
.arg(config.signer_arg(signer_arg()))
.arg(config.dump_transaction_message_arg(dump_transaction_message()))
⋮----
fn offline_args(self) -> Self {
struct NullArgsConfig {}
impl ArgsConfig for NullArgsConfig {}
self.offline_args_config(&NullArgsConfig {})

================
File: clap-v3-utils/Cargo.toml
================
[package]
name = "solana-clap-v3-utils"
description = "Solana utilities for the clap v3"
documentation = "https://docs.rs/solana-clap-utils"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
name = "solana_clap_v3_utils"

[features]
agave-unstable-api = []
elgamal = ["dep:solana-zk-sdk"]

[dependencies]
chrono = { workspace = true, features = ["default"] }
clap = { version = "3.2.23", features = ["cargo"] }
rpassword = { workspace = true }
solana-clock = { workspace = true }
solana-cluster-type = { workspace = true }
solana-commitment-config = { workspace = true }
solana-derivation-path = { workspace = true }
# the borsh feature is required to use the value_parser macro
solana-hash = { workspace = true, features = ["borsh"] }
solana-keypair = { workspace = true, features = ["seed-derivable"] }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-presigner = { workspace = true }
solana-pubkey = { workspace = true }
solana-remote-wallet = { workspace = true }
solana-seed-derivable = { workspace = true }
solana-seed-phrase = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-zk-sdk = { workspace = true, optional = true }
thiserror = { workspace = true }
tiny-bip39 = { workspace = true }
uriparse = { workspace = true }
url = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
solana-clap-v3-utils = { path = ".", features = ["agave-unstable-api"] }
solana-pubkey = { workspace = true, features = ["rand"] }
solana-system-interface = { workspace = true, features = ["bincode"] }
tempfile = { workspace = true }

================
File: cli/src/address_lookup_table.rs
================
pub enum AddressLookupTableCliCommand {
⋮----
pub trait AddressLookupTableSubCommands {
⋮----
impl AddressLookupTableSubCommands for App<'_, '_> {
fn address_lookup_table_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Address lookup table management")
.setting(AppSettings::SubcommandRequiredElseHelp)
.subcommand(
⋮----
.about("Create a lookup table")
.arg(
⋮----
.long("authority")
.alias("authority-signer")
.value_name("AUTHORITY_PUBKEY")
.takes_value(true)
.validator(is_pubkey_or_keypair)
.help(
⋮----
.long("payer")
.value_name("PAYER_SIGNER")
⋮----
.validator(is_valid_signer)
⋮----
.about("Permanently freezes a lookup table")
⋮----
.index(1)
.value_name("LOOKUP_TABLE_ADDRESS")
⋮----
.required(true)
.validator(is_pubkey)
.help("Address of the lookup table"),
⋮----
.value_name("AUTHORITY_SIGNER")
⋮----
.long("bypass-warning")
.takes_value(false)
.help("Bypass the permanent lookup table freeze warning"),
⋮----
.about("Append more addresses to a lookup table")
⋮----
.long("addresses")
.value_name("ADDRESS_1,ADDRESS_2")
⋮----
.use_delimiter(true)
⋮----
.help("Comma separated list of addresses to append"),
⋮----
.about("Permanently deactivates a lookup table")
⋮----
.help("Bypass the permanent lookup table deactivation warning"),
⋮----
.about("Permanently closes a lookup table")
⋮----
.long("recipient")
.value_name("RECIPIENT_ADDRESS")
⋮----
.about("Display information about a lookup table")
⋮----
.help("Address of the lookup table to show"),
⋮----
pub fn parse_address_lookup_table_subcommand(
⋮----
let (subcommand, sub_matches) = matches.subcommand();
⋮----
let mut bulk_signers = vec![Some(
⋮----
let authority_pubkey = if let Some(authority_pubkey) = pubkey_of(matches, "authority") {
⋮----
.signer_from_path(matches, wallet_manager)?
.pubkey()
⋮----
signer_of(matches, "payer", wallet_manager)
⋮----
bulk_signers.push(payer_signer);
Some(payer_pubkey)
⋮----
Some(
⋮----
.pubkey(),
⋮----
default_signer.generate_unique_signers(bulk_signers, matches, wallet_manager)?;
⋮----
payer_signer_index: signer_info.index_of(payer_pubkey).unwrap(),
⋮----
let lookup_table_pubkey = pubkey_of(matches, "lookup_table_address").unwrap();
⋮----
signer_of(matches, "authority", wallet_manager)
⋮----
bulk_signers.push(authority_signer);
Some(authority_pubkey)
⋮----
authority_signer_index: signer_info.index_of(authority_pubkey).unwrap(),
bypass_warning: matches.is_present("bypass_warning"),
⋮----
let new_addresses: Vec<Pubkey> = values_of(matches, "addresses").unwrap();
⋮----
let recipient_pubkey = if let Some(recipient_pubkey) = pubkey_of(matches, "recipient") {
⋮----
_ => unreachable!(),
⋮----
Ok(response)
⋮----
pub async fn process_address_lookup_table_subcommand(
⋮----
process_create_lookup_table(&rpc_client, config, *authority_pubkey, *payer_signer_index)
⋮----
process_freeze_lookup_table(
⋮----
process_extend_lookup_table(
⋮----
new_addresses.to_vec(),
⋮----
process_deactivate_lookup_table(
⋮----
process_close_lookup_table(
⋮----
} => process_show_lookup_table(&rpc_client, config, *lookup_table_pubkey).await,
⋮----
async fn process_create_lookup_table(
⋮----
.get_account_with_commitment(&sysvar::clock::id(), CommitmentConfig::finalized())
⋮----
let clock_account = get_clock_result.value.expect("Clock account doesn't exist");
let clock: Clock = from_account(&clock_account).ok_or_else(|| {
CliError::RpcRequestError("Failed to deserialize clock sysvar".to_string())
⋮----
let payer_address = payer_signer.pubkey();
⋮----
create_lookup_table(authority_address, payer_address, clock.slot);
let blockhash = rpc_client.get_latest_blockhash().await?;
⋮----
Some(&config.signers[0].pubkey()),
⋮----
let keypairs: Vec<&dyn Signer> = vec![config.signers[0], payer_signer];
tx.try_sign(&keypairs, blockhash)?;
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
preflight_commitment: Some(config.commitment.commitment),
⋮----
Err(err) => Err(format!("Create failed: {err}").into()),
Ok(signature) => Ok(config
⋮----
.formatted_string(&CliAddressLookupTableCreated {
lookup_table_address: lookup_table_address.to_string(),
signature: signature.to_string(),
⋮----
async fn process_freeze_lookup_table(
⋮----
.get_account_with_commitment(&lookup_table_pubkey, config.commitment)
⋮----
let lookup_table_account = get_lookup_table_result.value.ok_or_else(|| {
format!("Lookup table account {lookup_table_pubkey} not found, was it already closed?")
⋮----
return Err(format!(
⋮----
.into());
⋮----
return Err(String::from(FREEZE_LOOKUP_TABLE_WARNING).into());
⋮----
let authority_address = authority_signer.pubkey();
let freeze_lookup_table_ix = freeze_lookup_table(lookup_table_pubkey, authority_address);
⋮----
tx.try_sign(&[config.signers[0], authority_signer], blockhash)?;
⋮----
Err(err) => Err(format!("Freeze failed: {err}").into()),
Ok(signature) => Ok(config.output_format.formatted_string(&CliSignature {
⋮----
async fn process_extend_lookup_table(
⋮----
if new_addresses.is_empty() {
return Err("Lookup tables must be extended by at least one address".into());
⋮----
let extend_lookup_table_ix = extend_lookup_table(
⋮----
Some(payer_address),
⋮----
Err(err) => Err(format!("Extend failed: {err}").into()),
⋮----
async fn process_deactivate_lookup_table(
⋮----
return Err(String::from(DEACTIVATE_LOOKUP_TABLE_WARNING).into());
⋮----
deactivate_lookup_table(lookup_table_pubkey, authority_address);
⋮----
Err(err) => Err(format!("Deactivate failed: {err}").into()),
⋮----
async fn process_close_lookup_table(
⋮----
close_lookup_table(lookup_table_pubkey, authority_address, recipient_pubkey);
⋮----
Err(err) => Err(format!("Close failed: {err}").into()),
⋮----
async fn process_show_lookup_table(
⋮----
Ok(config
⋮----
.formatted_string(&CliAddressLookupTable {
lookup_table_address: lookup_table_pubkey.to_string(),
⋮----
.as_ref()
.map(ToString::to_string),
⋮----
.iter()
.map(ToString::to_string)
.collect(),

================
File: cli/src/checks.rs
================
pub async fn check_account_for_fee(
⋮----
check_account_for_multiple_fees(rpc_client, account_pubkey, &[message]).await
⋮----
pub async fn check_account_for_fee_with_commitment(
⋮----
check_account_for_multiple_fees_with_commitment(
⋮----
pub async fn check_account_for_multiple_fees(
⋮----
pub async fn check_account_for_multiple_fees_with_commitment(
⋮----
check_account_for_spend_multiple_fees_with_commitment(
⋮----
pub async fn check_account_for_spend_multiple_fees_with_commitment(
⋮----
let fee = get_fee_for_messages(rpc_client, messages).await?;
check_account_for_spend_and_fee_with_commitment(
⋮----
pub async fn check_account_for_spend_and_fee_with_commitment(
⋮----
.checked_add(fee)
.ok_or(CliError::InsufficientFundsForSpendAndFee(
build_balance_message(balance, false, false),
build_balance_message(fee, false, false),
⋮----
if !check_account_for_balance_with_commitment(
⋮----
return Err(CliError::InsufficientFundsForSpendAndFee(
⋮----
return Err(CliError::InsufficientFundsForFee(
⋮----
Ok(())
⋮----
pub async fn get_fee_for_messages(
⋮----
let fee = rpc_client.get_fee_for_message(*message).await?;
⋮----
.ok_or(CliError::BadParameter("Fee overflow".to_string()))?;
⋮----
Ok(total_fee)
⋮----
pub async fn check_account_for_balance(
⋮----
check_account_for_balance_with_commitment(
⋮----
pub async fn check_account_for_balance_with_commitment(
⋮----
.get_balance_with_commitment(account_pubkey, commitment)
⋮----
return Ok(true);
⋮----
Ok(false)
⋮----
pub fn check_unique_pubkeys(
⋮----
Err(CliError::BadParameter(format!(
⋮----
mod tests {
⋮----
async fn test_check_account_for_fees() {
⋮----
let account_balance_response = json!(Response {
⋮----
let message0 = Message::new(&[ix0], Some(&pubkey0));
⋮----
let message1 = Message::new(&[ix0, ix1], Some(&pubkey0));
⋮----
mocks.insert(RpcRequest::GetBalance, account_balance_response.clone());
let rpc_client = RpcClient::new_mock_with_mocks("".to_string(), mocks);
check_account_for_fee(&rpc_client, &pubkey, &message0)
⋮----
.expect("unexpected result");
let check_fee_response = json!(Response {
⋮----
mocks.insert(RpcRequest::GetFeeForMessage, check_fee_response);
⋮----
assert!(check_account_for_fee(&rpc_client, &pubkey, &message1)
⋮----
mocks.insert(RpcRequest::GetBalance, account_balance_response);
⋮----
assert!(
⋮----
check_account_for_multiple_fees(&rpc_client, &pubkey, &[&message0, &message0])
⋮----
async fn test_check_account_for_balance() {
⋮----
assert!(check_account_for_balance(&rpc_client, &pubkey, 1)
⋮----
async fn test_get_fee_for_messages() {
⋮----
// No messages, no fee.
assert_eq!(get_fee_for_messages(&rpc_client, &[]).await.unwrap(), 0);
// One message w/ one signature, a fee.
⋮----
assert_eq!(
⋮----
// No signatures, no fee.
⋮----
fn test_check_unique_pubkeys() {
⋮----
check_unique_pubkeys((&pubkey0, "foo".to_string()), (&pubkey1, "bar".to_string()))
⋮----
check_unique_pubkeys((&pubkey0, "foo".to_string()), (&pubkey1, "foo".to_string()))
⋮----
assert!(check_unique_pubkeys(

================
File: cli/src/clap_app.rs
================
pub fn get_clap_app<'ab, 'v>(name: &str, about: &'ab str, version: &'v str) -> App<'ab, 'v> {
⋮----
.about(about)
.version(version)
.setting(AppSettings::SubcommandRequiredElseHelp)
.arg(
⋮----
.long("skip-preflight")
.global(true)
.takes_value(false)
.help("Skip the preflight check when sending transactions"),
⋮----
.arg({
⋮----
.short("C")
.long("config")
.value_name("FILEPATH")
.takes_value(true)
⋮----
.help("Configuration file to use");
⋮----
arg.default_value(config_file)
⋮----
.short("u")
.long("url")
.value_name("URL_OR_MONIKER")
⋮----
.validator(is_url_or_moniker)
.help(
⋮----
.long("ws")
.value_name("URL")
⋮----
.validator(is_url)
.help("WebSocket URL for the solana cluster"),
⋮----
.short("k")
.long("keypair")
.value_name("KEYPAIR")
⋮----
.help("Filepath or URL to a keypair"),
⋮----
.long("commitment")
⋮----
.possible_values(&[
⋮----
.value_name("COMMITMENT_LEVEL")
.hide_possible_values(true)
⋮----
.long("verbose")
.short("v")
⋮----
.help("Show additional information"),
⋮----
.long("use-tpu-client")
⋮----
.help("Use TPU client when sending transactions."),
⋮----
.long("no-address-labels")
⋮----
.help("Do not use address labels in the output"),
⋮----
.long("output")
.value_name("FORMAT")
⋮----
.possible_values(&["json", "json-compact"])
.help("Return information in specified output format"),
⋮----
.long(SKIP_SEED_PHRASE_VALIDATION_ARG.long)
⋮----
.help(SKIP_SEED_PHRASE_VALIDATION_ARG.help),
⋮----
.long("rpc-timeout")
.value_name("SECONDS")
⋮----
.default_value(DEFAULT_RPC_TIMEOUT_SECONDS)
⋮----
.hidden(hidden_unless_forced())
.help("Timeout value for RPC requests"),
⋮----
.long("confirm-timeout")
⋮----
.default_value(DEFAULT_CONFIRM_TX_TIMEOUT_SECONDS)
⋮----
.help("Timeout value for initial transaction status"),
⋮----
.cluster_query_subcommands()
.feature_subcommands()
.inflation_subcommands()
.nonce_subcommands()
.program_subcommands()
.program_v4_subcommands()
.address_lookup_table_subcommands()
.stake_subcommands()
.validator_info_subcommands()
.vote_subcommands()
.wallet_subcommands()
.subcommand(
⋮----
.about("Solana command-line tool configuration settings")
.aliases(&["get", "set"])
⋮----
.about("Get current config settings")
⋮----
.index(1)
.value_name("CONFIG_FIELD")
⋮----
.help("Return a specific config setting"),
⋮----
.about("Set a config setting")
.group(
⋮----
.args(&["json_rpc_url", "websocket_url", "keypair", "commitment"])
.multiple(true)
.required(true),
⋮----
.about("Import a list of address labels")
⋮----
.value_name("FILENAME")
⋮----
.help("YAML file of address labels"),
⋮----
.about("Export the current address labels")
⋮----
.help("YAML file to receive the current address labels"),
⋮----
.about("Generate completion scripts for various shells")
⋮----
.long("shell")
.short("s")
⋮----
.possible_values(&["bash", "fish", "zsh", "powershell", "elvish"])
.default_value("bash"),

================
File: cli/src/cli.rs
================
pub enum CliCommand {
⋮----
pub struct CliCommandInfo {
⋮----
impl CliCommandInfo {
pub fn without_signers(command: CliCommand) -> Self {
⋮----
signers: vec![],
⋮----
pub enum CliError {
⋮----
fn from(error: Box<dyn error::Error>) -> Self {
CliError::DynamicProgramError(error.to_string())
⋮----
fn from(error: solana_rpc_client_nonce_utils::Error) -> Self {
⋮----
pub struct CliConfig<'a> {
⋮----
pub(crate) fn pubkey(&self) -> Result<Pubkey, SignerError> {
if !self.signers.is_empty() {
self.signers[0].try_pubkey()
⋮----
Err(SignerError::Custom(
"Default keypair must be set if pubkey arg not provided".to_string(),
⋮----
pub fn recent_for_tests() -> Self {
⋮----
preflight_commitment: Some(CommitmentConfig::processed().commitment),
⋮----
impl Default for CliConfig<'_> {
fn default() -> CliConfig<'static> {
⋮----
pubkey: Some(Pubkey::default()),
⋮----
rpc_timeout: Duration::from_secs(u64::from_str(DEFAULT_RPC_TIMEOUT_SECONDS).unwrap()),
⋮----
u64::from_str(DEFAULT_CONFIRM_TX_TIMEOUT_SECONDS).unwrap(),
⋮----
pub fn parse_command(
⋮----
let response = match matches.subcommand() {
⋮----
let shell_choice = match matches.value_of("shell") {
⋮----
_ => unreachable!(),
⋮----
get_clap_app(
crate_name!(),
crate_description!(),
⋮----
.gen_completions_to("solana", shell_choice, &mut stdout());
⋮----
("block", Some(matches)) => parse_get_block(matches),
⋮----
parse_get_recent_prioritization_fees(matches)
⋮----
("block-height", Some(matches)) => parse_get_block_height(matches),
("block-production", Some(matches)) => parse_show_block_production(matches),
("block-time", Some(matches)) => parse_get_block_time(matches),
("catchup", Some(matches)) => parse_catchup(matches, wallet_manager),
⋮----
Ok(CliCommandInfo::without_signers(CliCommand::ClusterDate))
⋮----
Ok(CliCommandInfo::without_signers(CliCommand::ClusterVersion))
⋮----
("epoch", Some(matches)) => parse_get_epoch(matches),
("epoch-info", Some(matches)) => parse_get_epoch_info(matches),
⋮----
parse_feature_subcommand(matches, default_signer, wallet_manager)
⋮----
("first-available-block", Some(_matches)) => Ok(CliCommandInfo::without_signers(
⋮----
Ok(CliCommandInfo::without_signers(CliCommand::GetGenesisHash))
⋮----
("gossip", Some(_matches)) => Ok(CliCommandInfo::without_signers(CliCommand::ShowGossip)),
⋮----
parse_inflation_subcommand(matches, default_signer, wallet_manager)
⋮----
("largest-accounts", Some(matches)) => parse_largest_accounts(matches),
("leader-schedule", Some(matches)) => parse_leader_schedule(matches),
⋮----
Ok(CliCommandInfo::without_signers(CliCommand::LiveSlots))
⋮----
("logs", Some(matches)) => parse_logs(matches, wallet_manager),
("ping", Some(matches)) => parse_cluster_ping(matches, default_signer, wallet_manager),
⋮----
.unwrap()
.length();
let use_lamports_unit = matches.is_present("lamports");
Ok(CliCommandInfo::without_signers(CliCommand::Rent {
⋮----
("slot", Some(matches)) => parse_get_slot(matches),
("stakes", Some(matches)) => parse_show_stakes(matches, wallet_manager),
("supply", Some(matches)) => parse_supply(matches),
("total-supply", Some(matches)) => parse_total_supply(matches),
("transaction-count", Some(matches)) => parse_get_transaction_count(matches),
⋮----
parse_transaction_history(matches, wallet_manager)
⋮----
("validators", Some(matches)) => parse_show_validators(matches),
⋮----
parse_authorize_nonce_account(matches, default_signer, wallet_manager)
⋮----
parse_nonce_create_account(matches, default_signer, wallet_manager)
⋮----
("nonce", Some(matches)) => parse_get_nonce(matches, wallet_manager),
("new-nonce", Some(matches)) => parse_new_nonce(matches, default_signer, wallet_manager),
("nonce-account", Some(matches)) => parse_show_nonce_account(matches, wallet_manager),
⋮----
parse_withdraw_from_nonce_account(matches, default_signer, wallet_manager)
⋮----
("upgrade-nonce-account", Some(matches)) => parse_upgrade_nonce_account(matches),
⋮----
.exit(),
⋮----
parse_program_subcommand(matches, default_signer, wallet_manager)
⋮----
parse_program_v4_subcommand(matches, default_signer, wallet_manager)
⋮----
parse_address_lookup_table_subcommand(matches, default_signer, wallet_manager)
⋮----
let max_stake_percent = value_t_or_exit!(matches, "max_percent", f32);
Ok(CliCommandInfo::without_signers(
⋮----
parse_create_stake_account(matches, default_signer, wallet_manager, !CHECKED)
⋮----
parse_create_stake_account(matches, default_signer, wallet_manager, CHECKED)
⋮----
parse_stake_delegate_stake(matches, default_signer, wallet_manager)
⋮----
("redelegate-stake", _) => Err(CliError::CommandNotRecognized(
⋮----
.to_string(),
⋮----
parse_stake_withdraw_stake(matches, default_signer, wallet_manager)
⋮----
parse_stake_deactivate_stake(matches, default_signer, wallet_manager)
⋮----
parse_split_stake(matches, default_signer, wallet_manager)
⋮----
parse_merge_stake(matches, default_signer, wallet_manager)
⋮----
parse_stake_authorize(matches, default_signer, wallet_manager, !CHECKED)
⋮----
parse_stake_authorize(matches, default_signer, wallet_manager, CHECKED)
⋮----
parse_stake_set_lockup(matches, default_signer, wallet_manager, !CHECKED)
⋮----
parse_stake_set_lockup(matches, default_signer, wallet_manager, CHECKED)
⋮----
("stake-account", Some(matches)) => parse_show_stake_account(matches, wallet_manager),
("stake-history", Some(matches)) => parse_show_stake_history(matches),
("stake-minimum-delegation", Some(matches)) => parse_stake_minimum_delegation(matches),
("validator-info", Some(matches)) => match matches.subcommand() {
⋮----
parse_validator_info_command(matches, default_signer, wallet_manager)
⋮----
("get", Some(matches)) => parse_get_validator_info_command(matches),
⋮----
parse_create_vote_account(matches, default_signer, wallet_manager)
⋮----
parse_vote_update_validator(matches, default_signer, wallet_manager)
⋮----
parse_vote_update_commission(matches, default_signer, wallet_manager)
⋮----
("vote-authorize-voter", Some(matches)) => parse_vote_authorize(
⋮----
("vote-authorize-withdrawer", Some(matches)) => parse_vote_authorize(
⋮----
("vote-authorize-voter-checked", Some(matches)) => parse_vote_authorize(
⋮----
("vote-authorize-withdrawer-checked", Some(matches)) => parse_vote_authorize(
⋮----
("vote-account", Some(matches)) => parse_vote_get_account_command(matches, wallet_manager),
⋮----
parse_withdraw_from_vote_account(matches, default_signer, wallet_manager)
⋮----
parse_close_vote_account(matches, default_signer, wallet_manager)
⋮----
("account", Some(matches)) => parse_account(matches, wallet_manager),
("address", Some(matches)) => Ok(CliCommandInfo {
⋮----
signers: vec![default_signer.signer_from_path(matches, wallet_manager)?],
⋮----
("airdrop", Some(matches)) => parse_airdrop(matches, default_signer, wallet_manager),
("balance", Some(matches)) => parse_balance(matches, default_signer, wallet_manager),
("confirm", Some(matches)) => match matches.value_of("signature").unwrap().parse() {
Ok(signature) => Ok(CliCommandInfo::without_signers(CliCommand::Confirm(
⋮----
_ => Err(CliError::BadParameter("Invalid signature".to_string())),
⋮----
parse_create_address_with_seed(matches, default_signer, wallet_manager)
⋮----
parse_find_program_derived_address(matches)
⋮----
("decode-transaction", Some(matches)) => parse_decode_transaction(matches),
⋮----
let signer_path = resolve_signer(matches, "signer", wallet_manager)?;
Ok(CliCommandInfo::without_signers(CliCommand::ResolveSigner(
⋮----
("transfer", Some(matches)) => parse_transfer(matches, default_signer, wallet_manager),
⋮----
parse_sign_offchain_message(matches, default_signer, wallet_manager)
⋮----
parse_verify_offchain_signature(matches, default_signer, wallet_manager)
⋮----
eprintln!("{}", matches.usage());
Err(CliError::CommandNotRecognized(
"no subcommand given".to_string(),
⋮----
Ok(response)
⋮----
pub type ProcessResult = Result<String, Box<dyn std::error::Error>>;
pub async fn process_command(config: &CliConfig<'_>) -> ProcessResult {
⋮----
println_name_value("RPC URL:", &config.json_rpc_url);
println_name_value("Default Signer Path:", &config.keypair_path);
if config.keypair_path.starts_with("usb://") {
⋮----
.pubkey()
.map(|pubkey| format!("{pubkey:?}"))
.unwrap_or_else(|_| "Unavailable".to_string());
println_name_value("Pubkey:", &pubkey);
⋮----
println_name_value("Commitment:", &config.commitment.commitment.to_string());
⋮----
let rpc_client = if config.rpc_client.is_none() {
⋮----
config.json_rpc_url.to_string(),
⋮----
// Primarily for testing
config.rpc_client.as_ref().unwrap().clone()
⋮----
// Cluster Query Commands
// Get address of this client
CliCommand::Address => Ok(format!("{}", config.pubkey()?)),
// Return software version of solana-cli and cluster entrypoint node
⋮----
process_catchup(
&rpc_client.clone(),
⋮----
node_json_rpc_url.clone(),
⋮----
CliCommand::ClusterDate => process_cluster_date(&rpc_client, config).await,
CliCommand::ClusterVersion => process_cluster_version(&rpc_client, config).await,
⋮----
} => process_create_address_with_seed(config, from_pubkey.as_ref(), seed, program_id),
⋮----
process_feature_subcommand(&rpc_client, config, feature_subcommand).await
⋮----
process_find_program_derived_address(config, seeds, program_id)
⋮----
CliCommand::FirstAvailableBlock => process_first_available_block(&rpc_client).await,
CliCommand::GetBlock { slot } => process_get_block(&rpc_client, config, *slot).await,
⋮----
process_get_block_time(&rpc_client, config, *slot).await
⋮----
process_get_recent_priority_fees(&rpc_client, config, accounts, *limit_num_slots).await
⋮----
CliCommand::GetEpoch => process_get_epoch(&rpc_client, config).await,
CliCommand::GetEpochInfo => process_get_epoch_info(&rpc_client, config).await,
CliCommand::GetGenesisHash => process_get_genesis_hash(&rpc_client).await,
CliCommand::GetSlot => process_get_slot(&rpc_client, config).await,
CliCommand::GetBlockHeight => process_get_block_height(&rpc_client, config).await,
⋮----
process_largest_accounts(&rpc_client, config, filter.clone()).await
⋮----
CliCommand::GetTransactionCount => process_get_transaction_count(&rpc_client, config).await,
⋮----
process_inflation_subcommand(&rpc_client, config, inflation_subcommand).await
⋮----
process_leader_schedule(&rpc_client, config, *epoch).await
⋮----
CliCommand::LiveSlots => process_live_slots(config),
CliCommand::Logs { filter } => process_logs(config, filter),
⋮----
Some({
⋮----
rpc_client.clone(),
⋮----
.unwrap_or_else(|err| {
eprintln!("Could not create TpuClient {err:?}");
⋮----
process_ping(
Some(&tpu_client),
⋮----
} => process_calculate_rent(&rpc_client, config, *data_length, *use_lamports_unit).await,
⋮----
process_show_block_production(&rpc_client, config, *epoch, *slot_limit).await
⋮----
CliCommand::ShowGossip => process_show_gossip(&rpc_client, config).await,
⋮----
process_show_stakes(
⋮----
vote_account_pubkeys.as_deref(),
withdraw_authority.as_ref(),
⋮----
process_wait_for_max_stake(&rpc_client, config, *max_stake_percent).await
⋮----
process_show_validators(
⋮----
process_supply(&rpc_client, config, *print_accounts).await
⋮----
CliCommand::TotalSupply => process_total_supply(&rpc_client, config).await,
⋮----
process_transaction_history(
⋮----
// Nonce Commands
// Assign authority to nonce account
⋮----
process_authorize_nonce_account(
⋮----
memo.as_ref(),
⋮----
// Create nonce account
⋮----
process_create_nonce_account(
⋮----
seed.clone(),
⋮----
// Get the current nonce
⋮----
process_get_nonce(&rpc_client, config, nonce_account_pubkey).await
⋮----
// Get a new nonce
⋮----
process_new_nonce(
⋮----
// Show the contents of a nonce account
⋮----
process_show_nonce_account(
⋮----
// Withdraw lamports from a nonce account
⋮----
process_withdraw_from_nonce_account(
⋮----
// Upgrade nonce account out of blockhash domain.
⋮----
process_upgrade_nonce_account(
⋮----
// Program Deployment
⋮----
// This command is not supported any longer
// Error message is printed on the previous stage
⋮----
// Deploy a custom program to the chain
⋮----
process_program_subcommand(rpc_client, config, program_subcommand).await
⋮----
// Deploy a custom program v4 to the chain
⋮----
process_program_v4_subcommand(rpc_client, config, program_subcommand).await
⋮----
// Stake Commands
// Create stake account
⋮----
process_create_stake_account(
⋮----
nonce_account.as_ref(),
⋮----
process_deactivate_stake_account(
⋮----
seed.as_ref(),
⋮----
process_delegate_stake(
⋮----
process_split_stake(
⋮----
rent_exempt_reserve.as_ref(),
⋮----
process_merge_stake(
⋮----
process_show_stake_account(
⋮----
process_show_stake_history(&rpc_client, config, *use_lamports_unit, *limit_results)
⋮----
process_stake_authorize(
⋮----
process_stake_set_lockup(
⋮----
process_withdraw_stake(
⋮----
process_stake_minimum_delegation(&rpc_client, config, *use_lamports_unit).await
⋮----
// Validator Info Commands
// Return all or single validator info
⋮----
process_get_validator_info(&rpc_client, config, *info_pubkey).await
⋮----
// Publish validator info
⋮----
process_set_validator_info(
⋮----
// Vote Commands
// Create vote account
⋮----
process_create_vote_account(
⋮----
process_show_vote_account(
⋮----
process_withdraw_from_vote_account(
⋮----
process_close_vote_account(
⋮----
process_vote_authorize(
⋮----
process_vote_update_validator(
⋮----
process_vote_update_commission(
⋮----
// Wallet Commands
// Request an airdrop from Solana Faucet;
⋮----
process_airdrop(&rpc_client, config, pubkey, *lamports).await
⋮----
// Check client balance
⋮----
} => process_balance(&rpc_client, config, pubkey, *use_lamports_unit).await,
// Confirm the last client transaction by signature
CliCommand::Confirm(signature) => process_confirm(&rpc_client, config, signature).await,
⋮----
process_decode_transaction(config, transaction)
⋮----
Ok(path.to_string())
⋮----
Ok("Signer is valid".to_string())
⋮----
process_show_account(&rpc_client, config, pubkey, output_file, *use_lamports_unit).await
⋮----
process_transfer(
⋮----
derived_address_seed.clone(),
derived_address_program_id.as_ref(),
⋮----
// Address Lookup Table Commands
⋮----
process_address_lookup_table_subcommand(rpc_client, config, subcommand).await
⋮----
process_sign_offchain_message(config, message)
⋮----
} => process_verify_offchain_signature(config, signer_pubkey, signature, message),
⋮----
pub async fn request_and_confirm_airdrop(
⋮----
let recent_blockhash = rpc_client.get_latest_blockhash().await?;
⋮----
.request_airdrop_with_blockhash(to_pubkey, lamports, &recent_blockhash)
⋮----
.confirm_transaction_with_spinner(&signature, &recent_blockhash, config.commitment)
⋮----
Ok(signature)
⋮----
pub fn common_error_adapter<E>(ix_error: &InstructionError) -> Option<E>
⋮----
pub fn to_str_error_adapter<E>(ix_error: &InstructionError) -> Option<E>
⋮----
InstructionError::Custom(code) => E::try_from(*code).ok(),
⋮----
pub fn log_instruction_custom_error_to_str<E>(
⋮----
pub fn log_instruction_custom_error<E>(
⋮----
pub fn log_instruction_custom_error_ex<E, F>(
⋮----
let maybe_tx_err = err.get_transaction_error();
⋮----
if let Some(specific_error) = error_adapter(&ix_error) {
return Err(specific_error.into());
⋮----
Err(err.into())
⋮----
signature: sig.clone().to_string(),
⋮----
Ok(output_format.formatted_string(&signature))
⋮----
mod tests {
⋮----
fn make_tmp_path(name: &str) -> String {
let out_dir = std::env::var("FARF_DIR").unwrap_or_else(|_| "farf".to_string());
⋮----
let path = format!("{}/tmp/{}-{}", out_dir, name, keypair.pubkey());
⋮----
fn test_generate_unique_signers() {
⋮----
let default_keypair_file = make_tmp_path("keypair_file");
write_keypair_file(&default_keypair, &default_keypair_file).unwrap();
⋮----
.generate_unique_signers(vec![], &matches, &mut None)
.unwrap();
assert_eq!(signer_info.signers.len(), 0);
⋮----
.generate_unique_signers(vec![None, None], &matches, &mut None)
⋮----
assert_eq!(signer_info.signers.len(), 1);
assert_eq!(signer_info.index_of(None), Some(0));
assert_eq!(signer_info.index_of(Some(solana_pubkey::new_rand())), None);
let keypair0 = keypair_from_seed(&[1u8; 32]).unwrap();
let keypair0_pubkey = keypair0.pubkey();
let keypair0_clone = keypair_from_seed(&[1u8; 32]).unwrap();
let keypair0_clone_pubkey = keypair0.pubkey();
let signers: Vec<Option<Box<dyn Signer>>> = vec![
⋮----
.generate_unique_signers(signers, &matches, &mut None)
⋮----
assert_eq!(signer_info.signers.len(), 2);
⋮----
assert_eq!(signer_info.index_of(Some(keypair0_pubkey)), Some(1));
assert_eq!(signer_info.index_of(Some(keypair0_clone_pubkey)), Some(1));
⋮----
vec![Some(Box::new(keypair0)), Some(Box::new(keypair0_clone))];
⋮----
assert_eq!(signer_info.index_of(Some(keypair0_pubkey)), Some(0));
let keypair0 = keypair_from_seed(&[2u8; 32]).unwrap();
⋮----
let keypair1 = keypair_from_seed(&[3u8; 32]).unwrap();
let keypair1_pubkey = keypair1.pubkey();
let message = vec![0, 1, 2, 3];
let presigner0 = Presigner::new(&keypair0.pubkey(), &keypair0.sign_message(&message));
let presigner0_pubkey = presigner0.pubkey();
let presigner1 = Presigner::new(&keypair1.pubkey(), &keypair1.sign_message(&message));
let presigner1_pubkey = presigner1.pubkey();
⋮----
assert_eq!(signer_info.index_of(Some(keypair1_pubkey)), Some(1));
assert_eq!(signer_info.index_of(Some(presigner0_pubkey)), Some(0));
assert_eq!(signer_info.index_of(Some(presigner1_pubkey)), Some(1));
⋮----
fn test_cli_parse_command() {
let test_commands = get_clap_app("test", "desc", "version");
⋮----
let pubkey_string = format!("{pubkey}");
⋮----
let keypair_file = make_tmp_path("keypair_file");
write_keypair_file(&default_keypair, &keypair_file).unwrap();
let keypair = read_keypair_file(&keypair_file).unwrap();
⋮----
// Test Airdrop Subcommand
⋮----
.clone()
.get_matches_from(vec!["test", "airdrop", "50", &pubkey_string]);
assert_eq!(
⋮----
let test_balance = test_commands.clone().get_matches_from(vec![
⋮----
.get_matches_from(vec!["test", "balance", "--lamports"]);
⋮----
let signature_string = format!("{signature:?}");
⋮----
.get_matches_from(vec!["test", "confirm", &signature_string]);
⋮----
.get_matches_from(vec!["test", "confirm", "deadbeef"]);
assert!(parse_command(&test_bad_signature, &default_signer, &mut None).is_err());
⋮----
let from_str = from_pubkey.to_string();
⋮----
let test_create_address_with_seed = test_commands.clone().get_matches_from(vec![
⋮----
.get_matches_from(vec!["test", "resolve-signer", &keypair_file]);
⋮----
.get_matches_from(vec!["test", "resolve-signer", &pubkey_string]);
⋮----
let test_sign_offchain = test_commands.clone().get_matches_from(vec![
⋮----
let message = OffchainMessage::new(0, b"Test Message").unwrap();
⋮----
let signature = keypair.sign_message(&message.serialize().unwrap());
let test_verify_offchain = test_commands.clone().get_matches_from(vec![
⋮----
async fn test_cli_process_command() {
⋮----
rpc_client: Some(Arc::new(RpcClient::new_mock("succeeds".to_string()))),
json_rpc_url: "http://127.0.0.1:8899".to_string(),
⋮----
let pubkey = keypair.pubkey().to_string();
config.signers = vec![&keypair];
⋮----
assert_eq!(process_command(&config).await.unwrap(), pubkey);
⋮----
assert_eq!(process_command(&config).await.unwrap(), "50 lamports");
⋮----
assert_eq!(process_command(&config).await.unwrap(), "0.00000005 SOL");
⋮----
.into_vec()
.map(Signature::try_from)
⋮----
let bob_pubkey = bob_keypair.pubkey();
⋮----
let vote_account_info_response = json!(Response {
⋮----
mocks.insert(RpcRequest::GetAccountInfo, vote_account_info_response);
let rpc_client = Some(Arc::new(RpcClient::new_mock_with_mocks(
"".to_string(),
⋮----
authorized_voter: Some(bob_pubkey),
⋮----
config.signers = vec![&keypair, &bob_keypair, &identity_keypair];
let result = process_command(&config).await;
assert!(result.is_ok());
⋮----
let rpc_client = RpcClient::new_mock_with_mocks("".to_string(), mocks);
⋮----
rpc_client: Some(Arc::new(rpc_client)),
⋮----
config.signers = vec![&keypair, &bob_keypair];
⋮----
config.signers = vec![&keypair, &split_stake_account];
⋮----
config.signers = vec![&keypair, &merge_stake_account];
⋮----
assert_eq!(process_command(&config).await.unwrap(), "0");
⋮----
assert_eq!(process_command(&config).await.unwrap(), "1234");
⋮----
config.signers = vec![];
⋮----
from_pubkey: Some(from_pubkey),
seed: "seed".to_string(),
⋮----
let address = process_command(&config).await;
⋮----
Pubkey::create_with_seed(&from_pubkey, "seed", &stake::id()).unwrap();
assert_eq!(address.unwrap(), expected_address.to_string());
⋮----
pubkey: Some(to),
⋮----
assert!(process_command(&config).await.is_ok());
config.rpc_client = Some(Arc::new(RpcClient::new_mock("sig_not_found".to_string())));
⋮----
assert_eq!(process_command(&config).await.unwrap(), "Not found");
config.rpc_client = Some(Arc::new(RpcClient::new_mock("account_in_use".to_string())));
⋮----
config.rpc_client = Some(Arc::new(RpcClient::new_mock("fails".to_string())));
⋮----
assert!(process_command(&config).await.is_err());
⋮----
message: message.clone(),
⋮----
signature: result.unwrap().parse().unwrap(),
⋮----
fn test_parse_transfer_subcommand() {
⋮----
// Test Transfer Subcommand, SOL
let from_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
let from_pubkey = from_keypair.pubkey();
let from_string = from_pubkey.to_string();
let to_keypair = keypair_from_seed(&[1u8; 32]).unwrap();
let to_pubkey = to_keypair.pubkey();
let to_string = to_pubkey.to_string();
⋮----
.get_matches_from(vec!["test", "transfer", &to_string, "42"]);
⋮----
.get_matches_from(vec!["test", "transfer", &to_string, "ALL"]);
⋮----
let test_transfer = test_commands.clone().get_matches_from(vec![
⋮----
let blockhash_string = blockhash.to_string();
⋮----
let from_sig = from_keypair.sign_message(&[0u8]);
let from_signer = format!("{from_pubkey}={from_sig}");
⋮----
let nonce_address_string = nonce_address.to_string();
let nonce_authority = keypair_from_seed(&[2u8; 32]).unwrap();
let nonce_authority_file = make_tmp_path("nonce_authority_file");
write_keypair_file(&nonce_authority, &nonce_authority_file).unwrap();
⋮----
let derived_address_seed = "seed".to_string();
⋮----
fn test_cli_completions() {
let mut clap_app = get_clap_app("test", "desc", "version");
⋮----
let mut buf: Vec<u8> = vec![];
clap_app.gen_completions_to("solana", shell, &mut buf);
assert!(!buf.is_empty());

================
File: cli/src/cluster_query.rs
================
pub trait ClusterQuerySubCommands {
⋮----
impl ClusterQuerySubCommands for App<'_, '_> {
fn cluster_query_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Get a confirmed block")
.arg(
⋮----
.long("slot")
.validator(is_slot)
.value_name("SLOT")
.takes_value(true)
.index(1),
⋮----
.subcommand(
⋮----
.about("Get recent prioritization fees")
⋮----
.value_name("ACCOUNTS")
⋮----
.multiple(true)
.index(1)
.help(
⋮----
.long("limit-num-slots")
.value_name("SLOTS")
⋮----
.help("Limit the number of slots to the last <N> slots"),
⋮----
.about("Wait for a validator to catch up to the cluster")
.arg(pubkey!(
⋮----
.index(2)
.value_name("OUR_URL")
⋮----
.validator(is_url)
⋮----
.long("follow")
.takes_value(false)
.help("Continue reporting progress even after the validator has caught up"),
⋮----
.long("our-localhost")
⋮----
.value_name("PORT")
.default_value(DEFAULT_RPC_PORT_STR)
.validator(is_port)
⋮----
.arg(Arg::with_name("log").long("log").takes_value(false).help(
⋮----
.subcommand(SubCommand::with_name("cluster-date").about(
⋮----
.about("Get the version of the cluster entrypoint"),
⋮----
.about("Get the first available block in the storage"),
⋮----
.about("Get estimated production time of a block")
.alias("get-block-time")
⋮----
.help("Slot number of the block to query"),
⋮----
.about("Display leader schedule")
⋮----
.long("epoch")
⋮----
.value_name("EPOCH")
.validator(is_epoch)
.help("Epoch to show leader schedule for [default: current]"),
⋮----
.about("Get information about the current epoch")
.alias("get-epoch-info"),
⋮----
.about("Get the genesis hash")
.alias("get-genesis-hash"),
⋮----
.about("Get current slot")
.alias("get-slot"),
⋮----
.subcommand(SubCommand::with_name("block-height").about("Get current block height"))
.subcommand(SubCommand::with_name("epoch").about("Get current epoch"))
⋮----
.about("Get addresses of largest cluster accounts")
⋮----
.long("circulating")
⋮----
.help("Filter address list to only circulating accounts"),
⋮----
.long("non-circulating")
⋮----
.conflicts_with("circulating")
.help("Filter address list to only non-circulating accounts"),
⋮----
.about("Get information about the cluster supply of SOL")
⋮----
.long("print-accounts")
⋮----
.help("Print list of non-circulating account addresses"),
⋮----
.about("Get total number of SOL")
.setting(AppSettings::Hidden),
⋮----
.about("Get current transaction count")
.alias("get-transaction-count"),
⋮----
.about("Submit transactions sequentially")
⋮----
.short("i")
.long("interval")
.value_name("SECONDS")
⋮----
.default_value("2")
.help("Wait interval seconds between submitting the next transaction"),
⋮----
.short("c")
.long("count")
.value_name("NUMBER")
⋮----
.help("Stop after submitting count transactions"),
⋮----
.short("D")
.long("print-timestamp")
⋮----
.short("t")
.long("timeout")
⋮----
.default_value("15")
.help("Wait up to timeout seconds for transaction confirmation"),
⋮----
.arg(compute_unit_price_arg())
.arg(blockhash_arg()),
⋮----
.about("Show information about the current slot progression"),
⋮----
.about("Stream transaction logs")
⋮----
.long("include-votes")
⋮----
.conflicts_with("address")
.help("Include vote transactions when monitoring all transactions"),
⋮----
.about("Show information about block production")
.alias("show-block-production")
⋮----
.help("Epoch to show block production for [default: current epoch]"),
⋮----
.long("slot-limit")
⋮----
.about("Show the current gossip network nodes")
.alias("show-gossip"),
⋮----
.about("Show stake account information")
⋮----
.long("lamports")
⋮----
.help("Display balance in lamports instead of SOL"),
⋮----
.about("Show summary information about the current validators")
.alias("show-validators")
⋮----
.long("number")
.short("n")
⋮----
.help("Number the validators"),
⋮----
.long("reverse")
.short("r")
⋮----
.help("Reverse order while sorting"),
⋮----
.long("sort")
⋮----
.possible_values(&[
⋮----
.default_value("stake")
.help("Sort order (does not affect JSON output)"),
⋮----
.long("keep-unstaked-delinquents")
⋮----
.help("Don't discard unstaked, delinquent validators"),
⋮----
.long("delinquent-slot-distance")
⋮----
.value_name("SLOT_DISTANCE")
⋮----
.help(concatcp!(
⋮----
.about(
⋮----
.long("limit")
⋮----
.value_name("LIMIT")
⋮----
.default_value("1000")
.help("Maximum number of transaction signatures to return"),
⋮----
.long("before")
.value_name("TRANSACTION_SIGNATURE")
⋮----
.help("Start with the first signature older than this one"),
⋮----
.long("until")
⋮----
.long("show-transactions")
⋮----
.help("Display the full transactions"),
⋮----
.long("max-percent")
.value_name("PERCENT")
⋮----
.about("Calculate rent-exempt-minimum value for a given account data field length.")
⋮----
.value_name("DATA_LENGTH_OR_MONIKER")
.required(true)
.validator(|s| {
⋮----
.map(|_| ())
.map_err(|e| e.to_string())
⋮----
.help("Display rent in lamports instead of SOL"),
⋮----
pub fn parse_catchup(
⋮----
let node_pubkey = pubkey_of_signer(matches, "node_pubkey", wallet_manager)?;
let mut our_localhost_port = value_t!(matches, "our_localhost", u16).ok();
// if there is no explicitly specified --our-localhost,
// disable the guess mode (= our_localhost_port)
if matches.occurrences_of("our_localhost") == 0 {
⋮----
let node_json_rpc_url = value_t!(matches, "node_json_rpc_url", String).ok();
// requirement of node_pubkey is relaxed only if our_localhost_port
if our_localhost_port.is_none() && node_pubkey.is_none() {
return Err(CliError::BadParameter(
⋮----
.into(),
⋮----
let follow = matches.is_present("follow");
let log = matches.is_present("log");
Ok(CliCommandInfo::without_signers(CliCommand::Catchup {
⋮----
pub fn parse_cluster_ping(
⋮----
let interval = Duration::from_secs(value_t_or_exit!(matches, "interval", u64));
let count = if matches.is_present("count") {
Some(value_t_or_exit!(matches, "count", u64))
⋮----
let timeout = Duration::from_secs(value_t_or_exit!(matches, "timeout", u64));
let blockhash = value_of(matches, BLOCKHASH_ARG.name);
let print_timestamp = matches.is_present("print_timestamp");
let compute_unit_price = value_of(matches, COMPUTE_UNIT_PRICE_ARG.name);
Ok(CliCommandInfo {
⋮----
signers: vec![default_signer.signer_from_path(matches, wallet_manager)?],
⋮----
pub fn parse_get_block(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
let slot = value_of(matches, "slot");
Ok(CliCommandInfo::without_signers(CliCommand::GetBlock {
⋮----
pub fn parse_get_recent_prioritization_fees(
⋮----
let accounts = values_of(matches, "accounts").unwrap_or(vec![]);
let limit_num_slots = value_of(matches, "limit_num_slots");
Ok(CliCommandInfo::without_signers(
⋮----
pub fn parse_get_block_time(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
⋮----
Ok(CliCommandInfo::without_signers(CliCommand::GetBlockTime {
⋮----
pub fn parse_get_epoch(_matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
Ok(CliCommandInfo::without_signers(CliCommand::GetEpoch))
⋮----
pub fn parse_get_epoch_info(_matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
Ok(CliCommandInfo::without_signers(CliCommand::GetEpochInfo))
⋮----
pub fn parse_get_slot(_matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
Ok(CliCommandInfo::without_signers(CliCommand::GetSlot))
⋮----
pub fn parse_get_block_height(_matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
Ok(CliCommandInfo::without_signers(CliCommand::GetBlockHeight))
⋮----
pub fn parse_largest_accounts(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
let filter = if matches.is_present("circulating") {
Some(RpcLargestAccountsFilter::Circulating)
} else if matches.is_present("non_circulating") {
Some(RpcLargestAccountsFilter::NonCirculating)
⋮----
pub fn parse_supply(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
let print_accounts = matches.is_present("print_accounts");
Ok(CliCommandInfo::without_signers(CliCommand::Supply {
⋮----
pub fn parse_total_supply(_matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
Ok(CliCommandInfo::without_signers(CliCommand::TotalSupply))
⋮----
pub fn parse_get_transaction_count(_matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
⋮----
pub fn parse_show_stakes(
⋮----
let use_lamports_unit = matches.is_present("lamports");
⋮----
pubkeys_of_multiple_signers(matches, "vote_account_pubkeys", wallet_manager)?;
let withdraw_authority = pubkey_of(matches, "withdraw_authority");
Ok(CliCommandInfo::without_signers(CliCommand::ShowStakes {
⋮----
pub fn parse_show_validators(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
⋮----
let number_validators = matches.is_present("number");
let reverse_sort = matches.is_present("reverse");
let keep_unstaked_delinquents = matches.is_present("keep_unstaked_delinquents");
let delinquent_slot_distance = value_of(matches, "delinquent_slot_distance");
let sort_order = match value_t_or_exit!(matches, "sort", String).as_str() {
⋮----
_ => unreachable!(),
⋮----
pub fn parse_transaction_history(
⋮----
let address = pubkey_of_signer(matches, "address", wallet_manager)?.unwrap();
let before = match matches.value_of("before") {
Some(signature) => Some(
⋮----
.parse()
.map_err(|err| CliError::BadParameter(format!("Invalid signature: {err}")))?,
⋮----
let until = match matches.value_of("until") {
⋮----
let limit = value_t_or_exit!(matches, "limit", usize);
let show_transactions = matches.is_present("show_transactions");
⋮----
pub async fn process_catchup(
⋮----
let progress_bar = new_spinner_progress_bar();
progress_bar.set_message("Connecting...");
⋮----
let gussed_default = Some(format!("http://localhost:{our_localhost_port}"));
if node_json_rpc_url.is_some() && node_json_rpc_url != gussed_default {
// go to new line to leave this message on console
println!(
⋮----
let (node_client, node_pubkey) = if our_localhost_port.is_some() {
let client = RpcClient::new(node_json_rpc_url.unwrap());
let guessed_default = Some(client.get_identity().await?);
⋮----
(if node_pubkey.is_some() && node_pubkey != guessed_default {
⋮----
.unwrap(),
⋮----
let cluster_nodes = rpc_client.get_cluster_nodes().await?;
⋮----
.iter()
.find(|contact_info| contact_info.pubkey == node_pubkey.to_string())
⋮----
progress_bar.set_message(format!("RPC service not found for {node_pubkey}"));
⋮----
.set_message(format!("Contact information not found for {node_pubkey}"));
⋮----
sleep(sleep_interval);
⋮----
unreachable!()
⋮----
match node_client.get_identity().await {
⋮----
if let ClientErrorKind::Reqwest(err) = err.kind() {
progress_bar.set_message(format!("Connection failed: {err}"));
⋮----
return Err(Box::new(err));
⋮----
return Err(format!(
⋮----
.into());
⋮----
if rpc_client.get_identity().await? == node_pubkey {
return Err(
⋮----
async fn get_slot_while_retrying(
⋮----
match client.get_slot_with_commitment(commitment).await {
⋮----
return Ok(r);
⋮----
return Err(e.into());
⋮----
*retry_count = retry_count.saturating_add(1);
⋮----
println!("Retrying({}/{max_retry_count}): {e}\n", *retry_count);
⋮----
sleep(Duration::from_secs(1));
⋮----
let start_node_slot: i64 = get_slot_while_retrying(
⋮----
.try_into()?;
let start_rpc_slot: i64 = get_slot_while_retrying(
⋮----
let start_slot_distance = start_rpc_slot.saturating_sub(start_node_slot);
⋮----
// humbly retry; the reference node (rpc_client) could be spotty,
// especially if pointing to api.meinnet-beta.solana.com at times
let rpc_slot: i64 = get_slot_while_retrying(
⋮----
let node_slot: i64 = get_slot_while_retrying(
⋮----
progress_bar.finish_and_clear();
return Ok(format!(
⋮----
let slot_distance = rpc_slot.saturating_sub(node_slot);
let slots_per_second = previous_slot_distance.saturating_sub(slot_distance) as f64
/ sleep_interval.as_secs_f64();
let average_time_remaining = if slot_distance == 0 || total_sleep_interval.is_zero() {
"".to_string()
⋮----
let distance_delta = start_slot_distance.saturating_sub(slot_distance);
⋮----
distance_delta as f64 / total_sleep_interval.as_secs_f64();
⋮----
(slot_distance as f64 / average_catchup_slots_per_second).round();
if !average_time_remaining.is_normal() {
⋮----
format!(" (AVG: {average_catchup_slots_per_second:.1} slots/second (falling))")
⋮----
// important not to miss next scheduled lead slots
let total_node_slot_delta = node_slot.saturating_sub(start_node_slot);
⋮----
total_node_slot_delta as f64 / total_sleep_interval.as_secs_f64();
⋮----
.round();
format!(
⋮----
progress_bar.set_message(format!(
⋮----
println!();
⋮----
total_sleep_interval = total_sleep_interval.saturating_add(sleep_interval);
⋮----
pub async fn process_cluster_date(rpc_client: &RpcClient, config: &CliConfig<'_>) -> ProcessResult {
⋮----
.get_account_with_commitment(&sysvar::clock::id(), config.commitment)
⋮----
let clock: Clock = from_account(&clock_account).ok_or_else(|| {
CliError::RpcRequestError("Failed to deserialize clock sysvar".to_string())
⋮----
Ok(config.output_format.formatted_string(&block_time))
⋮----
Err(format!("AccountNotFound: pubkey={}", sysvar::clock::id()).into())
⋮----
pub async fn process_cluster_version(
⋮----
let remote_version = rpc_client.get_version().await?;
⋮----
Ok(format!("{remote_version:?}"))
⋮----
Ok(remote_version.to_string())
⋮----
pub async fn process_first_available_block(rpc_client: &RpcClient) -> ProcessResult {
let first_available_block = rpc_client.get_first_available_block().await?;
Ok(format!("{first_available_block}"))
⋮----
pub fn parse_leader_schedule(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
let epoch = value_of(matches, "epoch");
⋮----
pub async fn process_leader_schedule(
⋮----
let epoch_info = rpc_client.get_epoch_info().await?;
let epoch = epoch.unwrap_or(epoch_info.epoch);
if epoch > epoch_info.epoch.saturating_add(1) {
return Err(format!("Epoch {epoch} is more than one epoch in the future").into());
⋮----
let epoch_schedule = rpc_client.get_epoch_schedule().await?;
let first_slot_in_epoch = epoch_schedule.get_first_slot_in_epoch(epoch);
⋮----
.get_leader_schedule(Some(first_slot_in_epoch))
⋮----
if leader_schedule.is_none() {
⋮----
format!("Unable to fetch leader schedule for slot {first_slot_in_epoch}").into(),
⋮----
let leader_schedule = leader_schedule.unwrap();
⋮----
for (pubkey, leader_slots) in leader_schedule.iter() {
for slot_index in leader_slots.iter() {
if *slot_index >= leader_per_slot_index.len() {
leader_per_slot_index.resize(slot_index.saturating_add(1), "?");
⋮----
let mut leader_schedule_entries = vec![];
for (slot_index, leader) in leader_per_slot_index.iter().enumerate() {
leader_schedule_entries.push(CliLeaderScheduleEntry {
slot: first_slot_in_epoch.saturating_add(slot_index as u64),
leader: leader.to_string(),
⋮----
Ok(config.output_format.formatted_string(&CliLeaderSchedule {
⋮----
pub async fn process_get_recent_priority_fees(
⋮----
let fees = rpc_client.get_recent_prioritization_fees(accounts).await?;
⋮----
let mut total = Saturating(0);
let fees_len: u64 = fees.len().try_into().unwrap();
let num_slots = limit_num_slots.unwrap_or(fees_len).min(fees_len).max(1);
let mut cli_fees = Vec::with_capacity(fees.len());
⋮----
.into_iter()
.skip(fees_len.saturating_sub(num_slots) as usize)
⋮----
min = min.min(prioritization_fee);
max = max.max(prioritization_fee);
⋮----
cli_fees.push(CliPrioritizationFee {
⋮----
Ok(config
⋮----
.formatted_string(&CliPrioritizationFeeStats {
⋮----
average: total.0.checked_div(num_slots).unwrap_or(0),
⋮----
pub async fn process_get_block(
⋮----
.get_slot_with_commitment(CommitmentConfig::finalized())
⋮----
.get_block_with_config(
⋮----
encoding: Some(UiTransactionEncoding::Base64),
commitment: Some(CommitmentConfig::confirmed()),
max_supported_transaction_version: Some(0),
⋮----
.into();
⋮----
Ok(config.output_format.formatted_string(&cli_block))
⋮----
pub async fn process_get_block_time(
⋮----
let timestamp = rpc_client.get_block_time(slot).await?;
⋮----
pub async fn process_get_epoch(rpc_client: &RpcClient, _config: &CliConfig<'_>) -> ProcessResult {
⋮----
Ok(epoch_info.epoch.to_string())
⋮----
pub async fn process_get_epoch_info(
⋮----
.get_recent_performance_samples(Some(60))
⋮----
.ok()
.and_then(|samples| {
let (slots, secs) = samples.iter().fold(
⋮----
slots.saturating_add(*num_slots),
secs.saturating_add((*sample_period_secs).into()),
⋮----
secs.saturating_mul(1000).checked_div(slots)
⋮----
.unwrap_or(clock::DEFAULT_MS_PER_SLOT);
⋮----
.saturating_sub(epoch_info.slot_index);
⋮----
.get_blocks_with_limit(epoch_expected_start_slot, 1)
⋮----
.and_then(|slot_vec| slot_vec.first().cloned())
.unwrap_or(epoch_expected_start_slot);
⋮----
.get_block_time(first_block_in_epoch)
⋮----
.map(|time| {
time.saturating_sub(
⋮----
.saturating_sub(epoch_expected_start_slot)
.saturating_mul(average_slot_time_ms)
.saturating_div(1000) as i64,
⋮----
.get_block_time(epoch_info.absolute_slot)
⋮----
.ok();
⋮----
Ok(config.output_format.formatted_string(&cli_epoch_info))
⋮----
pub async fn process_get_genesis_hash(rpc_client: &RpcClient) -> ProcessResult {
let genesis_hash = rpc_client.get_genesis_hash().await?;
Ok(genesis_hash.to_string())
⋮----
pub async fn process_get_slot(rpc_client: &RpcClient, _config: &CliConfig<'_>) -> ProcessResult {
let slot = rpc_client.get_slot().await?;
Ok(slot.to_string())
⋮----
pub async fn process_get_block_height(
⋮----
let block_height = rpc_client.get_block_height().await?;
Ok(block_height.to_string())
⋮----
pub fn parse_show_block_production(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
let epoch = value_t!(matches, "epoch", Epoch).ok();
let slot_limit = value_t!(matches, "slot_limit", u64).ok();
⋮----
pub async fn process_show_block_production(
⋮----
.get_epoch_info_with_commitment(CommitmentConfig::finalized())
⋮----
return Err(format!("Epoch {epoch} is in the future").into());
⋮----
epoch_schedule.get_last_slot_in_epoch(epoch),
⋮----
std::cmp::max(end_slot.saturating_sub(slot_limit), first_slot_in_epoch)
⋮----
.get_account_with_commitment(&sysvar::slot_history::id(), CommitmentConfig::finalized())
⋮----
.unwrap();
let slot_history: SlotHistory = from_account(&slot_history_account).ok_or_else(|| {
CliError::RpcRequestError("Failed to deserialize slot history".to_string())
⋮----
if start_slot >= slot_history.oldest() && end_slot <= slot_history.newest() {
⋮----
.filter(|slot| slot_history.check(*slot) == slot_history::Check::Found)
.collect();
⋮----
let minimum_ledger_slot = rpc_client.minimum_ledger_slot().await?;
⋮----
progress_bar.println(format!(
⋮----
let confirmed_blocks = rpc_client.get_blocks(start_slot, Some(end_slot)).await?;
⋮----
let start_slot_index = start_slot.saturating_sub(first_slot_in_epoch) as usize;
let end_slot_index = end_slot.saturating_sub(first_slot_in_epoch) as usize;
⋮----
.saturating_sub(start_slot_index)
.saturating_add(1);
let total_blocks_produced = confirmed_blocks.len();
assert!(total_blocks_produced <= total_slots);
let total_slots_skipped = total_slots.saturating_sub(total_blocks_produced);
⋮----
progress_bar.set_message(format!("Fetching leader schedule for epoch {epoch}..."));
⋮----
.get_leader_schedule_with_commitment(Some(start_slot), CommitmentConfig::finalized())
⋮----
return Err(format!("Unable to fetch leader schedule for slot {start_slot}").into());
⋮----
leader_per_slot_index.resize(total_slots, "?".to_string());
⋮----
let pubkey = format_labeled_address(pubkey, &config.address_labels);
⋮----
leader_per_slot_index[slot_index.saturating_sub(start_slot_index)]
.clone_from(&pubkey);
⋮----
let mut individual_slot_status = vec![];
for (leader, slot_index) in leader_per_slot_index.iter().zip(0u64..) {
let slot = start_slot.saturating_add(slot_index);
let slot_count: &mut u64 = leader_slot_count.entry(leader).or_insert(0);
*slot_count = slot_count.saturating_add(1);
let skipped_slots: &mut u64 = leader_skipped_slots.entry(leader).or_insert(0);
⋮----
if confirmed_blocks_index < confirmed_blocks.len() {
⋮----
confirmed_blocks_index = confirmed_blocks_index.saturating_add(1);
⋮----
individual_slot_status.push(CliSlotStatus {
⋮----
leader: (*leader).to_string(),
⋮----
*skipped_slots = skipped_slots.saturating_add(1);
⋮----
.map(|(leader, leader_slots)| {
let skipped_slots = *leader_skipped_slots.get(leader).unwrap();
let blocks_produced = leader_slots.saturating_sub(skipped_slots);
⋮----
identity_pubkey: (**leader).to_string(),
⋮----
leaders.sort_by(|a, b| a.identity_pubkey.partial_cmp(&b.identity_pubkey).unwrap());
⋮----
Ok(config.output_format.formatted_string(&block_production))
⋮----
pub async fn process_largest_accounts(
⋮----
.get_largest_accounts_with_config(RpcLargestAccountsConfig {
commitment: Some(config.commitment),
⋮----
Ok(config.output_format.formatted_string(&largest_accounts))
⋮----
pub async fn process_supply(
⋮----
let supply_response = rpc_client.supply().await?;
let mut supply: CliSupply = supply_response.value.into();
⋮----
Ok(config.output_format.formatted_string(&supply))
⋮----
pub async fn process_total_supply(
⋮----
let supply = rpc_client.supply().await?.value;
Ok(format!(
⋮----
pub async fn process_get_transaction_count(
⋮----
let transaction_count = rpc_client.get_transaction_count().await?;
Ok(transaction_count.to_string())
⋮----
pub async fn process_ping<P, M, C>(
⋮----
let (signal_sender, signal_receiver) = unbounded();
⋮----
let _ = signal_sender.send(());
⋮----
// It's possible to set the ctrl-c handler more than once in testing
⋮----
result => result.expect("Error setting Ctrl-C handler"),
⋮----
let mut cli_pings = vec![];
⋮----
let mut blockhash = rpc_client.get_latest_blockhash().await?;
⋮----
let to = config.signers[0].pubkey();
let compute_unit_limit = if compute_unit_price.is_some() {
let ixs = vec![system_instruction::transfer(
⋮----
.with_compute_unit_config(&ComputeUnitConfig {
⋮----
let message = Message::new(&ixs, Some(&config.signers[0].pubkey()));
ComputeUnitLimit::Static(simulate_for_compute_unit_limit(rpc_client, &message).await?)
⋮----
'mainloop: for seq in 0..count.unwrap_or(u64::MAX) {
⋮----
if fixed_blockhash.is_none() && now.duration_since(blockhash_acquired).as_secs() > 60 {
// Fetch a new blockhash every minute
let new_blockhash = rpc_client.get_new_latest_blockhash(&blockhash).await?;
⋮----
lamports = lamports.saturating_add(1);
⋮----
Message::new(&ixs, Some(&config.signers[0].pubkey()))
⋮----
let (message, _) = resolve_spend_tx_and_check_account_balance(
⋮----
&config.signers[0].pubkey(),
⋮----
tx.try_sign(&config.signers, blockhash)?;
⋮----
.duration_since(UNIX_EPOCH)
.unwrap()
.as_micros();
format!("[{}.{:06}] ", micros / 1_000_000, micros % 1_000_000)
⋮----
match tpu_client.try_send_transaction(&tx).await {
Ok(()) => Ok(*tx.signatures.first().unwrap()),
Err(err) => Err(format!("TPU send error: {err}")),
⋮----
.send_transaction(&tx)
⋮----
.map_err(|err| err.to_string())
⋮----
let signature_status = rpc_client.get_signature_status(&signature).await?;
let elapsed_time = Instant::now().duration_since(transaction_sent);
⋮----
let elapsed_time_millis = elapsed_time.as_millis() as u64;
confirmation_time.push_back(elapsed_time_millis);
⋮----
signature: Some(signature.to_string()),
ms: Some(elapsed_time_millis),
⋮----
timestamp: timestamp(),
⋮----
lamports: Some(lamports),
⋮----
eprint!("{cli_ping_data}");
cli_pings.push(cli_ping_data);
confirmed_count = confirmed_count.saturating_add(1);
⋮----
error: Some(err.to_string()),
⋮----
// Sleep for half a slot
⋮----
.recv_timeout(Duration::from_millis(clock::DEFAULT_MS_PER_SLOT / 2))
.is_ok()
⋮----
submit_count = submit_count.saturating_add(1);
if signal_receiver.recv_timeout(*interval).is_ok() {
⋮----
let confirmation_stats = if !confirmation_time.is_empty() {
let samples: Vec<f64> = confirmation_time.iter().map(|t| *t as f64).collect();
let dist = criterion_stats::Distribution::from(samples.into_boxed_slice());
let mean = dist.mean();
Some(CliPingConfirmationStats {
min: dist.min(),
⋮----
max: dist.max(),
std_dev: dist.std_dev(Some(mean)),
⋮----
source_pubkey: config.signers[0].pubkey().to_string(),
fixed_blockhash: fixed_blockhash.map(|_| blockhash.to_string()),
⋮----
Ok(config.output_format.formatted_string(&cli_ping))
⋮----
pub fn parse_logs(
⋮----
let address = pubkey_of_signer(matches, "address", wallet_manager)?;
let include_votes = matches.is_present("include_votes");
⋮----
Some(address) => RpcTransactionLogsFilter::Mentions(vec![address.to_string()]),
⋮----
Ok(CliCommandInfo::without_signers(CliCommand::Logs { filter }))
⋮----
pub fn process_logs(config: &CliConfig, filter: &RpcTransactionLogsFilter) -> ProcessResult {
⋮----
filter.clone(),
⋮----
match receiver.recv() {
⋮----
println!("Transaction executed in slot {}:", logs.context.slot);
println!("  Signature: {}", logs.value.signature);
⋮----
println!("  Log Messages:");
⋮----
println!("    {log}");
⋮----
return Ok(format!("Disconnected: {err}"));
⋮----
pub fn process_live_slots(config: &CliConfig) -> ProcessResult {
⋮----
let mut message = "".to_string();
let slot_progress = new_spinner_progress_bar();
slot_progress.set_message("Connecting...");
⋮----
slot_progress.set_message("Connected.");
⋮----
slot_progress.println(spacer);
⋮----
if exit.load(Ordering::Relaxed) {
eprintln!("{message}");
client.shutdown().unwrap();
⋮----
if last_root_update.elapsed().as_secs() >= 5 {
⋮----
slots_per_second = root.saturating_sub(last_root) as f64
/ last_root_update.elapsed().as_secs() as f64;
⋮----
message = if slots_per_second.is_nan() {
format!("{new_info:?}")
⋮----
slot_progress.set_message(message.clone());
⋮----
let slot_delta = (new_info.slot as i64).saturating_sub(previous.slot as i64);
let root_delta = (new_info.root as i64).saturating_sub(previous.root as i64);
⋮----
let prev_root = format!(
⋮----
slot_progress.println(&prev_root);
let new_root = format!(
⋮----
slot_progress.println(prev_root);
slot_progress.println(new_root);
⋮----
current = Some(new_info);
⋮----
eprintln!("disconnected: {err}");
⋮----
Ok("".to_string())
⋮----
pub async fn process_show_gossip(rpc_client: &RpcClient, config: &CliConfig<'_>) -> ProcessResult {
⋮----
.map(|node| CliGossipNode::new(node, &config.address_labels))
⋮----
.formatted_string(&CliGossipNodes(nodes)))
⋮----
pub async fn process_show_stakes(
⋮----
use crate::stake::build_stake_state;
// Both vote and identity pubkeys are supported to identify validator stakes.
// For identity pubkeys, fetch corresponding vote pubkey.
⋮----
let vote_account_progress_bar = new_spinner_progress_bar();
vote_account_progress_bar.set_message("Searching for matching vote accounts...");
let vote_accounts = rpc_client.get_vote_accounts().await?;
⋮----
pubkeys.iter().map(|pubkey| pubkey.to_string()).collect();
⋮----
.chain(vote_accounts.delinquent)
.filter_map(|vote_acc| {
(pubkeys.remove(&vote_acc.node_pubkey) || pubkeys.remove(&vote_acc.vote_pubkey))
.then_some(vote_acc.vote_pubkey)
⋮----
if !pubkeys.is_empty() {
return Err(CliError::RpcRequestError(format!(
⋮----
vote_account_progress_bar.finish_and_clear();
⋮----
encoding: Some(solana_account_decoder::UiAccountEncoding::Base64),
⋮----
let stake_account_progress_bar = new_spinner_progress_bar();
stake_account_progress_bar.set_message("Fetching stake accounts...");
// Use server-side filtering if only one vote account is provided
if vote_account_pubkeys.len() == 1 {
program_accounts_config.filters = Some(vec![
// Filter by `StakeStateV2::Stake(_, _)`
⋮----
// Filter by `Delegation::voter_pubkey`, which begins at byte offset 124
⋮----
// withdrawer filter
⋮----
withdraw_authority_pubkey.as_ref(),
⋮----
let filters = program_accounts_config.filters.get_or_insert(vec![]);
filters.push(withdrawer_filter);
⋮----
.get_program_ui_accounts_with_config(&stake::program::id(), program_accounts_config)
⋮----
let stake_history_account = rpc_client.get_account(&stake_history::id()).await?;
let clock_account = rpc_client.get_account(&sysvar::clock::id()).await?;
⋮----
let stake_history = from_account(&stake_history_account).ok_or_else(|| {
CliError::RpcRequestError("Failed to deserialize stake history".to_string())
⋮----
let new_rate_activation_epoch = get_feature_activation_epoch(
⋮----
stake_account_progress_bar.finish_and_clear();
let mut stake_accounts: Vec<CliKeyedStakeState> = vec![];
⋮----
let stake_account: Account = stake_ui_account.decode().expect(
⋮----
if let Ok(stake_state) = stake_account.state() {
⋮----
if vote_account_pubkeys.is_empty() {
stake_accounts.push(CliKeyedStakeState {
stake_pubkey: stake_pubkey.to_string(),
stake_state: build_stake_state(
⋮----
if vote_account_pubkeys.is_empty()
|| vote_account_pubkeys.contains(&stake.delegation.voter_pubkey.to_string())
⋮----
if stake_accounts.is_empty() {
Ok("No stake accounts found".into())
⋮----
.formatted_string(&CliStakeVec::new(stake_accounts)))
⋮----
pub async fn process_wait_for_max_stake(
⋮----
.wait_for_max_stake(config.commitment, max_stake_percent)
⋮----
Ok(format!("Done waiting, took: {}s", now.elapsed().as_secs()))
⋮----
pub async fn process_show_validators(
⋮----
progress_bar.set_message("Fetching vote accounts...");
⋮----
.get_vote_accounts_with_config(RpcGetVoteAccountsConfig {
keep_unstaked_delinquents: Some(keep_unstaked_delinquents),
⋮----
progress_bar.set_message("Fetching block production...");
⋮----
.get_block_production()
⋮----
.map(|(identity, (leader_slots, blocks_produced))| {
⋮----
100. * (leader_slots.saturating_sub(blocks_produced)) as f64 / leader_slots as f64,
⋮----
progress_bar.set_message("Fetching version information...");
⋮----
for contact_info in rpc_client.get_cluster_nodes().await? {
node_version.insert(
⋮----
.and_then(|version| CliVersion::from_str(&version).ok())
.unwrap_or_else(CliVersion::unknown_version),
⋮----
.chain(vote_accounts.delinquent.iter())
.map(|vote_account| vote_account.activated_stake)
⋮----
.sum();
let total_current_stake = total_active_stake.saturating_sub(total_delinquent_stake);
⋮----
.map(|vote_account| {
⋮----
.get(&vote_account.node_pubkey)
.cloned()
⋮----
skip_rate.get(&vote_account.node_pubkey).cloned(),
⋮----
for validator in current_validators.iter() {
⋮----
.entry(validator.version.clone())
.or_default();
*current_validators = current_validators.saturating_add(1);
*current_active_stake = current_active_stake.saturating_add(validator.activated_stake);
⋮----
for validator in delinquent_validators.iter() {
⋮----
*delinquent_validators = delinquent_validators.saturating_add(1);
⋮----
delinquent_active_stake.saturating_add(validator.activated_stake);
⋮----
.chain(delinquent_validators)
⋮----
for validator in validators.iter() {
⋮----
skip_rate_len = skip_rate_len.saturating_add(1);
⋮----
Ok(config.output_format.formatted_string(&cli_validators))
⋮----
pub async fn process_transaction_history(
⋮----
.get_signatures_for_address_with_config(
⋮----
limit: Some(limit),
⋮----
.map(|result| {
⋮----
signature.verbose = Some(CliHistoryVerbose {
⋮----
.formatted_string(&CliHistorySignatureVec::new(cli_signatures)))
⋮----
let mut cli_transactions = vec![];
⋮----
.get_transaction_with_config(
⋮----
transaction_with_meta.transaction.decode().unwrap();
let json_transaction = decoded_transaction.json_encode();
transaction = Some(CliTransaction {
⋮----
slot: Some(slot),
⋮----
prefix: "  ".to_string(),
sigverify_status: vec![],
⋮----
get_transaction_error = Some(format!("{err:?}"));
⋮----
cli_transactions.push(CliTransactionConfirmation {
⋮----
.formatted_string(&CliHistoryTransactionVec::new(cli_transactions)))
⋮----
struct CliRentCalculation {
// lamports_per_* fields are deprecated since all accounts must be rent
// exempt; however, they are kept here for the sake of compatibility.
⋮----
impl CliRentCalculation {
fn build_balance_message(&self, lamports: u64) -> String {
build_balance_message(lamports, self.use_lamports_unit, true)
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
let exempt_minimum = self.build_balance_message(self.rent_exempt_minimum_lamports);
writeln_name_value(f, "Rent-exempt minimum:", &exempt_minimum)
⋮----
impl QuietDisplay for CliRentCalculation {}
impl VerboseDisplay for CliRentCalculation {}
⋮----
pub enum RentLengthValue {
⋮----
impl RentLengthValue {
pub fn length(&self) -> usize {
⋮----
pub struct RentLengthValueError(pub String);
impl FromStr for RentLengthValue {
type Err = RentLengthValueError;
fn from_str(s: &str) -> Result<Self, Self::Err> {
let s = s.to_ascii_lowercase();
match s.as_str() {
"nonce" => Ok(Self::Nonce),
"stake" => Ok(Self::Stake),
"system" => Ok(Self::System),
"vote" => Ok(Self::Vote),
⋮----
.map(Self::Bytes)
.map_err(|_| RentLengthValueError(s)),
⋮----
pub async fn process_calculate_rent(
⋮----
if data_length > MAX_PERMITTED_DATA_LENGTH.try_into().unwrap() {
eprintln!(
⋮----
let rent_account = rpc_client.get_account(&sysvar::rent::id()).await?;
let rent: Rent = rent_account.deserialize_data()?;
let rent_exempt_minimum_lamports = rent.minimum_balance(data_length);
⋮----
Ok(config.output_format.formatted_string(&cli_rent_calculation))
⋮----
mod tests {
⋮----
fn make_tmp_file() -> (String, NamedTempFile) {
let tmp_file = NamedTempFile::new().unwrap();
(String::from(tmp_file.path().to_str().unwrap()), tmp_file)
⋮----
fn test_parse_command() {
let test_commands = get_clap_app("test", "desc", "version");
⋮----
let (default_keypair_file, mut tmp_file) = make_tmp_file();
write_keypair(&default_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
.clone()
.get_matches_from(vec!["test", "cluster-date"]);
assert_eq!(
⋮----
.get_matches_from(vec!["test", "cluster-version"]);
⋮----
.get_matches_from(vec!["test", "block-time", &slot.to_string()]);
⋮----
.get_matches_from(vec!["test", "epoch"]);
⋮----
.get_matches_from(vec!["test", "epoch-info"]);
⋮----
.get_matches_from(vec!["test", "genesis-hash"]);
⋮----
let test_get_slot = test_commands.clone().get_matches_from(vec!["test", "slot"]);
⋮----
.get_matches_from(vec!["test", "total-supply"]);
⋮----
.get_matches_from(vec!["test", "transaction-count"]);
⋮----
let test_ping = test_commands.clone().get_matches_from(vec![

================
File: cli/src/compute_budget.rs
================
pub(crate) enum UpdateComputeUnitLimitResult {
⋮----
fn get_compute_unit_limit_instruction_index(message: &Message) -> Option<usize> {
⋮----
.iter()
.enumerate()
.find_map(|(ix_index, instruction)| {
let ix_program_id = message.program_id(ix_index)?;
⋮----
matches!(
⋮----
.then_some(ix_index)
⋮----
async fn simulate_for_compute_unit_limit_unchecked(
⋮----
let transaction = Transaction::new_unsigned(message.clone());
⋮----
.simulate_transaction_with_config(
⋮----
commitment: Some(rpc_client.commitment()),
⋮----
return Err(err.into());
⋮----
.expect("compute units unavailable");
u32::try_from(units_consumed).map_err(Into::into)
⋮----
pub(crate) async fn simulate_for_compute_unit_limit(
⋮----
if get_compute_unit_limit_instruction_index(message).is_none() {
return Err("No compute unit limit instruction found".into());
⋮----
simulate_for_compute_unit_limit_unchecked(rpc_client, message).await
⋮----
pub(crate) async fn simulate_and_update_compute_unit_limit(
⋮----
let Some(compute_unit_limit_ix_index) = get_compute_unit_limit_instruction_index(message)
⋮----
return Ok(UpdateComputeUnitLimitResult::NoInstructionFound);
⋮----
simulate_for_compute_unit_limit_unchecked(rpc_client, message).await?;
⋮----
.saturating_mul(100_u64.saturating_add(*n as u64))
.saturating_div(100) as u32
⋮----
Ok(UpdateComputeUnitLimitResult::UpdatedInstructionIndex(
⋮----
Ok(UpdateComputeUnitLimitResult::SimulationNotConfigured)
⋮----
pub(crate) struct ComputeUnitConfig {
⋮----
pub(crate) trait WithComputeUnitConfig {
⋮----
impl WithComputeUnitConfig for Vec<Instruction> {
fn with_compute_unit_config(mut self, config: &ComputeUnitConfig) -> Self {
⋮----
self.push(ComputeBudgetInstruction::set_compute_unit_price(
⋮----
self.push(ComputeBudgetInstruction::set_compute_unit_limit(

================
File: cli/src/feature.rs
================
pub enum ForceActivation {
⋮----
pub enum FeatureCliCommand {
⋮----
pub enum CliFeatureStatus {
⋮----
impl PartialOrd for CliFeatureStatus {
fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
Some(self.cmp(other))
⋮----
impl Ord for CliFeatureStatus {
fn cmp(&self, other: &Self) -> Ordering {
⋮----
self_active_slot.cmp(other_active_slot)
⋮----
pub struct CliFeature {
⋮----
impl PartialOrd for CliFeature {
⋮----
impl Ord for CliFeature {
⋮----
(&self.status, &self.id).cmp(&(&other.status, &other.id))
⋮----
pub struct CliFeatures {
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
if !self.features.is_empty() {
writeln!(
⋮----
write!(f, "{software_versions}")?;
⋮----
write!(f, "{feature_sets}")?;
⋮----
Ok(())
⋮----
impl QuietDisplay for CliFeatures {}
impl VerboseDisplay for CliFeatures {}
⋮----
pub struct CliClusterFeatureSets {
⋮----
pub struct CliClusterSoftwareVersions {
⋮----
let mut max_software_version_len = software_version_title.len();
let mut max_stake_percent_len = stake_percent_title.len();
let mut max_rpc_percent_len = rpc_percent_title.len();
⋮----
.iter()
.map(|software_version_stats| {
let stake_percent = format!("{:.2}%", software_version_stats.stake_percent);
let rpc_percent = format!("{:.2}%", software_version_stats.rpc_percent);
let software_version = software_version_stats.software_version.to_string();
max_software_version_len = max_software_version_len.max(software_version.len());
max_stake_percent_len = max_stake_percent_len.max(stake_percent.len());
max_rpc_percent_len = max_rpc_percent_len.max(rpc_percent.len());
⋮----
.collect();
⋮----
let me = self.tool_software_version.to_string() == software_version;
⋮----
writeln!(f)
⋮----
let mut max_software_versions_len = software_versions_title.len();
let mut max_feature_set_len = feature_set_title.len();
⋮----
.map(|feature_set_info| {
⋮----
.map(ToString::to_string)
⋮----
let software_versions = software_versions.join(", ");
⋮----
"unknown".to_string()
⋮----
feature_set_info.feature_set.to_string()
⋮----
let stake_percent = format!("{:.2}%", feature_set_info.stake_percent);
let rpc_percent = format!("{:.2}%", feature_set_info.rpc_percent);
max_software_versions_len = max_software_versions_len.max(software_versions.len());
max_feature_set_len = max_feature_set_len.max(feature_set.len());
⋮----
write!(
⋮----
impl QuietDisplay for CliClusterFeatureSets {}
impl VerboseDisplay for CliClusterFeatureSets {}
⋮----
pub struct CliFeatureSetStats {
⋮----
pub struct CliSoftwareVersionStats {
⋮----
async fn check_rpc_genesis_hash(
⋮----
if let Some(genesis_hash) = cluster_type.get_genesis_hash() {
let rpc_genesis_hash = rpc_client.get_genesis_hash().await?;
⋮----
return Err(format!(
⋮----
.into());
⋮----
pub trait FeatureSubCommands {
⋮----
impl FeatureSubCommands for App<'_, '_> {
fn feature_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Runtime feature management")
.setting(AppSettings::SubcommandRequiredElseHelp)
.subcommand(
⋮----
.about("Query runtime feature status")
.arg(
⋮----
.value_name("ADDRESS")
.validator(is_valid_pubkey)
.index(1)
.multiple(true)
.help("Feature status to query [default: all known features]"),
⋮----
.long("display-all")
.help("display all features regardless of age"),
⋮----
.about("Activate a runtime feature")
⋮----
.value_name("FEATURE_KEYPAIR")
.validator(is_valid_signer)
⋮----
.required(true)
.help("The signer for the feature to activate"),
⋮----
.value_name("CLUSTER")
.possible_values(&ClusterType::STRINGS)
⋮----
.help("The cluster to activate the feature on"),
⋮----
.long("yolo")
.hidden(hidden_unless_forced())
⋮----
.help("Override activation sanity checks. Don't use this flag"),
⋮----
.arg(fee_payer_arg()),
⋮----
.about("Revoke a pending runtime feature")
⋮----
.help("The signer for the feature to revoke"),
⋮----
.help("The cluster to revoke the feature on"),
⋮----
fn known_feature(feature: &Pubkey) -> Result<(), CliError> {
if FEATURE_NAMES.contains_key(feature) {
⋮----
Err(CliError::BadParameter(format!(
⋮----
pub fn parse_feature_subcommand(
⋮----
let response = match matches.subcommand() {
⋮----
let cluster = value_t_or_exit!(matches, "cluster", ClusterType);
let (feature_signer, feature) = signer_of(matches, "feature", wallet_manager)?;
⋮----
signer_of(matches, FEE_PAYER_ARG.name, wallet_manager)?;
let force = match matches.occurrences_of("force") {
⋮----
let signer_info = default_signer.generate_unique_signers(
vec![fee_payer, feature_signer],
⋮----
let feature = feature.unwrap();
known_feature(&feature)?;
⋮----
fee_payer: signer_info.index_of(fee_payer_pubkey).unwrap(),
⋮----
let mut features = if let Some(features) = pubkeys_of(matches, "features") {
⋮----
known_feature(feature)?;
⋮----
FEATURE_NAMES.keys().cloned().collect()
⋮----
matches.is_present("display_all") || features.len() < FEATURE_NAMES.len();
features.sort();
⋮----
_ => unreachable!(),
⋮----
Ok(response)
⋮----
pub async fn process_feature_subcommand(
⋮----
} => process_status(rpc_client, config, features, *display_all).await,
⋮----
} => process_activate(rpc_client, config, *feature, *cluster, *force, *fee_payer).await,
⋮----
} => process_revoke(rpc_client, config, *feature, *cluster, *fee_payer).await,
⋮----
struct FeatureSetStatsEntry {
⋮----
struct ClusterInfoStatsEntry {
⋮----
struct ClusterInfoStats {
⋮----
impl ClusterInfoStats {
fn aggregate_by_feature_set(&self) -> HashMap<u32, FeatureSetStatsEntry> {
⋮----
let map_entry = feature_set_map.entry(*feature_set).or_default();
⋮----
map_entry.software_versions.push(software_version.clone());
⋮----
for stats_entry in feature_set_map.values_mut() {
⋮----
.sort_by(|l, r| l.cmp(r).reverse());
⋮----
fn aggregate_by_software_version(&self) -> HashMap<CliVersion, ClusterInfoStatsEntry> {
⋮----
.entry(software_version.clone())
.or_default();
⋮----
async fn cluster_info_stats(rpc_client: &RpcClient) -> Result<ClusterInfoStats, ClientError> {
⋮----
struct StatsEntry {
⋮----
.get_cluster_nodes()
⋮----
.into_iter()
.map(|contact_info| {
⋮----
contact_info.rpc.is_some(),
⋮----
.and_then(|v| CliVersion::from_str(&v).ok())
.unwrap_or_else(CliVersion::unknown_version),
⋮----
let vote_accounts = rpc_client.get_vote_accounts().await?;
⋮----
.map(|vote_account| vote_account.activated_stake)
.sum();
⋮----
.map(
⋮----
total_active_stake = total_active_stake.saturating_add(activated_stake);
(node_pubkey.clone(), activated_stake)
⋮----
let feature_set = feature_set.unwrap_or(0);
⋮----
.entry((feature_set, version))
⋮----
if let Some(vote_stake) = vote_stakes.get(&node_id) {
*stake_lamports = stake_lamports.saturating_add(*vote_stake);
⋮----
*rpc_nodes_count = rpc_nodes_count.saturating_add(1);
total_rpc_nodes = total_rpc_nodes.saturating_add(1);
⋮----
Ok(ClusterInfoStats {
⋮----
.filter_map(
⋮----
Some((
⋮----
.collect(),
⋮----
async fn feature_activation_allowed(
⋮----
let cluster_info_stats = cluster_info_stats(rpc_client).await?;
let feature_set_stats = cluster_info_stats.aggregate_by_feature_set();
⋮----
.get(&tool_feature_set)
⋮----
.unwrap_or_default();
⋮----
.aggregate_by_software_version()
⋮----
.map(|(software_version, stats)| CliSoftwareVersionStats {
⋮----
software_versions.sort_by(|l, r| l.software_version.cmp(&r.software_version).reverse());
Some(CliClusterSoftwareVersions {
⋮----
.map(|(feature_set, stats_entry)| CliFeatureSetStats {
⋮----
feature_sets.sort_by(|l, r| {
⋮----
.cmp(&r.software_versions[0])
.reverse()
⋮----
.partial_cmp(&r.stake_percent)
.unwrap()
⋮----
l.rpc_percent.partial_cmp(&r.rpc_percent).unwrap().reverse()
⋮----
Some(CliClusterFeatureSets {
⋮----
Ok((
⋮----
pub(super) fn status_from_account(account: Account) -> Option<CliFeatureStatus> {
from_account(&account).map(|feature| match feature.activated_at {
⋮----
async fn get_feature_status(
⋮----
.get_account(feature_id)
⋮----
.map(status_from_account)
.map_err(|e| e.into())
⋮----
pub async fn get_feature_is_active(
⋮----
get_feature_status(rpc_client, feature_id)
⋮----
.map(|status| matches!(status, Some(CliFeatureStatus::Active(_))))
⋮----
pub async fn get_feature_activation_epoch(
⋮----
.ok()
.and_then(|account| from_account(&account))
.and_then(|feature| feature.activated_at);
let epoch_schedule = rpc_client.get_epoch_schedule().await?;
Ok(activation_slot.map(|slot| epoch_schedule.get_epoch(slot)))
⋮----
async fn process_status(
⋮----
let current_slot = rpc_client.get_slot().await?;
⋮----
current_slot.checked_sub(DEFAULT_MAX_ACTIVE_DISPLAY_AGE_SLOTS)
⋮----
let mut features = vec![];
for feature_ids in feature_ids.chunks(MAX_MULTIPLE_ACCOUNTS) {
⋮----
.get_multiple_accounts(feature_ids)
⋮----
.zip(feature_ids)
.map(|(account, feature_id)| {
let feature_name = FEATURE_NAMES.get(feature_id).unwrap();
⋮----
.and_then(status_from_account)
.map(|feature_status| CliFeature {
id: feature_id.to_string(),
description: feature_name.to_string(),
⋮----
.unwrap_or_else(|| {
⋮----
.filter(|feature| match (filter, &feature.status) {
⋮----
features.append(&mut feature_chunk);
⋮----
features.sort_unstable();
⋮----
feature_activation_allowed(rpc_client, features.len() <= 1).await?;
⋮----
Ok(config.output_format.formatted_string(&feature_set))
⋮----
async fn process_activate(
⋮----
check_rpc_genesis_hash(&cluster, rpc_client).await?;
⋮----
.get_multiple_accounts(&[feature_id])
⋮----
.next()
.unwrap();
⋮----
if from_account(&account).is_some() {
return Err(format!("{feature_id} has already been activated").into());
⋮----
if !feature_activation_allowed(rpc_client, false).await?.0 {
⋮----
return Err(
⋮----
.into(),
⋮----
ForceActivation::Yes => println!("FEATURE ACTIVATION FORCED"),
⋮----
return Err("Feature activation is not allowed at this time".into())
⋮----
.get_minimum_balance_for_rent_exemption(Feature::size_of())
⋮----
let blockhash = rpc_client.get_latest_blockhash().await?;
let (message, _) = resolve_spend_tx_and_check_account_balance(
⋮----
&fee_payer.pubkey(),
⋮----
&activate_with_lamports(&feature_id, &fee_payer.pubkey(), lamports),
Some(&fee_payer.pubkey()),
⋮----
transaction.try_sign(&config.signers, blockhash)?;
println!(
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
async fn process_revoke(
⋮----
let account = rpc_client.get_account(&feature_id).await.ok();
match account.and_then(status_from_account) {
⋮----
return Err(format!("{feature_id} has already been fully activated").into());
⋮----
return Err(format!("{feature_id} has not been submitted for activation").into());
⋮----
&[revoke_pending_activation(&feature_id)],

================
File: cli/src/inflation.rs
================
pub enum InflationCliCommand {
⋮----
pub trait InflationSubCommands {
⋮----
impl InflationSubCommands for App<'_, '_> {
fn inflation_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Show inflation information")
.subcommand(
⋮----
.about("Show inflation rewards for a set of addresses")
.arg(pubkey!(
⋮----
.arg(
⋮----
.long("rewards-epoch")
.takes_value(true)
.value_name("EPOCH")
.help("Display rewards for specific epoch [default: latest epoch]"),
⋮----
pub fn parse_inflation_subcommand(
⋮----
let command = match matches.subcommand() {
⋮----
let addresses = pubkeys_of(matches, "addresses").unwrap();
let rewards_epoch = value_of(matches, "rewards_epoch");
⋮----
Ok(CliCommandInfo::without_signers(CliCommand::Inflation(
⋮----
pub async fn process_inflation_subcommand(
⋮----
InflationCliCommand::Show => process_show(rpc_client, config).await,
⋮----
process_rewards(rpc_client, config, addresses, *rewards_epoch).await
⋮----
async fn process_show(rpc_client: &RpcClient, config: &CliConfig<'_>) -> ProcessResult {
let governor = rpc_client.get_inflation_governor().await?;
let current_rate = rpc_client.get_inflation_rate().await?;
⋮----
Ok(config.output_format.formatted_string(&inflation))
⋮----
async fn process_rewards(
⋮----
.get_inflation_reward(addresses, rewards_epoch)
⋮----
.map_err(|err| {
⋮----
format!("Rewards not available for epoch {epoch}")
⋮----
format!("Rewards not available {err}")
⋮----
let epoch_schedule = rpc_client.get_epoch_schedule().await?;
let mut epoch_rewards: Vec<CliKeyedEpochReward> = vec![];
⋮----
let epoch_metadata = if let Some(Some(first_reward)) = rewards.iter().find(|&v| v.is_some()) {
⋮----
for (reward, address) in rewards.iter().zip(addresses) {
⋮----
let block_time = if let Some(block_time) = block_times.get(&reward.effective_slot) {
⋮----
let block_time = rpc_client.get_block_time(reward.effective_slot).await?;
block_times.insert(reward.effective_slot, block_time);
⋮----
epoch_rewards.push(CliKeyedEpochReward {
address: address.to_string(),
⋮----
Some(CliEpochRewardsMetadata {
⋮----
Ok(config.output_format.formatted_string(&cli_rewards))

================
File: cli/src/lib.rs
================
macro_rules! ACCOUNT_STRING {
⋮----
macro_rules! pubkey {
⋮----
extern crate const_format;
pub mod address_lookup_table;
pub mod checks;
pub mod clap_app;
pub mod cli;
pub mod cluster_query;
pub mod compute_budget;
pub mod feature;
pub mod inflation;
pub mod memo;
pub mod nonce;
pub mod program;
pub mod program_v4;
pub mod spend_utils;
pub mod stake;
pub mod test_utils;
pub mod validator_info;
pub mod vote;
pub mod wallet;

================
File: cli/src/main.rs
================
fn parse_settings(matches: &ArgMatches<'_>) -> Result<bool, Box<dyn error::Error>> {
let parse_args = match matches.subcommand() {
⋮----
let Some(config_file) = matches.value_of("config_file") else {
println!(
⋮----
return Ok(false);
⋮----
let mut config = Config::load(config_file).unwrap_or_default();
match matches.subcommand() {
⋮----
if let Some(field) = subcommand_matches.value_of("specific_setting") {
⋮----
commitment.commitment.to_string(),
⋮----
_ => unreachable!(),
⋮----
println_name_value_or(&format!("{field_name}:"), &value, setting_type);
⋮----
println_name_value("Config File:", config_file);
println_name_value_or("RPC URL:", &json_rpc_url, url_setting_type);
println_name_value_or("WebSocket URL:", &websocket_url, ws_setting_type);
println_name_value_or("Keypair Path:", &keypair_path, keypair_setting_type);
println_name_value_or(
⋮----
&commitment.commitment.to_string(),
⋮----
if let Some(url) = subcommand_matches.value_of("json_rpc_url") {
config.json_rpc_url = normalize_to_url_if_moniker(url);
// Revert to a computed `websocket_url` value when `json_rpc_url` is
// changed
config.websocket_url = "".to_string();
⋮----
if let Some(url) = subcommand_matches.value_of("websocket_url") {
config.websocket_url = url.to_string();
⋮----
if let Some(keypair) = subcommand_matches.value_of("keypair") {
config.keypair_path = keypair.to_string();
⋮----
if let Some(commitment) = subcommand_matches.value_of("commitment") {
config.commitment = commitment.to_string();
⋮----
config.save(config_file)?;
⋮----
let filename = value_t_or_exit!(subcommand_matches, "filename", PathBuf);
config.import_address_labels(&filename)?;
⋮----
println!("Address labels imported from {filename:?}");
⋮----
config.export_address_labels(&filename)?;
println!("Address labels exported to {filename:?}");
⋮----
Ok(parse_args)
⋮----
pub fn parse_args<'a>(
⋮----
let config = if let Some(config_file) = matches.value_of("config_file") {
Config::load(config_file).unwrap_or_default()
⋮----
matches.value_of("json_rpc_url").unwrap_or(""),
⋮----
let rpc_timeout = value_t_or_exit!(matches, "rpc_timeout", u64);
⋮----
value_t_or_exit!(matches, "confirm_transaction_initial_timeout", u64);
⋮----
matches.value_of("websocket_url").unwrap_or(""),
⋮----
let default_signer_arg_name = "keypair".to_string();
⋮----
matches.value_of(&default_signer_arg_name).unwrap_or(""),
⋮----
} = parse_command(matches, &default_signer, wallet_manager)?;
if signers.is_empty() {
⋮----
default_signer.generate_unique_signers(vec![None], matches, wallet_manager)
⋮----
signers.extend(signer_info.signers);
⋮----
let verbose = matches.is_present("verbose");
⋮----
matches.value_of("commitment").unwrap_or(""),
⋮----
let address_labels = if matches.is_present("no_address_labels") {
⋮----
let skip_preflight = matches.is_present("skip_preflight");
let use_tpu_client = matches.is_present("use_tpu_client");
Ok((
⋮----
signers: vec![],
⋮----
preflight_commitment: Some(commitment.commitment),
⋮----
async fn main() -> Result<(), Box<dyn error::Error>> {
⋮----
let matches = get_clap_app(
crate_name!(),
crate_description!(),
⋮----
.get_matches();
do_main(&matches)
⋮----
.map_err(|err| DisplayError::new_as_boxed(err).into())
⋮----
async fn do_main(matches: &ArgMatches<'_>) -> Result<(), Box<dyn error::Error>> {
if parse_settings(matches)? {
⋮----
let (mut config, signers) = parse_args(matches, &mut wallet_manager)?;
config.signers = signers.iter().map(|s| s.as_ref()).collect();
let result = process_command(&config).await?;
println!("{result}");
⋮----
Ok(())

================
File: cli/src/memo.rs
================
pub trait WithMemo {
⋮----
impl WithMemo for Vec<Instruction> {
fn with_memo<T: AsRef<str>>(mut self, memo: Option<T>) -> Self {
⋮----
let memo = memo.as_ref();
⋮----
program_id: Pubkey::from(id().to_bytes()),
accounts: vec![],
data: memo.as_bytes().to_vec(),
⋮----
self.push(memo_ix);

================
File: cli/src/nonce.rs
================
pub trait NonceSubCommands {
⋮----
impl NonceSubCommands for App<'_, '_> {
fn nonce_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Assign account authority to a new entity")
.arg(pubkey!(
⋮----
.arg(nonce_authority_arg())
.arg(memo_arg())
.arg(compute_unit_price_arg()),
⋮----
.subcommand(
⋮----
.about("Create a nonce account")
.arg(
⋮----
.index(1)
.value_name("ACCOUNT_KEYPAIR")
.takes_value(true)
.required(true)
.validator(is_valid_signer)
.help("Keypair of the nonce account to fund"),
⋮----
.index(2)
.value_name("AMOUNT")
⋮----
.validator(is_amount_or_all)
.help(
⋮----
.long("seed")
.value_name("STRING")
⋮----
.about("Get the current nonce value")
.alias("get-nonce")
⋮----
.about("Generate a new nonce, rendering the existing nonce useless")
⋮----
.about("Show the contents of a nonce account")
.alias("show-nonce-account")
⋮----
.long("lamports")
.takes_value(false)
.help("Display balance in lamports instead of SOL"),
⋮----
.about("Withdraw SOL from the nonce account")
⋮----
.index(3)
⋮----
.validator(is_amount)
.help("The amount to withdraw from the nonce account, in SOL"),
⋮----
.about(
⋮----
pub fn parse_authorize_nonce_account(
⋮----
let nonce_account = pubkey_of_signer(matches, "nonce_account_pubkey", wallet_manager)?.unwrap();
let new_authority = pubkey_of_signer(matches, "new_authority", wallet_manager)?.unwrap();
let memo = matches.value_of(MEMO_ARG.name).map(String::from);
⋮----
signer_of(matches, NONCE_AUTHORITY_ARG.name, wallet_manager)?;
⋮----
let signer_info = default_signer.generate_unique_signers(
vec![payer_provided, nonce_authority],
⋮----
let compute_unit_price = value_of(matches, COMPUTE_UNIT_PRICE_ARG.name);
Ok(CliCommandInfo {
⋮----
nonce_authority: signer_info.index_of(nonce_authority_pubkey).unwrap(),
⋮----
pub fn parse_nonce_create_account(
⋮----
signer_of(matches, "nonce_account_keypair", wallet_manager)?;
let seed = matches.value_of("seed").map(|s| s.to_string());
⋮----
let nonce_authority = pubkey_of_signer(matches, NONCE_AUTHORITY_ARG.name, wallet_manager)?;
⋮----
vec![payer_provided, nonce_account],
⋮----
nonce_account: signer_info.index_of(nonce_account_pubkey).unwrap(),
⋮----
pub fn parse_get_nonce(
⋮----
pubkey_of_signer(matches, "nonce_account_pubkey", wallet_manager)?.unwrap();
Ok(CliCommandInfo::without_signers(CliCommand::GetNonce(
⋮----
pub fn parse_new_nonce(
⋮----
pub fn parse_show_nonce_account(
⋮----
let use_lamports_unit = matches.is_present("lamports");
Ok(CliCommandInfo::without_signers(
⋮----
pub fn parse_withdraw_from_nonce_account(
⋮----
pubkey_of_signer(matches, "destination_account_pubkey", wallet_manager)?.unwrap();
let lamports = lamports_of_sol(matches, "amount").unwrap();
⋮----
pub(crate) fn parse_upgrade_nonce_account(
⋮----
let nonce_account = pubkey_of(matches, "nonce_account_pubkey").unwrap();
⋮----
/// Check if a nonce account is initialized with the given authority and hash
pub fn check_nonce_account(
⋮----
pub fn check_nonce_account(
⋮----
match state_from_account(nonce_account)? {
⋮----
if &data.blockhash() != nonce_hash {
Err(Error::InvalidHash {
⋮----
expected: data.blockhash(),
⋮----
.into())
⋮----
Err(Error::InvalidAuthority {
⋮----
Ok(())
⋮----
State::Uninitialized => Err(Error::InvalidStateForOperation.into()),
⋮----
pub async fn process_authorize_nonce_account(
⋮----
let latest_blockhash = rpc_client.get_latest_blockhash().await?;
⋮----
let ixs = vec![authorize_nonce_account(
⋮----
.with_memo(memo)
.with_compute_unit_config(&ComputeUnitConfig {
⋮----
let mut message = Message::new(&ixs, Some(&config.signers[0].pubkey()));
simulate_and_update_compute_unit_limit(&compute_unit_limit, rpc_client, &mut message).await?;
⋮----
tx.try_sign(&config.signers, latest_blockhash)?;
check_account_for_fee_with_commitment(
⋮----
&config.signers[0].pubkey(),
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
pub async fn process_create_nonce_account(
⋮----
let nonce_account_pubkey = config.signers[nonce_account].pubkey();
⋮----
check_unique_pubkeys(
(&config.signers[0].pubkey(), "cli keypair".to_string()),
(&nonce_account_address, "nonce_account".to_string()),
⋮----
.get_minimum_balance_for_rent_exemption(State::size())
⋮----
let nonce_authority = nonce_authority.unwrap_or_else(|| config.signers[0].pubkey());
⋮----
let ixs = if let Some(seed) = seed.clone() {
create_nonce_account_with_seed(
&config.signers[0].pubkey(), // from
&nonce_account_address,      // to
&nonce_account_pubkey,       // base
&seed,                       // seed
⋮----
create_nonce_account(
⋮----
Message::new(&ixs, Some(&config.signers[0].pubkey()))
⋮----
let (message, lamports) = resolve_spend_tx_and_check_account_balance(
⋮----
if let Ok(nonce_account) = get_account(rpc_client, &nonce_account_address).await {
let err_msg = if state_from_account(&nonce_account).is_ok() {
format!("Nonce account {nonce_account_address} already exists")
⋮----
format!("Account {nonce_account_address} already exists and is not a nonce account")
⋮----
return Err(CliError::BadParameter(err_msg).into());
⋮----
return Err(CliError::BadParameter(format!(
⋮----
.into());
⋮----
pub async fn process_get_nonce(
⋮----
match get_account_with_commitment(rpc_client, nonce_account_pubkey, config.commitment)
⋮----
.and_then(|ref a| state_from_account(a))?
⋮----
State::Uninitialized => Ok("Nonce account is uninitialized".to_string()),
State::Initialized(ref data) => Ok(format!("{:?}", data.blockhash())),
⋮----
pub async fn process_new_nonce(
⋮----
(nonce_account, "nonce_account_pubkey".to_string()),
⋮----
if let Err(err) = rpc_client.get_account(nonce_account).await {
⋮----
let ixs = vec![advance_nonce_account(
⋮----
pub async fn process_show_nonce_account(
⋮----
get_account_with_commitment(rpc_client, nonce_account_pubkey, config.commitment).await?;
⋮----
match state_from_account(&nonce_account)? {
⋮----
cli_nonce_account.nonce = Some(data.blockhash().to_string());
⋮----
Some(data.fee_calculator.lamports_per_signature);
cli_nonce_account.authority = Some(data.authority.to_string());
⋮----
Ok(config.output_format.formatted_string(&cli_nonce_account))
⋮----
pub async fn process_withdraw_from_nonce_account(
⋮----
let ixs = vec![withdraw_nonce_account(
⋮----
pub(crate) async fn process_upgrade_nonce_account(
⋮----
let ixs = vec![upgrade_nonce_account(nonce_account)]
⋮----
mod tests {
⋮----
fn make_tmp_file() -> (String, NamedTempFile) {
let tmp_file = NamedTempFile::new().unwrap();
(String::from(tmp_file.path().to_str().unwrap()), tmp_file)
⋮----
fn test_parse_command() {
let test_commands = get_clap_app("test", "desc", "version");
⋮----
let (default_keypair_file, mut tmp_file) = make_tmp_file();
write_keypair(&default_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
let (keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&nonce_account_keypair, tmp_file.as_file_mut()).unwrap();
let nonce_account_pubkey = nonce_account_keypair.pubkey();
let nonce_account_string = nonce_account_pubkey.to_string();
let (authority_keypair_file, mut tmp_file2) = make_tmp_file();
⋮----
write_keypair(&nonce_authority_keypair, tmp_file2.as_file_mut()).unwrap();
// Test AuthorizeNonceAccount Subcommand
let test_authorize_nonce_account = test_commands.clone().get_matches_from(vec![
⋮----
assert_eq!(
⋮----
let test_create_nonce_account = test_commands.clone().get_matches_from(vec![
⋮----
let test_get_nonce = test_commands.clone().get_matches_from(vec![
⋮----
.clone()
.get_matches_from(vec!["test", "new-nonce", &keypair_file]);
let nonce_account = read_keypair_file(&keypair_file).unwrap();
⋮----
let test_new_nonce = test_commands.clone().get_matches_from(vec![
⋮----
let test_show_nonce_account = test_commands.clone().get_matches_from(vec![
⋮----
let test_withdraw_from_nonce_account = test_commands.clone().get_matches_from(vec![
⋮----
let test_upgrade_nonce_account = test_commands.clone().get_matches_from(vec![
⋮----
fn test_check_nonce_account() {
⋮----
let blockhash = *durable_nonce.as_hash();
⋮----
assert!(check_nonce_account(&valid.unwrap(), &nonce_pubkey, &blockhash).is_ok());
⋮----
check_nonce_account(&invalid_owner.unwrap(), &nonce_pubkey, &blockhash).unwrap_err()
⋮----
assert_eq!(err, Error::InvalidAccountOwner,);
⋮----
check_nonce_account(&invalid_data.unwrap(), &nonce_pubkey, &blockhash).unwrap_err()
⋮----
assert_eq!(err, Error::InvalidAccountData,);
⋮----
let invalid_durable_nonce = DurableNonce::from_blockhash(&hash(b"invalid"));
⋮----
let invalid_hash = Account::new_data(1, &data, &system_program::ID).unwrap();
⋮----
check_nonce_account(&invalid_hash, &nonce_pubkey, &blockhash).unwrap_err()
⋮----
check_nonce_account(&invalid_authority.unwrap(), &nonce_pubkey, &blockhash).unwrap_err()
⋮----
check_nonce_account(&invalid_state.unwrap(), &nonce_pubkey, &blockhash).unwrap_err()
⋮----
assert_eq!(err, Error::InvalidStateForOperation,);
⋮----
fn test_account_identity_ok() {
let nonce_account = nonce_account::create_account(1).into_inner();
assert_eq!(account_identity_ok(&nonce_account), Ok(()));
⋮----
fn test_state_from_account() {
let mut nonce_account = nonce_account::create_account(1).into_inner();
assert_eq!(state_from_account(&nonce_account), Ok(State::Uninitialized));
⋮----
.set_state(&Versions::new(State::Initialized(data.clone())))
.unwrap();
⋮----
fn test_data_from_helpers() {
⋮----
let state = state_from_account(&nonce_account).unwrap();
⋮----
assert_eq!(data_from_state(&state), Ok(&data));
assert_eq!(data_from_account(&nonce_account), Ok(data));

================
File: cli/src/program_v4.rs
================
pub struct AdditionalCliConfig {
⋮----
impl AdditionalCliConfig {
fn from_matches(matches: &ArgMatches<'_>) -> Self {
⋮----
use_rpc: matches.is_present("use-rpc"),
sign_only: matches.is_present(SIGN_ONLY_ARG.name),
dump_transaction_message: matches.is_present(DUMP_TRANSACTION_MESSAGE.name),
⋮----
compute_unit_price: value_t!(matches, "compute_unit_price", u64).ok(),
⋮----
pub enum ProgramV4CliCommand {
⋮----
pub trait ProgramV4SubCommands {
⋮----
impl ProgramV4SubCommands for App<'_, '_> {
fn program_v4_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Program V4 management")
.setting(AppSettings::SubcommandRequiredElseHelp)
.subcommand(
⋮----
.about("Deploy a new or redeploy an existing program")
.arg(
⋮----
.index(1)
.value_name("PATH-TO-ELF")
.takes_value(true)
.help("./target/deploy/program.so"),
⋮----
.long("start-offset")
.value_name("START_OFFSET")
⋮----
.help("Optionally starts writing at this byte offset"),
⋮----
.long("end-offset")
.value_name("END_OFFSET")
⋮----
.help("Optionally stops writing after this byte offset"),
⋮----
.long("program-keypair")
.value_name("PROGRAM_SIGNER")
⋮----
.validator(is_valid_signer)
.help("Program account signer for deploying a new program"),
⋮----
.long("program-id")
.value_name("PROGRAM_ID")
⋮----
.help("Program address for redeploying an existing program"),
⋮----
.long("buffer")
.value_name("BUFFER_SIGNER")
⋮----
.help("Optional intermediate buffer account to write data to"),
⋮----
.long("authority")
.value_name("AUTHORITY_SIGNER")
⋮----
.help(
⋮----
.arg(Arg::with_name("use-rpc").long("use-rpc").help(
⋮----
.offline_args()
.arg(compute_unit_price_arg()),
⋮----
.about("Reverse deployment or close a program entirely")
⋮----
.required(true)
.help("Executable program's address"),
⋮----
.long("close-program-entirely")
.help("Reset the program account and retrieve its funds"),
⋮----
.about("Transfer the authority of a program to a different address")
⋮----
.long("new-authority")
.value_name("NEW_AUTHORITY_SIGNER")
⋮----
.help("New program authority"),
⋮----
.about("Finalize a program to make it immutable")
⋮----
.long("next-version")
.value_name("NEXT_VERSION")
⋮----
.about("Display information about a buffer or program")
⋮----
.long("all")
.conflicts_with("program-id")
.conflicts_with("authority")
.help("Show accounts for all authorities"),
⋮----
.arg(pubkey!(
⋮----
.about("Download the executable of a program to a file")
⋮----
pub fn parse_program_v4_subcommand(
⋮----
let (subcommand, sub_matches) = matches.subcommand();
⋮----
let mut bulk_signers = vec![Some(
⋮----
.value_of("path-to-elf")
.map(|location| location.to_string());
let program_address = pubkey_of(matches, "program-id");
⋮----
signer_of(matches, "program-keypair", wallet_manager)
⋮----
bulk_signers.push(program_signer);
Some(program_pubkey)
⋮----
pubkey_of_signer(matches, "program-keypair", wallet_manager)?
⋮----
signer_of(matches, "buffer", wallet_manager)
⋮----
if program_address.is_none() && program_pubkey.is_none() {
program_pubkey = Some(buffer_pubkey);
⋮----
bulk_signers.push(buffer_signer);
Some(buffer_pubkey)
⋮----
pubkey_of_signer(matches, "buffer", wallet_manager)?
⋮----
let (authority, authority_pubkey) = signer_of(matches, "authority", wallet_manager)?;
bulk_signers.push(authority);
⋮----
default_signer.generate_unique_signers(bulk_signers, matches, wallet_manager)?;
let program_signer_index = signer_info.index_of_or_none(program_pubkey);
let buffer_signer_index = signer_info.index_of_or_none(buffer_pubkey);
let upload_signer_index = buffer_signer_index.or(program_signer_index);
⋮----
.index_of(authority_pubkey)
.expect("Authority signer is missing");
assert!(
⋮----
program_address: program_address.or(program_pubkey).unwrap(),
⋮----
upload_signer_index: path_to_elf.as_ref().and(upload_signer_index),
⋮----
upload_range: value_t!(matches, "start-offset", usize).ok()
..value_t!(matches, "end-offset", usize).ok(),
⋮----
program_address: pubkey_of(matches, "program-id")
.expect("Program address is missing"),
⋮----
.expect("Authority signer is missing"),
close_program_entirely: matches.is_present("close-program-entirely"),
⋮----
signer_of(matches, "new-authority", wallet_manager)?;
bulk_signers.push(new_authority);
⋮----
.index_of(new_authority_pubkey)
.expect("New authority signer is missing"),
⋮----
signer_of(matches, "next-version", wallet_manager)
⋮----
bulk_signers.push(next_version);
⋮----
next_version_signer_index: pubkey_of(matches, "next-version")
.and_then(|pubkey| signer_info.index_of(Some(pubkey)))
.unwrap_or(authority_signer_index),
⋮----
if let Some(authority) = pubkey_of_signer(matches, "authority", wallet_manager)? {
⋮----
.signer_from_path(matches, wallet_manager)?
.pubkey()
⋮----
account_pubkey: pubkey_of(matches, "program-id"),
⋮----
all: matches.is_present("all"),
⋮----
output_location: matches.value_of("path-to-elf").unwrap().to_string(),
⋮----
_ => unreachable!(),
⋮----
Ok(response)
⋮----
pub async fn process_program_v4_subcommand(
⋮----
.map_err(|err| format!("Unable to open program file: {err}"))?;
file.read_to_end(&mut program_data)
.map_err(|err| format!("Unable to read program file: {err}"))?;
⋮----
process_deploy_program(
⋮----
buffer_address.as_ref(),
upload_signer_index.as_ref(),
⋮----
upload_range.clone(),
⋮----
process_retract_program(
⋮----
process_transfer_authority_of_program(
⋮----
process_finalize_program(
⋮----
} => process_show(rpc_client, config, *account_pubkey, *authority, *all).await,
⋮----
} => process_dump(rpc_client, config, *account_pubkey, output_location).await,
⋮----
pub async fn process_deploy_program(
⋮----
let payer_pubkey = config.signers[0].pubkey();
let authority_pubkey = config.signers[*auth_signer_index].pubkey();
// Check requested command makes sense given the on-chain state
⋮----
.get_account_with_commitment(program_address, config.commitment)
⋮----
.get_account_with_commitment(buffer_address, config.commitment)
⋮----
.get_minimum_balance_for_rent_exemption(
LoaderV4State::program_data_offset().saturating_add(program_data.len()),
⋮----
.as_ref()
.map(|account| loader_v4::check_id(&account.owner))
.unwrap_or(false);
⋮----
.map(|index| &config.signers[*index].pubkey() == program_address)
.unwrap_or(false)
⋮----
// Deploy new program
⋮----
return Err(
⋮----
.into(),
⋮----
// Redeploy an existing program
⋮----
if let Some(program_account) = program_account.as_ref() {
⋮----
return Err(format!("{program_address} is not owned by loader-v4").into());
⋮----
if let Some(buffer_account) = buffer_account.as_ref() {
⋮----
return Err(format!("{} is not owned by loader-v4", buffer_address.unwrap()).into());
⋮----
// Download feature set
⋮----
.keys()
.cloned()
⋮----
.chunks(MAX_MULTIPLE_ACCOUNTS)
⋮----
.get_multiple_accounts(feature_ids)
⋮----
.into_iter()
.zip(feature_ids)
.for_each(|(account, feature_id)| {
let activation_slot = account.and_then(status_from_account);
⋮----
feature_set.activate(feature_id, slot);
⋮----
&feature_set.runtime_features(),
⋮----
feature_set.is_active(&raise_cpi_nesting_limit_to_8::id()),
⋮----
.unwrap();
// Verify the program
⋮----
upload_range.start.unwrap_or(0)..upload_range.end.unwrap_or(program_data.len());
⋮----
(MAX_PERMITTED_DATA_LENGTH as usize).saturating_sub(LoaderV4State::program_data_offset());
if program_data.len() > MAX_LEN {
return Err(format!(
⋮----
.into());
⋮----
if upload_range.end > program_data.len() {
⋮----
.map_err(|err| format!("ELF error: {err}"))?;
⋮----
// Create and add retract and set_program_length instructions
⋮----
build_retract_instruction(program_account, program_address, &authority_pubkey)?
⋮----
initial_instructions.insert(0, retract_instruction);
⋮----
build_set_program_length_instructions(
rpc_client.clone(),
⋮----
program_data.len() as u32,
⋮----
if !set_program_length_instructions.is_empty() {
initial_instructions.append(&mut set_program_length_instructions);
⋮----
let upload_address = buffer_address.unwrap_or(program_address);
let (upload_account, upload_account_length) = if buffer_address.is_some() {
(buffer_account, upload_range.len())
⋮----
(program_account, program_data.len())
⋮----
.map(|account| account.lamports)
.unwrap_or(0);
// Create and add create_buffer message
if let Some(upload_account) = upload_account.as_ref() {
⋮----
initial_instructions.append(&mut vec![
⋮----
initial_instructions.append(&mut instruction::create_buffer(
⋮----
// Create and add write messages
let mut write_messages = vec![];
if upload_signer_index.is_none() {
if upload_account.is_none() {
⋮----
if upload_range.is_empty() {
return Err(format!("Attempting to upload empty range {upload_range:?}").into());
⋮----
Some(&payer_pubkey),
⋮----
let chunk_size = calculate_max_chunk_size(first_write_message);
for (chunk, i) in program_data[upload_range.clone()]
.chunks(chunk_size)
.zip(0usize..)
⋮----
write_messages.push(vec![instruction::write(
⋮----
let final_instructions = if buffer_address == Some(program_address) {
// Upload to buffer only and skip actual deployment
⋮----
// Redeployment with a buffer
vec![
⋮----
// Initial deployment or redeployment without a buffer
vec![instruction::deploy(program_address, &authority_pubkey)]
⋮----
send_messages(
⋮----
if initial_instructions.is_empty() {
⋮----
vec![initial_instructions]
⋮----
if final_instructions.is_empty() {
⋮----
vec![final_instructions]
⋮----
lamports_required.saturating_sub(existing_lamports),
config.output_format.formatted_string(&CliProgramId {
program_id: program_address.to_string(),
⋮----
async fn process_retract_program(
⋮----
return Err("Program account does not exist".into());
⋮----
build_retract_instruction(&program_account, program_address, &authority_pubkey)?;
⋮----
instructions.push(retract_instruction);
⋮----
instructions.push(set_program_length_instruction);
} else if instructions.is_empty() {
return Err("Program is retracted already".into());
⋮----
vec![instructions],
⋮----
async fn process_transfer_authority_of_program(
⋮----
return Err(format!("Unable to find the account {program_address}").into());
⋮----
let new_authority_pubkey = config.signers[*new_auth_signer_index].pubkey();
let messages = vec![vec![instruction::transfer_authority(
⋮----
async fn process_finalize_program(
⋮----
let next_version_pubkey = config.signers[*next_version_signer_index].pubkey();
let messages = vec![vec![instruction::finalize(
⋮----
async fn process_show(
⋮----
.get_account_with_commitment(&program_address, config.commitment)
⋮----
Ok(config.output_format.formatted_string(&CliProgramV4 {
⋮----
owner: account.owner.to_string(),
authority: state.authority_address_or_next_version.to_string(),
⋮----
.len()
.saturating_sub(LoaderV4State::program_data_offset()),
status: status.to_string(),
⋮----
Err(format!("{program_address} program state is invalid").into())
⋮----
Err(format!("{program_address} is not owned by loader-v4").into())
⋮----
Err(format!("Unable to find the account {program_address}").into())
⋮----
let authority_pubkey = if all { None } else { Some(authority) };
let programs = get_programs(rpc_client, config, authority_pubkey).await?;
Ok(config.output_format.formatted_string(&programs))
⋮----
pub async fn process_dump(
⋮----
.get_account_with_commitment(&account_pubkey, config.commitment)
⋮----
f.write_all(&account.data[LoaderV4State::program_data_offset()..])?;
Ok(format!("Wrote program to {output_location}"))
⋮----
Err(format!("{account_pubkey} is not owned by loader-v4").into())
⋮----
Err(format!("Unable to find the account {account_pubkey}").into())
⋮----
Err("No account specified".into())
⋮----
pub fn process_deploy_program_sync(
⋮----
tokio::runtime::Handle::current().block_on(async {
// Convert blocking RpcClient to nonblocking
⋮----
rpc_client_blocking.url(),
rpc_client_blocking.commitment(),
⋮----
/// Synchronous wrapper for process_dump
/// Use this when calling from a blocking context within an async runtime
⋮----
/// Use this when calling from a blocking context within an async runtime
/// Accepts a blocking RpcClient and converts it internally to nonblocking
⋮----
/// Accepts a blocking RpcClient and converts it internally to nonblocking
///
⋮----
///
/// # Performance Note
⋮----
/// # Performance Note
/// This function creates a new RpcClient on each call and has overhead from
⋮----
/// This function creates a new RpcClient on each call and has overhead from
/// sync-async-sync bridging. For better performance, consider using the async
⋮----
/// sync-async-sync bridging. For better performance, consider using the async
/// `process_dump()` directly with a nonblocking RpcClient.
⋮----
/// `process_dump()` directly with a nonblocking RpcClient.
#[deprecated(
⋮----
pub fn process_dump_sync(
⋮----
process_dump(
⋮----
async fn send_messages(
⋮----
.get_blockhash(&rpc_client, config.commitment)
⋮----
let mut messages = Vec::with_capacity(message_prototypes.len());
for instructions in message_prototypes.into_iter() {
⋮----
&instructions.with_compute_unit_config(&compute_unit_config),
⋮----
simulate_and_update_compute_unit_limit(
⋮----
messages.push(message);
⋮----
let initial_messages = simulate_messages(initial_messages).await?;
let write_messages = simulate_messages(write_messages).await?;
let final_messages = simulate_messages(final_messages).await?;
let mut fee = Saturating(0);
for message in initial_messages.iter() {
fee += rpc_client.get_fee_for_message(message).await?;
⋮----
for message in final_messages.iter() {
⋮----
// Assume all write messages cost the same
if let Some(message) = write_messages.first() {
⋮----
.get_fee_for_message(message)
⋮----
.saturating_mul(write_messages.len() as u64);
⋮----
check_account_for_spend_and_fee_with_commitment(
⋮----
.map(|signer_index| {
⋮----
.iter()
.find(|signer| signer.pubkey() == key)
.unwrap()
⋮----
tx.try_sign(&signers, blockhash)?;
⋮----
return_signers_with_config(
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
.map_err(|err| format!("Failed to send message: {err}").into())
.map(|_| String::new())
⋮----
for message in initial_messages.into_iter() {
let result = send_or_return_message(message).await?;
⋮----
return Ok(result);
⋮----
if !write_messages.is_empty() {
⋮----
.send_and_confirm_messages_with_spinner(
⋮----
// `solana_client` type currently required by `send_and_confirm_transactions_in_parallel_v2`
Some(
⋮----
.expect("Should return a valid tpu client"),
⋮----
send_and_confirm_transactions_in_parallel_v2(
⋮----
resign_txs_count: Some(5),
⋮----
.map_err(|err| format!("Data writes to account failed: {err}"))?
⋮----
.flatten()
⋮----
if !transaction_errors.is_empty() {
⋮----
error!("{transaction_error:?}");
⋮----
return Err(format!("{} write transactions failed", transaction_errors.len()).into());
⋮----
for message in final_messages.into_iter() {
⋮----
Ok(ok_result)
⋮----
fn build_retract_instruction(
⋮----
return Ok(None);
⋮----
"Program authority does not match with the provided authority address".into(),
⋮----
Retracted => Ok(None),
LoaderV4Status::Deployed => Ok(Some(instruction::retract(buffer_address, authority))),
LoaderV4Status::Finalized => Err("Program is immutable".into()),
⋮----
Err("Program account's state could not be deserialized".into())
⋮----
async fn build_set_program_length_instructions(
⋮----
LoaderV4State::program_data_offset().saturating_add(program_data_length as usize);
⋮----
.get_minimum_balance_for_rent_exemption(expected_account_data_len)
⋮----
return Ok((Vec::default(), lamports_required));
⋮----
if !account.data.is_empty() {
⋮----
if matches!(status, LoaderV4Status::Finalized) {
return Err("Program is immutable".into());
⋮----
return Err("Program account's state could not be deserialized".into());
⋮----
match account.data.len().cmp(&expected_account_data_len) {
⋮----
let extra_lamports_required = lamports_required.saturating_sub(account.lamports);
Ok((
⋮----
Ok((vec![set_program_length_instruction], 0))
⋮----
return Err("Program account has less lamports than required for its size".into());
⋮----
Ok((vec![], 0))
⋮----
async fn get_accounts_with_filter(
⋮----
.get_program_ui_accounts_with_config(
⋮----
filters: Some(filters),
⋮----
encoding: Some(UiAccountEncoding::Base64),
data_slice: Some(UiDataSliceConfig { offset: 0, length }),
⋮----
Ok(results)
⋮----
async fn get_programs(
⋮----
vec![]
⋮----
let results = get_accounts_with_filter(
⋮----
let mut programs = vec![];
for (program, account) in results.iter() {
let data_bytes = account.data.decode().expect(
⋮----
if let Ok(state) = solana_loader_v4_program::get_state(data_bytes.as_slice()) {
⋮----
programs.push(CliProgramV4 {
program_id: program.to_string(),
⋮----
return Err(format!("Error parsing Program account {program}").into());
⋮----
Ok(CliProgramsV4 { programs })
⋮----
mod tests {
⋮----
fn program_authority() -> solana_keypair::Keypair {
keypair_from_seed(&[3u8; 32]).unwrap()
⋮----
fn rpc_client_no_existing_program() -> RpcClient {
RpcClient::new_mock("succeeds".to_string())
⋮----
fn rpc_client_with_program_data(data: &str, loader_is_owner: bool) -> RpcClient {
⋮----
let account_info_response = json!(Response {
⋮----
mocks.insert(RpcRequest::GetAccountInfo, account_info_response);
RpcClient::new_mock_with_mocks("".to_string(), mocks)
⋮----
fn rpc_client_wrong_account_owner() -> RpcClient {
rpc_client_with_program_data(
⋮----
fn rpc_client_wrong_authority() -> RpcClient {
⋮----
fn rpc_client_with_program_retracted() -> RpcClient {
⋮----
fn rpc_client_with_program_deployed() -> RpcClient {
⋮----
fn rpc_client_with_program_finalized() -> RpcClient {
⋮----
async fn test_deploy() {
⋮----
let mut file = File::open("tests/fixtures/noop.so").unwrap();
file.read_to_end(&mut program_data).unwrap();
let payer = keypair_from_seed(&[1u8; 32]).unwrap();
let program_signer = keypair_from_seed(&[2u8; 32]).unwrap();
let authority_signer = program_authority();
config.signers.push(&payer);
config.signers.push(&program_signer);
config.signers.push(&authority_signer);
assert!(process_deploy_program(
⋮----
async fn test_redeploy() {
⋮----
async fn test_redeploy_from_source() {
⋮----
let buffer_signer = keypair_from_seed(&[2u8; 32]).unwrap();
⋮----
config.signers.push(&buffer_signer);
⋮----
async fn test_retract() {
⋮----
assert!(process_retract_program(
⋮----
async fn test_transfer_authority() {
⋮----
let new_authority_signer = program_authority();
⋮----
config.signers.push(&new_authority_signer);
assert!(process_transfer_authority_of_program(
⋮----
async fn test_finalize() {
⋮----
let next_version_signer = keypair_from_seed(&[4u8; 32]).unwrap();
⋮----
config.signers.push(&next_version_signer);
assert!(process_finalize_program(
⋮----
fn make_tmp_path(name: &str) -> String {
let out_dir = std::env::var("FARF_DIR").unwrap_or_else(|_| "farf".to_string());
⋮----
let path = format!("{}/tmp/{}-{}", out_dir, name, keypair.pubkey());
⋮----
fn test_cli_parse_deploy() {
let test_commands = get_clap_app("test", "desc", "version");
⋮----
let keypair_file = make_tmp_path("keypair_file");
write_keypair_file(&default_keypair, &keypair_file).unwrap();
⋮----
let program_keypair_file = make_tmp_path("program_keypair_file");
write_keypair_file(&program_keypair, &program_keypair_file).unwrap();
⋮----
let buffer_keypair_file = make_tmp_path("buffer_keypair_file");
write_keypair_file(&buffer_keypair, &buffer_keypair_file).unwrap();
⋮----
let authority_keypair_file = make_tmp_path("authority_keypair_file");
write_keypair_file(&authority_keypair, &authority_keypair_file).unwrap();
let test_command = test_commands.clone().get_matches_from(vec![
⋮----
assert_eq!(
⋮----
fn test_cli_parse_retract() {
⋮----
fn test_cli_parse_transfer_authority() {
⋮----
let new_authority_keypair_file = make_tmp_path("new_authority_keypair_file");
write_keypair_file(&new_authority_keypair, &new_authority_keypair_file).unwrap();
⋮----
fn test_cli_parse_finalize() {
⋮----
let next_version_keypair_file = make_tmp_path("next_version_keypair_file");
write_keypair_file(&next_version_keypair, &next_version_keypair_file).unwrap();

================
File: cli/src/program.rs
================
pub enum ProgramCliCommand {
⋮----
pub trait ProgramSubCommands {
⋮----
impl ProgramSubCommands for App<'_, '_> {
fn program_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Program management")
.setting(AppSettings::SubcommandRequiredElseHelp)
.arg(
⋮----
.long("skip-fee-check")
.hidden(hidden_unless_forced())
.takes_value(false)
.global(true),
⋮----
.subcommand(
⋮----
.about("Deploy an upgradeable program")
⋮----
.index(1)
.value_name("PROGRAM_FILEPATH")
.takes_value(true)
.help("/path/to/program.so"),
⋮----
.arg(fee_payer_arg())
⋮----
.long("buffer")
.value_name("BUFFER_SIGNER")
⋮----
.validator(is_valid_signer)
.help(
⋮----
.long("upgrade-authority")
.value_name("UPGRADE_AUTHORITY_SIGNER")
⋮----
.arg(pubkey!(
⋮----
.long("final")
.help("The program will not be upgradeable"),
⋮----
.long("max-len")
.value_name("max_len")
⋮----
.required(false)
⋮----
.long("allow-excessive-deploy-account-balance")
⋮----
.long("max-sign-attempts")
⋮----
.validator(is_parsable::<u64>)
.default_value("5")
⋮----
.arg(Arg::with_name("use_rpc").long("use-rpc").help(
⋮----
.arg(compute_unit_price_arg())
⋮----
.long("no-auto-extend")
⋮----
.help("Don't automatically extend the program's data account size"),
⋮----
.long("skip-feature-verify")
⋮----
.about("Upgrade an upgradeable program")
⋮----
.offline_args(),
⋮----
.about("Writes a program into a buffer account")
⋮----
.required(true)
⋮----
.long("buffer-authority")
.value_name("BUFFER_AUTHORITY_SIGNER")
⋮----
.help("Buffer authority [default: the default configured keypair]"),
⋮----
.about("Set a new buffer authority")
⋮----
.value_name("BUFFER_PUBKEY")
⋮----
.help("Public key of the buffer"),
⋮----
.about("Set a new program authority")
⋮----
.value_name("PROGRAM_ADDRESS")
⋮----
.help("Address of the program to upgrade"),
⋮----
.long("new-upgrade-authority")
.value_name("NEW_UPGRADE_AUTHORITY")
.required_unless("final")
⋮----
.conflicts_with("new_upgrade_authority")
⋮----
.long("skip-new-upgrade-authority-signer-check")
.requires("new_upgrade_authority")
⋮----
.about("Display information about a buffer or program")
⋮----
.value_name("ACCOUNT_ADDRESS")
⋮----
.help("Address of the buffer or program to show"),
⋮----
.long("programs")
.conflicts_with("account")
.conflicts_with("buffers")
.required_unless_one(&["account", "buffers"])
.help("Show every upgradeable program that matches the authority"),
⋮----
.long("buffers")
⋮----
.conflicts_with("programs")
.required_unless_one(&["account", "programs"])
.help("Show every upgradeable buffer that matches the authority"),
⋮----
.long("all")
⋮----
.conflicts_with("buffer_authority")
.help("Show accounts for all authorities"),
⋮----
.long("lamports")
⋮----
.help("Display balance in lamports instead of SOL"),
⋮----
.about("Write the program data to a file")
⋮----
.help("Address of the buffer or program"),
⋮----
.index(2)
.value_name("OUTPUT_FILEPATH")
⋮----
.about("Close a program or buffer account and withdraw all lamports")
⋮----
.help("Address of the program or buffer account to close"),
⋮----
.required_unless("account")
.help("Close all buffer accounts that match the authority"),
⋮----
.long("authority")
.alias("buffer-authority")
.value_name("AUTHORITY_SIGNER")
⋮----
.long("bypass-warning")
⋮----
.help("Bypass the permanent program closure warning"),
⋮----
.about(
⋮----
.value_name("PROGRAM_ID")
⋮----
.validator(is_valid_pubkey)
.help("Address of the program to extend"),
⋮----
.value_name("ADDITIONAL_BYTES")
⋮----
.validator(is_parsable::<u32>)
⋮----
.about("Migrates an upgradeable program to loader-v4")
⋮----
.arg(compute_unit_price_arg()),
⋮----
.setting(AppSettings::Hidden),
⋮----
pub fn parse_program_subcommand(
⋮----
let (subcommand, sub_matches) = matches.subcommand();
let matches_skip_fee_check = matches.is_present("skip_fee_check");
⋮----
.map(|m| m.is_present("skip_fee_check"))
.unwrap_or(false);
⋮----
signer_of(matches, FEE_PAYER_ARG.name, wallet_manager)?;
let mut bulk_signers = vec![
⋮----
fee_payer, // if None, default signer will be supplied
⋮----
.value_of("program_location")
.map(|location| location.to_string());
⋮----
signer_of(matches, "buffer", wallet_manager)
⋮----
bulk_signers.push(buffer_signer);
Some(buffer_pubkey)
⋮----
pubkey_of_signer(matches, "buffer", wallet_manager)?
⋮----
signer_of(matches, "program_id", wallet_manager)
⋮----
bulk_signers.push(program_signer);
Some(program_pubkey)
⋮----
pubkey_of_signer(matches, "program_id", wallet_manager)?
⋮----
signer_of(matches, "upgrade_authority", wallet_manager)?;
bulk_signers.push(upgrade_authority);
let max_len = value_of(matches, "max_len");
⋮----
default_signer.generate_unique_signers(bulk_signers, matches, wallet_manager)?;
let compute_unit_price = value_of(matches, "compute_unit_price");
let max_sign_attempts = value_of(matches, "max_sign_attempts").unwrap();
let auto_extend = !matches.is_present("no_auto_extend");
let skip_feature_verify = matches.is_present("skip_feature_verify");
⋮----
fee_payer_signer_index: signer_info.index_of(fee_payer_pubkey).unwrap(),
program_signer_index: signer_info.index_of_or_none(program_pubkey),
⋮----
buffer_signer_index: signer_info.index_of_or_none(buffer_pubkey),
⋮----
.index_of(upgrade_authority_pubkey)
.unwrap(),
is_final: matches.is_present("final"),
⋮----
use_rpc: matches.is_present("use_rpc"),
⋮----
let sign_only = matches.is_present(SIGN_ONLY_ARG.name);
let dump_transaction_message = matches.is_present(DUMP_TRANSACTION_MESSAGE.name);
⋮----
let buffer_pubkey = pubkey_of_signer(matches, "buffer", wallet_manager)
.unwrap()
.unwrap();
let program_pubkey = pubkey_of_signer(matches, "program_id", wallet_manager)
⋮----
signer_of(matches, "buffer_authority", wallet_manager)?;
bulk_signers.push(buffer_authority);
⋮----
program_location: matches.value_of("program_location").unwrap().to_string(),
⋮----
.index_of(buffer_authority_pubkey)
⋮----
let buffer_pubkey = pubkey_of(matches, "buffer").unwrap();
⋮----
pubkey_of_signer(matches, "new_buffer_authority", wallet_manager)?.unwrap();
let signer_info = default_signer.generate_unique_signers(
vec![
⋮----
buffer_authority_index: signer_info.index_of(buffer_authority_pubkey),
⋮----
let program_pubkey = pubkey_of(matches, "program_id").unwrap();
let is_final = matches.is_present("final");
⋮----
pubkey_of_signer(matches, "new_upgrade_authority", wallet_manager)?
⋮----
let mut signers = vec![
⋮----
if !is_final && !matches.is_present("skip_new_upgrade_authority_signer_check") {
⋮----
signer_of(matches, "new_upgrade_authority", wallet_manager)?;
signers.push(new_upgrade_authority_signer);
⋮----
default_signer.generate_unique_signers(signers, matches, wallet_manager)?;
if matches.is_present("skip_new_upgrade_authority_signer_check") || is_final {
⋮----
upgrade_authority_index: signer_info.index_of(upgrade_authority_pubkey),
⋮----
.expect("upgrade authority is missing from signers"),
⋮----
.index_of(new_upgrade_authority)
.expect("new upgrade authority is missing from signers"),
⋮----
pubkey_of_signer(matches, "buffer_authority", wallet_manager)?
⋮----
.signer_from_path(matches, wallet_manager)?
.pubkey()
⋮----
account_pubkey: pubkey_of(matches, "account"),
⋮----
get_programs: matches.is_present("programs"),
get_buffers: matches.is_present("buffers"),
all: matches.is_present("all"),
use_lamports_unit: matches.is_present("lamports"),
⋮----
output_location: matches.value_of("output_location").unwrap().to_string(),
⋮----
let account_pubkey = if matches.is_present("buffers") {
⋮----
pubkey_of(matches, "account")
⋮----
pubkey_of_signer(matches, "recipient_account", wallet_manager)?
⋮----
signer_of(matches, "authority", wallet_manager)?;
⋮----
authority_index: signer_info.index_of(authority_pubkey).unwrap(),
⋮----
bypass_warning: matches.is_present("bypass_warning"),
⋮----
let additional_bytes = value_of(matches, "additional_bytes").unwrap();
⋮----
authority_signer_index: signer_info.index_of(authority_pubkey).unwrap(),
⋮----
_ => unreachable!(),
⋮----
Ok(response)
⋮----
pub async fn process_program_subcommand(
⋮----
process_program_deploy(
⋮----
process_program_upgrade(
⋮----
process_write_buffer(
⋮----
process_set_authority(
⋮----
Some(*buffer_pubkey),
⋮----
Some(*new_buffer_authority),
⋮----
Some(*program_pubkey),
⋮----
process_set_authority_checked(
⋮----
process_show(
⋮----
} => process_dump(&rpc_client, config, *account_pubkey, output_location).await,
⋮----
process_close(
⋮----
process_extend_program(
⋮----
process_migrate_program(
⋮----
fn get_default_program_keypair(program_location: &Option<String>) -> Keypair {
⋮----
keypair_file.push(program_location);
let mut filename = keypair_file.file_stem().unwrap().to_os_string();
filename.push("-keypair");
keypair_file.set_file_name(filename);
keypair_file.set_extension("json");
if let Ok(keypair) = read_keypair_file(keypair_file.to_str().unwrap()) {
⋮----
async fn process_program_deploy(
⋮----
let (buffer_words, buffer_mnemonic, buffer_keypair) = create_ephemeral_keypair()?;
⋮----
(true, Some(config.signers[i]), config.signers[i].pubkey())
⋮----
Some(&buffer_keypair as &dyn Signer),
buffer_keypair.pubkey(),
⋮----
let default_program_keypair = get_default_program_keypair(program_location);
⋮----
(Some(config.signers[i]), config.signers[i].pubkey())
⋮----
Some(&default_program_keypair as &dyn Signer),
default_program_keypair.pubkey(),
⋮----
.get_account_with_commitment(&program_pubkey, config.commitment)
⋮----
return Err(format!(
⋮----
.into());
⋮----
// Continue an initial deploy
⋮----
}) = account.state()
⋮----
.get_account_with_commitment(&programdata_address, config.commitment)
⋮----
if program_authority_pubkey.is_none() {
return Err(
format!("Program {program_pubkey} is no longer upgradeable").into()
⋮----
if program_authority_pubkey != Some(upgrade_authority_signer.pubkey()) {
⋮----
// Do upgrade
⋮----
return Err(format!("{program_pubkey} is not an upgradeable program").into());
⋮----
fetch_feature_set(&rpc_client).await?
⋮----
&& feature_set.is_active(&agave_feature_set::enable_loader_v4::id())
⋮----
warn!("Loader-v4 is available now. Please migrate your program.");
⋮----
let program_data = read_and_verify_elf(program_location, feature_set)?;
let program_len = program_data.len();
⋮----
fetch_buffer_program_data(
⋮----
Some(program_len),
⋮----
upgrade_authority_signer.pubkey(),
⋮----
let buffer_program_data = fetch_verified_buffer_program_data(
⋮----
(vec![], buffer_program_data.len(), Some(buffer_program_data))
⋮----
return Err("Program location required if buffer not supplied".into());
⋮----
"Max length specified not large enough to accommodate desired program".into(),
⋮----
.get_minimum_balance_for_rent_exemption(UpgradeableLoaderState::size_of_programdata(
⋮----
if program_signer.is_none() {
⋮----
"Initial deployments require a keypair be provided for the program id".into(),
⋮----
do_process_program_deploy(
rpc_client.clone(),
⋮----
&[program_signer.unwrap(), upgrade_authority_signer],
⋮----
do_process_program_upgrade(
⋮----
if result.is_ok() && is_final {
⋮----
Some(program_pubkey),
⋮----
Some(upgrade_authority_signer_index),
⋮----
if result.is_err() && !buffer_provided {
report_ephemeral_mnemonic(buffer_words, buffer_mnemonic, &buffer_pubkey);
⋮----
async fn fetch_verified_buffer_program_data(
⋮----
fetch_buffer_program_data(rpc_client, config, None, buffer_pubkey, buffer_authority)
⋮----
return Err(format!("Buffer account {buffer_pubkey} not found").into());
⋮----
verify_elf(&buffer_program_data, feature_set).map_err(|err| {
format!("Buffer account {buffer_pubkey} has invalid program data: {err:?}")
⋮----
Ok(buffer_program_data)
⋮----
async fn fetch_buffer_program_data(
⋮----
.get_account_with_commitment(&buffer_pubkey, config.commitment)
⋮----
return Ok(None);
⋮----
if let Ok(UpgradeableLoaderState::Buffer { authority_address }) = account.state() {
if authority_address.is_none() {
return Err(format!("Buffer {buffer_pubkey} is immutable").into());
⋮----
if authority_address != Some(buffer_authority) {
⋮----
return Err(format!("{buffer_pubkey} is not an upgradeable loader buffer account").into());
⋮----
if account.data.len() < min_buffer_data_len {
⋮----
.split_off(UpgradeableLoaderState::size_of_buffer_metadata());
Ok(Some(buffer_program_data))
⋮----
async fn process_program_upgrade(
⋮----
.get_blockhash(&rpc_client, config.commitment)
⋮----
&upgrade_authority_signer.pubkey(),
&fee_payer_signer.pubkey(),
⋮----
Some(&fee_payer_signer.pubkey()),
⋮----
// Using try_partial_sign here because fee_payer_signer might not be the fee payer we
// end up using for this transaction (it might be NullSigner in `--sign-only` mode).
tx.try_partial_sign(signers, blockhash)?;
return_signers_with_config(
⋮----
fetch_verified_buffer_program_data(
⋮----
let fee = rpc_client.get_fee_for_message(&message).await?;
check_account_for_spend_and_fee_with_commitment(
⋮----
tx.try_sign(signers, blockhash)?;
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
.map_err(|e| format!("Upgrading program failed: {e}"))?;
⋮----
program_id: program_id.to_string(),
signature: Some(final_tx_sig.to_string()),
⋮----
Ok(config.output_format.formatted_string(&program_id))
⋮----
async fn process_write_buffer(
⋮----
let (words, mnemonic, buffer_keypair) = create_ephemeral_keypair()?;
⋮----
let buffer_program_data = fetch_buffer_program_data(
⋮----
buffer_authority.pubkey(),
⋮----
program_data.len()
⋮----
let result = do_process_write_buffer(
⋮----
program_data.len(),
⋮----
if result.is_err() && buffer_signer_index.is_none() && buffer_signer.is_some() {
report_ephemeral_mnemonic(words, mnemonic, &buffer_pubkey);
⋮----
async fn process_set_authority(
⋮----
return Err("Set authority requires the current authority".into());
⋮----
trace!("Set a new authority");
⋮----
.get_blockhash(rpc_client, config.commitment)
⋮----
&authority_signer.pubkey(),
new_authority.as_ref(),
⋮----
Some(&config.signers[0].pubkey()),
⋮----
return Err("Buffer authority cannot be None".into());
⋮----
return Err("Program or Buffer not provided".into());
⋮----
.map_err(|e| format!("Setting authority failed: {e}"))?;
⋮----
.map(|pubkey| pubkey.to_string())
.unwrap_or_else(|| "none".to_string()),
account_type: if program_pubkey.is_some() {
⋮----
Ok(config.output_format.formatted_string(&authority))
⋮----
async fn process_set_authority_checked(
⋮----
trace!("Set a new (checked) authority");
⋮----
&new_authority_signer.pubkey(),
⋮----
authority: new_authority_signer.pubkey().to_string(),
⋮----
async fn get_buffers(
⋮----
let mut filters = vec![RpcFilterType::Memcmp(Memcmp::new_base58_encoded(
⋮----
filters.push(RpcFilterType::Memcmp(Memcmp::new_base58_encoded(
⋮----
authority_pubkey.as_ref(),
⋮----
let results = get_accounts_with_filter(
⋮----
let mut buffers = vec![];
for (address, ui_account) in results.iter() {
let account: Account = ui_account.decode().expect(
⋮----
buffers.push(CliUpgradeableBuffer {
address: address.to_string(),
⋮----
return Err(format!("Error parsing Buffer account {address}").into());
⋮----
Ok(CliUpgradeableBuffers {
⋮----
async fn get_programs(
⋮----
let mut programs = vec![];
for (programdata_address, programdata_ui_account) in results.iter() {
let programdata_account: Account = programdata_ui_account.decode().expect(
⋮----
}) = programdata_account.state()
⋮----
let mut bytes = vec![2, 0, 0, 0];
bytes.extend_from_slice(programdata_address.as_ref());
let filters = vec![RpcFilterType::Memcmp(Memcmp::new_base58_encoded(0, &bytes))];
let results = get_accounts_with_filter(rpc_client, filters, 0).await?;
if results.len() != 1 {
⋮----
programs.push(CliUpgradeableProgram {
program_id: results[0].0.to_string(),
owner: programdata_account.owner.to_string(),
programdata_address: programdata_address.to_string(),
⋮----
.len()
.saturating_sub(UpgradeableLoaderState::size_of_programdata_metadata()),
⋮----
return Err(format!("Error parsing ProgramData account {programdata_address}").into());
⋮----
Ok(CliUpgradeablePrograms {
⋮----
async fn get_accounts_with_filter(
⋮----
.get_program_ui_accounts_with_config(
⋮----
filters: Some(filters),
⋮----
encoding: Some(UiAccountEncoding::Base64),
data_slice: Some(UiDataSliceConfig { offset: 0, length }),
⋮----
Ok(results)
⋮----
async fn process_show(
⋮----
.get_account_with_commitment(&account_pubkey, config.commitment)
⋮----
Ok(config.output_format.formatted_string(&CliProgram {
program_id: account_pubkey.to_string(),
owner: account.owner.to_string(),
data_len: account.data.len(),
⋮----
Ok(config
⋮----
.formatted_string(&CliUpgradeableProgram {
⋮----
data_len: programdata_account.data.len().saturating_sub(
⋮----
Err(format!("Program {account_pubkey} has been closed").into())
⋮----
account.state()
⋮----
.formatted_string(&CliUpgradeableBuffer {
address: account_pubkey.to_string(),
⋮----
.saturating_sub(UpgradeableLoaderState::size_of_buffer_metadata()),
⋮----
Err(format!(
⋮----
.into())
⋮----
Err(format!("{account_pubkey} is not an SBF program").into())
⋮----
Err(format!("Unable to find the account {account_pubkey}").into())
⋮----
let authority_pubkey = if all { None } else { Some(authority_pubkey) };
let programs = get_programs(rpc_client, authority_pubkey, use_lamports_unit).await?;
Ok(config.output_format.formatted_string(&programs))
⋮----
let buffers = get_buffers(rpc_client, authority_pubkey, use_lamports_unit).await?;
Ok(config.output_format.formatted_string(&buffers))
⋮----
Err("Invalid parameters".to_string().into())
⋮----
async fn process_dump(
⋮----
f.write_all(&account.data)?;
Ok(format!("Wrote program to {output_location}"))
⋮----
programdata_account.state()
⋮----
f.write_all(program_data)?;
⋮----
} else if let Ok(UpgradeableLoaderState::Buffer { .. }) = account.state() {
⋮----
Err("No account specified".into())
⋮----
async fn close(
⋮----
let blockhash = rpc_client.get_latest_blockhash().await?;
⋮----
Some(&authority_signer.pubkey()),
⋮----
tx.try_sign(&[config.signers[0], authority_signer], blockhash)?;
⋮----
)) = err.kind()
⋮----
return Err("Closing a buffer account is not supported by the cluster".into());
⋮----
return Err("Closing a program account is not supported by the cluster".into());
⋮----
return Err(format!("Close failed: {err}").into());
⋮----
Ok(())
⋮----
async fn process_close(
⋮----
match account.state() {
⋮----
if authority_address != Some(authority_signer.pubkey()) {
⋮----
close(
⋮----
.formatted_string(&CliUpgradeableBuffers {
buffers: vec![CliUpgradeableBuffer {
⋮----
.get_account_with_commitment(&programdata_pubkey, config.commitment)
⋮----
if authority_pubkey != Some(authority_signer.pubkey()) {
⋮----
return Err(String::from(CLOSE_PROGRAM_WARNING).into());
⋮----
Some(&account_pubkey),
⋮----
Ok(config.output_format.formatted_string(
⋮----
_ => Err(format!("{account_pubkey} is not a Program or Buffer account").into()),
⋮----
let buffers = get_buffers(
⋮----
Some(authority_signer.pubkey()),
⋮----
let mut closed = vec![];
for buffer in buffers.buffers.iter() {
if close(
⋮----
.is_ok()
⋮----
closed.push(buffer.clone());
⋮----
async fn process_extend_program(
⋮----
let payer_pubkey = config.signers[0].pubkey();
⋮----
return Err("Additional bytes must be greater than zero".into());
⋮----
Some(program_account) => Ok(program_account),
None => Err(format!("Unable to find program {program_pubkey}")),
⋮----
return Err(format!("Account {program_pubkey} is not an upgradeable program").into());
⋮----
let programdata_pubkey = match program_account.state() {
⋮----
}) => Ok(programdata_pubkey),
_ => Err(format!(
⋮----
Some(programdata_account) => Ok(programdata_account),
None => Err(format!("Program {program_pubkey} is closed")),
⋮----
let upgrade_authority_address = match programdata_account.state() {
⋮----
}) => Ok(upgrade_authority_address),
_ => Err(format!("Program {program_pubkey} is closed")),
⋮----
.ok_or_else(|| format!("Program {program_pubkey} is not upgradeable"))?;
if authority_signer.pubkey() != upgrade_authority_address {
⋮----
let feature_set = fetch_feature_set(rpc_client).await?;
⋮----
if feature_set.is_active(&agave_feature_set::enable_extend_program_checked::id()) {
⋮----
Some(&payer_pubkey),
⋮----
let mut tx = Transaction::new_unsigned(Message::new(&[instruction], Some(&payer_pubkey)));
⋮----
return Err("Extending a program is not supported by the cluster".into());
⋮----
return Err(format!("Extend program failed: {err}").into());
⋮----
.formatted_string(&CliUpgradeableProgramExtended {
program_id: program_pubkey.to_string(),
⋮----
async fn process_migrate_program(
⋮----
}) = program_account.state()
⋮----
return Err(format!("Program {program_pubkey} is closed").into());
⋮----
if authority_signer.pubkey() != upgrade_authority_address.unwrap_or(program_pubkey) {
⋮----
&vec![loader_v3_instruction::migrate_program(
⋮----
.with_compute_unit_config(&ComputeUnitConfig {
⋮----
simulate_and_update_compute_unit_limit(&ComputeUnitLimit::Simulated, rpc_client, &mut message)
⋮----
return Err("Migrating a program is not supported by the cluster".into());
⋮----
return Err(format!("Migrate program failed: {err}").into());
⋮----
.formatted_string(&CliUpgradeableProgramMigrated {
⋮----
pub fn calculate_max_chunk_size(baseline_msg: Message) -> usize {
⋮----
signatures: vec![
⋮----
.unwrap() as usize;
PACKET_DATA_SIZE.saturating_sub(tx_size).saturating_sub(1)
⋮----
async fn do_process_program_deploy(
⋮----
program_data: &[u8], // can be empty, hence we have program_len
⋮----
(vec![], 0, buffer_program_data)
⋮----
&buffer_authority_signer.pubkey(),
⋮----
vec![0; program_len],
⋮----
let initial_message = if !initial_instructions.is_empty() {
Some(Message::new_with_blockhash(
&initial_instructions.with_compute_unit_config(&ComputeUnitConfig {
⋮----
// Create and add write messages
⋮----
let instructions = vec![instruction].with_compute_unit_config(&ComputeUnitConfig {
⋮----
Message::new_with_blockhash(&instructions, Some(&fee_payer_signer.pubkey()), &blockhash)
⋮----
let mut write_messages = vec![];
let chunk_size = calculate_max_chunk_size(create_msg(0, Vec::new()));
for (chunk, i) in program_data.chunks(chunk_size).zip(0usize..) {
let offset = i.saturating_mul(chunk_size);
if chunk != &buffer_program_data[offset..offset.saturating_add(chunk.len())] {
write_messages.push(create_msg(offset as u32, chunk.to_vec()));
⋮----
// Create and add final message
⋮----
&program_signers[0].pubkey(),
⋮----
&program_signers[1].pubkey(),
⋮----
.get_minimum_balance_for_rent_exemption(UpgradeableLoaderState::size_of_program())
⋮----
check_payer(
⋮----
fee_payer_signer.pubkey(),
⋮----
let final_tx_sig = send_deploy_messages(
⋮----
Some(buffer_authority_signer),
Some(program_signers),
⋮----
program_id: program_signers[0].pubkey().to_string(),
signature: final_tx_sig.as_ref().map(ToString::to_string),
⋮----
async fn do_process_write_buffer(
⋮----
let _final_tx_sig = send_deploy_messages(
⋮----
buffer: buffer_pubkey.to_string(),
⋮----
Ok(config.output_format.formatted_string(&buffer))
⋮----
async fn do_process_program_upgrade(
⋮----
&buffer_signer.pubkey(),
&upgrade_authority.pubkey(),
⋮----
extend_program_data_if_needed(
⋮----
let buffer_signer_pubkey = buffer_signer.pubkey();
let upgrade_authority_pubkey = upgrade_authority.pubkey();
⋮----
let instructions = vec![loader_v3_instruction::write(
⋮----
(None, vec![], 0)
⋮----
let final_instructions = vec![loader_v3_instruction::upgrade(
⋮----
let final_message = Some(final_message);
⋮----
Some(upgrade_authority),
Some(&[upgrade_authority]),
⋮----
// Attempts to look up the program data account, and adds an extend program data instruction if the
// program data account is too small.
async fn extend_program_data_if_needed(
⋮----
let program_data_address = get_program_data_address(program_id);
⋮----
.get_account_with_commitment(&program_data_address, commitment)
⋮----
// Program data has not been allocated yet.
return Ok(());
⋮----
let upgrade_authority_address = match program_data_account.state() {
⋮----
_ => Err(format!("Program {program_id} is closed")),
⋮----
.ok_or_else(|| format!("Program {program_id} is not upgradeable"))?;
⋮----
let max_permitted_data_length = usize::try_from(MAX_PERMITTED_DATA_LENGTH).unwrap();
⋮----
.saturating_sub(UpgradeableLoaderState::size_of_programdata(0));
⋮----
let current_len = program_data_account.data.len();
let additional_bytes = required_len.saturating_sub(current_len);
⋮----
// Current allocation is sufficient.
⋮----
u32::try_from(additional_bytes).expect("`u32` is big enough to hold an account size");
⋮----
Some(fee_payer),
⋮----
loader_v3_instruction::extend_program(program_id, Some(fee_payer), additional_bytes)
⋮----
initial_instructions.push(instruction);
⋮----
fn read_and_verify_elf(
⋮----
.map_err(|err| format!("Unable to open program file: {err}"))?;
⋮----
file.read_to_end(&mut program_data)
.map_err(|err| format!("Unable to read program file: {err}"))?;
verify_elf(&program_data, feature_set)?;
Ok(program_data)
⋮----
fn verify_elf(
⋮----
// Verify the program
let program_runtime_environment = create_program_runtime_environment_v1(
&feature_set.runtime_features(),
⋮----
feature_set.is_active(&raise_cpi_nesting_limit_to_8::id()),
⋮----
.map_err(|err| format!("ELF error: {err}"))?;
⋮----
.map_err(|err| format!("ELF error: {err}").into())
⋮----
async fn check_payer(
⋮----
let mut fee = Saturating(0);
⋮----
fee += rpc_client.get_fee_for_message(message).await?;
⋮----
if let Some(message) = write_messages.first() {
⋮----
.get_fee_for_message(message)
⋮----
.saturating_mul(write_messages.len() as u64);
⋮----
async fn send_deploy_messages(
⋮----
trace!("Preparing the required accounts");
simulate_and_update_compute_unit_limit(compute_unit_limit, &rpc_client, &mut message)
⋮----
let mut initial_transaction = Transaction::new_unsigned(message.clone());
⋮----
// Most of the initial_transaction combinations require both the fee-payer and new program
// account to sign the transaction. One (transfer) only requires the fee-payer signature.
// This check is to ensure signing does not fail on a KeypairPubkeyMismatch error from an
// extraneous signature.
⋮----
initial_transaction.try_sign(
&[fee_payer_signer, initial_signer, write_signer.unwrap()],
⋮----
initial_transaction.try_sign(&[fee_payer_signer, initial_signer], blockhash)?;
⋮----
initial_transaction.try_sign(&[fee_payer_signer], blockhash)?;
⋮----
.map_err(|err| format!("Account allocation failed: {err}"))?;
⋮----
return Err("Buffer account not created yet, must provide a key pair".into());
⋮----
if !write_messages.is_empty() {
⋮----
trace!("Writing program data");
// Simulate the first write message to get the number of compute units
// consumed and then reuse that value as the compute unit limit for all
// write messages.
⋮----
let mut message = write_messages[0].clone();
⋮----
simulate_and_update_compute_unit_limit(
⋮----
// Write messages are all assumed to be identical except
// the program data being written. But just in case that
// assumption is broken, assert that we are only ever
// changing the instruction data for a compute budget
// instruction.
assert_eq!(msg.program_id(ix_index), Some(&compute_budget::id()));
⋮----
.clone_from(&message.instructions[ix_index].data);
⋮----
.send_and_confirm_messages_with_spinner(
⋮----
// `solana_client` type currently required by `send_and_confirm_transactions_in_parallel_v2`
⋮----
config.websocket_url.as_str(),
⋮----
Some(
⋮----
.expect("Should return a valid tpu client"),
⋮----
send_and_confirm_transactions_in_parallel_v2(
⋮----
resign_txs_count: Some(max_sign_attempts),
⋮----
.map_err(|err| format!("Data writes to account failed: {err}"))?
.into_iter()
.flatten()
⋮----
if !transaction_errors.is_empty() {
⋮----
error!("{transaction_error:?}");
⋮----
format!("{} write transactions failed", transaction_errors.len()).into(),
⋮----
trace!("Deploying program");
⋮----
let mut signers = final_signers.to_vec();
signers.push(fee_payer_signer);
final_tx.try_sign(&signers, blockhash)?;
return Ok(Some(
⋮----
.map_err(|e| format!("Deploying program failed: {e}"))?,
⋮----
Ok(None)
⋮----
fn create_ephemeral_keypair(
⋮----
let new_keypair = keypair_from_seed(seed.as_bytes())?;
Ok((WORDS, mnemonic, new_keypair))
⋮----
fn report_ephemeral_mnemonic(words: usize, mnemonic: bip39::Mnemonic, ephemeral_pubkey: &Pubkey) {
let phrase: &str = mnemonic.phrase();
let divider = String::from_utf8(vec![b'='; phrase.len()]).unwrap();
eprintln!("{divider}\nRecover the intermediate account's ephemeral keypair file with");
eprintln!("`solana-keygen recover` and the following {words}-word seed phrase:");
eprintln!("{divider}\n{phrase}\n{divider}");
eprintln!("To resume a deploy, pass the recovered keypair as the");
eprintln!("[BUFFER_SIGNER] to `solana program deploy` or `solana program write-buffer'.");
eprintln!("Or to recover the account's lamports, use:");
eprintln!("{divider}\nsolana program close {ephemeral_pubkey}\n{divider}");
⋮----
async fn fetch_feature_set(
⋮----
.keys()
.cloned()
⋮----
.chunks(MAX_MULTIPLE_ACCOUNTS)
⋮----
.get_multiple_accounts(feature_ids)
⋮----
.zip(feature_ids)
.for_each(|(account, feature_id)| {
let activation_slot = account.and_then(status_from_account);
⋮----
feature_set.activate(feature_id, slot);
⋮----
Ok(feature_set)
⋮----
mod tests {
⋮----
fn make_tmp_path(name: &str) -> String {
let out_dir = std::env::var("FARF_DIR").unwrap_or_else(|_| "farf".to_string());
⋮----
let path = format!("{}/tmp/{}-{}", out_dir, name, keypair.pubkey());
⋮----
fn test_cli_parse_deploy() {
let test_commands = get_clap_app("test", "desc", "version");
⋮----
let keypair_file = make_tmp_path("keypair_file");
write_keypair_file(&default_keypair, &keypair_file).unwrap();
⋮----
let test_command = test_commands.clone().get_matches_from(vec![
⋮----
assert_eq!(
⋮----
let buffer_keypair_file = make_tmp_path("buffer_keypair_file");
write_keypair_file(&buffer_keypair, &buffer_keypair_file).unwrap();
⋮----
let test = test_commands.clone().get_matches_from(vec![
⋮----
let program_keypair_file = make_tmp_path("program_keypair_file");
write_keypair_file(&program_keypair, &program_keypair_file).unwrap();
⋮----
let authority_keypair_file = make_tmp_path("authority_keypair_file");
write_keypair_file(&authority_keypair, &authority_keypair_file).unwrap();
⋮----
fn test_cli_parse_upgrade() {
⋮----
fn test_cli_parse_write_buffer() {
⋮----
// defaults
⋮----
fn test_cli_parse_set_upgrade_authority() {
⋮----
let new_authority_pubkey_file = make_tmp_path("authority_keypair_file");
write_keypair_file(&new_authority_pubkey, &new_authority_pubkey_file).unwrap();
⋮----
write_keypair_file(&new_authority_pubkey, new_authority_pubkey_file).unwrap();
⋮----
write_keypair_file(&authority, &authority_keypair_file).unwrap();
⋮----
fn test_cli_parse_set_buffer_authority() {
⋮----
let new_authority_keypair_file = make_tmp_path("authority_keypair_file");
write_keypair_file(&new_authority_keypair, &new_authority_keypair_file).unwrap();
⋮----
fn test_cli_parse_show() {
⋮----
fn test_cli_parse_close() {
⋮----
fn test_cli_parse_extend_program() {
⋮----
fn test_cli_parse_migrate_program() {
⋮----
async fn test_cli_keypair_file() {
⋮----
let deploy_path = make_tmp_path("deploy");
let mut program_location = PathBuf::from(deploy_path.clone());
program_location.push("noop");
program_location.set_extension("so");
let mut pathbuf = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
pathbuf.push("tests");
pathbuf.push("fixtures");
pathbuf.push("noop");
pathbuf.set_extension("so");
let program_keypair_location = program_location.with_file_name("noop-keypair.json");
std::fs::create_dir_all(deploy_path).unwrap();
std::fs::copy(pathbuf, program_location.as_os_str()).unwrap();
write_keypair_file(&program_pubkey, program_keypair_location).unwrap();
⋮----
rpc_client: Some(Arc::new(RpcClient::new_mock("".to_string()))),
⋮----
program_location: Some(program_location.to_str().unwrap().to_string()),
⋮----
signers: vec![&default_keypair],
⋮----
let result = process_command(&config).await;
let json: Value = serde_json::from_str(&result.unwrap()).unwrap();
⋮----
.as_object()
⋮----
.get("programId")
⋮----
.as_str()

================
File: cli/src/spend_utils.rs
================
pub enum SpendAmount {
⋮----
impl Default for SpendAmount {
fn default() -> Self {
⋮----
impl SpendAmount {
pub fn new(amount: Option<u64>, sign_only: bool) -> Self {
⋮----
_ => panic!("ALL amount not supported for sign-only operations"),
⋮----
pub fn new_from_matches(matches: &ArgMatches<'_>, name: &str) -> Self {
let sign_only = matches.is_present(SIGN_ONLY_ARG.name);
let amount = lamports_of_sol(matches, name);
if amount.is_some() {
⋮----
match matches.value_of(name).unwrap_or("ALL") {
⋮----
_ => panic!("Only specific amounts are supported for sign-only operations"),
⋮----
struct SpendAndFee {
⋮----
pub async fn resolve_spend_tx_and_check_account_balance<F>(
⋮----
resolve_spend_tx_and_check_account_balances(
⋮----
pub async fn resolve_spend_tx_and_check_account_balances<F>(
⋮----
let (message, SpendAndFee { spend, fee: _ }) = resolve_spend_message(
⋮----
Ok((message, spend))
⋮----
.get_account_with_commitment(from_pubkey, commitment)
⋮----
.unwrap_or_default();
⋮----
.get_minimum_balance_for_rent_exemption(account.data.len())
⋮----
from_balance = from_balance.saturating_sub(active_stake);
⋮----
from_balance = from_balance.saturating_sub(activating_stake);
⋮----
from_balance = from_balance.saturating_sub(from_rent_exempt_minimum);
⋮----
let (message, SpendAndFee { spend, fee }) = resolve_spend_message(
⋮----
Some(blockhash),
⋮----
if from_balance == 0 || from_balance < spend.saturating_add(fee) {
return Err(CliError::InsufficientFundsForSpendAndFee(
build_balance_message(spend, false, false),
build_balance_message(fee, false, false),
⋮----
return Err(CliError::InsufficientFundsForSpend(
⋮----
if !check_account_for_balance_with_commitment(rpc_client, fee_pubkey, fee, commitment)
⋮----
return Err(CliError::InsufficientFundsForFee(
⋮----
async fn resolve_spend_message<F>(
⋮----
// If the from account is the same as the fee payer, it's impossible
⋮----
from_account_transferable_balance.saturating_sub(from_rent_exempt_minimum)
⋮----
let mut dummy_message = build_message(lamports);
⋮----
simulate_and_update_compute_unit_limit(
⋮----
Some((ix_index, dummy_message.instructions[ix_index].data.clone()))
⋮----
get_fee_for_messages(rpc_client, &[&dummy_message]).await?,
⋮----
build_message(lamports),
⋮----
from_account_transferable_balance.saturating_sub(fee)
⋮----
lamports = lamports.saturating_sub(from_rent_exempt_minimum);
⋮----
Ok((message, spend_and_fee))

================
File: cli/src/stake.rs
================
fn stake_authority_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(STAKE_AUTHORITY_ARG.long)
.takes_value(true)
.value_name("KEYPAIR")
.validator(is_valid_signer)
.help(STAKE_AUTHORITY_ARG.help)
⋮----
fn withdraw_authority_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(WITHDRAW_AUTHORITY_ARG.long)
⋮----
.help(WITHDRAW_AUTHORITY_ARG.help)
⋮----
fn custodian_arg<'a, 'b>() -> Arg<'a, 'b> {
⋮----
.long(CUSTODIAN_ARG.long)
⋮----
.help(CUSTODIAN_ARG.help)
⋮----
pub(crate) struct StakeAuthorization {
⋮----
pub struct StakeAuthorizationIndexed {
⋮----
struct SignOnlySplitNeedsRent {}
impl ArgsConfig for SignOnlySplitNeedsRent {
fn sign_only_arg<'a, 'b>(&self, arg: Arg<'a, 'b>) -> Arg<'a, 'b> {
arg.requires("rent_exempt_reserve_sol")
⋮----
pub trait StakeSubCommands {
⋮----
impl StakeSubCommands for App<'_, '_> {
fn stake_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Create a stake account")
.arg(
⋮----
.index(1)
.value_name("STAKE_ACCOUNT_KEYPAIR")
⋮----
.required(true)
⋮----
.help(
⋮----
.index(2)
.value_name("AMOUNT")
⋮----
.validator(is_amount_or_all)
⋮----
.arg(pubkey!(
⋮----
.long("seed")
.value_name("STRING")
⋮----
.long("lockup-epoch")
.value_name("NUMBER")
⋮----
.long("lockup-date")
.value_name("RFC3339 DATETIME")
.validator(is_rfc3339_datetime)
⋮----
.value_name("PUBKEY")
⋮----
.validator(is_valid_pubkey)
.help(STAKE_AUTHORITY_ARG.help),
⋮----
.help(WITHDRAW_AUTHORITY_ARG.help),
⋮----
.long("from")
⋮----
.help("Source account of funds [default: cli config keypair]"),
⋮----
.offline_args()
.nonce_args(false)
.arg(fee_payer_arg())
.arg(memo_arg())
.arg(compute_unit_price_arg()),
⋮----
.subcommand(
⋮----
.about("Create a stake account, checking the withdraw authority as a signer")
⋮----
.about("Delegate stake to a vote account")
⋮----
.long("force")
.takes_value(false)
.hidden(hidden_unless_forced()) // Don't document this argument to discourage its use
.help("Override vote account sanity checks (use carefully!)"),
⋮----
.arg(stake_authority_arg())
⋮----
.setting(AppSettings::Hidden)
⋮----
.multiple(true)
.hidden(hidden_unless_forced()),
⋮----
.about("Authorize a new signing keypair for the given stake account")
⋮----
.arg(withdraw_authority_arg())
⋮----
.arg(custodian_arg())
⋮----
.long("no-wait")
⋮----
.about(
⋮----
.long("new-stake-authority")
⋮----
.required_unless("new_withdraw_authority")
.help("New authorized staker"),
⋮----
.long("new-withdraw-authority")
⋮----
.required_unless("new_stake_authority")
.help("New authorized withdrawer"),
⋮----
.about("Deactivate the delegated stake from the stake account")
⋮----
.long("delinquent")
⋮----
.conflicts_with(SIGN_ONLY_ARG.name)
⋮----
.about("Duplicate a stake account, splitting the tokens between the two")
⋮----
.value_name("SPLIT_STAKE_ACCOUNT")
⋮----
.help("Keypair of the new stake account"),
⋮----
.index(3)
⋮----
.validator(is_amount)
⋮----
.help("The amount to move into the new stake account, in SOL"),
⋮----
.offline_args_config(&SignOnlySplitNeedsRent {})
⋮----
.arg(compute_unit_price_arg())
⋮----
.long("rent-exempt-reserve-sol")
⋮----
.about("Merges one stake account into another")
⋮----
.about("Withdraw the unstaked SOL from the stake account")
⋮----
.validator(is_amount_or_all_or_available)
⋮----
.about("Set Lockup for the stake account")
⋮----
.group(
⋮----
.args(&["lockup_epoch", "lockup_date", "new_custodian"])
⋮----
.required(true),
⋮----
.long("custodian")
⋮----
.help("Keypair of the existing custodian [default: cli config pubkey]"),
⋮----
.about("Set Lockup for the stake account, checking the new authority as a signer")
⋮----
.long("new-custodian")
⋮----
.help("Keypair of a new lockup custodian"),
⋮----
.about("Show the contents of a stake account")
.alias("show-stake-account")
⋮----
.long("lamports")
⋮----
.help("Display balance in lamports instead of SOL"),
⋮----
.long("with-rewards")
⋮----
.help("Display inflation rewards"),
⋮----
.long("csv")
⋮----
.help("Format stake rewards data in csv"),
⋮----
.long("starting-epoch")
⋮----
.value_name("NUM")
.requires("with_rewards")
.help("Start displaying from epoch NUM"),
⋮----
.long("num-rewards-epochs")
⋮----
.validator(|s| is_within_range(s, 1..=50))
.default_value_if("with_rewards", None, "1")
⋮----
.about("Show the stake history")
.alias("show-stake-history")
⋮----
.long("limit")
⋮----
.default_value("10")
.validator(|s| s.parse::<usize>().map(|_| ()).map_err(|e| e.to_string()))
⋮----
.about("Get the stake minimum delegation amount")
⋮----
.help("Display minimum delegation in lamports instead of SOL"),
⋮----
pub fn parse_create_stake_account(
⋮----
let seed = matches.value_of("seed").map(|s| s.to_string());
let epoch = value_of(matches, "lockup_epoch").unwrap_or(0);
let unix_timestamp = unix_timestamp_from_rfc3339_datetime(matches, "lockup_date").unwrap_or(0);
let custodian = pubkey_of_signer(matches, "custodian", wallet_manager)?.unwrap_or_default();
let staker = pubkey_of_signer(matches, STAKE_AUTHORITY_ARG.name, wallet_manager)?;
⋮----
signer_of(matches, WITHDRAW_AUTHORITY_ARG.name, wallet_manager)?
⋮----
pubkey_of_signer(matches, WITHDRAW_AUTHORITY_ARG.name, wallet_manager)?,
⋮----
let sign_only = matches.is_present(SIGN_ONLY_ARG.name);
let dump_transaction_message = matches.is_present(DUMP_TRANSACTION_MESSAGE.name);
⋮----
let nonce_account = pubkey_of_signer(matches, NONCE_ARG.name, wallet_manager)?;
let memo = matches.value_of(MEMO_ARG.name).map(String::from);
⋮----
signer_of(matches, NONCE_AUTHORITY_ARG.name, wallet_manager)?;
let (fee_payer, fee_payer_pubkey) = signer_of(matches, FEE_PAYER_ARG.name, wallet_manager)?;
let (from, from_pubkey) = signer_of(matches, "from", wallet_manager)?;
⋮----
signer_of(matches, "stake_account", wallet_manager)?;
let mut bulk_signers = vec![fee_payer, from, stake_account];
if nonce_account.is_some() {
bulk_signers.push(nonce_authority);
⋮----
if withdrawer_signer.is_some() {
bulk_signers.push(withdrawer_signer);
⋮----
default_signer.generate_unique_signers(bulk_signers, matches, wallet_manager)?;
let compute_unit_price = value_of(matches, COMPUTE_UNIT_PRICE_ARG.name);
Ok(CliCommandInfo {
⋮----
stake_account: signer_info.index_of(stake_account_pubkey).unwrap(),
⋮----
signer_info.index_of(withdrawer)
⋮----
nonce_authority: signer_info.index_of(nonce_authority_pubkey).unwrap(),
⋮----
fee_payer: signer_info.index_of(fee_payer_pubkey).unwrap(),
from: signer_info.index_of(from_pubkey).unwrap(),
⋮----
pub fn parse_stake_delegate_stake(
⋮----
pubkey_of_signer(matches, "stake_account_pubkey", wallet_manager)?.unwrap();
⋮----
pubkey_of_signer(matches, "vote_account_pubkey", wallet_manager)?.unwrap();
let force = matches.is_present("force");
⋮----
let nonce_account = pubkey_of(matches, NONCE_ARG.name);
⋮----
signer_of(matches, STAKE_AUTHORITY_ARG.name, wallet_manager)?;
⋮----
let mut bulk_signers = vec![stake_authority, fee_payer];
⋮----
stake_authority: signer_info.index_of(stake_authority_pubkey).unwrap(),
⋮----
pub fn parse_stake_authorize(
⋮----
signer_of(matches, "new_stake_authority", wallet_manager)?
⋮----
pubkey_of_signer(matches, "new_stake_authority", wallet_manager)?,
⋮----
// Withdraw authority may also change the staker
if authority.is_none() {
⋮----
new_authorizations.push(StakeAuthorization {
⋮----
bulk_signers.push(authority);
if new_staker.is_some() {
bulk_signers.push(new_staker_signer);
⋮----
signer_of(matches, "new_withdraw_authority", wallet_manager)?
⋮----
pubkey_of_signer(matches, "new_withdraw_authority", wallet_manager)?,
⋮----
signer_of(matches, WITHDRAW_AUTHORITY_ARG.name, wallet_manager)?;
⋮----
if new_withdrawer_signer.is_some() {
bulk_signers.push(new_withdrawer_signer);
⋮----
let (custodian, custodian_pubkey) = signer_of(matches, "custodian", wallet_manager)?;
let no_wait = matches.is_present("no_wait");
bulk_signers.push(fee_payer);
⋮----
if custodian.is_some() {
bulk_signers.push(custodian);
⋮----
if new_authorizations.is_empty() {
return Err(CliError::BadParameter(
"New authorization list must include at least one authority".to_string(),
⋮----
.into_iter()
.map(
⋮----
authority: signer_info.index_of(authority_pubkey).unwrap(),
new_authority_signer: signer_info.index_of(Some(new_authority_pubkey)),
⋮----
.collect();
⋮----
custodian: custodian_pubkey.and_then(|_| signer_info.index_of(custodian_pubkey)),
⋮----
pub fn parse_split_stake(
⋮----
signer_of(matches, "split_stake_account", wallet_manager)?;
let lamports = lamports_of_sol(matches, "amount").unwrap();
⋮----
let mut bulk_signers = vec![stake_authority, fee_payer, split_stake_account];
⋮----
let rent_exempt_reserve = lamports_of_sol(matches, "rent_exempt_reserve_sol");
⋮----
split_stake_account: signer_info.index_of(split_stake_account_pubkey).unwrap(),
⋮----
pub fn parse_merge_stake(
⋮----
let source_stake_account_pubkey = pubkey_of(matches, "source_stake_account_pubkey").unwrap();
⋮----
pub fn parse_stake_deactivate_stake(
⋮----
let deactivate_delinquent = matches.is_present("delinquent");
⋮----
let seed = value_t!(matches, "seed", String).ok();
⋮----
pub fn parse_stake_withdraw_stake(
⋮----
pubkey_of_signer(matches, "destination_account_pubkey", wallet_manager)?.unwrap();
⋮----
let mut bulk_signers = vec![withdraw_authority, fee_payer];
⋮----
withdraw_authority: signer_info.index_of(withdraw_authority_pubkey).unwrap(),
⋮----
pub fn parse_stake_set_lockup(
⋮----
let epoch = value_of(matches, "lockup_epoch");
let unix_timestamp = unix_timestamp_from_rfc3339_datetime(matches, "lockup_date");
⋮----
signer_of(matches, "new_custodian", wallet_manager)?
⋮----
pubkey_of_signer(matches, "new_custodian", wallet_manager)?,
⋮----
let mut bulk_signers = vec![custodian, fee_payer];
⋮----
if new_custodian_signer.is_some() {
bulk_signers.push(new_custodian_signer);
⋮----
signer_info.index_of(new_custodian)
⋮----
custodian: signer_info.index_of(custodian_pubkey).unwrap(),
⋮----
pub fn parse_show_stake_account(
⋮----
let use_lamports_unit = matches.is_present("lamports");
let use_csv = matches.is_present("csv");
let with_rewards = if matches.is_present("with_rewards") {
Some(value_of(matches, "num_rewards_epochs").unwrap())
⋮----
let starting_epoch = value_of(matches, "starting_epoch");
Ok(CliCommandInfo::without_signers(
⋮----
pub fn parse_show_stake_history(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
⋮----
let limit_results = value_of(matches, "limit").unwrap();
⋮----
pub fn parse_stake_minimum_delegation(
⋮----
pub async fn process_create_stake_account(
⋮----
Pubkey::create_with_seed(&stake_account.pubkey(), seed, &stake::program::id())?
⋮----
stake_account.pubkey()
⋮----
check_unique_pubkeys(
(&from.pubkey(), "from keypair".to_string()),
(&stake_account_address, "stake_account".to_string()),
⋮----
staker: staker.unwrap_or(from.pubkey()),
withdrawer: withdrawer.unwrap_or(from.pubkey()),
⋮----
&from.pubkey(),
⋮----
&stake_account.pubkey(),
⋮----
.with_memo(memo)
.with_compute_unit_config(&ComputeUnitConfig {
⋮----
Some(&fee_payer.pubkey()),
⋮----
&nonce_authority.pubkey(),
⋮----
Message::new(&ixs, Some(&fee_payer.pubkey()))
⋮----
.get_blockhash(rpc_client, config.commitment)
⋮----
.get_minimum_balance_for_rent_exemption(StakeStateV2::size_of())
⋮----
let (message, lamports) = resolve_spend_tx_and_check_account_balances(
⋮----
&fee_payer.pubkey(),
⋮----
if let Ok(stake_account) = rpc_client.get_account(&stake_account_address).await {
⋮----
format!("Stake account {stake_account_address} already exists")
⋮----
format!("Account {stake_account_address} already exists and is not a stake account")
⋮----
return Err(CliError::BadParameter(err_msg).into());
⋮----
return Err(CliError::BadParameter(format!(
⋮----
.into());
⋮----
check_nonce_account(&nonce_account, &nonce_authority.pubkey(), &recent_blockhash)?;
⋮----
tx.try_partial_sign(&config.signers, recent_blockhash)?;
return_signers_with_config(
⋮----
tx.try_sign(&config.signers, recent_blockhash)?;
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
pub async fn process_stake_authorize(
⋮----
let custodian = custodian.map(|index| config.signers[index]);
⋮----
Some(get_stake_account_state(rpc_client, stake_account_pubkey, config.commitment).await?)
⋮----
} in new_authorizations.iter()
⋮----
(stake_account_pubkey, "stake_account_pubkey".to_string()),
(new_authority_pubkey, "new_authorized_pubkey".to_string()),
⋮----
StakeStateV2::Stake(Meta { authorized, .. }, ..) => Some(authorized),
StakeStateV2::Initialized(Meta { authorized, .. }) => Some(authorized),
⋮----
StakeAuthorize::Staker => check_current_authority(
⋮----
&authority.pubkey(),
⋮----
check_current_authority(&[authorized.withdrawer], &authority.pubkey())?;
⋮----
return Err(CliError::RpcRequestError(format!(
⋮----
if new_authority_signer.is_some() {
ixs.push(stake_instruction::authorize_checked(
stake_account_pubkey, // stake account to update
&authority.pubkey(),  // currently authorized
new_authority_pubkey, // new stake signer
*authorization_type,  // stake or withdraw
custodian.map(|signer| signer.pubkey()).as_ref(),
⋮----
ixs.push(stake_instruction::authorize(
⋮----
simulate_and_update_compute_unit_limit(&compute_unit_limit, rpc_client, &mut message).await?;
⋮----
check_account_for_fee_with_commitment(
⋮----
.send_transaction_with_config(&tx, config.send_transaction_config)
⋮----
pub async fn process_deactivate_stake_account(
⋮----
let ixs = vec![if deactivate_delinquent {
⋮----
pub async fn process_withdraw_stake(
⋮----
let ixs = vec![stake_instruction::withdraw(
⋮----
let (message, _) = resolve_spend_tx_and_check_account_balances(
⋮----
pub async fn process_split_stake(
⋮----
if split_stake_account_seed.is_none() {
⋮----
(&fee_payer.pubkey(), "fee-payer keypair".to_string()),
⋮----
&split_stake_account.pubkey(),
"split_stake_account".to_string(),
⋮----
(stake_account_pubkey, "stake_account".to_string()),
⋮----
Pubkey::create_with_seed(&split_stake_account.pubkey(), seed, &stake::program::id())?
⋮----
split_stake_account.pubkey()
⋮----
let stake_minimum_delegation = rpc_client.get_stake_minimum_delegation().await?;
⋮----
let lamports = Sol(lamports);
let stake_minimum_delegation = Sol(stake_minimum_delegation);
⋮----
owner if owner == stake::program::id() => Err(CliError::BadParameter(format!(
⋮----
if !account.data.is_empty() {
Err(CliError::BadParameter(format!(
⋮----
Ok(account.lamports)
⋮----
_ => Err(CliError::BadParameter(format!(
⋮----
if let Ok(stake_account) = rpc_client.get_account(&split_stake_account_address).await {
check_stake_account(stake_account)?
⋮----
rent_exempt_reserve.saturating_sub(current_balance)
⋮----
let mut ixs = vec![];
⋮----
ixs.push(system_instruction::transfer(
⋮----
ixs.append(
⋮----
&stake_authority.pubkey(),
⋮----
pub async fn process_merge_stake(
⋮----
"source_stake_account".to_string(),
⋮----
if let Ok(stake_account) = rpc_client.get_account(stake_account_address).await {
⋮----
pub async fn process_stake_set_lockup(
⋮----
let ixs = vec![if new_custodian_signer.is_some() {
⋮----
get_stake_account_state(rpc_client, stake_account_pubkey, config.commitment).await?;
⋮----
StakeStateV2::Stake(Meta { lockup, .. }, ..) => Some(lockup),
StakeStateV2::Initialized(Meta { lockup, .. }) => Some(lockup),
⋮----
check_current_authority(&[lockup.custodian], &custodian.pubkey())?;
⋮----
fn u64_some_if_not_zero(n: u64) -> Option<u64> {
⋮----
Some(n)
⋮----
pub fn build_stake_state(
⋮----
} = stake.delegation.stake_activating_and_deactivating(
⋮----
let lockup = if lockup.is_in_force(clock, None) {
Some(lockup.into())
⋮----
credits_observed: Some(stake.credits_observed),
delegated_stake: Some(stake.delegation.stake),
⋮----
Some(stake.delegation.voter_pubkey.to_string())
⋮----
activation_epoch: Some(if stake.delegation.activation_epoch < u64::MAX {
⋮----
Some(stake.delegation.deactivation_epoch)
⋮----
authorized: Some(authorized.into()),
⋮----
rent_exempt_reserve: Some(*rent_exempt_reserve),
active_stake: u64_some_if_not_zero(effective),
activating_stake: u64_some_if_not_zero(activating),
deactivating_stake: u64_some_if_not_zero(deactivating),
⋮----
credits_observed: Some(0),
⋮----
async fn get_stake_account_state(
⋮----
.get_account_with_commitment(stake_account_pubkey, commitment_config)
⋮----
.ok_or_else(|| {
CliError::RpcRequestError(format!("{stake_account_pubkey:?} account does not exist"))
⋮----
stake_account.state().map_err(|err| {
CliError::RpcRequestError(format!(
⋮----
.into()
⋮----
pub(crate) fn check_current_authority(
⋮----
if !permitted_authorities.contains(provided_current_authority) {
Err(CliError::RpcRequestError(format!(
⋮----
Ok(())
⋮----
pub async fn get_epoch_boundary_timestamps(
⋮----
let epoch_end_time = rpc_client.get_block_time(reward.effective_slot).await?;
let mut epoch_start_slot = epoch_schedule.get_first_slot_in_epoch(reward.epoch);
⋮----
return Err("epoch_start_time not found".to_string().into());
⋮----
match rpc_client.get_block_time(epoch_start_slot).await {
⋮----
.checked_add(1)
.ok_or("Reached last slot that fits into u64")?;
⋮----
Ok((epoch_start_time, epoch_end_time))
⋮----
pub fn make_cli_reward(
⋮----
let wallclock_epoch_duration = epoch_end_time.checked_sub(epoch_start_time)?;
⋮----
reward.amount as f64 / (reward.post_balance.saturating_sub(reward.amount)) as f64;
⋮----
Some(CliEpochReward {
⋮----
apr: Some(apr * 100.0),
⋮----
pub(crate) async fn fetch_epoch_rewards(
⋮----
let mut all_epoch_rewards = vec![];
let epoch_schedule = rpc_client.get_epoch_schedule().await?;
⋮----
.get_epoch_info()
⋮----
.saturating_sub(num_epochs as u64)
⋮----
.get_inflation_reward(&[*address], Some(rewards_epoch))
⋮----
get_epoch_boundary_timestamps(rpc_client, reward, &epoch_schedule).await?;
let block_time = rpc_client.get_block_time(reward.effective_slot).await?;
⋮----
make_cli_reward(reward, block_time, epoch_start_time, epoch_end_time)
⋮----
all_epoch_rewards.push(cli_reward);
⋮----
eprintln!("Rewards not available for epoch {rewards_epoch}");
⋮----
num_epochs = num_epochs.saturating_sub(1);
rewards_epoch = rewards_epoch.saturating_add(1);
⋮----
Ok(all_epoch_rewards)
⋮----
pub async fn process_show_stake_account(
⋮----
let stake_account = rpc_client.get_account(stake_account_address).await?;
let state = get_account_stake_state(
⋮----
Ok(config.output_format.formatted_string(&state))
⋮----
pub async fn get_account_stake_state(
⋮----
match stake_account.state() {
⋮----
let stake_history_account = rpc_client.get_account(&stake_history::id()).await?;
let stake_history = from_account(&stake_history_account).ok_or_else(|| {
CliError::RpcRequestError("Failed to deserialize stake history".to_string())
⋮----
let clock_account = rpc_client.get_account(&clock::id()).await?;
let clock: Clock = from_account(&clock_account).ok_or_else(|| {
CliError::RpcRequestError("Failed to deserialize clock sysvar".to_string())
⋮----
let new_rate_activation_epoch = get_feature_activation_epoch(
⋮----
let mut state = build_stake_state(
⋮----
if state.stake_type == CliStakeType::Stake && state.activation_epoch.is_some() {
⋮----
state.epoch_rewards = match fetch_epoch_rewards(
⋮----
Ok(rewards) => Some(rewards),
⋮----
eprintln!("Failed to fetch epoch rewards: {error:?}");
⋮----
Ok(state)
⋮----
Err(err) => Err(CliError::RpcRequestError(format!(
⋮----
pub async fn process_show_stake_history(
⋮----
from_account::<StakeHistory, _>(&stake_history_account).ok_or_else(|| {
⋮----
let mut entries: Vec<CliStakeHistoryEntry> = vec![];
for entry in stake_history.deref().iter().take(limit_results) {
entries.push(entry.into());
⋮----
Ok(config.output_format.formatted_string(&stake_history_output))
⋮----
pub async fn process_delegate_stake(
⋮----
(&config.signers[0].pubkey(), "cli keypair".to_string()),
⋮----
// Sanity check the vote account to ensure it is attached to a validator that has recently
// voted at the tip of the ledger
⋮----
vote_pubkey: Some(vote_account_pubkey.to_string()),
keep_unstaked_delinquents: Some(true),
commitment: Some(rpc_client.commitment()),
⋮----
.get_vote_accounts_with_config(get_vote_accounts_config)
⋮----
// filter should return at most one result
⋮----
.first()
.or_else(|| delinquent.first())
.ok_or(CliError::RpcRequestError(format!(
⋮----
.get_slot()
⋮----
.map(|slot| slot.saturating_sub(DELINQUENT_VALIDATOR_SLOT_DISTANCE))?;
⋮----
Err(CliError::BadParameter(
"Unable to delegate. Vote account has no root slot".to_string(),
⋮----
Err(CliError::DynamicProgramError(format!(
⋮----
println!("--force supplied, ignoring: {err}");
⋮----
// DelegateStake parses a VoteState, which may change between simulation and execution
⋮----
let ixs = vec![stake_instruction::delegate_stake(
⋮----
pub async fn process_stake_minimum_delegation(
⋮----
.get_stake_minimum_delegation_with_commitment(config.commitment)
⋮----
Ok(config
⋮----
.formatted_string(&stake_minimum_delegation_output))
⋮----
mod tests {
⋮----
fn make_tmp_file() -> (String, NamedTempFile) {
let tmp_file = NamedTempFile::new().unwrap();
(String::from(tmp_file.path().to_str().unwrap()), tmp_file)
⋮----
fn test_parse_command() {
let test_commands = get_clap_app("test", "desc", "version");
⋮----
let (default_keypair_file, mut tmp_file) = make_tmp_file();
write_keypair(&default_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
let (keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&stake_account_keypair, tmp_file.as_file_mut()).unwrap();
let stake_account_pubkey = stake_account_keypair.pubkey();
let (stake_authority_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&stake_authority_keypair, tmp_file.as_file_mut()).unwrap();
let (custodian_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&custodian_keypair, tmp_file.as_file_mut()).unwrap();
// stake-authorize subcommand
let stake_account_string = stake_account_pubkey.to_string();
⋮----
let new_stake_string = new_stake_authority.to_string();
⋮----
let new_withdraw_string = new_withdraw_authority.to_string();
let test_stake_authorize = test_commands.clone().get_matches_from(vec![
⋮----
assert_eq!(
⋮----
let (withdraw_authority_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&withdraw_authority_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
let test_authorize = test_commands.clone().get_matches_from(vec![
⋮----
let (authority_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&authority_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
let blockhash_string = format!("{blockhash}");
⋮----
let pubkey = keypair.pubkey();
let sig = keypair.sign_message(&[0u8]);
let signer = format!("{}={}", keypair.pubkey(), sig);
⋮----
let pubkey2 = keypair2.pubkey();
let sig2 = keypair.sign_message(&[0u8]);
let signer2 = format!("{}={}", keypair2.pubkey(), sig2);
⋮----
let (nonce_keypair_file, mut nonce_tmp_file) = make_tmp_file();
⋮----
write_keypair(&nonce_authority_keypair, nonce_tmp_file.as_file_mut()).unwrap();
let nonce_account_pubkey = nonce_authority_keypair.pubkey();
let nonce_account_string = nonce_account_pubkey.to_string();
⋮----
let (fee_payer_keypair_file, mut fee_payer_tmp_file) = make_tmp_file();
⋮----
write_keypair(&fee_payer_keypair, fee_payer_tmp_file.as_file_mut()).unwrap();
let fee_payer_pubkey = fee_payer_keypair.pubkey();
let fee_payer_string = fee_payer_pubkey.to_string();
⋮----
let sig = fee_payer_keypair.sign_message(&[0u8]);
let signer = format!("{fee_payer_string}={sig}");
⋮----
let custodian_string = format!("{custodian}");
⋮----
let authorized_string = format!("{authorized}");
let test_create_stake_account = test_commands.clone().get_matches_from(vec![
⋮----
let test_create_stake_account2 = test_commands.clone().get_matches_from(vec![
⋮----
let (withdrawer_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&withdrawer_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
assert!(parse_command(&test_create_stake_account, &default_signer, &mut None).is_err());
⋮----
let nonce_account_string = nonce_account.to_string();
let offline = keypair_from_seed(&[2u8; 32]).unwrap();
let offline_pubkey = offline.pubkey();
let offline_string = offline_pubkey.to_string();
let offline_sig = offline.sign_message(&[3u8]);
let offline_signer = format!("{offline_pubkey}={offline_sig}");
⋮----
let nonce_hash_string = nonce_hash.to_string();
⋮----
let vote_account_string = vote_account_pubkey.to_string();
let test_delegate_stake = test_commands.clone().get_matches_from(vec![
⋮----
let sig1 = Keypair::new().sign_message(&[0u8]);
let signer1 = format!("{key1}={sig1}");
⋮----
let sig2 = Keypair::new().sign_message(&[0u8]);
let signer2 = format!("{key2}={sig2}");
⋮----
let test_withdraw_stake = test_commands.clone().get_matches_from(vec![
⋮----
let test_deactivate_stake = test_commands.clone().get_matches_from(vec![
⋮----
let (split_stake_account_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&split_stake_account_keypair, tmp_file.as_file_mut()).unwrap();
let test_split_stake_account = test_commands.clone().get_matches_from(vec![
⋮----
let nonce_auth = keypair_from_seed(&[2u8; 32]).unwrap();
let nonce_auth_pubkey = nonce_auth.pubkey();
let nonce_auth_string = nonce_auth_pubkey.to_string();
let nonce_sig = nonce_auth.sign_message(&[0u8]);
let nonce_signer = format!("{nonce_auth_pubkey}={nonce_sig}");
let stake_auth = keypair_from_seed(&[3u8; 32]).unwrap();
let stake_auth_pubkey = stake_auth.pubkey();
let stake_auth_string = stake_auth_pubkey.to_string();
let stake_sig = stake_auth.sign_message(&[0u8]);
let stake_signer = format!("{stake_auth_pubkey}={stake_sig}");
⋮----
let test_merge_stake_account = test_commands.clone().get_matches_from(vec![

================
File: cli/src/test_utils.rs
================
macro_rules! check_balance {
⋮----
pub async fn check_ready(rpc_client: &RpcClient) {
⋮----
.get_slot_with_commitment(CommitmentConfig::processed())
⋮----
.unwrap()
⋮----
sleep(Duration::from_millis(DEFAULT_MS_PER_SLOT));
⋮----
pub async fn wait_n_slots(rpc_client: &RpcClient, n: u64) -> u64 {
let slot = rpc_client.get_slot().await.unwrap();
⋮----
let new_slot = rpc_client.get_slot().await.unwrap();
if new_slot.saturating_sub(slot) >= n {
⋮----
pub async fn wait_for_next_epoch_plus_n_slots(rpc_client: &RpcClient, n: u64) -> (Epoch, u64) {
let current_epoch = rpc_client.get_epoch_info().await.unwrap().epoch;
let next_epoch = current_epoch.saturating_add(1);
println!("waiting for epoch {next_epoch} plus {n} slots");
⋮----
let next_epoch = rpc_client.get_epoch_info().await.unwrap().epoch;
⋮----
let new_slot = wait_n_slots(rpc_client, n).await;

================
File: cli/src/validator_info.rs
================
pub fn check_details_length(string: String) -> Result<(), String> {
if string.len() > MAX_LONG_FIELD_LENGTH {
Err(format!(
⋮----
Ok(())
⋮----
pub fn check_total_length(info: &ValidatorInfo) -> Result<(), String> {
let size = serialized_size(&info).unwrap();
⋮----
pub fn check_url(string: String) -> Result<(), String> {
is_url(string.clone())?;
if string.len() > MAX_SHORT_FIELD_LENGTH {
⋮----
pub fn is_short_field(string: String) -> Result<(), String> {
⋮----
fn verify_keybase(
⋮----
if let Some(keybase_username) = keybase_username.as_str() {
⋮----
format!("https://keybase.pub/{keybase_username}/solana/validator-{validator_pubkey:?}");
⋮----
if client.head(&url).send()?.status().is_success() {
⋮----
.into())
⋮----
Err(format!("keybase_username could not be parsed as String: {keybase_username}").into())
⋮----
fn parse_args(matches: &ArgMatches<'_>) -> Value {
⋮----
map.insert(
"name".to_string(),
Value::String(matches.value_of("name").unwrap().to_string()),
⋮----
if let Some(url) = matches.value_of("website") {
map.insert("website".to_string(), Value::String(url.to_string()));
⋮----
if let Some(icon_url) = matches.value_of("icon_url") {
map.insert("iconUrl".to_string(), Value::String(icon_url.to_string()));
⋮----
if let Some(details) = matches.value_of("details") {
map.insert("details".to_string(), Value::String(details.to_string()));
⋮----
if let Some(keybase_username) = matches.value_of("keybase_username") {
⋮----
"keybaseUsername".to_string(),
Value::String(keybase_username.to_string()),
⋮----
fn parse_validator_info(
⋮----
return Err(format!("{pubkey} is not a validator info account").into());
⋮----
let key_list: ConfigKeys = deserialize(&account.data)?;
if !key_list.keys.is_empty() {
⋮----
let validator_info_string: String = deserialize(get_config_data(&account.data)?)?;
⋮----
Ok((validator_pubkey, validator_info))
⋮----
Err(format!("{pubkey} could not be parsed as a validator info account").into())
⋮----
pub trait ValidatorInfoSubCommands {
⋮----
impl ValidatorInfoSubCommands for App<'_, '_> {
fn validator_info_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Publish/get Validator info on Solana")
.setting(AppSettings::SubcommandRequiredElseHelp)
.subcommand(
⋮----
.about("Publish Validator info on Solana")
.arg(
⋮----
.short("p")
.long("info-pubkey")
.value_name("PUBKEY")
.takes_value(true)
.validator(is_pubkey)
.help("The pubkey of the Validator info account to update"),
⋮----
.index(1)
.value_name("NAME")
⋮----
.required(true)
.validator(is_short_field)
.help("Validator name"),
⋮----
.short("w")
.long("website")
.value_name("URL")
⋮----
.validator(check_url)
.help("Validator website url"),
⋮----
.short("i")
.long("icon-url")
⋮----
.help("Validator icon URL"),
⋮----
.short("n")
.long("keybase")
.value_name("USERNAME")
⋮----
.hidden(hidden_unless_forced()) // Being phased out
.help("Validator Keybase username"),
⋮----
.short("d")
.long("details")
.value_name("DETAILS")
⋮----
.validator(check_details_length)
.help("Validator description"),
⋮----
.long("force")
.takes_value(false)
.hidden(hidden_unless_forced()) // Don't document this argument to discourage its use
.help("Override keybase username validity check"),
⋮----
.arg(compute_unit_price_arg()),
⋮----
.about("Get and parse Solana Validator info")
⋮----
.help(
⋮----
pub fn parse_validator_info_command(
⋮----
let info_pubkey = pubkey_of(matches, "info_pubkey");
let compute_unit_price = value_of(matches, COMPUTE_UNIT_PRICE_ARG.name);
// Prepare validator info
let validator_info = parse_args(matches);
Ok(CliCommandInfo {
⋮----
force_keybase: matches.is_present("force"),
⋮----
signers: vec![default_signer.signer_from_path(matches, wallet_manager)?],
⋮----
pub fn parse_get_validator_info_command(
⋮----
Ok(CliCommandInfo::without_signers(
⋮----
pub async fn process_set_validator_info(
⋮----
// Validate keybase username
if let Some(string) = validator_info.get("keybaseUsername") {
⋮----
println!("--force supplied, skipping Keybase verification");
⋮----
let result = verify_keybase(&config.signers[0].pubkey(), string);
if result.is_err() {
result.map_err(|err| {
CliError::BadParameter(format!("Invalid validator keybase username: {err}"))
⋮----
let validator_string = serde_json::to_string(&validator_info).unwrap();
⋮----
let result = check_total_length(&validator_info);
⋮----
CliError::BadParameter(format!("Maximum size for validator info: {err}"))
⋮----
// Check for existing validator-info account
⋮----
.get_program_accounts(&solana_config_interface::id())
⋮----
.iter()
.filter(
⋮----
Ok(key_list) => key_list.keys.contains(&(validator_info::id(), false)),
⋮----
.find(|(pubkey, account)| {
let (validator_pubkey, _) = parse_validator_info(pubkey, account).unwrap();
validator_pubkey == config.signers[0].pubkey()
⋮----
// Create validator-info keypair to use if info_pubkey not provided or does not exist
⋮----
info_keypair.pubkey()
⋮----
// Check existence of validator-info account
let balance = rpc_client.get_balance(&info_pubkey).await.unwrap_or(0);
let keys = vec![
⋮----
.checked_add(serialized_size(&ConfigKeys { keys: keys.clone() }).unwrap())
.expect("ValidatorInfo and two keys fit into a u64");
⋮----
.get_minimum_balance_for_rent_exemption(data_len as usize)
⋮----
if info_pubkey != info_keypair.pubkey() {
println!("Account {info_pubkey:?} does not exist. Generating new keypair...");
info_pubkey = info_keypair.pubkey();
⋮----
vec![config.signers[0], &info_keypair]
⋮----
vec![config.signers[0]]
⋮----
let keys = keys.clone();
⋮----
println!(
⋮----
&config.signers[0].pubkey(),
⋮----
keys.clone(),
⋮----
.with_compute_unit_config(&ComputeUnitConfig {
⋮----
instructions.extend_from_slice(&[config_instruction::store(
⋮----
Message::new(&instructions, Some(&config.signers[0].pubkey()))
⋮----
let instructions = vec![config_instruction::store(
⋮----
// Submit transaction
let latest_blockhash = rpc_client.get_latest_blockhash().await?;
let (message, _) = resolve_spend_tx_and_check_account_balance(
⋮----
tx.try_sign(&signers, latest_blockhash)?;
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
println!("Success! Validator info published at: {info_pubkey:?}");
println!("{signature_str}");
Ok("".to_string())
⋮----
pub async fn process_get_validator_info(
⋮----
vec![(
⋮----
.into_iter()
.filter(|(_, validator_info_account)| {
⋮----
.collect()
⋮----
let mut validator_info_list: Vec<CliValidatorInfo> = vec![];
if validator_info.is_empty() {
println!("No validator info accounts found");
⋮----
for (validator_info_pubkey, validator_info_account) in validator_info.iter() {
⋮----
parse_validator_info(validator_info_pubkey, validator_info_account)?;
validator_info_list.push(CliValidatorInfo {
identity_pubkey: validator_pubkey.to_string(),
info_pubkey: validator_info_pubkey.to_string(),
⋮----
Ok(config
⋮----
.formatted_string(&CliValidatorInfoVec::new(validator_info_list)))
⋮----
mod tests {
⋮----
fn test_check_details_length() {
let short_details = (0..MAX_LONG_FIELD_LENGTH).map(|_| "X").collect::<String>();
assert_eq!(check_details_length(short_details), Ok(()));
⋮----
.map(|_| "X")
⋮----
assert_eq!(
⋮----
fn test_check_url() {
⋮----
assert_eq!(check_url(url.to_string()), Ok(()));
⋮----
assert!(check_url(long_url.to_string()).is_err());
⋮----
assert!(check_url(non_url.to_string()).is_err());
⋮----
fn test_is_short_field() {
⋮----
assert_eq!(is_short_field(name.to_string()), Ok(()));
⋮----
assert!(is_short_field(long_name.to_string()).is_err());
⋮----
fn test_verify_keybase_username_not_string() {
⋮----
fn test_parse_args() {
let matches = get_clap_app("test", "desc", "version").get_matches_from(vec![
⋮----
let subcommand_matches = matches.subcommand();
assert_eq!(subcommand_matches.0, "validator-info");
assert!(subcommand_matches.1.is_some());
let subcommand_matches = subcommand_matches.1.unwrap().subcommand();
assert_eq!(subcommand_matches.0, "publish");
⋮----
let matches = subcommand_matches.1.unwrap();
let expected = json!({
⋮----
assert_eq!(parse_args(matches), expected);
⋮----
fn test_validator_info_serde() {
⋮----
info.insert("name".to_string(), Value::String("Alice".to_string()));
let info_string = serde_json::to_string(&Value::Object(info)).unwrap();
⋮----
info: info_string.clone(),
⋮----
assert_eq!(serialized_size(&validator_info).unwrap(), 24);
⋮----
let deserialized: ValidatorInfo = deserialize(&[
⋮----
.unwrap();
assert_eq!(deserialized.info, info_string);
⋮----
fn test_parse_validator_info() {
⋮----
let keys = vec![(validator_info::id(), false), (pubkey, true)];
⋮----
let info_string = serde_json::to_string(&Value::Object(info.clone())).unwrap();
⋮----
let data = serialize(&(config, validator_info)).unwrap();
⋮----
fn test_parse_validator_info_not_validator_info_account() {
assert!(parse_validator_info(
⋮----
fn test_parse_validator_info_empty_key_list() {
let config = ConfigKeys { keys: vec![] };
⋮----
fn test_validator_info_max_space() {
⋮----
"Max Length String KWpP299aFCBWvWg1MHpSuaoTsud7cv8zMJsh99aAtP8X1s26yrR1".to_string();
⋮----
.to_string();
⋮----
info.insert("name".to_string(), Value::String(max_short_string.clone()));
info.insert(
"website".to_string(),
Value::String(max_short_string.clone()),
⋮----
info.insert("details".to_string(), Value::String(max_long_string));

================
File: cli/src/vote.rs
================
pub trait VoteSubCommands {
⋮----
impl VoteSubCommands for App<'_, '_> {
fn vote_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Create a vote account")
.arg(
⋮----
.index(1)
.value_name("ACCOUNT_KEYPAIR")
.takes_value(true)
.required(true)
.validator(is_valid_signer)
.help("Vote account keypair to create"),
⋮----
.index(2)
.value_name("IDENTITY_KEYPAIR")
⋮----
.help("Keypair of validator that will vote with this account"),
⋮----
.arg(pubkey!(
⋮----
.long("commission")
.value_name("PERCENTAGE")
⋮----
.default_value("100")
.help("The commission taken on reward redemption (0-100)"),
⋮----
.long("allow-unsafe-authorized-withdrawer")
.takes_value(false)
.help(
⋮----
.long("seed")
.value_name("STRING")
⋮----
.offline_args()
.nonce_args(false)
.arg(fee_payer_arg())
.arg(memo_arg())
.arg(compute_unit_price_arg()),
⋮----
.subcommand(
⋮----
.about("Authorize a new vote signing keypair for the given vote account")
⋮----
.value_name("AUTHORIZED_KEYPAIR")
⋮----
.help("Current authorized vote signer."),
⋮----
.about("Authorize a new withdraw signing keypair for the given vote account")
⋮----
.help("Current authorized withdrawer."),
⋮----
.about(
⋮----
.index(3)
.value_name("NEW_AUTHORIZED_KEYPAIR")
⋮----
.help("New authorized vote signer."),
⋮----
.help("New authorized withdrawer."),
⋮----
.about("Update the vote account's validator identity")
⋮----
.help("Keypair of new validator that will vote with this account"),
⋮----
.help("Authorized withdrawer keypair"),
⋮----
.about("Update the vote account's commission")
⋮----
.validator(is_valid_percentage)
.help("The new commission"),
⋮----
.about("Show the contents of a vote account")
.alias("show-vote-account")
⋮----
.long("lamports")
⋮----
.help("Display balance in lamports instead of SOL"),
⋮----
.long("with-rewards")
⋮----
.help("Display inflation rewards"),
⋮----
.long("csv")
⋮----
.help("Format rewards in a CSV table"),
⋮----
.long("starting-epoch")
⋮----
.value_name("NUM")
.requires("with_rewards")
.help("Start displaying from epoch NUM"),
⋮----
.long("num-rewards-epochs")
⋮----
.validator(|s| is_within_range(s, 1..=50))
.default_value_if("with_rewards", None, "1")
⋮----
.about("Withdraw lamports from a vote account into a specified account")
⋮----
.value_name("AMOUNT")
⋮----
.validator(is_amount_or_all)
⋮----
.long("authorized-withdrawer")
⋮----
.help("Authorized withdrawer [default: cli config keypair]"),
⋮----
.about("Close a vote account and withdraw all funds remaining")
⋮----
pub fn parse_create_vote_account(
⋮----
let (vote_account, vote_account_pubkey) = signer_of(matches, "vote_account", wallet_manager)?;
let seed = matches.value_of("seed").map(|s| s.to_string());
⋮----
signer_of(matches, "identity_account", wallet_manager)?;
let commission = value_t_or_exit!(matches, "commission", u8);
let authorized_voter = pubkey_of_signer(matches, "authorized_voter", wallet_manager)?;
⋮----
pubkey_of_signer(matches, "authorized_withdrawer", wallet_manager)?.unwrap();
let allow_unsafe = matches.is_present("allow_unsafe_authorized_withdrawer");
let sign_only = matches.is_present(SIGN_ONLY_ARG.name);
let dump_transaction_message = matches.is_present(DUMP_TRANSACTION_MESSAGE.name);
⋮----
let nonce_account = pubkey_of_signer(matches, NONCE_ARG.name, wallet_manager)?;
let memo = matches.value_of(MEMO_ARG.name).map(String::from);
⋮----
signer_of(matches, NONCE_AUTHORITY_ARG.name, wallet_manager)?;
let (fee_payer, fee_payer_pubkey) = signer_of(matches, FEE_PAYER_ARG.name, wallet_manager)?;
let compute_unit_price = value_of(matches, COMPUTE_UNIT_PRICE_ARG.name);
⋮----
if authorized_withdrawer == vote_account_pubkey.unwrap() {
return Err(CliError::BadParameter(
⋮----
.to_owned(),
⋮----
if authorized_withdrawer == identity_pubkey.unwrap() {
⋮----
let mut bulk_signers = vec![fee_payer, vote_account, identity_account];
if nonce_account.is_some() {
bulk_signers.push(nonce_authority);
⋮----
default_signer.generate_unique_signers(bulk_signers, matches, wallet_manager)?;
Ok(CliCommandInfo {
⋮----
vote_account: signer_info.index_of(vote_account_pubkey).unwrap(),
⋮----
identity_account: signer_info.index_of(identity_pubkey).unwrap(),
⋮----
nonce_authority: signer_info.index_of(nonce_authority_pubkey).unwrap(),
⋮----
fee_payer: signer_info.index_of(fee_payer_pubkey).unwrap(),
⋮----
pub fn parse_vote_authorize(
⋮----
pubkey_of_signer(matches, "vote_account_pubkey", wallet_manager)?.unwrap();
let (authorized, authorized_pubkey) = signer_of(matches, "authorized", wallet_manager)?;
⋮----
let nonce_account = pubkey_of(matches, NONCE_ARG.name);
⋮----
let mut bulk_signers = vec![fee_payer, authorized];
⋮----
signer_of(matches, "new_authorized", wallet_manager)?;
bulk_signers.push(new_authorized_signer);
new_authorized_pubkey.unwrap()
⋮----
pubkey_of_signer(matches, "new_authorized_pubkey", wallet_manager)?.unwrap()
⋮----
authorized: signer_info.index_of(authorized_pubkey).unwrap(),
⋮----
signer_info.index_of(Some(new_authorized_pubkey))
⋮----
pub fn parse_vote_update_validator(
⋮----
signer_of(matches, "new_identity_account", wallet_manager)?;
⋮----
signer_of(matches, "authorized_withdrawer", wallet_manager)?;
⋮----
let mut bulk_signers = vec![fee_payer, authorized_withdrawer, new_identity_account];
⋮----
new_identity_account: signer_info.index_of(new_identity_pubkey).unwrap(),
withdraw_authority: signer_info.index_of(authorized_withdrawer_pubkey).unwrap(),
⋮----
pub fn parse_vote_update_commission(
⋮----
let mut bulk_signers = vec![fee_payer, authorized_withdrawer];
⋮----
pub fn parse_vote_get_account_command(
⋮----
let use_lamports_unit = matches.is_present("lamports");
let use_csv = matches.is_present("csv");
let with_rewards = if matches.is_present("with_rewards") {
Some(value_of(matches, "num_rewards_epochs").unwrap())
⋮----
let starting_epoch = value_of(matches, "starting_epoch");
Ok(CliCommandInfo::without_signers(
⋮----
pub fn parse_withdraw_from_vote_account(
⋮----
pubkey_of_signer(matches, "destination_account_pubkey", wallet_manager)?.unwrap();
⋮----
let mut bulk_signers = vec![fee_payer, withdraw_authority];
⋮----
withdraw_authority: signer_info.index_of(withdraw_authority_pubkey).unwrap(),
⋮----
pub fn parse_close_vote_account(
⋮----
let signer_info = default_signer.generate_unique_signers(
vec![fee_payer, withdraw_authority],
⋮----
pub async fn process_create_vote_account(
⋮----
let vote_account_pubkey = vote_account.pubkey();
⋮----
check_unique_pubkeys(
(&config.signers[0].pubkey(), "cli keypair".to_string()),
(&vote_account_address, "vote_account".to_string()),
⋮----
let identity_pubkey = identity_account.pubkey();
⋮----
(&identity_pubkey, "identity_pubkey".to_string()),
⋮----
.get_minimum_balance_for_rent_exemption(VoteStateV4::size_of())
⋮----
.max(1);
⋮----
authorized_voter: authorized_voter.unwrap_or(identity_pubkey),
⋮----
create_vote_account_config.with_seed = Some((&vote_account_pubkey, seed));
⋮----
&config.signers[0].pubkey(),
⋮----
.with_memo(memo)
.with_compute_unit_config(&ComputeUnitConfig {
⋮----
Some(&fee_payer.pubkey()),
⋮----
&nonce_authority.pubkey(),
⋮----
Message::new(&ixs, Some(&fee_payer.pubkey()))
⋮----
.get_blockhash(rpc_client, config.commitment)
⋮----
let (message, _) = resolve_spend_tx_and_check_account_balances(
⋮----
&fee_payer.pubkey(),
⋮----
.get_account_with_commitment(&vote_account_address, config.commitment)
⋮----
format!("Vote account {vote_account_address} already exists")
⋮----
format!(
⋮----
return Err(CliError::BadParameter(err_msg).into());
⋮----
check_nonce_account(&nonce_account, &nonce_authority.pubkey(), &recent_blockhash)?;
⋮----
tx.try_partial_sign(&config.signers, recent_blockhash)?;
return_signers_with_config(
⋮----
tx.try_sign(&config.signers, recent_blockhash)?;
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
pub async fn process_vote_authorize(
⋮----
let new_authorized_signer = new_authorized.map(|index| config.signers[index]);
⋮----
Some(
get_vote_account(rpc_client, vote_account_pubkey, config.commitment)
⋮----
let current_epoch = rpc_client.get_epoch_info().await?.epoch;
⋮----
.get_authorized_voter(current_epoch)
.ok_or_else(|| {
⋮----
"Invalid vote account state; no authorized voters found".to_string(),
⋮----
check_current_authority(
⋮----
&authorized.pubkey(),
⋮----
if signer.is_interactive() {
return Err(CliError::BadParameter(format!(
⋮----
.into());
⋮----
(&authorized.pubkey(), "authorized_account".to_string()),
(new_authorized_pubkey, "new_authorized_pubkey".to_string()),
⋮----
check_current_authority(&[vote_state.authorized_withdrawer], &authorized.pubkey())?
⋮----
let vote_ix = if new_authorized_signer.is_some() {
⋮----
vote_account_pubkey,   // vote account to update
&authorized.pubkey(),  // current authorized
new_authorized_pubkey, // new vote signer/withdrawer
vote_authorize,        // vote or withdraw
⋮----
let ixs = vec![vote_ix]
⋮----
simulate_and_update_compute_unit_limit(&compute_unit_limit, rpc_client, &mut message).await?;
⋮----
check_account_for_fee_with_commitment(
⋮----
pub async fn process_vote_update_validator(
⋮----
let new_identity_pubkey = new_identity_account.pubkey();
⋮----
(vote_account_pubkey, "vote_account_pubkey".to_string()),
(&new_identity_pubkey, "new_identity_account".to_string()),
⋮----
let ixs = vec![vote_instruction::update_validator_identity(
⋮----
pub async fn process_vote_update_commission(
⋮----
let ixs = vec![vote_instruction::update_commission(
⋮----
pub(crate) async fn get_vote_account(
⋮----
.get_account_with_commitment(vote_account_pubkey, commitment_config)
⋮----
CliError::RpcRequestError(format!("{vote_account_pubkey:?} account does not exist"))
⋮----
return Err(CliError::RpcRequestError(format!(
⋮----
VoteStateV4::deserialize(&vote_account.data, vote_account_pubkey).map_err(|_| {
⋮----
"Account data could not be deserialized to vote state".to_string(),
⋮----
Ok((vote_account, vote_state))
⋮----
pub async fn process_show_vote_account(
⋮----
get_vote_account(rpc_client, vote_account_address, config.commitment).await?;
let epoch_schedule = rpc_client.get_epoch_schedule().await?;
⋮----
.get_account_with_commitment(
⋮----
.ok()
.and_then(|response| response.value)
.and_then(|account| from_account(&account))
.and_then(|feature| feature.activated_at);
let tvc_activation_epoch = tvc_activation_slot.map(|s| epoch_schedule.get_epoch(s));
let mut votes: Vec<CliLandedVote> = vec![];
let mut epoch_voting_history: Vec<CliEpochVotingHistory> = vec![];
if !vote_state.votes.is_empty() {
⋮----
votes.push(vote.into());
⋮----
for (epoch, credits, prev_credits) in vote_state.epoch_credits.iter().copied() {
let credits_earned = credits.saturating_sub(prev_credits);
let slots_in_epoch = epoch_schedule.get_slots_in_epoch(epoch);
let is_tvc_active = tvc_activation_epoch.map(|e| epoch >= e).unwrap_or_default();
⋮----
epoch_voting_history.push(CliEpochVotingHistory {
⋮----
Ok(rewards) => Some(rewards),
⋮----
eprintln!("Failed to fetch epoch rewards: {error:?}");
⋮----
validator_identity: vote_state.node_pubkey.to_string(),
authorized_voters: (&vote_state.authorized_voters).into(),
authorized_withdrawer: vote_state.authorized_withdrawer.to_string(),
credits: vote_state.credits(),
⋮----
recent_timestamp: vote_state.last_timestamp.clone(),
⋮----
inflation_rewards_collector: vote_state.inflation_rewards_collector.to_string(),
block_revenue_collector: vote_state.block_revenue_collector.to_string(),
⋮----
.map(|bytes| bs58::encode(bytes).into_string()),
⋮----
Ok(config.output_format.formatted_string(&vote_account_data))
⋮----
pub async fn process_withdraw_from_vote_account(
⋮----
let ixs = vec![withdraw(
⋮----
let current_balance = rpc_client.get_balance(vote_account_pubkey).await?;
⋮----
let balance_remaining = current_balance.saturating_sub(withdraw_amount);
⋮----
pub async fn process_close_vote_account(
⋮----
.get_vote_accounts_with_config(RpcGetVoteAccountsConfig {
vote_pubkey: Some(vote_account_pubkey.to_string()),
⋮----
.into_iter()
.chain(vote_account_status.delinquent)
.next()
⋮----
return Err(format!(
⋮----
let latest_blockhash = rpc_client.get_latest_blockhash().await?;
⋮----
let mut message = Message::new(&ixs, Some(&fee_payer.pubkey()));
⋮----
tx.try_sign(&config.signers, latest_blockhash)?;
⋮----
mod tests {
⋮----
fn make_tmp_file() -> (String, NamedTempFile) {
let tmp_file = NamedTempFile::new().unwrap();
(String::from(tmp_file.path().to_str().unwrap()), tmp_file)
⋮----
fn test_parse_command() {
let test_commands = get_clap_app("test", "desc", "version");
⋮----
let pubkey = keypair.pubkey();
let pubkey_string = pubkey.to_string();
⋮----
let pubkey2 = keypair2.pubkey();
let pubkey2_string = pubkey2.to_string();
let sig2 = keypair2.sign_message(&[0u8]);
let signer2 = format!("{}={}", keypair2.pubkey(), sig2);
⋮----
let (default_keypair_file, mut tmp_file) = make_tmp_file();
write_keypair(&default_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
let blockhash_string = format!("{blockhash}");
⋮----
// Test VoteAuthorize SubCommand
let test_authorize_voter = test_commands.clone().get_matches_from(vec![
⋮----
assert_eq!(
⋮----
let (authorized_keypair_file, mut tmp_file) = make_tmp_file();
write_keypair(&authorized_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
let authorized_sig = authorized_keypair.sign_message(&[0u8]);
let authorized_signer = format!("{}={}", authorized_keypair.pubkey(), authorized_sig);
⋮----
let (voter_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&voter_keypair, tmp_file.as_file_mut()).unwrap();
⋮----
assert!(parse_command(&test_authorize_voter, &default_signer, &mut None).is_err());
let (identity_keypair_file, mut tmp_file) = make_tmp_file();
⋮----
let authorized_withdrawer = Keypair::new().pubkey();
write_keypair(&identity_keypair, tmp_file.as_file_mut()).unwrap();
let (keypair_file, mut tmp_file) = make_tmp_file();
⋮----
write_keypair(&keypair, tmp_file.as_file_mut()).unwrap();
let test_create_vote_account = test_commands.clone().get_matches_from(vec![
⋮----
let test_create_vote_account2 = test_commands.clone().get_matches_from(vec![
⋮----
let identity_sig = identity_keypair.sign_message(&[0u8]);
let identity_signer = format!("{}={}", identity_keypair.pubkey(), identity_sig);
⋮----
let test_create_vote_account3 = test_commands.clone().get_matches_from(vec![
⋮----
let test_create_vote_account4 = test_commands.clone().get_matches_from(vec![
⋮----
let test_update_validator = test_commands.clone().get_matches_from(vec![
⋮----
let test_update_commission = test_commands.clone().get_matches_from(vec![
⋮----
let test_withdraw_from_vote_account = test_commands.clone().get_matches_from(vec![
⋮----
let (withdraw_authority_file, mut tmp_file) = make_tmp_file();
write_keypair(&withdraw_authority, tmp_file.as_file_mut()).unwrap();
⋮----
let authorized_sig = withdraw_authority.sign_message(&[0u8]);
let authorized_signer = format!("{}={}", withdraw_authority.pubkey(), authorized_sig);
⋮----
let test_close_vote_account = test_commands.clone().get_matches_from(vec![

================
File: cli/src/wallet.rs
================
pub trait WalletSubCommands {
⋮----
impl WalletSubCommands for App<'_, '_> {
fn wallet_subcommands(self) -> Self {
self.subcommand(
⋮----
.about("Show the contents of an account")
.alias("account")
.arg(pubkey!(
⋮----
.arg(
⋮----
.long("output-file")
.short("o")
.value_name("FILEPATH")
.takes_value(true)
.help("Write the account data to this file"),
⋮----
.long("lamports")
.takes_value(false)
.help("Display balance in lamports instead of SOL"),
⋮----
.subcommand(
⋮----
.about("Get your public key")
⋮----
.long("confirm-key")
⋮----
.help("Confirm key on device; only relevant if using remote wallet"),
⋮----
.about("Request SOL from a faucet")
⋮----
.index(1)
.value_name("AMOUNT")
⋮----
.validator(is_amount)
.required(true)
.help("The airdrop amount to request, in SOL"),
⋮----
.about("Get your balance")
⋮----
.about("Confirm transaction by signature")
⋮----
.value_name("TRANSACTION_SIGNATURE")
⋮----
.help("The transaction signature to confirm"),
⋮----
.after_help(CONFIRM_AFTER_HELP_MESSAGE),
⋮----
.about(
⋮----
.value_name("SEED_STRING")
⋮----
.validator(is_derived_address_seed)
.help("The seed.  Must not take more than 32 bytes to encode as utf-8"),
⋮----
.index(2)
.value_name("PROGRAM_ID")
⋮----
.help(
⋮----
.about("Generate a program derived account address with a seed")
⋮----
.min_values(0)
.value_name("SEED")
⋮----
.validator(is_structured_seed)
.help(SEEDS_ARG_HELP_MESSAGE),
⋮----
.about("Decode a serialized transaction")
⋮----
.value_name("TRANSACTION")
⋮----
.help("transaction to decode"),
⋮----
.value_name("ENCODING")
.possible_values(&["base58", "base64"])
.default_value("base58")
⋮----
.help("transaction encoding"),
⋮----
.value_name("SIGNER_KEYPAIR")
⋮----
.validator(is_valid_signer)
.help("The signer path to resolve"),
⋮----
.about("Transfer funds between system accounts")
.alias("pay")
⋮----
.validator(is_amount_or_all)
⋮----
.help("The amount to send, in SOL; accepts keyword ALL"),
⋮----
.long("no-wait")
⋮----
.long("derived-address-seed")
⋮----
.requires("derived_address_program_id")
⋮----
.hidden(hidden_unless_forced()),
⋮----
.long("derived-address-program-id")
⋮----
.requires("derived_address_seed")
⋮----
.long("allow-unfunded-recipient")
⋮----
.help("Complete the transfer even if the recipient address is not funded"),
⋮----
.offline_args()
.nonce_args(false)
.arg(memo_arg())
.arg(fee_payer_arg())
.arg(compute_unit_price_arg()),
⋮----
.about("Sign off-chain message")
⋮----
.value_name("STRING")
⋮----
.help("The message text to be signed"),
⋮----
.long("version")
⋮----
.value_name("VERSION")
.required(false)
.default_value("0")
.validator(|p| match p.parse::<u8>() {
Err(_) => Err(String::from("Must be unsigned integer")),
Ok(_) => Ok(()),
⋮----
.help("The off-chain message version"),
⋮----
.about("Verify off-chain message signature")
⋮----
.help("The text of the original message"),
⋮----
.value_name("SIGNATURE")
⋮----
.help("The message signature to verify"),
⋮----
fn resolve_derived_address_program_id(matches: &ArgMatches<'_>, arg_name: &str) -> Option<Pubkey> {
matches.value_of(arg_name).and_then(|v| {
let upper = v.to_ascii_uppercase();
match upper.as_str() {
"NONCE" | "SYSTEM" => Some(system_program::id()),
"STAKE" => Some(stake::id()),
"VOTE" => Some(solana_vote_program::id()),
_ => pubkey_of(matches, arg_name),
⋮----
pub fn parse_account(
⋮----
let account_pubkey = pubkey_of_signer(matches, "account_pubkey", wallet_manager)?.unwrap();
let output_file = matches.value_of("output_file");
let use_lamports_unit = matches.is_present("lamports");
Ok(CliCommandInfo::without_signers(CliCommand::ShowAccount {
⋮----
output_file: output_file.map(ToString::to_string),
⋮----
pub fn parse_airdrop(
⋮----
let pubkey = pubkey_of_signer(matches, "to", wallet_manager)?;
let signers = if pubkey.is_some() {
vec![]
⋮----
vec![default_signer.signer_from_path(matches, wallet_manager)?]
⋮----
let lamports = lamports_of_sol(matches, "amount").unwrap();
Ok(CliCommandInfo {
⋮----
pub fn parse_balance(
⋮----
let pubkey = pubkey_of_signer(matches, "pubkey", wallet_manager)?;
⋮----
use_lamports_unit: matches.is_present("lamports"),
⋮----
pub fn parse_decode_transaction(matches: &ArgMatches<'_>) -> Result<CliCommandInfo, CliError> {
let blob = value_t_or_exit!(matches, "transaction", String);
let binary_encoding = match matches.value_of("encoding").unwrap() {
⋮----
_ => unreachable!(),
⋮----
if let Some(transaction) = encoded_transaction.decode() {
Ok(CliCommandInfo::without_signers(
⋮----
Err(CliError::BadParameter(
"Unable to decode transaction".to_string(),
⋮----
pub fn parse_create_address_with_seed(
⋮----
let from_pubkey = pubkey_of_signer(matches, "from", wallet_manager)?;
let signers = if from_pubkey.is_some() {
⋮----
let program_id = resolve_derived_address_program_id(matches, "program_id").unwrap();
let seed = matches.value_of("seed").unwrap().to_string();
⋮----
pub fn parse_find_program_derived_address(
⋮----
let program_id = resolve_derived_address_program_id(matches, "program_id")
.ok_or_else(|| CliError::BadParameter("PROGRAM_ID".to_string()))?;
⋮----
.values_of("seeds")
.map(|seeds| {
⋮----
.map(|value| {
let (prefix, value) = value.split_once(':').unwrap();
⋮----
"pubkey" => Pubkey::from_str(value).unwrap().to_bytes().to_vec(),
"string" => value.as_bytes().to_vec(),
"hex" => Vec::<u8>::from_hex(value).unwrap(),
"u8" => u8::from_str(value).unwrap().to_le_bytes().to_vec(),
"u16le" => u16::from_str(value).unwrap().to_le_bytes().to_vec(),
"u32le" => u32::from_str(value).unwrap().to_le_bytes().to_vec(),
"u64le" => u64::from_str(value).unwrap().to_le_bytes().to_vec(),
"u128le" => u128::from_str(value).unwrap().to_le_bytes().to_vec(),
"i16le" => i16::from_str(value).unwrap().to_le_bytes().to_vec(),
"i32le" => i32::from_str(value).unwrap().to_le_bytes().to_vec(),
"i64le" => i64::from_str(value).unwrap().to_le_bytes().to_vec(),
"i128le" => i128::from_str(value).unwrap().to_le_bytes().to_vec(),
"u16be" => u16::from_str(value).unwrap().to_be_bytes().to_vec(),
"u32be" => u32::from_str(value).unwrap().to_be_bytes().to_vec(),
"u64be" => u64::from_str(value).unwrap().to_be_bytes().to_vec(),
"u128be" => u128::from_str(value).unwrap().to_be_bytes().to_vec(),
"i16be" => i16::from_str(value).unwrap().to_be_bytes().to_vec(),
"i32be" => i32::from_str(value).unwrap().to_be_bytes().to_vec(),
"i64be" => i64::from_str(value).unwrap().to_be_bytes().to_vec(),
"i128be" => i128::from_str(value).unwrap().to_be_bytes().to_vec(),
// Must be unreachable due to arg validator
_ => unreachable!("parse_find_program_derived_address: {prefix}:{value}"),
⋮----
.unwrap_or_default();
⋮----
pub fn parse_transfer(
⋮----
let to = pubkey_of_signer(matches, "to", wallet_manager)?.unwrap();
let sign_only = matches.is_present(SIGN_ONLY_ARG.name);
let dump_transaction_message = matches.is_present(DUMP_TRANSACTION_MESSAGE.name);
let no_wait = matches.is_present("no_wait");
⋮----
let nonce_account = pubkey_of_signer(matches, NONCE_ARG.name, wallet_manager)?;
⋮----
signer_of(matches, NONCE_AUTHORITY_ARG.name, wallet_manager)?;
let memo = matches.value_of(MEMO_ARG.name).map(String::from);
let (fee_payer, fee_payer_pubkey) = signer_of(matches, FEE_PAYER_ARG.name, wallet_manager)?;
let (from, from_pubkey) = signer_of(matches, "from", wallet_manager)?;
let allow_unfunded_recipient = matches.is_present("allow_unfunded_recipient");
let mut bulk_signers = vec![fee_payer, from];
if nonce_account.is_some() {
bulk_signers.push(nonce_authority);
⋮----
default_signer.generate_unique_signers(bulk_signers, matches, wallet_manager)?;
let compute_unit_price = value_of(matches, COMPUTE_UNIT_PRICE_ARG.name);
⋮----
.value_of("derived_address_seed")
.map(|s| s.to_string());
⋮----
resolve_derived_address_program_id(matches, "derived_address_program_id");
⋮----
nonce_authority: signer_info.index_of(nonce_authority_pubkey).unwrap(),
⋮----
fee_payer: signer_info.index_of(fee_payer_pubkey).unwrap(),
from: signer_info.index_of(from_pubkey).unwrap(),
⋮----
pub fn parse_sign_offchain_message(
⋮----
let version: u8 = value_of(matches, "version").unwrap();
let message_text: String = value_of(matches, "message")
.ok_or_else(|| CliError::BadParameter("MESSAGE".to_string()))?;
let message = OffchainMessage::new(version, message_text.as_bytes())
.map_err(|_| CliError::BadParameter("VERSION or MESSAGE".to_string()))?;
⋮----
signers: vec![default_signer.signer_from_path(matches, wallet_manager)?],
⋮----
pub fn parse_verify_offchain_signature(
⋮----
let signer_pubkey = pubkey_of_signer(matches, "signer", wallet_manager)?;
let signers = if signer_pubkey.is_some() {
⋮----
let signature = value_of(matches, "signature")
.ok_or_else(|| CliError::BadParameter("SIGNATURE".to_string()))?;
⋮----
pub async fn process_show_account(
⋮----
let account = rpc_client.get_account(account_pubkey).await?;
⋮----
let mut account_string = config.output_format.formatted_string(&cli_account);
⋮----
f.write_all(account_string.as_bytes())?;
writeln!(&mut account_string)?;
writeln!(&mut account_string, "Wrote account to {output_file}")?;
⋮----
f.write_all(data)?;
⋮----
writeln!(&mut account_string, "Wrote account data to {output_file}")?;
} else if !data.is_empty() {
⋮----
writeln!(&mut account_string, "{:?}", data.hex_dump())?;
⋮----
Ok(account_string)
⋮----
pub async fn process_airdrop(
⋮----
config.pubkey()?
⋮----
println!(
⋮----
let pre_balance = rpc_client.get_balance(&pubkey).await?;
let result = request_and_confirm_airdrop(rpc_client, config, &pubkey, lamports).await;
⋮----
println!("{signature_cli_message}");
let current_balance = rpc_client.get_balance(&pubkey).await?;
if current_balance < pre_balance.saturating_add(lamports) {
println!("Balance unchanged");
println!("Run `solana confirm -v {signature:?}` for more info");
Ok("".to_string())
⋮----
Ok(build_balance_message(current_balance, false, true))
⋮----
pub async fn process_balance(
⋮----
let balance = rpc_client.get_balance(&pubkey).await?;
⋮----
Ok(config.output_format.formatted_string(&balance_output))
⋮----
pub async fn process_confirm(
⋮----
.get_signature_statuses_with_history(&[*signature])
⋮----
.get_transaction_with_config(
⋮----
encoding: Some(UiTransactionEncoding::Base64),
commitment: Some(CommitmentConfig::confirmed()),
max_supported_transaction_version: Some(0),
⋮----
transaction_with_meta.transaction.decode().unwrap();
let json_transaction = decoded_transaction.json_encode();
transaction = Some(CliTransaction {
⋮----
slot: Some(slot),
⋮----
prefix: "  ".to_string(),
sigverify_status: vec![],
⋮----
get_transaction_error = Some(format!("{err:?}"));
⋮----
confirmation_status: Some(transaction_status.confirmation_status()),
⋮----
err: transaction_status.err.clone().map(Into::into),
⋮----
Ok(config.output_format.formatted_string(&cli_transaction))
⋮----
Err(err) => Err(CliError::RpcRequestError(format!("Unable to confirm: {err}")).into()),
⋮----
pub fn process_decode_transaction(
⋮----
decoded_transaction: transaction.clone(),
transaction: transaction.json_encode(),
⋮----
prefix: "".to_string(),
⋮----
Ok(config.output_format.formatted_string(&decode_transaction))
⋮----
pub fn process_create_address_with_seed(
⋮----
Ok(address.to_string())
⋮----
pub fn process_find_program_derived_address(
⋮----
println!("Seeds: {seeds:?}");
⋮----
let seeds_slice = seeds.iter().map(|x| &x[..]).collect::<Vec<_>>();
⋮----
address: address.to_string(),
⋮----
Ok(config.output_format.formatted_string(&result))
⋮----
pub async fn process_transfer(
⋮----
let mut from_pubkey = from.pubkey();
⋮----
.get_blockhash(rpc_client, config.commitment)
⋮----
.get_balance_with_commitment(to, config.commitment)
⋮----
return Err(format!(
⋮----
.into());
⋮----
let derived_parts = derived_address_seed.zip(derived_address_program_id);
⋮----
Some((base_pubkey, seed, program_id, from_pubkey))
⋮----
let compute_unit_limit = if nonce_account.is_some() {
⋮----
let ixs = if let Some((base_pubkey, seed, program_id, from_pubkey)) = with_seed.as_ref() {
vec![system_instruction::transfer_with_seed(
⋮----
.with_memo(memo)
.with_compute_unit_config(&ComputeUnitConfig {
⋮----
vec![system_instruction::transfer(&from_pubkey, to, lamports)]
⋮----
Some(&fee_payer.pubkey()),
⋮----
&nonce_authority.pubkey(),
⋮----
Message::new(&ixs, Some(&fee_payer.pubkey()))
⋮----
let (message, _) = resolve_spend_tx_and_check_account_balances(
⋮----
&fee_payer.pubkey(),
⋮----
tx.try_partial_sign(&config.signers, recent_blockhash)?;
return_signers_with_config(
⋮----
check_nonce_account(&nonce_account, &nonce_authority.pubkey(), &recent_blockhash)?;
⋮----
tx.try_sign(&config.signers, recent_blockhash)?;
⋮----
.send_transaction_with_config(&tx, config.send_transaction_config)
⋮----
.send_and_confirm_transaction_with_spinner_and_config(
⋮----
pub fn process_sign_offchain_message(
⋮----
Ok(message.sign(config.signers[0])?.to_string())
⋮----
pub fn process_verify_offchain_signature(
⋮----
config.signers[0].pubkey()
⋮----
if message.verify(&signer, signature)? {
Ok("Signature is valid".to_string())
⋮----
Err(CliError::InvalidSignature.into())

================
File: cli/tests/fixtures/build.sh
================
set -ex
cd "$(dirname "$0")"
make -C ../../../programs/sbf/c/
cp ../../../programs/sbf/c/out/noop.so .
cat noop.so noop.so noop.so > noop_large.so
cp ../../../programs/sbf/c/out/alt_bn128.so .

================
File: cli/tests/address_lookup_table.rs
================
async fn test_cli_create_extend_and_freeze_address_lookup_table() {
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
Some(faucet_addr),
⋮----
config.json_rpc_url = test_validator.rpc_url();
config.signers = vec![&keypair];
⋮----
process_command(&config).await.unwrap();
⋮----
authority_pubkey: keypair.pubkey(),
⋮----
serde_json::from_str(&process_command(&config).await.unwrap()).unwrap();
let lookup_table_pubkey = Pubkey::from_str(&response.lookup_table_address).unwrap();
⋮----
assert_eq!(
⋮----
let new_addresses: Vec<Pubkey> = (0..5).map(|_| Pubkey::new_unique()).collect();
⋮----
new_addresses: new_addresses.clone(),
⋮----
} = serde_json::from_str(&process_command(&config).await.unwrap()).unwrap();
⋮----
assert!(last_extended_slot > 0);
⋮----
let process_err = process_command(&config).await.unwrap_err();
assert_eq!(process_err.to_string(), FREEZE_LOOKUP_TABLE_WARNING);
⋮----
assert!(authority.is_none());
⋮----
async fn test_cli_create_and_deactivate_address_lookup_table() {
⋮----
assert_eq!(process_err.to_string(), DEACTIVATE_LOOKUP_TABLE_WARNING);
⋮----
assert_ne!(deactivation_slot, u64::MAX);

================
File: cli/tests/cluster_query.rs
================
async fn test_ping(use_tpu_client: bool, compute_unit_price: Option<u64>) {
⋮----
let fee = FeeStructure::default().get_max_fee(1, 0);
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
Some(faucet_addr),
⋮----
RpcClient::new_with_commitment(test_validator.rpc_url(), CommitmentConfig::processed());
⋮----
let signer_pubkey = default_signer.pubkey();
⋮----
config.json_rpc_url = test_validator.rpc_url();
config.websocket_url = test_validator.rpc_pubsub_url();
config.signers = vec![&default_signer];
⋮----
request_and_confirm_airdrop(&rpc_client, &config, &signer_pubkey, LAMPORTS_PER_SOL)
⋮----
.unwrap();
check_balance!(LAMPORTS_PER_SOL, &rpc_client, &signer_pubkey);
check_ready(&rpc_client).await;
⋮----
count: Some(count),
⋮----
process_command(&config).await.unwrap();

================
File: cli/tests/nonce.rs
================
async fn test_nonce(
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
Some(faucet_addr),
⋮----
RpcClient::new_with_commitment(test_validator.rpc_url(), CommitmentConfig::processed());
let json_rpc_url = test_validator.rpc_url();
⋮----
config_payer.json_rpc_url.clone_from(&json_rpc_url);
⋮----
config_payer.signers = vec![&payer];
request_and_confirm_airdrop(
⋮----
&config_payer.signers[0].pubkey(),
⋮----
.unwrap();
check_balance!(
⋮----
let nonce_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
config_nonce.signers = vec![&nonce_keypair];
let nonce_account = if let Some(seed) = seed.as_ref() {
⋮----
&config_nonce.signers[0].pubkey(),
⋮----
.unwrap()
⋮----
nonce_keypair.pubkey()
⋮----
Some(nonce_authority.pubkey())
⋮----
config_payer.signers.push(&nonce_keypair);
⋮----
process_command(&config_payer).await.unwrap();
⋮----
check_balance!(1000 * LAMPORTS_PER_SOL, &rpc_client, &nonce_account);
config_payer.signers.pop();
⋮----
let first_nonce_string = process_command(&config_payer).await.unwrap();
let first_nonce = first_nonce_string.parse::<Hash>().unwrap();
⋮----
let second_nonce_string = process_command(&config_payer).await.unwrap();
let second_nonce = second_nonce_string.parse::<Hash>().unwrap();
assert_eq!(first_nonce, second_nonce);
let mut authorized_signers: Vec<&dyn Signer> = vec![&payer];
⋮----
authorized_signers.push(&nonce_authority);
⋮----
config_payer.signers.clone_from(&authorized_signers);
⋮----
let third_nonce_string = process_command(&config_payer).await.unwrap();
let third_nonce = third_nonce_string.parse::<Hash>().unwrap();
assert_ne!(first_nonce, third_nonce);
⋮----
check_balance!(900 * LAMPORTS_PER_SOL, &rpc_client, &nonce_account);
check_balance!(100 * LAMPORTS_PER_SOL, &rpc_client, &payee_pubkey);
⋮----
new_authority: new_authority.pubkey(),
⋮----
process_command(&config_payer).await.unwrap_err();
config_payer.signers = vec![&payer, &new_authority];
⋮----
check_balance!(800 * LAMPORTS_PER_SOL, &rpc_client, &nonce_account);
check_balance!(200 * LAMPORTS_PER_SOL, &rpc_client, &payee_pubkey);
⋮----
async fn test_create_account_with_seed() {
⋮----
let offline_nonce_authority_signer = keypair_from_seed(&[1u8; 32]).unwrap();
let online_nonce_creator_signer = keypair_from_seed(&[2u8; 32]).unwrap();
⋮----
&offline_nonce_authority_signer.pubkey(),
⋮----
&online_nonce_creator_signer.pubkey(),
⋮----
check_balance!(0, &rpc_client, &to_address);
check_ready(&rpc_client).await;
let creator_pubkey = online_nonce_creator_signer.pubkey();
let authority_pubkey = offline_nonce_authority_signer.pubkey();
let seed = authority_pubkey.to_string()[0..32].to_string();
⋮----
Pubkey::create_with_seed(&creator_pubkey, &seed, &system_program::id()).unwrap();
check_balance!(0, &rpc_client, &nonce_address);
⋮----
creator_config.json_rpc_url = test_validator.rpc_url();
creator_config.signers = vec![&online_nonce_creator_signer];
⋮----
seed: Some(seed),
nonce_authority: Some(authority_pubkey),
⋮----
process_command(&creator_config).await.unwrap();
check_balance!(241 * LAMPORTS_PER_SOL, &rpc_client, &nonce_address);
⋮----
.and_then(|ref a| solana_rpc_client_nonce_utils::data_from_account(a))
⋮----
.blockhash();
⋮----
authority_config.signers = vec![&offline_nonce_authority_signer];
⋮----
process_command(&authority_config).await.unwrap_err();
⋮----
nonce_account: Some(nonce_address),
⋮----
let sign_only_reply = process_command(&authority_config).await.unwrap();
let sign_only = parse_sign_only_reply_string(&sign_only_reply);
let authority_presigner = sign_only.presigner_of(&authority_pubkey).unwrap();
assert_eq!(sign_only.blockhash, nonce_hash);
⋮----
submit_config.json_rpc_url = test_validator.rpc_url();
submit_config.signers = vec![&authority_presigner];
⋮----
process_command(&submit_config).await.unwrap();
⋮----
check_balance!(10 * LAMPORTS_PER_SOL, &rpc_client, &to_address);

================
File: cli/tests/program.rs
================
fn test_validator_genesis(mint_keypair: &Keypair) -> TestValidatorGenesis {
⋮----
.fee_rate_governor(FeeRateGovernor::new(0, 0))
.rent(Rent {
⋮----
.faucet_addr(Some(run_local_faucet_with_unique_port_for_tests(
mint_keypair.insecure_clone(),
⋮----
fn setup_rpc_client(config: &mut CliConfig) -> Arc<RpcClient> {
⋮----
config.json_rpc_url.to_string(),
⋮----
config.rpc_client = Some(rpc_client.clone());
⋮----
async fn expect_command_failure(
⋮----
let error_actual = process_command(config)
⋮----
.expect_err(should_fail_because);
let error_actual = error_actual.to_string();
assert!(
⋮----
async fn expect_account_absent(rpc_client: &RpcClient, pubkey: Pubkey, absent_because: &str) {
⋮----
.get_account(&pubkey)
⋮----
.expect_err(absent_because);
⋮----
async fn test_cli_program_deploy_non_upgradeable() {
⋮----
let mut noop_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
noop_path.push("tests");
noop_path.push("fixtures");
noop_path.push("noop");
noop_path.set_extension("so");
⋮----
let test_validator = test_validator_genesis(&mint_keypair)
.start_async_with_mint_address(&mint_keypair, SocketAddrSpace::Unspecified)
⋮----
.expect("validator start failed");
⋮----
config.json_rpc_url = test_validator.rpc_url();
let rpc_client = setup_rpc_client(&mut config);
let mut file = File::open(noop_path.to_str().unwrap()).unwrap();
⋮----
file.read_to_end(&mut program_data).unwrap();
⋮----
.get_minimum_balance_for_rent_exemption(UpgradeableLoaderState::size_of_programdata(
program_data.len(),
⋮----
.unwrap();
⋮----
.get_minimum_balance_for_rent_exemption(UpgradeableLoaderState::size_of_program())
⋮----
config.signers = vec![&keypair];
⋮----
lamports: 4 * minimum_balance_for_programdata, // min balance for rent exemption for three programs + leftover for tx processing
⋮----
process_command(&config).await.unwrap();
⋮----
program_location: Some(noop_path.to_str().unwrap().to_string()),
⋮----
let response = process_command(&config).await;
let json: Value = serde_json::from_str(&response.unwrap()).unwrap();
⋮----
.as_object()
.unwrap()
.get("programId")
⋮----
.as_str()
⋮----
let program_id = Pubkey::from_str(program_id_str).unwrap();
let account0 = rpc_client.get_account(&program_id).await.unwrap();
assert_eq!(account0.lamports, minimum_balance_for_program);
assert_eq!(account0.owner, bpf_loader_upgradeable::id());
assert!(account0.executable);
⋮----
Pubkey::find_program_address(&[program_id.as_ref()], &bpf_loader_upgradeable::id());
let programdata_account = rpc_client.get_account(&programdata_pubkey).await.unwrap();
assert_eq!(
⋮----
assert_eq!(programdata_account.owner, bpf_loader_upgradeable::id());
assert!(!programdata_account.executable);
⋮----
// Test custom address
⋮----
config.signers = vec![&keypair, &custom_address_keypair];
⋮----
program_signer_index: Some(1),
⋮----
.get_account(&custom_address_keypair.pubkey())
⋮----
assert_eq!(account1.lamports, minimum_balance_for_program);
assert_eq!(account1.owner, bpf_loader_upgradeable::id());
assert!(account1.executable);
⋮----
&[custom_address_keypair.pubkey().as_ref()],
⋮----
expect_command_failure(
⋮----
&format!(
⋮----
// Attempt to deploy to account with excess balance
⋮----
config.signers = vec![&custom_address_keypair];
⋮----
// Anything over minimum_balance_for_programdata should trigger an error.
⋮----
// Use forcing parameter to deploy to account with excess balance
⋮----
async fn test_cli_program_deploy_no_authority() {
⋮----
let max_len = program_data.len();
⋮----
// Deploy a program
config.signers = vec![&keypair, &upgrade_authority];
⋮----
// Attempt to upgrade the program
⋮----
program_pubkey: Some(program_id),
⋮----
&format!("Program {program_id} is no longer upgradeable"),
⋮----
async fn test_cli_program_deploy_feature(enable_feature: bool, skip_preflight: bool) {
⋮----
let mut program_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
program_path.push("tests");
program_path.push("fixtures");
program_path.push("alt_bn128");
program_path.set_extension("so");
⋮----
let mut test_validator_builder = test_validator_genesis(&mint_keypair);
⋮----
test_validator_builder.deactivate_features(&[enable_alt_bn128_syscall::id()]);
⋮----
let mut file = File::open(program_path.to_str().unwrap()).unwrap();
⋮----
program_location: Some(program_path.to_str().unwrap().to_string()),
⋮----
let res = process_command(&config).await;
assert!(res.is_ok());
⋮----
assert!(response
⋮----
async fn test_cli_program_upgrade_with_feature(enable_feature: bool) {
⋮----
let mut syscall_program_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
syscall_program_path.push("tests");
syscall_program_path.push("fixtures");
syscall_program_path.push("alt_bn128");
syscall_program_path.set_extension("so");
⋮----
let blockhash = rpc_client.get_latest_blockhash().await.unwrap();
let mut file = File::open(syscall_program_path.to_str().unwrap()).unwrap();
⋮----
file.read_to_end(&mut large_program_data).unwrap();
let max_program_data_len = large_program_data.len();
⋮----
config.signers = vec![&online_signer];
⋮----
config.signers = vec![&offline_signer];
⋮----
config.signers = vec![&online_signer, &offline_signer, &program_signer];
⋮----
program_signer_index: Some(2),
program_pubkey: Some(program_signer.pubkey()),
⋮----
max_len: Some(max_program_data_len),
⋮----
create_buffer_with_offline_authority(
⋮----
program_pubkey: program_signer.pubkey(),
buffer_pubkey: buffer_signer.pubkey(),
⋮----
blockhash_query: BlockhashQuery::new(Some(blockhash), true, None),
⋮----
let sig_response = process_command(&config).await.unwrap();
let sign_only = parse_sign_only_reply_string(&sig_response);
let offline_pre_signer = sign_only.presigner_of(&offline_signer.pubkey()).unwrap();
config.signers = vec![&offline_pre_signer, &program_signer];
⋮----
format!(
⋮----
.as_str(),
⋮----
async fn test_cli_program_deploy_with_authority() {
⋮----
config.signers = vec![&keypair, &upgrade_authority, &program_keypair];
⋮----
program_pubkey: Some(program_keypair.pubkey()),
⋮----
max_len: Some(max_len),
⋮----
.get_account(&program_keypair.pubkey())
⋮----
assert_eq!(program_account.lamports, minimum_balance_for_program);
assert_eq!(program_account.owner, bpf_loader_upgradeable::id());
assert!(program_account.executable);
⋮----
&[program_keypair.pubkey().as_ref()],
⋮----
let program_pubkey = Pubkey::from_str(program_pubkey_str).unwrap();
let program_account = rpc_client.get_account(&program_pubkey).await.unwrap();
⋮----
Pubkey::find_program_address(&[program_pubkey.as_ref()], &bpf_loader_upgradeable::id());
⋮----
program_pubkey: Some(program_pubkey),
⋮----
config.signers = vec![&keypair, &upgrade_authority, &new_upgrade_authority];
⋮----
.presigner_of(&new_upgrade_authority.pubkey())
⋮----
config.signers = vec![&keypair, &upgrade_authority, &offline_pre_signer];
⋮----
blockhash_query: BlockhashQuery::new(Some(blockhash), false, None),
⋮----
.get("authority")
⋮----
config.signers = vec![&keypair, &new_upgrade_authority];
⋮----
account_pubkey: Some(program_pubkey),
authority_pubkey: keypair.pubkey(),
⋮----
upgrade_authority_index: Some(1),
⋮----
assert_eq!(new_upgrade_authority_str, "none");
⋮----
&format!("Program {program_pubkey} is no longer upgradeable"),
⋮----
} = programdata_account.state().unwrap()
⋮----
assert_eq!(upgrade_authority_address, None);
⋮----
panic!("not a ProgramData account");
⋮----
assert_eq!("none", authority_pubkey_str);
⋮----
async fn test_cli_program_upgrade_auto_extend(skip_preflight: bool) {
⋮----
let mut noop_large_path = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
noop_large_path.push("tests");
noop_large_path.push("fixtures");
noop_large_path.push("noop_large");
noop_large_path.set_extension("so");
⋮----
let mut file = File::open(noop_large_path.to_str().unwrap()).unwrap();
⋮----
file.read_to_end(&mut program_data_large).unwrap();
let max_len = program_data_large.len();
⋮----
program_location: Some(noop_large_path.to_str().unwrap().to_string()),
⋮----
async fn test_cli_program_close_program() {
⋮----
wait_n_slots(&rpc_client, 1).await;
let close_account = rpc_client.get_account(&programdata_pubkey).await.unwrap();
⋮----
account_pubkey: Some(program_keypair.pubkey()),
⋮----
expect_account_absent(
⋮----
let recipient_account = rpc_client.get_account(&recipient_pubkey).await.unwrap();
assert_eq!(programdata_lamports, recipient_account.lamports);
⋮----
async fn test_cli_program_extend_program() {
⋮----
preflight_commitment: Some(CommitmentConfig::processed().commitment),
⋮----
assert_eq!(expected_len, programdata_account.data.len());
⋮----
file.read_to_end(&mut new_program_data).unwrap();
let new_max_len = new_program_data.len();
⋮----
program_pubkey: program_keypair.pubkey(),
⋮----
async fn test_cli_program_migrate_program() {
⋮----
compute_unit_price: Some(1),
⋮----
async fn test_cli_program_write_buffer() {
⋮----
program_location: noop_path.to_str().unwrap().to_string(),
⋮----
.get("buffer")
⋮----
let new_buffer_pubkey = Pubkey::from_str(buffer_pubkey_str).unwrap();
let buffer_account = rpc_client.get_account(&new_buffer_pubkey).await.unwrap();
assert_eq!(buffer_account.lamports, minimum_balance_for_buffer_default);
assert_eq!(buffer_account.owner, bpf_loader_upgradeable::id());
if let UpgradeableLoaderState::Buffer { authority_address } = buffer_account.state().unwrap() {
assert_eq!(authority_address, Some(keypair.pubkey()));
⋮----
panic!("not a buffer account");
⋮----
config.signers = vec![&keypair, &buffer_keypair];
⋮----
buffer_signer_index: Some(1),
buffer_pubkey: Some(buffer_keypair.pubkey()),
⋮----
.get_account(&buffer_keypair.pubkey())
⋮----
assert_eq!(buffer_account.lamports, minimum_balance_for_buffer);
⋮----
config.signers = vec![];
⋮----
account_pubkey: Some(buffer_keypair.pubkey()),
⋮----
config.signers = vec![&keypair, &buffer_keypair, &authority_keypair];
⋮----
assert_eq!(authority_address, Some(authority_keypair.pubkey()));
⋮----
let buffer_pubkey = Pubkey::from_str(buffer_pubkey_str).unwrap();
let buffer_account = rpc_client.get_account(&buffer_pubkey).await.unwrap();
⋮----
account_pubkey: Some(buffer_pubkey),
⋮----
let close_account = rpc_client.get_account(&buffer_pubkey).await.unwrap();
assert_eq!(minimum_balance_for_buffer, close_account.lamports);
⋮----
config.signers = vec![&keypair, &authority_keypair];
⋮----
assert_eq!(minimum_balance_for_buffer, recipient_account.lamports);
⋮----
.get_account(&keypair.pubkey())
⋮----
account_pubkey: Some(new_buffer_pubkey),
recipient_pubkey: keypair.pubkey(),
⋮----
let recipient_account = rpc_client.get_account(&keypair.pubkey()).await.unwrap();
⋮----
let program_data_len = file.seek(SeekFrom::End(0)).unwrap() as usize;
⋮----
let large_program_data_len = file.seek(SeekFrom::End(0)).unwrap() as usize;
⋮----
async fn test_cli_program_write_buffer_feature(enable_feature: bool) {
⋮----
program_location: program_path.to_str().unwrap().to_string(),
⋮----
assert!(response.is_ok());
⋮----
async fn test_cli_program_set_buffer_authority() {
⋮----
buffer_pubkey: buffer_keypair.pubkey(),
buffer_authority_index: Some(0),
new_buffer_authority: new_buffer_authority.pubkey(),
⋮----
assert_eq!(authority_address, Some(new_buffer_authority.pubkey()));
⋮----
config.signers = vec![&keypair, &new_buffer_authority];
⋮----
buffer_authority_index: Some(1),
new_buffer_authority: buffer_keypair.pubkey(),
⋮----
assert_eq!(authority_address, Some(buffer_keypair.pubkey()));
⋮----
async fn test_cli_program_mismatch_buffer_authority() {
⋮----
config.signers = vec![&keypair, &buffer_keypair, &buffer_authority];
⋮----
assert_eq!(authority_address, Some(buffer_authority.pubkey()));
⋮----
config.signers = vec![&keypair, &buffer_authority];
⋮----
async fn test_cli_program_deploy_with_offline_signing(use_offline_signer_as_fee_payer: bool) {
⋮----
let online_signer_identity = NullSigner::new(&online_signer.pubkey());
⋮----
config.signers.push(&online_signer_identity);
⋮----
buffer_pubkey: program_signer.pubkey(),
⋮----
config.signers.push(&online_signer);
⋮----
&[program_signer.pubkey().as_ref()],
⋮----
async fn test_cli_program_show() {
⋮----
.get("address")
⋮----
.get("dataLen")
⋮----
.as_u64()
⋮----
assert_eq!(max_len, data_len as usize);
⋮----
config.signers = vec![&keypair, &authority_keypair, &program_keypair];
⋮----
let min_slot = rpc_client.get_slot().await.unwrap();
⋮----
let max_slot = rpc_client.get_slot().await.unwrap();
⋮----
.get("programdataAddress")
⋮----
.get("lastDeploySlot")
⋮----
assert!(deployed_slot >= min_slot);
assert!(deployed_slot <= max_slot);
⋮----
async fn test_cli_program_dump() {
⋮----
let current_exe = env::current_exe().unwrap();
PathBuf::from(current_exe.parent().unwrap().parent().unwrap())
⋮----
out_file.set_file_name("out.txt");
⋮----
output_location: out_file.clone().into_os_string().into_string().unwrap(),
⋮----
let mut file = File::open(out_file).unwrap();
⋮----
file.read_to_end(&mut out_data).unwrap();
assert_eq!(program_data.len(), out_data.len());
for i in 0..program_data.len() {
assert_eq!(program_data[i], out_data[i]);
⋮----
async fn create_buffer_with_offline_authority<'a>(
⋮----
config.signers = vec![online_signer, buffer_signer];
⋮----
buffer_pubkey: Some(buffer_signer.pubkey()),
⋮----
process_command(config).await.unwrap();
⋮----
.get_account(&buffer_signer.pubkey())
⋮----
assert_eq!(authority_address, Some(online_signer.pubkey()));
⋮----
config.signers = vec![online_signer];
⋮----
new_buffer_authority: offline_signer.pubkey(),
⋮----
assert_eq!(authority_address, Some(offline_signer.pubkey()));
⋮----
async fn test_cli_program_deploy_with_args(compute_unit_price: Option<u64>, use_rpc: bool) {
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
.rpc_config(JsonRpcConfig {
⋮----
faucet_addr: Some(faucet_addr),
⋮----
.get_signatures_for_address_with_config(
&keypair.pubkey(),
⋮----
commitment: Some(CommitmentConfig::confirmed()),
⋮----
.into_iter()
.rev()
.map(|status| Signature::from_str(&status.signature).unwrap())
.collect();
async fn fetch_and_decode_transaction(
⋮----
.get_transaction_with_config(
⋮----
encoding: Some(UiTransactionEncoding::Base64),
⋮----
.decode()
⋮----
.into_legacy_transaction()
⋮----
assert!(signatures.len() >= 4);
let initial_tx = fetch_and_decode_transaction(&rpc_client, &signatures[1]).await;
let write_tx = fetch_and_decode_transaction(&rpc_client, &signatures[2]).await;
let final_tx = fetch_and_decode_transaction(&rpc_client, signatures.last().unwrap()).await;
⋮----
let ix_len = tx.message.instructions.len();
⋮----
assert_matches!(
⋮----
async fn test_cli_program_v4() {
⋮----
config.signers = vec![
⋮----
pubkey: Some(program_keypair.pubkey()),
⋮----
program_address: program_keypair.pubkey(),
⋮----
upload_signer_index: Some(2),
⋮----
path_to_elf: Some(noop_path.to_str().unwrap().to_string()),
⋮----
assert!(process_command(&config).await.is_ok());
⋮----
assert_eq!(program_account.owner, loader_v4::id());
⋮----
buffer_address: Some(buffer_keypair.pubkey()),
upload_signer_index: Some(3),
⋮----
.unwrap_err();
⋮----
program_address: buffer_keypair.pubkey(),
⋮----
assert_eq!(buffer_account.owner, loader_v4::id());
assert!(buffer_account.executable);

================
File: cli/tests/request_airdrop.rs
================
async fn test_cli_request_airdrop() {
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
Some(faucet_addr),
⋮----
bob_config.json_rpc_url = test_validator.rpc_url();
⋮----
bob_config.signers = vec![&keypair];
let sig_response = process_command(&bob_config).await;
sig_response.unwrap();
⋮----
RpcClient::new_with_commitment(test_validator.rpc_url(), CommitmentConfig::processed());
⋮----
.get_balance(&bob_config.signers[0].pubkey())
⋮----
.unwrap();
assert_eq!(balance, 50 * LAMPORTS_PER_SOL);

================
File: cli/tests/stake.rs
================
async fn test_stake_delegation_force() {
⋮----
let authorized_withdrawer = Keypair::new().pubkey();
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
.fee_rate_governor(FeeRateGovernor::new(0, 0))
.rent(Rent {
⋮----
.epoch_schedule(EpochSchedule::custom(
⋮----
.faucet_addr(Some(faucet_addr))
.warp_slot(DELINQUENT_VALIDATOR_SLOT_DISTANCE * 2)
.start_async_with_mint_address(&mint_keypair, SocketAddrSpace::Unspecified)
⋮----
.expect("validator start failed");
⋮----
RpcClient::new_with_commitment(test_validator.rpc_url(), CommitmentConfig::processed());
⋮----
config.json_rpc_url = test_validator.rpc_url();
config.signers = vec![&default_signer];
request_and_confirm_airdrop(
⋮----
&config.signers[0].pubkey(),
⋮----
.unwrap();
⋮----
config.signers = vec![&default_signer, &vote_keypair];
⋮----
process_command(&config).await.unwrap();
⋮----
config.signers = vec![&default_signer, &stake_keypair];
⋮----
stake_account_pubkey: stake_keypair.pubkey(),
vote_account_pubkey: vote_keypair.pubkey(),
⋮----
config.signers = vec![&default_signer, &stake_keypair2];
⋮----
wait_for_next_epoch_plus_n_slots(&rpc_client, 1).await;
⋮----
stake_account_pubkey: stake_keypair2.pubkey(),
⋮----
process_command(&config).await.unwrap_err();
⋮----
async fn test_seed_stake_delegation_and_deactivation(compute_unit_price: Option<u64>) {
⋮----
Some(faucet_addr),
⋮----
let validator_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
⋮----
config_validator.json_rpc_url = test_validator.rpc_url();
config_validator.signers = vec![&validator_keypair];
⋮----
&config_validator.signers[0].pubkey(),
⋮----
check_balance!(
⋮----
.expect("bad seed");
⋮----
seed: Some("hi there".to_string()),
⋮----
process_command(&config_validator).await.unwrap();
⋮----
vote_account_pubkey: test_validator.vote_account_address(),
⋮----
async fn test_stake_delegation_and_withdraw_available() {
⋮----
let stake_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
⋮----
config_validator.signers.push(&stake_keypair);
⋮----
config_validator.signers.pop();
⋮----
check_balance!(0, &rpc_client, &recipient_pubkey);
⋮----
&stake_keypair.pubkey(),
⋮----
check_balance!(55 * LAMPORTS_PER_SOL, &rpc_client, &stake_keypair.pubkey());
⋮----
check_balance!(5 * LAMPORTS_PER_SOL, &rpc_client, &recipient_pubkey);
⋮----
check_balance!(55 * LAMPORTS_PER_SOL, &rpc_client, &recipient_pubkey);
⋮----
async fn test_stake_delegation_and_withdraw_all() {
⋮----
process_command(&config_validator).await.unwrap_err();
⋮----
async fn test_stake_delegation_and_deactivation(compute_unit_price: Option<u64>) {
⋮----
async fn test_offline_stake_delegation_and_deactivation(compute_unit_price: Option<u64>) {
⋮----
config_payer.json_rpc_url = test_validator.rpc_url();
⋮----
config_offline.signers = vec![&offline_keypair];
process_command(&config_offline).await.unwrap_err();
⋮----
&config_offline.signers[0].pubkey(),
⋮----
staker: Some(config_offline.signers[0].pubkey()),
⋮----
let blockhash = rpc_client.get_latest_blockhash().await.unwrap();
⋮----
let sig_response = process_command(&config_offline).await.unwrap();
let sign_only = parse_sign_only_reply_string(&sig_response);
assert!(sign_only.has_all_signers());
⋮----
.presigner_of(&config_offline.signers[0].pubkey())
⋮----
config_payer.signers = vec![&offline_presigner];
⋮----
process_command(&config_payer).await.unwrap();
⋮----
async fn test_nonced_stake_delegation_and_deactivation(compute_unit_price: Option<u64>) {
⋮----
let config_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
⋮----
config.signers = vec![&config_keypair];
⋮----
.get_minimum_balance_for_rent_exemption(NonceState::size())
⋮----
config.signers.push(&stake_keypair);
⋮----
nonce_authority: Some(config.signers[0].pubkey()),
⋮----
&nonce_account.pubkey(),
⋮----
.and_then(|ref a| solana_rpc_client_nonce_utils::data_from_account(a))
.unwrap()
.blockhash();
⋮----
Source::NonceAccount(nonce_account.pubkey()),
⋮----
nonce_account: Some(nonce_account.pubkey()),
⋮----
async fn test_stake_authorize(compute_unit_price: Option<u64>) {
⋮----
let offline_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
⋮----
let offline_authority_pubkey = config_offline.signers[0].pubkey();
⋮----
let stake_account_pubkey = stake_keypair.pubkey();
⋮----
let online_authority_pubkey = online_authority.pubkey();
config.signers.pop();
⋮----
new_authorizations: vec![StakeAuthorizationIndexed {
⋮----
let stake_account = rpc_client.get_account(&stake_account_pubkey).await.unwrap();
let stake_state: StakeStateV2 = stake_account.state().unwrap();
⋮----
_ => panic!("Unexpected stake state!"),
⋮----
assert_eq!(current_authority, online_authority_pubkey);
⋮----
let online_authority2_pubkey = online_authority2.pubkey();
⋮----
let withdraw_authority_pubkey = withdraw_authority.pubkey();
config.signers.push(&online_authority);
⋮----
new_authorizations: vec![
⋮----
assert_eq!(current_staker, online_authority2_pubkey);
assert_eq!(current_withdrawer, withdraw_authority_pubkey);
⋮----
config.signers.push(&online_authority2);
⋮----
assert_eq!(current_authority, offline_authority_pubkey);
⋮----
let nonced_authority_pubkey = nonced_authority.pubkey();
⋮----
let sign_reply = process_command(&config_offline).await.unwrap();
let sign_only = parse_sign_only_reply_string(&sign_reply);
⋮----
let offline_presigner = sign_only.presigner_of(&offline_authority_pubkey).unwrap();
config.signers = vec![&offline_presigner];
⋮----
assert_eq!(current_authority, nonced_authority_pubkey);
⋮----
config.signers = vec![&default_signer, &nonce_account];
⋮----
nonce_authority: Some(offline_authority_pubkey),
⋮----
config_offline.signers.push(&nonced_authority);
⋮----
assert_eq!(sign_only.blockhash, nonce_hash);
⋮----
let nonced_authority_presigner = sign_only.presigner_of(&nonced_authority_pubkey).unwrap();
config.signers = vec![&offline_presigner, &nonced_authority_presigner];
⋮----
assert_ne!(nonce_hash, new_nonce_hash);
⋮----
async fn test_stake_authorize_with_fee_payer() {
⋮----
let fee_one_sig = FeeStructure::default().get_max_fee(1, 0);
let fee_two_sig = FeeStructure::default().get_max_fee(2, 0);
⋮----
let default_pubkey = default_signer.pubkey();
⋮----
let payer_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
⋮----
config_payer.signers = vec![&payer_keypair];
⋮----
let payer_pubkey = config_payer.signers[0].pubkey();
⋮----
config_offline.signers = vec![&offline_signer];
⋮----
let offline_pubkey = config_offline.signers[0].pubkey();
⋮----
request_and_confirm_airdrop(&rpc_client, &config, &default_pubkey, 5_000_000_000_000)
⋮----
check_balance!(5_000_000_000_000, &rpc_client, &config.signers[0].pubkey());
request_and_confirm_airdrop(&rpc_client, &config_payer, &payer_pubkey, 5_000_000_000_000)
⋮----
check_balance!(5_000_000_000_000, &rpc_client, &payer_pubkey);
⋮----
check_balance!(5_000_000_000_000, &rpc_client, &offline_pubkey);
check_ready(&rpc_client).await;
⋮----
config.signers = vec![&default_signer, &payer_keypair];
⋮----
check_balance!(5_000_000_000_000 - fee_two_sig, &rpc_client, &payer_pubkey);
⋮----
let offline_presigner = sign_only.presigner_of(&offline_pubkey).unwrap();
⋮----
async fn test_stake_split(compute_unit_price: Option<u64>) {
⋮----
.get_minimum_balance_for_rent_exemption(StakeStateV2::size_of())
⋮----
check_balance!(50_000_000_000_000, &rpc_client, &config.signers[0].pubkey());
⋮----
check_balance!(1_000_000_000_000, &rpc_client, &offline_pubkey);
⋮----
staker: Some(offline_pubkey),
withdrawer: Some(offline_pubkey),
⋮----
check_balance!(10 * stake_balance, &rpc_client, &stake_account_pubkey,);
⋮----
let nonce_account = keypair_from_seed(&[1u8; 32]).unwrap();
⋮----
nonce_authority: Some(offline_pubkey),
⋮----
check_balance!(minimum_nonce_balance, &rpc_client, &nonce_account.pubkey());
⋮----
let split_account = keypair_from_seed(&[2u8; 32]).unwrap();
check_balance!(0, &rpc_client, &split_account.pubkey());
config_offline.signers.push(&split_account);
⋮----
rent_exempt_reserve: Some(minimum_balance),
⋮----
config.signers = vec![&offline_presigner, &split_account];
⋮----
check_balance!(8 * stake_balance, &rpc_client, &stake_account_pubkey);
⋮----
async fn test_stake_set_lockup(compute_unit_price: Option<u64>) {
⋮----
custodian: config.signers[0].pubkey(),
⋮----
withdrawer: Some(config.signers[0].pubkey()),
⋮----
unix_timestamp: Some(1_581_534_570),
epoch: Some(200),
⋮----
assert_eq!(
⋮----
assert_eq!(current_lockup.epoch, lockup.epoch.unwrap());
assert_eq!(current_lockup.custodian, config.signers[0].pubkey());
⋮----
let online_custodian_pubkey = online_custodian.pubkey();
⋮----
unix_timestamp: Some(1_581_534_571),
epoch: Some(201),
custodian: Some(online_custodian_pubkey),
⋮----
unix_timestamp: Some(1_581_534_572),
epoch: Some(202),
⋮----
config.signers = vec![&default_signer, &online_custodian];
⋮----
assert_eq!(current_lockup.custodian, online_custodian_pubkey);
⋮----
unix_timestamp: Some(1_581_534_573),
epoch: Some(203),
custodian: Some(offline_pubkey),
⋮----
let nonce_account_pubkey = nonce_account.pubkey();
⋮----
check_balance!(minimum_nonce_balance, &rpc_client, &nonce_account_pubkey);
⋮----
unix_timestamp: Some(1_581_534_576),
epoch: Some(222),
⋮----
nonce_account: Some(nonce_account_pubkey),
⋮----
assert_eq!(current_lockup.custodian, offline_pubkey);
⋮----
async fn test_offline_nonced_create_stake_account_and_withdraw(compute_unit_price: Option<u64>) {
⋮----
let default_signer = keypair_from_seed(&[1u8; 32]).unwrap();
⋮----
let offline_signer = keypair_from_seed(&[2u8; 32]).unwrap();
⋮----
check_balance!(200_000_000_000, &rpc_client, &config.signers[0].pubkey());
⋮----
check_balance!(100_000_000_000, &rpc_client, &offline_pubkey);
⋮----
let nonce_account = keypair_from_seed(&[3u8; 32]).unwrap();
let nonce_pubkey = nonce_account.pubkey();
config.signers.push(&nonce_account);
⋮----
let stake_keypair = keypair_from_seed(&[4u8; 32]).unwrap();
let stake_pubkey = stake_keypair.pubkey();
config_offline.signers.push(&stake_keypair);
⋮----
nonce_account: Some(nonce_pubkey),
⋮----
let stake_presigner = sign_only.presigner_of(&stake_pubkey).unwrap();
config.signers = vec![&offline_presigner, &stake_presigner];
⋮----
check_balance!(50_000_000_000, &rpc_client, &stake_pubkey);
⋮----
let recipient = keypair_from_seed(&[5u8; 32]).unwrap();
let recipient_pubkey = recipient.pubkey();
config_offline.signers.pop();
⋮----
check_balance!(50_000_000_000, &rpc_client, &recipient_pubkey);
⋮----
config_offline.signers = vec![&offline_signer, &stake_keypair];
⋮----
seed: Some(seed.to_string()),
⋮----
Pubkey::create_with_seed(&stake_pubkey, seed, &stake::program::id()).unwrap();
check_balance!(50_000_000_000, &rpc_client, &seed_address);
⋮----
async fn test_stake_checked_instructions() {
⋮----
let withdrawer_pubkey = withdrawer_keypair.pubkey();
⋮----
withdrawer: Some(withdrawer_pubkey),
withdrawer_signer: Some(1),
⋮----
config.signers = vec![&default_signer, &stake_keypair, &withdrawer_keypair];
⋮----
let staker_pubkey = staker_keypair.pubkey();
⋮----
config.signers = vec![&default_signer, &staker_keypair];
⋮----
assert_eq!(current_authority, staker_pubkey);
⋮----
let new_withdrawer_pubkey = new_withdrawer_keypair.pubkey();
config.signers = vec![&default_signer, &withdrawer_keypair];
⋮----
config.signers = vec![
⋮----
assert_eq!(current_authority, new_withdrawer_pubkey);
⋮----
let custodian_pubkey = custodian.pubkey();
⋮----
custodian: Some(custodian_pubkey),
⋮----
config.signers = vec![&default_signer, &new_withdrawer_keypair];
⋮----
new_custodian_signer: Some(1),
⋮----
config.signers = vec![&default_signer, &new_withdrawer_keypair, &custodian];
⋮----
new_custodian_signer: Some(2),
⋮----
assert_eq!(current_lockup.custodian, custodian_pubkey);
⋮----
async fn test_stake_minimum_delegation() {
⋮----
let result = process_command(&config).await;
assert_matches!(result, Ok(..));

================
File: cli/tests/transfer.rs
================
async fn test_transfer(skip_preflight: bool) {
⋮----
let fee_one_sig = FeeStructure::default().get_max_fee(1, 0);
let fee_two_sig = FeeStructure::default().get_max_fee(2, 0);
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
Some(faucet_addr),
⋮----
RpcClient::new_with_commitment(test_validator.rpc_url(), CommitmentConfig::processed());
⋮----
config.json_rpc_url = test_validator.rpc_url();
config.signers = vec![&default_signer];
⋮----
let sender_pubkey = config.signers[0].pubkey();
⋮----
request_and_confirm_airdrop(&rpc_client, &config, &sender_pubkey, 5 * LAMPORTS_PER_SOL)
⋮----
.unwrap();
check_balance!(5 * LAMPORTS_PER_SOL, &rpc_client, &sender_pubkey);
check_balance!(0, &rpc_client, &recipient_pubkey);
check_ready(&rpc_client).await;
⋮----
process_command(&config).await.unwrap();
check_balance!(
⋮----
check_balance!(LAMPORTS_PER_SOL, &rpc_client, &recipient_pubkey);
⋮----
assert!(process_command(&config).await.is_err());
⋮----
offline.signers = vec![&default_offline_signer];
⋮----
process_command(&offline).await.unwrap_err();
let offline_pubkey = offline.signers[0].pubkey();
request_and_confirm_airdrop(&rpc_client, &offline, &offline_pubkey, LAMPORTS_PER_SOL)
⋮----
check_balance!(LAMPORTS_PER_SOL, &rpc_client, &offline_pubkey);
let blockhash = rpc_client.get_latest_blockhash().await.unwrap();
⋮----
let sign_only_reply = process_command(&offline).await.unwrap();
let sign_only = parse_sign_only_reply_string(&sign_only_reply);
assert!(sign_only.has_all_signers());
let offline_presigner = sign_only.presigner_of(&offline_pubkey).unwrap();
config.signers = vec![&offline_presigner];
⋮----
check_balance!(1_500_000_000, &rpc_client, &recipient_pubkey);
let nonce_account = keypair_from_seed(&[3u8; 32]).unwrap();
⋮----
.get_minimum_balance_for_rent_exemption(NonceState::size())
⋮----
config.signers = vec![&default_signer, &nonce_account];
⋮----
&nonce_account.pubkey(),
⋮----
.and_then(|ref a| solana_rpc_client_nonce_utils::data_from_account(a))
.unwrap()
.blockhash();
⋮----
Source::NonceAccount(nonce_account.pubkey()),
⋮----
nonce_account: Some(nonce_account.pubkey()),
⋮----
check_balance!(2_500_000_000, &rpc_client, &recipient_pubkey);
⋮----
assert_ne!(nonce_hash, new_nonce_hash);
⋮----
nonce_account: nonce_account.pubkey(),
⋮----
check_balance!(2_900_000_000, &rpc_client, &recipient_pubkey);
⋮----
async fn test_transfer_multisession_signing() {
⋮----
let offline_from_signer = keypair_from_seed(&[2u8; 32]).unwrap();
let offline_fee_payer_signer = keypair_from_seed(&[3u8; 32]).unwrap();
let from_null_signer = NullSigner::new(&offline_from_signer.pubkey());
⋮----
request_and_confirm_airdrop(
⋮----
&offline_from_signer.pubkey(),
⋮----
&offline_fee_payer_signer.pubkey(),
⋮----
check_balance!(0, &rpc_client, &to_pubkey);
⋮----
fee_payer_config.signers = vec![&offline_fee_payer_signer, &from_null_signer];
⋮----
process_command(&fee_payer_config).await.unwrap_err();
⋮----
let sign_only_reply = process_command(&fee_payer_config).await.unwrap();
⋮----
assert!(!sign_only.has_all_signers());
⋮----
.presigner_of(&offline_fee_payer_signer.pubkey())
⋮----
from_config.signers = vec![&fee_payer_presigner, &offline_from_signer];
⋮----
process_command(&from_config).await.unwrap_err();
⋮----
let sign_only_reply = process_command(&from_config).await.unwrap();
⋮----
.presigner_of(&offline_from_signer.pubkey())
⋮----
config.signers = vec![&fee_payer_presigner, &from_presigner];
⋮----
check_balance!(LAMPORTS_PER_SOL, &rpc_client, &offline_from_signer.pubkey(),);
⋮----
check_balance!(42 * LAMPORTS_PER_SOL, &rpc_client, &to_pubkey);
⋮----
async fn test_transfer_all(compute_unit_price: Option<u64>) {
⋮----
let lamports_per_signature = FeeStructure::default().get_max_fee(1, 0);
⋮----
let mut instructions = vec![system_instruction::transfer(
⋮----
instructions.push(ComputeBudgetInstruction::set_compute_unit_limit(450));
instructions.push(ComputeBudgetInstruction::set_compute_unit_price(
⋮----
Message::new_with_blockhash(&instructions, Some(&default_signer.pubkey()), &blockhash);
⋮----
.get_fee_for_message(&sample_message)
⋮----
request_and_confirm_airdrop(&rpc_client, &config, &sender_pubkey, 500_000)
⋮----
check_balance!(500_000, &rpc_client, &sender_pubkey);
⋮----
check_balance!(0, &rpc_client, &sender_pubkey);
check_balance!(500_000 - fee, &rpc_client, &recipient_pubkey);
⋮----
async fn test_transfer_unfunded_recipient() {
⋮----
request_and_confirm_airdrop(&rpc_client, &config, &sender_pubkey, 50_000)
⋮----
check_balance!(50_000, &rpc_client, &sender_pubkey);
⋮----
process_command(&config).await.unwrap_err();
⋮----
async fn test_transfer_with_seed() {
⋮----
let fee = FeeStructure::default().get_max_fee(1, 0);
⋮----
let derived_address_seed = "seed".to_string();
⋮----
request_and_confirm_airdrop(&rpc_client, &config, &sender_pubkey, LAMPORTS_PER_SOL)
⋮----
request_and_confirm_airdrop(&rpc_client, &config, &derived_address, 5 * LAMPORTS_PER_SOL)
⋮----
check_balance!(LAMPORTS_PER_SOL, &rpc_client, &sender_pubkey);
check_balance!(5 * LAMPORTS_PER_SOL, &rpc_client, &derived_address);
⋮----
derived_address_seed: Some(derived_address_seed),
derived_address_program_id: Some(derived_address_program_id),
⋮----
check_balance!(LAMPORTS_PER_SOL - fee, &rpc_client, &sender_pubkey);
check_balance!(5 * LAMPORTS_PER_SOL, &rpc_client, &recipient_pubkey);
check_balance!(0, &rpc_client, &derived_address);

================
File: cli/tests/validator_info.rs
================
async fn test_publish(compute_unit_price: Option<u64>) {
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
Some(faucet_addr),
⋮----
RpcClient::new_with_commitment(test_validator.rpc_url(), CommitmentConfig::processed());
let validator_keypair = keypair_from_seed(&[0u8; 32]).unwrap();
⋮----
config_validator.json_rpc_url = test_validator.rpc_url();
config_validator.signers = vec![&validator_keypair];
request_and_confirm_airdrop(
⋮----
&config_validator.signers[0].pubkey(),
⋮----
.unwrap();
check_balance!(
⋮----
validator_info: json!({ "name": "test" }),
⋮----
process_command(&config_validator).await.unwrap();

================
File: cli/tests/vote.rs
================
async fn test_vote_authorize_and_withdraw(compute_unit_price: Option<u64>) {
⋮----
let faucet_addr = run_local_faucet_with_unique_port_for_tests(mint_keypair.insecure_clone());
⋮----
Some(faucet_addr),
⋮----
RpcClient::new_with_commitment(test_validator.rpc_url(), CommitmentConfig::processed());
⋮----
config.json_rpc_url = test_validator.rpc_url();
config.signers = vec![&default_signer];
request_and_confirm_airdrop(&rpc_client, &config, &config.signers[0].pubkey(), 100_000)
⋮----
.unwrap();
⋮----
let vote_account_pubkey = vote_account_keypair.pubkey();
config.signers = vec![&default_signer, &vote_account_keypair];
⋮----
authorized_withdrawer: config.signers[0].pubkey(),
⋮----
process_command(&config).await.unwrap();
⋮----
.get_account(&vote_account_keypair.pubkey())
⋮----
VoteStateV4::deserialize(vote_account.data(), &vote_account_keypair.pubkey()).unwrap();
⋮----
assert_eq!(authorized_withdrawer, config.signers[0].pubkey());
⋮----
.get_minimum_balance_for_rent_exemption(VoteStateV4::size_of())
⋮----
.unwrap()
.max(1);
check_balance!(expected_balance, &rpc_client, &vote_account_pubkey);
⋮----
new_authorized_pubkey: first_withdraw_authority.pubkey(),
⋮----
assert_eq!(authorized_withdrawer, first_withdraw_authority.pubkey());
⋮----
config.signers = vec![&default_signer, &first_withdraw_authority];
⋮----
new_authorized_pubkey: withdraw_authority.pubkey(),
⋮----
new_authorized: Some(1),
⋮----
process_command(&config).await.unwrap_err();
config.signers = vec![
⋮----
new_authorized: Some(2),
⋮----
assert_eq!(authorized_withdrawer, withdraw_authority.pubkey());
⋮----
config.signers = vec![&default_signer, &withdraw_authority];
⋮----
check_balance!(1_000, &rpc_client, &destination_account);
⋮----
config.signers.push(&new_identity_keypair);
⋮----
check_balance!(0, &rpc_client, &vote_account_pubkey);
check_balance!(expected_balance, &rpc_client, &destination_account);
⋮----
async fn test_offline_vote_authorize_and_withdraw(compute_unit_price: Option<u64>) {
⋮----
config_payer.json_rpc_url = test_validator.rpc_url();
config_payer.signers = vec![&default_signer];
⋮----
config_offline.signers = vec![&offline_keypair];
process_command(&config_offline).await.unwrap_err();
request_and_confirm_airdrop(
⋮----
&config_payer.signers[0].pubkey(),
⋮----
check_balance!(100_000, &rpc_client, &config_payer.signers[0].pubkey());
⋮----
&config_offline.signers[0].pubkey(),
⋮----
check_balance!(100_000, &rpc_client, &config_offline.signers[0].pubkey());
⋮----
config_payer.signers = vec![&default_signer, &vote_account_keypair];
⋮----
authorized_withdrawer: offline_keypair.pubkey(),
⋮----
process_command(&config_payer).await.unwrap();
⋮----
assert_eq!(authorized_withdrawer, offline_keypair.pubkey());
⋮----
let blockhash = rpc_client.get_latest_blockhash().await.unwrap();
⋮----
let sig_response = process_command(&config_offline).await.unwrap();
let sign_only = parse_sign_only_reply_string(&sig_response);
assert!(sign_only.has_all_signers());
⋮----
.presigner_of(&config_offline.signers[0].pubkey())
⋮----
config_payer.signers = vec![&offline_presigner];
⋮----
let fee_payer_null_signer = NullSigner::new(&default_signer.pubkey());
config_offline.signers = vec![&fee_payer_null_signer, &withdraw_authority];
⋮----
.presigner_of(&config_offline.signers[1].pubkey())
⋮----
config_payer.signers = vec![&default_signer, &offline_presigner];
⋮----
let new_identity_null_signer = NullSigner::new(&new_identity_keypair.pubkey());
config_offline.signers = vec![
⋮----
process_command(&config_offline).await.unwrap();
⋮----
config_payer.signers = vec![&default_signer, &offline_presigner, &new_identity_keypair];

================
File: cli/.gitignore
================
/target/
/farf/

================
File: cli/Cargo.toml
================
[package]
name = "solana-cli"
documentation = "https://docs.rs/solana-cli"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[[bin]]
name = "solana"
path = "src/main.rs"

[features]
default = ["remote-wallet-hidraw"]
agave-unstable-api = []
dev-context-only-utils = ["solana-faucet/dev-context-only-utils"]
remote-wallet-hidraw = ["solana-remote-wallet/linux-static-hidraw"]
remote-wallet-libusb = ["solana-remote-wallet/linux-static-libusb"]

[dependencies]
agave-feature-set = { workspace = true }
agave-logger = { workspace = true }
agave-syscalls = { workspace = true }
bincode = { workspace = true }
bs58 = { workspace = true }
clap = { workspace = true }
console = { workspace = true }
const_format = { workspace = true }
criterion-stats = { workspace = true }
crossbeam-channel = { workspace = true }
ctrlc = { workspace = true, features = ["termination"] }
hex = { workspace = true }
humantime = { workspace = true }
log = { workspace = true }
num-traits = { workspace = true }
pretty-hex = { workspace = true }
reqwest = { workspace = true, features = ["blocking", "brotli", "deflate", "gzip", "rustls-tls", "rustls-tls-native-roots", "json"] }
semver = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account = "=3.2.0"
solana-account-decoder = { workspace = true }
solana-address-lookup-table-interface = { workspace = true }
solana-borsh = "=3.0.0"
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-cli-output = { workspace = true }
solana-client = { workspace = true }
solana-clock = "=3.0.0"
solana-cluster-type = "=3.0.0"
solana-commitment-config = "=3.0.0"
solana-compute-budget-interface = { version = "=3.0.0", features = ["borsh"] }
solana-config-interface = "=2.0.0"
solana-connection-cache = { workspace = true }
solana-epoch-schedule = "=3.0.0"
solana-feature-gate-interface = { version = "=3.0.0", features = ["bincode"] }
solana-fee-calculator = "=3.0.0"
solana-fee-structure = "=3.0.0"
solana-hash = "=3.1.0"
solana-instruction = "=3.0.0"
solana-keypair = "=3.0.1"
solana-loader-v3-interface = { version = "=6.1.0", features = ["bincode"] }
solana-loader-v4-interface = "=3.1.0"
solana-loader-v4-program = { workspace = true }
solana-message = "=3.0.1"
solana-native-token = "=3.0.0"
solana-nonce = "=3.0.0"
solana-offchain-message = { version = "=3.0.0", features = ["verify"] }
solana-packet = "=3.0.0"
solana-program-runtime = { workspace = true }
solana-pubkey = { version = "=3.0.0", default-features = false }
solana-pubsub-client = { workspace = true }
solana-quic-client = { workspace = true }
solana-remote-wallet = { workspace = true }
solana-rent = "=3.0.0"
solana-rpc-client = { workspace = true, features = ["default"] }
solana-rpc-client-api = { workspace = true }
solana-rpc-client-nonce-utils = { workspace = true, features = ["clap"] }
solana-sbpf = { workspace = true, features = ["jit"] }
solana-sdk-ids = "=3.0.0"
solana-signature = { version = "=3.1.0", default-features = false }
solana-signer = "=3.0.0"
solana-slot-history = "=3.0.0"
solana-stake-interface = "=2.0.1"
solana-system-interface = { version = "=2.0", features = ["bincode"] }
solana-sysvar = "=3.0.0"
solana-tps-client = { workspace = true }
solana-tpu-client = { workspace = true, features = ["default"] }
solana-transaction = "=3.0.2"
solana-transaction-error = "=3.0.0"
solana-transaction-status = { workspace = true }
solana-transaction-status-client-types = { workspace = true }
solana-udp-client = { workspace = true }
solana-version = { workspace = true }
solana-vote-program = { workspace = true }
spl-memo-interface = { version = "=2.0.0" }
thiserror = { workspace = true }
tiny-bip39 = { workspace = true }
tokio = { workspace = true }

[dev-dependencies]
assert_matches = { workspace = true }
solana-client = { workspace = true, features = ["dev-context-only-utils"] }
solana-faucet = { workspace = true, features = ["dev-context-only-utils"] }
solana-net-utils = { workspace = true }
solana-nonce-account = { workspace = true }
solana-presigner = { workspace = true }
solana-rpc = { workspace = true }
solana-sha256-hasher = { workspace = true }
solana-test-validator = { workspace = true }
solana-tps-client = { workspace = true, features = ["dev-context-only-utils"] }
tempfile = { workspace = true }
test-case = { workspace = true }

================
File: cli-config/src/config_input.rs
================
pub enum SettingType {
⋮----
pub struct ConfigInput {
⋮----
impl ConfigInput {
fn default_keypair_path() -> String {
⋮----
fn default_json_rpc_url() -> String {
⋮----
fn default_websocket_url() -> String {
⋮----
fn default_commitment() -> CommitmentConfig {
⋮----
fn first_nonempty_setting(
⋮----
.into_iter()
.find(|(_, value)| !value.is_empty())
.expect("no nonempty setting")
⋮----
fn first_setting_is_some<T>(
⋮----
.find(|(_, value)| value.is_some())
.expect("all settings none");
(setting_type, setting_option.unwrap())
⋮----
pub fn compute_websocket_url_setting(
⋮----
Self::first_nonempty_setting(vec![
⋮----
pub fn compute_json_rpc_url_setting(
⋮----
let (setting_type, url_or_moniker) = Self::first_nonempty_setting(vec![
⋮----
(setting_type, normalize_to_url_if_moniker(url_or_moniker))
⋮----
pub fn compute_keypair_path_setting(
⋮----
pub fn compute_commitment_config(
⋮----
Self::first_setting_is_some(vec![
⋮----
impl Default for ConfigInput {
fn default() -> ConfigInput {

================
File: cli-config/src/config.rs
================
use std::sync::LazyLock;
⋮----
dirs_next::home_dir().map(|mut path| {
path.extend([".config", "solana", "cli", "config.yml"]);
path.to_str().unwrap().to_string()
⋮----
pub struct Config {
⋮----
impl Default for Config {
fn default() -> Self {
⋮----
let mut keypair_path = dirs_next::home_dir().expect("home directory");
keypair_path.extend([".config", "solana", "id.json"]);
keypair_path.to_str().unwrap().to_string()
⋮----
let json_rpc_url = "https://api.mainnet-beta.solana.com".to_string();
let websocket_url = "".to_string();
⋮----
address_labels.insert(
"11111111111111111111111111111111".to_string(),
"System Program".to_string(),
⋮----
let commitment = "confirmed".to_string();
⋮----
impl Config {
pub fn load(config_file: &str) -> Result<Self, io::Error> {
⋮----
pub fn save(&self, config_file: &str) -> Result<(), io::Error> {
⋮----
pub fn compute_websocket_url(json_rpc_url: &str) -> String {
let json_rpc_url: Option<Url> = json_rpc_url.parse().ok();
if json_rpc_url.is_none() {
return "".to_string();
⋮----
let json_rpc_url = json_rpc_url.unwrap();
let is_secure = json_rpc_url.scheme().eq_ignore_ascii_case("https");
let mut ws_url = json_rpc_url.clone();
⋮----
.set_scheme(if is_secure { "wss" } else { "ws" })
.expect("unable to set scheme");
if let Some(port) = json_rpc_url.port() {
let port = port.checked_add(1).expect("port out of range");
ws_url.set_port(Some(port)).expect("unable to set port");
⋮----
ws_url.to_string()
⋮----
pub fn import_address_labels<P>(&mut self, filename: P) -> Result<(), io::Error>
⋮----
for (address, label) in imports.into_iter() {
self.address_labels.insert(address, label);
⋮----
Ok(())
⋮----
pub fn export_address_labels<P>(&self, filename: P) -> Result<(), io::Error>
⋮----
mod test {
⋮----
fn compute_websocket_url() {
assert_eq!(
⋮----
assert_eq!(Config::compute_websocket_url("garbage"), String::new());

================
File: cli-config/src/lib.rs
================
mod config;
mod config_input;
⋮----
pub fn load_config_file<T, P>(config_file: P) -> Result<T, io::Error>
⋮----
serde_yaml::from_reader(file).map_err(|err| io::Error::other(format!("{err:?}")))?;
Ok(config)
⋮----
pub fn save_config_file<T, P>(config: &T, config_file: P) -> Result<(), io::Error>
⋮----
serde_yaml::to_string(config).map_err(|err| io::Error::other(format!("{err:?}")))?;
if let Some(outdir) = config_file.as_ref().parent() {
create_dir_all(outdir)?;
⋮----
file.write_all(b"---\n")?;
file.write_all(&serialized.into_bytes())?;
Ok(())

================
File: cli-config/Cargo.toml
================
[package]
name = "solana-cli-config"
documentation = "https://docs.rs/solana-cli-config"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
dirs-next = { workspace = true }
serde = { workspace = true }
serde_yaml = { workspace = true }
solana-clap-utils = { workspace = true }
solana-commitment-config = { workspace = true }
url = { workspace = true }

[dev-dependencies]
anyhow = { workspace = true }

================
File: cli-output/src/cli_output.rs
================
static CHECK_MARK: Emoji = Emoji("✅ ", "");
static CROSS_MARK: Emoji = Emoji("❌ ", "");
static WARNING: Emoji = Emoji("⚠️", "!");
⋮----
pub enum OutputFormat {
⋮----
impl OutputFormat {
pub fn formatted_string<T>(&self, item: &T) -> String
⋮----
OutputFormat::Display => format!("{item}"),
⋮----
QuietDisplay::write_str(item, &mut s).unwrap();
⋮----
VerboseDisplay::write_str(item, &mut s).unwrap();
⋮----
OutputFormat::Json => serde_json::to_string_pretty(item).unwrap(),
OutputFormat::JsonCompact => serde_json::to_value(item).unwrap().to_string(),
⋮----
pub fn from_matches(matches: &ArgMatches<'_>, output_name: &str, verbose: bool) -> Self {
⋮----
.value_of(output_name)
.map(|value| match value {
⋮----
_ => unreachable!(),
⋮----
.unwrap_or(if verbose {
⋮----
pub struct CliPrioritizationFeeStats {
⋮----
impl QuietDisplay for CliPrioritizationFeeStats {}
impl VerboseDisplay for CliPrioritizationFeeStats {
fn write_str(&self, f: &mut dyn std::fmt::Write) -> fmt::Result {
writeln!(f, "{:<11} prioritization_fee", "slot")?;
⋮----
write!(f, "{fee}")?;
⋮----
write!(f, "{self}")
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
writeln!(
⋮----
pub struct CliPrioritizationFee {
⋮----
impl QuietDisplay for CliPrioritizationFee {}
impl VerboseDisplay for CliPrioritizationFee {}
⋮----
writeln!(f, "{:<11} {}", self.slot, self.prioritization_fee)
⋮----
pub struct CliAccount {
⋮----
pub struct CliAccountNewConfig {
⋮----
impl Default for CliAccountNewConfig {
fn default() -> Self {
⋮----
impl CliAccount {
pub fn new<T: ReadableAccount>(address: &Pubkey, account: &T, use_lamports_unit: bool) -> Self {
⋮----
pub fn new_with_config<T: ReadableAccount>(
⋮----
pubkey: address.to_string(),
account: encode_ui_account(
⋮----
impl QuietDisplay for CliAccount {}
impl VerboseDisplay for CliAccount {}
⋮----
writeln!(f)?;
writeln_name_value(f, "Public Key:", &self.keyed_account.pubkey)?;
writeln_name_value(
⋮----
&build_balance_message(
⋮----
writeln_name_value(f, "Owner:", &self.keyed_account.account.owner)?;
⋮----
&self.keyed_account.account.executable.to_string(),
⋮----
&self.keyed_account.account.rent_epoch.to_string(),
⋮----
Ok(())
⋮----
pub struct CliBlockProduction {
⋮----
impl QuietDisplay for CliBlockProduction {}
impl VerboseDisplay for CliBlockProduction {}
⋮----
pub struct CliBlockProductionEntry {
⋮----
pub struct CliSlotStatus {
⋮----
pub struct CliEpochInfo {
⋮----
impl QuietDisplay for CliEpochInfo {}
impl VerboseDisplay for CliEpochInfo {}
⋮----
&self.epoch_info.block_height.to_string(),
⋮----
writeln_name_value(f, "Slot:", &self.epoch_info.absolute_slot.to_string())?;
writeln_name_value(f, "Epoch:", &self.epoch_info.epoch.to_string())?;
⋮----
writeln_name_value(f, "Transaction Count:", &transaction_count.to_string())?;
⋮----
&format!("[{start_slot}..{end_slot})"),
⋮----
&format!("{:>3.3}%", self.epoch_completed_percent),
⋮----
&format!(
⋮----
slot_to_duration(self.epoch_info.slot_index, self.average_slot_time_ms),
Some("* estimated based on current slot durations"),
⋮----
let time_remaining = slot_to_duration(remaining_slots_in_epoch, self.average_slot_time_ms);
⋮----
writeln!(f, "{annotation}")?;
⋮----
fn slot_to_duration(slot: Slot, slot_time_ms: u64) -> Duration {
⋮----
pub struct CliValidatorsStakeByVersion {
⋮----
pub enum CliValidatorsSortOrder {
⋮----
pub struct CliValidators {
⋮----
impl QuietDisplay for CliValidators {}
impl VerboseDisplay for CliValidators {}
⋮----
fn write_vote_account(
⋮----
fn non_zero_or_dash(v: u64, max_v: u64) -> String {
⋮----
"        -      ".into()
⋮----
format!("{v:>9} (  0)")
} else if v > max_v.saturating_sub(100) {
format!("{:>9} ({:>3})", v, -(max_v.saturating_sub(v) as isize))
⋮----
format!("{v:>9}      ")
⋮----
((self.validators.len() + 1) as f64).log10().floor() as usize + 1
⋮----
let header = style(format!(
⋮----
.bold();
writeln!(f, "{header}")?;
let mut sorted_validators = self.validators.clone();
⋮----
sorted_validators.sort_by_key(|a| a.delinquent);
⋮----
sorted_validators.sort_by_key(|a| a.commission);
⋮----
sorted_validators.sort_by_key(|a| a.epoch_credits);
⋮----
sorted_validators.sort_by(|a, b| a.identity_pubkey.cmp(&b.identity_pubkey));
⋮----
sorted_validators.sort_by_key(|a| a.last_vote);
⋮----
sorted_validators.sort_by_key(|a| a.root_slot);
⋮----
sorted_validators.sort_by(|a, b| a.vote_account_pubkey.cmp(&b.vote_account_pubkey));
⋮----
sorted_validators.sort_by(|a, b| {
use std::cmp::Ordering;
⋮----
(Some(a), Some(b)) => a.partial_cmp(&b).unwrap_or(Ordering::Equal),
⋮----
sorted_validators.sort_by_key(|a| a.activated_stake);
⋮----
(&a.version, a.activated_stake).cmp(&(&b.version, b.activated_stake))
⋮----
sorted_validators.reverse();
⋮----
.iter()
.map(|v| v.root_slot)
.max()
.unwrap_or_default();
⋮----
.map(|v| v.last_vote)
⋮----
for (i, validator) in sorted_validators.iter().enumerate() {
⋮----
sorted_validators.len() - i
⋮----
write!(f, "{num:padding$} ")?;
⋮----
write_vote_account(
⋮----
if self.validators.len() > 100 {
⋮----
&format!("{:.2}%", self.average_stake_weighted_skip_rate,),
⋮----
&format!("{:.2}%", self.average_skip_rate),
⋮----
&build_balance_message(self.total_active_stake, self.use_lamports_unit, true),
⋮----
writeln!(f, "{}", style("Stake By Version:").bold())?;
for (version, info) in self.stake_by_version.iter().rev() {
⋮----
pub struct CliValidator {
⋮----
impl CliValidator {
pub fn new(
⋮----
pub fn new_delinquent(
⋮----
fn _new(
⋮----
.find_map(|(epoch, credits, pre_credits)| {
⋮----
Some((*credits, credits.saturating_sub(*pre_credits)))
⋮----
.unwrap_or((0, 0));
⋮----
identity_pubkey: format_labeled_address(&vote_account.node_pubkey, address_labels),
vote_account_pubkey: format_labeled_address(&vote_account.vote_pubkey, address_labels),
⋮----
pub struct CliHistorySignatureVec(Vec<CliHistorySignature>);
impl CliHistorySignatureVec {
pub fn new(list: Vec<CliHistorySignature>) -> Self {
Self(list)
⋮----
impl QuietDisplay for CliHistorySignatureVec {}
impl VerboseDisplay for CliHistorySignatureVec {
fn write_str(&self, w: &mut dyn std::fmt::Write) -> std::fmt::Result {
⋮----
writeln!(w, "{} transactions found", self.0.len())
⋮----
write!(f, "{signature}")?;
⋮----
writeln!(f, "{} transactions found", self.0.len())
⋮----
pub struct CliHistorySignature {
⋮----
impl QuietDisplay for CliHistorySignature {}
impl VerboseDisplay for CliHistorySignature {
⋮----
.as_ref()
.expect("should have verbose.is_some()");
⋮----
writeln!(f, "{}", self.signature)
⋮----
pub struct CliHistoryVerbose {
⋮----
pub struct CliHistoryTransactionVec(Vec<CliTransactionConfirmation>);
impl CliHistoryTransactionVec {
pub fn new(list: Vec<CliTransactionConfirmation>) -> Self {
⋮----
impl QuietDisplay for CliHistoryTransactionVec {}
impl VerboseDisplay for CliHistoryTransactionVec {}
⋮----
pub struct CliNonceAccount {
⋮----
impl QuietDisplay for CliNonceAccount {}
impl VerboseDisplay for CliNonceAccount {}
⋮----
let nonce = self.nonce.as_deref().unwrap_or("uninitialized");
writeln!(f, "Nonce blockhash: {nonce}")?;
⋮----
writeln!(f, "Fee: {fees} lamports per signature")?;
⋮----
writeln!(f, "Fees: uninitialized")?;
⋮----
let authority = self.authority.as_deref().unwrap_or("uninitialized");
writeln!(f, "Authority: {authority}")
⋮----
pub struct CliStakeVec(Vec<CliKeyedStakeState>);
impl CliStakeVec {
pub fn new(list: Vec<CliKeyedStakeState>) -> Self {
⋮----
impl QuietDisplay for CliStakeVec {}
impl VerboseDisplay for CliStakeVec {
⋮----
writeln!(w)?;
⋮----
write!(f, "{state}")?;
⋮----
pub struct CliKeyedStakeState {
⋮----
impl QuietDisplay for CliKeyedStakeState {}
impl VerboseDisplay for CliKeyedStakeState {
⋮----
writeln!(w, "Stake Pubkey: {}", self.stake_pubkey)?;
⋮----
writeln!(f, "Stake Pubkey: {}", self.stake_pubkey)?;
write!(f, "{}", self.stake_state)
⋮----
pub struct CliEpochReward {
⋮----
pub struct CliKeyedEpochReward {
⋮----
pub struct CliEpochRewardsMetadata {
⋮----
pub struct CliKeyedEpochRewards {
⋮----
impl QuietDisplay for CliKeyedEpochRewards {}
impl VerboseDisplay for CliKeyedEpochRewards {
⋮----
if self.rewards.is_empty() {
writeln!(w, "No rewards found in epoch")?;
return Ok(());
⋮----
writeln!(w, "Epoch: {}", metadata.epoch)?;
⋮----
writeln!(w, "Epoch Rewards:")?;
⋮----
writeln!(w, "  {:<44}  No rewards in epoch", keyed_reward.address,)?;
⋮----
writeln!(f, "No rewards found in epoch")?;
⋮----
writeln!(f, "Epoch: {}", metadata.epoch)?;
⋮----
writeln!(f, "Epoch Rewards:")?;
⋮----
writeln!(f, "  {:<44}  No rewards in epoch", keyed_reward.address,)?;
⋮----
fn show_votes_and_credits(
⋮----
if votes.is_empty() {
⋮----
let newest_history_entry = epoch_voting_history.iter().next_back();
⋮----
for vote in votes.iter().rev() {
write!(
⋮----
writeln!(f, " (latency {})", vote.latency)?;
⋮----
if !epoch_voting_history.is_empty() {
⋮----
for entry in epoch_voting_history.iter().rev() {
⋮----
if let Some(oldest) = epoch_voting_history.iter().next() {
⋮----
enum Format {
⋮----
macro_rules! format_as {
⋮----
fn show_epoch_rewards(
⋮----
if epoch_rewards.is_empty() {
⋮----
format_as!(
⋮----
pub struct CliStakeState {
⋮----
impl QuietDisplay for CliStakeState {}
impl VerboseDisplay for CliStakeState {
⋮----
write!(w, "{self}")?;
⋮----
writeln!(w, "Credits Observed: {credits}")?;
⋮----
fn show_inactive_stake(
⋮----
let deactivating_stake = me.deactivating_stake.or(me.active_stake);
⋮----
fn show_active_stake(
⋮----
.map(|d| me.current_epoch <= d)
.unwrap_or(true)
⋮----
let active_stake = me.active_stake.unwrap_or(0);
⋮----
let activating_stake = me.activating_stake.or_else(|| {
if me.active_stake.is_none() {
Some(delegated_stake)
⋮----
fn show_authorized(f: &mut fmt::Formatter, authorized: &CliAuthorized) -> fmt::Result {
writeln!(f, "Stake Authority: {}", authorized.staker)?;
writeln!(f, "Withdraw Authority: {}", authorized.withdrawer)?;
⋮----
fn show_lockup(f: &mut fmt::Formatter, lockup: Option<&CliLockup>) -> fmt::Result {
⋮----
writeln!(f, "Lockup Epoch: {}", lockup.epoch)?;
⋮----
writeln!(f, "Lockup Custodian: {}", lockup.custodian)?;
⋮----
CliStakeType::RewardsPool => writeln!(f, "Stake account is a rewards pool")?,
CliStakeType::Uninitialized => writeln!(f, "Stake account is uninitialized")?,
⋮----
writeln!(f, "Stake account is undelegated")?;
show_authorized(f, self.authorized.as_ref().unwrap())?;
show_lockup(f, self.lockup.as_ref())?;
⋮----
self.active_stake.is_some()
|| self.activating_stake.is_some()
|| self.deactivating_stake.is_some()
⋮----
.map(|de| de > self.current_epoch)
⋮----
let delegated_stake = self.delegated_stake.unwrap();
⋮----
show_active_stake(self, f, delegated_stake)?;
show_inactive_stake(self, f, delegated_stake)?;
⋮----
show_epoch_rewards(f, &self.epoch_rewards, self.use_csv)?
⋮----
pub enum CliStakeType {
⋮----
pub struct CliStakeHistory {
⋮----
impl QuietDisplay for CliStakeHistory {}
impl VerboseDisplay for CliStakeHistory {}
⋮----
fn from((epoch, entry): &(Epoch, StakeHistoryEntry)) -> Self {
⋮----
pub struct CliStakeHistoryEntry {
⋮----
pub struct CliAuthorized {
⋮----
fn from(authorized: &Authorized) -> Self {
⋮----
staker: authorized.staker.to_string(),
withdrawer: authorized.withdrawer.to_string(),
⋮----
pub struct CliLockup {
⋮----
fn from(lockup: &Lockup) -> Self {
⋮----
custodian: lockup.custodian.to_string(),
⋮----
pub struct CliValidatorInfoVec(Vec<CliValidatorInfo>);
impl CliValidatorInfoVec {
pub fn new(list: Vec<CliValidatorInfo>) -> Self {
⋮----
impl QuietDisplay for CliValidatorInfoVec {}
impl VerboseDisplay for CliValidatorInfoVec {}
⋮----
if self.0.is_empty() {
writeln!(f, "No validator info accounts found")?;
⋮----
write!(f, "{validator_info}")?;
⋮----
pub struct CliValidatorInfo {
⋮----
impl QuietDisplay for CliValidatorInfo {}
impl VerboseDisplay for CliValidatorInfo {}
⋮----
writeln_name_value(f, "Validator Identity:", &self.identity_pubkey)?;
writeln_name_value(f, "  Info Address:", &self.info_pubkey)?;
for (key, value) in self.info.iter() {
⋮----
&format!("  {}:", to_title_case(key)),
value.as_str().unwrap_or("?"),
⋮----
pub struct CliVoteAccount {
⋮----
impl QuietDisplay for CliVoteAccount {}
impl VerboseDisplay for CliVoteAccount {}
⋮----
writeln!(f, "Validator Identity: {}", self.validator_identity)?;
writeln!(f, "Vote Authority: {}", self.authorized_voters)?;
writeln!(f, "Withdraw Authority: {}", self.authorized_withdrawer)?;
writeln!(f, "Credits: {}", self.credits)?;
writeln!(f, "Commission: {}%", self.commission)?;
⋮----
writeln!(f, "BLS Public Key: {bls_key}")?;
⋮----
show_votes_and_credits(f, &self.votes, &self.epoch_voting_history)?;
show_epoch_rewards(f, &self.epoch_rewards, self.use_csv)?;
⋮----
pub struct CliAuthorizedVoters {
⋮----
impl QuietDisplay for CliAuthorizedVoters {}
impl VerboseDisplay for CliAuthorizedVoters {}
⋮----
if let Some((_epoch, current_authorized_voter)) = self.authorized_voters.first_key_value() {
write!(f, "{current_authorized_voter}")?;
⋮----
write!(f, "None")?;
⋮----
if self.authorized_voters.len() > 1 {
⋮----
.last_key_value()
.expect("CliAuthorizedVoters::authorized_voters.len() > 1");
⋮----
fn from(authorized_voters: &AuthorizedVoters) -> Self {
⋮----
for (epoch, voter) in authorized_voters.iter() {
voter_map.insert(*epoch, voter.to_string());
⋮----
pub struct CliEpochVotingHistory {
⋮----
pub struct CliLandedVote {
⋮----
fn from(landed_vote: &LandedVote) -> Self {
⋮----
slot: landed_vote.slot(),
confirmation_count: landed_vote.confirmation_count(),
⋮----
pub struct CliBlockTime {
⋮----
impl QuietDisplay for CliBlockTime {}
impl VerboseDisplay for CliBlockTime {}
⋮----
writeln_name_value(f, "Block:", &self.slot.to_string())?;
writeln_name_value(f, "Date:", &unix_timestamp_to_string(self.timestamp))
⋮----
pub struct CliLeaderSchedule {
⋮----
impl QuietDisplay for CliLeaderSchedule {}
impl VerboseDisplay for CliLeaderSchedule {}
⋮----
writeln!(f, "  {:<15} {:<44}", entry.slot, entry.leader)?;
⋮----
pub struct CliLeaderScheduleEntry {
⋮----
pub struct CliInflation {
⋮----
impl QuietDisplay for CliInflation {}
impl VerboseDisplay for CliInflation {}
⋮----
writeln!(f, "{}", style("Inflation Governor:").bold())?;
if (self.governor.initial - self.governor.terminal).abs() < f64::EPSILON {
⋮----
pub struct CliSignOnlyData {
⋮----
impl QuietDisplay for CliSignOnlyData {}
impl VerboseDisplay for CliSignOnlyData {}
⋮----
writeln_name_value(f, "Blockhash:", &self.blockhash)?;
if let Some(message) = self.message.as_ref() {
writeln_name_value(f, "Transaction Message:", message)?;
⋮----
if !self.signers.is_empty() {
writeln!(f, "{}", style("Signers (Pubkey=Signature):").bold())?;
for signer in self.signers.iter() {
writeln!(f, " {signer}")?;
⋮----
if !self.absent.is_empty() {
writeln!(f, "{}", style("Absent Signers (Pubkey):").bold())?;
for pubkey in self.absent.iter() {
writeln!(f, " {pubkey}")?;
⋮----
if !self.bad_sig.is_empty() {
writeln!(f, "{}", style("Bad Signatures (Pubkey):").bold())?;
for pubkey in self.bad_sig.iter() {
⋮----
pub struct CliSignature {
⋮----
impl QuietDisplay for CliSignature {}
impl VerboseDisplay for CliSignature {}
⋮----
writeln_name_value(f, "Signature:", &self.signature)?;
⋮----
pub struct CliAccountBalances {
⋮----
impl QuietDisplay for CliAccountBalances {}
impl VerboseDisplay for CliAccountBalances {}
⋮----
pub struct CliSupply {
⋮----
fn from(rpc_supply: RpcSupply) -> Self {
⋮----
impl QuietDisplay for CliSupply {}
impl VerboseDisplay for CliSupply {}
⋮----
&format!("{} SOL", build_balance_message(self.total, false, false)),
⋮----
writeln_name_value(f, "Non-Circulating Accounts:", " ")?;
⋮----
writeln!(f, "  {account}")?;
⋮----
pub struct CliFeesInner {
⋮----
impl QuietDisplay for CliFeesInner {}
impl VerboseDisplay for CliFeesInner {}
⋮----
&self.lamports_per_signature.to_string(),
⋮----
.map(|s| s.to_string())
⋮----
writeln_name_value(f, "Last valid block height:", &last_valid_block_height)
⋮----
pub struct CliFees {
⋮----
impl QuietDisplay for CliFees {}
impl VerboseDisplay for CliFees {}
⋮----
match self.inner.as_ref() {
Some(inner) => write!(f, "{inner}"),
None => write!(f, "Fees unavailable"),
⋮----
impl CliFees {
pub fn some(
⋮----
inner: Some(CliFeesInner {
⋮----
blockhash: blockhash.to_string(),
⋮----
pub fn none() -> Self {
⋮----
pub struct CliTokenAccount {
⋮----
impl QuietDisplay for CliTokenAccount {}
impl VerboseDisplay for CliTokenAccount {}
⋮----
writeln_name_value(f, "Address:", &self.address)?;
⋮----
&account.token_amount.real_number_string_trimmed(),
⋮----
let mint = format!(
⋮----
writeln_name_value(f, "Mint:", &mint)?;
writeln_name_value(f, "Owner:", &account.owner)?;
writeln_name_value(f, "State:", &format!("{:?}", account.state))?;
⋮----
writeln!(f, "Delegation:")?;
writeln_name_value(f, "  Delegate:", delegate)?;
let allowance = account.delegated_amount.as_ref().unwrap();
writeln_name_value(f, "  Allowance:", &allowance.real_number_string_trimmed())?;
⋮----
account.close_authority.as_ref().unwrap_or(&String::new()),
⋮----
pub struct CliProgramId {
⋮----
impl QuietDisplay for CliProgramId {}
impl VerboseDisplay for CliProgramId {}
⋮----
writeln_name_value(f, "Program Id:", &self.program_id)?;
⋮----
writeln_name_value(f, "Signature:", signature)?;
⋮----
pub struct CliProgramBuffer {
⋮----
impl QuietDisplay for CliProgramBuffer {}
impl VerboseDisplay for CliProgramBuffer {}
⋮----
writeln_name_value(f, "Buffer:", &self.buffer)
⋮----
pub enum CliProgramAccountType {
⋮----
pub struct CliProgramAuthority {
⋮----
impl QuietDisplay for CliProgramAuthority {}
impl VerboseDisplay for CliProgramAuthority {}
⋮----
writeln_name_value(f, "Account Type:", &format!("{:?}", self.account_type))?;
writeln_name_value(f, "Authority:", &self.authority)
⋮----
pub struct CliProgram {
⋮----
impl QuietDisplay for CliProgram {}
impl VerboseDisplay for CliProgram {}
⋮----
writeln_name_value(f, "Owner:", &self.owner)?;
⋮----
&format!("{:?} ({:#x?}) bytes", self.data_len, self.data_len),
⋮----
pub struct CliProgramV4 {
⋮----
impl QuietDisplay for CliProgramV4 {}
impl VerboseDisplay for CliProgramV4 {}
⋮----
writeln_name_value(f, "Authority:", &self.authority)?;
⋮----
&self.last_deploy_slot.to_string(),
⋮----
writeln_name_value(f, "Status:", &self.status)?;
⋮----
pub struct CliProgramsV4 {
⋮----
impl QuietDisplay for CliProgramsV4 {}
impl VerboseDisplay for CliProgramsV4 {}
⋮----
for program in self.programs.iter() {
⋮----
pub struct CliUpgradeableProgram {
⋮----
impl QuietDisplay for CliUpgradeableProgram {}
impl VerboseDisplay for CliUpgradeableProgram {}
⋮----
writeln_name_value(f, "ProgramData Address:", &self.programdata_address)?;
⋮----
&build_balance_message(self.lamports, self.use_lamports_unit, true),
⋮----
pub struct CliUpgradeablePrograms {
⋮----
impl QuietDisplay for CliUpgradeablePrograms {}
impl VerboseDisplay for CliUpgradeablePrograms {}
⋮----
pub struct CliUpgradeableProgramClosed {
⋮----
impl QuietDisplay for CliUpgradeableProgramClosed {}
impl VerboseDisplay for CliUpgradeableProgramClosed {}
⋮----
pub struct CliUpgradeableProgramExtended {
⋮----
impl QuietDisplay for CliUpgradeableProgramExtended {}
impl VerboseDisplay for CliUpgradeableProgramExtended {}
⋮----
pub struct CliUpgradeableProgramMigrated {
⋮----
impl QuietDisplay for CliUpgradeableProgramMigrated {}
impl VerboseDisplay for CliUpgradeableProgramMigrated {}
⋮----
pub struct CliUpgradeableBuffer {
⋮----
impl QuietDisplay for CliUpgradeableBuffer {}
impl VerboseDisplay for CliUpgradeableBuffer {}
⋮----
writeln_name_value(f, "Buffer Address:", &self.address)?;
⋮----
pub struct CliUpgradeableBuffers {
⋮----
impl QuietDisplay for CliUpgradeableBuffers {}
impl VerboseDisplay for CliUpgradeableBuffers {}
⋮----
for buffer in self.buffers.iter() {
⋮----
pub struct CliAddressLookupTable {
⋮----
impl QuietDisplay for CliAddressLookupTable {}
impl VerboseDisplay for CliAddressLookupTable {}
⋮----
writeln_name_value(f, "Lookup Table Address:", &self.lookup_table_address)?;
⋮----
writeln_name_value(f, "Authority:", authority)?;
⋮----
writeln_name_value(f, "Authority:", "None (frozen)")?;
⋮----
writeln_name_value(f, "Deactivation Slot:", "None (still active)")?;
⋮----
writeln_name_value(f, "Deactivation Slot:", &self.deactivation_slot.to_string())?;
⋮----
writeln_name_value(f, "Last Extended Slot:", "None (empty)")?;
⋮----
&self.last_extended_slot.to_string(),
⋮----
if self.addresses.is_empty() {
writeln_name_value(f, "Address Table Entries:", "None (empty)")?;
⋮----
writeln!(f, "{}", style("Address Table Entries:".to_string()).bold())?;
⋮----
for (index, address) in self.addresses.iter().enumerate() {
writeln!(f, "  {index:<5}  {address}")?;
⋮----
pub struct CliAddressLookupTableCreated {
⋮----
impl QuietDisplay for CliAddressLookupTableCreated {}
impl VerboseDisplay for CliAddressLookupTableCreated {}
⋮----
pub struct ReturnSignersConfig {
⋮----
pub fn return_signers(
⋮----
return_signers_with_config(tx, output_format, &ReturnSignersConfig::default())
⋮----
pub fn return_signers_with_config(
⋮----
let cli_command = return_signers_data(tx, config);
Ok(output_format.formatted_string(&cli_command))
⋮----
pub fn return_signers_data(tx: &Transaction, config: &ReturnSignersConfig) -> CliSignOnlyData {
let verify_results = tx.verify_with_results();
⋮----
.zip(tx.message.account_keys.iter())
.zip(verify_results)
.for_each(|((sig, key), res)| {
⋮----
signers.push(format!("{key}={sig}"))
⋮----
absent.push(key.to_string());
⋮----
bad_sig.push(key.to_string());
⋮----
let message_data = tx.message_data();
Some(BASE64_STANDARD.encode(message_data))
⋮----
blockhash: tx.message.recent_blockhash.to_string(),
⋮----
pub fn parse_sign_only_reply_string(reply: &str) -> SignOnly {
let object: Value = serde_json::from_str(reply).unwrap();
let blockhash_str = object.get("blockhash").unwrap().as_str().unwrap();
let blockhash = blockhash_str.parse::<Hash>().unwrap();
⋮----
let signer_strings = object.get("signers");
⋮----
.as_array()
.unwrap()
⋮----
.map(|signer_string| {
let mut signer = signer_string.as_str().unwrap().split('=');
let key = Pubkey::from_str(signer.next().unwrap()).unwrap();
let sig = Signature::from_str(signer.next().unwrap()).unwrap();
⋮----
.collect();
⋮----
let signer_strings = object.get("absent");
⋮----
.map(|val| {
let s = val.as_str().unwrap();
Pubkey::from_str(s).unwrap()
⋮----
let signer_strings = object.get("badSig");
⋮----
.get("message")
.and_then(|o| o.as_str())
.map(|m| m.to_string());
⋮----
pub enum CliSignatureVerificationStatus {
⋮----
impl CliSignatureVerificationStatus {
pub fn verify_transaction(tx: &VersionedTransaction) -> Vec<Self> {
tx.verify_with_results()
⋮----
.zip(&tx.signatures)
.map(|(stat, sig)| match stat {
⋮----
.collect()
⋮----
Self::None => write!(f, "none"),
Self::Pass => write!(f, "pass"),
Self::Fail => write!(f, "fail"),
⋮----
pub struct CliBlock {
⋮----
impl QuietDisplay for CliBlock {}
impl VerboseDisplay for CliBlock {}
⋮----
writeln!(f, "Slot: {}", self.slot)?;
⋮----
writeln!(f, "Blockhash: {}", self.encoded_confirmed_block.blockhash)?;
⋮----
writeln!(f, "Block Height: {block_height:?}")?;
⋮----
if !self.encoded_confirmed_block.rewards.is_empty() {
let mut rewards = self.encoded_confirmed_block.rewards.clone();
rewards.sort_by(|a, b| a.pubkey.cmp(&b.pubkey));
⋮----
writeln!(f, "Rewards:")?;
⋮----
self.encoded_confirmed_block.transactions.iter().enumerate()
⋮----
writeln!(f, "Transaction {index}:")?;
writeln_transaction(
⋮----
&transaction_with_meta.transaction.decode().unwrap(),
transaction_with_meta.meta.as_ref(),
⋮----
pub struct CliTransaction {
⋮----
impl QuietDisplay for CliTransaction {}
impl VerboseDisplay for CliTransaction {}
⋮----
self.meta.as_ref(),
⋮----
if !self.sigverify_status.is_empty() {
Some(&self.sigverify_status)
⋮----
pub struct CliTransactionConfirmation {
⋮----
impl QuietDisplay for CliTransactionConfirmation {}
impl VerboseDisplay for CliTransactionConfirmation {
⋮----
write!(w, "{transaction}")?;
⋮----
writeln!(w, "Unable to get finalized transaction details: {err}")?;
⋮----
write!(w, "{self}")
⋮----
None => write!(f, "Not found"),
⋮----
write!(f, "Transaction failed: {err}")
⋮----
write!(f, "{confirmation_status:?}")
⋮----
pub struct CliGossipNode {
⋮----
impl CliGossipNode {
pub fn new(info: RpcContactInfo, labels: &HashMap<String, String>) -> Self {
⋮----
ip_address: info.gossip.map(|addr| addr.ip().to_string()),
identity_label: labels.get(&info.pubkey).cloned(),
⋮----
gossip_port: info.gossip.map(|addr| addr.port()),
tpu_port: info.tpu.map(|addr| addr.port()),
rpc_host: info.rpc.map(|addr| addr.to_string()),
pubsub_host: info.pubsub.map(|addr| addr.to_string()),
⋮----
tpu_quic_port: info.tpu_quic.map(|addr| addr.port()),
⋮----
fn unwrap_to_string_or_none<T>(option: Option<T>) -> String
⋮----
unwrap_to_string_or_default(option, "none")
⋮----
fn unwrap_to_string_or_default<T>(option: Option<T>, default: &str) -> String
⋮----
.map(|v| v.to_string())
.unwrap_or_else(|| default.to_string())
⋮----
impl QuietDisplay for CliGossipNode {}
impl VerboseDisplay for CliGossipNode {}
⋮----
pub struct CliGossipNodes(pub Vec<CliGossipNode>);
⋮----
for node in self.0.iter() {
writeln!(f, "{node}")?;
⋮----
writeln!(f, "Nodes: {}", self.0.len())
⋮----
impl QuietDisplay for CliGossipNodes {}
impl VerboseDisplay for CliGossipNodes {}
⋮----
pub struct CliPing {
⋮----
writeln_name_value(f, "Source Account:", &self.source_pubkey)?;
⋮----
write!(f, "{ping}")?;
⋮----
writeln!(f, "--- transaction statistics ---")?;
write!(f, "{}", self.transaction_stats)?;
⋮----
write!(f, "{confirmation_stats}")?;
⋮----
impl QuietDisplay for CliPing {}
impl VerboseDisplay for CliPing {}
⋮----
pub struct CliPingData {
⋮----
format!(
⋮----
impl QuietDisplay for CliPingData {}
impl VerboseDisplay for CliPingData {}
⋮----
pub struct CliPingTxStats {
⋮----
impl QuietDisplay for CliPingTxStats {}
impl VerboseDisplay for CliPingTxStats {}
⋮----
pub struct CliPingConfirmationStats {
⋮----
impl QuietDisplay for CliPingConfirmationStats {}
impl VerboseDisplay for CliPingConfirmationStats {}
⋮----
pub struct CliBalance {
⋮----
impl QuietDisplay for CliBalance {
⋮----
let balance_message = build_balance_message_with_config(self.lamports, &config);
write!(w, "{balance_message}")
⋮----
impl VerboseDisplay for CliBalance {
⋮----
let balance_message = build_balance_message_with_config(self.lamports, &self.config);
write!(f, "{balance_message}")
⋮----
pub struct CliFindProgramDerivedAddress {
⋮----
impl QuietDisplay for CliFindProgramDerivedAddress {}
impl VerboseDisplay for CliFindProgramDerivedAddress {}
⋮----
write!(f, "{}", self.address)?;
⋮----
mod tests {
⋮----
fn test_return_signers() {
struct BadSigner {
⋮----
impl BadSigner {
pub fn new(pubkey: Pubkey) -> Self {
⋮----
impl Signer for BadSigner {
fn try_pubkey(&self) -> Result<Pubkey, SignerError> {
Ok(self.pubkey)
⋮----
fn try_sign_message(&self, _message: &[u8]) -> Result<Signature, SignerError> {
Ok(Signature::from([1u8; 64]))
⋮----
fn is_interactive(&self) -> bool {
⋮----
let present: Box<dyn Signer> = Box::new(keypair_from_seed(&[2u8; 32]).unwrap());
⋮----
let from = present.pubkey();
let fee_payer = absent.pubkey();
let nonce_auth = bad.pubkey();
⋮----
vec![transfer(&from, &to, 42)],
Some(&fee_payer),
⋮----
let signers = vec![present.as_ref(), absent.as_ref(), bad.as_ref()];
⋮----
tx.try_partial_sign(&signers, blockhash).unwrap();
let res = return_signers(&tx, &OutputFormat::JsonCompact).unwrap();
let sign_only = parse_sign_only_reply_string(&res);
assert_eq!(sign_only.blockhash, blockhash);
assert_eq!(sign_only.message, None);
assert_eq!(sign_only.present_signers[0].0, present.pubkey());
assert_eq!(sign_only.absent_signers[0], absent.pubkey());
assert_eq!(sign_only.bad_signers[0], bad.pubkey());
let res_data = return_signers_data(&tx, &ReturnSignersConfig::default());
assert_eq!(
⋮----
.to_string();
⋮----
let res = return_signers_with_config(&tx, &OutputFormat::JsonCompact, &config).unwrap();
⋮----
assert_eq!(sign_only.message, Some(expected_msg.clone()));
⋮----
let res_data = return_signers_data(&tx, &config);
⋮----
fn test_verbose_quiet_output_formats() {
⋮----
struct FallbackToDisplay {}
⋮----
fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
write!(f, "display")
⋮----
impl QuietDisplay for FallbackToDisplay {}
impl VerboseDisplay for FallbackToDisplay {}
⋮----
assert_eq!(&OutputFormat::Display.formatted_string(&f), "display");
assert_eq!(&OutputFormat::DisplayQuiet.formatted_string(&f), "display");
⋮----
struct DiscreteVerbosityDisplay {}
⋮----
impl QuietDisplay for DiscreteVerbosityDisplay {
⋮----
write!(w, "quiet")
⋮----
impl VerboseDisplay for DiscreteVerbosityDisplay {
⋮----
write!(w, "verbose")
⋮----
assert_eq!(&OutputFormat::DisplayQuiet.formatted_string(&f), "quiet");
⋮----
fn test_output_format_from_matches() {
let app = App::new("test").arg(
⋮----
.long("output")
.value_name("FORMAT")
.global(true)
.takes_value(true)
.possible_values(&["json", "json-compact"])
.help("Return information in specified output format"),
⋮----
.clone()
.get_matches_from(vec!["test", "--output", "json"]);
⋮----
.get_matches_from(vec!["test", "--output", "json-compact"]);
⋮----
let matches = app.clone().get_matches_from(vec!["test"]);
⋮----
fn test_format_vote_account() {
let epoch_rewards = vec![
⋮----
validator_identity: Pubkey::default().to_string(),
epoch_rewards: Some(epoch_rewards),
⋮----
let s = format!("{c}");
⋮----
println!("{s}");

================
File: cli-output/src/cli_version.rs
================
pub struct CliVersion(Option<semver::Version>);
impl CliVersion {
pub fn unknown_version() -> Self {
Self(None)
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
⋮----
None => "unknown".to_string(),
Some(version) => version.to_string(),
⋮----
write!(f, "{s}")
⋮----
fn from(version: semver::Version) -> Self {
Self(Some(version))
⋮----
impl FromStr for CliVersion {
type Err = semver::Error;
fn from_str(s: &str) -> Result<Self, Self::Err> {
⋮----
Some(semver::Version::from_str(s)?)
⋮----
Ok(CliVersion(version_option))
⋮----
impl Serialize for CliVersion {
fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
⋮----
serializer.serialize_str(&self.to_string())
⋮----
fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
⋮----
CliVersion::from_str(s).map_err(serde::de::Error::custom)

================
File: cli-output/src/display.rs
================
pub struct BuildBalanceMessageConfig {
⋮----
impl Default for BuildBalanceMessageConfig {
fn default() -> Self {
⋮----
fn is_memo_program(k: &Pubkey) -> bool {
let k_str = k.to_string();
(k_str == spl_memo_v1_id().to_string()) || (k_str == spl_memo_v3_id().to_string())
⋮----
pub fn build_balance_message_with_config(
⋮----
lamports.to_string()
⋮----
let sol_str = format!("{sol:.9}");
⋮----
.trim_end_matches('0')
.trim_end_matches('.')
.to_string()
⋮----
format!(" lamport{ess}")
⋮----
" SOL".to_string()
⋮----
"".to_string()
⋮----
format!("{value}{unit}")
⋮----
pub fn build_balance_message(lamports: u64, use_lamports_unit: bool, show_unit: bool) -> String {
build_balance_message_with_config(
⋮----
// Pretty print a "name value"
pub fn println_name_value(name: &str, value: &str) {
let styled_value = if value.is_empty() {
style("(not set)").italic()
⋮----
style(value)
⋮----
println!("{} {}", style(name).bold(), styled_value);
⋮----
pub fn writeln_name_value(f: &mut dyn fmt::Write, name: &str, value: &str) -> fmt::Result {
⋮----
writeln!(f, "{} {}", style(name).bold(), styled_value)
⋮----
pub fn println_name_value_or(name: &str, value: &str, setting_type: SettingType) {
⋮----
println!(
⋮----
pub fn format_labeled_address(pubkey: &str, address_labels: &HashMap<String, String>) -> String {
let label = address_labels.get(pubkey);
⋮----
Some(label) => format!(
⋮----
None => pubkey.to_string(),
⋮----
pub fn println_signers(
⋮----
println!();
println!("Blockhash: {blockhash}");
if !signers.is_empty() {
println!("Signers (Pubkey=Signature):");
signers.iter().for_each(|signer| println!("  {signer}"))
⋮----
if !absent.is_empty() {
println!("Absent Signers (Pubkey):");
absent.iter().for_each(|pubkey| println!("  {pubkey}"))
⋮----
if !bad_sig.is_empty() {
println!("Bad Signatures (Pubkey):");
bad_sig.iter().for_each(|pubkey| println!("  {pubkey}"))
⋮----
struct CliAccountMeta {
⋮----
fn format_account_mode(meta: CliAccountMeta) -> String {
format!(
"{}r{}{}", // accounts are always readable...
⋮----
fn write_transaction<W: io::Write>(
⋮----
write_block_time(w, block_time, timezone, prefix)?;
⋮----
.static_account_keys()
.iter()
.map(AccountKeyType::Known);
⋮----
.address_table_lookups()
.map(transform_lookups_to_unknown_keys)
.unwrap_or_default();
static_keys_iter.chain(dynamic_keys).collect()
⋮----
write_version(w, transaction.version(), prefix)?;
write_recent_blockhash(w, message.recent_blockhash(), prefix)?;
write_signatures(w, &transaction.signatures, sigverify_status, prefix)?;
⋮----
for (account_index, account) in account_keys.iter().enumerate() {
⋮----
is_signer: message.is_signer(account_index),
is_writable: message.is_maybe_writable(account_index, Some(&reserved_account_keys)),
is_invoked: message.is_invoked(account_index),
⋮----
write_account(
⋮----
format_account_mode(account_meta),
⋮----
for (instruction_index, instruction) in message.instructions().iter().enumerate() {
⋮----
.map(|account_index| (account_keys[*account_index as usize], *account_index));
write_instruction(
⋮----
if let Some(address_table_lookups) = message.address_table_lookups() {
write_address_table_lookups(w, address_table_lookups, prefix)?;
⋮----
write_status(w, &transaction_status.status, prefix)?;
write_fees(w, transaction_status.fee, prefix)?;
write_balances(w, transaction_status, prefix)?;
write_compute_units_consumed(
⋮----
transaction_status.compute_units_consumed.clone().into(),
⋮----
write_log_messages(w, transaction_status.log_messages.as_ref().into(), prefix)?;
write_return_data(w, transaction_status.return_data.as_ref().into(), prefix)?;
write_rewards(w, transaction_status.rewards.as_ref().into(), prefix)?;
⋮----
writeln!(w, "{prefix}Status: Unavailable")?;
⋮----
Ok(())
⋮----
fn transform_lookups_to_unknown_keys(
⋮----
.enumerate()
.flat_map(|(lookup_index, lookup)| {
⋮----
.map(move |table_index| AccountKeyType::Unknown {
⋮----
unknown_writable_keys.chain(unknown_readonly_keys).collect()
⋮----
enum CliTimezone {
⋮----
fn write_block_time<W: io::Write>(
⋮----
CliTimezone::Local => format!("{:?}", Local.timestamp_opt(block_time, 0).unwrap()),
CliTimezone::Utc => format!("{:?}", Utc.timestamp_opt(block_time, 0).unwrap()),
⋮----
writeln!(w, "{prefix}Block Time: {block_time_output}",)?;
⋮----
fn write_version<W: io::Write>(
⋮----
TransactionVersion::Legacy(_) => "legacy".to_string(),
TransactionVersion::Number(number) => number.to_string(),
⋮----
writeln!(w, "{prefix}Version: {version}")
⋮----
fn write_recent_blockhash<W: io::Write>(
⋮----
writeln!(w, "{prefix}Recent Blockhash: {recent_blockhash:?}")
⋮----
fn write_signatures<W: io::Write>(
⋮----
sigverify_status.iter().map(|s| format!(" ({s})")).collect()
⋮----
vec!["".to_string(); signatures.len()]
⋮----
signatures.iter().zip(&sigverify_statuses).enumerate()
⋮----
writeln!(
⋮----
enum AccountKeyType<'a> {
⋮----
fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
⋮----
Self::Known(address) => write!(f, "{address}"),
⋮----
write!(
⋮----
fn write_account<W: io::Write>(
⋮----
fn write_instruction<'a, W: io::Write>(
⋮----
writeln!(w, "{prefix}Instruction {instruction_index}")?;
⋮----
for (index, (account_address, account_index)) in instruction_accounts.enumerate() {
⋮----
writeln!(w, "{prefix}  {vote_instruction:?}")?;
⋮----
writeln!(w, "{prefix}  {stake_instruction:?}")?;
⋮----
writeln!(w, "{prefix}  {system_instruction:?}")?;
⋮----
} else if is_memo_program(program_pubkey) {
⋮----
writeln!(w, "{prefix}  Data: \"{s}\"")?;
⋮----
writeln!(w, "{}  Data: {:?}", prefix, instruction.data)?;
⋮----
fn write_address_table_lookups<W: io::Write>(
⋮----
for (lookup_index, lookup) in address_table_lookups.iter().enumerate() {
writeln!(w, "{prefix}Address Table Lookup {lookup_index}",)?;
writeln!(w, "{}  Table Account: {}", prefix, lookup.account_key,)?;
⋮----
fn write_rewards<W: io::Write>(
⋮----
if !rewards.is_empty() {
writeln!(w, "{prefix}Rewards:",)?;
⋮----
fn write_status<W: io::Write>(
⋮----
fn write_fees<W: io::Write>(w: &mut W, transaction_fee: u64, prefix: &str) -> io::Result<()> {
⋮----
fn write_balances<W: io::Write>(
⋮----
assert_eq!(
⋮----
.zip(transaction_status.post_balances.iter())
⋮----
fn write_return_data<W: io::Write>(
⋮----
UiReturnDataEncoding::Base64 => BASE64_STANDARD.decode(data).map_err(|err| {
io::Error::other(format!("could not parse data as {encoding:?}: {err:?}"))
⋮----
if !raw_return_data.is_empty() {
⋮----
writeln!(w, "{}  {:?}", prefix, raw_return_data.hex_dump())?;
⋮----
fn write_compute_units_consumed<W: io::Write>(
⋮----
writeln!(w, "{prefix}Compute Units Consumed: {cus}")?;
⋮----
fn write_log_messages<W: io::Write>(
⋮----
if !log_messages.is_empty() {
writeln!(w, "{prefix}Log Messages:",)?;
⋮----
writeln!(w, "{prefix}  {log_message}")?;
⋮----
pub fn println_transaction(
⋮----
if write_transaction(
⋮----
.is_ok()
⋮----
print!("{s}");
⋮----
pub fn writeln_transaction(
⋮----
let write_result = write_transaction(
⋮----
if write_result.is_ok() {
⋮----
write!(f, "{s}")?;
⋮----
pub fn new_spinner_progress_bar() -> ProgressBar {
⋮----
progress_bar.set_style(
⋮----
.template("{spinner:.green} {wide_msg}")
.expect("ProgressStyle::template direct input to be correct"),
⋮----
progress_bar.enable_steady_tick(Duration::from_millis(100));
⋮----
pub fn unix_timestamp_to_string(unix_timestamp: UnixTimestamp) -> String {
⋮----
Some(ndt) => ndt.to_rfc3339_opts(SecondsFormat::Secs, true),
None => format!("UnixTimestamp {unix_timestamp}"),
⋮----
mod test {
⋮----
fn new_test_keypair() -> Keypair {
let secret = ed25519_dalek::SecretKey::from_bytes(&[0u8; 32]).unwrap();
⋮----
Keypair::try_from(keypair.to_bytes().as_ref()).unwrap()
⋮----
fn new_test_v0_transaction() -> VersionedTransaction {
let keypair = new_test_keypair();
⋮----
account_keys: vec![keypair.pubkey(), account_key],
address_table_lookups: vec![MessageAddressTableLookup {
⋮----
instructions: vec![CompiledInstruction::new_from_raw_parts(
⋮----
.unwrap()
⋮----
fn test_write_legacy_transaction() {
⋮----
instructions: vec![CompiledInstruction::new_from_raw_parts(1, vec![], vec![0])],
⋮----
status: Ok(()),
⋮----
pre_balances: vec![5000, 10_000],
post_balances: vec![0, 9_900],
⋮----
log_messages: Some(vec!["Test message".to_string()]),
⋮----
rewards: Some(vec![Reward {
⋮----
return_data: Some(TransactionReturnData {
⋮----
data: vec![1, 2, 3],
⋮----
compute_units_consumed: Some(1234u64),
cost_units: Some(5678),
⋮----
write_transaction(
⋮----
Some(&meta.into()),
⋮----
Some(&sigverify_status),
Some(1628633791),
⋮----
.unwrap();
let bytes = write_buffer.into_inner().unwrap();
String::from_utf8(bytes).unwrap()
⋮----
".replace("\\0", "") // replace marker used to subvert trailing whitespace linter on CI
⋮----
fn test_write_v0_transaction() {
let versioned_tx = new_test_v0_transaction();
⋮----
writable: vec![address_table_entry1],
readonly: vec![address_table_entry2],
⋮----
pre_balances: vec![5000, 10_000, 15_000, 20_000],
post_balances: vec![0, 10_000, 14_900, 20_000],
⋮----
compute_units_consumed: Some(2345u64),
⋮----
fn test_format_labeled_address() {
let pubkey = Pubkey::default().to_string();
⋮----
assert_eq!(format_labeled_address(&pubkey, &address_labels), pubkey);
address_labels.insert(pubkey.to_string(), "Default Address".to_string());
⋮----
address_labels.insert(
pubkey.to_string(),
"abcdefghijklmnopqrstuvwxyz1234567890".to_string(),
⋮----
fn test_unix_timestamp_to_string() {
assert_eq!(unix_timestamp_to_string(1628633791), "2021-08-10T22:16:31Z");

================
File: cli-output/src/lib.rs
================
mod cli_output;
pub mod cli_version;
pub mod display;
⋮----
pub trait QuietDisplay: std::fmt::Display {
fn write_str(&self, w: &mut dyn std::fmt::Write) -> std::fmt::Result {
write!(w, "{self}")
⋮----
pub trait VerboseDisplay: std::fmt::Display {

================
File: cli-output/Cargo.toml
================
[package]
name = "solana-cli-output"
documentation = "https://docs.rs/solana-cli-output"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
Inflector = { workspace = true }
agave-reserved-account-keys = { workspace = true }
base64 = { workspace = true }
chrono = { workspace = true, features = ["default", "serde"] }
clap = "2.33.0"
console = { workspace = true }
humantime = { workspace = true }
indicatif = { workspace = true }
pretty-hex = { workspace = true }
semver = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
solana-account = { workspace = true }
solana-account-decoder = { workspace = true }
solana-bincode = { workspace = true }
solana-clap-utils = { workspace = true }
solana-cli-config = { workspace = true }
solana-clock = { workspace = true }
solana-epoch-info = { workspace = true, features = ["serde"] }
solana-hash = { workspace = true }
solana-message = { workspace = true }
solana-packet = { workspace = true }
solana-pubkey = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-signature = { workspace = true }
solana-stake-interface = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true, features = ["verify"] }
solana-transaction-error = { workspace = true }
solana-transaction-status = { workspace = true }
solana-transaction-status-client-types = { workspace = true }
solana-vote-program = { workspace = true }
spl-memo-interface = { workspace = true }

[dev-dependencies]
ed25519-dalek = { workspace = true }
solana-keypair = { workspace = true }
solana-signer = { workspace = true }
solana-transaction-context = { workspace = true }

================
File: client/src/nonblocking/mod.rs
================
pub mod tpu_client;
pub mod blockhash_query {
⋮----
pub mod nonce_utils {
⋮----
pub mod pubsub_client {
⋮----
pub mod rpc_client {

================
File: client/src/nonblocking/tpu_client.rs
================
pub struct TpuClient<
⋮----
pub async fn send_transaction(&self, transaction: &Transaction) -> bool {
self.tpu_client.send_transaction(transaction).await
⋮----
pub async fn send_wire_transaction(&self, wire_transaction: Vec<u8>) -> bool {
⋮----
.send_wire_transaction(wire_transaction)
⋮----
pub async fn try_send_transaction(&self, transaction: &Transaction) -> TransportResult<()> {
self.tpu_client.try_send_transaction(transaction).await
⋮----
pub async fn try_send_wire_transaction(
⋮----
.try_send_wire_transaction(wire_transaction)
⋮----
pub async fn try_send_wire_transaction_batch(
⋮----
.try_send_wire_transaction_batch(wire_transactions)
⋮----
pub async fn new(
⋮----
return Err(TpuSenderError::Custom(String::from(
⋮----
pub async fn new_with_connection_cache(
⋮----
Ok(Self {
⋮----
pub async fn send_and_confirm_messages_with_spinner<T: Signers + ?Sized>(
⋮----
.send_and_confirm_messages_with_spinner(messages, signers)
⋮----
pub fn rpc_client(&self) -> &RpcClient {
self.tpu_client.rpc_client()
⋮----
pub async fn shutdown(&mut self) {
self.tpu_client.shutdown().await

================
File: client/src/connection_cache.rs
================
pub use solana_connection_cache::connection_cache::Protocol;
⋮----
pub enum ConnectionCache {
⋮----
type QuicBaseClientConnection = <QuicPool as ConnectionPool>::BaseClientConnection;
type UdpBaseClientConnection = <UdpPool as ConnectionPool>::BaseClientConnection;
pub enum BlockingClientConnection {
⋮----
pub enum NonblockingClientConnection {
⋮----
impl NotifyKeyUpdate for ConnectionCache {
fn update_key(&self, key: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
⋮----
Self::Udp(_) => Ok(()),
Self::Quic(backend) => backend.update_key(key),
⋮----
impl ConnectionCache {
pub fn new(name: &'static str) -> Self {
⋮----
None, // client_endpoint
Some(cert_info),
None, // stake_info
⋮----
/// Create a quic connection_cache
    pub fn new_quic(name: &'static str, connection_pool_size: usize) -> Self {
⋮----
pub fn new_quic(name: &'static str, connection_pool_size: usize) -> Self {
⋮----
pub fn new_quic_for_tests(name: &'static str, connection_pool_size: usize) -> Self {
⋮----
Some(solana_net_utils::sockets::bind_to_localhost_unique().unwrap()),
⋮----
/// Create a quic connection_cache with more client options
    pub fn new_with_client_options(
⋮----
pub fn new_with_client_options(
⋮----
let connection_pool_size = 1.max(connection_pool_size);
let mut config = QuicConfig::new().unwrap();
⋮----
config.update_client_certificate(cert_info.0, cert_info.1);
⋮----
config.update_client_endpoint(client_socket);
⋮----
config.set_staked_nodes(stake_info.0, stake_info.1);
⋮----
BackendConnectionCache::new(name, connection_manager, connection_pool_size).unwrap();
⋮----
pub fn protocol(&self) -> Protocol {
⋮----
pub fn with_udp(name: &'static str, connection_pool_size: usize) -> Self {
⋮----
pub fn use_quic(&self) -> bool {
matches!(self, Self::Quic(_))
⋮----
pub fn get_connection(&self, addr: &SocketAddr) -> BlockingClientConnection {
⋮----
Self::Quic(cache) => BlockingClientConnection::Quic(cache.get_connection(addr)),
Self::Udp(cache) => BlockingClientConnection::Udp(cache.get_connection(addr)),
⋮----
pub fn get_nonblocking_connection(&self, addr: &SocketAddr) -> NonblockingClientConnection {
⋮----
NonblockingClientConnection::Quic(cache.get_nonblocking_connection(addr))
⋮----
NonblockingClientConnection::Udp(cache.get_nonblocking_connection(addr))
⋮----
macro_rules! dispatch {
⋮----
pub(crate) use dispatch;
impl ClientConnection for BlockingClientConnection {
dispatch!(fn server_addr(&self) -> &SocketAddr);
dispatch!(fn send_data(&self, buffer: &[u8]) -> TransportResult<()>);
dispatch!(fn send_data_async(&self, buffer: Arc<Vec<u8>>) -> TransportResult<()>);
dispatch!(fn send_data_batch(&self, buffers: &[Vec<u8>]) -> TransportResult<()>);
dispatch!(fn send_data_batch_async(&self, buffers: Vec<Vec<u8>>) -> TransportResult<()>);
⋮----
async fn send_data(&self, buffer: &[u8]) -> TransportResult<()> {
⋮----
Self::Quic(cache) => Ok(cache.send_data(buffer).await?),
Self::Udp(cache) => Ok(cache.send_data(buffer).await?),
⋮----
async fn send_data_batch(&self, buffers: &[Vec<u8>]) -> TransportResult<()> {
⋮----
Self::Quic(cache) => Ok(cache.send_data_batch(buffers).await?),
Self::Udp(cache) => Ok(cache.send_data_batch(buffers).await?),
⋮----
mod tests {
⋮----
fn test_connection_with_specified_client_endpoint() {
let port_range = localhost_port_range_for_tests();
⋮----
bind_to(IpAddr::V4(Ipv4Addr::LOCALHOST), port_range.next().unwrap()).unwrap();
⋮----
Some(client_socket),
⋮----
let port1 = port_range.next().unwrap();
⋮----
let conn = connection_cache.get_connection(&addr);
assert_eq!(conn.server_addr().port(), port1);
let port2 = port_range.next().unwrap();
⋮----
assert_eq!(conn.server_addr().port(), port2);

================
File: client/src/lib.rs
================
pub mod connection_cache;
pub mod nonblocking;
pub mod send_and_confirm_transactions_in_parallel;
pub mod tpu_client;
pub mod transaction_executor;
pub use solana_rpc_client::mock_sender_for_cli;
pub mod blockhash_query {
⋮----
pub mod client_error {
⋮----
pub mod nonce_utils {
⋮----
pub mod pubsub_client {
⋮----
pub mod rpc_client {
⋮----
pub mod rpc_config {
⋮----
pub mod rpc_custom_error {
⋮----
pub mod rpc_filter {
⋮----
pub mod rpc_request {
⋮----
pub mod rpc_response {
⋮----
pub mod rpc_sender {

================
File: client/src/send_and_confirm_transactions_in_parallel.rs
================
type QuicTpuClient = TpuClient<QuicPool, QuicConnectionManager, QuicConfig>;
⋮----
struct TransactionData {
⋮----
struct BlockHashData {
⋮----
pub struct SendAndConfirmConfigV2 {
⋮----
pub fn send_and_confirm_transactions_in_parallel_blocking_v2<T: Signers + ?Sized>(
⋮----
let fut = send_and_confirm_transactions_in_parallel_v2(
rpc_client.get_inner_client().clone(),
⋮----
tokio::task::block_in_place(|| rpc_client.runtime().block_on(fut))
⋮----
fn create_blockhash_data_updating_task(
⋮----
.get_latest_blockhash_with_commitment(rpc_client.commitment())
⋮----
*blockhash_data_rw.write().await = BlockHashData {
⋮----
if let Ok(block_height) = rpc_client.get_block_height().await {
current_block_height.store(block_height, Ordering::Relaxed);
⋮----
fn create_transaction_confirmation_task(
⋮----
let mut last_block_height = current_block_height.load(Ordering::Relaxed);
⋮----
if !unconfirmed_transaction_map.is_empty() {
let current_block_height = current_block_height.load(Ordering::Relaxed);
⋮----
.iter()
.filter(|x| {
⋮----
.map(|x| *x.key())
.collect();
⋮----
transactions_to_verify.chunks(MAX_GET_SIGNATURE_STATUSES_QUERY_ITEMS)
⋮----
if let Ok(result) = rpc_client.get_signature_statuses(signatures).await {
⋮----
for (signature, status) in signatures.iter().zip(statuses.into_iter()) {
⋮----
.filter(|status| {
status.satisfies_commitment(rpc_client.commitment())
⋮----
.and_then(|status| {
⋮----
.remove(signature)
.map(|(_, data)| (status, data))
⋮----
num_confirmed_transactions.fetch_add(1, Ordering::Relaxed);
⋮----
errors_map.insert(data.index, error);
⋮----
struct SendingContext {
⋮----
fn progress_from_context_and_block_height(
⋮----
.load(std::sync::atomic::Ordering::Relaxed),
⋮----
async fn send_transaction_with_rpc_fallback(
⋮----
tpu_client.send_wire_transaction(serialized_transaction.clone()),
⋮----
.unwrap_or(false)
⋮----
.send_transaction_with_config(
⋮----
preflight_commitment: Some(rpc_client.commitment().commitment),
⋮----
match e.kind() {
⋮----
context.error_map.insert(index, transaction_error.clone());
⋮----
match TransactionError::from(ui_transaction_error.clone()) {
⋮----
context.error_map.insert(index, err);
⋮----
return Err(TpuSenderError::from(e));
⋮----
Ok(())
⋮----
async fn sign_all_messages_and_send<T: Signers + ?Sized>(
⋮----
let current_transaction_count = messages_with_index.len();
let mut futures = vec![];
for (counter, (index, message)) in messages_with_index.iter().enumerate() {
let mut transaction = Transaction::new_unsigned(message.clone());
futures.push(async move {
tokio::time::sleep(SEND_INTERVAL.saturating_mul(counter as u32)).await;
let blockhashdata = *context.blockhash_data_rw.read().await;
⋮----
.try_sign(signers, blockhashdata.blockhash)
.expect("Transaction should be signable");
⋮----
serialize(&transaction).expect("Transaction should serialize");
⋮----
context.unconfirmed_transaction_map.insert(
⋮----
serialized_transaction: serialized_transaction.clone(),
⋮----
message: message.clone(),
⋮----
let progress = progress_from_context_and_block_height(
⋮----
progress.set_message_for_confirmed_transactions(
⋮----
&format!(
⋮----
send_transaction_with_rpc_fallback(
⋮----
join_all(futures)
⋮----
.into_iter()
⋮----
async fn confirm_transactions_till_block_height_and_resend_unexpired_transaction_over_tpu(
⋮----
let unconfirmed_transaction_map = context.unconfirmed_transaction_map.clone();
let current_block_height = context.current_block_height.clone();
let transactions_to_confirm = unconfirmed_transaction_map.len();
⋮----
.map(|x| x.last_valid_block_height)
.max();
⋮----
let progress = progress_from_context_and_block_height(context, max_valid_block_height);
⋮----
while !unconfirmed_transaction_map.is_empty()
&& current_block_height.load(Ordering::Relaxed) <= max_valid_block_height
⋮----
let block_height = current_block_height.load(Ordering::Relaxed);
⋮----
.filter(|x| block_height < x.last_valid_block_height)
.map(|x| x.serialized_transaction.clone())
⋮----
send_staggered_transactions(
⋮----
.max()
⋮----
async fn send_staggered_transactions(
⋮----
let current_transaction_count = wire_transactions.len();
⋮----
.enumerate()
.map(|(counter, transaction)| async move {
⋮----
progress_from_context_and_block_height(context, last_valid_block_height);
⋮----
tpu_client.send_wire_transaction(transaction),
⋮----
join_all(futures).await;
⋮----
pub async fn send_and_confirm_transactions_in_parallel_v2<T: Signers + ?Sized>(
⋮----
.map(|x| {
let mut transaction = Transaction::new_unsigned(x.clone());
transaction.try_sign(signers, blockhash)
⋮----
let block_height = rpc_client.get_block_height().await?;
⋮----
let progress_bar = config.with_spinner.then(|| {
⋮----
progress_bar.set_message("Setting up...");
⋮----
let block_data_task = create_blockhash_data_updating_task(
rpc_client.clone(),
blockhash_data_rw.clone(),
current_block_height.clone(),
⋮----
let transaction_confirming_task = create_transaction_confirmation_task(
⋮----
unconfirmed_transasction_map.clone(),
error_map.clone(),
num_confirmed_transactions.clone(),
⋮----
let total_transactions = messages.len();
⋮----
let signing_count = config.resign_txs_count.unwrap_or(1);
⋮----
unconfirmed_transaction_map: unconfirmed_transasction_map.clone(),
blockhash_data_rw: blockhash_data_rw.clone(),
num_confirmed_transactions: num_confirmed_transactions.clone(),
current_block_height: current_block_height.clone(),
error_map: error_map.clone(),
⋮----
for expired_blockhash_retries in (0..signing_count).rev() {
⋮----
messages.iter().cloned().enumerate().collect()
⋮----
.map(|x| (x.index, x.message.clone()))
.collect()
⋮----
if messages_with_index.is_empty() {
⋮----
unconfirmed_transasction_map.clear();
sign_all_messages_and_send(
⋮----
confirm_transactions_till_block_height_and_resend_unexpired_transaction_over_tpu(
⋮----
if unconfirmed_transasction_map.is_empty() {
⋮----
progress_bar.println(format!(
⋮----
block_data_task.abort();
transaction_confirming_task.abort();
⋮----
let mut transaction_errors = vec![None; messages.len()];
for iterator in error_map.iter() {
transaction_errors[*iterator.key()] = Some(iterator.value().clone());
⋮----
Ok(transaction_errors)
⋮----
Err(TpuSenderError::Custom("Max retries exceeded".into()))

================
File: client/src/tpu_client.rs
================
pub enum TpuClientWrapper {
⋮----
pub struct TpuClient<
⋮----
pub fn send_transaction(&self, transaction: &Transaction) -> bool {
self.tpu_client.send_transaction(transaction)
⋮----
pub fn send_wire_transaction(&self, wire_transaction: Vec<u8>) -> bool {
self.tpu_client.send_wire_transaction(wire_transaction)
⋮----
pub fn try_send_transaction(&self, transaction: &Transaction) -> TransportResult<()> {
self.tpu_client.try_send_transaction(transaction)
⋮----
pub fn try_send_transaction_batch(&self, transactions: &[Transaction]) -> TransportResult<()> {
self.tpu_client.try_send_transaction_batch(transactions)
⋮----
pub fn try_send_wire_transaction(&self, wire_transaction: Vec<u8>) -> TransportResult<()> {
self.tpu_client.try_send_wire_transaction(wire_transaction)
⋮----
pub fn new(
⋮----
return Err(TpuSenderError::Custom(String::from(
⋮----
pub fn new_with_connection_cache(
⋮----
Ok(Self {
⋮----
pub fn send_and_confirm_messages_with_spinner<T: Signers + ?Sized>(
⋮----
.send_and_confirm_messages_with_spinner(messages, signers)
⋮----
pub fn rpc_client(&self) -> &RpcClient {
self.tpu_client.rpc_client()

================
File: client/src/transaction_executor.rs
================
type PendingQueue = Vec<(Signature, u64, u64)>;
pub struct TransactionExecutor {
⋮----
impl TransactionExecutor {
pub fn new(entrypoint_addr: SocketAddr) -> Self {
⋮----
pub fn new_with_url<U: ToString>(url: U) -> Self {
⋮----
pub fn new_with_rpc_client(client: Arc<RpcClient>) -> Self {
⋮----
let sig_clear_t = Self::start_sig_clear_thread(exit.clone(), &sigs, &cleared, &client);
⋮----
pub fn num_outstanding(&self) -> usize {
self.sigs.read().unwrap().len()
⋮----
pub fn push_transactions(&self, txs: Vec<Transaction>) -> Vec<u64> {
let mut ids = vec![];
let new_sigs = txs.into_iter().filter_map(|tx| {
let id = self.counter.fetch_add(1, Ordering::Relaxed);
ids.push(id);
match self.client.send_transaction(&tx) {
⋮----
return Some((sig, timestamp(), id));
⋮----
info!("error: {e:#?}");
⋮----
let mut sigs_w = self.sigs.write().unwrap();
sigs_w.extend(new_sigs);
⋮----
pub fn drain_cleared(&self) -> Vec<u64> {
std::mem::take(&mut *self.cleared.write().unwrap())
⋮----
pub fn close(self) {
self.exit.store(true, Ordering::Relaxed);
self.sig_clear_t.join().unwrap();
⋮----
fn start_sig_clear_thread(
⋮----
let sigs = sigs.clone();
let cleared = cleared.clone();
let client = client.clone();
⋮----
.name("solSigClear".to_string())
.spawn(move || {
⋮----
while !exit.load(Ordering::Relaxed) {
let sigs_len = sigs.read().unwrap().len();
⋮----
let mut sigs_w = sigs.write().unwrap();
⋮----
.chunks(200)
.flat_map(|sig_chunk| {
let only_sigs: Vec<_> = sig_chunk.iter().map(|s| s.0).collect();
⋮----
.get_signature_statuses(&only_sigs)
.expect("status fail")
⋮----
.collect();
⋮----
let start_len = sigs_w.len();
let now = timestamp();
let mut new_ids = vec![];
⋮----
while i != sigs_w.len() {
⋮----
debug!("error: {e:?}");
if e.status.is_ok() {
⋮----
new_ids.push(sigs_w.remove(i).2);
⋮----
let final_sigs_len = sigs_w.len();
drop(sigs_w);
cleared.write().unwrap().extend(new_ids);
start.stop();
debug!(
⋮----
if last_log.elapsed().as_millis() > 5000 {
info!(
⋮----
sleep(Duration::from_millis(200));
⋮----
.unwrap()

================
File: client/.gitignore
================
/target/
/farf/

================
File: client/Cargo.toml
================
[package]
name = "solana-client"
description = "Solana Client"
documentation = "https://docs.rs/solana-client"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
async-trait = { workspace = true }
bincode = { workspace = true }
dashmap = { workspace = true }
futures = { workspace = true }
futures-util = { workspace = true }
indexmap = { workspace = true }
indicatif = { workspace = true }
log = { workspace = true }
quinn = { workspace = true }
rayon = { workspace = true }
solana-account = { workspace = true }
solana-client-traits = { workspace = true }
solana-commitment-config = { workspace = true }
solana-connection-cache = { workspace = true }
solana-epoch-info = { workspace = true }
solana-hash = { workspace = true }
solana-instruction = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-message = { workspace = true }
solana-net-utils = { workspace = true }
solana-pubkey = { workspace = true }
solana-pubsub-client = { workspace = true }
solana-quic-client = { workspace = true }
solana-quic-definitions = { workspace = true }
solana-rpc-client = { workspace = true, features = ["default"] }
solana-rpc-client-api = { workspace = true }
solana-rpc-client-nonce-utils = { workspace = true }
solana-signature = { workspace = true }
solana-signer = { workspace = true }
solana-streamer = { workspace = true }
solana-time-utils = { workspace = true }
solana-tpu-client = { workspace = true, features = ["default"] }
solana-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
solana-transaction-status-client-types = { workspace = true }
solana-udp-client = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tokio-util = { workspace = true }

[dev-dependencies]
crossbeam-channel = { workspace = true }

================
File: client-test/tests/client.rs
================
fn pubsub_addr() -> SocketAddr {
⋮----
fn test_rpc_client() {
⋮----
TestValidator::with_no_fees(alice.pubkey(), None, SocketAddrSpace::Unspecified);
⋮----
let client = RpcClient::new(test_validator.rpc_url());
assert_eq!(
⋮----
assert!(client.get_account(&bob_pubkey).is_err());
assert_eq!(client.get_balance(&bob_pubkey).unwrap(), 0);
let original_alice_balance = client.get_balance(&alice.pubkey()).unwrap();
let blockhash = client.get_latest_blockhash().unwrap();
⋮----
let signature = client.send_transaction(&tx).unwrap();
⋮----
while now.elapsed().as_secs() <= 20 {
⋮----
.confirm_transaction_with_commitment(&signature, CommitmentConfig::processed())
.unwrap();
⋮----
sleep(Duration::from_millis(500));
⋮----
assert!(confirmed_tx);
⋮----
fn test_account_subscription() {
let pubsub_addr = pubsub_addr();
⋮----
} = create_genesis_config(10_000);
⋮----
let blockhash = bank.last_blockhash();
⋮----
let bank0 = bank_forks.read().unwrap().get(0).unwrap();
⋮----
bank_forks.write().unwrap().insert(bank1);
⋮----
exit.clone(),
⋮----
bank_forks.clone(),
⋮----
check_server_is_ready_or_panic(&pubsub_addr, 10, Duration::from_millis(300));
let config = Some(RpcAccountInfoConfig {
commitment: Some(CommitmentConfig::finalized()),
⋮----
format!("ws://0.0.0.0:{}/", pubsub_addr.port()),
&bob.pubkey(),
⋮----
let tx = system_transaction::transfer(&alice, &bob.pubkey(), 100, blockhash);
⋮----
.read()
.unwrap()
.get(1)
⋮----
.process_transaction(&tx)
⋮----
subscriptions.notify_subscribers(commitment_slots);
⋮----
let expected = json!({
⋮----
let response = receiver.recv();
⋮----
let actual = serde_json::to_value(response).unwrap();
⋮----
errors.push((expected, actual));
⋮----
Err(_) => eprintln!("unexpected websocket receive timeout"),
⋮----
exit.store(true, Ordering::Relaxed);
trigger.cancel();
client.shutdown().unwrap();
pubsub_service.close().unwrap();
assert_eq!(errors, [].to_vec());
⋮----
fn test_block_subscription() {
⋮----
let rent_exempt_amount = bank.get_minimum_balance_for_rent_exemption(0);
⋮----
let ledger_path = get_tmp_ledger_path_auto_delete!();
let blockstore = Blockstore::open(ledger_path.path()).unwrap();
⋮----
let bank = bank_forks.read().unwrap().working_bank();
⋮----
let max_complete_transaction_status_slot = Arc::new(AtomicU64::new(blockstore.max_root()));
bank.transfer(rent_exempt_amount, &alice, &keypair2.pubkey())
⋮----
populate_blockstore_for_tests(
create_test_transaction_entries(
vec![&alice, &keypair1, &keypair2, &keypair3],
bank.clone(),
⋮----
blockstore.clone(),
⋮----
Some(RpcBlockSubscribeConfig {
commitment: Some(CommitmentConfig {
⋮----
encoding: Some(UiTransactionEncoding::Json),
transaction_details: Some(TransactionDetails::Signatures),
⋮----
let slot = bank_forks.read().unwrap().highest_slot();
subscriptions.notify_gossip_subscribers(slot);
let maybe_actual = receiver.recv_timeout(Duration::from_millis(400));
⋮----
let versioned_block = blockstore.get_complete_block(slot, false).unwrap();
⋮----
.encode_with_options(
⋮----
assert_eq!(actual.value.slot, slot);
assert!(block.eq(&actual.value.block.unwrap()));
⋮----
eprintln!("unexpected websocket receive timeout");
assert_eq!(Some(e), None);
⋮----
fn test_program_subscription() {
⋮----
let config = Some(RpcProgramAccountsConfig {
⋮----
let response = receiver.recv_timeout(Duration::from_millis(100));
⋮----
notifications.push(response.clone());
pubkeys.insert(response.value.pubkey);
⋮----
assert_eq!(notifications.len(), 1);
assert!(pubkeys.contains(&bob.pubkey().to_string()));
⋮----
fn test_root_subscription() {
⋮----
let GenesisConfigInfo { genesis_config, .. } = create_genesis_config(10_000);
⋮----
PubsubClient::root_subscribe(format!("ws://0.0.0.0:{}/", pubsub_addr.port())).unwrap();
let roots = vec![1, 2, 3];
subscriptions.notify_roots(roots.clone());
⋮----
errors.push((expected, response));
⋮----
fn test_slot_subscription() {
⋮----
let client_request = format!("ws://0.0.0.0:{}/", pubsub_addr.port())
.into_client_request()
.map_err(Box::new)
⋮----
let (mut client, receiver) = PubsubClient::slot_subscribe(client_request).unwrap();
⋮----
subscriptions.notify_slot(i + 1, i, i);
⋮----
errors.push((actual, expected));
⋮----
async fn test_slot_subscription_async() {
⋮----
fn wait_until(atomic: &Arc<AtomicU64>, value: u64) {
⋮----
while atomic.load(Ordering::Relaxed) != value {
if now.elapsed() > Duration::from_secs(5) {
panic!("wait for too long")
⋮----
sleep(Duration::from_millis(1))
⋮----
check_server_is_ready_or_panic(&pubsub_addr, 10, Duration::from_millis(100));
sync_service.store(1, Ordering::Relaxed);
wait_until(&sync_service, 2);
subscriptions.notify_slot(1, 0, 0);
sync_service.store(3, Ordering::Relaxed);
wait_until(&sync_service, 4);
subscriptions.notify_slot(2, 1, 1);
sync_service.store(5, Ordering::Relaxed);
wait_until(&sync_service, 6);
⋮----
wait_until(&sync_client, 1);
let url = format!("ws://0.0.0.0:{}/", pubsub_addr.port());
⋮----
let (mut notifications, unsubscribe) = pubsub_client.slot_subscribe().await.unwrap();
sync_client.store(2, Ordering::Relaxed);
wait_until(&sync_client, 3);
⋮----
sync_client.store(4, Ordering::Relaxed);
wait_until(&sync_client, 5);
⋮----
sync_client.store(6, Ordering::Relaxed);
unsubscribe().await;
⋮----
fn check_server_is_ready_or_panic(
⋮----
retry = retry.checked_sub(1).unwrap();
⋮----
if connect(format!("ws://{socket_addr}")).is_ok() {
⋮----
sleep(sleep_duration);
⋮----
panic!("server hasn't been ready");

================
File: client-test/tests/send_and_confirm_transactions_in_parallel.rs
================
fn create_messages(from: Pubkey, to: Pubkey) -> (Vec<Message>, u64) {
let mut messages = vec![];
⋮----
let amount_to_transfer = (i as u64).checked_mul(LAMPORTS_PER_SOL).unwrap();
⋮----
let message = Message::new(&[ix], Some(&from));
messages.push(message);
sum = sum.checked_add(amount_to_transfer).unwrap();
⋮----
fn test_send_and_confirm_transactions_in_parallel_without_tpu_client() {
⋮----
TestValidator::with_no_fees(alice.pubkey(), None, SocketAddrSpace::Unspecified);
⋮----
let alice_pubkey = alice.pubkey();
let rpc_client = Arc::new(RpcClient::new(test_validator.rpc_url()));
assert_eq!(
⋮----
let original_alice_balance = rpc_client.get_balance(&alice.pubkey()).unwrap();
let (messages, sum) = create_messages(alice_pubkey, bob_pubkey);
let txs_errors = send_and_confirm_transactions_in_parallel_blocking_v2(
rpc_client.clone(),
⋮----
resign_txs_count: Some(5),
⋮----
preflight_commitment: Some(CommitmentConfig::confirmed().commitment),
⋮----
assert!(txs_errors.is_ok());
assert!(txs_errors.unwrap().iter().all(|x| x.is_none()));
⋮----
fn test_send_and_confirm_transactions_in_parallel_with_tpu_client() {
⋮----
let ws_url = test_validator.rpc_pubsub_url();
⋮----
rpc_client.get_inner_client().clone(),
ws_url.as_str(),
⋮----
let tpu_client = rpc_client.runtime().block_on(tpu_client_fut).unwrap();
⋮----
Some(tpu_client),

================
File: client-test/.gitignore
================
/target/
/farf/

================
File: client-test/Cargo.toml
================
[package]
name = "solana-client-test"
description = "Solana RPC Test"
documentation = "https://docs.rs/solana-client-test"
publish = false
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[features]
agave-unstable-api = []

[dependencies]
futures-util = { workspace = true }
serde_json = { workspace = true }
solana-client = { workspace = true }
solana-clock = { workspace = true }
solana-commitment-config = { workspace = true }
solana-keypair = { workspace = true }
solana-ledger = { workspace = true }
solana-measure = { workspace = true }
solana-merkle-tree = { workspace = true }
solana-message = { workspace = true }
solana-native-token = { workspace = true }
solana-perf = { workspace = true }
solana-pubkey = { workspace = true }
solana-pubsub-client = { workspace = true }
solana-rayon-threadlimit = { workspace = true }
solana-rpc = { workspace = true }
solana-rpc-client = { workspace = true }
solana-rpc-client-api = { workspace = true }
solana-runtime = { workspace = true }
solana-signer = { workspace = true }
solana-system-interface = { workspace = true }
solana-system-transaction = { workspace = true }
solana-test-validator = { workspace = true }
solana-transaction-status = { workspace = true }
solana-version = { workspace = true }
systemstat = { workspace = true }
tokio = { workspace = true, features = ["full"] }
tungstenite = { workspace = true, features = ["rustls-tls-webpki-roots"] }

[dev-dependencies]
agave-logger = { workspace = true }
solana-net-utils = { workspace = true }
solana-rpc = { workspace = true, features = ["dev-context-only-utils"] }
solana-runtime = { workspace = true, features = ["dev-context-only-utils"] }

================
File: clippy.toml
================
too-many-arguments-threshold = 9

# Disallow specific methods from being used
disallowed-methods = [
    { path = "std::net::UdpSocket::bind", reason = "Use solana_net_utils::bind_with_config, bind_to, etc instead for proper socket configuration." },
    { path = "tokio::net::UdpSocket::bind", reason = "Use solana_net_utils::bind_to_async or bind_to_with_config_non_blocking instead for proper socket configuration." },
    { path = "lazy_static::initialize", reason = "Deprecated. Use std::{cell::{LazyCell, OnceCell}, sync::{LazyLock, OnceLock}} as appropriate." }
]

disallowed-macros = [
    { path = "solana_sdk::saturating_add_assign", reason = "Deprecated. Use std::num::Saturating<T> instead." },
    { path = "lazy_static::lazy_static", reason = "Deprecated. Use std::{cell::{LazyCell, OnceCell}, sync::{LazyLock, OnceLock}} as appropriate." },
]

================
File: compute-budget/src/compute_budget_limits.rs
================
type MicroLamports = u128;
⋮----
pub struct ComputeBudgetLimits {
⋮----
impl Default for ComputeBudgetLimits {
fn default() -> Self {
⋮----
impl ComputeBudgetLimits {
pub fn get_compute_budget_and_limits(
⋮----
pub fn get_prioritization_fee(&self) -> u64 {
get_prioritization_fee(self.compute_unit_price, u64::from(self.compute_unit_limit))
⋮----
fn get_prioritization_fee(compute_unit_price: u64, compute_unit_limit: u64) -> u64 {
⋮----
(compute_unit_price as u128).saturating_mul(compute_unit_limit as u128);
⋮----
.saturating_add(MICRO_LAMPORTS_PER_LAMPORT.saturating_sub(1) as u128)
.checked_div(MICRO_LAMPORTS_PER_LAMPORT as u128)
.and_then(|fee| u64::try_from(fee).ok())
.unwrap_or(u64::MAX)
⋮----
fn from(val: ComputeBudgetLimits) -> Self {
⋮----
get_prioritization_fee(val.compute_unit_price, u64::from(val.compute_unit_limit));
⋮----
fn from(val: &ComputeBudgetLimits) -> Self {
⋮----
mod test {
⋮----
fn test_new_with_no_fee() {
⋮----
assert_eq!(get_prioritization_fee(0, compute_units), 0);
⋮----
fn test_new_with_compute_unit_price() {
assert_eq!(
⋮----
assert_eq!(get_prioritization_fee(MICRO_LAMPORTS_PER_LAMPORT, 1), 1);
⋮----
assert_eq!(get_prioritization_fee(200, 100_000), 20);
⋮----
assert_eq!(get_prioritization_fee(u64::MAX, u64::MAX), u64::MAX);

================
File: compute-budget/src/compute_budget.rs
================
pub struct ComputeBudget {
⋮----
impl Default for ComputeBudget {
fn default() -> Self {
⋮----
impl ComputeBudget {
pub fn new_with_defaults(simd_0268_active: bool, simd_0339_active: bool) -> Self {
⋮----
pub fn from_budget_and_cost(
⋮----
pub fn to_budget(&self) -> SVMTransactionExecutionBudget {
⋮----
pub fn to_cost(&self) -> SVMTransactionExecutionCost {
⋮----
pub fn get_compute_budget_and_limits(
⋮----
budget: self.to_budget(),

================
File: compute-budget/src/lib.rs
================
pub mod compute_budget;
pub mod compute_budget_limits;

================
File: compute-budget/Cargo.toml
================
[package]
name = "solana-compute-budget"
description = "Solana compute budget"
documentation = "https://docs.rs/solana-compute-budget"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []
dev-context-only-utils = [
    "dep:qualifier_attr",
    "solana-program-runtime/dev-context-only-utils",
]
frozen-abi = ["dep:solana-frozen-abi", "solana-fee-structure/frozen-abi"]

[dependencies]
qualifier_attr = { workspace = true, optional = true }
solana-fee-structure = { workspace = true }
solana-frozen-abi = { workspace = true, optional = true, features = [
    "frozen-abi",
] }
solana-program-runtime = { workspace = true }

[lints]
workspace = true

================
File: compute-budget-instruction/benches/process_compute_budget_instructions.rs
================
fn build_sanitized_transaction(
⋮----
Some(&payer_keypair.pubkey()),
⋮----
fn bench_process_compute_budget_instructions_empty(c: &mut Criterion) {
⋮----
c.benchmark_group("bench_process_compute_budget_instructions_empty")
.throughput(Throughput::Elements(NUM_TRANSACTIONS_PER_ITER as u64))
.bench_function("0 instructions", |bencher| {
let tx = build_sanitized_transaction(&Keypair::new(), &[]);
bencher.iter(|| {
(0..NUM_TRANSACTIONS_PER_ITER).for_each(|_| {
assert!(process_compute_budget_instructions(
⋮----
fn bench_process_compute_budget_instructions_no_builtins(c: &mut Criterion) {
⋮----
c.benchmark_group("bench_process_compute_budget_instructions_no_builtins")
⋮----
.bench_function(
format!("{num_instructions} dummy Instructions"),
⋮----
.map(|_| {
⋮----
DUMMY_PROGRAM_ID.parse().unwrap(),
⋮----
vec![],
⋮----
.collect();
let tx = build_sanitized_transaction(&Keypair::new(), &ixs);
⋮----
fn bench_process_compute_budget_instructions_compute_budgets(c: &mut Criterion) {
⋮----
c.benchmark_group("bench_process_compute_budget_instructions_compute_budgets")
⋮----
.bench_function("4 compute-budget instructions", |bencher| {
let ixs = vec![
⋮----
fn bench_process_compute_budget_instructions_builtins(c: &mut Criterion) {
⋮----
c.benchmark_group("bench_process_compute_budget_instructions_builtins")
⋮----
.bench_function("4 dummy builtins", |bencher| {
⋮----
fn bench_process_compute_budget_instructions_mixed(c: &mut Criterion) {
⋮----
c.benchmark_group("bench_process_compute_budget_instructions_mixed")
⋮----
format!("{num_instructions} mixed instructions"),
⋮----
ixs.extend(vec![
⋮----
let tx = build_sanitized_transaction(&payer_keypair, &ixs);
⋮----
criterion_group!(
⋮----
criterion_main!(benches);

================
File: compute-budget-instruction/src/builtin_programs_filter.rs
================
pub(crate) enum ProgramKind {
⋮----
pub(crate) struct BuiltinProgramsFilter {
⋮----
impl BuiltinProgramsFilter {
pub(crate) fn new() -> Self {
⋮----
pub(crate) fn get_program_kind(&mut self, index: usize, program_id: &Pubkey) -> ProgramKind {
⋮----
.get_mut(index)
.expect("program id index is sanitized")
.get_or_insert_with(|| Self::check_program_kind(program_id))
⋮----
fn check_program_kind(program_id: &Pubkey) -> ProgramKind {
if !MAYBE_BUILTIN_KEY[program_id.as_ref()[0] as usize] {
⋮----
match get_builtin_migration_feature_index(program_id) {
⋮----
mod test {
⋮----
fn get_program_kind() {
⋮----
assert!(test_store.program_kind[index].is_none());
assert_eq!(
⋮----
panic!("MIGRATING_BUILTINS_COSTS must only contain BuiltinCost::Migrating");
⋮----
fn test_get_program_kind_out_of_bound_index() {

================
File: compute-budget-instruction/src/compute_budget_instruction_details.rs
================
struct MigrationBuiltinFeatureCounter {
migrating_builtin: [Saturating<u16>; MIGRATING_BUILTINS_COSTS.len()],
⋮----
impl Default for MigrationBuiltinFeatureCounter {
fn default() -> Self {
⋮----
migrating_builtin: [Saturating(0); MIGRATING_BUILTINS_COSTS.len()],
⋮----
pub struct ComputeBudgetInstructionDetails {
⋮----
impl ComputeBudgetInstructionDetails {
pub fn try_from<'a>(
⋮----
for (i, (program_id, instruction)) in instructions.clone().enumerate() {
if filter.is_compute_budget_program(instruction.program_id_index as usize, program_id) {
compute_budget_instruction_details.process_instruction(i as u8, &instruction)?;
⋮----
.is_none()
⋮----
// reiterate to collect builtin details
⋮----
match filter.get_program_kind(instruction.program_id_index as usize, program_id) {
⋮----
.get_mut(core_bpf_migration_feature_index)
.expect(
⋮----
Ok(compute_budget_instruction_details)
⋮----
pub fn sanitize_and_convert_to_compute_budget_limits(
⋮----
// Sanitize requested heap size
⋮----
return Err(TransactionError::InstructionError(
⋮----
.min(MAX_HEAP_FRAME_BYTES);
// Calculate compute unit limit
⋮----
.map_or_else(
|| self.calculate_default_compute_unit_limit(feature_set),
⋮----
.min(MAX_COMPUTE_UNIT_LIMIT);
⋮----
.map_or(0, |(_index, requested_compute_unit_price)| {
⋮----
.ok_or(TransactionError::InvalidLoadedAccountsDataSizeLimit)?
⋮----
.min(MAX_LOADED_ACCOUNTS_DATA_SIZE_BYTES);
Ok(ComputeBudgetLimits {
⋮----
fn process_instruction(&mut self, index: u8, instruction: &SVMInstruction) -> Result<()> {
⋮----
match try_from_slice_unchecked(instruction.data) {
⋮----
if self.requested_heap_size.is_some() {
return Err(duplicate_instruction_error);
⋮----
self.requested_heap_size = Some((index, bytes));
⋮----
if self.requested_compute_unit_limit.is_some() {
⋮----
self.requested_compute_unit_limit = Some((index, compute_unit_limit));
⋮----
if self.requested_compute_unit_price.is_some() {
⋮----
self.requested_compute_unit_price = Some((index, micro_lamports));
⋮----
if self.requested_loaded_accounts_data_size_limit.is_some() {
⋮----
self.requested_loaded_accounts_data_size_limit = Some((index, bytes));
⋮----
_ => return Err(invalid_instruction_data_error),
⋮----
Ok(())
⋮----
fn sanitize_requested_heap_size(bytes: u32) -> bool {
(MIN_HEAP_FRAME_BYTES..=MAX_HEAP_FRAME_BYTES).contains(&bytes) && bytes.is_multiple_of(1024)
⋮----
fn calculate_default_compute_unit_limit(&self, feature_set: &FeatureSet) -> u32 {
// evaluate if any builtin has migrated with feature_set
⋮----
.iter()
.enumerate()
.fold((0, 0), |(migrated, not_migrated), (index, count)| {
if count.0 > 0 && feature_set.is_active(get_migration_feature_id(index)) {
⋮----
.saturating_add(u32::from(num_not_migrated))
.saturating_mul(MAX_BUILTIN_ALLOCATION_COMPUTE_UNIT_LIMIT)
.saturating_add(
⋮----
.saturating_add(u32::from(num_migrated))
.saturating_mul(DEFAULT_INSTRUCTION_COMPUTE_UNIT_LIMIT),
⋮----
mod test {
⋮----
fn build_sanitized_transaction(instructions: &[Instruction]) -> SanitizedTransaction {
⋮----
Some(&payer_keypair.pubkey()),
⋮----
fn test_try_from_request_heap() {
let tx = build_sanitized_transaction(&[
Instruction::new_with_bincode(Pubkey::new_unique(), &(), vec![]),
⋮----
let expected_details = Ok(ComputeBudgetInstructionDetails {
requested_heap_size: Some((1, 40 * 1024)),
num_non_compute_budget_instructions: Saturating(2),
num_non_migratable_builtin_instructions: Saturating(1),
num_non_builtin_instructions: Saturating(2),
⋮----
assert_eq!(
⋮----
fn test_try_from_compute_unit_limit() {
⋮----
requested_compute_unit_limit: Some((1, u32::MAX)),
⋮----
fn test_try_from_compute_unit_price() {
⋮----
requested_compute_unit_price: Some((1, u64::MAX)),
⋮----
fn test_try_from_loaded_accounts_data_size_limit() {
⋮----
requested_loaded_accounts_data_size_limit: Some((1, u32::MAX)),
⋮----
fn prep_feature_minimial_cus_for_builtin_instructions(
⋮----
fn test_sanitize_and_convert_to_compute_budget_limits() {
⋮----
// empty details, default ComputeBudgetLimits with 0 compute_unit_limits
⋮----
// no compute-budget instructions, all default ComputeBudgetLimits except cu-limit
⋮----
num_non_compute_budget_instructions: Saturating(4),
⋮----
num_non_builtin_instructions: Saturating(3),
⋮----
prep_feature_minimial_cus_for_builtin_instructions(&instruction_details);
⋮----
let expected_heap_size_err = Err(TransactionError::InstructionError(
⋮----
// invalid: requested_heap_size can't be zero
⋮----
requested_compute_unit_limit: Some((1, 0)),
requested_compute_unit_price: Some((2, 0)),
requested_heap_size: Some((3, 0)),
requested_loaded_accounts_data_size_limit: Some((4, 1024)),
⋮----
requested_heap_size: Some((3, MIN_HEAP_FRAME_BYTES - 1)),
⋮----
requested_heap_size: Some((3, MAX_HEAP_FRAME_BYTES + 1)),
⋮----
requested_heap_size: Some((3, MIN_HEAP_FRAME_BYTES + 1024 + 1)),
⋮----
requested_heap_size: Some((3, 40 * 1024)),
requested_loaded_accounts_data_size_limit: Some((4, 0)),
⋮----
requested_compute_unit_price: Some((2, u64::MAX)),
requested_heap_size: Some((3, MAX_HEAP_FRAME_BYTES)),
requested_loaded_accounts_data_size_limit: Some((4, u32::MAX)),
⋮----
requested_compute_unit_limit: Some((1, val)),
requested_compute_unit_price: Some((2, val as u64)),
requested_heap_size: Some((3, val)),
requested_loaded_accounts_data_size_limit: Some((4, val)),
⋮----
fn test_builtin_program_migration() {
⋮----
panic!("MIGRATING_BUILTINS_COSTS must only contain BuiltinCost::Migrating");
⋮----
assert_eq!(get_migration_feature_id(*position), feature_id);
assert_eq!(get_migration_feature_position(feature_id), *position);
⋮----
Instruction::new_with_bincode(*program_id, &(), vec![]),
⋮----
num_non_builtin_instructions: Saturating(1),
⋮----
.migrating_builtin[*position] = Saturating(1);
let expected_details = Ok(expected_details);
⋮----
assert_eq!(details, expected_details);
let details = details.unwrap();
⋮----
let cu_limits = details.sanitize_and_convert_to_compute_budget_limits(&feature_set);
⋮----
feature_set.activate(feature_id, 0);

================
File: compute-budget-instruction/src/compute_budget_program_id_filter.rs
================
pub(crate) struct ComputeBudgetProgramIdFilter {
⋮----
impl ComputeBudgetProgramIdFilter {
pub(crate) fn new() -> Self {
⋮----
pub(crate) fn is_compute_budget_program(&mut self, index: usize, program_id: &Pubkey) -> bool {
⋮----
.get_mut(index)
.expect("program id index is sanitized")
.get_or_insert_with(|| Self::check_program_id(program_id))
⋮----
fn check_program_id(program_id: &Pubkey) -> bool {
if !MAYBE_BUILTIN_KEY[program_id.as_ref()[0] as usize] {

================
File: compute-budget-instruction/src/instructions_processor.rs
================
pub fn process_compute_budget_instructions<'a>(
⋮----
.sanitize_and_convert_to_compute_budget_limits(feature_set)
⋮----
mod tests {
⋮----
macro_rules! test {
⋮----
fn test_process_instructions() {
test!(
⋮----
fn test_process_loaded_accounts_data_size_limit_instruction() {
⋮----
let expected_result = Ok(ComputeBudgetLimits {
⋮----
loaded_accounts_bytes: NonZeroU32::new(data_size).unwrap(),
⋮----
let data_size = MAX_LOADED_ACCOUNTS_DATA_SIZE_BYTES.get() + 1;
⋮----
let data_size = MAX_LOADED_ACCOUNTS_DATA_SIZE_BYTES.get();
let expected_result = Err(TransactionError::DuplicateInstruction(2));
⋮----
fn test_process_mixed_instructions_without_compute_budget() {
⋮----
Instruction::new_with_bincode(Pubkey::new_unique(), &0_u8, vec![]),
transfer(&payer_keypair.pubkey(), &Pubkey::new_unique(), 2),
⋮----
Some(&payer_keypair.pubkey()),
⋮----
Ok(ComputeBudgetLimits {
⋮----
let result = process_compute_budget_instructions(
⋮----
assert_eq!(result, expected_result);

================
File: compute-budget-instruction/src/lib.rs
================
mod builtin_programs_filter;
pub mod compute_budget_instruction_details;
mod compute_budget_program_id_filter;
pub mod instructions_processor;

================
File: compute-budget-instruction/Cargo.toml
================
[package]
name = "solana-compute-budget-instruction"
description = "Solana Compute Budget Instruction"
documentation = "https://docs.rs/solana-compute-budget-instruction"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[package.metadata.docs.rs]
targets = ["x86_64-unknown-linux-gnu"]

[lib]
crate-type = ["lib"]
name = "solana_compute_budget_instruction"

[features]
agave-unstable-api = []
dev-context-only-utils = []

[dependencies]
agave-feature-set = { workspace = true }
log = { workspace = true }
solana-borsh = { workspace = true }
solana-builtins-default-costs = { workspace = true }
solana-compute-budget = { workspace = true }
solana-compute-budget-interface = { workspace = true, features = ["borsh"] }
solana-instruction = { workspace = true }
solana-packet = { workspace = true }
solana-pubkey = { workspace = true }
solana-sdk-ids = { workspace = true }
solana-svm-transaction = { workspace = true }
solana-transaction-error = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
bincode = { workspace = true }
criterion = { workspace = true }
rand = { workspace = true }
solana-builtins-default-costs = { workspace = true, features = ["dev-context-only-utils"] }
solana-compute-budget-instruction = { path = ".", features = ["agave-unstable-api"] }
solana-hash = { workspace = true }
solana-keypair = { workspace = true }
solana-message = { workspace = true }
solana-signer = { workspace = true }
solana-stake-interface = { workspace = true }
solana-system-interface = { workspace = true }
solana-transaction = { workspace = true, features = ["blake3"] }

[[bench]]
name = "process_compute_budget_instructions"
harness = false

[lints]
workspace = true

================
File: connection-cache/src/nonblocking/client_connection.rs
================
pub trait ClientConnection {

================
File: connection-cache/src/nonblocking/mod.rs
================
pub mod client_connection;

================
File: connection-cache/src/client_connection.rs
================
pub struct ClientStats {
⋮----
pub trait ClientConnection: Sync + Send {

================
File: connection-cache/src/connection_cache_stats.rs
================
pub struct ConnectionCacheStats {
⋮----
impl ConnectionCacheStats {
pub fn add_client_stats(
⋮----
self.total_client_stats.total_connections.fetch_add(
client_stats.total_connections.load(Ordering::Relaxed),
⋮----
self.total_client_stats.connection_reuse.fetch_add(
client_stats.connection_reuse.load(Ordering::Relaxed),
⋮----
self.total_client_stats.connection_errors.fetch_add(
client_stats.connection_errors.load(Ordering::Relaxed),
⋮----
self.total_client_stats.zero_rtt_accepts.fetch_add(
client_stats.zero_rtt_accepts.load(Ordering::Relaxed),
⋮----
self.total_client_stats.zero_rtt_rejects.fetch_add(
client_stats.zero_rtt_rejects.load(Ordering::Relaxed),
⋮----
self.total_client_stats.make_connection_ms.fetch_add(
client_stats.make_connection_ms.load(Ordering::Relaxed),
⋮----
self.total_client_stats.send_timeout.fetch_add(
client_stats.send_timeout.load(Ordering::Relaxed),
⋮----
self.total_client_stats.send_packets_us.fetch_add(
client_stats.send_packets_us.load(Ordering::Relaxed),
⋮----
self.total_client_stats.successful_packets.fetch_add(
client_stats.successful_packets.load(Ordering::Relaxed),
⋮----
self.total_client_stats.prepare_connection_us.fetch_add(
client_stats.prepare_connection_us.load(Ordering::Relaxed),
⋮----
.fetch_add(num_packets as u64, Ordering::Relaxed);
self.total_batches.fetch_add(1, Ordering::Relaxed);
⋮----
self.batch_success.fetch_add(1, Ordering::Relaxed);
⋮----
self.batch_failure.fetch_add(1, Ordering::Relaxed);
⋮----
pub(super) fn report(&self, name: &'static str) {
⋮----
.swap(0, Ordering::Relaxed);
⋮----
.swap(0, Ordering::Relaxed)
⋮----
datapoint_info!(

================
File: connection-cache/src/connection_cache.rs
================
pub enum Protocol {
⋮----
pub trait ConnectionManager: Send + Sync + 'static {
⋮----
pub struct ConnectionCache<
R, // ConnectionPool
S, // ConnectionManager
T, // NewConnectionConfig
⋮----
pub fn new(
⋮----
let config = connection_manager.new_connection_config();
Ok(Self::new_with_config(
⋮----
pub fn new_with_config(
⋮----
info!("Creating ConnectionCache {name}, pool size: {connection_pool_size}");
⋮----
let connection_pool_size = 1.max(connection_pool_size);
⋮----
Self::create_connection_async_thread(map.clone(), receiver, stats.clone());
⋮----
fn create_connection_async_thread(
⋮----
.name("solQAsynCon".to_string())
.spawn(move || loop {
let recv_result = receiver.recv();
⋮----
let map = map.read().unwrap();
let pool = map.get(&addr);
⋮----
let conn = pool.get(idx);
⋮----
drop(map);
let conn = conn.new_blocking_connection(addr, stats.clone());
let result = conn.send_data(&[]);
debug!("Create async connection result {result:?} for {addr}");
⋮----
.unwrap()
⋮----
pub fn update_key(&self, key: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
let mut map = self.map.write().unwrap();
map.clear();
self.connection_manager.update_key(key)
⋮----
fn create_connection(
⋮----
get_connection_map_lock_measure.stop();
*lock_timing_ms = lock_timing_ms.saturating_add(get_connection_map_lock_measure.as_ms());
⋮----
.get(addr)
.map(|pool| pool.check_pool_status(self.connection_pool_size))
.unwrap_or(PoolStatus::Empty);
⋮----
if matches!(pool_status, PoolStatus::Empty) {
⋮----
if matches!(pool_status, PoolStatus::PartiallyFull) {
debug!("Triggering async connection for {addr:?}");
⋮----
Some(&self.sender),
⋮----
let pool = map.get(addr).unwrap();
let connection = pool.borrow_connection();
⋮----
connection_cache_stats: self.stats.clone(),
⋮----
fn create_connection_internal(
⋮----
// evict a connection if the cache is reaching upper bounds
⋮----
let existing_index = map.get_index_of(addr);
while map.len() >= MAX_CONNECTIONS {
let mut rng = rng();
let n = rng.random_range(0..MAX_CONNECTIONS);
⋮----
map.swap_remove_index(n);
⋮----
get_connection_cache_eviction_measure.stop();
⋮----
map.entry(*addr)
.and_modify(|pool| {
if matches!(
⋮----
let idx = pool.add_connection(config, addr);
⋮----
debug!(
⋮----
sender.send((idx, *addr)).unwrap();
⋮----
.or_insert_with(|| {
let mut pool = connection_manager.new_connection_pool();
pool.add_connection(config, addr);
⋮----
get_connection_cache_eviction_measure.as_ms(),
⋮----
fn get_or_add_connection(
⋮----
let map = self.map.read().unwrap();
⋮----
let mut lock_timing_ms = get_connection_map_lock_measure.as_ms();
⋮----
.should_update(CONNECTION_STAT_SUBMISSION_INTERVAL);
⋮----
} = match map.get(addr) {
⋮----
let pool_status = pool.check_pool_status(self.connection_pool_size);
⋮----
// create more connection and put it in the pool
⋮----
self.create_connection(&mut lock_timing_ms, addr)
⋮----
debug!("Creating connection async for {addr}");
⋮----
// Upgrade to write access by dropping read lock and acquire write lock
⋮----
get_connection_map_measure.stop();
⋮----
map_timing_ms: get_connection_map_measure.as_ms(),
⋮----
fn get_connection_and_log_stats(
⋮----
} = self.get_or_add_connection(addr);
⋮----
connection_cache_stats.report(self.name);
⋮----
.fetch_add(1, Ordering::Relaxed);
⋮----
.fetch_add(map_timing_ms, Ordering::Relaxed);
⋮----
.fetch_add(num_evictions, Ordering::Relaxed);
⋮----
.fetch_add(eviction_timing_ms, Ordering::Relaxed);
⋮----
get_connection_measure.stop();
⋮----
.fetch_add(lock_timing_ms, Ordering::Relaxed);
⋮----
.fetch_add(get_connection_measure.as_ms(), Ordering::Relaxed);
⋮----
pub fn get_connection(&self, addr: &SocketAddr) -> Arc<<<P as ConnectionPool>::BaseClientConnection as BaseClientConnection>::BlockingClientConnection>{
let (connection, connection_cache_stats) = self.get_connection_and_log_stats(addr);
connection.new_blocking_connection(*addr, connection_cache_stats)
⋮----
pub fn get_nonblocking_connection(
⋮----
connection.new_nonblocking_connection(*addr, connection_cache_stats)
⋮----
pub enum ConnectionPoolError {
⋮----
pub enum ClientError {
⋮----
pub trait NewConnectionConfig: Sized + Send + Sync + 'static {
⋮----
pub enum PoolStatus {
⋮----
pub trait ConnectionPool: Send + Sync + 'static {
⋮----
/// Add a connection to the pool and return its index
    fn add_connection(&mut self, config: &Self::NewConnectionConfig, addr: &SocketAddr) -> usize;
/// Get the number of current connections in the pool
    fn num_connections(&self) -> usize;
/// Get a connection based on its index in the pool, without checking if the
    fn get(&self, index: usize) -> Result<Arc<Self::BaseClientConnection>, ConnectionPoolError>;
/// Get a connection from the pool. It must have at least one connection in the pool.
    /// This randomly picks a connection in the pool.
⋮----
/// This randomly picks a connection in the pool.
    fn borrow_connection(&self) -> Arc<Self::BaseClientConnection> {
⋮----
fn borrow_connection(&self) -> Arc<Self::BaseClientConnection> {
⋮----
let n = rng.random_range(0..self.num_connections());
self.get(n).expect("index is within num_connections")
⋮----
/// Check if we need to create a new connection. If the count of the connections
    /// is smaller than the pool size and if there is no connection at all.
⋮----
/// is smaller than the pool size and if there is no connection at all.
    fn check_pool_status(&self, required_pool_size: usize) -> PoolStatus {
⋮----
fn check_pool_status(&self, required_pool_size: usize) -> PoolStatus {
if self.num_connections() == 0 {
⋮----
} else if self.num_connections() < required_pool_size {
⋮----
pub trait BaseClientConnection {
⋮----
struct GetConnectionResult<T> {
connection: Arc</*BaseClientConnection:*/ T>,
⋮----
struct CreateConnectionResult<T> {
⋮----
mod tests {
⋮----
struct MockUdpPool {
⋮----
impl ConnectionPool for MockUdpPool {
type NewConnectionConfig = MockUdpConfig;
type BaseClientConnection = MockUdp;
/// Add a connection into the pool and return its index in the pool.
        fn add_connection(
⋮----
fn add_connection(
⋮----
let connection = self.create_pool_entry(config, addr);
let idx = self.connections.len();
self.connections.push(connection);
⋮----
fn num_connections(&self) -> usize {
self.connections.len()
⋮----
fn get(
⋮----
.get(index)
.cloned()
.ok_or(ConnectionPoolError::IndexOutOfRange)
⋮----
fn create_pool_entry(
⋮----
Arc::new(MockUdp(config.udp_socket.clone()))
⋮----
struct MockUdpConfig {
⋮----
impl Default for MockUdpConfig {
fn default() -> Self {
⋮----
udp_socket: Arc::new(bind_to_localhost_unique().unwrap()),
⋮----
impl NewConnectionConfig for MockUdpConfig {
fn new() -> Result<Self, ClientError> {
Ok(Self {
⋮----
bind_to_localhost_unique().map_err(Into::<ClientError>::into)?,
⋮----
struct MockUdp(Arc<UdpSocket>);
impl BaseClientConnection for MockUdp {
type BlockingClientConnection = MockUdpConnection;
type NonblockingClientConnection = MockUdpConnection;
fn new_blocking_connection(
⋮----
_socket: self.0.clone(),
⋮----
fn new_nonblocking_connection(
⋮----
struct MockUdpConnection {
⋮----
struct MockConnectionManager {}
impl ConnectionManager for MockConnectionManager {
type ConnectionPool = MockUdpPool;
⋮----
fn new_connection_pool(&self) -> Self::ConnectionPool {
⋮----
fn new_connection_config(&self) -> Self::NewConnectionConfig {
MockUdpConfig::new().unwrap()
⋮----
fn update_key(&self, _key: &Keypair) -> Result<(), Box<dyn std::error::Error>> {
Ok(())
⋮----
impl BlockingClientConnection for MockUdpConnection {
fn server_addr(&self) -> &SocketAddr {
⋮----
fn send_data(&self, _buffer: &[u8]) -> TransportResult<()> {
unimplemented!()
⋮----
fn send_data_async(&self, _data: Arc<Vec<u8>>) -> TransportResult<()> {
⋮----
fn send_data_batch(&self, _buffers: &[Vec<u8>]) -> TransportResult<()> {
⋮----
fn send_data_batch_async(&self, _buffers: Vec<Vec<u8>>) -> TransportResult<()> {
⋮----
impl NonblockingClientConnection for MockUdpConnection {
⋮----
async fn send_data(&self, _data: &[u8]) -> TransportResult<()> {
⋮----
async fn send_data_batch(&self, _buffers: &[Vec<u8>]) -> TransportResult<()> {
⋮----
fn get_addr(rng: &mut ChaChaRng) -> SocketAddr {
let a = rng.random_range(1..255);
let b = rng.random_range(1..255);
let c = rng.random_range(1..255);
let d = rng.random_range(1..255);
let addr_str = format!("{a}.{b}.{c}.{d}:80");
addr_str.parse().expect("Invalid address")
⋮----
fn test_connection_cache() {
⋮----
// Allow the test to run deterministically
// with the same pseudorandom sequence between runs
// and on different platforms - the cryptographic security
// property isn't important here but ChaChaRng provides a way
⋮----
.unwrap();
⋮----
.map(|_| {
let addr = get_addr(&mut rng);
connection_cache.get_connection(&addr);
⋮----
let map = connection_cache.map.read().unwrap();
assert!(map.len() == MAX_CONNECTIONS);
addrs.iter().for_each(|addr| {
let conn = &map.get(addr).expect("Address not found").get(0).unwrap();
let conn = conn.new_blocking_connection(*addr, connection_cache.stats.clone());
assert_eq!(
⋮----
let addr = &get_addr(&mut rng);
connection_cache.get_connection(addr);
let port = addr.port();
let addr_with_quic_port = SocketAddr::new(addr.ip(), port);
⋮----
let _conn = map.get(&addr_with_quic_port).expect("Address not found");
⋮----
fn test_overflow_address() {
⋮----
ConnectionCache::new("connection_cache_test", connection_manager, 1).unwrap();
let conn = connection_cache.get_connection(&addr);
assert_ne!(port, 0u16);
assert_eq!(BlockingClientConnection::server_addr(&*conn).port(), port);

================
File: connection-cache/src/lib.rs
================
pub mod client_connection;
pub mod connection_cache;
pub mod connection_cache_stats;
pub mod nonblocking;
⋮----
extern crate solana_metrics;

================
File: connection-cache/Cargo.toml
================
[package]
name = "solana-connection-cache"
description = "Solana Connection Cache"
documentation = "https://docs.rs/solana-connection-cache"
version = { workspace = true }
authors = { workspace = true }
repository = { workspace = true }
homepage = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[features]
agave-unstable-api = []

[dependencies]
async-trait = { workspace = true }
bincode = { workspace = true }
crossbeam-channel = { workspace = true }
futures-util = { workspace = true }
indexmap = { workspace = true }
indicatif = { workspace = true, optional = true }
log = { workspace = true }
rand = { workspace = true }
rayon = { workspace = true }
solana-keypair = { workspace = true }
solana-measure = { workspace = true }
solana-metrics = { workspace = true }
solana-time-utils = { workspace = true }
solana-transaction-error = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true, features = ["full"] }

[dev-dependencies]
agave-logger = { workspace = true }
rand_chacha = { workspace = true }
solana-net-utils = { workspace = true }

================
File: CONTRIBUTING.md
================
# Solana Coding Guidelines

The goal of these guidelines is to improve developer productivity by allowing
developers to jump into any file in the codebase and not need to adapt to
inconsistencies in how the code is written. The codebase should appear as if it
had been authored by a single developer. If you don't agree with a convention,
submit a PR patching this document and let's discuss! Once the PR is accepted,
*all* code should be updated as soon as possible to reflect the new
conventions.

## First Time Contributors

First time contributors should read through this guide in its entirety before
opening a pull request. Additionally, pull requests from external (and
especially first time) contributors that make inconsequential changes may be
closed without merging at the discretion of Agave maintainers. Pull requests
will be deemed consequential or not on a case by case basis. As an example,
spelling and/or grammar fixes will almost always be considered inconsequential,
unless they materially correct the message.

## Pull Requests

Small, frequent PRs are much preferred to large, infrequent ones. A large PR is
difficult to review, can block others from making progress, and can quickly get
its author into "rebase hell". A large PR oftentimes arises when one change
requires another, which requires another, and then another. When you notice
those dependencies, put the fix into a commit of its own, then checkout a new
branch, and cherry-pick it.

```bash
$ git commit -am "Fix foo, needed by bar"
$ git checkout master
$ git checkout -b fix-foo
$ git cherry-pick fix-bar
$ git push --set-upstream origin fix-foo
```

Open a PR to start the review process and then jump back to your original
branch to keep making progress. Consider rebasing to make your fix the first
commit:

```bash
$ git checkout fix-bar
$ git rebase -i master <Move fix-foo to top>
```

Once the commit is merged, rebase the original branch to purge the
cherry-picked commit:

```bash
$ git pull --rebase upstream master
```

Any changes that break consensus must be behind a feature gate and must have
a merged SIMD.

All changes should have unit and integration tests that cover at least 90% of
added code paths. These tests should run quickly and not be flaky.

All changes should be stress-tested with relevant test cases, if relevant test cases
are not present, then write them.

All changes should be benchmarked and evidence posted to the PR.
Microbenchmark results along with mainnet/testnet validator timings or profiles,
bench-tps or other relevant integration benchmarks. Any code that adds complexity
should be justified by a comisurate improvement in speed.

All changes should be reviewed by subject matter experts.

All changes should be merged to master branch first and only critical changes should
be backported to release branches.

Duplicate code should generally be avoided.

Features should be activated on testnet before mainnet in the closest configuration to mainnet as possible
Relevant metrics need to be monitored and appropriate follow-up given after feature activation.

Avoid “hack” or “one-off” solutions, prefer well-architected designs which are not fragile.

Only use unwrap() in cases where you can prove it will never panic, and in cases where panic on
unwrap() is desirable, prefer .expect().

Prefer not to break semver, if absolutely necessary increment the appropriate versioning.

Add a changelog entry for any appropriate changes that add features that should be called
out in the release notes for new versions.

Don't mix refactoring changes and logical changes together.

### How big is too big?

If there are no functional changes, PRs can be very large and that's no
problem. If, however, your changes are making meaningful changes or additions,
then about 1,000 lines of changes is about the most you should ask a Solana
maintainer to review.

### Should I send small PRs as I develop large, new components?

Add only code to the codebase that is ready to be deployed. If you are building
a large library, consider developing it in a separate git repository. When it
is ready to be integrated, the repository maintainers will work with you to decide
on a path forward. Smaller libraries may be copied in whereas very large ones
may be pulled in with a package manager.

## Getting Pull Requests Merged

There is no single person assigned to watching GitHub PR queue and ushering you
through the process. Typically, you will ask the person that wrote a component
to review changes to it. You can find the author using `git blame` or asking on
Discord.  When working to get your PR merged, it's most important to understand
that changing the code is your priority and not necessarily a priority of the
person you need an approval from. Also, while you may interact the most with
the component author, you should aim to be inclusive of others. Providing a
detailed problem description is the most effective means of engaging both the
component author and other potentially interested parties.

Consider opening all PRs as Draft Pull Requests first. Using a draft PR allows
you to kickstart the CI automation, which typically takes between 10 and 30
minutes to execute. Use that time to write a detailed problem description. Once
the description is written and CI succeeds, click the "Ready to Review" button
and add reviewers. Adding reviewers before CI succeeds is a fast path to losing
reviewer engagement. Not only will they be notified and see the PR is not yet
ready for them, they will also be bombarded with additional notifications
each time you push a commit to get past CI or until they "mute" the PR. Once
muted, you'll need to reach out over some other medium, such as Discord, to
request they have another look. When you use draft PRs, no notifications are
sent when you push commits and edit the PR description. Use draft PRs
liberally.  Don't bug the humans until you have gotten past the bots.

### What should be in my PR description?

Reviewing code is hard work and generally involves an attempt to guess the
author's intent at various levels. Please assume reviewer time is scarce and do
what you can to make your PR as consumable as possible. Inspired by techniques
for writing good whitepapers, the guidance here aims to maximize reviewer
engagement.

Assume the reviewer will spend no more than a few seconds reading the PR title.
If it doesn't describe a noteworthy change, don't expect the reviewer to click
to see more.

Next, like the abstract of a whitepaper, the reviewer will spend ~30 seconds
reading the PR problem description. If what is described there doesn't look
more important than competing issues, don't expect the reviewer to read on.

Next, the reviewer will read the proposed changes. At this point, the reviewer
needs to be convinced the proposed changes are a *good* solution to the problem
described above.  If the proposed changes, not the code changes, generates
discussion, consider closing the PR and returning with a design proposal
instead.

Finally, once the reviewer understands the problem and agrees with the approach
to solving it, the reviewer will view the code changes. At this point, the
reviewer is simply looking to see if the implementation actually implements
what was proposed and if that implementation is maintainable. When a concise,
readable test for each new code path is present, the reviewer can safely ignore
the details of its implementation. When those tests are missing, expect to
either lose engagement or get a pile of review comments as the reviewer
attempts to consider every ambiguity in your implementation.

### The PR Title

The PR title should contain a brief summary of the change, from the perspective
of the user. Examples of good titles:

* Add rent to accounts
* Fix out-of-memory error in validator
* Clean up `process_message()` in runtime

The conventions here are all the same as a good git commit title:

* First word capitalized and in the imperative mood, not past tense ("add", not
  "added")
* No trailing period
* What was done, whom it was done to, and in what context

### The PR Problem Statement

The git repo implements a product with various features. The problem statement
should describe how the product is missing a feature, how a feature is
incomplete, or how the implementation of a feature is somehow undesirable. If
an issue being fixed already describes the problem, go ahead and copy-paste it.
As mentioned above, reviewer time is scarce. Given a queue of PRs to review,
the reviewer may ignore PRs that expect them to click through links to see if
the PR warrants attention.

### The Proposed Changes

Typically the content under the "Proposed changes" section will be a bulleted
list of steps taken to solve the problem. Oftentimes, the list is identical to
the subject lines of the git commits contained in the PR. It's especially
generous (and not expected) to rebase or reword commits such that each change
matches the logical flow in your PR description.

### The PR / Issue Labels

Labels make it easier to manage and track PRs / issues.  Below some common labels
that we use in Solana.  For the complete list of labels, please refer to the
[label page](https://github.com/anza-xyz/agave/issues/labels):

* "feature-gate": when you add a new feature gate or modify the behavior of
an existing feature gate, please add the "feature-gate" label to your PR.
New feature gates should also always have a corresponding tracking issue
(go to "New Issue" -> "Feature Gate Tracker [Get Started](https://github.com/anza-xyz/agave/issues/new?assignees=&labels=feature-gate&template=1-feature-gate.yml&title=Feature+Gate%3A+)")
and should be updated each time the feature is activated on a cluster.

* "automerge": When a PR is labelled with "automerge", the PR will be
automatically merged once CI passes.  In general, this label should only
be used for small hot-fix (fewer than 100 lines) or automatic generated
PRs.  If you're uncertain, it's usually the case that the PR is not
qualified as "automerge".

* "good first issue": If you happen to find an issue that is non-urgent and
self-contained with moderate scope, you might want to consider attaching
"good first issue" to it as it might be a good practice for newcomers.

### When will my PR be reviewed?

PRs are typically reviewed and merged in under 7 days. If your PR has been open
for longer, it's a strong indicator that the reviewers aren't confident the
change meets the quality standards of the codebase. You might consider closing
it and coming back with smaller PRs and longer descriptions detailing what
problem it solves and how it solves it. Old PRs will be marked stale and then
closed automatically 7 days later.

### How to manage review feedback?

After a reviewer provides feedback, you can quickly say "acknowledged, will
fix" using a thumb's up emoji. If you're confident your fix is exactly as
prescribed, add a reply "Fixed in COMMIT\_HASH" and mark the comment as
resolved. If you're not sure, reply "Is this what you had in mind?
COMMIT\_HASH" and if so, the reviewer will reply and mark the conversation as
resolved. Marking conversations as resolved is an excellent way to engage more
reviewers. Leaving conversations open may imply the PR is not yet ready for
additional review.

### When will my PR be re-reviewed?

Recall that once your PR is opened, a notification is sent every time you push
a commit.  After a reviewer adds feedback, they won't be checking on the status
of that feedback after every new commit. Instead, directly mention the reviewer
when you feel your PR is ready for another pass.

### Is your PR easy to say "yes" to?

PRs that are easier to review are more likely to be reviewed. Strive to make
your PR easy to say "yes" to.

Non-exhaustive list of things that make it *harder* to review:

* Additional changes that are orthogonal to the problem statement and proposed
  changes. Instead move those changes to a different PR.
* Renaming variables/functions/types unnecessarily and/or without explanation.
* Not following established conventions in the function/module/crate/repo.
* Changing whitespace: moving code and/or reformatting code. Make such changes
  in a separate PR.
* Force-pushing the branch unnecessarily; this makes it harder to track any
  previous comments on specific lines of code, and also harder to track changes
  already reviewed from previous commits.
  * When force-pushing is required—for example to handle a merge conflict—and
    no new changes have been made since the previous review, indicating as such
    is beneficial.
 * Not responding to comments from previous rounds of review. Follow the
   guidance in [How to manage review feedback?](#how-to-manage-review-feedback).

Non-exhaustive list of things that make it *easier* to review:

* Adding tests for all new/changed behavior.
* Including in the PR's description any non-automated testing that was
  performed.
* Including relevant results for changes that target performance improvements.

Note that these lists are *independent* of how simple/complicated the actual
*code* changes are.

## Draft Pull Requests

If you want early feedback on your PR, use GitHub's "Draft Pull Request"
mechanism. Draft PRs are a convenient way to collaborate with the Agave
maintainers without triggering notifications as you make changes. When you feel
your PR is ready for a broader audience, you can transition your draft PR to a
standard PR with the click of a button.

Do not add reviewers to draft PRs.  GitHub doesn't automatically clear
approvals when you click "Ready for Review", so a review that meant "I approve
of the direction" suddenly has the appearance of "I approve of these changes."
Instead, add a comment that mentions the usernames that you would like a review
from. Ask explicitly what you would like feedback on.

## Rust coding conventions

* All Rust code is formatted using the latest version of `rustfmt`. Once
  installed, it will be updated automatically when you update the compiler with
`rustup`.

* All Rust code is linted with Clippy. If you'd prefer to ignore its advice, do
  so explicitly:

  ```rust
  #[allow(clippy::too_many_arguments)]
  ```

  Note: Clippy defaults can be overridden in the top-level file `.clippy.toml`.

* For variable names, when in doubt, spell it out. The mapping from type names
  to variable names is to lowercase the type name, putting an underscore before
each capital letter. Variable names should *not* be abbreviated unless being
used as closure arguments and the brevity improves readability. When a function
has multiple instances of the same type, qualify each with a prefix and
underscore (i.e. alice\_keypair) or a numeric suffix (i.e. tx0).

* For function and method names, use `<verb>_<subject>`. For unit tests, that
  verb should always be `test` and for benchmarks the verb should always be
`bench`. Avoid namespacing function names with some arbitrary word. Avoid
abbreviating words in function names.

* As they say, "When in Rome, do as the Romans do." A good patch should
  acknowledge the coding conventions of the code that surrounds it, even in the
case where that code has not yet been updated to meet the conventions described
here.


## Terminology

Inventing new terms is allowed, but should only be done when the term is widely
used and understood. Avoid introducing new 3-letter terms, which can be
confused with 3-letter acronyms.

[Terms currently in use](https://solana.com/docs/terminology)


## Design Proposals

This Agave validator client's architecture is described by docs generated from markdown files in the `docs/src/`
directory and viewable on the official [Agave Validator Client](https://docs.anza.xyz) documentation website.

Current design proposals may be viewed on the docs site:

1. [Accepted Proposals](https://docs.anza.xyz/proposals/accepted-design-proposals)
2. [Implemented Proposals](https://docs.anza.xyz/implemented-proposals/implemented-proposals)

New design proposals should follow this guide on [how to submit a design proposal](./docs/src/proposals.md#submit-a-design-proposal).





================================================================
End of Codebase
================================================================
